journal of artificial intelligence research                  

submitted        published      

improving delete relaxation heuristics through explicitly
represented conjunctions
emil keyder

emilkeyder gmail com

jorg hoffmann

hoffmann cs uni saarland de

saarland university
      saarbrucken  germany

patrik haslum

patrik haslum anu edu au

the australian national university   nicta
canberra act       australia

abstract
heuristic functions based on the delete relaxation compute upper and lower bounds
on the optimal delete relaxation heuristic h    and are of paramount importance in both
optimal and satisficing planning  here we introduce a principled and flexible technique
for improving h    by augmenting delete relaxed planning tasks with a limited amount of
delete information  this is done by introducing special fluents that explicitly represent
conjunctions of fluents in the original planning task  rendering h  the perfect heuristic h
in the limit  previous work has introduced a method in which the growth of the task is
potentially exponential in the number of conjunctions introduced  we formulate an alternative technique relying on conditional effects  limiting the growth of the task to be linear
in this number  we show that this method still renders h  the perfect heuristic h in the
limit  we propose techniques to find an informative set of conjunctions to be introduced in
different settings  and analyze and extend existing methods for lower bounding and upperbounding h  in the presence of conditional effects  we evaluate the resulting heuristic
functions empirically on a set of ipc benchmarks  and show that they are sometimes much
more informative than standard delete relaxation heuristics 

   introduction
planning as heuristic search is one of the most successful approaches to planning  some
of the most informative heuristic functions for domain independent planning are obtained
as the estimated cost of the delete relaxation of the original planning task  the delete
relaxation simplifies planning tasks by assuming that every variable value  once achieved 
persists during the execution of the rest of the plan  the cost of an optimal plan for the
resulting relaxed planning task  denoted h    is np complete to compute  however whether
some plan for the delete relaxed task exists can be checked in polynomial time  bylander 
       for satisficing planning  where the heuristic does not have to be admissible  the
latter fact can be exploited to upper bound h    by generating some not necessarily optimal
plan for the delete relaxed task  hoffmann   nebel         for optimal planning  lowerbounding methods have been devised based on the analysis of landmarks  logical formulas
over the set of actions that state necessary properties of delete relaxed plans  karpas  
c
    
ai access foundation  all rights reserved 

fikeyder  hoffmann    haslum

domshlak        helmert   domshlak         these cost estimates for the delete relaxed
task can then be used to guide heuristic search in the state space of the original task 
since delete relaxation heuristics were first proposed  bonet   geffner         much
work has been done to improve them  one approach focuses on better approximation
schemes for h    obtaining tighter upper bounds and thus better non admissible estimates
 hoffmann   nebel        keyder   geffner               or tighter lower bounds that
correspond to more informative admissible heuristics  helmert   domshlak        bonet
  helmert         in many domains  however  it is important that the heuristic be able to
take into account delete information  hoffmann         and indeed there is a long tradition
of works proposing heuristics that do so  several of these extend the delete relaxation to
capture strictly more information  fox   long        helmert        helmert   geffner 
      cai  hoffmann    helmert        katz  hoffmann    domshlak         while some
consider only the delete relaxation but attempt to find low conflict relaxed plans  baier  
botea         or generate modified heuristic values based on taking conflicts into account
to some extent  do   kambhampati        gerevini  saetti    serina         here  we
approach this problem by taking inspiration from the admissible hm family of heuristics
 haslum   geffner         an important property of the heuristics we introduce  shared
with some other recent work in this direction  is that our technique renders h  the perfect
heuristic h in the limit  in other words  the technique offers a trade off between the amount
of delete information considered and the computational overhead of doing so  at one end
of that continuum  delete relaxed plans become plans for the original task 
the hm heuristic function considers the cost of making true simultaneously sets of fluents
of size  m  the cost of the planning task is then estimated by recursively taking the cost
of a set of fluents  such as the goal or a set of action preconditions  to be the cost of its
most costly subset of size  m  and ignoring the cost of achieving the remaining fluents in
the set  as each possible subset of size m of the fluents in the task must be considered  the
size of the representation required to compute hm is exponential in m  the hm heuristics
provide the guarantee that there exists an m such that hm   h  trivially satisfied when m
is the total number of fluents in the task   however  the value of m required to achieve this
is usually so large as to make computing h with this method infeasible in practice 
the hm heuristic has recently been recast as the hmax   h  cost of a planning task m
with no deletes  haslum         this is achieved by representing conjunctions of fluents c
of size  m in the original task with new fluents c   here called  fluents  and modifying
the initial state  goal  and operators of the planning task so as to capture the reasoning
performed by hm over these sets within the computation of hmax   however  h   m   is
not admissible  since a separate copy of the same action may be needed to establish each
 fluent   and thus the m compilation is not useful for obtaining admissible estimates that
are more informative than h    the more recent c construction  haslum        fixes this
issue  introducing an action copy for every subset of  fluents that may be established   at
the cost of growing the task representation exponentially in the number of  fluents rather
than linearly as in the m representation  on the other hand  c offers the possibility of a
more fine grained tradeoff between representation size and heuristic accuracy  by allowing
the choice of an arbitrary set of conjunctions c and corresponding  fluents  which need not
all be of the same size   this stands in contrast to the hm heuristic and the m compilation 
in which all sets or conjunctions of size  m are represented 
   

fiimproving delete relaxation heuristics through explicit conjunctions

haslum        proposed to repeatedly solve c optimally  within an iterative procedure
that adds new conjunctions to the set c in each iteration  the relaxed plans that are
computed therefore gradually become closer  in some sense  to being plans for the original
task  we instead explore the idea of using this kind of construction for obtaining heuristic
functions for guiding search 
c
we introduce a related construction c
ce that is similar to    but that makes use of
conditional effects to limit the growth of the task to be worst case linear  rather than
exponential  in  c   this gain in size comes at the price of some information loss relative to
c   however  as we show  this information loss does not affect the fundamental property
of tending towards the perfect heuristic h if enough conjunctions are introduced  like
c    c
ce is perfect in the limit  i  e  there always exists a set of conjunctions c such that

h   c
ce     h   furthermore  while information may be lost  this is not always the case 
indeed  it is possible to construct families of planning tasks for which c
ce can represent the
 
c
same heuristic function as c for the same set of conjunctions c  i  e   h   c
ce     h     
c
but for which the representation of ce occupies exponentially less space 
having said that  the theoretical advantage of c
ce does not tend to materialize in practice
 or at least in the commonly used benchmarks   while without further optimizations c
indeed grows too quickly to be practical  it turns out that mutex pruning techniques  eliminating compiled actions with conflicting preconditions  are extremely effective at keeping
c
the size of c at bay  we therefore consider both c
ce and    evaluating their usefulness
for devising improved heuristic functions  we focus on two main questions 
 a  how to obtain upper and lower bounds for h  in compiled tasks 
 b  how to choose a set of conjunctions c so as to maximize the information gained from
their addition to the planning task 
in response to question  a   we analyze and extend three state of the art methods for
estimating h    in the satisficing setting  upper bounding h     we consider the problem
of finding low cost relaxed plans that can be scheduled so as to minimize the cost of the
sequence of actions required to trigger a given set of conditional effects  avoiding unnecessary
repeated applications of the same action  this problem has been only scantily addressed
in previous work  here we show that the problem of optimal action scheduling for a given
set of effects is np complete  and generalize the approximation technique used in the ff
planner  hoffmann   nebel        
in the optimal setting  lower bounding h     we consider the lm cut heuristic  helmert
  domshlak        as well as admissible heuristics based on fluent landmarks  karpas
  domshlak         for the former  our findings are mostly negative  first  we show
that even though the introduction of  fluents cannot decrease hmax or h    which lowerand upper bound lm cut  respectively  the lm cut heuristic value can decrease  second 
we show that neither of the two straightforward adaptations of the lm cut algorithm to
problems with conditional effects maintains both admissibility and domination of hmax    for
the latter  we show that c
ce can be used to generate more informative fluent landmarks 
recent work  keyder  richter    helmert        extracts landmarks from the m task 
   a more sophisticated adaptation of lm cut  based on the idea of context splitting  has recently been
proposed by        it maintains both properties 

   

fikeyder  hoffmann    haslum

this allows the discovery of  fluent landmarks corresponding to conjunctive landmarks in
the original task  but suffers due to the large number of  fluents that must be considered 
the c
ce compilation offers the possibility of discovering interesting conjunctive landmarks
of unbounded size  while avoiding growing the size of the compilation unnecessarily 
in response to question  b   we devise a range of strategies depending on the purpose
c
for which the c
ce or  compilation is to be used  most of these are parameterized in terms
of the allowed growth of the compiled task relative to the original task  and thus allow a
trade off between the informativeness of the heuristic and its computational overhead  we
evaluate the resulting heuristics on a wide range of benchmarks from the international planning competition  varying the relevant algorithm parameters to determine their individual
effect on performance  as the results show  in several domains our heuristics are much more
informative than previous ones  leading to significantly improved performance 
we next define the basic concepts  section     before moving on to the formal definition
m
c
of the c
ce compilation and the previously introduced  and  compilations  section    
c
m
c
in section    we analyze ce and its relation to  and  from a theoretical perspective 
section   discusses the practical issues that arise in using the compilations for the purpose
of satisficing planning  and describes the obtained experimental results  while section   does
the same for the case of optimal planning  finally  section   summarizes the main points
of the paper and indicates some possible future research directions 

   preliminaries
our planning model is based on the propositional strips formalization  to which we add
action costs and conditional effects  states and operators are defined in terms of a set f
of propositional variables  or fluents  a state s  f is given by the set of fluents that are
true in that state  a planning task is described by a   tuple    hf  a  i  gi  where f
is a set of such variables  a is the set of actions  i  f is the initial state  and g  f
describes the set of goal states  given by  s   g  s   each action a  a consists of
a   tuple hpre a   add a   del a   ce a i  where pre a   add a   and del a  are subsets of f  
the action has a cost cost a   r 
    by ce a     ce a             ce a n    we denote the set of
conditional effects of action a  each of which is a triple hc a i   add a i   del a i i of subsets of
f   to simplify some of our notations  we require that add a   del a      we do not need
to impose any restrictions on the deletes del a i of conditional effects  because conditional
effects will be used only within the delete relaxation  if ce a     for all a  a   has no
conditional effects  and we say that it is a strips planning task 
an action a is applicable in s if pre a   s  the result of applying it is given by
 
 
s a     s    del a  
del a i      add a  
add a i  
 i c a i s 

 i c a i s 

a plan for s is a sequence
pn of actions    a            an whose application in s results in a goal
state  the cost of  is i   cost ai     is optimal if its cost is minimal among all plans for
s  we will often denote optimal plans with     a plan for i is also called a plan for   or
simply a plan 
a heuristic for  is a function h mapping states of  into r 
    the perfect heuristic

h maps each state s to the cost of an optimal plan for s  a heuristic h is admissible if
   

fiimproving delete relaxation heuristics through explicit conjunctions

h s   h  s  for all s  by h      we denote a heuristic function for  whose value in s is
given by estimating the cost of the corresponding state s  in a modified task     we specify
  in terms of the transformation of    hf  a  i  gi into     hf     a    i     g  i  s  is obtained
by applying to s the same transformation used to obtain i   from i  it is sometimes useful
to make explicit that h is a heuristic computed on  itself  we will denote that by h   
note that the modified task   is used only for the computation of the heuristic function 
in particular  the actual search for a plan is performed in the state space of the original
planning task  
the delete relaxation   of a planning task  is obtained by discarding all delete
effects  formally      hf  a    i  gi  where a     hpre a   add a     ce   a i   a  a   and
ce   a     hc a i   add a i   i   ce a i  ce a    the cost of each action a   a  is the same
as the cost of the corresponding action cost a   the optimal delete relaxation heuristic h 
is defined as the cost h      of an optimal plan for    
we denote the power set of f with p f      c   c  f    in the context of hm  
c
   and c
ce   we refer to fluent subsets c  p f   as sets or conjunctions interchangeably 
throughout the paper  we assume that conjunctions are non unit  i  e    c      
a landmark in a planning task is a logical formula  over the set of fluents f such that
every valid plan  makes  true in some state  hoffmann  porteous    sebastia        
orderings between landmarks are statements about the order in which these states occur 
a natural ordering   n   means that if a state sj satisfies     there is some state si
occurring before sj in which   is satisfied  a necessary ordering   nec   means that  
is always true in the state immediately before the state in which   becomes true  while a
greedy necessary ordering   gn   means that this relationship holds the first time that
  is made true  note that the necessary ordering   nec   implies the greedy necessary
ordering   gn     but not vice versa  a landmark graph g is a directed graph whose
nodes are landmarks  and whose labelled edges correspond to the known orderings between
these landmarks 

   the m   c and c
ce compilations
the m compilation  haslum        was the first technique proposed that made use of
the idea of  fluents that explicitly represent conjunctions in the original task  given a
conjunction c  f   c is a new fluent c   f unique to c  i  e   if c    c  then c    c   
in defining m and the other compilations that we discuss  we use the shorthand x c  
x   c   c  c  c  x   where x  f is a set of fluents  and c  p f   is a set of
conjunctions  in other words  x c consists of the set of fluents x itself  together with new
fluents c whose intention is to represent the conjunctions c  c that are contained in x 
c  x 
definition    the m compilation  given a strips planning task    hf  a  i  gi
and a parameter m  z    m is the planning task hf c   ac   i c   gc i  where c    c   c 
f       c   m   and ac contains a as well as an action ac for each pair a  a  c  c
such that del a   c    and add a   c      and ac is given by del ac       ce ac       and
pre ac      pre a    c   add a   c
add ac     add a    c    c   c  c    add a   c  
   

fikeyder  hoffmann    haslum

the parameter m here indicates the maximum size of the conjunctions to be represented
explicitly in the resulting compiled task  a  fluent is inserted  by definition of f c   cf 
above  for each c  f where      c   m  these c are then added to all fluent sets in the
task  such as the initial state  action preconditions  and goals  containing the associated
set c  furthermore  a linear  in  c   number of representatives of each action a are added
to the task to model the situation in which the elements of c that are not made true by a
are already true before a is applied  and a adds the remaining fluents in c while deleting
none of them  thereby making every fluent in c  and therefore c   true  this compilation
allows the admissible hm cost of the original task to be computed as the hmax cost of this
compiled task 
the non admissibility of h  m     h   m   is due to the construction of the action representatives ac   sets of fluents that are simultaneously made true with a single application
of an action a in  may require several representatives of a to explicitly achieve the same
effect in m   consider for example an action a adding a fluent p in a state in which q and
r are already true  in  this makes the fluents p  q  and r true simultaneously  whereas in
    two different representatives of a are required  one with c    p  q  adding  p q    and
one with c    p  r  adding  p r   
the c compilation solves this problem by instead creating a number of representatives of a exponential in the number of  fluents which may be made true by a  each
of these representatives corresponds to an application of a that makes a set of  fluents
true  haslum         following the above example  separate representatives of a would be
introduced for each of the  fluent sets     p q       p r     and   p q     p r     and the
representative resulting from the last of these could be applied to make the two  fluents
true simultaneously  c also differs from m in that it allows the choice of a set c  p f   
and introduces fluents c for only those c  c  rather than for all subsets of size at most
m  
definition    the c compilation  given a strips planning task    hf  a  i  gi
and a set of non unit conjunctions c  p f    c is the planning task hf c   ac   i c   gc i 
 
where ac contains an action ac for each pair a  a  c    c such that c   c    
    del a   c      add a   c       and
    c  c  c  c   add a   c        c  c     
 

 

 

and ac is given by del ac       ce ac       and
 
 
pre ac      pre a  
 c    add a   c
c  c  
c 

add a      add a    pre a    del a   c   c    c   c    
   there are three differences between our definition and haslums        definition of the the actions in
c   first  haslums definition features delete effects  ensuring that real  non relaxed  plans correspond
to plans in the original task  since we only consider delete relaxations of the compiled task  we can safely
omit these  second  we allow the sets c   used in the construction of actions to contain conjunctions c
 
with c  add a    pre a    del a    and third  add ac   contains the  fluents c where c  pre a    del a  
these latter two differences keep our definitions simpler  the redundant action representatives and
redundant add effects that they cause are easily pruned in practice 

   

fiimproving delete relaxation heuristics through explicit conjunctions

 

the representatives ac of a enforce  for every c   c     that no part of c  is deleted 
 
and that the non added part of c  is true already before ac is executed  constraint    
 
ensures a form of non redundancy  if ac adds a  fluent c    then it also adds all  fluents
c such that c  c    as all fluents in c necessarily become true with the application of
the action  note that  differently than m   add effects in c include  fluents representing
conjunctions of fluents added by the action with prevail fluents  non deleted preconditions  
this is necessary for admissibility of h   the primary purpose of c    but not needed for
the computation of h   the primary purpose of m   
c enumerates all possible subsets of c in constructing the representatives of each
action and therefore grows exponentially in  c   this exponentiality is reminiscent of the
canonical conditional effects compilation used to convert planning tasks with conditional
effects into classical strips planning tasks with exponentially more actions  gazen  
knoblock         the c
ce compilation that we introduce here is the result of applying
roughly the reverse transformation to c   resulting in a closely related planning task with
a linear  in  c   number of conditional effects 
definition    the c
ce compilation  given a strips planning task    hf  a  i  gi
c
c
c
c
and a set of non unit conjunctions c  p f    c
ce is the planning task hf   ace   i   g i
where
c
c
c
c
ac
ce    hpre a    add a    del a    ce a  i   a  a  

and ac is given by
pre ac     pre a c
add ac      add a    pre a    del a   c
del ac     
ce ac      h pre a    c   add a   c    c    i
  c  c  c  del a      c  add a      
rather than enumerating the sets of  fluents that may be made true by an action  c
ce
uses conditional effects to implicitly describe the conditions under which each is made true 
the only information lost in doing so is the information encoded by cross context  fluents
in preconditions  which appear in action representatives in c   but not in the preconditions
c   in
or effect conditions of the corresponding actions in c
ce   for action representatives a
 
c   these are  fluents y  pre ac   where there exists no c  c   s t  y   c   add a   
pre a   in the situation discussed above  for example   q r  is a precondition for the action
representative that adds both  p q  and  p r  in c   but does not appear in the condition
of any conditional effects of the corresponding action in c
ce   since effect conditions are
determined individually for each c   such conditions are never included  we will return to
this below when discussing the theoretical relationship between c and c
ce  
example   consider the strips planning task  adapted from helmert   geffner       
with variables  x            xn   y   initial state i    x    y   goal g    xn    and unit cost actions
a   h   y     i

bi   h xi   y    xi       y   i
   

fikeyder  hoffmann    haslum

for i              n    
the optimal solution to this planning task takes the form b    a  b    a          bn    and has
cost  n   in the delete relaxation of the task  the fact that y is deleted after each application
of bi is ignored  and the optimal plan has cost n 
when a  fluent xi  y is introduced in the c
ce compilation  it is added to the precondition
of the action bi   and new conditional effects ce a i of the form h xi      xi  y     i are created
for action a  no conditional effects are added to any of the b actions  as each deletes y and
therefore cannot be an achiever of any  fluent  this increases the optimal delete relaxation
cost of the task by    as a new instance of a must be added to the relaxed plan to achieve
the newly introduced precondition of bi   if all  fluents of the form  xi  y  are introduced 
the delete relaxation cost of c
ce becomes  n     the optimal cost 
the same set of conjunctions renders the delete relaxation cost of c perfect  i  e  
 n      however  the size of c given that conjunction set is exponential in n  as action
a may in principle achieve any subset of the conjunctions  every such subset c   induces a
 
separate representative ac in ac  
regarding the m compilation  h    hmax      also gives the optimal cost of this task 
however  its computation requires the consideration of  n    fluent pairs  rather than the
linear number of  fluents that need to be introduced in c
ce   as we shall see below  theorem     the example can be easily extended so that m must scale with n for hm to become
c
m
perfect  thus showing an exponential separation between c
ce and both  and   
an important practical optimization for both c and c
ce is mutex pruning  if mutex
information about the original planning task is available  specifically if we are given  some 
m tuples of fluents that are not reachable in conjunction  then we can discard from the
compiled task any action representatives and conditional effects which require such an
m tuple  without losing admissibility of the compilation  namely  the value of h   c  
 respectively h   c
ce    after this mutex pruning is bounded from above by the value of
 
 
 
h   c    respectively h   c
ce    for a larger set c  c of conjunctions  if we include all
m
 fluents of size at most m  then all h mutexes are found  i  e   none of the respective fluents is reachable in the compiled task  exploiting available mutex information allows us
to make the compilation more informed without having to add all these additional  fluents 
helping to keep the compilation small 
another optimization we use is to eliminate dominated preconditions  whenever we add
a fluent c to the precondition of an action  or to the condition of a conditional effect  we
remove from that condition all fluents p  c and  fluents  c    c   c   that is because
achieving c implies achieving these fluents as well  and methods that count their cost
separately  such as  for example  hadd and related heuristics  would incur an overestimation 
note  however  that this does not eliminate the duplication caused by  fluents representing different fluent sets that have a non empty intersection  consider  for example  an
action a with pre a     p  q  r   if c     p  q    q  r    then pre a      p q     q r     and
the cost of achieving q will implicitly be counted twice in the hadd estimate of the cost of
applying a  as a possible solution  we considered replacing overlapping  fluents c   c  with
cc    this  however  did not consistently improve any of the heuristics that we compute
from the compiled tasks 
   

fiimproving delete relaxation heuristics through explicit conjunctions

   theoretical properties of c
ce
 
c
we now discuss some theoretical properties of c
ce   considering the cost h  ce   of its
optimal solutions instead of more practical approximations  note that for c
ce and the version
of c considered here  h    h as no delete effects are present   where only proof sketches
are shown  the full proofs can be found in appendix a  we first show the fundamental and
expected property 

theorem    consistency and admissibility  h   c
ce   is consistent and admissible 
proof  regarding consistency  given s  a such that s a    s  in   we need to show that
 
c
 
  c
 c in c   then
h   c
ce   s   cost a    h  ce   s    let   s   be an optimal plan for s
ce
 c  sc  ac   and c is a task with no
ac     s c   is necessarily a plan for sc in c
ce   as s
ce
deletes  admissibility follows from consistency together with the fact that h   c
ce   s     
on goal states s 
furthermore  the  ideal  delete relaxation lower bound can only improve as we add  fluents 
theorem    h   c
ce   grows monotonically with c  given a planning task  and sets
 
 
c 
c  c of non unit conjunctions  h   c
ce    h  ce   
c
c
 
proof  this follows from the fact that given any plan    ac
            an  for  ce      
 
 
 
c
c
c
c
c
c
a            an constitutes a plan for ce   we show by induction that i  a           ai   
c
 
c 
c   
i c  ac
           ai      c   c  c   c    which shows the result since the goal of ce is g
gc    c   c  c   c      and gc  sc    if  is a valid plan 
 
for i      the induction hypothesis holds since i c   i c    c   c  c   c     by definition 
 
 
 
 
c
c
c
c 
c
 
for i      ac
i is applicable in i  a           ai    since pre ai     pre ai      c   c  c   c   
 
 
 
c
 
c
c c
and i c  ac
           ai     i  a           ai      c   c  c   c   by the induction hypothesis 
c
c
c
c
c
c
for  c   c   i  a           ai     i  a           ai      c  c      either c  add ai  c   which
 
c 
implies c  add ac
i   due to the definition of ce   or there exists some conditional effect
c
 
cej  ac
i     h pre a    c    add a       c    i  since c  c   there must exist a  corresponding
 
c
c 
conditional effect in ce by definition  and its condition must be true in i c  ac
           ai   
by the induction hypothesis 

for the special case where c       theorem   gives us 
 
corollary    h   c
ce   dominates h     given a planning task  and a set of non unit
 
c
 
conjunctions c  h  ce    h    

the domination can be strict  as follows trivially from convergence to h  theorem   below  
we now consider the relationship between the c and c
ce compilations  as mentioned
above  information encoded by cross context preconditions is lost when moving from the
c
exponential c to the linear c
ce   estimates obtained from ce may therefore be inferior to
those obtained from c  
theorem    h   c   dominates h   c
ce    given a planning task  and a set of nonunit conjunctions c  h   c    h   c
  
there are cases in which the inequality is strict 
ce
   

fikeyder  hoffmann    haslum

proof sketch  the standard conditional effects compilation into strips  gazen   knoblock 
c
       applied to c
ce   is equivalent to  except for the presence of cross context preconditions in c   given this  any plan for c is also a plan for c
ce   yet the inverse is not the
c c
c
 
n
 
 
      ac
case  to show the first part  we show by induction that i c  ac
n    i  a            an  ce  
 
where i        ce denotes the result of applying a sequence of actions to the initial state i c in
c
c
ce   since the goal of both tasks is defined as g   this shows the desired result 
the strictness result follows from the fact that it is possible to construct tasks in which
the cross context preconditions discussed above play a role  leading to situations in which
c
there exist plans for c
ce that are shorter than the minimum length plans for   
in the proof of strictness  appendix a   we show a planning task for which the h   c  
value is strictly larger than the h   c
ce   value when c is chosen to be all conjunctions
of size    this implies that there exist tasks in which it is necessary to consider strictly
larger conjunctions in c
ce to obtain equally good heuristic estimates as are obtained with
c
c   this is not necessarily problematic however  as differently from hm   c
ce and  do not
introduce all conjunctions of a given size  and are therefore not exponential in the maximum
size of the conjunctions considered 
c
the advantage of c
ce over  is that it is potentially exponentially smaller in  c   the
above domination therefore must be qualified against this reduction in size  furthermore 
c
ce preserves the ability to compute a perfect heuristic given a sufficiently large set c of
conjunctions  we first consider the equivalent result for c   already proved by haslum
        we provide an alternative proof here that can be conveniently adapted to show the
c
same property for c
ce   the key to our proof for  is the following equivalence between
h   m   and h   c   
lemma   given a planning task   and c    c  p f          c   m   h   m     h   c   
proof sketch  m and c are identical except for their action sets  h  values are computed
by considering only a single add effect at a time  the inequality h   m    h   c   is
 
then easy to see by verifying that  for every add effect c of an action ac in c  unless
 
c  pre ac   and thus is redundant   the action ac in m dominates it  i  e   c  add ac  
 
and pre ac    pre ac    the proof is similar for the inequality h   c    h   m    observing
that for any action ac in m and non redundant add effect  there exists a dominating action
 
ac in c  
theorem    h   c   is perfect in the limit  given a planning task   there exists c
such that h   c     h    
proof  it is known that h      hm    for sufficiently high values of m  haslum  
geffner         and as shown by haslum         hm      h   m    by lemma    for
c    c  p f          c   m   we have h   m     h   c    choosing an appropriate
m and the corresponding c  we thus have that h      hm      h   m     h   c   
together with the fact that h   c    h   c    and since h   c    h    by admissibility
of h   c    the claim follows 
 
c
 
c
to show the same claim for c
ce   all that remains is to relate h     to h  ce   

   

fiimproving delete relaxation heuristics through explicit conjunctions

lemma   given a planning task  and a set of non unit conjunctions c  h   c   
h   c
ce   
c
proof sketch  consider a planning task c
no cc identical to  except that it drops cross 
context  fluents from preconditions  we show that  a  h  c    h   c
no cc    and  b 
 
c
 
c
h  no cc    h  ce   
similarly to the proof of lemma     a  is easy to see by showing that every add effect
 
c    in c   we simply set c    to
c of an action ac in c
no cc is dominated by an action a
the minimal subset of c   that contains c and satisfies condition     of definition    in other
words  we reduce c   to get rid of any cross context  fluents  
 
c
for  b   it suffices to show that h   c
no cc    h  ce    this holds because  for any action
c
 
a in a relaxed plan for ce   if c is the set of conjunctions that are added by conditional
 
effects of a when it is applied in the plan  then the action representative ac in c
no cc has
the same preconditions as a  and can be used to achieve the same set of fluents 

theorem    h   c
ce   is perfect in the limit  given a planning task   there exists c
such that h   c
 
 
h    
ce
proof  choosing an appropriate m and c  we have h      hm      h   m    and 
by lemma    h   m     h   c    with lemma    we get h   c    h   c
ce    since  by
     this shows the claim 
theorem    h   c
 

h
ce
note that  with theorem    theorem   is actually a corollary of theorem    our presentation is chosen to make the relation between the two results  and the role of the two lemmas 
clearer 
the proofs of theorems   and   rely on obtaining perfect hm   which is clearly unfeasible
in general since this involves enumerating all subsets of fluents  and hence all possible states 
in the worst case  however  c and c
ce offer flexibility in allowing us to choose the set
c  while selecting all subsets guarantees a perfect heuristic  this may be achieved with
much less effort  which is especially beneficial when using c
ce whose growth in  c  is linear 

indeed  there are task families for which obtaining h takes exponential effort with hm   and
requires exponentially sized c   yet for which c
ce remains small 
m and c   there exist parameterized task
theorem    expressive power of c
ce vs  h
families k such that

   if hm  k     h  k   then m  k 

c
   h   c
k     h  k   implies that the number of action representatives in k is exponential in k  and

   for any k there exists ck such that  ck    and therefore the number of conditional effects
 
c

in  k  c
ce   is polynomial in k  and  b  h   k  ce     h  k   
proof  members of one such family are given by the combination of k planning tasks of
the type shown in example    each of size k  that share among them the action a and the
fluent y that needs to be made true after each step  k then has k goals  and hm   h iff
m  k 
   

fikeyder  hoffmann    haslum

c
for both c
k and  k  ce to be perfect  k  fluents  xi    y            xik   y  must be introduced for each of the individual subtasks i  leading to a total of k    fluents  if any one of
these  fluents is not present  then the precondition for the action bij in  k  c
ce   or similarly
its representative with c        in c
 
have
only
the
individual
fluent
preconditions
y  xij  
k
and in consequence one of the a actions reestablishing y can be left out of the plan  the
number of conditional effects created in  k  c
ce is linear in the number of  fluents added 
however  the number of action representatives in  k  c is exponential in k  the action a
adds the fluent y  that belongs to all  fluents  and hence a has one representative for each
subset of the  fluents 

when using h   c
ce   in practice  we will not typically be able to choose a c that results
in a perfect heuristic  instead  we try to pick a set c that yields an informative heuristic
without making the size of the representation impractical to work with 

   heuristics for satisficing planning
we now consider the practical issues involved in using c
ce for satisficing planning  section     deals with the extraction of relaxed plans  and section     deals with strategies for
choosing the set of conjunctions c  section     presents our experiments with the resulting
setup 
    relaxed planning with conditional effects
techniques for extracting relaxed plans in the presence of conditional effects have long been
known  hoffmann   nebel         here  we refine and extend those techniques  they are
particularly important in our context as  unlike in most ipc benchmarks  the structure
of the conditional effects in c
ce can be rather complex  involving multiple dependencies
between different actions  and even between different executions of the same action  
non admissible delete relaxation heuristics are typically obtained with a relaxed plan
extraction algorithm  keyder   geffner         the different variants of this algorithm
are characterized by the best supporter function bs   f   a they use  in all cases  bs p 
is an action adding p that minimizes some estimate of the cost of making p true  when
no conditional effects are present  the algorithms compute a set of actions  that can be
scheduled to form a relaxed plan for the planning task  formally  the algorithms construct
a relaxed plan  according to the following equations  keyder   geffner        
 
  
if p  s
 p   
bs p    pre bs p    otherwise
 
 p    
 p 
pp

existing methods for choosing best supporters  such as hadd or hmax   can easily be
extended to conditional effects by treating each conditional effect in the task as a separate
   we remark that similar issues arise in approaches compiling uncertainty into classical planning with
conditional effects  palacios   geffner        bonet  palacios    geffner         so our techniques may
turn out to be useful there as well 

   

fiimproving delete relaxation heuristics through explicit conjunctions

action  in particular  this is the method employed  using hmax   to compute the ff heuristic
function  hoffmann   nebel         more precisely  for each relaxed conditional effect
ce a  
i with condition c a i and add add a i   an action ai with the same add effect add ai    
add a   add a i and precondition pre ai     pre a   c a i is created  the set of effects  g 
as defined by the rules above then forms a relaxed plan  the presence of conditional effects 
however  implies that there is a problem of how to schedule that relaxed plan  different
schedules may require different numbers of action applications  as multiple applications of
a single action a can be avoided by making the conditions of multiple desired effects true
before a given application of a 
for illustration  consider a planning task where an action move briefcase has n conditional effects  each of which conditionally transports an object from location a to location
b if it is inside the briefcase  using the representation above  a distinct moving action is
generated for each conditional effect  so one possible schedule of the relaxed plan repeatedly
puts an object in the briefcase  applies move briefcase  then proceeds to the next object 
this plan is n    steps longer than the optimal relaxed plan  which first places all of the
objects in the briefcase and then applies move briefcase once 
in other words  as a single action execution may trigger several conditional effects at
once  there may exist a relaxed plan of length less than   g    the question then arises of
how to optimally schedule the relaxed plan  minimizing the number of action applications
required  ff uses a simple approximate solution to this problem  that we outline and
improve upon below  but we first note that the problem of scheduling conditional relaxed
plans  scrp  is actually np complete 
theorem    scheduling conditional relaxed plans  let   be a relaxed planning task
with conditional effects and  g  a set of effects that  viewed as a set of independent actions 
constitutes a plan for     deciding whether there exists a sequence of actions of length  k
such that all conditional effects in  g  are triggered is np complete 
proof  membership follows from the fact that given a sequence of k actions  it can easily be
checked in polynomial time whether all conditional effects in  g  are triggered  hardness
follows by reduction from the shortest common supersequence problem  scs   garey  
johnson         a supersequence of a string x   d        dm over the alphabet  is a string
over the same alphabet that belongs to the language l    d          dm    given an
instance of the scs problem with strings x            xn over the alphabet        that asks
whether there exists a supersequence of these strings with length  k  we construct a
planning task with conditional effects    hf  a  i  gi  where
s
 f   ni    yij      j   xi   
 a    a    a     where az          ce az     and ce az   is given by the set of conditional
effects
n  x 
i   
 
 hyij   yij     i   xij   z 
i   j  

 i    y             yn   
 g    y  x              yn xn    
   

fikeyder  hoffmann    haslum

the two actions a  and a  correspond to the addition of the symbols   and   respectively
to the supersequence that is implicitly being constructed  and a fluent yij encodes the fact
that the current string constitutes a supersequence for a prefix xi            xij    it can then
be seen that a valid plan for the planning task must trigger all of the conditional effects
of the task  yet such a sequence of actions with length  k exists iff there is a common
supersequence of x            xn with length  k  this transformation of the scs problem into
a planning task with conditional effects is polynomial  which shows the claim 
note that theorem   does not relate to the  known  hardness of optimal relaxed planning 
we wish only to schedule effects that we have already selected and which we know to form
a relaxed plan  this source of complexity has  as yet  been overlooked in the literature 
given this hardness result  we employ a greedy minimization technique that we call
conditional effect merging  starting with the trivial schedule containing one action execution
for each effect in  g   we consider pairs of effects e  e    g  that are conditional effects of
the same action a  the two effects are merged into a single execution of a if their conditions
can be achieved without the use of either of their add effects  ffs approximation method
applies similar reasoning  but captures only a special case in which this condition holds 
when both e and e  appear in the same layer of the relaxed planning graph  which trivially
implies that the conditions of these effects are independently achievable  however  the
same may also be the case for effects in different layers of the relaxed planning graph  here
we devise a strictly more general technique  capturing this form of independence between
effects using what we call the best supporter graph  bsg  representation of the relaxed plan
 for simplicity  we assume here that the task has a single goal fluent g    which if needed can
be achieved by introducing a new action end whose preconditions are the original goals 
and that adds g    
definition    best supporter graph  given a relaxed planning task   and a best supporter function bs  the best supporter graph is a directed acyclic graph    hv  ei  where
v    g   with  g  as above  e    hv  v   i   p  pre v      v   bs p    each vertex is
labeled with the action whose conditional effect it represents  and each edge is labelled with
the set of preconditions  p   p  pre v      v   bs p   
the nodes of this graph represent conditional effects that appear in the relaxed plan  and
there exists an edge hv  v   i between two nodes if the effect represented by v is the best
supporter of a  pre condition of the effect represented by v      bs being a valid best supporter
function  i  e   the relaxed plan  g  generated by bs being sound  is a sufficient condition
for  being acyclic  and it can easily be shown that any topological sort of  is a sound
relaxed plan  this implies that  if there is no path in  between two conditional effects of
the same action  they can occur as the result of the same action application  and therefore
can be merged into a single occurrence of the action  these nodes are then removed from
the bsg  and a new node is added that represents both effects  combining their incoming
and outgoing edges  this process can be repeated until no further node merges are possible 
the algorithm runs in polynomial time and is sound in that it results in a bsg of which
any topological sort constitutes a relaxed plan for   it does not  however  guarantee an
optimal scheduling of the original plan 
   the edge labels will be used in our procedure choosing the conjunction set c  described in section     

   

fiimproving delete relaxation heuristics through explicit conjunctions

for example  consider again the task where move briefcase has n conditional effects
transporting an object from location a to location b if it is inside the briefcase  the nodes
in the bsg are n put into briefcase oi   actions  one for each object oi    as well as n copies
of move briefcase a  b   one for the conditional effect regarding each object oi    there is
one edge from put into briefcase oi   to the respective copy of move briefcase a  b   labeled
with in briefcase oi    there is therefore no path in the graph from any move briefcase a  b 
node to another  and they will be merged into a single node by the conditional effect merging
algorithm  all topological sorts of that merged bsg correspond to optimal relaxed plans 
    choosing c for relaxed planning
algorithm   shows the main procedure for computing the set of conjunctions c used to form
the c
ce task  the algorithm is applied once  before the start of the search  to the initial
c
state of the planning task  the resulting c
ce  or    task is then used for all subsequent
heuristic evaluations  conditional effect merging is not used during the conflict extraction
phase in any configuration discussed below  i  e   we use the original non merged bsg as
stated in definition   
algorithm    choosing c for relaxed plan heuristics 
c 
   relaxedplan c
ce  
while  not a plan for  and size c
ce     bound do
c   c  findconflicts  
   relaxedplan c
ce  
algorithm   is  at a high level  very similar to the procedure previously introduced for
computing incremental cost lower bounds based on the c construction  haslum        
the algorithm repeatedly generates relaxed plans for the initial state of the current compiled
task  it adds new conjunctions to c based on the conflicts that are found in the current
plan  i  e   based on how the current relaxed plan fails when executed in the original planning
task   the process stops when either no further conflicts can be found  implying that the
current relaxed plan for c
ce is a plan for the original planning task  or when a user specified
c
bound on the size of ce is reached  we will express this bound in terms of the size of c
ce
compared to   see below   we will also sometimes impose a bound on the runtime of the
algorithm 
if no bound is specified  and if findconflicts   returns at least one new conjunction as
long as  is not a plan for   algorithm   is a complete planning algorithm in its own right 
we report results for this usage of the algorithm in our experiments below  if the relaxed
plan generated in each iteration is optimal  algorithm   can be used to compute a sequence
of admissible cost estimates that converges to the optimal plan cost  haslum         our
focus  however  is on the use of c
ce for generating inadmissible heuristic functions  we
therefore use a tractable  non optimal  relaxed planning procedure  and impose a bound
that typically stops algorithm   before a plan for the original task has been found 
it remains to specify the findconflicts procedure  given a relaxed plan that fails to
execute in the original planning task   how to select the set of new conjunctions c  one
   

fikeyder  hoffmann    haslum

answer to this question has been provided by the previous use of algorithm   to compute
plan cost lower bounds  haslum         because our aim is different  computing heuristics
for satisficing search based planning  we make a number of changes to the previously
proposed version of findconflicts  section       summarizes the original procedure  and
section       describes the changes we make to it 
      conflict extraction for incremental plan cost lower bounds
given an optimal relaxed plan that is not a plan for the original planning task  haslums
       version of findconflicts returns a set of conjunctions c that prevents the same relaxed
plan from being a solution in the next iteration  this ensures progress  in the sense that
the cost of the relaxed plan will eventually increase  or prove to be the real plan cost  to
describe the conflict extraction procedure  we need two definitions 
definition    relaxed plan dependency graph  let  be a non redundant plan for
the relaxed planning task     construct a directed graph g    with one node va for each
action a in   plus a node vg representing the goal  let pre v  denote the precondition of
node v  which is pre a  for a node va and g for node vg   g  s  has a directed edge from
va to v   iff pre v     is not relaxed reachable using the set of actions in  minus  a   the
edge is labelled with the subset of pre v     that is relaxed unreachable with these actions  the
relaxed plan dependency graph  rpdg    is the transitive reduction of g    
the rpdg is similar to the bsg above  definition     but encodes the necessary dependencies between actions in a relaxed plan  a path from a node va to a node vb in the
rpdg implies that a precedes b in every valid sequencing of   in this case  vb is said
to be ordered after va   in contrast  the bsg encodes the intentions of the relaxed plan
heuristic  in the form of the chosen best supporters  and may impose orderings that need
not be respected in every valid sequencing of the plan  e  g  if a fluent p is added by another
action in the relaxed plan that is not the best supporter of p   because the relaxed plan is
non redundant  meaning that no action can be removed without invalidating it  there is a
path from every action node in the rpdg to the goal node 
definition    dependency closure  let  be a non redundant plan for the relaxed planning task     and let v and v   be nodes in rpdg    where v   is ordered after v  a simple
q 
q 
qm
dependency path is a path v  v          v   from v to v   in rpdg    where each edge
is labelled by one fluent  chosen arbitrarily  from the edge label in rpdg     whenever
v   is ordered after v  a simple dependency path from v to v   exists   a dependency closure
from v to v   is a minimal  w r t  subset  union of paths  such that     it contains a simple
dependency path from v to v     and     if q is the fluent that labels an edge from a node v   
in the closure  and a is an action with q  add a   where a is not the action associated with
v      then the closure contains a simple dependency path from v to the node corresponding to
a   such a path is guaranteed to exist  
recall that input to findconflicts is a plan    that is valid for the delete relaxation  
but not for the original planning task  when delete effects are considered  because  is
valid for     the preconditions of all actions in   as well as all goals  must be made true at
   

fiimproving delete relaxation heuristics through explicit conjunctions

   
vd
p 
r
   
vf
q 

p
vd

q 

   

qn

vf

pn
vj
qm

 b 

 a 

figure    relaxed plan failure scenarios  wavy edges show deletions of a precondition 
some point  thus  if  fails to solve the original task  it must be the case that some action
d  which we call the deleter  deletes a precondition of some other action f   which we called
the failed action  note that the failed action here can also be the goal  let p  pre f  
be the deleted fluent  the procedure distinguishes two cases  based on the relation between
nodes vd and vf in the rpdg 
in the first case  illustrated in figure    a   vf is ordered after vd   choose a dependency
closure from vd to vf   and let l be the set of fluents labelling the edges in this closure  the
set of conflicts generated is   p  q    q  l    note that p   l  and thus each conflict is a
proper conjunction  
if the first case does not hold  then vd and vf are unordered  they must have a nearest
common descendant node  vj   in the rpdg  so we are in the situation illustrated in figure  
 b   choose a dependency closure from vd to vj   and let l  be the set of fluents labelling
the edges in this closure  likewise  choose a dependency closure from vf to vj   and let l 
be the set of fluents labelling the edges in this closure  the set of conflicts generated is then
  q  q       q  l    q    l    p   
theorem    haslum        theorem    let    a            an be a non redundant plan
for the delete relaxed task   that is not valid for the original task   and let c be a set of
conjunctions extracted by the procedure described above  no action sequence      a             a n
such that each a i is a representative of ai is a valid plan for c  
      changes to conflict extraction for satisficing planning
there are a number of differences between our setting and that of haslum         in
particular  although  fluents are collected only in the initial state  the resulting c
ce task
will be used for heuristic evaluations of all states encountered in the search  and growth
in the size of the c
ce task will incur an overhead on each heuristic evaluation  thus  our
objective is to find a set c that will make the heuristic more accurate across all states 
while keeping the size of c limited  on the other hand  computing non optimal relaxed
plans is computationally far cheaper than optimal relaxed planning  so we can afford more
iterations in algorithm   
therefore  we make the following modifications to the strategy  first  we use the bsg
instead of the rpdg  the necessity of orderings in the latter does not extend beyond
the current  initial  state  and therefore is not useful for our purpose  the bsg is more
representative of the relaxed plans found by the non optimal relaxed planning procedure 
second  we introduce just a single  fluent in each iteration of algorithm    most of the
time  this does cause a new relaxed plan to be found  which allows the algorithm to focus
on finding a small number conflicts that are useful in a wide range of states  the chosen
conflict is  p  qn   in the case depicted in figure    a   and  pn   qm   in that of figure    b  
   

fikeyder  hoffmann    haslum

intuitively  this works better in our setting because the set of all conflicts generated by the
same plan failure tends to be redundant  and thus needlessly grows the size of the task
leading to slow evaluation times without much gain in informativeness 
these changes do not affect the fundamental property of algorithm    that it converges
to a real plan  to show convergence  the only property that findconflicts must have is that
it returns at least one new conjunction whenever  fails to solve the original task  our
variant still gives that guarantee 
lemma   assume we eliminate dominated preconditions   in c   let    a            an be
a non redundant plan for c that is not valid for the original task   and let c be the
conjunction extracted by the procedure described above  then c   c 
proof  this is simply because  in both possible relaxed plan failure scenarios  figure     the
chosen conjunction c    x  y    x  y     p  qn   respectively  x  y     pn   qm    is contained
in the precondition of the failed action f   assuming that c    x  y   c  as we eliminate
dominated preconditions  no action precondition in c contains both x and y  hence  in
that case  c cannot be the chosen conjunction 
theorem    convergence of conflict extraction  assume we eliminate dominated preconditions in c   and algorithm   is run without a size bound  then eventually  will be
a plan for  
proof  follows from lemma   as the set of possible conjunctions is finite 
contrasting theorem   with haslums variant  theorem     the latter gives a stronger
convergence guarantee  only  in the sense that it guarantees a certain minimum progress
is made in each iteration 
lemma    and thus theorem    holds in the same way for c
ce   i  e   when  is a sequence
c
of conditional effects in ce   that  viewed as a set of independent actions  constitutes a nonredundant plan for c
ce   we rely on eliminating dominated preconditions here as that
makes the proof very simple  and we use the technique in practice anyway  we did not
verify whether or not convergence holds also if dominated preconditions are not eliminated 
we conjecture that it does 
since there can be multiple conflicts in the bsg of a relaxed plan  in our experiments
we choose  arbitrarily  one that minimizes the number of conditional effects  or strips
actions  in the case of c   created  we place a bound on the factor x by which c
ce exceeds
the size of the original planning task   precisely  when x      no  fluents or conditional
 
effects are added  and c
ce      resulting in a standard relaxed plan heuristic  for growth
bounds x       fluents are added until the number of conditional effects in the task reaches
 x       a   for c   x limits the total number of actions in the task as a multiple of  a  

   recall that eliminating dominated preconditions means that  whenever we add a fluent c to the precondition of an action  or to the condition of a conditional effect  we remove from that condition all fluents
p  c and  fluents  c    c   c  

   

fiimproving delete relaxation heuristics through explicit conjunctions

example   consider again the strips planning task from example    with variables
 x            xn   y   initial state i    x    y   goal g    xn    and unit cost actions
a   h   y     i

bi   h xi   y    xi       y   i

for i              n    
as previously discussed  setting c    x   y           xn   y   renders the delete relaxation
perfect  i  e   results in the relaxed plan having to re establish y in between every two bactions  exactly that set c is iteratively selected by our procedure 
assuming a best supporter function based on either of hadd or hmax   in the first iteration
of algorithm   the bsg will be 

b 

x 

b 

x 

   

b 

bn 

xn 

bn 

the relaxed plan fails to execute when trying to apply the second action  b    the corresponding failure scenario matches figure    a  

y
b 

b 

x 

the chosen conflict thus is  y  x     with the now non empty set of conjunctions c containing just that single conjunction  the precondition of b  contains  x   y  which must be
established using action a  so that the bsg now takes the form  note that the dominated
preconditions y and xi of b  are eliminated  

b 

x 

a

 x   y 

b 

b 

x 

   

bn 

xn 

bn 

the relaxed plan now fails to execute when trying to apply the fourth action  b    the
corresponding failure scenario is 

y
b 

x 

b 

the chosen conflict now is  y  x     iterating the procedure will  in the same manner  select
exactly the set c above one by one  at the end of which the relaxed plan will solve the original
planning task 
    experiments
we evaluate the impact of using the c
ce compilation in a relaxed plan heuristic in the context of a greedy search  the expected impact of using a heuristic based on the improved
relaxation is two fold  on the one hand  it should make the heuristic more informative 
   

fikeyder  hoffmann    haslum

enabling the search to find plans with fewer node evaluations  on the other hand  there is a
computational overhead associated with the growth of the problem  slowing down heuristic
evaluations  we examine both of these effects individually  as well as their combined influence on coverage  or the set of problems that the planner is able to solve within given time
and memory bounds  which we take to be the main measure of performance 
in this study  we do not consider the objective of producing plans of high quality  as
measured by plan length or cost   that is not because plan quality is unimportant  rather 
the rationale for this decision is methodological  seeking a high quality plan is not the
same problem as seeking to find a plan with minimum search effort  particularly when
quality is measured by non unit action costs  and the requirements on heuristics for the
two problems are quite different  here  we have chosen to focus on one  viz  search efficiency 
as measured by coverage and node evaluations  rather than conflate the two  the choice of
a plain greedy search algorithm is also motivated by this decision  as a consequence  we
treat all actions as having a unit cost of    previous experiments have shown that in the
context of greedy search  distinguishing action costs in heuristic calculation tends to result
in lower coverage  richter   westphal         however  to at least assess the impact of our
heuristics on plan quality  we do report data regarding plan length 
we next describe the experiment setup and baseline  we then discuss heuristic informativeness  computational overhead  the impact of conditional effect merging  the impact on
plan length of using c
ce heuristics  a comparison with other state of the art heuristics for
the same problem  the difference between using the c and c
ce compilations  and finding
plans with no search 
      experiment setup and baseline
the compilation and associated heuristics were implemented in the fast downward planner
 helmert         and used in a greedy best first search  with lazy evaluation and a second
open list  with boosting  for states resulting from preferred operators  the planners were
tested on all of the strips domains from the          editions of the international
planning competition  ipc   for domains from the last two ipcs  only the most recent
sets of instances were used  all experiments were run on opteron      processors with the
settings used in the competition  a memory limit of  gb and a time limit of    minutes 
the baseline planner configuration uses the relaxed plan heuristic  with best supporters
identified by hadd   on the unmodified planning task  i  e   with the growth bound x      
it is a known fact that greedy search  and in particular greedy search with lazy evaluation
and a strong bias towards preferred operators  can be highly sensitive to small changes
in the relaxed plan  even changes that do not alter the heuristic value but rather only the
operators that are preferred  unfortunately  this fact is very rarely taken into account when
heuristics are compared in the context of greedy search  since the introduction of  fluents
alters the structure of the relaxed plan  we believe it is particularly important to determine
whether the resulting differences in planner performance are really due to the relaxed plan
being more  or less  informative 
therefore  as a first step towards accounting for the brittleness of experiments with
greedy heuristic search  we introduce a simple variance measure and use it to decide when the
results of our experiments should be considered significant  variance in the performance
   

fiimproving delete relaxation heuristics through explicit conjunctions

of the baseline planner is measured by randomizing the choice of supporters with equal
hadd values in the construction of the relaxed plan and measuring the maximum deviation
from the results of the baseline planner over five repeated runs  the results are shown in
columns labeled mad  tables   and     for each domain and for the problem set as a
whole  the deviation is defined by the differences in coverage and in the median number of
heuristic evaluations  note that we are not interested in whether randomization helps
or hurts the search  but rather in the magnitude of the variation that it causes  when
c
comparing the results of the planner using heuristics based on c
ce or  under different
growth bounds with the results of the baseline planner  we consider the difference between
them to be significant if it is greater in magnitude than the maximum deviation observed
with randomization of the baseline  this should not be interpreted as significance in the
statistical sense  although  if we assumed that randomization affects all heuristics equally 
we could estimate the probability of the hypothesis of no difference   but simply as setting
a reasonable threshold for what counts as a substantial difference in search performance 

      heuristic informativeness
the comparison of heuristic informativeness is summarized in the right half of table   
which shows the ratio of the median  per domain  over tasks solved by both planners 
number of heuristic evaluations for the baseline planner to that of the planners using the
c
ce  based heuristics  in just under half the domains  the difference in informativeness of the
c
ce  based heuristics compared to the baseline does not exceed the threshold for significance
set by the sensitivity study  shown in the mad column   among the domains where
there is a significant difference  in the majority using the c
ce  based heuristics reduces the
number of node evaluations  indicating that the augmented heuristics are more informative 
in most of these cases  the ratio grows as more  fluents are added  i e   as the growth bound
x is increased  the most drastic example can be seen in the floortile domain  where all
c
ce  based heuristics evaluate four or more orders of magnitude fewer nodes  compared to
the standard delete relaxation heuristic  this allows them to easily solve all of the instances
in the domain  by comparison  no planner in ipc      was able to solve more than   of
the    instances in this domain  in the woodworking domain  c
ce heuristics are more than
two orders of magnitude more informative  but there is no associated increase in coverage
as all tasks are solved by all configurations 
in roughly a third of the domains there is a consistent  or nearly consistent  loss of
informativeness  though in most of them it is not significant  note that this loss of informativeness does not always correlate with a loss in coverage  this can be attributed to different
factors  including the small magnitudes of loss  as well as the fact that the ratio of node
evaluations is taken only over tasks solved by both the planners compared  another issue is
that dramatic coverage losses are often due to the computational overhead incurred by the
c
ce compilation  in particular  in the openstacks and satellite domains  the decrease in the
number of tasks solved with the c
ce  based heuristics matches almost exactly the number of
tasks for which the conflict selection and compilation process fails to complete within the
     seconds allocated per task  we get back to this in the next subsection 
   

fikeyder  hoffmann    haslum

it is worth noting that the quality of c
ce  based heuristics can be highly sensitive to the
precise choice of  fluents used in the compilation   hence  there may exist better policies
for making this choice than the relatively simple one we have used here 
the ho and po colums in table    coverage only  examine the effect of the new
heuristic function  respectively the new preferred operators returned by that function  in
separation  po corresponds to a configuration that uses the relaxed plan from the c
ce task
 built with x       and a timeout of t     s  discussed in section        only to identify
preferred operators  together with the heuristic value from the baseline  x      heuristic 
ho  on the other hand  uses the heuristic values obtained with x       and the preferred
operators from x      interestingly  either heuristic values or preferred operators alone are
sufficient to greatly improve coverage in floortile  the domain in which our techniques have
the greatest impact  both the ho and po configurations are able to solve every instance
of this domain   the effect in other domains is mixed  with both configurations solving
sometimes more  sometimes fewer instances 
      computational overhead
the computational overhead of the c
ce  based heuristics  compared to the standard relaxed
plan heuristic  stems from two sources      the time spent on computing the set of  fluents
to add to the problem  and     the greater overhead of heuristic evaluation in the c
ce task 
table   shows three measures of their impact 
the first four columns  under timeouts  show the number of instances in which the
construction of the c
ce task does not finish within      seconds  while the second set of
four columns  under      sec  shows the number of instances for which the construction
time exceeds    seconds  inclusive of those instances in the first set of columns   note that
this behavior  spending a large amount of time on the c
ce construction without reaching
the growth bound  is partly due to our strategy for selecting  fluents  since we purposely
choose those  fluents which will increase the size of the compiled task the least  while
there are several domains in which construction time frequently exceeds    seconds  this
does not happen in those domains where the c
ce  based heuristic are most informative  such
as floortile and woodworking  this suggests that imposing a time limit on the construction
of the c
ce task will incur only a small loss of informativeness  we present coverage results
for such a strategy  using a    second time limit  in table   below  it is significantly better
than the baseline planner  and compares favourably with other state of the art heuristics 
as expected  in most domains evaluating heuristics on the c
ce task is slower than on the
standard delete relaxation  and tends to slow down more as the growth bound x increases 
due to the larger number of fluents and actions in the compiled planning task  the median
slowdown per domain is typically of the same order as x itself  and exceeds one order of
   indeed  the results reported in our earlier paper  keyder  hoffmann    haslum        show an increase
in informativeness in the barman and parcprinter domains 
   a plausible explanation for this is the behavior with respect to dead end states  intuitively  where the
robot has painted itself into a corner  that are unrecognized under the standard delete relaxation
heuristic  i  e   where a relaxed plan for  exists  it appears that c
ce is highly effective at fixing this
issue  while under x     search encounters millions of states with hff        ho encounters only
ff
few states with hff  c
 c
ce       suggesting that h
ce   prunes dead ends early on   and po encounters
no such states at all  suggesting that hff  c
 
preferred
operators prevent the search from entering the
ce
dead end regions in the first place  

   

fiimproving delete relaxation heuristics through explicit conjunctions

domain
x   mad
airport     
  
barman     
  
blocksworld     
  
depots     
  
driverlog     
  
elevators     
  
floortile     
 
freecell     
  
grid    
 
gripper     
  
logistics       
  
logistics       
  
miconic      
   
mprime     
  
mystery     
  
nomystery     
 
openstacks     
  
parcprinter     
  
parking     
  
pathways     
  
pegsol     
  
pipes notk     
  
pipes tank     
  
psr     
  
rovers     
  
satellite     
  
scanalyzer     
  
sokoban     
  
tidybot     
  
tpp     
  
transport     
  
trucks     
  
visitall     
  
woodwork     
  
zenotravel     
  
total       
    

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

   


coverage
x  
po ho
                   
  
               
               
   
   
   
   
   
  

    
            
   
   
   
   
   
  

                 
                       
     
         
   
   
   
   
   
  

   
   
   
   
   
  

   
   
   
   
   
  

     
        
   
   
   
   
   
   
  

   
   
   
   
   
  

                 
           
           
             
    
        
  
          
   
   
   
   
   
  

     
   
     
  

          
      
   
   
   
   
   
  

   
   
     
  
  
  
           
                
    
        
   
       
  
  
   
   
   
   
   
  

              
     
            
             
   
   
   
   
   
  

   
   
   
   
   
  

               

median node evaluations ratio
x  
   
 
   
 
    
        
        
        
        
    
        
        
        
      
 
        
        
        
        
     
        
        
        
        
             
        
        
        
    
        
        
        
        
              
         
         
         
    
        
        
        
        
             
        
        
        
 
        
        
                 
    
        
        
        
        
             
        
        
        
          
        
        
        
             
        
        
        
             
        
        
        
     
        
        
        
         
             
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
   
        
        
    
        
        
        
        
     
        
        
        
        
    
   
   
   
        
 
   
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
    
        
        
        
        
                
         
          
          
    
        
        
        
        
mad

table    planner coverage and heuristic informativeness using c
ce with varying growth bounds 
without conditional effect merging  coverage shows the number of problems solved for the baseline
configuration  x       and the difference  increase decrease  relative to the baseline for the other
configurations  po uses only the preferred operators obtained from the c
ce compilation with
x       and t     s  returning the x     heuristic value  while ho uses the heuristic values
obtained with x       and t     s  and the preferred operators from x      heuristic informativeness
is measured by the ratio of the per domain median number of node evaluations  comparing the
baseline to our configurations  across those instances solved by both configurations   normalized so
that the smaller value is    that is  an entry m     means that the baseline planner requires m times
as many heuristic evaluations as the other planner  columns labeled mad show the magnitude
of the maximum deviation  in coverage and ratio  from the baseline in our sensitivity study  values
in bold are those that exceed this threshold  and which we therefore consider to be significant 

magnitude only in the floortile domain when x        somewhat surprisingly  there are
domains in which heuristic evaluations become faster as more  fluents are added  a possible
explanation for this is that because we eliminate dominated preconditions  cf  section    
the number of action preconditions decreases and the delete relaxation hypergraph of the
c
ce becomes more graph like as a result 
   

fikeyder  hoffmann    haslum

domain
airport     
barman     
blocksworld     
depots     
driverlog     
elevators     
floortile     
freecell     
grid    
gripper     
logistics       
logistics       
miconic      
mprime     
mystery     
nomystery     
openstacks     
parcprinter     
parking     
pathways     
pegsol     
pipes notank     
pipes tank     
psr     
rovers     
satellite     
scanalyzer     
sokoban     
tidybot     
tpp     
transport     
trucks     
visitall     
woodwork     
zenotravel     
total       

timeouts
x  
           
 
 
 
 

   
  

     sec
x  
 
   
  
  

 
  

 

 

 

 

 

  

  

  

  

 

 

 

 

 

  

 

 

  

  

  

  

  

  
 

  
 

  
 

 
 
 

 
  
 

 
  
 

 
  
 

 

 
 
  
 
 

 
 
  
 
 

 
 
  
 
  

  

   

   

   

 

 

  

  

  

 

 

  

 
  

ratio of median evaluations sec
x  
   
 
   
 
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
         
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
   
   
        
        
        
        
        
        
        
        
                 
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
                 
        
        
   
        
        

table    computational overhead of c
ce   the first set of columns  timeouts  shows the number

of tasks for which the c
ce construction does not finish within the      second time limit  and the
second set       sec  shows the number of tasks for which construction time exceeds    seconds
 inclusive of those in the first set of columns   to improve readability  only non zero entries are
shown  i e   the blank cells in these columns are zeroes   the last set of columns shows the median
 per domain  over commonly solved tasks  ratio of heuristic evaluations per second for the baseline
planner  x      to that of the other planner  x       an entry m     means that the baseline planner
performs m times as many heuristic evaluations per second as the other planner 

      conditional effect merging
in a majority of domains  conditional effect merging slightly increases or does not change
the informativeness of the c
ce  based heuristics  some exceptions to this are the logistics  
and gripper domains  where merging results in a heuristic that is twice as informative
 using the same ratio of median number of evaluations metric as presented in table     the
nomystery domain where it is an order of magnitude more informative  for the problems
that are solved by both heuristics   and the barman domain where it is four times less
informative  in general  higher informativeness occurs in domains in which all tasks are
   

fiimproving delete relaxation heuristics through explicit conjunctions

solved by all planners  so it does not result in increased coverage  indeed  as shown in
table   below  conditional effect merging proves to be detrimental to the overall coverage
c
of the planner using the c
ce  based heuristic  the best ce configuration with conditional
effect merging solves  in total  only   more tasks than the standard relaxed plan heuristic 
while the same configuration without conditional effect merging solves    more tasks 
the runtime overhead of the merging procedure is quite small  as the transitive closure
operation required to check whether there is a path between two nodes of the bsg can be
implemented very efficiently when the graph is known to be directed acyclic  as is the case
here  for x        comparing the c
ce  based heuristic with conditional effect merging to
that without  the ratio of the median number of heuristic evaluations per second  the same
metric as used on the right hand side of table    shows a maximal per domain slow down
of       and an across domain average of       coverage decreases in domains other than
barman therefore appear to be due to the sensitivity of search to small changes in the
heuristic function  rather than due to the time taken to compute that function  
      plan length with c
ce
to determine the effect of using c
ce heuristics on plan quality  we compare the length of the
plans found with c
heuristics
to
those found with x      the standard delete relaxation
ce
heuristic  the plan length measure is equivalent to plan quality in the unit cost setting 
we consider the median ratio of plan length found with the standard delete relaxation
heuristic to that found with the c
ce heuristic  over the set of instances that are solved by
both configurations  table     in general  we do not observe large differences  with the
median ratio staying close to    one notable exception is the blocksworld domain  in which
heuristics based on the c
ce compilation consistently find significantly shorter plans  this
c
results from the ce heuristics ability to deduce implicit ordering constraints in the domain 
avoiding actions that lead to temporary improvements during greedy search but that later
need to be reversed  adding to plan length  c
ce also leads to shorter plans in the gripper 
mprime  and woodworking domains  but tends to result in longer plans in the barman and
grid domains 
      comparison to the state of the art
table   shows coverage for a variety of heuristics and planners  the best configurations
for each of the two compilations with x     achieve better overall coverage results than
the standard relaxed plan heuristic  the best performing heuristics obtained with the c
ce
compilation without conditional effect merging  and the c compilation  give coverages of
     and      respectively  the difference between these and the coverage of the baseline
planner is greater than the significance threshold  these numbers also far exceed the coverage obtained with the hcea heuristic  but fall short of the      instances solved with the
dual heuristic approach used by lama  however  combining lama and the best c
ce  nm
configuration in a portfolio planner that runs lama for      seconds and search with the
c
ce  nm heuristic for     seconds results in a coverage of      out of      solvable problems 
almost all of this difference results from the c
ce  based heuristics superior performance in
the floortile  and to a lesser extent  airport domains 
   

fikeyder  hoffmann    haslum

domain
airport
barman
blocksworld
depots
driverlog
elevators
floortile
freecell
grid
gripper
logistics  
logistics  
miconic
mprime
mystery
nomystery
openstacks
parcprinter
parking
pathways
pegsol
pipesworld
pipesworld
psr
rovers
satellite
scanalyzer
sokoban
tidybot
tpp
transport
trucks
visitall
woodwork
zenotravel

x      
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

x
 
 
    
    
 
 
 
 
 
    
 
    
 
    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
    
    
 
 
    
    
    
    
    
 
    
 
    
 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
 
    

x      
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

x
 
 
    
    
 
 
 
 
 
    
 
    
    
    
 
 
 
    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
    
    
 
 
    
    
    
    
    
 
    
 
 
 
    
    
    
 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
 
    

table    median ratio of length of plans found with x      to the length of plans found with c
ce
with different values of x  over instances solved by both planners  an entry m     means that the
baselines plans are m times longer than those of the other planner  no conditional effect merging
was used 

      c vs  c
ce
given a fixed number of  fluents  the difference in size between the c and the c
ce
compilations is exponential in the worst case  mutex pruning  however  can mitigate much
of the growth of c   consider  for example  an action in the c
ce compilation that has n
different conditional effects  if mutexes are not considered  one would expect the number of
action representatives generated for the same set of  fluents in c to be  n   if  however 
each of the n  fluents generating the conditional effects can be shown to be mutex with
one another  the number of action representatives generated in c is also only n 
we have found in our experiments that this effect leads to much slower growth in c
than what might be expected  consider figure    each point on the graph represents a
   

fiimproving delete relaxation heuristics through explicit conjunctions

domain
airport     
barman     
blocksworld     
depots     
driverlog     
elevators     
floortile     
freecell     
grid    
gripper     
logistics       
logistics       
miconic      
mprime     
mystery     
nomystery     
openstacks     
parcprinter     
parking     
pathways     
pegsol     
pipes notank     
pipes tank     
psr     
rovers     
satellite     
scanalyzer     
sokoban     
tidybot     
tpp     
transport     
trucks     
visitall     
woodwork     
zenotravel     
total       

x  
  
  
  
  
  
  
 
  
 
  
  
  
   
  
  
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    

mad
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

 
  

c
ce
  
  
  
  
  
  
  
  
 
  
  
  
   
  
  
 
  
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    

coverage
c
 nm
c
ce
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
  
  
  
  
  
  
   
   
  
  
  
  
 
  
  
  
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
    

hff
  
  
  
  
  
  
 
  
 
  
  
  
   
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
  
  
    

hcea
  
 
  
  
  
  
 
  
 
  
  
  
   
  
  
 
  
  
  
  
  
  
  
  
  
  
  
 
  
  
  
  
 
 
  
   

lama
  
  
  
  
  
  
 
  
 
  
  
  
   
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    

pf
  
  
  
  
  
  
  
  
 
  
  
  
   
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    

table    comparison of state of the art heuristics for satisficing planning  columns x     and
mad are as in table    column c
ce shows coverage for the best configuration  in terms of overall
coverage  with that compilation when using conditional effect merging  namely x        t       
c
c
ce  nm is the best ce configuration without conditional effect merging  which happens to use the
c
same x and t    is the best c configuration where x      which again happens to use the same
x and t   entries in bold in these columns are those where the difference from the baseline planner
exceeds our threshold for significance  given by the mad column   column pf shows coverage
for a portfolio planner that runs lama for      seconds and c
ce for     seconds 
single problem instance  from the same instance set as before   paired with a value of x 
c
for each of c
ce  x axis  and   y axis   we measure the ratio of growth in  a  over growth
in  f    i  e   the factor by which the compilation increased the size of the action set encoding
 measured by the number of actions in c and by the number of conditional effects in c
ce   
divided by the factor by which the compilation increased the number of fluents  in other
   

fikeyder  hoffmann    haslum

 e   
action set growth uent set growth  c

action set growth uent set growth  c

   
 
   
 
   
 
   
 
   
 

 

   

 

   

 

   

 

   

 

   

action set growth uent set growth  cce

 e   
 e   
      
     
    
   
  
 

 

  

   

   

   

   

   

   

action set growth uent set growth  cce

 a 

 b 

figure    growth in problem size as ratio of growth in  a  to growth in  f    with  a  and without
 b  mutex pruning  each point corresponds to a single instance and value of x  f  x    x is also
shown for reference 

words  we assess the growth of the encoding over the number of conjunctions  c   which in
theory is worst case exponential for c but linear for c
ce  
when mutex pruning is not used  figure    b    the growth of a in c is rapid and the
ratio quickly increases to millions  with mutex pruning  figure    a    the growth in c is
still faster than in c
ce   but the difference is much smaller 
      finding plans with no search
when no growth or time limit is imposed on the construction of the c or c
ce tasks 
algorithm   can be used as a complete planning algorithm  while it is not competitive with
heuristic search methods  it is nevertheless interesting to observe some performance details
of this algorithm in various domains  the coverage obtained with this algorithm using
the c and c
ce compilations  as well as some statistics about the growth of the compiled
c
tasks  are shown in table    the difference between c
ce and  is much more visible here 
since the number of  fluents added is  in general  much larger than in the growth bounded
constructions we have used for heuristic computation  c
ce is able to rapidly add a much
larger number of  fluents  and can therefore find relaxed plans that are solutions to the
c
original planning task as well  overall  c
ce solves     tasks compared to     for    and
solves an equal or greater number of tasks in all except   domains 
considering individual domains  it can be seen that c and c
ce are able to solve all or
almost all of the tasks in certain domains such as logistics    mprime  mystery  parcprinter 
psr  and woodworking  in some of these domains  even the addition of a small amount of
information is sufficient to obtain relaxed plans that are plans for the original task  and the
maximum x values required to solve all tasks are quite low  this is the case in the mprime 
mystery  and woodworking domains  where the maximum required x values are            
and       respectively  in others such as elevators  openstacks  transport  and visitall 
where even the smallest tasks are quite large and many different plans are possible  it is not
possible to introduce enough  fluents to disqualify all of the possible relaxed plans that do
not constitute real plans  and no tasks can be solved 
   

fiimproving delete relaxation heuristics through explicit conjunctions

domain
airport     
barman sat     
blocksworld     
depots     
driverlog     
elevators sat       
floortile sat       
freecell     
grid    
gripper     
logistics       
logistics       
miconic      
mprime     
mystery     
nomystery sat       
openstacks sat       
parcprinter sat       
parking sat       
pathways noneg     
pegsol sat       
pipes notank     
pipes tank     
psr     
rovers     
satellite     
scanalyzer sat       
sokoban sat       
tidybot sat       
tpp     
transport sat       
trucks     
visitall sat       
woodwork sat       
zenotravel     
total       

cov 
  
 
  
  
  
 
 
 
 
  
  
  
   
  
  
 
 
  
 
 
  
  
 
  
  
  
 
 
 
  
 
  
 
  
  
   

min
    
    
    
    
     
     
    
    
    
    
    
    
    
     
    
    
     
    
    
    
    
    
    
     
    
    
    
    

c
ce
max
     
      
     
     
      
     
     
      
     
     
      
    
    
     
     
     
      
      
     
     
      
     
     
     
     
     
    
     

c
med
    
     
    
    
      
     
    
     
    
     
     
    
    
     
    
    
      
     
     
    
    
     
     
     
     
     
    
    

cov 
  
 
  
  
  
 
 
 
 
 
  
  
  
  
  
 
 
 
 
 
 
 
 
  
  
 
  
 
 
 
 
 
 
  
  
   

min
    
    
    
    
     
      
    
    
    
    
    
    
    
     
    
    
     
    
    
    
    
    
    
    
    
    
    

max
      
      
      
     
      
      
     
      
      
      
       
     
    
     
      
      
      
      
       
      
      
      
    
      
      
    
      

med
    
     
     
    
      
      
    
     
     
    
     
    
    
     
     
     
      
     
     
     
     
     
    
     
     
    
     

table    solving planning tasks with no search  table shows coverage for the c and c
ce compilations  and the minimum  maximum  and median values of x for solved tasks 

   heuristics for optimal planning
we now consider admissible heuristics  for optimal planning  section     considers the
lm cut heuristic  showing that there are certain complications that make it difficult to
obtain improved heuristic estimates from both c and c
ce   in section      we consider an
alternative method to lower bound h    namely admissible cost partitioning heuristics based
on the conjunctive landmarks that can be obtained from c
ce   we detail how to choose c
in that setting  and present our experimental results in section     
   

fikeyder  hoffmann    haslum

a 
i

a 
a 

g 

ac
 

g 

g 
i

a 
a 
a 

g 

ac
 

 

g 

g 
 a 

 

 g   g   g   
ac
 

 

 b 

figure    lm cut in   and in the c compilation  for example   
    lm cut
the state of the art admissible approximation of h  is computed by the lm cut algorithm
 helmert   domshlak         a logical approach to obtaining admissible heuristics from
c and c
ce is therefore to apply lm cut to these compilations  unfortunately  it turns out
that there are several serious obstacles to this  before discussing these issues  we first give
a brief description of the lm cut algorithm  and then present the simpler case of the c
compilation  where the additional complication of conditional effects is not present 
before lm cut can be computed on a planning task with no deletes  a simple transformation is first applied that replaces the goal set g with a single goal that can only be achieved
with a goal achievement action whose precondition set is g  and adds a dummy precondition to all actions whose precondition set is empty  lm cut then initializes hlm cut     
and  repeats the following steps until hmax  g  becomes        compute hmax       apply a
precondition choice function  pcf  to each action precondition pre a  that removes from
pre a  all but one of the fluents p  pre a  for which hmax  p  is maximal      construct the
justification graph whose vertices are the fluents and whose arcs are the precondition effect
pairs according to the pcf      find a cut l between the initial state and the goal in the
justification graph  given by the set of actions that enters the goal zone  i  e   the set of
fluents from which the goal can be reached at   cost  and     add costmin    minal cost a 
to the heuristic value hlm cut   and reduce the cost of each a  l by costmin   as proved by
helmert and domshlak         this algorithm has two fundamental properties  namely  i 
admissibility  hlm cut  h    and  ii  domination of hmax   hmax  hlm cut  
while it would be expected that the heuristic obtained in this manner from c would
be strictly more informative than that obtained from the original planning task   this
turns out not to be the case  indeed  the heuristic can become strictly less informative 
example   let    hf  a  i  gi be given by f    g    g    g     a    a    a    a     where
ai   h   gi      i  i     and g    g    g    g     figure     in words   has three goals  each
of which is achievable by a single action  valid plans for  apply each action once in any
order  to make all of the goals true  the cuts found by the lm cut algorithm for this task are
 a      a     and  a     regardless of the pcfs chosen  and the lm cut algorithm therefore
always computes the optimal cost    consider now the c compilation that results from the
set c     g    g    g      f c then contains the single  fluent  g   g   g      and a representative
 
 
of each action ac
i constructed with the sole non empty subset c     g    g    g     of c  the
first cut found by lm cut then contains these three representatives  as each adds the most
expensive goal  g   g   g      for any of the possible pcfs  the next cut is then the last  and
the final heuristic estimate is only   as only two cuts have been found  if  for example  the
   

fiimproving delete relaxation heuristics through explicit conjunctions

 

 

c
precondition choice function chooses g  as the hmax justifier for ac
  and a    and g  as the
 
max
c
h
justifier for a    the cut is  a    a     after that cut  the goal can be reached at   cost
  g  g  g   
via a    a    a          so hmax is   and lm cut stops 

 
c
note that  similarly to h   c
ce    cf  theorem    it is not possible for either h     or
hmax  c   to decrease with the addition of  fluents  and that in this example the hmax
cost of the task actually increases  from   to    with the addition of the  fluent  g   g   g     
however  the type of interactions that are introduced are difficult for the lm cut algorithm
to reason about  resulting in worse admissible bounds in practice  lm cut of course continues to dominate hmax   proving that if a sufficient number of  fluents are added  lm cut
will eventually tend towards the optimal cost of the task 

the weakness pointed out by example   is inherited in the application of the lm cut
algorithm to the c
ce compilation  furthermore  that application involves an additional complication that proves formidable  lm cut is not defined for conditional effects  and therefore
cannot be directly applied to the c
ce task  it turns out that of the two straightforward
adaptations of the algorithm to problems with conditional effects  neither preserves both
properties  i  admissibility and  ii  domination of hmax  
to see why  ii  is at stake  consider a planning task  with a single action a that
has two conditional effects ce a     h p    q   i and ce a     h q    r   i  initial state
 p   and goal  r   we have h       hmax        due to the critical path ha  ai  and the
justification graph considered by lm cut consists of this same sequence  the first cut found
is  a   when the cost of a is reduced  the remaining task has hmax cost    resulting in the
cost estimate hlm cut     
the issue here is that different conditional effects of an action may be part of the same
critical path  a natural approach is therefore to reduce costs per individual conditional
effect  rather than for all of the effects of the action at once  unfortunately  it turns out that
this does not preserve admissibility  i   indeed  as we detail in example    appendix a  
there exist strips tasks  whose c
ce compilations have the following property  there
exists an action a such that reducing its cost globally when it is first encountered in a cut
leads to a heuristic estimate that is less than hmax  c
ce    while treating each of its effects
 
c
separately leads to an estimate greater than h  ce     h    
there is therefore no simple strategy for dealing with conditional effects that preserves
both  i  and  ii  on all planning tasks  since admissibility cannot be sacrificed  we must
reduce costs globally and give up on dominating hmax   as a particular implication of doing
 
c

so  despite theorem   which shows that hmax  c
ce     h  ce   converges to h     such
convergence is not guaranteed for hlm cut  c
ce    this could of course be fixed by using
max hmax   hlm cut   as the heuristic value  yet as hmax is typically not informative  this
strategy is not useful in practice 
as we detail in section     below  on the ipc benchmarks  using a with lm cut computed on either c or c
ce often results in larger search spaces as more  fluents are introduced  in all but a few cases  overall performance is worse with hlm cut  c   or hlm cut  c
ce  
than with hlm cut     it remains an open question whether this can be improved 
   

fikeyder  hoffmann    haslum

    c
ce landmarks
landmarks in planning tasks are formulas  over the set of fluents f that have the property that they are made true in some state during the execution of any valid plan  as
the problem of checking whether even a single fluent is a landmark for a planning task
is pspace complete  approaches to finding landmarks have in the past focused on the
delete relaxation  in which setting whether a fluent is a landmark or not can be checked
in polynomial time  it has recently been shown that the maximum fixpoint solution to a
set of simple recursive equations defines the complete set of single fact delete relaxation
landmarks  in other words those landmark formulas that consist of only a single literal
   p  keyder et al          this solution can be computed by an algorithm that repeatedly updates the set of such landmarks for each fluent and action in the planning task 
until convergence  this method can naturally handle conditional effects by treating them
as independent actions  as described in section     
it has also been shown that these equations can be applied to any and or graph
structure  not necessarily corresponding to the delete relaxation of a planning task  this
insight has been used to obtain landmarks from the m task  single  fluent landmarks
in m correspond to conjunctive landmarks in  that are not necessarily landmarks of the
delete relaxation     this approach suffers  however  from the large number of  fluents
that are considered in m   rendering landmark generation impractical for the m compilations of larger tasks  here we aim to take advantage of the flexibility of the c
ce compilation
to obtain non delete relaxation landmarks for the original task  while considering only a
focused set of  fluents and not all those of a given size m  as before  this allow us to
consider larger conjunctions while keeping the size of the delete relaxation task low 
when using c
ce for landmark finding  to focus the technique and keep its overhead at
bay  we choose the set of conjunctions c so as to guarantee that every  fluent is a landmark
in c
ce  and therefore in the original planning task   this is accomplished by extracting from
the landmark graph sets of landmarks that are simultaneously achieved  
definition    simultaneously achieved landmarks  a set of landmarks ls       
        n   is simultaneously achieved if lc           n is a landmark in  
maximal sets of simultanously achieved landmarks can easily be extracted from a set of
landmarks and orderings  given an initial set of landmarks l and a set of orderings  the
following are sets of sets of simultaneously achieved landmarks 
 lg        g      
 lnec         nec       l 
 lgn         gn       l 
lg contains a single set that is made up of the landmarks in l that are entailed by g  since
any valid plan must make all goals true in its final state  they are necessarily simultaneously
achieved  given a landmark   lnec contains a set which has as its elements all those landmarks  that are ordered necessarily before   due to the definition of necessary orderings 
all of these  must be simultaneously true in every state that immediately precedes a state
in which  becomes true  lgn is a similar set  yet since greedy necessary orderings are
   

fiimproving delete relaxation heuristics through explicit conjunctions

weaker than necessary orderings  can sometimes contain sets that do not appear in lnec  
and therefore result in a larger overall set of conjunctive landmarks  note that all necessary orderings are also greedy necessary orderings  and each conjunctive landmark that
results from a set of necessary orderings is therefore a subset of some conjunctive landmark
that results from greedy necessary orderings  we include conjunctive landmarks that result
from necessary orderings as they result in stronger necessary orderings between the added
conjunctive landmark and   landmark heuristics can then sometimes infer that these
conjunctive landmarks must be reachieved if a landmark that they are ordered necessarily
before has to be reachieved  this is not the case for the conjunctive landmarks derived
from greedy necessary orderings  as they only need to be achieved to make the landmarks
they are ordered before true for the first time 
algorithm    choosing c for landmark generation 
c 
l   findlandmarks c
ce  
repeat
c   c  simultaneouslyachieved l 
l   findlandmarks c
ce  
until simultaneouslyachieved l   c

our strategy for choosing c for landmark generation is shown in algorithm    while
new conjunctive landmarks l   p       pn can be discovered  the corresponding fluents
 p       pn   are added to c
ce and the landmark computation step is repeated  note that the
process may go through several iterations  and is run until a fixpoint is reached  as the
addition of the new  fluents to the c
ce task can result in the discovery of new landmarks 
the process terminates when all the new conjunctive landmarks that are discovered already
exist as  fluents in c
ce   we note that this method of choosing c does have the desired
c
property mentioned above  all  fluents introduced in c
ce represent fact landmarks in ce
and conjunctive landmarks in the original task  
the above strategy works especially well in domains in which many landmarks have several landmarks that are necessarily or greedy necessarily ordered before them  one domain
where this occurs is in blocksworld  see an illustration in figure     where the method is
able to find extremely informative conjunctive landmarks that allow it to optimally solve
more tasks than any other heuristic we tested 
    experiments
we consider the performance of the lm cut heuristic hlm cut on the c and c
ce compilations  and that of the admissible landmark cost partitioning heuristic hlm introduced by
karpas and domshlak        with different landmark generation schemes  including the
lm cut is used with the a search algorithm  while for hlm
landmarks obtained from c
ce   h

we use lm a   a variant which is more effective when there are known fluent landmarks
 karpas   domshlak         the benchmarks  computers  and time memory limits are the
same as those used in in section     
   

fikeyder  hoffmann    haslum

informativeness
coverage
domain
c      
c
 
   
orig 
x
 
 
c       c
ce
ce      
airport
         
         
  
  
  
  
barman opt
        
        
 
 
 
 
blocksworld
        
        
  
  
  
  
depots
        
        
 
 
 
 
driverlog
        
         
  
  
  
  
elevators opt  
        
        
  
  
  
  
floortile opt  
         
         
 
 
  
  
freecell
        
        
  
  
  
 
grid
        
        
 
 
 
 
gripper
   
   
 
 
 
 
logistics  
        
         
  
  
  
  
logistics  
        
         
 
 
 
 
miconic
          
                  
  
  
mprime
         
        
  
  
  
  
mystery
        
        
  
  
  
  
nomystery opt  
          
          
  
  
 
 
openstacks opt  
   
   
  
  
  
  
parcprinter opt  
        
        
  
  
  
  
parking opt  
 
 
 
 
pathways noneg
         
         
 
 
 
 
pegsol opt  
        
        
  
  
  
  
pipes notank
        
        
  
  
  
  
pipes tank
        
        
  
  
 
 
psr
        
        
  
  
  
  
rovers
        
        
 
 
 
 
satellite
        
         
 
 
 
 
scanalyzer opt  
        
        
  
  
 
 
sokoban opt  
        
        
  
  
  
  
tidybot opt  
        
         
  
  
 
 
tpp
        
        
 
 
 
 
transport opt  
        
        
 
 
 
 
trucks
        
         
  
  
 
 
visitall opt  
        
        
  
  
  
  
woodwork opt  
        
        
  
  
 
 
zenotravel
         
          
  
  
 
 
total
       
   
   
c
table    lm cut with c
ce and    the two columns on the left show the ratio of the summed

number of heuristic evaluations for tasks solved by both configurations  comparing the standard
lm cut that results from x     to lm cut computed on c and c
ce with x        for example 
the first entry in the table             shows that lm cut computed on c with a growth bound of
x       evaluates  in sum over the commonly solved tasks  nearly    times as many states as lm cut
computed on the standard delete relaxation  the last   columns show coverage  column original
shows results obtained with fast downwards implementation of lm cut  which applies only to the
standard delete relaxation   while column x     shows the results of our implementation of lmcut on the unmodified delete relaxation  with any differences between the two being purely due to
implementation details   entries in bold indicate the highest coverage in each domain  and in total 

   

fiimproving delete relaxation heuristics through explicit conjunctions

clear a 
gn
clear b 
handempty
ontable b 

clear a 
holding b 

nat

nat

clear b 
handempty
on b  a 
clear c 
handempty
ontable c 

clear b 
clear d 
gn
on b  a 
holding c 
nat ontable d 
gn

on b  a 
on c  b 
on d  c 

nec

clear c 
clear c 
clear d 
on b  a  gn handempty
on c  b 
on b  a 
holding d 
on c  b 
ontable d 

figure    landmarks graph found with the c
ce compilation for a small blocksworld task  in which
all blocks are initially on the table and g    on b  a   on c  b   on d  c    some smaller conjunctive
landmarks and single fluent landmarks are omitted 

      lm cut with c and c
ce
to evaluate the impact of using the c and c
ce compilations on lm cut  we constructed
c
c
the  and ce tasks following the same procedure as described in section      repeatedly
selecting conflicts until the increase in the size of the compiled task reached a fixed growth
bound x  conflict selection was based on hmax supporters rather than hadd supporters  as
hmax plays a key role in the computation of lm cut  and also resulted in better performance 
other than that  the procedure used to generate the c and c
ce tasks was the same  we
c
tested each value of x from the set                  for both  and c
ce   we observed that
x       dominated the larger values of x on a domain by domain and overall basis  and
therefore report results only for these two configurations  the only exception to this is the
mystery domain  in which c with x       and x     solved    tasks compared to    for
x       
overall  the heuristic computed by the lm cut algorithm on the standard delete relaxation   dominates that computed on c and c
ce   both in terms of informativeness and
in terms of coverage  the first two columns of table   show that for the large majority
of domains  search using lm cut computed on c and c
ce performs many more heuristic
evaluations in tasks that are solved by both configurations  in the airport domain  for
instance  lm cut on the standard delete relaxation requires approximately    times fewer
heuristic evaluations to solve the same set of tasks as either c or c
ce   in most other domains  the situation is less extreme  but the standard delete relaxation continues to give the
better heuristic estimates  the exceptions to this are the blocksworld  elevators  floortile 
freecell  grid  mprime  mystery  pegsol  psr  transport and woodworking domains  in
 
which at least one of c or c
ce yields more informative heuristic estimates than    most
impressively  c and c
ce give estimates that are respectively    and    times more informative than the estimates obtained from   in the floortile domain  and estimates using
c are    times more informative than those of   on the mprime domain  in terms of
coverage  this translates into    tasks solved with both c and c
ce in the floortile domain 
compared to   for the standard version of lm cut  and    tasks solved with c in the
   

fikeyder  hoffmann    haslum

mprime domain  compared to     in the mystery domain  coverage is increased by    in all
other domains  the coverage achieved with c and c
ce  based lm cut is less than or equal
to the coverage achieved with standard lm cut  overall  the standard version of lm cut
solves     planning tasks as compared to     for c and     for c
ce   though a large part
of this difference     tasks  comes from the miconic domain  the difference in the remaining
domains is still significant 
comparing c and c
ce   it can be seen that the additional loss of information resulting
from the treatment of conditional effects in lm cut leads to worse heuristic estimates when
c
using c
ce   as expected from the theoretical result that ce grows only linearly with the
number of  fluents  the number of  fluents that are added to the task when using the
c
c
ce compilation is almost always higher than when using    however the treatment of
conditional effects in lm cut  described above  turns out to greatly degrade performance 
c
and lm cut using c
ce is more informative than lm cut using  in only   domains 
      admissible landmark heuristics with c
ce landmarks
the admissible landmark heuristic  hlm   uses action cost partitioning to derive heuristic
values from a collection of  ordered  landmarks  distributing the cost of each action over the
set of landmarks it achieves  karpas   domshlak         cost partitioning can be done in
different ways  optimal cost partitioning is tractable  and yields the best possible heuristic
value for a given set of landmarks  but in practice is so slow that coverage suffers  uniform
partitioning generally achieves a better time informativeness trade off  and therefore better
coverage 
to evaluate the potential informativeness of the landmarks obtained with c
ce using the
iterative technique described in section      we used these landmarks in the optimal cost
partitioning setting  since this setting makes the best possible use of information present
in the given landmarks  we compared their informativeness to that of the heuristic using
landmarks obtained from the m compilation with m     and m     and the sound and
complete landmark generation algorithm  keyder et al          the results are shown in the
first two columns of table    they show the ratio of total number of heuristic evaluations 
per domain  over all tasks solved by all configurations  for hlm using landmarks from   only
to the heuristic using landmarks from   and c
ce   respectively  note that the landmarks
  compilations contain the landmarks obtained from  
generated from both the c
and

ce
as a subset  and that hlm with optimal partitioning over the   or c
ce landmarks therefore
dominates hlm with optimal partitioning over   landmarks  hence the ratio is always
greater than   
in   of the    domains considered  neither the addition of   landmarks nor c
ce landmarks leads to a more informative heuristic  cases in which both columns show the value    
of the remaining    domains  both schemes improve over   landmarks to an equal degree
in    and   improves over   to a greater degree than c
ce in     in one case  blocksworld 
c
landmarks
are
much
more
informative
than
landmarks
found by both the other methce
ods  and improve informativeness over the baseline heuristic using only   landmarks by a
factor of     
uniform cost partitioning divides the cost of each action evenly over the set of landmarks
that it achieves  rather than searching for a partitioning that maximizes the heuristic value
   

fiimproving delete relaxation heuristics through explicit conjunctions

domain

airport
barman opt
blocksworld
depots
driverlog
elevators opt  
floortile opt  
freecell
grid
gripper
logistics  
logistics  
miconic
mprime
mystery
nomystery opt  
openstacks opt  
parcprinter opt  
parking opt  
pathways noneg
pegsol opt  
pipes notank
pipes tank
psr
rovers
satellite
scanalyzer opt  
sokoban opt  
tidybot opt  
tpp
transport opt  
trucks
visitall opt  
woodwork opt  
zenotravel
total

informativeness
coverage
 optimal partitioning   uniform partitioning 
 
c
 
 
c
ce
ce
    
    
  
  
  
 
 
 
     
      
  
  
  
    
    
 
 
 
    
    
  
 
 
    
    
  
  
  
    
    
 
 
 
    
    
  
  
  
    
    
 
 
 
 
 
 
 
 
     
     
  
  
  
    
    
 
 
 
    
    
   
   
   
    
 
  
  
  
    
 
  
  
  
    
    
  
  
  
 
 
  
 
  
    
 
  
 
  
    
 
 
 
 
 
 
 
 
 
    
 
  
  
  
    
    
  
  
  
    
    
  
  
  
    
    
  
  
  
 
 
 
 
 
    
    
 
 
 
    
    
 
 
 
    
 
  
  
  
    
    
  
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
    
    
 
 
 
 
 
 
 
 
   
   
   

table    hlm with landmarks generated from the delete relaxation          keyder et al        
and c
ce   the two columns on the left show the ratio of the summed number of heuristic evaluations
for tasks solved by both configurations  comparing   and c
ce to the baseline of using only landmarks
from     using optimal cost partitioning in hlm   more landmarks can only yield better lower bounds 
and indeed all the ratios are     which is why we do not use a m     presentation  differently from
the previous tables   the right most three columns show coverage  using uniform cost partitioning 
the heuristic with uniform partitioning solves more tasks than with optimal cost partitioning under
all the landmark generation schemes considered  entries in bold indicate best results  per domain
and in total 

   

fikeyder  hoffmann    haslum

in each state  this can make the hlm heuristic weaker  though typically not by much  but
also makes it much faster to compute  leading to better coverage in general  we confirmed
that uniform cost partitioning results in higher coverage than optimal cost partitioning in
all domains for all three of the landmark generation schemes we considered 
the three right most columns in table   show the coverage achieved with hlm and the
three landmark generation schemes in this setting  it can be seen that using only   landmarks results in greater coverage than combining them with either   or c
ce landmarks 
 
c
compared to the heuristic using  landmarks  using ce landmarks solve as many or more
tasks in every domain except the nomystery and rovers domains  c
ce landmarks outperform   landmarks in two domains  blocksworld  where using c
landmarks
the planner
ce
finds optimal solutions to    out of the    tasks  more than any other tested heuristic  and
in miconic  by   instance  in the other domains  the use of c
ce landmarks either has no
 
effect or worsens coverage compared to    interestingly  while the informativeness of the
lm cut heuristic increases greatly with the c and c
ce compilations in the floortile domain  there is no corresponding increase when the compilations are used to find landmarks 
this is because few conjunctive landmarks  besides the goal  are found 

   conclusions and open questions
there is a long tradition of works attempting to devise heuristics taking into account some
delete effects  however  techniques rendering h  perfect in the limit  and thus allowing
to smoothly interpolate between h  and h  have been proposed only quite recently  by
haslum        and katz et al         respectively  we have extended haslums approach
by introducing a new compilation method with linear  vs  worst case exponential  growth 
and demonstrated the machinery needed for using the approach to generate heuristics  our
evaluation shows that  in some domains  informedness can be dramatically improved at a
small cost in terms of computational overhead 
the main open issue lies in our use of the words in some domains here  in most
domains  the gain in informativeness is small  and in some domains overall performance
suffers dramatically  while no domain independent planning technique can work well in
every domain  and while a simple portfolio approach  cf  column pf in table    suffices
to improve the state of the art in satisficing planning  the extent of the per domain performance variation of our technique is dramatic  can we obtain an understanding of what
causes these phenomena  and ultimately exploit that understanding to devise more reliable effective practical methods  is the unchanged or worse performance in many domains
due to fundamental limitations of the technique  or only due to its particular instantiation
 especially the selection of  fluents  as run in our experiments 
from a practical perspective  answering these questions comes down to the exploration
of techniques for predicting the impact of adding  fluents  and for making more informed
decisions about which  fluents to add  we have observed that changes in domain formulation  random reorderings  and small changes to the heuristic criteria used in  fluent
selection can have a large impact on both heuristic informativeness and coverage  further research to formulate new heuristic criteria and improve existing ones therefore could 
potentially  provide better performance across a wide range of domains  it might also be in   

fiimproving delete relaxation heuristics through explicit conjunctions

teresting to systematically explore the impact of random arbitrary changes  and to attempt
building complementary strength compilations to be combined into effective portfolios 
from a theoretical perspective  we are currently approaching the above questions in
terms of analyzing the conditions under which a small  polynomial size  set of  fluents
suffices to render h  perfect  applied to individual domains  this analysis offers a way
of answering the question of whether a lack of performance improvement is due to an
essential limitation or only due to choosing the wrong set of  fluents  our hope is
to eventually obtain syntactic criteria  e  g   based on causal graph structure  that can be
automatically applied to arbitrary planning task descriptions  serving to select the  fluents
 or to exclude subsets of  fluents from consideration  in a targeted manner  our first
results in that direction have already been published in hsdip    hoffmann  steinmetz 
  haslum        
our observations in optimal planning pose many questions for future work  a simple one
is whether more effective c
ce landmarks could be extracted by not restricting our techniques
to adding only  fluents guaranteed to be landmarks  the more daunting challenges regard
lm cut  our observations suggest that the methods we use suffer greatly from suboptimal
choices of precondition choice functions  pcfs   it would therefore be worthwhile to investigate new methods for obtaining better pcfs  another important direction is to develop
extensions of lm cut to conditional effects that guarantee both admissibility and domination of hmax   a simple yet impractical method is to multiply out the conditional effects
 enumerating all subsets thereof   a more sophisticated method based on context splitting  where distinctions between different occurences of the same action are introduced in
a targeted manner only where necessary  has recently been proposed  roger  pommerening 
  helmert        
in summary  explicitly represented conjunctions clearly exhibit the potential to dramatically improve delete relaxation heuristics  but much remains to be done in order to
understand and use them effectively 

acknowledgments
part of the work leading to this publication was carried out while emil keyder and jorg
hoffmann were working at inria grand est  nancy  france  nicta is funded by the
australian government through the department of communications and the australian
research council through the ict centre of excellence program  we thank the university
of freiburg for allowing us to use their computional resources 

appendix a  proofs
theorem    h   c   dominates h   c
ce    given a planning task  and a set of con 
c
 
c
junctions c  h      h  ce    there are cases in which the inequality is strict 
proof  this follows from the fact that any plan for c is also a plan for c
ce   yet the
cn i be a plan for c  
 
inverse is not the case  to show the first part  let    hac
 
 
 
 
 
a
n
 
we show that the same sequence of actions constitutes a plan for c
ce   by showing by
cn    i c  ac           ac     where i        
 
induction that i c  ac
 
 
 
 
 
a
denotes
the result of
ce
n
n ce
 
 
   

fikeyder  hoffmann    haslum

applying a sequence of actions to the initial state i c in c
ce   since the goal in both tasks
is defined to be gc   this shows the desired result  for the base case  the initial state in
c
both c and c
ce is i   and the subset relation holds  for the inductive case  assume
ci 
c 
c
c
c
c
c
i  a            ai     i  ac
            ai   ce   since the precondition of ai in ce is a subset of
c
c
c c
c
i
the precondition of ac
i in  for all a and all ci   ai can be applied in i  a            ai   ce by
c
i
the induction hypothesis  we then need to show that all fluents added by ac
i in  are also
ci
c
c c
c
c
added by ac
i in ce when applied in i  a            ai   ce   the add effect of ai in  consists
c
of the union of two sets   add a    pre a    del a      which is also the add effect of ac
i in
ci
c
 
c
 
ce and therefore added  and  c   c  ci    since ai was applicable in    each of its
s
ci 
 
preconditions  pre a  c  ci  c   add a   c must be true in i c  ac
            ai     and therefore
c
 
in i c  ac
            ai   ce   by the induction hypothesis  for c  c  and therefore c  ci   as ci 
c
c
c   ac
i in ce has a conditional effect with effect c and condition  pre a    c   add a     
ci
c
which applies because its condition is a subset of the precondition of ai in    this shows
the desired property 
for strictness  consider the planning task with fluent set f    p    p    r  g    g     initial
state i    p     goal g    g    g     and actions
ap    h p      p      r  p     i ar   h   r     i
ag    h p    r    g       i ag    h p    r    g       i
let c    c  f    c        the only optimal plan for both  and c is the sequence
har   ag    ap    ar   ag  i  in the case of c this follows from the fact that the plan must include
ag  and ag  as they are the only actions achieving the two goals  and therefore must achieve
their precondition  fluents  p   r  and  p   r    respectively  each of these  fluents can
be achieved only with ar   as no action achieves either of the p fluents without deleting
r  there is no single representative of ar that achieves both  p   r  and  p   r    as such a
representative would have the precondition  p   p      which is unreachable  since the only
action achieving p  deletes p    a plan in c therefore must contain ag    ag    at least two
instances of ar   and ap   
this no longer holds  however  when considering c
ce   for which the action sequence
hap    ar   ag    ag  i is a plan that contains only   actions  in c
ce   the two possible  fluents
added by ar    p   r  and  p   r    are treated independently  and a separate conditional effect
is created for each  with the conditions p  and p  respectively  once p  and p  have been
achieved separately  a single application of the action ar is then sufficient to achieve the
two  fluents  without making true the  unreachable  cross context  fluent  p   p      in this
and similar cases  there exist plans for c
ce that are shorter than the minimum length plans
for c  
given a strips task    hf  a  i  g  costi  the h  heuristic for a set p of fluents  is
defined as follows  bonet   geffner        

 
if p  s
 
h  p   
min a padd a   h   pre a     cost a  otherwise
h   p     max h   p 
pp

   

fiimproving delete relaxation heuristics through explicit conjunctions

the value of the heuristic for a given planning task is taken to be the h  cost of the goal
g  h       h   g  
lemma   given a planning task  and c    c  p f          c   m   h   c     h   m   
proof  let    hf  a  i  gi  m and c are identical except for the action sets  we
denote the action set of m by ac  m   and that of c by ac  c    there are no deletes
and conditional effects in either of ac  m   and ac  c    so we will ignore these in what
follows 
we first show that h   c    h   m    then that h   m    h   c    each direction
is based on the following two observations  first  for any strips planning task  we can
split up the actions over their singleton add effects  without affecting h    precisely  given
an action a and p  add a    pre a   we denote by a p  the action where pre a p     pre a 
and add a p      p   replacing each a with all its split up actions a p   i  e  generating a
split up action a p  for every non redundant add effect of a   h  remains the same  second 
say that every split up action a p  in action set a is dominated by an action a  in action set
a    i  e   pre a     pre a p   and add a     add a p    then h  using a  is a lower bound on
h  using a 
we now prove that h   c    h   m    for every a  a and c  f so that      c   m 
del a   c     and add a   c      ac  m   contains the action ac given by pre ac    
 pre a    c   add a   c   and add ac     add a    c    c   c  c    add a   c    let
 
p  add ac    if p  add a   then ac for c      dominates ac  p   say p   c  where
c    pre ac    to obtain a dominating action in ac  c    we define 
c       c    c   del a   c       add a   c        c    c   
we have c    c  and for all c    c   the conditions     del a   c       add a   c       and
    c  c     c  c    add a   c        c  c     of definition   are
s obviously satisfied 
 
 
thus ac  c   contains the action ac given by pre ac      pre a   c   c    c     add a   c
 
and add ac      add a    pre a    del a   c   c     c    c      we now prove that  a 
 
 
pre ac    pre ac   and  b  p   c   add ac   
regarding  a   for every c    c   we have
s
c     add a   c    add a   c   add a   and thus c   c    c     add a    c    add a   c   add a  
regarding  b   we need to prove that c   c  del a c      add a c       and c   c    the
first and the last of these properties are obvious  the second one is direct from construction 
as for the third one  add a   c       this is true because otherwise we would have c  
c   add a  implying in contradiction to construction that c   pre ac   
it remains to prove that h   m    h   c    for every a  a and c    c with conditions
 
 
    and     as stated above  ac  c   contains the action ac   let p  add ac    if p is not
a  fluent  then either p  add a  or p  pre a    del a   the latter case is irrelevant  and
 
no split up action is generated   in the former case  setting c    add a  we get that ac  p 
is dominated by ac in ac  m    say p   c   then at least one of the following cases
must hold   a  c  c   or  b  c   pre a    del a   or  c  c   add a    pre a    del a   
and c  add a       in case  a   it follows directly by definition that ac  ac  m   which
 
 
dominates ac  p   in case  b   p   c  pre ac   so that case is irrelevant  in case  c  
 
c  add a      and c  del a     so again ac  ac  m   which dominates ac  p   this
concludes the proof 
   

fikeyder  hoffmann    haslum

lemma   given a planning task  and a set of non unit conjunctions c  h   c   
h   c
ce   
c
proof  consider the planning task c
no cc that is identical to  except that it does not
include cross context preconditions  that is  the precondition of each action representative
 
ac is modified to be the following 

 

pre ac     pre a c 

 

 pre a    c    add a   c

c  c  

 
c
 
c
we show that  a  h   c    h   c
no cc    and  b  h  no cc    h  ce   

we first prove  a   as in the proof of lemma    it suffices to prove that  for every split 
c
up action ac  p  of c
no cc   there exists a dominating action in    if p is not a  fluent 
  
 
then ac in c for c       dominates ac  p   otherwise  say p   c    then at least one of
the following cases must hold   a  c   c   or  b  c    add a    pre a    del a     in case  b  
  
 
again ac in c for c       dominates ac  p   in case  a   to obtain a dominating action
  
ac in c   we define
c        c    c   del a   c       add a   c        c    c   
all c    c    satisfy the conditions     del a   c       add a   c       and     c  c  
  
  c  c    add a   c        c  c     of definition    so indeed ac is an action in c  
  
  
we obviously have c   c    and thus p  add ac    it remains to prove that pre ac   
 
pre ac  p    this is so  intuitively  because c    corresponds to the single conjunction c   plus
subsumed conjunctions  and hence no cross context fluents arise s specifically  for every
  
c    c    we have c    add a   c   add a   thus pre ac      pre a  c   c     c    add a   c  
 
 pre a    c    add a   c   the latter is obviously contained in pre ac  p    concluding the
proof of  a  
 
c
 
c
it remains to prove  b   since h   c
no cc    h  no cc    it suffices to prove that h  no cc  
 
c
 
c
h  ce    consider a state s  and a relaxed plan ce for s in ce   for each action ace in
    representing action a of the original task   let c   be the set of conjunctions c whose
ce
  in c   c   obviously qualifies for
 fluents are added by ace during the execution of ce
ce

constraint     in definition    it qualifies for constraint     because any conditional effect
for c with that property will be triggered by ace if the conditional effect for a suitable c  is
c   of a  define the action sequence   
triggered  thus c
no cc includes the representative a
c   in    adds the same fluents as a  
c 
in c
ce
no cc to be the sequence of these a   obviously  a
and its precondition is the union of that of ace and of its conditional effects that fire  thus
 
c
 
c
   is a relaxed plan for c
no cc   it follows that h  no cc    h  ce   as desired 

example   consider the strips planning task  with variables  i  p  q  r  z  g    g    g    
initial state i    i   goal g    g    g    g     and actions a as follows 
   



fiimproving delete relaxation heuristics through explicit conjunctions

name
pre
add del ce cost
aqz
 i 
 q 
z 
 
 
i
ari
 i 
 r 
 
 
pz
 i  q 
 p   z  
 
aiq
apz
 r 
 p 
 z 

 
r
g 
 p  z   g   
 
 
apz
agiq 
 i  q   g   
 
 
g 
ar
 r   g   
 
 
we set c     i  q    p  z    the only operator adding part of  i  q  is aqz
i which adds q 
qz
pz pz
the only operators adding part of  p  z  are ai which adds z  and aiq   ar which add p 
pz
since aiq
and apz
both delete z  they cannot be used to establish the conjunction  p  z  
r
thus the actions of c
ce are 
name
pre
add del
ce cost
qz
qz
ai
 i   q  z 
 ce ai  
 
ari
 i 
 r 


 
 i 
q 

 
 p 


 
apz
i q
iq
pz
ar
 r 
 p 


 
 p  z  p z    g   


 
agpz 
g 
aiq
 i  q  i q    g   


 
 r   g   


 
agr 
where ce aqz
i   contains two conditional effects 
name
c
add del
i q
ei
 i   i q  


ep p z
 p   p z  

pz
clearly  with respect to hmax   the  fluents in the preconditions of aiq
  agpz    and agiq  dominate the respective other preconditions of these actions  as pointed out in section      in our
implementation we actually remove the other preconditions   thus lm cuts justification
graph on c
ce would have the structure shown in figure   
 
c
we have hmax  c
ce        due to the cost of achieving g    as for h  ce    to construct
qz pz
a plan for c
ce the only choice we have is how to establish p  we can use ai   aiq   or we
can use ari   apz
r   in the latter case  we make do with a single application of the conditionalg  g 
r pz qz g 
effects action aqz
i   by the relaxed plan     hai   ar   ai   apz   aiq   ar i  whose cost is     in
qz
the former case  we must use ai twice  first for i q   then for p z  yielding the relaxed
pz qz g 
g  r g 
 
c
plan     haqz
i   aiq   ai   apz   aiq   ai   ar i  whose cost is     thus h  ce         since  in
pz
the execution of     the only delete is that of ar   which is not true anyhow in the state of
execution    also solves the original task  and we get h      h   c
ce        
now consider lm cut  and say that we produced the cut for the conditional effects action
p z
aqz
i that connects p to p z via the conditional effect ep   the two options discussed in
section     are to  a  reduce the cost of aqz
i globally  sticking to the original definition of


lm cut  or to  b  reduce the cost only of ep p z   because the other conditional effect ei i q is
p z
part of an optimal cost path to ep and thus serves to justify its hmax value  each of these
options violates one of the essential properties of lm cut 

   

fikeyder  hoffmann    haslum



z

p



i q
aqz
i   ei

aqz
i

i

q

aqz
i

 


ei i q

 

r

arg 

p z

apz
iq


ei i q

i q

apz
r

ari

p z
aqz
i   ep

agiq 

agpz 

g 

g 

g 

figure    illustration of lm cut justification graphs for c
ce in example    the dashed
edges correspond to preconditions that are not critical  hmax  maximizing  at the start  but
that become critical at some point during the execution of lm cut 


p z
 a  in this configuration  lm cut produces the cuts  agpz     cost      aqz
i   ep    cost    
pz pz
g 
 cost      ar   aiq    cost      aiq    cost      ari    cost     note here that  after the
i q
p z
qz
qz
cut  aqz
i   ep    the cost of ai is reduced to   globally  in particular  the cut  ai   ei  
is not produced  we get the heuristic value hlm cut       hmax  c
ce         so here lm cut
does not dominate hmax  
 b  in this configuration  lm cut can produce the following cuts  at the start  for every
p z
possible precondition choice function  pcf    we get the cuts  agpz     cost    and  aqz
i   ep  
 cost     now hmax is   for each of g    g    say the pcf selects g    and we get the cut  agiq   
pz
max is  
 cost     now  the pcf has to select g    getting the cut  apz
r   aiq    cost     then  h
g 
for each of g    g    say the pcf selects g    say further that the pcf selects p z for apz  another
choice would be z   and selects i q for apz
 another choice would be q   thus remaining
iq
i q pz
in the non dashed part of figure    then we get the cut  aqz
i   ei   ar    cost    because
g  can be reached at   cost from both p and i q  we would get the same cut for any pcf
selecting g    at this point   now  hmax is   for g  and   for each of g    g    so we get the cut
 agr     cost     at this point  hmax is   for all goal facts  say lm cut selects g    and thus
we get the cut  ari    cost    because that is the only way to achieve r  then finally hmax
i q
is   for g  only  yielding the cut  aqz
i   ei    cost     overall  we get the heuristic value
hlm cut        h      h   c
ce         so here lm cut is not admissible 

 agr   

bibliography
baier  j  a     botea  a          improving planning performance using low conflict relaxed
plans  in gerevini  a   howe  a   cesta  a     refanidis  i   eds    proceedings of the
   

fiimproving delete relaxation heuristics through explicit conjunctions

  th international conference on automated planning and scheduling  icaps    
pp        thessaloniki  greece  aaai press 
bonet  b     geffner  h          planning as heuristic search  artificial intelligence        
        
bonet  b     helmert  m          strengthening landmark heuristics via hitting sets  in
coelho  h   studer  r     wooldridge  m   eds    proceedings of the   th european
conference on artificial intelligence  ecai     pp          lisbon  portugal  ios
press 
bonet  b   palacios  h     geffner  h          automatic derivation of memoryless policies
and finite state controllers using classical planners  in gerevini  a   howe  a   cesta 
a     refanidis  i   eds    proceedings of the   th international conference on automated planning and scheduling  icaps     pp        thessaloniki  greece  aaai
press 
bylander  t          the computational complexity of propositional strips planning 
artificial intelligence                  
cai  d   hoffmann  j     helmert  m          enhancing the context enhanced additive
heuristic with precedence constraints  in gerevini  a   howe  a   cesta  a     refanidis  i   eds    proceedings of the   th international conference on automated planning
and scheduling  icaps     pp        thessaloniki  greece  aaai press 
do  m  b     kambhampati  s          sapa  a domain independent heuristic metric temporal planner  in cesta  a     borrajo  d   eds    recent advances in ai planning   th
european conference on planning  ecp     lecture notes in artificial intelligence 
pp          toledo  spain  springer verlag 
fox  m     long  d          stan   a hybrid planning strategy based on subproblem
abstraction  the ai magazine               
garey  m  r     johnson  d  s          computers and intractabilitya guide to the
theory of np completeness  freeman  san francisco  ca 
gazen  b  c     knoblock  c          combining the expressiveness of ucpop with
the efficiency of graphplan  in steel  s     alami  r   eds    recent advances in ai
planning   th european conference on planning  ecp     lecture notes in artificial
intelligence  pp          toulouse  france  springer verlag 
gerevini  a   saetti  a     serina  i          planning through stochastic local search and
temporal action graphs  journal of artificial intelligence research             
haslum  p     geffner  h          admissible heuristics for optimal planning  in chien  s  
kambhampati  r     knoblock  c   eds    proceedings of the  th international conference on artificial intelligence planning systems  aips     pp          breckenridge  co  aaai press 
haslum  p          hm  p     h   p m    alternative characterisations of the generalisation
from hmax to hm   in gerevini  a   howe  a   cesta  a     refanidis  i   eds    proceedings of the   th international conference on automated planning and scheduling
 icaps     pp          thessaloniki  greece  aaai press 
   

fikeyder  hoffmann    haslum

haslum  p          incremental lower bounds for additive cost planning problems  in
bonet  b   mccluskey  l   silva  j  r     williams  b   eds    proceedings of the   nd
international conference on automated planning and scheduling  icaps     pp 
      sao paulo  brasil  aaai press 
helmert  m          the fast downward planning system  journal of artificial intelligence
research             
helmert  m     domshlak  c          landmarks  critical paths and abstractions  whats
the difference anyway   in gerevini  a   howe  a   cesta  a     refanidis  i   eds   
proceedings of the   th international conference on automated planning and scheduling  icaps     pp          thessaloniki  greece  aaai press 
helmert  m     geffner  h          unifying the causal graph and additive heuristics  in
rintanen  j   nebel  b   beck  j  c     hansen  e   eds    proceedings of the   th
international conference on automated planning and scheduling  icaps     pp 
        sydney  australia  aaai press 
hoffmann  j          where ignoring delete lists works  local search topology in planning
benchmarks  journal of artificial intelligence research             
hoffmann  j     nebel  b          the ff planning system  fast plan generation through
heuristic search  journal of artificial intelligence research             
hoffmann  j   porteous  j     sebastia  l          ordered landmarks in planning  journal
of artificial intelligence research             
hoffmann  j   steinmetz  m     haslum  p          what does it take to render h    c  
perfect   in proceedings of the  th workshop on heuristics and search for domain
independent planning  at icaps   
karpas  e     domshlak  c          cost optimal planning with landmarks  in boutilier  c 
 ed    proceedings of the   st international joint conference on artificial intelligence
 ijcai     pp            pasadena  california  usa  morgan kaufmann 
katz  m   hoffmann  j     domshlak  c          who said we need to relax all variables  
in borrajo  d   fratini  s   kambhampati  s     oddi  a   eds    proceedings of the
  rd international conference on automated planning and scheduling  icaps    
pp          rome  italy  aaai press 
keyder  e     geffner  h          heuristics for planning with action costs revisited  in
ghallab  m   ed    proceedings of the   th european conference on artificial intelligence  ecai     pp          patras  greece  wiley 
keyder  e     geffner  h          trees of shortest paths vs  steiner trees  understanding
and improving delete relaxation heuristics  in boutilier  c   ed    proceedings of the
  st international joint conference on artificial intelligence  ijcai     pp      
      pasadena  california  usa  morgan kaufmann 
keyder  e   hoffmann  j     haslum  p          semi relaxed plan heuristics  in bonet  b  
mccluskey  l   silva  j  r     williams  b   eds    proceedings of the   nd international conference on automated planning and scheduling  icaps     pp         
sao paulo  brasil  aaai press 
   

fiimproving delete relaxation heuristics through explicit conjunctions

keyder  e   richter  s     helmert  m          sound and complete landmarks for and or
graphs  in coelho  h   studer  r     wooldridge  m   eds    proceedings of the   th
european conference on artificial intelligence  ecai     pp          lisbon  portugal  ios press 
palacios  h     geffner  h          compiling uncertainty away in conformant planning
problems with bounded width  journal of artificial intelligence research         
    
richter  s     westphal  m          the lama planner  guiding cost based anytime
planning with landmarks  journal of artificial intelligence research             
roger  g   pommerening  f     helmert  m          optimal planning in the presence of
conditional effects  extending lm cut with context splitting  in schaub  t   ed   
proceedings of the   st european conference on artificial intelligence  ecai    
prague  czech republic  ios press  to appear 

   

fi