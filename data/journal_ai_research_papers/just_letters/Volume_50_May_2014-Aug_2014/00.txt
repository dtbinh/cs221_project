journal of artificial intelligence research               

submitted        published      

topic based dissimilarity and sensitivity models
for translation rule selection
min zhang

minzhang   suda   edu   cn

provincial key laboratory for computer information processing technology 
soochow university  suzhou  china

xinyan xiao

xiaoxinyan   ict  ac   cn

iip key lab  institute of computing technology 
chinese academy of sciences  china

deyi xiong

dyxiong   suda   edu   cn

provincial key laboratory for computer information processing technology 
soochow university  suzhou  china

qun liu

liuqun   ict  ac   cn

cngl  school of computing  dublin city university  ireland
iip key lab  institute of computing technology 
chinese academy of sciences  china

abstract
translation rule selection is a task of selecting appropriate translation rules for an ambiguous
source language segment  as translation ambiguities are pervasive in statistical machine translation  we introduce two topic based models for translation rule selection which incorporates global
topic information into translation disambiguation  we associate each synchronous translation rule
with source  and target side topic distributions with these topic distributions  we propose a topic
dissimilarity model to select desirable  less dissimilar  rules by imposing penalties for rules with a
large value of dissimilarity of their topic distributions to those of given documents  in order to encourage the use of non topic specific translation rules  we also present a topic sensitivity model to
balance translation rule selection between generic rules and topic specific rules  furthermore  we
project target side topic distributions onto the source side topic model space so that we can benefit
from topic information of both the source and target language  we integrate the proposed topic dissimilarity and sensitivity model into hierarchical phrase based machine translation for synchronous
translation rule selection  experiments show that our topic based translation rule selection model
can substantially improve translation quality 

   introduction
translation rules are bilingual segments  that establish translation equivalences between the source
and target language  they are widely used in statistical machine translation  smt  with various representations ranging from word pairs to bilingual phrases and synchronous rules in word   phraseand syntax based smt respectively  normally  a large number of translation rules can be learnt
from bilingual training data for a single source segment which occurs in different contexts  for
example  xiong  zhang  and li        observe that each chinese verb can be translated with more
   here a segment is defined as a string of terminals and or nonterminals 

c
    
ai access foundation  all rights reserved 

fiz hang   x iao   x iong     l iu

than     different translation rules on average  therefore how to select an appropriate translation
rule for an ambiguous source segment is a very crucial issue in smt 
traditionally the appropriateness of a translation rule is measured with multiple probabilities
estimated from word aligned data  such as bidirectional translation probabilities  koehn  och   
marcu         as such probabilities fail to capture local and global contexts of highly ambiguous
source segments  they are not sufficient to select correct translation rules for these segments  therefore various approaches have been proposed to capture rich contexts at the sentence level to help
select proper translation rules for phrase   carpuat   wu      a  or syntax based smt  chan  ng 
  chiang        he  liu    lin        liu  he  liu    lin         these studies show that local
features  such as surrounding words  syntactic information and so on  are helpful for translation rule
selection 
beyond these contextual features at the sentence level  we conjecture that translation rules are
also related to high level global information  such as the topic  hofmann        blei  ng    jordan 
      information at the document level  in order to visualize the relatedness between translation
rules and document topics  we show four hierarchical phrase based translation rules with their topic
distributions in figure    from the figure  we can observe that
 first  translation rules can be divided into two categories in terms of their topic distributions 
topic sensitive rules  i e   topic specific rules  and topic insensitive rules  i e   non topic specific or generic rules   the former rules  e g   the translation rule  a    b  and  d  in figure
   have much higher distribution probabilities on a few specific topics than other topics  the
latter rules  e g   the translation rule  c  of figure    have an even distribution over all topics 
 second  topic information can be used to disambiguate ambiguous source segments  in figure
   translation rule  b  and  c  have the same source segment  however their topic distributions
are quite different  rule  b  distributes on the topic about international relations with the
highest probability  which suggests that rule  b  is much more related to this topic than other
topics  in contrast  rule  c  has an even distribution over all topics  therefore in a document
on international relations  rule  b  will be more appropriate than rule  c  for the source
x   
segment 



these two observations suggest that different translation rules have different topic distributions and
document level topic information can be used to benefit translation rule selection 
in this article  we propose a framework for translation rule selection that exactly capitalizes on
document level topic information  the proposed topic based translation rule selection framework
associates each translation rule with a topic distribution  rule topic distribution  on both the source
and target side  each source document is also annotated with its corresponding topic distribution
 document topic distribution   dissimilarity between the document topic distribution and rule topic
distribution is calculated and used to help select translation rules that are related to documents in
terms of topics  in particular 
 given a document to be translated  we use a topic dissimilarity model to calculate the dissimilarity of each translation rule to the document based on their topic distributions  our
translation system will penalize candidate translations with high dissimilarities  
   section   explains why our system penalizes candidate translations with high dissimilarities 

 

fit opic  based d issimilarity

and

s ensitivity m odels

   

   

   

   

   

   

 

 u  operational capability

 

 a 

 

  

  

  

  

 
  

 

   

   

   

   

   

   

 
 

 

 c 

 x
  

 

 
  

  

  

 

 b 

  

 

 give x 

 d  x 

 x
  

  

    x
 

  

  

  

  

  

 grants x 

 

  

 

  

  

 held talks x  x 

figure    four synchronous rules with topic distributions  each sub graph shows a rule with its
topic distribution  where the x axis shows the topic index and the y axis the topic probability  notably  the rule  b  and rule  c  shares the same source chinese string  but they
have different topic distributions due to the different english translations 

 the dissimilarity between a topic insensitive translation rule and a given source document
computed by our topic dissimilarity model is often very high as documents are normally
topic sensitive  we dont want to penalize these generic topic insensitive rules  therefore
we further propose a topic sensitivity model which rewards topic insensitive rules so as to
complement the topic dissimilarity model 
 we associate each translation rule with a rule topic distribution on both the source and target side  in order to calculate the dissimilarity between target side rule topic distributions
of translation rules and source side document topic distributions of given documents during
decoding  we project the target side rule topic distributions of translation rules onto the space
of source side document topic model by one to many mapping 
we use a hierarchical phrase based smt system  chiang        to validate the effectiveness of
our topic based models for translation rule selection  experiments on chinese english translation
tasks  section    show that our method outperforms the baseline hierarchial phrase based system
by      b leu points on large scale training data 
the use of topic based dissimilarity and sensitivity models to improve smt was first presented
in our previous paper  xiao  xiong  zhang  liu    lin         in this article  we provide more
detailed comparison to related work and formulations of the two models as well as the integration

 

fiz hang   x iao   x iong     l iu

procedure  more importantly  we carry out large scale experiments with more bilingual and monolingual training data and incorporate a detailed analysis of the output of topic based dissimilarity
and sensitivity models at both the document and translation hypothesis level 
the rest of this article is organized as follows  section   introduces related work  section  
provides background knowledge about statistical machine translation and topic modeling  section  
elaborates our topic based translation rule selection framework  including the topic dissimilarity and
topic sensitivity model  section   discusses how we estimate rule topic and document topic distributions and how we project target side rule topic distributions onto the source side topic space in a
one to many mapping fashion  section   presents the integration of the topic based translation rule
selection models into hierarchical phrase based smt  section   describes a series of experiments
that verify the effectiveness of our approach  section   provides a detailed analysis of the output of
our models  section   gives some suggestions for bilingual topic modeling from the perspective of
machine translation  finally  we conclude in section    with future directions 

   related work
our topic based dissimilarity and sensitivity models for translation rule selection are related to three
categories of work in smt  translation rule selection  topic models for smt and document level
translation  in this section  we introduce related approaches of the three categories and highlight the
differences of our method from previous work 
    translation rule selection
as we mentioned before  translation rule selection is a very important task in smt  several approaches have been proposed for it recently  carpuat and wu explore both word and phrase sense
disambiguation  wsd and psd  for translation rule selection in phrase based smt  carpuat   wu 
    a      b   their wsd and psd system integrate sentence level local collocation features  experiments show that multi word psd can improve phrase selection  also following the wsd line 
chan et al         integrate a wsd system into hierarchical phrase based smt for lexical selection
or the selection of short phrases of length   or    their wsd system also adopts sentence level
features of local collocations  surrounding words and so on 
different from lexical or phrasal selection using wsd psd  he et al         propose a maximum entropy  maxent  based model for context dependent synchronous rule selection in hierarchical phrase based smt  local context features such as phrase boundary words and part of speech
information are incorporated into the model  liu et al         extends the selection method of he
et al  to integrate a similar maxent based rule selection model into a tree to string syntax based
smt system  liu  liu    lin         their model uses syntactic information from source parse
trees as features 
the significant difference between our topic based rule selection framework and previous approaches on translation rule selection is that we use global topic information to help select translation rules for ambiguous source segments rather than sentence level local context features 
    topic models for smt
topic modeling  hofmann        blei et al         is a popular technique for discovering underlying
topic structures of documents  recent years have witnessed that topic models have been explored
 

fit opic  based d issimilarity

and

s ensitivity m odels

for smt  zhao and xing              and tam  lane  and schultz        have proposed topicspecific lexicon translation adaptation models to improve translation quality  such models focus on
word level translations  they first estimate word translation probabilities conditioned on topics  and
then adapt lexical translation probabilities of phrases by these topic conditioned probabilities  since
modern smt systems use synchronous rules or bilingual phrases to translate sentences  we believe
that it is more reasonable to incorporate topic models for phrase or synchronous rule selection than
lexical selection 
gong  zhang  and zhou        adopt a topic model to filter out phrase pairs that are not consistent with source documents in terms of their topics  they assign a topic for each document to
be translated  similarly  each phrase pair is also assigned with one topic  a phrase pair will be
discarded if its topic mismatches the document topic  the differences from their work are twofold 
first  we calculate the dissimilarities of translation rules to documents based on their topic distributions instead of comparing the best topics assigned to translation rules and those of documents 
second  we integrate topic information into smt in a soft constraint manner via our topic based
models  they explore topic information in a hard constraint fashion by discarding translation rules
with unmatched topics 
topic models are also used for domain adaptation on translation and language models in smt 
foster and kuhn        describe a mixture model approach for smt adaptation  they divide a
training corpus into different domains  each of which is used to train a domain specific translation
model  during decoding  they combine a general domain translation model with a specific domain
translation model that is selected according to various text distances calculated by topic model 
tam et al         and ruiz and federico        use a bilingual topic model to project latent topic
distributions across languages  based on the bilingual topic model  they apply source side topic
weights onto the target side topic model so as to adapt the target side n gram language model 
    document level machine translation
since we incorporate document topic information into smt  our work is also related to documentlevel machine translation  tiedemann        integrates cache based language and translation models that are built from recently translated sentences into smt  gong  zhang  and zhou        further
extend this cache based approach by introducing two additional caches  a static cache that stores
phrases extracted from documents in training data which are similar to the document in question and
a topic cache with target language topic words  xiao  zhu  yao  and zhang        try to solve the
translation consistency issue in document level translation by introducing a hard constraint where
ambiguous source words are required to be consistently translated into the most frequent translation options  ture  oard  and resnik        soften this consistency constraint by integrating three
counting features into the decoder  these studies normally focus on the surface structure to capture inter sentence dependencies for document level machine translation while we explore the topic
structure of a document for document translation 

   preliminaries
we establish in this section some background knowledge about both statistical machine translation
and topic modeling  although the introduction here is short  it is sufficient for understanding our

 

fiz hang   x iao   x iong     l iu

sub models
pi
logp  ei  f i  
p i
logp  f i  ei  
p i
logplex  ei  f i  
p i
logplex  f i  ei  
p  e 
logp  ei  e     ei   
pi 
  log ei   f i  
 e 
i

descriptions
direct translation probabilities
inverse translation probabilities
direct lexical translation probabilities
inverse lexical translation probabilities
language model
reordering model
word count
rule count

table    the most widely used sub models of statistical machine translation  i is the number of
translation rules that are used to generate the target sentence e given the source sentence
f   ei and f i are the target and source side of a translation rule ri  

topic based dissimilarity and sensitivity models that try to bridge the gap between topic modeling
and statistical machine translation 
    statistical machine translation
given a source sentence f   most smt systems find the best translation e among all possible translations as follows 
hp
i 

m

 exp

h
 f 
e 
m m
 
hp
i
e   argmax p
m

 
e
e exp
  m hm  f  e  
  
 
  m
x
   
m hm  f  e 
  argmax exp
e

  argmax
e

m  

 

m
x

 

m hm  f  e 

m  

where hm  f  e  is a feature function defined on the source sentence f and the corresponding
transla i
hp
p
m

tion e  m is the weight of the feature function  since the normalization e exp
  m hm  f  e  

is constant for all possible translations e   we do not need to calculate it during decoding 
the weighted model in the equation     is a log linear model  the feature functions hm  f  e 
are also referred to as sub models  as they are components of the log linear model  in table   
we show the most widely used feature functions in smt  most of them can be easily factored over
translation rules  which facilitates the application of dynamic programming in decoding  we will
show that our proposed topic based dissimilarity and sensitivity models can be also easily factorized
in section   
   this notation is used when we want to emphasize that a sub model is a component of the log linear model  otherwise
we just call them models  such as a language model  a reordering model and so on 

 

fit opic  based d issimilarity

and

s ensitivity m odels

in the log linear model of smt  the sub models are trained separately and combined under
the assumption that they are independent of each other  the associated weights s can be tuned
using minimum error rate training  mert   och        or the margin infused relaxed algorithm
 mira   chiang  marton    resnik         note that the normalization factor in the equation    
is not calculated in these training algorithms  this is because these algorithms directly optimize the
log linear model of smt towards some translation quality measure such as bleu  feature weights
that are optimized towards criteria such as maximum mutual information  mmi  are not necessarily
optimal with respect to translation quality  och        
as we integrate the proposed two models into the log linear model of a hierarchical phrasebased smt system  section    in order to validate the effectiveness of the two models  we provide
more details about hierarchical phrase based smt  chiang        in this section  translation rules
in hierarchial phrase based smt are synchronous context free grammar rules  which can be denoted
as follows 
x  h    i
   
where x is an undifferentiated nonterminal   and  are strings of terminals and nonterminals  on
the source and target side respectively   denotes the one to one mapping between nonterminals in
 and nonterminals in   these rules can be automatically extracted from word aligned bilingual
training data  in addition to these rules  two special rules are also introduced into hierarchical
phrase based smt 
s  hx    x  i
s  hs  x    s  x  i

   

these two rules are used to serially concatenate nonterminal xs in a monotonic manner to form an
initial symbol s  the start symbol of the grammar of hierarchical phrase based smt 
the log linear model of hierarchical phrase based smt can be formulated as follows 
 
x
log t r     lm logplm  e    wp  e    rp i
   
w d    exp
rd

where d is a derivation defined as a set of triples  r  i  j   each of which denotes an application of a
translation rule that spans words i from j on the source side  i is the number of translation rules in
d  the probability of a translation rule r is defined as
t r    p      p      plex      plex     

   

where the lexical translation probabilities plex     and plex     estimate the probabilities that
the words in  translate the words in  in a word by word fashion  koehn et al         
    topic modeling
topic modeling is used to discover topics that occur in a collection of documents  both latent
dirichlet allocation  lda   blei et al         and probabilistic latent semantic analysis  plsa 
   in order to simplify the decoder implementation  at most two nonterminals are allowed in hierarchical translation
rules 

 

fiz hang   x iao   x iong     l iu

 hofmann        are topic models  as lda is the most widely used topic model  we exploit it to
mine topics for our translation rule selection 
lda views each document as a mixture of various topics  each of which is a probability distribution over words  more particularly  lda works in a generative process as follows 
 for each document dj   sample a document topic distribution  per document topic distribution  j from a dirichlet distribution dir    j  dir   
 for each word wj i of nj words in the document dj  
 sample a topic assignment zj i  multinomial j   
 sample the word wj i  multinomial zj i   where zj i is the per topic word distribution of topic zj i drawn from dir   
generally speaking  lda contains two groups of parameters  the first group of parameters
characterizes document topic distributions  j    which record the distribution of each document over
topics  the second group of parameters is used for topic word distributions  k    which represent
each topic as a distribution over words 
given a document collection with observed words w    wj i    the goal of lda inference is to
compute the values for these two sets of parameters  and  as well as the latent topic assignments
z    zj i    the inference is complicated due to the latent topic assignments z  an efficient inference
algorithm that has been proposed to address this problem is collapsed gibbs sampling  griffiths
  steyvers         where the two sets of parameters  and  are integrated out of the lda model 
and only the latent topic assignments z are sampled from p  z w   once we obtain the values of z 
we can estimate  and  by recovering their posterior distributions given z and w  in section    we
will use these two sets of estimated parameters and the topic assignments of words to calculate the
parameters of our models 

   topic based dissimilarity and sensitivity models
in this section  we elaborate our topic based models for translation rule selection  including a topic
dissimilarity model and a topic sensitivity model 
    topic dissimilarity model
sentences should be translated in accordance with their topics  zhao   xing              tam
et al          take the translation rule  b  in figure   as an example  if the source side of rule
 b  occurs in a document on international relations  we hope to encourage the application of rule
 b  rather than rule  c   this can be achieved by calculating the dissimilarity between probability
distributions of a translation rule and a document over topics 
in order to calculate such a topic dissimilarity for translation rule selection  we associate both
the source and target side of a translation rule with a rule topic distribution p  z  r    where  is
the placeholder for the source side f or target side e  r is the source or target side of a translation
rule r  and z is the corresponding topic of r   therefore each translation rule has two rule topic
distributions  p  zf  rf   on the source side and p  ze  re   on the target side 
 

fit opic  based d issimilarity

and

s ensitivity m odels

supposing there are k topics  the two distributions can be represented by a k dimension vector  the k th component p  z   k r   denotes the probability of topic k given r   the sourceand target side rule topic distributions are separately estimated from training data  the estimation
method is described in section    where we also discuss the reason why we estimate them in a
separate manner 
analogously  we represent the topic information of a document d to be translated by a documenttopic distribution p  z d   which is also a k dimension vector  the k th dimension p  z   k d 
is the topic proportion for topic k in document d  different from the rule topic distribution  the
document topic distribution can be directly inferred by an off the shelf lda tool 
based on the defined rule topic and document topic distributions  we can measure the dissimilarity of a translation rule to a document so as to decide whether the rule is suitable for the document
in translation  traditionally  the similarity of two probability distributions is calculated by information measurements such as jensen shannon divergence  lin        or hellinger distance  blei  
lafferty        
here we adopt the hellinger distance  hd  to measure the topic dissimilarity  which is symmetric and widely used for comparing two probability distributions  blei   lafferty         given
a rule topic distribution p  z  r   and a document topic distribution p  z d   hd is computed as
follows 
k p
 
x
p
p  z   k d   p  z   k r  
   
hd p  z d   p  z  r     
k  

let d be a derivation as defined in section      let p z r  represent corresponding rule topic
distributions for all rules in d  our topic dissimilarity model dsim p  z d   p z r   on a derivation
d is defined on the hd of the equation     as follows
x
dsim p  z d   p z r    
hd p  z d   p  z  r   
   
rd

obviously  the larger the hellinger distance between a candidate translation yielded by a derivation
and a document  the larger the dissimilarity between them  with the topic dissimilarity model
defined above  we aim to select translation rules that are similar to the document to be translated in
terms of their topics 
    topic sensitivity model
before we introduce the topic sensitivity model  lets revisit figure    we can easily find that the
probability of rule  c  distributes evenly over all topics  this indicates that it is insensitive to topics 
and can be therefore applied on any topics  in contrast  the distributions of the other three rules
peak on a few topics  generally speaking  a topic insensitive rule has a fairly flat distribution over
all topics  while a topic sensitive rule has a sharp distribution over a few topics 
as a document typically focuses on a few topics  it has a sharp distribution over these topics 
in other words  documents are normally topic sensitive  since the distribution of a topic insensitive
rule is fairly flat  the dissimilarity between a topic insensitive rule and a topic sensitive document
will be very low  therefore  our system with the proposed topic dissimilarity model will punish
topic insensitive rules 
 

fiz hang   x iao   x iong     l iu

however  topic insensitive rules may be more preferable than topic sensitive rules if neither of
them are similar to given documents  for a document about a topic of love  the rule  b  and  c  in
figure   are both dissimilar to the document as rule  b  relates to the international relations topic
and rule  c  is topic insensitive  nevertheless  since rule  c  occurs more frequently across various
topics  we prefer rule  c  to rule  b  when we translate a document about love 
to address such issue of the topic dissimilarity model  we further propose a topic sensitivity
model  the model employs an entropy based metric to measure the topic sensitivity of a rule as
follows
k
x
p  z   k r    log p  z   k r   
   
h p  z  r      
k  

according to this equation  a topic insensitive rule normally has a large entropy while a topicsensitive rule has a smaller entropy 
given a derivation d and rule topic distributions p z r  for rules in d  the topic sensitivity
model is defined as follows 
x
h p  z  r   
   
sen p z r    
rd

incorporating the topic sensitivity model with the topic dissimilarity model  we enable our smt
system to balance the selection of topic sensitive and topic insensitive rules  given rules with approximately equal values of topic dissimilarity  we prefer topic insensitive rules 

   estimation
unlike document topic distributions that can be directly learned by lda tools  we need to estimate
rule topic distributions for translation rules  as we want to exploit topic information of both the
source and target language  we separately train two monolingual topic models on the source and
target side  and learn correspondences between the two topic models via word alignments in the
bilingual training data 
particularly  we adopt two rule topic distributions for each translation rule     the source side
rule topic distribution p  zf  rf   and the    the target side rule topic distribution p  ze  re    both of
which are defined in section      these two rule topic distributions are estimated using trained
topic models in the same way  section       notably  only source language documents are available
during decoding  in order to compute the dissimilarity between the target side rule topic distribution
of a translation rule and the source side document topic distribution of a given document we need
to project the target side rule topic distribution of a translation rule onto the space of the source side
topic model  section      
we can also establish alternative approaches to the estimation of rule topic distributions via
multilingual topic models  mimno  wallach  naradowsky  smith    mccallum        boyd graber
  blei        or bilingual topic models that also infer word to word alignments in document pairs
 zhao   xing               the former multilingual topic models only require that documents in
different languages are comparable in terms of content similarity  in contrast  the latter bilingual
topic models require that documents are parallel  i e   translations of each other  so as to capture
word alignments 



  

fit opic  based d issimilarity

z

n

and

s ensitivity m odels

z

w

n

m

z

w
z

topic
correspondence

n 

z

m

k

k

target

source

 a 

w

nl

n

m

word
alignment

n

i

w

m

m
w

z

z

e

j

w
 

l

 

l

k

m

f

j

a

s

k

k

b

t

target

source

 a  

 b 

 c 

figure    graphical model representations of  a  our bilingual topic model   b  polylingual topic
model of mimno et al          and  c  bilingual topic model of zhao and xing       
where s is the number of parallel sentence pairs in a document  a is the word alignment
between a source and target sentence  for simplicity  we do not display hmm transitions
among word alignments a  subfigure  a   shows how we build topic correspondences between the source and target language after source and target topics are separately learned
as shown in  a  

the biggest difference between our method and these multilingual bilingual topic models is that
they use the same per tuple topic distribution  for all documents in the same tuple  here we define
the tuple as a set of documents in different languages  a per tuple topic distribution is similar to
a per document topic distribution  the only difference between them is that the per tuple topic
distribution is shared by all documents in the tuple 
topic assignments for words in these languages are naturally connected since they are sampled
from the same topic distribution  in contrast  we assume that each document on the source target
side has its own sampled document specific distribution over topics  topic correspondences between the source and target document are learned by projection via word alignments  we visualize
this difference in figure   
yet another difference between our models and the topic specific lexicon translation model of
zhao and xing        is that they use their bilingual topics to improve smt at the word level
instead of the rule level  since a synchronous rule is rarely factorized into individual words  we
believe that it is more reasonable to incorporate the topic model directly at the rule level rather than
the word level  in section        we empirically compare our model with the topic specific lexicon
translation model 
tam et al         also construct two monolingual topic models for parallel source and target
documents  they build the topic correspondences between source and target documents by enforcing a one to one topic mapping constraint  we project target side topics onto the space of the
source side topic model in a one to many fashion  in section        we compare these two different
methods for building topic correspondences 

  

fiz hang   x iao   x iong     l iu

    rule topic distribution estimation
we estimate rule topic distributions from word aligned bilingual training corpus with document
boundaries explicitly given  the source  and target side rule topic distributions are estimated in the
same way  therefore  for simplicity  we only describe the estimation of the source side rule topic
distribution p  zf  rf   of a translation rule in this section 
the estimation of rule topic distributions is analogous to the traditional estimation of rule translation probabilities  chiang         in addition to the word aligned corpus  the input for rule topic
distribution estimation also contains source side document topic distributions inferred by lda tool 
we first extract translation rules from bilingual training data in a traditional way  when the
source side of a translation rule rf is extracted from a source language document df with a documenttopic distribution p  zf  df    we obtain an instance  rf   p  zf  df       where  is the fraction count
of an instance as described by chiang         in this way  we can collect a set of instances i
    rf   p  zf  df       with different document topic distributions for each translation rule  using
these instances  we calculate the probability p  zf   k rf   of rf over topic k as follows 
p
  p  zf   k df  
    
p  zf   k rf     pk ii
p

k    
ii   p  zf   k  df  

based on this equation  we can obtain two rule topic distributions p  zf  rf   and p  ze  re   for each
rule using the source  and target side document topic distributions p  zf  df   and p  ze  de   respectively 
    target side rule topic distribution projection

as described in the previous section  we also estimate target side rule topic distributions  however  we can not directly use the equation     to calculate the dissimilarity between the target side
rule topic distribution p  ze  re   of a translation rule and the source side document topic distribution
p  zf  df   of a source language document that is to be translated  in order to measure this dissimilarity  we need to project target side topics onto the source side topic space  the projection takes
the following two steps 
 first  we calculate a correspondence probability p zf  ze   for each pair of a target side topic
ze and a source side topic zf   which are inferred by the two separately trained monolingual
topic models respectively 
 second  we project the target side rule topic distribution of a translation rule onto the sourceside topic space using the correspondence probabilities learned in the first step 
in the first step  we estimate the topic to topic correspondence probabilities using co occurrence
counts of topic assignments of source and target words in the word aligned corpus  the topic assignments of source target words are inferred by the two monolingual topic models  with these topic
assignments  we characterize a sentence pair  f  e  as  zf   ze   a   where zf and ze are two vectors
containing topic assignments for words in the source and target sentence f and e respectively  and a
is a set of word alignment links   i  j   between the source and target sentence  particularly  a link
 i  j  represents that a source side position i aligns to a target side position j 
  

fit opic  based d issimilarity

and

s ensitivity m odels

with these notations  we calculate the co occurrence count of a source side topic kf and a
target side topic ke as follows 
x x
 zfi   kf     zej   ke  
    
 zf  ze  a   i j a

where zfi and zej are topic assignments for words fi and ej respectively   x  y  is the kronecker
function  which is   if x   y and   otherwise 
we then compute the topic to topic correspondence probability of p  zf   kf  ze   ke   by
normalizing the co occurrence count as follows 
p
p
 zf  ze  a 
 i j a  zfi   kf     zej   ke  
p
p
    
p  zf   kf  ze   ke    
 zf  ze  a 
 i j a  zej   ke  

overall  after the first step  we obtain a topic to topic correspondence matrix mke kf   where the
item mi j represents the probability p  zf   i ze   j  
in the second step  given the correspondence matrix mke kf   we project the target side ruletopic distribution p  ze  re   to the source side topic space by multiplication as follows 
t  p  ze  re      p  ze  re    mke kf

    

in this way  we get a second distribution for a translation rule in the source side topic space  which
we call projected target side topic distribution t  p  ze  re    
word alignment noises may be introduced in the equation       which in turn may flatten the
sharpness of the projected topic distributions calculated in the equation       in order to decrease
the flattening effects of word alignment noises  we take the following action in practice  if the
topic to topic correspondence probability p  zf   kf  ze   ke   calculated via word alignments is
 
where k is the predefined number of topics  we set it to   and then re normalize all
less than k
other correspondence probabilities of the target side topic ke  
obviously  our projection method allows one target side topic ze to align to multiple source side
topics  this is different from the one to one correspondence used by tam et al          we investigate the correspondence matrix mke kf obtained from our training data  we find that the topic
correspondence between the source and target language is not necessarily one to one  typically  the
correspondence probability p  zf   kf  ze   ke   of a target side topic mainly distributes over two
or three source side topics  table   shows an example of a target side topic with its three mainly
aligned source side topics 

   integration
we incorporate our topic dissimilarity and sensitivity model as two new features into a hierarchical
phrase based system  chiang        under the log linear discriminative framework  och   ney 
       the dissimilarity values are positive as hellinger distances are positive  the weight of this
dissimilarity feature tuned by mert will be negative  therefore the log linear model will favor
those candidate translations with lower values of the dissimilarity feature  less dissimilar   in other
words  translation rules that are more similar to the document to be translated in terms of their topics
will be selected 
  

fiz hang   x iao   x iong     l iu

e topic
enterprises
rural
state
agricultural
market
reform
production
peasants
owned
enterprise
p  zf  ze  

 agricultural 
  rural 
 peasant 
u reform 
 finance 
 social 
 safety 
n adjust 
 policy 
  income 

f topic  

 enterprise 
  market 
ik state 
i company 
 k finance 
  bank 
 investment 
 n manage 
u reform 
e operation 

f topic  

u develop 
l economic 
e technology  
i china 
e technique 
 industry 
  structure 
m  innovation 
  accelerate 
u reform 

f topic  

    

    

    

table    an example of topic to topic correspondence  the last line shows the correspondence
probability  each column shows a topic represented by its top    topical words  the first
column is a target side topic  while the remaining three columns are source side topics 

one possible side effect of the integration of such a dissimilarity feature is that our system will
favour translations generated by fewer translation rules against those generated by more translation
rules because more translation rules result in higher dissimilarity  see the equation       that is to
say  the topic based dissimilarity feature also acts as a translation rule count penalty on derivations 
fortunately  however  we also use a translation rule count feature  see the last row in table    which
normally favours translations yielded by a derivation with a large number of translation rules  this
feature will balance against the mentioned side effect of our topic based dissimilarity feature 
as each translation rule is associated with a source side rule topic distribution and a projected
target side rule topic distribution during decoding  we add four features as follows  
 dsim p  zf  d   p zf  rf     or dsimsrc   topic dissimilarity feature on source side rule topic
distributions 
 dsim p  zf  d   t  p ze  re      or dsimtrg   topic dissimilarity feature on projected targetside rule topic distributions 
 sen p zf  rf     or sensrc   topic sensitivity feature on source side rule topic distributions 
 sen t  p ze  re     or sentrg   topic sensitivity feature on projected target side rule topic
distributions 
the source side and projected target side rule topic distributions for translation rules can be
calculated before decoding as described in the last section  during decoding  we first infer the
topic distribution p  zf  d  for a given document of the source language  when a translation rule is
adopted in a derivation  the scores of the four features will be updated correspondingly according to
the equation     and      obviously  the computational cost of these features is rather small 
   since the glue rule and rules of unknown words are not extracted from training data  we just set the values of the four
features for these rules to zero 

  

fit opic  based d issimilarity

and

s ensitivity m odels

for topic specific lexicon translation models  zhao   xing        tam et al          they first
calculate topic specific translation probabilities by normalizing the entire lexicon translation table
and then adapt the lexical weights of translation rules correspondingly during decoding  this makes
the decoder run slower  therefore  comparing with previous topic specific lexicon translation methods  our method provides a more efficient way for incorporating topic models into smt 

   experiments
in this section  we conducted two groups of experiments to validate the effectiveness of our topicbased translation rule selection framework  in the first group of experiments  we use medium scale
bilingual data to train our smt system and topic models  the purpose of this group of experiments
is to quickly answer the following questions 
 is our topic dissimilarity model able to improve translation rule selection in terms of b leu 
furthermore  are the source side and target side rule topic distributions complementary to
each other 
 is it helpful to introduce the topic sensitivity model to distinguish topic insensitive and topicsensitive rules 
 is our topic based method better than previous topic specific lexicon translation method  zhao
  xing        in terms of both b leu and decoding speed 
after we confirm the efficacy of our topic based dissimilarity and sensitivity model on mediumscale training data  we conducted a second group of experiments on large scale training data to
further investigate the following questions 
 is our one to many target side rule topic projection method better than previous methods
proposed by zhao and xing        or tam et al         
 what are the effects of our models on various types of rules  such as phrase rules and rules
with non terminals 
 what else can we achieve if we use more monolingual data to train topic models 
    setup
we carried out our experiments on nist chinese to english translation  we used the nist evaluation set of       mt    as our development set  and sets of mt   mt   as the test sets  the
numbers of documents in mt    mt    mt   are          and     respectively  case insensitive
nist b leu  papineni  roukos  ward    zhu        was used to measure translation performance 
we used minimum error rate training  och        to optimize the feature weights 
in our medium scale experiments  we used the fbis corpus as our bilingual training data  which
contains        documents     k sentence pairs with    m chinese words and     m english
words  in our large scale experiments  the bilingual training data consists of ldc    e    ldc    t    ldc    t    ldc    t   and ldc    t    hong kong hansards laws news  

  

fiz hang   x iao   x iong     l iu

these selected corpora contain         documents and     m sentences  on average  each document has      sentences 
we obtained symmetric word alignments of training data by first running giza    och   ney 
      in both directions and then applying the refinement rule grow diag final and  koehn et al  
       our hierarchical phrase translation rules were extracted from word aligned training data 
we used the srilm toolkit  stolcke        to train language models on the xinhua portion of the
gigaword corpus  which contains    m english words  we trained a   gram language model
for our medium scale experiments and a   gram language model for our large scale experiments 
in order to train the two monolingual topic models on the source and target side of our bilingual
training data  we used the open source lda tool gibbslda     gibsslda   is an implementation
of lda using gibbs sampling for parameter estimation and inference  the source  and targetside topic models were separately estimated from the chinese and english part of the bilingual
training data  we set the number of topic k      for both the source  and target side topic models 
and used the default setting of the tool for training and inference   during decoding  we inferred
the document topic distribution for each document in the dev test sets before translation using the
trained source side topic model  note that the topic inference on the dev test sets was performed
after all parameters of the two topic models were estimated on the training data 
the case insensitive bleu   was used as our evaluation metric  we performed the statistical
significance in bleu differences using the paired bootstrap re sampling  koehn         in order
to alleviate the impact of the instability of mert  we ran the tuning process three times for all our
large scale experiments and presented the average bleu scores on the three runs following the
suggestion by clark  dyer  lavie  and smith       
    medium scale experiments
in this section  we conducted medium scale experiments to investigate the effectiveness of our two
topic based models for translation rule selection 
      e ffect

of

t opic d issimilarity m odel

we quickly investigated the effectiveness of our topic dissimilarity and sensitivity model using
medium scale training data  results are shown in table    from the table  we can observe that
 if we use the topic dissimilarity model only with the source side or projected target side ruletopic distributions  dsimsrc dsimtrg in the table  see descriptions in section     we can
obtain an absolute improvement of           b leu points over the baseline 
 if we combine the two topic dissimilarity features together  we can achieve a further improvement of      b leu points over dsimsrc 
these two observations show that our topic dissimilarity model is able to improve translation quality
in terms of b leu 
   http   gibbslda sourceforge net 
   we determine k by testing                        in our preliminary experiments  we find that k      produces
a slightly better performance than other values  in order to improve the stability of the topic estimation  we run the
tool multiple times and use the best model with respect to the log likelihood 

  

fit opic  based d issimilarity

system
baseline
topiclex
dsimsrc
dsimtrg
dsimsrc dsimtrg
dsim sen

mt  
     
     
     
     
     
     

and

s ensitivity m odels

mt  
     
     
     
     
     
     

avg
     
     
     
     
     
     

speed
    
   
    
    
    
    

table    results of our topic dissimilarity and sensitivity model in terms of b leu and speed  words
per second   comparing with the traditional hierarchical system  baseline  and the system with the topic specific lexicon translation model  topiclex   dsimsrc and dsimtrg are topic dissimilarity features on the source side and projected target side ruletopic distributions respectively  dsim sen activates both the two dissimilarity features
and the two sensitivity features as described in section    avg denotes average b leu
scores on the two test sets  scores in bold are significantly better than baseline  p         
speed denotes the number of words translated per second 
rule type
phrase
monotone
reordering
all

count
   m
    m
   m
    m

src sen   
    
    
    
    

trg sen   
    
    
    
    

table    percentages of topic sensitive rules listed by rule types according to entropies of their
source side  src  and target side  trg  rule topic distributions  phrase rules are fully
lexicalized  while monotone and reordering rules contain nonterminals 

in order to gain insights into why the topic dissimilarity model is helpful for translation rule
selection  we further investigate how many rules are topic sensitive  as described in section     
we use entropy to measure whether a translation rule is topic sensitive based on its rule topic distribution  if the entropy of a translation rule calculated by the equation     is smaller than a certain
threshold  the rule is topic sensitive  since documents often focus on a few topics  we use the average entropy of document topic distributions of all training documents as the threshold  we compare
entropies of source side and target side rule topic distributions against this threshold  our findings
are shown in table          translation rules are topic sensitive rules if we compare entropies of
their source side rule topic distributions against the threshold  if we compare entropies of targetside rule topic distributions against the threshold  topic sensitive rules account for      these
strongly suggest that most rules only occur in documents with specific topics and topic information
can be used to improve translation rule selection 
      e ffect

of

t opic s ensitivity m odel

as we can see from table    there are still about     translation rules which are generic  not sensitive to any topics  these rules are also widely used in documents  as mentioned before  our
  

fiz hang   x iao   x iong     l iu

topic dissimilarity model always punishes such rules as documents are normally topic specific  we
therefore introduce a topic sensitivity model to complement the topic dissimilarity model  the experiment result of this model is show in the last line of table    we obtain a further improvement of
     b leu points when incorporating the topic sensitivity model  this indicates that it is necessary
to distinguish topic insensitive and topic sensitive rules 
      c omparison

with

t opic  s pecific l exicon t ranslation m odel

we also compared our topic models against the topic specific lexicon translation model proposed by
zhao and xing         they introduce a framework to combine hidden markov model  hmm  and
lda topic model for smt  which is shown in figure    in their framework  each bilingual sentence
pair has a single topic assignment sampled from the document pair topic distribution   then all
words of the target language  e g   english  are sampled given the sentence pair topic assignment
and a monolingual per topic word distribution   after that  word alignments and words of the
source language are sampled from a first order markov process and a topic specific translation
lexicon respectively 
zhao and xing integrate the topic specific word to word translation lexicons estimated from
their bilingual topic model described above into the topic specific lexicon translation model  which
is formulated as follows 
p  we  wf   df    p  wf  we   df  p  we  df  
x
 
p  wf  we   z   k p  we  z   k p  z   k df  

    

k

in this model  the probability of a candidate translation we for a source word wf in a source document df is calculated by marginalizing over all topics and corresponding topic specific translation
lexicons  we simplify the estimation of p wf  we   z   k  by directly computing these probabilities
on our word aligned corpus associated with target side topic assignments that are inferred from the
target side topic model  despite this simplification  the improvement of our implementation is comparable with the improvement obtained by zhao and xing         given a new document  we need
to adapt the lexical translation weights of rules  the adapted lexicon translation model is integrated
as a new feature into the log linear discriminative framework 
we show the comparison results in table    the topic specific lexicon translation model is
better than the baseline by     b leu points  however  our topic based method  the combination of
topic dissimilarity and sensitivity models  outperforms the baseline by      b leu points 
we also compare these two methods in terms of the decoding speed  words second   the baseline translates      words per second  while the system with the topic specific lexicon translation
model only translates     words in one second  the overhead of the topic specific lexicon translation model mainly comes from the adaptation of lexical weights  it takes       of the time to do
the adaptation  in contrast  our method has a speed of      words per second for each sentence on
average  which is three times faster than the topic specific lexicon translation method 
    large scale experiments
in this section  we investigated deeper into our models with the second group of experiments on
large scale training data 
  

fit opic  based d issimilarity

      e ffect

of

and

s ensitivity m odels

o ne   to  m any p rojection

as we discussed in section      we need to project target side topics onto source side topic space
so as to calculate the dissimilarity between a target side rule topic distribution and a source side
document topic distribution  we propose a one to many projection method for this issue  in order
to investigate the effectiveness of this method  we conducted experiments with large scale training
data to compare it with the following   other methods 
 one to one mapping we enforce a one to one mapping between source side and target side
topics  similar to the method by tam et al          we achieve this by aligning a target side
topic to the corresponding source side topic with the largest correspondence probability as
calculated in section     
 marginalization over word alignments following zhao and xing         we first obtain
topics on the target side using lda and then retrieve topics of the source language through a
marginalization over word alignments as follows 
x
p  wf  k   
p  wf  we  p  we  z   k 
    
we

 combination of the source and target language documents we concatenate each target document and its aligned source document into one document  we then run the lda tool on these
combined documents to train one topic model with mixed language words  during decoding 
we use the trained topic model to infer topics only on source documents 
in order to compare our one to many projection method with the three methods described above 
we only add the target side topic dissimilarity feature  dsimtrg  to the log linear translation model 
the experiment results are reported in table    clearly  all the four methods achieve improvements
over the baseline  however  our one to many projection method performs better than all three other
methods  in particular 
 our method outperforms the one to one topic mapping method  which indicates that sourceside and target side topics do not exactly match in a one to one correspondence manner 
 the reason that the marginalization method performs the worse among the four methods may
be that the topic model is trained only on target documents 
 surprisingly  the combination method performs quite well  this shows that the lda model
can find hidden topics even on mixed language documents 

      e ffect
rules

of the

t opic  based rule s election f ramework

on

various t ypes

of

we conducted experiments to further investigate the effect of our topic based models for various
types of rules selection  particularly  we divide translation rules in hierarchical phrase based smt
into three types     phrase rules  which only contain terminals and are the same as bilingual phrase
pairs used in phrase based system     monotone rules  which contain non terminals and produce
  

fiz hang   x iao   x iong     l iu

system
baseline
one to one
marginalization
combination
one to many

mt  
     
     
     
     
     

mt  
     
     
     
     
     

avg
     
     
     
     
     

table    effect of our one to many topic projection method against other methods  marginalization  marginalization over word alignments  combination  combination of the source
and target language documents 
system
baseline
phrase rule
monotone rule
reordering rule
all

mt  
     
     
     
     
     

mt  
     
     
     
     
     

avg
     
     
     
     
     

table    effect of our topic based rule selection models on three types of rules  phrase rules are
fully lexicalized  while monotone and reordering rules contain nonterminals 

monotone translations  and finally    reordering rules  which also contain non terminals but change
the order of translations  we define the monotone and reordering rules according to chiang et al 
       
when we study the impact of our topic based models on translation rule type a  we activate all
of the four features described in section   only on those rules of type a  topic dissimilarity and
sensitivity features on the other two types of translation rules are deactivated 
table   shows the experiment results  from the table  we can observe that
 our topic based models achieve the highest improvement of      b leu points over the baseline on phrase rules among the three types of translation rules  this is reasonable as phrase
rules consist of topical words 
 we also obtain improvements of     and      b leu points over the baseline on the monotone
and reordering rules respectively  this shows that our models are also able to help select
appropriate translation rules with non terminals 
 when we activate the topic dissimilarity and sensitivity models on all translation rules  we can
still achieve an additional improvement of      b leu points  in total  our models outperform
the baseline by an absolute improvement of     b leu points 
      e ffect

of

m ore m onolingual data

comparing table   and table    we find that our topic based dissimilarity and sensitivity models
trained with medium scale data  about   k documents  collectively achieve an improvement of     
  

fit opic  based d issimilarity

and

s ensitivity m odels

system
baseline
dsimsrc   sensrc   dsimtrg   sentrg
dsimsrc   sensrc   dsimtrg   sentrg
dsimsrc   sensrc   dsimtrg   sentrg
dsimsrc   sensrc   dsimtrg   sentrg

mt  
     
     
     
     
     

mt  
     
     
     
     
     

avg
     
     
     
     
     

table    effect of using more monolingual data to train topic models  the features in bold are the
topic based dissimilarity sensitivity model where the lda topic model is trained using the
combination of source target part of the large scale bilingual data and the corresponding
monolingual corpus 

b leu points over the baseline while the two models trained with large scale data  about    k documents  obtain an improvement of     b leu points  this suggests that further performance gains
may be obtained if we have more data  as parallel bilingual data with document boundaries provided is not easily accessible  we try to collect monolingual data of the source or and target language 
our interest is to study whether we can gain further improvements by using more monolingual data
to train our topic models 
we used a chinese monolingual corpus where documents were collected from the chinese sohu
weblog in        the collected chinese corpus contains    k documents with      m chinese
words  we also used an english monolingual corpus where documents were collected from the
english blog authorship corpus  schler  koppel  argamon    pennebaker         the english
monolingual corpus consists of    k documents with   m english words  we combined this new
chinese corpus with the source part of our large scale bilingual data to train a source side lda topic
model st   the english monolingual corpus is also combined with the target part of the large scale
bilingual data to train a target side lda topic model t t  
we then used the two topic models st and t t to infer topics for the test sets  topic information on the source and target part of the large scale bilingual training data inferred by st and t t
was used to estimate source side rule topic distributions and projected target side rule topic distributions  in this way  we can obtain a new topic based dissimilarity and sensitivity model on the
source target side 
experiment results are shown in table    unfortunately  we can not obtain any further improvements by training topic models on larger data  such as the combination of chinese monolingual
corpus and the source part of our bilingual training data  instead  the performance drops from      
to       if we use the topic model st to build the source side dissimilarity and sensitivity features
and to       if we adopt the topic model t t to build the target side dissimilarity and sensitivity
features 
one reason for the lower performance with larger topic model training data may be that we only
use    topics  using more topics may improve our models on these larger corpora  in order to
investigate this  we conducted new experiments with more topics than     we trained our sourceside topic model using the combination of source part of the large scale bilingual data and the sohu
weblog data  based on this topic model  we built our source side topic dissimilarity model and
   http   blog sohu com  

  

fiz hang   x iao   x iong     l iu

system
baseline
k     
k     
k      
k      

mt  
     
     
     
     
     

mt  
     
     
     
     
     

avg
     
     
     
     
     

table    experiment results with different number of topics  k   only the source side topic dissimilarity model  dsimsrc  is integrated into the smt system 
test set
mt  
mt  

monosrc
     
     

bisrc
     
     

monobisrc
     
     

table    the hellinger distances of the mt      test sets to the chinese monolingual corpus
 monosrc  and the source part of the bilingual training data  bisrc  as well as their combination  monobisrc  in terms of their average document topic distributions 

integrated it into our smt system  experiment results are shown in table    from this table  we
find that using more topics is not able to improve our model on these corpora 
yet another reason may be that the additional monolingual corpus is not similar to the test sets in
terms of their topic distributions  in order to examine this hypothesis  we inferred document topic
distributions for all documents in the test sets  the chinese monolingual corpus and the source part of
the bilingual corpus using the topic model st   we then average these document topic distributions
and obtain four average document topic distributions for mt    mt    the chinese monolingual
corpus and the source part of the bilingual corpus respectively  these average topic distributions can be approximated as the corpus topic distributions over the four corpora  we calculate the
hellinger distances between the corpus topic distributions of the test sets and those of the chinese
monolingual corpus and the source part of the bilingual training data  which are shown in table   
from the table  we can clearly find that the additional monolingual corpus is much less similar
to the test sets comparing with the bilingual training corpus  the hellinger distance of the test
set mt   to the monobisrc corpus is almost twice as large as that to the bilingual training data
       vs          a topic model trained on such an enlarged corpus will make our topic based
models select translation rules that are not similar to the documents of the test sets in terms of topic
distributions  this suggests that we should select additional monolingual data that are similar to the
test sets if we want to obtain further improvements 
we further conducted a new group of experiments to empirically examine this hypothesis by
translating a web domain test set that is similar to the additional weblog corpus in terms of their
topics  we used the web portion of the nist mt   set as our new development set and the web
portion of the nist mt   as the new test set  results are displayed in table     which show that
the additional monolingual data can improve the performance this time  this again suggests that we
should select monolingual corpus that is similar to our test sets to learn topics for our topic based
dissimilarity and sensitivity models 
  

fit opic  based d issimilarity

and

s ensitivity m odels

system
baseline
dsimsrc   sensrc   dsimtrg   sentrg
dsimsrc   sensrc   dsimtrg   sentrg

mt   web
     
     
     

table     results of translating a web domain test set with our topic based models trained on the
data augmented with the monolingual weblog corpus  the features in bold are the topicbased dissimilarity sensitivity model where the lda topic model is trained using the
combination of source target part of the large scale bilingual data and the corresponding
monolingual corpus  mt   web is the web portion of the nist mt   test set 

   analysis
in this section  we will study more details of our topic based models for translation rule selection by
looking at the differences that they make on target documents and individual translation hypotheses 
these differences will help us gain some insights into how the presented models improve translation
quality  in the analysis  both the baseline system and the system that is enhanced with the proposed
topic based models  all four features in section   activated  are trained with the large scale bilingual
data as described in section      for notational convenience  hereafter we refer to the baseline
system as base and the system enhanced with our topic based dissimilarity and sensitivity models
as topsel 
    differences on target documents
in order to measure the impact that our topic based models have on target documents  we calculate
the hellinger distances between target documents generated by the base   topsel system and reference documents generated by human in terms of their topics inferred by the target side lda topic
model according to the following   steps  the target side lda topic model is trained on the target
part of the large scale bilingual data described in section     
 use the target side lda topic model to infer document topic distribution for each document
in reference translations  called reference distribution  
 use the target side lda topic model to infer document topic distribution for each target document generated by the base system  called base distribution  
 similarly  we can obtain
topsel system 

topsel

distribution on each target document generated by the

 calculate the dissimilarity between the base and reference distribution as well as that between the topsel and reference distribution according to the equation      these dissimilarities are first averaged on all documents and then averaged on four reference translations 
table    shows the calculated dissimilarities  according to the equation      the smaller the
hellinger distance between two items  the more similar they are  the average hellinger distance
between topsel and reference documents is       while the distance between base and reference
  

fiz hang   x iao   x iong     l iu

system
base
topsel

mt  
     
     

mt  
     
     

avg
     
     

table     dissimilarities  measured by hellinger distance  between reference documents and target
documents generated by the base and topsel system in terms of their topics according
to the equation     

more similar    
less similar    
p 

mt  
  
  
    

table     the number of target documents generated by
erence documents than those by base 

mt  
  
  
    
topsel

that are more less similar to ref 

documents is        therefore  the target documents generated by the topsel system in both
mt   and mt   are more similar to the documents in reference translations than those by the
baseline system  we further calculate the number of target documents generated by topsel that
are more less similar to reference documents than those by base based on the average hellinger
distances  these numbers are shown in table     according to a sign test using these numbers 
topsel is statistically significantly better than the baseline system in terms of the similarity of
translations generated by the two systems to human generated translations 
    differences on translation hypotheses
we now look deeper into translation hypotheses to understand how our models select translation
rules  table    shows three translation examples that compare the baseline against the system
enhanced with our topic based models  in order to conduct a quantitative comparison  we calculate
dissimilarity values  measured by hellinger distance  of all underlined phrases in table    using
our topic based dissimilarity model  the dissimilarity values are computed between the projected
target side rule topic distributions of the underlined phrases and the source side document topic
distributions of the corresponding documents where these phrase are used  the values are shown in
table    
from the two tables  we can easily observe that the system with the topic based dissimilarity
model prefers those target phrases that have smaller hellinger distances to the documents where they
occur in terms of topic distributions  in contrast  the baseline is not able to use this document level
topic information for translation rule selection  figure   further shows the topic distributions of the
source side document  the topsel phrase allow and the base phrase permit in eg     the
major topics of the source side document are topic    and     the topsel phrase allow mainly
distributes over    different topics  including topic    and    while the base phrase permit
mainly over    different topics which do not include topic    
   the distribution probability over these topics is larger than      

  

fit opic  based d issimilarity

source

s ensitivity m odels

              

he described the  northern limit line  and unlawful  
he referred to the  northern limit line  is not legitimate  
reference he pointed out that the  northern limit line  is not legitimate  
source
base
how would permit its love others also accepted by the people  
topsel
will allow their love of love others also accepted by the people
reference how would someone allow the person he loves to accept other peoples
love at the same time
source
base
at present   the internet is not entitled to such a statutory right to leave
topsel
at present the internet does not enjoy such a statutory right to leave
reference at present the internet does not enjoy these rights
base

eg   

and

topsel

no   n gc o       o   o

eg   

  p  k    n   

eg   

table     translation examples from the nist mt      test sets  comparing the baseline with
the system enhanced with the topic based models  the underlined words highlight the
difference between the enhanced models and the baseline 
phrase
unlawful
not legitimate
permit
allow
entitled to
enjoy

hd
    
    
    
    
    
    

table     dissimilarity values  measured by hellinger distance  of the underlined phrases in table    between their projected target side rule topic distributions and the corresponding
source side document topic distributions of documents calculated by our topic based dissimilarity model 

   discussion on bilingual topic modeling
although topic models are widely adopted in monolingual text analysis  bilingual or multilingual
topic models are less explored  especially those tailored for multilingual tasks such as machine
translation  in this section we try to provide some suggestions for bilingual topic modeling from
the perspective of statistical machine translation as well as our practice on the integration of topic models into smt  these suggestions are listed as follows  some of which are also our future
directions 
 investigation on topic divergences across different languages cross language divergences
are pervasive and become one of big challenges for machine translation  dorr         such
language level divergences hint that divergences at the topic or concept level may also exist
across languages  this may explain why our one to many topic projection from the target side
  

fiz hang   x iao   x iong     l iu

figure    topic distributions of the source side document  a   the
the base phrase permit shown in eg    of table    

topsel

phrase allow  b  and

to the source side is better than the one to one mapping  although mimno et al         have
studied on topic divergences using wikipedia articles  we believe that a deeper and wider
investigation on topic divergence is needed as it will shed new light on how we can build
better bilingual topic models 
 adding more linguistic assumptions into topic modeling practices in smt show that integrating more linguistic knowledge into machine translation normally generates better translations
 chiang et al          we believe that adding more linguistic assumptions beyond bag ofwords will also improve topic modeling  a flexible topic modeling framework that allows us
to integrate rich linguistic knowledge in the form of features will definitely further facilitate
the application of topic models in natural language processing 
 joint modeling of topic induction and synchronous grammar induction synchronous grammar induction for machine translation is a task of automatically learning translation rules from
bilingual data  blunsom  cohn  dyer    osborne        xiao   xiong         as bayesian
approaches are successfully used in both topic modeling and synchronous grammar induction  joint modeling of them is an very interesting direction  which will also benefit grammar
adaptation from one domain to another domain in machine translation 

    conclusions
in this article we have presented a topic based translation rule selection framework which incorporates the topic information from both the source and target language for translation rule disambiguation  particularly  we use a topic dissimilarity model to select appropriate translation rules for
documents according to the similarities between translation rules and documents  we also adopt a
  

fit opic  based d issimilarity

and

s ensitivity m odels

topic sensitivity model to complement the topic dissimilarity model in order to balance translation
rule selection between topic sensitive and topic insensitive rules  in order to calculate dissimilarities between source  and target side topic distributions  we project topic distributions on the target
side onto the source side topic model space in a new and efficient way 
we have integrated our topic based rule selection models into a hierarchical phrase based smt
system  experiments on medium large scale training data show that
 our topic dissimilarity and sensitivity model are able to substantially improve translation
quality in terms of b leu and improve translation rule selection on various types of rules  i e  
phrase monotone reordering rules  
 our method is better than previous topic specific lexicon translation method in both translation quality and decoding speed 
 the proposed one to many projection method also outperforms various other methods such
as one to one mapping  marginalization via word alignments and so on 
 if we want to use additional monolingual corpus to train topic models  we should first investigate whether the new monolingual corpus is similar to the test data in terms of topic
distributions 
topic models can provide global and document level information for machine translation  in
the future  we would like to use topic models to address document level machine translation issues 
such as coherence and cohesion  barzilay   lee        hardmeier  nivre    tiedemann         we
also want to integrate our topic based models into linguistically syntax based machine translation
for syntactic translation rule selection  liu et al         

acknowledgments
the work was sponsored by the national natural science foundation of china under projects
         and           qun lius work was partially supported by science foundation ireland
 grant no     ce i      as part of the cngl at dublin city university  we would like to thank
three anonymous reviewers for their insightful comments  the corresponding author of this article
is deyi xiong 

references
barzilay  r     lee  l          catching the drift  probabilistic content models  with applications to
generation and summarization  in susan dumais  d  m     roukos  s   eds    hlt naacl
      main proceedings  pp          boston  massachusetts  usa  association for computational linguistics 
blei  d  m     lafferty  j  d          a correlated topic model of science  aas             
blei  d  m   ng  a     jordan  m          latent dirichlet allocation  jmlr             

  

fiz hang   x iao   x iong     l iu

blunsom  p   cohn  t   dyer  c     osborne  m          a gibbs sampler for phrasal synchronous
grammar induction  in proceedings of the joint conference of the   th annual meeting of
the acl and the  th international joint conference on natural language processing of the
afnlp  pp          suntec  singapore  association for computational linguistics 
boyd graber  j     blei  d  m          multilingual topic models for unaligned text  in proceedings
of the twenty fifth conference on uncertainty in artificial intelligence  uai     pp       
arlington  virginia  united states  auai press 
carpuat  m     wu  d       a   how phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation  in proceedings of the   th conference on
theoretical and methodological issues in machine translation  pp       
carpuat  m     wu  d       b   improving statistical machine translation using word sense disambiguation  in proceedings of the      joint conference on empirical methods in natural
language processing and computational natural language learning  emnlp conll   pp 
      prague  czech republic  association for computational linguistics 
chan  y  s   ng  h  t     chiang  d          word sense disambiguation improves statistical machine translation  in proceedings of the   th annual meeting of the association of computational linguistics  pp        prague  czech republic  association for computational
linguistics 
chiang  d          a hierarchical phrase based model for statistical machine translation  in proc 
acl      
chiang  d          hierarchical phrase based translation  computational linguistics            
    
chiang  d   marton  y     resnik  p          online large margin training of syntactic and structural
translation features  in proceedings of the      conference on empirical methods in natural language processing  pp          honolulu  hawaii  association for computational
linguistics 
clark  j  h   dyer  c   lavie  a     smith  n  a          better hypothesis testing for statistical
machine translation  controlling for optimizer instability  in proceedings of the   th annual
meeting of the association for computational linguistics  human language technologies 
pp          portland  oregon  usa 
dorr  b  j          machine translation divergences  a formal description and proposed solution 
computational linguistics                
foster  g     kuhn  r          mixture model adaptation for smt  in proc  of the second workshop
on statistical machine translation  pp          prague  czech republic 
gong  z   zhang  m     zhou  g          cache based document level statistical machine translation  in proc  emnlp      
gong  z   zhang  y     zhou  g          statistical machine translation based on lda  in proc 
iucs       p          



griffiths  t  l     steyvers  m          finding scientific topics  proceedings of the national
academy of sciences      suppl               

  

fit opic  based d issimilarity

and

s ensitivity m odels

hardmeier  c   nivre  j     tiedemann  j          document wide decoding for phrase based statistical machine translation  in proceedings of the      joint conference on empirical
methods in natural language processing and computational natural language learning 
pp            jeju island  korea  association for computational linguistics 
he  z   liu  q     lin  s          improving statistical machine translation using lexicalized rule
selection  in proceedings of the   nd international conference on computational linguistics
 coling        pp          manchester  uk  coling      organizing committee 
hofmann  t          probabilistic latent semantic analysis  in proc  of uai       pp         
koehn  p          statistical significance tests for machine translation evaluation  in proceedings
of emnlp       pp          barcelona  spain 
koehn  p   och  f  j     marcu  d          statistical phrase based translation  in proc  hlt naacl
     
lin  j          divergence measures based on the shannon entropy  ieee trans  inf  theor         
       
liu  q   he  z   liu  y     lin  s          maximum entropy based rule selection model for syntaxbased statistical machine translation  in proceedings of the      conference on empirical methods in natural language processing  pp        honolulu  hawaii  association for
computational linguistics 
liu  y   liu  q     lin  s          tree to string alignment template for statistical machine translation  in proc  acl      
mimno  d   wallach  h  m   naradowsky  j   smith  d  a     mccallum  a          polylingual
topic models  in proc  of emnlp      
och  f  j     ney  h          discriminative training and maximum entropy models for statistical
machine translation  in proc  acl      
och  f  j          minimum error rate training in statistical machine translation  in proc  acl      
och  f  j     ney  h          a systematic comparison of various statistical alignment models 
computational linguistics              
papineni  k   roukos  s   ward  t     zhu  w  j          bleu  a method for automatic evaluation
of machine translation  in proc  acl      
ruiz  n     federico  m          topic adaptation for lecture translation through bilingual latent
semantic models  in proceedings of the sixth workshop on statistical machine translation 
schler  j   koppel  m   argamon  s     pennebaker  j  w          effects of age and gender on
blogging  in aaai spring symposium  computational approaches to analyzing weblogs  pp 
       
stolcke  a          srilm  an extensible language modeling toolkit  in proc  icslp      
tam  y  c   lane  i  r     schultz  t          bilingual lsa based adaptation for statistical machine
translation  machine translation                
tiedemann  j          context adaptation in statistical machine translation using models with exponentially decaying cache  in proceedings of the      workshop on domain adaptation for

  

fiz hang   x iao   x iong     l iu

natural language processing  pp       uppsala  sweden  association for computational
linguistics 
ture  f   oard  d  w     resnik  p          encouraging consistent translation choices  in proceedings of the      conference of the north american chapter of the association for computational linguistics  human language technologies  pp          montreal  canada  association for computational linguistics 
xiao  t   zhu  j   yao  s     zhang  h          document level consistency verification in machine
translation  in proceedings of the      mt summit xiii  pp          xiamen  china 
xiao  x     xiong  d          max margin synchronous grammar induction for machine translation  in proceedings of the      conference on empirical methods in natural language
processing  pp          seattle  washington  usa  association for computational linguistics 
xiao  x   xiong  d   zhang  m   liu  q     lin  s          a topic similarity model for hierarchical phrase based translation  in proceedings of the   th annual meeting of the association
for computational linguistics  volume    long papers   pp          jeju island  korea 
association for computational linguistics 
xiong  d   zhang  m     li  h          modeling the translation of predicate argument structure
for smt  in proceedings of the   th annual meeting of the association for computational
linguistics  volume    long papers   pp          jeju island  korea  association for computational linguistics 
zhao  b     xing  e  p          hm bitam  bilingual topic exploration  word alignment  and
translation  in proc  nips      
zhao  b     xing  e  p          bitam  bilingual topic admixture models for word alignment  in
proc  acl      

  

fi