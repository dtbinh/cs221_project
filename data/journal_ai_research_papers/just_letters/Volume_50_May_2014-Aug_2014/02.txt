journal of artificial intelligence research          

submitted        published      

a decision theoretic model of assistance
alan fern

afern   eecs   oregonstate   edu

school of eecs  oregon state university  corvallis  or usa

sriraam natarajan

natarasr   indiana   edu

soic  indiana university  bloomington  in usa

kshitij judah

judahk   eecs   oregonstate   edu

school of eecs  oregon state university  corvallis  or usa

prasad tadepalli

tadepall   eecs   oregonstate   edu

school of eecs  oregon state university  corvallis  or usa

abstract
there is a growing interest in intelligent assistants for a variety of applications from sorting
email to helping people with disabilities to do their daily chores  in this paper  we formulate the
problem of intelligent assistance in a decision theoretic framework  and present both theoretical
and empirical results  we first introduce a class of pomdps called hidden goal mdps  hgmdps  
which formalizes the problem of interactively assisting an agent whose goal is hidden and whose
actions are observable  in spite of its restricted nature  we show that optimal action selection
for hgmdps is pspace complete even for deterministic dynamics  we then introduce a more
restricted model called helper action mdps  hamdps   which are sufficient for modeling many
real world problems  we show classes of hamdps for which efficient algorithms are possible 
more interestingly  for general hamdps we show that a simple myopic policy achieves a near
optimal regret  compared to an oracle assistant that knows the agents goal  we then introduce
more sophisticated versions of this policy for the general case of hgmdps that we combine with
a novel approach for quickly learning about the agent being assisted  we evaluate our approach
in two game like computer environments where human subjects perform tasks  and in a real world
domain of providing assistance during folder navigation in a computer desktop environment  the
results show that in all three domains the framework results in an assistant that substantially reduces
user effort with only modest computation 

   introduction
personalized ai systems that interactively assist their human users have received significant attention in recent years  yorke smith  saadati  myers    morley        lieberman        myers  berry 
blythe  conleyn  gervasio  mcguinness  morley  pfeffer  pollack    tambe         however  an
overarching formal framework of interactive assistance that captures all these different systems and
provides a theoretical foundation is largely missing  in this paper we address this lacuna by introducing a general framework of decision theoretic assistance  analyzing the problem complexity under
different assumptons  proposing different heuristic solutions  and evaluating their effectiveness 
we consider a model where the assistant observes a goal oriented agent and must select assistive
actions in order to best help the agent achieve its goals  in real applications  this requires that the
assistant be able to handle uncertainty about the environment and the agent  to reason about varying
c
    
ai access foundation  all rights reserved 

fif ern   natarajan   j udah     tadepalli

action costs  to handle unforeseen situations  and to adapt to the agent over time  here we consider
a decision theoretic model  based on partially observable markov decision processes  pomdps  
which naturally handles these features  providing a formal basis for designing intelligent assistants 
the first contribution of this work is to formulate the problem of selecting assistive actions as
a class of partially observable markov decision processes  pomdps  called hidden goal mdps
 hgmdps   which jointly models the application environment along with the agents policy and
hidden goals  a key feature of this approach is that it explicitly reasons about the environment and
the agent  which provides the potential flexibility for assisting in ways unforeseen by the developer
as new situations are encountered  thus  the developer need not design a hand coded assistive
policy for each preconceived application scenario  instead  when using our framework  the burden
on the developer is to provide a model of the application domain and the agent  or alternatively a
mechanism for learning one or both of these models from experience  our framework then uses
those models in an attempt to compute  in any situation  whether assistance could be beneficial and
if so what assistive action to select 
the second contribution of this work is to analyze the properties of this formulation  despite
the restricted nature of hgmdps  the complexity of determining if an hgmdp has a finite horizon
policy of a given value is pspace complete even in deterministic environments  this motivates a
more restricted model called helper action mdp  hamdp   where the assistant executes a helper
action at each step  the agent is obliged to accept the helper action if it is helpful for its goal and
receives a reward bonus  or cost reduction  by doing so  otherwise  the agent can continue with
its own preferred action without any reward or penalty to the assistant  we show classes of this
problem that are complete for pspace and np  we also show that for the class of hamdps with
deterministic agents there are polynomial time algorithms for minimizing the expected and worstcase regret relative to an oracle assistant that knows the goal of the agent  further  we show that the
optimal worst case regret can be characterized by a graph theoretic property called the tree rank of
the corresponding all goals policy tree and can be computed in linear time 
in principle  given a hgmdp  one could apply a pomdp solver in order to arrive at an optimal
assistant policy  unfortunately  the relatively poor scalability of pomdp solvers will often force us
to utilize approximate heuristic solutions  this is particularly true when the assistant is continually
learning updated models of the agent and or environment  which results in a sequence of more
accurate hgmdps  each of which needs to be solved  a third contribution of our work is a set
of myopic action selection mecahnisms that approximate the optimal policy  for hamdps  we
analyze a myopic heuristic and show that it has a regret which is upper bounded by the entropy
of the goal distribution for hamdps  furthermore we give a variant of this policy that is able
to achieve worst case and expected regret that is logarithmic in the number of goals without any
prior knowledge of the goal distribution  we also describe two other approaches that are based
on a combination of explicit goal estimation  myopic heuristics  and bounded search and are more
generally applicable to hgmdps 
in order for the above approach to be useful  the hgmdp must incorporate a reasonably accurate model of the agent being assisted  a fourth contribution of our work is to describe a novel
model based bootstrapping mechanism for quickly learning the agent policy  which is important
for the usability of an assistant early in its lifetime  the main idea is to assume that that agent is
close to rational in the decision theoretic sense  which motivates defining a prior on agent policies
that places higher probability on policies that are closer to optimal  this prior in combination with

  

fia d ecision  t heoretic m odel of a ssistance

bayesian updates allows for the agent model to be learned quickly when the rationality assumption
is approximately satisfied 
the final contribution of our work is to evaluate our framework in three domains  first we
consider two game like computer environments with human subjects  the results in these domains
show that the assistants resulting from our framework substantially reduce the amount of work
performed by the human subjects  we also consider a more realistic domain  the folder navigator
 bao  herlocker    dietterich        of the task tracer project  in this domain  the user navigates
the directory structure searching for a particular location to open or save a file  which is unknown to
the assistant  the job of the assistant is to predict the users destination folder and take actions that
provide short cuts to reach it  the results show that our generic assistant framework compares
favorably to the hand coded solution of bao et al 
the remainder of this paper is organized as follows  in the next section  we introduce our formal problem setup in terms of hgmdps  followed by an analysis of computational complexity and
guarantees for myopic heuristics in the case of hamdps  next  we present our approximate solution approach for general hgmdps based on goal estimation and online action selection  finally
we give an empirical evaluation of the approach in three domains and conclude with a discussion of
related and future work 

   hidden goal markov decision processes
throughout the paper we will refer to the entity that we are attempting to assist as the agent and the
assisting entity as the assistant  we consider an episodic problem setting where at the beginning of
each episode the agent begins in some world state and selects a goal from a finite set of possible
goals  the goal set  for example  might contain all possible dishes that the agent might be interested
in cooking  or all the possible destination folders that the agent may possibly navigate to  importantly  the assistant can fully observe the world state and the agents actions  but cannot observe the
goal of the agent  we model the interaction between the agent and assistant as sequential where
the agent and assistant alternate turns  taking a single action per turn  possibly noop    the episode
ends when either the agents or the assistants action leads to a goal state  an immediate reward
is accumulated after each action during the episode  the total reward of an episode is equal to the
sum of the rewards obtained during the episode  note that the available actions for the agent and
assistant need not be the same and may have varying rewards  since the assistant and the agent
share the same objective  all rewards are viewed from the perspective of the agent  the objective of
the assistant is to behave in a way that maximizes the expected total reward of an episode 
more formally  we model the above interaction via hidden goal markov decision processes
 hgmdps   an hgmdp is an mdp in which the goal of the user is not observed while the rest
of the environment is completely observed  an hgmdp is a tuple hs  g  a  a    t  r    is   ig i
where s is a set of states  g is a finite set of possible agent goals  a is the set of agent actions  and
a  is the set of assistant actions  typically a  will include a noop action  which allows the assistant
to decide not to provide any assistance at a particular decision epoch  t is the transition function
where t  s  g  a  s    is the probability of a transition to state s  from s after taking action a  a  a 
when the agents goal is g  r is the reward function which maps s  g   a  a    to real values  
   we consider this strictly alternating turn model for simplicity  however  it is straightforward to use this model to
capture interactions that are not strictly alternating  e g  allowing the assistant or agent to take multiple actions in a
row 

  

fif ern   natarajan   j udah     tadepalli

is the agents policy that maps s  g to distributions over a and need not be optimal in any sense 
is  ig   is an initial state  goal  distribution 
an assistant policy for an hgmdp defines a distribution over actions given the sequence of
preceding observations  i e   the sequence of state action pairs  it is important that the assistant policy depends on the history rather than only the current state since the entire history can potentially
provide evidence about the goal of the agent  which may be necessary for selecting an appropriate
action  we must now define an objective function that will be used to evaluate the value of a particular assistant policy  we consider a finite horizon episodic problem setting  where each hgmdp
episode begins by drawing an initial state s  is and a goal g  ig   the process then alternates
between the agent and the assistant executing actions  including noops  in the environment until the
horizon or a terminal state is reached  the agent is assumed to select actions according to   in
many domains  a terminal goal state will be reached within the horizon  though in general  goals
can have arbitrary impact on the reward function  the reward for the episode is equal to the sum
of the rewards of the actions executed by the agent and assistant during the episode  the objective
of the assistant is to reason about the hgmdp and observed state action history in order to select
actions that maximize the expected  or worst case  total reward of an episode 
before proceeding further it is worth reviewing some of the assumptions of the above formulation and the potential implications 
 partial observability  the definition of the hgmdp is very similar to the definition of a
partially observable markov decision process  pomdp   in fact  the hgmdp is a special
case of the pomdp where the unobserved part of the state space consists of a single component which corresponds to the goal of the user  for simplicity we have assumed that the
world state is fully observable  this choice is not fundamental to our framework and one can
imagine relatively straightforward extensions of our techniques that model the environment
as a partially observable mdp  pomdp  where the world states are not fully observable  in
section    we shed some light on the hardness of hgmdps and describe some specialized
heuristic solutions with performance guarantees 
 agent policy  we have also assumed that the agent is modeled as a memoryless reactive
policy that gives a distribution over actions conditioned on only the current world state and
goal  this assumption is also not fundamental to our framework and one can also extend
it to include more complex models of the user  for example  that include hierarchical goal
structures  such an extension has been explored previously  natarajan  tadepalli    fern 
      
 sequential interaction  we have also assumed for simplicity an interaction model between
the assistant and agent that involves interleaved  sequential actions rather than parallel actions 
this  for example  precludes the assistant from taking actions in parallel with the agent  while
parallel assistive actions are useful in many cases  there are many domains where sequential
actions are the norm  we are especially motivated by domains such as intelligent desktop
assistants that help store and retrieve files  filter spam  sort email  etc   and smart homes
that open doors  switch on appliances  and so on  many opportunities for assistance in these
domains are of the sequential variety  in many cases  tasks that appear to require parallel
activity can often be formulated as a set of threads where each thread is sequential and hence

  

fia d ecision  t heoretic m odel of a ssistance

can be formulated as a separate assistant  extending our framework to handle general parallel
assistance is an interesting future direction 
 goal dependent transitions  the dependence of the reward and policy on the goal allows
the model to capture the agents desires and behavior under each goal  the dependence of t
on the goal is less intuitive and in many cases there will be no dependence when t is used
only to model the dynamics of the environment  however  we allow goal dependence of t
for generality of modeling  for example  it can be convenient to model basic communication
actions of the agent as changing aspects of the state  and the result of such actions will often
be goal dependent 
there are two main obstacles to solving the problem of intelligent assistance in our framework 
first  in many scenarios  initially the hgmdp will not be directly at our disposal since we will lack
accurate information about the agent policy  and or the goal distribution ig   this is often due to
the fact that the assistant will be deployed for a variety of initially unknown agents  rather  the assistant will find itself in an environment and be only given the possible set of goals  as described in
section    our approach to this difficulty is to learn an approximate hgmdp by estimating the agent
policy  and the goal distribution ig   furthermore  we will also describe a bootstrapping mechanism for learning these approximations quickly  the second obstacle to solving the hgmdp is the
generally high computational complexity of solving hgmdps  to deal with this issue section  
considers various approximate techniques for efficiently solving the hgmdps 
it should be mentioned that it is possible to provide assistance using simpler domain specific
engineered solutions  in particular  the domains we consider later could be solved using several less
expensive solutions and do not require the machinery of hgmdps  in fact  in one of our domains 
we compare our model against an existing supervised learning method  the goal of our work is
to provide a domain independent framework that can potentially encapsulate several such assistant
systems with the hope that it gives rise to a robust understanding and methodology of building such
systems with much less human effort in the future 

   theoretical analysis
in this section  we will analyze the hardness of solving hgmdps  and show that despite being a
special case of pomdps  they are just as hard  this motivates a new model called the helper action
mdp  hamdp  which is more restricted and is more amenable to approximate solutions  we will
then introduce a myopic heuristic to solve hamdps and analyze its performance  we will also
analyze some special cases of hamdps which permit more efficient solutions 
    complexity results on hidden goal mdps
given knowledge of the agents goal g in an hgmdp  the assistants problem reduces to solving an
mdp over assistant actions  the mdp transition function captures both the state change due to the
assistant action and also the ensuing state change due to the agent action selected according to 
given g  likewise the reward function on a transition captures the reward due to the assistant action
and the ensuing agent action conditioned on g  the optimal policy for this mdp corresponds to an
optimal assistant policy for g  however  since the real assistant will often have uncertainty about
the agents goal  it is unlikely that this optimal performance will be achieved 

  

fif ern   natarajan   j udah     tadepalli

we can view an hgmdp as a collection of  g  mdps that share the same state space  where
the assistant is placed in one of the mdps at the beginning of each episode  but cannot observe
which one  each mdp is the result of fixing the goal component of the hgmdp definition to one
of the goals  this collection can be easily modeled as a restricted type of partially observable mdp
 pomdp  with a state space s  g  the s component is completely observable  while the g component is unobservable but only changes at the beginning of each episode  according to ig   and
remains constant throughout an episode  furthermore  each pomdp transition provides observations of the agent action  which gives direct evidence about the unchanging g component  from
this perspective hgmdps appear to be a significant restriction over general pomdps  however 
our first result shows that despite this restriction the worst case complexity is not reduced even for
deterministic dynamics 
given an hgmdp m   a horizon m   o  m     and a reward target r   the short term reward
maximization problem asks whether there exists a history dependent assistant policy that achieves
an expected finite horizon reward of at least r   for general pomdps this problem is pspacecomplete  papadimitriou   tsitsiklis        mundhenk         and for pomdps with deterministic
dynamics  it is np complete  littman         however  we have the following result 
theorem    short term reward maximization for hgmdps with deterministic dynamics is pspacecomplete 
proof  membership in pspace follows from the fact that any hgmdp can be polynomially encoded as a pomdp for which policy existence is in pspace  to show pspace hardness  we
reduce the pspace complete problem tqbf  truth of quantified boolean formula  to the problem
of the existence of a history dependent assistant policy of expected reward  r 
let  be a quantified boolean formula in the form x  x  x        xn  c   x            xn         
 cm  x            xn     where each ci is a disjunctive clause  for us  each goal gi is a clause  the
agent chooses a goal uniformly randomly from the set of goals formed from  and hides it from the
assistant  the states consist of pairs of the form  v  i   where v         is the current value of the
goal clause  and i is the next variable to set  the actions of the assistant are to set the existentially
quantified variables  the agent simulates setting the universally quantified variables by choosing
actions from the set        with equal probability  the episode terminates when all the variables are
set  and the assistant gets a reward of   if the value of the clause is   at the end and a reward of  
otherwise 
note that the assistant does not get any useful informtion about the goal until the termination
of the episode  if  is satisfiable  then there is an assistant policy that leads to a reward of  
over all goals and all choices of agent actions  and hence has an expected value of   over the goal
distribution  if not  then at least one of the goals will not be satisfied for some setting of the universal
quantifiers  leading to an expected value      hence the tqbf problem reduces to deciding if a
hgmdp has a policy of expected reward    
this result shows that any pomdp can be encoded as an hgmdp with deterministic dynamics  where the stochastic dynamics of the pomdp are captured via the stochastic agent policy in the
hgmdp  however  the hgmdps resulting from the pspace hardness reduction are quite pathological compared to those that are likely to arise in realistic assistant domains  most importantly  the
agents actions provide practically no information about the agents goal until the end of an episode 
when it is too late to exploit the knowledge  this suggests that we search for restricted classes of
hgmdps that will allow for efficient solutions with performance guarantees 
  

fia d ecision  t heoretic m odel of a ssistance

    helper action mdps
the motivation for helper action mdps  hamdps  is to place restrictions on the agent and assistant that avoid the following three complexities that arise in general hgmdps     the agent can
behave arbitrarily poorly if left unassisted and as such the agent actions may not provide significant evidence about the goal     the agent is free to effectively ignore the assistants help and not
exploit the results of assistive action  even when doing so would be beneficial  and    the assistant
actions have the possibility of negatively impacting the agent compared to not having an assistant 
hamdps will address the first issue by assuming that the agent is competent at  approximately 
maximizing reward without the assistant  the second and the third issues will be addressed by assuming that the agent will always detect and exploit helpful actions and that the assistant actions
never hurt the agent even when they are unhelpful 
informally  the hamdp provides the assistant with a helper action for each of the agents actions  whenever a helper action h is executed directly before the corresponding agent action a 
the agent receives a bonus reward of    however  the agent will only accept the helper action h
 by taking a  and hence receive the bonus  if a is an action that the agent considers to be good for
achieving the goal without the assistant  thus  the primary objective of the assistant in an hamdp
is to maximize the number of helper actions that get accepted by the agent  
while simple  this model captures much of the essence of assistive domains where assistant
actions cause minimal harm and the agent is able to detect and accept good assistance when it
arises 
an hamdp is an hgmdp hs  g  a  a    t  r    is   ig i with the following constraints 
 the agent and assistant actions sets are a    a            an   and a     h            hn    so that for
each ai there is a corresponding helper action hi  
 the state space is s   w   w  a     where w is a set of world states  states in w  a 
encode the current world state and the previous assistant action 
 the reward function r is   for all assistant actions  for agent actions the reward is zero
unless the agent selects the action ai in state  s  hi   which gives a reward of    that is  the
agent receives a bonus of   whenever it takes an action directly after the corresponding helper
action 
 the assistant always acts from states in w   and t is such that taking hi in s deterministically
transitions to  s  hi   
 the agent always acts from states in s  a    resulting in states in s according to a transition function that does not depend on hi   i e  t   s  hi    g  ai   s      t    s  g  ai   s    for some
transition function t    
 finally  for the agent policy  let  s  g  be a function that returns a set of actions and p  s  g 
be a distribution over those actions  we will view  s  g  as the set of actions that the agent
   note that here we have assumed all bonus rewards are   for simplicity  most of our results below are easily extended
to non uniform positive bonus rewards  in particular for our main result concerning bounding the regret of the
myopic policy  the analogous result simply includes a constant factor equal to the maximum possible reward bonus 
the exceptions are theorems   and    which we do not currently have analogous results for non uniform reward
bonuses 

  

fif ern   natarajan   j udah     tadepalli

considers acceptable  or equally good  in state s for goal g  the agent policy  always selects
ai after its helper action hi whenever ai is acceptable  that is    s  hi    g    ai whenever
ai   s  g   otherwise the agent draws an action from p  s  g  
in a hamdp  the primary impact of an assistant action is to influence the reward of the following
agent action  also notice that hamdps do not have rewards that are inherent to the underlying
environment  rather  the only rewards are the bonuses received whenever the agent accepts a helper
action  while we could have defined the model to include environmental reward in addition to
helper bonuses  this unnecessarily complicates the model  as the following hardness result shows  
instead  we assume that the inherent environmental reward is already captured by the agent policy
via  s  g   which is considered to contain actions that approximately optimize this reward 
as an example  the hamdp model captures both the doorman domain  and the desktop domain in our experiments  in the doorman domain  the helper actions correspond to opening doors
for the agent  which reduces the cost of navigating from one room to another  in the desktop domain  the helper actions correspond to offering shortcuts to a users destination folders  importantly
opening an incorrect door or offering an incorrect shortcut does not increase the  physical  cost to
the agent over having no assistant at all  which is a key property of hamdps 
despite the apparent simplification of hamdps over hgmdps  it turns out that somewhat
surprisingly the worst case computational complexity is not reduced 
theorem    short term reward maximization for hamdps is pspace complete 
proof  membership in pspace follows easily since hamdps are a specialization of hgmdps 
the proof of pspace hardness is identical to that of theorem   except that here  instead of the
agents actions  the stochastic environment models the universal quantifiers  the agent accepts
all actions until the last one and sets the variable as suggested by the assistant  after each of the
assistants actions  the environment chooses a value for the universally quantified variable with equal
probability  the last action is accepted by the agent if the goal clause evaluates to    otherwise not 
there is a history dependent policy whose expected reward is greater than or equal to the number
of existential variables if and only if the quantified boolean formula is satisfiable 
unlike the case of hgmdps  the stochastic dynamics are essential for pspace hardness as will be
shown in a later section  despite this negative result  the following sections demonstrate the utility of the hamdp restriction by giving performance guarantees for simple policies and improved
complexity results  so far  there are no analogous results for general hgmdps 
    myopic heuristic analysis
hamdps are closely related to the sequential prediction framework of littlestone         in this
framework  in each round the learner is shown a new instance for which it predicts a binary label  if
the prediction is incorrect  a mistake is made  and the learner is given the true label  in the realizable
setting  the true labels are determined by a hypothesis in a target class  an optimal prediction
algorithm minimizes the upper bound on the number of mistakes over all possible hypotheses  we
can view the helper action in hamdp as a prediction of the user action  maximizing the bonus
reward in hamdp is equivalent to minimizing the number of mistakes in sequential prediction 
unlike sequential prediction  here the predictions  actions  need not be binary  while in sequential
prediction the sequence of states are arbitrarily chosen  here we assume that they are generated by
  

fia d ecision  t heoretic m odel of a ssistance

a markov process  in spite of these differences  some of the results of sequential prediction can be
adapted to hamdps albeit using a different terminology  however  we derive all our results from
first principles for consistency 
given an assistant policy      the regret of a particular episode is the extra reward that an oracle
assistant with knowledge of the goal would achieve over      for hamdps the oracle assistant can
always achieve a reward equal to the finite horizon m  because it can always select a helper action
that will be accepted by the agent  thus  the regret of an execution of    in a hamdp is equal to
the number of helper actions that are not accepted by the agent  which we will call mispredictions 
from above we know that optimizing regret is pspace hard and thus here we focus on bounding
the expected and worst case regret of the assistant  we now introduce our first myopic heuristic and
show that it is able to achieve regret bounds that are logarithmic in the number of goals 
coarsened posterior heuristic  intuitively  our myopic heuristic will select an action that has
the highest probability of being accepted with respect to a coarsened version of the posterior
distribution over goals  the myopic policy in state s given history h is based on the consistent goal
set c h   which is the set of goals that have non zero probability with respect to history h  it is
straightforward to maintain c h  after each observation  observations include the world state and
the agents actions   the myopic policy is defined as 
 s  h    arg max ig  c h   g s  a  
a

where g s  a     g   a   s  g   is the set of goals for which the agent considers a to be an
acceptable action in state s  the expression ig  c h   g s  a   can be viewed as the probability
mass of g s  a  under a coarsened goal posterior which assigns goals outside of c h  probability
zero and otherwise weighs them proportional to the prior 
theorem    for any hamdp the expected regret of the coarsened posterior heuristic is bounded
above by the entropy of the goal distribution h ig   
proof  the main idea of the proof is to show that after each misprediction of the myopic policy  i e 
the selected helper action is not accepted by the agent  the uncertainty about the goal is reduced by
a constant factor  which will allow us to bound the total number of mispredictions on any trajectory 
consider a misprediction step where the coarsened posterior heuristic selects helper action hi in
state s given history h  but the agent does not accept the action and instead selects a    ai   by the
definition of the myopic policy we know that ig  c h   g s  ai     ig  c h   g s  a     since
otherwise the assistant would not have chosen hi   from this fact we now argue that ig  c h      
ig  c h     where h   is the history after the misprediction  that is  the probability mass under
ig of the consistent goal set after the misprediction is less than half that of the consistent goal set
before the misprediction  to show this we will consider two cases     ig  c h   g s  ai     
ig  c h      and    ig  c h   g s  ai     ig  c h      in the first case  we immediately get
that ig  c h   g s  a      ig  c h      combining this with the fact that c h      c h  
g s  a   we get the desired result that ig  c h       ig  c h      in the second case  note that
c h      c h    g s  a    g s  ai   
 c h    c h   g s  ai   
combining this with our assumption for the second case immediately implies that ig  c h      
ig  c h     
  

fif ern   natarajan   j udah     tadepalli

the above shows that if a misprediction is made between histories h and h   then ig  c h      
ig  c h      this implies that for any episode  after n mispredictions resulting in a history hn  
ig  c hn      n   now consider an arbitrary episode where the true goal is g  we know that
ig  g  is a lower bound on ig  c hn     which implies that ig  g    n or equivalently that n 
 log ig  g    thus for any episode with goal g the maximum number of mistakes is bounded by
 log ig  g    using this fact we get that thepexpected number of mispredictions during an episode
with respect to ig is bounded above by  g ig  g  log ig  g     h ig    which completes the
proof 
since h ig    log  g    this result implies that for hamdps the expected regret of the myopic
policy is no more than logarithmic in the number of goals  furthermore  as the uncertainty about
the goal decreases  decreasing h ig    the regret bound improves until we get a regret of   when ig
puts all mass on a single goal  it turns out that this logarithmic bound is asymptotically tight in the
worst case 
theorem    there exists a hamdp such that for any assistant policy the expected regret is at least
log  g     
proof  consider a deterministic hamdp such that the environment is structured as a binary tree
of depth log  g    where each leaf corresponds to one of the  g  goals  by considering a uniform
goal distribution it is easy to verify that at any node in the tree there is an equal chance that the true
goal is in the left or right sub tree during any episode  thus  any policy will have a     chance of
committing a misprediction at each step of an episode  since each episode is of length log  g    the
expected regret of an episode for any policy is log  g     
resolving the gap between the myopic policy bound and this regret lower bound is an open problem 
      a pproximate g oal d istributions  
  instead of the true underlying
suppose that the assistant uses an approximate goal distribution ig
goal distribution ig when computing the myopic policy  that is  the assistant selects actions that
   c h   g s  a    which we will refer to as the myopic policy relative to i     the
maximize ig
g
  instead of i can be bounded in terms of the kullbackleibler  kl  diverextra regret for using ig
g
     which is zero when i  
gence  kullback   leibler        between these distributions kl ig k ig
g
equals ig  

theorem    for any hamdp with goal distribution ig   the expected regret of the myopic policy
  is bounded above by h i     kl i k i     
with respect to distribution ig
g
g
g
proof  the proof is similar to that of theorem    except that since the myopic policy is with respect
  rather than i   we derive that  on any episode  the maximum number of mispredictions n is
to ig
g

  

fia d ecision  t heoretic m odel of a ssistance

   g    using this fact  the average number of mispredictions is given by 
bounded above by  log ig

p

g

p

g

p

g

 

 
  
ig  g  log   
ig  g 


 
ig  g  log   
    log ig  g    log ig  g    
ig  g 
x
ig  g 
ig  g  log ig  g  
ig  g  log   
 
ig  g 
g
 
h ig     kl ig k ig
  

note that for any random variable x with distribution p over a finite domain of size n   we
have kl p k u     log n    h p    where u is the uniform distribution  thus  a consequence of
theorem   is that the myopic policy with respect to the uniform goal distribution has expected regret
bounded by log  g   for any hamdp  showing that logarithmic regret can be achieved without
knowledge of ig   this can be strengthened to hold for worst case regret 
theorem    for any hamdp  the worst case and hence expected regret of the myopic policy with
with respect to the uniform goal distribution is bounded above by log  g   
proof  the proof of theorem   shows that the number of mispredictions on any episode is bounded
     in our case i        g  which shows a worst case regret bound of log  g   
above by  log ig
g
this immediately implies that the expected regret bound of the uniform myopic policy is bounded
by log  g   
    deterministic agent policies
we now consider several special cases of hamdps  first  we restrict the agents policy to be
deterministic for each goal  i e   s  g  has at most a single action for each state goal pair  s  g  
theorem    the myopic policy achieves the optimal expected reward for hamdps with deterministic agent policies 
the proof is given in the appendix  it is sometimes desirable to minimize the worst possible regret
compared to an oracle assistant who knows the agents goal  as we show below  this can be captured
by a graph theoretic notion of tree rank that generalizes the rank of decision trees  ehrenfeucht  
haussler        
definition    the rank of a rooted tree is the rank of its root node  if a node is a leaf node then
rank node       else if a node has at least two distinct children c  and c  with equal highest ranks
among all children  then rank node       rank c     otherwise rank node    the rank of the highest
ranked child 
the optimal trajectory tree  ott  of a hamdp in deterministic environments is a tree where
the nodes represent the states of the hamdp reached by the prefixes of optimal action sequences
for different goals starting from the initial state   each node in the tree represents a state and a set of
   if there are multiple initial states  we build an ott for each initial state  then the rank would be the maximum of the
ranks of all trees 

  

fif ern   natarajan   j udah     tadepalli

goals for which it is on the optimal path from the initial state  since the agent policy is deterministic 
there is at most one trajectory per goal in the tree  hence the size of the optimal trajectory tree is
bounded by the number of goals times the maximum length of any trajectory  which is at most the
size of the state space in deterministic domains  the following lemma follows by induction on the
depth of the optimal trajectory tree  the proof is in the appendix 
lemma    the minimum worst case regret of any policy for an hamdp for deterministic environments and deterministic agent policies is equal to the tree rank of its optimal trajectory tree 
this leads to the following 
theorem    if the agent policy is deterministic  the problem of minimizing the maximum regret in
hamdps in deterministic environments is in p 
proof  we first construct the optimal trajectory tree  we then compute its rank in linear time while
simultaneously computing the optimal minimax policy using the recursive definition of tree rank 
the result then follows from lemma   
    bounded branching factor policies
the assumption of a deterministic agent policy may be too restrictive in many domains  we now
consider agent policies which may have a constant number of possible actions in  s  g  for each
state goal pair as defined below 
definition    the branching factor of a hamdp is the largest number of possible actions in  s  g 
by the agent in any state for any goal and any assistants action 
the doorman domain of section     has a branching factor of   since there are at most two optimal
actions to reach any goal from any state 
theorem    minimizing the worst case regret in finite horizon hamdps in deterministic environments with a constant branching factor k is np complete 
the proof is in the appendix  we can also show that minimizing the expected regret for a bounded
k is np hard  we conjecture that this problem is also in np  but this question remains open 

   solving practical hgmdps
although hamdps offer a theoretically elegant framework  the requirements of practical assistant
systems are not easily satisfed by its assumptions  in this section  we consider the more general
problem of solving hgmdps and offer some practical heuristic solutions that are inspired by our
theoretical analysis 
in principle we could use a general purpose pomdp solver to solve hgmdps  while the
pomdp solvers based on point based methods and search based methods have become more efficient over the years  they are still too inefficient to be used in an interactive setting  where the
parameters of the pomdp are continually being updated  moreover  as the analysis in the previous
section suggests  simple myopic heuristics based on the knowledge of the goal distribution and the
optimal policies given the goals appear promising to yield respectable performance  for this reason 
we adopt an approach based on bayesian goal estimation followed by heuristic action selection  and
evaluate it in three different domains  below  we first give an overview of our solution algorithm
and then describe each of the components in more detail 
  

fia d ecision  t heoretic m odel of a ssistance

    overview
in this section  we will assume that we are given an hgmdp m   delegating the problem of learning
m to section    let ot   o         ot be an observation sequence observed by the assistant from the
beginning of the current trajectory until time t  each observation is a tuple of a world state and the
previously selected action  by either the assistant or agent   given ot and m our goal is to compute
an assistant action whose value is  close to  optimal 
to motivate the approach  it is useful to consider some special characteristics of the hgmdp 
most importantly  the belief state corresponds to a distribution over the agents goal  since the agent
is assumed to be goal directed  the observed agent actions provide substantial evidence about what
the goal might and might not be  in fact  even if the assistant does nothing  the agents goals will
often be rapidly revealed by analyzing the relevance of the agents initial actions to the possible
goals  in such cases  this suggests that the state goal estimation problem for the hgmdp may be
solved quite effectively by just observing how the agents actions relate to the various possible goals 
rather than requiring the assistant to select actions explicitly for the purpose of information gathering
about the agents goals  in other words  in such cases  we can expect purely  or nearly  myopic
action selection strategies  which avoid reasoning about information gathering  will be effective 
reasoning about information gathering is one of the key complexities involved in solving pomdps
compared to mdps  here we leverage the intuitive properties of the hgmdp to gain tractability
by limiting or completely avoiding such reasoning  of course  as shown by our pspace hardness
results  goals will not always be rapidly revealed and non myopic reasoning will be essential 
we note that in some cases  the assistant will have pure information gathering actions at its
disposal  e g  asking the agent a question  while we do not consider such actions in our experiments 
we believe that such actions can be handled naturally in this framework by incorporating only a
small amount of look ahead search 
with the above motivation  our assistant architecture  depicted in figure    alternates between
goal estimation and action selection as follows 
   after observing the agents next action  we update the goal distribution based on the hgmdp
model 
   based on the updated distribution we evaluate the effectiveness of assistant actions  including
noop  by building a sparse sampling look ahead tree of bounded depth  perhaps just depth
one   where leaves are evaluated via a myopic heuristic 
the key element of the architecture is the computation of the myopic heuristics  on top of this
heuristic  we can optionally obtain non myopic behavior via search by building a look ahead sparsesampling tree  our experiments show that such search can improve performance by a small margin
at a significant computational cost  we note that the idea of utilizing myopic heuristics to select
actions in pomdps is not new  see for example  cassandra        geffner   bonet         and
similar methods have been used previously with success in applications such as computer bridge
 ginsberg         the main contribution here is to show that this approach is particularly well
suited to our setting and to evaluate some efficiently computable heuristics specifically designed for
solving hgmdps  below we describe the goal estimation and action selection operations in more
detail 

  

fif ern   natarajan   j udah     tadepalli

goal estimation

p g 

action selection

assistant
ot

at

wt
environment

user
ut

figure    depiction of the assistant architecture  the agent has a hidden goal and selects actions
ut that cause the environment to change world state wt   typically moving closer to the
goal  the assistant  upper rectangle  is able to observe the world state along with the
observations generated by the environment  which in our setting contain the user agent
actions along with the world state  the assistant is divided into two components  first 
the goal estimation component computes a posterior over agent goals p  g  given the observations  second  the action selection component uses the goal distribution to compute
the best assistive action at via a combination of bounded search and myopic heuristic
computation  the best action might be noop in cases where none of the other assistive
actions has higher utility for the user 

  

fia d ecision  t heoretic m odel of a ssistance

    goal estimation
given an hgmdp with agent policy  and initial goal distribution ig   our objective is to maintain
the posterior goal distribution p  g ot    which gives the probability of the agent having goal g
conditioned on observation sequence ot   note that since we have assumed that the assistant cannot
affect the agents goal  only observations related to the agents actions are relevant to the posterior 
given the agent policy   it is straightforward to incrementally update the posterior p  g ot   upon
each of the agents actions 
at the beginning of each episode we initialize the goal distribution p  g o    to ig   on timestep t
of the episode  if ot does not involve an agent action  then we leave the distribution unchanged  otherwise  when the agent selects action a in state s  we update the posterior according to p  g ot    
   z   p  g ot      a s  g   where z is a normalizing constant  that is  the distribution is adjusted to place more weight on goals that are more likely to cause the agent to execute action a in s 
the accuracy of goal estimation relies on how well the policy  learned by the assistant reflects the
true agent policy  as described in section      we use a model based bootstrapping approach for
estimating  and update this estimate at the end of each episode  provided that the agent is close to
optimal  as in our experimental domains  this approach can lead to rapid goal estimation  even early
in the lifetime of the assistant 
we have assumed for simplicity that the actions of the agent are directly observable  in some
domains  it is more natural to assume that only the state of the world is observable  rather than the
actual action identities  in these cases  after observing the agent transitioning from s to s  we can
use the mdp transition function t to marginalize over possible agent actions yielding the update 
p  g ot        z   p  g ot    

x

 a s  g t  s  a  s    

aa

    action selection
given the hgmdp m and a distribution over goals p  g ot    we now address the problem of
selecting an assistive action  our mechanisms utilize a combination of bounded look ahead search
and myopic heuristic computations  by increasing the amount of look ahead search the actions
returned will be closer to optimal at the cost of more computation  fortunately  for many hgmdps 
useful assistant actions can be computed with relatively little or no search  we first describe several
myopic heuristics that can be used either for greedy action selection or in combination with search 
next  we review how to utilize sparse sampling to obtain non myopic action selection 
      action s election h euristics
to explain the action selection procedure  we introduce the idea of an assistant mdp relative to a
goal g and m   which we will denote by m  g   the mdp m  g  is identical to m except that we
change the initial goal distribution such that p  g    g       that is  the goal is always fixed to g in
each episode  since the only hidden component of m s state space was the goal  fixing the goal in
m  g  makes the state fully observable  yielding an mdp  each episode in m  g  evolves by drawing
an initial world state and then selecting assistant actions until the goal g is achieved  note that the
state transition after an assistant action a  in state s is the result of successive state transitions  first
due to the assistant action and then due to the ensuing agent action  which is selected based on the
agent policy and goal g  an optimal policy for m  g  gives the optimal assistive action assuming
  

fif ern   natarajan   j udah     tadepalli

that the agent is acting to achieve goal g  we will denote the q function of m  g  by qg  s  a  
which is the expected cost of executing action a in state s and then following the optimal policy 
we now consider a second heuristic for action selection  which accounts for non uniform rewards and the true goal posterior  unlike the coarsened posterior heuristic introduced in section     
it is simply the expected q value of an action over assistant mdps  and has also been called the qmdp
method by cassandra         the heuristic value for assistant action a in state s given observations
ot is
x
qg  s  a   p  g ot   
h s  a  ot    
g

intuitively h s  a  ot   measures the utility of taking an action under the assumption that all
goal ambiguity is resolved in one step  thus  this heuristic will not value the information gathering
utility of an action  rather  the heuristic will favor assistant actions that make progress toward goals
with high posterior probability  when the goal posterior is highly ambiguous this will often lead
the assistant to prefer noop  which at least does not hurt progress toward the goal  note that this
heuristic  as well as the others below  can be used to evaluate the utility of a state s  rather than a
state action pair  by maximizing over all actions maxa h s  a  ot   
the primary computational complexity of computing h is to solve the assistant mdps for each
goal in order to obtain the q functions  technically  since the transition functions of the assistant
mdps depend on the approximate agent policy   we must re solve each mdp after updating the
 estimate at the end of each episode  see section     for policy learning   however  using incremental dynamic programming methods such as prioritized sweeping  moore   atkeson        can
alleviate much of the computational cost  in particular  before deploying the assistant we can solve
each mdp offline based on the default agent policy given by the boltzmann bootstrapping distribution describe in section      after deployment  prioritized sweeping can be used to incrementally
update the q values based on the learned refinements we make to  
when it is not practical to exactly solve the assistant mdps  we may resort to various approximations  we consider two approximations in our experiments  one is to replace the users policy
to be used in computing the assistant mdp with a fixed default user policy  eliminating the need to
compute the assistant mdp at every step  we denote this approximation by hd   another approximation uses the simulation technique of policy rollout  bertsekas   tsitsiklis        to approximate
qg  s  a  in the expression for h  this is done by first simulating the effect of taking action a in
state s and then using  to estimate the expected cost for the agent to achieve g from the resulting
state  that is  we approximate qg  s  a  by assuming that the assistant will only select a single
initial action followed by only agent actions  more formally  let cn    s  g  be a function that simulates n trajectories of  achieving the goal from state s and then averaging the trajectory costs 
the
to h s  a  ot   except that we replace qg  s  a  with the expectation
p heuristic h r is identical
 
s  s t  s  a  s    c   s   g   we can also combine both of these heuristics  using a fixed default
user policy and policy rollouts  which we denote by hd r  
      s parse s ampling
all of the above heuristics are somewhat myopic in the sense that they do not take into account
potentially persistent ambiguity about the agents goal and do not consider the use of information
gathering actions to resolve the ambiguity  in cases where it is beneficial to consider some nonmyopic reasoning  one can combine these heuristics with shallow search in the belief space of

  

fia d ecision  t heoretic m odel of a ssistance

the assistant mdp  for this purpose we utilize depth d bounded sparse sampling trees  kearns 
mansour    ng        to compute an approximation to the q function for a given belief state
 st   ot    denoted by qd  st   a  ot    given a particular belief state  the assistant will then select the
action that maximizes qd   note that for convenience we represent the belief state as a pair of the
current state st and observation history ot   this is a lossless representation of the belief state since
the posterior goal distribution can be computed exactly from ot and the goal is the only hidden
portion of the pomdp state 
the base case q   st   a  ot   will be equal to one of our myopic heuristics described above 
increasing the depth d will result in looking ahead d state transitions and then evaluating one of
our heuristics  by looking ahead it is possible to track the potential changes to the belief state after
taking certain actions and then determine whether those changes in belief would be beneficial with
respect to providing better assistance  sparse sampling does such look ahead by approximately
computing 
qd  s  a  o    e r s  g  a    v d   s    o    
d

d

v  s  o    max q  s  a  o 
a

   
   

where g is a random variable distributed according to the goal posterior p  g o  and  s    o    is
a random variable that represents the belief state after taking action a in belief state  s  o   in
particular  s  is the world state arrived at and o  is simply the observation sequence o extended
with the observation obtained during the state transition  the first term in the above expectation
represents the immediate reward of the assistant action a when the goal is g 
sparse sampling approximates the above expectation by averaging a set of b samples of successor belief states  the sparse sampling pseudo code is presented in table        given an input belief
state  s  o   assistant action a  heuristic h  depth bound d  and sampling width b the algorithm returns  an approximation of  qd  s  a  o   first  if the depth bound is equal to zero the heuristic
value is returned  otherwise b samples of observations resulting from taking action a in belief state
 s  o  are generated  the observations will be of the form oi    s i   ai   si    where s i is the state
resulting from taking action a in state s  ai is the ensuing agent action selected in s i based on a goal
drawn from the goal posterior  and si is the result of taking action ai in state s i   each observation oi
corresponds to a new belief state  si    o  oi    where  o  oi   is simply the concatenation of oi to o 
the code then recursively computes a value for each of these belief states by maximizing qd over
all actions and then averages the results 
as b and d become large  sparse sampling will produce an arbitrarily close approximation to the
true q function of the belief state mdp  the computational complexity of sparse sampling is linear
in b and exponential in d  thus the depth must be kept small for real time operation 

   learning hgmdps
in this section  we tackle the problem of learning the hgmdp while interacting with the environment to assist the agent  we assume that the set of goals g is known to the agent  the primary
role for learning is to acquire the agents policy and goal distribution  this assumption is natural
in situations where the assistant is being applied many times in the same environment  for possibly
different agents  for example  in a desktop environment  the environment mdp corresponds to a
description of the various desktop functionalities  which remains fixed across users  if one is not
  

fif ern   natarajan   j udah     tadepalli

 given  heuristic function h  belief state  s  o   action a  depth bound d  sampling width b
 return  an approximation qd  s  a  o  of the value of a in belief state  s  o 
   if d     then return h s  a  o 
   sample a set of b observations  o            ob   resulting from taking action a in
belief state  s  o  as follows 
 a  sample s i from the environment mdp transition function t  s  a   
 b  sample a goal gi from p  gi  o 
 c  sample an agent action ai from the agent policy   s i   gi  
 d  oi    s i   ai   si    where si is sample from the environment mdp transition
function t  s i   ai    
   for each oi    s i   gi   ai   si   compute vi   maxa  qd   si   a     o  oi   
p
   return qd  s  a  o     b i r s  gi   a    vi

table    pseudo code for sparse sampling in the hgmdp
provided with a description of the mdp then it is typically straightforward to learn this model with
the primary cost being a longer warming up period for the assistant 
relaxing the assumption that we are provided with the set of possible goals is more problematic
in our current framework  as we saw in section    our solution methods depend on knowing this set
of goals and it is not clear how to learn these from observations  since the goals  unlike states and
actions  are not directly observable to the assistant  extending our framework so that the assistant
can automatically infer the set of possible user goals  or allow the user to define their own goals  is
an interesting future direction  we note  however  it is often possible for a designer to enumerate a
set of user goals before deployment that while perhaps not complete  allows for useful assistance to
be provided 
    maximum likelihood estimates
it is straightforward to estimate the goal distribution g  and agent policy  by simply observing the
agents actions  possibly while being assisted  and to compute empirical estimates of the relevant
quantities  this can be done by storing the goal achieved at the end of each episode along with
the set of world state action pairs observed for the agent during the episode  the estimate of ig
can then be based on observed frequency of each goal  usually with laplace correction to avoid
extreme values of the probabilities   likewise  the estimate of  a s  g  is simply the frequency for
which action a was taken by the agent when in state s and having goal g  while in the limit these
maximum likelihood estimates will converge to the correct values of the true hgmdp  in practice
convergence can be slow  this slow convergence can lead to poor performance in the early stages
of the assistants lifetime  to alleviate this problem we propose an approach for bootstrapping the
learning of the agent policy  
  

fia d ecision  t heoretic m odel of a ssistance

    model based bootstrapping
we will leverage our environment mdp model in order to bootstrap the learning of the agent policy 
in particular  we assume that the agent is near optimal in the sense that  for a particular goal and
world state  he is more likely to select actions that are close to optimal  this is not unrealistic
in many application domains that might benefit from intelligent assistants  in particular  there are
many tasks  that are conceptually simple for humans  yet are quite tedious  e g   navigating through
the directory structure of a computer desktop  performing optimally in such tasks is not difficult for
humans 
given the near rationality assumption  we initialize the estimate of the agents policy to a
prior that is biased toward optimal agent actions  to do this we will consider the environment mdp
with the assistant actions removed and solve for the q function q a  s  g  using mdp planning
techniques  the q function gives the expected cost of executing agent action a in world state s and
then acting optimally to achieve goal g using only agent actions  in a world without an assistant 
a rational agent would always select actions that maximize the q function for any state and goal 
furthermore  a close to rational agent would prefer actions that achieve higher q values to highly
suboptimal actions  we first define the boltzmann distribution  which will be used to define our
prior 
 
 a s  g   
exp k  q a  s  g  
   
z s  g 
where z s  g  is a normalizing constant  and k is a temperature constant  using larger values
of k skews the distribution more heavily toward optimal actions  given this definition  our prior
distribution over   w  g  is taken to be a dirichlet with parameters               a     where i  
    ai  s  g   here   is a parameter that controls the strength of the prior  intuitively   can be
thought of as the number of pseudo actions represented by the prior  with each i representing the
number of those pseudo actions that involved agent action ai   since the dirichlet is conjugate to
the multinomial distribution  which is the form of   s  g   it is easy to update the posterior over
  s  g  after each observation  one can then take the mode or mean of this posterior to be the point
estimate of the agent policy used to define the hgmdp 
in our experiments  we found that this prior provides a good initial proxy for the actual agent
policy  allowing for the assistant to be immediately useful  further updating of the posterior tunes
the assistant better to the peculiarities of a given agent  for example  in many cases there are
multiple optimal actions and the posterior will come to reflect any systematic bias among equally
good actions that an agent has  computationally the main obstacle to this approach is computing the
q function  which needs to be done only once for a given application domain since the environment
mdp is constant  using dynamic programming this can be accomplished in polynomial time in the
number of states and goals  when this is not practical  a number of alternatives exist including the
use of factored mdp algorithms  boutilier et al          approximate solution methods  boutilier
et al         guestrin et al          or developing domain specific solutions 
finally  in this work  we utilize an uninformative prior over the goal distribution  an interesting
future direction would be to bootstrap the goal distribution estimate based on observations from a
population of agents 

  

fif ern   natarajan   j udah     tadepalli

   experimental results
in this section  we present the results of conducting user studies and simulations in three domains 
two game like environments and a folder predictor domain for an intelligent desktop assistant  in
the user studies in the two game like domains  for each episode  the users and the assistants actions
were recorded  these user studies were performed using    human subjects  graduate students in cs
department at oregon state university  over a single session  the ratio of the cost of achieving the
goal with the assistants help to the optimal cost without the assistant was calculated and averaged
over the multiple trials for each user  we present similar results for the simulations as well  the third
domain is a folder predictor domain  where we simulated the user and used one of our heuristics to
generate the top   recommended folders for the user  we present the number of clicks required on an
average for the user to reach her desired folder  two of these three domains  namely  the doorman
domain and the folder predictor domain  fall under the category of hamdps since the assistive
actions can be merely viewed as helper actions that the agent can ignore  the kitchen domain on
the other hand needs a slightly more general formulation since the agent and the assistant do not
strictly alternate  and the assistants actions cannot be ignored by the agent 
    doorman domain
in the doorman domain  there is an agent and a set of possible goals such as collect wood  food and
gold  some of the grid cells are blocked  each cell has four doors and the agent has to open the
door to move to the next cell  see figure     the door closes after one time step so that at any time
only one door is open  the goal of the assistant is to help the user reach his goal faster by opening
the correct doors 
a state is a tuple hs  di  where s stands for the the agents cell and d is the door that is open  the
total number of states is         squares with   possibilities for the door   the actions of the agent
are to open door and to move in each of the   directions or to pickup whatever is in the cell  for a
total of   actions  the assistant can open the doors or perform a noop    actions  the agents and
the assistants actions strictly alternate in this domain  satisfying the definition of hamdps  there
is a reward of    or a cost of    if the user has to open the door and no reward to the assistants
action  the trial ends when the agent picks up the desired object  note that while we have included
a noop action for the assistant  in this domain the action is never selected  since the cost of opening
a wrong door and noop are the same  while there is no potential benefit of selecting noop 
in this experiment  we evaluated two heuristics  one where we fixed the user policy to the default
policy in the hgmdp creation  hd   avoiding the need for repeated computation of the hgmdp at
every step and the second where we use the policy rollout to calculate the q values  hr    in each
trial  the system chooses a goal and one of the two heuristics at random  the user is shown the
goal and he tries to achieve it  always starting from the center square  after every users action  the
assistant opens a door or does nothing  the user may pass through the door or open a different door 
after the user achieves the goal  the trial ends  and a new one begins  the assistant then uses the
users trajectory to update the agents policy 
the results of the user studies for the doorman domain are presented in tabe    the first two
rows give cumulative results for the user study when actions are selected greedily according to hr
and hd respectively  rather than reporting the negtive rewards  the table shows the total number of

  

fia d ecision  t heoretic m odel of a ssistance

figure    doorman domain  the agents goal is to fetch a resource  the grid cells are separated by
doors that must be opened before passing through 

actions for all trials across all users without the assistant n  and the total number of actions with the
assistant u  and the average of percentage savings     u n   over all trials and over all the users  
as can be seen  both the methods reduce the number of actions by more than      note that
an assistant that selects among the four doors at random would reduce the number of actions by
only     in comparison  an omniscient assistant who knows the users goal reduces the number
of actions by      this is not      because the first door is always opened by the user  in our
experiments  if we do not count the users first action  the number of actions reduces by       it
can be observed that hr appears to have a slight edge over hd   one possible reason for this is that
while using hd   we do not re solve the mdp after updating the user policy  while hr is always
using the updated user policy  thus  rollout is reasoning with a more accurate model of the user 
heuristic
hr
hd
hr
d      b    
d      b    
d      b    
d      b    

total
actions
n
   
   
    
    
    
    
    

user
actions
u
   
   
   
   
   
   
   

fractional savings
    u n  
          
          
           
           
           
         
           

time
per
action  in secs
      
      
     
     
    
     
    

table    results of experiments in the doorman domain  the first two rows of the table present the
results of the user studies while the rest of the table presents the results of the simulation 

   this gives a pessimistic estimate of the usefulness of the assistant assuming an optimal user and is a measure of utility
normalized by the optimal utility without the aid of the assistant 
   note that this first action requirement is easily aviodable  it is simply the equivalent of an on switch to indicate that
the user is ready to move about in the grid  we can also replace this requirement by explicitly adding an on button to
the interface to start a new episode 

  

fif ern   natarajan   j udah     tadepalli

another interesting observation is that there are individual differences among the users  some
users always prefer a fixed path to the goal regardless of the assistants actions  some users are
more flexible  from the survey we conducted at the end of the experiment  we learned that one of
the features that the users liked was that the system was tolerant to their choice of suboptimal paths 
the data reveals that the system was able to reduce the costs by approximately     even when the
users chose suboptimal trajectories 
we also conducted experiments using sparse sampling with non zero depths  we considered
depths of d     and d     while using sampling widths of b     or b      the leaves of the sparse
sampling tree are evaluated using hr which simply applies rollout to the user policy  hence sparse
sampling of d     and b      would correspond to the heuristic hr   for these experiments  we did
not conduct user studies  due to the high cost and effort required for humans in such studies  but
simulated the human users by choosing actions according to policies learned from their observed
actions from the previous user study  the results are presented in the last   rows of table    note that
the absolute numbers of actions in the user studies and the simulations are not comparable as they
are based on different numbers of trajectories  the human users were tested on fewer trajectories
to minimize their fatigue  we see that sparse sampling increased the average run time  last column 
by an order of magnitude  but is able to produce a reduction in average cost for the user  this result
is not surprising in hindsight  for in the simulated experiments  sparse sampling is able to sample
from the exact user policy  i e  it is sampling from the learned policy  which is also being used
for simulations   these results suggest that a small amount of non myopic reasoning can have a
positive benefit with a substantial computation cost  note  however  that the bulk of the benefit
realized by the assistant can be obtained without such reasoning  showing that the myopic heuristics
are well suited to this domain 
    kitchen domain
in the kitchen domain  the goals of the agent are to cook various dishes  there are   shelves with
  ingredients each  each dish has a recipe  represented as a partially ordered plan  the ingredients
can be fetched in any order  but should be mixed before they are heated  the shelves have doors
that must be opened before fetching ingredients and only one door can be open at a time 
there are   different recipes  the state consists of the location of each of the ingredients
 bowl shelf table   the mixing state and temperature state of the ingredient  if it is in the bowl 
and the door that is open  the state also includes the action history to preserve the ordering of the
plans for the recipes  the users actions are  open the doors  fetch the ingredients  pour them into
the bowl  mix  heat and bake the contents of the bowl  or replace an ingredient back to the shelf  the
assistant can perform all user actions except for pouring the ingredients or replacing an ingredient
back to the shelf  we restricted the assistant from pouring ingredients as it is an irreversible action  the reward for all non pour actions is     experiments were conducted on    human subjects
who are computer science graduate students  unlike in the doorman domain  here we allowed the
the assistant to take multiple consecutive actions  the turn switches to the user when the assistant
executes the noop action 
this domain has a large state space and hence it is not possible to update the user policy after
every trajectory  hence  the two heuristics that we compare both use the default user policy  the
second heuristic in addition uses policy rollout to compare the actions  in other words  we compare
hd and hd r   the results of the user studies are shown in top part of the table    as in the doorman

  

fia d ecision  t heoretic m odel of a ssistance

figure    the kitchen domain  the user is to prepare the dishes described in the recipes on the
right  the assistants actions are shown in the bottom frame 

domain  the total number of agent actions with and without the assistant  and the percentage reduction due to the assistant are presented  the number of user actions was summed over    users and
the cumulative results are presented  it can be observed that hd r performs better than hd   it was
observed from the experiments that the hd r technique was more aggressive in choosing non noop
actions than hd   which would wait until the goal distribution is highly skewed toward a particular
goal 
heuristic
hd r
hd
hd r
d      b    
d      b    
d      b    
d      b    

total
actions
n
    
    
    
    
    
    
    

user
actions
u
    
    
    
    
    
    
    

fractional savings
    u n  
           
           
            
            
           
           
           

time
per
action  secs 
     
     
     
     
     
     
     

table    results of experiments in the kitchen domain  the first two rows of the table present the
results of the user studies while the last   rows present the results of the simulation 

we compared the use of sparse sampling and our heuristic on simulated user trajectories for
this domain as well  see the last   rows of table     again  the absolute numbers of actions of the
user studies are not comparable to that of simuations due to different numbers of trajectories in each
case  since sparse sampling considers a larger number of trajectories than the other methods  the
policies learned are sometimes better than those learned from other heuristics  although they took
more time to execute  however  there is no significant difference between the solution quality of
rollouts and sparse sampling on simulations  showing that our myopic heuristics are performing as
  

fif ern   natarajan   j udah     tadepalli

well as sparse sampling with much less computation  sparse sampling with higher depths requires
an order of magnitude more computation time when compared to the rollout 
    folder predictor
in this section  we present the evaluation of our framework on a real world domain  as a part of
the task tracer project  dragunov  dietterich  johnsrude  mclaughlin  li    herlocker        
researchers developed a file location system called folder predictor  bao et al          the idea
behind the folder predictor is that by learning about the users file access patterns  the assistant can
help the user with his file accesses by predicting the folder in which the file has to be accessed or
saved 
in this setting  the goal of the folder predictor is to minimize the number of clicks of the user 
the predictor would choose the top three folders that would minimize the cost and then append them
to the ui  shown in ovals in figure     also  the user is taken to the first recommended folder  so if
the users target folder is the first recommended folder  the user would reach the folder in zero clicks
and reach the second or the third recommended folder in one click  the user can either choose one
of the recommendations or navigate through the windows folder hierarchy if the recommendations
are not relevant 

figure    folder predictor  bao et al         
bao et al  considered the problem as a supervised learning problem and implemented a costsensitive algorithm for the predictions with the cost being the number of clicks of the user  bao
et al          but  their algorithm does not take into account the response of the user to their
predictions  for instance  if the user chooses to ignore the recommended folders and navigates the
folder hierarchy  they do not make any re predictions  this is due to the fact that their model is
a one time prediction and does not consider the user responses  also  their algorithm considers
a restricted set of previously accessed folders and their ancestors as possible destinations  this
precludes handling the possibility of user accessing a new folder 
our decision theoretic model naturally handles the case of re predictions by changing the recommendations in response to the user actions  as a first step  we used the data collected from their
user interface and used our model to make predictions  we use the users response to our predic  

fia d ecision  t heoretic m odel of a ssistance

tions to make further predictions  also  to handle the possibility of a new folder  we consider all
the folders in the folder hierarchies for each prediction  we used a mixture density to obtain the
probability distribution over the folders 
p  f       p   f            pl  f  
here p  is the probability according to bao et als algorithm         pl is the uniform probability distribution over the set of folders and   is ratio of the number of times a previously accessed
folder has been accessed to the total number of folder accesses 
the idea behind using the above density function is that during the early stages of a task  the user
will be accessing new folders while in later stages the user will access the folders of a particular task
hierarchy  hence as the number of folder accesses increases the value of   increases and would
eventually converge to    and hence the resulting distribution would converge to p    the data set
consists of a collection of requests to open a file  open  and save a file  saveas   ordered by time 
each request contains information such as  the type of request  open or saveas   the current task 
the destination folder  etc  the data set consists of a total of     open saveas requests  the folder
hierarchy consists of     folders 
the state space consists of   parts  the current folder that the user is accessing and the three
recommendations two of which are unordered  this would correspond to a state space of size
             
    the action of the user is either to choose a recommended folder or select
a different folder  the action of the assistant corresponds to choosing the top   folders and the
action space is of size         
    the reward in our case was the negative of the number of user
clicks  in this domain  the assistant and the users actions strictly alternate as the assistant revises its
predictions after every user action  the prior distribution was initialized using the rewards computed
by the model developed by bao et al         
we applied the decision theoretic model to the data set  for each request  our assistant would
make the prediction using the hd r heuristic  which uses the default user policy and the rollout
method  and then the user is simulated  the user would accept the recommendation if it shortens
his path to the goal  and otherwise would act according to his optimal policy  the user here is
considered close to optimal  which is not unrealistic in the real world  to compare our results  we
also used the model developed by bao et al  in the data set and present the results in table   
restricted folder set
all folders

one time prediction
      
     

with repredictions
    
      

table    results of the experiments in the folder predictor domain  the numbers indicate the average number of clicks required by the agent to reach his her correct folder  the entry in
the top left hand cell is the performance of the current task tracer  while the one in the
bottom right hand cell is the performance of the decision theoretic assistant 

the table shows the average cost of folder navigation for   different cases  bao et als original
algorithm  their algorithm modified to include mixture distributions and our model with and without
mixture distributions  it can be seen that our model with the use of mixture distributions has the
least user cost for navigation and hence is the most effective  bao et  al have shown that their
  

fif ern   natarajan   j udah     tadepalli

algorithm performs significantly better than the windows default prediction which has an average
of     clicks per folder navigation  this improvement can be attributed to the two modifications
mentioned earlier  first  the use of re predictions in our model which is natural to the decisiontheoretic framework while their model makes a one time prediction and hence cannot make use
of the users response to the recommendations  secondly  considering all folders in the hierarchy
for prediction including the possibility of the user accessing a new folder is found to be useful  it
can be observed that either of the modifications yields a lower cost than the original algorithm  but
combining the two changes is significantly more effective 

   discussion and related work
our work is inspired by the growing interest and success in building useful software assistants
 yorke smith et al         lieberman        myers et al          some of this effort is focused
on building desktop assistants that help with tasks such as calendar scheduling  refanidis  alexiadis    yorke smith         email filtering  cohen  carvalho    mitchell         on line diagnostics  skaanning  jensen    kjaerulff         and travel planning  ambite  barish  knoblock 
muslea  oh    minton         each of these tasks typically requires designing a software system
around specialized technologies and algorithms  for example  email filtering is typically posed as a
supervised learning problem  cohen et al          travel planning combines information gathering
with search and constraint propagation  ambite et al          and printer diagnostics is formulated
as bayesian network inference  skaanning et al          other approaches focus on socially assistive robots setting where a robot is designed to aid human agents in achieving their goals  johnson 
cuijpers  juol  torta  simonov  frisiello  bazzani  yan  weber  wermter  et al          unfortunately the plethora of systems and approaches lacks an overarching conceptual framework  which
makes it difficult to build on each others work  in this paper  we argue that a decision theoretic
approach provides such a common framework and allows the design of systems that respond to
novel situations in a flexible manner reducing the need for pre programmed behaviors  we formulate a general version of the assistantship problem that involves inferring the users goals and taking
actions to minimize the expected costs 
earlier work on learning apprentice systems focused on learning from the users by observation  mahadevan  mitchell  mostow  steinberg    tadepalli        mitchell  caruana  freitag 
j mcdermott    zabowski         this work is also closely related to learning from demonstration
or programming by demonstration  johnson        konidaris  kuindersma  grupen    barto       
atkeson   schaal        cypher        lau  wolfman  domingos    weld         the emphasis
in these systems is to provide an interface where the computer system can unobtrusively observe the
human user doing a task and learn to do it by itself  the human acts both as a user and as a teacher 
the performance of the system is measured by how quickly the system learns to imitate the user 
i e   in the supervised learning setting  note that imitation and assistance are two different things in
general  while we expect our secretaries to learn about us  they are not typically expected to replace
us  in our setting  the assistants goal is to reduce the expected cost of users problem solving  if the
user and the assistant are capable of exactly the same set of actions  and if the assistants actions cost
nothing compared to the users  then it makes sense for the assistant to try to completely replace the
human  even in this case  the assistantship framework is different from learning from demonstration
in that it still requires the assistant to infer the users goal from his actions before trying to achieve
it  moreover  the assistant might learn to solve the goal by itself by reasoning about its action set

  

fia d ecision  t heoretic m odel of a ssistance

rather than by being shown examples of how to do it by the user  in general  however  the action
set of the user and the assistant may be different  and supervised learning is not appropriate  for
example  this is the case in our folder predictor  the system needs to decide which set of folders
to present to the user  and the user needs to decide which of those to choose  it is awkward if not
impossible to formulate this problem as supervised learning or programming by demonstration 
taking the decision theoretic view helps us approach the assistantship problem in a principled
manner taking into account the uncertainty in the users goals and the costs of taking different
actions  the assistant chooses an action whose expected cost is the lowest  the framework naturally
prevents the assistant from taking actions  other than noop  when there is no assistive action which
is expected to reduce the overall cost for the user  rather than learning from the user how to behave 
in our framework the assistant learns the users policy  this is again similar to a secretary who learns
the habits of his boss  not so much to imitate her  but to help in the most effective way  in this work
we assumed that the user mdp is small enough that it can be solved exactly given the users goals 
this assumption may not always be valid  and it makes sense in those cases to learn from the user
how to behave  it is most natural to treat this as a case where the users actions provide exploratory
guidance to the system  clouse   utgoff        driessens         this gives an opportunity for the
system to imitate the user when it knows nothing better and improve upon the users policy when it
can 
there have been other personal assistant systems that are based on pomdp models  however 
these systems are formulated as domain specific pomdps and are solved offline  for instance 
the coach system helped people suffering from dementia by giving them appropriate prompts as
needed in their daily activities  boger  poupart  hoey  boutilier  fernie    mihailidis         they
use a plan graph to keep track of the users progress and then estimate the users responsiveness to
determine the best prompting strategy  a distinct difference from our approach is that there is only
a single fixed goal of washing hands  and the only hidden variable is the user responsiveness which
is either low or high  rather  in our formulation the goal is a random variable that is hidden to the
assistant  since their state action space is significantly smaller       as against        states in our
folder predictor domain   it is possible for them to solve the pomdp exactly  given that we need
to re solve the pomdp after every user action  it becomes prohibitively expensive  yet another
difference is that the length of the trajectory to the goal is small in their case and hence a plan graph
would suffice to capture the user policy  in our model  we do not restrict to a plan graph and instead
solve the user mdp to bootstrap the policy  they have mentioned learning the user policy as a
future direction  in our work  even though we start with an initial estimate of the user policy  we
update it after every goal is achieved  this can be considered as online learning of user policy with
a reasonably good prior  we note that a combination of these two frameworks  one for modeling
users responsiveness and the other for modeling users goal  would be useful  where the assistant
infers both the agent goals and other relevant hidden properties of the user  such as responsiveness 
in electric elves  the assistant takes on many of the mundane responsibilities of the human
agent including rescheduling a meeting should it appear that the user is likely to miss it  again a
domain specific pomdp is formulated and solved offline using a variety of techniques  in one such
approach  since the system monitors users in short regular intervals  radical changes in the belief
states are usually not possible and are pruned from the search space  varakantham  maheswaran 
  tambe         neither exact nor approximate pomdp solvers are feasible in our online setting 
where the pomdp is changing as we learn about the user  and must be repeatedly solved  they are
either too costly to run  boger et al          or too complex to implement as a baseline  e g   electric
  

fif ern   natarajan   j udah     tadepalli

elves  varakantham et al          our experiments demonstrate that simple methods such as onestep look ahead followed by rollouts would work well in many domains where the pomdps are
solved online  in a distinct but related work  doshi   gmytrasiewicz         the authors introduce
the setting of interactive pomdps  where each agent models the other agents beliefs  clearly  this
is more general and more complex than ordinary pomdps  our model is simpler and assumes that
the agent is oblivious to the presence and beliefs of the assistant  while the simplified model suffices
in many domains  relaxing this assumption without sacrificing tractability would be interesting 
there have been several dialogue systems proposed and many of them are based on decisiontheoretic principles  walker        singh  litman  kearns    walker         for instance  the
njfun system is designed as an mdp to provide assistance to the user by interacting with the user
and providing the answer to the users questions  it uses an automatic speech recognizer  asr  to
interpret the human dialogues and uses a dialogue policy to choose the best action  the response  
the goals of the user could be a set of standard queries such as locations of restaurants  wineries 
shopping centers etc  the state space would be the dialogue states  i e   the current state of the
dialogue between the user and assistant  such as greeting  choice state etc   the observations are
the interpretations of the dialogues of the human by the asr  the njfun system can be usefully
modeled as an hgmdp  where the goal of the assistant is to infer the users query given the observations and provide appropriate response  the initial assistant policy can be learned from training
data  in a manner similar to the dialogue policy of the njfun system 
our work is also related to on line plan recognition and can be naturally extended to include
hierarchies as in the hierarchical versions of hmms  bui  venkatesh    west        and pcfgs
 pynadath   wellman         blaylock and allen describe a statistical approach to goal recognition
that uses maximum likelihood estimates of goal schemas and parameters  blaylock   allen        
these approaches do not have the notion of cost or reward  by incorporating plan recognition in the
decision theoretic context  we obtain a natural notion of optimal assistance  namely maximizing the
expected utility 
there has been substantial research in the area of user modeling  horvitz et al  took a bayesian
approach to model whether a user needs assistance based on his actions and attributes and provided
assistance as needed in a spreadsheet application  horvitz et al          hui and boutilier used a
similar idea for assistance with text editing  hui   boutilier         they use dbns with handcoded
parameters to infer the type of the user and compute the expected utility of assisting the user  it
would be interesting to explore the use of ideas from plan recognition  charniak   goldman       
gal  reddy  shieber  rubin    grosz        chu  song  kautz    levinson        in our system
to take into account the users intentions and attitudes while computing the optimal policy for the
assistant 
recently  there have been methods proposed for solving pomdps called point based methods  pineau  gordon    thrun        porta  vlassis  spaan    poupart        kurniawati  hsu 
  lee        shani  pineau    kaplow         an example of this method is point based value
iteration  pbvi   pineau et al         porta et al         that takes a set of belief points b as input
and maintains a set of pomdp  vectors at each iteration  each iteration produces a new set of vectors that are optimal for each belief point with respect to the  vectors in the previous iteration 
the approximation made by pbvi when compared to value iteration is that there is no guarantee
that the set of  vectors is optimal for the entire belief space  by omitting some  vectors  pbvi
maintains a constant run time per iteration  application of efficient point based methods such as

  

fia d ecision  t heoretic m odel of a ssistance

pbvi to the decision theoretic assistance problem and evaluation of their performance compared to
the policy rollout and sparse sampling methods remains a promising research direction 

   summary and future work
we introduced a decision theoretic framework for assistant systems and described the hgmdp
as an appropriate model for selecting assistive actions  the computational complexity of hgmdps
motivated the definition of a simpler model called hamdp  which allows efficient myopic heurstics
and more tractable special cases 
we also described an approximate solution approach based on iteratively estimating the agents
goal and selecting actions using myopic heuristics  our evaluation using human subjects in two
game like domains show that the approach can significantly help the user  we also demonstrated in
a real world folder predictor that the decision theoretic framework was more effective than the state
of the art techniques for folder prediction 
one future direction is to consider more complex domains where the assistant is able to do a series of activities in parallel with the agent  another possible direction is to assume hierarchical goal
structure for the user and do goal estimation in that context  recently  the assistantship model was
extended to hierarchical and relational settings  natarajan et al         by including parameterized
task hierarchies and conditional relational influences as prior knowledge of the assistant  this prior
knowledge would relax the assumption that the user mdp can be solved tractably  this knowledge
was compiled into an underlying dynamic bayesian network  and bayesian network inference algorithms were used to infer a distribution of users goals given a sequence of her atomic actions 
the parameters for the users policy were estimated by observing the users actions 
our framework can be naturally extended to the case where the environment is partially observable to the agent and or to the assistant  this requires recognizing actions taken to gather
information  e g   opening the fridge to decide what to make based on what is available  incorporating more sophisticated user modeling that includes users forgetting their goals  not paying attention
to an important detail  and or changing their intentions would be extremely important for building
practical systems  the assistive technology can also be very useful if the assistant can quickly learn
new tasks from expert users and transfer the knowledge to novice users during training 

acknowledgements
this material is based upon work supported by the defense advanced research projects agency
 darpa   through the department of the interior  nbc  acquisition services division  under contract no  nbchd        any opinions  findings  and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily reflect the views of darpa  alan
fern and prasad tadepalli gratefully acknowledge the following grants  nsf iis         and onr
n                 sriraam natarajan thanks army research office grant number w   nf          under the young investigator program 

appendix a  proof of theorem  
according to the theory of pomdps  the optimal action in a pomdp maximizes the sum of the
immediate expected reward and the value of the resulting belief state  of the assistant   kaelbling 

  

fif ern   natarajan   j udah     tadepalli

littman    cassandra         when the agent policy is deterministic  the initial goal distribution
ig and the history of agent actions and states h fully captures the belief state of the agent  let
v  ig   h  represent the value of the current belief state  then the value function of the belief state
is given by by the following bellman equation  where h   stands for the history after the assistants
action hi and the agents action aj  
v  ig   h    max e r  s  hi    g  aj      v  ig   h    
hi

h 

   

since there is only one agents action in  s  g   the agent action aj   the subsequent state s  in
and its value do not depend on hi   hence the best helper action h of the assistant is given by 
h  ig   h    arg max e r  s  hi    g  a  s  g   
hi
x
  arg max
ig  g i ai   s  g  
hi

gc h 

  arg max ig  c h   g s  ai   
hi

where c h  is the set of goals consistent with the current history h  and g s  ai   is the set of goals
for which ai is good in state s  i ai   s  g   is an indicator function which is     if ai   s  g  
note that h is exactly the myopic policy   

appendix b  proof of lemma  
the worst case regret of any pair  s  g  in the hamdp is given by the following bellman equation 
assuming that g is the set of possible goals of the agent and the current state is s  regret s  g     
if s is a terminal state or all goals in g are satisfied in s  otherwise  regret s  g    mini maxj  i
 regret si   gi        regret sj   gj     where  si   gi   and  sj   gj   are in children  s  g   
here the outer min is due to the assistant picking a helper action for a goal in gi to maximize
the reward and the inner max is due to the agent either accepting it  or picking a different goal to
minimize the reward  the proof is by induction  each node in the trajectory tree represents a state
and a set of goals g for which that state is on the optimal path 
basis  if  s  g  is a leaf node  it is either a terminal state or all goals in g are satisfied in s  hence
its rank equals its reward which is   
inductive step  suppose that the induction is true for all children of  s  g   we will consider two
cases 
case    there is a unique child of  s  g  representing  s    g    which has the highest regret among
all its children  by inductive hypothesis  rank  s    g       regret s    g     if the assistant chooses
the helper action that corresponds to  s    g     the agent can only choose actions that yield lower
regret in the worst case  choosing any other helper action would increase the regret  since the
agent could then choose a  and add   to the regret  so we have  regret s  g   regret s    g     
rank  s    g       rank  s  g   
case    there are at least two children  s    g    and  s    g    of  s  g  which have the highest rank
among all its children  by inductive hypothesis  rank  s    g       rank  s    g       regret s    g   
  regret s    g     here the agent can increase the regret by   more by choosing a goal in g  if the
assistant chooses g  and vice versa  hence  regret s  g     regret s    g        rank  s    g      
   

fia d ecision  t heoretic m odel of a ssistance

rank  s  g   
hence in both cases  we have shown that regret s  g  is rank  s  g     

appendix c  proof of theorem  
we first show that the problem is in np  we build a tree representation of a history dependent policy
for each initial state  every node in the tree is represeted by a triple  s  i  g   where s is a state 
g is a set of goals for which it is on a good path  and i is the index of the helper action chosen
by the policy at that node  the root node corresponds to a possible initial state and the initial goal
set ig   the children of a node in the tree represent possible successor nodes  sj   j  gj   reached by
the agents response to hi   whether by accepting hi and executing ai or by executing other actions 
the children resulting from ai are called accepted  and the latter are called rejected  note that
multiple children can result from the same action because the dynamics are a function of the agents
goal 
we now guess a policy tree and check that its maximum regret  i e  the maximum number of
rejected children in any path from the root to a leaf  is within bounds  to verify that the optimal
policy tree is of polynomial size we note that the number of leaf nodes is upper bounded by  g  
maxg n  g   where n  g  is the number of leaf nodes generated by the goal g  to estimate n  g  
we start from the root and navigate downwards  for any node that contains g in its goal set  if some
accepted child contains g  then it will be the only child that will be reached for g  if not  there is a
misprediction and there are at most k children reached  hence  the number of nodes reached for g
grows geometrically with the number of mispredictions  from theorem    since there are at most
log  g  mispredictions in any such path  n  g   k log   g    k logk  g  log  k    g log  k   hence the
total number of all leaf nodes of the tree is bounded by  g   log k   and the total number of nodes
in the tree is bounded by m g   log k   where m is the number of steps to the horizon  since this is
polynomial in the problem parameters  the problem is in np 
to show np hardness  we reduce   sat to the given problem  we consider each   literal clause
ci of a propositional formula  as a possible goal  the rest of the proof is identical to that of
theorem   except that all variables are set by the assistant since there are no universal quantifiers 
the agent only rejects the setting of the last variable in the clause if the clause evaluates to    the
worst regret on any goal is   iff the   sat problem has a satisfying assignment   

references
ambite  j  l   barish  g   knoblock  c  a   muslea  m   oh  j     minton  s          getting from
here to there  interactive planning and agent execution for optimizing travel  in proceedings
of the fourteenth conference on innovative applications of artificial intelligence  pp     
    
atkeson  c  g     schaal  s          learning tasks from a single demonstration  in proceedings
of ieee international conference on robotics and automation  pp           
bao  x   herlocker  j  l     dietterich  t  g          fewer clicks and less frustration  reducing the
cost of reaching the right folder  in proceedings of the eleventh international conference on
intelligent user interfaces  pp         
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 

   

fif ern   natarajan   j udah     tadepalli

blaylock  n     allen  j  f          statistical goal parameter recognition  in proceedings of the
fourteenth international conference on automated planning and scheduling  pp         
boger  j   poupart  p   hoey  j   boutilier  c   fernie  g     mihailidis  a          a decisiontheoretic approach to task assistance for persons with dementia  in proceedings of the nineteenth international joint conference on artificial intelligence  pp           
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions
and computational leverage  journal of artificial intelligence research          
bui  h   venkatesh  s     west  g          policy recognition in the abstract hidden markov models 
journal of artificial intelligence research             
cassandra  a  r          exact and approximate algorithms for partially observable markov decision processes  ph d  thesis  brown university 
charniak  e     goldman  r          plan recognition in stories and in life  corr  abs           
chu  y   song  y   kautz  h     levinson  r          when did you start doing that thing that you
do  interactive activity recognition and prompting  in proceedings of the twenty fifth aaai
conference workshop on artificial intelligence and smarter living  pp       
clouse  j  a     utgoff  p  e          a teaching method for reinforcement learning  in proceedings
of the ninth international workshop on machine learning  pp        
cohen  w  w   carvalho  v  r     mitchell  t  m          learning to classify email into speech acts 
in proceedings of the conference on empirical methods in natural language processing  pp 
       
cypher  a          watch what i do  programming by demonstration  mit press 
doshi  p     gmytrasiewicz  p          a particle filtering algorithm for interactive pomdps  in
proceedings of the workshop on modeling other agents from observations  pp       
dragunov  a  n   dietterich  t  g   johnsrude  k   mclaughlin  m   li  l     herlocker  j  l         
tasktracer  a desktop environment to support multi tasking knowledge workers  in proceedings of the tenth international conference on intelligent user interfaces  pp       
driessens  k          adding guidance to relational reinforcement learning  in third freiburgleuven workshop on machine learning 
ehrenfeucht  a     haussler  d          learning decision trees from random examples  information and computation                
gal  y   reddy  s   shieber  s   rubin  a     grosz  b          plan recognition in exploratory
domains  artificial intelligence                   
geffner  h     bonet  b          solving large pomdps using real time dynamic programming  in
proceedings of aaai fall symposium on pompds 
ginsberg  m  l          gib  steps toward an expert level bridge playing program  in proceedings of the sixteenth international joint conference on artificial intelligence  pp         
guestrin  c   koller  d   parr  r     venkataraman  s          efficient solution algorithms for
factored mdps  journal of artificial intelligence research             

   

fia d ecision  t heoretic m odel of a ssistance

horvitz  e   breese  j   heckerman  d   hovel  d     rommelse  k          the lumiere project 
bayesian user modeling for inferring the goals and needs of software users  in proceedings
of the fourteenth conference on uncertainty in artificial intelligence  pp         
hui  b     boutilier  c          whos asking for help   a bayesian approach to intelligent assistance  in proceedings of the eleventh international conference on intelligent user interfaces 
pp         
johnson  d   cuijpers  r   juol  j   torta  e   simonov  m   frisiello  a   bazzani  m   yan  w  
weber  c   wermter  s   et al          socially assistive robots  a comprehensive approach to
extending independent living  international journal of social robotics               
johnson  m          inverse optimal control for deterministic continuous time nonlinear systems 
ph d  thesis  university of illinois at urbana champaign 
kaelbling  l   littman  m     cassandra  a          planning and acting in partially bservable
stochastic domains  artificial intelligence                  
kearns  m  j   mansour  y     ng  a  y          a sparse sampling algorithm for near optimal
planning in large markov decision processes  in proceedings of the sixteenth international
joint conference on artificial intelligence  pp           
konidaris  g   kuindersma  s   grupen  r     barto  a          robot learning from demonstration
by constructing skill trees  the international journal of robotics research                
kullback  s     leibler  r          on information and sufficiency  the annals of mathematical
statistics              
kurniawati  h   hsu  d     lee  w          sarsop  efficient point based pomdp planning by
approximating optimally reachable belief spaces  in proceedings of robotics  science and
systems iv 
lau  t   wolfman  s   domingos  p     weld  d          programming by demonstration using
version space algebra  machine learning                  
lieberman  h          user interface goals  ai opportunities  ai magazine              
littlestone  n          learning quickly when irrelevant attributes abound  a new linear threshold
algorithm  machine learning               
littman  m  l            algorithms for sequential decision making  ph d  thesis  brown university 
mahadevan  s   mitchell  t  m   mostow  j   steinberg  l  i     tadepalli  p          an apprenticebased approach to knowledge acquisition   artificial intelligence             
mitchell  t  m   caruana  r   freitag  d   j mcdermott    zabowski  d          experience with a
learning personal assistant  communications of the acm              
moore  a  w     atkeson  c  g          prioritized sweeping  reinforcement learning with less
data and less time  machine learning             
mundhenk  m          the complexity of planning with partially observable markov decision
processes  ph d  thesis  friedrich schiller universitdt 

   

fif ern   natarajan   j udah     tadepalli

myers  k   berry  p   blythe  j   conleyn  k   gervasio  m   mcguinness  d   morley  d   pfeffer 
a   pollack  m     tambe  m          an intelligent personal assistant for task and time
management  in ai magazine  vol      pp       
natarajan  s   tadepalli  p     fern  a          a relational hierarchical model for decision theoretic
assistance  in proceedings of the seventeenth annual international conference on inductive
logic programming  pp         
papadimitriou  c     tsitsiklis  j          the complexity of markov decision processes  mathematics of operations research                
pineau  j   gordon  g     thrun  s          point based value iteration  an anytime algorithm
for pomdps  in proceedings of the eighteenth international joint conference on artificial
intelligence  pp             
porta  j   vlassis  n   spaan  m     poupart  p          point based value iteration for continuous
pomdps  journal of machine learning research              
pynadath  d  v     wellman  m  p          probabilistic state dependent grammars for plan recognition  in proceedings of the sixteenth conference on uncertainty in artificial intelligence 
pp         
refanidis  i   alexiadis  a     yorke smith  n          beyond calendar mashups  intelligent calendaring  in proceedings of the twenty first international conference on automated planning
and scheduling system demonstrations 
shani  g   pineau  j     kaplow  r          a survey of point based pomdp solvers  autonomous
agents and multi agent systems             
singh  s  p   litman  d  j   kearns  m  j     walker  m  a          optimizing dialogue management with reinforcement learning  experiments with the njfun system   journal of artificial
intelligence research             
skaanning  c   jensen  f  v     kjaerulff  u          printer troubleshooting using bayesian networks  in proceedings of the thirteenth international conference on industrial and engineering applications of artificial intelligence and expert systems  pp         
varakantham  p   maheswaran  r  t     tambe  m          exploiting belief bounds  practical
pomdps for personal assistant agents  in proceedings of the fourth internation conference
on autonomous agents and multiagent systems  pp         
walker  m  a          an application of reinforcement learning to dialogue strategy selection in a
spoken dialogue system for email  journal of artificial intelligence research             
yorke smith  n   saadati  s   myers  k     morley  d          the design of a proactive personal
agent for task management  international journal on artificial intelligence tools           
    

   

fi