journal of artificial intelligence research                  

submitted        published      

the generalized a  architecture
pedro f  felzenszwalb

pff cs uchicago edu

department of computer science
university of chicago
chicago  il      

david mcallester

mcallester tti c org

toyota technological institute at chicago
chicago  il      

abstract
we consider the problem of computing a lightest derivation of a global structure using
a set of weighted rules  a large variety of inference problems in ai can be formulated in
this framework  we generalize a  search and heuristics derived from abstractions to a
broad class of lightest derivation problems  we also describe a new algorithm that searches
for lightest derivations using a hierarchy of abstractions  our generalization of a  gives a
new algorithm for searching and or graphs in a bottom up fashion 
we discuss how the algorithms described here provide a general architecture for addressing the pipeline problem  the problem of passing information back and forth between
various stages of processing in a perceptual system  we consider examples in computer vision and natural language processing  we apply the hierarchical search algorithm to the
problem of estimating the boundaries of convex objects in grayscale images and compare
it to other search methods  a second set of experiments demonstrate the use of a new
compositional model for finding salient curves in images 

   introduction
we consider a class of problems defined by a set of weighted rules for composing structures
into larger structures  the goal in such problems is to find a lightest  least cost  derivation
of a global structure derivable with the given rules  a large variety of classical inference
problems in ai can be expressed within this framework  for example the global structure
might be a parse tree  a match of a deformable object model to an image  or an assignment
of values to variables in a markov random field 
we define a lightest derivation problem in terms of a set of statements  a set of weighted
rules for deriving statements using other statements and a special goal statement  in each
case we are looking for the lightest derivation of the goal statement  we usually express a
lightest derivation problem using rule schemas that implicitly represent a very large set
of rules in terms of a small number of rules with variables  lightest derivation problems
are formally equivalent to search in and or graphs  nilsson         but we find that our
formulation is more natural for the applications we are interested in 
one of the goals of this research is the construction of algorithms for global optimization
across many levels of processing in a perceptual system  as described below our algorithms
can be used to integrate multiple stages of a processing pipeline into a single global optimization problem that can be solved efficiently 
c
    
ai access foundation  all rights reserved 

fifelzenszwalb   mcallester

dynamic programming is a fundamental technique for designing efficient inference algorithms  good examples are the viterbi algorithm for hidden markov models  rabiner 
      and chart parsing methods for stochastic context free grammars  charniak        
the algorithms described here can be used to speed up the solution of problems normally
solved using dynamic programming  we demonstrate this for a specific problem  where the
goal is to estimate the boundary of a convex object in a cluttered image  in a second set
of experiments we show how our algorithms can be used to find salient curves in images 
we describe a new model for salient curves based on a compositional rule that enforces
long range shape constraints  this leads to a problem that is too large to be solved using
classical dynamic programming methods 
the algorithms we consider are all related to dijkstras shortest paths algorithm  dsp 
 dijkstra        and a  search  hart  nilsson    raphael         both dsp and a  can be
used to find a shortest path in a cyclic graph  they use a priority queue to define an order
in which nodes are expanded and have a worst case running time of o m log n   where n
is the number of nodes in the graph and m is the number of edges  in dsp and a  the
expansion of a node v involves generating all nodes u such that there is an edge from v to u 
the only difference between the two methods is that a  uses a heuristic function to avoid
expanding non promising nodes 
knuth gave a generalization of dsp that can be used to solve a lightest derivation
problem with cyclic rules  knuth         we call this knuths lightest derivation algorithm
 kld   in analogy to dijkstras algorithm  kld uses a priority queue to define an order in
which statements are expanded  here the expansion of a statement v involves generating
all conclusions that can be derived in a single step using v and other statements already
expanded  as long as each rule has a bounded number of antecedents kld also has a worst
case running time of o m log n   where n is the number of statements in the problem
and m is the number of rules  nilssons ao  algorithm        can also be used to solve
lightest derivation problems  although ao  can use a heuristic function  it is not a true
generalization of a   it does not use a priority queue  only handles acyclic rules  and can
require o m n   time even when applied to a shortest path problem   in particular  ao 
and its variants use a backward chaining technique that starts at the goal and repeatedly
refines subgoals  while a  is a forward chaining algorithm  
klein and manning        described an a  parsing algorithm that is similar to kld but
can use a heuristic function  one of our contributions is a generalization of this algorithm
to arbitrary lightest derivation problems  we call this algorithm a  lightest derivation
 a ld   the method is forward chaining  uses a priority queue to control the order in
which statements are expanded  handles cyclic rules and has a worst case running time of
o m log n   for problems where each rule has a small number of antecedents  a ld can be
seen as a true generalization of a  to lightest derivation problems  for a lightest derivation
problem that comes from a shortest path problem a ld is identical to a  
of course the running times seen in practice are often not well predicted by worst case
analysis  this is specially true for problems that are very large and defined implicitly  for
example  we can use dynamic programming to solve a shortest path problem in an acyclic
graph in o m   time  this is better than the o m log n   bound for dsp  but for implicit
   there are extensions that handle cyclic rules  jimenez   torras        
   ao  is backward chaining in terms of the inference rules defining a lightest derivation problem 

   

fithe generalized a  architecture

graphs dsp can be much more efficient since it expands nodes in a best first order  when
searching for a shortest path from a source to a goal  dsp will only expand nodes v with
d v   w   here d v  is the length of a shortest path from the source to v  and w is the
length of a shortest path from the source to the goal  in the case of a  with a monotone and
admissible heuristic function  h v   it is possible to obtain a similar bound when searching
implicit graphs  a  will only expand nodes v with d v    h v   w  
the running time of kld and a ld can be expressed in a similar way  when solving a
lightest derivation problem  kld will only expand statements v with d v   w   here d v 
is the weight of a lightest derivation for v  and w is the weight of a lightest derivation of the
goal statement  furthermore  a ld will only expand statements v with d v    h v   w  
here the heuristic function  h v   gives an estimate of the additional weight necessary for
deriving the goal statement using a derivation of v  the heuristic values used by a ld are
analogous to the distance from a node to the goal in a graph search problem  the notion
used by a    we note that these heuristic values are significantly different from the ones
used by ao   in the case of ao  the heuristic function  h v   would estimate the weight
of a lightest derivation for v 
an important difference between a ld and ao  is that a ld computes derivations
in a bottom up fashion  while ao  uses a top down approach  each method has advantages  depending on the type of problem being solved  for example  a classical problem in
computer vision involves grouping pixels into long and smooth curves  we can formulate
the problem in terms of finding smooth curves between pairs of pixels that are far apart 
for an image with n pixels there are  n    such pairs  a straight forward implementation
of a top down algorithm would start by considering these  n    possibilities  a bottomup algorithm would start with o n  pairs of nearby pixels  in this case we expect that a
bottom up grouping method would be more efficient than a top down method 
the classical ao  algorithm requires the set of rules to be acyclic  jimenez and torras
       extended the method to handle cyclic rules  another top down algorithm that can
handle cyclic rules is described by bonet and geffner         hansen and zilberstein       
described a search algorithm for problems where the optimal solutions themselves can be
cyclic  the algorithms described in this paper can handle problems with cyclic rules but
require that the optimal solutions be acyclic  we also note that ao  can handle rules with
non superior weight functions  as defined in section    while kld requires superior weight
functions  a ld replaces this requirement by a requirement on the heuristic function 
a well known method for defining heuristics for a  is to consider an abstract or relaxed
search problem  for example  consider the problem of solving a rubiks cube in a small
number of moves  suppose we ignore the edge and center pieces and solve only the corners 
this is an example of a problem abstraction  the number of moves necessary to put the
corners in a good configuration is a lower bound on the number of moves necessary to solve
the original problem  there are fewer corner configurations than there are full configurations
and that makes it easier to solve the abstract problem  in general  shortest paths to the
goal in an abstract problem can be used to define an admissible and monotone heuristic
function for solving the original problem with a  
here we show that abstractions can also be used to define heuristic functions for a ld 
in a lightest derivation problem the notion of a shortest path to the goal is replaced by
the notion of a lightest context  where a context for a statement v is a derivation of the
   

fifelzenszwalb   mcallester

goal with a hole that can be filled in by a derivation of v  the computation of lightest
abstract contexts is itself a lightest derivation problem 
abstractions are related to problem relaxations defined by pearl         while abstractions often lead to small problems that are solved through search  relaxations can lead to
problems that still have a large state space but may be simple enough to be solved in closed
form  the definition of abstractions that we use for lightest derivation problems includes
relaxations as a special case 
another contribution of our work is a hierarchical search method that we call ha ld 
this algorithm can effectively use a hierarchy of abstractions to solve a lightest derivation
problem  the algorithm is novel even in the case of classical search  shortest paths  problem  ha ld searches for lightest derivations and contexts at every level of abstraction
simultaneously  more specifically  each level of abstraction has its own set of statements
and rules  the search for lightest derivations and contexts at each level is controlled by a
single priority queue  to understand the running time of ha ld  let w be the weight of a
lightest derivation of the goal in the original  not abstracted  problem  for a statement v
in the abstraction hierarchy let d v  be the weight of a lightest derivation for v at its level
of abstraction  let h v  be the weight of a lightest context for the abstraction of v  defined
at one level above v in the hierarchy   let k be the total number of statements in the
hierarchy with d v    h v   w   hal d expands at most  k statements before solving
the original problem  the factor of two comes from the fact that the algorithm computes
both derivations and contexts at each level of abstraction 
previous algorithms that use abstractions for solving search problems include methods based on pattern databases  culberson   schaeffer        korf        korf   felner 
       hierarchical a   ha   hida    holte  perez  zimmer    macdonald        holte 
grajkowski    tanner        and coarse to fine dynamic programming  cfdp   raphael 
       pattern databases have made it possible to compute solutions to impressively large
search problems  these methods construct a lookup table of shortest paths from a node
to the goal at all abstract states  in practice the approach is limited to tables that remain
fixed over different problem instances  or relatively small tables if the heuristic must be
recomputed for each instance  for example  for the rubiks cube we can precompute the
number of moves necessary to solve every corner configuration  this table can be used to
define a heuristic function when solving any full configuration of the rubiks cube  both
ha  and hida  use a hierarchy of abstractions and can avoid searching over all nodes at
any level of the hierarchy  on the other hand  in directed graphs these methods may still
expand abstract nodes with arbitrarily large heuristic values  it is also not clear how to
generalize ha  and hida  to lightest derivation problems that have rules with more than
one antecedent  finally  cfdp is related to ao  in that it repeatedly solves ever more
refined problems using dynamic programming  this leads to a worst case running time of
o n m    we will discuss the relationships between ha ld and these other hierarchical
methods in more detail in section   
we note that both a  search and related algorithms have been previously used to solve
a number of problems that are not classical state space search problems  this includes the
traveling salesman problem  zhang   korf         planning  edelkamp         multiple
sequence alignment  korf  zhang  thayer    hohwald         combinatorial problems on
graphs  felner        and parsing using context free grammars  klein   manning        
   

fithe generalized a  architecture

the work by bulitko  sturtevant  lu  and yau        uses a hierarchy of state space abstractions for real time search 
    the pipeline problem
a major problem in artificial intelligence is the integration of multiple processing stages to
form a complete perceptual system  we call this the pipeline problem  in general we have
a concatenation of systems where each stage feeds information to the next  in vision  for
example  we might have an edge detector feeding information to a boundary finding system 
which in turn feeds information to an object recognition system 
because of computational constraints and the need to build modules with clean interfaces
pipelines often make hard decisions at module boundaries  for example  an edge detector
typically constructs a boolean array that indicates weather or not an edge was detected
at each image location  but there is general recognition that the presence of an edge at a
certain location can depend on the context around it  people often see edges at places where
the image gradient is small if  at higher cognitive level  it is clear that there is actually an
object boundary at that location  speech recognition systems try to address this problem
by returning n best lists  but these may or may not contain the actual utterance  we would
like the speech recognition system to be able to take high level information into account
and avoid the hard decision of exactly what strings to output in its n best list 
a processing pipeline can be specified by describing each of its stages in terms of rules for
constructing structures using structures produced from a previous stage  in a vision system
one stage could have rules for grouping edges into smooth curves while the next stage could
have rules for grouping smooth curves into objects  in this case we can construct a single
lightest derivation problem representing the entire system  moreover  a hierarchical set of
abstractions can be applied to the entire pipeline  by using ha ld to compute lightest
derivations a complete scene interpretation derived at one level of abstraction guides all
processing stages at a more concrete level  this provides a mechanism that enables coarse
high level processing to guide low level computation  we believe that this is an important
property for implementing efficient perceptual pipelines that avoid making hard decisions
between processing stages 
we note that the formulation of a complete computer vision system as a lightest derivation problem is related to the work by geman  potter  and chi         tu  chen  yuille 
and zhu        and jin and geman         in these papers image understanding is posed
as a parsing problem  where the goal is to explain the image in terms of a set of objects that
are formed by the  possibly recursive  composition of generic parts  tu et al         use
data driven mcmc to compute optimal parses while geman et al         and jin and
geman        use a bottom up algorithm for building compositions in a greedy fashion 
neither of these methods are guaranteed to compute an optimal scene interpretation  we
hope that ha ld will provide a more principled computational technique for solving large
parsing problems defined by compositional models 
    overview
we begin by formally defining lightest derivation problems in section    that section also
discusses dynamic programming and the relationship between lightest derivation problems
   

fifelzenszwalb   mcallester

and and or graphs  in section   we describe knuths lightest derivation algorithm  in
section   we describe a ld and prove its correctness  section   shows how abstractions
can be used to define mechanically constructed heuristic functions for a ld  we describe
ha ld in section   and discuss its use in solving the pipeline problem in section    section   discusses the relationship between ha ld and other hierarchical search methods  in
sections   and    we present some experimental results  we conclude in section    

   lightest derivation problems
let  be a set of statements and r be a set of inference rules of the following form 
a    w 
  
 
an   wn
c   g w            wn  
here the antecedents ai and the conclusion c are statements in   the weights wi are
non negative real valued variables and g is a non negative real valued weight function  for
a rule with no antecedents the function g is simply a non negative real value  throughout
the paper we also use a            an g c to denote an inference rule of this type 
a derivation of c is a finite tree rooted at a rule a            an g c with n children  where
the i th child is a derivation of ai   the leaves of this tree are rules with no antecedents 
every derivation has a weight that is the value obtained by recursive application of the
functions g along the derivation tree  figure   illustrates a derivation tree 
intuitively a rule a            an g c says that if we can derive the antecedents ai with
weights wi then we can derive the conclusion c with weight g w            wn    the problem
we are interested in is to compute a lightest derivation of a special goal statement 
all of the algorithms discussed in this paper assume that the weight functions g associated with a lightest derivation problem are non decreasing in each variable  this is a
fundamental property ensuring that lightest derivations have an optimal substructure property  in this case lightest derivations can be constructed from other lightest derivations 
to facilitate the runtime analysis of algorithms we assume that every rule has a small
number of antecedents  we use n to denote the number of statements in a lightest derivation
problem  while m denotes the number of rules  for most of the problems we are interested
in n and m are very large but the problem can be implicitly defined in a compact way 
by using a small number of rules with variables as in the examples below  we also assume
that n  m since statements that are not in the conclusion of some rule are clearly not
derivable and can be ignored 
    dynamic programming
we say that a set of rules is acyclic if there is an ordering o of the statements in  such
that for any rule with conclusion c the antecedents are statements that come before c in
the ordering  dynamic programming can be used to solve a lightest derivation problem if
   

fithe generalized a  architecture

a 
a 
a 
c

derivation
of a 

derivation
of a 

derivation
of a 

figure    a derivation of c is a tree of rules rooted at a rule r with conclusion c  the
children of the root are derivations of the antecedents in r  the leafs of the tree
are rules with no antecedents 

the functions g in each rule are non decreasing and the set of rules is acyclic  in this case
lightest derivations can be computed sequentially in terms of an acyclic ordering o  at the
i th step a lightest derivation of the i th statement is obtained by minimizing over all rules
that can be used to derive that statement  this method takes o m   time to compute a
lightest derivation for each statement in  
we note that for cyclic rules it is sometimes possible to compute lightest derivations by
taking multiple passes over the statements  we also note that some authors would refer
to dijkstras algorithm  and kld  as a dynamic programming method  in this paper we
only use the term when referring to algorithms that compute lightest derivations in a fixed
order that is independent of the solutions computed along the way  this includes recursive
implementations that use memoization  
    examples
rules for computing shortest paths from a single source in a weighted graph are shown
in figure    we assume that we are given a weighted graph g    v  e   where wxy is a
non negative weight for each edge  x  y   e and s is a distinguished start node  the first
rule states that there is a path of weight zero to the start node s  the second set of rules
state that if there is a path to a node x we can extend that path with an edge from x to
y to obtain an appropriately weighted path to a node y  there is a rule of this type for
each edge in the graph  a lightest derivation of path x  corresponds to shortest path from
s to x  note that for general graphs these rules can be cyclic  figure   illustrates a graph
   

fifelzenszwalb   mcallester

   

path s     
    for each  x  y   e 
path x    w
path y    w   wxy
figure    rules for computing shortest paths in a graph 

b

c

path d    w

path c    w

path b    w   wdb

path b    w   wcb

path s    w

path e    w

path d    w   wsd

path c    w   wec

d
a
e
s
path s    w
path s     
path e    w   wse

path s     

figure    a graph with two highlighted paths from s to b and the corresponding derivations
using rules from figure   

and two different derivations of path b  using the rules just described  these corresponds
to two different paths from s to b 
rules for chart parsing are shown in figure    we assume that we are given a weighted
context free grammar in chomsky normal form  charniak         i e   a weighted set of
productions of the form x  s and x  y z where x  y and z are nonterminal symbols
and s is a terminal symbol  the input string is given by a sequence of terminals  s            sn   
   

fithe generalized a  architecture

    for each production x  si  

phrase x  i  i        w x  si  
    for each production x  y z and    i   j   k  n     
phrase y  i  j    w 
phrase z  j  k    w 
phrase x  i  k    w    w    w x  y z 
figure    rules for parsing with a context free grammar 
the first set of rules state that if the grammar contains a production x  si then there is a
phrase of type x generating the i th entry of the input with weight w x  si    the second
set of rules state that if the grammar contains a production x  y z and there is a phrase
of type y from i to j and a phrase of type z from j to k then there is an  appropriately
weighted  phrase of type x from i to k  let s be the start symbol of the grammar  the
goal of parsing is to find a lightest derivation of phrase s     n       these rules are acyclic
because when phrases are composed together they form longer phrases 
    and or graphs
lightest derivation problems are closely related to and or graphs  let  and r be a set
of statements and rules defining a lightest derivation problem  to convert the problem to
an and or graph representation we can build a graph with a disjunction node for each
statement in  and a conjunction node for each rule in r  there is an edge from each statement to each rule deriving that statement  and an edge from each rule to its antecedents 
the leaves of the and or graph are rules with no antecedents  now derivations of a
statement using rules in r can be represented by solutions rooted at that statement in the
corresponding and or graph  conversely  it is also possible to represent any and or
graph search problem as a lightest derivation problem  in this case we can view each node
in the graph as a statement in  and build an appropriate set of rules r 

   knuths lightest derivation
knuth        described a generalization of dijkstras shortest paths algorithm that we call
knuths lightest derivation  kld   knuths algorithm can be used to solve a large class of
lightest derivation problems  the algorithm allows the rules to be cyclic but requires that
the weight functions associated with each rule be non decreasing and superior  specifically
we require the following two properties on the weight function g in each rule 
non decreasing 
superior 

if wi   wi then g w            wi            wn    g w            wi           wn  
g w            wn    wi
   

fifelzenszwalb   mcallester

for example 
g x            xn     x         xn
g x            xn     max x            xn  
are both non decreasing and superior functions 
knuths algorithm computes lightest derivations in non decreasing weight order  since
we are only interested in a lightest derivation of a special goal statement we can often stop
the algorithm before computing the lightest derivation of every statement 
a weight assignment is an expression of the form  b   w  where b is a statement in 
and w is a non negative real value  we say that the weight assignment  b   w  is derivable
if there is a derivation of b with weight w  for any set of rules r  statement b  and weight
w we write r    b   w  if the rules in r can be used to derive  b   w   let   b  r  be
the infimum of the set of weights derivable for b 
  b  r    inf w   r    b   w   
given a set of rules r and a statement goal   we are interested in computing a derivation
of goal with weight   goal   r  
we define a bottom up logic programming language in which we can easily express the
algorithms we wish to discuss throughout the rest of the paper  each algorithm is defined
by a set of rules with priorities  we encode the priority of a rule by writing it along the
line separating the antecedents and the conclusion as follows 
a    w 
  
 
an   wn
p w            wn  
c   g w            wn  
we call a rule of this form a prioritized rule  the execution of a set of prioritized rules
p is defined by the procedure in figure    the procedure keeps track of a set s and a
priority queue q of weight assignments of the form  b   w   initially s is empty and q
contains weight assignments defined by rules with no antecedents at the priorities given by
those rules  we iteratively remove the lowest priority assignment  b   w  from q  if b
already has an assigned weight in s then the new assignment is ignored  otherwise we add
the new assignment to s and expand it  every assignment derivable from  b   w  and
other assignments already in s using some rule in p is added to q at the priority specified
by the rule  the procedure stops when the queue is empty 
the result of executing a set of prioritized rules is a set of weight assignments  moreover 
the procedure can implicitly keep track of derivations by remembering which assignments
were used to derive an item that is inserted in the queue 
lemma    the execution of a finite set of prioritized rules p derives every statement that
is derivable with rules in p  
proof  each rule causes at most one item to be inserted in the queue  thus eventually q
is empty and the algorithm terminates  when q is empty every statement derivable by a
   

fithe generalized a  architecture

procedure run p  
   s  
   initialize q with assignments defined by rules with no antecedents at their priorities
   while q is not empty
  
remove the lowest priority element  b   w  from q
  
if b has no assigned weight in s
  
s  s    b   w  
  
insert assignments derivable from  b   w  and other assignments in s using
some rule in p into q at the priority specified by the rule
   return s
figure    running a set of prioritized rules 
single rule using antecedents with weight in s already has a weight in s  this implies that
every derivable statement has a weight in s 
now we are ready to define knuths lightest derivation algorithm  the algorithm is
easily described in terms of prioritized rules 
definition    knuths lightest derivation   let r be a finite set of non decreasing and
superior rules  define a set of prioritized rules k r  by setting the priority of each rule in
r to be the weight of the conclusion  kld is given by the execution of k r  
we can show that while running k r   if  b   w  is added to s then w     b  r  
this means that all assignments in s represent lightest derivations  we can also show that
assignments are inserted into s in non decreasing weight order  if we stop the algorithm as
soon as we insert a weight assignment for goal into s we will expand all statements b such
that   b  r      goal   r  and some statements b such that   b  r      goal   r   these
properties follow from a more general result described in the next section 
    implementation
the algorithm in figure   can be implemented to run in o m log n   time  where n and
m refer to the size of the problem defined by the prioritized rules p  
in practice the set of prioritized rules p is often specified implicitly  in terms of a small
number of rules with variables  in this case the problem of executing p is closely related to
the work on logical algorithms described by mcallester        
the main difficulty in devising an efficient implementation of the procedure in figure  
is in step    in that step we need to find weight assignments in s that can be combined
with  b   w  to derive new weight assignments  the logical algorithms work shows how a
set of inference rules with variables can be transformed into a new set of rules  such that
every rule has at most two antecedents and is in a particularly simple form  moreover 
this transformation does not increase the number of rules too much  once the rules are
transformed their execution can be implemented efficiently using a hashtable to represent
s  a heap to represent q and indexing tables that allow us to perform step   quickly 
   

fifelzenszwalb   mcallester

consider the second set of rules for parsing in figure    these can be represented by
a single rule with variables  moreover the rule has two antecedents  when executing the
parsing rules we keep track of a table mapping a value for j to statements phrase y  i  j 
that have a weight in s  using this table we can quickly find statements that have a weight
in s and can be combined with a statement of the form phrase z  j  k   similarly we keep
track of a table mapping a value for j to statements phrase z  j  k  that have a weight in
s  the second table lets us quickly find statements that can be combined with a statement
of the form phrase y  i  j   we refer the reader to  mcallester        for more details 

   a  lightest derivation
our a  lightest derivation algorithm  a ld  is a generalization of a  search to lightest
derivation problems that subsumes a  parsing  the algorithm is similar to kld but it can
use a heuristic function to speed up computation  consider a lightest derivation problem
with rules r and goal statement goal   knuths algorithm will expand any statement b
such that   b  r      goal   r   by using a heuristic function a ld can avoid expanding
statements that have light derivations but are not part of a light derivation of goal  
let r be a set of rules with statements in   and h be a heuristic function assigning
a weight to each statement  here h b  is an estimate of the additional weight required to
derive goal using a derivation of b  we note that in the case of a shortest path problem this
weight is exactly the distance from a node to the goal  the value   b  r    h b  provides
a figure of merit for each statement b  the a  lightest derivation algorithm expands
statements in order of their figure of merit 
we say that a heuristic function is monotone if for every rule a            an g c in r
and derivable weight assignments  ai   wi   we have 
wi   h ai    g w            wn     h c  

   

this definition agrees with the standard notion of a monotone heuristic function for rules
that come from a shortest path problem  we can show that if h is monotone and h goal      
then h is admissible under an appropriate notion of admissibility  for the correctness of
a ld  however  it is only required that h be monotone and that h goal   be finite  in this
case monotonicity implies that the heuristic value of every statement c that appears in a
derivation of goal is finite  below we assume that h c  is finite for every statement  if h c 
is not finite we can ignore c and every rule that derives c 
definition    a  lightest derivation   let r be a finite set of non decreasing rules and h
be a monotone heuristic function for r  define a set of prioritized rules a r  by setting
the priority of each rule in r to be the weight of the conclusion plus the heuristic value 
g w            wn     h c   a ld is given by the execution of a r  
now we show that the execution of a r  correctly computes lightest derivations and
that it expands statements in order of their figure of merit values 
theorem    during the execution of a r   if  b   w   s then w     b  r  
proof  the proof is by induction on the size of s  the statement is trivial when s    
suppose the statement was true right before the algorithm removed  b   wb   from q and
   

fithe generalized a  architecture

added it to s  the fact that  b   wb    q implies that the weight assignment is derivable
and thus wb    b  r  
suppose t is a derivation of b with weight wb    wb   consider the moment right before
the algorithm removed  b   wb   from q and added it to s  let a            an g c be a
rule in t such that the antecedents ai have a weight in s while the conclusion c does not 
let wc   g   a    r             an   r    by the induction hypothesis the weight of ai in s is
  ai   r   thus  c   wc    q at priority wc   h c   let wc  be the weight that t assigns to
c  since g is non decreasing we know wc  wc    since h is monotone wc   h c   wb   h b  
this follows by using the monotonicity condition along the path from c to b in t   now
note that wc   h c    wb   h b  which in turn implies that  b   wb   is not the weight
assignment in q with minimum priority 
theorem    during the execution of a r  statements are expanded in order of the figure
of merit value   b  r    h b  
proof  first we show that the minimum priority of q does not decrease throughout the
execution of the algorithm  suppose  b   w  is an element in q with minimum priority 
removing  b   w  from q does not decrease the minimum priority  now suppose we add
 b   w  to s and insert assignments derivable from  b   w  into q  since h is monotone
the priority of every assignment derivable from  b   w  is at least the priority of  b   w  
a weight assignment  b   w  is expanded when it is removed from q and added to s 
by the last theorem w     b  r  and by the definition of a r  this weight assignment was
queued at priority   b  r    h b   since we removed  b   w  from q this must be the
minimum priority in the queue  the minimum priority does not decrease over time so we
must expand statements in order of their figure of merit value 
if we have accurate heuristic functions a ld can be much more efficient than kld 
consider a situation where we have a perfect heuristic function  that is  suppose h b 
is exactly the additional weight required to derive goal using a derivation of b  now the
figure of merit   b  r    h b  equals the weight of a lightest derivation of goal that uses
b  in this case a ld will derive goal before expanding any statements that are not part
of a lightest derivation of goal  
the correctness kld follows from the correctness of a ld  for a set of non decreasing
and superior rules we can consider the trivial heuristic function h b       the fact that
the rules are superior imply that this heuristic is monotone  the theorems above imply
that knuths algorithm correctly computes lightest derivations and expands statements in
order of their lightest derivable weights 

   heuristics derived from abstractions
here we consider the case of additive rules  rules where the weight of the conclusion is
the sum of the weights of the antecedents plus a non negative value v called the weight of
the rule  we denote such a rule by a            an v c  the weight of a derivation using
additive rules is the sum of the weights of the rules that appear in the derivation tree 
a context for a statement b is a finite tree of rules such that if we add a derivation of
b to the tree we get a derivation of goal   intuitively a context for b is a derivation of goal
with a hole that can be filled in by a derivation of b  see figure    
   

fifelzenszwalb   mcallester

   
   
goal

a 
a 
a 
context for c

c

context for a 

derivation
of a 

derivation
of a 

derivation
of a 

figure    a derivation of goal defines contexts for the statements that appear in the derivation tree  note how a context for c together with a rule a    a    a   c and
derivations of a  and a  define a context for a   

for additive rules  each context has a weight that is the sum of weights of the rules in it 
let r be a set of additive rules with statements in   for b   we define   context b   r 
to be the weight of a lightest context for b  the value   b  r      context b   r  is the
weight of a lightest derivation of goal that uses b 
contexts can be derived using rules in r together with context rules c r  defined as
follows  first  goal has an empty context with weight zero  this is captured by a rule with
no antecedents   context goal    for each rule a            an v c in r we put n rules in
c r   these rules capture the notion that a context for c and derivations of aj for j    i
define a context for ai  
context c   a            ai    ai             an v context ai   
figure   illustrates how a context for c together with derivations of a  and a  and a rule
a    a    a   c define a context for a   
   

fithe generalized a  architecture

we say that a heuristic function h is admissible if h b     context b   r   admissible
heuristic functions never over estimate the weight of deriving goal using a derivation of a
particular statement  the heuristic function is perfect if h b      context b   r   now we
show how to obtain admissible and monotone heuristic functions from abstractions 
    abstractions
let    r  be a lightest derivation problem with statements  and rules r  an abstraction
of    r  is given by a problem      r    and a map abs         such that for every rule
a            an v c in r there is a rule abs a             abs an   v  abs c  in r  with v    v 
below we show how an abstraction can be used to define a monotone and admissible heuristic
function for the original problem 
we usually think of abs as defining a coarsening of  by mapping several statements
into the same abstract statement  for example  for a parser abs might map a lexicalized
nonterminal n phouse to the nonlexicalized nonterminal n p   in this case the abstraction
defines a smaller problem on the abstract statements  abstractions can often be defined in
a mechanical way by starting with a map abs from  into some set of abstract statements
    we can then project the rules in r from  into   using abs to get a set of abstract
rules  typically several rules in r will map to the same abstract rule  we only need to
keep one copy of each abstract rule  with a weight that is a lower bound on the weight of
the concrete rules mapping into it 
every derivation in    r  maps to an abstract derivation so we have   abs c   r    
  c  r   if we let the goal of the abstract problem be abs goal   then every context in    r 
maps to an abstract context and we see that   context abs c    r       context c   r  
this means that lightest abstract context weights form an admissible heuristic function 
h c      context abs c    r    
now we show that this heuristic function is also monotone 
consider a rule a            an v c in r and let  ai   wi   be weight assignments derivable
using r  in this case there is a rule abs a             abs an   v  abs c  in r  where v    v
and  abs ai     wi    is derivable using r  where wi   wi   by definition of contexts  in the
abstract problem  we have 
x
  context abs ai     r     v    
wj      context abs c    r    
j  i

since v    v and wj   wj we have 
  context abs ai     r     v  

x

wj     context abs c    r    

j  i

plugging in the heuristic function h from above and adding wi to both sides 
x
wi   h ai    v  
wj   h c  
j

which is exactly the monotonicity condition in equation     for an additive rule 
   

fifelzenszwalb   mcallester

if the abstract problem defined by      r    is relatively small we can efficiently compute
lightest context weights for every statement in   using dynamic programming or kld 
we can store these weights in a pattern database  a lookup table  to serve as a heuristic
function for solving the concrete problem using a ld  this heuristic may be able to stop
a ld from exploring a lot of non promising structures  this is exactly the approach that
was used by culberson and schaeffer        and korf        for solving very large search
problems  the results in this section show that pattern databases can be used in the more
general setting of lightest derivations problems  the experiments in section    demonstrate
the technique in a specific application 

   hierarchical a  lightest derivation
the main disadvantage of using pattern databases is that we have to precompute context
weights for every abstract statement  this can often take a lot of time and space  here we
define a hierarchical algorithm  ha ld  that searches for lightest derivations and contexts
in an entire abstraction hierarchy simultaneously  this algorithm can often solve the most
concrete problem without fully computing context weights at any level of abstraction 
at each level of abstraction the behavior ha ld is similar to the behavior of a ld
when using an abstraction derived heuristic function  the hierarchical algorithm queues
derivations of a statement c at a priority that depends on a lightest abstract context for
c  but now abstract contexts are not computed in advance  instead  abstract contexts are
computed at the same time we are computing derivations  until we have an abstract context
for c  derivations of c are stalled  this is captured by the addition of context abs c  
as an antecedent to each rule that derives c 
we define an abstraction hierarchy with m levels to be a sequence of lightest derivation problems with additive rules  k   rk   for    k  m    with a single abstraction
function abs  for    k   m    the abstraction function maps k onto k     we require that  k     rk     be an abstraction of  k   rk   as defined in the previous section 
if a            an v c is in rk then there exists a rule abs a             abs an   v  abs c  in
rk   with v    v  the hierarchical algorithm computes lightest derivations of statements
in k using contexts from k   to define heuristic values  we extend abs so that it maps
m  to a most abstract set of statements m containing a single element   since abs is
onto we have  k     k      that is  the number of statements decrease as we go up the
abstraction hierarchy  we denote by abs k the abstraction function from   to k obtained
by composing abs with itself k times 
we are interested in computing a lightest derivation of a goal statement goal      let
goal k   abs k  goal   be the goal at each level of abstraction  the hierarchical algorithm is
defined by the set of prioritized rules h in figure    rules labeled up compute derivations
of statements at one level of abstraction using context weights from the level above to define
priorities  rules labeled base and down compute contexts in one level of abstraction
using derivation weights at the same level to define priorities  the rules labeled start 
and start  start the inference by handling the most abstract level 
the execution of h starts by computing a derivation and context for  with start 
and start   it continues by deriving statements in m  using up rules  once the
lightest derivation of goal m  is found the algorithm derives a context for goal m  with a
   

fithe generalized a  architecture

 

start  
  

start  

 
context      

base 

goal k   w
w
context goal k      

up 

context abs c     wc
a    w 
  
 
an   wn
v   w         wn   wc
c   v   w         wn

down 

context c    wc
a    w 
  
 
an   wn
v   wc   w         wn
context ai     v   wc   w         wn  wi

figure    prioritized rules h defining ha ld  base rules are defined for    k  m    
up and down rules are defined for each rule a            an v c  rk with
   k  m    

base rule and starts computing contexts for other statements in m  using down rules 
in general ha ld interleaves the computation of derivations and contexts at each level of
abstraction since the execution of h uses a single priority queue 
note that no computation happens at a given level of abstraction until a lightest derivation of the goal has been found at the level above  this means that the structure of the
abstraction hierarchy can be defined dynamically  for example  as in the cfdp algorithm 
we could define the set of statements at each level of abstraction by refining the statements
that appear in a lightest derivation of the goal at the level above  here we assume a static
abstraction hierarchy 
for each statement c  k with    k  m    we use   c  to denote the weight of
a lightest derivation for c using rk while   context c   denotes the weight of a lightest
context for c using rk   for the most abstract level we define         context        
   

fifelzenszwalb   mcallester

below we show that ha ld correctly computes lightest derivations and lightest contexts
at every level of abstraction  moreover  the order in which derivations and contexts are
expanded is controlled by a heuristic function defined as follows  for c  k with    k 
m    define a heuristic value for c using contexts at the level above and a heuristic value
for context c  using derivations at the same level 
h c      context abs c    
h context c       c  
for the most abstract level we define h     h context         let a generalized statement
 be either an element of k for    k  m or an expression of the form context c  for
c  k   we define an intrinsic priority for  as follows 
p           h   
for c  k   we have that p context c   is the weight of a lightest derivation of goal k that
uses c  while p c  is a lower bound on this weight 
the results from sections   and   cannot be used directly to show the correctness of
ha ld  this is because the rules in figure   generate heuristic values at the same time
they generate derivations that depend on those heuristic values  intuitively we must show
that during the execution of the prioritized rules h  each heuristic value is available at an
appropriate point in time  the next lemma shows that the rules in h satisfy a monotonicity
property with respect to the intrinsic priority of generalized statements  theorem   proves
the correctness of the hierarchical algorithm 
lemma    monotonicity   for each rule             m   in the hierarchical algorithm  if
the weight of each antecedent i is   i   and the weight of the conclusion is  then
 a  the priority of the rule is    h   
 b     h    p i   
proof  for the rules start  and start  the result follows from the fact that the rules
have no antecedents and h     h context        
consider a rule labeled base with w     goal k    to see  a  note that  is always zero
and the priority of the rule is w   h context goal k     for  b  we note that p goal k    
  goal k   which equals the priority of the rule 
now consider a rule labeled up with wc     context abs c    and wi     ai   for all i 
for part  a  note how the priority of the rule is    wc and h c    wc   for part  b  consider
the first antecedent of the rule  we have h context abs c        abs c      c     and
p context abs c      wc   h context abs c        wc   now consider an antecedent
ai   if abs ai      then p ai     wi     wc   if abs ai       then we can show that
h ai       context abs ai      wc     wi   this implies that p ai     wi   h ai       wc  
finally consider a rule labeled down with wc     context c   and wj     aj   for all
j  for part  a  note that the priority of the rule is    wi and h context ai      wi   for
ppart
 b  consider the first antecedent of the rule  we have h context c       c   v   j wj
and we see that p context c     wc   h c      wi   now consider an antecedent aj   if
abs aj      then h aj       and p aj     wj     wi   if abs aj       we can show that
h aj       wi  wj   hence p aj     wj   h aj       wi  
   

fithe generalized a  architecture

theorem    the execution of h maintains the following invariants 
   if     w   s then w       
   if     w   q then it has priority w   h   
   if p     p q  then           s
here p q  denotes the smallest priority in q 
proof  in the initial state of the algorithm s is empty and q contains only        and
 context        at priority    for the initial state invariant   is true since s is empty 
invariant   follows from the definition of h   and h context     and invariant   follows
from the fact that p q      and p      for all   let s and q denote the state of the
algorithm immediately prior to an iteration of the while loop in figure   and suppose the
invariants are true  let s   and q  denote the state of the algorithm after the iteration 
we will first prove invariant   for s     let     w  be the element removed from q in
this iteration  by the soundness of the rules we have w       if w       then clearly
invariant   holds for s     if w       invariant   implies that p q    w  h         h  
and by invariant   we know that s contains           in this case s     s 
invariant   for q  follows from invariant   for q  invariant   for s     and part  a  of the
monotonicity lemma 
finally  we consider invariant   for s   and q    the proof is by reverse induction on the
abstraction level of   we say that  has level k if   k or  is of the form context c 
with c  k   in the reverse induction  the base case considers  at level m  initially the
algorithm inserts        and  context        in the queue with priority    if p q       
then s   must contain        and  context         hence invariant   holds for s   and q 
with  at level m 
now we assume that invariant   holds for s   and q  with  at levels greater than k
and consider level k  we first consider statements c  k   since the rules rk are additive 
every statement c derivable with rk has a lightest derivation  a derivation with weight
  c    this follows from the correctness of knuths algorithm  moreover  for additive
rules  subtrees of lightest derivations are also lightest derivations  we show by structural
induction that for any lightest derivation with conclusion c such that p c    p q    we
have  c     c    s     consider a lightest derivation in rk with conclusion c such that
p c    p q     the final rule in this derivation a            an v c corresponds to an up rule
where we add an antecedent for context abs c    by part  b  of the monotonicity lemma
all the antecedents of this up rule have intrinsic priority less than p q     by the induction
hypothesis on lightest derivations we have  ai     ai     s     since invariant   holds for
statements at levels greater than k we have  context abs c       context abs c      s    
this implies that at some point the up rule was used to derive  c     c   at priority p c  
but p c    p q    and hence this item must have been removed from the queue  therefore
s   must contain  c   w  for some w and  by invariant    w     c  
now we consider  of the form context c  with c  k   as before we see that c rk   is
additive and thus every statement derivable with c rk   has a lightest derivation and subtrees
of lightest derivations are lightest derivations themselves  we prove by structural induction
   

fifelzenszwalb   mcallester

that for any lightest derivation t with conclusion context c  such that p context c    
p q    we have  context c      context c     s     suppose the last rule of t is of the form 
context c   a            ai    ai             an v context ai   
this rule corresponds to a down rule where we add an antecedent for ai   by part  b  of
the monotonicity lemma all the antecedents of this down rule have intrinsic priority less
than p q     by invariant   for statements in k and by the induction hypothesis on lightest
derivations using c rk    all antecedents of the down rule have their lightest weight in
s     so at some point  context ai       context ai     was derived at priority p ai    now
p ai     p q    implies the item was removed from the queue and  by invariant    we have
 context ai       context ai      s    
now suppose the last  and only  rule in t is   context goal k    this rule corresponds to a base rule where we add goal k as an antecedent  note that p goal k    
  goal k     p context goal k    and hence p goal k     p q     by invariant   for statements
in k we have  goal k     goal k    in s   and at some point the base rule was used to queue
 context goal k       context goal k     at priority p context goal k     as in the previous cases
p context goal k      p q    implies  context goal k       context goal k      s    
the last theorem implies that generalized statements  are expanded in order of their
intrinsic priority  let k be the number of statements c in the entire abstraction hierarchy
with p c   p goal       goal    for every statement c we have that p c   p context c   
we conclude that ha ld expands at most  k generalized statements before computing a
lightest derivation of goal  
    example
now we consider the execution of ha ld in a specific example  the example illustrates
how ha ld interleaves the computation of structures at different levels of abstraction 
consider the following abstraction hierarchy with   levels 
     x            xn   y            yn   z            zn   goal           x  y  z  goal     




  x 
i xi  

















y 

y
 


  
 i i
xi   yj ij goal    
x  y   goal    
  r   
 
r   








z 
 
y

z
 
x 
y

x




 







 i i   i
z   goal    
zi i goal    
with abs xi     x  abs yi     y   abs zi     z and abs goal      goal   
   initially s    and q           and  context        at priority    
   when        comes off the queue it gets put in s but nothing else happens 
   when  context        comes off the queue it gets put in s  now statements in  
have an abstract context in s  this causes up rules that come from rules in r  with
no antecedents to fire  putting  x      and  y      in q at priority   
   

fithe generalized a  architecture

   when  x      and  y      come off the queue they get put in s  causing two up
rules to fire  putting  goal        at priority   and  z      at priority   in the queue 
   we have 
s             context          x        y      
q     goal        at priority     z      at priority   
   at this point  goal        comes off the queue and goes into in s  a base rule fires
putting  context goal          in the queue at priority   
    context goal          comes off the queue  this is the base case for contexts in     two
down rules use  context goal            x      and  y      to put  context x      
and  context y        in q at priority   
    context x       comes off the queue and gets put in s  now we have an abstract
context for each xi      so up rules to put  xi   i  in q at priority i     
   now  context y        comes off the queue and goes into s  as in the previous step
up rules put  yi   i  in q at priority i     
    we have 
s             context          x        y        goal        
 context goal            context x         context y        
q     xi   i  and  yi   i  at priority i     for    i  n   z      at priority   
    next both  x       and  y       will come off the queue and go into s  this causes
an up rule to put  goal        in the queue at priority   
     goal        comes off the queue and goes into s  the algorithm can stop now since
we have a derivation of the most concrete goal 
note how ha ld terminates before fully computing abstract derivations and contexts 
in particular  z      is in q but z was never expanded  moreover context z  is not even
in the queue  if we keep running the algorithm it would eventually derive context z   and
that would allow the zi to be derived 

   the perception pipeline
figure   shows a hypothetical run of the hierarchical algorithm for a processing pipeline
of a vision system  in this system weighted statements about edges are used to derive
weighted statements about contours which provide input to later stages ultimately resulting
in statements about recognized objects 
it is well known that the subjective presence of edges at a particular image location can
depend on the context in which a given image patch appears  this can be interpreted in
the perception pipeline by stating that higher level processes  those later in the pipeline
 influence low level interpretations  this kind of influence happens naturally in a lightest
   

fifelzenszwalb   mcallester

m 

edges

contours

recognition

 

edges

contours

recognition

 

edges

contours

recognition

figure    a vision system with several levels of processing  forward arrows represent the
normal flow of information from one stage of processing to the next  backward
arrows represent the computation of contexts  downward arrows represent the
influence of contexts 

derivation problem  for example  the lightest derivation of a complete scene analysis might
require the presence of an edge that is not locally apparent  by implementing the whole
system as a single lightest derivation problem we avoid the need to make hard decisions
between stages of the pipeline 
the influence of late pipeline stages in guiding earlier stages is pronounced if we use
ha ld to compute lightest derivations  in this case the influence is apparent not only
in the structure of the optimal solution but also in the flow of information across different
stages of processing  in ha ld a complete interpretation derived at one level of abstraction
guides all processing stages at a more concrete level  structures derived at late stages of
the pipeline guide earlier stages through abstract context weights  this allows the early
processing stages to concentrate computational efforts in constructing structures that will
likely be part of the globally optimal solution 
while we have emphasized the use of admissible heuristics  we note that the a  architecture  including ha ld  can also be used with inadmissible heuristic functions  of course
this would break our optimality guarantees   inadmissible heuristics are important because
admissible heuristics tend to force the first few stages of a processing pipeline to generate
too many derivations  as derivations are composed their weights increase and this causes a
large number of derivations to be generated at the first few stages of processing before the
first derivation reaches the end of the pipeline  inadmissible heuristics can produce behavior
similar to beam search  derivations generated in the first stage of the pipeline can flow
through the whole pipeline quickly  a natural way to construct inadmissible heuristics is to
simply scale up an admissible heuristic such as the ones obtained from abstractions  it is
then possible to construct a hierarchical algorithm where inadmissible heuristics obtained
from one level of abstraction are used to guide search at the level below 

   other hierarchical methods
in this section we compare ha ld to other hierarchical search methods 
   

fithe generalized a  architecture

    coarse to fine dynamic programming
ha ld is related to the coarse to fine dynamic programming  cfdp  method described by
raphael         to understand the relationship consider the problem of finding the shortest
path from s to t in a trellis graph like the one shown in figure   a   here we have k columns
of n nodes and every node in one column is connected to a constant number of nodes in
the next column  standard dynamic programming can be used to find the shortest path
in o kn  time  both cfdp and ha ld can often find the shortest path much faster  on
the other hand the worst case behavior of these algorithms is very different as we describe
below  with cfdp taking significantly more time than ha ld 
the cfdp algorithm works by coarsening the graph  grouping nodes in each column
into a small number of supernodes as illustrated in figure   b   the weight of an edge
between two supernodes a and b is the minimum weight between nodes a  a and b  b 
the algorithm starts by using dynamic programming to find the shortest path p from s
to t in the coarse graph  this is shown in bold in figure   b   the supernodes along p
are partitioned to define a finer graph as shown in figure   c  and the procedure repeated 
eventually the shortest path p will only go through supernodes of size one  corresponding
to a path in the original graph  at this point we know that p must be a shortest path from
s to t in the original graph  in the best case the optimal path in each iteration will be a
refinement of the optimal path from the previous iteration  this would result in o log n 
shortest paths computations  each in fairly coarse graphs  on the other hand  in the worst
case cfdp will take  n  iterations to refine the whole graph  and many of the iterations
will involve finding shortest paths in large graphs  in this case cfdp takes  kn    time
which is much worst than the standard dynamic programming approach 
now suppose we use ha ld to find the shortest path from s to t in a graph like the
one in figure   a   we can build an abstraction hierarchy with o log n  levels where each
supernode at level i contains  i nodes from one column of the original graph  the coarse
graph in figure   b  represents the highest level of this abstraction hierarchy  note that
ha ld will consider a small number  o log n   of predefined graphs while cfdp can end
up considering a much larger number   n   of graphs  in the best case scenario ha ld
will expand only the nodes that are in the shortest path from s to t at each level of the
hierarchy  in the worst case ha ld will compute a lightest path and context for every
node in the hierarchy  here a context for a node v is a path from v to t   at the i th
abstraction level we have a graph with o kn  i   nodes and edges  ha ld will spend at
most o kn log kn   i   time computing paths and contexts at level i  summing over levels
we get at most o kn log kn   time total  which is not much worst than the o kn  time
taken by the standard dynamic programming approach 
    hierarchical heuristic search
our hierarchical method is also related to the ha  and hida  algorithms described by
holte et al         and holte et al          these methods are restricted to shortest paths
problems but they also use a hierarchy of abstractions  a heuristic function is defined for
each level of abstraction using shortest paths to the goal at the level above  the main
idea is to run a  or ida  to compute a shortest path while computing heuristic values ondemand  let abs map a node to its abstraction and let g be the goal node in the concrete
   

fifelzenszwalb   mcallester

s

t

 a 

s

t

s

 b 

t

 c 

figure     a  original dynamic programming graph   b  coarse graph with shortest path
shown in bold   c  refinement of the coarse graph along the shortest path 

graph  whenever the heuristic value for a concrete node v is needed we call the algorithm
recursively to find the shortest path from abs v  to abs g   this recursive call uses heuristic
values defined from a further abstraction  computed through deeper recursive calls 
it is not clear how to generalize ha  and hida  to lightest derivation problems that
have rules with multiple antecedents  another disadvantage is that these methods can
potentially stall in the case of directed graphs  for example  suppose that when using
ha  or hida  we expand a node with two successors x and y  where x is close to the goal
but y is very far  at this point we need a heuristic value for x and y  and we might have
to spend a long time computing a shortest path from abs y  to abs g   on the other hand 
ha ld would not wait for this shortest path to be fully computed  intuitively ha ld
would compute shortest paths from abs x  and abs y  to abs g  simultaneously  as soon
as the shortest path from abs x  to abs g  is found we can start exploring the path from x
to g  independent of how long it would take to compute a path from abs y  to abs g  
   

fithe generalized a  architecture

r 
r 

r 

r 
r 
r 
r 

r 

figure     a convex set specified by a hypothesis  r            r    

   convex object detection
now we consider an application of ha ld to the problem of detecting convex objects in
images  we pose the problem using a formulation similar to the one described by raphael
        where the optimal convex object around a point can be found by solving a shortest
path problem  we compare ha ld to other search methods  including cfdp and a 
with pattern databases  the results indicate that ha ld performs better than the other
methods over a wide range of inputs 
let x be a reference point inside a convex object  we can represent the object boundary
using polar coordinates with respect to a coordinate system centered at x  in this case the
object is described by a periodic function r   specifying the distance from x to the object
boundary as a function of the angle   here we only specify r   at a finite number of angles
             n     and assume the boundary is a straight line segment between sample points 
we also assume the object is contained in a ball of radius r around x and that r   is an
integer  thus an object is parametrized by  r            rn     where ri      r      an example
with n     angles is shown in figure    
not every hypothesis  r            rn     specifies a convex object  the hypothesis describes
a convex set exactly when the object boundary turns left at each sample point  i   ri   as
i increases  let c ri    ri   ri     be a boolean function indicating when three sequential
values for r   define a boundary that is locally convex at i  the hypothesis  r            rn    
is convex when it is locally convex at each i  
throughout this section we assume that the reference point x is fixed in advance  our
goal is to find an optimal convex object around a given reference point  in practice
reference locations can be found using a variety of methods such as a hough transform 
   this parametrization of convex objects is similar but not identical to the one used by raphael        

   

fifelzenszwalb   mcallester

let d i  ri   ri     be an image data cost measuring the evidence for a boundary segment
from  i   ri   to  i     ri      we consider the problem of finding a convex object for which
the sum of the data costs along the whole boundary is minimal  that is  we look for a
convex hypothesis minimizing the following energy function 
e r            rn      

n
 
x

d i  ri   ri     

i  

the data costs can be precomputed and specified by a lookup table with o n r    entries 
in our experiments we use a data cost based on the integral of the image gradient along
each boundary segment  another approach would be to use the data term described by
raphael        where the cost depends on the contrast between the inside and the outside
of the object measured within the pie slice defined by i and i    
an optimal convex object can be found using standard dynamic programming techniques  let b i  r    r    ri    ri   be the cost of an optimal partial convex object starting at
r  and r  and ending at ri  and ri   here we keep track of the last two boundary points to
enforce the convexity constraint as we extend partial objects  we also have to keep track
of the first two boundary points to enforce that rn   r  and the convexity constraint at r   
we can compute b using the recursive formula 
b    r    r    r    r      d    r    r    
b i      r    r    ri   ri       min b i  r    r    ri    ri     d i  ri   ri     
ri 

where the minimization is over choices for ri  such that c ri    ri   ri       true  the
cost of an optimal object is given by the minimum value of b n  r    r    rn     r    such that
c rn     r    r      true  an optimal object can be found by tracing back as in typical dynamic programming algorithms  the main problem with this approach is that the dynamic
programming table has o n r    entries and it takes o r  time to compute each entry  the
overall algorithm runs in o n r    time which is quite slow 
now we show how optimal convex objects can be defined in terms of a lightest derivation
problem  let convex  i  r    r    ri    ri   denote a partial convex object starting at r  and r 
and ending at ri  and ri   this corresponds to an entry in the dynamic programming table
described above  define the set of statements 
    convex  i  a  b  c  d    i      n    a  b  c  d      r        goal   
an optimal convex object corresponds to a lightest derivations of goal using the rules in
figure     the first set of rules specify the cost of a partial object from r  to r    the
second set of rules specify that an object ending at ri  and ri can be extended with a
choice for ri   such that the boundary is locally convex at ri   the last set of rules specify
that a complete convex object is a partial object from r  to rn such that rn   r  and the
boundary is locally convex at r   
to construct an abstraction hierarchy we define l nested partitions of the radius space
    r     into ranges of integers  in an abstract statement instead of specifying an integer
value for r   we will specify the range in which r   is contained  to simplify notation we
   

fithe generalized a  architecture

    for r    r       r     

convex     r    r    r    r      d    r    r   
    for r    r    ri    ri   ri        r     such that c ri    ri   ri       true 
convex  i  r    r    ri    ri     w
convex  i      r    r    ri   ri       w   d i  ri   ri    
    for r    r    rn        r     such that c rn     r    r      true 
convex  n  r    r    rn     r      w
goal   w
figure     rules for finding an optimal convex object 
assume that r is a power of two  the k th partition p k contains r  k ranges  each with
 k consecutive integers  the j th range in p k is given by  j   k    j        k     
the statements in the abstraction hierarchy are 
k    convex  i  a  b  c  d    i      n    a  b  c  d  p k     goal k   
for k      l      a range in p   contains a single integer so       let f map a range
in p k to the range in p k   containing it  for statements in level k   l    we define the
abstraction function 
abs convex  i  a  b  c  d     convex  i  f  a   f  b   f  c   f  d   
abs goal k     goal k    
the abstract rules use bounds on the data costs for boundary segments between  i   si  
and  i     si     where si and si   are ranges in p k  
dk  i  si   si      

min
d i  ri   ri     
ri  si
ri    si  

since each range in p k is the union of two ranges in p k  one entry in dk can be computed
quickly  in constant time  once dk  is computed  the bounds for all levels can be computed in o n r    time total  we also need abstract versions of the convexity constraints 
for si    si   si    p k   let c k  si    si   si       true if there exist integers ri    ri and ri  
in si    si and si   respectively such that c ri    ri   ri       true  the value of c k can be
defined in closed form and evaluated quickly using simple geometry 
the rules in the abstraction hierarchy are almost identical to the rules in figure    
the rules in level k are obtained from the original rules by simply replacing each instance
of     r     by p k   c by c k and d by dk  
   

fifelzenszwalb   mcallester

standard dp
cfdp
ha ld
a  with pattern database in  
a  with pattern database in  

       seconds
     seconds
    seconds
     seconds
     seconds

table    running time comparison for the example in figure    
    experimental results
figure    shows an example image with a set of reference locations that we selected manually
and the optimal convex object found around each reference point  there are    reference
locations and we used n      and r      to parametrize each object  table   compares the
running time of different optimization algorithms we implemented for this problem  each
line shows the time it took to solve all    problems contained in the example image using
a particular search algorithm  the standard dp algorithm uses the dynamic programming
solution outlined above  the cfdp method is based on the algorithm by raphael       
but modified for our representation of convex objects  our hierarchical a  algorithm uses
the abstraction hierarchy described here  for a  with pattern databases we used dynamic
programming to compute a pattern database at a particular level of abstraction  and then
used this database to provide heuristic values for a   note that for the problem described
here the pattern database depends on the input  the running times listed include the time
it took to compute the pattern database in each case 
we see that cfdp  ha ld and a  with pattern databases are much more efficient than
the standard dynamic programming algorithm that does not use abstractions  ha ld is
slightly faster then the other methods in this example  note that while the running time
varies from algorithm to algorithm the output of every method is the same as they all find
globally optimum objects 
for a quantitative evaluation of the different search algorithms we created a large set of
problems of varying difficulty and size as follows  for a given value of r we generated square
images of width and height    r      each image has a circle with radius less than r near
the center and the pixels in an image are corrupted by independent gaussian noise  the
difficulty of a problem is controlled by the standard deviation    of the noise  figure   
shows some example images and optimal convex object found around their centers 
the graph in figure    shows the running time  in seconds  of the different search
algorithms as a function of the noise level when the problem size is fixed at r       
each sample point indicates the average running time over     random inputs  the graph
shows running times up to a point after which the circles can not be reliably detected  we
compared ha ld with cfdp and a  using pattern databases  pd  and pd    here pd 
and pd  refer to a  with a pattern database defined in   and   respectively  since the
pattern database needs to be recomputed for each input there is a trade off in the amount
of time spent computing the database and the accuracy of the heuristic it provides  we
see that for easy problems it is better to use a smaller database  defined at a higher level
of abstraction  while for harder problems it is worth spending time computing a bigger
database  ha ld outperforms the other methods in every situation captured here 
   

fithe generalized a  architecture

 a 

 b 
figure      a  reference locations   b  optimal convex objects 

   

fifelzenszwalb   mcallester

figure     random images with circles and the optimal convex object around the center of
each one  with n      and r         the noise level in the images is       

figure    shows the running time of the different methods as a function of the problem
size r  on problems with a fixed noise level of         as before each sample point
indicates the average running time taken over     random inputs  we see that the running
time of the pattern database approach grows quickly as the problem size increases  this is
because computing the database at any fixed level of abstraction takes o n r    time  on
the other hand the running time of both cfdp and ha ld grows much slower  while
cfdp performed essentially as well as ha ld in this experiment  the graph in figure   
shows that ha ld performs better as the difficulty of the problem increases 

    finding salient curves in images
a classical problem in computer vision involves finding salient curves in images  intuitively
the goal is to find long and smooth curves that go along paths with high image gradient 
the standard way to pose the problem is to define a saliency score and search for curves
optimizing that score  most methods use a score defined by a simple combination of local
terms  for example  the score usually depends on the curvature and the image gradient at
each point of a curve  this type of score can often be optimized efficiently using dynamic
programming or shortest paths algorithms  montanari        shashua   ullman       
basri   alter        williams   jacobs        
here we consider a new compositional model for finding salient curves  an important
aspect of this model is that it can capture global shape constraints  in particular  it looks
for curves that are almost straight  something that can not be done using local constraints
alone  local constraints can enforce small curvature at each point of a curve  but this is
   

fithe generalized a  architecture

figure     running time of different search algorithms as a function of the noise level  in
the input  each sample point indicates the average running time taken over    
random inputs  in each case n      and r        see text for discussion 

not enough to prevent curves from turning and twisting around over long distances  the
problem of finding the most salient curve in an image with the compositional model defined
here can be solved using dynamic programming  but the approach is too slow for practical
use  shortest paths algorithms are not applicable because of the compositional nature of
the model  instead we can use a ld with a heuristic function derived from an abstraction
 a pattern database  
let c  be a curve with endpoints a and b and c  be a curve with endpoints b and c 
the two curves can be composed to form a curve c from a to c  we define the weight of the
composition to be the sum of the weights of c  and c  plus a shape cost that depends on
the geometric arrangement of points  a  b  c   figure    illustrates the idea and the shape
costs we use  note that when c  and c  are long  the arrangement of their endpoints reflect
non local geometric properties  in general we consider composing c  and c  if the angle
formed by ab and bc is at least    and the lengths of c  and c  are approximately equal 
these constraints reduce the total number of compositions and play an important role in
the abstract problem defined below 
besides the compositional rule we say that if a and b are nearby locations  then there is
a short curve with endpoints a and b  this forms a base case for creating longer curves  we
   

fifelzenszwalb   mcallester

figure     running time of different search algorithms as a function of the problem size r 
each sample point indicates the average running time taken over     random
inputs  in each case n      and         see text for discussion 

b
a

t

c

figure     a curve with endpoints  a  c  is formed by composing curves with endpoints
 a  b  and  b  c   we assume that t      the cost of the composition is
proportional to sin   t   this cost is scale invariant and encourages curves to be
relatively straight 

assume that these short curves are straight  and their weight depends only on the image
data along the line segment from a to b  we use a data term  seg a  b   that is zero if the
image gradient along pixels in ab is perpendicular to ab  and higher otherwise 
figure    gives a formal definition of the two rules in our model  the constants k  and
k  specify the minimum and maximum length of the base case curves  while l is a constant
   

fithe generalized a  architecture

    for pixels a  b  c where the angle between ab and bc is at least    and for    i  l 
curve a  b  i    w 
curve b  c  i    w 
curve a  c  i        w    w    shape a  b  c 
    for pixels a  b with k     a  b    k   

curve a  b       seg a  b 
figure     rules for finding almost straight curves between a pair of endpoints  here l 
k  and k  are constants  while shape a  b  c  is a function measuring the cost of
a composition 

controlling the maximum depth of derivations  a derivation of curve a  b  i  encodes a curve
from a to b  the value i can be seen as an approximate measure of arclength  a derivation
of curve a  b  i  is a full binary tree of depth i that encodes a curve with length between
 i k  and  i k    we let k     k  to allow for curves of any length 
the rules in figure    do not define a good measure of saliency by themselves because
they always prefer short curves over long ones  we can define the saliency of a curve in
terms of its weight minus its arclength  so that salient curves will be light and long  let 
be a positive constant  we consider finding the lightest derivation of goal using 
curve a  b  i    w
goal   w   i
for an n  n image there are  n    statements of the form curve a  c  i   moreover  if a
and c are far apart there are  n  choices for a midpoint b defining the two curves that
are composed in a lightest derivation of curve a  c  i   this makes a dynamic programming
solution to the lightest derivation problem impractical  we have tried using kld but even
for small images the algorithm runs out of memory after a few minutes  below we describe
an abstraction we have used to define a heuristic function for a ld 
consider a hierarchical set of partitions of an image into boxes  the i th partition is
defined by tiling the image into boxes of  i   i pixels  the partitions form a pyramid with
boxes of different sizes at each level  each box at level i is the union of   boxes at the level
below it  and the boxes at level   are the pixels themselves  let fi  a  be the box containing
a in the i th level of the pyramid  now define
abs curve a  b  i     curve fi  a   fi  b   i  
   

fifelzenszwalb   mcallester

a
b

a
b
d
c

d
c

figure     the abstraction maps each curve statement to a statement about curves between
boxes  if i   j then curve a  b  i  gets coarsened more than curve c  d  j   since
light curves are almost straight  i   j usually implies that   a  b       c  d   

figure    illustrates how this map selects a pyramid level for an abstract statement  intuitively abs defines an adaptive coarsening criteria  if a and b are far from each other  a
curve from a to b must be long  which in turn implies that we map a and b to boxes in a
coarse partition of the image  this creates an abstract problem that has a small number of
statements without losing too much information 
to define the abstract problem we also need to define a set of abstract rules  recall
that for every concrete rule r we need a corresponding abstract rule r  where the weight
of r  is at most the weight of r  there are a small number of rules with no antecedents in
figure     for each concrete rule seg a b  curve a  b     we define a corresponding abstract
rule  seg a b  abs curve a  b       the compositional rules from figure    lead to abstract
rules for composing curves between boxes 
curve a  b  i   curve b  c  i  v curve a    c     i      
where a  b and c are boxes at the i th pyramid level while a  and c   are the boxes at
level i     containing a and c respectively  the weight v should be at most shape a  b  c 
where a  b and c are arbitrary pixels in a  b and c respectively  we compute a value for
v by bounding the orientations of the line segments ab and bc between boxes 
   

fithe generalized a  architecture

         pixels  running time     seconds           

         pixels  running time     seconds           

         pixels  running time     seconds           
figure     the most salient curve in different images  the running time is the sum of
the time spent computing the pattern database and the time spent solving the
concrete problem 

   

fifelzenszwalb   mcallester

figure     an example where the most salient curve goes over locations with essentially no
local evidence for a the curve at those locations 

the abstract problem defined above is relatively small even in large images  so we can
use the pattern database approach outlined in section      for each input image we use
kld to compute lightest context weights for every abstract statement  we then use these
weights as heuristic values for solving the concrete problem with a ld  figure    illustrates
some of the results we obtained using this method  it seems like the abstract problem is
able to capture that most short curves can not be extended to a salient curve  it took
about one minute to find the most salient curve in each of these images  figure    lists the
dimensions of each image and the running time in each case 
note that our algorithm does not rely on an initial binary edge detection stage  instead
the base case rules allow for salient curves to go over any pixel  even if there is no local
evidence for a boundary at a particular location  figure    shows an example where this
happens  in this case there is a small part of the horse back that blends with the background
if we consider local properties alone 
the curve finding algorithm described in this section would be very difficult to formulate
without a ld and the general notion of heuristics derived from abstractions for lightest
derivation problems  however  using the framework introduced in this paper it becomes
relatively easy to specify the algorithm 
in the future we plan to compose the rules for computing salient curves with rules for
computing more complex structures  the basic idea of using a pyramid of boxes for defining
an abstract problem should be applicable to a variety of problems in computer vision 
   

fithe generalized a  architecture

    conclusion
although we have presented some preliminary results in the last two sections  we view the
main contribution of this paper as providing a general architecture for perceptual inference 
dijkstras shortest paths algorithm and a  search are both fundamental algorithms with
many applications  knuth noted the generalization of dijkstras algorithm to more general
problems defined by a set of recursive rules  in this paper we have given similar generalizations for a  search and heuristics derived from abstractions  we have also described
a new method for solving lightest derivation problems using a hierarchy of abstractions 
finally  we have outlined an approach for using these generalizations in the construction of
processing pipelines for perceptual inference 

acknowledgments
this material is based upon work supported by the national science foundation under
grant no          and         

references
basri  r     alter  t          extracting salient curves from images  an analysis of the
saliency network  in ieee conference on computer vision and pattern recognition 
bonet  b     geffner  h          an algorithm better than ao   in proceedings of the
national conference on artificial intelligence 
bulitko  v   sturtevant  n   lu  j     yau  t          state abstraction in real time heuristic
search  technical report  university of alberta  department of computer science 
charniak  e          statistical language learning  mit press 
culberson  j     schaeffer  j          pattern databases  computational intelligence         
       
dijkstra  e          a note on two problems in connection with graphs  numerical mathematics            
edelkamp  s          symbolic pattern databases in heuristic search panning  in international conference on ai planning and scheduling 
felner  a          finding optimal solutions to the graph partitioning problem with heuristic
search  annals of mathematics and artificial intelligence                   
geman  s   potter  d     chi  z          composition systems  quarterly of applied
mathematics         
hansen  e     zilberstein  s          lao   a heuristic search algorithm that finds solutions
with loops  artificial intelligence            
hart  p   nilsson  n     raphael  b          a formal basis for the heuristic determination
of minimal cost paths  ieee transactions on systems science and cybernetics        
       
holte  r   grajkowski  j     tanner  b          hierarchical heuristic search revisited  in
symposium on abstraction  reformulation and approximation 
   

fifelzenszwalb   mcallester

holte  r   perez  m   zimmer  r     macdonald  a          hierarchical a   searching abstraction hierarchies efficiently  in proceedings of the national conference on artificial
intelligence 
jimenez  p     torras  c          an efficient algorithm for searching implicit and or
graphs with cycles  artificial intelligence           
jin  y     geman  s          context and hierarchy in a probabilistic image model  in
ieee conference on computer vision and pattern recognition 
klein  d     manning  c          a  parsing  fast exact viterbi parse selection  in proceedings of the hlt naacl 
knuth  d          a generalization of dijkstras algorithm  information processing letters 
          
korf  r          finding optimal solutions to rubiks cube using pattern databases  in
proceedings of the national conference on artificial intelligence 
korf  r     felner  a          disjoint pattern database heuristics  artificial intelligence 
         
korf  r   zhang  w   thayer  i     hohwald  h          frontier search  journal of the
acm                 
mcallester  d          on the complexity analysis of static analyses  journal of the acm 
               
montanari  u          on the optimal detection of curves in noisy pictures  communications
of the acm         
nilsson  n          principles of artificial intelligence  morgan kaufmann 
pearl  j          heuristics  intelligent search strategies for computer problem solving 
addison wesley 
rabiner  l          a tutorial on hidden markov models and selected applications in speech
recognition  proceedings of the ieee                 
raphael  c          coarse to fine dynamic programming  ieee transactions on pattern
analysis and machine intelligence                    
shashua  a     ullman  s          structural saliency  the detection of globally salient
structures using a locally connected network  in ieee international conference on
computer vision 
tu  z   chen  x   yuille  a     zhu  s          image parsing  unifying segmentation 
detection  and recognition  international journal of computer vision             
    
williams  l     jacobs  d          local parallel computation of stochastic completion
fields  in ieee conference on computer vision and pattern recognition 
zhang  w     korf  r          a study of complexity transitions on the asymmetric traveling
salesman problem  artificial intelligence         

   

fi