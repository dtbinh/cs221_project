journal of artificial intelligence research                  

submitted        published      

combination strategies for semantic role labeling
mihai surdeanu
llus marquez
xavier carreras
pere r  comas

surdeanu lsi upc edu
lluism lsi upc edu
carreras lsi upc edu
pcomas lsi upc edu

technical university of catalonia 
c  jordi girona     
      barcelona  spain

abstract
this paper introduces and analyzes a battery of inference models for the problem of semantic role labeling  one based on constraint satisfaction  and several strategies that model
the inference as a meta learning problem using discriminative classiers  these classiers
are developed with a rich set of novel features that encode proposition and sentence level
information  to our knowledge  this is the rst work that   a  performs a thorough analysis of learning based inference models for semantic role labeling  and  b  compares several
inference strategies in this context  we evaluate the proposed inference strategies in the
framework of the conll      shared task using only automatically generated syntactic
information  the extensive experimental evaluation and analysis indicates that all the
proposed inference strategies are successful they all outperform the current best results
reported in the conll      evaluation exercise but each of the proposed approaches has
its advantages and disadvantages  several important traits of a state of the art srl combination strategy emerge from this analysis   i  individual models should be combined at the
granularity of candidate arguments rather than at the granularity of complete solutions 
 ii  the best combination strategy uses an inference model based in learning  and  iii  the
learning based inference benets from max margin classiers and global feedback 

   introduction
natural language understanding  nlu  is a subeld of articial intelligence  ai  that
deals with the extraction of the semantic information available in natural language texts 
this knowledge is used to develop high level applications requiring textual and document
understanding  such as question answering or information extraction  nlu is a complex
ai complete problem that needs to venture well beyond the syntactic analysis of natural
language texts  while the state of the art in nlu is still far from reaching its goals  recent
research has made important progress in a subtask of nlu  semantic role labeling  the
task of semantic role labeling  srl  is the process of detecting basic event structures such
as who did what to whom  when and where  see figure   for a sample sentence annotated
with such an event frame 
    motivation
srl has received considerable interest in the past few years  gildea   jurafsky       
surdeanu  harabagiu  williams    aarseth        xue   palmer        pradhan  hac
    
ai access foundation  all rights reserved 

fisurdeanu  marquez  carreras    comas

cioglu  krugler  ward  martin    jurafsky      a  carreras   marquez         it was
shown that the identication of such event frames has a signicant contribution for many
nlu applications such as information extraction  surdeanu et al          question answering  narayanan   harabagiu         machine translation  boas         summarization  melli  wang  liu  kashani  shi  gu  sarkar    popowich         and coreference
resolution  ponzetto   strube      b      a  
from a syntactic perspective  most machine learning srl approaches can be classied
in one of two classes  approaches that take advantage of complete syntactic analysis of text 
pioneered by gildea and jurafsky         and approaches that use partial syntactic analysis 
championed by previous evaluations performed within the conference on computational
natural language learning  conll   carreras   marquez               the wisdom
extracted from the rst representation indicates that full syntactic analysis has a signicant
contribution to srl performance  when using hand corrected syntactic information  gildea
  palmer         on the other hand  when only automatically generated syntax is available 
the quality of the information provided through full syntax decreases because the state ofthe art of full parsing is less robust and performs worse than the tools used for partial
syntactic analysis  under such real world conditions  the dierence between the two srl
approaches  with full or partial syntax  is not that high  more interestingly  the two srl
strategies perform better for different semantic roles  for example  models that use full
syntax recognize agent and theme roles better  whereas models based on partial syntax are
better at recognizing explicit patient roles  which tend to be farther from the predicate and
accumulate more parsing errors  marquez  comas  gimenez    catala        
    approach
in this article we explore the implications of the above observations by studying strategies
for combining the output of several independent srl systems  which take advantage of
dierent syntactic views of the text  in a given sentence  our combination models receive
labeled arguments from individual systems  and produce an overall argument structure
for the corresponding sentence  the proposed combination strategies exploit several levels
of information  local and global features  from individual models  and constraints on the
argument structure  in this work  we investigate three dierent approaches 
 the rst combination model has no parameters to estimate  it only makes use of the
argument probabilities output by the individual models and constraints over argument
structures to build the overall solution for each sentence  we call this model inference
with constraint satisfaction 
 the second approach implements a cascaded inference model with local learning  rst 
for each type of argument  a classier trained oine decides whether a candidate is
or is not a nal argument  next  the candidates that passed the previous step are
combined into a solution consistent with the constraints over argument structures 
we refer to this model as inference with local learning 
 the third inference model is global  a number of online ranking functions  one for
each argument type  are trained to score argument candidates so that the correct
argument structure for the complete sentence is globally ranked at the top  we call
this model inference with global learning 
   

ficombination strategies for semantic role labeling

s

np

np

vp
np

pp

the luxury auto maker last year sold       cars in the u s 
a 
agent

amtmp
temporal
marker

p

a 
predicate object

amloc
locative
marker

figure    sample sentence from the propbank corpus 
the proposed combination strategies are general and do not depend on the way in which
candidate arguments are collected  we empirically prove it by experimenting not only
with individual srl systems developed in house  but also with the    best systems at the
conll      shared task evaluation 
    contribution
the work introduced in this paper has several novel points  to our knowledge  this is
the rst work that thoroughly explores an inference model based on meta learning  the
second and third inference models introduced  in the context of srl  we investigate metalearning combination strategies based on rich  global representations in the form of local
and global features  and in the form of structural constraints of solutions  our empirical
analysis indicates that these combination strategies outperform the current state of the
art  note that all the combination strategies proposed in this paper are not re ranking
approaches  haghighi  toutanova    manning        collins         whereas re ranking
selects the overall best solution from a pool of complete solutions of the individual models 
our combination approaches combine candidate arguments  or incomplete solutions  from
different individual models  we show that our approach has better potential  i e   the upper
limit on the f  score is higher and performance is better on several corpora 
a second novelty of this paper is that it performs a comparative analysis of several combination strategies for srl  using the same framework i e   the same pool of
candidates and the same evaluation methodology  while a large number of combination
approaches have been previously analyzed in the context of srl or in the larger context of
predicting structures in natural language texts e g   inference based on constraint satisfaction  koomen  punyakanok  roth    yih        roth   yih         inference based in
local learning  marquez et al          re ranking  collins        haghighi et al         etc 
it is still not clear which strategy performs best for semantic role labeling  in this paper we
   

fisurdeanu  marquez  carreras    comas

provide empirical answers to several important questions in this respect  for example  is a
combination strategy based on constraint satisfaction better than an inference model based
on learning  or  how important is global feedback in the learning based inference model 
our analysis indicates that the following issues are important traits of a state of the art
combination srl system   i  the individual models are combined at argument granularity
rather than at the granularity of complete solutions  typical of re ranking    ii  the best
combination strategy uses an inference model based in learning  and  iii  the learning based
inference benets from max margin classiers and global feedback 
the paper is organized as follows  section   introduces the semantic corpora used for
training and evaluation  section   overviews the proposed combination approaches  the
individual srl models are introduced in section   and evaluated in section    section  
lists the features used by the three combination models introduced in this paper  the
combination models themselves are described in section    section   introduces an empirical analysis of the proposed combination methods  section   reviews related work and
section    concludes the paper 

   semantic corpora
in this paper we have used propbank  an approximately one million word corpus annotated
with predicate argument structures  palmer  gildea    kingsbury         to date  propbank addresses only predicates lexicalized by verbs  besides predicate argument structures 
propbank contains full syntactic analysis of its sentences  because it extends the wall street
journal  wsj  part of the penn treebank  a corpus that was previously annotated with
syntactic information  marcus  santorini    marcinkiewicz        
for any given predicate  a survey was carried out to determine the predicate usage  and 
if required  the usages were divided into major senses  however  the senses are divided
more on syntactic grounds than semantic  following the assumption that syntactic frames
are a direct reection of underlying semantics  the arguments of each predicate are numbered sequentially from a  to a   generally  a  stands for agent  a  for theme or direct
object  and a  for indirect object  benefactive or instrument  but semantics tend to be verb
specic  additionally  predicates might have adjunctive arguments  referred to as ams  for
example  am loc indicates a locative and am tmp indicates a temporal  figure   shows a
sample propbank sentence where one predicate  sold  has   arguments  both regular and
adjunctive arguments can be discontinuous  in which case the trailing argument fragments
are prexed by c   e g    a  both funds  are  predicate expected   ca  to begin operation
around march     finally  propbank contains argument references  typically pronominal  
which share the same label with the actual argument prexed with r   
in this paper we do not use any syntactic information from the penn treebank  instead 
we develop our models using automatically generated syntax and named entity  ne  labels 
made available by the conll      shared task evaluation  carreras   marquez        
from the conll data  we use the syntactic trees generated by the charniak parser  char   in the original propbank annotations  co referenced arguments appear as a single item  with no differentiation between the referent and the reference  here we use the version of the data used in the conll
shared tasks  where reference arguments were automatically separated from their corresponding referents
with simple pattern matching rules 

   

ficombination strategies for semantic role labeling

niak        to develop two individual models based on full syntactic analysis  and the chunk
i e   basic syntactic phrase labels and clause boundaries to construct a partial syntax
model  all individual models use the provided ne labels 
switching from hand corrected to automatically generated syntactic information means
that the propbank assumption that each argument  or argument fragment for discontinuous arguments  maps to one syntactic phrase no longer holds  due to errors of the syntactic
processors  our analysis of the propbank data indicates that only        of the semantic
arguments can be matched to exactly one phrase generated by the charniak parser  essentially  this means that srl approaches that make the assumption that each semantic
argument maps to one syntactic construct can not recognize almost    of the arguments 
the same statement can be made about approaches based on partial syntax with the caveat
that in this setup arguments have to match a sequence of chunks  however  one expects
that the degree of compatibility between syntactic chunks and semantic arguments is higher
due to the ner granularity of the syntactic elements and because chunking algorithms perform better than full parsing algorithms  indeed  our analysis of the same propbank data
supports this observation         of the semantic arguments can be matched to a sequence
of chunks generated by the conll syntactic chunker 
following the conll      setting we evaluated our system not only on propbank but
also on a fresh test set  derived from the brown corpus  this second evaluation allows us
to investigate the robustness of the proposed combination models 

   overview of the combination strategies
in this paper we introduce and analyze three combination strategies for the problem of
semantic role labeling  the three combination strategies are implemented on a shared
framework detailed in figure   which consists of several stages   a  generation of candidate arguments   b  candidate scoring  and nally  c  inference  for clarity  we describe
rst the proposed combination framework  i e   the vertical ow in figure    then  we move
to an overview of the three combination methodologies  shown horizontally in figure   
in the candidate generation step  we merge the solutions of three individual srl models
into a unique pool of candidate arguments  the individual srl models range from complete
reliance on full parsing to using only partial syntactic information  for example  model  
is developed as a sequential tagger  using the b i o tagging scheme  with only partial
syntactic information  basic phrases and clause boundaries   whereas model   uses full
syntactic analysis of the text and handles only arguments that map into exactly one syntactic
constituent  we detail the individual srl models in section   and empirically evaluate them
in section   
in the candidate scoring phrase  we re score all candidate arguments using both local
information  e g   the syntactic structure of the candidate argument  and global information 
e g   how many individual models have generated similar candidate arguments  we describe
all the features used for candidate scoring in section   
finally  in the inference stage the combination models search for the best solution that
is consistent with the domain constraints  e g   two arguments for the same predicate cannot
overlap or embed  a predicate may not have more than one core argument  a      etc 
   

fisurdeanu  marquez  carreras    comas

reliance on full syntax

model  

model  

model  

candidate
generation
candidate argument
pool

constraint
satisfaction
engine

solution

inference with
constraint satisfaction

learning
 batch 

learning
 online 

dynamic
programming
engine

dynamic
programming
engine

solution

candidate
scoring

inference

solution

inference with
local learning

inference with
global learning

figure    overview of the proposed combination strategies 

all the combination approaches proposed in this paper share the same candidate argument pool  this guarantees that the results obtained by the dierent strategies on the
same corpus are comparable  on the other hand  even though the candidate generation
step is shared  the three combination methodologies dier signicantly in their scoring and
inference models 
the rst combination strategy analyzed  inference with constraint satisfaction  skips the
candidate scoring step completely and uses instead the probabilities output by the individual srl models for each candidate argument  if the individual models raw activations are
not actual probabilities we convert them to probabilities using the softmax function  bishop 
       before passing them to the inference component  the inference is implemented using
a constraint satisfaction model that searches for the solution that maximizes a certain
compatibility function  the compatibility function models not only the probability of the
global solution but also the consistency of the solution according to the domain constraints 
this combination strategy is based on the technique presented by koomen et al         
the main dierence between the two systems is in the candidate generation step  we use
three independent individual srl models  whereas komen et al  used the same srl model
   

ficombination strategies for semantic role labeling

trained on dierent syntactic views of the data  i e   the top parse trees generated by the
charniak and collins parsers  charniak        collins         furthermore  we take our
argument candidates from the set of complete solutions generated by the individual models  whereas komen et al  take them from dierent syntactic trees  before constructing any
complete solution  the obvious advantage of the inference model with constraint satisfaction is that it is unsupervised  no learning is necessary for candidate scoring  because the
scores of the individual models are used  on the other hand  the constraint satisfaction
model requires that the individual models provide raw activations  and  moreover  that the
raw activations be convertible to true probabilities 
the second combination strategy proposed in this article  inference with local learning 
re scores all candidates in the pool using a set of binary discriminative classiers  the
classiers assign to each argument a score measuring the condence that the argument
is part of the correct  global solution  the classiers are trained in batch mode and are
completely decoupled from the inference module  the inference component is implemented
using a cky based dynamic programming algorithm  younger         the main advantage
of this strategy is that candidates are re scored using signicantly more information than
what is available to each individual model  for example  we incorporate features that count
the number of individual systems that generated the given candidate argument  several
types of overlaps with candidate arguments of the same predicate and also with arguments
of other predicates  structural information based on both full and partial syntax  etc  we
describe the rich feature set used for the scoring of candidate arguments in section    also 
this combination approach does not depend on the argument probabilities of the individual
srl models  but can incorporate them as features  if available   this combination approach
is more complex than the previous strategy because it has an additional step that requires
supervised learning  candidate scoring  nevertheless  this does not mean that additional
corpus is necessary  using cross validation  the candidate scoring classiers can be trained
on the same corpus used to train the individual srl models  moreover  we show in section  
that we obtain excellent performance even when the candidate scoring classiers are trained
on signicantly less data than the individual srl models 
finally  the inference strategy with global learning investigates the contribution of global
information to the inference model based on learning  this strategy incorporates global
information in the previous inference model in two ways  first and most importantly  candidate scoring is now trained online with global feedback from the inference component  in
other words  the online learning algorithm corrects the mistakes found when comparing the
correct solution with the one generated after inference  second  we integrate global information in the actual inference component  instead of performing inference for each proposition
independently  we now do it for the whole sentence at once  this allows implementation of
additional global domain constraints  e g   arguments attached to dierent predicates can
not overlap 
all the combination strategies proposed are described in detail in section   and evaluated
in section   
   

fisurdeanu  marquez  carreras    comas

   individual srl models
this section introduces the three individual srl models used by all the combination strategies discussed in this paper  the rst two models are variations of the same algorithm  they
both model the srl problem as a sequential tagging task  where each semantic argument
is matched to a sequence of non embedding phrases  but model   uses only partial syntax
 chunks and clause boundaries   whereas model   uses full syntax  the third model takes a
more traditional approach by assuming that there exists a one to one mapping between
semantic arguments and syntactic phrases 
it is important to note that all the combination strategies introduced later in the paper
are independent of the individual srl models used  in fact  in section   we describe
experiments that use not only these individual models but also the best performing srl
systems at the conll      evaluation  carreras   marquez         nevertheless  we
choose to focus mainly on the individual srl approaches presented in this section for
completeness and to show that state of the art performance is possible with relatively simple
srl models 
    models   and  
these models approach srl as a sequential tagging task  in a pre processing step  the
input syntactic structures are traversed in order to select a subset of constituents organized
sequentially  i e   non embedding   the output of this process is a sequential tokenization of
the input sentence for each of the verb predicates  labeling these tokens with appropriate
tags allows us to codify the complete argument structure of each predicate in the sentence 
more precisely  given a verb predicate  the sequential tokens are selected as follows 
first  the input sentence is split into disjoint sequential segments using as markers for
segment start end the verb position and the boundaries of all the clauses that include
the corresponding predicate constituent  second  for each segment  the set of top most
non overlapping syntactic constituents completely falling inside the segment are selected
as tokens  finally  these tokens are labeled with b i o tags  depending if they are at the
beginning  inside  or outside of a predicate argument  note that this strategy provides a set
of sequential tokens covering the complete sentence  also  it is independent of the syntactic
annotation explored  assuming it provides clause boundaries 
consider the example in figure    which depicts the propbank annotation of two verb
predicates of a sentence  release and hope  and the corresponding partial and full parse
trees  since both verbs are in the main clause of the sentence  only two segments of the
sentence are considered for both predicates  i e   those dening the left and right contexts
of the verbs   w   others       w   just  and  w   from       w    big time  for predicate release 
and  w   others       w      and  w    the       w    big time  for the predicate hope   figure  
shows the resulting tokenization for both predicates and the two alternative syntactic structures  in this case  the correct argument annotation can be recovered in all cases  assuming
perfect labeling of the tokens 
it is worth noting that the resulting number of tokens to annotate is much lower than
the number of words in all cases  also  the codications coming from full parsing have
substantially fewer tokens than those coming from partial parsing  for example  for the
predicate hope  the dierence in number of tokens between the two syntactic views is
   

ficombination strategies for semantic role labeling

clause
np

i

 

vp
clause

vp
 

np

 

advp

 

ii

np

pp

vp

 

 

 

 

others   just

released from the majors   hope the senior league

will be their bridge back into the bigtime 

clause
 

 

np

i

 

advp

    ii
others   just

a 

amtmp

iii

vp

iv  

pp

v

 

np

vi

 

vp

np

 
  vii
released from the majors   hope the senior league
p
a 

clause

vp

np

viii

advp pp

np

will be their bridge back into the bigtime 

a 
p

a 

figure    annotation of an example sentence with two alternative syntactic structures  the
lower tree corresponds to a partial parsing annotation  pp  with base chunks and
clause structure  while the upper represents a full parse tree  fp   semantic roles
for two predicates  release and hope  are also provided for the sentence  the
encircled nodes in both trees correspond to the selected nodes by the process
of sequential tokenization of the sentence  we mark the selected nodes for the
predicate release with western numerals and the nodes selected for hope with
roman numerals  see figure   for more details 

particularly large    vs    tokens   obviously  the coarser the token granularity  the easier
the problem of assigning correct output labelings  i e   there are less tokens to label and
also the long distance relations among sentence constituents can be better captured   on
the other hand  a coarser granularity tends to introduce more unrecoverable errors in the
pre processing stage  there is a clear trade o  which is dicult to solve in advance  by
using the two models in a combination scheme we can take advantage of the diverse sentence
tokenizations  see sections   and    
compared to the more common tree node labeling approaches  e g   the following
model     the b i o annotation of tokens has the advantage of permitting to correctly annotate some arguments that do not match a unique syntactic constituent  on the bad side 
the heuristic pre selection of only some candidate nodes for each predicate  i e   the nodes
that sequentially cover the sentence  makes the number of unrecoverable errors higher  another source of errors common to all strategies are the errors introduced by real partial full
parsers  we have calculated that due to syntactic errors introduced in the pre processing
stage  the upper bound recall gures are        for model   and        for model   using
the datasets dened in section   
   

fisurdeanu  marquez  carreras    comas

words
   others
    
   just
   released
   from
   the
   majors
    
   hope
    the
    senior
    league
    will
    be
    their
    bridge
    back
    into
    the
    big time

releasepp
   b a 
   o
   b am tmp

   b a 
   i a 
   o
   o

tokens
releasefp
hopepp
   b a 
i  b a 
   o
ii  i a 
   b am tmp
iii  i a 

iv  i a 
v  i a 
   b a 
vi  i a 
   o

hopefp

i  b a 

vii  i a 




viii  b a 

ii  b a 

   o
   o

figure    sequential tokenization of the sentence in figure   according to the two syntactic
views and predicates  pp stands for partial parsing and fp for full parsing   the
sentence and semantic role annotations are vertically displayed  each token is
numbered with the indexes that appear in the tree nodes of figure   and contains
the b i o annotation needed to codify the proper semantic role structure 

approaching srl as a sequential tagging task is not new  hacioglu  pradhan  ward 
martin  and jurafsky        presented a system based on sequential tagging of base chunks
with b i o labels  which was the best performing srl system at the conll      shared
task  carreras   marquez         the novelty of our approach resides in the fact that the
sequence of syntactic tokens to label is extracted from a hierarchical syntactic annotation
 either a partial or a full parse tree  and it is not restricted to base chunks  i e   a token
may correspond to a complex syntactic phrase or even a clause  
      features
once the tokens selected are labeled with b i o tags  they are converted into training
examples by considering a rich set of features  mainly borrowed from state of the art systems  gildea   jurafsky        carreras  marquez    chrupala        xue   palmer 
       these features codify properties from   a  the focus token   b  the target predicate 
 c  the sentence fragment between the token and predicate  and  d  the dynamic context 
i e   b i o labels previously generated  we describe these four feature sets next  
   features extracted from partial parsing and named entities are common to model   and    while features
coming from full parse trees only apply to model   

   

ficombination strategies for semantic role labeling

constituent structure features 
 constituent type and head  extracted using the head word rules of collins        
if the rst element is a pp chunk  then the head of the rst np is extracted  for
example  the type of the constituent in the u s  in figure   is pp  but its head is
u s  instead of in 
 first and last words and pos tags of the constituent  e g   in in and u s  nnp
for the constituent in the u s  in figure   
 pos sequence  if it is less than   tags long  e g   indtnnp for the above sample
constituent 
       grams of the pos sequence 
 bag of words of nouns  adjectives  and adverbs  for example  the bag of nouns for
the constituent the luxury auto maker is  luxury  auto  maker  
 top sequence  sequence of types of the top most syntactic elements in the constituent
 if it is less than   elements long   in the case of full parsing this corresponds to the
right hand side of the rule expanding the constituent node  for example  the top
sequence for the constituent in the u s  is innp 
       grams of the top sequence 
 governing category as described by gildea and jurafsky         which indicates if np
arguments are dominated by a sentence  typical for subjects  or a verb phrase  typical
for objects   for example  the governing category for the constituent       cars in
figure   is vp  which hints that its corresponding semantic role will be object 
 namedentity  indicating if the constituent embeds or strictly matches a named entity
along with its type  for example  the constituent in the u s  embeds a locative
named entity  u s  
 tmp  indicating if the constituent embeds or strictly matches a temporal keyword
 automatically extracted from am tmp arguments of the training set   among the most
common temporal cue words extracted are  year  yesterday  week  month 
etc  we used a total of     cue words 
 previous and following words and pos tag of the constituent  for example  the
previous word for the constituent last year in figure   is maker nn  and the next
one is sold vbd 
 the same features characterizing focus constituents are extracted for the two previous
and following tokens  provided they are inside the boundaries of the current segment 
predicate structure features 
 predicate form  lemma  and pos tag  e g   sold  sell  and vbd for the predicate in
figure   
 chunk type and cardinality of verb phrase in which verb is included  single word or
multi word  for example  the predicate in figure   is included in a single word vp
chunk 
   

fisurdeanu  marquez  carreras    comas

 the predicate voice  we distinguish ve voice types  active  passive  copulative 
innitive  and progressive 
 binary ag indicating if the verb is a start end of a clause 
 sub categorization rule  i e   the phrase structure rule that expands the predicates
immediate parent  e g   s  np np vp for the predicate in figure   
predicate constituent features 
 relative position  distance in words and chunks  and level of embedding  in number of
clause levels  with respect to the constituent  for example  the constituent in the
u s  in figure   appears after the predicate  at a distance of   words or   chunk 
and its level of embedding is   
 constituent path as described by gildea and jurafsky        and all       grams of
path constituents beginning at the verb predicate or ending at the constituent  for
example  the syntactic path between the constituent the luxury auto maker and
the predicate sold in figure   is np  s  vp  vbd 
 partial parsing path as described by carreras et al         and all       grams of path
elements beginning at the verb predicate or ending at the constituent  for example 
the path np   pp   np   s  vp  vbd indicates that from the current np token
to the predicate there are pp  np  and s constituents to the right  positive sign  at
the same level of the token and then the path descends through the clause and a vp
to nd the predicate  the dierence from the previous constituent path is that we do
not have up arrows anymore but we introduce horizontal  left right  movements at
the same syntactic level 
 syntactic frame as described by xue and palmer         the syntactic frame captures
the overall sentence structure using the predicate and the constituent as pivots  for
example  the syntactic frame for the predicate sold and the constituent in the
u s  is npnpvpnppp  with the current predicate and constituent emphasized 
knowing that there are other noun phrases before the predicate lowers the probability
that this constituent serves as an agent  or a   
dynamic features 
 biotag of the previous token  when training  the correct labels of the left context
are used  when testing  this feature is dynamically codied as the tag previously
assigned by the srl tagger 
      learning algorithm and sequence tagging
we used generalized adaboost with real valued weak classiers  schapire   singer        as
the base learning algorithm  our version of the algorithm learns xed depth small decision
trees as weak rules  which are then combined in the ensemble constructed by adaboost 
we implemented a simple one vs all decomposition to address multi class classication  in
this way  a separate binary classier has to be learned for each b x and i x argument label
plus an extra classier for the o decision 
   

ficombination strategies for semantic role labeling

adaboost binary classiers are then used for labeling test sequences  from left to right 
using a recurrent sliding window approach with information about the tags assigned to the
preceding tokens  as explained in the previous list of features  left tags already assigned
are dynamically codied as features  empirically  we found that the optimal left context to
be taken into account reduces to only the previous token 
we tested two dierent tagging procedures  first  a greedy left to right assignment of
the best scored label for each token  second  a viterbi search of the label sequence that
maximizes the probability of the complete sequence  in this case  the classiers predictions
were converted into probabilities using the softmax function described in section      no
signicant improvements were obtained from the latter  we selected the former  which is
faster  as our basic tagging algorithm for the experiments 
finally  this tagging model enforces three basic constraints   a  the b i o output labeling must codify a correct structure   b  arguments cannot overlap with clause nor chunk
boundaries  and  c  for each verb  a    arguments not present in propbank frames  taking
the union of all rolesets for the dierent verb senses  are not considered 
    model  
the third individual srl model makes the strong assumption that each predicate argument
maps to one syntactic constituent  for example  in figure   a  maps to a noun phrase 
am loc maps to a prepositional phrase  etc  this assumption holds well on hand corrected
parse trees and simplies signicantly the srl process because only one syntactic constituent has to be correctly classied in order to recognize one semantic argument  on the
other hand  this approach is limited when using automatically generated syntactic trees  for
example  only        of the arguments can be mapped to one of the syntactic constituents
produced by the charniak parser 
using a bottom up approach  model   maps each argument to the rst syntactic constituent that has the exact same boundaries and then climbs as high as possible in the
tree across unary production chains  we currently ignore all arguments that do not map
to a single syntactic constituent  the argument constituent mapping is performed on the
training set as preprocessing step  figure   shows a mapping example between the semantic
arguments of one verb and the corresponding sentence syntactic structure 
once the mapping process completes  model   extracts a rich set of lexical  syntactic 
and semantic features  most of these features are inspired from previous work in parsing
and srl  collins        gildea   jurafsky        surdeanu et al         pradhan et al  
    a   we describe the complete feature set implemented in model   next 
      features
similarly to models   and   we group the features in three categories  based on the properties
they codify   a  the argument constituent   b  the target predicate  and  c  the relation
between the constituent and predicate syntactic constituents 
constituent structure features 
 the syntactic label of the candidate constituent 

 the constituent head word  suffixes of length       and    lemma  and pos tag 
   

fisurdeanu  marquez  carreras    comas

 the constituent content word  suffixes of length       and    lemma  pos tag  and ne
label  content words  which add informative lexicalized information dierent from the
head word  were detected using the heuristics of surdeanu et al          for example 
the head word of the verb phrase had placed is the auxiliary verb had  whereas
the content word is placed  similarly  the content word of prepositional phrases is
not the preposition itself  which is selected as the head word   but rather the head
word of the attached phrase  e g   u s  for the prepositional phrase in the u s  
 the first and last constituent words and their pos tags 
 ne labels included in the candidate phrase 
 binary features to indicate the presence of temporal cue words  i e   words that appear
often in am tmp phrases in training  we used the same list of temporal cue words as
models   and   
 for each treebank syntactic label we added a feature to indicate the number of such
labels included in the candidate phrase 
 the top sequence of the constituent  constructed similarly to model    
 the phrase label  head word and pos tag of the constituent parent  left sibling  and
right sibling 
predicate structure features 
 the predicate word and lemma 
 the predicate voice  same denition as models   and   
 a binary feature to indicate if the predicate is frequent  i e   it appears more than
twice in the training data  or not 
 sub categorization rule  same denition as models   and   
predicate constituent features 
 the path in the syntactic tree between the argument phrase and the predicate as
a chain of syntactic labels along with the traversal direction  up or down   it is
computed similarly to model   
 the length of the above syntactic path 
 the number of clauses  s  phrases  in the path  we store the overall clause count
and also the number of clauses in the ascending and descending part of the path 
 the number of verb phrases  vp  in the path  similarly to the above feature  we store
three numbers  overall verb count  and the verb count in the ascending descending
part of the path 
 generalized syntactic paths  we generalize the path in the syntactic tree  when it
appears with more than   elements  using two templates   a  arg  ancestor  ni 
pred  where arg is the argument label  pred is the predicate label  ancestor is the
label of the common ancestor  and ni is instantiated with each of the labels between
   

ficombination strategies for semantic role labeling

pred and ancestor in the full path  and  b  arg  ni  ancestor  pred  where ni is
instantiated with each of the labels between arg and ancestor in the full path  for
example  in the path np  s  vp  sbar  s  vp the argument label is the rst np  the
predicate label is the last vp  and the common ancestors label is the rst s  hence 
using the last template  this path is generalized to the following three features  np 
s  vp  vp  np  s  sbar  vp  and np  s  s  vp  this generalization reduces the
sparsity of the complete constituent predicate path feature using a dierent strategy
than models   and    which implement a n gram based approach 
 the subsumption count  i e   the dierence between the depths in the syntactic tree
of the argument and predicate constituents  this value is   if the two phrases share
the same parent 
 the governing category  similar to models   and   

 the surface distance between the predicate and the argument phrases encoded as 
the number of tokens  verb terminals  vb    commas  and coordinations  cc  between the argument and predicate phrases  and a binary feature to indicate if the two
constituents are adjacent  for example  the surface distance between the argument
candidate others and the predicate hope in the figure   example  others  just
released from the majors  hope the senior league    is   tokens    verb    commas 
and   coordinations  these features  originally proposed by collins        for his dependency parsing model  capture robust  syntax independent information about the
sentence structure  for example  a constituent is unlikely to be the argument of a
verb if another verb appears between the two phrases 

 a binary feature to indicate if the argument starts with a predicate particle  i e   a
token seen with the rp  pos tag and directly attached to the predicate in training 
the motivation for this feature is to avoid the inclusion of predicate particles in the
argument constituent  for example  without this feature  a srl system will tend to
incorrectly include the predicate particle in the argument for the text  take  a  over
the organization   because the marked text is commonly incorrectly parsed as a
prepositional phrase and a large number of prepositional phrases directly attached to
a verb are arguments for the corresponding predicate 
      classifier
similarly to models   and    model   trains one vs all classiers using adaboost for the most
common argument labels  to reduce the sample space  model   selects training examples
 both positive and negative  only from   a  the rst clause that includes the predicate  or
 b  from phrases that appear to the left of the predicate in the sentence  more than    
of the argument constituents fall into one of these classes 
at prediction time the classiers are combined using a simple greedy technique that
iteratively assigns to each predicate the argument classied with the highest condence  for
each predicate we consider as candidates all am attributes  but only numbered attributes
indicated in the corresponding propbank frame  additionally  this greedy strategy enforces
a limited number of domain knowledge constraints in the generated solution   a  arguments
can not overlap in any form   b  no duplicate arguments are allowed for a     and  c  each
   

fisurdeanu  marquez  carreras    comas

predicate can have numbered arguments  i e   a     only from the subset present in its
propbank frame  these constraints are somewhat dierent from the constraints used by
models   and     i  model   does not use the b i o representation hence the constraint
that the b i o labeling be correct does not apply  and  ii  models   and   do not enforce
the constraint that numbered arguments can not be duplicated because its implementation
is not straightforward in this architecture 

   performance of the individual models
in this section we analyze the performance of the three individual srl models proposed 
our three srl systems were trained using the complete conll      training set  propbank treebank sections   to      to avoid the overtting of the syntactic processors i e  
part of speech tagger  chunker  and charniaks full parser we partitioned the propbank
training set into ve folds and for each fold we used the output of the syntactic processors
that were trained on the other four folds  the models were tuned on a separate development partition  treebank section     and evaluated on two corpora   a  treebank section
    which consists of wall street journal  wsj  documents  and  b  on three sections of the
brown corpus  semantically annotated by the propbank team for the conll      shared
task evaluation 
all the classiers for our individual models were developed using adaboost with decision trees of depth    i e   each branch may represent a conjunction of at most   basic
features   each classication model was trained for up to       rounds  we applied some
simplications to keep training times and memory requirements inside admissible bounds 
 a  we have trained only the most frequent argument labels  top    for model    top   
for model    and top    for model     b  we discarded all features occurring less than   
times in the training set  and  c  for each model   classier  we have limited the number of
negative training samples to the rst         negative samples extracted in the propbank
traversal   
table   summarizes the results of the three models on the wsj and brown corpora 
we include the percentage of perfect propositions detected by each model  pprops  
i e   predicates recognized with all their arguments  the overall precision  recall  and f 
measure    the results summarized in table   indicate that all individual systems have
a solid performance  although none of them would rank in the top   in the conll     evaluation  carreras   marquez         their performance is comparable to the best
individual systems presented at that evaluation exercise    consistently with other systems
evaluated on the brown corpus  all our models experience a severe performance drop in this
corpus  due to the lower performance of the linguistic processors 
as expected  the models based on full parsing    and    perform better than the model
based on partial syntax  but  interestingly  the dierence is not large  e g   less than   points
   the distribution of samples for the model   classifiers is very biased towards negative samples because  in
the worst case  any syntactic constituent in the same sentence with the predicate is a potential argument 
   the significance intervals for the f  measure have been obtained using bootstrap resampling  noreen 
       f  rates outside of these intervals are assumed to be significantly different from the related f 
rate  p         
   the best performing srl systems at conll were a combination of several subsystems  see section  
for details 

   

ficombination strategies for semantic role labeling

wsj
model  
model  
model  
brown
model  
model  
model  

pprops
      
      
      

precision
      
      
      

recall
      
      
      

f 
         
         
         

      
      
      

      
      
      

      
      
      

         
         
         

table    overall results of the individual models in the wsj and brown test sets 
model   f 
model   f 
model   f 

a 
     
     
     

a 
     
     
     

a 
     
     
     

a 
     
     
     

a 
     
     
     

table    f  scores of the individual systems for the a   arguments in the wsj test 
in f  in the wsj corpus   evincing that having base syntactic chunks and clause boundaries
is enough to obtain competitive performance  more importantly  the full parsing models are
not always better than the partial syntax model  table   lists the f  measure for the three
models for the rst ve numbered arguments  table   shows that model    our overall
best performing individual system  achieves the best f measure for a  and a   typically
subjects and direct objects   but model    the partial syntax model  performs best for
the a   typically indirect objects  instruments  or benefactives   the explanation for this
behavior is that indirect objects tend to be farther from their predicates and accumulate
more parsing errors  from the models based on full syntax  model   has better recall
whereas model   has better precision  because model   lters out all candidate arguments
that do not match a single syntactic constituent  generally  table   shows that all models
have strong and weak points  this is further justication for our focus on combination
strategies that combine several independent models 

   features of the combination models
as detailed in section    in this paper we analyze two classes of combination strategies for
the problem of semantic role labeling   a  an inference model with constraint satisfaction 
which nds the set of candidate arguments that maximizes a global cost function  and  b 
two inference strategies based on learning  where candidates are scored and ranked using
discriminative classiers  from the perspective of the feature space  the main dierence
between these two types of combination models is that the input of the rst combination strategy is limited to the argument probabilities produced by the individual systems 
whereas the last class of combination approaches incorporates a much larger feature set in
their ranking classiers  for robustness  in this paper we use only features that are extracted from the solutions provided by the individual systems  hence are independent of the
   

fisurdeanu  marquez  carreras    comas

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
a 

a 

v

v

a 

a 

v

a 

a 

a 

m 

m 

m 

figure    sample solutions proposed for the same predicate by three individual srl models 
m   m  and m   argument candidates are displayed vertically for each system 

individual models    we describe all these features next  all examples given in this section
are based on figures   and   
voting features  these features quantify the votes received by each argument from the
individual systems  this set includes the following features 
 the label of the candidate argument  e g   a  for the rst argument proposed by system
m  in figure   
 the number of systems that generated an argument with this label and span  for the
example shown in figure    this feature has value   for the argument a  proposed by
m  and   for m s a   because system m  proposed the same argument 
 the unique ids of all the systems that generated an argument with this label and
span  e g   m  and m  for the argument a  proposed by m  or m  in figure   
 the argument sequence for this predicate for all the systems that generated an argument with this label and span  for example  the argument sequence generated by
system m  for the proposition illustrated in figure   is  a    v   a    a   this feature attempts to capture information at proposition level  e g   a combination model
might learn to trust model m  more for the argument sequence a    v   a    a  
m  for another sequence  etc 
same predicate overlap features  these features measure the overlap between dierent
arguments produced by the individual srl models for the same predicate 
   with the exception of the argument probabilities  which are required by the constraint satisfaction model 

   

ficombination strategies for semantic role labeling

 the number and unique ids of all the systems that generated an argument with the
same span but different label  for the example shown in figure    these features have
values   and m  for the argument a  proposed by m   because model m  proposed
argument a  with the same span 
 the number and unique ids of all the systems that generated an argument included
in the current argument  for the candidate argument a  proposed by model m  in
figure    these features have values   and m   because m  generated argument a  
which is included in m s a  
 in the same spirit  we generate the number and unique ids of all the systems that
generated an argument that contains the current argument  and the number and
unique ids of all the systems that generated an argument that overlaps  but does
not include nor contain  the current argument 
other predicate overlap features  these features quantify the overlap between dierent arguments produced by the individual srl models for other predicates  we generate the
same features as the previous feature group  with the dierence that we now compare arguments generated for dierent predicates  the motivation for these overlap features is that 
according to the propbank annotations  no form of overlap is allowed among arguments
attached to the same predicate  and only inclusion or containment is permitted between
arguments assigned to dierent predicates  the overlap features are meant to detect when
these domain constraints are not satised by a candidate argument  which is an indication 
if the evidence is strong  that the candidate is incorrect 
partial syntax features  these features codify the structure of the argument and the
distance between the argument and the predicate using only partial syntactic information 
i e   chunks and clause boundaries  see figure   for an example   note that these features
are inherently dierent from the features used by model    because model   evaluates each
individual chunk part of a candidate argument  whereas here we codify properties of the
complete argument constituent  we describe the partial syntax features below 
 length in tokens and chunks of the argument constituent  e g     and   for argument
a  in figure   
 the sequence of chunks included in the argument constituent  e g   pp np for the
argument am loc in figure    if the chunk sequence is too large  we store n grams of
length    for the start and end of the sequence 
 the sequence of clause boundaries  i e   clause beginning or ending  included in the
argument constituent 
 the named entity types included in the argument constituent  e g   location for the
am loc argument in figure   
 position of the argument  before after the predicate in the sentence  e g   after for a 
in figure   
 a boolean ag to indicate if the argument constituent is adjacent to the predicate 
e g   false for a  and true for a  in figure   
   

fisurdeanu  marquez  carreras    comas

clause

np

np

vp

np

pp

np

the luxury auto maker last year sold       cars in the u s 
a 

amtmp

p

a 

amloc

figure    sample proposition with partial syntactic information 
 the sequence of chunks between the argument constituent and the predicate  e g   the
chunk sequence between the predicate and the argument am loc in figure   is  np 
similarly to the above chunk sequence feature  if the sequence is too large  we store
starting and ending n grams 
 the number of chunks between the predicate and the argument  e g     for am loc in
figure   
 the sequence of clause boundaries between the argument constituent and the predicate 
 the clause subsumption count  i e   the dierence between the depths in the clause
tree of the argument and predicate constituents  this value is   if the two phrases are
included in the same clause 
full syntax features  these features codify the structure of the argument constituent 
the predicate  and the distance between the two using full syntactic information  the
full syntax features are replicated from model    see section       which assumes that a
one to one mapping from semantic constituents to syntactic phrases exists  unlike model  
which ignores arguments that can not be matched against a syntactic constituent  if such
an exact mapping does not exist due to the inclusion of candidates from models   and    we
generate an approximate mapping from the unmapped semantic constituent to the largest
phrase that is included in the given span and has the same left boundary as the semantic constituent  this heuristic guarantees that we capture at least some of the semantic
constituents syntactic structure 
the motivation for the partial and full syntax features is to learn the preferences of
the individual srl models  for example  with these features a combination classier might
learn to trust model m  for arguments that are closer than   chunks to the predicate  model
m  when the predicate argument syntactic path is np  s  vp  sbar  s  vp  etc 
individual systems argument probabilities  each individual model outputs a condence score for each of their proposed arguments  these scores are converted into probabilities using the softmax function as described in detail in section      the combination
strategy based on constraint satisfaction  section      uses these probabilities as they are 
while the other two strategies based on meta learning  section      have to discretize the
probabilities to include them as features  to do so  each probability value is matched to
   

ficombination strategies for semantic role labeling

one of ve probability intervals and the corresponding interval is used as the feature  the
probability intervals are dynamically constructed for each argument label and each individual system such that the corresponding system predictions for this argument label are
uniformly distributed across the intervals 
in section     we empirically analyze the contribution of each of these proposed feature
sets to the performance of our best combination model 

   combination strategies
in this section we detail the combination strategies proposed in this paper   a  a combination
model with constraint satisfaction  which aims at nding the set of candidate arguments
that maximizes a global cost function  and  b  two combination models with inference based
on learning  where candidates are scored and ranked using discriminative classiers  in the
previous section we described the complete feature set made available to all approaches 
here we focus on the machine learning paradigm deployed by each of the combination
models 
    inference with constraint satisfaction
the constraint satisfaction model selects a subset of candidate arguments that maximizes
a compatibility function subject to the fulllment of a set of structural constraints that
ensure consistency of the solution  the compatibility function is based on the probabilities
given by individual srl models to the candidate arguments  in this work we use integer
linear programming to solve the constraint satisfaction problem  this approach was rst
proposed by roth and yih        and applied to semantic role labeling by punyakanok 
roth  yih  and zimak         koomen et al          among others  we follow the setting
of komen et al   which is taken as a reference 
as a rst step  the scores from each model are normalized into probabilities  the scores
yielded by the classiers are signed and unbounded real numbers  but experimental evidence
shows that the condence in the predictions  taken as the absolute value of the raw scores 
correlates well with the classication accuracy  thus  the softmax function  bishop       
is used to convert the set of unbounded scores into probabilities  if there are k possible
output labels for a given argument and sco li   denotes the score of label li output by a
xed srl model  then the estimated probability for this label is 
esco li  
p li     pk
sco lj  
j   e
the  parameter of the above formula can be empirically adjusted to avoid overly skewed
probability distributions and to normalize the scores of the three individual models to a
similar range of values  see more details about our experimental setting in section     
candidate selection is performed via integer linear programming  ilp   the program
goal is to maximize a compatibility function modeling the global condence of the selected
set of candidates  subject to a set of linear constraints  all the variables involved in the
task take integer values and may appear in rst degree polynomials only 
an abstract ilp process can be described in a simple fashion as  given a set of variables v    v            vn    it aims to maximize the global compatibility of a label assignment
   

fisurdeanu  marquez  carreras    comas

 l            ln   to these variables  a local compatibility function cv  l  denes the compatibility
of assigning label l to variable v  the global compatibility function c l            ln   is taken
as the sum of each local assignment compatibility  so the goal of the ilp process can be
written as 
argmax c l            ln     argmax
l       ln

l       ln

n
x

cvi  li  

i  

where the constraints are described in a set of accompanying integer linear equations involving the variables of the problem 
if one wants to codify soft constraints instead of hard  there is the possibility of considering them as a penalty component in the compatibility function  in this case  each constraint
r  r can be seen as a function which takes the current label assignment and outputs a
real number  which is   when the constraint is satised and a positive number when not 
indicating the penalty imposed to the compatibility function  the new expression of the
compatibility function to maximize is 
c l            ln    

n
x
i  

cvi  li   

x

r l            ln  

rr

note that the hard constraints can also be simulated in this setting by making them output
a very large positive number when they are violated 
in our particular problem  we have a binary valued variable vi for each of the n argument candidates generated by the srl models  i e   li labels are in         given a label
assignment  the arguments with li     are selected to form the solution  while the others
 those where li      are ltered out  for each variable vi   we also have the probability
values  pij   calculated from the score of model j on argument i  according to the softmax
formula described above    in a rst approach  the compatibility function cv  li   equals to
p
 
  m
j   pij  li   where the number of models  m   is   in our case  
under this denition  maximizing the compatibility function is equivalent to maximizing
the sum of the probabilities given by the models to the argument candidates considered in
the solution  since this function is always positive  the global score increases directly with
the number of selected candidates  as a consequence  the model is biased towards the
maximization of the number of candidates included in the solution  e g   tending to select
a lot of small non overlapping arguments   following koomen et al          this bias can
be corrected by adding a new score oi   which sums to the compatibility function when the
i th candidate is not selected in the solution  the global compatibility function needs to be
rewritten to encompass this new information  formalized as an ilp equation  it looks like 
argmax c l            ln     argmax

l     n

l     n

m
n x
x

 

i   j  

pij  li   oi     li  

   if model j does not propose argument i then we consider pij     
   instead of accumulating the probabilities of all models for a given candidate argument  one could consider
a different variable for each model prediction and introduce a constraint forcing all these variables to
take the same value at the end of the optimization problem  the two alternatives are equivalent 

   

ficombination strategies for semantic role labeling

where the constraints are expressed in separated integer linear equations  it is not possible
to dene a priori the value of oi   komen et al  used a validation corpus to empirically
estimate a constant value for all oi  i e   independent from the argument candidate     we
will use exactly the same solution of working with a single constant value  to which we will
refer as o 
regarding the consistency constraints  we have considered the following six 
   two candidate arguments for the same verb can not overlap nor embed 
   a verb may not have two core arguments with the same type label a  a  
   if there is an argument r x for a verb  there has to be also an x argument for the
same verb 
   if there is an argument c x for a verb  there has to be also an x argument before the
c x for the same verb 
   arguments from two dierent verbs can not overlap  but they can embed 
   two dierent verbs can not share the same am x  r am x or c x arguments 
constraints    are also included in our reference work  punyakanok et al          no
other constraints from that paper need to be checked here since each individual model
outputs only consistent solutions  constraints   and    which restrict the set of compatible
arguments among dierent predicates in the sentence  are original to this work  in the
integer linear programming setting the constraints are written as inequalities  for example 
if ai is the argument label of the i th candidate and vi its verb predicate  constraint number
p
  is written as   ai  a  vi  v  li     for a given verb v and argument label a  the other
constraints have similar translations into inequalities 
constraint satisfaction optimization will be applied in two dierent ways to obtain
the complete output annotation of a sentence  in the rst one  we proceed verb by verb
independently to nd their best selection of candidate arguments using only constraints
  through    we call this approach local optimization  in the second scenario all the
candidate arguments in the sentence are considered at once and constraints   through   are
enforced  we will refer to this second strategy as global optimization  in both scenarios the
compatibility function will be the same  but constraints need some rewriting in the global
scenario because they have to include information about the concrete predicate 
in section     we will extensively evaluate the presented inference model based on constraint satisfaction  and we will describe some experiments covering the following topics   a 
the contribution of each of the proposed constraints   b  the performance of local vs  global
optimization  and  c  the precisionrecall tradeo by varying the value of the bias correction
parameter 
    inference based on learning
this combination model consists of two stages  a candidate scoring phase  which scores
candidate arguments in the pool using a series of discriminative classiers  and an inference
stage  which selects the best overall solution that is consistent with the domain constraints 
   instead of working with a constant  one could try to set the oi value for each candidate  taking into
account some contextual features of the candidate  we plan to explore this option in the near future 

   

fisurdeanu  marquez  carreras    comas

the rst and most important component of this combination strategy is the candidate
scoring module  which assigns to each candidate argument a score equal to the condence
that this argument is part of the global solution  it is formed by discriminative functions 
one for each role label  below  we devise two dierent strategies to train the discriminative
functions 
after scoring candidate arguments  the nal global solution is built by the inference
module  which looks for the best scored argument structure that satises the domain specic
constraints  here  a global solution is a subset of candidate arguments  and its score is
dened as the sum of condence values of the arguments that form it  we currently consider
three constraints to determine which solutions are valid 
 a  candidate arguments for the same predicate can not overlap nor embed 
 b  in a predicate  no duplicate arguments are allowed for the numbered arguments a    
 c  arguments of a predicate can be embedded within arguments of other predicates but
they can not overlap 
the set of constraints can be extended with any other rules  but in our particular case  we
know that some constraints  e g   providing only arguments indicated in the corresponding propbank frame  are already guaranteed by the individual models  and others  e g  
constraints   and   in the previous sub section  have no positive impact on the overall
performance  see section     for the empirical analysis   the inference algorithm we use
is a bottom up cky based dynamic programming strategy  younger         it builds the
solution that maximizes the sum of argument condences while satisfying the constraints 
in cubic time 
next  we describe two dierent strategies to train the functions that score candidate
arguments  the rst is a local strategy  each function is trained as a binary batch classier 
independently of the combination process which enforces the domain constraints  the
second is a global strategy  functions are trained as online rankers  taking into account the
interactions that take place during the combination process to decide between one argument
or another 
in both training strategies  the discriminative functions employ the same representation of arguments  using the complete feature set described in section    we analyze the
contribution of each feature group in section     our intuition was that the rich feature
space introduced in section   should allow the gathering of sucient statistics for robust
scoring of the candidate arguments  for example  the scoring classiers might learn that
a candidate is to be trusted if   a  two individual systems proposed it   b  if its label is
a  and it was generated by model    or  c  if it was proposed by model   within a certain
argument sequence 
      learning local classifiers
this combination process follows a cascaded architecture  in which the learning component
is decoupled from the inference module  in particular  the training strategy consists of
training a binary classier for each role label  the target of each label based classier is to
determine whether a candidate argument actually belongs to the correct proposition of the
corresponding predicate  and to output a condence value for this decision 
   

ficombination strategies for semantic role labeling

the specic training strategy is as follows  the training data consists of a pool of
labeled candidate arguments  proposed by individual systems   each candidate is either
positive  in that it is actually a correct argument of some sentence  or negative  if it is
not correct  the strategy trains a binary classier for each role label l  independently of
other labels  to do so  it concentrates on the candidate arguments of the data that have
label l  this forms a dataset for binary classication  specic to the label l  with it  a
binary classier can be trained using any of the existing techniques for binary classication 
with the only requirement that our combination strategy needs condence values with each
binary prediction  in section   we provide experiments using svms to train such local
classiers 
in all  each classier is trained independently of other classiers and the inference module  looking globally at the combination process  each classier can be seen as an argument
ltering component that decides which candidates are actual arguments using a much richer
representation than the individual models  in this context  the inference engine is used as a
conict resolution engine  to ensure that the combined solutions are valid argument structures for sentences 
      learning global rankers
this combination process couples learning and inference  i e   the scoring functions are
trained to behave accurately within the inference module  in other words  the training
strategy here is global  the target is to train a global function that maps a set of argument
candidates for a sentence into a valid argument structure  in our setting  the global function
is a composition of scoring functions one for each label  same as the previous strategy 
unlike the previous strategy  which is completely decoupled from the inference engine  here
the policy to map a set of candidates into a solution is that determined by the inference
engine 
in recent years  research has been very active in global learning methods for tagging 
parsing and  in general  structure prediction problems  collins        taskar  guestrin   
koller        taskar  klein  collins  koller    manning        tsochantaridis  hofmann 
joachims    altun         in this article  we make use of the simplest technique for global
learning  an online learning approach that uses perceptron  collins         the general
idea of the algorithm is similar to the original perceptron  rosenblatt         correcting
the mistakes of a linear predictor made while visiting training examples  in an additive
manner  the key point for learning global rankers relies on the criteria that determines
what is a mistake for the function being trained  an idea that has been exploited in a
similar way in multiclass and ranking scenarios by crammer and singer      a      b  
the perceptron algorithm in our combination system works as follows  pseudocode
of the algorithm is given in figure     let         l be the possible role labels  and let
w    w        wl   be the set of parameter vectors of the scoring functions  one for each
label  perceptron initializes the vectors in w to zero  and then proceeds to cycle through
the training examples  visiting one at a time  in our case  a training example is a pair  y  a  
where y is the correct solution of the example and a is the set of candidate arguments for
it  note that both y and a are sets of labeled arguments  and thus we can make use of
the set dierence  we will note as a a particular argument  as l the label of a  and as
   

fisurdeanu  marquez  carreras    comas

initialization  for each wl  w do wl    
training  
for t           t do
for each training example  y  a  do
y   inference a  w 
for each a  y   y do
let l be the label of a
wl   wl    a 
for each a  y   y do
let l be the label of a
wl   wl   a 
output  w
figure    perceptron global learning algorithm

 a  the vector of features described in section    with each example  perceptron performs
two steps  first  it predicts the optimal solution y according to the current setting of
w  note that the prediction strategy employs the complete combination model  including
the inference component  second  perceptron corrects the vectors in w according to the
mistakes seen in y  arguments with label l seen in y and not in y are promoted in vector
wl   on the other hand  arguments in y and not in y are demoted in wl   this correction rule
moves the scoring vectors towards missing arguments  and away from predicted arguments
that are not correct  it is guaranteed that  as perceptron visits more and more examples 
this feedback rule will improve the accuracy of the global combination function when the
feature space is almost linearly separable  freund   schapire        collins        
in all  this training strategy is global because the mistakes that perceptron corrects are
those that arise when comparing the predicted structure with the correct one  in contrast 
a local strategy identies mistakes looking individually at the sign of scoring predictions  if
some candidate argument is  is not  in the correct solution and the current scorers predict
a negative  positive  condence value  then the corresponding scorer is corrected with that
candidate argument  note that this is the same criteria used to generate training data for
classiers trained locally  in section   we compare these approaches empirically 
as a nal note  for simplicity we have described perceptron in its most simple form 
however  the perceptron version we use in the experiments reported in section   incorporates two well known extensions  kernels and averaging  freund   schapire        collins
  duy         similar to svm  perceptron is a kernel method  that is  it can be represented in dual form  and the dot product between example vectors can be generalized by
a kernel function that exploits richer representations  on the other hand  averaging is a
technique that increases the robustness of predictions during testing  in the original form 
test predictions are computed with the parameters that result from the training process 
in the averaged version  test predictions are computed with an average of all parameter
vectors that are generated during training  after every update  details of the technique can
be found in the original article of freund   schapire 
   

ficombination strategies for semantic role labeling

 

development
brown
wsj

    
    

acuracy

    
    
   
    
    
    
    
 

  

  

  

  

  

  

  

  

  

   

reject rate    

figure    rejection curves of the estimated output probabilities of the individual models 

   experimental results
in this section we analyze the performance of the three combination strategies previously
described   a  inference with constraint satisfaction   b  learning based inference with local
rankers  and  c  learning based inference with global rankers  for the bulk of the experiments we use candidate arguments generated by the three individual srl models described
in section   and evaluated in section   
    experimental settings
all combination strategies  with one exception  detailed below  were trained using the
complete conll      training set  propbank treebank sections   to      to minimize the
overtting of the individual srl models on the training data  we partitioned the training
corpus into ve folds and for each fold we used the output of the individual models when
trained on the remaining four folds  the models were tuned on a separate development
partition  treebank section     and evaluated on two corpora   a  treebank section    
and  b  on the three annotated sections of the brown corpus 
for the constraint satisfaction model  we converted the scores of the output arguments of
the three srl models into probabilities using the softmax function explained in section     
the development set  section     was used to tune the  parameter of the softmax formula
to a nal value of     for all models  in order to assess the quality of this procedure  we
plot in figure   the rejection curves of the estimated output probabilities with respect to
classication accuracy on the development and test sets  wsj and brown   to calculate
these plots  the probability estimates of all three models are put together in a set and sorted
in decreasing order  at a certain level of rejection  n    the curve in figure   plots the
percentage of correct arguments when the lowest scoring n  subset is rejected  with few
   

fisurdeanu  marquez  carreras    comas

exceptions  the curves are increasing and smooth  indicating a good correlation between
probability estimates and classication accuracy 
as a last experiment  in section     we analyze the behavior of the proposed combination
strategies when the candidate pool is signicantly larger  for this experiment we used the
top    best performing systems at the conll      shared task evaluation  in this setup
there are two signicant dierences from the experiments that used our in house individual
systems   a  we had access only to the systems outputs on the propbank development
section and on the two test sections  and  b  the argument probabilities of the individual
models were not available  thus  instead of the usual training set  we had to train our
combination models on the propbank development section with a smaller feature set  note
also that the development set is only       of the size of the regular training set  we
evaluated the resulting combination models on the same two testing sections  wsj and
brown 
    lower and upper bounds of the combination strategies
before we venture into the evaluation of the combination strategies  we explore the lower
and upper bounds of the combinations models on the given corpus and individual models 
this analysis is important in order to understand the potential of the proposed approach
and to see how close we actually are to realizing it 
the performance upper bound is calculated with an oracle combination system with
a perfect ltering classier that selects only correct candidate arguments and discards all
others  for comparison purposes  we have implemented a second oracle system that simulates a re ranking approach  for each predicate it selects the candidate frame i e   the
complete set of arguments for the corresponding predicate proposed by a single model
with the highest f  score  table   lists the results obtained on the wsj and brown corpora
by these two oracle systems using all three individual models  the combination system
is the oracle that simulates the combination strategies proposed in this paper  which break
candidate frames and work with individual candidate arguments  note that the precision of
this oracle combination system is not      because in the case of discontinuous arguments 
fragments that pass the oracle lter are considered incorrect by the scorer when the corresponding argument is not complete  e g   an argument a  appears without the continuation
c a   the re ranking columns list the results of the second oracle system  which selects
entire candidate frames 
table   indicates that the upper limit of the combination approaches proposed in this
paper is relatively high  the f  of the combination oracle system is over    points higher
than our best individual system in the wsj test set  and over    points higher in the
brown corpus  see table     furthermore  our analysis indicates that the potential of
our combination strategy is higher than that of re ranking strategies  which are limited to
the performance of the best complete frame in the candidate pool  by allowing the recombination of arguments from the individual candidate solutions this threshold is raised
signicantly  over   f  points in wsj and over   f  points in brown 
table   lists the distribution of the candidate arguments from the individual models in
the selection performed by the combination oracle system  for conciseness  we list only the
core numbered arguments and we focus on the wsj corpus   of   indicates the percent   

ficombination strategies for semantic role labeling

wsj
brown

pprops
      
      

combination
precision recall
      
      
      
      

f 
     
     

pprops
      
      

re ranking
precision recall
      
      
      
      

f 
     
     

table    performance upper limits detected by the two oracle systems 
a 
a 
a 
a 
a 

 of  
      
      
      
      
      

 of  
      
      
      
      
      

model  
     
     
      
      
     

model  
     
     
     
     
     

model  
     
     
     
     
     

table    distribution of the individual systems arguments in the upper limit selection  for
a a  in the wsj test set 

age of correct arguments where all   models agreed   of   indicates the percentage of
correct arguments where any   models agreed  and the other columns indicate the percentage of correct arguments detected by a single model  table   indicates that  as expected 
two or more individual models agreed on a large percentage of the correct arguments  nevertheless  a signicant number of correct arguments  e g   over     of a   come from a single
individual system  this proves that  in order to achieve maximum performance  one has
to look beyond simple voting strategies that favor arguments with high agreement between
individual systems 
we propose two lower bounds for the performance of the combination models using two
baseline systems 
 the rst baseline is recall oriented  it merges all the arguments generated by the
individual systems  for conict resolution  the baseline uses an approximate inference
algorithm consisting of two steps   i  candidate arguments are sorted using a radix
sort that orders the candidate arguments in descending order of   a  number of models
that agreed on this argument   b  argument length in tokens  and  c  performance of
the individual system      ii  candidates are iteratively appended to the global solution
only if they do not violate any of the domain constraints with the arguments already
selected 
 the second baseline is precision oriented  it considers only arguments where all three
individual systems agreed  for conict resolution it uses the same strategy as the
previous baseline system 
table   shows the performance of these two baseline models  as expected  the precisionoriented baseline obtains a precision signicantly higher than the best individual model
 table     but its recall suers because the individual models do not agree on a fairly large
number of candidate arguments  the recall oriented baseline is more balanced  as expected
the recall is higher than any individual model and the precision does not drop too much
    this combination produced the highest scoring baseline model 

   

fisurdeanu  marquez  carreras    comas

wsj
baseline
baseline
brown
baseline
baseline

recall
precision

pprops
      
      

prec 
      
      

recall
      
      

f 
         
         

recall
precision

      
      

      
      

      
      

     
     

   
   

table    performance of the baseline models on the wsj and brown test sets 
because the inference strategy lters out many unlikely candidates  overall  the recalloriented baseline performs best  with an f       points higher than the best individual
model on the wsj corpus  and      points lower on the brown corpus 
    performance of the combination system with constraint satisfaction
in the constraint satisfaction setting the arguments output by individual models       and
  are recombined into an expected better solution that satises a set of constraints  we
have run the inference model based on constraint satisfaction described in section     using
the xpress mp ilp solver     the main results are summarized in table    the variants
presented in that table are the following  pred by pred stands for local optimization 
which processes each verb predicate independently from others  while full sentence stands
for global optimization  i e   resolving all the verb predicates of the sentence at the same
time  the column labeled constraints shows the particular constraints applied at each
conguration  the o column presents the value of the parameter for correcting the bias
towards candidate overgeneration  concrete values are empirically set to maximize the f 
measure on the development set  o     corresponds to a setting in which no bias correction
is applied 
some clear conclusions can be drawn from table    first  we observe that any optimization variant obtains f  results above both the individual systems  table    and the
baseline combination schemes  table     the best combination model scores      f  points
in wsj and      in brown higher than the best individual system  taking into account
that no learning is performed  it is clear that constraint satisfaction is a simple yet formal
setting that achieves good results 
a somewhat surprising result is that all performance improvements come from constraints   and    i e   no overlapping nor embedding among arguments of the same verb 
and no repetition of core arguments in the same verb   constraints   and   are harmful 
while the sentence level constraints    and    have no impact on the overall performance    
our analysis of the proposed constraints yielded the following explanations 
 constraint number   prevents the assignment of an r x argument when the referred
argument x is not present  this makes the inference miss some easy r x arguments
    xpress mp is a dash optimization product that is free for academic usage 
    in section     we will see that when the learning strategy incorporates global feedback  performing a
sentence level inference is slightly better than proceeding predicate by predicate 

   

ficombination strategies for semantic role labeling

wsj
pred by pred

full sentence

brown
full sentence

constraints
 
   
     
     
       
     
     
       
       

o
    
    
    
    
    
    
    
    
 

pprops
      
      
      
      
      
      
      
      
      

precision
      
      
      
      
      
      
      
      
      

recall
      
      
      
      
      
      
      
      
      

f 
         
         
         
         
         
         
         
         
         

       
       

    
 

      
      

      
      

      
      

         
         

table    results  on wsj and brown test sets  obtained by multiple variants of the constraint satisfaction approach

when the x argument is not correctly identied  e g   constituents that start with
 that  which  who  followed by a verb are always r a    furthermore  this constraint
presents a lot of exceptions  up to        of the r x arguments in the wsj test set do
not have the referred argument x  e g   when in the law tells them when to do so  
therefore the hard application of constraint   prevents the selection of some correct
r x candidates  the ocial evaluation script from conll       srl eval  does not
require this constraint to be satised to consider a solution consistent 
 the srl eval script requires that constraint number    i e   a c x tag is not accepted
without a preceding x argument  be fullled for a candidate solution to be considered
consistent  but when it nds a solution violating the constraint its behavior is to
convert the rst c x  without a preceding x  into x  it turns out that this simple
post processing strategy is better than forcing a coherent solution in the inference step
because it allows to recover from the error when an argument has been completely
recognized but labeled only with c x tags 
 regarding sentence level constraints  we observed that in our setting  inference using
local constraints       rarely produces a solution with inconsistencies at sentence
level    this makes constraint   useless since it is almost never violated  constraint
number    i e   no sharing of ams among dierent verbs  is more ad hoc and represents
a less universal principle in srl  the number of exceptions to that constraint  in
the wsj test set  is      for the gold standard data and      in the output of
the inference that uses only local constraints        forcing the fulllment of this
constraint makes the inference process commit as many errors as corrections  making
its eect negligible 
    this fact is partly explained by the small number of overlapping arguments in the candidate pool
produced by the three individual models 

   

fisurdeanu  marquez  carreras    comas

  
  

  

precision
recall
f 

  

  

  
 

  

 

  

precision
recall
f 

  

  

  

  

  

  

  
    

    

 

   

   

   

   

  
    

 

value of o

    

 

   

   

   

   

 

value of o

figure    precision recall plots  with respect to the bias correcting parameter  o   for the
wsj development and test sets  left and right plots  respectively  

considering that some of the constraints are not universal  i e   exceptions exist in the
gold standard  it seems reasonable to convert them into soft constraints  this can be done
by precomputing their compatibility from corpora counts using  for instance  point wise
mutual information  and incorporating its eect in the compatibility function as explained in
section      this softening could  in principle  increase the overall recall of the combination 
unfortunately  our initial experiments showed no dierences between the hard and soft
variants 
finally  the dierences between the optimized values of the bias correcting parameter
and o     are clearly explained by observing precision and recall values  the default
version tends to overgenerate argument assignments  which implies a higher recall at a cost
of a lower precision  on the contrary  the f  optimized variant is more conservative and
needs more evidence to select a candidate  as a result  the precision is higher but the recall
is lower  a side eect of being restrictive with argument assignments  is that the number
of correctly annotated complete propositions is also lower in the optimized setting 
the preference for a high precision vs  a high recall system is mostly task dependant 
it is interesting to note that in this constraint satisfaction setting  adjusting the precision
recall tradeo can be easily done by varying the value of the bias correcting score  in
figure    we plot the precisionrecall curves with respect to dierent values of the o parameter  the optimization is done using constraints          and     as expected  high values
of o promote precision and demote recall  while lower values of o do just the contrary  also 
we see that there is a wide range of values for which the combined f  measure is almost
constant  the approximate intervals are marked using vertical lines   making it possible to
select dierent recall and precision values with a global performance  f    near to the optimal  parenthetically  note also that the optimal value estimated on the development set
 o        generalizes very well to the wsj test set 
   

ficombination strategies for semantic role labeling

wsj
models
models
models
models
brown
models
models
models
models

   
   
   
     

pprops
      
      
      
      

prec 
      
      
      
      

recall
      
      
      
      

f 
         
         
         
         

f  improvement
     
     
     
     

   
   
   
     

      
      
      
      

      
      
      
      

      
      
      
      

         
         
         
         

     
     
     
     

table    overall results of the learning based inference with local rankers on the wsj and
brown test sets 

    performance of the combination system with local rankers
we implemented the candidate scoring classiers for this combination strategy using support vector machines  svm  with polynomial kernels of degree    which performed slightly
better than other types of svms or adaboost  we have implemented the svm classiers
with the svmlight software     outside of changing the default kernel to polynomial we
have not modied the default parameters  for the experiments reported in this section 
we trained models for all   possible combinations of our   individual systems  using the
complete feature set introduced in section    the dynamic programming engine used for
the actual inference processes each predicate independently  similar to the pred by pred
approach in the previous sub section  
table   summarizes the performance of the combined systems on the wsj and brown
corpora  table   indicates that our combination strategy is always successful  the results
of all combination systems improve upon their individual models  table    and their f 
scores are always better than the baselines  table     the last column in the table shows
the f  improvement of the combination model w r t  the best individual model in each set 
as expected  the highest scoring combined system includes all three individual models  its
f  measure is      points higher than the best individual model  model    in the wsj test
set and      points higher in the brown test set  note that with any combination of two
individual systems we outperform the current state of the art  see section   for details   this
is empirical proof that robust and successful combination strategies for the srl problem
are possible  table   also indicates that  even though the partial parsing model  model    is
the worst performing individual model  its contribution to the ensemble is very important 
indicating that the information it provides is indeed complementary to the other models 
for instance  in wsj the performance of the combination of the two best individual models
 models      is worse than the combinations using model    models     and      
    http   svmlight joachims org 

   

fisurdeanu  marquez  carreras    comas

wsj
fs 
  fs 
  fs 
  fs 
  fs 
  fs 
brown
fs 
  fs 
  fs 
  fs 
  fs 
  fs 

pprops
      
      
      
      
      
      

prec 
      
      
      
      
      
      

recall
      
      
      
      
      
      

f 
         
         
         
         
         
         

      
      
      
      
      
      

      
      
      
      
      
      

      
      
      
      
      
      

         
         
         
         
         
         

table    feature analysis for the learning based inference with local rankers 
due to its simple architecture i e   no feedback from the conict resolution component
to candidate ltering this inference model is a good framework to study the contribution of
the features proposed in section    for this study we group the features into   sets  fs  the
voting features  fs  the overlap features with arguments for the same predicate  fs  the
overlap features with arguments for other predicates  fs  the partial syntax features  fs 
the full syntax features  and fs  the probabilities generated by the individual systems
for the candidate arguments  using these sets we constructed   combination models by
increasing the number of features made available to the argument ltering classiers  e g  
the rst system uses only fs   the second system adds fs  to the rst systems features 
fs  is added for the third system  etc  table   lists the performance of these   systems
for the two test corpora  this empirical analysis indicates that the feature sets with the
highest contribution are 
 fs   which boosts the f  score of the combined system      points  wsj  and     
points  brown  over our best individual system  this is yet another empirical proof
that voting is a successful combination strategy 
 fs   with a contribution of      points  wsj  and      points  brown  to the f 
score  these numbers indicate that the ltering classier is capable of learning some
of the preferences of the individual models for certain syntactic structures 
 fs   which contributes      points  wsj  and      points  brown  to the f  score 
these results promote the idea that information about the overall sentence structure 
in our case inter predicate relations  can be successfully used for the problem of srl 
to our knowledge  this is novel 
all the proposed features have a positive contribution to the performance of the combined
system  overall  we achieve an f  score that is      points  wsj  and      points  brown 
higher than the best performing combined system at the conll      shared task evaluation
 see section   for details  
   

ficombination strategies for semantic role labeling

    performance of the combination system with global rankers
in this section we report experiments with the global perceptron algorithm described in
section        that globally trains the scoring functions as rankers  similar to the local
svm models  we use polynomial kernels of degree    furthermore  the predictions at test
time used averages of the parameter vectors  following the technique of freund and schapire
       
we were interested in two main aspects  first  we evaluate the eect of training the
scoring functions with perceptron using two dierent update rules  one global and the other
local  the global feedback rule  detailed in section        corrects the mistakes found when
comparing the correct argument structure with the one that results from the inference  this
is noted as global feedback   in contrast  the local feedback rule corrects the mistakes
found before inference  when each candidate argument is handled independently  ignoring
the global argument structure generated  this is noted as local feedback   second  we
analyze the eect of using dierent constraints in the inference module  to this extent 
we congured the inference module in two ways  the rst processes the predicates of a
sentence independently  and thus might select overlapping arguments of dierent predicates 
which is incorrect according to the domain constraints  this one is noted as pred by pred
inference   the second processes all predicates jointly  and enforces a hierarchical structure
of arguments  where arguments never overlap  and arguments of a predicate are allowed to
embed arguments of other predicates  this is noted as full sentence inference   from this
perspective  the model with local update and pred by pred inference is almost identical
to the local combination strategy described in section      with the unique dierence that
here we use perceptron instead of svm  this apparently minute dierence turns out to
be signicant for our empirical analysis because it allows us to measure the contribution
of both svm margin maximization and global feedback to the classier based combination
strategy  see section      
we trained four dierent models  with local or global feedback  and with predicate bypredicate or joint inference  each model was trained for   epochs on the training data 
and evaluated on the development data after each training epoch  we selected the best
performing point on development  and evaluated the models on the test data  table  
reports the results on test data 
looking at results  a rst impression is that the dierence in f  measure is not very
signicant among dierent congurations  however  some observations can be pointed out 
global methods achieve much better recall gures  whereas local methods prioritize the
precision of the system  overall  global methods achieve a more balanced tradeo between
precision and recall  which contributes to a better f  measure 
looking at pred by pred versus full sentence inference  it can be seen that only
the global methods are sensitive to the dierence  note that a local model is trained
independently of the inference module  thus  adding more constraints to the inference
engine does not change the parameters of the local model  at testing time  the dierent
inference congurations do not aect the results  in contrast  the global models are trained
dependently of the inference module  when moving from pred by pred to full sentence
inference  consistency is enforced between argument structures of dierent predicates  and
this benets both the precision and recall of the method  the global learning algorithm
   

fisurdeanu  marquez  carreras    comas

wsj
pred by pred  local
full sentence  local
pred by pred  global
full sentence  global
brown
pred by pred  local
full sentence  local
pred by pred  global
full sentence  global

pprops
      
      
      
      

prec 
      
      
      
      

recall
      
      
      
      

f 
         
         
         
         

      
      
      
      

      
      
      
      

      
      
      
      

         
         
         
         

table    test results of the combination system with global rankers  four congurations are
evaluated  that combine pred by pred or full sentence inference with local
or global feedback 

improves both in precision and recall when coupled with a joint inference process that
considers more constraints in the solution 
nevertheless  the combination system with local svm classiers  presented in the previous section  achieves marginally better f  score than the global learning method         vs 
       in wsj   this is explained by the dierent machine learning algorithms  we discuss
this issue in detail in section       the better f  score is accomplished by a much better
precision in the local approach         vs         in wsj   whereas the recall is lower in
the local than in the global approach         vs         in wsj   on the other hand  the
global strategy produces more completely correct annotations  see the pprops column 
than any of the local strategies investigated  see tables   and     this is to be expected 
considering that the global strategy optimizes a sentence level cost function  somewhat
surprisingly  the number of perfect propositions generated by the global strategy is lower
than the number of perfect propositions produced by the constraint satisfaction approach 
we discuss this result in section     
    scalability of the combination strategies
all the combination experiments reported up to this point used the candidate arguments
generated by the three individual srl models introduced in section    while these experiments do provide an empirical comparison of the three inference models proposed  they do
not answer an obvious scalability question  how do the proposed combination approaches
scale when the number of candidate arguments increases but their quality diminishes  we
are mainly interested in answering this question for the last two combination models  which
use inference based on learning with local or global rankers  for two reasons   a  they
performed better than the constraint satisfaction model in our previous experiments  and
 b  because they have no requirements on the individual srl systems outputs unlike
the constraint satisfaction model which requires the argument probabilities of the individual models they can be coupled to pools of candidates generated by any individual srl
model 
   

ficombination strategies for semantic role labeling

koomen
pradhan 
haghighi
marquez
pradhan

surdeanu
tsai
che
moschitti
tjongkimsang
yi

ozgencil

wsj
prec 
recall
      
      
             
             
      
      






      

      

      

     

      

      

      

     






      
      
      
      
      

      
      
      
      
      

      
      
      
      
      

     
     
     
     
     

      
      
      
      
      

      
      
      
      
      

      
      
      
      
      

     
     
     
     
     







f 
     
     
     
     

pprops
      
      
      
      

brown
prec 
recall
      
      
             
             
      
      

pprops
      
      
      
      

f 
     
     
     
     

      

      

      

     

      

      

      

     

      

      

      

     

      

      

      

     

table     performance of the best systems at conll       the pradhan  contains postevaluation improvements  the top   systems are actually combination models
themselves  the second column marks with  the systems not used in our evaluation  pradhan  which was replaced by its improved version pradhan   and
yi  due to format errors in the submitted data 

for this scalability analysis  we use as individual srl models the top    systems at the
conll      shared task evaluation  table    summarizes the performance of these systems
on the same two test corpora used in our previous experiments  as table    indicates  the
performance of these systems varies widely  there is a dierence of   f  points in the wsj
corpus and of over   f  points in the brown corpus between the best and the worst system
in this set 
for the combination experiments we generated   candidate pools using the top         

   and    individual systems labeled with
in table     we had to make two changes
to the experimental setup used in the rst part of this section   a  we trained our combined models on the propbank development section because we did not have access to the
individual systems outputs on the propbank training partition  and  b  from the feature
set introduced in section   we did not use the individual systems argument probabilities
because the raw activations of the individual models classiers were not available  note
that under these settings the size of the training corpus is    times smaller than the size of
the training set used in the previous experiments 
table    shows the upper limits of these setups using the combination and reranking oracle systems introduced in section      besides performance numbers  we also
list in table    the average number of candidates per sentence for each setup  i e   number
of unique candidate arguments    args  sent   for the combination oracle and number
of unique candidate frames    frames sent   for the re ranking oracle  table    lists
the performance of our combined models with both local feedback  section        and global
feedback  section         the combination strategy with global rankers uses joint inference
and global feedback  see the description in the previous sub section  
   

fisurdeanu  marquez  carreras    comas

wsj
c 
c 
c 
c 
c  
brown
c 
c 
c 
c 
c  

  args  sent 
    
    
     
     
     
    
    
    
     
     

combination
prec 
recall
      
      
      
      
      
      
      
      
             
      
      
      
      
      

      
      
      
      
      

f 
     
     
     
     
     

re ranking
  frames sent 
prec 
    
      
    
      
    
      
    
      
    
      

     
     
     
     
     

    
    
    
    
    

      
      
      
      
      

recall
      
      
      
      
      

f 
     
     
     
     
     

      
      
      
      
      

     
     
     
     
     

table     performance upper limits determined by the oracle systems on the    best systems at conll       ck stands for combination of the top k systems from
table       args  sent  indicates the average number of candidate arguments
per sentence for the combination oracle    frames sent  indicates the average
number of candidate frames per sentence for the re ranking oracle  the latter
can be larger than the number of systems in the combination because on average
there are multiple predicates per sentence 

wsj
c 
c 
c 
c 
c  
brown
c 
c 
c 
c 
c  

pprops
      
      
      
      
      

local
prec 
      
      
      
      
      

ranker
recall
      
      
      
      
      

f 
        
        
        
        
        

pprops
     
     
     
     
     

global ranker
prec 
recall
             
             
             
             
             

f 
        
        
        
        
        

      
      
      
      
      

      
      
      
      
      

      
      
      
      
      

        
        
        
        
        

     
     
     
     
     

      
      
      
      
      

        
        
        
        
        

      
      
      
      
      

table     local versus global ranking for combinations of the    best systems at conll      ck stands for combination of the top k systems from table    

we can draw several conclusions from these experiments  first  the performance upper limit of re ranking is always lower than that that of the argument based combination
strategy  even when the number of candidates is large  for example  when all    individual
models are used  the f  upper limit of our approach in the brown corpus is       whereas
the f  upper limit for re ranking is        however  the enhanced potential of our combination approach does not imply a signicant increase in computational cost  table    shows
   

ficombination strategies for semantic role labeling

that the number of candidate arguments that must be handled by combination approaches
is not that much higher than the number of candidate frames input to the re ranking system  especially when the number of individual models is high  for example  when all   
individual models are used  the combination approaches must process around    arguments
per sentence  whereas re ranking approaches must handle approximately   frames per sentence  the intuition behind this relatively small dierence in computational cost is that 
even though the number of arguments is signicantly larger than the number of frames 
the dierence between the number of unique candidates for the two approaches is not high
because the probability of repeated arguments is higher than the probability of repeated
frames 
the second conclusion is that all our combination models boost the performance of the
corresponding individual systems  for example  the best   system combination achieves an
f  score approximately   points higher than the best individual model in both the wsj
and brown corpus  as expected  the combination models reach a performance plateau
around     individual systems  when the quality of the individual models starts to drop
signicantly  nevertheless  considering that the top   individual systems use combination
strategies themselves and the amount of training data for this experiment was quite small 
these results show the good potential of the combination models analyzed in this paper 
the third observation is that the relation previously observed between local and global
rankers holds  our combination model with local rankers has better precision  but the model
with global rankers always has better recall and generally better pprops score  overall 
the model with local rankers obtains better f  scores and scales better as the number
of individual systems increases  we discuss these dierences in more detail in the next
sub section 
finally  table    indicates that the potential recall of this experiment  shown in the
left most block in the table  is higher than the potential recall when combining our three
individual srl systems  see table          higher in the wsj test set  and      higher in
the brown test set  this was expected  considering that both the number and the quality
of the candidate arguments in this last experiment is higher  however  even after this
improvement  the potential recall of our combination strategies is far from       thus 
combining the solutions of the n best state of the art srl systems still does not have
the potential to properly solve the srl problem  future work should focus on recallboosting strategies  e g   using candidate arguments of the individual systems before the
individual complete solutions are generated  because in this step many candidate arguments
are eliminated 
    discussion
the experimental results presented in this section indicate that all proposed combination
strategies are successful  all three combination models provide statistically signicant improvements over the individual models and the baselines in all setups  an immediate  but
somewhat shallow  comparison of the three combination strategies investigated indicates
that   a  the best combination strategy for the srl problem is a max margin local metalearner   b  the global ranking approach for the meta learner is important but it does not
   

fisurdeanu  marquez  carreras    comas

have the same contribution as a max margin strategy  and  c  the constraint satisfaction
model performs the worst of all the strategies tried 
however  in most experiments the dierences between the combination approaches investigated are small  a more reasonable observation is that each combination strategy has
its own advantages and disadvantages and dierent approaches are suitable for dierent
applications and data  we discuss these dierences below 
if the argument probabilities of individual systems are available  the combination model
based on constraint satisfaction is an attractive choice  it is a simple  unsupervised strategy that obtains competitive performance  furthermore  the constraint satisfaction model
provides an elegant and customizable framework to tune the balance between precision and
recall  see section       with this framework we currently obtain the highest recall of all
combination models        higher than the best recall obtained by the meta learning approaches on the wsj corpus  and       higher than the meta learning models on the brown
corpus  the higher recall implies also higher percentage of predicates that are completely
correctly annotated  the best pprops numbers in table   are the best of all combination
strategies  the cause for this high dierence in recall in favor of the constraint satisfaction
approach is that the candidate scoring of the learning based inference acts implicitly as a
lter  all candidates whose score i e   the classier condence that the candidate is part of
the correct solution is negative are discarded  which negatively aects the overall recall 
hence  constraint satisfaction is a better solution for srl based nlp applications which
require that predicate argument frames be extracted with high recall  for example  in information extraction  predicate argument tuples are ltered with subsequent high precision 
domain specic constraints  surdeanu et al          hence it is paramount that the srl
model have high recall 
nevertheless  in many cases the argument probabilities of the individual srl models are
not available  either because the models do not generate them  e g   rule based systems  or
because the individual models are available only as black boxes  which do not oer access to
internal information  under these conditions  we showed that combination strategies based
on meta learning are a viable alternative  in fact  these approaches obtain the highest
f  scores  see section      and obtain excellent performance even with small amounts of
training data  see section       as previously mentioned  because candidate scoring acts
as lter  the learning based inference tends to favor precision over recall  their precision
is       higher than the best precision of the constraint satisfaction models in the wsj
corpus  and       higher in the brown corpus  this preference for precision over recall is
more pronounced in the learning based inference with local rankers  section      than in
the inference model with global rankers  section       our hypothesis for what causes the
global ranking model to be less precision biased is that in this conguration the ratio of
errors on positive versus negative samples is more balanced  thinking in the strategy that
perceptron follows  a local approach updates at every candidate with incorrect prediction
sign  whereas a global approach only updates at candidates that should or should not be in
the complete solution  after enforcing the domain constraints  in other words  the number
of negative updates which drives the precision bias is reduced in the global approach 
because some of the false positives generated by the ranking classiers are eliminated by
the domain constraints  thus  because candidate scoring is trained to optimize accuracy 
   

ficombination strategies for semantic role labeling

wsj
global feedback
max margin
brown
global feedback
max margin

pprops
      
      

prec 
      
      

recall
      
      

f 
     
     

      
      

      
      

      
      

     
     

table     contribution of global feedback and max margin to the learning based inference 
the baseline is the pred by pred  local model in table   

fewer candidate arguments will be eliminated by the meta learner with global rankers  which
translates into a better balance between precision and recall 
another important conclusion of our analysis of global versus local ranking for the
learning based inference is that a max margin approach for the candidate scoring classiers
is more important than having global feedback for inference  in fact  considering that the
only dierence between the model with predicate by predicate inference with local feedback
in section      pred by pred  local  versus the best model in section       fs   is that the
latter uses svm classiers whereas the former uses perceptron  we can compute the exact
contribution of both max margin and global feedback     for convenience  we summarize this
analysis in table     that table indicates that max margin yields a consistent improvement
of both precision and recall  whereas the contribution of global feedback is more in reducing
the dierence between precision and recall by boosting recall and decreasing precision  the
benet of max margin classiers is even more evident in table     which shows that the
local ranking model with max margin classiers generalizes better than the global ranking
model when the amount of training data is reduced signicantly 
even though in this paper we have analyzed several combination approaches with three
independent implementations  the proposed models are in fact compatible with each other 
various combinations of the proposed strategies are immediately possible  for example  the
constraint satisfaction model can be applied on the output probabilities of the candidate
scoring component introduced in section      such a model eliminates the dependency
on the output scores of the individual srl models but retains all the advantages of the
constraint satisfaction model  e g   the formal framework to tune the balance between precision and recall  another possible combination of the approaches introduced in this paper
is to use max margin classiers in the learning based inference with global feedback  e g  
by using a global training method for margin maximization such as svmstruct  tsochantaridis et al          this model would indeed have an increased training time     but could
leverage the advantages of both max margin classiers and inference with global feedback
 summarized in table      finally  another attractive approach is stacking  i e   n levels of chained meta learning  for example  we could cascade the learning based inference
model with global rankers  which boosts recall  with the learning based inference with local
rankers  which favors precision 
    the contribution of global feedback is given by the model with joint inference and global feedback  full
sentence  global  in section     
    this was the main reason why we chose perceptron for the proposed online strategies 

   

fisurdeanu  marquez  carreras    comas

   related work
the   best performing systems at the conll      shared task included a combination of
dierent base subsystems to increase robustness and to gain coverage and independence
from parse errors  therefore  they are closely related to the work of this paper  the rst
four rows in table    summarize their results under exactly the same experimental setting
as the one used in this paper 
koomen et al         used a   layer architecture close to ours  the pool of candidates is
generated by   a  running a full syntax srl system on alternative input information  collins
parsing  and   best trees from charniaks parser   and  b   taking all candidates that pass
a lter from the set of dierent parse trees  the combination of candidates is performed
in an elegant global inference procedure as constraint satisfaction  which  formulated as
integer linear programming  is solved eciently  this is dierent from our work  where
we break complete solutions from any number of srl systems and we also investigate
a meta learning combination approach in addition to the ilp inference  koomen et al s
system was the best performing system at conll       see table     
haghighi et al         implemented double re ranking on top of several outputs from a
base srl model  the re ranking is performed  rst  on a set of n best solutions obtained
by the base system run on a single parse tree  and  then  on the set of best candidates
coming from the n best parse trees  this was the second best system at conll     
 third row in table      compared to our decomposition and re combination approach 
the re ranking setting has the advantage of allowing the denition of global features that
apply to complete candidate solutions  according to a follow up work by the same authors
 toutanova  haghighi    manning         these global features are the source of the major
performance improvements of the re ranking system  in contrast  we focus more on features
that exploit the redundancy between the individual models  e g   overlap between individual
candidate arguments  and we add global information at frame level only from the complete
solutions provided by individual models  the main drawback of re ranking compared to our
approach is that the dierent individual solutions can not be combined because re ranking
is forced to select a complete candidate solution  this implies that its overall performance
strongly depends on the ability of the base model to generate the complete correct solution
in the set of n best candidates  this drawback is evident in the lower performance upper
limit of the re ranking approach  see tables   and     and in the performance of the actual
system our best combination strategy achieves an f  score over   points higher than
haghighi et al  in both wsj and brown    
finally  pradhan  hacioglu  ward  martin  and jurafsky      b  followed a stacking
approach by learning two individual systems based on full syntax  whose outputs are used to
generate features to feed the training stage of a nal chunk by chunk srl system  although
the ne granularity of the chunking based system allows to recover from parsing errors  we
nd this combination scheme quite ad hoc because it forces to break argument candidates
into chunks in the last stage 
    recently  yih and toutanova        reported improved numbers for this system        f  for wsj and
      for brown  however  these numbers are not directly comparable with the systems presented in
this paper because they fixed a significant bug in the representation of quotes in the input data  a bug
that is still present in our data 

   

ficombination strategies for semantic role labeling

outside of the conll shared task evaluation  roth and yih        reached the conclusion that the quality of the local argument classiers is more important than the global
feedback from the inference component  this is also one of the conclusions drawn by this paper  our contribution is that we have shown that this hypothesis holds in a more complex
framework  combination of several state of the art individual models  whereas roth and
yih experimented with a single individual model  only numbered arguments  and a slightly
simplied problem representation  b i o over basic chunks  additionally  our more detailed
experiments allowed us to show clearly that the contribution of max margin is higher than
that of global learning in several corpora and for several combinations of individual systems 
punyakanok  roth  and yih        showed that the performance of individual srl
models  particularly argument identication  is signicantly improved when full parsing is
used and argument boundaries are restricted to match syntactic constituents  similarly to
our model     we believe that the approach used by our models   and    where candidate
arguments do not have to match a single syntactic constituent  has increased robustness
because it has a built in mechanism to handle some syntax errors  when an argument constituent is incorrectly fragmented into multiple phrases  our empirical results support this
claim  model   performs better than both model   and the models proposed by punyakanok
et al  a second advantage of the strategy proposed in this paper is that the same model
can be deployed using full syntax  model    or partial syntax  model    
pradhan  ward  hacioglu  martin  and jurafsky      c  implement a srl combination
strategy at constituent level that  similarly to our approach  combines dierent syntactic
views of the data based on full and partial syntactic analysis  however  unlike our approach 
pradhan et al s work uses only a simple greedy inference strategy based on the probabilities
of the candidate arguments  whereas in this paper we introduce and analyze three dierent
combination algorithms  our analysis yielded a combination system that outperforms the
current state of the art 
previous work in the more general eld of predicting structures in natural language
texts has indicated that the combination of several individual models improves overall performance in the given task  collins        rst proposed a learning layer based on ranking
to improve the performance of a generative syntactic parser  in that approach  a reranker
was trained to select the best solution from a pool of solutions produced by the generative
parser  in doing so  the reranker dealt with complete parse trees  and represented them with
rich features that exploited dependencies not considered in the generative method  on the
other hand  it was computationally feasible to train the reranker  because the base method
reduced the number of possible parse trees for a sentence from an exponential number  w r t 
sentence length  to a few tens  more recently  global discriminative learning methods for
predicting structures have been proposed  laerty  mccallum    pereira        collins 
            taskar et al               tsochantaridis et al          all of them train a
single discriminative ranking function to detect structures in a sentence  a major property
of these methods is that they model the problem discriminatively  so that arbitrary and
rich representations of structures can be used  furthermore  the training process in these
methods is global  in that parameters are set to maximize measures not only related to
local accuracies  i e   on recognizing parts of a structure   but also related to the global
accuracy  i e   on recognizing complete structures   in this article  the use of global and
rich representations is also a major motivation 
   

fisurdeanu  marquez  carreras    comas

    conclusions
this paper introduces and analyzes three combination strategies in the context of semantic
role labeling  the rst model implements an inference strategy with constraint satisfaction
using integer linear programming  the second uses inference based on learning where the
candidates are scored using discriminative classiers using only local information  and the
third and last inference model builds on the previous strategy by adding global feedback
from the conict resolution component to the ranking classiers  the meta learners used
by the inference process are developed with a rich set of features that includes voting
statistics i e   how many individual systems proposed a candidate argument overlap
with arguments from the same and other predicates in the sentence  structure and distance
information coded using partial and full syntax  and probabilities from the individual srl
models  if available   to our knowledge  this is the rst work that   a  introduces a thorough
inference model based on learning for semantic role labeling  and  b  performs a comparative
analysis of several inference strategies in the context of srl 
the results presented suggest that the strategy of decomposing individual solutions and
performing a learning based re combination for constructing the nal solution has advantages over other approaches  e g   re ranking a set of complete candidate solutions  of
course  this is a task dependant conclusion  in the case of semantic role labeling  our approach is relatively simple since the re combination of argument candidates has to fulll
only a few set of structural constraints to generate a consistent solution  if the target structure is more complex  e g   a full parse tree  the re combination step might be too complex
from both the learning and search perspectives 
our evaluation indicates that all proposed combination approaches are successful  they
all provide signicant improvements over the best individual model and several baseline
combination algorithms in all setups  out of the three combination strategies investigated 
the best f  score is obtained by the learning based inference using max margin classiers 
while all the proposed approaches have their own advantages and drawbacks  see section    
for a detailed discussion of dierences among the proposed inference models  several important features of a state of the art srl combination strategy emerge from this analysis 
 i  individual models should be combined at the granularity of candidate arguments rather
than at the granularity of complete solutions or frames   ii  the best combination strategy
uses an inference model based in learning   iii  the learning based inference benets from
max margin classiers and global feedback  and  iv  the inference at sentence level  i e  
considering all predicates at the same time  proves only slightly useful when the learning is
performed also globally  using feedback from the complete solution after inference 
last but not least  the results obtained with the best combination strategy developed
in this work outperform the current state of the art  these results are empirical proof that
a srl system with good performance can be built by combining a small number  three in
our experiments  of relatively simple srl models 

acknowledgments
we would like to thank the jair reviewers for their valuable comments 
this research has been partially supported by the european commission  chil project 
   

ficombination strategies for semantic role labeling

ip         pascal network  ist              and the spanish ministry of education
and science  trangram  tin           c        mihai surdeanu is a research fellow
within the ramon y cajal program of the spanish ministry of education and science  we
are also grateful to dash optimization for the free academic use of xpress mp 

references
bishop  c          neural networks for pattern recognition  oxford university press 
boas  h  c          bilingual framenet dictionaries for machine translation  in proceedings
of lrec      
carreras  x     marquez  l          introduction to the conll      shared task  semantic
role labeling  in proceedings of conll      
carreras  x     marquez  l          introduction to the conll      shared task  semantic
role labeling  in proceedings of conll      
carreras  x   marquez  l     chrupala  g          hierarchical recognition of propositional
arguments with perceptrons  in proceedings of conll      shared task 
charniak  e          a maximum entropy inspired parser  in proceedings of naacl 
collins  m          head driven statistical models for natural language parsing  phd
dissertation  university of pennsylvania 
collins  m          discriminative reranking for natural language parsing  in proceedings
of the   th international conference on machine learning  icml     stanford  ca
usa 
collins  m          discriminative training methods for hidden markov models  theory and
experiments with perceptron algorithms  in proceedings of the sigdat conference
on empirical methods in natural language processing  emnlp    
collins  m          parameter estimation for statistical parsing models  theory and practice of distribution free methods  in bunt  h   carroll  j     satta  g   eds    new
developments in parsing technology  chap     kluwer 
collins  m     duy  n          new ranking algorithms for parsing and tagging  kernels
over discrete structures  and the voted perceptron  in proceedings of the   th annual
meeting of the association for computational linguistics  acl   
crammer  k     singer  y       a   a family of additive online algorithms for category
ranking  journal of machine learning research              
crammer  k     singer  y       b   ultraconservative online algorithms for multiclass
problems  journal of machine learning research            
freund  y     schapire  r  e          large margin classication using the perceptron
algorithm  machine learning                 
gildea  d     jurafsky  d          automatic labeling of semantic roles  computational
linguistics         
   

fisurdeanu  marquez  carreras    comas

gildea  d     palmer  m          the necessity of syntactic parsing for predicate argument
recognition  in proceedings of the   th annual conference of the association for
computational linguistics  acl     
hacioglu  k   pradhan  s   ward  w   martin  j  h     jurafsky  d          semantic
role labeling by tagging syntactic chunks  in proceedings of the  th conference on
computational natural language learning  conll       
haghighi  a   toutanova  k     manning  c          a joint model for semantic role labeling 
in proceedings of conll      shared task 
koomen  p   punyakanok  v   roth  d     yih  w          generalized inference with
multiple semantic role labeling systems  in proceedings of conll      shared task 
laerty  j   mccallum  a     pereira  f          conditonal random elds  probabilistic models for segmenting and labeling sequence data  in proceedings of the   th
international conference on machine learning  icml    
marcus  m   santorini  b     marcinkiewicz  m          building a large annotated corpus
of english  the penn treebank  computational linguistics         
marquez  l   comas  p   gimenez  j     catala  n          semantic role labeling as
sequential tagging  in proceedings of conll      shared task 
melli  g   wang  y   liu  y   kashani  m  m   shi  z   gu  b   sarkar  a     popowich 
f          description of squash  the sfu question answering summary handler
for the duc      summarization task  in proceedings of document understanding
workshop  hlt emnlp annual meeting 
narayanan  s     harabagiu  s          question answering based on semantic structures 
in international conference on computational linguistics  coling       
noreen  e  w          computer intensive methods for testing hypotheses  john wiley  
sons 
palmer  m   gildea  d     kingsbury  p          the proposition bank  an annotated
corpus of semantic roles  computational linguistics         
ponzetto  s  p     strube  m       a   exploiting semantic role labeling  wordnet and
wikipedia for coreference resolution  in proceedings of the human language technolgy
conference of the north american chapter of the association for computational linguistics 
ponzetto  s  p     strube  m       b   semantic role labeling for coreference resolution  in
companion volume of the proceedings of the   th meeting of the european chapter
of the association for computational linguistics 
pradhan  s   hacioglu  k   krugler  v   ward  w   martin  j  h     jurafsky  d       a  
support vector learning for semantic argument classication  machine learning     
     
pradhan  s   hacioglu  k   ward  w   martin  j  h     jurafsky  d       b   semantic role
chunking combining complementary syntactic views  in proceedings of conll      
   

ficombination strategies for semantic role labeling

pradhan  s   ward  w   hacioglu  k   martin  j  h     jurafsky  d       c   semantic role
labeling using dierent syntactic views  in proceedings of the   rd annual conference
of the association for computational linguistics 
punyakanok  v   roth  d     yih  w          the necessity of syntactic parsing for semantic role labeling  in proceedings of the international joint conference on artificial
intelligence  ijcai  
punyakanok  v   roth  d   yih  w     zimak  d          semantic role labeling via integer linear programming inference  in proceedings of the international conference on
computational linguistics  coling    
rosenblatt  f          the perceptron  a probabilistic model for information storage and
organization in the brain  psychological review             
roth  d     yih  w          a linear programming formulation for global inference in
natural language tasks  in proceedings of the annual conference on computational
natural language learning  conll        pp      boston  ma 
roth  d     yih  w          integer linear programming inference for conditional random
elds  in proceedings of the international conference on machine learning  icml  
schapire  r  e     singer  y          improved boosting algorithms using condence rated
predictions  machine learning         
surdeanu  m   harabagiu  s   williams  j     aarseth  p          using predicate argument
structures for information extraction  in proceedings of the   st annual meeting of
the association for computational linguistics  acl       
taskar  b   guestrin  c     koller  d          max margin markov networks  in proceedings
of the   th annual conference on neural information processing systems  nips    
vancouver  canada 
taskar  b   klein  d   collins  m   koller  d     manning  c          max margin parsing 
in proceedings of the emnlp      
toutanova  k   haghighi  a     manning  c          joint learning improves semantic role
labeling  in proceedings of the   rd annual meeting of the association for computational linguistics  acl     pp          ann arbor  mi  usa  association for
computational linguistics 
tsochantaridis  i   hofmann  t   joachims  t     altun  y          support vector machine
learning for interdependent and structured output spaces  in proceedings of the   st
international conference on machine learning  icml    
xue  n     palmer  m          calibrating features for semantic role labeling  in proceedings
of emnlp      
yih  s  w     toutanova  k          automatic semantic role labeling  in tutorial of
the human language technolgy conference of the north american chapter of the
association for computational linguistics 
younger  d  h          recognition and parsing of context free languages in n  time 
information and control                 

   

fi