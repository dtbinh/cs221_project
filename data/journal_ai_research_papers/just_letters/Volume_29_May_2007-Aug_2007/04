journal artificial intelligence research                  

submitted        published      

combination strategies semantic role labeling
mihai surdeanu
llus marquez
xavier carreras
pere r  comas

surdeanu lsi upc edu
lluism lsi upc edu
carreras lsi upc edu
pcomas lsi upc edu

technical university catalonia 
c  jordi girona     
      barcelona  spain

abstract
paper introduces analyzes battery inference models problem semantic role labeling  one based constraint satisfaction  several strategies model
inference meta learning problem using discriminative classiers  classiers
developed rich set novel features encode proposition sentence level
information  knowledge  rst work that   a  performs thorough analysis learning based inference models semantic role labeling   b  compares several
inference strategies context  evaluate proposed inference strategies
framework conll      shared task using automatically generated syntactic
information  extensive experimental evaluation analysis indicates
proposed inference strategies successful outperform current best results
reported conll      evaluation exercise proposed approaches
advantages disadvantages  several important traits state of the art srl combination strategy emerge analysis   i  individual models combined
granularity candidate arguments rather granularity complete solutions 
 ii  best combination strategy uses inference model based learning   iii 
learning based inference benets max margin classiers global feedback 

   introduction
natural language understanding  nlu  subeld articial intelligence  ai 
deals extraction semantic information available natural language texts 
knowledge used develop high level applications requiring textual document
understanding  question answering information extraction  nlu complex
ai complete problem needs venture well beyond syntactic analysis natural
language texts  state art nlu still far reaching goals  recent
research made important progress subtask nlu  semantic role labeling 
task semantic role labeling  srl  process detecting basic event structures
whom  where  see figure   sample sentence annotated
event frame 
    motivation
srl received considerable interest past years  gildea   jurafsky       
surdeanu  harabagiu  williams    aarseth        xue   palmer        pradhan  hac
    
ai access foundation  rights reserved 

fisurdeanu  marquez  carreras    comas

cioglu  krugler  ward  martin    jurafsky      a  carreras   marquez        
shown identication event frames signicant contribution many
nlu applications information extraction  surdeanu et al          question answering  narayanan   harabagiu         machine translation  boas         summarization  melli  wang  liu  kashani  shi  gu  sarkar    popowich         coreference
resolution  ponzetto   strube      b      a  
syntactic perspective  machine learning srl approaches classied
one two classes  approaches take advantage complete syntactic analysis text 
pioneered gildea jurafsky         approaches use partial syntactic analysis 
championed previous evaluations performed within conference computational
natural language learning  conll   carreras   marquez               wisdom
extracted rst representation indicates full syntactic analysis signicant
contribution srl performance  using hand corrected syntactic information  gildea
  palmer         hand  automatically generated syntax available 
quality information provided full syntax decreases state ofthe art full parsing less robust performs worse tools used partial
syntactic analysis  real world conditions  dierence two srl
approaches  with full partial syntax  high  interestingly  two srl
strategies perform better different semantic roles  example  models use full
syntax recognize agent theme roles better  whereas models based partial syntax
better recognizing explicit patient roles  tend farther predicate
accumulate parsing errors  marquez  comas  gimenez    catala        
    approach
article explore implications observations studying strategies
combining output several independent srl systems  take advantage
dierent syntactic views text  given sentence  combination models receive
labeled arguments individual systems  produce overall argument structure
corresponding sentence  proposed combination strategies exploit several levels
information  local global features  from individual models  constraints
argument structure  work  investigate three dierent approaches 
rst combination model parameters estimate  makes use
argument probabilities output individual models constraints argument
structures build overall solution sentence  call model inference
constraint satisfaction 
second approach implements cascaded inference model local learning  rst 
type argument  classier trained oine decides whether candidate
nal argument  next  candidates passed previous step
combined solution consistent constraints argument structures 
refer model inference local learning 
third inference model global  number online ranking functions  one
argument type  trained score argument candidates correct
argument structure complete sentence globally ranked top  call
model inference global learning 
   

ficombination strategies semantic role labeling



np

np

vp
np

pp

luxury auto maker last year sold       cars u s 
a 
agent

amtmp
temporal
marker

p

a 
predicate object

amloc
locative
marker

figure    sample sentence propbank corpus 
proposed combination strategies general depend way
candidate arguments collected  empirically prove experimenting
individual srl systems developed house     best systems
conll      shared task evaluation 
    contribution
work introduced paper several novel points  knowledge 
rst work thoroughly explores inference model based meta learning  the
second third inference models introduced  context srl  investigate metalearning combination strategies based rich  global representations form local
global features  form structural constraints solutions  empirical
analysis indicates combination strategies outperform current state
art  note combination strategies proposed paper re ranking
approaches  haghighi  toutanova    manning        collins         whereas re ranking
selects overall best solution pool complete solutions individual models 
combination approaches combine candidate arguments  incomplete solutions 
different individual models  show approach better potential  i e   upper
limit f  score higher performance better several corpora 
second novelty paper performs comparative analysis several combination strategies srl  using framework i e   pool
candidates evaluation methodology  large number combination
approaches previously analyzed context srl larger context
predicting structures natural language texts e g   inference based constraint satisfaction  koomen  punyakanok  roth    yih        roth   yih         inference based
local learning  marquez et al          re ranking  collins        haghighi et al         etc 
still clear strategy performs best semantic role labeling  paper
   

fisurdeanu  marquez  carreras    comas

provide empirical answers several important questions respect  example 
combination strategy based constraint satisfaction better inference model based
learning  or  important global feedback learning based inference model 
analysis indicates following issues important traits state of the art
combination srl system   i  individual models combined argument granularity
rather granularity complete solutions  typical re ranking    ii  best
combination strategy uses inference model based learning   iii  learning based
inference benets max margin classiers global feedback 
paper organized follows  section   introduces semantic corpora used
training evaluation  section   overviews proposed combination approaches 
individual srl models introduced section   evaluated section    section  
lists features used three combination models introduced paper 
combination models described section    section   introduces empirical analysis proposed combination methods  section   reviews related work
section    concludes paper 

   semantic corpora
paper used propbank  approximately one million word corpus annotated
predicate argument structures  palmer  gildea    kingsbury         date  propbank addresses predicates lexicalized verbs  besides predicate argument structures 
propbank contains full syntactic analysis sentences  extends wall street
journal  wsj  part penn treebank  corpus previously annotated
syntactic information  marcus  santorini    marcinkiewicz        
given predicate  survey carried determine predicate usage  and 
required  usages divided major senses  however  senses divided
syntactic grounds semantic  following assumption syntactic frames
direct reection underlying semantics  arguments predicate numbered sequentially a  a   generally  a  stands agent  a  theme direct
object  a  indirect object  benefactive instrument  semantics tend verb
specic  additionally  predicates might adjunctive arguments  referred ams 
example  am loc indicates locative am tmp indicates temporal  figure   shows
sample propbank sentence one predicate  sold    arguments  regular
adjunctive arguments discontinuous  case trailing argument fragments
prexed c   e g    a  funds   predicate expected   ca  begin operation
around march     finally  propbank contains argument references  typically pronominal  
share label actual argument prexed r   
paper use syntactic information penn treebank  instead 
develop models using automatically generated syntax named entity  ne  labels 
made available conll      shared task evaluation  carreras   marquez        
conll data  use syntactic trees generated charniak parser  char   original propbank annotations  co referenced arguments appear single item  differentiation referent reference  use version data used conll
shared tasks  reference arguments automatically separated corresponding referents
simple pattern matching rules 

   

ficombination strategies semantic role labeling

niak        develop two individual models based full syntactic analysis  chunk
i e   basic syntactic phrase labels clause boundaries construct partial syntax
model  individual models use provided ne labels 
switching hand corrected automatically generated syntactic information means
propbank assumption argument  or argument fragment discontinuous arguments  maps one syntactic phrase longer holds  due errors syntactic
processors  analysis propbank data indicates        semantic
arguments matched exactly one phrase generated charniak parser  essentially  means srl approaches make assumption semantic
argument maps one syntactic construct recognize almost    arguments 
statement made approaches based partial syntax caveat
setup arguments match sequence chunks  however  one expects
degree compatibility syntactic chunks semantic arguments higher
due ner granularity syntactic elements chunking algorithms perform better full parsing algorithms  indeed  analysis propbank data
supports observation         semantic arguments matched sequence
chunks generated conll syntactic chunker 
following conll      setting evaluated system propbank
fresh test set  derived brown corpus  second evaluation allows us
investigate robustness proposed combination models 

   overview combination strategies
paper introduce analyze three combination strategies problem
semantic role labeling  three combination strategies implemented shared
framework detailed figure   consists several stages   a  generation candidate arguments   b  candidate scoring  nally  c  inference  clarity  describe
rst proposed combination framework  i e   vertical ow figure    then  move
overview three combination methodologies  shown horizontally figure   
candidate generation step  merge solutions three individual srl models
unique pool candidate arguments  individual srl models range complete
reliance full parsing using partial syntactic information  example  model  
developed sequential tagger  using b i o tagging scheme  partial
syntactic information  basic phrases clause boundaries   whereas model   uses full
syntactic analysis text handles arguments map exactly one syntactic
constituent  detail individual srl models section   empirically evaluate
section   
candidate scoring phrase  re score candidate arguments using local
information  e g   syntactic structure candidate argument  global information 
e g   many individual models generated similar candidate arguments  describe
features used candidate scoring section   
finally  inference stage combination models search best solution
consistent domain constraints  e g   two arguments predicate cannot
overlap embed  predicate may one core argument  a      etc 
   

fisurdeanu  marquez  carreras    comas

reliance full syntax

model  

model  

model  

candidate
generation
candidate argument
pool

constraint
satisfaction
engine

solution

inference
constraint satisfaction

learning
 batch 

learning
 online 

dynamic
programming
engine

dynamic
programming
engine

solution

candidate
scoring

inference

solution

inference
local learning

inference
global learning

figure    overview proposed combination strategies 

combination approaches proposed paper share candidate argument pool  guarantees results obtained dierent strategies
corpus comparable  hand  even though candidate generation
step shared  three combination methodologies dier signicantly scoring
inference models 
rst combination strategy analyzed  inference constraint satisfaction  skips
candidate scoring step completely uses instead probabilities output individual srl models candidate argument  individual models raw activations
actual probabilities convert probabilities using softmax function  bishop 
       passing inference component  inference implemented using
constraint satisfaction model searches solution maximizes certain
compatibility function  compatibility function models probability
global solution consistency solution according domain constraints 
combination strategy based technique presented koomen et al         
main dierence two systems candidate generation step  use
three independent individual srl models  whereas komen et al  used srl model
   

ficombination strategies semantic role labeling

trained dierent syntactic views data  i e   top parse trees generated
charniak collins parsers  charniak        collins         furthermore  take
argument candidates set complete solutions generated individual models  whereas komen et al  take dierent syntactic trees  constructing
complete solution  obvious advantage inference model constraint satisfaction unsupervised  learning necessary candidate scoring 
scores individual models used  hand  constraint satisfaction
model requires individual models provide raw activations  and  moreover 
raw activations convertible true probabilities 
second combination strategy proposed article  inference local learning 
re scores candidates pool using set binary discriminative classiers 
classiers assign argument score measuring condence argument
part correct  global solution  classiers trained batch mode
completely decoupled inference module  inference component implemented
using cky based dynamic programming algorithm  younger         main advantage
strategy candidates re scored using signicantly information
available individual model  example  incorporate features count
number individual systems generated given candidate argument  several
types overlaps candidate arguments predicate arguments
predicates  structural information based full partial syntax  etc 
describe rich feature set used scoring candidate arguments section    also 
combination approach depend argument probabilities individual
srl models  but incorporate features  available   combination approach
complex previous strategy additional step requires
supervised learning  candidate scoring  nevertheless  mean additional
corpus necessary  using cross validation  candidate scoring classiers trained
corpus used train individual srl models  moreover  show section  
obtain excellent performance even candidate scoring classiers trained
signicantly less data individual srl models 
finally  inference strategy global learning investigates contribution global
information inference model based learning  strategy incorporates global
information previous inference model two ways  first importantly  candidate scoring trained online global feedback inference component 
words  online learning algorithm corrects mistakes found comparing
correct solution one generated inference  second  integrate global information actual inference component  instead performing inference proposition
independently  whole sentence once  allows implementation
additional global domain constraints  e g   arguments attached dierent predicates
overlap 
combination strategies proposed described detail section   evaluated
section   
   

fisurdeanu  marquez  carreras    comas

   individual srl models
section introduces three individual srl models used combination strategies discussed paper  rst two models variations algorithm 
model srl problem sequential tagging task  semantic argument
matched sequence non embedding phrases  model   uses partial syntax
 chunks clause boundaries   whereas model   uses full syntax  third model takes
traditional approach assuming exists one to one mapping
semantic arguments syntactic phrases 
important note combination strategies introduced later paper
independent individual srl models used  fact  section   describe
experiments use individual models best performing srl
systems conll      evaluation  carreras   marquez         nevertheless 
choose focus mainly individual srl approaches presented section
completeness show state of the art performance possible relatively simple
srl models 
    models    
models approach srl sequential tagging task  pre processing step 
input syntactic structures traversed order select subset constituents organized
sequentially  i e   non embedding   output process sequential tokenization
input sentence verb predicates  labeling tokens appropriate
tags allows us codify complete argument structure predicate sentence 
precisely  given verb predicate  sequential tokens selected follows 
first  input sentence split disjoint sequential segments using markers
segment start end verb position boundaries clauses include
corresponding predicate constituent  second  segment  set top most
non overlapping syntactic constituents completely falling inside segment selected
tokens  finally  tokens labeled b i o tags  depending
beginning  inside  outside predicate argument  note strategy provides set
sequential tokens covering complete sentence  also  independent syntactic
annotation explored  assuming provides clause boundaries 
consider example figure    depicts propbank annotation two verb
predicates sentence  release hope  corresponding partial full parse
trees  since verbs main clause sentence  two segments
sentence considered predicates  i e   dening left right contexts
verbs   w   others       w   just   w   from       w    big time  predicate release 
 w   others       w       w    the       w    big time  predicate hope   figure  
shows resulting tokenization predicates two alternative syntactic structures  case  correct argument annotation recovered cases  assuming
perfect labeling tokens 
worth noting resulting number tokens annotate much lower
number words cases  also  codications coming full parsing
substantially fewer tokens coming partial parsing  example 
predicate hope  dierence number tokens two syntactic views
   

ficombination strategies semantic role labeling

clause
np



 

vp
clause

vp
 

np

 

advp

 

ii

np

pp

vp

 

 

 

 

others  

released majors   hope senior league

bridge back bigtime 

clause
 

 

np



 

advp

    ii
others  

a 

amtmp

iii

vp

iv  

pp

v

 

np

vi

 

vp

np

 
  vii
released majors   hope senior league
p
a 

clause

vp

np

viii

advp pp

np

bridge back bigtime 

a 
p

a 

figure    annotation example sentence two alternative syntactic structures 
lower tree corresponds partial parsing annotation  pp  base chunks
clause structure  upper represents full parse tree  fp   semantic roles
two predicates  release hope  provided sentence 
encircled nodes trees correspond selected nodes process
sequential tokenization sentence  mark selected nodes
predicate release western numerals nodes selected hope
roman numerals  see figure   details 

particularly large    vs    tokens   obviously  coarser token granularity  easier
problem assigning correct output labelings  i e   less tokens label
long distance relations among sentence constituents better captured  
hand  coarser granularity tends introduce unrecoverable errors
pre processing stage  clear trade o  dicult solve advance 
using two models combination scheme take advantage diverse sentence
tokenizations  see sections      
compared common tree node labeling approaches  e g   following
model     b i o annotation tokens advantage permitting correctly annotate arguments match unique syntactic constituent  bad side 
heuristic pre selection candidate nodes predicate  i e   nodes
sequentially cover sentence  makes number unrecoverable errors higher  another source errors common strategies errors introduced real partial full
parsers  calculated due syntactic errors introduced pre processing
stage  upper bound recall gures        model          model   using
datasets dened section   
   

fisurdeanu  marquez  carreras    comas

words
   others
    
  
   released
  
  
   majors
    
   hope
   
    senior
    league
   
   
   
    bridge
    back
   
   
    big time

releasepp
   b a 
  
   b am tmp

   b a 
   a 
  
  

tokens
releasefp
hopepp
   b a 
i  b a 
  
ii  a 
   b am tmp
iii  a 

iv  a 
v  a 
   b a 
vi  a 
  

hopefp

i  b a 

vii  a 




viii  b a 

ii  b a 

  
  

figure    sequential tokenization sentence figure   according two syntactic
views predicates  pp stands partial parsing fp full parsing  
sentence semantic role annotations vertically displayed  token
numbered indexes appear tree nodes figure   contains
b i o annotation needed codify proper semantic role structure 

approaching srl sequential tagging task new  hacioglu  pradhan  ward 
martin  jurafsky        presented system based sequential tagging base chunks
b i o labels  best performing srl system conll      shared
task  carreras   marquez         novelty approach resides fact
sequence syntactic tokens label extracted hierarchical syntactic annotation
 either partial full parse tree  restricted base chunks  i e   token
may correspond complex syntactic phrase even clause  
      features
tokens selected labeled b i o tags  converted training
examples considering rich set features  mainly borrowed state of the art systems  gildea   jurafsky        carreras  marquez    chrupala        xue   palmer 
       features codify properties from   a  focus token   b  target predicate 
 c  sentence fragment token predicate   d  dynamic context 
i e   b i o labels previously generated  describe four feature sets next  
   features extracted partial parsing named entities common model      features
coming full parse trees apply model   

   

ficombination strategies semantic role labeling

constituent structure features 
constituent type head  extracted using head word rules collins        
rst element pp chunk  head rst np extracted 
example  type constituent u s  figure   pp  head
u s  instead in 
first last words pos tags constituent  e g   in in u s  nnp
constituent u s  figure   
pos sequence  less   tags long  e g   indtnnp sample
constituent 
      grams pos sequence 
bag of words nouns  adjectives  adverbs  example  bag of nouns
constituent luxury auto maker  luxury  auto  maker  
top sequence  sequence types top most syntactic elements constituent
 if less   elements long   case full parsing corresponds
right hand side rule expanding constituent node  example  top
sequence constituent u s  innp 
      grams top sequence 
governing category described gildea jurafsky         indicates np
arguments dominated sentence  typical subjects  verb phrase  typical
objects   example  governing category constituent       cars
figure   vp  hints corresponding semantic role object 
namedentity  indicating constituent embeds strictly matches named entity
along type  example  constituent u s  embeds locative
named entity  u s  
tmp  indicating constituent embeds strictly matches temporal keyword
 automatically extracted am tmp arguments training set   among
common temporal cue words extracted are  year  yesterday  week  month 
etc  used total     cue words 
previous following words pos tag constituent  example 
previous word constituent last year figure   maker nn  next
one sold vbd 
features characterizing focus constituents extracted two previous
following tokens  provided inside boundaries current segment 
predicate structure features 
predicate form  lemma  pos tag  e g   sold  sell  vbd predicate
figure   
chunk type cardinality verb phrase verb included  single word
multi word  example  predicate figure   included single word vp
chunk 
   

fisurdeanu  marquez  carreras    comas

predicate voice  distinguish voice types  active  passive  copulative 
innitive  progressive 
binary ag indicating verb start end clause 
sub categorization rule  i e   phrase structure rule expands predicates
immediate parent  e g   np np vp predicate figure   
predicate constituent features 
relative position  distance words chunks  level embedding  in number
clause levels  respect constituent  example  constituent
u s  figure   appears predicate  distance   words   chunk 
level embedding   
constituent path described gildea jurafsky              grams
path constituents beginning verb predicate ending constituent 
example  syntactic path constituent luxury auto maker
predicate sold figure   np vp vbd 
partial parsing path described carreras et al               grams path
elements beginning verb predicate ending constituent  example 
path np   pp   np   vp vbd indicates current np token
predicate pp  np  constituents right  positive sign 
level token path descends clause vp
nd predicate  dierence previous constituent path
arrows anymore introduce horizontal  left right  movements
syntactic level 
syntactic frame described xue palmer         syntactic frame captures
overall sentence structure using predicate constituent pivots 
example  syntactic frame predicate sold constituent
u s  npnpvpnppp  current predicate constituent emphasized 
knowing noun phrases predicate lowers probability
constituent serves agent  or a   
dynamic features 
biotag previous token  training  correct labels left context
used  testing  feature dynamically codied tag previously
assigned srl tagger 
      learning algorithm sequence tagging
used generalized adaboost real valued weak classiers  schapire   singer       
base learning algorithm  version algorithm learns xed depth small decision
trees weak rules  combined ensemble constructed adaboost 
implemented simple one vs all decomposition address multi class classication 
way  separate binary classier learned b x i x argument label
plus extra classier decision 
   

ficombination strategies semantic role labeling

adaboost binary classiers used labeling test sequences  left right 
using recurrent sliding window approach information tags assigned
preceding tokens  explained previous list features  left tags already assigned
dynamically codied features  empirically  found optimal left context
taken account reduces previous token 
tested two dierent tagging procedures  first  greedy left to right assignment
best scored label token  second  viterbi search label sequence
maximizes probability complete sequence  case  classiers predictions
converted probabilities using softmax function described section     
signicant improvements obtained latter  selected former 
faster  basic tagging algorithm experiments 
finally  tagging model enforces three basic constraints   a  b i o output labeling must codify correct structure   b  arguments cannot overlap clause chunk
boundaries   c  verb  a    arguments present propbank frames  taking
union rolesets dierent verb senses  considered 
    model  
third individual srl model makes strong assumption predicate argument
maps one syntactic constituent  example  figure   a  maps noun phrase 
am loc maps prepositional phrase  etc  assumption holds well hand corrected
parse trees simplies signicantly srl process one syntactic constituent correctly classied order recognize one semantic argument 
hand  approach limited using automatically generated syntactic trees 
example         arguments mapped one syntactic constituents
produced charniak parser 
using bottom up approach  model   maps argument rst syntactic constituent exact boundaries climbs high possible
tree across unary production chains  currently ignore arguments map
single syntactic constituent  argument constituent mapping performed
training set preprocessing step  figure   shows mapping example semantic
arguments one verb corresponding sentence syntactic structure 
mapping process completes  model   extracts rich set lexical  syntactic 
semantic features  features inspired previous work parsing
srl  collins        gildea   jurafsky        surdeanu et al         pradhan et al  
    a   describe complete feature set implemented model   next 
      features
similarly models     group features three categories  based properties
codify   a  argument constituent   b  target predicate   c  relation
constituent predicate syntactic constituents 
constituent structure features 
syntactic label candidate constituent 

constituent head word  suffixes length          lemma  pos tag 
   

fisurdeanu  marquez  carreras    comas

constituent content word  suffixes length          lemma  pos tag  ne
label  content words  add informative lexicalized information dierent
head word  detected using heuristics surdeanu et al          example 
head word verb phrase placed auxiliary verb had  whereas
content word placed  similarly  content word prepositional phrases
preposition  which selected head word   rather head
word attached phrase  e g   u s  prepositional phrase u s  
first last constituent words pos tags 
ne labels included candidate phrase 
binary features indicate presence temporal cue words  i e   words appear
often am tmp phrases training  used list temporal cue words
models     
treebank syntactic label added feature indicate number
labels included candidate phrase 
top sequence constituent  constructed similarly model    
phrase label  head word pos tag constituent parent  left sibling 
right sibling 
predicate structure features 
predicate word lemma 
predicate voice  denition models     
binary feature indicate predicate frequent  i e   appears
twice training data  not 
sub categorization rule  denition models     
predicate constituent features 
path syntactic tree argument phrase predicate
chain syntactic labels along traversal direction  up down  
computed similarly model   
length syntactic path 
number clauses  s  phrases  path  store overall clause count
number clauses ascending descending part path 
number verb phrases  vp  path  similarly feature  store
three numbers  overall verb count  verb count ascending descending
part path 
generalized syntactic paths  generalize path syntactic tree 
appears   elements  using two templates   a  arg ancestor ni
pred  arg argument label  pred predicate label  ancestor
label common ancestor  ni instantiated labels
   

ficombination strategies semantic role labeling

pred ancestor full path   b  arg ni ancestor pred  ni
instantiated labels arg ancestor full path 
example  path np vp sbar vp argument label rst np 
predicate label last vp  common ancestors label rst s  hence 
using last template  path generalized following three features  np
vp vp  np sbar vp  np vp  generalization reduces
sparsity complete constituent predicate path feature using dierent strategy
models      implement n gram based approach 
subsumption count  i e   dierence depths syntactic tree
argument predicate constituents  value   two phrases share
parent 
governing category  similar models     

surface distance predicate argument phrases encoded as 
number tokens  verb terminals  vb    commas  coordinations  cc  argument predicate phrases  binary feature indicate two
constituents adjacent  example  surface distance argument
candidate others predicate hope figure   example  others 
released majors  hope senior league      tokens    verb    commas 
  coordinations  features  originally proposed collins        dependency parsing model  capture robust  syntax independent information
sentence structure  example  constituent unlikely argument
verb another verb appears two phrases 

binary feature indicate argument starts predicate particle  i e  
token seen rp  pos tag directly attached predicate training 
motivation feature avoid inclusion predicate particles
argument constituent  example  without feature  srl system tend
incorrectly include predicate particle argument text  take  a 
organization   marked text commonly incorrectly parsed
prepositional phrase large number prepositional phrases directly attached
verb arguments corresponding predicate 
      classifier
similarly models      model   trains one vs all classiers using adaboost
common argument labels  reduce sample space  model   selects training examples
 both positive negative  from   a  rst clause includes predicate 
 b  phrases appear left predicate sentence     
argument constituents fall one classes 
prediction time classiers combined using simple greedy technique
iteratively assigns predicate argument classied highest condence 
predicate consider candidates attributes  numbered attributes
indicated corresponding propbank frame  additionally  greedy strategy enforces
limited number domain knowledge constraints generated solution   a  arguments
overlap form   b  duplicate arguments allowed a      c 
   

fisurdeanu  marquez  carreras    comas

predicate numbered arguments  i e   a     subset present
propbank frame  constraints somewhat dierent constraints used
models       i  model   use b i o representation hence constraint
b i o labeling correct apply   ii  models     enforce
constraint numbered arguments duplicated implementation
straightforward architecture 

   performance individual models
section analyze performance three individual srl models proposed 
three srl systems trained using complete conll      training set  propbank treebank sections        avoid overtting syntactic processors i e  
part of speech tagger  chunker  charniaks full parser partitioned propbank
training set folds fold used output syntactic processors
trained four folds  models tuned separate development partition  treebank section     evaluated two corpora   a  treebank section
    consists wall street journal  wsj  documents   b  three sections
brown corpus  semantically annotated propbank team conll      shared
task evaluation 
classiers individual models developed using adaboost decision trees depth    i e   branch may represent conjunction   basic
features   classication model trained       rounds  applied
simplications keep training times memory requirements inside admissible bounds 
 a  trained frequent argument labels  top    model    top   
model    top    model     b  discarded features occurring less   
times training set   c  model   classier  limited number
negative training samples rst         negative samples extracted propbank
traversal   
table   summarizes results three models wsj brown corpora 
include percentage perfect propositions detected model  pprops  
i e   predicates recognized arguments  overall precision  recall  f 
measure    results summarized table   indicate individual systems
solid performance  although none would rank top   conll     evaluation  carreras   marquez         performance comparable best
individual systems presented evaluation exercise    consistently systems
evaluated brown corpus  models experience severe performance drop
corpus  due lower performance linguistic processors 
expected  models based full parsing       perform better model
based partial syntax  but  interestingly  dierence large  e g   less   points
   distribution samples model   classifiers biased towards negative samples because 
worst case  syntactic constituent sentence predicate potential argument 
   significance intervals f  measure obtained using bootstrap resampling  noreen 
       f  rates outside intervals assumed significantly different related f 
rate  p         
   best performing srl systems conll combination several subsystems  see section  
details 

   

ficombination strategies semantic role labeling

wsj
model  
model  
model  
brown
model  
model  
model  

pprops
      
      
      

precision
      
      
      

recall
      
      
      

f 
         
         
         

      
      
      

      
      
      

      
      
      

         
         
         

table    overall results individual models wsj brown test sets 
model   f 
model   f 
model   f 

a 
     
     
     

a 
     
     
     

a 
     
     
     

a 
     
     
     

a 
     
     
     

table    f  scores individual systems a   arguments wsj test 
f  wsj corpus   evincing base syntactic chunks clause boundaries
enough obtain competitive performance  importantly  full parsing models
always better partial syntax model  table   lists f  measure three
models rst numbered arguments  table   shows model    overall
best performing individual system  achieves best f measure a  a   typically
subjects direct objects   model    partial syntax model  performs best
a   typically indirect objects  instruments  benefactives   explanation
behavior indirect objects tend farther predicates accumulate
parsing errors  models based full syntax  model   better recall
whereas model   better precision  model   lters candidate arguments
match single syntactic constituent  generally  table   shows models
strong weak points  justication focus combination
strategies combine several independent models 

   features combination models
detailed section    paper analyze two classes combination strategies
problem semantic role labeling   a  inference model constraint satisfaction 
nds set candidate arguments maximizes global cost function   b 
two inference strategies based learning  candidates scored ranked using
discriminative classiers  perspective feature space  main dierence
two types combination models input rst combination strategy limited argument probabilities produced individual systems 
whereas last class combination approaches incorporates much larger feature set
ranking classiers  robustness  paper use features extracted solutions provided individual systems  hence independent
   

fisurdeanu  marquez  carreras    comas

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
a 

a 

v

v

a 

a 

v

a 

a 

a 

m 

m 

m 

figure    sample solutions proposed predicate three individual srl models 
m   m  m   argument candidates displayed vertically system 

individual models    describe features next  examples given section
based figures     
voting features features quantify votes received argument
individual systems  set includes following features 
label candidate argument  e g   a  rst argument proposed system
m  figure   
number systems generated argument label span 
example shown figure    feature value   argument a  proposed
m    m s a   system m  proposed argument 
unique ids systems generated argument label
span  e g   m  m  argument a  proposed m  m  figure   
argument sequence predicate systems generated argument label span  example  argument sequence generated
system m  proposition illustrated figure   is  a    v   a    a   feature attempts capture information proposition level  e g   combination model
might learn trust model m  argument sequence a    v   a    a  
m  another sequence  etc 
same predicate overlap features features measure overlap dierent
arguments produced individual srl models predicate 
   exception argument probabilities  required constraint satisfaction model 

   

ficombination strategies semantic role labeling

number unique ids systems generated argument
span different label  example shown figure    features
values   m  argument a  proposed m   model m  proposed
argument a  span 
number unique ids systems generated argument included
current argument  candidate argument a  proposed model m 
figure    features values   m   m  generated argument a  
included m s a  
spirit  generate number unique ids systems
generated argument contains current argument  number
unique ids systems generated argument overlaps
include contain current argument 
other predicate overlap features features quantify overlap dierent arguments produced individual srl models predicates  generate
features previous feature group  dierence compare arguments generated dierent predicates  motivation overlap features that 
according propbank annotations  form overlap allowed among arguments
attached predicate  inclusion containment permitted
arguments assigned dierent predicates  overlap features meant detect
domain constraints satised candidate argument  indication 
evidence strong  candidate incorrect 
partial syntax features features codify structure argument
distance argument predicate using partial syntactic information 
i e   chunks clause boundaries  see figure   example   note features
inherently dierent features used model    model   evaluates
individual chunk part candidate argument  whereas codify properties
complete argument constituent  describe partial syntax features below 
length tokens chunks argument constituent  e g       argument
a  figure   
sequence chunks included argument constituent  e g   pp np
argument am loc figure    chunk sequence large  store n grams
length    start end sequence 
sequence clause boundaries  i e   clause beginning ending  included
argument constituent 
named entity types included argument constituent  e g   location
am loc argument figure   
position argument  before after predicate sentence  e g   a 
figure   
boolean ag indicate argument constituent adjacent predicate 
e g   false a  true a  figure   
   

fisurdeanu  marquez  carreras    comas

clause

np

np

vp

np

pp

np

luxury auto maker last year sold       cars u s 
a 

amtmp

p

a 

amloc

figure    sample proposition partial syntactic information 
sequence chunks argument constituent predicate  e g  
chunk sequence predicate argument am loc figure   is  np 
similarly chunk sequence feature  sequence large  store
starting ending n grams 
number chunks predicate argument  e g     am loc
figure   
sequence clause boundaries argument constituent predicate 
clause subsumption count  i e   dierence depths clause
tree argument predicate constituents  value   two phrases
included clause 
full syntax features features codify structure argument constituent 
predicate  distance two using full syntactic information 
full syntax features replicated model    see section       assumes
one to one mapping semantic constituents syntactic phrases exists  unlike model  
ignores arguments matched syntactic constituent 
exact mapping exist due inclusion candidates models     
generate approximate mapping unmapped semantic constituent largest
phrase included given span left boundary semantic constituent  heuristic guarantees capture least semantic
constituents syntactic structure 
motivation partial full syntax features learn preferences
individual srl models  example  features combination classier might
learn trust model m  arguments closer   chunks predicate  model
m  predicate argument syntactic path np vp sbar vp  etc 
individual systems argument probabilities individual model outputs condence score proposed arguments  scores converted probabilities using softmax function described detail section      combination
strategy based constraint satisfaction  section      uses probabilities are 
two strategies based meta learning  section      discretize
probabilities include features  so  probability value matched
   

ficombination strategies semantic role labeling

one probability intervals corresponding interval used feature 
probability intervals dynamically constructed argument label individual system corresponding system predictions argument label
uniformly distributed across intervals 
section     empirically analyze contribution proposed feature
sets performance best combination model 

   combination strategies
section detail combination strategies proposed paper   a  combination
model constraint satisfaction  aims nding set candidate arguments
maximizes global cost function   b  two combination models inference based
learning  candidates scored ranked using discriminative classiers 
previous section described complete feature set made available approaches 
focus machine learning paradigm deployed combination
models 
    inference constraint satisfaction
constraint satisfaction model selects subset candidate arguments maximizes
compatibility function subject fulllment set structural constraints
ensure consistency solution  compatibility function based probabilities
given individual srl models candidate arguments  work use integer
linear programming solve constraint satisfaction problem  approach rst
proposed roth yih        applied semantic role labeling punyakanok 
roth  yih  zimak         koomen et al          among others  follow setting
komen et al   taken reference 
rst step  scores model normalized probabilities  scores
yielded classiers signed unbounded real numbers  experimental evidence
shows condence predictions  taken absolute value raw scores 
correlates well classication accuracy  thus  softmax function  bishop       
used convert set unbounded scores probabilities  k possible
output labels given argument sco li   denotes score label li output
xed srl model  estimated probability label is 
esco li  
p li     pk
sco lj  
j   e
parameter formula empirically adjusted avoid overly skewed
probability distributions normalize scores three individual models
similar range values  see details experimental setting section     
candidate selection performed via integer linear programming  ilp   program
goal maximize compatibility function modeling global condence selected
set candidates  subject set linear constraints  variables involved
task take integer values may appear rst degree polynomials only 
abstract ilp process described simple fashion as  given set variables v    v            vn    aims maximize global compatibility label assignment
   

fisurdeanu  marquez  carreras    comas

 l            ln   variables  local compatibility function cv  l  denes compatibility
assigning label l variable v  global compatibility function c l            ln   taken
sum local assignment compatibility  goal ilp process
written as 
argmax c l            ln     argmax
l       ln

l       ln

n
x

cvi  li  

i  

constraints described set accompanying integer linear equations involving variables problem 
one wants codify soft constraints instead hard  possibility considering penalty component compatibility function  case  constraint
r r seen function takes current label assignment outputs
real number    constraint satised positive number not 
indicating penalty imposed compatibility function  new expression
compatibility function maximize is 
c l            ln    

n
x
i  

cvi  li  

x

r l            ln  

rr

note hard constraints simulated setting making output
large positive number violated 
particular problem  binary valued variable vi n argument candidates generated srl models  i e   li labels         given label
assignment  arguments li     selected form solution  others
 those li      ltered out  variable vi   probability
values  pij   calculated score model j argument i  according softmax
formula described above    rst approach  compatibility function cv  li   equals
p
 
 
j   pij  li   number models      case  
denition  maximizing compatibility function equivalent maximizing
sum probabilities given models argument candidates considered
solution  since function always positive  global score increases directly
number selected candidates  consequence  model biased towards
maximization number candidates included solution  e g   tending select
lot small non overlapping arguments   following koomen et al          bias
corrected adding new score oi   sums compatibility function
i th candidate selected solution  global compatibility function needs
rewritten encompass new information  formalized ilp equation  looks like 
argmax c l            ln     argmax

l     n

l     n


n x
x

 

i   j  

pij  li   oi    li  

   model j propose argument consider pij     
   instead accumulating probabilities models given candidate argument  one could consider
different variable model prediction introduce constraint forcing variables
take value end optimization problem  two alternatives equivalent 

   

ficombination strategies semantic role labeling

constraints expressed separated integer linear equations  possible
dene priori value oi   komen et al  used validation corpus empirically
estimate constant value oi  i e   independent argument candidate    
use exactly solution working single constant value 
refer o 
regarding consistency constraints  considered following six 
   two candidate arguments verb overlap embed 
   verb may two core arguments type label a  a  
   argument r x verb  x argument
verb 
   argument c x verb  x argument
c x verb 
   arguments two dierent verbs overlap  embed 
   two dierent verbs share am x  r am x c x arguments 
constraints    included reference work  punyakanok et al         
constraints paper need checked since individual model
outputs consistent solutions  constraints      restrict set compatible
arguments among dierent predicates sentence  original work 
integer linear programming setting constraints written inequalities  example 
ai argument label i th candidate vi verb predicate  constraint number
p
  written as   ai  a vi  v  li    given verb v argument label a 
constraints similar translations inequalities 
constraint satisfaction optimization applied two dierent ways obtain
complete output annotation sentence  rst one  proceed verb verb
independently nd best selection candidate arguments using constraints
     call approach local optimization  second scenario
candidate arguments sentence considered constraints    
enforced  refer second strategy global optimization  scenarios
compatibility function same  constraints need rewriting global
scenario include information concrete predicate 
section     extensively evaluate presented inference model based constraint satisfaction  describe experiments covering following topics   a 
contribution proposed constraints   b  performance local vs  global
optimization   c  precisionrecall tradeo varying value bias correction
parameter 
    inference based learning
combination model consists two stages  candidate scoring phase  scores
candidate arguments pool using series discriminative classiers  inference
stage  selects best overall solution consistent domain constraints 
   instead working constant  one could try set oi value candidate  taking
account contextual features candidate  plan explore option near future 

   

fisurdeanu  marquez  carreras    comas

rst important component combination strategy candidate
scoring module  assigns candidate argument score equal condence
argument part global solution  formed discriminative functions 
one role label  below  devise two dierent strategies train discriminative
functions 
scoring candidate arguments  nal global solution built inference
module  looks best scored argument structure satises domain specic
constraints  here  global solution subset candidate arguments  score
dened sum condence values arguments form it  currently consider
three constraints determine solutions valid 
 a  candidate arguments predicate overlap embed 
 b  predicate  duplicate arguments allowed numbered arguments a    
 c  arguments predicate embedded within arguments predicates
overlap 
set constraints extended rules  particular case 
know constraints  e g   providing arguments indicated corresponding propbank frame  already guaranteed individual models  others  e g  
constraints     previous sub section  positive impact overall
performance  see section     empirical analysis   inference algorithm use
bottom up cky based dynamic programming strategy  younger         builds
solution maximizes sum argument condences satisfying constraints 
cubic time 
next  describe two dierent strategies train functions score candidate
arguments  rst local strategy  function trained binary batch classier 
independently combination process enforces domain constraints 
second global strategy  functions trained online rankers  taking account
interactions take place combination process decide one argument
another 
training strategies  discriminative functions employ representation arguments  using complete feature set described section    we analyze
contribution feature group section     intuition rich feature
space introduced section   allow gathering sucient statistics robust
scoring candidate arguments  example  scoring classiers might learn
candidate trusted if   a  two individual systems proposed it   b  label
a  generated model     c  proposed model   within certain
argument sequence 
      learning local classifiers
combination process follows cascaded architecture  learning component
decoupled inference module  particular  training strategy consists
training binary classier role label  target label based classier
determine whether candidate argument actually belongs correct proposition
corresponding predicate  output condence value decision 
   

ficombination strategies semantic role labeling

specic training strategy follows  training data consists pool
labeled candidate arguments  proposed individual systems   candidate either
positive  actually correct argument sentence  negative 
correct  strategy trains binary classier role label l  independently
labels  so  concentrates candidate arguments data
label l  forms dataset binary classication  specic label l  it 
binary classier trained using existing techniques binary classication 
requirement combination strategy needs condence values
binary prediction  section   provide experiments using svms train local
classiers 
all  classier trained independently classiers inference module  looking globally combination process  classier seen argument
ltering component decides candidates actual arguments using much richer
representation individual models  context  inference engine used
conict resolution engine  ensure combined solutions valid argument structures sentences 
      learning global rankers
combination process couples learning inference  i e   scoring functions
trained behave accurately within inference module  words  training
strategy global  target train global function maps set argument
candidates sentence valid argument structure  setting  global function
composition scoring functions one label  previous strategy 
unlike previous strategy  completely decoupled inference engine 
policy map set candidates solution determined inference
engine 
recent years  research active global learning methods tagging 
parsing and  general  structure prediction problems  collins        taskar  guestrin   
koller        taskar  klein  collins  koller    manning        tsochantaridis  hofmann 
joachims    altun         article  make use simplest technique global
learning  online learning approach uses perceptron  collins         general
idea algorithm similar original perceptron  rosenblatt         correcting
mistakes linear predictor made visiting training examples  additive
manner  key point learning global rankers relies criteria determines
mistake function trained  idea exploited
similar way multiclass ranking scenarios crammer singer      a      b  
perceptron algorithm combination system works follows  pseudocode
algorithm given figure     let         l possible role labels  let
w    w        wl   set parameter vectors scoring functions  one
label  perceptron initializes vectors w zero  proceeds cycle
training examples  visiting one time  case  training example pair  y  a  
correct solution example set candidate arguments
it  note sets labeled arguments  thus make use
set dierence  note particular argument  l label a 
   

fisurdeanu  marquez  carreras    comas

initialization  wl w wl    
training  
         
training example  y  a 
  inference a  w 
 
let l label
wl   wl    a 
 
let l label
wl   wl  a 
output  w
figure    perceptron global learning algorithm

 a  vector features described section    example  perceptron performs
two steps  first  predicts optimal solution according current setting
w  note prediction strategy employs complete combination model  including
inference component  second  perceptron corrects vectors w according
mistakes seen y  arguments label l seen promoted vector
wl   hand  arguments demoted wl   correction rule
moves scoring vectors towards missing arguments  away predicted arguments
correct  guaranteed that  perceptron visits examples 
feedback rule improve accuracy global combination function
feature space almost linearly separable  freund   schapire        collins        
all  training strategy global mistakes perceptron corrects
arise comparing predicted structure correct one  contrast 
local strategy identies mistakes looking individually sign scoring predictions 
candidate argument  is not  correct solution current scorers predict
negative  positive  condence value  corresponding scorer corrected
candidate argument  note criteria used generate training data
classiers trained locally  section   compare approaches empirically 
nal note  simplicity described perceptron simple form 
however  perceptron version use experiments reported section   incorporates two well known extensions  kernels averaging  freund   schapire        collins
  duy         similar svm  perceptron kernel method  is  represented dual form  dot product example vectors generalized
kernel function exploits richer representations  hand  averaging
technique increases robustness predictions testing  original form 
test predictions computed parameters result training process 
averaged version  test predictions computed average parameter
vectors generated training  every update  details technique
found original article freund   schapire 
   

ficombination strategies semantic role labeling

 

development
brown
wsj

    
    

acuracy

    
    
   
    
    
    
    
 

  

  

  

  

  

  

  

  

  

   

reject rate    

figure    rejection curves estimated output probabilities individual models 

   experimental results
section analyze performance three combination strategies previously
described   a  inference constraint satisfaction   b  learning based inference local
rankers   c  learning based inference global rankers  bulk experiments use candidate arguments generated three individual srl models described
section   evaluated section   
    experimental settings
combination strategies  with one exception  detailed below  trained using
complete conll      training set  propbank treebank sections        minimize
overtting individual srl models training data  partitioned training
corpus folds fold used output individual models
trained remaining four folds  models tuned separate development
partition  treebank section     evaluated two corpora   a  treebank section    
 b  three annotated sections brown corpus 
constraint satisfaction model  converted scores output arguments
three srl models probabilities using softmax function explained section     
development set  section     used tune parameter softmax formula
nal value     models  order assess quality procedure 
plot figure   rejection curves estimated output probabilities respect
classication accuracy development test sets  wsj brown   calculate
plots  probability estimates three models put together set sorted
decreasing order  certain level rejection  n    curve figure   plots
percentage correct arguments lowest scoring n  subset rejected 
   

fisurdeanu  marquez  carreras    comas

exceptions  curves increasing smooth  indicating good correlation
probability estimates classication accuracy 
last experiment  section     analyze behavior proposed combination
strategies candidate pool signicantly larger  experiment used
top    best performing systems conll      shared task evaluation  setup
two signicant dierences experiments used in house individual
systems   a  access systems outputs propbank development
section two test sections   b  argument probabilities individual
models available  thus  instead usual training set  train
combination models propbank development section smaller feature set  note
development set       size regular training set 
evaluated resulting combination models two testing sections  wsj
brown 
    lower upper bounds combination strategies
venture evaluation combination strategies  explore lower
upper bounds combinations models given corpus individual models 
analysis important order understand potential proposed approach
see close actually realizing it 
performance upper bound calculated oracle combination system
perfect ltering classier selects correct candidate arguments discards
others  comparison purposes  implemented second oracle system simulates re ranking approach  predicate selects candidate frame i e  
complete set arguments corresponding predicate proposed single model
highest f  score  table   lists results obtained wsj brown corpora
two oracle systems using three individual models  combination system
oracle simulates combination strategies proposed paper  break
candidate frames work individual candidate arguments  note precision
oracle combination system      case discontinuous arguments 
fragments pass oracle lter considered incorrect scorer corresponding argument complete  e g   argument a  appears without continuation
c a   re ranking columns list results second oracle system  selects
entire candidate frames 
table   indicates upper limit combination approaches proposed
paper relatively high  f  combination oracle system    points higher
best individual system wsj test set     points higher
brown corpus  see table     furthermore  analysis indicates potential
combination strategy higher re ranking strategies  limited
performance best complete frame candidate pool  allowing recombination arguments individual candidate solutions threshold raised
signicantly    f  points wsj   f  points brown 
table   lists distribution candidate arguments individual models
selection performed combination oracle system  conciseness  list
core numbered arguments focus wsj corpus    indicates percent   

ficombination strategies semantic role labeling

wsj
brown

pprops
      
      

combination
precision recall
      
      
      
      

f 
     
     

pprops
      
      

re ranking
precision recall
      
      
      
      

f 
     
     

table    performance upper limits detected two oracle systems 
a 
a 
a 
a 
a 

 
      
      
      
      
      

 
      
      
      
      
      

model  
     
     
      
      
     

model  
     
     
     
     
     

model  
     
     
     
     
     

table    distribution individual systems arguments upper limit selection 
a a  wsj test set 

age correct arguments   models agreed    indicates percentage
correct arguments   models agreed  columns indicate percentage correct arguments detected single model  table   indicates that  expected 
two individual models agreed large percentage correct arguments  nevertheless  signicant number correct arguments  e g       a   come single
individual system  proves that  order achieve maximum performance  one
look beyond simple voting strategies favor arguments high agreement
individual systems 
propose two lower bounds performance combination models using two
baseline systems 
rst baseline recall oriented  merges arguments generated
individual systems  conict resolution  baseline uses approximate inference
algorithm consisting two steps   i  candidate arguments sorted using radix
sort orders candidate arguments descending order of   a  number models
agreed argument   b  argument length tokens   c  performance
individual system      ii  candidates iteratively appended global solution
violate domain constraints arguments already
selected 
second baseline precision oriented  considers arguments three
individual systems agreed  conict resolution uses strategy
previous baseline system 
table   shows performance two baseline models  expected  precisionoriented baseline obtains precision signicantly higher best individual model
 table     recall suers individual models agree fairly large
number candidate arguments  recall oriented baseline balanced  expected
recall higher individual model precision drop much
    combination produced highest scoring baseline model 

   

fisurdeanu  marquez  carreras    comas

wsj
baseline
baseline
brown
baseline
baseline

recall
precision

pprops
      
      

prec 
      
      

recall
      
      

f 
         
         

recall
precision

      
      

      
      

      
      

     
     

   
   

table    performance baseline models wsj brown test sets 
inference strategy lters many unlikely candidates  overall  recalloriented baseline performs best  f       points higher best individual
model wsj corpus       points lower brown corpus 
    performance combination system constraint satisfaction
constraint satisfaction setting arguments output individual models      
  recombined expected better solution satises set constraints 
run inference model based constraint satisfaction described section     using
xpress mp ilp solver     main results summarized table    variants
presented table following  pred by pred stands local optimization 
processes verb predicate independently others  full sentence stands
global optimization  i e   resolving verb predicates sentence
time  column labeled constraints shows particular constraints applied
conguration  column presents value parameter correcting bias
towards candidate overgeneration  concrete values empirically set maximize f 
measure development set      corresponds setting bias correction
applied 
clear conclusions drawn table    first  observe optimization variant obtains f  results individual systems  table   
baseline combination schemes  table     best combination model scores      f  points
wsj      brown higher best individual system  taking account
learning performed  clear constraint satisfaction simple yet formal
setting achieves good results 
somewhat surprising result performance improvements come constraints      i e   overlapping embedding among arguments verb 
repetition core arguments verb   constraints     harmful 
sentence level constraints       impact overall performance    
analysis proposed constraints yielded following explanations 
constraint number   prevents assignment r x argument referred
argument x present  makes inference miss easy r x arguments
    xpress mp dash optimization product free academic usage 
    section     see learning strategy incorporates global feedback  performing
sentence level inference slightly better proceeding predicate predicate 

   

ficombination strategies semantic role labeling

wsj
pred by pred

full sentence

brown
full sentence

constraints
 
   
     
     
       
     
     
       
       


    
    
    
    
    
    
    
    
 

pprops
      
      
      
      
      
      
      
      
      

precision
      
      
      
      
      
      
      
      
      

recall
      
      
      
      
      
      
      
      
      

f 
         
         
         
         
         
         
         
         
         

       
       

    
 

      
      

      
      

      
      

         
         

table    results  wsj brown test sets  obtained multiple variants constraint satisfaction approach

x argument correctly identied  e g   constituents start
 that  which  who  followed verb always r a    furthermore  constraint
presents lot exceptions         r x arguments wsj test set
referred argument x  e g   law tells so  
therefore hard application constraint   prevents selection correct
r x candidates  ocial evaluation script conll       srl eval 
require constraint satised consider solution consistent 
srl eval script requires constraint number    i e   c x tag accepted
without preceding x argument  fullled candidate solution considered
consistent  nds solution violating constraint behavior
convert rst c x  without preceding x  x  turns simple
post processing strategy better forcing coherent solution inference step
allows recover error argument completely
recognized labeled c x tags 
regarding sentence level constraints  observed setting  inference using
local constraints       rarely produces solution inconsistencies sentence
level    makes constraint   useless since almost never violated  constraint
number    i e   sharing ams among dierent verbs  ad hoc represents
less universal principle srl  number exceptions constraint 
wsj test set       gold standard data      output
inference uses local constraints        forcing fulllment
constraint makes inference process commit many errors corrections  making
eect negligible 
    fact partly explained small number overlapping arguments candidate pool
produced three individual models 

   

fisurdeanu  marquez  carreras    comas

  
  

  

precision
recall
f 

  

  

  
 

  

 

  

precision
recall
f 

  

  

  

  

  

  

  
    

    

 

   

   

   

   

  
    

 

value

    

 

   

   

   

   

 

value

figure    precision recall plots  respect bias correcting parameter  o  
wsj development test sets  left right plots  respectively  

considering constraints universal  i e   exceptions exist
gold standard  seems reasonable convert soft constraints  done
precomputing compatibility corpora counts using  instance  point wise
mutual information  incorporating eect compatibility function explained
section      softening could  principle  increase overall recall combination 
unfortunately  initial experiments showed dierences hard soft
variants 
finally  dierences optimized values bias correcting parameter
    clearly explained observing precision recall values  default
version tends overgenerate argument assignments  implies higher recall cost
lower precision  contrary  f  optimized variant conservative
needs evidence select candidate  result  precision higher recall
lower  side eect restrictive argument assignments  number
correctly annotated complete propositions lower optimized setting 
preference high precision vs  high recall system mostly task dependant 
interesting note constraint satisfaction setting  adjusting precision
recall tradeo easily done varying value bias correcting score 
figure    plot precisionrecall curves respect dierent values parameter  the optimization done using constraints              expected  high values
promote precision demote recall  lower values contrary  also 
see wide range values combined f  measure almost
constant  the approximate intervals marked using vertical lines   making possible
select dierent recall precision values global performance  f    near optimal  parenthetically  note optimal value estimated development set
 o        generalizes well wsj test set 
   

ficombination strategies semantic role labeling

wsj
models
models
models
models
brown
models
models
models
models

   
   
   
     

pprops
      
      
      
      

prec 
      
      
      
      

recall
      
      
      
      

f 
         
         
         
         

f  improvement
     
     
     
     

   
   
   
     

      
      
      
      

      
      
      
      

      
      
      
      

         
         
         
         

     
     
     
     

table    overall results learning based inference local rankers wsj
brown test sets 

    performance combination system local rankers
implemented candidate scoring classiers combination strategy using support vector machines  svm  polynomial kernels degree    performed slightly
better types svms adaboost  implemented svm classiers
svmlight software     outside changing default kernel polynomial
modied default parameters  experiments reported section 
trained models   possible combinations   individual systems  using
complete feature set introduced section    dynamic programming engine used
actual inference processes predicate independently  similar pred by pred
approach previous sub section  
table   summarizes performance combined systems wsj brown
corpora  table   indicates combination strategy always successful  results
combination systems improve upon individual models  table    f 
scores always better baselines  table     last column table shows
f  improvement combination model w r t  best individual model set 
expected  highest scoring combined system includes three individual models 
f  measure      points higher best individual model  model    wsj test
set      points higher brown test set  note combination two
individual systems outperform current state art  see section   details  
empirical proof robust successful combination strategies srl problem
possible  table   indicates that  even though partial parsing model  model   
worst performing individual model  contribution ensemble important 
indicating information provides indeed complementary models 
instance  wsj performance combination two best individual models
 models      worse combinations using model    models          
    http   svmlight joachims org 

   

fisurdeanu  marquez  carreras    comas

wsj
fs 
  fs 
  fs 
  fs 
  fs 
  fs 
brown
fs 
  fs 
  fs 
  fs 
  fs 
  fs 

pprops
      
      
      
      
      
      

prec 
      
      
      
      
      
      

recall
      
      
      
      
      
      

f 
         
         
         
         
         
         

      
      
      
      
      
      

      
      
      
      
      
      

      
      
      
      
      
      

         
         
         
         
         
         

table    feature analysis learning based inference local rankers 
due simple architecture i e   feedback conict resolution component
candidate ltering inference model good framework study contribution
features proposed section    study group features   sets  fs 
voting features  fs  overlap features arguments predicate  fs 
overlap features arguments predicates  fs  partial syntax features  fs 
full syntax features  fs  probabilities generated individual systems
candidate arguments  using sets constructed   combination models
increasing number features made available argument ltering classiers  e g  
rst system uses fs   second system adds fs  rst systems features 
fs  added third system  etc  table   lists performance   systems
two test corpora  empirical analysis indicates feature sets
highest contribution are 
fs   boosts f  score combined system      points  wsj      
points  brown  best individual system  yet another empirical proof
voting successful combination strategy 
fs   contribution      points  wsj       points  brown  f 
score  numbers indicate ltering classier capable learning
preferences individual models certain syntactic structures 
fs   contributes      points  wsj       points  brown  f  score 
results promote idea information overall sentence structure 
case inter predicate relations  successfully used problem srl 
knowledge  novel 
proposed features positive contribution performance combined
system  overall  achieve f  score      points  wsj       points  brown 
higher best performing combined system conll      shared task evaluation
 see section   details  
   

ficombination strategies semantic role labeling

    performance combination system global rankers
section report experiments global perceptron algorithm described
section        globally trains scoring functions rankers  similar local
svm models  use polynomial kernels degree    furthermore  predictions test
time used averages parameter vectors  following technique freund schapire
       
interested two main aspects  first  evaluate eect training
scoring functions perceptron using two dierent update rules  one global
local  global feedback rule  detailed section        corrects mistakes found
comparing correct argument structure one results inference  this
noted global feedback   contrast  local feedback rule corrects mistakes
found inference  candidate argument handled independently  ignoring
global argument structure generated  this noted local feedback   second 
analyze eect using dierent constraints inference module  extent 
congured inference module two ways  rst processes predicates
sentence independently  thus might select overlapping arguments dierent predicates 
incorrect according domain constraints  this one noted pred by pred
inference   second processes predicates jointly  enforces hierarchical structure
arguments  arguments never overlap  arguments predicate allowed
embed arguments predicates  this noted full sentence inference  
perspective  model local update pred by pred inference almost identical
local combination strategy described section      unique dierence
use perceptron instead svm  apparently minute dierence turns
signicant empirical analysis allows us measure contribution
svm margin maximization global feedback classier based combination
strategy  see section      
trained four dierent models  local global feedback  predicate bypredicate joint inference  model trained   epochs training data 
evaluated development data training epoch  selected best
performing point development  evaluated models test data  table  
reports results test data 
looking results  rst impression dierence f  measure
signicant among dierent congurations  however  observations pointed out 
global methods achieve much better recall gures  whereas local methods prioritize
precision system  overall  global methods achieve balanced tradeo
precision recall  contributes better f  measure 
looking pred by pred versus full sentence inference  seen
global methods sensitive dierence  note local model trained
independently inference module  thus  adding constraints inference
engine change parameters local model  testing time  dierent
inference congurations aect results  contrast  global models trained
dependently inference module  moving pred by pred full sentence
inference  consistency enforced argument structures dierent predicates 
benets precision recall method  global learning algorithm
   

fisurdeanu  marquez  carreras    comas

wsj
pred by pred  local
full sentence  local
pred by pred  global
full sentence  global
brown
pred by pred  local
full sentence  local
pred by pred  global
full sentence  global

pprops
      
      
      
      

prec 
      
      
      
      

recall
      
      
      
      

f 
         
         
         
         

      
      
      
      

      
      
      
      

      
      
      
      

         
         
         
         

table    test results combination system global rankers  four congurations
evaluated  combine pred by pred full sentence inference local
global feedback 

improves precision recall coupled joint inference process
considers constraints solution 
nevertheless  combination system local svm classiers  presented previous section  achieves marginally better f  score global learning method         vs 
       wsj   explained dierent machine learning algorithms  we discuss
issue detail section       better f  score accomplished much better
precision local approach         vs         wsj   whereas recall lower
local global approach         vs         wsj   hand 
global strategy produces completely correct annotations  see pprops column 
local strategies investigated  see tables       expected 
considering global strategy optimizes sentence level cost function  somewhat
surprisingly  number perfect propositions generated global strategy lower
number perfect propositions produced constraint satisfaction approach 
discuss result section     
    scalability combination strategies
combination experiments reported point used candidate arguments
generated three individual srl models introduced section    experiments provide empirical comparison three inference models proposed 
answer obvious scalability question  proposed combination approaches
scale number candidate arguments increases quality diminishes 
mainly interested answering question last two combination models  which
use inference based learning local global rankers  two reasons   a 
performed better constraint satisfaction model previous experiments 
 b  requirements individual srl systems outputs unlike
constraint satisfaction model requires argument probabilities individual models coupled pools candidates generated individual srl
model 
   

ficombination strategies semantic role labeling

koomen
pradhan 
haghighi
marquez
pradhan

surdeanu
tsai
che
moschitti
tjongkimsang
yi

ozgencil

wsj
prec 
recall
      
      
             
             
      
      






      

      

      

     

      

      

      

     






      
      
      
      
      

      
      
      
      
      

      
      
      
      
      

     
     
     
     
     

      
      
      
      
      

      
      
      
      
      

      
      
      
      
      

     
     
     
     
     







f 
     
     
     
     

pprops
      
      
      
      

brown
prec 
recall
      
      
             
             
      
      

pprops
      
      
      
      

f 
     
     
     
     

      

      

      

     

      

      

      

     

      

      

      

     

      

      

      

     

table     performance best systems conll       pradhan  contains postevaluation improvements  top   systems actually combination models
themselves  second column marks systems used evaluation  pradhan  replaced improved version pradhan  
yi  due format errors submitted data 

scalability analysis  use individual srl models top    systems
conll      shared task evaluation  table    summarizes performance systems
two test corpora used previous experiments  table    indicates 
performance systems varies widely  dierence   f  points wsj
corpus   f  points brown corpus best worst system
set 
combination experiments generated   candidate pools using top         

      individual systems labeled
table     make two changes
experimental setup used rst part section   a  trained combined models propbank development section access
individual systems outputs propbank training partition   b  feature
set introduced section   use individual systems argument probabilities
raw activations individual models classiers available  note
settings size training corpus    times smaller size
training set used previous experiments 
table    shows upper limits setups using combination reranking oracle systems introduced section      besides performance numbers 
list table    average number candidates per sentence setup  i e   number
unique candidate arguments    args  sent   combination oracle number
unique candidate frames    frames sent   re ranking oracle  table    lists
performance combined models local feedback  section        global
feedback  section         combination strategy global rankers uses joint inference
global feedback  see description previous sub section  
   

fisurdeanu  marquez  carreras    comas

wsj
c 
c 
c 
c 
c  
brown
c 
c 
c 
c 
c  

  args  sent 
    
    
     
     
     
    
    
    
     
     

combination
prec 
recall
      
      
      
      
      
      
      
      
             
      
      
      
      
      

      
      
      
      
      

f 
     
     
     
     
     

re ranking
  frames sent 
prec 
    
      
    
      
    
      
    
      
    
      

     
     
     
     
     

    
    
    
    
    

      
      
      
      
      

recall
      
      
      
      
      

f 
     
     
     
     
     

      
      
      
      
      

     
     
     
     
     

table     performance upper limits determined oracle systems    best systems conll       ck stands combination top k systems
table       args  sent  indicates average number candidate arguments
per sentence combination oracle    frames sent  indicates average
number candidate frames per sentence re ranking oracle  latter
larger number systems combination average
multiple predicates per sentence 

wsj
c 
c 
c 
c 
c  
brown
c 
c 
c 
c 
c  

pprops
      
      
      
      
      

local
prec 
      
      
      
      
      

ranker
recall
      
      
      
      
      

f 
        
        
        
        
        

pprops
     
     
     
     
     

global ranker
prec 
recall
             
             
             
             
             

f 
        
        
        
        
        

      
      
      
      
      

      
      
      
      
      

      
      
      
      
      

        
        
        
        
        

     
     
     
     
     

      
      
      
      
      

        
        
        
        
        

      
      
      
      
      

table     local versus global ranking combinations    best systems conll      ck stands combination top k systems table    

draw several conclusions experiments  first  performance upper limit re ranking always lower argument based combination
strategy  even number candidates large  example     individual
models used  f  upper limit approach brown corpus       whereas
f  upper limit re ranking        however  enhanced potential combination approach imply signicant increase computational cost  table    shows
   

ficombination strategies semantic role labeling

number candidate arguments must handled combination approaches
much higher number candidate frames input re ranking system  especially number individual models high  example    
individual models used  combination approaches must process around    arguments
per sentence  whereas re ranking approaches must handle approximately   frames per sentence  intuition behind relatively small dierence computational cost that 
even though number arguments signicantly larger number frames 
dierence number unique candidates two approaches high
probability repeated arguments higher probability repeated
frames 
second conclusion combination models boost performance
corresponding individual systems  example  best   system combination achieves
f  score approximately   points higher best individual model wsj
brown corpus  expected  combination models reach performance plateau
around     individual systems  quality individual models starts drop
signicantly  nevertheless  considering top   individual systems use combination
strategies amount training data experiment quite small 
results show good potential combination models analyzed paper 
third observation relation previously observed local global
rankers holds  combination model local rankers better precision  model
global rankers always better recall generally better pprops score  overall 
model local rankers obtains better f  scores scales better number
individual systems increases  discuss dierences detail next
sub section 
finally  table    indicates potential recall experiment  shown
left most block table  higher potential recall combining three
individual srl systems  see table          higher wsj test set       higher
brown test set  expected  considering number quality
candidate arguments last experiment higher  however  even
improvement  potential recall combination strategies far       thus 
combining solutions n best state of the art srl systems still
potential properly solve srl problem  future work focus recallboosting strategies  e g   using candidate arguments individual systems
individual complete solutions generated  step many candidate arguments
eliminated 
    discussion
experimental results presented section indicate proposed combination
strategies successful  three combination models provide statistically signicant improvements individual models baselines setups  immediate  but
somewhat shallow  comparison three combination strategies investigated indicates
that   a  best combination strategy srl problem max margin local metalearner   b  global ranking approach meta learner important
   

fisurdeanu  marquez  carreras    comas

contribution max margin strategy   c  constraint satisfaction
model performs worst strategies tried 
however  experiments dierences combination approaches investigated small  reasonable observation combination strategy
advantages disadvantages dierent approaches suitable dierent
applications data  discuss dierences below 
argument probabilities individual systems available  combination model
based constraint satisfaction attractive choice  simple  unsupervised strategy obtains competitive performance  furthermore  constraint satisfaction model
provides elegant customizable framework tune balance precision
recall  see section       framework currently obtain highest recall
combination models        higher best recall obtained meta learning approaches wsj corpus        higher meta learning models brown
corpus  higher recall implies higher percentage predicates completely
correctly annotated  best pprops numbers table   best combination
strategies  cause high dierence recall favor constraint satisfaction
approach candidate scoring learning based inference acts implicitly
lter  candidates whose score i e   classier condence candidate part
correct solution negative discarded  negatively aects overall recall 
hence  constraint satisfaction better solution srl based nlp applications
require predicate argument frames extracted high recall  example  information extraction  predicate argument tuples ltered subsequent high precision 
domain specic constraints  surdeanu et al          hence paramount srl
model high recall 
nevertheless  many cases argument probabilities individual srl models
available  either models generate them  e g   rule based systems 
individual models available black boxes  oer access
internal information  conditions  showed combination strategies based
meta learning viable alternative  fact  approaches obtain highest
f  scores  see section      obtain excellent performance even small amounts
training data  see section       previously mentioned  candidate scoring acts
lter  learning based inference tends favor precision recall  precision
      higher best precision constraint satisfaction models wsj
corpus        higher brown corpus  preference precision recall
pronounced learning based inference local rankers  section     
inference model global rankers  section       hypothesis causes
global ranking model less precision biased conguration ratio
errors positive versus negative samples balanced  thinking strategy
perceptron follows  local approach updates every candidate incorrect prediction
sign  whereas global approach updates candidates
complete solution  enforcing domain constraints  words  number
negative updates drives precision bias reduced global approach 
false positives generated ranking classiers eliminated
domain constraints  thus  candidate scoring trained optimize accuracy 
   

ficombination strategies semantic role labeling

wsj
global feedback
max margin
brown
global feedback
max margin

pprops
      
      

prec 
      
      

recall
      
      

f 
     
     

      
      

      
      

      
      

     
     

table     contribution global feedback max margin learning based inference 
baseline pred by pred  local model table   

fewer candidate arguments eliminated meta learner global rankers 
translates better balance precision recall 
another important conclusion analysis global versus local ranking
learning based inference max margin approach candidate scoring classiers
important global feedback inference  fact  considering
dierence model predicate by predicate inference local feedback
section      pred by pred  local  versus best model section       fs  
latter uses svm classiers whereas former uses perceptron  compute exact
contribution max margin global feedback     convenience  summarize
analysis table     table indicates max margin yields consistent improvement
precision recall  whereas contribution global feedback reducing
dierence precision recall boosting recall decreasing precision 
benet max margin classiers even evident table     shows
local ranking model max margin classiers generalizes better global ranking
model amount training data reduced signicantly 
even though paper analyzed several combination approaches three
independent implementations  proposed models fact compatible other 
various combinations proposed strategies immediately possible  example 
constraint satisfaction model applied output probabilities candidate
scoring component introduced section      model eliminates dependency
output scores individual srl models retains advantages
constraint satisfaction model  e g   formal framework tune balance precision recall  another possible combination approaches introduced paper
use max margin classiers learning based inference global feedback  e g  
using global training method margin maximization svmstruct  tsochantaridis et al          model would indeed increased training time     could
leverage advantages max margin classiers inference global feedback
 summarized table      finally  another attractive approach stacking  i e   n levels chained meta learning  example  could cascade learning based inference
model global rankers  boosts recall  learning based inference local
rankers  favors precision 
    contribution global feedback given model joint inference global feedback  full
sentence  global  section     
    main reason chose perceptron proposed online strategies 

   

fisurdeanu  marquez  carreras    comas

   related work
  best performing systems conll      shared task included combination
dierent base subsystems increase robustness gain coverage independence
parse errors  therefore  closely related work paper  rst
four rows table    summarize results exactly experimental setting
one used paper 
koomen et al         used   layer architecture close ours  pool candidates
generated by   a  running full syntax srl system alternative input information  collins
parsing    best trees charniaks parser    b   taking candidates pass
lter set dierent parse trees  combination candidates performed
elegant global inference procedure constraint satisfaction  which  formulated
integer linear programming  solved eciently  dierent work 
break complete solutions number srl systems investigate
meta learning combination approach addition ilp inference  koomen et al s
system best performing system conll       see table     
haghighi et al         implemented double re ranking top several outputs
base srl model  re ranking performed  rst  set n best solutions obtained
base system run single parse tree  and  then  set best candidates
coming n best parse trees  second best system conll     
 third row table      compared decomposition re combination approach 
re ranking setting advantage allowing denition global features
apply complete candidate solutions  according follow up work authors
 toutanova  haghighi    manning         global features source major
performance improvements re ranking system  contrast  focus features
exploit redundancy individual models  e g   overlap individual
candidate arguments  add global information frame level complete
solutions provided individual models  main drawback re ranking compared
approach dierent individual solutions combined re ranking
forced select complete candidate solution  implies overall performance
strongly depends ability base model generate complete correct solution
set n best candidates  drawback evident lower performance upper
limit re ranking approach  see tables       performance actual
system best combination strategy achieves f  score   points higher
haghighi et al  wsj brown    
finally  pradhan  hacioglu  ward  martin  jurafsky      b  followed stacking
approach learning two individual systems based full syntax  whose outputs used
generate features feed training stage nal chunk by chunk srl system  although
ne granularity chunking based system allows recover parsing errors 
nd combination scheme quite ad hoc forces break argument candidates
chunks last stage 
    recently  yih toutanova        reported improved numbers system        f  wsj
      brown  however  numbers directly comparable systems presented
paper fixed significant bug representation quotes input data  bug
still present data 

   

ficombination strategies semantic role labeling

outside conll shared task evaluation  roth yih        reached conclusion quality local argument classiers important global
feedback inference component  one conclusions drawn paper  contribution shown hypothesis holds complex
framework  combination several state of the art individual models  whereas roth
yih experimented single individual model  numbered arguments  slightly
simplied problem representation  b i o basic chunks  additionally  detailed
experiments allowed us show clearly contribution max margin higher
global learning several corpora several combinations individual systems 
punyakanok  roth  yih        showed performance individual srl
models  particularly argument identication  signicantly improved full parsing
used argument boundaries restricted match syntactic constituents  similarly
model     believe approach used models      candidate
arguments match single syntactic constituent  increased robustness
built in mechanism handle syntax errors  argument constituent incorrectly fragmented multiple phrases  empirical results support
claim  model   performs better model   models proposed punyakanok
et al  second advantage strategy proposed paper model
deployed using full syntax  model    partial syntax  model    
pradhan  ward  hacioglu  martin  jurafsky      c  implement srl combination
strategy constituent level that  similarly approach  combines dierent syntactic
views data based full partial syntactic analysis  however  unlike approach 
pradhan et al s work uses simple greedy inference strategy based probabilities
candidate arguments  whereas paper introduce analyze three dierent
combination algorithms  analysis yielded combination system outperforms
current state art 
previous work general eld predicting structures natural language
texts indicated combination several individual models improves overall performance given task  collins        rst proposed learning layer based ranking
improve performance generative syntactic parser  approach  reranker
trained select best solution pool solutions produced generative
parser  so  reranker dealt complete parse trees  represented
rich features exploited dependencies considered generative method 
hand  computationally feasible train reranker  base method
reduced number possible parse trees sentence exponential number  w r t 
sentence length  tens  recently  global discriminative learning methods
predicting structures proposed  laerty  mccallum    pereira        collins 
            taskar et al               tsochantaridis et al          train
single discriminative ranking function detect structures sentence  major property
methods model problem discriminatively  arbitrary
rich representations structures used  furthermore  training process
methods global  parameters set maximize measures related
local accuracies  i e   recognizing parts structure   related global
accuracy  i e   recognizing complete structures   article  use global
rich representations major motivation 
   

fisurdeanu  marquez  carreras    comas

    conclusions
paper introduces analyzes three combination strategies context semantic
role labeling  rst model implements inference strategy constraint satisfaction
using integer linear programming  second uses inference based learning
candidates scored using discriminative classiers using local information 
third last inference model builds previous strategy adding global feedback
conict resolution component ranking classiers  meta learners used
inference process developed rich set features includes voting
statistics i e   many individual systems proposed candidate argument overlap
arguments predicates sentence  structure distance
information coded using partial full syntax  probabilities individual srl
models  if available   knowledge  rst work that   a  introduces thorough
inference model based learning semantic role labeling   b  performs comparative
analysis several inference strategies context srl 
results presented suggest strategy decomposing individual solutions
performing learning based re combination constructing nal solution advantages approaches  e g   re ranking set complete candidate solutions 
course  task dependant conclusion  case semantic role labeling  approach relatively simple since re combination argument candidates fulll
set structural constraints generate consistent solution  target structure complex  e g   full parse tree  re combination step might complex
learning search perspectives 
evaluation indicates proposed combination approaches successful 
provide signicant improvements best individual model several baseline
combination algorithms setups  three combination strategies investigated 
best f  score obtained learning based inference using max margin classiers 
proposed approaches advantages drawbacks  see section    
detailed discussion dierences among proposed inference models  several important features state of the art srl combination strategy emerge analysis 
 i  individual models combined granularity candidate arguments rather
granularity complete solutions frames   ii  best combination strategy
uses inference model based learning   iii  learning based inference benets
max margin classiers global feedback   iv  inference sentence level  i e  
considering predicates time  proves slightly useful learning
performed globally  using feedback complete solution inference 
last least  results obtained best combination strategy developed
work outperform current state art  results empirical proof
srl system good performance built combining small number  three
experiments  relatively simple srl models 

acknowledgments
would thank jair reviewers valuable comments 
research partially supported european commission  chil project 
   

ficombination strategies semantic role labeling

ip         pascal network  ist              spanish ministry education
science  trangram  tin           c        mihai surdeanu research fellow
within ramon cajal program spanish ministry education science 
grateful dash optimization free academic use xpress mp 

references
bishop  c          neural networks pattern recognition  oxford university press 
boas  h  c          bilingual framenet dictionaries machine translation  proceedings
lrec      
carreras  x     marquez  l          introduction conll      shared task  semantic
role labeling  proceedings conll      
carreras  x     marquez  l          introduction conll      shared task  semantic
role labeling  proceedings conll      
carreras  x   marquez  l     chrupala  g          hierarchical recognition propositional
arguments perceptrons  proceedings conll      shared task 
charniak  e          maximum entropy inspired parser  proceedings naacl 
collins  m          head driven statistical models natural language parsing  phd
dissertation  university pennsylvania 
collins  m          discriminative reranking natural language parsing  proceedings
  th international conference machine learning  icml     stanford  ca
usa 
collins  m          discriminative training methods hidden markov models  theory
experiments perceptron algorithms  proceedings sigdat conference
empirical methods natural language processing  emnlp    
collins  m          parameter estimation statistical parsing models  theory practice distribution free methods  bunt  h   carroll  j     satta  g   eds    new
developments parsing technology  chap     kluwer 
collins  m     duy  n          new ranking algorithms parsing tagging  kernels
discrete structures  voted perceptron  proceedings   th annual
meeting association computational linguistics  acl   
crammer  k     singer  y       a   family additive online algorithms category
ranking  journal machine learning research              
crammer  k     singer  y       b   ultraconservative online algorithms multiclass
problems  journal machine learning research            
freund  y     schapire  r  e          large margin classication using perceptron
algorithm  machine learning                 
gildea  d     jurafsky  d          automatic labeling semantic roles  computational
linguistics         
   

fisurdeanu  marquez  carreras    comas

gildea  d     palmer  m          necessity syntactic parsing predicate argument
recognition  proceedings   th annual conference association
computational linguistics  acl     
hacioglu  k   pradhan  s   ward  w   martin  j  h     jurafsky  d          semantic
role labeling tagging syntactic chunks  proceedings  th conference
computational natural language learning  conll       
haghighi  a   toutanova  k     manning  c          joint model semantic role labeling 
proceedings conll      shared task 
koomen  p   punyakanok  v   roth  d     yih  w          generalized inference
multiple semantic role labeling systems  proceedings conll      shared task 
laerty  j   mccallum  a     pereira  f          conditonal random elds  probabilistic models segmenting labeling sequence data  proceedings   th
international conference machine learning  icml    
marcus  m   santorini  b     marcinkiewicz  m          building large annotated corpus
english  penn treebank  computational linguistics         
marquez  l   comas  p   gimenez  j     catala  n          semantic role labeling
sequential tagging  proceedings conll      shared task 
melli  g   wang  y   liu  y   kashani  m  m   shi  z   gu  b   sarkar  a     popowich 
f          description squash  sfu question answering summary handler
duc      summarization task  proceedings document understanding
workshop  hlt emnlp annual meeting 
narayanan  s     harabagiu  s          question answering based semantic structures 
international conference computational linguistics  coling       
noreen  e  w          computer intensive methods testing hypotheses  john wiley  
sons 
palmer  m   gildea  d     kingsbury  p          proposition bank  annotated
corpus semantic roles  computational linguistics         
ponzetto  s  p     strube  m       a   exploiting semantic role labeling  wordnet
wikipedia coreference resolution  proceedings human language technolgy
conference north american chapter association computational linguistics 
ponzetto  s  p     strube  m       b   semantic role labeling coreference resolution 
companion volume proceedings   th meeting european chapter
association computational linguistics 
pradhan  s   hacioglu  k   krugler  v   ward  w   martin  j  h     jurafsky  d       a  
support vector learning semantic argument classication  machine learning     
     
pradhan  s   hacioglu  k   ward  w   martin  j  h     jurafsky  d       b   semantic role
chunking combining complementary syntactic views  proceedings conll      
   

ficombination strategies semantic role labeling

pradhan  s   ward  w   hacioglu  k   martin  j  h     jurafsky  d       c   semantic role
labeling using dierent syntactic views  proceedings   rd annual conference
association computational linguistics 
punyakanok  v   roth  d     yih  w          necessity syntactic parsing semantic role labeling  proceedings international joint conference artificial
intelligence  ijcai  
punyakanok  v   roth  d   yih  w     zimak  d          semantic role labeling via integer linear programming inference  proceedings international conference
computational linguistics  coling    
rosenblatt  f          perceptron  probabilistic model information storage
organization brain  psychological review             
roth  d     yih  w          linear programming formulation global inference
natural language tasks  proceedings annual conference computational
natural language learning  conll        pp      boston  ma 
roth  d     yih  w          integer linear programming inference conditional random
elds  proceedings international conference machine learning  icml  
schapire  r  e     singer  y          improved boosting algorithms using condence rated
predictions  machine learning         
surdeanu  m   harabagiu  s   williams  j     aarseth  p          using predicate argument
structures information extraction  proceedings   st annual meeting
association computational linguistics  acl       
taskar  b   guestrin  c     koller  d          max margin markov networks  proceedings
  th annual conference neural information processing systems  nips    
vancouver  canada 
taskar  b   klein  d   collins  m   koller  d     manning  c          max margin parsing 
proceedings emnlp      
toutanova  k   haghighi  a     manning  c          joint learning improves semantic role
labeling  proceedings   rd annual meeting association computational linguistics  acl     pp          ann arbor  mi  usa  association
computational linguistics 
tsochantaridis  i   hofmann  t   joachims  t     altun  y          support vector machine
learning interdependent structured output spaces  proceedings   st
international conference machine learning  icml    
xue  n     palmer  m          calibrating features semantic role labeling  proceedings
emnlp      
yih  s  w     toutanova  k          automatic semantic role labeling  tutorial
human language technolgy conference north american chapter
association computational linguistics 
younger  d  h          recognition parsing context free languages n  time 
information control                 

   


