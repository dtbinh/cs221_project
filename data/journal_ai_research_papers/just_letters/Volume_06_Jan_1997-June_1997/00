journal artificial intelligence research              

submitted       published     

improved heterogeneous distance functions
d  randall wilson
tony r  martinez
computer science department
brigham young university
provo  ut        usa

randy  axon cs byu edu
martinez  cs byu edu

abstract
instance based learning techniques typically handle continuous linear input values well 
often handle nominal input attributes appropriately  value difference metric
 vdm  designed find reasonable distance values nominal attribute values 
largely ignores continuous attributes  requiring discretization map continuous values
nominal values  paper proposes three new heterogeneous distance functions  called
heterogeneous value difference metric  hvdm   interpolated value difference metric
 ivdm   windowed value difference metric  wvdm   new distance functions
designed handle applications nominal attributes  continuous attributes  both 
experiments    applications new distance metrics achieve higher classification accuracy
average three previous distance functions datasets nominal
continuous attributes 

   introduction
instance based learning  ibl   aha  kibler   albert        aha        wilson   martinez 
      wettschereck  aha   mohri        domingos        paradigm learning
algorithms typically store n available training examples  instances 
training set  t  learning  instance input vector x  output class c 
generalization  systems use distance function determine close new
input vector stored instance  use nearest instance instances predict
output class  i e   classify y   instance based learning algorithms referred
nearest neighbor techniques  cover   hart        hart        dasarathy         memorybased reasoning methods  stanfill   waltz        cost   salzberg        rachlin et al        
overlap significantly instance based paradigm well  algorithms much
success wide variety applications  real world classification tasks  
many neural network models make use distance functions  including radial basis
function networks  broomhead   lowe        renals   rohwer        wasserman        
counterpropagation networks  hecht nielsen         art  carpenter   grossberg         selforganizing maps  kohonen        competitive learning  rumelhart   mcclelland        
distance functions used many fields besides machine learning neural networks 
including statistics  atkeson  moore   schaal         pattern recognition  diday       
michalski  stepp   diday         cognitive psychology  tversky        nosofsky        
     ai access foundation morgan kaufmann publishers  rights reserved 

fiwilson   martinez

many distance functions proposed decide instance
closest given input vector  michalski  stepp   diday        diday         many
metrics work well numerical attributes appropriately handle nominal  i e  
discrete  perhaps unordered  attributes 
value difference metric  vdm   stanfill   waltz        introduced define
appropriate distance function nominal  also called symbolic  attributes  modified value
difference metric  mvdm  uses different weighting scheme vdm used
pebls system  cost   salzberg        rachlin et al          distance metrics work well
many nominal domains  handle continuous attributes directly  instead 
rely upon discretization  lebowitz        schlimmer         degrade generalization
accuracy  ventura   martinez        
many real world applications nominal linear attributes  including 
example  half datasets uci machine learning database repository  merz  
murphy         paper introduces three new distance functions appropriate
previous functions applications nominal continuous attributes 
new distance functions incorporated many learning systems areas
study  augmented weighting schemes  wettschereck  aha   mohri       
atkeson  moore   schaal        enhancements system provides 
choice distance function influences bias learning algorithm  bias rule
method causes algorithm choose one generalized output another  mitchell 
       learning algorithm must bias order generalize  shown
learning algorithm generalize accurately summed
possible problems  schaffer         unless information problem
training data available   follows distance function strictly better
terms generalization ability  considering possible problems equal
probability 
however  higher probability one class problems occurring another 
learning algorithms generalize accurately others  wolpert        
better summed problems  problems
perform well likely occur  sense  one algorithm distance function
improvement another higher probability good generalization
another  better matched kinds problems likely occur 
many learning algorithms use bias simplicity  mitchell        wolpert       
generalize  bias appropriatemeaning leads good generalization
accuracyfor wide variety real world applications  though meaning simplicity varies
depending upon representational language learning algorithm  biases 
decisions made basis additional domain knowledge particular problem  mitchell 
       improve generalization 
light  distance functions presented paper appropriate
used comparison average yield improved generalization accuracy
collection    applications  results theoretically limited set datasets 
hope datasets representative problems interest  and occur
frequently  real world  distance functions presented useful
cases  especially involving continuous nominal input attributes 
section   provides background information distance functions used previously  section  
 

fiimproved heterogeneous distance functions

introduces distance function combines euclidean distance vdm handle
continuous nominal attributes  sections     present two extensions value
difference metric allow direct use continuous attributes  section   introduces
interpolated value difference metric  ivdm   uses interpolation probabilities avoid
problems related discretization  section   presents windowed value difference metric
 wvdm   uses detailed probability density function similar interpolation
process 
section   presents empirical results comparing three commonly used distance functions
three new functions presented paper  results obtained using
distance functions instance based learning system    datasets  results indicate
new heterogeneous distance functions appropriate previously used functions
datasets nominal linear attributes  achieve higher average
generalization accuracy datasets  section   discusses related work  section  
provides conclusions future research directions 

   previous distance functions
mentioned introduction  many learning systems depend upon good
distance function successful  variety distance functions available uses 
including minkowsky  batchelor         mahalanobis  nadler   smith         camberra 
chebychev  quadratic  correlation  chi square distance metrics  michalski  stepp  
diday        diday         context similarity measure  biberman         contrast
model  tversky         hyperrectangle distance functions  salzberg        domingos       
others  several functions defined figure   
although many distance functions proposed  far commonly
used euclidean distance function  defined as 


e x  y   

 xa ya   

   

a  

x two input vectors  one typically stored instance 
input vector classified  number input variables  attributes 
application  square root often computed practice  closest instance s 
still closest  regardless whether square root taken 
alternative function  city block manhattan distance function  requires less
computation defined as 
m x  y   



xa ya

   

a  

euclidean manhattan distance functions equivalent minkowskian rdistance function  batchelor        r        respectively 

 

fiwilson   martinez

minkowsky 

euclidean 


r
d x  y    xi yi
i  

camberra 

 

r

manhattan   city block 



 
  xi yi  

d x  y   

i  

x

d x  y   
x
 


i  

chebychev 



d x  y    xi yi
i  



d x  y    max xi yi
i  



d x  y     x y t q x y     xi yi  q ji  x j j  
quadratic 

q problem specific positive
j   i  
definite weight matrix
v covariance matrix    am 
mahalanobis 
aj vector values
d x  y     det v     x y t v    x y 
attribute j occuring training set
instances    n 

correlation 
 xi xi   yi yi  
xi   yi average value
i  
d x  y   
attribute
occuring training set 


 
 
 xi xi    yi yi  
i  

i  


  xi
chi square  d x  y   


sumi sizex size
i  


kendalls rank correlation 
sign x          x     
x      x      respectively 

d x  y     

 

sumi sum values attribute
occuring training set  sizex
sum values vector x 

i 
 

sign xi x j  sign yi j  
n n    i   j  

figure    equations selected distance functions 
 x vectors attribute values  
     normalization
one weakness basic euclidean distance function one input attributes
relatively large range  overpower attributes  example  application
two attributes  b  values         b values
      bs influence distance function usually overpowered
influence  therefore  distances often normalized dividing distance attribute
range  i e   maximum minimum  attribute  distance attribute
approximate range       order avoid outliers  common divide
standard deviation instead range  trim range removing highest lowest
percent  e g       data consideration defining range  possible
map value outside range minimum maximum value avoid normalized
values outside range       domain knowledge often used decide method
appropriate 
related idea normalization using attribute weights weighting
 

fiimproved heterogeneous distance functions

schemes  many learning systems use distance functions incorporate various weighting
schemes distance calculations  wettschereck  aha   mohri        atkeson  moore  
schaal         improvements presented paper independent schemes 
various weighting schemes  as well enhancements instance pruning
techniques  used conjunction new distance functions presented here 
     attribute types
none distance functions shown figure    including euclidean distance  appropriately
handle non continuous input attributes 
attribute linear nominal  linear attribute continuous discrete 
continuous  or continuously valued  attribute uses real values  mass planet
velocity object  linear discrete  or integer  attribute discrete set
linear values  number children 
argued value stored computer discrete level  reason
continuous attributes treated differently many different values
value may appear rarely  perhaps particular application   causes
problems algorithms vdm  described section      depend testing two
values equality  two continuous values rarely equal  though may
quite close other 
nominal  or symbolic  attribute discrete attribute whose values necessarily
linear order  example  variable representing color might values red 
green  blue  brown  black white  could represented integers     
respectively  using linear distance measurement         values makes little
sense case 
     heterogeneous euclidean overlap metric  heom 
one way handle applications continuous nominal attributes use
heterogeneous distance function uses different attribute distance functions different
kinds attributes  one approach used use overlap metric nominal
attributes normalized euclidean distance linear attributes 
purposes comparison testing  define heterogeneous distance function
similar used ib   ib  ib   aha  kibler   albert        aha        well
used giraud carrier   martinez         function defines distance
two values x given attribute as 
x unknown  else
  

da  x  y    overlap x  y   nominal  else
rn  diff  x  y 



   

unknown attribute values handled returning attribute distance    i e   maximal
distance  either attribute values unknown  function overlap rangenormalized difference rn diff defined as 
   x  
overlap x  y   
   otherwise

 

   

fiwilson   martinez

rn  diff  x  y   

  x y 
rangea

   

value rangea used normalize attributes  defined as 
rangea  maxa  mina

   

max mina maximum minimum values  respectively  observed
training set attribute a  means possible new input vector value
outside range produce difference value greater one  however  cases
rare  occur  large difference may acceptable anyway  normalization
serves scale attribute point differences almost always less one 
definition da returns value  typically  range       whether
attribute nominal linear  overall distance two  possibly heterogeneous  input
vectors x given heterogeneous euclidean overlap metric function heom x y  


da  xa   ya   

heom x  y   

   

a  

distance function removes effects arbitrary ordering nominal values 
overly simplistic approach handling nominal attributes fails make use additional
information provided nominal attribute values aid generalization 
     value difference metric  vdm 
value difference metric  vdm  introduced stanfill waltz        provide
appropriate distance function nominal attributes  simplified version vdm  without
weighting schemes  defines distance two values x attribute as 
c na x c

na y c
vdma  x  y   

n
na y
a x
c  

q

c

  pa x c pa y c

q

   

c  


na x number instances training set value x attribute a 
na x c number instances value x attribute output class c 
c number output classes problem domain 
q constant  usually     
p a x c conditional probability output class c given attribute
value x  i e   p c   xa   seen      pa x c defined as 
pa x c  

na x c
na x

   

na x sum na x c classes  i e  
c

na x   na x c
c  

 

    

fiimproved heterogeneous distance functions

sum pa x c c classes   fixed value x 
using distance measure vdma x y   two values considered closer
similar classifications  i e   similar correlations output classes   regardless
order values may given in  fact  linear discrete attributes values
remapped randomly without changing resultant distance measurements 
example  attribute color three values red  green blue  application
identify whether object apple  red green would considered closer
red blue former two similar correlations output class apple 
original vdm algorithm  stanfill   waltz        makes use feature weights
included equations  variants vdm  cost   salzberg       
rachlin et al         domingos        used alternate weighting schemes  discussed
earlier  new distance functions presented paper independent schemes
cases make use similar enhancements 
one problem formulas presented define
done value appears new input vector never appeared training set 
attribute never value x instance training set  na x c c   
n a x  which sum na x c classes     cases p a x c       
undefined  nominal attributes  way know probability
value  since inherent ordering values  paper assign
p a x c default value   cases  though possible let pa x c     c  c
number output classes  since sum pa x c c      c always      
distance function used directly continuous attributes  values
potentially unique  case na x   every value x  na x c   one value c
  others given value x  addition  new vectors likely unique
values  resulting division zero problem above  even value   substituted
     resulting distance measurement nearly useless 
even values unique  often enough different values continuous
attribute statistical sample unreliably small value  distance measure
still untrustworthy  problems  inappropriate use vdm directly
continuous attributes 
     discretization
one approach problem using vdm continuous attributes discretization
 lebowitz        schlimmer        ventura         models used vdm
variants  cost   salzberg        rachlin et al         mohri   tanaka       
discretized continuous attributes somewhat arbitrary number discrete ranges 
treated values nominal  discrete unordered  values  method advantage
generating large enough statistical sample nominal value p values
significance  however  discretization lose much important information available
continuous values  example  two values discretized range considered
equal even opposite ends range  effects reduce generalization
accuracy  ventura   martinez        
paper propose three new alternatives  presented following three
sections  section   presents heterogeneous distance function uses euclidean distance
linear attributes vdm nominal attributes  method requires careful attention
 

fiwilson   martinez

problem normalization neither nominal linear attributes regularly given
much weight 
sections     present two distance functions  interpolated value difference
metric  ivdm  windowed value difference metric  wvdm   use discretization
collect statistics determine values pa x c continuous values occurring training
set instances  retain continuous values later use  generalization 
value pa y c continuous value interpolated two values p  namely 
p a x  c pa x  c  x   x   ivdm wvdm essentially different techniques
nonparametric probability density estimation  tapia   thompson       
determine values p class  generic version vdm algorithm  called
discretized value difference metric  dvdm  used comparisons two new
algorithms 

   heterogeneous value difference metric  hvdm 
discussed previous section  euclidean distance function inappropriate
nominal attributes  vdm inappropriate continuous attributes  neither sufficient
use heterogeneous application  i e   one nominal continuous
attributes 
section  define heterogeneous distance function hvdm returns distance
two input vectors x y  defined follows 
hvdm x  y   



da   xa   ya  

    

a  

number attributes  function da x y  returns distance two
values x attribute defined as 
x unknown  otherwise   
  

da  x  y    normalized  vdma  x  y   nominal
normalized  diff  x  y   linear



    

function da x y  uses one two functions  defined section       depending
whether attribute nominal linear  note practice square root     
typically performed distance always positive  nearest neighbor s  still
nearest whether distance squared  however  models  e g  
distance weighted k nearest neighbor  dudani        require square root
evaluated 
many applications contain unknown input values must handled appropriately
practical system  quinlan         function da x y  therefore returns distance   either
x unknown  done aha  kibler   albert        giraud carrier   martinez
        complicated methods tried  wilson   martinez        
little effect accuracy 
function hvdm similar function hoem given section      except
 

fiimproved heterogeneous distance functions

uses vdm instead overlap metric nominal values normalizes differently 
similar distance function used rise      domingos        
important differences noted section     
section     presents three alternatives normalizing nominal linear attributes 
section     presents experimental results show one schemes provides better
normalization two set several datasets  section     gives empirical results
comparing hvdm two commonly used distance functions 
     normalization
discussed section      distances often normalized dividing distance
variable range attribute  distance input variable range
      policy used heom section      however  dividing range allows
outliers  extreme values  profound effect contribution attribute 
example  variable values range       almost every case one
exceptional  and possibly erroneous  value     dividing range would almost
always result value less      robust alternative presence outliers
divide values standard deviation reduce effect extreme values typical
cases 
new heterogeneous distance metric hvdm  situation complicated
nominal numeric distance values come different types measurements 
numeric distances computed difference two linear values  normalized
standard deviation  nominal attributes computed sum c differences
probability values  where c number output classes   therefore necessary find
way scale two different kinds measurements approximately range
give variable similar influence overall distance measurement 
since     values normal distribution fall within two standard deviations
mean  difference numeric values divided   standard deviations scale
value range usually width    function normalized diff therefore defined
shown equation    
normalized  diff  x  y   

xy
 

    

standard deviation numeric values attribute a 
three alternatives function normalized vdm considered use
heterogeneous distance function  labeled n   n  n   definitions
given below 
n   normalized  vdm a  x  y   

c n
a x c



c  

n   normalized  vdm   x  y   

c n
a x c



c  

 

na x

na x



na y c



na y c

    

na y

na y

 

    

fiwilson   martinez

c n
a x c

n   normalized  vdm a  x  y    c  

c  

na x



na y c

 

    

na y

function n  equation     q    similar formula used pebls
 rachlin et al         rise  domingos        nominal attributes 
n  uses q    thus squaring individual differences  analogous using euclidean
distance instead manhattan distance  though slightly expensive computationally 
formula hypothesized robust n  favors class
correlations fairly similar rather close different  n 
would able distinguish two  practice square root taken 
individual attribute distances squared hvdm function 
n  function used heterogeneous radial basis function networks  wilson  
martinez         hvdm first introduced 
     normalization experiments
order determine whether normalization scheme n   n  n  gave unfair weight
either nominal linear attributes  experiments run    databases machine
learning database repository university california  irvine  merz   murphy        
datasets experiment least nominal linear attributes 
thus require heterogeneous distance function 
experiment  five fold cross validation used  five trials 
distance instance test set instance training set
computed  computing distance attribute  normalized diff function
used linear attributes  normalized vdm function n   n   n  used  in
three respective experiments  nominal attributes 
average distance  i e   sum distances divided number comparisons 
computed attribute  average linear attributes database
computed averages listed heading avglin table   

database
anneal
australian
bridges
crx
echocardiogram
flag
heart
heart cleveland
heart hungarian
heart long beach va
heart more
heart swiss
hepatitis
horse colic
soybean large
average

avglin
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

n 
avgnom
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

n 
avgnom
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

n 
avgnom
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

 nom 
  
 
 
 
 
  
 
 
 
 
 
 
  
  
  
  

 lin 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 

table    average attribute distance linear nominal attributes 
  

 c
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 

fiimproved heterogeneous distance functions

average distance

average distance

average distance

averages nominal attributes three normalization schemes
listed headings avgnom table   well  average distance linear
variables exactly regardless whether n   n  n  used  average
given once  table   lists number nominal   nom   number linear
  lin   attributes database  along number output classes   c  
seen overall averages first four columns last row
table    n  closer n  n   however  important understand reasons behind
difference order know normalization scheme n  robust general 
figures     graphically display averages shown table   headings n   n 
n   respectively  ordered left right number output classes 
hypothesized number output classes grows  normalization would get worse
n  indeed appropriate add scaling factor c sum  length
line indicates much difference
average distance nominal attributes
nominal
 
linear attributes  ideal normalization scheme
linear
would difference zero  longer lines
  
indicate worse normalization 
  
number output classes grows 
  
difference n  linear distances
  
nominal distances grows wider cases 
n   hand  seems remain quite close
 
                               avg
independent number output classes 
number output classes
interestingly  n  almost poorly n   even
figure    average distances n  
though use scaling factor c 
apparently squaring factor provides
nominal
 
well rounded distance metric nominal attributes
linear
similar provided using euclidean distance
  
instead manhattan distance linear attributes 
  
underlying hypothesis behind performing
  
normalization proper normalization
  
typically improve generalization accuracy 
nearest neighbor classifier  with k    
 
                               avg
implemented using hvdm distance metric 
number output classes
system tested heterogeneous
figure    average distances n  
datasets appearing table   using three
different normalization schemes discussed above 
 
nominal
using ten fold cross validation  schaffer        
linear
results summarized table   
  
normalization schemes used training sets
  
test sets trial  bold entries indicate
  
scheme highest accuracy 
  
asterisk indicates difference greater
   next highest scheme 
 
                               avg
seen table  normalization
number output classes
scheme n  highest accuracy  n 
figure    average distances n  
  

fiwilson   martinez

substantially lower two  n 
database
n 
n 
n 
n  highest accuracy
anneal
     
           
  domains  significantly  n 
australian
     
           
bridges
     
           
   higher   times compared n 
crx
     
           
   higher one dataset 
echocardiogram
     
           
n  higher two
flag
     
            
heart cleveland
     
            
one dataset  lower average
heart hungarian
     
            
accuracy n  
heart long beach va
     
            
results support hypothesis
heart more
     
           
heart
     
           
normalization scheme n 
heart swiss
     
            
achieves higher generalization accuracy
hepatitis
     
           
n  n   on datasets  due
horse colic
                  
soybean large
     
            
robust normalization though
average
     
           
accuracy n  almost good n  
note proper normalization
table    generalization accuracy
using n   n  n  
always necessarily improve generalization
accuracy  one attribute
important others classification  giving higher weight may improve
classification  therefore  important attribute given higher weight accidentally
poor normalization  may actually improve generalization accuracy  however 
random improvement typically case  proper normalization improve
generalization cases used typical applications 
consequence results  n  used normalization scheme hvdm 
function normalized vdm defined      
     empirical results hvdm vs  euclidean hoem
nearest neighbor classifier  with k    using three distance functions listed table  
tested    datasets uci machine learning database repository     datasets 
results obtained    datasets least nominal attributes shown
table   
results approximately equivalent datasets linear attributes  results
remaining datasets shown here  found section       fold crossvalidation used  three distance metrics used training sets test sets
trial 
results experiments shown table    first column lists name
database   test means database originally meant used test set 
instead used entirety separate database   second column shows results
obtained using euclidean distance function normalized standard deviation
attributes  including nominal attributes  next column shows generalization accuracy
obtained using hoem metric  uses range normalized euclidean distance linear
attributes overlap metric nominal attributes  final column shows accuracy
obtained using hvdm distance function uses standard deviation normalized
euclidean distance  i e   normalized diff defined equation     linear attributes
normalized vdm function n  nominal attributes 
highest accuracy obtained database shown bold  entries euclid 
  

fiimproved heterogeneous distance functions

hoem columns significantly
higher hvdm  at     higher
confidence level  using two tailed
paired test  marked
asterisk     
entries
significantly lower hvdm
marked less than sign     
seen table   
hvdm distance functions overall
average accuracy higher
two metrics    
hvdm achieved high higher
generalization accuracy
two distance functions      
datasets  euclidean distance
function highest    datasets 
hoem highest   
datasets 
hvdm significantly higher
euclidean distance function
   datasets  significantly lower
   similarly  hvdm higher
hoem   datasets 
significantly lower   
results support hypothesis
hvdm handles nominal attributes
appropriately euclidean
distance heterogeneous
euclidean overlap metric  thus
tends achieve higher generalization
accuracy typical applications 

database
euclid 
anneal
     
audiology
       
audiology test
       
australian
     
bridges
     
crx
     
echocardiogram
     
flag
       
heart cleveland
     
heart hungarian
       
heart long beach va      
heart more
     
heart swiss
       
hepatitis
     
horse colic
     
house votes   
       
image segmentation
     
led   
       
led creator
       
monks   test
     
monks   test
       
monks   test
       
mushroom
      
promoters
       
soybean large
       
soybean small
      
thyroid allbp
     
thyroid allhyper
     
thyroid allhypo
     
thyroid allrep
     
thyroid dis
     
thyroid hypothyroid      
thyroid sick euthyroid      
thyroid sick
       
zoo
     
average 
     

hoem
     
       
     
     
     
     
     
     
     
     
       
     
     
     
     
       
     
       
       
     
       
       
      
       
     
      
     
     
       
     
     
     
     
       
     
     

hvdm
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      
     
     
      
     
     
     
     
     
     
     
     
     
     

table    generalization accuracy
euclidean  hoem  hvdm distance functions 

   interpolated value difference metric  ivdm 
section section   introduce distance functions allow vdm applied
directly continuous attributes  alleviates need normalization attributes 
cases provides better measure distance continuous attributes linear
distance 
example  consider application input attribute height output class
indicates whether person good candidate fighter pilot particular airplane 
individuals heights significantly preferred height might
considered poor candidates  thus could beneficial consider heights
similar preferred height  even though farther apart
linear sense 

  

fiwilson   martinez

hand  linear attributes linearly distant values tend indicate different
classifications handled appropriately  interpolated value difference metric
 ivdm  handles situations  handles heterogeneous applications robustly 
generic version vdm distance function  called discretized value difference
metric  dvdm  used comparisons extensions vdm presented paper 
     ivdm learning algorithm
original value difference metric  vdm  uses statistics derived training set
instances determine probability pa x c output class c given input value x
attribute a 
using ivdm  continuous values discretized equal width intervals  though
continuous values retained later use   integer supplied user 
unfortunately  currently little guidance value use  value
large reduce statistical strength values p  value small allow
discrimination among classes  purposes paper  use heuristic
determine automatically  let   c  whichever greatest  c number
output classes problem domain  current research examining sophisticated
techniques determining good values s  cross validation  statistical
methods  e g   tapia   thompson        p        early experimental results indicate
value may critical long c n  n number instances
training set  
width wa discretized interval attribute given by 
wa  

maxa mina


    

max mina maximum minimum value  respectively  occurring
training set attribute a 
example  consider iris database uci machine learning databases 
iris database four continuous input attributes  first sepal length  let
training set consisting         available training instances  test set
consisting remaining     
one division training set  values sepal length attribute ranged
         three output classes database  let s    resulting
width                         note since discretization part learning process 
would unfair use instances test set help determine discretize
values  discretized value v continuous value x attribute integer   s 
given by 
x  discrete  else

v   discretizea  x    s  x   max   else
 x min     w    




    

deciding upon finding w a  discretized values continuous attributes
  

fiimproved heterogeneous distance functions

used discrete values nominal attributes finding pa x c  figure   lists pseudo code
done 
learnp training set t 
attribute
instance
let x input value attribute instance i 
v   discretizea x   which x discrete 
let c output class instance i 
increment na v c   
increment na v   
discrete value v  of attribute a 
class c
na v  
pa v c  
else pa v c   na v c   na v
return   d array pa v c 

figure    pseudo code finding pa x c 

probability

first attribute iris database  values pa x c displayed figure   
five discretized ranges x  probability three corresponding
output classes shown bar heights  note heights three bars sum    
discretized range  bold integers indicate discretized value range 
example  sepal length greater equal      less      would discretized
value   
   

   
   
   
   
   
   
   
   
   
   
 

    

output class 
   iris
setosa

    
    
    

    
    
   

 

    

         

   iris
viginica

         

 

   iris
versicolor

    

   

 

    

    

 

sepal length  in cm 

       
      

   

bold  
discretized
range number 

figure    pa x c a    x       c       first attribute iris database 
     ivdm dvdm generalization
thus far dvdm ivdm algorithms learn identically  however  point dvdm
algorithm need retain original continuous values use discretized
values generalization  hand  ivdm use continuous values 
generalization  algorithm nearest neighbor classifier use distance
function dvdm  defined follows 
dvdm x  y   



vdma  discretizea  xa    discretizea  ya   

a  

  

 

    

fiwilson   martinez

discretizea defined equation      vdma defined equation     
q    repeat convenience 
vdma  x  y   

c

pa x c pa y c

 

    

c  

unknown input values  quinlan        treated simply another discrete value  done
 domingos        

a 
b 

 
   
   

y 

   

input attributes
 
 
   
   
   
   
   

   

 
   
   

  
  

output class
   iris setosa 
   iris versicolor 

   

table    example iris database 
example  consider two training instances b shown table    new
input vector classified  attribute a    discretized values a  b       
   respectively  using values figure    distance attribute   is 
                                               

probability class  

distance b    since discretized value 
note b values different ends range    actually nearly
close are  spite fact  discretized distance function says b
equal happen fall discretized range 
ivdm uses interpolation alleviate problems  ivdm assumes pa x c values
hold true midpoint range  interpolates midpoints find p
attribute values 
figure   shows p values second output class  iris versicolor  function
first attribute value  sepal length   dashed line indicates p value used dvdm 
solid line shows ivdm uses 
   
   
   
   
   
   
   
   
   
   
 

center
points
dvdm
ivdm

 

   

 

      

             
      
sepal length  in cm 

   

bold  
discretized
range number 

figure    p  x   values dvdm ivdm attribute    class   iris database 
  

fiimproved heterogeneous distance functions

distance function interpolated value difference metric defined as 
ivdm x  y   



ivdma  xa   ya   

    

a  

ivdma defined as 
discrete
vdma  x  y  
c
 
ivdma  x  y   
p  x  pa c  y    otherwise
a c
c  

    

formula determining interpolated probability value pa c x  continuous value x
attribute class c is 


x mida u
pa c  x    pa u c  
   pa u   c pa u c  
mida u   mida u

    

equation  mida u mida u   midpoints two consecutive discretized ranges
mida u x   mida u    pa u c probability value discretized range u 
taken probability value midpoint range u  and similarly pa u   c  
value u found first setting u   discretizea x   subtracting   u x   mida u 
value mida u found follows 
mida u   mina   widtha    u    

    

probability class

figure   shows values pa c x  attribute a   iris database three output
classes  i e  c           since data points outside range mina  maxa 
probability value pa u c taken   u     u   s  seen visually
diagonal lines sloping toward zero outer edges graph  note sum
probabilities three output classes sum     every point midpoint range  
midpoint range   
   
   
   
   
   
   
   
   
   
   
 

output class 
   iris setosa
   iris versicolor
   iris viginica

 

   

 

      

             
      
sepal length  in cm 

   

bold  
discretized
range number 

figure    interpolated probability values attribute   iris database 
  

fiwilson   martinez


b

value
   
   

p     v 
    
    

p     v 
    
    

p     v 
    
    



   

    

    

    

ivdm  v y 
    
    

vdm  v y 
    
 

table    example ivdm vs  vdm 
using ivdm example instances table    values first attribute
discretized dvdm  used find interpolated probability values 
example  value      p  c x  interpolates midpoints      returning
values shown table   three classes  instance value     
falls midpoints      instance b value      falls
midpoints     
seen table    ivdm  using single attribute distance function ivdm 
returns distance indicates closer b  for first attribute  
certainly case here  dvdm  using
discretized vdm   hand  returns
database
dvdm
ivdm
annealing
     
       
distance indicates value
australian
       
     
equal b  quite far a 
bridges
     
     
illustrating problems involved
credit screening
     
     
echocardiogram
      
      
using discretization 
flag
     
     
ivdm dvdm algorithms
glass
     
       
implemented tested    datasets
heart disease
     
     
heart  cleveland 
     
     
uci machine learning databases 
heart
 hungarian 
     
     
results    datasets contain
heart  long beach va       
     
least continuous attributes
heart  more 
     
     
shown table     since ivdm
heart  swiss 
     
     
hepatitis
     
     
dvdm equivalent domains
horse colic
     
     
discrete attributes  results
image segmentation
     
     
remaining datasets deferred section
ionosphere
     
     
iris
     
     
       fold cross validation
liver disorders
     
     
used  average accuracy
pima indians diabetes
     
     
database    trials shown
satellite image
     
       
shuttle
     
       
table    bold values indicate value
sonar
     
     
highest dataset  asterisks    
thyroid  allbp 
     
     
indicates difference statistically
thyroid  allhyper 
     
       
thyroid  allhypo 
     
       
significant     confidence level
thyroid  allrep 
     
       
higher  using two tailed paired t test 
thyroid  dis 
     
     
set datasets  ivdm
thyroid  hypothyroid 
     
       
higher average generalization accuracy
thyroid  sick 
     
       
thyroid  sick euthyroid       
       
overall discretized algorithm 
vehicle
     
       
ivdm obtained higher generalization
vowel
     
       
accuracy dvdm      
wine
     
       
average 
     
     
cases     significant
    level above  dvdm higher
table    generalization dvdm vs  ivdm 
  

fiimproved heterogeneous distance functions

accuracy   cases  one difference statistically significant 
results indicate interpolated distance function typically appropriate
discretized value difference metric applications one continuous
attributes  section   contains comparisons ivdm distance functions 

   windowed value difference metric  wvdm 
ivdm algorithm thought sampling value pa u c midpoint mida u
discretized range u  p sampled first finding instances value
attribute range mida u w      na u incremented instance 
n a u c incremented instance whose output class c 
p a u c   na u c   na u computed  ivdm interpolates sampled points
provide continuous rough approximation function pa c x   possible sample p
points thus provide closer approximation function pa c x   may
turn provide accurate distance measurements values 
figure   shows pseudo code windowed value difference metric  wvdm  
wvdm samples value p a x c value x occurring training set
define 
instance a     n  list n instances sorted ascending order attribute a 
instance a  i  val a  value attribute instance a  i  
x
center value current window  i e   x instance a  i  val a  
p a  i  c 
probability pa x c output class c given input value x
attribute a  note index  value itself 
n c 
number na x c instances current window output class c 
n
total number na x instances current window 
instance a  in  first instance window 
instance a  out  first instance outside window   i e   window contains
instances instance a  in  out     
w a 
window width attribute a 
learnwvdm training set t 
continuous attribute
sort instance a     n  ascending order attribute a  using quicksort 
initialize n n c        i e   start empty window  
i    n
let x instance a  i  val a  
   expand window include instances range
 out   n   instance a  out  val a     x   w a     
increment n c   c the class instance a  out  
increment n 
increment out 
   shrink window exclude instances longer range
 in   out   instance a  in  val a     x   w a     
decrement n c   c the class instance a  in  
decrement n 
increment in 
   compute probability value class current window
class c    c
p a  i  c    n c    n   i e   pa x c   na x c   na x  
return   d array p a  i  c  

figure    pseudo code wvdm learning algorithm 
  

fiwilson   martinez

attribute a  instead midpoints range  fact  discretized ranges
even used wvdm continuous attributes  except determine appropriate window
width  wa  range width used dvdm ivdm  pseudo code
learning algorithm used determine pa x c attribute value x given figure   
value x occurring training set attribute a  p sampled finding
instances value attribute range x w      computing na x 
na x c  pa x c   na x c   na x before  thus  instead fixed number sampling
points  window instances  centered training instance  used determining
probability given point  technique similar concept shifted histogram estimators
 rosenblatt        parzen window techniques  parzen        
attribute values sorted  using o nlogn  sorting algorithm  allow
sliding window used thus collect needed statistics o n  time attribute 
sorted order retained attribute binary search performed o log
n  time generalization 
values occurring sampled points interpolated ivdm  except
many points available  new value interpolated two
closer  precise values ivdm 
wvdm find p attribute a continuous value x 
   find pa x c c    c  given value x attribute a 
find instance a  i  val a  x instance a  i    val a   binary search  
x    instance a  i  val a 
 unless i    case x  min a     w a       
x    instance a  i    val a   unless i n  case x  max a     w a       
class c    c
p  p a  i  c 
 unless i    case p    
p  p a  i    c   unless i n  case p    
pa x c   p      x x    x  x       p    p  
return array pa x    c 

figure     pseudo code wvdm probability interpolation  see figure   definitions  
pseudo code interpolation algorithm given figure     algorithm takes
value x attribute returns vector c probability values pa x c c    c  first
binary search find two consecutive instances sorted list instances
attribute surround x  probability class interpolated
stored two surrounding instances   the exceptions noted parenthesis handle
outlying values interpolating towards   done ivdm  
probability values input vectors attribute values computed 
used vdm function discrete probability values are 
wvdm distance function defined as 
wvdm x  y   



wvdma  xa   ya   

    

a  

wvdma defined as 
discrete
vdma  x  y  
c
 
wvdma  x  y   
p
pa y c   otherwise
a x c
c  

  

    

fiprobability class

improved heterogeneous distance functions

   
   
   
   
   
   
   
   
   
   
 

output class 
   iris setosa
   iris versicolor
   iris viginica

 

 

 

 

 

sepal length  in cm 

figure     example wvdm probability landscape 
pa x c interpolated probability value continuous value x computed
figure     note typically finding distance new input vector
instance training set  since
instances training set used
database
dvdm wvdm
define probability attribute
annealing
     
     
australian
     
     
values  binary search interpolation
bridges
     
     
unnecessary training instances
credit screening
     
     
immediately recall stored
echocardiogram
      
     
probability values  unless pruning techniques
flag
     
     
glass
     
       
used 
heart disease
     
     
one drawback approach
heart  cleveland 
     
     
increased storage needed retain c
heart  hungarian 
     
     
heart  long beach va       
     
probability values attribute value
heart  more 
     
     
training set  execution time
heart  swiss 
     
     
significantly increased ivdm
hepatitis
     
     
horse colic
     
     
dvdm   see section     discussion
image
segmentation
     
     
efficiency considerations  
ionosphere
     
     
figure    shows probability values
iris
     
     
liver disorders
     
     
three classes first attribute
pima indians diabetes
     
     
iris database again  time using
satellite image
     
       
windowed sampling technique  comparing
shuttle
     
       
figure    figure   reveals
sonar
     
     
thyroid  allbp 
     
     
attribute ivdm provides approximately
thyroid  allhyper 
     
     
overall shape  misses much
thyroid  allhypo 
     
     
detail  example  peak occurring
thyroid  allrep 
     
     
thyroid  dis 
     
     
output class   approximately sepal
thyroid  hypothyroid 
     
       
length       figure   flat line
thyroid  sick 
     
       
misses peak entirely  due mostly
thyroid  sick euthyroid       
       
vehicle
     
       
somewhat arbitrary position
vowel
     
       
midpoints probability values
wine
     
       
sampled 
average 
     
     
table   summarizes results testing
table    generalization wvdm vs  dvdm 
  

fiwilson   martinez

wvdm algorithm datasets dvdm ivdm  bold entry indicates
highest two accuracy measurements  asterisk     indicates difference
statistically significant     confidence level  using two tailed paired t test 
set databases  wvdm average      accurate dvdm
overall  wvdm higher average accuracy dvdm       databases 
significantly higher    dvdm higher    databases  none
differences statistically significant 
section   provides comparisons wvdm distance functions  including
ivdm 

   empirical comparisons analysis distance functions
section compares distance functions discussed paper  nearest neighbor
classifier implemented using six different distance functions  euclidean
 normalized standard deviation  hoem discussed section    hvdm discussed
section    dvdm ivdm discussed section    wvdm discussed section
   figure    summarizes definition distance function 
functions use
overall distance function 

d x  y   



da  xa   ya   

a  

distance
function
euclidean

definition da xa ya  attribute type 
linear
continuous
discrete nominal
xa ya
xa ya



hoem

xa ya
rangea

  xa   ya
  xa ya

hvdm

xa ya
 

vdma  xa   ya  

dvdm

vdma disca xa  disca ya  

vdma  xa   ya  

ivdm

ivdma xa ya 
interpolate probabilities
range midpoints 

vdma  xa   ya  

wvdm

wvdma xa ya 
interpolate probabilities
adjacent values 

vdma  xa   ya  

rangea   maxa mina   vdma  x  y   

c

pa x c pa y c

 

c  

figure     summary distance function definitions 
distance function tested    datasets uci machine learning databases 
  

fiimproved heterogeneous distance functions

using    fold cross validation  average accuracy    trials reported
test table    highest accuracy achieved dataset shown bold 
names three new distance functions presented paper  hvdm  ivdm wvdm 
shown bold identify them 
table   lists number instances database   inst    number
continuous  con   integer  int  i e   linear discrete   nominal  nom  input attributes 

database
euclid hoem
annealing
     
     
audiology
     
     
audiology  test 
     
     
australian
     
     
breast cancer
     
     
bridges
     
     
credit screening
     
     
echocardiogram
     
     
flag
     
     
glass
     
     
heart disease
     
     
heart  cleveland 
     
     
heart  hungarian 
     
     
heart  long beach va 
     
     
heart  more 
     
     
heart  swiss 
     
     
hepatitis
     
     
horse colic
     
     
house votes   
     
     
image segmentation
     
     
ionosphere
     
     
iris
     
     
led    noise
     
     
led
     
     
liver disorders
     
     
monks  
     
     
monks  
     
     
monks  
     
     
mushroom
             
pima indians diabetes
     
     
promoters
     
     
satellite image
     
     
shuttle
     
     
sonar
     
     
soybean  large 
     
     
soybean  small 
             
thyroid  allbp 
     
     
thyroid  allhyper 
     
     
thyroid  allhypo 
     
     
thyroid  allrep 
     
     
thyroid  dis 
     
     
thyroid  hypothyroid 
     
     
thyroid  sick euthyroid       
     
thyroid  sick 
     
     
vehicle
     
     
vowel
     
     
wine
     
     
zoo
     
     
average 
     
     

n c e
hvdm
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     
     
     
     
     

f u n
dvdm
     
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     
     
     
     
     

c n
  inputs
ivdm wvdm  inst  con int nom
     
     
   
   
  
     
     
   
   
  
     
     
  
   
  
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
      
     
   
   
 
     
     
   
   
  
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
     
          
   
 
     
     
   
   
 
     
     
   
   
  
     
     
   
   
  
     
     
   
   
  
     
     
        
 
     
     
        
 
     
     
   
   
 
     
           
   
  
     
          
   
 
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
             
   
   
 
                  
   
  
     
     
   
   
 
     
     
   
   
  
     
               
 
     
          
   
 
     
     
        
 
     
     
   
   
  
             
  
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
     
        
 
     
     
        
 
     
     
        
 
     
     
  
   
  
     
     

table    summary generalization accuracy

  

fiwilson   martinez

set    datasets  three new distance functions  hvdm  ivdm wvdm 
substantially better euclidean distance hoem  ivdm highest average accuracy
         almost    higher average euclidean distance           indicating
robust distance function datasets  especially nominal
attributes  wvdm slightly lower ivdm        accuracy  somewhat
surprisingly  dvdm slightly higher hvdm datasets  even though uses
discretization instead linear distance continuous attributes  four vdm based
distance functions outperformed euclidean distance hoem 
   datasets  euclidean distance highest accuracy    times  hoem
highest   times  hvdm      dvdm      ivdm     wvdm     
datasets continuous attributes  four vdm based distance functions
 hvdm  dvdm  ivdm wvdm  equivalent  datasets  vdm based
distance functions achieve average accuracy       compared       hoem
      euclidean  indicating substantial superiority problems 
datasets nominal attributes  euclidean hvdm equivalent 
distance functions perform average except dvdm  averages
   less others  indicating detrimental effects discretization  euclidean
hoem similar definitions applications without nominal attributes  except
euclidean normalized standard deviation hoem normalized range
attribute  interesting average accuracy datasets slightly higher
euclidean hoem  indicating standard deviation may provide better normalization
datasets  however  difference small  less      datasets
contain many outliers  difference probably negligible case 
one disadvantage scaling attributes standard deviation attributes
almost always value  e g   boolean attribute almost always   
given large weightnot due scale  relative frequencies attribute
values  related problem occur hvdm  skewed class distribution
 i e   many instances classes others   p values quite
small classes quite large others  either case difference  pa x c   pa y c 
correspondingly small  thus nominal attributes get little weight
compared linear attributes  phenomenon noted ting              
recognized problems hypothyroid dataset  future research address
normalization problems look automated solutions  fortunately  dvdm  ivdm
wvdm suffer either problem  attributes scaled amount
cases  may part account success hvdm
experiments 
datasets nominal continuous attributes  hvdm slightly higher
euclidean distance datasets  turn slightly higher hoem  indicating
overlap metric may much improvement heterogeneous databases 
dvdm  ivdm wvdm higher euclidean distance datasets 
ivdm lead 
     effects sparse data
distance functions use vdm require statistics determine distance  therefore
hypothesized generalization accuracy might lower vdm based distance functions
  

fiimproved heterogeneous distance functions

euclidean distance hoem little data available  vdmbased functions would increase accuracy slowly others instances
made available  sufficient number instances allowed reasonable sample size
determine good probability values 

     

 average generalization accuracy

     

     

euclidean
     
hoem
hvdm

     

dvdm
ivdm

     

wvdm
     
 

  

  
  
 instances used

  

   

figure     average accuracy amount data increases 
test hypothesis  experiments used obtain results shown table  
repeated using part available training data  figure    shows generalization
accuracy test set improves percentage available training instances used
learning generalization increased          generalization accuracy values
shown averages    datasets table   
surprisingly  vdm based distance functions increased accuracy fast faster
euclidean hoem even little data available  may
little data available  random positioning sample data input space
greater detrimental affect accuracy error statistical sampling vdm based
functions 
interesting note figure    six distance functions seem pair
three distinct pairs  interpolated vdm based distance functions  ivdm wvdm 
maintain highest accuracy  two vdm based functions next  functions
based linear overlap distance remain lowest early graph 
  

fiwilson   martinez

     efficiency considerations
section considers storage requirements  learning speed  generalization speed
algorithms presented paper 
       storage
distance functions must store entire training set  requiring o nm  storage 
n number instances training set number input attributes
application  unless instance pruning technique used  euclidean hoem
functions  necessary  even amount storage restrictive n
grows large 
hvdm  dvdm  ivdm  probabilities pa x c attributes  only discrete
attributes hvdm  must stored  requiring o mvc  storage  v average number
attribute values discrete  or discretized  attributes c number output
classes application  possible instead store array da x y   vdma x y  hvdm
dvdm  storage would o mv    savings c   v 
wvdm  c probability values must stored continuous attribute value 
resulting o nmc  storage typically much larger o mvc  n usually
much larger v  and cannot less   necessary store list  pointers to 
instances attribute  requiring additional o mn  storage  thus total storage
wvdm o  c   nm    o cnm  
distance function
euclidean
hoem
hvdm
dvdm
ivdm
wvdm

storage
o mn 
o mn 
o mn mvc 
o mn mvc 
o mn mvc 
o cmn 

learning time
o mn 
o mn 
o mn mvc 
o mn mvc 
o mn mvc 
o mnlogn mvc 

generalization time
o mn 
o mn 
o mnc  o mn 
o mnc  o mn 
o mnc  o mn 
o mnc 

table    summary efficiency six distance metrics 
table   summarizes storage requirements system  wvdm one
distance functions requires significantly storage others 
applications  n critical factor  distance functions could used
conjunction instance pruning techniques reduce storage requirements  see section  
list several techniques reduce number instances retained training set
subsequent generalization 
       l earning speed
takes nm time read training set  takes additional  nm time find standard
deviation attributes euclidean distance  nm time find ranges hoem 
computing vdm statistics hvdm  dvdm ivdm takes mn mvc time 
approximately o mn   computing wvdm statistics takes mnlogn mnc time 
approximately o mnlogn  
general  learning time quite acceptable distance functions 
  

fiimproved heterogeneous distance functions

       generalization speed
assuming distance function must compare new input vector training instances 
euclidean hoem take o mn  time  hvdm  ivdm dvdm take o mnc   unless
da x y stored instead pa x c hvdm  case search done o mn 
time   wvdm takes o logn mnc    o mnc  time 
though c typically fairly small  generalization process require
significant amount time and or computational resources n grows large  techniques
k d trees  deng   moore        wess  althoff   derwand        sproull       
projection  papadimitriou   bentley        reduce time required locate nearest
neighbors training set  though algorithms may require modification handle
continuous nominal attributes  pruning techniques used reduce storage  as section
       reduce number instances must searched generalization 

   related work
distance functions used variety fields  including instance based learning  neural
networks  statistics  pattern recognition  cognitive psychology  see section  
references   section   lists several commonly used distance functions involving numeric
attributes 
normalization often desirable using linear distance function euclidean
distance attributes arbitrarily get weight others  dividing
range standard deviation normalize numerical attributes common practice  turney
       turney   halasz        investigated contextual normalization  standard
deviation mean used normalization continuous attributes depend context
input vector obtained  paper attempt use contextual
normalization  instead use simpler methods normalizing continuous attributes 
focus normalize appropriately continuous nominal attributes 
value distance metric  vdm  introduced stanfill   waltz         uses
attribute weights used functions presented paper  modified value
difference metric  mvdm   cost   salzberg        rachlin et al         use attribute
weights instead uses instance weights  assumed systems use discretization
 lebowitz        schlimmer        handle continuous attributes 
ventura        ventura   martinez        explored variety discretization methods
use systems use discrete input attributes  found using discretization
preprocess data often degraded accuracy  recommended machine learning algorithms
designed handle continuous attributes directly 
ting              used several different discretization techniques conjunction
mvdm ib   aha  kibler   albert         results showed improved generalization
accuracy using discretization  discretization allowed algorithm use mvdm
attributes instead using linear distance continuous attributes  thus avoided
normalization problems discussed sections          paper  similar
results seen slightly higher results dvdm  which discretizes continuous
attributes uses vdm  compared hvdm  which uses linear distance
continuous attributes   paper  dvdm uses equal width intervals discretization 
  

fiwilson   martinez

tings algorithms make use advanced discretization techniques 
domingos        uses heterogeneous distance function similar hvdm rise
system  hybrid rule instance based learning system  however  rise uses normalization
scheme similar n  sections          square individual attribute
distances 
mohri   tanaka        use statistical technique called quantification method ii  qm  
derive attribute weights  present distance functions handle nominal
continuous attributes  transform nominal attributes values boolean
attributes  one time  weights attribute actually
correspond individual attribute values original data 
turney        addresses cross validation error voting  i e  using values k     
instance based learning systems  explores issues related selecting parameter k  i e  
number neighbors used decide classification   paper use k     order
focus attention distance functions themselves  accuracy would improved
applications using k     
ivdm wvdm use nonparametric density estimation techniques  tapia   thompson 
      determining values p use computing distances  parzen windows  parzen 
      shifting histograms  rosenblatt        similar concept techniques 
especially wvdm  techniques often use gaussian kernels advanced
techniques instead fixed sized sliding window  experimented gaussianweighted kernels well results slightly worse either wvdm ivdm  perhaps
increased overfitting 
paper applies distance function problem classification  input
vector mapped discrete output class  distance functions could used
systems perform regression  atkeson  moore   schaal        atkeson        cleveland  
loader         output real value  often interpolated nearby points 
kernel regression  deng   moore        
mentioned section     elsewhere  pruning techniques used reduce
storage requirements instance based systems improve classification speed  several
techniques introduced  including ib   aha  kibler   albert        aha        
condensed nearest neighbor rule  hart         reduced nearest neighbor rule  gates        
selective nearest neighbor rule  rittler et al          typical instance based learning
algorithm  zhang         prototype methods  chang         hyperrectangle techniques
 salzberg        wettschereck   dietterich         rule based techniques  domingos        
random mutation hill climbing  skalak        cameron jones        others  kibler   aha 
      tomek        wilson        

   conclusions   future research areas
many learning systems depend reliable distance function achieve accurate
generalization  euclidean distance function many distance functions
inappropriate nominal attributes  hoem function throws away information
achieve much better accuracy euclidean function itself 
value difference metric  vdm  designed provide appropriate measure

  

fiimproved heterogeneous distance functions

distance two nominal attribute values  however  current systems use vdm
often discretize continuous data discrete ranges  causes loss information
often corresponding loss generalization accuracy 
paper introduced three new distance functions  heterogeneous value difference
function  hvdm  uses euclidean distance linear attributes vdm nominal attributes 
uses appropriate normalization  interpolated value difference metric  ivdm 
windowed value difference metric  wvdm  handle continuous attributes within
paradigm vdm  ivdm wvdm provide classification accuracy higher
average discretized version algorithm  dvdm  datasets continuous
attributes examined  equivalent dvdm applications without
continuous attributes 
experiments    datasets  ivdm wvdm achieved higher average accuracy
hvdm  better dvdm  hoem euclidean distance  ivdm
slightly accurate wvdm requires less time storage  thus would seem
desirable distance function heterogeneous applications similar used
paper  properly normalized euclidean distance achieves comparable generalization
accuracy nominal attributes  situations still appropriate
distance function 
learning system used obtain generalization accuracy results paper nearest
neighbor classifier  hvdm  ivdm wvdm distance functions used knearest neighbor classifier k     incorporated wide variety systems
allow handle continuous values including instance based learning algorithms  such
pebls   radial basis function networks  distance based neural networks  new
distance metrics used areas statistics  cognitive psychology  pattern
recognition areas distance heterogeneous input vectors
interest  distance functions used conjunction weighting schemes
improvements system provides 
new distance functions presented show improved average generalization   
datasets used experimentation  hoped datasets representative kinds
applications face real world  new distance functions
continue provide improved generalization accuracy cases 
future research look determining conditions distance function
appropriate particular application  look closely problem selecting
window width  look possibility smoothing wvdms probability landscape
avoid overfitting  new distance functions used conjunction variety
weighting schemes provide robust generalization presence noise
irrelevant attributes  well increase generalization accuracy wide variety
applications 

references
aha  david w           tolerating noisy  irrelevant novel attributes instance based
learning algorithms  international journal man machine studies  vol      pp          
aha  david w   dennis kibler  marc k  albert          instance based learning
algorithms  machine learning  vol     pp        
  

fiwilson   martinez

atkeson  chris          using local models control movement  d  s  touretzky  ed   
advances neural information processing systems    san mateo  ca  morgan kaufmann 
atkeson  chris  andrew moore  stefan schaal          locally weighted learning 
appear artificial intelligence review 
batchelor  bruce g           pattern recognition  ideas practice  new york  plenum press 
pp        
biberman  yoram          context similarity measure  proceedings european
conference machine learning  ecml      catalina  italy  springer verlag  pp        
broomhead  d  s   d  lowe         multi variable functional interpolation adaptive
networks  complex systems  vol     pp          
cameron jones  r  m           instance selection encoding length heuristic random
mutation hill climbing  proceedings eighth australian joint conference
artificial intelligence  pp         
carpenter  gail a   stephen grossberg          massively parallel architecture
self organizing neural pattern recognition machine  computer vision  graphics 
image processing  vol      pp         
chang  chin liang          finding prototypes nearest neighbor classifiers  ieee
transactions computers  vol      no      pp            
cleveland  w  s   c  loader          computational methods local regression 
technical report     murray hill  nj  at t bell laboratories  statistics department 
cost  scott  steven salzberg          weighted nearest neighbor algorithm
learning symbolic features  machine learning  vol      pp        
cover  t  m   p  e  hart          nearest neighbor pattern classification  institute
electrical electronics engineers transactions information theory  vol      no    
pp        
dasarathy  belur v           nearest neighbor  nn  norms  nn pattern classification
techniques  los alamitos  ca  ieee computer society press 
deng  kan  andrew w  moore          multiresolution instance based learning 
appear proceedings international joint conference artificial intelligence
 ijcai    
diday  edwin          recent progress distance similarity measures pattern
recognition  second international joint conference pattern recognition  pp          
domingos  pedro          rule induction instance based learning  unified approach 
appear      international joint conference artificial intelligence  ijcai     
dudani  sahibsingh a           distance weighted k nearest neighbor rule  ieee
transactions systems  man cybernetics  vol     no     april       pp          
  

fiimproved heterogeneous distance functions

gates  g  w           reduced nearest neighbor rule  ieee transactions information
theory  vol  it     no     pp          
giraud carrier  christophe  tony martinez          efficient metric heterogeneous
inductive learning applications attribute value language  intelligent systems  pp 
        
hart  p  e           condensed nearest neighbor rule  institute electrical
electronics engineers transactions information theory  vol      pp          
hecht nielsen  r           counterpropagation networks  applied optics  vol      no      pp 
          
kibler  d   david w  aha          learning representative exemplars concepts 
initial case study  proceedings fourth international workshop machine
learning  irvine  ca  morgan kaufmann  pp        
kohonen  teuvo          self organizing map  proceedings ieee  vol      no 
   pp            
lebowitz  michael          categorizing numeric information generalization  cognitive
science  vol     pp          
merz  c  j   p  m  murphy          uci repository machine learning databases 
irvine  ca  university california irvine  department information computer
science  internet  http   www ics uci edu  mlearn mlrepository html 
michalski  ryszard s   robert e  stepp  edwin diday          recent advance data
analysis  clustering objects classes characterized conjunctive concepts 
progress pattern recognition  vol     laveen n  kanal azriel rosenfeld  eds   
new york  north holland  pp        
mitchell  tom m           need biases learning generalizations  j  w  shavlik
  t  g  dietterich  eds    readings machine learning  san mateo  ca  morgan
kaufmann        pp          
mohri  takao  hidehiko tanaka          optimal weighting criterion case
indexing numeric symbolic attributes  d  w  aha  ed    case based
reasoning  papers      workshop  technical report ws        menlo park 
ca  aiii press  pp          
nadler  morton  eric p  smith          pattern recognition engineering  new york 
wiley  pp          
nosofsky  robert m           attention  similarity  identification categorization
relationship  journal experimental psychology  general  vol       no     pp        
papadimitriou  christos h   jon louis bentley          worst case analysis nearest
neighbor searching projection  lecture notes computer science  vol     
automata languages programming  pp          
  

fiwilson   martinez

parzen  emanuel          estimation probability density function mode  annals
mathematical statistics  vol      pp            
quinlan  j  r           unknown attribute values induction  proceedings  th
international workshop machine learning  san mateo  ca  morgan kaufmann  pp 
        
rachlin  john  simon kasif  steven salzberg  david w  aha          towards better
understanding memory based bayesian classifiers  proceedings
eleventh international machine learning conference  new brunswick  nj  morgan
kaufmann  pp          
renals  steve  richard rohwer          phoneme classification experiments using radial
basis functions  proceedings ieee international joint conference neural
networks  ijcnn     vol     pp          
rittler  g  l   h  b  woodruff  s  r  lowry  t  l  isenhour          algorithm
selective nearest neighbor decision rule  ieee transactions information theory 
vol      no     pp          
rosenblatt  murray          remarks nonparametric estimates density function 
annals mathematical statistics  vol      pp          
rumelhart  d  e   j  l  mcclelland          parallel distributed processing  mit press 
ch     pp          
salzberg  steven          nearest hyperrectangle learning method  machine learning 
vol     pp          
schaffer  cullen          selecting classification method cross validation  machine
learning  vol      no    
schaffer  cullen          conservation law generalization performance  proceedings
eleventh international conference machine learning  ml     morgan
kaufmann       
schlimmer  jeffrey c           learning representation change  proceedings
sixth national conference artificial intelligence  aaai     vol     pp          
skalak  d  b           prototype feature selection sampling random mutation hill
climbing algorithsm  proceedings eleventh international conference
machine learning  ml     morgan kaufman  pp          
sproull  robert f           refinements nearest neighbor searching k dimensional
trees  algorithmica  vol     pp          
stanfill  c   d  waltz          toward memory based reasoning  communications
acm  vol      december       pp            

  

fiimproved heterogeneous distance functions

tapia  richard a   james r  thompson          nonparametric probability density
estimation  baltimore  md  johns hopkins university press 
ting  kai ming          discretization continuous valued attributes instance based
learning  technical report no       basser department computer science  university
sydney  australia 
ting  kai ming          discretisation lazy learning  appear special issue
lazy learning artificial intelligence review 
tomek  ivan          experiment edited nearest neighbor rule  ieee
transactions systems  man  cybernetics  vol     no     june       pp          
turney  peter          theoretical analyses cross validation error voting instancebased learning  journal experimental theoretical artificial intelligence  jetai  
pp          
turney  peter          exploiting context learning classify  proceedings
european conference machine learning  vienna  austria  springer verlag  pp         
turney  peter  michael halasz          contextual normalization applied aircraft gas
turbine engine diagnosis  journal applied intelligence  vol     pp          
tversky  amos          features similarity  psychological review  vol      no     pp         
ventura  dan          discretization preprocessing step supervised learning
models  masters thesis  department computer science  brigham young university 
ventura  dan  tony r  martinez         empirical comparison discretization
methods  proceedings tenth international symposium computer
information sciences  pp          
wasserman  philip d           advanced methods neural computing  new york  ny  van
nostrand reinhold  pp          
wess  stefan  klaus dieter althoff guido derwand          using k d trees improve
retrieval step case based reasoning  stefan wess  klaus dieter althoff    m  m 
richter  eds    topics case based reasoning  berlin  springer verlag  pp          
wettschereck  dietrich  thomas g  dietterich          experimental comparison
nearest neighbor nearest hyperrectangle algorithms  machine learning  vol     
no     pp       
wettschereck  dietrich  david w  aha  takao mohri          review comparative
evaluation feature weighting methods lazy learning algorithms  technical
report aic         washington  d c   naval research laboratory  navy center
applied research artificial intelligence 

  

fiwilson   martinez

wilson  d  randall  tony r  martinez          potential prototype styles
generalization  proceedings sixth australian joint conference artifical
intelligence  ai     pp          
wilson  d  randall  tony r  martinez          heterogeneous radial basis functions 
proceedings international conference neural networks  icnn     vol     pp 
          
wilson  dennis l           asymptotic properties nearest neighbor rules using edited
data  ieee transactions systems  man  cybernetics  vol     no     pp          
wolpert  david h           overfitting avoidance bias  technical report sfi tr            santa fe  nm  santa fe institute 
zhang  jianping          selecting typical instances instance based learning  proceedings
ninth international conference machine learning 

  


