journal artificial intelligence research               

submitted       published     

screen  learning flat syntactic semantic spoken
language analysis using artificial neural networks
stefan wermter
volker weber

wermter informatik uni hamburg de
weber informatik uni hamburg de

department computer science
university hamburg
      hamburg  germany

abstract
previous approaches analyzing spontaneously spoken language often based
encoding syntactic semantic knowledge manually symbolically 
progress using statistical connectionist language models  many current
spoken language systems still use relatively brittle  hand coded symbolic grammar
symbolic semantic component 
contrast  describe so called screening approach learning robust processing
spontaneously spoken language  screening approach analysis uses shallow sequences category representations analyzing utterance various syntactic 
semantic dialog levels  rather using deeply structured symbolic analysis 
use connectionist analysis  screening approach aims supporting speech
language processing using     data driven learning     robustness connectionist
networks  order test approach  developed screen system
based new robust  learned analysis 
paper  focus detailed description screen s architecture 
syntactic semantic analysis  interaction speech recognizer  detailed
evaluation analysis robustness uence noisy incomplete input 
main result paper representations allow robust processing
spontaneous spoken language deeply structured representations  particular 
show fault tolerance learning capability connectionist networks support
analysis providing robust spoken language processing within overall
hybrid symbolic connectionist framework 

   introduction
recently fields speech processing well language processing seen
efforts examine possibility integrating speech language processing  von hahn
  pyka        jurafsky et al       b  waibel et al         ward        menzel        geutner
et al         wermter et al          new large speech language corpora
developed rapidly  new techniques examined particularly support
properties speech language processing  although quite
approaches spoken language analysis  mellish        young et al         hauenstein  
weber        ward         emphasized learning syntactic semantic
analysis spoken language using hybrid connectionist  architecture topic
c      ai access foundation morgan kaufmann publishers  rights reserved 

fiwermter   weber

paper goal screen    however  learning important reduction
knowledge acquisition  automatic system adaptation  increasing system
portability new domains  different previous approaches  paper
demonstrate hybrid connectionist learning techniques used providing
robust analysis faulty spoken language 
processing spoken language different processing written language  successful techniques text processing may useful spoken language processing 
processing spoken language less constrained  contains errors less strict regularities written language  errors occur levels spoken language processing 
instance  acoustic errors  repetitions  false starts repairs prominent spontaneously spoken language  furthermore  incorrectly analyzed words  unforeseen grammatical semantic constructions occur often spoken language  order deal
important problems  real world  language analysis  robust processing necessary 
therefore cannot expect existing techniques context free tree representations
proven work written language simply transferred spoken
language 
instance  consider speech recognizer produced correct german sentence hypothesis  ich meine naturlich marz   english translation   i mean course
march    standard techniques text processing   chart parsers context free
grammars   may able produce deeply structured tree representations many correct
sentences shown figure   
sentence
verb phrase

noun group

pronoun

verb group

verb

noun group

adverb

ich  i  meine  mean  natrlich  of course 

noun

mrz  march 

figure    tree representation correctly recognized sentence
however  currently speech recognizers still far perfect produce many word
errors possible rely perfect sentence hypothesis  therefore  incorrect
   sometimes connectionist networks called artificial neural networks  use
term  connectionist networks   term  hybrid connectionist architecture  refer
architecture emphasizes use connectionist networks rule use
symbolic representations higher levels might needed 
   symbolic connectionist robust enterprise natural language

  

fiscreen  flat syntactic semantic spoken language analysis

variations  ich meine ich marz    i mean march     ich hatte ich marz    i
march    ich ich meine marz    i mean march   analyzed  however 
context free grammars single syntactic semantic category error may prevent
complete tree built  standard top down chart parsers may fail completely  however  suboptimal sentence hypotheses analyzed since sometimes sentence
hypotheses best possible output produced speech recognizer  furthermore 
lot content extracted even partially incorrect sentence hypotheses 
instance   i march  plausible agent  i  said something
time  march   therefore  robust analysis able analyze sentence
hypotheses ideally break input 

    screening approach  flat representations support robustness
examples incorrect variations sentence hypotheses  in depth structured
syntactic semantic representation advantageous since arbitrary word order spontaneous errors make often impossible determine desired deep highly
structured representation  furthermore  deep highly structured representation may
many restrictions appropriate spontaneously spoken language  however 
maybe even important  certain tasks necessary perform in depth
analysis  while  instance  inferences story understanding require in depth
understanding  dyer         tasks information extraction spoken language
need much in depth analysis  instance  output parser used
translating speech recognizer sentence hypothesis  eh ich meine eh ich marz    eh
mean eh march    may sucient extract agent   i   uttered   mean  
time   march    contrast deeply structured representation  screening approach
aims reaching robust representation spoken language  screening approach
shallow analysis based category sequences  called representations  various
syntactic semantic levels 
representation structures utterance u words w  wn according
syntactic semantic properties words contexts  e g   according sequence
basic abstract syntactic categories  instance  phrase  a meeting london 
described representation  determiner noun preposition noun  basic
syntactic level representation  noun group noun group prepositional group
prepositional group  abstract syntactic level  similar representations used
semantic categories  dialog act categories  etc 
kase
 rubbish 
noun

noun group
negation

ich
 i 
pronoun
animate
noun group
agent

meine
 mean 
verb
utter
verb group
action

naturlich
 of course 
adverb
nil
special group
miscellaneous

marz
 march 
noun
time
noun group
time

figure    utterance representation
  

fiwermter   weber

figure   gives example representation correct sentence hypothesis
 kase ich meine naturlich marz    rubbish mean course march    first line shows
sentence  second literal translation  third line describes basic syntactic
category word  fourth line shows basic semantic category  last two lines
illustrate syntactic semantic categories phrase level 
kase
 rubbish 
noun

noun group
negation

ich
 i 
pronoun
animate
noun group
agent

hatte
 had 
verb

verb group
action

ich
 i 
pronoun
animate
noun group
agent

marz
 march 
noun
time
noun group
time

figure    utterance representation
figure   gives example representation incorrect sentence hypothesis
 kase ich hatte ich marz    rubbish march    parser spoken language
able process sentence hypotheses far possible  use
representations support necessary robustness  example  analysis
least provide animate agent noun group   i   made statement
specific time noun group   march    flat representations potential support
robustness better since minimal sequential structure  even error
occurs whole representation still built  contrast  standard tree structured
representations many decisions made construct deeply structured
representation  therefore possibilities make incorrect decisions 
particular noisy spontaneously spoken language  chose representations
rather highly structured representations desired robustness
mistakes speech language systems 

    flat representations learned hybrid connectionist framework

robust spoken language analysis using representations could pursued different
approaches  therefore want motivate use hybrid connectionist approach 
uses connectionist networks far possible rule use symbolic
knowledge  use connectionist networks 
important  due distributed fault tolerance  connectionist networks support
robustness  rumelhart et al         sun        connectionist networks number properties relevant spoken language analysis  instance 
connectionist networks well known learning generalization capabilities 
learning capabilities allow induce regularities directly examples  training
examples representative task  noisy robust processing supported
inductive connectionist learning 
furthermore  hybrid connectionist architecture property different knowledge sources take advantage learning generalization capabilities connectionist networks  hand  knowledge   task control knowledge  
  

fiscreen  flat syntactic semantic spoken language analysis

rules known represented directly symbolic representations  since humans apparently symbolic inferencing based real neural networks  abstract models
symbolic representations connectionist networks additional potential
shed light human language processing capabilities  respect  approach
differs candidates robust processing  statistical taggers statistical
n grams  statistical techniques used robust analysis  charniak       
statistical techniques n grams relate human cognitive language capabilities simple recurrent connectionist networks relationships human
cognitive language capabilities  elman        
screen new hybrid connectionist system developed examination
syntactic semantic analysis spoken language  earlier work explored
scanning understanding written texts  wermter        wermter   lochel       
wermter   peters         based experience started completely new project
screen explore learned fault tolerant analysis spontaneously spoken language
processing  preliminary successful case studies transcripts developed
screen system using knowledge generated speech recognizer  previous work 
gave brief summary screen specific focus segmentation parsing dialog
act processing  wermter   weber      a   paper  focus detailed description
screen s architecture  syntactic semantic analysis  interaction
speech recognizer  detailed evaluation analysis robustness uence
noisy incomplete input 

    organization claim paper
paper structured follows  section   provide detailed description
examples noise spoken language  noise introduced human speaker
speech recognizer  noise spoken language analysis motivates representations whose categories described section    basic abstract categories
syntactic semantic level explained section  section   motivate
explain design screen architecture  brief functional overview  show
overall architecture explain details individual modules connectionist
network level  order demonstrate behavior analysis spoken language
provide various detailed examples section    using several representative sentences
walk reader detailed step by step analysis  behavior system explained  provide overall analysis screen system section   
evaluate system s individual networks  compare performance simple recurrent networks statistical n gram techniques  show simple recurrent networks
performed better     grams syntactic semantic prediction  furthermore
provide overall system evaluation  examine overall performance uence
additional noise  supply results transfer different second domain  finally
compare approach approaches conclude representations based
connectionist networks provide robust learned spoken language analysis 
want point paper make argument deeply structured symbolic representations language processing general  usually  deeply
structured representation built  course due additional knowledge con  

fiwermter   weber

tains  potential powerful relationships interpretations greater
representation  instance  in depth analysis required tasks making
detailed planning inferences reading text stories  however  screening approach
motivated based noisy spoken language analysis  noisy spoken language analysis 
representations support robustness  connectionist networks effective providing robustness due learned fault tolerance  main contribution
paper  demonstrate building evaluating computational hybrid
connectionist architecture screen based at  robust  learned processing 

   processing spoken language

goal learn process spontaneously spoken language syntactic semantic
level fault tolerant manner  section give motivating examples spoken
language 

     noise  spoken language

domain paper arrangement meetings business partners 
currently use     spoken dialog turns     utterances domain  one
turn consists one subsequent utterances speaker     
utterances  thousands utterance hypotheses generated processed
based underlying speech recognizer  german utterance examples domain
shown together literal english translation  important note
english translations word for word translations 
   kase ich meine naturlich marz
 rubbish mean course march 
   der vierzehnte ist ein mittwoch richtig
 the fourteenth wednesday right 
   hm sechsten april bin ich leider auer hause
 eh sixth april unfortunately home 
   ich dachte noch der nachsten woche auf jeden fall noch im april
 so thought still next week case still april 
   gut prima vielen dank dann ist das ja kein problem
 good great many thanks yeah problem 
   oh das ist schlecht da habe ich um vierzehn uhr dreiig einen termin beim zahnarzt
 oh bad fourteen o clock thirty date dentist 
   ja genau allerdings habe ich da von neun bis vier uhr schon einen arzttermin
 yes exactly however nine four o clock already doctorappointment 
see  spoken language contains many performance phenomena  among
exclamations   rubbish   see example     interjections   eh    so    oh   see examples   
  

fiscreen  flat syntactic semantic spoken language analysis

      new starts   there       see example     furthermore  syntactic
semantic constraints spoken language less strict written text  instance 
word order spontaneously spoken language often different written language 
therefore  spoken language  noisier  written language even transcribed
sentences  well known parsing strategies text processing   rely
wellformedness criteria   directly applicable analyzing spoken language 

     noise  speech recognizer

want analyze spoken language computational model 
 noise  introduced humans speaking  noise  introduced limitations speech recognizers  typical speech recognizers produce many separated word
hypotheses different plausibilities time based given speech signal  word
hypotheses connected word hypothesis sequence evaluated
providing basis analysis  typically  word hypothesis consists four parts    
 pause 
                

hm  eh 

ich  i 

                

    e   

                

wie  how 
    e   

htte  had 
                

                

 on 
    e   

april  april 
    e   

                

ich  i 
                

                

ich  i 

leider  unfortunately 
    e   

                

    e   

sechsten  sixth 

    e   

                

    e   

                

bin  am 
    e   

                

                

    e   

    e   

wenn  if 
    e   

ich  i 
    e   

                

    e   

leider  unfortunately 
                

    e   

auer  out of 
                

hause  home 

    e   

                

    e   

                

 pause 
                

    e   

 not recognized 
    e   

figure    simple word graph spoken utterance   ahm sechsten april bin ich
leider auer hause    eh sixth april unfortunately home   
node represents word hypothesis  arrow represents possible
subsequent word hypotheses  word hypothesis shown word
string  start time  end time interval acoustic plausibility 
  

fiwermter   weber

start time seconds     end time seconds     word string hypothesis 
   plausibility hypothesis based confidence speech recognizer  show simple word graph    practice  word graphs spontaneous speech
much longer leading comprehensive word hypothesis sequences  however  illustrating
properties speech input focus relatively short simple word graph
 figure    
word hypotheses overlap time constitute directed graph called word
graph  node word graph represents one word hypothesis  two hypotheses
graph generated word hypotheses connected end time first word
hypothesis directly start time second word hypothesis  instance 
word hypothesis  am    on   ending      hypothesis  sechsten    sixth  
starting      connected word hypothesis sequence 
hm
 eh 
hm
 eh 
 sec

ich
 i 


 on 

sechsten
 sixth 

april
 april 

bin
 am 

leider
 unfort  

auer
 out of 

hause
 home 


 on 

sechsten
 sixth 

april
 april 

wenn ich ich leider
 if   i   i   unfort  

auer
 out of 

hause
 home 

 sec

ich
 i 

 sec

figure    two examples word hypothesis sequences word graph
example word graph simple  however  shown figure    possible
word hypothesis sequence desired  a hm sechsten april bin ich leider
auer hause    eh sixth april unfortunately home    sequence
 a hm ich sechsten april wenn ich ich leider auer hause    eh sixth april
unfortunately home    consequently  deal incorrectly recognized
words extraordinary order  therefore syntactic semantic analysis
fault tolerant order process noisy word hypothesis sequences 

   flat category representation  intermediate connecting
representation
section describe category representations  first  show
categories syntactic analysis depict categories semantic
analysis 
   speech input form test word graphs taken so called blaubeuren meeting
corpus  particular word graphs used provided project partners general test
purposes verbmobil project  particularly generated testing parsing strategies 
therefore speech recognizer fine tuned produce relatively small word graphs relatively
high word accuracy      vocabulary size hmm recognizer      average number
hypotheses per word        dialogs 

  

fiscreen  flat syntactic semantic spoken language analysis

    categories flat syntactic analysis

flat syntactic analysis assignment syntactic categories sequence words  e g  
word hypothesis sequence generated speech recognizer  flat representations
phrase group level support local structural decisions  local structural decisions deal
problem phrase group  abstract syntactic category  word belongs to 
case local  directly preceding words phrase group uence
current decision  instance  determiner  the  could part prepositional group
 in mine  part starting noun group  the old mine   is  local structural
decisions depending local context made based analysis 
syntactic analysis developed level basic syntactic categories
abstract syntactic categories  syntactic categories may vary depending language  degree detail intended structural representation  however 
general approach rather independent specifically used categories  fact 
used syntactic categories two different domains  railway counter interactions
business meeting arrangements  basic syntactic categories used noun 
verb  preposition  pronoun  numeral  past participle  pause  adjective  adverb  conjunction 
determiner  interjection other  shown abbreviations table   
category
noun  n 
verb  v 
preposition  r 
pronoun  u 
numeral  m 
participle  p 
pause    

examples
date  april
meet  choose
at 
i 
fourteenth
taken
pause

category
adjective  j 
adverb  a 
conjunction  c 
determiner  d 
interjection  i 
 o 

examples
late
often
and 
the 
eh  oh
particles

table    basic syntactic categories
abstract syntactic categories used verb group  noun group  adverbial group 
prepositional group  conjunction group  modus group  special group interjection group 
abstract syntactic categories shown table   
category
verb group  vg 
noun group  ng 
adverbial group  ag 
prepositional group  pg 
conjunction group  cg 
modus group  mg 
special group  sg 
interjection group  ig 

examples
mean  would propose
date  next possible slot
later  early possible
dining hall
and  either    
interrogatives  confirmations  when  long  yes
additives politeness  please 
interjections  pauses  eh  oh

table    abstract syntactic categories
  

fiwermter   weber

categories express main syntactic properties phrases 
basic abstract syntactic categories widely used different parsers  however 
approach representations crucially rely specific set basic
abstract syntactic categories  goal train  learn generalize syntactic
analysis based abstract syntactic categories basic syntactic categories  local syntactic decisions made far possible  local syntactic ambiguities
phrase group level  abstract syntactic categories  dealt global ambiguities prepositional phrase attachment dealt since need
additional knowledge  e g   semantics module  complete syntax trees
certain preference  which might turn wrong based semantic knowledge  
syntactic representation goes far possible using local syntactic knowledge
disambiguation 

    categories flat semantic analysis

since semantic analysis domain dependent  semantic categories differ different
domains  worked particularly two domains  railway counter interactions  called 
regensburg train corpus  business meeting arrangements  called  blaubeuren meeting
corpus       overlap semantic categories train corpus
category
select  sel 
suggest  sug 
meet  meet 
utter  utter 
 is 
 have 
move  move 
aux  aux 
question  quest 
physical  phys 
animate  anim 
abstract  abs 
 here 
source  src 
destination  dest 
location  loc 
time  time 
negative evaluation  no 
positive evaluation  yes 
nil  nil 

examples
select  choose
propose  suggest
meet  join
say  think
is 
had 
come  go
would  could
question words  where 
physical objects  building  oce
animate objects  i 
abstract objects  date
time location state words  prepositions  at 
time location source words  prepositions 
time location destination words  prepositions 
hamburg  pittsburgh
tomorrow    o  clock  april
no  bad
yes  good
words  without  specific semantics  e g   determiner 

table    basic semantic categories
meeting corpus  wermter   weber      b   differences occurred mainly verbs 
e g   need events frequent railway counter interactions suggestevents frequent business meeting interactions  semantic categories
  

fiscreen  flat syntactic semantic spoken language analysis

category
action  act 
aux action  aux 
agent  agent 
object  obj 
recipient  recip 
instrument  instr 
manner  manner 
time at  tm at 
time from  tm frm 
time to  tm to 
loc at  lc at 
loc from  lc frm 
loc to  lc to 
confirmation  conf 
negation  neg 
question  quest 
misc  misc 

examples
action full verb events  meet  select
auxiliary action auxiliary events  would
agent action 
object action  date
recipient action 
instrument action  using elevator
achieve action  without changing rooms
time  morning
start time   am
end time   pm
location  frankfurt  new york
start location  boston  dortmund
end location  hamburg
confirmation phrase  ok great  yes wonderful
negation phrase  stop 
question phrases  time
miscellaneous words  e g   politeness  please  eh

table    abstract semantic categories
railway counter interactions described previous work  weber   wermter        
primarily focus semantic categories meeting corpus  basic
semantic categories word shown table    higher level abstraction 
word belong abstract semantic category  possible abstract semantic categories
shown table    summary  categories provide basis analysis 
word represented syntactically semantically context four categories two
basic two abstract levels 

   architecture screen system

section want describe constraints principles important
system design  outlined motivated introduction  screening approach
at  robust  learned analysis spoken language based category sequences  called
representations  various syntactic semantic levels  order test screening
approach  designed implemented hybrid connectionist screen system
processes spontaneously spoken language using learned connectionist representations 
summarize main requirements order motivate specific system design
explained subsequent subsections 

    general motivation architecture

consider learning extremely important spoken language analysis several
reasons  learning reduces knowledge acquisition increases portability  particularly
spoken language analysis  underlying rules regularities dicult
formulate often reliable  furthermore  cases  inductive learning may detect
  

fiwermter   weber

unknown implicit regularities  want use connectionist learning simple recurrent
networks rather forms learning  e g   decision trees  primarily
inherent fault tolerance connectionist networks  knowledge
sequence words categories learned simple recurrent networks 
fault tolerance often occurring language errors ected system
design  commonly occurring errors  interjections  pauses  word repairs 
phrase repairs   however  fault tolerance cannot go far try model class
occurring errors  number potentially occurring errors unpredictable constructions far large  screen  want incorporate explicit fault tolerance using
specific modules correction well implicit fault tolerance using connectionist network techniques inherently fault tolerant due support similarity based
processing  fact  even word completely unknown  recurrent networks use
empty input may even assign correct category sucient previous context 
flat representations  motivated sections      may support robust spokenlanguage analysis  however  connectionist representations provide full recursive power arbitrary syntactic semantic symbolic knowledge structures  contrast
context free parsers  representations provide better basis robust processing
automatic knowledge acquisition inductive learning  however  argued use potentially unrestricted recursion well known context free grammar
parsers provides computational model recursive power humans
order understand language  order better support robustness  want use
representations spontaneous language analysis 
incremental processing speech  syntax  semantics dialog processing parallel
allows us start language analysis parallel speech recognizer finished
analysis  incremental processing advantage providing analysis results
early stage  example  syntactic semantic processing occur parallel
slightly behind speech processing  analyzing spoken language based speech
recognizer output  want consider many competing paths word hypothesis sequences
parallel 
respect hybrid representations  examine hybrid connectionist architecture
using connectionist networks useful want use symbolic processing wherever necessary  symbolic processing useful complex control
large system  hand learning robust analysis  use feedforward
simple recurrent networks many modules try use rather homogeneous  supervised
networks 

    overview architecture

screen parallel integrated hybrid architecture  wermter        various

main properties 

   outside module  difference communication symbolic
connectionist module  previous hybrid architectures emphasized different
symbolic connectionist representations  different representations screen
benefit common module interface  outside connectionist symbolic
  

fiscreen  flat syntactic semantic spoken language analysis

module communication identically realized symbolic lists contain values
connectionist units 
   previous hybrid symbolic connectionist architectures usually within
either symbolic connectionist module  hendler        faisal   kwasny       
medsker         screen global state described collection individual
symbolic connectionist modules  processing parallel long one module
need input second module 
   communication among symbolic connectionist modules organized via
messages  hybrid architectures often used either activation
values symbolic structures  used messages consisting lists symbols
associated activation plausibility values provide communication medium
supports connectionist processing well symbolic processing 
give overview various parts screen  see figure    
important output consists syntactic semantic category representations based
input incrementally recognized parallel word hypotheses  speech recognizer
generates many incorrect word hypotheses time  even correctly recognized speech
contain many errors introduced humans  representation used since
fault tolerant robust than  instance  context free tree representation since
tree representation requires many decisions representation 
module system  instance disambiguation abstract syntactic categories  contains connectionist network symbolic program  integration symbolic
connectionist representations occurs encapsulation symbolic connectionist
processes module level  connectionist networks embedded symbolic modules
communicate via messages 
however  essential parts needed purposes learning spokenlanguage analysis why  starting output individual word hypotheses
speech recognizer  first need component receives incremental stream
individual parallel word hypotheses produces incremental stream word hypothesis sequences  see figure     call part speech sequence construction part 
needed transforming parallel overlapping individual word hypotheses word hypothesis sequences  word hypothesis sequences different quality goal
find work best word hypothesis sequences  therefore need speech
evaluation part combine speech related plausibilities syntactic semantic
plausibilities order restrict attention best found word hypothesis sequences 
furthermore  need part analyzes best found word hypothesis sequences
according syntactic semantic representation  category part receives
stream current word hypothesis sequences  two word hypothesis sequences
shown figure    part provides interpretation word hypothesis sequence
basic syntactic categories  abstract syntactic categories  basic semantic categories 
abstract semantic categories  is  word hypothesis sequence assigned four
graded preferences four word categories 
human speech analyzed speech recognizer may contain many errors  question
arises extent want consider errors  analysis several hundred
  

fiwermter   weber

two word hypotheses sequences 
  

output analysis
  

kse

ich

meine

natrlich

 rubbish 

 i 

 mean 

 of course   march 

n

ng

u

ng



neg

anim

agent utter act

v

vg



sg

n

ng

nill

misc

time

tmat

kse

ich

htte

ich

 rubbish 

 i 

 had 

 i 

n

ng

u

ng



neg

anim

agent

v

mrz

mrz
 march 

vg

u

act

anim  agent  time

 ng 

n

ng
tmat

    
syntactic semantic hypotheses

case frame part

dialog part
learned
flat
syntactic
semantic
analysis

correction part
speech evaluation part

category part
constructed word hypotheses sequences 
kse ich meine natrlich mrz
   rubbish mean course march

speech sequence construction part

kse ich htte ich mrz
march

   rubbish

    

word hypotheses
word hypotheses generated speech recognizer 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

input speech recognizer
current word hypothesis

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

 pause 
kse
ich
ich
meine
meine
htte
etliche
ich
mrz
da
natrlich
mrz
aus
 pause 

figure    overview screen
  

 rubbish 
 i 
 i 
 mean 
 mean 
 had 
 several 
 i 
 march 
 there 
 of course 
 march 
 out 

        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  

fiscreen  flat syntactic semantic spoken language analysis

transcripts speech recognizer outputs revealed errors occur
often regularly  interjections  pauses  word repairs  phrase repairs 
therefore designed correction part receives hypotheses words deals
frequently occurring errors spoken language explicitly 
parts outlined far build center integration speech related
language related knowledge fault tolerant learning architecture  therefore
focus parts paper  however  want process complete dialog turns
contain several individual utterances need know certain utterance
starts constituents belong utterance  task performed case
frame part fills frame incrementally segments speaker s turn utterances 
long term perspective screen provide analysis tasks spoken
utterance translation information extraction  besides syntactic semantic analysis
utterance  intended dialog acts convey important additional knowledge  therefore 
dialog part needed assigning dialog acts utterances  instance utterance
request suggestion  fact  already fully implemented case frame part
dialog part utterances  however  describe details two
parts paper since described elsewhere  wermter   lochel        
learning screen based concepts supervised learning instance feedforward networks  rumelhart et al          simple recurrent networks  elman       
general recurrent plausibility networks  wermter         general  recurrent plausibility networks allow arbitrary number context hidden layers considering long
distance dependencies  however  many network modules screen attempted
keep individual networks simple homogeneous  therefore  first version
described used variations feedforward networks  rumelhart et al        
simple recurrent networks  elman         due greater potential sequential
context representations  recurrent plausibility networks might provide improvements
optimizations simple recurrent networks  however  primarily interested
overall real world hybrid connectionist architecture screen rather optimization single networks  following description give detailed examples
individual networks 

    detailed view

motivated various parts screen  give detailed description
architecture screen respect modules syntactic semantic
analysis word hypothesis sequences  therefore  focus speech related parts 
categorization part correction part  figure   shows detailed overview
parts  basic data ow shown arrows  many modules generate hypotheses
used subsequent modules higher level  hypotheses illustrated
rising arrows  modules  output contains local predictive hypotheses  sometimes
called local top down hypotheses  used modules lower level 
hypotheses illustrated falling arrows  local predictive hypotheses used
correction part eliminate  repaired utterance parts speech evaluation part
eliminate syntactically semantically implausible word hypothesis sequences 
   means repaired utterance parts actually marked deleted 

  

fiwermter   weber

segment parser

dia act

frame

slots
dialog act
type
verb form

   

case frame part  

 

 

reject
utter
meinen
 mean 

 

 

dialog part
phrase error

 

 

 

 

bas syn eq

bas sem eq

 

 

    

    

correction part

lex start eq

lex word eq

 

 

word error


pause  interjection 
hesitation  unresolved
phonetic material 

interjection

 

   

pause error

pause

dialog lexicon

 

abs syn eq
 

 

   

    

 

 

sem speech error

 

phrase start 

 

bas syn pre

 
 

separates
syn speech error

abs sem eq

 

abs syn cat

abs sem cat

 

 

  

  

  

  

bas sem pre

  

  

  

  

bas syn dis

 

 

bas sem dis

  

  

  

  

 

 
verb  pronoun
utter  nil

speech evaluation part

syntactic lexicon
semantic lexicon

category part

 
con sequ hyps
kse ich meine

 rubbish mean 
constructed word hypotheses sequences 
kse ich
kse ich meine

 rubbish i 
 rubbish mean 

speech sequence construction part

figure    detailed overview screen  abbreviations functionality
modules described text 
  

fiscreen  flat syntactic semantic spoken language analysis

cases arrows would complex used numbers illustrate
data ow individual modules 
      speech sequence construction part

speech sequence construction part receives stream parallel word hypotheses
generates stream word hypothesis sequences within module con sequ hyps
bottom figure    based current word hypotheses many word hypothesis sequences
may possible  cases reduce number current word hypotheses  e g  
know time passed far specific word hypothesis sequence cannot
extended anymore time current word hypothesis  case
eliminate sequence since word hypothesis sequences could reach end
sentence candidates successful speech interpretation 
furthermore  use speech plausibility values individual word hypothesis
determine speech plausibility word hypothesis sequence  using
best word hypothesis sequences reduce large space possible sequences 
generated stream word hypothesis sequences similar set partial n best
representations generated pruned incrementally speech analysis rather
end speech analysis process 
      speech evaluation part

speech evaluation part computes plausibilities based syntactic semantic knowledge order evaluate word hypothesis sequences  part contains modules
detection speech related errors  currently  performance speech recognizers
spontaneously spoken speaker independent speech general still far perfect 
typically  many word hypotheses generated certain signal    therefore  many hypothesized words produced speech recognizer incorrect speech confidence
value word hypothesis alone provide enough evidence finding desired
string signal  therefore goal speech evaluation part provide preference
filtering unlikely word hypothesis sequences  syn speech error sem speecherror two modules decide current word hypothesis syntactically
 semantically  plausible extension current word hypothesis sequence  syntactic
 semantic  plausibility based basic syntactic  semantic  category disambiguation
prediction 
summary  word hypothesis sequence acoustic confidence based
speech recognizer  syntactic confidence based syn speech error  semantic
confidence based sem speech error  three values integrated weighted
equally  determine best word hypothesis sequences  way  two modules
   hmm speech recognizer used generating word hypotheses domain word accuracy
    best match word graph desired transcript utterance 
recognizer particularly optimized task domain order able examine
robustness language level  unoptimized version task domain currently    
word accuracy 
   integration speech  syntax  semantics confidence values provided better results
using one two three knowledge sources 

  

fiwermter   weber

act evaluator speech recognizer well filter language processing
part 
statistical models speech recognition  bigram trigram models used language models filtering best possible hypotheses  used simple recurrent
networks since networks performed slightly better bigram trigram model
implemented comparison  sauerland         later section    
show detailed comparison simple recurrent networks n gram models  for n
            reason better performance internal representation simple
recurrent network restrict covered context fixed number two
three words potential learn required context needed 
output layer
   units
n

j

v



r

c

u







p



 

      connections

context layer

hidden layer
     
conn 

input layer

     units

   copy

      connections

   units
n

j

v



r

c

u







p



 

disambiguated representation  ich    i   bas syn dis

figure    network architecture syntactic prediction speech evaluation part
 bas syn pre   abbreviations explained table   
knowledge syntactic semantic plausibility provided prediction
networks  bas syn pre bas sem pre  speech evaluation part disambiguation networks  bas syn dis bas sem dis  categorization part  example  show network bas syn pre figure    previous basic syntactic
category currently considered word hypothesis sequence input network 
example  ich    i   word hypothesis sequence  kase ich meine    rubbish
mean   found pronoun  u   therefore  syntactic category representation
 ich    i   contains     pronoun  u  category  categories receive     
input network consists    units    categories  output
network size  unit vector represents plausibility
predicted basic syntax category last word current word hypothesis sequence 
plausibility unit representing desired basic syntactic category  found
bas syn dis  taken syntactic plausibility currently considered word hypothesis
sequence syn speech error  example  meine    mean   found verb
  

fiscreen  flat syntactic semantic spoken language analysis

 v   therefore plausibility verb  v  taken syntax plausibility  selection
marked box output layer bas syn pre figure    
summary  syntactic  semantic  plausibility word hypothesis sequence evaluated degree agreement disambiguated syntactic  semantic  category
current word predicted syntactic  semantic  category previous word 
since decisions current state whole sequence made  preceding
context represented copying hidden layer current word context layer
next word based srn network structure  elman         connections
network n m connections except connections hidden layer
context layer simply used copy store internal preceding state
context layer later processing next word comes in  general  speech
evaluation part provides ranking current word hypothesis sequences equally
weighted combination acoustic  syntactic  semantic plausibility 
      category part

module bas syn dis performs basic syntactic disambiguation  see figure     input
module sequence potentially ambiguous syntactic word representations  one
word utterance time  module disambiguates syntactic category
representation according syntactic possibilities previous context  output
preference disambiguated syntactic category  syntactic disambiguation task
learned simple recurrent network  input output network ambiguous
disambiguated syntactic category representations  figure   show example
input representation  meine    mean    my   verb pronoun 
however  sequence  ich meine    i mean     meine  verb therefore
network receives disambiguated verb category representation alone 
module bas sem dis similar module bas syn dis instead receiving
potentially ambiguous syntactic category input producing disambiguated syntactic
category output  module bas sem dis receives semantic category representation
lexicon provides disambiguated semantic category representation output 
semantic disambiguation learned simple recurrent network provides mapping ambiguous semantic word representation disambiguated semantic word
representation  modules bas syn dis bas sem dis provide disambiguation
subsequent tasks association abstract categories test category
equality word error detection possible 
module abs syn cat supplies mapping disambiguated basic syntactic
category representations abstract syntactic category representations  see figure     
module provides abstract syntactic categorization realized simple
recurrent network  module important providing abstract interpretation
utterance preparing input detection phrase errors  figure    shows
disambiguated basic syntactic representation  meine    mean   verb  
small preference pronoun   mapped verb group category higher
abstract syntactic category representation  based number basic abstract
syntactic categories    input units basic syntactic categories   output
units abstract syntactic categories 
  

fiwermter   weber

output layer
   units
n

j

v



r

c

u







p



 

      connections

hidden layer

context layer
     
conn 

input layer

     units

   copy

      connections

   units
n

j

v



r

c

u





p



 

ambiguous representation
 meine   verb pronoun 

syntactic lexicon
meine

kse
 rubbish 



verb
pronoun

ich meine
 i   mean 

current word hypotheses sequence

figure    network architecture basic syntactic disambiguation  bas syn dis  
abbreviations explained table   
module abs sem cat parallel module abs syn cat uses basic semantic
category representations input abstract semantic category representations output 
similar previous modules  used simple recurrent network learn
mapping represent sequential context  input network basic
semantic category representation word  output abstract category
preference 
described four networks provide basis fault tolerant analysis
detection errors  furthermore  module phrase start distinguishing
abstract categories  task module indicate boundaries subsequent
abstract categories delimiter  use boundaries determine abstract
syntactic abstract semantic category phrase    earlier experiments provided
support take abstract syntactic category first word phrase final
abstract syntactic category phrase  since phrase starts  e g   prepositions  good
   figure   show uence phrase start delimiter abstract syntactic semantic
categorization dotted lines 

  

fiscreen  flat syntactic semantic spoken language analysis

output layer
  units
ng

vg

pg

cg

ag mg

sg

ig

      connections

hidden layer

context layer
     
conn 

input layer

      connections

     units

   copy

   units
n
j
v

r
c
u

disambiguated representation  meine   mean 





p



 

figure     network architecture abstract syntactic categorization  abs syn cat  
abbreviations explained table   
indicators abstract syntactic categories  wermter   lochel         hand 
earlier experiments supported take abstract semantic category last word
phrase final abstract semantic category phrase  since phrase ends  e g   nouns 
good indicators abstract semantic categories  wermter   peters         furthermore  phrase start gives us opportunity distinguish two equal subsequent abstract
categories two phrases  instance  phrase  in hamburg monday 
know border exists first second prepositional
phrase 
      correction part

correction part contains modules detecting pauses  interjections  well repetitions repairs words phrases  see figure     modules detecting pause
errors pause error  pause interjection  modules pause interjection receive currently processed word detect potential occurrence pause
interjection  respectively  output modules input module pauseerror  soon pause interjection detected  word marked deleted
therefore virtually eliminated input stream   elimination interjections
pauses desired   instance speech translation task   order provide inter   pauses interjections sometimes provide clues repairs  nakatani   hirschberg        although
currently use clues repair detection  compared lexical  syntactic  semantic
equality constituents  interjections pauses provide relatively weak indicators repairs since
occur relatively often places sentence  however  since mark interjections
pauses deleted could make use knowledge future necessary 

  

fiwermter   weber

pretation errors possible  since three modules basically occurrence
tests realized symbolic representations 
second main cluster modules correction part modules
responsible detection word related errors  then  word repairs  am sechsten
april bin ich ich    on sixth april i    wir haben ein termin treffen    we
date meeting   dealt with  certain preferences finding repetitions
repairs word level  among preferences lexical equality two
subsequent words  symbolic module lex word eq   equality two basic syntactic
category representations  connectionist module bas syn eq   equality basic
semantic categories two words  connectionist module bas sem eq   example
three modules  show test syntactic equality  bas syn eq  figure    
output layer
  units
equal equal
    connections

hidden layer
  units
input layer

         connections

n j v r c u p  

n j v r c u p  

    
units

disambiguated representation second  ich   i 

disambiguated repr  first  ich   i 

output layer
  units
equal equal
    connections

hidden layer
  units
input layer

         connections

n j v r c u p  
disambiguated repr   termin   date 

n j v r c u p  

    
units

disambiguated representation  treffen   meeting 

figure     network architecture equality basic syntactic category representation  bas syn eq   abbreviations explained table   
  

fiscreen  flat syntactic semantic spoken language analysis

two output units plausible implausible outcome used since network two output units gave consistently better results compared network
one output unit  with   plausible   implausible   reason network two output units performed better separation weights plausible
implausible hidden output layer  order receive single value  two output values integrated according formula  unit         unit     then  output
three equality modules value       represents equality  
represents inequality  although single preference may sucient  common
uence provides reasonable basis detecting word repairs word repetitions
module word error  then  word repairs repetitions eliminated original
utterance  since modules word related errors based two representations two
subsequent input words since context play minor role  use feedforward
networks modules  hand  simple test lexical equality
two words lex word eq represented effectively using symbolic representation 
third main cluster correction part consists modules detection
correction phrase errors  example phrase error is   wir brauchen
den fruheren termin den spateren termin    we need earlier date later date   
preferences phrase errors lexical start two subsequent phrases
equal  abstract syntactic categories equal abstract semantic categories
equal  three preferences modules lex start eq  abs syn eq
abs sem eq  modules receive two input representations two corresponding
words two phrases  lex start eq receives two lexical words  abs syn eq two abstract
syntactic category representations  abs sem eq two abstract semantic category representations  output three modules value toward   equality toward
  otherwise  values input module phrase error finally decides
whether phrase replaced another phrase  lexical equality two words
discrete test  implemented lex start eq symbolically  preferences
phrase error implemented feedforward networks 

   detailed analysis examples

section detailed look processing output speech recognizer
producing syntactic semantic interpretation concurrent word hypothesis
sequences  also called sentence hypothesis here  

    overall environment

overall processing incremental left right  time multiple sentence
hypotheses processed parallel  figure    shows snapshot screen     s
utterance  time snapshot shows first three sentence hypotheses
german words together  literal  english translations   rubbish mean    rubbish i    rubbish had    screen environment allows user view inspect
incremental generation word hypothesis sequences  partial sentence hypotheses 
preferred syntactic semantic categories basic abstract level 
sentence hypothesis illustrated horizontally  certain time many sentence hypotheses
active parallel  ranked according descending plausibility
  

fiwermter   weber

screen   symbolic connectionist robust enterprise natural language
quit

line

stop

go

single step

 

  sentencehypotheses  time      s  system      s  display 

n



ng

sug

neg conf

kse  rubbish 

n



 

ng

sug

neg conf

kse  rubbish 

n



ng

sug

neg conf

kse  rubbish 

u

ng

sug

anim agent conf

ich  i 

v

nil

sug

utter nil

conf

meine  mean 

u

nil

sug

anim

nil

conf

ng

sug

ich  i 

u

anim agent conf

ich  i 

v

nil

sug



nil

conf

htte  had 

 

                                                                                          
n
j
v

r
c
u



p

 

figure     first snapshot sentence  kase ich meine naturlich marz   rubbish mean
course march    abbreviations explained table      below 
second pop up window illustrates full preferences word  meine 
  mean   basic syntactic categories 
sentence hypotheses  snapshot figure    currently three sentence
hypotheses preferred current sentence hypothesis consists  rubbish mean  
sentence hypotheses syntactically semantically plausible starts 
underlying variations introduced speech recognizer produced different word
hypotheses slightly overlapping signal parts sentence  besides speech plausibility  syntax semantics help choosing better sentence hypotheses  currently
combine speech recognition plausibility  syntactic plausibility  semantic
plausibility compute plausibility sentence hypotheses multiplication
respective normalized plausibility values      since speech recognizer
contain syntactic semantic knowledge  sequence hypothesis rated plausible based
speech knowledge alone may neglect potential syntactic semantic regularity 
  

fiscreen  flat syntactic semantic spoken language analysis

using corresponding syntactic semantic plausibility values sentence hypothesis
integrate acoustic  syntactic  semantic knowledge 
word hypothesis shown preferred basic syntactic hypothesis  upper left
square word hypothesis   preferred abstract syntactic hypothesis  upper middle
square   preferred basic semantic hypothesis  lower left square   preferred abstract
semantic hypothesis  lower middle square   preferred dialog act  upper right square   
integrated acoustic  syntactic semantic confidence partial sentence hypothesis point  lower right square   size square illustrates strength
hypothesis  full black square means preferred hypothesis close one 
instance  word hypothesis  ich    i   first sentence hypothesis
hypothesis pronoun  u  basic syntactic category  noun group  ng 
abstract syntactic category  animate object  anim  basic semantic category 
agent abstract semantic category  suggestion  sug  dialog act  furthermore  length vertical bar word hypotheses indicate plausibility
new phrase start 
another example  see representation example word  meine   could
verb  mean  pronoun  my  german  used throughout
network descriptions  see figure     network correct preference  meine 
verb  v   figure    shows preference well zoomed illustration
less favored preferences second pop up window below  see  ambiguous pronoun preference u received second strongest activation
preferences close    shown activation preferences output values
corresponding network basic syntactic categorization  shown activation value
snapshots shows preferred hypothesis hypotheses
shown request   
within display scroll descending ascending sentence
hypotheses  furthermore scroll left right analyzing specific longer word
hypothesis sequences  step mode allows screen system wait
interactive mouse click process next incoming word hypothesis
detailed analysis  step mode adapted different number steps  word
hypotheses  switched completely one decides analyze sentence
hypotheses later end word hypotheses  preferred possible
syntactic semantic hypotheses shown  therefore many different hypotheses appear
size  however  clicking one squares less confident
hypotheses displayed well 
   dialog acts use are  accept  acc   query  query   reject  rej   request suggest  re s  
request state  re s   state  state   suggest  sug   miscellaneous  misc   since paper focuses
syntactic semantic aspects screen elaborate implemented dialog
part here  details dialog act processing described previously  wermter   lochel 
      
    snapshots figure    abstract syntactic semantic categories yet computed
therefore represented nil  next processing step computation performed
seen next figure    

  

fiwermter   weber

    analyzing final snapshot short sentence hypotheses

figure    illustrate final state     s utterance  eight possible sentence hypotheses remained see first four figure     starting
fourth sentence hypothesis  kase ich hatte ich marz    rubbish march  
see lower rated sentence hypothesis desired sentence  lower ranked
hypotheses good examples current state of the art speech recognizers alone
able produce reliable sentence hypotheses  since problem analyzing spontaneous
speaker independent speech complex  therefore syntactic semantic components spontaneous language take account highly irregular
sequences shown below  however  interesting observe underlying connectionist networks always produce preference syntactic semantic interpretation
abstract basic level  fact  although lower ranked sentence hypotheses
constitute desired sentence assigned syntactic semantic categories
correct individual word hypotheses  course may cases network
could make wrong decision uncertain word hypotheses  however syntactic
semantic processing never break possible sentence hypothesis 
respect different well known methods symbolic context free chart parsers 
look top ranked sentence hypothesis  kase ich meine naturlich marz    rubbish mean course march   desired sentence  plausible
screen   symbolic connectionist robust enterprise natural language
quit

go

line

stop

single step

 

  sentencehypotheses  time      s  system      s  display 

n

ng dsug



neg conf

kse

 

ng dsug

anim agent conf

ich

n

ng dsug



neg conf

kse

u

ng dsug

ng dsug

anim agent conf



neg conf

kse

u

ng dsug

ng dsug

anim agent conf



neg conf

u

utter act

conf

v

vg dsug

utter act

v



ng dsug

v



htte



sg dsug

nill

misc conf

natrlich

conf



sg dsug

nill

misc conf

natrlich

vg dsug

act

conf

htte

anim agent conf

ich

vg dsug

meine

ich

n

v

meine

ich

n

kse

u

u

ng dsug

anim agent conf

ich

vg dsug

act

conf

u

n

ng dsug

time tmat conf

mrz

n

ng dsug

time tmat conf

mrz

n

ng dsug

time tmat conf

mrz

ng dsug

anim agent conf

ich

n

ng dsug

time tmat conf

mrz
 

figure     final snapshot sentence  kase ich meine naturlich marz   rubbish mean
course march   
  

fiscreen  flat syntactic semantic spoken language analysis

sentence based speech language plausibility  furthermore  see assigned categories correct  german word  kase    rubbish   found noun
part noun group expresses negation   ich    i   starts new phrase 
pronoun noun group represents animate agent  following
german word  meine  particularly interesting since used verb sense
 mean  pronoun sense  my   therefore  connectionist network
basic syntactic classification disambiguate two possibilities based
preceding context  network learned take consideration preceding
context able choose correct basic syntactic category verb  v  rather
pronoun  u  word  meine    mean    time new phrase start
found well  following word  naturlich    of course   highest preference
adverb special group  finally  word  marz    march   assigned highest
plausibility noun noun group well time something happens 

    phrase starts phrase groups longer sentence hypotheses

focus detailed analysis second example   a hm ja genau allerdings
habe ich da von neun bis vier uhr schon einen arzttermin   literally translated
screen   symbolic connectionist robust enterprise natural language
quit

go

line

stop

single step

 

   sentencehypotheses  time      s  system      s  display 



yes

mg dacc

conf conf

ja

yes

mg dacc

conf conf

ja

drej

conf conf

j

yes

yes

mg dacc

conf conf

ja



nill

mg

drej

conf conf

yes

mg dacc

conf conf



nill



sg

drej

neg conf





sg

drej

neg conf

allerdings

mg dmisc

misc conf

dennoch





allerdings

genau



ja

yes

mg

genau



 

j

mg dmisc

misc conf

dennoch





sg dmisc

neg conf

allerdings





sg dmisc

neg conf

allerdings

v

vg

drej



act

conf

habe

vg

drej



act

conf

habe

u

nill

act

conf



ng

drej

misc conf

act

conf

anim agent conf

u

nill

es

drej

misc conf

nill

sg

drej

misc conf

nill

sg dmisc

misc conf

da

misc conf

drej

tmat conf

r

pg

drej

tmat conf

r



nil dmisc

nil

conf

von



nill

da

pg

von



ng dmisc

r

von



ng dmisc

ich

habe

nill

sg

da

u

vg dmisc



da

es

habe

v

drej

anim agent conf

vg dmisc



ng

ich

v

v

u

sg dmisc

misc conf

r



nil dacc

nil

conf

von

 

figure     first part snapshot sentence  a hm ja genau allerdings habe ich
da von neun bis vier uhr schon einen arzttermin   literal translation   yes
exactly however nine four o clock already doctorappointment   improved translation   eh yes exactly however
doctor appointment nine four o clock   
  

fiwermter   weber

screen   symbolic connectionist robust enterprise natural language
quit

line

stop

go

single step

 

   sentencehypotheses  time      s  system      s  display 



pg

misc

time tmat conf

neun  nine 



pg

misc

neun  nine 



pg

misc

time tmat conf

pg

misc conf

r

nil

r

nil

acc

r

nil

bis  to 



pg

misc

time tmat conf

vier  four 

pg

misc

misc conf



pg

pg

misc

misc

time tmat conf

misc conf



pg

misc

misc

time tmat conf

misc conf



pg

misc

time tmat conf

n

pg

misc

time tmat conf

n

pg

misc

time tmat conf

uhr  oclock 

misc

time tmat conf

zehn  ten 

pg

uhr  oclock 

zehn  ten 

pg

n

uhr  oclock 

vier  four 

bis  to 

time tmat conf

neun  nine 

misc

bis  to 

neun  nine 



nil

pg

bis  to 

time tmat conf

 

r

n

pg

acc

time tmat conf

uhr  oclock 



nil

sg

misc

misc conf

schon  already 



nil

sg

misc

misc conf

schon  already 



nil

sg

misc

misc conf

schon  already 



nil

sg

acc

misc conf

schon  already 



nil

ng

res

misc conf

einen  a 



nil

ng

res

misc conf

nil

ng

res

misc conf

nil

ng

abs

obj

conf

n

ng

res

abs

obj

conf

n

ng

res

abs

obj

conf

arzttermin doc app

res

misc conf

einen  a 

res

arzttermin doc app

einen  a 



ng

arzttermin doc app

einen  a 



n

n

ng

res

abs

obj

conf

arzttermin doc app

 

figure     second part snapshot sentence  a hm ja genau allerdings habe ich
da von neun bis vier uhr schon einen arzttermin    yes exactly however
nine four o clock already doctor appointment   
sentence analyzed is   eh yes exactly however nine four o clock
already doctor appointment   better non literal translation would be   eh yes
exactly however doctor appointment nine four o clock  
analysis first sentence hypotheses  interjection  ahm    eh   detected
corresponding module correction part eliminated respective
sentence hypotheses 
figure    figure    show best found four sentence hypotheses 
categories sentence hypotheses look similar keep separate
hypotheses since differ time stamps speech confidence values 
two snapshots longer example illustrate uence
phrase starts  sequences  von neun    from nine    bis vier uhr    to four
o clock   constitute two phrase groups clearly separated black bar
prepositions  von    from    bis    to    words  neun    nine   
 vier    four     uhr    o clock   start another phrase group  since underlying connectionist network learning phrase boundaries simple recurrent network
example demonstrates network learned preceding context  without
learned preposition  von    from    bis    to   noun
  

fiscreen  flat syntactic semantic spoken language analysis

 uhr    o clock   within prepositional phrase group could
part noun phrase another context  vier uhr pat gut    four o clock fits
well   

    dealing noise repairs
finally focus example simple word graph shown beginning
paper page      a hm sechsten april bin ich leider auer hause   literal
translation  eh  th april unfortunately home   using sentence
give example interjection simple word repair  dealing hesitations
repairs large area spontaneous language processing main topic
paper  a detailed discussion repairs screen found previous work 
weber   wermter         nevertheless  sake illustration completeness
show ability screen deal interjections word repairs  first snapshot
figure    shows start example sentence     s  leading interjection
 eh  eliminated already 
furthermore  see second word hypothesis sequence shows two subsequent
word hypotheses  ich    i    possible since two word hypotheses
screen   symbolic connectionist robust enterprise natural language
quit

go

line

stop

single step

 

   sentencehypotheses  time      s  system      s  display 

r

pg

sug

tmat conf

 on 

r

pg

sug

 on 

r

pg

sug

tmat conf

time tmat conf



pg

sug

time tmat conf



pg

sug

time tmat conf

sechsten   th 

pg

sug

tmat conf

 on 

sug

sechsten   th 

 on 

r

pg

sechsten   th 

tmat conf

 





pg

sug

time tmat conf

sechsten   th 

n

pg

sug

time tmat conf

april  april 

n

pg

sug

time tmat conf

pg

sug

time tmat conf

pg



act

conf

v

vg

sug



act

conf

u

pg

sug

u

ng

state

anim agent conf

ich  i 

u

nil

state

anim

nil

conf

u

nil

state

anim

nil

conf

ich  i 

u

nil

sug

anim

nil

conf

u

nil

sug

anim

nil

conf

ng

state

ich  i 

sug

anim recip conf

ich  i 

time tmat conf

april  april 

state

htte  had 

april  april 

n

vg

bin  am 

april  april 

n

v

ich  i 

v

vg

state



act

conf

bin  am 

u

anim agent conf

ich  i 

ich  i 

 

figure     first snapshot sentence  a hm sechsten april bin ich leider auer
hause    eh  th april unfortunately home   
  

fiwermter   weber

generated speech recognizer could connected  case
four word hypotheses shown below 
start time
    s
    s
    s
    s

end time
    s
    s
    s
    s

word hypothesis
ich  i 
ich  i 
ich  i 
ich  i 

speech plausibility
        e   
        e   
        e   
        e   

using speech knowledge word hypotheses  possible connect
second hypothesis runs     s     s fourth hypothesis runs
    s     s  example noise generated speech recognizer  since
desired sentence contains one word  ich    i   sentence hypothesis
point contains two  repetition treated eliminated way actual
word repairs language  reasons occurrence repairs different
effect repeated word same  therefore  case repeated  ich    i  
eliminated sentence sequence  figure    show final snapshot
sentence  see word repairs occur top ranked sentence hypothesis
desired sentence 
screen   symbolic connectionist robust enterprise natural language
quit

line

stop

go

single step

 

   sentencehypotheses  time      s  system      s  display 



pg

sug

time tmat conf

sechsten   th 



pg

sug

time tmat conf

 

sechsten   th 



pg

sug

time tmat conf

sechsten   th 



pg

sug

time tmat conf

sechsten   th 

n

pg

sug

time tmat conf

april  april 

n

pg

sug

time tmat conf

pg

sug

time tmat conf

pg



act

conf

u

pg

sug

u

ng

state

anim agent conf

ich  i 

sug

anim recip conf

u

v

vg

state

ng

sug

anim recip conf



act

conf

vg

state

ng

state

anim agent conf



act

conf

u



sg

rej

neg conf





sg

rej

neg conf

leider  unfort  

ich  i 

v

bin  am 

u



leider  unfort  

ich  i 

bin  am 

time tmat conf

april  april 

state

ich  i 

april  april 

n

vg

bin  am 

april  april 

n

v





sg

rej

neg conf

leider  unfort  

ng

state

anim agent conf

ich  i 





sg

rej

neg conf

leider  unfort  

r

pg

rej

lcat conf

auer  out of 

r

pg

rej

lcat conf

auer  out of 

r

pg

rej

lcat conf

auer  out of 

r

pg

rej

lcat conf

auer  out of 

n

pg

rej

phys lcat conf

hause  home 

n

pg

rej

phys lcat conf

hause  home 

n

pg

rej

phys lcat conf

hause  home 

n

pg

rej

phys lcat conf

hause  home 

 

figure     final snapshot sentence  a hm sechsten april bin ich leider auer
hause    eh  th april unfortunately home   
  

fiscreen  flat syntactic semantic spoken language analysis

general  language repairs  screen deal elimination interjections
pauses  repair word repetitions  word corrections  where words may
different  categories same  well simple forms phrase repairs  where
phrase repeated replaced another phrase  

   design analysis screen
section describe design choices screen  particular focus
issues use connectionist networks  reach high accuracy little training 
screen compared systems design principles 

    use connectionist networks screen 
past  n gram based techniques used successfully tasks syntactic
category prediction part speech tagging  therefore  possible ask developed simple recurrent networks screen  subsection provide detailed
comparison simple recurrent networks n gram techniques prediction basic
syntactic categories  chose task detailed comparison since currently
dicult task simple recurrent network screen  purposefully
choose subtask simple recurrent network high accuracy 
prediction task since dicult predict category compared disambiguating among categories  instance  chose dicult prediction relatively
low network performance order  extremely  fair comparison n gram
techniques 
primarily interested generalization behavior new unknown input 
therefore figure    shows accuracy syntactic prediction unknown test
set  word several different syntactic categories follow syntactic
categories excluded  instance  determiner  the  adjective noun
follow   the short        the appointment   determiner  the  preposition
implausible occur probably excluded  therefore important
know many categories ruled figure    shows relationship
prediction accuracy number excluded categories n grams simple
recurrent network  as described figure    
expect  techniques  n grams recurrent networks  prediction
accuracy higher categories excluded performance lower
many categories excluded  however  interestingly  see simple
recurrent networks performed better   grams    grams    grams    grams   grams 
furthermore  interesting note higher n grams necessarily lead better
performance  instance    grams   grams perform worse   grams since
would probably need much larger training sets 
comparison n grams       simple recurrent networks
semantic prediction received result simple recurrent networks performed
better n grams  performance best n gram often slightly worse
performance simple recurrent network  indicates n grams
reasonably useful technique  however  comparisons simple recurrent networks per  

fiwermter   weber

testset
   

correct prediction  

  

  

  

srn
 gram
 gram
 gram
 gram
 gram

  

 
 

 

 

 

 

  

  

number excluded categories

figure     comparison simple recurrent network n grams
formed least slightly better best n grams  therefore  used simple recurrent
networks primary technique connectionist sequence learning screen 
explain result  n grams   grams still perform reasonably well
task simple recurrent networks closest performance  however 
simple recurrent networks perform slightly better since contain fixed
limited context  many sequences  simple recurrent network may primarily use
directly preceding word representation make prediction  however  exceptions
context required recurrent network memory internal reduced
representation preceding context  therefore  potential exible
respect context size 
n grams may perform optimally extremely fast  question arises
much time necessary compute new category using new input current
context network  general networks differ slightly size typically
contain several hundred weights  typical representative simple recurrent network
   input units     hidden units    output units     context units      weights
takes      sparc ultra compute new category within whole forward sweep 
  

fiscreen  flat syntactic semantic spoken language analysis

since techniques smoothed n grams basically rely ecient table look up
precomputed values  course typical n gram techniques still faster  however  due
fixed size context may perform well simple recurrent networks  furthermore  computing next possible categories     s fast enough current version
screen  sake explanation one could argue screen contains   
networks modules typical utterance contains    words  single utterance hypothesis could performed      s  however  different text tagging  single
sentences process word graphs  depending specific utterance      word
hypothesis sequences could generated processed  furthermore
book keeping required keeping best word hypotheses  loading appropriate networks appropriate word hypotheses  etc  potentially large number
word hypotheses  additional book keeping performance  number individual
modules syntax  semantics dialog processing explain total analysis time
whole unoptimized screen system order seconds although single recurrent
network performs order      s 

    improvement hypothesis space
subsection analyze extent syntactic semantic prediction
knowledge used improve best found sentence hypotheses  illustrate
pruning performance hypothesis space integrating acoustic  syntactic  semantic knowledge  speech recognizer alone provides acoustic confidence
values  screen adds syntactic semantic knowledge  knowledge sources
weighted equally order compute single plausibility value current word hypothesis sequence  plausibility value used speech construction part prune
hypothesis space select currently best word hypothesis sequences  several
word hypothesis sequences processed incremental parallel  given time
  
n best incremental word hypothesis sequences kept  
syntactic semantic plausibility values based basic syntactic semantic prediction  bas syn pre bas sem pre  next possible categories
word selection preference determined basic syntactic respectively semantic category  bas syn dis bas sem dis      performance disambiguation
modules         test set  prediction modules performance    
    semantic syntactic test set  respectively want exclude least
     possible categories  performance allows us computation syntactic
semantic plausibility syn speech error sem speech error  based
combined acoustic  syntactic  semantic knowledge  first tests     turns show
accuracy constructed sentence hypotheses screen could increased
    using acoustic syntactic plausibilities     using acoustic 
syntactic  semantic plausibilities  wermter   weber      a  
    experiments low values  n       provided best overall performance 
    explained detail section      

  

fiwermter   weber

    screen s network performance networks yield high
accuracy little training

evaluating performance screen s categorization part meeting corpus
first show percentages correctly classified words important networks
categorization  bas syn dis  bas sem dis  abs syn cat  abs sem cat  phrase start 
    turns corpus     utterances      words          
words     turns used training      testing  usually data used
training testing  preliminary earlier experiments used     training
    testing  however  performance unknown test set similar    
training set     test set  therefore  used testing training data since
interested generalization performance unknown instances test
set compared training performance known instances 
first sight  might seem relatively little data training  statistical techniques
information retrieval techniques often work large texts individual lexical word
items  need much less material get reasonable performance since work
syntactic semantic representations rather words  would stress
use syntactic semantic category representations      words training
testing rather lexical words themselves  therefore  category representation
requires much less training data lexical word representation would required 
side effect  training time reduced     training set  keeping
performance     test set  is  training used category representations
   dialog turns  testing generalization category representations remaining
    dialog turns 
table   shows test results individual networks unknown test set 
networks trained      epochs learning rate          hidden units 
configuration provided best performance network architectures 
general tested network architectures      hidden units  learning parameters
            learning rule used generalized delta rule  rumelhart et al  
       assigned output category representation word counted correct
category maximum activation desired category 
module

accuracy test set
bas syn dis
   
bas sem dis
   
abs syn cat
   
abs sem cat
   
phrase start
   
word error
   
phrase error
   

table    performance individual networks test set meeting corpus
performance basic syntactic disambiguation     unknown test
set  current syntactic  text  taggers reach     accuracy texts  however 
  

fiscreen  flat syntactic semantic spoken language analysis

big difference text speech parsing due spontaneous noise
spoken language  interjections  pauses  repetitions  repairs  new starts
 ungrammatical  syntactic varieties spoken language domain reasons
typical accuracy syntactic text taggers reached 
hand see     accuracy basic semantic disambiguation
relatively high semantics  evidence noisy  ungrammatical 
variety spoken language hurts syntax less semantics  due domain dependence
semantic classifications dicult compare explain semantic performance 
however  different study within domain railway interactions could reach
similar performance  for details see section       experiments syntactic results
better semantic results  indicating syntactic classification easier
learn generalize  furthermore  syntactic results close     noisy
spoken language consider good comparison     regular
text language 
performance abstract categories somewhat lower basic categories since evaluation word introduces unavoidable errors  instance 
 in  network cannot yet know time location follow  make
early decision already  general  networks perform relatively well dicult
real world corpus  given eliminate sentence reason took
spontaneous sentences spoken 
furthermore  use transcripts spontaneous language training domain
meeting arrangements  utterances questions answers dates
locations  restricts potential syntactic semantic constructions  certainly
benefit restricted domain  furthermore  mappings ambiguous
learning  e g   noun part noun group prepositional group  mappings
relatively unambiguous  e g   verb part verb group   would expect
performance mixed arbitrary domains random spoken sentences
various topics passers by city  however  performance somewhat
restricted domains learned promising manner  for transfer different
domain see section       evidence simple recurrent networks
provide good performance using small training data restricted domain 

    screen s overall output performance
described individual network performance  focus
performance running system  performance running screen system
different performance individual networks number reasons  first 
individual networks trained separately order support modular architecture 
running screen system  however  connectionist networks receive input
underlying networks  therefore  actual input connectionist network
running screen system may differ original training test sets  second 
spoken sentences may contain errors interjections word repairs  part
individual network training  running screen system able detect
correct certain interjections  word corrections phrase corrections  therefore  system
network performance differ dis uencies  third  want evaluate
  

fiwermter   weber

performance abstract semantic categorization abstract syntactic categorization
particularly interested certain sentence parts  abstract syntactic categorization 
e g   detection prepositional phrase  consider beginning
phrase significant function word  e g   preposition  important
location syntactic categorization  contrast  abstract semantic categorization 
content word end phrase group  directly next phrase start 
important 
correct syntactic output representation    
correct semantic output representation    

table    overall syntactic semantic accuracy running screen system
unknown test set meeting corpus
expect based explanation previous paragraph  overall accuracy output complete running system lower performance
individual modules  fact  true table   shows overall syntactic
semantic phrase accuracy running screen system      assigned syntactic
phrase representations unknown test set correct     assigned semantic
phrase representations  slight performance drop partially explained
uncertain input underlying networks uenced
networks  hand  cases various decisions different modules  e g 
three modules lexical  syntactic semantic category equality two words 
combined order clean errors  e g  wrong decision one single module  
general  given     dialog turns test set completely unrestricted  unknown real world spontaneous language turns  believe overall performance
quite promising 

    screen s overall performance incomplete lexicon

one important property screen robustness  therefore  interesting question
screen would behave could receive incomplete input lexicon 
situations realistic since speakers could use new words speech recognizer
seen before  furthermore  test robustness techniques  standard
context free parsers usually cannot provide analysis words missing lexicon 
screen would break missing input representations  although course
expect overall classification performance must drop less reliable input provided 
order test situation controlled uence removing items
lexicon  first tested scenario randomly eliminated    syntactic
semantic lexicon representations  word unknown  screen used single syntactic
single semantic average default vector instead  average default vector contained
normalized frequency syntactic respectively semantic category across lexicon 
even without    lexicon entries utterances could still analyzed  screen
break missing word representations attempts provide analysis good
  

fiscreen  flat syntactic semantic spoken language analysis

correct syntactic output representation    
correct semantic output representation    

table    overall syntactic semantic accuracy running screen system
meeting corpus unknown test set    lexicon entries eliminated
possible  expected  table   shows performance drop overall syntactic
semantic accuracy  however  compared         performance complete
lexicon  see table    still find     syntactic output representations    
semantic output representations correct eliminating    lexicon entries 
correct syntactic output representation    
correct semantic output representation    

table    overall syntactic semantic accuracy running screen system
meeting corpus unknown test set     lexicon entries
eliminated
another experiment eliminated     syntactic semantic lexicon entries 
case  syntactic accuracy still     semantic accuracy     
eliminating     lexicon led syntactic accuracy reduction        
versus      semantic accuracy reduction         versus       general
see experiments percentage accuracy reduction much less
percentage eliminated lexicon entries demonstrating screen s robustness working
incomplete lexicon 

    comparison results new different domain

order compare performance techniques  show results
experiments different spoken regensburg train corpus  intention cannot
describe experiments domain level detail done
blaubeuren meeting corpus paper  however  provide summary order
provide point reference comparison experiments meeting corpus 
comparison serves another additional possibility judge results meeting
corpus 
different domain chose     dialog turns railway counter  people ask
questions receive answers train connections  typical utterance is   yes need
eh sleeping car pause pause regensburg hamburg   used exactly
screen communication architecture process spoken utterances domain 
architecture used      dialog turns used training     
  

fiwermter   weber

testing unseen unknown utterances  syntactic processing  even used exactly
network structure  since expect much syntactic differences
two domains  semantic processing retrained semantic networks  different
categories used semantic classification  particular actions  actions
meetings  e g   visit  meet  predominant meeting corpus  actions
selecting connections  e g   choose  select  important train corpus  wermter  
weber      b   give reader impression portability screen 
would estimate     original human effort  system architecture  networks  could
used new domain  remaining     needed necessary new
semantic tagging training new domain 
module

accuracy test set
bas syn dis
   
bas sem dis
   
abs syn cat
   
abs sem cat
   
phrase start
   
word error
   
phrase error
   

table    performance individual networks test set train corpus
table   shows performance test set train corpus  compare
results meeting corpus  table    results train corpus see
particular abstract syntactic processing almost meeting corpus
     table   compared     table    abstract semantic processing better
meeting corpus      table   compared     table     modules dealing
explicit robustness repairs  phrase start  word repair errors  phrase repair errors 
show almost performance      vs          vs          vs      
correct syntactic output representation    
correct semantic output representation    

table     overall syntactic semantic accuracy running screen system
unknown test set different train corpus
comparison summarize overall performance different train
domain  table    shows screen syntactic performance two
domains  compare table     different domain essentially confirm
previous results syntactic processing performance      vs        however  semantic
processing appears harder train domain since performance     lower
    meeting domain  however  semantic processing  semantic tagging
semantic classification often found much harder syntactic processing general 
  

fiscreen  flat syntactic semantic spoken language analysis

difference still within range usual performance differences syntax
semantics  since semantic categories agents  locations  time expressions
two domains dicult action categorization mainly responsible
difference semantic performance two domains 
general transfer one domain another requires limited amount
hand modeling  course  syntactic semantic categories specified
lexicon transcripts  syntactically semantically tagged transcript sentences
direct basis generating training sets networks  generating
trainings sets main manual effort transferring system new domain 
generation training sets performed training networks
proceed automatically  training typical single recurrent network takes
order hours  much less manual work required transferring standard
symbolic parser new domain generating new syntactic semantic grammar 

    illustrative comparison argument based symbolic parser
made point screen s learned representations robust
hand coded deeply structured representations  would elaborate point
compelling illustrative argument  consider different variations sentence hypotheses
speech recognizer figure        correct sentence hypothesis   am sechsten
april bin ich auer hause    on  th april home      partially incorrect
  input  sechsten april bin ich auer hause
 on  th april home 
  output 

pp

  
vp

np

np

ng

ng

r

pp

v

np

adjg
n

u

adj
am on 

sechsten  th april april 

r

ng
n

bin am 

  input  sechsten april ich ich auer haus
 on  th april home 
  output nil  no analysis possible 

ich i 

auer out of  hause home 

  

figure     two sentence hypotheses speech recognizer  first hypothesis
analyzed  second partially incorrect hypothesis cannot analyzed
anymore symbolic parser 
  

fiwermter   weber

sentence hypothesis   am sechsten april ich ich auer hause    on  th april
home    focusing syntactic analysis  used existing chart parser existing
grammar used extensively real world parsing sentence
level  wermter         necessary significant adaptation addition rule
n g   u pronouns  part original grammar  rule states
pronoun u  e g    i   noun group  ng  
run first sentence hypothesis symbolic context free parser receive desired syntactic analysis shown figure     run second slightly
incorrect sentence hypothesis parser receive analysis  the syntactic category abbreviations figure    used manner throughout
paper  see table       furthermore usual   s  stands sentence   adjg  adjective group   np  complex nominal phrase   vp  verb phrase  literal english
translations shown brackets  
reason second sentence hypothesis could parsed context free
chart parser speech recognizer generated incorrect output  verb
second sentence hypothesis additional pronoun  i   mistakes
occur rather frequently based imperfectness current speech recognition technology 
course one could argue grammar relaxed made exible
deal mistakes  however  rules fault detection integrated
grammar parser complicated grammar parser  even
important  impossible predict possible mistakes integrate symbolic
context free grammar  finally  relaxing grammar dealing mistakes using
explicit specific rules might lead additional mistakes grammar
extremely underspecified 
shown  instance figure     screen problems dealing
speech recognizer variations mistakes  main difference standard
context free symbolic chart parser analysis screen s analysis screen learned
provide analysis noisy conditions context free parser handcoded provide structural analysis  emphasized
make argument structural representations per se general 
structure provided better  particularly tasks require structured
world knowledge  however  robustness major concern  lower syntactic
semantic spoken language analysis  learned analysis provides robustness 

    comparisons related hybrid systems
recently  connectionist networks received lot attention computational learning
mechanisms written language processing  reilly   sharkey        miikkulainen       
feldman        barnden   holyoak        wermter         paper however 
focused examination hybrid connectionist techniques spoken language processing  previous approaches speech language processing processing often
sequential  is  one module speech recognizer syntactic analyzer completed work next module semantic analyzer started work  contrast 
screen works incrementally allows system     modules running par  

fiscreen  flat syntactic semantic spoken language analysis

allel      integrate knowledge sources early      compute analysis
similar humans since humans start process sentences may completed 
compare approach related work systems  head to head comparison different system dicult based different computer environments
whether systems accessed adapted easily input  furthermore 
different systems typically used different purposes different language corpora 
grammars  rules  etc  however  made extensive effort fair conceptual
comparison 
parsec  jain        hybrid connectionist system embedded larger
speech translation effort janus  waibel et al          input parsec sentences 
output case role representations  system consists several connectionist modules
associated symbolic transformation rules providing transformations suggested
connectionist networks  parsec s philosophy use connectionist networks
triggering symbolic transformations  screen uses connectionist networks transformations themselves  screen s philosophy use connectionist networks wherever
possible symbolic rules necessary 
found symbolic processing particularly useful simple known tests  like lexical
equality  complex control tasks whole system  when module communicate module   much actual transformational work done
trained connectionist networks  contrast design philosophy parsec
connectionist modules provide control knowledge transformation
performed  selected transformation actually performed symbolic procedure  screen uses connectionist modules transformations symbolic control 
parsec uses connectionist modules control symbolic procedures transformations 
different screen  parsec receives sentence hypotheses either sentence transcripts n best hypotheses janus system  approach receives incremental
word hypotheses used speech construction part build sentence hypotheses  part used prune hypothesis space determine best sentence
hypotheses  analysis screen semantic syntactic plausibilities
partial sentence hypothesis still uence partial sentence hypotheses
processed 
parsec screen modular architecture tested advantage
connectionist module learn relatively easy subtask  contrast
development parsec experience modularity requires less training time 
furthermore  modules screen able work independently
parallel  addition syntactic semantic knowledge  parsec make use
prosodic knowledge screen currently use prosodic hints  hand 
screen contains modules learning dialog act assignment modules
currently part parsec  learning dialog act processing important determining
intended meaning utterance  wermter   lochel        
recent extensions based parsec provide structure use annotated
linguistic features  bu et al          authors state  implemented  based
parsec  connectionist system  approximate shift reduce parser 
connectionist shift reduce parser substantially differs original parsec architecture 
  

fiwermter   weber

refer  parsec extension   parsec extension labels complete
sentence first level categories  first level categories input
network order provide second level categories complete sentence
on  highest level sentence symbol added 
using recursion step parsec extension provide deeper structural
interpretations screen currently does  however  recursion step construction structure price  first  labels np noun phrase
defined lexical items lexicon  second  important  complete utterance
labeled n th level categories processing n   th level categories
starts  therefore several parses  e g     utterance  his big brother loved himself  
utterance necessary  means recent parsec extension
powerful screen original parsec system jain respect opportunity provide deeper structural interpretations  however  time
parsec extension looses possibility process utterances incremental manner 
however  incrementality important property spoken language processing
screen  besides fact humans process language incremental left to right
manner  allows screen prune search space incoming word hypotheses
early 
comparing parsec screen  parsec aims supporting symbolic rules using symbolic transformations  triggered connectionist networks  integrating linguistic features  currently  linguistic features recent parsec extension  bu et al  
      provide structural morphological knowledge screen does  therefore 
currently appears easier integrate parsec extension larger systems
high level linguistic processing  fact  parsec used context janus
framework  hand  screen aims robust incremental processing
using word hypothesis space  specific repair modules  representations 
particular  screen emphasizes robustness spoken language processing  since
contains explicit repair mechanisms implicit robustness  explicit robustness covers
often occurring errors  interjections  pauses  word phrase repairs  explicit modules 
less predictable types errors supported implicit similaritybased robustness connectionist networks themselves  general  representations generated extension parsec provide better support deeper structures
screen  screen provides better support incremental robust processing 
recent extension based parsec called feaspar  overall parsing performance
syntactic semantic feature accuracy        although additional improvements
shown using subsequent search techniques parsing results  consider
subsequent search techniques better parses since would violate incremental
processing  bu         without using subsequent search techniques screen reaches
overall semantic syntactic accuracy         shown table    however
pointed out  screen feaspar use different input sentences  features
architectures 
besides parsec berp trains systems focus hybrid spoken language processing  berp  berkeley restaurant project  current project employs multiple
different representations speech language analysis  wooters        jurafsky et al        
    b   task berp act knowledge consultant giving advice choos  

fiscreen  flat syntactic semantic spoken language analysis

ing restaurants  different components berp  feature extractor receives
digitized acoustic data extracts features  features used connectionist phonetic probability estimation  output connectionist feedforward network
used viterbi decoder uses multiple pronunciation lexicon different language models  e g  bigram  hand coded grammar rules   output decoder word
strings transformed database queries stochastic chart parser  finally 
dialog manager controls dialog user ask questions 
berp screen common ability deal errors humans
speech recognizer well relatively analysis  however  reaching robustness berp probabilistic chart parser used compute possible fragments first 
then  additional fragment combination algorithm used combining fragments
cover greatest number input words  different sequential process
first computing fragments utterance combining fragments  screen
uses incremental processing desirably provides best possible interpretation 
sense screen s language analysis weaker general  screen s analysis never
break produce best possible interpretation noisy utterances  strategy
may particularly useful incremental translation  hand  berp s language
analysis stronger restricted  berp s analysis may stop fragment level
contradictory fragments  strategy may particularly useful question
answering additional world knowledge necessary available 
trains related spoken language project building planning assistant
reason time  actions  events  allen        allen et al         
goal building general framework natural language processing planning
train scheduling  trains needs lot commonsense knowledge  scenario  person
interacts system order find solutions train scheduling cooperative
manner  person assumed know goals scheduling
system supposed details domain  utterance person
parsed syntactic semantic parser  linguistic reasoning completed
modules scoping reference resolution  linguistic reasoning  conversation
acts determined system dialog manager responses generated based
template driven natural language generator  performance phenomena spoken language
repairs false starts currently dealt already  heeman   allen      b 
    a   compared screen  trains project focuses processing spoken
language in depth planning level  screen uses primarily connectionist
language analysis  trains uses chart parser generalized phrase structure grammar 

   discussion
first focus learned processing spoken language processing 
started screen project  predetermined whether deep analysis
screening analysis would particularly appropriate robust analysis spoken
sentences  deep analysis highly structured representations less appropriate since
unpredictable faulty variations spoken language limit usefulness deep structured knowledge representations much case written language  deep
interpretations structured representations   instance possible hpsg
  

fiwermter   weber

grammars text processing   make great deal assumptions predictions
hold faulty spoken language  furthermore  learned generating
semantic syntactic representation even need use deep interpretation
certain tasks  instance  translating two languages necessary
disambiguate prepositional phrase attachment ambiguities since process
translation disambiguations may get ambiguous target language 
however  use structure level words phrases syntax semantics respectively  learned single semantics level rather four
syntax semantics levels sucient since syntax necessary detecting phrase
boundaries  one could argue one syntactic abstract phrase representation one
abstract semantic phrase representation may enough  however  found basic
syntactic semantic representations word level make task easier subsequent abstract analysis phrase level  furthermore  basic syntactic semantic
representations necessary tasks well  instance judgment
plausibility sequence syntactic semantic categories  plausibility used
filter finding good word hypothesis sequences  therefore  argue processing
faulty spoken language   task sentence translation question answering  
need much less structured representations typically used well known parsers
need structured representations single level tagger 
previous work made early experiences related connectionist
networks analyzing text phrases  moving analyzing text phrases analyzing unrestricted spoken utterances  tremendous differences two tasks  found
phrase oriented analysis used scan  wermter        advantageous principle
spoken language analysis phrase oriented analysis common learning text
speech processing  however  learned spoken language analysis needs much
sophisticated architecture  particular  since spoken language contains many unpredictable errors variations  fault tolerance robustness much important 
connectionist networks inherent implicit robustness based similarity based
processing gradual numerical representations  addition  found classes
relatively often occurring mistakes  explicit robustness provided
machinery interjections  word phrase repairs  furthermore  architecture
consider processing potentially large number competing word hypothesis
sequences rather single sentence phrase text processing 
now  focus learned connectionist hybrid architectures  beginning predetermine whether connectionist methods would
particularly useful control individual modules both  however  development screen system became clear general task
spoken language understanding  individual subtasks syntactic analysis
fault tolerant  noise  spoken language  due humans speechrecognizers well  especially unforeseeable variations often occur spontaneously spoken
language cannot predefined well advance symbolic rules general manner 
fault tolerance task level could supported particularly well inherent
fault tolerance connectionist networks individual tasks support inductive
learning algorithms  learned robust understanding spoken language
connectionist networks particularly effective within individual subtasks 
  

fiscreen  flat syntactic semantic spoken language analysis

quite lot work control connectionist networks  however 
many cases approaches concentrated control single networks 
recently work control modular architectures  sumida       
jacobs et al       b  jain        jordan   jacobs        miikkulainen         instance 
approach jacobs jordan  jacobs et al       b  jordan   jacobs         task
knowledge control knowledge learned both  task knowledge learned individual
task networks  higher control networks responsible learning single task
network responsible producing output  originally open question whether
connectionist control would possible processing spoken language  automatic
modular task decomposition  jacobs et al       a  done simple forms function
approximation  complex problems understanding spoken language real world
environments still need designer based modular task decomposition necessary tasks 
learned connectionist control architecture lot modules
subtasks currently seems beyond capabilities current connectionist networks 
shown connectionist control possible limited number connectionist modules  miikkulainen        jain         instance miikkulainen shows
connectionist segmenter connectionist stack control parser analyze embedded clauses  however  communication paths still restricted within
three modules  especially real world system spoken language understanding speech  syntax  semantics dialog processing translation extremely
dicult learn coordinate different activities  especially large parallel stream
word hypothesis sequences  believe may possible future  however
currently connectionist control screen restricted detection certain hesitations
phenomena corrections 
considering screening analysis spoken language hybrid connectionist techniques together  developed followed general guideline  or design philosophy  
using little knowledge necessary getting far possible using connectionist
networks wherever possible symbolic representations necessary  guideline
led us     robust representation spoken language analysis    
use hybrid connectionist techniques support task choice possibly
appropriate knowledge structure  many hybrid systems contain small portion
connectionist representations addition many modules  e g  berp  wooters 
      jurafsky et al             b   janus  waibel et al          trains  allen        allen
et al          contrast  important subtasks screen performed directly
many connectionist networks 
furthermore  learned syntactic semantic representations could give
surprisingly good training test results trained tested medium corpus
     words     dialog turns  good results mostly due
learned internal weight representation local context adds sequentiality
category assignments  without internal weight representation preceding context
syntactic semantic categorization perform equally well  choice
recurrent networks crucial many sequential category assignments  therefore
networks techniques hold potential especially medium size domains
restricted amount training material available  statistical techniques often
  

fiwermter   weber

used large data sets  work well medium data sets  connectionist
techniques used work well medium size domains 
used techniques ported different domains used different purposes  even different sets categories would used learning networks
able extract syntactic regularities automatically  besides domain arranging
business meetings ported screen domain interactions railway
counter comparable syntactic semantic results  two domains differed primarily semantic categories  syntactic categories  and networks  screen
could used directly 
screen potential scaling up  fact  based imperfect output
speech recognizer  several thousand sentence hypotheses already processed 
new words processed  syntactic semantic basic categories simply
entered lexicon  structure individual networks change  new units
added therefore networks retrained 
amount hand coding restricted primarily symbolic control module
interaction labeling training material individual networks 
changed domain railway counter interactions  could use identical control 
well syntactic networks  semantic networks retrained due
different domain 
far focused supervised learning simple recurrent networks feedforward networks  supervised learning still requires training set manual labeling
work still done  although especially medium size corpora labeling examples
easier instance designing complete rule bases would nice automate
knowledge acquisition even further  currently plan build sophisticated lexicon component provide support automatic lexicon design  riloff       
dynamic lexicon entry determination using local context  miikkulainen        
furthermore  screen could expanded speech construction evaluation
part  syntactic semantic hypotheses could used interaction
speech recognizer  currently syntactic semantic hypotheses speech evaluation
part used exclude unlikely word hypothesis sequences language modules 
however  hypotheses connectionist networks syntax semantics  
particular modules basic syntactic semantic category prediction   could
used directly process recognition future order provide
syntactic semantic feedback speech recognizer early stage  besides syntax
semantics  cue phrases  stress intonation could provide additional knowledge
speech language processing  hirschberg        gupta   touretzky         issues
additional major efforts future 

   conclusions
described underlying principles  implemented architecture  evaluation new screening approach learning analysis spoken language  work
makes number original contributions fields artificial intelligence advances
state art several perspectives  perspective symbolic connectionist design argue hybrid solution  connectionist networks used
  

fiscreen  flat syntactic semantic spoken language analysis

wherever useful symbolic processing used control higher level analysis  furthermore  shown recurrent networks provided better syntactic
semantic prediction results     grams  perspective connectionist networks
alone  demonstrated connectionist networks fact used real world
spoken language analysis  perspective natural language processing argue
hybrid system design advantageous integrating speech language since lower
speech related processing supported fault tolerant learning connectionist networks
higher processing control supported symbolic knowledge structures  general  properties support parallel rather sequential  learned rather coded 
fault tolerant rather strict processing spoken language 
main result paper learned representations support robust processing spoken language better in depth structured representations connectionist networks provide fault tolerance reach robustness  due noise
spontaneous language  interjections  pauses  repairs  repetitions  false starts  ungrammaticalities  additional false word hypotheses speech recognizer  complex structured possibly recursive representations often cannot computed using standard symbolic
representations context free parsers  hand  tasks information
extraction spoken language may need in depth structured representation  believe hybrid connectionist techniques considerable potential
tasks  instance information extraction restricted noisy spoken language domains  in depth understanding inferencing story interpretation needs
complex structured representations  shallow understanding instance information
extraction noisy speech language environments benefit at  robust learned
representations 

acknowledgements
research funded german federal ministry research technology
 bmbf  grant    iv   a  german research association  dfg 
grant dfg ha           grant dfg           would thank s  haack 
m  lochel  m  meurer  u  sauerland  m  schrattenholzer work screen 
well david bean  alexandra klein  steven minton  johanna moore  ellen riloff five
anonymous referees comments earlier versions paper 

references
allen  j  f          trains    parsing system  user s manual  tech  rep  trains
tn       university rochester  computer science department 
allen  j  f   schubert  l  k   ferguson  g   heeman  p   hwang  c  h   kato  t   light 
m   martin  n  g   miller  b  w   poesio  m     traum  d  r          trains
project  case study building conversational planning agent  journal experimental theoretical ai          
  

fiwermter   weber

barnden  j  a     holyoak  k  j   eds            advances connectionist neural
computation theory  vol      ablex publishing corporation 
bu  f  d          feaspar   feature structure parser learning parse spontaneous
speech  ph d  thesis  university karlsruhe  karlsruhe  frg 
bu  f  d   polzin  t  s     waibel  a          learning complex output representations
connectionist parsing spoken language  proceedings international conference acoustics  speech signal processing  vol     pp           adelaide 
australia 
charniak  e          statistical language learning  mit press  cambridge  ma 
dyer  m  g          in depth understanding  computer model integrated processing
narrative comprehension  mit press  cambridge  ma 
elman  j  l          finding structure time  cognitive science                  
faisal  k  a     kwasny  s  c          design hybrid deterministic parser  proceedings    th international conference computational linguistics  pp        
helsinki  finnland 
feldman  j  a          structured connectionist models language learning  artificial
intelligence review                 
geutner  p   suhm  b   bu  f  d   kemp  t   mayfield  l   mcnair  a  e   rogina  i  
schultz  t   sloboda  t   ward  w   woszczyna  m     waibel  a          integrating
different learning approaches multilingual spoken language translation system 
wermter  s   riloff  e     scheler  g   eds    connectionist  statistical symbolic approaches learning natural language processing  pp           springer 
heidelberg 
gupta  p     touretzky  d  s          connectionist models linguistic theory  investigations stress systems language  cognitive science               
hauenstein  a     weber  h  h          investigation tightly coupled time synchronous
speech language interfaces using unification grammar  proceedings   th
national conference artificial intelligence workshop integration natural
language speech processing  pp         seattle  washington 
heeman  p  a     allen  j       a   detecting correcting speech repairs  proceedings
  nd annual meeting association computational linguistics  pp      
     las cruces  nm 
heeman  p  a     allen  j       b   tagging speech repairs  proceedings human
language technology workshop  pp           plainsboro  nj 
hendler  j  a          marker passing microfeatures  towards hybrid symbolic connectionist model  cognitive science                 
  

fiscreen  flat syntactic semantic spoken language analysis

hirschberg  j          pitch accent context  predicting intonational prominence
text  artificial intelligence              
jacobs  r  a   jordan  m  i     barto  a  g       a   task decomposition competition modular connectionist architecture  vision tasks 
cognitive science              
jacobs  r  a   jordan  m  i   nowlan  s  j     hinton  g  e       b   adaptive mixtures
local experts  neural computation               
jain  a  n          parsing complex sentences structured connectionist networks 
neural computation                 
jordan  m  i     jacobs  r  a          hierarchies adaptive experts  moody  j  e  
hanson  s  j     lippmann  r  r   eds    advances neural information processing
systems    pp           morgan kaufmann  san mateo  ca 
jurafsky  d   wooters  c   tajchman  g   segal  j   stolcke  a   fosler  e     morgan 
n       a   berkeley restaurant project  proceedings international
conference speech language processing  pp             yokohama  japan 
jurafsky  d   wooters  c   tajchman  g   segal  j   stolcke  a     morgan  n       b   integrating experimental models syntax  phonology  accent dialect speech recognizer  investigation tightly coupled time synchronous speech  proceedings
  th national conference artificial intelligence workshop integration
natural language speech processing  pp           seattle  washington 
medsker  l  r          hybrid neural network expert systems  kluwer academic
publishers  boston 
mellish  c  s          chart based techniques parsing ill formed input  proceedings   th annual meeting association computational linguistics  pp 
         vancouver  canada 
menzel  w          parsing spoken language time constraints  cohn  a  g 
 ed    proceedings   th european conference artificial intelligence  pp      
     amsterdam 
miikkulainen  r          subsymbolic natural language processing  integrated model
scripts  lexicon memory  mit press  cambridge  ma 
miikkulainen  r          subsymbolic case role analysis sentences embedded clauses 
cognitive science            
nakatani  c     hirschberg  j          speech first model repair detection correction  proceedings   st annual meeting association computational
linguistics  pp        columbus  ohio 
reilly  r  g     sharkey  n  e   eds            connectionist approaches natural language processing  lawrence erlbaum associates  hillsdale  nj 
  

fiwermter   weber

riloff  e          automatically constructing dictionary information extraction tasks 
proceedings   th national conference artificial intelligence  pp          
washington  dc 
rumelhart  d  e   hinton  g  e     williams  r  j          learning internal representations error propagation  rumelhart  d  e   mcclelland  j  l     pdp
research group  eds    parallel distributed processing  vol      pp           mit press 
cambridge  ma 
sauerland  u          konzeption und implementierung einer speech language
schnittstelle  master s thesis  university hamburg  computer science department 
hamburg  frg 
sumida  r  a          dynamic inferencing parallel distributed semantic networks 
proceedings   th annual meeting cognitive science society  pp          
boston  chicago 
sun  r          integrating rules connectionism robust common sense reasoning 
wiley sons  new york 
von hahn  w     pyka  c          system architectures speech understanding
language processing  heyer  g     haugeneder  h   eds    applied linguistics 
wiesbaden 
waibel  a   jain  a  n   mcnair  a   tebelskis  j   osterholtz  l   saito  h   schmidbauer 
o   sloboda  t     woszczyna  m          janus  speech to speech translation using
connectionist non connectionist techniques  moody  j  e   hanson  s  j    
lippmann  r  r   eds    advances neural information processing systems    pp 
         morgan kaufmann  san mateo  ca 
ward  n          approach tightly coupled syntactic semantic processing speech
understanding  proceedings   th national conference artificial intelligence workshop integration natural language speech processing  pp 
       seattle  washington 
weber  v     wermter  s          towards learning semantics spontaneous dialog utterances hybrid framework  hallam  j   ed    hybrid problems  hybrid solutions
  proceedings   th biennial conference ai cognitive science  pp 
         sheeld  uk 
weber  v     wermter  s          artificial neural networks repairing language 
proceedings  th international conference neural networks applications  pp           marseille  fra 
wermter  s     weber  v       a   interactive spoken language processing hybrid
connectionist system  ieee computer   theme issue interactive natural language
processing  july        
  

fiscreen  flat syntactic semantic spoken language analysis

wermter  s          hybride symbolische und subsymbolische verarbeitung beispiel
der sprachverarbeitung  duwe  i   kurfe  f   paa  g     vogel  s   eds   
herbstschule konnektionismus und neuronale netze  gesellschaft fur mathematik und
datenverarbeitung  gmd   sankt augustin  frg 
wermter  s          hybrid connectionist natural language processing  chapman
hall  thompson international  london  uk 
wermter  s     lochel  m          connectionist learning syntactic analysis
speech language systems  marinaro  m     morasso  p  g   eds    proceedings
international conference artificial neural networks  vol     pp          
sorrento  italy 
wermter  s     lochel  m          learning dialog act processing  proceedings
  th international conference computational linguistics  pp           kopenhagen  denmark 
wermter  s     peters  u          learning incremental case assignment based modular
connectionist knowledge sources  werbos  p   szu  h     widrow  b   eds    proceedings world congress neural networks  vol     pp           san diego 
ca 
wermter  s   riloff  e     scheler  g   eds            connectionist  statistical symbolic
approaches learning natural language processing  springer  berlin 
wermter  s     weber  v       b   artificial neural networks automatic knowledge acquisition multiple real world language domains  proceedings  th international conference neural networks applications  pp           marseille 
fra 
wooters  c  c          lexical modeling speaker independent speech understanding
system  tech  rep  tr         international computer science institute  berkeley 
young  s  r   hauptmann  a  g   ward  w  h   smith  e     werner  p          high
level knowledge sources usable speech recognition systems  communications
acm              

  


