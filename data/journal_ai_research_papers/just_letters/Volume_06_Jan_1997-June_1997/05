journal artificial intelligence research                 

submitted        published     

connectionist theory refinement 
genetically searching space network topologies
david w  opitz

opitz cs umt edu

jude w  shavlik

shavlik cs wisc edu

department computer science
university montana
missoula  mt       usa

computer sciences department
university wisconsin
     w  dayton st 
madison  wi       usa

abstract

algorithm learns set examples ideally able exploit
available resources  a  abundant computing power  b  domain specific knowledge
improve ability generalize  connectionist theory refinement systems  use background knowledge select neural network s topology initial weights  proven
effective exploiting domain specific knowledge  however  exploit available computing power  weakness occurs lack ability refine
topology neural networks produce  thereby limiting generalization  especially
given impoverished domain theories  present regent algorithm uses
 a  domain specific knowledge help create initial population knowledge based neural networks  b  genetic operators crossover mutation  specifically designed
knowledge based networks  continually search better network topologies  experiments three real world domains indicate new algorithm able significantly
increase generalization compared standard connectionist theory refinement system 
well previous algorithm growing knowledge based networks 

   introduction
many scientific industrial problems better understood learning samples
task  reason  machine learning statistics communities devote considerable research effort inductive learning algorithms  often  however  learning
algorithms fail capitalize number potentially available resources  domainspecific knowledge computing power  improve ability generalize  using
domain specific knowledge desirable inductive learners start approximately correct theory achieve improved  generalization   accuracy examples
seen training  significantly fewer training examples  ginsberg        ourston
  mooney        pazzani   kibler        towell   shavlik         making effective use
available computing power desirable because  many applications  important
obtain concepts generalize well induce concepts quickly  article  present algorithm  called regent  refining  genetic evolution  network
topologies   utilizes available computer time extensively search neural network

c      ai access foundation morgan kaufmann publishers  rights reserved 

fiopitz   shavlik

topology best explains training data minimizing changes domain specific
theory 
inductive learning systems utilize set approximately correct  domain specific
inference rules  called domain theory  describe currently known
domain  called theory refinement systems  making use knowledge
shown important since rules may contain insight easily obtainable
current set training examples  ourston   mooney        pazzani   kibler       
towell   shavlik         domains  expert created theory willing
wait weeks  even months  learning system produce improved theory 
thus  given rapid growth computing power  believe important learning
techniques able trade expense large numbers computing cycles gains
predictive accuracy  analogous anytime planning techniques  dean   boddy        
believe machine learning researchers create better anytime learning algorithms 
learning algorithms produce good concept quickly  continue search
concept space  reporting new  best  concept whenever one found 
concentrate connectionist theory refinement systems  since shown
frequently generalize better many inductive learning theory refinement
systems  fu        lacher  hruska    kuncicky        towell         kbann  towell  
shavlik        example connectionist system  translates provided
domain theory neural network  thereby determining network s topology 
refines reformulated rules using backpropagation  rumelhart  hinton    williams 
       however  kbann  connectionist theory refinement systems
alter network topologies  suffer given impoverished domain theories   ones
missing rules needed adequately learn true concept  opitz   shavlik       
towell   shavlik         topgen  opitz   shavlik        improvement
systems  heuristically searches space possible network topologies adding
hidden nodes neural representation domain theory  topgen showed statistically
significant improvements kbann several real world domains  opitz   shavlik        
however  paper empirically show topgen nevertheless suffers
considers simple expansions kbann network 
address limitation  broaden types topologies topgen considers
using genetic algorithms  gas   choose gas two reasons  first  gas
shown effective optimization techniques ecient use global
information  goldberg        holland        mitchell         second  gas inherent
quality makes suitable anytime learning   off line  application mode
 dejong         gas simulate many alternatives output best alternative seen
far 
new algorithm  regent  proceeds first trying generate  domain
theory  diversified initial population  produces new candidate networks via
genetic operators crossover mutation  networks trained using
backpropagation  regent s crossover operator tries maintain rule structure
network  mutation operator adds nodes network using topgen algorithm  hence  genetic operators specialized connectionist theory refinement 
experiments reported herein show regent better able search network topologies topgen 
   

ficonnectionist theory refinement

rest paper organized follows  next section  brie argue
importance effectively exploiting data  theory  available computer time
learning process  review kbann topgen algorithms  present
details regent algorithm section    followed empirical results
three human genome project domains  section    discuss results  well
future work  review related work concluding 

   using data  prior knowledge  available cpu cycles
system learns set labeled examples called inductive learner  alternately  supervised  empirical  similarity based learner   output example
provided teacher  set labeled examples given learner called
training set  task inductive learning generate training set concept
description correctly predicts output future examples 
training set  many inductive learning algorithms previously studied  e g  
michalski        quinlan        rumelhart et al          algorithms differ
concept representation language  method  or bias  constructing
concept within language  differences important since determine
concepts classifier induce 
alternative inductive learning paradigm build concept description
set examples  querying experts field directly assembling set
rules describe concept  i e   build expert system  waterman         problem
building expert systems theories derived interviewing experts tend
approximately correct  thus  expert provided domain theory usually
good first approximation concept learned  inaccuracies frequently exposed
empirical testing 
theory refinement systems  ginsberg        ourston   mooney        pazzani   kibler 
      towell   shavlik        systems revise theory basis collection
examples  systems try improve theory making minimal repairs theory
make consistent training data  changes initial domain theory
kept minimum theory presumably contains useful information  even
completely correct  hybrid learning systems designed learn
theory data  empirical tests shown achieve high generalization
significantly fewer examples purely inductive learning techniques  ourston   mooney 
      pazzani   kibler        towell   shavlik         thus  ideal inductive learning
system able incorporate background knowledge available
form domain theory improve ability generalize 
indicated earlier  available computer time important resource since  a  computing power rapidly increasing   b  problems expert willing wait
lengthy period improved concept  reasons  one develop  anytime 
learning algorithms continually improve quality answer time  dean
boddy        defined criteria anytime algorithm be   a  algorithm
suspended resumed minimal overhead   b  algorithm stopped
time return answer   c  algorithm must return answers improve
   

fiopitz   shavlik

x
x

x

x

output

x x

x

x

x

input

figure    classical regression example smooth function  the solid curve 
fit noisy data points  the x s  probably better predictor
high degree polynomial  the dashed curve  

time  criteria created planning scheduling algorithms 
apply inductive learning algorithms well  
standard inductive learners  backpropagation  rumelhart et al        
id   quinlan         unable continually improve answers  at least
receive additional training examples   fact  run long  algorithms tend
 overfit  training set  holder         overfitting occurs learning algorithm
produces concept captures much information training examples 
enough general characteristics domain whole  concepts
great job classifying training instances  poor job generalizing
new examples   ultimate measure success  help illustrate point  consider
typical regression case shown figure    here  fitting noisy data high degree
polynomial likely lead poor generalization 
general framework use encouraging algorithm improve answer
time quite simple  spend computer time considering many different possible
concept descriptions  scoring possibility  always keeping description scores
best  framework anytime respect scoring function  scoring
function approximate measure generalization obviously still prone
problems overfitting  thus guarantee generalization monotonically
decrease time  nevertheless  assuming accurate scoring function  long
considering wide range good possibilities  quality best concept likely
improve longer period time 
   use term anytime learning differs grefenstette ramsey         use
mean continuous learning changing environment 

   

ficonnectionist theory refinement

   review kbann topgen
goal research exploit prior knowledge available computing cycles
search neural network likely generalize best  proceed
choosing  initial guess  network defined kbann algorithm 
continually refine topology find best network concept  presenting
new algorithm  regent   give overview kbann algorithm well
initial approach refining kbann created network s topology  topgen  

    kbann algorithm
kbann  towell   shavlik        works translating domain theory consisting set

propositional rules directly neural network  see figure     figure  a shows prologlike rule set defines membership category a  figure  b represents hierarchical
structure rules  solid lines representing necessary dependencies dotted lines
representing prohibitory dependencies  figure  c represents network kbann creates
translation  sets biases nodes representing disjuncts output
near   least one high weighted antecedents satisfied  nodes
representing conjuncts must high weighted antecedents satisfied  i e   near
  positive links near   negative links   otherwise activations near    kbann
creates nodes b  b  figure  c handle two rules disjunctively defining b 
thin lines figure  c represent low weighted links kbann adds allow rules
add new antecedents backpropagation training  following network initialization 
kbann uses available training instances refine network links  refer towell
       towell shavlik        details 
kbann successfully applied several real world problems  control
chemical plant  scott  shavlik    ray         protein folding  maclin   shavlik        


  b  c 
b   d  f  g 

b



b 

b   d  f  i 
c   h  j  k 

 a 

b

c

e f g h j k
 b 

e

b 

f

g h
 c 

c

j k

figure    kbann s translation knowledge base neural network  panel  a  shows
sample propositional rule set prolog  clocksin   mellish        notation 
panel  b  illustrates rule set s corresponding and or dependency tree 
panel  c  shows resulting network created kbann s translation 
   

fiopitz   shavlik

finding genes sequence dna  opitz   shavlik        towell   shavlik        
ecg patient monitoring  watrous  towell    glassman         case  kbann
shown produce improvements generalization standard neural networks small
numbers training examples  fact  towell        favorably compared kbann
wide variety algorithms  including purely symbolic theory refinement systems 
version promoter splice junction tasks include testbeds section   
training kbann created network alters antecedents existing rules 
capability inducing new rules add additional
hidden nodes training  instance  kbann unable add third rule
inferring b figure   s example  help illustrate point  consider following
example  assume figure   s target concept consists figure  a s domain theory plus
rule 
b    d  e  g 
although trained kbann network shown figure  c possible examples
target concept  unable completely learn conditions true 
topology kbann network must modified order learn new rule 
studies show  opitz   shavlik        towell        kbann effective
removing extraneous rules antecedents expert provided domain theory  generalization ability suffers given  impoverished  domain theories   theories
missing rules antecedents needed adequately learn true concept  ideal connectionist theory refinement algorithm  therefore  able dynamically expand
topology network training 

    topgen algorithm

topgen  opitz   shavlik        addresses kbann s limitation heuristically searching
space possible expansions knowledge based neural network   network
whose topology determined direct mapping dependencies domain theory
 e g   kbann network   topgen proceeds first training kbann network 
placing search queue  cycle  topgen takes best network search
queue  estimates errors occur network  adds new nodes response
estimates  trains new networks  places back queue  topgen judges
errors occur network using training examples increment two counters
node  one false negatives one false positives 
figure   illustrates possible ways topgen add nodes one networks 
symbolic rule base uses negation by failure  one decrease false negatives either
dropping antecedents existing rules adding new rules rule base  kbann
effective removing antecedents existing rules  unable add new rules 
therefore  topgen adds nodes  intended decreasing false negatives  fashion
analogous adding new rule rule base  existing node node  topgen
adds new node child  see figure  a   fully connects new node input
nodes  existing node node  topgen creates new node
parent original node another new node topgen fully connects
inputs  see figure  c   topgen moves outgoing links original node  a
figure  c  become outgoing links new node 
   

ficonnectionist theory refinement

existing node

decrease false negatives

decrease false positives
   

   

   

new
node





new
node


b

b

c

node

c

new
node

b

c

 a 

 b 
   



c

node



new
node


b

   

new
node

   

b
b

c

c

 c 

new
node

 d 

figure    possible ways add new nodes knowledge based neural network  arcs indicate nodes   decrease false negatives  wish broaden applicability node  conversely  decrease false positives  wish
constrain node 
symbolic rule base  one decrease false positives either adding antecedents
existing rules removing rules rule base  kbann effectively remove
rules  less effective adding antecedents rules unable invent  i e  
constructively induce  michalski        new terms antecedents  thus topgen adds new
nodes  intended decrease false positives  fashion analogous adding new
constructively induced antecedents network  figures  b  d illustrates
done  analogous figures  a  c explained above   refer opitz shavlik       
      details 
topgen showed statistically significant improvements kbann several real world
domains  comparative experiments simpler approach adding nodes verified
new nodes must added intelligent manner  opitz   shavlik        
article  however  increase number networks topgen considers search
show increase generalization primarily limited first networks
considered  therefore  topgen much  anytime  algorithm  rather first
step towards one  mostly due fact topgen considers larger networks contain original kbann network subgraphs  however  one increases
number networks considered  one increase variety networks considered
   

fiopitz   shavlik

search  broadening range networks considered search
topology space major focus paper 

   regent algorithm

new algorithm  regent  tries broaden types networks topgen considers
use gas  view regent two phases   a  genetically searching
topology space   b  training network using backpropagation s gradient
descent method  regent uses domain theory aid phases  uses theory
help guide search topology space give good starting point weight
space 
table   summarizes regent algorithm  regent first sets aside validation set
 from part training instances  use scoring different networks  perturbs kbann produced network create initial set candidate networks  next 
regent trains networks using backpropagation places population  cycle  regent creates new networks crossing mutating networks
current population randomly picked proportional fitness  i e  
validation set correctness   trains new networks places
population  searches  regent keeps network lowest validation set
error best concept seen far  breaking ties choosing smaller network
application occam s razor  parallel version regent trains many candidate networks time using condor system  litzkow  livny    mutka        
runs jobs idle workstations 
diverse initial population broaden types networks regent considers
search  however  since domain theory may provide useful information may
present training set  still desirable use theory generating initial
population  regent creates diversity around domain theory randomly perturbing
kbann network various nodes  regent perturbs node either deleting it 
adding new nodes manner analogous one topgen s four methods adding

goal  search best network topology describing domain theory data 

   set aside validation set training instances 
   perturb kbann produced network multiple ways create initial networks  train
networks using backpropagation place population 
   loop forever 
 a  create new networks using crossover mutation operators 
 b  train networks backpropagation  score validation set  place
population 
 c  new network network lowest validation set error seen far  breaking
ties preferring smallest network   report current best concept 

table    regent algorithm 
   

ficonnectionist theory refinement

crossover two networks 

goal  crossover two networks generate two new network topologies 
   divide network s hidden nodes sets b using dividenodes 

   set forms one network  set b forms another  new network created follows 
 a  network inherits weight w parent nodes j either inherited
input output nodes 
 b  link unconnected nodes levels near zero weights 
 c  adjust node biases keep original function node  see text explanation  
ji

dividenodes 
goal 

divide hidden nodes sets b  probabilistically maintaining
network s rule structure 
hidden node assigned set b 
 i  collect unassigned hidden nodes whose output linked either previouslyassigned nodes output nodes 
 ii  set set b empty 
node collected part  i   randomly assign set set b 

else

probabilistically add nodes collected part  i  set set b  equation  
shows probability assigned set a  probability assigned
set b one minus value 

table    regent s method crossing networks 
nodes   should happen multiple theories domain 
used seed population  

    regent s crossover operator

regent crosses two networks first dividing nodes parent network

two sets  b  combining nodes set form two new networks  i e  
nodes two sets form one network  nodes two b sets form another  
table   summarizes regent s method crossover figure   illustrates
example  regent divides nodes  one level  time  starting level nearest
output nodes  considering level  either set set b empty  cycles
node level randomly assigns either set  neither set empty  nodes
probabilistically placed set  following equation calculates probability
   although one define level several different ways  define node s level longest path
output node 

   

fiopitz   shavlik

original
networks
crossed


output

output

input

input

output

output

input

input

resulting
networks
figure    regent s method crossing two networks  hidden nodes
original network divided sets b  nodes two sets
form one new network  nodes two b sets form another  grey lines
represent low weighted links added fully connect neighboring levels 
given node assigned set a 

pj a jwjij
prob node   seta    p jw j   p jw j  
j  a ji
j  b ji

   

j   means node j member set wji weight value node
node j   probability belonging set b one minus probability 
probabilities  regent tends assign set nodes heavily linked
together  helps minimize destruction rule structure crossed over
networks  since nodes belonging syntactic rule connected heavily linked
weights  thus  regent s crossover operator produces new networks crossing over rules 
rather simply crossing over nodes 
regent must next decide connect nodes newly created networks 
first  new network inherits weight values parents links  a  connect
two nodes inherited new network   b  connect inherited hidden
node input output node   c  directly connect input node output node 
adds randomly set  low weighted links unconnected nodes consecutive
levels 
finally  adjusts bias nodes help maintain original function 
instance  regent removes positively weighted incoming link node 
decrements node s bias subtracting product link s magnitude
   

ficonnectionist theory refinement

average activation  over set training examples  entering link 
bias node needs slightly less sum positive weights
incoming links  see towell shavlik       details   regent increments
bias node analogous amount removes negatively weighted incoming
links  since bias node slightly greater sum negative
weights incoming links node inactive incoming negatively
weighted linked nodes active positively weighted linked nodes inactive  

    regent s mutation operator

regent mutates networks applying variant topgen  regent uses topgen s
method incrementing false negatives false positives counters node  regent adds nodes  based values counters  way topgen does 

since neural learning effective removing unwanted antecedents rules knns
 see section       regent considers adding nodes  deleting them  mutation  thus  mutation operator adds diversity population  still maintaining
directed  heuristic search technique choosing add nodes  directedness
necessary currently unable evaluate thousand possible
networks per day 

    additional details
regent adds newly trained networks population validation set correctness better equal existing member population  regent

replaces member  replaces member lowest correctness  ties broken
choosing oldest member   techniques  goldberg         replacing
member nearest new candidate network  promote diverse populations  however 
want promote diversity expense decreased generalization  future
research topic  plan investigate incorporating diversity promoting techniques
able consider tens thousands networks 
regent considered lamarckian    genetic hillclimbing algorithm  ackley        
since performs local optimizations individuals  passes successful optimizations
offspring  ability individuals learn smooth fitness landscape
facilitate subsequent learning  thus  lamarckian learning lead large increase
learning speed solution quality  ackley   littman        farmer   belin        

   experimental results
section  test regent three real world human genome project problems
aid locating genes dna sequences  recognizing promoters  splice junctions 
ribosome binding sites   domains  input short segment dna nucleotides
 about     elements long  task learn predict dna subsequence contains
biologically important site  domain accompanied domain theory generated
dna expert  m  noordewier  
   lamarckian evolution theory based inheritance characteristics acquired lifetime 

   

fiopitz   shavlik

promoter domain contains     positive examples      negative examples    
rules  splice junction domain contains       examples distributed equally among three
classes     rules  finally  ribosome binding sites  rbs  domain  contains    
positive examples        negative examples     rules   note promoter data set
domain theory later version one appears towell         domains
available university wisconsin machine learning  uw ml  site via world
wide web  ftp   ftp cs wisc edu machine learning shavlik group datasets  
anonymous ftp  ftp cs wisc edu  machine learning shavlik group datasets  
first directly compare regent topgen kbann  perform
lesion study  regent  particular  investigate value adding randomly
created networks regent s initial population examine utility regent s
genetic operators 

    experimental methodology

results article ten fold cross validation runs  ten fold cross
validation data set first partitioned ten equal sized sets  set turn
used test set classifier trains nine sets  fold  regent
run population size     network trained using backpropagation  parameter
settings neural networks include learning rate       momentum term     
number training epochs     first two standard settings   
epochs may fewer typically found neural network literature  set   
help avoid overfitting  set aside validation set consisting     training
examples regent use scoring function 

    generalization ability regent

section s experiments compare test set accuracy  i e   generalization  regent
topgen s  figure   shows test set error kbann  topgen  regent
search space network topologies  horizontal line graph results
kbann algorithm  drew horizontal line sake visual comparison 
recall kbann considers single network  first point graph 
one network considered  nearly three algorithms  since start
kbann network  however  topgen regent differ slightly kbann since
must set aside part training set score candidate networks  notice
topgen stops improving considering       networks generalization
ability regent better topgen s point  reason occasional
upward movements figure   due fact validation set  or scoring
function  inexact estimate true generalization error  as results
ten fold cross validation  
figure   presents test set error topgen regent consider
    candidate topologies  standard neural network results fully connected 
single layer  feed forward neural network  fold  trained    networks containing
    hidden nodes used validation set choose best network  results
   lesion study one components algorithm individually disabled ascertain
contribution full algorithm s performance  kibler   langley        

   

ficonnectionist theory refinement

    

  
ribosome binding sites

  
kbann
topgen

  

regent

testset error

  

  

  

splice junctions

  

  

  
promoters
  

 

   

   

   

   

networks considered
figure    error rates three human genome problems 

   

   

fiopitz   shavlik

show kbann generalizes much better best standard networks  thus
confirming kbann s effectiveness generating good network topologies  topgen
able improve kbann network  regent able significantly decrease error
rate kbann topgen   for benchmark purposes  regent error rate
     ten fold cross validation full splice junction dataset      examples
commonly used machine learning researchers  
table   contains number hidden nodes final networks produced kbann 
topgen  regent  results demonstrate regent produces networks
larger kbann s topgen s networks  even though topgen adds nodes
search   regent s networks larger  necessarily mean
 complex   inspected sample networks found large
portions network either used  e g   weights insignificantly small 
functional duplications groups hidden nodes 
one could prune weights nodes regent s search  however  pruning
prematurely reduce variety structures available recombination crossover
 koza         real life organisms  instance  super uous dna believed
enhance rate evolution  watson  hopkins  roberts  argetsinger steitz    weiner 
       however  pruning network size genetic search may unwise  one
could prune regent s final network using  say  hassibi stork s        optimal brain
surgeon algorithm  post pruning process may increase future classification speed
network  well increase comprehensibility possibly accuracy 

    lesion study regent
section  describe lesion study performed regent  since single run
regent takes four cpu days consider     networks  single ten fold cross
     
    

    

    

testset error

    
  

    
    

  

         

key

    

standard nn

    

    

    

kbann
topgen

  

regent
  

rbs

splice junctions

promoters

figure    test set error rates topgen regent consider     networks  pairwise  one tailed t tests indicate regent differs standard nn  kbann 
topgen     confidence level three problems 
   

ficonnectionist theory refinement

domain
kbann topgen
regent
rbs
  
                      
splice junction
  
                      
promoters
  
                      
table    number hidden nodes networks produced kbann  topgen 
regent  columns show mean number hidden nodes found within
networks  standard deviations contained within parentheses 
report standard deviations kbann since uses one network 

validation takes  a minimum of     cpu days  therefore  given inherent similarity
investigating various aspects regent multiple datasets  feasible
run experiments section     confidence level reached cases
 assuming level actually exists   nonetheless  results convey important
information various components regent  and  shown previous section 
complete regent algorithm generate statistically significant improvements
existing algorithms 
      including non knns regent s population

correct theory may quite different initial domain theory  thus 
section investigate whether one include  initial population networks 
variety networks obtained directly domain theory  currently  regent
creates initial population always perturbing kbann network  include networks
obtained domain theory  first randomly pick number hidden
nodes include network  randomly create hidden nodes network 
adding new nodes randomly selected output hidden node using one
topgen s four methods adding new nodes  refer figure     adding nodes
manner creates random networks whose node structure analogous dependencies found
symbolic rule bases  thus creating networks suitable regent s crossover mutation
operators 
table   shows test set error regent various percentages knowledge based
neural networks  knns  present initial population  first row contains results
initializing regent purely random initial population  i e   population contains
knns   second row lists results regent creates half population
domain theory  half randomly  finally  last row contains results
seeding entire population domain theory 
results suggest including  initial population  networks
created domain theory increases regent s test set error three domains 
occurs randomly generated networks correct knns 
   

fiopitz   shavlik

   knn
    knn
     knn

rbs
    
    
    

splice junction
    
    
    

promoters
    
    
    

table    test set error considering     networks  row gives pergentage
knns present initial population  pairwise  one tailed t tests indicate
initializing regent      knns differs    knns     confidence
level three domains  however  difference runs    
     knns significant level 

thus offspring original knn quickly replace random networks  hence  diversity population suffers compared methods start whole population
knns  assuming domain theory  malicious   therefore better seed
entire population kbann network  domain theory indeed malicious
contain information promotes spurious correlations data  would
reasonable randomly create  whole  population  running regent
without domain theory allows one investigate utility theory 
results interesting ga point view  forrest mitchell       
showed gas perform poorly complex problems basic building blocks either
 a  non trivial find  b  get split crossover  seeding initial population
domain theory  as regent does  help define basic building blocks
problems 
      value regent s mutation

typically gas  mutation secondary operation sparingly used  goldberg         however  regent s mutation directed approach heuristically adds
nodes knns provenly effective manner  i e   uses topgen   therefore reasonable hypothesize one apply mutation operator frequently
traditionally done gas  results section test hypothesis 
figure   presents test set error regent varying percentages mutation
 versus crossover  creating new networks step  a table    graph plots four
curves   a     mutation  i e   regent uses crossover    b      mutation   c     
mutation   d       mutation  performing mutations tests value solely using
crossover       mutation tests ecacy mutation operator itself  note
     mutation topgen different search strategy  instead keeping
open list heuristic search  population knns generated members
population improved proportional fitness  two curves     
    mutation  test synergy two operators  performing     mutation
   

ficonnectionist theory refinement

    

  

ribosome binding sites
  
   mutation
    mutation

  

    mutation
     mutation

testset error

  

  

  

splice junctions
  

  

  

promoters
  

 

   

   

   

   

   

networks considered
figure    error rates regent different fractions mutation versus crossover
considering     networks  arguably due inherent similarity
algorithms  limited number runs due computational complexity 
results significant     confidence level 

   

fiopitz   shavlik

closer traditional ga viewpoint mutation secondary operation     
mutation means operations equally valuable   previous experiments
section used     mutation     crossover  
differences statistically significant  results nevertheless suggest
synergy exists two operations  except middle portion
promoter domain  results show that  qualitatively  using operations
time better using either operation alone  fact  equally mixing mutation
crossover operator better three curves three domains regent
considered     networks  result particularly pronounced splice junction
domain 
      value regent s crossover
regent tries cross rules networks  rather blindly crossing

nodes  probabilistically dividing nodes network two sets
nodes belonging rule tend belong set  section 
test ecacy regent s crossover comparing variant
randomly assigns nodes two sets  rather using dividenodes table    
table   contains results test     networks considered 
first row  regent random crossover  regent randomly breaks hidden nodes
two sets  second row  regent assigns nodes two sets according table
   cases  regent creates half networks mutation operator 
half crossover operator  although differences statistically significant 
results suggest keeping rule structure networks intact crossover
important  otherwise  basic building blocks networks  i e   rules  get split
crossover  studies shown importance keeping intact basic building
blocks crossover  forrest   mitchell        goldberg        

regent random crossover
regent

promoters splice junction rbs
    
    
    
    
    
    

table    test set error two runs regent   a  randomly crossing  nodes 
networks   b  one crossing  rules  network  defined
equation     runs considered     networks used half crossover  half
mutation  results significant     confidence level 
slight difference learning algorithms long run times limited
runs ten fold cross validation 

   discussion future work

towell        showed kbann generalized better many machine learning algorithms promoter splice junction domains  the rbs dataset exist then  
   

ficonnectionist theory refinement

despite success  regent able effectively use available computer cycles significantly improve generalization kbann previous improvement kbann 
topgen algorithm  regent reduces kbann s test set error     rbs domain      splice junction domain      promoter domain  reduces
topgen s test set error     rbs domain      splice junction domain 
    promoter domain  also  regent s ability use available computing time
aided inherently parallel  since train many networks simultaneously 
results show regent s two genetic operators complement other 
crossover operator considers large variety network topologies probabilistically combining rules contained within two  successful  knns  mutation  hand  makes
smaller  directed improvements members population  time adding
diversity population adding new rules population  equal use operators  therefore  allows wide variety topologies considered well allowing
incremental improvements members population 
since regent searches many candidate networks  important
able recognize networks likely generalize best  mind 
first planned extension regent develop test different network evaluation functions  currently use validation set  however  validation sets several drawbacks 
first  keeping aside validation set decreases number training instances available
network  second  performance validation set noisy approximator
true error  mackay        weigend  huberman    rumelhart         finally 
increase number networks searched  regent may start selecting networks
overfit validation set  fact  explains occasional upward trend test set error 
topgen regent  figure   
avoid problem overfitting data  common regression trick cost
function includes  smoothness  term along error term  best function 
then  smoothest function fits data well  neural networks  one
add estimated error smoothness component measure complexity
network  complexity network cannot simply estimated counting
number possible parameters  since tends significant duplication
function weight network  especially early training process  weigend 
       two techniques try take account effective size network
generalized prediction error  moody        bayesian methods  mackay        
quinlan cameron jones        propose adding additional term accuracy
smoothness term takes account length time spent searching  coin
term  oversearching  describe phenomenon extensive searching causes
lower predictive accuracy  claim oversearching orthogonal overfitting 
thus complexity based methods alone cannot prevent oversearching  increase
number networks consider search  may start oversearching 
thus plan investigate adding oversearching penalty term well 
indicated earlier  regent lamarckian passes local optimizations individuals  i e   trained weights network  offspring  viable alternative  called
baldwin effect  ackley   littman        baldwin        belew   mitchell        hinton  
nowlan         local search still change fitness individual  backpropagation learning case   pass changes offspring  this form
   

fiopitz   shavlik

evolution darwinian nature   even though learned explicitly coded
genetic material  individuals best able learn offspring 
thus learning still impacts evolution  fact form evolution sometimes outperform forms lamarckian evolution employ local search strategy  whitley 
gordon    mathias         future work investigate utility baldwin effect
regent  case would cross trained networks  instead cross
initial weight settings backpropagation learning took place 
finally  often times multiple  even con icting  theories domain  future work  then  investigate ways using domain theories seed
initial population  although results section       show including randomly generated networks degrades generalization performance  seeding population multiple
approximately correct theories degrade generalization  assuming networks
initial correctness  thus regent able naturally
combine good parts multiple theories  also  given domain theory  many
different equivalent ways represent theory using set propositional rules 
representation leads different network topology  even though network
starts theory  topologies may conducive neural refinement 

   related work

regent mainly differs previous work an anytime  theory refinement sys 

tem continually searches  non hillclimbing manner  improvements domain
theory  summary  work unique provides connectionist approach
attempts effectively utilize available background knowledge available computer cycles
generate best concept possible  broken rest section four parts 
 a  connectionist theory refinement algorithms   b  purely symbolic theory refinement algorithms   c  algorithms find appropriate domain specific neural network topology 
 d  optimization algorithms wrapped around induction algorithms 

    connectionist theory refinement techniques

begin discussion connectionist theory refinement systems  systems
developed refine many types rule bases  instance  number systems
proposed revising certainty factor rule bases  fu        lacher et al        
mahoney   mooney         finite state automata  maclin   shavlik        omlin   giles 
       push down automata  das  giles    sun         fuzzy logic rules  berenji       
masuoka  watanabe  kawamura  owada    asakawa         mathematical equations
 roscheisen  hofmann    tresp        scott et al          systems work
kbann first translating domain knowledge neural network  modifying
weights resulting network  attempts  which describe next 
made dynamically adjust resulting network s topology training  as regent
does  
topgen regent  fletcher obradovic        present approach
adds nodes kbann network  system constructs single layer nodes  fully
connected input output nodes   off side  kbann network 
generate new hidden nodes using variant baum lang s        constructive
   

ficonnectionist theory refinement

algorithm  baum lang s algorithm first divides feature space hyperplanes 
find hyperplane randomly selecting two points different classes 
localizing suitable split points  baum lang repeat process
generate fixed number hyperplanes  fletcher obradovic map
baum lang s hyperplanes one new hidden node  thus defining weights
input layer hidden node  fletcher obradovic s algorithm change
weights kbann portion network  modifications initial rule base
solely left constructed hidden nodes  thus  system take advantage
kbann s strength removing unwanted antecedents rules original rule
base  fact  topgen compared favorably similar technique added nodes
side kbann  opitz   shavlik        regent outperformed topgen
article s experiments 
rapture  mahoney   mooney        designed domain theories containing probabilistic rules  connectionist theory refinement systems  rapture first translates
domain theory neural network  refines weights network
modified backpropagation algorithm  regent  rapture able dynamically
refine topology network  using upstart algorithm  frean 
      add new nodes network  aside designed probabilistic rules 
rapture differs regent adds nodes intention completely
learning training set  generalizing well  thus  rapture hillclimbs
training set learned  regent continually searches topology space looking network
minimizes scoring function s error  also  rapture initially creates links
specified domain theory  explicitly adds links id  s  quinlan 
      information gain metric  regent  hand  fully connect consecutive layers
networks  allowing rule possibility adding antecedents training 
daid algorithm  towell   shavlik        extension kbann uses
domain theory help train kbann network  since kbann effective dropping antecedents adding them  daid tries find potentially useful inputs features
mentioned domain theory  backing up errors lowest level
domain theory  computing correlations features  daid increases
weight links potentially useful input features based correlations 
daid mainly differs regent refine topology kbann network  thus  daid addresses kbann s limitation effectively adding antecedents 
still unable introduce new rules constructively induce new antecedents  daid
therefore suffer impoverished domain theories  notice since daid improvement training knns  regent use daid train network considers
search  however  done so  
opitz shavlik        used variant regent learning algorithm
generating neural network  ensemble   neural network ensemble successful
technique outputs set separately trained neural networks combined
form one unified prediction  drucker  cortes  jackel  lecun    vapnik        hansen
  salamon        perrone         since regent considers many networks  select
subset final population networks ensemble minimal extra cost  previous
work  though  shown ideal ensemble one networks accurate
make errors different parts input space  hansen   salamon       
   

fiopitz   shavlik

krogh   vedelsby         result  opitz shavlik        changed scoring
function regent  fit  network one accurate
disagreed members population much possible  addition 
algorithm  addemup  actively tries generate good candidates emphasizing
current population s erroneous examples backpropagation training  result
alterations  addemup able create enough diversity among population
networks able effectively exploit knowledge domain theory  opitz
shavlik        show addemup able generate significantly better ensemble
using domain theory either running addemup without benefit theory
simply combining regent s final population networks  actively searching highly
diverse population  however  aid searching single best network  fact 
single best network produced addemup significantly worse regent s single
best network three domains 

    purely symbolic theory refinement techniques
additional work related regent includes purely symbolic theory refinement systems
modify domain theory directly initial form  systems focl  pazzani
  kibler        forte  richards   mooney        first order  theory refinement
systems revise predicate logic theories  one drawback systems
currently generalize well connectionist approaches many real world problems 
dna promoter task  cohen        
several genetic based  first order logic  multimodal concept learners
 greene   smith        janikow         giordana saitta        showed integrate one system  regal  giordana  saitta    zini        neri   saitta        
deductive engine ml smart  bergadano  giordana    ponsero        help
refine incomplete inconsistent domain theory  version works first using automated theorem prover recognize unresolved literals proof  uses ga based
regal induce corrections literals  regent  hand  use genetic
algorithms  along neural learning  refine whole domain theory time 
dogma  hekanaho        recently proposed ga based learner use background knowledge learn description language regal  current restrictions 
however  force representation language domain theory propositional rules 
dogma converts   at  set background rules  i e   handle intermediate
conclusions  individual bitstrings used building blocks higher level
concept  dogma focus theory refinement  rather builds completely new
theory using substructures background knowledge  term approach
theory suggested theory guided  hekanaho        
several systems  including ours  proposed refining propositional rule bases 
early approaches could handle improvements overly specific theories  danyluk 
      specializations overly general theories  flann   dietterich         later systems
rtls  ginsberg         either  ourston   mooney         ptr  koppel  feldman 
  segre         tgci  donoho   rendell        later able handle types
refinements  discuss either system representative propositional
systems 
   

ficonnectionist theory refinement

either four theory revision operators   a  removing antecedents rule   b 
adding antecedents rule   c  removing rules rule base   d  inventing new
rules  either uses operators make revisions domain theory correctly
classify previously misclassified training examples without undefining
correctly classified examples  either uses inductive learning algorithms invent new
rules  currently uses id   quinlan        induction component 
even though regent s mutation operator add nodes manner analogous
symbolic system adds antecedents rules  underlying learning algorithm  connectionist   towell        showed kbann outperformed either promoter
task  regent outperformed kbann article  kbann s power domain
largely attributed ability make  fine grain  refinements domain theory
 towell         either s diculty domain  baffes mooney       
presented extension called neither mofn able learn  of n rules  
rules true n antecedents true  improvement generated
concept closely matches kbann s generalization performance 
want minimize changes theory  want expense accuracy  however  donoho rendell        demonstrate existing
theory refinement systems  either  suffer able make small 
local changes domain theory  thus  accurate theory significantly far
structure initial theory  systems forced either become trapped
local maximum similar initial theory  forced drop entire rules replace
new rules inductively created purely scratch  regent
suffer translates theory less restricting representation
neural networks  donoho   rendell         also  regent able reconfigure
structure domain genetic algorithms 
many authors reported results using varying subsets splice junction domain
 e g   donoho rendell       mahoney       neri saitta       towell shavlik        authors used different training set sizes  nevertheless worthwhile
qualitatively discuss conclusions here  towell shavlik        compared
kbann numerous machine learning algorithms learning algorithm
given training set      examples  kbann s generalization ability compared favorably
algorithms splice domain regent  turn  compared favorably
kbann article  donoho rendell        showed purely symbolic approach
converged performance kbann around     examples  mahoney        showed 
using training set sizes     examples  rapture algorithm generalized
better kbann domain  results look similar regent  finally 
neri saitta        showed generalization ability ga based regal compares favorably purely symbolic  non ga based techniques  used slightly
different training set sizes article  regent compares well results
reported paper 

    finding appropriate network topologies
third area related work covers techniques attempt find good domaindependent topology dynamically refining network s topology training  many
   

fiopitz   shavlik

studies shown generalization ability neural network depends topology network  baum   haussler        tishby  levin    solla         trying
find appropriate topology  one approach construct modify topology
incremental fashion  network shrinking algorithms start many parameters 
remove nodes weights training  hassibi   stork        le cun  denker   
solla        mozer   smolensky         network growing algorithms  hand 
start parameters  add nodes weights training  blanziere
  katenkamp        fahlman   lebiere        frean         obvious difference regent algorithms regent uses domain knowledge
symbolic rule refinement techniques help determine network s topology  also 
algorithms restructure network based solely training set error  regent
minimized validation set error 
instead incrementally finding appropriate topology  one mount  richer 
search hillclimbing space topologies  one common approach
combine genetic algorithms neural networks  as regent does   genetic algorithms
applied neural networks two different ways   a  optimize connection
weights fixed topology   b  optimize topology network  techniques
solely use genetic algorithms optimize weights  montana   davis        whitley
  hanson        performed competitively gradient based training algorithms 
however  one problem genetic algorithms ineciency fine tuned local search 
thus scalability methods question  yao         kitano      b  presents
method combines genetic algorithms backpropagation  using
genetic algorithm determine starting weights network 
refined backpropagation  regent differs kitano s method use domain
theory help determine network s starting weights genetically search  instead 
appropriate network topologies 
methods use genetic algorithms optimize network topology similar
regent use backpropagation train network s weights 
methods  many directly encode link network  miller  todd    hegde 
      oliker  furst    maimon        schiffmann  joost    werner         methods
relatively straightforward implement  good fine tuning small networks
 miller et al          however  scale well since require large matrices
represent links large networks  yao         techniques  dodd       
harp  samad    guha        kitano      a  encode important features
network  number hidden layers  number hidden nodes
layer  etc  indirect encoding schemes evolve different sets parameters along
network s topology shown good scalability  yao        
techniques  koza   rice        oliker et al         evolve architecture
connection weights time  however  combination two levels evolution
greatly increases search space 
regent mainly differs genetic algorithm based training methods designed knowledge based neural networks  thus regent uses domain specific knowledge
symbolic rule refinement techniques aid determining network s topology
initial weight setting  regent differs explicitly encode networks 
rather  spirit lamarkian evolution  passes trained network weights off   

ficonnectionist theory refinement

spring  final difference algorithms restructure network
based solely training set error  regent minimizes validation set error 

    wrapping optimization around learning

end related work discussion brief overview methods combine global
local optimization strategies  local search algorithms iteratively improve estimate
minimum searching local neighborhood current solution  local minima
guaranteed global minima   many inductive learning methods often
equated local optimization techniques  rumelhart et al          global optimization
methods  such gas   hand  perform sophisticated search across
multiple local minima good finding regions search space nearoptimal solutions found  however  usually good refining solution
 once close near optimal solution  local optimization strategies  hart        
recent research shown desirable emply global local search
strategy  hart        
hybrid gas  such regent  combine local search traditional ga 
focus hybrid ga algorithms section  two tiered search strategy
employed researchers well  kohavi   john        provost   buchanan       
schaffer         gas combined many local search methods  bala  huang 
vafaie  dejong    wechsler        belew        hinton   nowlan        turney        
neural networks common choice local search strategy hybrid ga
systems discussed ga neural network hybrids section      two
common forms hybrid gas  lamarckian based evolution darwinian based evolution  the baldwin effect   lamarckian evolution encodes local improvements directly
genetic material  darwinian evolution leaves genetic material unchanged
learning  discussed section    authors use lamarckian local search techniques many shown numerous cases lamarckian evolution outperforms
non lamarckian local search  belew  mcinerney    schraudolph        hart        judson 
colvin  meza  huffa    gutierrez        

   conclusion

ideal inductive learning algorithm able exploit available resources
extensive computing power domain specific knowledge improve ability generalize  kbann  towell   shavlik        shown effective translating
domain theory neural network  however  kbann suffers alter
topology  topgen  opitz   shavlik        improved kbann algorithm using
available computer power search effective places add nodes kbann network 
however  show empirically topgen suffers restricting search expansions
kbann network  unable improve performance searching beyond
topologies  therefore topgen unable exploit available computing power
increase correctness induced concept 
present new algorithm  regent  uses specialized genetic algorithm
broaden types topologies considered topgen s search  experiments indicate
regent able significantly increase generalization topgen  hence  new
   

fiopitz   shavlik

algorithm successful overcoming topgen s limitation searching small portion
space possible network topologies  so  regent able generate
good solution quickly  using kbann  able continually improve solution
searches concept space  therefore  regent takes step toward true anytime theory
refinement system able make effective use problem specific knowledge
available computing cycles 

acknowledgements
work supported oce naval research grant n                national
science foundation grant iri           thanks richard maclin  richard sutton 
three anonymous reviewers helpful comments  extended version
paper published machine learning  proceedings eleventh international conference 
pp           new brunswick  nj  morgan kaufmann  david opitz completed portion
work graduate student university wisconsin professor
university minnesota  duluth 

references
ackley  d          connectionist machine genetic hillclimbing  kluwer  norwell 
ma 
ackley  d     littman  m          interactions learning evolution  langton 
c   taylor  c   farmer  c     rasmussen  s   eds    artificial life ii  pp          
redwood city  ca  addison wesley 
ackley  d     littman  m          case lamarckian evolution  langton  c   ed   
artificial life iii  pp        redwood city  ca  addison wesley 
baffes  p     mooney  r          symbolic revision theories m of n rules 
proceedings thirteenth international joint conference artificial intelligence 
pp             chambery  france  morgan kaufmann 
bala  j   huang  j   vafaie  h   dejong  k     wechsler  h          hybrid learning using
genetic algorithms decision trees pattern classification  proceedings
fourteenth international joint conference artificial intelligence  pp          
montreal  canada  morgan kaufmann 
baldwin  j          physical social heredity  american naturalist              
baum  e     haussler  d          size net gives valid generalization  neural computation             
baum  e     lang  k          constructing hidden units using examples queries 
lippmann  r   moody  j     touretzky  d   eds    advances neural information
processing systems  vol     pp           san mateo  ca  morgan kaufmann 
   

ficonnectionist theory refinement

belew  r          evolution  learning culture  computational metaphors adaptive
search  complex systems           
belew  r   mcinerney  j     schraudolph  n          evolving networks  using genetic
algorithm connectionist learning  langton  c   taylor  c   farmer  c    
rasmussen  s   eds    artificial life ii  pp           redwood city  ca  addisonwesley 
belew  r     mitchell  m          adaptive individuals evolving populations  models
algorithms  addison wesley  massachusetts 
berenji  h          refinement approximate reasoning based controllers reinforcement
learning  proceedings eighth international machine learning workshop  pp 
         evanston  il  morgan kaufmann 
bergadano  f   giordana  a     ponsero  s          deduction top down inductive
learning  proceedings sixth international workshop machine learning 
pp         ithaca  ny  morgan kaufmann 
blanziere  e     katenkamp  p          learning radial basis function networks on line 
proceedings thirteenth international conference machine learning  pp 
       bari  italy  morgan kaufmann 
clocksin  w     mellish  c          programming prolog  springer verlag  new york 
cohen  w          compiling prior knowledge explicit bias  proceedings
ninth international conference machine learning  pp           aberdeen 
scotland  morgan kaufmann 
danyluk  a          finding new rules incomplete theories  explicit biases induction
contextual information  proceedings sixth international workshop
machine learning  pp         ithaca  ny  morgan kaufmann 
das  a   giles  c     sun  g          using prior knowledge nnpda learn
context free languages  hanson  s   cowan  j     giles  c   eds    advances
neural information processing systems  vol     pp         san mateo  ca  morgan
kaufmann 
dean  t     boddy  m          analysis time dependent planning  proceedings
seventh national conference artificial intelligence  pp         st  paul  mn 
morgan kaufmann 
dejong  k          analysis behavior class genetic adaptive systems 
ph d  thesis  university michigan  ann arbor  mi 
dodd  n          optimization network structure using genetic techniques  proceedings
ieee international joint conference neural networks  vol  iii  pp          
paris  ieee press 
   

fiopitz   shavlik

donoho  s     rendell  l          rerepresenting restructuring domain theories 
constructive induction approach  journal artificial intelligence research         
    
drucker  h   cortes  c   jackel  l   lecun  y     vapnik  v          boosting
machine learning algorithms  proceedings eleventh international conference
machine learning  pp         new brunswick  nj  morgan kaufmann 
fahlman  s     lebiere  c          cascade correlation learning architecture  touretzky  d   ed    advances neural information processing systems  vol     pp      
     san mateo  ca  morgan kaufmann 
farmer  j     belin  a          artificial life  coming evolution  langton  c   taylor 
c   farmer  j  d     rasmussen  s   eds    artificial life ii  pp           redwood
city  ca  addison wesley 
flann  n     dietterich  t          study explanation based methods inductive
learning  machine learning             
fletcher  j     obradovic  z          combining prior symbolic knowledge constructive
neural network learning  connection science             
forrest  s     mitchell  m          makes problem hard genetic algorithm 
anomalous results explanation  machine learning              
frean  m          upstart algorithm  method constructing training feedforward neural networks  neural computation             
fu  l          integration neural heuristics knowledge based inference  connection
science             
ginsberg  a          theory reduction  theory revision  retranslation  proceedings
eighth national conference artificial intelligence  pp           boston  ma 
aaai mit press 
giordana  a     saitta  l          regal  integrated system relations using genetic
algorithms  proceedings second international workshop multistrategy
learning  pp           harpers ferry  wv 
giordana  a   saitta  l     zini  f          learning disjunctive concepts means genetic algorithms  proceedings eleventh international conference machine
learning  pp          new brunswick  nj  morgan kaufmann 
goldberg  d          genetic algorithms search  optimization  machine learning 
addison wesley  reading  ma 
greene  d     smith  s          competition based induction decision models
examples  machine learning              
   

ficonnectionist theory refinement

grefenstette  j     ramsey  c          approach anytime learning  proceedings
ninth international conference machine learning  pp           aberdeen 
scotland  morgan kaufmann 
hansen  l     salamon  p          neural network ensembles  ieee transactions
pattern analysis machine intelligence               
harp  s   samad  t     guha  a          designing application specific neural networks
using genetic algorithm  touretzky  d   ed    advances neural information
processing systems  vol     pp           san mateo  ca  morgan kaufmann 
hart  w          adaptive global optimization local search  ph d  thesis  university
california  san diego 
hassibi  b     stork  d          second order derivatives network pruning  optimal brain
surgeon  hanson  s   cowan  j     giles  c   eds    advances neural information
processing systems  vol     pp           san mateo  ca  morgan kaufmann 
hekanaho  j          background knowledge ga based concept learning  proceedings
thirteenth international conference machine learning  pp           bari 
italy  morgan kaufmann 
hinton  g     nowlan  s          learning guide evolution  complex systems    
        
holder  l          maintaining utility learned knowledge using model based control  ph d  thesis  computer science department  university illinois urbanachampaign 
holland  j          adaptation natural artificial systems  university michigan
press  ann arbor  mi 
janikow  c          knowledge intensive ga supervised learning  machine learning 
            
judson  r   colvin  m   meza  j   huffa  a     gutierrez  d          intelligent configuration search techniques outperform random search large molecules  international
journal quantum chemistry          
kibler  d     langley  p          machine learning experimental science  proceedings third european working session learning  pp        edinburgh 
uk 
kitano  h       a   designing neural networks using genetic algorithms graph generation system  complex systems             
kitano  h       b   empirical studies speed convergence neural network training using genetic algorithms  proceedings eighth national conference
artificial intelligence  pp           boston  ma  aaai mit press 
   

fiopitz   shavlik

kohavi  r     john  g          wrappers feature subset selection  artificial intelligence 
koppel  m   feldman  r     segre  a          bias driven revision logical domain theories 
journal artificial intelligence research             
koza  j          genetic programming  mit press  cambridge  ma 
koza  j     rice  j          genetic generation weights architectures
neural network  international joint conference neural networks  vol     pp 
         seattle  wa  ieee press 
krogh  a     vedelsby  j          neural network ensembles  cross validation  active
learning  tesauro  g   touretzky  d     leen  t   eds    advances neural
information processing systems  vol     pp           cambridge  ma  mit press 
lacher  r   hruska  s     kuncicky  d          back propagation learning expert networks  ieee transactions neural networks           
le cun  y   denker  j     solla  s          optimal brain damage  touretzky  d   ed   
advances neural information processing systems  vol     pp           san mateo 
ca  morgan kaufmann 
litzkow  m   livny  m     mutka  m          condor   hunter idle workstations 
proceedings eighth international conference distributed computing systems 
pp           san jose  ca  computer society press 
mackay  d          practical bayesian framework backpropagation networks  neural
computation             
maclin  r     shavlik  j          using knowledge based neural networks improve algorithms  refining chou fasman algorithm protein folding  machine learning 
            
mahoney  j          combining symbolic connectionist learning methods refine
certainty factor rule bases  ph d  thesis  university texas  austin  tx 
mahoney  j     mooney  r          combining connectionist symbolic learning refine
certainty factor rule bases  connection science             
mahoney  j     mooney  r          comparing methods refining certainty factor rulebases  proceedings eleventh international conference machine learning 
pp           new brunswick  nj  morgan kaufmann 
masuoka  r   watanabe  n   kawamura  a   owada  y     asakawa  k          neurofuzzy
system   fuzzy inference using structured neural network  proceedings
international conference fuzzy logic   neural networks  pp           iizuka 
japan 
michalski  r          theory methodology inductive learning  artificial intelligence 
            
   

ficonnectionist theory refinement

miller  g   todd  p     hegde  s          designing neural networks using genetic algorithms  proceedings third international conference genetic algorithms 
pp           arlington  va  morgan kaufmann 
mitchell  m          introduction genetic algorithms  mit press  cambridge  ma 
mitchell  t          generalization search  artificial intelligence              
montana  d     davis  l          training feedforward networks using genetic algorithms 
proceedings eleventh international joint conference artificial intelligence 
pp           detroit  mi  morgan kaufmann 
moody  j          effective number parameters  analysis generalization
regularization nonlinear learning systems  moody  j   hanson  s     lippmann 
r   eds    advances neural information processing systems  vol     pp          
san mateo  ca  morgan kaufmann 
mozer  m  c     smolensky  p          using relevance reduce network size automatically 
connection science          
neri  f     saitta  l          exploring power genetic search learning symbolic
classifiers  ieee transactions pattern analisys machine intelligence 
oliker  s   furst  m     maimon  o          distributed genetic algorithm neural
network design training  complex systems             
omlin  c     giles  c          training second order recurrent neural networks using hints 
proceedings ninth international conference machine learning  pp      
     aberdeen  scotland  morgan kaufmann 
opitz  d     shavlik  j          heuristically expanding knowledge based neural networks 
proceedings thirteenth international joint conference artificial intelligence  pp             chambery  france  morgan kaufmann 
opitz  d     shavlik  j          dynamically adding symbolically meaningful nodes
knowledge based neural networks  knowledge based systems             
opitz  d     shavlik  j          actively searching effective neural network ensemble 
connection science             
ourston  d     mooney  r          theory refinement combining analytical empirical
methods  artificial intelligence              
pazzani  m     kibler  d          utility knowledge inductive learning  machine
learning           
perrone  m          improving regression estimation  averaging methods variance
reduction extension general convex measure optimization  ph d  thesis 
brown university  providence  ri 
   

fiopitz   shavlik

provost  f     buchanan  b          inductive policy  pragmatics bias selection 
machine learning            
quinlan  j          induction decision trees  machine learning            
quinlan  j     cameron jones  r          lookahead pathology decision tree induction  proceedings fourteenth international joint conference artificial
intelligence  pp             montreal  canada  morgan kaufmann 
richards  b     mooney  r          automated refinement first order horn clause domain
theories  machine learning             
roscheisen  m   hofmann  r     tresp  v          neural control rolling mills  incorporating domain theories overcome data deficiency  moody  j   hanson  s    
lippmann  r   eds    advances neural information processing systems  vol     pp 
         san mateo  ca  morgan kaufmann 
rumelhart  d   hinton  g     williams  r          learning internal representations
error propagation  rumelhart  d     mcclelland  j   eds    parallel distributed
processing  explorations microstructure cognition  volume    foundations 
pp           mit press  cambridge  ma 
schaffer  c          selecting classification method cross validation  machine learning 
            
schiffmann  w   joost  m     werner  r          synthesis performance analysis
multilayer neural network architectures  tech  rep      university koblenz  institute
physics 
scott  g   shavlik  j     ray  w          refining pid controllers using neural networks 
neural computation             
tishby  n   levin  e     solla  s          consistent inference probabilities layered
networks  predictions generalization  international joint conference neural
networks  pp           washington  d c  ieee press 
towell  g          symbolic knowledge neural networks  insertion  refinement 
extraction  ph d  thesis  computer sciences department  university wisconsin 
madison  wi 
towell  g     shavlik  j          using symbolic learning improve knowledge based neural
networks  proceedings tenth national conference artificial intelligence 
pp           san jose  ca  aaai mit press 
towell  g     shavlik  j          knowledge based artificial neural networks  artificial
intelligence              
turney  p          cost sensitive classification  empirical evaluation hybrid genetic
decision tree induction algorithm  journal artificial intelligence research         
    
   

ficonnectionist theory refinement

waterman  d          guide expert systems  addison wesley  reading  ma 
watrous  r   towell  g     glassman  m          synthesize  optimize  analyze  repeat
 soar   application neural network tools ecg patient monitoring  proceedings symposium nonlinear theory applications  pp          
honolulu  hawaii 
watson  j  d   hopkins  n  h   roberts  j  w   argetsinger steitz  j     weiner  a  m 
        molecular biology gene  fourth edition   benjamin cummings  menlo
park  ca 
weigend  a          overfitting effective number hidden units  proceedings      connectionist models summer school  pp           boulder  co 
lawrence erlbaum associates 
weigend  a   huberman  b     rumelhart  d          predicting future  connectionist
approach  international journal neural systems  i          
whitley  d   gordon  s     mathias  k          lamarckian evolution  baldwin effect
function optimization  davidor  y   schwefel  h     manner  r   eds    parallel
problem solving nature   ppsn iii  pp        springer verlag 
whitley  d     hanson  t          optimizing neural networks using faster  accurate genetic search  proceedings third international conference genetic
algorithms  pp           arlington  va  morgan kaufmann 
yao  x          evolutionary artificial neural networks  international journal neural
systems             

   


