journal of artificial intelligence research               

submitted       published     

screen  learning a flat syntactic and semantic spoken
language analysis using artificial neural networks
stefan wermter
volker weber

wermter informatik uni hamburg de
weber informatik uni hamburg de

department of computer science
university of hamburg
      hamburg  germany

abstract
previous approaches of analyzing spontaneously spoken language often have been based
on encoding syntactic and semantic knowledge manually and symbolically  while there
has been some progress using statistical or connectionist language models  many current
spoken language systems still use a relatively brittle  hand coded symbolic grammar or
symbolic semantic component 
in contrast  we describe a so called screening approach for learning robust processing
of spontaneously spoken language  a screening approach is a at analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic 
semantic and dialog levels  rather than using a deeply structured symbolic analysis  we
use a at connectionist analysis  this screening approach aims at supporting speech and
language processing by using     data driven learning and     robustness of connectionist
networks  in order to test this approach  we have developed the screen system which is
based on this new robust  learned and at analysis 
in this paper  we focus on a detailed description of screen s architecture  the at
syntactic and semantic analysis  the interaction with a speech recognizer  and a detailed
evaluation analysis of the robustness under the inuence of noisy or incomplete input 
the main result of this paper is that at representations allow more robust processing of
spontaneous spoken language than deeply structured representations  in particular  we
show how the fault tolerance and learning capability of connectionist networks can support
a at analysis for providing more robust spoken language processing within an overall
hybrid symbolic connectionist framework 

   introduction
recently the fields of speech processing as well as language processing have both seen
efforts to examine the possibility of integrating speech and language processing  von hahn
  pyka        jurafsky et al       b  waibel et al         ward        menzel        geutner
et al         wermter et al          while new and large speech and language corpora are
being developed rapidly  new techniques have to be examined which particularly support
properties of both speech and language processing  although there have been quite a few
approaches to spoken language analysis  mellish        young et al         hauenstein  
weber        ward         they have not emphasized learning a syntactic and semantic
analysis of spoken language using a hybrid connectionist  architecture which is the topic
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiwermter   weber

of this paper and our goal in screen    however  learning is important for the reduction
of knowledge acquisition  for automatic system adaptation  and for increasing the system
portability for new domains  different from most previous approaches  in this paper we
demonstrate that hybrid connectionist learning techniques can be used for providing a
robust at analysis of faulty spoken language 
processing spoken language is very different from processing written language  and successful techniques for text processing may not be useful for spoken language processing 
processing spoken language is less constrained  contains more errors and less strict regularities than written language  errors occur on all levels of spoken language processing 
for instance  acoustic errors  repetitions  false starts and repairs are prominent in spontaneously spoken language  furthermore  incorrectly analyzed words  unforeseen grammatical and semantic constructions occur very often in spoken language  in order to deal with
these important problems for  real world  language analysis  robust processing is necessary 
therefore we cannot expect that existing techniques like context free tree representations
which have been proven to work for written language can simply be transferred to spoken
language 
for instance  consider that a speech recognizer has produced the correct german sentence hypothesis  ich meine naturlich marz   english translation   i mean of course
march    standard techniques from text processing   like chart parsers and context free
grammars   may be able to produce deeply structured tree representations for many correct
sentences as shown in figure   
sentence
verb phrase

noun group

pronoun

verb group

verb

noun group

adverb

ich  i  meine  mean  natrlich  of course 

noun

mrz  march 

figure    tree representation for a correctly recognized sentence
however  currently speech recognizers are still far from perfect and produce many word
errors so that it is not possible to rely on a perfect sentence hypothesis  therefore  incorrect
   sometimes connectionist networks are also called artificial neural networks  from now on we will use
only the term  connectionist networks   and the term  hybrid connectionist architecture  to refer to
an architecture which emphasizes the use of connectionist networks but does not rule out the use of
symbolic representations on higher levels where they might be needed 
   symbolic connectionist robust enterprise for natural language

  

fiscreen  flat syntactic and semantic spoken language analysis

variations like  ich meine ich marz    i mean i march     ich hatte ich marz    i had i
march   and  ich ich meine marz    i i mean march   have to be analyzed  however 
in context free grammars a single syntactic or semantic category error may prevent that a
complete tree can be built  and standard top down chart parsers may fail completely  however  suboptimal sentence hypotheses have to be analyzed since sometimes such sentence
hypotheses are the best possible output produced by a speech recognizer  furthermore 
a lot of the content can be extracted even from partially incorrect sentence hypotheses 
for instance  from  i had i march  it is plausible that an agent  i  said something about
the time  march   therefore  a robust analysis should be able to analyze such sentence
hypotheses and ideally should not break for any input 

    screening approach  flat representations support robustness
for such examples of incorrect variations of sentence hypotheses  an in depth structured
syntactic and semantic representation is not advantageous since more arbitrary word order and spontaneous errors make it often impossible to determine a desired deep highly
structured representation  furthermore  a deep highly structured representation may have
many more restrictions than appropriate for spontaneously spoken language  however  and
maybe even more important  for certain tasks it is not necessary to perform an in depth
analysis  while  for instance  inferences about story understanding require an in depth
understanding  dyer         tasks like information extraction from spoken language do not
need much of an in depth analysis  for instance  if the output of our parser were to be used
for translating a speech recognizer sentence hypothesis  eh ich meine eh ich marz    eh i
mean eh i march    it may be sucient to extract that an agent   i   uttered   mean   a
time   march    in contrast to a deeply structured representation  our screening approach
aims at reaching a at but robust representation of spoken language  a screening approach
is a shallow at analysis based on category sequences  called at representations  at various
syntactic and semantic levels 
a at representation structures an utterance u with words w  to wn according to the
syntactic and semantic properties of the words in their contexts  e g   according to a sequence
of basic or abstract syntactic categories  for instance  the phrase  a meeting in london 
can be described as a at representation  determiner noun preposition noun  at a basic
syntactic level and as a at representation  noun group noun group prepositional group
prepositional group  at an abstract syntactic level  similar at representations can be used
for semantic categories  dialog act categories  etc 
kase
 rubbish 
noun
no
noun group
negation

ich
 i 
pronoun
animate
noun group
agent

meine
 mean 
verb
utter
verb group
action

naturlich
 of course 
adverb
nil
special group
miscellaneous

marz
 march 
noun
time
noun group
at time

figure    utterance with its at representation
  

fiwermter   weber

figure   gives an example for a at representation for a correct sentence hypothesis
 kase ich meine naturlich marz    rubbish i mean of course march    the first line shows
the sentence  the second its literal translation  the third line describes the basic syntactic
category of each word  the fourth line shows the basic semantic category  the last two lines
illustrate the syntactic and semantic categories at the phrase level 
kase
 rubbish 
noun
no
noun group
negation

ich
 i 
pronoun
animate
noun group
agent

hatte
 had 
verb
have
verb group
action

ich
 i 
pronoun
animate
noun group
agent

marz
 march 
noun
time
noun group
at time

figure    utterance with its at representation
figure   gives an example for a at representation for the incorrect sentence hypothesis
 kase ich hatte ich marz    rubbish i had i march    a parser for spoken language
should be able to process such sentence hypotheses as far as possible  and we use at
representations to support the necessary robustness  in our example  the analysis should
at least provide that an animate agent and noun group   i   made some statement about a
specific time and noun group   march    flat representations have the potential to support
robustness better since they have only a minimal sequential structure  and even if an error
occurs the whole representation can still be built  in contrast  in standard tree structured
representations many more decisions have to be made to construct a deeply structured
representation  and therefore there are more possibilities to make incorrect decisions  in
particular with noisy spontaneously spoken language  so we chose at representations
rather than highly structured representations because of the desired robustness against
mistakes in speech language systems 

    flat representations learned in a hybrid connectionist framework

robust spoken language analysis using at representations could be pursued in different
approaches  therefore we want to motivate why we use a hybrid connectionist approach 
which uses connectionist networks as far as possible but does not rule out the use of symbolic
knowledge  so why do we use connectionist networks 
most important  due to their distributed fault tolerance  connectionist networks support
robustness  rumelhart et al         sun        but connectionist networks also have a number of other properties which are relevant for our spoken language analysis  for instance 
connectionist networks are well known for their learning and generalization capabilities 
learning capabilities allow to induce regularities directly from examples  if the training
examples are representative for the task  the noisy robust processing should be supported
by inductive connectionist learning 
furthermore  a hybrid connectionist architecture has the property that different knowledge sources can take advantage of the learning and generalization capabilities of connectionist networks  on the other hand  other knowledge   task or control knowledge   for
  

fiscreen  flat syntactic and semantic spoken language analysis

which rules are known can be represented directly in symbolic representations  since humans apparently do symbolic inferencing based on real neural networks  abstract models
as symbolic representations and connectionist networks have the additional potential to
shed some light on human language processing capabilities  in this respect  our approach
also differs from other candidates for robust processing  like statistical taggers or statistical
n grams  these statistical techniques can be used for robust analysis  charniak        but
statistical techniques like n grams do not relate to the human cognitive language capabilities while simple recurrent connectionist networks have more relationships to the human
cognitive language capabilities  elman        
screen is a new hybrid connectionist system developed for the examination of at
syntactic and semantic analysis of spoken language  in earlier work we have explored a
at scanning understanding for written texts  wermter        wermter   lochel       
wermter   peters         based on this experience we started a completely new project
screen to explore a learned fault tolerant at analysis for spontaneously spoken language
processing  after preliminary successful case studies with transcripts we have developed the
screen system for using knowledge generated from a speech recognizer  in previous work 
we gave a brief summary of screen with a specific focus on segmentation parsing and dialog
act processing  wermter   weber      a   in this paper  we focus on a detailed description
of screen s architecture  the at syntactic and semantic analysis  the interaction with a
speech recognizer  and a detailed evaluation analysis of the robustness under the inuence
of noisy or incomplete input 

    organization and claim of the paper
the paper is structured as follows  in section   we provide a more detailed description of
examples of noise in spoken language  noise can be introduced by the human speaker but
also by the speech recognizer  noise in spoken language analysis motivates the at representations whose categories are described in section    all basic and abstract categories at
the syntactic and semantic level are explained in this section  in section   we motivate and
explain the design of the screen architecture  after a brief functional overview  we show
the overall architecture and explain details of individual modules up to the connectionist
network level  in order to demonstrate the behavior of this at analysis of spoken language
we provide various detailed examples in section    using several representative sentences
we walk the reader through a detailed step by step analysis  after the behavior of the system has been explained  we provide the overall analysis of the screen system in section   
we evaluate the system s individual networks  compare the performance of simple recurrent networks with statistical n gram techniques  and show that simple recurrent networks
performed better than     grams for syntactic and semantic prediction  furthermore we
provide an overall system evaluation  examine the overall performance under the inuence
of additional noise  and supply results from a transfer to a different second domain  finally
we compare our approach to other approaches and conclude that at representations based
on connectionist networks provide a robust learned spoken language analysis 
we want to point out that this paper does not make an argument against deeply structured symbolic representations for language processing in general  usually  if a deeply
structured representation can be built  of course due to the additional knowledge it con  

fiwermter   weber

tains  its potential for more powerful relationships and interpretations will be greater than
that of a at representation  for instance  in depth analysis is required for tasks like making
detailed planning inferences while reading text stories  however  our screening approach is
motivated based on noisy spoken language analysis  for noisy spoken language analysis 
at representations support robustness  and connectionist networks are effective for providing such robustness due to their learned fault tolerance  this is a main contribution
of our paper  and we demonstrate this by building and evaluating a computational hybrid
connectionist architecture screen based on at  robust  and learned processing 

   processing spoken language

our goal is to learn to process spontaneously spoken language at a syntactic and semantic
level in a fault tolerant manner  in this section we will give motivating examples of spoken
language 

     noise  in spoken language

our domain in this paper is the arrangement of meetings between business partners  and
we currently use     spoken dialog turns with     utterances from this domain  one
turn consists of one or more subsequent utterances of the same speaker  for these    
utterances  thousands of utterance hypotheses can be generated and have to be processed
based on the underlying speech recognizer  german utterance examples from this domain
are shown below together with their literal english translation  it is important to note that
the english translations are word for word translations 
   kase ich meine naturlich marz
 rubbish i mean of course march 
   der vierzehnte ist ein mittwoch richtig
 the fourteenth is a wednesday right 
   a hm am sechsten april bin ich leider auer hause
 eh on sixth april am i unfortunately out of home 
   also ich dachte noch in der nachsten woche auf jeden fall noch im april
 so i thought still in the next week in any case still in april 
   gut prima vielen dank dann ist das ja kein problem
 good great many thanks then is this yeah no problem 
   oh das ist schlecht da habe ich um vierzehn uhr dreiig einen termin beim zahnarzt
 oh that is bad there have i at fourteen o clock thirty a date at dentist 
   ja genau allerdings habe ich da von neun bis vier uhr schon einen arzttermin
 yes exactly however have i there from nine to four o clock already a doctorappointment 
as we can see  spoken language contains many performance phenomena  among them
exclamations   rubbish   see example     interjections   eh    so    oh   see examples   
  

fiscreen  flat syntactic and semantic spoken language analysis

  and     new starts   there have i       see example     furthermore  the syntactic and
semantic constraints in spoken language are less strict than in written text  for instance  the
word order in spontaneously spoken language is often very different from written language 
therefore  spoken language is  noisier  than written language even for these transcribed
sentences  and well known parsing strategies from text processing   which can rely more on
wellformedness criteria   are not directly applicable for analyzing spoken language 

     noise  from a speech recognizer

if we want to analyze spoken language in a computational model  there is not only the
 noise  introduced by humans while speaking but also the  noise  introduced by the limitations of speech recognizers  typical speech recognizers produce many separated word
hypotheses with different plausibilities over time based on a given speech signal  such word
hypotheses can be connected to a word hypothesis sequence and have to be evaluated for
providing a basis for further analysis  typically  a word hypothesis consists of four parts    
 pause 
                

hm  eh 

ich  i 

                

    e   

                

wie  how 
    e   

htte  had 
                

                

am  on 
    e   

april  april 
    e   

                

ich  i 
                

                

ich  i 

leider  unfortunately 
    e   

                

    e   

sechsten  sixth 

    e   

                

    e   

                

bin  am 
    e   

                

                

    e   

    e   

wenn  if 
    e   

ich  i 
    e   

                

    e   

leider  unfortunately 
                

    e   

auer  out of 
                

hause  home 

    e   

                

    e   

                

 pause 
                

    e   

 not recognized 
    e   

figure    simple word graph for a spoken utterance   ahm am sechsten april bin ich
leider auer hause    eh on sixth april am i unfortunately out of home   
each node represents a word hypothesis  each arrow represents its possible
subsequent word hypotheses  each word hypothesis is shown with its word
string  start time  end time interval and acoustic plausibility 
  

fiwermter   weber

the start time in seconds     the end time in seconds     the word string of the hypothesis 
and    a plausibility of the hypothesis based on the confidence of the speech recognizer  below we show a simple word graph    in practice  word graphs for spontaneous speech can be
much longer leading to comprehensive word hypothesis sequences  however  for illustrating
the properties of the speech input we focus on this relatively short and simple word graph
 figure    
these word hypotheses can overlap in time and constitute a directed graph called word
graph  each node in this word graph represents one word hypothesis  two hypotheses in
this graph of generated word hypotheses can be connected if the end time of the first word
hypothesis is directly before the start time of the second word hypothesis  for instance  the
word hypothesis for  am    on   ending at      and the hypothesis  sechsten    sixth  
starting at      can be connected to a word hypothesis sequence 
hm
 eh 
hm
 eh 
 sec

ich
 i 

am
 on 

sechsten
 sixth 

april
 april 

bin
 am 

leider
 unfort  

auer
 out of 

hause
 home 

am
 on 

sechsten
 sixth 

april
 april 

wenn ich ich leider
 if   i   i   unfort  

auer
 out of 

hause
 home 

 sec

ich
 i 

 sec

figure    two examples for word hypothesis sequences in a word graph
our example word graph is very simple  however  as shown in figure    a possible
word hypothesis sequence is not only the desired  a hm am sechsten april bin ich leider
auer hause    eh on sixth april am i unfortunately out of home    but also the sequence
 a hm ich am sechsten april wenn ich ich leider auer hause    eh i on sixth april if i
i unfortunately out of home    consequently  we have to deal with incorrectly recognized
words in an extraordinary order  therefore syntactic and semantic analysis has to be very
fault tolerant in order to process such noisy word hypothesis sequences 

   flat category representation  an intermediate connecting
representation
in this section we will describe our at category representations  first  we will show the
categories for the syntactic analysis before we will depict the categories for the semantic
analysis 
   the speech input in the form of test word graphs was taken from the so called blaubeuren meeting
corpus  the particular word graphs we used here were provided by project partners for general test
purposes in the verbmobil project  they were particularly generated for testing parsing strategies 
therefore the speech recognizer was fine tuned to produce relatively small word graphs with a relatively
high word accuracy of      the vocabulary size for the hmm recognizer is      the average number
of hypotheses per word was     over    dialogs 

  

fiscreen  flat syntactic and semantic spoken language analysis

    categories for flat syntactic analysis

flat syntactic analysis is the assignment of syntactic categories to a sequence of words  e g  
the word hypothesis sequence generated by a speech recognizer  flat representations up to
the phrase group level support local structural decisions  local structural decisions deal
with the problem of which phrase group  abstract syntactic category  a word belongs to 
in this case the local  directly preceding words and their phrase group can inuence the
current decision  for instance  a determiner  the  could be part of a prepositional group
 in the mine  or part of a starting noun group  the old mine   that is  local structural
decisions depending on local context will be made based on a at analysis 
for at syntactic analysis we have developed a level of basic syntactic categories and
abstract syntactic categories  these syntactic categories may vary depending on the language  and the degree of detail of the intended structural representation  however  the
general approach is rather independent of the specifically used categories  in fact  we have
used the same syntactic categories for two different domains  railway counter interactions
and business meeting arrangements  the basic syntactic categories we used were noun 
verb  preposition  pronoun  numeral  past participle  pause  adjective  adverb  conjunction 
determiner  interjection and other  they are shown with their abbreviations in table   
category
noun  n 
verb  v 
preposition  r 
pronoun  u 
numeral  m 
participle  p 
pause    

examples
date  april
meet  choose
at  in
i  you
fourteenth
taken
pause

category
adjective  j 
adverb  a 
conjunction  c 
determiner  d 
interjection  i 
other  o 

examples
late
often
and  but
the  a
eh  oh
particles

table    basic syntactic categories
the abstract syntactic categories we used are verb group  noun group  adverbial group 
prepositional group  conjunction group  modus group  special group and interjection group 
these abstract syntactic categories are shown in table   
category
verb group  vg 
noun group  ng 
adverbial group  ag 
prepositional group  pg 
conjunction group  cg 
modus group  mg 
special group  sg 
interjection group  ig 

examples
mean  would propose
a date  the next possible slot
later  as early as possible
in the dining hall
and  either     or
interrogatives  confirmations  when  how long  yes
additives like politeness  please  then
interjections  pauses  eh  oh

table    abstract syntactic categories
  

fiwermter   weber

the categories should express main syntactic properties of the phrases  most of our
basic and abstract syntactic categories are widely used in different parsers  however  the
approach of at representations does not crucially rely on this specific set of basic and
abstract syntactic categories  our goal is to train  learn and generalize a at syntactic
analysis based on abstract syntactic categories and basic syntactic categories  local syntactic decisions should be made as far as possible  local syntactic ambiguities up to the
phrase group level  abstract syntactic categories  can be dealt with but more global ambiguities like prepositional phrase attachment will not be dealt with since they will need
additional knowledge  e g   from a semantics module  while complete syntax trees have
a certain preference  which might turn out to be wrong based on semantic knowledge   a
at syntactic representation goes as far as possible using only local syntactic knowledge for
disambiguation 

    categories for flat semantic analysis

since semantic analysis is domain dependent  the semantic categories can differ for different
domains  we have worked particularly on two domains  railway counter interactions  called 
regensburg train corpus  and business meeting arrangements  called  blaubeuren meeting
corpus   there was about     overlap between the semantic categories of the train corpus
category
select  sel 
suggest  sug 
meet  meet 
utter  utter 
is  is 
have  have 
move  move 
aux  aux 
question  quest 
physical  phys 
animate  anim 
abstract  abs 
here  here 
source  src 
destination  dest 
location  loc 
time  time 
negative evaluation  no 
positive evaluation  yes 
nil  nil 

examples
select  choose
propose  suggest
meet  join
say  think
is  was
had  have
come  go
would  could
question words  where  when
physical objects  building  oce
animate objects  i  you
abstract objects  date
time or location state words  prepositions  at  in
time or location source words  prepositions  from
time or location destination words  prepositions  to
hamburg  pittsburgh
tomorrow  at   o  clock  april
no  bad
yes  good
words  without  specific semantics  e g   determiner  a

table    basic semantic categories
and the meeting corpus  wermter   weber      b   differences occurred mainly for verbs 
e g   need events are very frequent in the railway counter interactions while suggestevents are frequent in the business meeting interactions  the semantic categories of the
  

fiscreen  flat syntactic and semantic spoken language analysis

category
action  act 
aux action  aux 
agent  agent 
object  obj 
recipient  recip 
instrument  instr 
manner  manner 
time at  tm at 
time from  tm frm 
time to  tm to 
loc at  lc at 
loc from  lc frm 
loc to  lc to 
confirmation  conf 
negation  neg 
question  quest 
misc  misc 

examples
action for full verb events  meet  select
auxiliary action for auxiliary events  would like
agent of an action  i
object of an action  a date
recipient of an action  to me
instrument for an action  using an elevator
how to achieve an action  without changing rooms
at what time  in the morning
start time  after  am
end time  before  pm
at which location  in frankfurt  in new york
start location  from boston  from dortmund
end location  to hamburg
confirmation phrase  ok great  yes wonderful
negation phrase  no stop  not
question phrases  at what time
miscellaneous words  e g   for politeness  please  eh

table    abstract semantic categories
railway counter interactions were described in previous work  weber   wermter        
here we will primarily focus on the semantic categories of the meeting corpus  the basic
semantic categories for a word are shown in table    at a higher level of abstraction  each
word can belong to an abstract semantic category  the possible abstract semantic categories
are shown in table    in summary  these categories provide a basis for a at analysis  each
word is represented syntactically and semantically in its context by four categories at two
basic and two abstract levels 

   the architecture of the screen system

in this section we want to describe the constraints and principles which are important for our
system design  as we outlined and motivated in the introduction  the screening approach
is a at  robust  learned analysis of spoken language based on category sequences  called
at representations  at various syntactic and semantic levels  in order to test this screening
approach  we designed and implemented the hybrid connectionist screen system which
processes spontaneously spoken language by using learned connectionist at representations 
here we summarize our main requirements in order to motivate the specific system design
which will be explained in the subsequent subsections 

    general motivation for the architecture

we consider learning to be extremely important for spoken language analysis for several
reasons  learning reduces knowledge acquisition and increases portability  particularly
in spoken language analysis  where the underlying rules and regularities are dicult to
formulate and often not reliable  furthermore  in some cases  inductive learning may detect
  

fiwermter   weber

unknown implicit regularities  we want to use connectionist learning in simple recurrent
networks rather than other forms of learning  e g   decision trees  primarily because of the
inherent fault tolerance of connectionist networks  but also because knowledge about the
sequence of words and categories can be learned in simple recurrent networks 
fault tolerance for often occurring language errors should be reected in the system
design  we do this for the commonly occurring errors  interjections  pauses  word repairs 
phrase repairs   however  fault tolerance cannot go so far as to try to model each class of
occurring errors  the number of potentially occurring errors and unpredictable constructions is far too large  in screen  we want to incorporate explicit fault tolerance by using
specific modules for correction as well as implicit fault tolerance by using connectionist network techniques which are inherently fault tolerant due to their support of similarity based
processing  in fact  even if a word is completely unknown  recurrent networks can use an
empty input and may even assign the correct category if there is sucient previous context 
flat representations  as motivated in sections   and    may support a robust spokenlanguage analysis  however  at connectionist representations do not provide the full recursive power of arbitrary syntactic or semantic symbolic knowledge structures  in contrast
to context free parsers  at representations provide a better basis for robust processing
and automatic knowledge acquisition by inductive learning  however  it can also be argued that the use of potentially unrestricted recursion of well known context free grammar
parsers provides a computational model with more recursive power than humans have in
order to understand language  in order to better support robustness  we want to use at
representations for spontaneous language analysis 
incremental processing of speech  syntax  semantics and dialog processing in parallel
allows us to start the language analysis in parallel before the speech recognizer has finished
its analysis  this incremental processing has the advantage of providing analysis results
at a very early stage  for example  syntactic and semantic processing occur in parallel
only slightly behind speech processing  when analyzing spoken language based on speech
recognizer output  we want to consider many competing paths of word hypothesis sequences
in parallel 
with respect to hybrid representations  we examine a hybrid connectionist architecture
using connectionist networks where they are useful but we also want to use symbolic processing wherever necessary  symbolic processing can be very useful for the complex control
in a large system  on the other hand for learning robust analysis  we use feedforward and
simple recurrent networks in many modules and try to use rather homogeneous  supervised
networks 

    an overview of the architecture

screen has a parallel integrated hybrid architecture  wermter        which has various

main properties 

   outside of a module  there is no difference in communication between a symbolic and
a connectionist module  while previous hybrid architectures emphasized different
symbolic and connectionist representations  the different representations in screen
benefit from a common module interface  outside of a connectionist or symbolic
  

fiscreen  flat syntactic and semantic spoken language analysis

module all communication is identically realized by symbolic lists which contain values
of connectionist units 
   while previous hybrid symbolic and connectionist architectures are usually within
either a symbolic or a connectionist module  hendler        faisal   kwasny       
medsker         in screen a global state is described as a collection of individual
symbolic and connectionist modules  processing can be parallel as long as one module
does not need input from a second module 
   the communication among the symbolic and connectionist modules is organized via
messages  while other hybrid architectures have often used either only activation
values or only symbolic structures  we used messages consisting of lists of symbols
with associated activation or plausibility values to provide a communication medium
which supports both connectionist processing as well as symbolic processing 
we will now give an overview of the various parts in screen  see figure     the
important output consists of at syntactic and semantic category representations based
on the input of incrementally recognized parallel word hypotheses  a speech recognizer
generates many incorrect word hypotheses over time  and even correctly recognized speech
can contain many errors introduced by humans  a at representation is used since it is
more fault tolerant and robust than  for instance  a context free tree representation since a
tree representation requires many more decisions than a at representation 
each module in the system  for instance the disambiguation of abstract syntactic categories  contains a connectionist network or a symbolic program  the integration of symbolic
and connectionist representations occurs as an encapsulation of symbolic and connectionist
processes at the module level  connectionist networks are embedded in symbolic modules
which can communicate with each other via messages 
however  what are the essential parts needed for our purposes of learning spokenlanguage analysis and why  starting from the output of individual word hypotheses of
a speech recognizer  we first need a component which receives an incremental stream of
individual parallel word hypotheses and produces an incremental stream of word hypothesis sequences  see figure     we call this part the speech sequence construction part  it is
needed for transforming parallel overlapping individual word hypotheses to word hypothesis sequences  these word hypothesis sequences have a different quality and the goal is
to find and work with the best word hypothesis sequences  therefore we need a speech
evaluation part which can combine speech related plausibilities with syntactic and semantic
plausibilities in order to restrict the attention to the best found word hypothesis sequences 
furthermore  we need a part which analyzes the best found word hypothesis sequences
according to their at syntactic and semantic representation  the category part receives
a stream of current word hypothesis sequences  two such word hypothesis sequences are
shown in figure    this part provides the interpretation of a word hypothesis sequence
with its basic syntactic categories  abstract syntactic categories  basic semantic categories 
and abstract semantic categories  that is  each word hypothesis sequence is assigned four
graded preferences for four word categories 
human speech analyzed by a speech recognizer may contain many errors  so the question
arises to what extent we want to consider these errors  an analysis of several hundred
  

fiwermter   weber

two word hypotheses sequences 
  

output to further analysis
  

kse

ich

meine

natrlich

 rubbish 

 i 

 mean 

 of course   march 

n

ng

u

ng

no

neg

anim

agent utter act

v

vg

a

sg

n

ng

nill

misc

time

tmat

kse

ich

htte

ich

 rubbish 

 i 

 had 

 i 

n

ng

u

ng

no

neg

anim

agent have

v

mrz

mrz
 march 

vg

u

act

anim  agent  time

 ng 

n

ng
tmat

    
syntactic semantic hypotheses

case frame part

dialog part
learned
flat
syntactic and
semantic
analysis

correction part
speech evaluation part

category part
constructed word hypotheses sequences 
kse ich meine natrlich mrz
   rubbish i mean of course march

speech sequence construction part

kse ich htte ich mrz
i had i march

   rubbish

    

word hypotheses
word hypotheses generated by speech recognizer 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

input from speech recognizer
current word hypothesis

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

 pause 
kse
ich
ich
meine
meine
htte
etliche
ich
mrz
da
natrlich
mrz
aus
 pause 

figure    overview of screen
  

 rubbish 
 i 
 i 
 mean 
 mean 
 had 
 several 
 i 
 march 
 there 
 of course 
 march 
 out 

        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  
        e  

fiscreen  flat syntactic and semantic spoken language analysis

transcripts and speech recognizer outputs revealed that there are some errors which occur
often and regularly  these are interjections  pauses  word repairs  and phrase repairs 
therefore we designed a correction part which receives hypotheses about words and deals
with most frequently occurring errors in spoken language explicitly 
these parts outlined so far build the center of the integration of speech related and
language related knowledge in a at fault tolerant learning architecture  and therefore we
will focus on these parts in this paper  however  if we want to process complete dialog turns
which can contain several individual utterances we need to know where a certain utterance
starts and which constituents belong to this utterance  this task is performed by a case
frame part which fills a frame incrementally and segments a speaker s turn into utterances 
the long term perspective of screen is to provide an analysis for tasks such as spoken
utterance translation or information extraction  besides the syntactic and semantic analysis
of an utterance  the intended dialog acts convey important additional knowledge  therefore 
a dialog part is needed for assigning dialog acts to utterances  for instance if an utterance is
a request or suggestion  in fact  we have already fully implemented the case frame part and
the dialog part for all our utterances  however  we will not describe the details of these two
parts in this paper since they have been described elsewhere  wermter   lochel        
learning in screen is based on concepts of supervised learning as for instance in feedforward networks  rumelhart et al          simple recurrent networks  elman        and
more general recurrent plausibility networks  wermter         in general  recurrent plausibility networks allow an arbitrary number of context and hidden layers for considering long
distance dependencies  however  for the many network modules in screen we attempted
to keep the individual networks simple and homogeneous  therefore  in our first version
described here we used only variations of feedforward networks  rumelhart et al        
and simple recurrent networks  elman         due to their greater potential for sequential
context representations  recurrent plausibility networks might provide improvements and
optimizations of simple recurrent networks  however  for now we are primarily interested
in an overall real world hybrid connectionist architecture screen rather than the optimization of single networks  in the following description we will give detailed examples of the
individual networks 

    a more detailed view

after we motivated the various parts in screen  we will now give a more detailed description
of the architecture of screen with respect to the modules for at syntactic and semantic
analysis of word hypothesis sequences  therefore  we will focus on the speech related parts 
the categorization part and correction part  figure   shows a more detailed overview of these
parts  the basic data ow is shown with arrows  many modules generate hypotheses which
are used in subsequent modules at a higher level  these hypotheses are illustrated with
rising arrows  in some modules  the output contains local predictive hypotheses  sometimes
called local top down hypotheses  which are used again in modules at a lower level  these
hypotheses are illustrated with falling arrows  local predictive hypotheses are used in the
correction part to eliminate  repaired utterance parts and in the speech evaluation part
to eliminate syntactically or semantically implausible word hypothesis sequences  in some
   this means that repaired utterance parts are actually only marked as deleted 

  

fiwermter   weber

segment parser

dia act

frame

slots
dialog act
type
verb form

   

case frame part  

 

 

reject
utter
meinen
 mean 

 

 

dialog part
phrase error

 

 

 

 

bas syn eq

bas sem eq

 

 

    

    

correction part

lex start eq

lex word eq

 

 

word error

is there a
pause  interjection 
hesitation  unresolved
phonetic material 

interjection

 

   

pause error

pause

dialog lexicon

 

abs syn eq
 

 

   

    

 

 

sem speech error

 

phrase start 

 

bas syn pre

 
 

separates
syn speech error

abs sem eq

 

abs syn cat

abs sem cat

 

 

  

  

  

  

bas sem pre

  

  

  

  

bas syn dis

 

 

bas sem dis

  

  

  

  

 

 
verb  pronoun
utter  nil

speech evaluation part

syntactic lexicon
semantic lexicon

category part

 
con sequ hyps
kse ich meine

 rubbish i mean 
constructed word hypotheses sequences 
kse ich
kse ich meine

 rubbish i 
 rubbish i mean 

speech sequence construction part

figure    more detailed overview of screen  the abbreviations and functionality of
the modules are described in the text 
  

fiscreen  flat syntactic and semantic spoken language analysis

cases where arrows would have been too complex we have used numbers to illustrate the
data ow between individual modules 
      speech sequence construction part

the speech sequence construction part receives a stream of parallel word hypotheses and
generates a stream of word hypothesis sequences within the module con sequ hyps at the
bottom of figure    based on the current word hypotheses many word hypothesis sequences
may be possible  in some cases we can reduce the number of current word hypotheses  e g  
if we know that time has passed so far that a specific word hypothesis sequence cannot
be extended anymore at the time of the current word hypothesis  in this case we can
eliminate this sequence since only word hypothesis sequences which could reach the end of
the sentence are candidates for a successful speech interpretation 
furthermore  we can use the speech plausibility values of the individual word hypothesis
to determine the speech plausibility of a word hypothesis sequence  by using only some
of the best word hypothesis sequences we can reduce the large space of possible sequences 
the generated stream of word hypothesis sequences is similar to a set of partial n best
representations which are generated and pruned incrementally during speech analysis rather
than at the end of the speech analysis process 
      speech evaluation part

the speech evaluation part computes plausibilities based on syntactic and semantic knowledge in order to evaluate word hypothesis sequences  this part contains the modules for
the detection of speech related errors  currently  the performance of speech recognizers
for spontaneously spoken speaker independent speech is in general still far from perfect 
typically  many word hypotheses are generated for a certain signal    therefore  many hypothesized words produced by a speech recognizer are incorrect and the speech confidence
value for a word hypothesis alone does not provide enough evidence for finding the desired
string for a signal  therefore the goal of the speech evaluation part is to provide a preference
for filtering out unlikely word hypothesis sequences  syn speech error and sem speecherror are two modules which decide if the current word hypothesis is a syntactically
 semantically  plausible extension of the current word hypothesis sequence  the syntactic
 semantic  plausibility is based on a basic syntactic  semantic  category disambiguation and
prediction 
in summary  each word hypothesis sequence has an acoustic confidence based on the
speech recognizer  a syntactic confidence based on syn speech error  and a semantic
confidence based on sem speech error  these three values are integrated and weighted
equally  to determine the best word hypothesis sequences  that way  these two modules can
   the hmm speech recognizer used for generating word hypotheses in our domain has a word accuracy
of about     for the best match between the word graph and the desired transcript utterance  this
recognizer was particularly optimized for this task and domain in order to be able to examine the
robustness at the language level  an unoptimized version for this task and domain currently has    
word accuracy 
   this integration of speech  syntax  and semantics confidence values provided better results than just
using one or two of these three knowledge sources 

  

fiwermter   weber

act as an evaluator for the speech recognizer as well as a filter for the language processing
part 
in statistical models for speech recognition  bigram or trigram models are used as language models for filtering out the best possible hypotheses  we used simple recurrent
networks since these networks performed slightly better than a bigram and a trigram model
which had been implemented for comparison  sauerland         later in section     we
will also show a detailed comparison of simple recurrent networks and n gram models  for n
            the reason for this better performance is the internal representation of a simple
recurrent network which does not restrict the covered context to a fixed number of two or
three words but has the potential to learn the required context that is needed 
output layer
   units
n

j

v

a

r

c

u

d

m

i

p

o

 

      connections

context layer

hidden layer
     
conn 

input layer

     units

   copy

      connections

   units
n

j

v

a

r

c

u

d

m

i

p

o

 

disambiguated representation of  ich    i   from bas syn dis

figure    network architecture for the syntactic prediction in the speech evaluation part
 bas syn pre   the abbreviations are explained in table   
the knowledge for the syntactic and semantic plausibility is provided by the prediction
networks  bas syn pre and bas sem pre  of the speech evaluation part and the disambiguation networks  bas syn dis and bas sem dis  of the categorization part  as an example  we show the network for bas syn pre in figure    the previous basic syntactic
category of the currently considered word hypothesis sequence is input to the network  in
our example  ich    i   from the word hypothesis sequence  kase ich meine    rubbish i
mean   is found to be a pronoun  u   therefore  the syntactic category representation for
 ich    i   contains a     for the pronoun  u  category  all other categories receive a     
the input to this network consists of    units for our    categories  the output of
the network has the same size  each unit of the vector represents a plausibility for the
predicted basic syntax category of the last word in the current word hypothesis sequence 
the plausibility of the unit representing the desired basic syntactic category  found by
bas syn dis  is taken as syntactic plausibility for the currently considered word hypothesis
sequence by syn speech error  in this example  meine    mean   is found to be a verb
  

fiscreen  flat syntactic and semantic spoken language analysis

 v   therefore the plausibility for a verb  v  will be taken as syntax plausibility  selection
marked by a box in the output layer of bas syn pre in figure    
in summary  the syntactic  semantic  plausibility of a word hypothesis sequence is evaluated by the degree of agreement between the disambiguated syntactic  semantic  category
of the current word and the predicted syntactic  semantic  category of the previous word 
since decisions about the current state of a whole sequence have to be made  the preceding
context is represented by copying the hidden layer for the current word to the context layer
for the next word based on an srn network structure  elman         all connections in
the network are n m connections except for the connections between the hidden layer and
the context layer which are simply used to copy and store the internal preceding state in
the context layer for later processing when the next word comes in  in general  the speech
evaluation part provides a ranking of the current word hypothesis sequences by the equally
weighted combination of acoustic  syntactic  and semantic plausibility 
      category part

the module bas syn dis performs a basic syntactic disambiguation  see figure     input to
this module is a sequence of potentially ambiguous syntactic word representations  one for
each word of an utterance at a time  then this module disambiguates the syntactic category
representation according to the syntactic possibilities and the previous context  the output
is a preference for a disambiguated syntactic category  this syntactic disambiguation task is
learned in a simple recurrent network  input and output of the network are the ambiguous
and disambiguated syntactic category representations  in figure   we show an example
input representation for  meine    mean    my   which can be a verb and a pronoun 
however  in the sequence  ich meine    i mean     meine  can only be a verb and therefore
the network receives the disambiguated verb category representation alone 
the module bas sem dis is similar to the module bas syn dis but instead of receiving
a potentially ambiguous syntactic category input and producing a disambiguated syntactic
category output  the module bas sem dis receives a semantic category representation from
the lexicon and provides a disambiguated semantic category representation output  this
semantic disambiguation is learned in a simple recurrent network which provides the mapping from the ambiguous semantic word representation to the disambiguated semantic word
representation  both modules bas syn dis and bas sem dis provide this disambiguation
so that subsequent tasks like the association of abstract categories and the test of category
equality for word error detection is possible 
the module abs syn cat supplies the mapping from disambiguated basic syntactic
category representations to the abstract syntactic category representations  see figure     
this module provides the abstract syntactic categorization and it is realized with a simple
recurrent network  this module is important for providing a at abstract interpretation
of an utterance and for preparing input for the detection of phrase errors  figure    shows
that the disambiguated basic syntactic representation of  meine    mean   as a verb   and
a very small preference for a pronoun   is mapped to the verb group category at the higher
abstract syntactic category representation  based on the number of our basic and abstract
syntactic categories there are    input units for the basic syntactic categories and   output
units for the abstract syntactic categories 
  

fiwermter   weber

output layer
   units
n

j

v

a

r

c

u

d

m

i

p

o

 

      connections

hidden layer

context layer
     
conn 

input layer

     units

   copy

      connections

   units
n

j

v

a

r

c

u

d

m

p

o

 

ambiguous representation
of  meine   verb pronoun 

syntactic lexicon
meine

kse
 rubbish 

i

verb
pronoun

ich meine
 i   mean 

current word hypotheses sequence

figure    network architecture for the basic syntactic disambiguation  bas syn dis  
the abbreviations are explained in table   
the module abs sem cat is a parallel module to abs syn cat but uses basic semantic
category representations as input and abstract semantic category representations as output 
similar to the previous modules  we also used a simple recurrent network to learn this
mapping and to represent the sequential context  the input to the network is the basic
semantic category representation for the word  and the output is an abstract category
preference 
these described four networks provide the basis for the fault tolerant at analysis and
the detection of errors  furthermore  there is the module phrase start for distinguishing
abstract categories  the task of this module is to indicate the boundaries of subsequent
abstract categories with a delimiter  we use these boundaries to determine the abstract
syntactic and abstract semantic category of a phrase    earlier experiments had provided
support to take the abstract syntactic category of the first word in a phrase as the final
abstract syntactic category of a phrase  since phrase starts  e g   prepositions  are good
   in figure   we show the inuence of the phrase start delimiter on the abstract syntactic and semantic
categorization with dotted lines 

  

fiscreen  flat syntactic and semantic spoken language analysis

output layer
  units
ng

vg

pg

cg

ag mg

sg

ig

      connections

hidden layer

context layer
     
conn 

input layer

      connections

     units

   copy

   units
n
j
v
a
r
c
u
d
disambiguated representation of  meine   mean 

m

i

p

o

 

figure     network architecture for the abstract syntactic categorization  abs syn cat  
the abbreviations are explained in table   
indicators for abstract syntactic categories  wermter   lochel         on the other hand 
earlier experiments supported to take the abstract semantic category of the last word of a
phrase as the final abstract semantic category of a phrase  since phrase ends  e g   nouns 
are good indicators for abstract semantic categories  wermter   peters         furthermore  the phrase start gives us an opportunity to distinguish two equal subsequent abstract
categories of two phrases  for instance  if we have a phrase like  in hamburg on monday 
we have to know where the border exists between the first and the second prepositional
phrase 
      correction part

the correction part contains modules for detecting pauses  interjections  as well as repetitions and repairs of words and phrases  see figure     the modules for detecting pause
errors are pause error  pause and interjection  the modules pause and interjection receive the currently processed word and detect the potential occurrence of a pause
and interjection  respectively  the output of these modules is input for the module pauseerror  as soon as a pause or interjection has been detected  the word is marked as deleted
and therefore virtually eliminated from the input stream   an elimination of interjections
and pauses is desired   for instance in a speech translation task   in order to provide an inter   pauses and interjections can sometimes provide clues for repairs  nakatani   hirschberg        although
currently we do not use these clues for repair detection  compared to the lexical  syntactic  and semantic
equality of constituents  interjections and pauses provide relatively weak indicators for repairs since they
also occur relatively often at other places in a sentence  however  since we just mark interjections and
pauses as deleted we could make use of this knowledge in the future if necessary 

  

fiwermter   weber

pretation with as few errors as possible  since these three modules are basically occurrence
tests they have been realized with symbolic representations 
the second main cluster of modules in the correction part are the modules which are
responsible for the detection of word related errors  then  word repairs as in  am sechsten
april bin ich ich    on sixth april am i i   or  wir haben ein termin treffen    we have
a date meeting   can be dealt with  there are certain preferences for finding repetitions
and repairs at the word level  among these preferences there is the lexical equality of two
subsequent words  symbolic module lex word eq   the equality of two basic syntactic
category representations  connectionist module bas syn eq   and the equality of the basic
semantic categories of two words  connectionist module bas sem eq   as an example for
the three modules  we show the test for syntactic equality  bas syn eq  in figure    
output layer
  units
equal not equal
    connections

hidden layer
  units
input layer

         connections

n j v a r c u d m i p o  

n j v a r c u d m i p o  

    
units

disambiguated representation of second  ich   i 

disambiguated repr  of first  ich   i 

output layer
  units
equal not equal
    connections

hidden layer
  units
input layer

         connections

n j v a r c u d m i p o  
disambiguated repr  of  termin   date 

n j v a r c u d m i p o  

    
units

disambiguated representation of  treffen   meeting 

figure     network architecture for the equality of basic syntactic category representation  bas syn eq   the abbreviations are explained in table   
  

fiscreen  flat syntactic and semantic spoken language analysis

two output units for plausible implausible outcome have been used here since the network with two output units gave consistently better results compared with a network with
only one output unit  with   for plausible and   for implausible   the reason why the network with two output units performed better is the separation of the weights for plausible
and implausible in the hidden output layer  in order to receive a single value  the two output values are integrated according to the formula  unit          unit     then  the output
of all three equality modules is a value between   and   where   represents equality and  
represents inequality  although a single such preference may not be sucient  the common
inuence provides a reasonable basis for detecting word repairs and word repetitions in the
module word error  then  word repairs and repetitions are eliminated from the original
utterance  since the modules for word related errors are based on two representations of two
subsequent input words and since context can only play a minor role  we use feedforward
networks for these modules  on the other hand  the simple test on lexical equality of the
two words in lex word eq is represented more effectively using symbolic representation 
the third main cluster in the correction part consists of modules for the detection
and correction of phrase errors  an example for a phrase error is   wir brauchen
den fruheren termin den spateren termin    we need the earlier date the later date   
there are preferences for phrase errors if the lexical start of two subsequent phrases is
equal  if the abstract syntactic categories are equal and if the abstract semantic categories
are equal  for these three preferences we have the modules lex start eq  abs syn eq
and abs sem eq  all these modules receive two input representations of two corresponding
words from two phrases  lex start eq receives two lexical words  abs syn eq two abstract
syntactic category representations  and abs sem eq two abstract semantic category representations  the output of these three modules is a value toward   for equality and toward
  otherwise  these values are input to the module phrase error which finally decides
whether a phrase is replaced by another phrase  as the lexical equality of two words is a
discrete test  we have implemented lex start eq symbolically  while the other preferences
for a phrase error have been implemented as feedforward networks 

   detailed analysis with examples

in this section we will have a detailed look at processing the output from a speech recognizer
and producing a at syntactic and semantic interpretation of concurrent word hypothesis
sequences  also called sentence hypothesis here  

    the overall environment

the overall processing is incremental from left to right  and any time multiple sentence
hypotheses are processed in parallel  figure    shows a snapshot of screen after     s of
the utterance  at this time the snapshot shows the first three sentence hypotheses as the
german words together with their  literal  english translations   rubbish i mean    rubbish i    rubbish i had    the screen environment allows the user to view and inspect
the incremental generation of word hypothesis sequences  partial sentence hypotheses  and
their most preferred syntactic and semantic categories at the basic and abstract level  each
sentence hypothesis is illustrated horizontally  at a certain time many sentence hypotheses
can be active in parallel  they are ranked according to the descending plausibility of the
  

fiwermter   weber

screen   symbolic connectionist robust enterprise for natural language
quit

on line

stop

go

single step

 

  sentencehypotheses  time      s  system      s  display 

n

no

ng

sug

neg conf

kse  rubbish 

n

no

 

ng

sug

neg conf

kse  rubbish 

n

no

ng

sug

neg conf

kse  rubbish 

u

ng

sug

anim agent conf

ich  i 

v

nil

sug

utter nil

conf

meine  mean 

u

nil

sug

anim

nil

conf

ng

sug

ich  i 

u

anim agent conf

ich  i 

v

nil

sug

have

nil

conf

htte  had 

 

                                                                                          
n
j
v
a
r
c
u
d
m
i
p
o
 

figure     first snapshot for sentence  kase ich meine naturlich marz   rubbish i mean
of course march    the abbreviations are explained in table   to    below 
the second pop up window illustrates the full preferences of the word  meine 
  mean   for its basic syntactic categories 
sentence hypotheses  so in the snapshot in figure    there are currently three sentence
hypotheses and the preferred current sentence hypothesis consists of  rubbish i mean  
all these sentence hypotheses are syntactically and semantically plausible starts  the
underlying variations are introduced by the speech recognizer which produced different word
hypotheses for slightly overlapping signal parts of the sentence  besides the speech plausibility  syntax and also semantics can help with choosing better sentence hypotheses  currently
we combine the speech recognition plausibility  the syntactic plausibility  and the semantic
plausibility to compute the plausibility of the sentence hypotheses as a multiplication of the
respective normalized plausibility values between   and    since the speech recognizer does
not contain syntactic and semantic knowledge  a sequence hypothesis rated plausible based
on speech knowledge alone may neglect the potential of syntactic and semantic regularity 
  

fiscreen  flat syntactic and semantic spoken language analysis

by using corresponding syntactic and semantic plausibility values for a sentence hypothesis
we can integrate acoustic  syntactic  and semantic knowledge 
each word hypothesis is shown with the preferred basic syntactic hypothesis  upper left
square of a word hypothesis   the preferred abstract syntactic hypothesis  upper middle
square   the preferred basic semantic hypothesis  lower left square   the preferred abstract
semantic hypothesis  lower middle square   the preferred dialog act  upper right square   
and the integrated acoustic  syntactic and semantic confidence of the partial sentence hypothesis up to that point  lower right square   the size of the square illustrates the strength
of the hypothesis  and a full black square means that a preferred hypothesis is close to one 
for instance  in the word hypothesis for  ich    i   in the first sentence hypothesis we have
the hypothesis of a pronoun  u  as the basic syntactic category  a noun group  ng  as the
abstract syntactic category  an animate object  anim  as the basic semantic category  an
agent as the abstract semantic category  and suggestion  sug  as dialog act  furthermore  the length of a vertical bar between word hypotheses indicate the plausibility for a
new phrase start 
as another example  we can see the representation of our example word  meine   could
be the verb  mean  or the pronoun  my  in german  which we have used throughout the
network descriptions  see figure     the network had a correct preference for  meine 
being a verb  v   figure    shows this preference as well as a zoomed illustration of all
other less favored preferences in a second pop up window below  as we can see  the ambiguous other pronoun preference u received the second strongest activation while all other
preferences are close to    these shown activation preferences are the output values of the
corresponding network for basic syntactic categorization  so any shown activation value in
our snapshots shows only the most preferred hypothesis while all other hypotheses can be
shown on request   
within the display we can scroll up and down the descending and ascending sentence
hypotheses  furthermore we can scroll left and right for analyzing specific longer word
hypothesis sequences  there is also a step mode which allows the screen system to wait
for an interactive mouse click to process the next incoming word hypothesis for a very
detailed analysis  this step mode can be adapted for a different number of steps  word
hypotheses  and it can be switched off completely if one decides to analyze the sentence
hypotheses later or at the end of all word hypotheses  only the preferred of all possible
syntactic and semantic hypotheses are shown  therefore many different hypotheses appear
to have the same size  however  by clicking on one of the squares the other less confident
hypotheses can be displayed as well 
   the dialog acts we use are  accept  acc   query  query   reject  rej   request suggest  re s  
request state  re s   state  state   suggest  sug   and miscellaneous  misc   since this paper focuses
on the syntactic and semantic aspects of screen we do not further elaborate on the implemented dialog
part here  further details on dialog act processing have been described previously  wermter   lochel 
      
    in the snapshots in figure    the abstract syntactic and semantic categories have not yet been computed
and therefore are represented as nil  in the next processing step this computation will be performed
which can be seen in next figure    

  

fiwermter   weber

    analyzing the final snapshot in short sentence hypotheses

in figure    we illustrate the final state after     s of the utterance  eight possible sentence hypotheses remained out of which we see the first four in figure     starting with the
fourth sentence hypothesis  kase ich hatte ich marz    rubbish i had i march   we can
see that this lower rated sentence hypothesis is not the desired sentence  the lower ranked
hypotheses are good examples that current state of the art speech recognizers alone will not
be able to produce reliable sentence hypotheses  since the problem of analyzing spontaneous
speaker independent speech is very complex  therefore the syntactic and semantic components for spontaneous language have to take into account that there will be highly irregular
sequences as shown below  however  it is interesting to observe that the underlying connectionist networks always produce a preference for the syntactic and semantic interpretation
at the abstract and basic level  in fact  although the lower ranked sentence hypotheses
do not constitute the desired sentence all assigned syntactic and semantic categories are
correct for the individual word hypotheses  of course there may be cases that a network
also could make a wrong decision for uncertain word hypotheses  however the syntactic
and semantic processing will never break for any possible sentence hypothesis  and is in this
respect different from more well known methods like symbolic context free chart parsers 
if we look at the top ranked sentence hypothesis  kase ich meine naturlich marz    rubbish i mean of course march   this is also the desired sentence  it is the most plausible
screen   symbolic connectionist robust enterprise for natural language
quit

go

on line

stop

single step

 

  sentencehypotheses  time      s  system      s  display 

n

ng dsug

no

neg conf

kse

 

ng dsug

anim agent conf

ich

n

ng dsug

no

neg conf

kse

u

ng dsug

ng dsug

anim agent conf

no

neg conf

kse

u

ng dsug

ng dsug

anim agent conf

no

neg conf

u

utter act

conf

v

vg dsug

utter act

v

have

ng dsug

v

have

htte

a

sg dsug

nill

misc conf

natrlich

conf

a

sg dsug

nill

misc conf

natrlich

vg dsug

act

conf

htte

anim agent conf

ich

vg dsug

meine

ich

n

v

meine

ich

n

kse

u

u

ng dsug

anim agent conf

ich

vg dsug

act

conf

u

n

ng dsug

time tmat conf

mrz

n

ng dsug

time tmat conf

mrz

n

ng dsug

time tmat conf

mrz

ng dsug

anim agent conf

ich

n

ng dsug

time tmat conf

mrz
 

figure     final snapshot for sentence  kase ich meine naturlich marz   rubbish i mean
of course march   
  

fiscreen  flat syntactic and semantic spoken language analysis

sentence based on speech and language plausibility  furthermore  we can see that the assigned categories are correct  the german word  kase    rubbish   is found to be a noun
as part of a noun group which expresses a negation   ich    i   starts a new phrase  that is
a pronoun as a noun group which represents an animate being and an agent  the following
german word  meine  is particularly interesting since it can be used as a verb in the sense
of  mean  but also as a pronoun in the sense of  my   therefore  the connectionist network
for the basic syntactic classification has to disambiguate these two possibilities based on
the preceding context  the network has learned to take into consideration the preceding
context and is able to choose the correct basic syntactic category verb  v  rather than
pronoun  u  for the word  meine    mean    at this time a new phrase start has been
found as well  the following word  naturlich    of course   has the highest preference for
an adverb and a special group  finally  the word  marz    march   is assigned the highest
plausibility for a noun and noun group as well as a time at which something happens 

    phrase starts and phrase groups in longer sentence hypotheses

now we will focus on a detailed analysis of a second example   a hm ja genau allerdings
habe ich da von neun bis vier uhr schon einen arzttermin   the literally translated
screen   symbolic connectionist robust enterprise for natural language
quit

go

on line

stop

single step

 

   sentencehypotheses  time      s  system      s  display 

a

yes

mg dacc

conf conf

ja

yes

mg dacc

conf conf

ja

drej

conf conf

j

yes

yes

mg dacc

conf conf

ja

a

nill

mg

drej

conf conf

yes

mg dacc

conf conf

a

nill

no

sg

drej

neg conf

a

no

sg

drej

neg conf

allerdings

mg dmisc

misc conf

dennoch

a

a

allerdings

genau

a

ja

yes

mg

genau

a

 

j

mg dmisc

misc conf

dennoch

a

no

sg dmisc

neg conf

allerdings

a

no

sg dmisc

neg conf

allerdings

v

vg

drej

have

act

conf

habe

vg

drej

have

act

conf

habe

u

nill

act

conf

have

ng

drej

misc conf

act

conf

anim agent conf

u

nill

es

drej

misc conf

nill

sg

drej

misc conf

nill

sg dmisc

misc conf

da

misc conf

drej

here tmat conf

r

pg

drej

here tmat conf

r

here

nil dmisc

nil

conf

von

a

nill

da

pg

von

a

ng dmisc

r

von

a

ng dmisc

ich

habe

nill

sg

da

u

vg dmisc

a

da

es

habe

v

drej

anim agent conf

vg dmisc

have

ng

ich

v

v

u

sg dmisc

misc conf

r

here

nil dacc

nil

conf

von

 

figure     first part of the snapshot for sentence  a hm ja genau allerdings habe ich
da von neun bis vier uhr schon einen arzttermin   literal translation   yes
exactly however have i there from nine to four o clock already a doctorappointment   improved translation   eh yes exactly however then i have
a doctor appointment from nine to four o clock   
  

fiwermter   weber

screen   symbolic connectionist robust enterprise for natural language
quit

on line

stop

go

single step

 

   sentencehypotheses  time      s  system      s  display 

m

pg

misc

time tmat conf

neun  nine 

m

pg

misc

neun  nine 

m

pg

misc

time tmat conf

pg

misc conf

r

nil

r

nil

acc

r

nil

bis  to 

m

pg

misc

time tmat conf

vier  four 

pg

misc

misc conf

m

pg

pg

misc

misc

time tmat conf

misc conf

m

pg

misc

misc

time tmat conf

misc conf

m

pg

misc

time tmat conf

n

pg

misc

time tmat conf

n

pg

misc

time tmat conf

uhr  oclock 

misc

time tmat conf

zehn  ten 

pg

uhr  oclock 

zehn  ten 

pg

n

uhr  oclock 

vier  four 

bis  to 

time tmat conf

neun  nine 

misc

bis  to 

neun  nine 

m

nil

pg

bis  to 

time tmat conf

 

r

n

pg

acc

time tmat conf

uhr  oclock 

a

nil

sg

misc

misc conf

schon  already 

a

nil

sg

misc

misc conf

schon  already 

a

nil

sg

misc

misc conf

schon  already 

a

nil

sg

acc

misc conf

schon  already 

d

nil

ng

res

misc conf

einen  a 

d

nil

ng

res

misc conf

nil

ng

res

misc conf

nil

ng

abs

obj

conf

n

ng

res

abs

obj

conf

n

ng

res

abs

obj

conf

arzttermin doc app

res

misc conf

einen  a 

res

arzttermin doc app

einen  a 

d

ng

arzttermin doc app

einen  a 

d

n

n

ng

res

abs

obj

conf

arzttermin doc app

 

figure     second part of the snapshot for sentence  a hm ja genau allerdings habe ich
da von neun bis vier uhr schon einen arzttermin    yes exactly however have
i there from nine to four o clock already a doctor appointment   
sentence to be analyzed is   eh yes exactly however have i there from nine to four o clock
already a doctor appointment   a better but non literal translation would be   eh yes
exactly however then i have a doctor appointment from nine to four o clock   during
the analysis of the first few sentence hypotheses  the interjection  ahm    eh   is detected
by the corresponding module in the correction part and is eliminated from the respective
sentence hypotheses 
in figure    and figure    we show the best found four sentence hypotheses  the
categories of these sentence hypotheses look similar but we have to keep these separate
hypotheses since they differ in their time stamps and their speech confidence values 
in these two snapshots of this longer example we can also illustrate the inuence of
the phrase starts  the sequences  von neun    from nine   and  bis vier uhr    to four
o clock   constitute two phrase groups which are clearly separated by the black bar before
the prepositions  von    from   and  bis    to    all the other words  neun    nine   
 vier    four    and  uhr    o clock   do not start another phrase group  since the underlying connectionist network for learning the phrase boundaries is a simple recurrent network
this example demonstrates that this network has learned the preceding context  without
having learned that there had been a preposition  von    from   or  bis    to   a noun
  

fiscreen  flat syntactic and semantic spoken language analysis

like  uhr    o clock   does not have to be within a prepositional phrase group but could
also be part of a noun phrase in another context like  vier uhr pat gut    four o clock fits
well   

    dealing with noise as repairs
finally we will focus on the example for the simple word graph shown in the beginning of
this paper on page      a hm am sechsten april bin ich leider auer hause   the literal
translation is  eh on  th april am i unfortunately out of home   using this sentence we
will give an example for an interjection and a simple word repair  dealing with hesitations
and repairs is a large area in spontaneous language processing and is not the main topic of
this paper  a more detailed discussion on repairs in screen can be found in previous work 
weber   wermter         nevertheless  for the sake of illustration and completeness we
show the ability of screen to deal with interjections and word repairs  the first snapshot
in figure    shows the start of our example sentence after     s  the leading interjection
 eh  has been eliminated already 
furthermore  we can see that the second word hypothesis sequence shows two subsequent
word hypotheses for  ich    i    this is possible since there were two word hypotheses
screen   symbolic connectionist robust enterprise for natural language
quit

go

on line

stop

single step

 

   sentencehypotheses  time      s  system      s  display 

r

pg

sug

here tmat conf

am  on 

r

pg

sug

am  on 

r

pg

sug

here tmat conf

time tmat conf

m

pg

sug

time tmat conf

m

pg

sug

time tmat conf

sechsten   th 

pg

sug

here tmat conf

am  on 

sug

sechsten   th 

am  on 

r

pg

sechsten   th 

here tmat conf

 

m

m

pg

sug

time tmat conf

sechsten   th 

n

pg

sug

time tmat conf

april  april 

n

pg

sug

time tmat conf

pg

sug

time tmat conf

pg

is

act

conf

v

vg

sug

have

act

conf

u

pg

sug

u

ng

state

anim agent conf

ich  i 

u

nil

state

anim

nil

conf

u

nil

state

anim

nil

conf

ich  i 

u

nil

sug

anim

nil

conf

u

nil

sug

anim

nil

conf

ng

state

ich  i 

sug

anim recip conf

ich  i 

time tmat conf

april  april 

state

htte  had 

april  april 

n

vg

bin  am 

april  april 

n

v

ich  i 

v

vg

state

is

act

conf

bin  am 

u

anim agent conf

ich  i 

ich  i 

 

figure     first snapshot for sentence  a hm am sechsten april bin ich leider auer
hause    eh on  th april am i unfortunately out of home   
  

fiwermter   weber

generated by the speech recognizer which could be connected  in this case there were the
four word hypotheses shown below 
start time
    s
    s
    s
    s

end time
    s
    s
    s
    s

word hypothesis
ich  i 
ich  i 
ich  i 
ich  i 

speech plausibility
        e   
        e   
        e   
        e   

just using this speech knowledge from the word hypotheses  it is possible to connect
the second hypothesis which runs from     s to     s with the fourth hypothesis which runs
from     s to     s  this is an example of noise generated by the speech recognizer  since
the desired sentence contains only one word  ich    i   but the sentence hypothesis at this
point contains two  this repetition can be treated and eliminated in the same way as actual
word repairs in language  while the reasons for the occurrence of such repairs are different
the effect of a repeated word is the same  therefore  in this case the repeated  ich    i  
is eliminated from the sentence sequence  in figure    we show the final snapshot of the
sentence  we can see that no word repairs occur in the top ranked sentence hypothesis
which is also the desired sentence 
screen   symbolic connectionist robust enterprise for natural language
quit

on line

stop

go

single step

 

   sentencehypotheses  time      s  system      s  display 

m

pg

sug

time tmat conf

sechsten   th 

m

pg

sug

time tmat conf

 

sechsten   th 

m

pg

sug

time tmat conf

sechsten   th 

m

pg

sug

time tmat conf

sechsten   th 

n

pg

sug

time tmat conf

april  april 

n

pg

sug

time tmat conf

pg

sug

time tmat conf

pg

is

act

conf

u

pg

sug

u

ng

state

anim agent conf

ich  i 

sug

anim recip conf

u

v

vg

state

ng

sug

anim recip conf

is

act

conf

vg

state

ng

state

anim agent conf

is

act

conf

u

no

sg

rej

neg conf

a

no

sg

rej

neg conf

leider  unfort  

ich  i 

v

bin  am 

u

a

leider  unfort  

ich  i 

bin  am 

time tmat conf

april  april 

state

ich  i 

april  april 

n

vg

bin  am 

april  april 

n

v

a

no

sg

rej

neg conf

leider  unfort  

ng

state

anim agent conf

ich  i 

a

no

sg

rej

neg conf

leider  unfort  

r

pg

rej

here lcat conf

auer  out of 

r

pg

rej

here lcat conf

auer  out of 

r

pg

rej

here lcat conf

auer  out of 

r

pg

rej

here lcat conf

auer  out of 

n

pg

rej

phys lcat conf

hause  home 

n

pg

rej

phys lcat conf

hause  home 

n

pg

rej

phys lcat conf

hause  home 

n

pg

rej

phys lcat conf

hause  home 

 

figure     final snapshot for sentence  a hm am sechsten april bin ich leider auer
hause    eh on  th april am i unfortunately out of home   
  

fiscreen  flat syntactic and semantic spoken language analysis

in general  for language repairs  screen can deal with the elimination of interjections
and pauses  the repair of word repetitions  word corrections  where the words may be
different  but their categories are the same  as well as simple forms of phrase repairs  where
a phrase is repeated or replaced by another phrase  

   design analysis of screen
in this section we will describe our design choices in screen  in particular we focus on the
issues why we use connectionist networks  why we reach high accuracy with little training 
and how screen can be compared to other systems and other design principles 

    why did we use connectionist networks in screen 
in the past  n gram based techniques have been used successfully for tasks like syntactic
category prediction or part of speech tagging  therefore  it is possible to ask why we developed simple recurrent networks in screen  in this subsection we will provide a detailed
comparison of simple recurrent networks and n gram techniques for the prediction of basic
syntactic categories  we chose this task for a detailed comparison since it is currently the
most dicult task for a simple recurrent network in screen  so purposefully we did not
choose a subtask for which a simple recurrent network had a very high accuracy  but the
prediction task since it is more dicult to predict a category compared to disambiguating among categories  for instance  so we chose the dicult prediction with a relatively
low network performance in order to be  extremely  fair for the comparison with n gram
techniques 
we are primarily interested in the generalization behavior for new unknown input 
therefore figure    shows the accuracy of the syntactic prediction for the unknown test
set  after each word several different syntactic categories can follow and some syntactic
categories are excluded  for instance  after a determiner  the  an adjective or a noun can
follow   the short        the appointment   but after a determiner  the  a preposition is
implausible to occur and should most probably be excluded  therefore it is important to
know how many categories can be ruled out and figure    shows the relationship between
the prediction accuracy and the number of excluded categories for n grams and our simple
recurrent network  as described in figure    
as we can expect  for both techniques  n grams and recurrent networks  the prediction
accuracy is higher if only a few categories have to be excluded and the performance is lower
if many categories have to be excluded  however  more interestingly  we can see that simple
recurrent networks performed better than   grams    grams    grams    grams and   grams 
furthermore  it is interesting to note that higher n grams do not necessarily lead to better
performance  for instance  the   grams and   grams perform worse than   grams since they
would probably need much larger training sets 
we did the same comparison of n grams       and simple recurrent networks also for
semantic prediction and received the same result that simple recurrent networks performed
better than n grams  the performance of the best n gram was often only slightly worse
than the performance of the simple recurrent network  which indicates that n grams are a
reasonably useful technique  however  in all comparisons simple recurrent networks per  

fiwermter   weber

testset
   

correct prediction in  

  

  

  

srn
 gram
 gram
 gram
 gram
 gram

  

 
 

 

 

 

 

  

  

number of excluded categories

figure     comparison between simple recurrent network and n grams
formed at least slightly better than the best n grams  therefore  we used simple recurrent
networks as our primary technique for connectionist sequence learning in screen 
how can we explain this result  n grams like   grams still perform reasonably well
for our task and simple recurrent networks are closest to their performance  however 
simple recurrent networks perform slightly better since they do not contain a fixed and
limited context  in many sequences  the simple recurrent network may primarily use the
directly preceding word representation to make a prediction  however  in some exceptions
more context is required and the recurrent network has a memory of the internal reduced
representation of the preceding context  therefore  it has the potential to be more exible
with respect to the context size 
n grams may not perform optimally but they are extremely fast  so the question arises
how much time is necessary to compute a new category using new input and the current
context for the network  in general our networks differ slightly in size but typically they
contain several hundred weights  for a typical representative simple recurrent network with
   input units     hidden units    output units  and    context units  and about     weights
it takes      s on a sparc ultra to compute a new category within the whole forward sweep 
  

fiscreen  flat syntactic and semantic spoken language analysis

since the techniques for smoothed n grams basically rely on an ecient table look up
of precomputed values  of course typical n gram techniques are still faster  however  due to
their fixed size context they may not perform as well as simple recurrent networks  furthermore  computing the next possible categories in     s is fast enough for our current version
of screen  for the sake of an explanation one could argue that screen contains about   
networks modules and a typical utterance contains    words  so a single utterance hypothesis could be performed in      s  however  different from text tagging  we do not have single
sentences but we process word graphs  depending on the specific utterance  about     word
hypothesis sequences could be generated and have to be processed  furthermore there is
some book keeping required for keeping the best word hypotheses  for loading the appropriate networks with the appropriate word hypotheses  etc  the potentially large number of
word hypotheses  the additional book keeping performance  and the number of individual
modules for syntax  semantics and dialog processing explain why the total analysis time of
the whole unoptimized screen system is in the order of seconds although a single recurrent
network performs in the order of      s 

    improvement in the hypothesis space
in this subsection we will analyze to what extent the syntactic and semantic prediction
knowledge can be used to improve the best found sentence hypotheses  we illustrate the
pruning performance in the hypothesis space by integrating acoustic  syntactic  and semantic knowledge  while the speech recognizer alone provides only acoustic confidence
values  screen adds syntactic and semantic knowledge  all these knowledge sources are
weighted equally in order to compute a single plausibility value for the current word hypothesis sequence  this plausibility value is used in the speech construction part to prune
the hypothesis space and to select the currently best word hypothesis sequences  several
word hypothesis sequences are processed incremental and in parallel  at a given time the
  
n best incremental word hypothesis sequences are kept  
the syntactic and semantic plausibility values are based on the basic syntactic and semantic prediction  bas syn pre and bas sem pre  of the next possible categories for a
word and the selection of a preference by the determined basic syntactic respectively semantic category  bas syn dis and bas sem dis      the performance of the disambiguation
modules is         for the test set  for the prediction modules the performance is    
and     for the semantic and syntactic test set  respectively if we want to exclude at least
  of the    possible categories  this performance allows us the computation of a syntactic
and semantic plausibility in syn speech error and sem speech error  based on the
combined acoustic  syntactic  and semantic knowledge  first tests on the     turns show
that the accuracy of the constructed sentence hypotheses of screen could be increased
by about     using acoustic and syntactic plausibilities and by about     using acoustic 
syntactic  and semantic plausibilities  wermter   weber      a  
    in our experiments low values  n       provided the best overall performance 
    this was explained in more detail in section      

  

fiwermter   weber

    screen s network performance and why the networks yield high
accuracy with little training

for evaluating the performance of screen s categorization part on the meeting corpus we
first show the percentages of correctly classified words for the most important networks for
categorization  bas syn dis  bas sem dis  abs syn cat  abs sem cat  phrase start 
there were     turns in this corpus with     utterances and      words      of the     
words and     turns was used for training      for testing  usually more data is used for
training than testing  in preliminary earlier experiments we had used     for training and
    for testing  however  the performance on the unknown test set was similar for the    
training set and     test set  therefore  we used more testing than training data since we
were more interested in the generalization performance for unknown instances in the test
set compared to the training performance for known instances 
at first sight  it might seem relatively little data for training  while statistical techniques
and information retrieval techniques often work on large texts and individual lexical word
items  we need much less material to get a reasonable performance since we work on the
syntactic and semantic representations rather than the words  we would like to stress that
we use the syntactic and semantic category representations of      words for training and
testing rather than the lexical words themselves  therefore  the category representation
requires much less training data than a lexical word representation would have required  as
a side effect  also training time was reduced for the     training set  while keeping the same
performance on the     test set  that is  for training we used category representations from
   dialog turns  for testing generalization the category representations from the remaining
    dialog turns 
table   shows the test results for individual networks on the unknown test set  these
networks were trained for      epochs with a learning rate of       and    hidden units 
this configuration had provided the best performance for most of the network architectures 
in general we tested network architectures from   to    hidden units  learning parameters
from     to         as learning rule we used the generalized delta rule  rumelhart et al  
       an assigned output category representation for a word was counted as correct if the
category with the maximum activation was the desired category 
module

accuracy on test set
bas syn dis
   
bas sem dis
   
abs syn cat
   
abs sem cat
   
phrase start
   
word error
   
phrase error
   

table    performance of the individual networks on the test set of the meeting corpus
the performance for the basic syntactic disambiguation was     on the unknown test
set  current syntactic  text  taggers can reach up to about     accuracy on texts  however 
  

fiscreen  flat syntactic and semantic spoken language analysis

there is a big difference between text and speech parsing due to the spontaneous noise
in spoken language  the interjections  pauses  repetitions  repairs  new starts and more
 ungrammatical  syntactic varieties in our spoken language domain are reasons why the
typical accuracy of other syntactic text taggers has not been reached 
on the other hand we see     accuracy for the basic semantic disambiguation which
is relatively high for semantics  so there is some evidence that the noisy  ungrammatical 
variety of spoken language hurts syntax but less semantics  due to the domain dependence
of semantic classifications it is more dicult to compare and explain semantic performance 
however  in a different study within the domain of railway interactions we could reach a
similar performance  for details see section       in all our experiments syntactic results
were better than the semantic results  indicating that the syntactic classification was easier
to learn and generalize  furthermore  our syntactic results were close to     for noisy
spoken language which we consider to be very good in comparison to     for more regular
text language 
the performance for the abstract categories is somewhat lower than for the basic categories since the evaluation at each word introduces some unavoidable errors  for instance 
after  in  the network cannot yet know if a time or location will follow  but has to make
an early decision already  in general  the networks perform relatively well on this dicult
real world corpus  given that we did not eliminate any sentence for any reason and took all
the spontaneous sentences as they had been spoken 
furthermore  we use transcripts of spontaneous language for training in the domain
of meeting arrangements  most utterances are questions and answers about dates and
locations  this restricts the potential syntactic and semantic constructions  and we certainly
benefit from the restricted domain  furthermore  while some mappings are ambiguous for
learning  e g   a noun can be part of a noun group or a prepositional group  other mappings
are relatively unambiguous  e g   a verb is part of a verb group   we would not expect
the same performance on mixed arbitrary domains like the random spoken sentences about
various topics from passers by in the city  however  the performance in somewhat more
restricted domains can be learned in a promising manner  for a transfer to a different
domain see section       so there is some evidence that simple recurrent networks can
provide good performance using small training data from a restricted domain 

    screen s overall output performance
while we just described the individual network performance  we will now focus on the
performance of the running system  the performance in the running screen system has to
be different from the performance of the individual networks for a number of reasons  first 
the individual networks are trained separately in order to support a modular architecture 
in the running screen system  however  connectionist networks receive their input from
other underlying networks  therefore  the actual input to a connectionist network in the
running screen system may also differ from the original training and test sets  second  the
spoken sentences may contain errors like interjections or word repairs  these have to be part
of the individual network training  but the running screen system is able to detect and
correct certain interjections  word corrections and phrase corrections  therefore  system
and network performance differ at such disuencies  third  if we want to evaluate the
  

fiwermter   weber

performance of abstract semantic categorization and abstract syntactic categorization we
are particularly interested in certain sentence parts  for abstract syntactic categorization 
e g   the detection of a prepositional phrase  we have to consider that the beginning of a
phrase with its significant function word  e g   preposition  should be the most important
location for syntactic categorization  in contrast  for abstract semantic categorization  the
content word at the end of a phrase group  directly before the next phrase start  is most
important 
correct at syntactic output representation    
correct at semantic output representation    

table    overall syntactic and semantic accuracy of the running screen system on the
unknown test set of the meeting corpus
as we should expect based on the explanation in the previous paragraph  the overall accuracy of the output of the complete running system should be lower than the performance
of the individual modules  in fact  this is true and table   shows the overall syntactic and
semantic phrase accuracy of the running screen system      of all assigned syntactic
phrase representations of the unknown test set are correct and     of all assigned semantic
phrase representations  the slight performance drop can be partially explained by the more
uncertain input from other underlying networks which themselves are inuenced by other
networks  on the other hand  in some cases the various decisions by different modules  e g 
the three modules for lexical  syntactic and semantic category equality of two words  can
be combined in order to clean up some errors  e g  a wrong decision by one single module  
in general  given that the     dialog turns of the test set were completely unrestricted  unknown real world and spontaneous language turns  we believe that the overall performance
is quite promising 

    screen s overall performance for an incomplete lexicon

one important property of screen is its robustness  therefore  it is an interesting question
how screen would behave if it could only receive incomplete input from its lexicon  such
situations are realistic since speakers could use new words which a speech recognizer has
not seen before  furthermore  we can test the robustness of our techniques  while standard
context free parsers usually cannot provide an analysis if words are missing from the lexicon 
screen would not break on missing input representations  although of course we have to
expect that the overall classification performance must drop if less reliable input is provided 
in order to test such a situation under the controlled inuence of removing items from
the lexicon  we first tested a scenario where we randomly eliminated    of the syntactic and
semantic lexicon representations  if a word was unknown  screen used a single syntactic
and single semantic average default vector instead  this average default vector contained
the normalized frequency of each syntactic respectively semantic category across the lexicon 
even without    of all lexicon entries all utterances could still be analyzed  so screen
does not break for missing word representations but attempts to provide an analysis as good
  

fiscreen  flat syntactic and semantic spoken language analysis

correct at syntactic output representation    
correct at semantic output representation    

table    overall syntactic and semantic accuracy of the running screen system for the
meeting corpus on the unknown test set after    of all lexicon entries were eliminated
as possible  as expected  table   shows a performance drop for the overall syntactic and
semantic accuracy  however  compared to the     and     performance for the complete
lexicon  see table    we still find that     of the syntactic output representations and    
of the semantic output representations are correct after eliminating    of all lexicon entries 
correct at syntactic output representation    
correct at semantic output representation    

table    overall syntactic and semantic accuracy of the running screen system for the
meeting corpus on the unknown test set after     of all lexicon entries were
eliminated
in another experiment we eliminated     of all syntactic and semantic lexicon entries 
in this case  the syntactic accuracy was still     and the semantic accuracy was     
eliminating     of the lexicon led to a syntactic accuracy reduction of only        
versus      and a semantic accuracy reduction of         versus       in general we
see that in all our experiments the percentage of accuracy reduction was much less than
the percentage of eliminated lexicon entries demonstrating screen s robustness for working
with an incomplete lexicon 

    comparison with the results in a new different domain

in order to compare the performance of our techniques  we will also show results from
experiments with a different spoken regensburg train corpus  our intention cannot be to
describe the experiments in this domain at the same level of detail as we have done for our
blaubeuren meeting corpus in this paper  however  we will provide a summary in order
to provide a point of reference and comparison for our experiments on the meeting corpus 
this comparison serves as another additional possibility to judge our results for the meeting
corpus 
as a different domain we chose     dialog turns at a railway counter  people ask
questions and receive answers about train connections  a typical utterance is   yes i need
eh a a sleeping car pause from pause regensburg to hamburg   we used exactly the
same screen communication architecture to process spoken utterances from this domain 
the same architecture was used      of the dialog turns was used for training      for
  

fiwermter   weber

testing on unseen unknown utterances  for syntactic processing  we even used exactly the
same network structure  since we did not expect much syntactic differences between the
two domains  only for semantic processing we retrained the semantic networks  different
categories had to be used for semantic classification  in particular for actions  while actions
about meetings  e g   visit  meet  were predominant in the meeting corpus  actions about
selecting connections  e g   choose  select  were important in the train corpus  wermter  
weber      b   just to give the reader an impression of the portability of screen  we
would estimate that     of the original human effort  system architecture  networks  could
be used in this new domain  most of the remaining     were needed for the necessary new
semantic tagging and training in the new domain 
module

accuracy on test set
bas syn dis
   
bas sem dis
   
abs syn cat
   
abs sem cat
   
phrase start
   
word error
   
phrase error
   

table    performance of the individual networks on the test set in the train corpus
table   shows the performance on the test set in the train corpus  if we compare our
results in the meeting corpus  table    with these results in the train corpus we see in
particular that the abstract syntactic processing is almost the same in the meeting corpus
     in table   compared to     in table    but the abstract semantic processing is better
in the meeting corpus      in table   compared to     in table     other modules dealing
with explicit robustness for repairs  phrase start  word repair errors  phrase repair errors 
show almost the same performance      vs          vs          vs      
correct at syntactic output representation    
correct at semantic output representation    

table     overall syntactic and semantic accuracy of the running screen system on the
unknown test set of a different train corpus
as a comparison we summarize here the overall performance for this different train
domain  table    shows that screen has about the same syntactic performance in the two
domains  compare with table     so in this different domain we can essentially confirm our
previous results for syntactic processing performance      vs        however  semantic
processing appears to be harder in the train domain since the performance of     is lower
than the     in the meeting domain  however  semantic processing  semantic tagging or
semantic classification is often found to be much harder than syntactic processing in general 
  

fiscreen  flat syntactic and semantic spoken language analysis

so that the difference is still within the range of usual performance differences in syntax and
semantics  since semantic categories like agents  locations  and time expressions are about
the same in these two domains the more dicult action categorization is mainly responsible
for this difference in semantic performance between the two domains 
in general the transfer from one domain to another only requires a limited amount of
hand modeling  of course  syntactic and semantic categories have to be specified for the
lexicon and the transcripts  these syntactically or semantically tagged transcript sentences
are the direct basis for generating the training sets for the networks  generating these
trainings sets is the main manual effort while transferring the system to a new domain 
after the generation of the training sets has been performed the training of the networks
can proceed automatically  the training of a typical single recurrent network takes in the
order of a few hours  so much less manual work is required than for transferring a standard
symbolic parser to a new domain and generating a new syntactic and semantic grammar 

    an illustrative comparison argument based on a symbolic parser
we have made the point that screen s learned at representations are more robust than
hand coded deeply structured representations  here we would like to elaborate this point
with a compelling illustrative argument  consider different variations of sentence hypotheses
from a speech recognizer in figure        a correct sentence hypothesis   am sechsten
april bin ich auer hause    on  th april am i out of home   and    a partially incorrect
  input  am sechsten april bin ich auer hause
 on  th april am i out of home 
  output 
s
pp

  
vp

np

np

ng

ng

r

pp

v

np

adjg
n

u

adj
am on 

sechsten  th april april 

r

ng
n

bin am 

  input  am sechsten april ich ich auer haus
 on  th april i i out of home 
  output nil  no analysis possible 

ich i 

auer out of  hause home 

  

figure     two sentence hypotheses from a speech recognizer  the first hypothesis can
be analyzed  the second partially incorrect hypothesis cannot be analyzed
anymore by the symbolic parser 
  

fiwermter   weber

sentence hypothesis   am sechsten april ich ich auer hause    on  th april i i out of
home    focusing on the syntactic analysis  we used an existing chart parser and an existing
grammar which had been used extensively for other real world parsing up to the sentence
level  wermter         the only necessary significant adaptation was the addition of a rule
n g   u for pronouns  which had not been part of the original grammar  this rule states
that a pronoun u  e g    i   can be a noun group  ng  
if we run the first sentence hypothesis through the symbolic context free parser we receive the desired syntactic analysis shown in figure     but if we run the second slightly
incorrect sentence hypothesis through the parser we do not receive any analysis  the syntactic category abbreviations in figure    are used in the same manner as throughout the
paper  see table       furthermore and as usual   s  stands for sentence   adjg  for adjective group   np  for complex nominal phrase   vp  for verb phrase  the literal english
translations are shown in brackets  
the reason why the second sentence hypothesis could not be parsed by the context free
chart parser was that the speech recognizer generated incorrect output  there is no verb
in the second sentence hypothesis and there is an additional pronoun  i   such mistakes
occur rather frequently based on the imperfectness of current speech recognition technology 
of course one could argue that the grammar should be relaxed and made more exible to
deal with such mistakes  however  the more rules for fault detection are integrated into
the grammar or the parser the more complicated the grammar or the parser  even more
important  it is impossible to predict all possible mistakes and integrate them into a symbolic
context free grammar  finally  relaxing the grammar for dealing with mistakes by using
explicit specific rules also might lead to other additional mistakes because the grammar now
has to be extremely underspecified 
as we have shown  for instance in figure     screen does not have problems dealing with
such speech recognizer variations and mistakes  the main difference between a standard
context free symbolic chart parser analysis and screen s analysis is that screen has learned
to provide a at analysis under noisy conditions but the context free parser has been handcoded to provide a more structural analysis  it should be emphasized here that we do
not make an argument against structural representations per se and in general  the more
structure that can be provided the better  particularly for tasks which require structured
world knowledge  however  if robustness is a major concern  as it is for lower syntactic and
semantic spoken language analysis  a learned at analysis provides more robustness 

    comparisons with related hybrid systems
recently  connectionist networks have received a lot of attention as computational learning
mechanisms for written language processing  reilly   sharkey        miikkulainen       
feldman        barnden   holyoak        wermter         in this paper however  we have
focused on the examination of hybrid connectionist techniques for spoken language processing  in most previous approaches to speech language processing processing was often
sequential  that is  one module like the speech recognizer or the syntactic analyzer completed its work before the next module like a semantic analyzer started to work  in contrast 
screen works incrementally which allows the system     to have modules running in par  

fiscreen  flat syntactic and semantic spoken language analysis

allel      to integrate knowledge sources very early  and     to compute the analysis more
similar to humans since humans start to process sentences before they may be completed 
we will now compare our approach to related work and systems  a head to head comparison with a different system is dicult based on different computer environments and
whether systems can be accessed and adapted easily for the same input  furthermore 
different systems are typically used for different purposes with different language corpora 
grammars  rules  etc  however  we have made an extensive effort for a fair conceptual
comparison 
parsec  jain        is a hybrid connectionist system which is embedded in a larger
speech translation effort janus  waibel et al          the input for parsec is sentences 
the output is case role representations  the system consists of several connectionist modules
with associated symbolic transformation rules for providing transformations suggested by
the connectionist networks  while it is parsec s philosophy to use connectionist networks
for triggering symbolic transformations  screen uses connectionist networks for the transformations themselves  it is screen s philosophy to use connectionist networks wherever
possible and symbolic rules only where they are necessary 
we found symbolic processing particularly useful for simple known tests  like lexical
equality  or for complex control tasks of the whole system  when does a module communicate to which other module   much of the actual transformational work can be done
by trained connectionist networks  this is in contrast to the design philosophy in parsec
where connectionist modules provide control knowledge which transformation should be
performed  then the selected transformation is actually performed by a symbolic procedure  so screen uses connectionist modules for transformations and a symbolic control 
while parsec uses connectionist modules for control and symbolic procedures for the transformations 
different from screen  parsec receives sentence hypotheses either as sentence transcripts or as n best hypotheses from the janus system  our approach receives incremental
word hypotheses which are used in the speech construction part to build sentence hypotheses  this part is also used to prune the hypothesis space and to determine the best sentence
hypotheses  so during the at analysis in screen the semantic and syntactic plausibilities
of a partial sentence hypothesis can still inuence which partial sentence hypotheses are
processed 
for parsec and for screen a modular architecture was tested which has the advantage
that each connectionist module has to learn a relatively easy subtask  in contrast to the
development of parsec it is our experience that modularity requires less training time 
furthermore  some modules in screen are able to work independently from each other
and in parallel  in addition to syntactic and semantic knowledge  parsec can make use of
prosodic knowledge while screen currently does not use prosodic hints  on the other hand 
screen also contains modules for learning dialog act assignment while such modules are
currently not part of parsec  learning dialog act processing is important for determining
the intended meaning of an utterance  wermter   lochel        
recent further extensions based on parsec provide more structure and use annotated
linguistic features  bu et al          the authors state that they  implemented  based
on parsec  a connectionist system  which should approximate a shift reduce parser  this
connectionist shift reduce parser substantially differs from the original parsec architecture 
  

fiwermter   weber

we will refer to it as the  parsec extension   this parsec extension labels a complete
sentence with its first level categories  these first level categories are input again to the
same network in order to provide second level categories for the complete sentence and so
on  until at the highest level the sentence symbol can be added 
using this recursion step the parsec extension can provide deeper and more structural
interpretations than screen currently does  however  this recursion step and the construction of the structure also have their price  first  labels like np for a noun phrase have to be
defined as lexical items in the lexicon  second  and more important  the complete utterance
is labeled with the n th level categories before processing with the n   th level categories
starts  therefore several parses  e g     for the utterance  his big brother loved himself  
through the utterance are necessary  this means that this recent parsec extension is more
powerful than screen and the original parsec system by jain with respect to the opportunity to provide deeper and more structural interpretations  however  at the same time this
parsec extension looses the possibility to process utterances in an incremental manner 
however  incrementality is a very important property in spoken language processing and
in screen  besides the fact that humans process language in an incremental left to right
manner  this also allows screen to prune the search space of incoming word hypotheses
very early 
comparing parsec and screen  parsec aims more at supporting symbolic rules by using symbolic transformations  triggered by connectionist networks  and by integrating linguistic features  currently  the linguistic features in the recent parsec extension  bu et al  
      provide more structural and morphological knowledge than screen does  therefore 
currently it appears to be easier to integrate the parsec extension into larger systems of
high level linguistic processing  in fact  parsec has been used in the context of the janus
framework  on the other hand  screen aims more at robust and incremental processing
by using a word hypothesis space  specific repair modules  and more at representations 
in particular  screen emphasizes more the robustness of spoken language processing  since
it contains explicit repair mechanisms and implicit robustness  explicit robustness covers
often occurring errors  interjections  pauses  word and phrase repairs  in explicit modules 
while other less predictable types of errors are only supported by the implicit similaritybased robustness from the connectionist networks themselves  in general  the representations generated by the extension of parsec provide better support for deeper structures
than screen  but screen provides better support for incremental robust processing  in a
more recent extension based on parsec called feaspar  the overall parsing performance was
a syntactic and semantic feature accuracy of        although additional improvements can
be shown using subsequent search techniques on the parsing results  we did not consider
such subsequent search techniques for better parses since they would violate incremental
processing  bu         without using subsequent search techniques screen reaches an
overall semantic and syntactic accuracy between     and     as shown in table    however
it should be pointed out  that screen and feaspar use different input sentences  features
and architectures 
besides parsec also the berp and trains systems focus on hybrid spoken language processing  berp  berkeley restaurant project  is a current project which employs multiple
different representations for speech language analysis  wooters        jurafsky et al        
    b   the task of berp is to act as a knowledge consultant for giving advice about choos  

fiscreen  flat syntactic and semantic spoken language analysis

ing restaurants  there are different components in berp  the feature extractor receives
digitized acoustic data and extracts features  these features are used in the connectionist phonetic probability estimation  the output of this connectionist feedforward network
is used in a viterbi decoder which uses a multiple pronunciation lexicon and different language models  e g  bigram  hand coded grammar rules   the output of the decoder are word
strings which are transformed into database queries by a stochastic chart parser  finally  a
dialog manager controls the dialog with the user and can ask questions 
berp and screen have in common the ability to deal with errors from humans and from
the speech recognizer as well as a relatively at analysis  however  for reaching this robustness in berp a probabilistic chart parser is used to compute all possible fragments at first 
then  an additional fragment combination algorithm is used for combining these fragments
so that they cover the greatest number of input words  different from this sequential process
of first computing all fragments of an utterance and then combining the fragments  screen
uses incremental processing and desirably provides the best possible interpretation  in this
sense screen s language analysis is weaker but more general  screen s analysis will never
break and produce the best possible interpretation for all noisy utterances  this strategy
may be particularly useful for incremental translation  on the other hand  berp s language
analysis is stronger but more restricted  berp s analysis may stop at the fragment level if
there are contradictory fragments  this strategy may be particularly useful for question
answering where additional world knowledge is necessary and available 
trains is a related spoken language project for building a planning assistant who can
reason about time  actions  and events  allen        allen et al          because of this
goal of building a general framework for natural language processing and planning for
train scheduling  trains needs a lot of commonsense knowledge  in the scenario  a person
interacts with the system in order to find solutions for train scheduling in a cooperative
manner  the person is assumed to know more about the goals of the scheduling while
the system is supposed to have the details of the domain  the utterance of a person is
parsed by a syntactic and semantic parser  further linguistic reasoning is completed by
modules for scoping and reference resolution  after the linguistic reasoning  conversation
acts are determined by a system dialog manager and responses are generated based on a
template driven natural language generator  performance phenomena in spoken language
like repairs and false starts can currently be dealt with already  heeman   allen      b 
    a   compared to screen  the trains project focuses more on processing spoken
language at an in depth planning level  while screen uses primarily a at connectionist
language analysis  trains uses a chart parser with a generalized phrase structure grammar 

   discussion
first we will focus on what has been learned for processing spoken language processing 
when we started the screen project  it was not predetermined whether a deep analysis
or a at screening analysis would be particularly appropriate for robust analysis of spoken
sentences  a deep analysis with highly structured representations is less appropriate since
the unpredictable faulty variations in spoken language limit the usefulness of deep structured knowledge representations much more than it is the case for written language  deep
interpretations and very structured representations   as for instance possible with hpsg
  

fiwermter   weber

grammars for text processing   make a great deal of assumptions and predictions which do
not hold for faulty spoken language  furthermore  we have learned that for generating a
semantic and syntactic representation we do not even need to use a deep interpretation for
certain tasks  for instance  for translating between two languages it is not necessary to
disambiguate all prepositional phrase attachment ambiguities since during the process of
translation these disambiguations may get ambiguous again in the target language 
however  we use some structure at the level of words and phrases for syntax and semantics respectively  we learned that a single at semantics level rather than the four at
syntax and semantics levels is not sucient since syntax is necessary for detecting phrase
boundaries  one could argue that one syntactic abstract phrase representation and one
abstract semantic phrase representation may be enough  however  we found that the basic
syntactic and semantic representations at the word level make the task easier for the subsequent abstract analysis at the phrase level  furthermore  the basic syntactic and semantic
representations are necessary for other tasks as well  for instance for the judgment of the
plausibility of a sequence of syntactic and semantic categories  this plausibility is used as
a filter for finding good word hypothesis sequences  therefore  we argue that for processing
faulty spoken language   for a task like sentence translation or question answering   we
need much less structured representations as are typically used in well known parsers but
we need more structured representations than those of a single level tagger 
in some of our previous work we had made early experiences with related connectionist
networks for analyzing text phrases  moving from analyzing text phrases to analyzing unrestricted spoken utterances  there are tremendous differences in the two tasks  we found that
the phrase oriented at analysis used in scan  wermter        is advantageous in principle
for spoken language analysis and the phrase oriented analysis is common to learning text
and speech processing  however  we learned that spoken language analysis needs a much
more sophisticated architecture  in particular  since spoken language contains many unpredictable errors and variations  fault tolerance and robustness are much more important 
connectionist networks have an inherent implicit robustness based on their similarity based
processing in gradual numerical representations  in addition  we found that for some classes
of relatively often occurring mistakes  there should be some explicit robustness provided by
machinery for interjections  word and phrase repairs  furthermore  the architecture has
to consider the processing of a potentially large number of competing word hypothesis
sequences rather than a single sentence or phrase for text processing 
now  we will focus on what has been learned about connectionist and hybrid architectures  in the beginning we did not predetermine whether connectionist methods would
be particularly useful for control or for individual modules or for both  however  during the development of the screen system it became clear that for the general task of
spoken language understanding  individual subtasks like syntactic analysis had to be very
fault tolerant because of the  noise  in spoken language  due both to humans and to speechrecognizers as well  especially unforeseeable variations often occur in spontaneously spoken
language and cannot be predefined well in advance as symbolic rules in a general manner 
this fault tolerance at the task level could be supported particularly well by the inherent
fault tolerance of connectionist networks for individual tasks and the support of inductive
learning algorithms  so we learned that for a at robust understanding of spoken language
connectionist networks are particularly effective within individual subtasks 
  

fiscreen  flat syntactic and semantic spoken language analysis

there has been quite a lot of work on control in connectionist networks  however 
in many cases these approaches have concentrated on control in single networks  only
recently there has been more work on control in modular architectures  sumida       
jacobs et al       b  jain        jordan   jacobs        miikkulainen         for instance 
in the approach by jacobs and jordan  jacobs et al       b  jordan   jacobs         task
knowledge and control knowledge are learned both  task knowledge is learned in individual
task networks  and higher control networks are responsible for learning when a single task
network is responsible for producing the output  originally it was an open question whether
a connectionist control would be possible for processing spoken language  while automatic
modular task decomposition  jacobs et al       a  can be done for simple forms of function
approximation  more complex problems like understanding spoken language in real world
environments still need designer based modular task decomposition for the necessary tasks 
we learned that connectionist control in an architecture with a lot of modules and
subtasks currently seems to be beyond the capabilities of current connectionist networks 
it has been shown that connectionist control is possible for a limited number of connectionist modules  miikkulainen        jain         for instance miikkulainen shows that
a connectionist segmenter and a connectionist stack can control a parser to analyze embedded clauses  however  the communication paths still have to be very restricted within
these three modules  especially for a real world system for spoken language understanding from speech  over syntax  semantics to dialog processing for translation it is extremely
dicult to learn to coordinate the different activities  especially for a large parallel stream
of word hypothesis sequences  we believe that it may be possible in the future  however
currently connectionist control in screen is restricted to the detection of certain hesitations
phenomena like corrections 
considering at screening analysis of spoken language and hybrid connectionist techniques together  we have developed and followed a general guideline  or design philosophy  
of using as little knowledge as necessary while getting as far as possible using connectionist
networks wherever possible and symbolic representations where necessary  this guideline
led us to     a at but robust representation of spoken language analysis and to     the
use of hybrid connectionist techniques which support the task by the choice of the possibly
most appropriate knowledge structure  many hybrid systems contain just a small portion
of connectionist representations in addition to many other modules  e g  berp  wooters 
      jurafsky et al             b   janus  waibel et al          trains  allen        allen
et al          in contrast  most of the important subtasks in screen are performed directly
by many connectionist networks 
furthermore  we have learned that at syntactic and semantic representations could give
surprisingly good training and test results when trained and tested with a medium corpus
of about      words in the     dialog turns  these good results are mostly due to the
learned internal weight representation and the local context which adds sequentiality to the
category assignments  without the internal weight representation of the preceding context
the syntactic and semantic categorization does not perform equally well  so the choice of
recurrent networks is crucial for many sequential category assignments  therefore these
networks and techniques hold potential especially for such medium size domains where a
restricted amount of training material is available  while statistical techniques are often
  

fiwermter   weber

used for very large data sets  but do not work well for medium data sets  the connectionist
techniques we used work well for medium size domains 
the used techniques can be ported to different domains and be used for different purposes  even if different sets of categories would have to be used the learning networks are
able to extract these syntactic regularities automatically  besides the domain of arranging
business meetings we have also ported screen to the domain of interactions at a railway
counter with comparable syntactic and semantic results  these two domains differed primarily in their semantic categories  while the syntactic categories  and networks  of screen
could be used directly 
screen has the potential for scaling up  in fact  based on the imperfect output of a
speech recognizer  several thousand sentence hypotheses have already been processed  if
new words are to be processed  their syntactic and semantic basic categories are simply
entered into the lexicon  the structure of individual networks does not change  new units
do not have to be added and therefore the networks do not have to be retrained 
the amount of hand coding is restricted primarily to the symbolic control of the module
interaction and to the labeling of the training material for the individual networks  when
we changed the domain to railway counter interactions  we could use the identical control 
as well as the syntactic networks  only the semantic networks had to be retrained due to
the different domain 
so far we have focused on supervised learning in simple recurrent networks and feedforward networks  supervised learning still requires a training set and some manual labeling
work still has to be done  although especially for medium size corpora labeling examples
is easier than for instance designing complete rule bases it would be nice to automate the
knowledge acquisition even further  currently we plan to build a more sophisticated lexicon component which will provide support for automatic lexicon design  riloff        and
dynamic lexicon entry determination using local context  miikkulainen        
furthermore  screen could be expanded at the speech construction and evaluation
part  the syntactic and semantic hypotheses could be used for more interaction with the
speech recognizer  currently syntactic and semantic hypotheses from the speech evaluation
part are used to exclude unlikely word hypothesis sequences from the language modules 
however  these hypotheses by the connectionist networks for syntax and semantics   in
particular the modules of basic syntactic and semantic category prediction   could also
be used directly into the process of recognition in the future in order to provide more
syntactic and semantic feedback to the speech recognizer at an early stage  besides syntax
and semantics  cue phrases  stress and intonation could provide additional knowledge for
speech language processing  hirschberg        gupta   touretzky         these issues will
be additional major efforts for the future 

   conclusions
we have described the underlying principles  the implemented architecture  and the evaluation of a new screening approach for learning the analysis of spoken language  this work
makes a number of original contributions to the fields of artificial intelligence and advances
the state of the art in several perspectives  from the perspective of symbolic and connectionist design we argue for a hybrid solution  where connectionist networks are used
  

fiscreen  flat syntactic and semantic spoken language analysis

wherever they are useful but symbolic processing is used for control and higher level analysis  furthermore  we have shown that recurrent networks provided better syntactic and
semantic prediction results than     grams  from the perspective of connectionist networks
alone  we have demonstrated that connectionist networks can in fact be used in real world
spoken language analysis  from the perspective of natural language processing we argue
that hybrid system design is advantageous for integrating speech and language since lower
speech related processing is supported by fault tolerant learning in connectionist networks
and higher processing and control is supported by symbolic knowledge structures  in general  these properties support parallel rather than sequential  learned rather than coded 
fault tolerant rather than strict processing of spoken language 
the main result of this paper is that learned at representations support robust processing of spoken language better than in depth structured representations and that connectionist networks provide a fault tolerance to reach this robustness  due to the noise in
spontaneous language  interjections  pauses  repairs  repetitions  false starts  ungrammaticalities  and also additional false word hypotheses by a speech recognizer  complex structured possibly recursive representations often cannot be computed using standard symbolic
representations like context free parsers  on the other hand  there are tasks like information
extraction from of spoken language which may not need an in depth structured representation  we believe our hybrid connectionist techniques have considerable potential for such
tasks  for instance for information extraction in restricted but noisy spoken language domains  while an in depth understanding like inferencing for story interpretation needs
complex structured representations  a shallow understanding for instance for information
extraction in noisy speech language environments will benefit from at  robust and learned
representations 

acknowledgements
this research was funded by the german federal ministry for research and technology
 bmbf  under grant    iv   a  and by the german research association  dfg  under
grant dfg ha           and grant dfg we           we would like to thank s  haack 
m  lochel  m  meurer  u  sauerland  and m  schrattenholzer for their work on screen  as
well as david bean  alexandra klein  steven minton  johanna moore  ellen riloff and five
anonymous referees for comments on earlier versions of this paper 

references
allen  j  f          the trains    parsing system  a user s manual  tech  rep  trains
tn       university of rochester  computer science department 
allen  j  f   schubert  l  k   ferguson  g   heeman  p   hwang  c  h   kato  t   light 
m   martin  n  g   miller  b  w   poesio  m     traum  d  r          the trains
project  a case study in building a conversational planning agent  journal of experimental and theoretical ai          
  

fiwermter   weber

barnden  j  a     holyoak  k  j   eds            advances in connectionist and neural
computation theory  vol      ablex publishing corporation 
bu  f  d          feaspar   a feature structure parser learning to parse spontaneous
speech  ph d  thesis  university of karlsruhe  karlsruhe  frg 
bu  f  d   polzin  t  s     waibel  a          learning complex output representations
in connectionist parsing of spoken language  in proceedings of the international conference on acoustics  speech and signal processing  vol     pp           adelaide 
australia 
charniak  e          statistical language learning  mit press  cambridge  ma 
dyer  m  g          in depth understanding  a computer model of integrated processing
for narrative comprehension  mit press  cambridge  ma 
elman  j  l          finding structure in time  cognitive science                  
faisal  k  a     kwasny  s  c          design of a hybrid deterministic parser  in proceedings of the    th international conference on computational linguistics  pp        
helsinki  finnland 
feldman  j  a          structured connectionist models and language learning  artificial
intelligence review                 
geutner  p   suhm  b   bu  f  d   kemp  t   mayfield  l   mcnair  a  e   rogina  i  
schultz  t   sloboda  t   ward  w   woszczyna  m     waibel  a          integrating
different learning approaches into a multilingual spoken language translation system 
in wermter  s   riloff  e     scheler  g   eds    connectionist  statistical and symbolic approaches to learning for natural language processing  pp           springer 
heidelberg 
gupta  p     touretzky  d  s          connectionist models and linguistic theory  investigations of stress systems in language  cognitive science               
hauenstein  a     weber  h  h          an investigation of tightly coupled time synchronous
speech language interfaces using a unification grammar  in proceedings of the   th
national conference on artificial intelligence workshop on the integration of natural
language and speech processing  pp         seattle  washington 
heeman  p  a     allen  j       a   detecting and correcting speech repairs  in proceedings
of the   nd annual meeting of the association for computational linguistics  pp      
     las cruces  nm 
heeman  p  a     allen  j       b   tagging speech repairs  in proceedings of the human
language technology workshop  pp           plainsboro  nj 
hendler  j  a          marker passing over microfeatures  towards a hybrid symbolic connectionist model  cognitive science                 
  

fiscreen  flat syntactic and semantic spoken language analysis

hirschberg  j          pitch accent in context  predicting intonational prominence from
text  artificial intelligence              
jacobs  r  a   jordan  m  i     barto  a  g       a   task decomposition through competition in a modular connectionist architecture  the what and where vision tasks 
cognitive science              
jacobs  r  a   jordan  m  i   nowlan  s  j     hinton  g  e       b   adaptive mixtures
of local experts  neural computation               
jain  a  n          parsing complex sentences with structured connectionist networks 
neural computation                 
jordan  m  i     jacobs  r  a          hierarchies of adaptive experts  in moody  j  e  
hanson  s  j     lippmann  r  r   eds    advances in neural information processing
systems    pp           morgan kaufmann  san mateo  ca 
jurafsky  d   wooters  c   tajchman  g   segal  j   stolcke  a   fosler  e     morgan 
n       a   the berkeley restaurant project  in proceedings of the international
conference on speech and language processing  pp             yokohama  japan 
jurafsky  d   wooters  c   tajchman  g   segal  j   stolcke  a     morgan  n       b   integrating experimental models of syntax  phonology  and accent dialect in a speech recognizer  an investigation of tightly coupled time synchronous speech  in proceedings
of the   th national conference on artificial intelligence workshop on the integration
of natural language and speech processing  pp           seattle  washington 
medsker  l  r          hybrid neural network and expert systems  kluwer academic
publishers  boston 
mellish  c  s          some chart based techniques for parsing ill formed input  in proceedings of the   th annual meeting of the association for computational linguistics  pp 
         vancouver  canada 
menzel  w          parsing of spoken language under time constraints  in cohn  a  g 
 ed    proceedings of the   th european conference on artificial intelligence  pp      
     amsterdam 
miikkulainen  r          subsymbolic natural language processing  an integrated model of
scripts  lexicon and memory  mit press  cambridge  ma 
miikkulainen  r          subsymbolic case role analysis of sentences with embedded clauses 
cognitive science            
nakatani  c     hirschberg  j          a speech first model for repair detection and correction  in proceedings of the   st annual meeting of the association for computational
linguistics  pp        columbus  ohio 
reilly  r  g     sharkey  n  e   eds            connectionist approaches to natural language processing  lawrence erlbaum associates  hillsdale  nj 
  

fiwermter   weber

riloff  e          automatically constructing a dictionary for information extraction tasks 
in proceedings of the   th national conference on artificial intelligence  pp          
washington  dc 
rumelhart  d  e   hinton  g  e     williams  r  j          learning internal representations by error propagation  in rumelhart  d  e   mcclelland  j  l     the pdp
research group  eds    parallel distributed processing  vol      pp           mit press 
cambridge  ma 
sauerland  u          konzeption und implementierung einer speech language
schnittstelle  master s thesis  university of hamburg  computer science department 
hamburg  frg 
sumida  r  a          dynamic inferencing in parallel distributed semantic networks  in
proceedings of the   th annual meeting of the cognitive science society  pp          
boston  chicago 
sun  r          integrating rules and connectionism for robust common sense reasoning 
wiley and sons  new york 
von hahn  w     pyka  c          system architectures for speech understanding and
language processing  in heyer  g     haugeneder  h   eds    applied linguistics 
wiesbaden 
waibel  a   jain  a  n   mcnair  a   tebelskis  j   osterholtz  l   saito  h   schmidbauer 
o   sloboda  t     woszczyna  m          janus  speech to speech translation using
connectionist and non connectionist techniques  in moody  j  e   hanson  s  j    
lippmann  r  r   eds    advances in neural information processing systems    pp 
         morgan kaufmann  san mateo  ca 
ward  n          an approach to tightly coupled syntactic semantic processing for speech
understanding  in proceedings of the   th national conference on artificial intelligence workshop on the integration of natural language and speech processing  pp 
       seattle  washington 
weber  v     wermter  s          towards learning semantics of spontaneous dialog utterances in a hybrid framework  in hallam  j   ed    hybrid problems  hybrid solutions
  proceedings of the   th biennial conference on ai and cognitive science  pp 
         sheeld  uk 
weber  v     wermter  s          artificial neural networks for repairing language  in
proceedings of the  th international conference on neural networks and their applications  pp           marseille  fra 
wermter  s     weber  v       a   interactive spoken language processing in a hybrid
connectionist system  ieee computer   theme issue on interactive natural language
processing  july        
  

fiscreen  flat syntactic and semantic spoken language analysis

wermter  s          hybride symbolische und subsymbolische verarbeitung am beispiel
der sprachverarbeitung  in duwe  i   kurfe  f   paa  g     vogel  s   eds   
herbstschule konnektionismus und neuronale netze  gesellschaft fur mathematik und
datenverarbeitung  gmd   sankt augustin  frg 
wermter  s          hybrid connectionist natural language processing  chapman and
hall  thompson international  london  uk 
wermter  s     lochel  m          connectionist learning of at syntactic analysis for
speech language systems  in marinaro  m     morasso  p  g   eds    proceedings
of the international conference on artificial neural networks  vol     pp          
sorrento  italy 
wermter  s     lochel  m          learning dialog act processing  in proceedings of
the   th international conference on computational linguistics  pp           kopenhagen  denmark 
wermter  s     peters  u          learning incremental case assignment based on modular
connectionist knowledge sources  in werbos  p   szu  h     widrow  b   eds    proceedings of the world congress on neural networks  vol     pp           san diego 
ca 
wermter  s   riloff  e     scheler  g   eds            connectionist  statistical and symbolic
approaches to learning for natural language processing  springer  berlin 
wermter  s     weber  v       b   artificial neural networks for automatic knowledge acquisition in multiple real world language domains  in proceedings of the  th international conference on neural networks and their applications  pp           marseille 
fra 
wooters  c  c          lexical modeling in a speaker independent speech understanding
system  tech  rep  tr         international computer science institute  berkeley 
young  s  r   hauptmann  a  g   ward  w  h   smith  e     werner  p          high
level knowledge sources in usable speech recognition systems  communications of the
acm              

  

fi