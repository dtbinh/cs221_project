journal of artificial intelligence research                 

submitted        published     

connectionist theory refinement 
genetically searching the space of network topologies
david w  opitz

opitz cs umt edu

jude w  shavlik

shavlik cs wisc edu

department of computer science
university of montana
missoula  mt       usa

computer sciences department
university of wisconsin
     w  dayton st 
madison  wi       usa

abstract

an algorithm that learns from a set of examples should ideally be able to exploit the
available resources of  a  abundant computing power and  b  domain specific knowledge to
improve its ability to generalize  connectionist theory refinement systems  which use background knowledge to select a neural network s topology and initial weights  have proven to
be effective at exploiting domain specific knowledge  however  most do not exploit available computing power  this weakness occurs because they lack the ability to refine the
topology of the neural networks they produce  thereby limiting generalization  especially
when given impoverished domain theories  we present the regent algorithm which uses
 a  domain specific knowledge to help create an initial population of knowledge based neural networks and  b  genetic operators of crossover and mutation  specifically designed for
knowledge based networks  to continually search for better network topologies  experiments on three real world domains indicate that our new algorithm is able to significantly
increase generalization compared to a standard connectionist theory refinement system  as
well as our previous algorithm for growing knowledge based networks 

   introduction
many scientific and industrial problems can be better understood by learning from samples
of the task  for this reason  the machine learning and statistics communities devote considerable research effort to inductive learning algorithms  often  however  these learning
algorithms fail to capitalize on a number of potentially available resources  such as domainspecific knowledge or computing power  that can improve their ability to generalize  using
domain specific knowledge is desirable because inductive learners that start with an approximately correct theory can achieve improved  generalization   accuracy on examples not
seen during training  with significantly fewer training examples  ginsberg        ourston
  mooney        pazzani   kibler        towell   shavlik         making effective use of
available computing power is desirable because  for many applications  it is more important
to obtain concepts that generalize well than it is to induce concepts quickly  in this article  we present an algorithm  called regent  refining  with genetic evolution  network
topologies   that utilizes available computer time to extensively search for a neural network

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiopitz   shavlik

topology that best explains the training data while minimizing changes to a domain specific
theory 
inductive learning systems that utilize a set of approximately correct  domain specific
inference rules  called a domain theory  which describe what is currently known about
the domain  are called theory refinement systems  making use of this knowledge has been
shown to be important since these rules may contain insight not easily obtainable from
the current set of training examples  ourston   mooney        pazzani   kibler       
towell   shavlik         for most domains  an expert who created the theory is willing
to wait for weeks  or even months  if a learning system can produce an improved theory 
thus  given the rapid growth in computing power  we believe it is important for learning
techniques to be able to trade off the expense of large numbers of computing cycles for gains
in predictive accuracy  analogous to anytime planning techniques  dean   boddy        
we believe machine learning researchers should create better anytime learning algorithms 
such learning algorithms should produce a good concept quickly  then continue to search
concept space  reporting the new  best  concept whenever one is found 
we concentrate on connectionist theory refinement systems  since they have been shown
to frequently generalize better than many other inductive learning and theory refinement
systems  fu        lacher  hruska    kuncicky        towell         kbann  towell  
shavlik        is an example of such a connectionist system  it translates the provided
domain theory into a neural network  thereby determining the network s topology  and
then refines the reformulated rules using backpropagation  rumelhart  hinton    williams 
       however  kbann  and other connectionist theory refinement systems that do not
alter their network topologies  suffer when given impoverished domain theories   ones that
are missing rules needed to adequately learn the true concept  opitz   shavlik       
towell   shavlik         topgen  opitz   shavlik        is an improvement over these
systems  it heuristically searches through the space of possible network topologies by adding
hidden nodes to the neural representation of the domain theory  topgen showed statistically
significant improvements over kbann in several real world domains  opitz   shavlik        
however  in this paper we empirically show that topgen nevertheless suffers because it only
considers simple expansions of the kbann network 
to address this limitation  we broaden the types of topologies that topgen considers
by using genetic algorithms  gas   we choose gas for two reasons  first  gas have
been shown to be effective optimization techniques because of their ecient use of global
information  goldberg        holland        mitchell         second  gas have an inherent
quality which makes them suitable for anytime learning  in  off line  application mode
 dejong         gas simulate many alternatives and output the best alternative seen so
far 
our new algorithm  regent  proceeds by first trying to generate  from the domain
theory  a diversified initial population  it then produces new candidate networks via the
genetic operators of crossover and mutation  after which these networks are trained using
backpropagation  regent s crossover operator tries to maintain the rule structure of the
network  while its mutation operator adds nodes to a network by using the topgen algorithm  hence  our genetic operators are specialized for connectionist theory refinement 
experiments reported herein show that regent is better able to search for network topologies than topgen 
   

ficonnectionist theory refinement

the rest of the paper is organized as follows  in the next section  we briey argue for
the importance of effectively exploiting data  theory  and available computer time in the
learning process  we then review the kbann and topgen algorithms  we present the
details of our regent algorithm in section    this is followed by empirical results from
three human genome project domains  in section    we discuss these results  as well as
future work  we then review related work before concluding 

   using data  prior knowledge  and available cpu cycles
a system that learns from a set of labeled examples is called an inductive learner  alternately  a supervised  empirical  or similarity based learner   the output for each example
is provided by a teacher  and the set of labeled examples given to a learner is called the
training set  the task of inductive learning is to generate from the training set a concept
description that correctly predicts the output of all future examples  not just those from
the training set  many inductive learning algorithms have been previously studied  e g  
michalski        quinlan        rumelhart et al          these algorithms differ both
in their concept representation language  and in their method  or bias  for constructing a
concept within this language  these differences are important since they determine which
concepts a classifier will induce 
an alternative to the inductive learning paradigm is to build a concept description not
from a set of examples  but by querying experts in the field and directly assembling a set of
rules that describe the concept  i e   build an expert system  waterman         a problem
with building expert systems is that the theories derived from interviewing the experts tend
to be only approximately correct  thus  while the expert provided domain theory is usually
a good first approximation of the concept to be learned  inaccuracies are frequently exposed
during empirical testing 
theory refinement systems  ginsberg        ourston   mooney        pazzani   kibler 
      towell   shavlik        are systems that revise a theory on the basis of a collection of
examples  these systems try to improve the theory by making minimal repairs to the theory
to make it consistent with the training data  changes to the initial domain theory should
be kept to a minimum because the theory presumably contains useful information  even if
it is not completely correct  these hybrid learning systems are designed to learn from both
theory and data  and empirical tests have shown them to achieve high generalization with
significantly fewer examples than purely inductive learning techniques  ourston   mooney 
      pazzani   kibler        towell   shavlik         thus  an ideal inductive learning
system should be able to incorporate any background knowledge that is available in the
form of a domain theory to improve its ability to generalize 
as indicated earlier  available computer time is also an important resource since  a  computing power is rapidly increasing  and  b  for most problems an expert is willing to wait a
lengthy period for an improved concept  for these reasons  one should develop  anytime 
learning algorithms that can continually improve the quality of their answer over time  dean
and boddy        defined the criteria for an anytime algorithm to be   a  the algorithm can
be suspended and then resumed with minimal overhead   b  the algorithm can be stopped
at any time and return an answer  and  c  the algorithm must return answers that improve
   

fiopitz   shavlik

x
x

x

x

output

x x

x

x

x

input

figure    this is a classical regression example where a smooth function  the solid curve 
that does not fit all of the noisy data points  the x s  is probably a better predictor
than a high degree polynomial  the dashed curve  

over time  while these criteria were created for planning and scheduling algorithms  they
can apply to inductive learning algorithms as well  
most standard inductive learners  such as backpropagation  rumelhart et al         and
id   quinlan         are unable to continually improve their answers  at least until they
receive additional training examples   in fact  if run too long  these algorithms tend to
 overfit  the training set  holder         overfitting occurs when the learning algorithm
produces a concept that captures too much information about the training examples  and
not enough about the general characteristics of the domain as a whole  while these concepts
do a great job of classifying the training instances  they do a poor job of generalizing to
new examples   our ultimate measure of success  to help illustrate this point  consider
the typical regression case shown in figure    here  fitting noisy data with a high degree
polynomial is likely to lead to poor generalization 
the general framework we use for encouraging our algorithm to improve its answer
over time is quite simple  we spend our computer time considering many different possible
concept descriptions  scoring each possibility  and always keeping the description that scores
the best  our framework is anytime with respect to the scoring function  the scoring
function is only an approximate measure of generalization and is obviously still prone to the
problems of overfitting  thus there is no guarantee that generalization will monotonically
decrease over time  nevertheless  assuming an accurate scoring function  then as long as we
are considering a wide range of good possibilities  the quality of our best concept is likely
to improve for a longer period of time 
   our use of the term anytime learning differs from that of grefenstette and ramsey         they use it
to mean continuous learning in a changing environment 

   

ficonnectionist theory refinement

   review of kbann and topgen
the goal of this research is to exploit both prior knowledge and available computing cycles
to search for the neural network that is most likely to generalize the best  we proceed
by choosing  as an initial guess  the network defined by the kbann algorithm  we then
continually refine this topology to find the best network for our concept  before presenting
our new algorithm  regent   we give an overview of the kbann algorithm as well as our
initial approach of refining a kbann created network s topology  topgen  

    the kbann algorithm
kbann  towell   shavlik        works by translating a domain theory consisting of a set of

propositional rules directly into a neural network  see figure     figure  a shows a prologlike rule set that defines membership in category a  figure  b represents the hierarchical
structure of these rules  with solid lines representing necessary dependencies and dotted lines
representing prohibitory dependencies  figure  c represents the network kbann creates
from this translation  it sets the biases so that nodes representing disjuncts have an output
near   only when at least one of their high weighted antecedents is satisfied  while nodes
representing conjuncts must have all of their high weighted antecedents satisfied  i e   near
  for positive links and near   for negative links   otherwise activations are near    kbann
creates nodes b  and b  in figure  c to handle the two rules disjunctively defining b  the
thin lines in figure  c represent low weighted links that kbann adds to allow these rules
to add new antecedents during backpropagation training  following network initialization 
kbann uses the available training instances to refine the network links  refer to towell
       or towell and shavlik        for more details 
kbann has been successfully applied to several real world problems  such as the control
of a chemical plant  scott  shavlik    ray         protein folding  maclin   shavlik        
a

a   b  not c 
b   d  f  g 

b

a

b 

b   d  not f  i 
c   h  j  k 
d
 a 

b

c

e f g h i j k
 b 

d e

b 

f

g h i
 c 

c

j k

figure    kbann s translation of a knowledge base into a neural network  panel  a  shows
a sample propositional rule set in prolog  clocksin   mellish        notation 
panel  b  illustrates this rule set s corresponding and or dependency tree  and
panel  c  shows the resulting network created by kbann s translation 
   

fiopitz   shavlik

finding genes in a sequence of dna  opitz   shavlik        towell   shavlik         and
ecg patient monitoring  watrous  towell    glassman         in each case  kbann was
shown to produce improvements in generalization over standard neural networks for small
numbers of training examples  in fact  towell        favorably compared kbann with
a wide variety of algorithms  including purely symbolic theory refinement systems  on a
version of the promoter and splice junction tasks that we include as testbeds in section   
while training the kbann created network alters the antecedents of existing rules  it
does not have the capability of inducing new rules because it does not add any additional
hidden nodes during training  for instance  kbann is unable to add a third rule for
inferring b in figure   s example  to help illustrate this point  consider the following
example  assume that figure   s target concept consists of figure  a s domain theory plus
the rule 
b    not d  e  g 
although we trained the kbann network shown in figure  c with all possible examples of
this target concept  it was unable to completely learn the conditions under which a is true 
the topology of the kbann network must be modified in order to learn this new rule 
studies show  opitz   shavlik        towell        that while kbann is effective at
removing extraneous rules and antecedents in an expert provided domain theory  its generalization ability suffers when given  impoverished  domain theories   theories that are
missing rules or antecedents needed to adequately learn the true concept  an ideal connectionist theory refinement algorithm  therefore  should be able to dynamically expand the
topology of its network during training 

    the topgen algorithm

topgen  opitz   shavlik        addresses kbann s limitation by heuristically searching
through the space of possible expansions to a knowledge based neural network   a network
whose topology is determined by the direct mapping of the dependencies of a domain theory
 e g   a kbann network   topgen proceeds by first training the kbann network  then
placing it on a search queue  in each cycle  topgen takes the best network from the search
queue  estimates where errors occur in the network  adds new nodes in response to these
estimates  trains these new networks  then places them back on the queue  topgen judges
where errors occur in a network by using training examples to increment two counters for
each node  one for false negatives and one for false positives 
figure   illustrates the possible ways topgen can add nodes to one of its networks  in a
symbolic rule base that uses negation by failure  one can decrease false negatives by either
dropping antecedents from existing rules or adding new rules to the rule base  kbann
is effective at removing antecedents from existing rules  but is unable to add new rules 
therefore  topgen adds nodes  intended for decreasing false negatives  in a fashion that is
analogous to adding a new rule to the rule base  if the existing node is an or node  topgen
adds a new node as its child  see figure  a   and fully connects this new node to the input
nodes  when the existing node is an and node  topgen creates a new or node that is
the parent of the original and node and another new node that topgen fully connects to
the inputs  see figure  c   topgen moves the outgoing links of the original node  a in
figure  c  to become the outgoing links of the new or node 
   

ficonnectionist theory refinement

existing node

decrease false negatives

decrease false positives
   

   

   

new
node

a

a

new
node

a
b

b

c

or node

c

new
node

b

c

 a 

 b 
   

a

c

and node

a

new
node

a
b

   

new
node

   

b
b

c

c

 c 

new
node

 d 

figure    possible ways to add new nodes to a knowledge based neural network  arcs indicate and nodes   to decrease false negatives  we wish to broaden the applicability of the node  conversely  to decrease false positives  we wish to further
constrain the node 
in a symbolic rule base  one can decrease false positives by either adding antecedents
to existing rules or removing rules from the rule base  kbann can effectively remove
rules  but it is less effective at adding antecedents to rules and is unable to invent  i e  
constructively induce  michalski        new terms as antecedents  thus topgen adds new
nodes  intended to decrease false positives  in a fashion that is analogous to adding new
constructively induced antecedents to the network  figures  b and  d illustrates how this is
done  analogous to figures  a and  c explained above   refer to opitz and shavlik       
      for more details 
topgen showed statistically significant improvements over kbann in several real world
domains  and comparative experiments with a simpler approach to adding nodes verified
that new nodes must be added in an intelligent manner  opitz   shavlik         in this
article  however  we increase the number of networks topgen considers during its search
and show that the increase in generalization is primarily limited to the first few networks
considered  therefore  topgen is not so much an  anytime  algorithm  but rather is a first
step towards one  this is mostly due to the fact that topgen only considers larger networks that contain the original kbann network as subgraphs  however  as one increases the
number of networks considered  one should also increase the variety of networks considered
   

fiopitz   shavlik

during the search  broadening the range of networks considered during the search through
topology space is the major focus of this paper 

   the regent algorithm

our new algorithm  regent  tries to broaden the types of networks that topgen considers
with the use of gas  we view regent as having two phases   a  genetically searching
through topology space  and  b  training each network using backpropagation s gradient
descent method  regent uses the domain theory to aid in both phases  it uses the theory
to help guide its search through topology space and to give a good starting point in weight
space 
table   summarizes the regent algorithm  regent first sets aside a validation set
 from part of the training instances  for use in scoring the different networks  it then perturbs the kbann produced network to create an initial set of candidate networks  next 
regent trains these networks using backpropagation and places them into the population  in each cycle  regent creates new networks by crossing over and mutating networks
from the current population that are randomly picked proportional to their fitness  i e  
validation set correctness   it then trains these new networks and places them into the
population  as it searches  regent keeps the network that has the lowest validation set
error as the best concept seen so far  breaking ties by choosing the smaller network in an
application of occam s razor  a parallel version of regent trains many candidate networks at the same time using the condor system  litzkow  livny    mutka         which
runs jobs on idle workstations 
a diverse initial population will broaden the types of networks regent considers during
its search  however  since the domain theory may provide useful information that may not be
present in the training set  it is still desirable to use this theory when generating the initial
population  regent creates diversity around the domain theory by randomly perturbing
the kbann network at various nodes  regent perturbs a node by either deleting it  or by
adding new nodes to it in a manner analogous to one of topgen s four methods for adding

goal  search for the best network topology describing the domain theory and data 

   set aside a validation set from the training instances 
   perturb the kbann produced network in multiple ways to create initial networks  then train
these networks using backpropagation and place them into the population 
   loop forever 
 a  create new networks using the crossover and mutation operators 
 b  train these networks with backpropagation  score with the validation set  and place into
the population 
 c  if a new network is the network with the lowest validation set error seen so far  breaking
ties by preferring the smallest network   report it as the current best concept 

table    the regent algorithm 
   

ficonnectionist theory refinement

crossover two networks 

goal  crossover two networks to generate two new network topologies 
   divide each network s hidden nodes into sets a and b using dividenodes 

   set a forms one network  while set b forms another  each new network is created as follows 
 a  a network inherits weight w from its parent if nodes i and j either are also inherited
or are input or output nodes 
 b  link unconnected nodes between levels with near zero weights 
 c  adjust node biases to keep original and or or function of each node  see text for explanation  
ji

dividenodes 
goal 

divide the hidden nodes into sets a and b  while probabilistically maintaining each
network s rule structure 
while some hidden node is not assigned to set a or b 
 i  collect those unassigned hidden nodes whose output is linked only to either previouslyassigned nodes or output nodes 
 ii  if set a or set b is empty 
for each node collected in part  i   randomly assign it to set a or set b 

else

probabilistically add the nodes collected in part  i  to set a or set b  equation  
shows the probability of being assigned to set a  the probability of being assigned
to set b is one minus this value 

table    regent s method for crossing over networks 
nodes   should there happen to be multiple theories about a domain  all of them can be
used to seed the population  

    regent s crossover operator

regent crosses over two networks by first dividing the nodes in each parent network into

two sets  a and b  then combining the nodes in each set to form two new networks  i e   the
nodes in the two a sets form one network  while the nodes in the two b sets form another  
table   summarizes regent s method for crossover and figure   illustrates it with an
example  regent divides nodes  one level  at a time  starting at the level nearest the
output nodes  when considering a level  if either set a or set b is empty  it cycles through
each node in that level and randomly assigns it to either set  if neither set is empty  nodes
are probabilistically placed into a set  the following equation calculates the probability of
   although one can define level several different ways  we define a node s level as the longest path from it
to an output node 

   

fiopitz   shavlik

original
networks
crossed
over

output

output

input

input

output

output

input

input

resulting
networks
figure    regent s method for crossing over two networks  the hidden nodes in each
original network are divided into the sets a and b  the nodes in the two a sets
form one new network  while the nodes in the two b sets form another  grey lines
represent low weighted links that are added to fully connect neighboring levels 
a given node being assigned to set a 

pj a jwjij
prob node i   seta    p jw j   p jw j  
j  a ji
j  b ji

   

where j   a means node j is a member of set a and wji is the weight value from node i
to node j   the probability of belonging to set b is one minus this probability  with these
probabilities  regent tends to assign to the same set those nodes that are heavily linked
together  this helps to minimize the destruction of the rule structure of the crossed over
networks  since nodes belonging to the same syntactic rule are connected by heavily linked
weights  thus  regent s crossover operator produces new networks by crossing over rules 
rather than simply crossing over nodes 
regent must next decide how to connect the nodes of the newly created networks 
first  a new network inherits all weight values from its parents on links that  a  connect
two nodes that are both inherited by the new network   b  connect an inherited hidden
node and an input or output node  or  c  directly connect an input node to an output node 
it then adds randomly set  low weighted links between unconnected nodes on consecutive
levels 
finally  it adjusts the bias of all and or or nodes to help maintain their original function 
for instance  if regent removes a positively weighted incoming link for an and node 
it decrements the node s bias by subtracting the product of the link s magnitude and the
   

ficonnectionist theory refinement

average activation  over the set of training examples  entering that link  we do this because
the bias for an and node needs to be slightly less than the sum of the positive weights on
the incoming links  see towell and shavlik       for more details   regent increments the
bias for an or node by an analogous amount when it removes negatively weighted incoming
links  since the bias for an or node should be slightly greater than the sum of the negative
weights on the incoming links so that the node is inactive only when all incoming negatively
weighted linked nodes are active and all positively weighted linked nodes are inactive  

    regent s mutation operator

regent mutates networks by applying a variant of topgen  regent uses topgen s
method for incrementing the false negatives and false positives counters for each node  regent then adds nodes  based on the values of these counters  the same way topgen does 

since neural learning is effective at removing unwanted antecedents and rules from knns
 see section       regent only considers adding nodes  and not deleting them  during mutation  thus  this mutation operator adds diversity to a population  while still maintaining
a directed  heuristic search technique for choosing where to add nodes  this directedness is
necessary because we currently are unable to evaluate more than a few thousand possible
networks per day 

    additional details
regent adds newly trained networks to the population only if their validation set correctness is better than or equal to an existing member of the population  when regent

replaces a member  it replaces the member having the lowest correctness  ties are broken
by choosing the oldest member   other techniques  goldberg         such as replacing the
member nearest the new candidate network  can promote diverse populations  however  we
do not want to promote diversity at the expense of decreased generalization  as a future
research topic  we plan to investigate incorporating diversity promoting techniques once we
are able to consider tens of thousands of networks 
regent can be considered a lamarckian    genetic hillclimbing algorithm  ackley        
since it performs local optimizations on individuals  then passes the successful optimizations
on to offspring  the ability of individuals to learn can smooth the fitness landscape and
facilitate subsequent learning  thus  lamarckian learning can lead to a large increase in
learning speed and solution quality  ackley   littman        farmer   belin        

   experimental results
in this section  we test regent on three real world human genome project problems
that aid in locating genes in dna sequences  recognizing promoters  splice junctions  and
ribosome binding sites   in these domains  the input is a short segment of dna nucleotides
 about     elements long  and the task is learn to predict if this dna subsequence contains a
biologically important site  each domain is also accompanied by a domain theory generated
by a dna expert  m  noordewier  
   lamarckian evolution is a theory based on the inheritance of characteristics acquired during a lifetime 

   

fiopitz   shavlik

the promoter domain contains     positive examples      negative examples  and   
rules  the splice junction domain contains       examples distributed equally among three
classes  and    rules  finally  the ribosome binding sites  rbs  domain  contains    
positive examples        negative examples  and    rules   note that the promoter data set
and domain theory is a later version of the one that appears in towell         these domains
are available at the university of wisconsin machine learning  uw ml  site via the world
wide web  ftp   ftp cs wisc edu machine learning shavlik group datasets   or
anonymous ftp  ftp cs wisc edu  then machine learning shavlik group datasets  
we first directly compare regent with topgen and kbann  we then perform a
lesion study  on regent  in particular  we investigate the value of adding randomly
created networks to regent s initial population and examine the utility of regent s
genetic operators 

    experimental methodology

all results in this article are from ten fold cross validation runs  for each ten fold cross
validation the data set is first partitioned into ten equal sized sets  then each set is in turn
used as the test set while the classifier trains on the other nine sets  in each fold  regent is
run with a population size of     each network is trained using backpropagation  parameter
settings for the neural networks include a learning rate of       a momentum term of     
and the number of training epochs of     the first two are standard settings and while   
epochs may be fewer than typically found in the neural network literature  we set it at   
to help avoid overfitting  we set aside a validation set consisting of     of the training
examples for regent to use as its scoring function 

    generalization ability of regent

this section s experiments compare the test set accuracy  i e   generalization  of regent
with topgen s  figure   shows the test set error of kbann  topgen  and regent as they
search through the space of network topologies  the horizontal line in each graph results
from the kbann algorithm  we drew a horizontal line for the sake of visual comparison 
recall that kbann only considers a single network  the first point of each graph  after
one network is considered  is nearly the same for all three algorithms  since they all start
with the kbann network  however  topgen and regent differ slightly from kbann since
they must set aside part of the training set to score their candidate networks  notice that
topgen stops improving after considering    to    networks and that the generalization
ability of regent is better than topgen s after this point  the reason for the occasional
upward movements in figure   is due to the fact that a validation set  or any scoring
function  is an inexact estimate of the true generalization error  as are the results of the
ten fold cross validation  
figure   presents the test set error of topgen and regent after they each consider
    candidate topologies  the standard neural network results are from a fully connected 
single layer  feed forward neural network  for each fold  we trained    networks containing
up to     hidden nodes and used a validation set to choose the best network  our results
   a lesion study is one where components of an algorithm are individually disabled to ascertain their
contribution to the full algorithm s performance  kibler   langley        

   

ficonnectionist theory refinement

    

  
ribosome binding sites

  
kbann
topgen

  

regent

testset error

  

  

  

splice junctions

  

  

  
promoters
  

 

   

   

   

   

networks considered
figure    error rates on the three human genome problems 

   

   

fiopitz   shavlik

show kbann generalizes much better than the best of these standard networks  thus further
confirming kbann s effectiveness in generating good network topologies  while topgen is
able to improve on the kbann network  regent is able to significantly decrease the error
rate over both kbann and topgen   for benchmark purposes  regent has an error rate
of      from a ten fold cross validation on the full splice junction dataset of      examples
commonly used by machine learning researchers  
table   contains the number of hidden nodes in the final networks produced by kbann 
topgen  and regent  the results demonstrate that regent produces networks that are
larger than both kbann s and topgen s networks  even though topgen only adds nodes
during its search   while regent s networks are larger  it does not necessarily mean that
they are more  complex   we inspected sample networks and found that there are large
portions of the network that are either not used  e g   their weights are insignificantly small 
or are functional duplications of other groups of hidden nodes 
one could prune weights and nodes during regent s search  however  such pruning can
prematurely reduce the variety of structures available for recombination during crossover
 koza         real life organisms  for instance  have superuous dna that are believed
to enhance the rate of evolution  watson  hopkins  roberts  argetsinger steitz    weiner 
       however  while pruning network size during genetic search may be unwise  one
could prune regent s final network using  say  hassibi and stork s        optimal brain
surgeon algorithm  this post pruning process may increase the future classification speed
of the network  as well as increase its comprehensibility and possibly its accuracy 

    lesion study of regent
in this section  we describe a lesion study we performed on regent  since a single run
of regent takes about four cpu days to consider     networks  a single ten fold cross
     
    

    

    

testset error

    
  

    
    

  

         

key

    

standard nn

    

    

    

kbann
topgen

  

regent
  

rbs

splice junctions

promoters

figure    test set error rates after topgen and regent each consider     networks  pairwise  one tailed t tests indicate that regent differs from standard nn  kbann 
and topgen at the     confidence level on all three problems 
   

ficonnectionist theory refinement

domain
kbann topgen
regent
rbs
  
                      
splice junction
  
                      
promoters
  
                      
table    number of hidden nodes in the networks produced by kbann  topgen  and
regent  the columns show the mean number of hidden nodes found within
these networks  standard deviations are contained within parentheses  we do not
report standard deviations for kbann since it uses only one network 

validation takes  a minimum of     cpu days  therefore  given the inherent similarity
of investigating various aspects of regent over multiple datasets  it is not feasible to
run all experiments in this section until a     confidence level is reached in all cases
 assuming that such a level actually exists   nonetheless  these results convey important
information about various components of regent  and  as shown in the previous section 
the complete regent algorithm does generate statistically significant improvements over
existing algorithms 
      including non knns in regent s population

the correct theory may be quite different from the initial domain theory  thus  in this
section we investigate whether one should include  in the initial population of networks 
a variety of networks not obtained directly from the domain theory  currently  regent
creates its initial population by always perturbing the kbann network  to include networks
that are not obtained from the domain theory  we first randomly pick the number of hidden
nodes to include in a network  then randomly create all of the hidden nodes in this network 
we do this by adding new nodes to a randomly selected output or hidden node using one
of topgen s four methods for adding new nodes  refer to figure     adding nodes in this
manner creates random networks whose node structure is analogous to dependencies found
in symbolic rule bases  thus creating networks suitable for regent s crossover and mutation
operators 
table   shows the test set error of regent with various percentages of knowledge based
neural networks  knns  present in the initial population  the first row contains the results
of initializing regent with a purely random initial population  i e   the population contains
no knns   the second row lists the results when regent creates half its population with
the domain theory  and the other half randomly  finally  the last row contains the results
of seeding the entire population with the domain theory 
these results suggest that including  in the initial population  networks that were not
created from the domain theory increases regent s test set error on all three domains 
this occurs because the randomly generated networks are not as correct as the knns  and
   

fiopitz   shavlik

   knn
    knn
     knn

rbs
    
    
    

splice junction
    
    
    

promoters
    
    
    

table    test set error after considering     networks  each row gives the pergentage of
knns present in the initial population  pairwise  one tailed t tests indicate that
initializing regent with      knns differs from    knns at the     confidence
level on all three domains  however  the difference between the runs of     and
     knns is not significant at this level 

thus offspring of the original knn quickly replace the random networks  hence  diversity in the population suffers compared to methods that start with a whole population of
knns  assuming the domain theory is not  malicious   it is therefore better to seed the
entire population from the kbann network  should the domain theory indeed be malicious
and contain information that promotes spurious correlations in the data  it would then be
reasonable to randomly create the  whole  population  running regent both with and
without the domain theory allows one to investigate the utility of that theory 
these results are also interesting from a ga point of view  forrest and mitchell       
showed that gas perform poorly on complex problems where the basic building blocks either
 a  are non trivial to find or  b  get split during crossover  seeding the initial population
with a domain theory  as regent does  can help define the basic building blocks for these
problems 
      value of regent s mutation

typically with gas  mutation is a secondary operation that is only sparingly used  goldberg         however  regent s mutation is a directed approach that heuristically adds
nodes to knns in a provenly effective manner  i e   it uses topgen   it is therefore reasonable to hypothesize that one should apply the mutation operator more frequently than
traditionally done in gas  the results in this section test this hypothesis 
figure   presents the test set error of regent with varying percentages of mutation
 versus crossover  when creating new networks in step  a of table    each graph plots four
curves   a     mutation  i e   regent only uses crossover    b      mutation   c     
mutation  and  d       mutation  performing no mutations tests the value of solely using
crossover  while      mutation tests the ecacy of the mutation operator by itself  note
that      mutation is just topgen with a different search strategy  instead of keeping
an open list for heuristic search  a population of knns are generated and members of
the population are improved proportional to their fitness  the other two curves      and
    mutation  test the synergy between the two operators  performing     mutation is
   

ficonnectionist theory refinement

    

  

ribosome binding sites
  
   mutation
    mutation

  

    mutation
     mutation

testset error

  

  

  

splice junctions
  

  

  

promoters
  

 

   

   

   

   

   

networks considered
figure    error rates of regent with different fractions of mutation versus crossover
after considering     networks  arguably due to the inherent similarity of the
algorithms  and the limited number of runs due to their computational complexity 
the results are not significant at the     confidence level 

   

fiopitz   shavlik

closer to the traditional ga viewpoint that mutation is a secondary operation  while    
mutation means that both operations are equally valuable   previous experiments in this
section used     mutation and     crossover  
while the differences are not all statistically significant  the results nevertheless suggest
that a synergy exists between the two operations  except for the middle portion of the
promoter domain  the results show that  qualitatively  using both operations at the same
time is better than using either operation alone  in fact  equally mixing the mutation and
crossover operator is better than the other three curves on all three domains once regent
has considered     networks  this result is particularly pronounced on the splice junction
domain 
      value of regent s crossover
regent tries to cross over the rules in the networks  rather than just blindly crossing over

nodes  it does this by probabilistically dividing the nodes in the network into two sets
where nodes belonging to the same rule tend to belong to the same set  in this section 
we test the ecacy of regent s crossover by comparing it to a variant of itself where it
randomly assigns nodes to two sets  rather than using dividenodes in table    
table   contains the results of this test after     networks were considered  in the
first row  regent random crossover  regent randomly breaks its hidden nodes into
two sets  while in the second row  regent assigns nodes to two sets according to table
   in both cases  regent creates half its networks with its mutation operator  and the
other half with crossover operator  although the differences are not statistically significant 
the results suggest that keeping the rule structure of the networks intact during crossover
is important  otherwise  the basic building blocks of the networks  i e   the rules  get split
during crossover  and studies have shown the importance of keeping intact the basic building
blocks during crossover  forrest   mitchell        goldberg        

regent random crossover
regent

promoters splice junction rbs
    
    
    
    
    
    

table    test set error of two runs of regent   a  randomly crossing over  nodes  in
the networks  and  b  one with crossing over  rules  in the network  defined by
equation     both runs considered     networks and used half crossover  half
mutation  the results are not significant at the     confidence level  there is only
a slight difference between the learning algorithms and the long run times limited
runs to a ten fold cross validation 

   discussion and future work

towell        showed kbann generalized better than many other machine learning algorithms on the promoter and splice junction domains  the rbs dataset did not exist then  
   

ficonnectionist theory refinement

despite this success  regent is able to effectively use available computer cycles to significantly improve generalization over both kbann and our previous improvement to kbann 
the topgen algorithm  regent reduces kbann s test set error by     for the rbs domain      for the splice junction domain  and     for the promoter domain  it reduces
topgen s test set error by     for the rbs domain      for the splice junction domain  and
    for the promoter domain  also  regent s ability to use available computing time is
further aided by being inherently parallel  since we can train many networks simultaneously 
further results show that regent s two genetic operators complement each other  the
crossover operator considers a large variety of network topologies by probabilistically combining rules contained within two  successful  knns  mutation  on the other hand  makes
smaller  directed improvements to members of the population  while at the same time adding
diversity to the population by adding new rules to the population  equal use of both operators  therefore  allows a wide variety of topologies to be considered as well as allowing
incremental improvements to members of the population 
since regent searches through many candidate networks  it is important for it to be
able to recognize the networks that are likely to generalize the best  with this in mind  our
first planned extension of regent is to develop and test different network evaluation functions  we currently use a validation set  however  validation sets have several drawbacks 
first  keeping aside a validation set decreases the number of training instances available
for each network  second  the performance of a validation set can be a noisy approximator
of the true error  mackay        weigend  huberman    rumelhart         finally  as
we increase the number of networks searched  regent may start selecting networks that
overfit the validation set  in fact  this explains the occasional upward trend in test set error 
from both topgen and regent  in figure   
to avoid the problem of overfitting the data  a common regression trick is to have a cost
function that includes a  smoothness  term along with the error term  the best function 
then  will be the smoothest function that also fits the data well  for neural networks  one
can add to the estimated error a smoothness component that is a measure of the complexity
of the network  the complexity of the network cannot simply be estimated by counting
the number of possible parameters  since there tends to be significant duplication in the
function of each weight in a network  especially early in the training process  weigend 
       two techniques that try to take into account the effective size of the network are
generalized prediction error  moody        and bayesian methods  mackay        
quinlan and cameron jones        propose adding an additional term to the accuracy
and smoothness term that takes into account length of time spent searching  they coin the
term  oversearching  to describe the phenomenon where more extensive searching causes
lower predictive accuracy  their claim is that oversearching is orthogonal to overfitting  and
thus these complexity based methods alone cannot prevent oversearching  as we increase
the number of networks we consider during a search  we too may start oversearching  and
thus plan to investigate adding an oversearching penalty term as well 
as indicated earlier  regent is lamarckian in that it passes local optimizations of individuals  i e   the trained weights of a network  to offspring  a viable alternative  called the
baldwin effect  ackley   littman        baldwin        belew   mitchell        hinton  
nowlan         is to have local search still change the fitness of an individual  backpropagation learning in this case   but then not pass these changes on to the offspring  this form of
   

fiopitz   shavlik

evolution is darwinian in nature   even though what is learned is not explicitly coded into
the genetic material  individuals who are best able to learn will have the most offspring 
thus learning still impacts evolution  in fact this form of evolution can sometimes outperform forms of lamarckian evolution that employ the same local search strategy  whitley 
gordon    mathias         future work is to investigate the utility of the baldwin effect
on regent  in this case we would not cross over the trained networks  but instead cross
over the initial weight settings before backpropagation learning took place 
finally  often times there are multiple  even conicting  theories about a domain  future work  then  is to investigate ways of using all of these domain theories to seed the
initial population  although the results in section       show that including randomly generated networks degrades generalization performance  seeding the population with multiple
approximately correct theories should not degrade generalization  assuming the networks
will have about the same initial correctness  thus regent should be able to naturally
combine good parts of multiple theories  also  for a given domain theory  there are many
different but equivalent ways to represent that theory using a set of propositional rules 
each representation leads to a different network topology  and even though each network
starts with the same theory  some topologies may be more conducive to neural refinement 

   related work

regent mainly differs from previous work in that it is an anytime  theory refinement sys 

tem that continually searches  in a non hillclimbing manner  for improvements to the domain
theory  in summary  our work is unique in that it provides a connectionist approach that
attempts to effectively utilize available background knowledge and available computer cycles
to generate the best concept possible  we have broken the rest of this section into four parts 
 a  connectionist theory refinement algorithms   b  purely symbolic theory refinement algorithms   c  algorithms that find an appropriate domain specific neural network topology 
and  d  optimization algorithms wrapped around induction algorithms 

    connectionist theory refinement techniques

we begin our discussion with connectionist theory refinement systems  these systems have
been developed to refine many types of rule bases  for instance  a number of systems
have been proposed for revising certainty factor rule bases  fu        lacher et al        
mahoney   mooney         finite state automata  maclin   shavlik        omlin   giles 
       push down automata  das  giles    sun         fuzzy logic rules  berenji       
masuoka  watanabe  kawamura  owada    asakawa         and mathematical equations
 roscheisen  hofmann    tresp        scott et al          most of these systems work like
kbann by first translating the domain knowledge into a neural network  then modifying
the weights of this resulting network  few attempts  which we describe next  have been
made to dynamically adjust the resulting network s topology during training  as regent
does  
like both topgen and regent  fletcher and obradovic        present an approach
that adds nodes to a kbann network  their system constructs a single layer of nodes  fully
connected between the input and output nodes   off to the side  of the kbann network 
they generate new hidden nodes using a variant of baum and lang s        constructive
   

ficonnectionist theory refinement

algorithm  baum and lang s algorithm first divides the feature space with hyperplanes 
they find each hyperplane by randomly selecting two points from different classes  then
localizing a suitable split between these points  baum and lang repeat this process until
they generate a fixed number of hyperplanes  fletcher and obradovic then map each of
baum and lang s hyperplanes into one new hidden node  thus defining the weights between
the input layer and that hidden node  fletcher and obradovic s algorithm does not change
the weights of the kbann portion of the network  so modifications to the initial rule base
are solely left to the constructed hidden nodes  thus  their system does not take advantage
of kbann s strength of removing unwanted antecedents and rules from the original rule
base  in fact  topgen compared favorably to a similar technique that also added nodes off
to the side of kbann  opitz   shavlik        and regent outperformed topgen in this
article s experiments 
rapture  mahoney   mooney        is designed for domain theories containing probabilistic rules  like most connectionist theory refinement systems  rapture first translates
the domain theory into a neural network  then refines the weights of the network with a
modified backpropagation algorithm  like regent  rapture is then able to dynamically
refine the topology of its network  it does this by using the upstart algorithm  frean 
      to add new nodes to the network  aside from being designed for probabilistic rules 
rapture differs from regent in that it adds nodes with the intention of completely
learning the training set  not generalizing well  thus  while rapture hillclimbs until the
training set is learned  regent continually searches topology space looking for a network
that minimizes the scoring function s error  also  rapture initially only creates links that
are specified in the domain theory  and only explicitly adds links through id  s  quinlan 
      information gain metric  regent  on the other hand  fully connect consecutive layers
in their networks  allowing each rule the possibility of adding antecedents during training 
the daid algorithm  towell   shavlik        is an extension to kbann that uses the
domain theory to help train the kbann network  since kbann is more effective at dropping antecedents than adding them  daid tries to find potentially useful inputs features
not mentioned in the domain theory  it does this by backing up errors to the lowest level of
the domain theory  then computing correlations with the features  daid then increases the
weight of the links from the potentially useful input features based on these correlations 
daid mainly differs from regent in that it does not refine the topology of the kbann network  thus  while daid addresses kbann s limitation of not effectively adding antecedents 
it is still unable to introduce new rules or constructively induce new antecedents  daid will
therefore suffer with impoverished domain theories  also notice that since daid is an improvement for training knns  regent can use daid to train each network it considers
during its search  however  we have not done so  
opitz and shavlik        used a variant of regent as their learning algorithm when
generating a neural network  ensemble   a neural network ensemble is a very successful
technique where the outputs of a set of separately trained neural networks are combined
to form one unified prediction  drucker  cortes  jackel  lecun    vapnik        hansen
  salamon        perrone         since regent considers many networks  it can select a
subset of the final population of networks as an ensemble at minimal extra cost  previous
work  though  has shown that an ideal ensemble is one where the networks are both accurate
and make their errors on different parts of the input space  hansen   salamon       
   

fiopitz   shavlik

krogh   vedelsby         as a result  opitz and shavlik        changed the scoring
function of regent so that a  fit  network was now one that was both accurate and
disagreed with the other members of the population as much as possible  in addition 
their algorithm  addemup  actively tries to generate good candidates by emphasizing the
current population s erroneous examples during backpropagation training  as a result of
these alterations  addemup is able to create enough diversity among the population of
networks to be able to effectively exploit the knowledge of the domain theory  opitz and
shavlik        show that addemup is able to generate a significantly better ensemble
using the domain theory than either running addemup without the benefit of the theory
or simply combining regent s final population of networks  actively searching for a highly
diverse population  however  does not aid in searching for the single best network  in fact 
the single best network produced by addemup is significantly worse than regent s single
best network on all three domains 

    purely symbolic theory refinement techniques
additional work related to regent includes purely symbolic theory refinement systems
that modify the domain theory directly in its initial form  systems such as focl  pazzani
  kibler        and forte  richards   mooney        are first order  theory refinement
systems that revise predicate logic theories  one drawback to these systems is that they
currently do not generalize as well as connectionist approaches on many real world problems 
such as the dna promoter task  cohen        
there have been several genetic based  first order logic  multimodal concept learners
 greene   smith        janikow         giordana and saitta        showed how to integrate one of these system  regal  giordana  saitta    zini        neri   saitta        
with the deductive engine of ml smart  bergadano  giordana    ponsero        to help
refine an incomplete or inconsistent domain theory  this version works by first using an automated theorem prover to recognize unresolved literals in a proof  then uses the ga based
regal to induce corrections to these literals  regent  on the other hand  use genetic
algorithms  along with neural learning  to refine the whole domain theory at the same time 
dogma  hekanaho        is a recently proposed ga based learner that can use background knowledge to learn the same description language as regal  current restrictions 
however  force the representation language of the domain theory to be propositional rules 
dogma converts a  at  set of background rules  i e   it does not handle intermediate
conclusions  into individual bitstrings that are used as building blocks for a higher level
concept  dogma does not focus on theory refinement  rather it builds a completely new
theory using substructures from the background knowledge  they term their approach as
being more theory suggested than theory guided  hekanaho        
several systems  including ours  have been proposed for refining propositional rule bases 
early such approaches could only handle improvements to overly specific theories  danyluk 
      or specializations to overly general theories  flann   dietterich         later systems
such as rtls  ginsberg         either  ourston   mooney         ptr  koppel  feldman 
  segre         and tgci  donoho   rendell        were later able to handle both types
of refinements  we discuss the either system as a representative of these propositional
systems 
   

ficonnectionist theory refinement

either has four theory revision operators   a  removing antecedents from a rule   b 
adding antecedents to a rule   c  removing rules from the rule base  and  d  inventing new
rules  either uses these operators to make revisions to the domain theory that correctly
classify some of the previously misclassified training examples without undefining any of
the correctly classified examples  either uses inductive learning algorithms to invent new
rules  it currently uses id   quinlan        as its induction component 
even though regent s mutation operator add nodes in a manner analogous to how
a symbolic system adds antecedents and rules  its underlying learning algorithm is  connectionist   towell        showed that kbann outperformed either on the promoter
task  and regent outperformed kbann in this article  kbann s power on this domain
is largely attributed to its ability to make  fine grain  refinements to the domain theory
 towell         because of either s diculty on this domain  baffes and mooney       
presented an extension to it called neither mofn that is able to learn m  of n rules  
rules that are true if m of the n antecedents are true  this improvement generated a
concept that more closely matches kbann s generalization performance 
while we want to minimize changes to a theory  we do not want to do it at the expense of accuracy  however  donoho and rendell        demonstrate that most existing
theory refinement systems  such as either  suffer in that they are only able to make small 
local changes to the domain theory  thus  when an accurate theory is significantly far in
structure from the initial theory  these systems are forced to either become trapped in a
local maximum similar to the initial theory  or are forced to drop entire rules and replace
them with new rules that are inductively created purely from scratch  regent does not
suffer from this in that it translates the theory into the less restricting representation of
neural networks  donoho   rendell         also  regent is able to further reconfigure
the structure of the domain with genetic algorithms 
many authors have reported results using varying subsets of the splice junction domain
 e g   donoho and rendell       mahoney       neri and saitta       and towell and shavlik        while these authors used different training set sizes  it is nevertheless worthwhile
to qualitatively discuss some of their conclusions here  towell and shavlik        compared
kbann with numerous machine learning algorithms where each learning algorithm was
given a training set of      examples  kbann s generalization ability compared favorably
with these algorithms on the splice domain and regent  in turn  compared favorably with
kbann in this article  donoho and rendell        showed their purely symbolic approach
converged to the performance of kbann at around     examples  mahoney        showed 
using training set sizes of up to     examples  that his rapture algorithm generalized
better than kbann on this domain  his results look similar to those of regent  finally 
neri and saitta        showed that the generalization ability of the ga based regal compares favorably to other purely symbolic  non ga based techniques  while they used slightly
different training set sizes than we did in this article  regent compares well to the results
reported in their paper 

    finding appropriate network topologies
our third area of related work covers techniques that attempt to find a good domaindependent topology by dynamically refining their network s topology during training  many
   

fiopitz   shavlik

studies have shown that the generalization ability of a neural network depends on the topology of the network  baum   haussler        tishby  levin    solla         when trying
to find an appropriate topology  one approach is to construct or modify a topology in an
incremental fashion  network shrinking algorithms start with too many parameters  then
remove nodes and weights during training  hassibi   stork        le cun  denker   
solla        mozer   smolensky         network growing algorithms  on the other hand 
start with too few parameters  then add more nodes and weights during training  blanziere
  katenkamp        fahlman   lebiere        frean         the most obvious difference between regent and these algorithms is that regent uses domain knowledge and
symbolic rule refinement techniques to help determine the network s topology  also  these
other algorithms restructure their network based solely on training set error  while regent
minimized validation set error 
instead of incrementally finding an appropriate topology  one can mount a  richer 
search than hillclimbing through the space of topologies  one common approach is to
combine genetic algorithms and neural networks  as regent does   genetic algorithms
have been applied to neural networks in two different ways   a  to optimize the connection
weights in a fixed topology  and  b  to optimize the topology of the network  techniques
that solely use genetic algorithms to optimize weights  montana   davis        whitley
  hanson        have performed competitively with gradient based training algorithms 
however  one problem with genetic algorithms is their ineciency in fine tuned local search 
thus the scalability of these methods are in question  yao         kitano      b  presents
a method that combines genetic algorithms with backpropagation  he does this by using
the genetic algorithm to determine the starting weights for a network  which are then
refined by backpropagation  regent differs from kitano s method in that we use a domain
theory to help determine each network s starting weights and genetically search  instead 
for appropriate network topologies 
most methods that use genetic algorithms to optimize a network topology are similar
to regent in that they also use backpropagation to train each network s weights  of
these methods  many directly encode each link in the network  miller  todd    hegde 
      oliker  furst    maimon        schiffmann  joost    werner         these methods
are relatively straightforward to implement  and are good at fine tuning small networks
 miller et al          however  they do not scale well since they require very large matrices
to represent all the links in large networks  yao         other techniques  dodd       
harp  samad    guha        kitano      a  only encode the most important features of
the network  such as the number of hidden layers  the number of hidden nodes at each
layer  etc  these indirect encoding schemes can evolve different sets of parameters along
with the network s topology and have been shown to have good scalability  yao        
some techniques  koza   rice        oliker et al         evolve both the architecture and
connection weights at the same time  however  the combination of the two levels of evolution
greatly increases the search space 
regent mainly differs from genetic algorithm based training methods in that it is designed for knowledge based neural networks  thus regent uses domain specific knowledge
and symbolic rule refinement techniques to aid in determining the network s topology and
initial weight setting  regent also differs in that it does not explicitly encode its networks 
rather  in the spirit of lamarkian evolution  it passes trained network weights to the off   

ficonnectionist theory refinement

spring  a final difference is that most of these other algorithms restructure their network
based solely on training set error  while regent minimizes validation set error 

    wrapping optimization around learning

we end our related work discussion with a brief overview of methods that combine global and
local optimization strategies  local search algorithms iteratively improve their estimate of
the minimum by searching in only a local neighborhood of the current solution  local minima
are not guaranteed to be global minima   many inductive learning methods are often
equated with local optimization techniques  rumelhart et al          global optimization
methods  such as gas   on the other hand  perform a more sophisticated search across
multiple local minima and are good at finding regions of the search space where nearoptimal solutions can be found  however  they are usually not as good at refining a solution
 once it is close to a near optimal solution  as local optimization strategies  hart        
recent research has shown that it is desirable to emply both a global and local search
strategy  hart        
hybrid gas  such as regent  combine local search with a more traditional ga  while
we focus on hybrid ga algorithms in this section  this two tiered search strategy has been
employed by other researchers as well  kohavi   john        provost   buchanan       
schaffer         gas have been combined with many local search methods  bala  huang 
vafaie  dejong    wechsler        belew        hinton   nowlan        turney        
neural networks are the most common choice for the local search strategy of hybrid ga
systems and we discussed ga neural network hybrids in the section      there are two
common forms of hybrid gas  lamarckian based evolution and darwinian based evolution  the baldwin effect   lamarckian evolution encodes its local improvements directly
into its genetic material  while darwinian evolution leaves the genetic material unchanged
after learning  as discussed in section    most authors use lamarckian local search techniques and many have shown numerous cases where lamarckian evolution outperforms
non lamarckian local search  belew  mcinerney    schraudolph        hart        judson 
colvin  meza  huffa    gutierrez        

   conclusion

an ideal inductive learning algorithm should be able to exploit the available resources of
extensive computing power and domain specific knowledge to improve its ability to generalize  kbann  towell   shavlik        has been shown to be effective at translating
a domain theory into a neural network  however  kbann suffers in that it does not alter
its topology  topgen  opitz   shavlik        improved the kbann algorithm by using
available computer power to search for effective places to add nodes to the kbann network 
however  we show empirically that topgen suffers from restricting its search to expansions
of the kbann network  and is unable to improve its performance after searching beyond
a few topologies  therefore topgen is unable to exploit all available computing power to
increase the correctness of an induced concept 
we present a new algorithm  regent  that uses a specialized genetic algorithm to
broaden the types of topologies considered during topgen s search  experiments indicate
that regent is able to significantly increase generalization over topgen  hence  our new
   

fiopitz   shavlik

algorithm is successful in overcoming topgen s limitation of only searching a small portion
of the space of possible network topologies  in doing so  regent is able to generate a
good solution quickly  by using kbann  then is able to continually improve this solution as
it searches concept space  therefore  regent takes a step toward a true anytime theory
refinement system that is able to make effective use of problem specific knowledge and
available computing cycles 

acknowledgements
this work was supported by oce of naval research grant n                and national
science foundation grant iri           thanks to richard maclin  richard sutton  and
three anonymous reviewers for their helpful comments  this is an extended version of a
paper published in machine learning  proceedings of the eleventh international conference 
pp           new brunswick  nj  morgan kaufmann  david opitz completed a portion of
this work while a graduate student at the university of wisconsin and a professor at the
university of minnesota  duluth 

references
ackley  d          a connectionist machine for genetic hillclimbing  kluwer  norwell 
ma 
ackley  d     littman  m          interactions between learning and evolution  in langton 
c   taylor  c   farmer  c     rasmussen  s   eds    artificial life ii  pp          
redwood city  ca  addison wesley 
ackley  d     littman  m          a case for lamarckian evolution  in langton  c   ed   
artificial life iii  pp        redwood city  ca  addison wesley 
baffes  p     mooney  r          symbolic revision of theories with m of n rules  in
proceedings of the thirteenth international joint conference on artificial intelligence 
pp             chambery  france  morgan kaufmann 
bala  j   huang  j   vafaie  h   dejong  k     wechsler  h          hybrid learning using
genetic algorithms and decision trees for pattern classification  in proceedings of
the fourteenth international joint conference on artificial intelligence  pp          
montreal  canada  morgan kaufmann 
baldwin  j          physical and social heredity  american naturalist              
baum  e     haussler  d          what size net gives valid generalization  neural computation             
baum  e     lang  k          constructing hidden units using examples and queries  in
lippmann  r   moody  j     touretzky  d   eds    advances in neural information
processing systems  vol     pp           san mateo  ca  morgan kaufmann 
   

ficonnectionist theory refinement

belew  r          evolution  learning and culture  computational metaphors for adaptive
search  complex systems           
belew  r   mcinerney  j     schraudolph  n          evolving networks  using the genetic
algorithm with connectionist learning  in langton  c   taylor  c   farmer  c    
rasmussen  s   eds    artificial life ii  pp           redwood city  ca  addisonwesley 
belew  r     mitchell  m          adaptive individuals in evolving populations  models
and algorithms  addison wesley  massachusetts 
berenji  h          refinement of approximate reasoning based controllers by reinforcement
learning  in proceedings of the eighth international machine learning workshop  pp 
         evanston  il  morgan kaufmann 
bergadano  f   giordana  a     ponsero  s          deduction in top down inductive
learning  in proceedings of the sixth international workshop on machine learning 
pp         ithaca  ny  morgan kaufmann 
blanziere  e     katenkamp  p          learning radial basis function networks on line 
in proceedings of the thirteenth international conference on machine learning  pp 
       bari  italy  morgan kaufmann 
clocksin  w     mellish  c          programming in prolog  springer verlag  new york 
cohen  w          compiling prior knowledge into an explicit bias  in proceedings of
the ninth international conference on machine learning  pp           aberdeen 
scotland  morgan kaufmann 
danyluk  a          finding new rules for incomplete theories  explicit biases for induction
with contextual information  in proceedings of the sixth international workshop on
machine learning  pp         ithaca  ny  morgan kaufmann 
das  a   giles  c     sun  g          using prior knowledge in an nnpda to learn
context free languages  in hanson  s   cowan  j     giles  c   eds    advances in
neural information processing systems  vol     pp         san mateo  ca  morgan
kaufmann 
dean  t     boddy  m          an analysis of time dependent planning  in proceedings of
the seventh national conference on artificial intelligence  pp         st  paul  mn 
morgan kaufmann 
dejong  k          an analysis of the behavior of a class of genetic adaptive systems 
ph d  thesis  university of michigan  ann arbor  mi 
dodd  n          optimization of network structure using genetic techniques  in proceedings
of the ieee international joint conference on neural networks  vol  iii  pp          
paris  ieee press 
   

fiopitz   shavlik

donoho  s     rendell  l          rerepresenting and restructuring domain theories  a
constructive induction approach  journal of artificial intelligence research         
    
drucker  h   cortes  c   jackel  l   lecun  y     vapnik  v          boosting and other
machine learning algorithms  in proceedings of the eleventh international conference
on machine learning  pp         new brunswick  nj  morgan kaufmann 
fahlman  s     lebiere  c          the cascade correlation learning architecture  in touretzky  d   ed    advances in neural information processing systems  vol     pp      
     san mateo  ca  morgan kaufmann 
farmer  j     belin  a          artificial life  the coming evolution  in langton  c   taylor 
c   farmer  j  d     rasmussen  s   eds    artificial life ii  pp           redwood
city  ca  addison wesley 
flann  n     dietterich  t          a study of explanation based methods for inductive
learning  machine learning             
fletcher  j     obradovic  z          combining prior symbolic knowledge and constructive
neural network learning  connection science             
forrest  s     mitchell  m          what makes a problem hard for a genetic algorithm 
some anomalous results and their explanation  machine learning              
frean  m          the upstart algorithm  a method for constructing and training feedforward neural networks  neural computation             
fu  l          integration of neural heuristics into knowledge based inference  connection
science             
ginsberg  a          theory reduction  theory revision  and retranslation  in proceedings of
the eighth national conference on artificial intelligence  pp           boston  ma 
aaai mit press 
giordana  a     saitta  l          regal  an integrated system for relations using genetic
algorithms  in proceedings of the second international workshop on multistrategy
learning  pp           harpers ferry  wv 
giordana  a   saitta  l     zini  f          learning disjunctive concepts by means of genetic algorithms  in proceedings of the eleventh international conference on machine
learning  pp          new brunswick  nj  morgan kaufmann 
goldberg  d          genetic algorithms in search  optimization  and machine learning 
addison wesley  reading  ma 
greene  d     smith  s          competition based induction of decision models from
examples  machine learning              
   

ficonnectionist theory refinement

grefenstette  j     ramsey  c          an approach to anytime learning  in proceedings
of the ninth international conference on machine learning  pp           aberdeen 
scotland  morgan kaufmann 
hansen  l     salamon  p          neural network ensembles  ieee transactions on
pattern analysis and machine intelligence               
harp  s   samad  t     guha  a          designing application specific neural networks
using the genetic algorithm  in touretzky  d   ed    advances in neural information
processing systems  vol     pp           san mateo  ca  morgan kaufmann 
hart  w          adaptive global optimization with local search  ph d  thesis  university
of california  san diego 
hassibi  b     stork  d          second order derivatives for network pruning  optimal brain
surgeon  in hanson  s   cowan  j     giles  c   eds    advances in neural information
processing systems  vol     pp           san mateo  ca  morgan kaufmann 
hekanaho  j          background knowledge in ga based concept learning  in proceedings
of the thirteenth international conference on machine learning  pp           bari 
italy  morgan kaufmann 
hinton  g     nowlan  s          how learning can guide evolution  complex systems    
        
holder  l          maintaining the utility of learned knowledge using model based control  ph d  thesis  computer science department  university of illinois at urbanachampaign 
holland  j          adaptation in natural and artificial systems  university of michigan
press  ann arbor  mi 
janikow  c          a knowledge intensive ga for supervised learning  machine learning 
            
judson  r   colvin  m   meza  j   huffa  a     gutierrez  d          do intelligent configuration search techniques outperform random search for large molecules  international
journal of quantum chemistry          
kibler  d     langley  p          machine learning as an experimental science  in proceedings of the third european working session on learning  pp        edinburgh 
uk 
kitano  h       a   designing neural networks using genetic algorithms with graph generation system  complex systems             
kitano  h       b   empirical studies on the speed of convergence of neural network training using genetic algorithms  in proceedings of the eighth national conference on
artificial intelligence  pp           boston  ma  aaai mit press 
   

fiopitz   shavlik

kohavi  r     john  g          wrappers for feature subset selection  artificial intelligence 
koppel  m   feldman  r     segre  a          bias driven revision of logical domain theories 
journal of artificial intelligence research             
koza  j          genetic programming  mit press  cambridge  ma 
koza  j     rice  j          genetic generation of both the weights and architectures for
a neural network  in international joint conference on neural networks  vol     pp 
         seattle  wa  ieee press 
krogh  a     vedelsby  j          neural network ensembles  cross validation  and active
learning  in tesauro  g   touretzky  d     leen  t   eds    advances in neural
information processing systems  vol     pp           cambridge  ma  mit press 
lacher  r   hruska  s     kuncicky  d          back propagation learning in expert networks  ieee transactions on neural networks           
le cun  y   denker  j     solla  s          optimal brain damage  in touretzky  d   ed   
advances in neural information processing systems  vol     pp           san mateo 
ca  morgan kaufmann 
litzkow  m   livny  m     mutka  m          condor   a hunter of idle workstations  in
proceedings of the eighth international conference on distributed computing systems 
pp           san jose  ca  computer society press 
mackay  d          a practical bayesian framework for backpropagation networks  neural
computation             
maclin  r     shavlik  j          using knowledge based neural networks to improve algorithms  refining the chou fasman algorithm for protein folding  machine learning 
            
mahoney  j          combining symbolic and connectionist learning methods to refine
certainty factor rule bases  ph d  thesis  university of texas  austin  tx 
mahoney  j     mooney  r          combining connectionist and symbolic learning to refine
certainty factor rule bases  connection science             
mahoney  j     mooney  r          comparing methods for refining certainty factor rulebases  in proceedings of the eleventh international conference on machine learning 
pp           new brunswick  nj  morgan kaufmann 
masuoka  r   watanabe  n   kawamura  a   owada  y     asakawa  k          neurofuzzy
system   fuzzy inference using a structured neural network  in proceedings of the
international conference on fuzzy logic   neural networks  pp           iizuka 
japan 
michalski  r          a theory and methodology of inductive learning  artificial intelligence 
            
   

ficonnectionist theory refinement

miller  g   todd  p     hegde  s          designing neural networks using genetic algorithms  in proceedings of the third international conference on genetic algorithms 
pp           arlington  va  morgan kaufmann 
mitchell  m          an introduction to genetic algorithms  mit press  cambridge  ma 
mitchell  t          generalization as search  artificial intelligence              
montana  d     davis  l          training feedforward networks using genetic algorithms  in
proceedings of the eleventh international joint conference on artificial intelligence 
pp           detroit  mi  morgan kaufmann 
moody  j          the effective number of parameters  an analysis of generalization and
regularization in nonlinear learning systems  in moody  j   hanson  s     lippmann 
r   eds    advances in neural information processing systems  vol     pp          
san mateo  ca  morgan kaufmann 
mozer  m  c     smolensky  p          using relevance to reduce network size automatically 
connection science          
neri  f     saitta  l          exploring the power of genetic search in learning symbolic
classifiers  in ieee transactions on pattern analisys and machine intelligence 
oliker  s   furst  m     maimon  o          a distributed genetic algorithm for neural
network design and training  complex systems             
omlin  c     giles  c          training second order recurrent neural networks using hints 
in proceedings of the ninth international conference on machine learning  pp      
     aberdeen  scotland  morgan kaufmann 
opitz  d     shavlik  j          heuristically expanding knowledge based neural networks 
in proceedings of the thirteenth international joint conference on artificial intelligence  pp             chambery  france  morgan kaufmann 
opitz  d     shavlik  j          dynamically adding symbolically meaningful nodes to
knowledge based neural networks  knowledge based systems             
opitz  d     shavlik  j          actively searching for an effective neural network ensemble 
connection science             
ourston  d     mooney  r          theory refinement combining analytical and empirical
methods  artificial intelligence              
pazzani  m     kibler  d          the utility of knowledge in inductive learning  machine
learning           
perrone  m          improving regression estimation  averaging methods for variance
reduction with extension to general convex measure optimization  ph d  thesis 
brown university  providence  ri 
   

fiopitz   shavlik

provost  f     buchanan  b          inductive policy  the pragmatics of bias selection 
machine learning            
quinlan  j          induction of decision trees  machine learning            
quinlan  j     cameron jones  r          lookahead and pathology in decision tree induction  in proceedings of the fourteenth international joint conference on artificial
intelligence  pp             montreal  canada  morgan kaufmann 
richards  b     mooney  r          automated refinement of first order horn clause domain
theories  machine learning             
roscheisen  m   hofmann  r     tresp  v          neural control for rolling mills  incorporating domain theories to overcome data deficiency  in moody  j   hanson  s    
lippmann  r   eds    advances in neural information processing systems  vol     pp 
         san mateo  ca  morgan kaufmann 
rumelhart  d   hinton  g     williams  r          learning internal representations by
error propagation  in rumelhart  d     mcclelland  j   eds    parallel distributed
processing  explorations in the microstructure of cognition  volume    foundations 
pp           mit press  cambridge  ma 
schaffer  c          selecting a classification method by cross validation  machine learning 
            
schiffmann  w   joost  m     werner  r          synthesis and performance analysis of
multilayer neural network architectures  tech  rep      university of koblenz  institute
for physics 
scott  g   shavlik  j     ray  w          refining pid controllers using neural networks 
neural computation             
tishby  n   levin  e     solla  s          consistent inference on probabilities in layered
networks  predictions and generalization  in international joint conference on neural
networks  pp           washington  d c  ieee press 
towell  g          symbolic knowledge and neural networks  insertion  refinement  and
extraction  ph d  thesis  computer sciences department  university of wisconsin 
madison  wi 
towell  g     shavlik  j          using symbolic learning to improve knowledge based neural
networks  in proceedings of the tenth national conference on artificial intelligence 
pp           san jose  ca  aaai mit press 
towell  g     shavlik  j          knowledge based artificial neural networks  artificial
intelligence              
turney  p          cost sensitive classification  empirical evaluation of a hybrid genetic
decision tree induction algorithm  journal of artificial intelligence research         
    
   

ficonnectionist theory refinement

waterman  d          a guide to expert systems  addison wesley  reading  ma 
watrous  r   towell  g     glassman  m          synthesize  optimize  analyze  repeat
 soar   application of neural network tools to ecg patient monitoring  in proceedings of the symposium on nonlinear theory and its applications  pp          
honolulu  hawaii 
watson  j  d   hopkins  n  h   roberts  j  w   argetsinger steitz  j     weiner  a  m 
        molecular biology of the gene  fourth edition   benjamin cummings  menlo
park  ca 
weigend  a          on overfitting and the effective number of hidden units  in proceedings of the      connectionist models summer school  pp           boulder  co 
lawrence erlbaum associates 
weigend  a   huberman  b     rumelhart  d          predicting the future  a connectionist
approach  international journal of neural systems  i          
whitley  d   gordon  s     mathias  k          lamarckian evolution  the baldwin effect
and function optimization  in davidor  y   schwefel  h     manner  r   eds    parallel
problem solving from nature   ppsn iii  pp        springer verlag 
whitley  d     hanson  t          optimizing neural networks using faster  more accurate genetic search  in proceedings of the third international conference on genetic
algorithms  pp           arlington  va  morgan kaufmann 
yao  x          evolutionary artificial neural networks  international journal of neural
systems             

   

fi