journal of artificial intelligence research              

submitted       published     

improved heterogeneous distance functions
d  randall wilson
tony r  martinez
computer science department
brigham young university
provo  ut        usa

randy  axon cs byu edu
martinez  cs byu edu

abstract
instance based learning techniques typically handle continuous and linear input values well 
but often do not handle nominal input attributes appropriately  the value difference metric
 vdm  was designed to find reasonable distance values between nominal attribute values  but it
largely ignores continuous attributes  requiring discretization to map continuous values into
nominal values  this paper proposes three new heterogeneous distance functions  called the
heterogeneous value difference metric  hvdm   the interpolated value difference metric
 ivdm   and the windowed value difference metric  wvdm   these new distance functions
are designed to handle applications with nominal attributes  continuous attributes  or both  in
experiments on    applications the new distance metrics achieve higher classification accuracy
on average than three previous distance functions on those datasets that have both nominal and
continuous attributes 

   introduction
instance based learning  ibl   aha  kibler   albert        aha        wilson   martinez 
      wettschereck  aha   mohri        domingos        is a paradigm of learning in which
algorithms typically store some or all of the n available training examples  instances  from a
training set  t  during learning  each instance has an input vector x  and an output class c 
during generalization  these systems use a distance function to determine how close a new
input vector y is to each stored instance  and use the nearest instance or instances to predict the
output class of y  i e   to classify y   some instance based learning algorithms are referred to as
nearest neighbor techniques  cover   hart        hart        dasarathy         and memorybased reasoning methods  stanfill   waltz        cost   salzberg        rachlin et al        
overlap significantly with the instance based paradigm as well  such algorithms have had much
success on a wide variety of applications  real world classification tasks  
many neural network models also make use of distance functions  including radial basis
function networks  broomhead   lowe        renals   rohwer        wasserman        
counterpropagation networks  hecht nielsen         art  carpenter   grossberg         selforganizing maps  kohonen        and competitive learning  rumelhart   mcclelland        
distance functions are also used in many fields besides machine learning and neural networks 
including statistics  atkeson  moore   schaal         pattern recognition  diday       
michalski  stepp   diday         and cognitive psychology  tversky        nosofsky        
      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiwilson   martinez

there are many distance functions that have been proposed to decide which instance is
closest to a given input vector  michalski  stepp   diday        diday         many of these
metrics work well for numerical attributes but do not appropriately handle nominal  i e  
discrete  and perhaps unordered  attributes 
the value difference metric  vdm   stanfill   waltz        was introduced to define an
appropriate distance function for nominal  also called symbolic  attributes  the modified value
difference metric  mvdm  uses a different weighting scheme than vdm and is used in the
pebls system  cost   salzberg        rachlin et al          these distance metrics work well
in many nominal domains  but they do not handle continuous attributes directly  instead  they
rely upon discretization  lebowitz        schlimmer         which can degrade generalization
accuracy  ventura   martinez        
many real world applications have both nominal and linear attributes  including  for
example  over half of the datasets in the uci machine learning database repository  merz  
murphy         this paper introduces three new distance functions that are more appropriate
than previous functions for applications with both nominal and continuous attributes  these
new distance functions can be incorporated into many of the above learning systems and areas
of study  and can be augmented with weighting schemes  wettschereck  aha   mohri       
atkeson  moore   schaal        and other enhancements that each system provides 
the choice of distance function influences the bias of a learning algorithm  a bias is a rule
or method that causes an algorithm to choose one generalized output over another  mitchell 
       a learning algorithm must have a bias in order to generalize  and it has been shown that
no learning algorithm can generalize more accurately than any other when summed over all
possible problems  schaffer         unless information about the problem other than the
training data is available   it follows then that no distance function can be strictly better than
any other in terms of generalization ability  when considering all possible problems with equal
probability 
however  when there is a higher probability of one class of problems occurring than another 
some learning algorithms can generalize more accurately than others  wolpert         this is
not because they are better when summed over all problems  but because the problems on which
they perform well are more likely to occur  in this sense  one algorithm or distance function can
be an improvement over another in that it has a higher probability of good generalization than
another  because it is better matched to the kinds of problems that will likely occur 
many learning algorithms use a bias of simplicity  mitchell        wolpert        to
generalize  and this bias is appropriatemeaning that it leads to good generalization
accuracyfor a wide variety of real world applications  though the meaning of simplicity varies
depending upon the representational language of each learning algorithm  other biases  such as
decisions made on the basis of additional domain knowledge for a particular problem  mitchell 
       can also improve generalization 
in this light  the distance functions presented in this paper are more appropriate than those
used for comparison in that they on average yield improved generalization accuracy on a
collection of    applications  the results are theoretically limited to this set of datasets  but the
hope is that these datasets are representative of other problems that will be of interest  and occur
frequently  in the real world  and that the distance functions presented here will be useful in
such cases  especially those involving both continuous and nominal input attributes 
section   provides background information on distance functions used previously  section  
 

fiimproved heterogeneous distance functions

introduces a distance function that combines euclidean distance and vdm to handle both
continuous and nominal attributes  sections   and   present two extensions of the value
difference metric which allow for direct use of continuous attributes  section   introduces the
interpolated value difference metric  ivdm   which uses interpolation of probabilities to avoid
problems related to discretization  section   presents the windowed value difference metric
 wvdm   which uses a more detailed probability density function for a similar interpolation
process 
section   presents empirical results comparing three commonly used distance functions with
the three new functions presented in this paper  the results are obtained from using each of the
distance functions in an instance based learning system on    datasets  the results indicate that
the new heterogeneous distance functions are more appropriate than previously used functions
on datasets with both nominal and linear attributes  in that they achieve higher average
generalization accuracy on these datasets  section   discusses related work  and section  
provides conclusions and future research directions 

   previous distance functions
as mentioned in the introduction  there are many learning systems that depend upon a good
distance function to be successful  a variety of distance functions are available for such uses 
including the minkowsky  batchelor         mahalanobis  nadler   smith         camberra 
chebychev  quadratic  correlation  and chi square distance metrics  michalski  stepp  
diday        diday         the context similarity measure  biberman         the contrast
model  tversky         hyperrectangle distance functions  salzberg        domingos       
and others  several of these functions are defined in figure   
although there have been many distance functions proposed  by far the most commonly
used is the euclidean distance function  which is defined as 
m

e x  y   

  xa  ya   

   

a  

where x and y are two input vectors  one typically being from a stored instance  and the other an
input vector to be classified  and m is the number of input variables  attributes  in the
application  the square root is often not computed in practice  because the closest instance s 
will still be the closest  regardless of whether the square root is taken 
an alternative function  the city block or manhattan distance function  requires less
computation and is defined as 
m x  y   

m

 xa  ya

   

a  

the euclidean and manhattan distance functions are equivalent to the minkowskian rdistance function  batchelor        with r     and    respectively 

 

fiwilson   martinez

minkowsky 

euclidean 

m
r
d x  y      xi  yi 
 i  

camberra 

 

r

manhattan   city block 

m

 
   xi  yi  

d x  y   

i  

m x y
i
d x  y     i
x
 
y
i
i   i

chebychev 

m

d x  y     xi  yi
i  

m

d x  y    max xi  yi
i  

m m

d x  y     x  y t q x  y        xi  yi  q ji   x j  y j  
quadratic 

q is a problem specific positive
j   i  
definite m  m weight matrix
v is the covariance matrix of a    am 
mahalanobis 
and aj is the vector of values for
d x  y     det v    m  x  y t v    x  y 
attribute j occuring in the training set
instances    n 
m
correlation 
  xi  xi   yi  yi  
xi   yi and is the average value for
i  
d x  y   
attribute
i occuring in the training set 
m
m
 
 
  xi  xi     yi  yi  
i  

i  

y 
   xi
chi square  d x  y    
 i 

sumi  sizex size y 
i  
m

kendalls rank correlation 
sign x        or   if x     
x      or x      respectively 

d x  y      

 

sumi is the sum of all values for attribute
i occuring in the training set  and sizex is
the sum of all values in the vector x 

m i 
 

 sign xi  x j  sign yi  y j  
n n     i   j  

figure    equations of selected distance functions 
 x and y are vectors of m attribute values  
     normalization
one weakness of the basic euclidean distance function is that if one of the input attributes has a
relatively large range  then it can overpower the other attributes  for example  if an application
has just two attributes  a and b  and a can have values from   to       and b has values only
from   to     then bs influence on the distance function will usually be overpowered by as
influence  therefore  distances are often normalized by dividing the distance for each attribute
by the range  i e   maximum minimum  of that attribute  so that the distance for each attribute is
in the approximate range       in order to avoid outliers  it is also common to divide by the
standard deviation instead of range  or to trim the range by removing the highest and lowest
few percent  e g       of the data from consideration in defining the range  it is also possible to
map any value outside this range to the minimum or maximum value to avoid normalized
values outside the range       domain knowledge can often be used to decide which method is
most appropriate 
related to the idea of normalization is that of using attribute weights and other weighting
 

fiimproved heterogeneous distance functions

schemes  many learning systems that use distance functions incorporate various weighting
schemes into their distance calculations  wettschereck  aha   mohri        atkeson  moore  
schaal         the improvements presented in this paper are independent of such schemes  and
most of the various weighting schemes  as well as other enhancements such as instance pruning
techniques  can be used in conjunction with the new distance functions presented here 
     attribute types
none of the distance functions shown in figure    including euclidean distance  appropriately
handle non continuous input attributes 
an attribute can be linear or nominal  and a linear attribute can be continuous or discrete  a
continuous  or continuously valued  attribute uses real values  such as the mass of a planet or
the velocity of an object  a linear discrete  or integer  attribute can have only a discrete set of
linear values  such as number of children 
it can be argued that any value stored in a computer is discrete at some level  the reason
continuous attributes are treated differently is that they can have so many different values that
each value may appear only rarely  perhaps only once in a particular application   this causes
problems for algorithms such as vdm  described in section      that depend on testing two
values for equality  because two continuous values will rarely be equal  though they may be
quite close to each other 
a nominal  or symbolic  attribute is a discrete attribute whose values are not necessarily in
any linear order  for example  a variable representing color might have values such as red 
green  blue  brown  black and white  which could be represented by the integers   through   
respectively  using a linear distance measurement such as     or     on such values makes little
sense in this case 
     heterogeneous euclidean overlap metric  heom 
one way to handle applications with both continuous and nominal attributes is to use a
heterogeneous distance function that uses different attribute distance functions on different
kinds of attributes  one approach that has been used is to use the overlap metric for nominal
attributes and normalized euclidean distance for linear attributes 
for the purposes of comparison during testing  we define a heterogeneous distance function
that is similar to that used by ib   ib  and ib   aha  kibler   albert        aha        as well
as that used by giraud carrier   martinez         this function defines the distance between
two values x and y of a given attribute a as 
if x or y is unknown  else
   

da  x  y    overlap x  y   if a is nominal  else
 rn  diff  x  y 
a


   

unknown attribute values are handled by returning an attribute distance of    i e   a maximal
distance  if either of the attribute values is unknown  the function overlap and the rangenormalized difference rn diff are defined as 
   if x   y
overlap x  y    
   otherwise

 

   

fiwilson   martinez

rn  diff a  x  y   

  x  y 
rangea

   

the value rangea is used to normalize the attributes  and is defined as 
rangea  maxa  mina

   

where max a and mina are the maximum and minimum values  respectively  observed in the
training set for attribute a  this means that it is possible for a new input vector to have a value
outside this range and produce a difference value greater than one  however  such cases are
rare  and when they do occur  a large difference may be acceptable anyway  the normalization
serves to scale the attribute down to the point where differences are almost always less than one 
the above definition for da returns a value which is  typically  in the range       whether the
attribute is nominal or linear  the overall distance between two  possibly heterogeneous  input
vectors x and y is given by the heterogeneous euclidean overlap metric function heom x y  
m

 da  xa   ya   

heom x  y   

   

a  

this distance function removes the effects of the arbitrary ordering of nominal values  but its
overly simplistic approach to handling nominal attributes fails to make use of additional
information provided by nominal attribute values that can aid in generalization 
     value difference metric  vdm 
the value difference metric  vdm  was introduced by stanfill and waltz        to provide an
appropriate distance function for nominal attributes  a simplified version of the vdm  without
the weighting schemes  defines the distance between two values x and y of an attribute a as 
c na x c

na y c
vdma  x  y    

n
na y
a x
c  

q

c

   pa x c  pa y c

q

   

c  

where
 na x is the number of instances in the training set t that have value x for attribute a 
 na x c is the number of instances in t that have value x for attribute a and output class c 
 c is the number of output classes in the problem domain 
 q is a constant  usually   or    and
 p a x c is the conditional probability that the output class is c given that attribute a has the
value x  i e   p c   xa   as can be seen from      pa x c is defined as 
pa x c  

na x c
na x

   

where na x is the sum of na x c over all classes  i e  
c

na x    na x c
c  

 

    

fiimproved heterogeneous distance functions

and the sum of pa x c over all c classes is   for a fixed value of a and x 
using the distance measure vdma x y   two values are considered to be closer if they have
more similar classifications  i e   more similar correlations with the output classes   regardless
of what order the values may be given in  in fact  linear discrete attributes can have their values
remapped randomly without changing the resultant distance measurements 
for example  if an attribute color has three values red  green and blue  and the application is
to identify whether or not an object is an apple  red and green would be considered closer than
red and blue because the former two both have similar correlations with the output class apple 
the original vdm algorithm  stanfill   waltz        makes use of feature weights that are
not included in the above equations  and some variants of vdm  cost   salzberg       
rachlin et al         domingos        have used alternate weighting schemes  as discussed
earlier  the new distance functions presented in this paper are independent of such schemes and
can in most cases make use of similar enhancements 
one problem with the formulas presented above is that they do not define what should be
done when a value appears in a new input vector that never appeared in the training set  if
attribute a never has value x in any instance in the training set  then na x c for all c will be    and
n a x  which is the sum of na x c over all classes  will also be    in such cases p a x c       
which is undefined  for nominal attributes  there is no way to know what the probability should
be for such a value  since there is no inherent ordering to the values  in this paper we assign
p a x c the default value of   in such cases  though it is also possible to let pa x c     c  where c
is the number of output classes  since the sum of pa x c for c      c is always      
if this distance function is used directly on continuous attributes  the values can all
potentially be unique  in which case na x is   for every value x  and na x c is   for one value of c
and   for all others for a given value x  in addition  new vectors are likely to have unique
values  resulting in the division by zero problem above  even if the value of   is substituted for
     the resulting distance measurement is nearly useless 
even if all values are not unique  there are often enough different values for a continuous
attribute that the statistical sample is unreliably small for each value  and the distance measure
is still untrustworthy  because of these problems  it is inappropriate to use the vdm directly on
continuous attributes 
     discretization
one approach to the problem of using vdm on continuous attributes is discretization
 lebowitz        schlimmer        ventura         some models that have used the vdm or
variants of it  cost   salzberg        rachlin et al         mohri   tanaka        have
discretized continuous attributes into a somewhat arbitrary number of discrete ranges  and then
treated these values as nominal  discrete unordered  values  this method has the advantage of
generating a large enough statistical sample for each nominal value that the p values have some
significance  however  discretization can lose much of the important information available in
the continuous values  for example  two values in the same discretized range are considered
equal even if they are on opposite ends of the range  such effects can reduce generalization
accuracy  ventura   martinez        
in this paper we propose three new alternatives  which are presented in the following three
sections  section   presents a heterogeneous distance function that uses euclidean distance for
linear attributes and vdm for nominal attributes  this method requires careful attention to the
 

fiwilson   martinez

problem of normalization so that neither nominal nor linear attributes are regularly given too
much weight 
in sections   and   we present two distance functions  the interpolated value difference
metric  ivdm  and the windowed value difference metric  wvdm   which use discretization
to collect statistics and determine values of pa x c for continuous values occurring in the training
set instances  but then retain the continuous values for later use  during generalization  the
value of pa y c for a continuous value y is interpolated between two other values of p  namely 
p a x  c and pa x  c  where x    y  x   ivdm and wvdm are essentially different techniques
for doing a nonparametric probability density estimation  tapia   thompson        to
determine the values of p for each class  a generic version of the vdm algorithm  called the
discretized value difference metric  dvdm  is used for comparisons with the two new
algorithms 

   heterogeneous value difference metric  hvdm 
as discussed in the previous section  the euclidean distance function is inappropriate for
nominal attributes  and vdm is inappropriate for continuous attributes  so neither is sufficient
on its own for use on a heterogeneous application  i e   one with both nominal and continuous
attributes 
in this section  we define a heterogeneous distance function hvdm that returns the distance
between two input vectors x and y  it is defined as follows 
hvdm x  y   

m

 da   xa   ya  

    

a  

where m is the number of attributes  the function da x y  returns a distance between the two
values x and y for attribute a and is defined as 
if x or y is unknown  otherwise   
  

da  x  y    normalized  vdma  x  y   if a is nominal
 normalized  diff  x  y   if a is linear
a


    

the function da x y  uses one of two functions  defined below in section       depending on
whether the attribute is nominal or linear  note that in practice the square root in      is not
typically performed because the distance is always positive  and the nearest neighbor s  will still
be nearest whether or not the distance is squared  however  there are some models  e g  
distance weighted k nearest neighbor  dudani        that require the square root to be
evaluated 
many applications contain unknown input values which must be handled appropriately in a
practical system  quinlan         the function da x y  therefore returns a distance of   if either
x or y is unknown  as is done by aha  kibler   albert        and giraud carrier   martinez
        other more complicated methods have been tried  wilson   martinez         but with
little effect on accuracy 
the function hvdm is similar to the function hoem given in section      except that it
 

fiimproved heterogeneous distance functions

uses vdm instead of an overlap metric for nominal values and it also normalizes differently  it
is also similar to the distance function used by rise      domingos         but has some
important differences noted below in section     
section     presents three alternatives for normalizing the nominal and linear attributes 
section     presents experimental results which show that one of these schemes provides better
normalization than the other two on a set of several datasets  section     gives empirical results
comparing hvdm to two commonly used distance functions 
     normalization
as discussed in section      distances are often normalized by dividing the distance for each
variable by the range of that attribute  so that the distance for each input variable is in the range
      this is the policy used by heom in section      however  dividing by the range allows
outliers  extreme values  to have a profound effect on the contribution of an attribute  for
example  if a variable has values which are in the range       in almost every case but with one
exceptional  and possibly erroneous  value of     then dividing by the range would almost
always result in a value less than      a more robust alternative in the presence of outliers is to
divide the values by the standard deviation to reduce the effect of extreme values on the typical
cases 
for the new heterogeneous distance metric hvdm  the situation is more complicated
because the nominal and numeric distance values come from different types of measurements 
numeric distances are computed from the difference between two linear values  normalized by
standard deviation  while nominal attributes are computed from a sum of c differences of
probability values  where c is the number of output classes   it is therefore necessary to find a
way to scale these two different kinds of measurements into approximately the same range to
give each variable a similar influence on the overall distance measurement 
since     of the values in a normal distribution fall within two standard deviations of the
mean  the difference between numeric values is divided by   standard deviations to scale each
value into a range that is usually of width    the function normalized diff is therefore defined
as shown below in equation    
normalized  diff a  x  y   

xy
  a

    

where a is the standard deviation of the numeric values of attribute a 
three alternatives for the function normalized vdm were considered for use in the
heterogeneous distance function  these are labeled n   n  and n   and the definitions of each
are given below 
n   normalized  vdm a  x  y   

c n
a x c



c  

n   normalized  vdm  a  x  y   

c n
a x c



c  

 

na x

na x



na y c



na y c

    

na y

na y

 

    

fiwilson   martinez

c n
a x c

n   normalized  vdm a  x  y    c   

c  

na x



na y c

 

    

na y

the function n  is equation     with q    this is similar to the formula used in pebls
 rachlin et al         and rise  domingos        for nominal attributes 
n  uses q    thus squaring the individual differences  this is analogous to using euclidean
distance instead of manhattan distance  though slightly more expensive computationally  this
formula was hypothesized to be more robust than n  because it favors having all of the class
correlations fairly similar rather than having some very close and some very different  n 
would not be able to distinguish between these two  in practice the square root is not taken 
because the individual attribute distances are themselves squared by the hvdm function 
n  is the function used in heterogeneous radial basis function networks  wilson  
martinez         where hvdm was first introduced 
     normalization experiments
in order to determine whether each normalization scheme n   n  and n  gave unfair weight to
either nominal or linear attributes  experiments were run on    databases from the machine
learning database repository at the university of california  irvine  merz   murphy         all
of the datasets for this experiment have at least some nominal and some linear attributes  and
thus require a heterogeneous distance function 
in each experiment  five fold cross validation was used  for each of the five trials  the
distance between each instance in the test set and each instance in the training set was
computed  when computing the distance for each attribute  the normalized diff function was
used for linear attributes  and the normalized vdm function n   n   or n  was used  in each of
the three respective experiments  for nominal attributes 
the average distance  i e   sum of all distances divided by number of comparisons  was
computed for each attribute  the average of all the linear attributes for each database was
computed and these averages are listed under the heading avglin in table   

database
anneal
australian
bridges
crx
echocardiogram
flag
heart
heart cleveland
heart hungarian
heart long beach va
heart more
heart swiss
hepatitis
horse colic
soybean large
average

avglin
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

n 
avgnom
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

n 
avgnom
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

n 
avgnom
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

 nom 
  
 
 
 
 
  
 
 
 
 
 
 
  
  
  
  

 lin 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 

table    average attribute distance for linear and nominal attributes 
  

 c
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 

fiimproved heterogeneous distance functions

average distance

average distance

average distance

the averages of all the nominal attributes for each of the three normalization schemes are
listed under the headings avgnom in table   as well  the average distance for linear
variables is exactly the same regardless of whether n   n  or n  is used  so this average is
given only once  table   also lists the number of nominal   nom   and number of linear
  lin   attributes in each database  along with the number of output classes   c  
as can be seen from the overall averages in the first four columns of the last row of
table    n  is closer than n  or n   however  it is important to understand the reasons behind
this difference in order to know if the normalization scheme n  will be more robust in general 
figures     graphically display the averages shown in table   under the headings n   n 
and n   respectively  ordered from left to right by the number of output classes  we
hypothesized that as the number of output classes grows  the normalization would get worse for
n  if it was indeed not appropriate to add the scaling factor c to the sum  the length of each
line indicates how much difference there is between
the average distance for nominal attributes and
nominal
 
linear attributes  an ideal normalization scheme
linear
would have a difference of zero  and longer lines
  
indicate worse normalization 
  
as the number of output classes grows  the
  
difference for n  between the linear distances and
  
the nominal distances grows wider in most cases 
n   on the other hand  seems to remain quite close
 
                               avg
independent of the number of output classes 
number of output classes
interestingly  n  does almost as poorly as n   even
figure    average distances for n  
though it does not use the scaling factor c 
apparently the squaring factor provides for a more
nominal
 
well rounded distance metric on nominal attributes
linear
similar to that provided by using euclidean distance
  
instead of manhattan distance on linear attributes 
  
the underlying hypothesis behind performing
  
normalization is that proper normalization will
  
typically improve generalization accuracy  a
nearest neighbor classifier  with k     was
 
                               avg
implemented using hvdm as the distance metric 
number of output classes
the system was tested on the heterogeneous
figure    average distances for n  
datasets appearing in table   using the three
different normalization schemes discussed above 
 
nominal
using ten fold cross validation  schaffer         and
linear
the results are summarized in table    all the
  
normalization schemes used the same training sets
  
and test sets for each trial  bold entries indicate
  
which scheme had the highest accuracy  an
  
asterisk indicates that the difference was greater that
   over the next highest scheme 
 
                               avg
as can be seen from the table  the normalization
number of output classes
scheme n  had the highest accuracy  and n  was
figure    average distances for n  
  

fiwilson   martinez

substantially lower than the other two  n 
database
n 
n 
n 
and n  each had the highest accuracy for
anneal
     
           
  domains  more significantly  n  was
australian
     
           
bridges
     
           
over    higher   times compared to n 
crx
     
           
being over    higher on just one dataset 
echocardiogram
     
           
n  was higher than the other two on just
flag
     
            
heart cleveland
     
            
one dataset  and had a lower average
heart hungarian
     
            
accuracy than n  
heart long beach va
     
            
these results support the hypothesis
heart more
     
           
heart
     
           
that the normalization scheme n 
heart swiss
     
            
achieves higher generalization accuracy
hepatitis
     
           
than n  or n   on these datasets  due to
horse colic
                  
soybean large
     
            
its more robust normalization though
average
     
           
accuracy for n  is almost as good as n  
note that proper normalization will not
table    generalization accuracy
using n   n  and n  
always necessarily improve generalization
accuracy  if one attribute is more
important than the others in classification  then giving it a higher weight may improve
classification  therefore  if a more important attribute is given a higher weight accidentally by
poor normalization  it may actually improve generalization accuracy  however  this is a
random improvement that is not typically the case  proper normalization should improve
generalization in more cases than not when used in typical applications 
as a consequence of the above results  n  is used as the normalization scheme for hvdm 
and the function normalized vdm is defined as in      
     empirical results of hvdm vs  euclidean and hoem
a nearest neighbor classifier  with k    using the three distance functions listed in table   was
tested on    datasets from the uci machine learning database repository  of these    datasets 
the results obtained on the    datasets that have at least some nominal attributes are shown in
table   
the results are approximately equivalent on datasets with only linear attributes  so the results
on the remaining datasets are not shown here  but can be found in section       fold crossvalidation was again used  and all three distance metrics used the same training sets and test sets
for each trial 
the results of these experiments are shown in table    the first column lists the name of the
database   test means the database was originally meant to be used as a test set  but was
instead used in its entirety as a separate database   the second column shows the results
obtained when using the euclidean distance function normalized by standard deviation on all
attributes  including nominal attributes  the next column shows the generalization accuracy
obtained by using the hoem metric  which uses range normalized euclidean distance for linear
attributes and the overlap metric for nominal attributes  the final column shows the accuracy
obtained by using the hvdm distance function which uses the standard deviation normalized
euclidean distance  i e   normalized diff as defined in equation     on linear attributes and the
normalized vdm function n  on nominal attributes 
the highest accuracy obtained for each database is shown in bold  entries in the euclid  and
  

fiimproved heterogeneous distance functions

hoem columns that are significantly
higher than hvdm  at a     or higher
confidence level  using a two tailed
paired t test  are marked with an
asterisk     
entries that are
significantly lower than hvdm are
marked with a less than sign     
as can be seen from table    the
hvdm distance functions overall
average accuracy was higher than that
of the other two metrics by over    
hvdm achieved as high or higher
generalization accuracy than the other
two distance functions in    of the   
datasets  the euclidean distance
function was highest in    datasets 
and hoem was highest in only   
datasets 
hvdm was significantly higher
than the euclidean distance function on
   datasets  and significantly lower on
only    similarly  hvdm was higher
than hoem on   datasets  and
significantly lower on only   
these results support the hypothesis
that hvdm handles nominal attributes
more appropriately than euclidean
distance or the heterogeneous
euclidean overlap metric  and thus
tends to achieve higher generalization
accuracy on typical applications 

database
euclid 
anneal
     
audiology
       
audiology test
       
australian
     
bridges
     
crx
     
echocardiogram
     
flag
       
heart cleveland
     
heart hungarian
       
heart long beach va      
heart more
     
heart swiss
       
hepatitis
     
horse colic
     
house votes   
       
image segmentation
     
led   
       
led creator
       
monks   test
     
monks   test
       
monks   test
       
mushroom
      
promoters
       
soybean large
       
soybean small
      
thyroid allbp
     
thyroid allhyper
     
thyroid allhypo
     
thyroid allrep
     
thyroid dis
     
thyroid hypothyroid      
thyroid sick euthyroid      
thyroid sick
       
zoo
     
average 
     

hoem
     
       
     
     
     
     
     
     
     
     
       
     
     
     
     
       
     
       
       
     
       
       
      
       
     
      
     
     
       
     
     
     
     
       
     
     

hvdm
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      
     
     
      
     
     
     
     
     
     
     
     
     
     

table    generalization accuracy of the
euclidean  hoem  and hvdm distance functions 

   interpolated value difference metric  ivdm 
in this section and section   we introduce distance functions that allow vdm to be applied
directly to continuous attributes  this alleviates the need for normalization between attributes 
it also in some cases provides a better measure of distance for continuous attributes than linear
distance 
for example  consider an application with an input attribute height and an output class that
indicates whether a person is a good candidate to be a fighter pilot in a particular airplane 
those individuals with heights significantly below or above the preferred height might both be
considered poor candidates  and thus it could be beneficial to consider their heights as more
similar to each other than to those of the preferred height  even though they are farther apart in a
linear sense 

  

fiwilson   martinez

on the other hand  linear attributes for which linearly distant values tend to indicate different
classifications should also be handled appropriately  the interpolated value difference metric
 ivdm  handles both of these situations  and handles heterogeneous applications robustly 
a generic version of the vdm distance function  called the discretized value difference
metric  dvdm  will be used for comparisons with extensions of vdm presented in this paper 
     ivdm learning algorithm
the original value difference metric  vdm  uses statistics derived from the training set
instances to determine a probability pa x c that the output class is c given the input value x for
attribute a 
when using ivdm  continuous values are discretized into s equal width intervals  though
the continuous values are also retained for later use   where s is an integer supplied by the user 
unfortunately  there is currently little guidance on what value of s to use  a value that is too
large will reduce the statistical strength of the values of p  while a value too small will not allow
for discrimination among classes  for the purposes of this paper  we use a heuristic to
determine s automatically  let s be   or c  whichever is greatest  where c is the number of
output classes in the problem domain  current research is examining more sophisticated
techniques for determining good values of s  such as cross validation  or other statistical
methods  e g   tapia   thompson        p        early experimental results indicate that the
value of s may not be critical as long as s  c and s  n  where n is the number of instances in
the training set  
the width wa of a discretized interval for attribute a is given by 
wa  

maxa  mina
s

    

where max a and mina are the maximum and minimum value  respectively  occurring in the
training set for attribute a 
as an example  consider the iris database from the uci machine learning databases  the
iris database has four continuous input attributes  the first of which is sepal length  let t be a
training set consisting of     of the     available training instances  and s be a test set
consisting of the remaining     
in one such division of the training set  the values in t for the sepal length attribute ranged
from     to      there are only three output classes in this database  so we let s    resulting in a
width of                         note that since the discretization is part of the learning process  it
would be unfair to use any instances in the test set to help determine how to discretize the
values  the discretized value v of a continuous value x for attribute a is an integer from   to s 
and is given by 
 x  if a is discrete  else

v   discretizea  x     s  if x   max a   else
  x  min     w    
a
a


    

after deciding upon s and finding w a  the discretized values of continuous attributes can be
  

fiimproved heterogeneous distance functions

used just like discrete values of nominal attributes in finding pa x c  figure   lists pseudo code
for how this is done 
learnp training set t 
for each attribute a
for each instance i in t
let x be the input value for attribute a of instance i 
v   discretizea x   which is just x if a is discrete 
let c be the output class of instance i 
increment na v c by   
increment na v by   
for each discrete value v  of attribute a 
for each class c
if na v  
then pa v c  
else pa v c   na v c   na v
return   d array pa v c 

figure    pseudo code for finding pa x c 

probability

for the first attribute of the iris database  the values of pa x c are displayed in figure    for
each of the five discretized ranges of x  the probability for each of the three corresponding
output classes are shown as the bar heights  note that the heights of the three bars sum to    
for each discretized range  the bold integers indicate the discretized value of each range  for
example  a sepal length greater than or equal to      but less than      would have a discretized
value of   
   

   
   
   
   
   
   
   
   
   
   
 

    

output class 
   iris
setosa

    
    
    

    
    
   

 

    

         

   iris
viginica

         

 

   iris
versicolor

    

   

 

    

    

 

sepal length  in cm 

       
      

   

bold  
discretized
range number 

figure    pa x c for a    x       c       on the first attribute of the iris database 
     ivdm and dvdm generalization
thus far the dvdm and ivdm algorithms learn identically  however  at this point the dvdm
algorithm need not retain the original continuous values because it will use only the discretized
values during generalization  on the other hand  the ivdm will use the continuous values 
during generalization  an algorithm such as a nearest neighbor classifier can use the distance
function dvdm  which is defined as follows 
dvdm x  y   

m

 vdma  discretizea  xa    discretizea  ya   

a  

  

 

    

fiwilson   martinez

where discretizea is as defined in equation      and vdma is defined as in equation      with
q    we repeat it here for convenience 
vdma  x  y   

c

 pa x c  pa y c

 

    

c  

unknown input values  quinlan        are treated as simply another discrete value  as was done
in  domingos        

a 
b 

 
   
   

y 

   

input attributes
 
 
   
   
   
   
   

   

 
   
   

  
  

output class
   iris setosa 
   iris versicolor 

   

table    example from the iris database 
as an example  consider two training instances a and b as shown in table    and a new
input vector y to be classified  for attribute a    the discretized values for a  b  and y are      
and    respectively  using values from figure    the distance for attribute   between y and a is 
                                               

probability of class  

while the distance between y and b is    since they have the same discretized value 
note that y and b have values on different ends of range    and are not actually nearly as
close as y and a are  in spite of this fact  the discretized distance function says that y and b are
equal because they happen to fall into the same discretized range 
ivdm uses interpolation to alleviate such problems  ivdm assumes that the pa x c values
hold true only at the midpoint of each range  and interpolates between midpoints to find p for
other attribute values 
figure   shows the p values for the second output class  iris versicolor  as a function of the
first attribute value  sepal length   the dashed line indicates what p value is used by dvdm 
and the solid line shows what ivdm uses 
   
   
   
   
   
   
   
   
   
   
 

center
points
dvdm
ivdm

 

   

 

      

             
      
sepal length  in cm 

   

bold  
discretized
range number 

figure    p  x   values from the dvdm and ivdm for attribute    class   of the iris database 
  

fiimproved heterogeneous distance functions

the distance function for the interpolated value difference metric is defined as 
ivdm x  y   

m

 ivdma  xa   ya   

    

a  

where ivdma is defined as 
if a is discrete
 vdma  x  y  
c
 
ivdma  x  y    
p  x   pa c  y    otherwise
  a c
c  

    

the formula for determining the interpolated probability value pa c x  of a continuous value x
for attribute a and class c is 


x  mida u
pa c  x    pa u c   
    pa u   c  pa u c  
 mida u    mida u 

    

in this equation  mida u and mida u   are midpoints of two consecutive discretized ranges such
that mida u  x   mida u    pa u c is the probability value of the discretized range u  which is
taken to be the probability value of the midpoint of range u  and similarly for pa u   c   the
value of u is found by first setting u   discretizea x   and then subtracting   from u if x   mida u 
the value of mida u can then be found as follows 
mida u   mina   widtha    u    

    

probability of class

figure   shows the values of pa c x  for attribute a   of the iris database for all three output
classes  i e  c       and     since there are no data points outside the range mina  maxa  the
probability value pa u c is taken to be   when u     or u   s  which can be seen visually by the
diagonal lines sloping toward zero on the outer edges of the graph  note that the sum of the
probabilities for the three output classes sum to     at every point from the midpoint of range  
through the midpoint of range   
   
   
   
   
   
   
   
   
   
   
 

output class 
   iris setosa
   iris versicolor
   iris viginica

 

   

 

      

             
      
sepal length  in cm 

   

bold  
discretized
range number 

figure    interpolated probability values for attribute   of the iris database 
  

fiwilson   martinez

a
b

value
   
   

p     v 
    
    

p     v 
    
    

p     v 
    
    

y

   

    

    

    

ivdm  v y 
    
    

vdm  v y 
    
 

table    example of ivdm vs  vdm 
using ivdm on the example instances in table    the values for the first attribute are not
discretized as they are with dvdm  but are used to find interpolated probability values  in that
example  y has a value of      so p  c x  interpolates between midpoints   and    returning the
values shown in table   for each of the three classes  instance a has a value of      which also
falls between midpoints   and    but instance b has a value of      which falls between
midpoints   and   
as can be seen from table    ivdm  using the single attribute distance function ivdm 
returns a distance which indicates that y is closer to a than b  for the first attribute   which is
certainly the case here  dvdm  using the
discretized vdm   on the other hand  returns
database
dvdm
ivdm
annealing
     
       
a distance which indicates that the value of
australian
       
     
y is equal to that of b  and quite far from a 
bridges
     
     
illustrating the problems involved with
credit screening
     
     
echocardiogram
      
      
using discretization 
flag
     
     
the ivdm and dvdm algorithms were
glass
     
       
implemented and tested on    datasets
heart disease
     
     
heart  cleveland 
     
     
from the uci machine learning databases 
heart
 hungarian 
     
     
the results for the    datasets that contain
heart  long beach va       
     
at least some continuous attributes are
heart  more 
     
     
shown in table     since ivdm and
heart  swiss 
     
     
hepatitis
     
     
dvdm are equivalent on domains with
horse colic
     
     
only discrete attributes  the results on the
image segmentation
     
     
remaining datasets are deferred to section
ionosphere
     
     
iris
     
     
       fold cross validation was again
liver disorders
     
     
used  and the average accuracy for each
pima indians diabetes
     
     
database over all    trials is shown in
satellite image
     
       
shuttle
     
       
table    bold values indicate which value
sonar
     
     
was highest for each dataset  asterisks    
thyroid  allbp 
     
     
indicates that the difference is statistically
thyroid  allhyper 
     
       
thyroid  allhypo 
     
       
significant at a     confidence level or
thyroid  allrep 
     
       
higher  using a two tailed paired t test 
thyroid  dis 
     
     
on this set of datasets  ivdm had a
thyroid  hypothyroid 
     
       
higher average generalization accuracy
thyroid  sick 
     
       
thyroid  sick euthyroid       
       
overall than the discretized algorithm 
vehicle
     
       
ivdm obtained higher generalization
vowel
     
       
accuracy than dvdm in    out of   
wine
     
       
average 
     
     
cases     of which were significant at the
    level or above  dvdm had a higher
table    generalization for dvdm vs  ivdm 
  

fiimproved heterogeneous distance functions

accuracy in   cases  but only one of those had a difference that was statistically significant 
these results indicate that the interpolated distance function is typically more appropriate
than the discretized value difference metric for applications with one or more continuous
attributes  section   contains further comparisons of ivdm with other distance functions 

   windowed value difference metric  wvdm 
the ivdm algorithm can be thought of as sampling the value of pa u c at the midpoint mida u of
each discretized range u  p is sampled by first finding the instances that have a value for
attribute a in the range mida u  w a      na u is incremented once for each such instance  and
n a u c is also incremented for each instance whose output class is c  after which
p a u c   na u c   na u is computed  ivdm then interpolates between these sampled points to
provide a continuous but rough approximation to the function pa c x   it is possible to sample p
at more points and thus provide a closer approximation to the function pa c x   which may in
turn provide for more accurate distance measurements between values 
figure   shows pseudo code for the windowed value difference metric  wvdm   the
wvdm samples the value of p a x c at each value x occurring in the training set for each
define 
instance a     n  as the list of all n instances in t sorted in ascending order by attribute a 
instance a  i  val a  as the value of attribute a for instance a  i  
x
as the center value of the current window  i e   x instance a  i  val a  
p a  i  c 
as the probability pa x c that the output class is c given the input value x for
attribute a  note that i is an index  the not value itself 
n c 
as the number na x c of instances in the current window with output class c 
n
as the total number na x of instances in the current window 
instance a  in  as the first instance in the window 
instance a  out  as the first instance outside the window   i e   the window contains
instances instance a  in  out     
w a 
as the window width for attribute a 
learnwvdm training set t 
for each continuous attribute a
sort instance a     n  in ascending order by attribute a  using a quicksort 
initialize n and n c  to    and in and out to    i e   start with an empty window  
for each i    n
let x instance a  i  val a  
   expand window to include all instances in range
while  out   n  and  instance a  out  val a     x   w a     
increment n c   where c the class of instance a  out  
increment n 
increment out 
   shrink window to exclude instances no longer in range
while  in   out  and  instance a  in  val a     x   w a     
decrement n c   where c the class of instance a  in  
decrement n 
increment in 
   compute the probability value for each class from the current window
for each class c    c
p a  i  c    n c    n   i e   pa x c   na x c   na x  
return the   d array p a  i  c  

figure    pseudo code for the wvdm learning algorithm 
  

fiwilson   martinez

attribute a  instead of only at the midpoints of each range  in fact  the discretized ranges are not
even used by wvdm on continuous attributes  except to determine an appropriate window
width  wa  which is the same as the range width used in dvdm and ivdm  the pseudo code
for the learning algorithm used to determine pa x c for each attribute value x is given in figure   
for each value x occurring in the training set for attribute a  p is sampled by finding the
instances that have a value for attribute a in the range x  w a      and then computing na x 
na x c  and pa x c   na x c   na x as before  thus  instead of having a fixed number s of sampling
points  a window of instances  centered on each training instance  is used for determining the
probability at a given point  this technique is similar in concept to shifted histogram estimators
 rosenblatt        and to parzen window techniques  parzen        
for each attribute the values are sorted  using an o nlogn  sorting algorithm  so as to allow a
sliding window to be used and thus collect the needed statistics in o n  time for each attribute 
the sorted order is retained for each attribute so that a binary search can be performed in o log
n  time during generalization 
values occurring between the sampled points are interpolated just as in ivdm  except that
there are now many more points available  so a new value will be interpolated between two
closer  more precise values than with ivdm 
wvdm find p attribute a continuous value x 
   find pa x c for c    c  given a value x for attribute a 
find i such that instance a  i  val a   x  instance a  i    val a   binary search  
x    instance a  i  val a 
 unless i    in which case x  min a     w a       
x    instance a  i    val a   unless i n  in which case x  max a     w a       
for each class c    c
p  p a  i  c 
 unless i    in which case p    
p  p a  i    c   unless i n  in which case p    
pa x c   p      x x    x  x       p    p  
return array pa x    c 

figure     pseudo code for the wvdm probability interpolation  see figure   for definitions  
the pseudo code for the interpolation algorithm is given in figure     this algorithm takes
a value x for attribute a and returns a vector of c probability values pa x c for c    c  it first
does a binary search to find the two consecutive instances in the sorted list of instances for
attribute a that surround x  the probability for each class is then interpolated between that
stored for each of these two surrounding instances   the exceptions noted in parenthesis handle
outlying values by interpolating towards   as is done in ivdm  
once the probability values for each of an input vectors attribute values are computed  they
can be used in the vdm function just as the discrete probability values are 
the wvdm distance function is defined as 
wvdm x  y   

m

 wvdma  xa   ya   

    

a  

and wvdma is defined as 
if a is discrete
vdma  x  y  
 c
 
wvdma  x  y    
p
 pa y c   otherwise
  a x c
 c  

  

    

fiprobability of class

improved heterogeneous distance functions

   
   
   
   
   
   
   
   
   
   
 

output class 
   iris setosa
   iris versicolor
   iris viginica

 

 

 

 

 

sepal length  in cm 

figure     example of the wvdm probability landscape 
where pa x c is the interpolated probability value for the continuous value x as computed in
figure     note that we are typically finding the distance between a new input vector and an
instance in the training set  since the
instances in the training set were used to
database
dvdm wvdm
define the probability at each of their attribute
annealing
     
     
australian
     
     
values  the binary search and interpolation is
bridges
     
     
unnecessary for training instances because
credit screening
     
     
they can immediately recall their stored
echocardiogram
      
     
probability values  unless pruning techniques
flag
     
     
glass
     
       
have been used 
heart disease
     
     
one drawback to this approach is the
heart  cleveland 
     
     
increased storage needed to retain c
heart  hungarian 
     
     
heart  long beach va       
     
probability values for each attribute value in
heart  more 
     
     
the training set  execution time is not
heart  swiss 
     
     
significantly increased over ivdm or
hepatitis
     
     
horse colic
     
     
dvdm   see section     for a discussion on
image
segmentation
     
     
efficiency considerations  
ionosphere
     
     
figure    shows the probability values for
iris
     
     
liver disorders
     
     
each of the three classes for the first attribute
pima indians diabetes
     
     
of the iris database again  this time using the
satellite image
     
       
windowed sampling technique  comparing
shuttle
     
       
figure    with figure   reveals that on this
sonar
     
     
thyroid  allbp 
     
     
attribute ivdm provides approximately the
thyroid  allhyper 
     
     
same overall shape  but misses much of the
thyroid  allhypo 
     
     
detail  for example  the peak occurring for
thyroid  allrep 
     
     
thyroid  dis 
     
     
output class   at approximately sepal
thyroid  hypothyroid 
     
       
length       in figure   there is a flat line
thyroid  sick 
     
       
which misses this peak entirely  due mostly to
thyroid  sick euthyroid       
       
vehicle
     
       
the somewhat arbitrary position of the
vowel
     
       
midpoints at which the probability values are
wine
     
       
sampled 
average 
     
     
table   summarizes the results of testing
table    generalization of wvdm vs  dvdm 
  

fiwilson   martinez

the wvdm algorithm on the same datasets as dvdm and ivdm  a bold entry again indicates
the highest of the two accuracy measurements  and an asterisk     indicates a difference that is
statistically significant at the     confidence level  using a two tailed paired t test 
on this set of databases  wvdm was an average of      more accurate than dvdm
overall  wvdm had a higher average accuracy than dvdm on    out of the    databases  and
was significantly higher on    while dvdm was only higher on    databases  and none of those
differences were statistically significant 
section   provides further comparisons of wvdm with other distance functions  including
ivdm 

   empirical comparisons and analysis of distance functions
this section compares the distance functions discussed in this paper  a nearest neighbor
classifier was implemented using each of six different distance functions  euclidean
 normalized by standard deviation  and hoem as discussed in section    hvdm as discussed
in section    dvdm and ivdm as discussed in section    and wvdm as discussed in section
   figure    summarizes the definition of each distance function 
all functions use the same
overall distance function 

d x  y   

m

 da  xa   ya   

a  

distance
function
euclidean

definition of da xa ya  for each attribute type 
linear
continuous
discrete nominal
xa  ya
xa  ya
a
a

hoem

xa  ya
rangea

  if xa   ya
  if xa  ya

hvdm

xa  ya
  a

vdma  xa   ya  

dvdm

vdma disca xa  disca ya  

vdma  xa   ya  

ivdm

ivdma xa ya 
interpolate probabilities
from range midpoints 

vdma  xa   ya  

wvdm

wvdma xa ya 
interpolate probabilities
from adjacent values 

vdma  xa   ya  

where rangea   maxa  mina   and vdma  x  y   

c

 pa x c  pa y c

 

c  

figure     summary of distance function definitions 
each distance function was tested on    datasets from the uci machine learning databases 
  

fiimproved heterogeneous distance functions

again using    fold cross validation  the average accuracy over all    trials is reported for
each test in table    the highest accuracy achieved for each dataset is shown in bold  the
names of the three new distance functions presented in this paper  hvdm  ivdm and wvdm 
are also shown in bold to identify them 
table   also lists the number of instances in each database   inst    and the number of
continuous  con   integer  int  i e   linear discrete   and nominal  nom  input attributes 
d i s t
database
euclid hoem
annealing
     
     
audiology
     
     
audiology  test 
     
     
australian
     
     
breast cancer
     
     
bridges
     
     
credit screening
     
     
echocardiogram
     
     
flag
     
     
glass
     
     
heart disease
     
     
heart  cleveland 
     
     
heart  hungarian 
     
     
heart  long beach va 
     
     
heart  more 
     
     
heart  swiss 
     
     
hepatitis
     
     
horse colic
     
     
house votes   
     
     
image segmentation
     
     
ionosphere
     
     
iris
     
     
led    noise
     
     
led
     
     
liver disorders
     
     
monks  
     
     
monks  
     
     
monks  
     
     
mushroom
             
pima indians diabetes
     
     
promoters
     
     
satellite image
     
     
shuttle
     
     
sonar
     
     
soybean  large 
     
     
soybean  small 
             
thyroid  allbp 
     
     
thyroid  allhyper 
     
     
thyroid  allhypo 
     
     
thyroid  allrep 
     
     
thyroid  dis 
     
     
thyroid  hypothyroid 
     
     
thyroid  sick euthyroid       
     
thyroid  sick 
     
     
vehicle
     
     
vowel
     
     
wine
     
     
zoo
     
     
average 
     
     

a n c e
hvdm
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     
     
     
     
     

f u n
dvdm
     
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     
     
     
     
     

c t i o n
  of inputs
ivdm wvdm  inst  con int nom
     
     
   
   
  
     
     
   
   
  
     
     
  
   
  
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
      
     
   
   
 
     
     
   
   
  
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
     
          
   
 
     
     
   
   
 
     
     
   
   
  
     
     
   
   
  
     
     
   
   
  
     
     
        
 
     
     
        
 
     
     
   
   
 
     
           
   
  
     
          
   
 
     
     
   
   
 
     
     
   
   
 
     
     
   
   
 
             
   
   
 
                  
   
  
     
     
   
   
 
     
     
   
   
  
     
               
 
     
          
   
 
     
     
        
 
     
     
   
   
  
             
  
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
          
   
  
     
     
        
 
     
     
        
 
     
     
        
 
     
     
  
   
  
     
     

table    summary of generalization accuracy

  

fiwilson   martinez

on this set of    datasets  the three new distance functions  hvdm  ivdm and wvdm  did
substantially better than euclidean distance or hoem  ivdm had the highest average accuracy
         and was almost    higher on average than euclidean distance           indicating
that it is a more robust distance function on these datasets  especially those with nominal
attributes  wvdm was only slightly lower than ivdm with        accuracy  somewhat
surprisingly  dvdm was slightly higher than hvdm on these datasets  even though it uses
discretization instead of a linear distance on continuous attributes  all four of the vdm based
distance functions outperformed euclidean distance and hoem 
out of the    datasets  euclidean distance had the highest accuracy    times  hoem was
highest   times  hvdm      dvdm      ivdm     and wvdm     
for datasets with no continuous attributes  all four of the vdm based distance functions
 hvdm  dvdm  ivdm and wvdm  are equivalent  on such datasets  the vdm based
distance functions achieve an average accuracy of       compared to       for hoem and
      for euclidean  indicating a substantial superiority on such problems 
for datasets with no nominal attributes  euclidean and hvdm are equivalent  and all the
distance functions perform about the same on average except for dvdm  which averages about
   less than the others  indicating the detrimental effects of discretization  euclidean and
hoem have similar definitions for applications without any nominal attributes  except that
euclidean is normalized by standard deviation while hoem is normalized by the range of each
attribute  it is interesting that the average accuracy over these datasets is slightly higher for
euclidean than hoem  indicating that the standard deviation may provide better normalization
on these datasets  however  the difference is small  less than      and these datasets do not
contain many outliers  so the difference is probably negligible in this case 
one disadvantage with scaling attributes by the standard deviation is that attributes which
almost always have the same value  e g   a boolean attribute that is almost always    will be
given a large weightnot due to scale  but because of the relative frequencies of the attribute
values  a related problem can occur in hvdm  if there is a very skewed class distribution
 i e   there are many more instances of some classes than others   then the p values will be quite
small for some classes and quite large for others  and in either case the difference  pa x c   pa y c 
will be correspondingly small  and thus nominal attributes will get very little weight when
compared to linear attributes  this phenomenon was noted by ting               where he
recognized such problems on the hypothyroid dataset  future research will address these
normalization problems and look for automated solutions  fortunately  dvdm  ivdm and
wvdm do not suffer from either problem  because all attributes are scaled by the same amount
in such cases  which may in part account for their success over hvdm in the above
experiments 
for datasets with both nominal and continuous attributes  hvdm is slightly higher than
euclidean distance on these datasets  which is in turn slightly higher than hoem  indicating
that the overlap metric may not be much of an improvement on heterogeneous databases 
dvdm  ivdm and wvdm are all higher than euclidean distance on such datasets  with
ivdm again in the lead 
     effects of sparse data
distance functions that use vdm require some statistics to determine distance  we therefore
hypothesized that generalization accuracy might be lower for vdm based distance functions
  

fiimproved heterogeneous distance functions

than for euclidean distance or hoem when there was very little data available  and that vdmbased functions would increase in accuracy more slowly than the others as more instances were
made available  until a sufficient number of instances allowed a reasonable sample size to
determine good probability values 

     

 average generalization accuracy

     

     

euclidean
     
hoem
hvdm

     

dvdm
ivdm

     

wvdm
     
 

  

  
  
 instances used

  

   

figure     average accuracy as the amount of data increases 
to test this hypothesis  the experiments used to obtain the results shown in table   were
repeated using only part of the available training data  figure    shows how the generalization
accuracy on the test set improves as the percentage of available training instances used for
learning and generalization is increased from    to       the generalization accuracy values
shown are the averages over all    of the datasets in table   
surprisingly  the vdm based distance functions increased in accuracy as fast or faster than
euclidean and hoem even when there was very little data available  it may be that when there
is very little data available  the random positioning of the sample data in the input space has a
greater detrimental affect on accuracy than does the error in statistical sampling for vdm based
functions 
it is interesting to note from figure    that the six distance functions seem to pair up into
three distinct pairs  the interpolated vdm based distance functions  ivdm and wvdm 
maintain the highest accuracy  the other two vdm based functions are next  and the functions
based only on linear and overlap distance remain lowest from very early in the graph 
  

fiwilson   martinez

     efficiency considerations
this section considers the storage requirements  learning speed  and generalization speed of
each of the algorithms presented in this paper 
       storage
all of the above distance functions must store the entire training set  requiring o nm  storage 
where n is the number of instances in the training set and m is the number of input attributes in
the application  unless some instance pruning technique is used  for the euclidean and hoem
functions  this is all that is necessary  but even this amount of storage can be restrictive as n
grows large 
for hvdm  dvdm  and ivdm  the probabilities pa x c for all m attributes  only discrete
attributes for hvdm  must be stored  requiring o mvc  storage  where v is the average number
of attribute values for the discrete  or discretized  attributes and c is the number of output
classes in the application  it is possible to instead store an array da x y   vdma x y  for hvdm
and dvdm  but the storage would be o mv    which is only a savings when c   v 
for wvdm  c probability values must be stored for each continuous attribute value 
resulting in o nmc  storage which is typically much larger than o mvc  because n is usually
much larger than v  and cannot be less   it is also necessary to store a list of  pointers to 
instances for each attribute  requiring an additional o mn  storage  thus the total storage for
wvdm is o  c   nm    o cnm  
distance function
euclidean
hoem
hvdm
dvdm
ivdm
wvdm

storage
o mn 
o mn 
o mn mvc 
o mn mvc 
o mn mvc 
o cmn 

learning time
o mn 
o mn 
o mn mvc 
o mn mvc 
o mn mvc 
o mnlogn mvc 

generalization time
o mn 
o mn 
o mnc  or o mn 
o mnc  or o mn 
o mnc  or o mn 
o mnc 

table    summary of efficiency for six distance metrics 
table   summarizes the storage requirements of each system  wvdm is the only one of
these distance functions that requires significantly more storage than the others  for most
applications  n is the critical factor  and all of these distance functions could be used in
conjunction with instance pruning techniques to reduce storage requirements  see section   for
a list of several techniques to reduce the number of instances retained in the training set for
subsequent generalization 
       l earning speed
it takes nm time to read in a training set  it takes an additional  nm time to find the standard
deviation of the attributes for euclidean distance  or just nm time to find the ranges for hoem 
computing vdm statistics for hvdm  dvdm and ivdm takes mn mvc time  which is
approximately o mn   computing wvdm statistics takes mnlogn mnc time  which is
approximately o mnlogn  
in general  the learning time is quite acceptable for all of these distance functions 
  

fiimproved heterogeneous distance functions

       generalization speed
assuming that each distance function must compare a new input vector to all training instances 
euclidean and hoem take o mn  time  hvdm  ivdm and dvdm take o mnc   unless
da x y has been stored instead of pa x c for hvdm  in which case the search is done in o mn 
time   wvdm takes o logn mnc    o mnc  time 
though m and c are typically fairly small  the generalization process can require a
significant amount of time and or computational resources as n grows large  techniques such
as k d trees  deng   moore        wess  althoff   derwand        sproull        and
projection  papadimitriou   bentley        can reduce the time required to locate nearest
neighbors from the training set  though such algorithms may require modification to handle both
continuous and nominal attributes  pruning techniques used to reduce storage  as in section
       will also reduce the number of instances that must be searched for generalization 

   related work
distance functions are used in a variety of fields  including instance based learning  neural
networks  statistics  pattern recognition  and cognitive psychology  see section   for
references   section   lists several commonly used distance functions involving numeric
attributes 
normalization is often desirable when using a linear distance function such as euclidean
distance so that some attributes do not arbitrarily get more weight than others  dividing by the
range or standard deviation to normalize numerical attributes is common practice  turney
       turney   halasz        investigated contextual normalization  in which the standard
deviation and mean used for normalization of continuous attributes depend on the context in
which the input vector was obtained  in this paper we do not attempt to use contextual
normalization  but instead use simpler methods of normalizing continuous attributes  and then
focus on how to normalize appropriately between continuous and nominal attributes 
the value distance metric  vdm  was introduced by stanfill   waltz         it uses
attribute weights not used by the functions presented in this paper  the modified value
difference metric  mvdm   cost   salzberg        rachlin et al         does not use attribute
weights but instead uses instance weights  it is assumed that these systems use discretization
 lebowitz        schlimmer        to handle continuous attributes 
ventura        ventura   martinez        explored a variety of discretization methods for
use in systems that can use only discrete input attributes  he found that using discretization to
preprocess data often degraded accuracy  and recommended that machine learning algorithms
be designed to handle continuous attributes directly 
ting              used several different discretization techniques in conjunction with
mvdm and ib   aha  kibler   albert         his results showed improved generalization
accuracy when using discretization  discretization allowed his algorithm to use mvdm on all
attributes instead of using a linear distance on continuous attributes  and thus avoided some of
the normalization problems discussed above in sections     and      in this paper  similar
results can be seen in the slightly higher results of dvdm  which also discretizes continuous
attributes and then uses vdm  when compared to hvdm  which uses linear distance on
continuous attributes   in this paper  dvdm uses equal width intervals for discretization  while
  

fiwilson   martinez

tings algorithms make use of more advanced discretization techniques 
domingos        uses a heterogeneous distance function similar to hvdm in his rise
system  a hybrid rule and instance based learning system  however  rise uses a normalization
scheme similar to n  in sections     and      and does not square individual attribute
distances 
mohri   tanaka        use a statistical technique called quantification method ii  qm   to
derive attribute weights  and present distance functions that can handle both nominal and
continuous attributes  they transform nominal attributes with m values into m boolean
attributes  only one of which is on at a time  so that weights for each attribute can actually
correspond to individual attribute values in the original data 
turney        addresses cross validation error and voting  i e  using values of k      in
instance based learning systems  and explores issues related to selecting the parameter k  i e  
number of neighbors used to decide on classification   in this paper we use k     in order to
focus attention on the distance functions themselves  but accuracy would be improved on some
applications by using k     
ivdm and wvdm use nonparametric density estimation techniques  tapia   thompson 
      in determining values of p for use in computing distances  parzen windows  parzen 
      and shifting histograms  rosenblatt        are similar in concept to these techniques 
especially to wvdm  these techniques often use gaussian kernels or other more advanced
techniques instead of a fixed sized sliding window  we have experimented with gaussianweighted kernels as well but results were slightly worse than either wvdm or ivdm  perhaps
because of increased overfitting 
this paper applies each distance function to the problem of classification  in which an input
vector is mapped into a discrete output class  these distance functions could also be used in
systems that perform regression  atkeson  moore   schaal        atkeson        cleveland  
loader         in which the output is a real value  often interpolated from nearby points  as in
kernel regression  deng   moore        
as mentioned in section     and elsewhere  pruning techniques can be used to reduce the
storage requirements of instance based systems and improve classification speed  several
techniques have been introduced  including ib   aha  kibler   albert        aha         the
condensed nearest neighbor rule  hart         the reduced nearest neighbor rule  gates        
the selective nearest neighbor rule  rittler et al          typical instance based learning
algorithm  zhang         prototype methods  chang         hyperrectangle techniques
 salzberg        wettschereck   dietterich         rule based techniques  domingos        
random mutation hill climbing  skalak        cameron jones        and others  kibler   aha 
      tomek        wilson        

   conclusions   future research areas
there are many learning systems that depend on a reliable distance function to achieve accurate
generalization  the euclidean distance function and many other distance functions are
inappropriate for nominal attributes  and the hoem function throws away information and does
not achieve much better accuracy than the euclidean function itself 
the value difference metric  vdm  was designed to provide an appropriate measure of

  

fiimproved heterogeneous distance functions

distance between two nominal attribute values  however  current systems that use the vdm
often discretize continuous data into discrete ranges  which causes a loss of information and
often a corresponding loss in generalization accuracy 
this paper introduced three new distance functions  the heterogeneous value difference
function  hvdm  uses euclidean distance on linear attributes and vdm on nominal attributes 
and uses appropriate normalization  the interpolated value difference metric  ivdm  and
windowed value difference metric  wvdm  handle continuous attributes within the same
paradigm as vdm  both ivdm and wvdm provide classification accuracy which is higher on
average than the discretized version of the algorithm  dvdm  on the datasets with continuous
attributes that we examined  and they are both equivalent to dvdm on applications without any
continuous attributes 
in our experiments on    datasets  ivdm and wvdm achieved higher average accuracy
than hvdm  and also did better than dvdm  hoem and euclidean distance  ivdm was
slightly more accurate than wvdm and requires less time and storage  and thus would seem to
be the most desirable distance function on heterogeneous applications similar to those used in
this paper  properly normalized euclidean distance achieves comparable generalization
accuracy when there are no nominal attributes  so in such situations it is still an appropriate
distance function 
the learning system used to obtain generalization accuracy results in this paper was a nearest
neighbor classifier  but the hvdm  ivdm and wvdm distance functions can be used with a knearest neighbor classifier with k     or incorporated into a wide variety of other systems to
allow them to handle continuous values including instance based learning algorithms  such as
pebls   radial basis function networks  and other distance based neural networks  these new
distance metrics can also be used in such areas as statistics  cognitive psychology  pattern
recognition and other areas where the distance between heterogeneous input vectors is of
interest  these distance functions can also be used in conjunction with weighting schemes and
other improvements that each system provides 
the new distance functions presented here show improved average generalization on the   
datasets used in experimentation  it is hoped that these datasets are representative of the kinds
of applications that we face in the real world  and that these new distance functions will
continue to provide improved generalization accuracy in such cases 
future research will look at determining under what conditions each distance function is
appropriate for a particular application  we will also look closely at the problem at selecting
the window width  and will look at the possibility of smoothing wvdms probability landscape
to avoid overfitting  the new distance functions will also be used in conjunction with a variety
of weighting schemes to provide more robust generalization in the presence of noise and
irrelevant attributes  as well as increase generalization accuracy on a wide variety of
applications 

references
aha  david w           tolerating noisy  irrelevant and novel attributes in instance based
learning algorithms  international journal of man machine studies  vol      pp          
aha  david w   dennis kibler  and marc k  albert          instance based learning
algorithms  machine learning  vol     pp        
  

fiwilson   martinez

atkeson  chris          using local models to control movement  in d  s  touretzky  ed   
advances in neural information processing systems    san mateo  ca  morgan kaufmann 
atkeson  chris  andrew moore  and stefan schaal          locally weighted learning  to
appear in artificial intelligence review 
batchelor  bruce g           pattern recognition  ideas in practice  new york  plenum press 
pp        
biberman  yoram          a context similarity measure  in proceedings of the european
conference on machine learning  ecml      catalina  italy  springer verlag  pp        
broomhead  d  s   and d  lowe         multi variable functional interpolation and adaptive
networks  complex systems  vol     pp          
cameron jones  r  m           instance selection by encoding length heuristic with random
mutation hill climbing  in proceedings of the eighth australian joint conference on
artificial intelligence  pp         
carpenter  gail a   and stephen grossberg          a massively parallel architecture for a
self organizing neural pattern recognition machine  computer vision  graphics  and
image processing  vol      pp         
chang  chin liang          finding prototypes for nearest neighbor classifiers  ieee
transactions on computers  vol      no      pp            
cleveland  w  s   and c  loader          computational methods for local regression 
technical report     murray hill  nj  at t bell laboratories  statistics department 
cost  scott  and steven salzberg          a weighted nearest neighbor algorithm for
learning with symbolic features  machine learning  vol      pp        
cover  t  m   and p  e  hart          nearest neighbor pattern classification  institute of
electrical and electronics engineers transactions on information theory  vol      no    
pp        
dasarathy  belur v           nearest neighbor  nn  norms  nn pattern classification
techniques  los alamitos  ca  ieee computer society press 
deng  kan  and andrew w  moore          multiresolution instance based learning  to
appear in the proceedings of the international joint conference on artificial intelligence
 ijcai    
diday  edwin          recent progress in distance and similarity measures in pattern
recognition  second international joint conference on pattern recognition  pp          
domingos  pedro          rule induction and instance based learning  a unified approach 
to appear in the      international joint conference on artificial intelligence  ijcai     
dudani  sahibsingh a           the distance weighted k nearest neighbor rule  ieee
transactions on systems  man and cybernetics  vol     no     april       pp          
  

fiimproved heterogeneous distance functions

gates  g  w           the reduced nearest neighbor rule  ieee transactions on information
theory  vol  it     no     pp          
giraud carrier  christophe  and tony martinez          an efficient metric for heterogeneous
inductive learning applications in the attribute value language  intelligent systems  pp 
        
hart  p  e           the condensed nearest neighbor rule  institute of electrical and
electronics engineers transactions on information theory  vol      pp          
hecht nielsen  r           counterpropagation networks  applied optics  vol      no      pp 
          
kibler  d   and david w  aha          learning representative exemplars of concepts  an
initial case study  proceedings of the fourth international workshop on machine
learning  irvine  ca  morgan kaufmann  pp        
kohonen  teuvo          the self organizing map  in proceedings of the ieee  vol      no 
   pp            
lebowitz  michael          categorizing numeric information for generalization  cognitive
science  vol     pp          
merz  c  j   and p  m  murphy          uci repository of machine learning databases 
irvine  ca  university of california irvine  department of information and computer
science  internet  http   www ics uci edu  mlearn mlrepository html 
michalski  ryszard s   robert e  stepp  and edwin diday          a recent advance in data
analysis  clustering objects into classes characterized by conjunctive concepts 
progress in pattern recognition  vol     laveen n  kanal and azriel rosenfeld  eds   
new york  north holland  pp        
mitchell  tom m           the need for biases in learning generalizations  in j  w  shavlik
  t  g  dietterich  eds    readings in machine learning  san mateo  ca  morgan
kaufmann        pp          
mohri  takao  and hidehiko tanaka          an optimal weighting criterion of case
indexing for both numeric and symbolic attributes  in d  w  aha  ed    case based
reasoning  papers from the      workshop  technical report ws        menlo park 
ca  aiii press  pp          
nadler  morton  and eric p  smith          pattern recognition engineering  new york 
wiley  pp          
nosofsky  robert m           attention  similarity  and the identification categorization
relationship  journal of experimental psychology  general  vol       no     pp        
papadimitriou  christos h   and jon louis bentley          a worst case analysis of nearest
neighbor searching by projection  lecture notes in computer science  vol     
automata languages and programming  pp          
  

fiwilson   martinez

parzen  emanuel          on estimation of a probability density function and mode  annals of
mathematical statistics  vol      pp            
quinlan  j  r           unknown attribute values in induction  in proceedings of the  th
international workshop on machine learning  san mateo  ca  morgan kaufmann  pp 
        
rachlin  john  simon kasif  steven salzberg  david w  aha          towards a better
understanding of memory based and bayesian classifiers  in proceedings of the
eleventh international machine learning conference  new brunswick  nj  morgan
kaufmann  pp          
renals  steve  and richard rohwer          phoneme classification experiments using radial
basis functions  in proceedings of the ieee international joint conference on neural
networks  ijcnn     vol     pp          
rittler  g  l   h  b  woodruff  s  r  lowry  and t  l  isenhour          an algorithm for a
selective nearest neighbor decision rule  ieee transactions on information theory 
vol      no     pp          
rosenblatt  murray          remarks on some nonparametric estimates of a density function 
annals of mathematical statistics  vol      pp          
rumelhart  d  e   and j  l  mcclelland          parallel distributed processing  mit press 
ch     pp          
salzberg  steven          a nearest hyperrectangle learning method  machine learning 
vol     pp          
schaffer  cullen          selecting a classification method by cross validation  machine
learning  vol      no    
schaffer  cullen          a conservation law for generalization performance  in proceedings
of the eleventh international conference on machine learning  ml     morgan
kaufmann       
schlimmer  jeffrey c           learning and representation change  in proceedings of the
sixth national conference on artificial intelligence  aaai     vol     pp          
skalak  d  b           prototype and feature selection by sampling and random mutation hill
climbing algorithsm  in proceedings of the eleventh international conference on
machine learning  ml     morgan kaufman  pp          
sproull  robert f           refinements to nearest neighbor searching in k dimensional
trees  algorithmica  vol     pp          
stanfill  c   and d  waltz          toward memory based reasoning  communications of the
acm  vol      december       pp            

  

fiimproved heterogeneous distance functions

tapia  richard a   and james r  thompson          nonparametric probability density
estimation  baltimore  md  the johns hopkins university press 
ting  kai ming          discretization of continuous valued attributes and instance based
learning  technical report no       basser department of computer science  university
of sydney  australia 
ting  kai ming          discretisation in lazy learning  to appear in the special issue on
lazy learning in artificial intelligence review 
tomek  ivan          an experiment with the edited nearest neighbor rule  ieee
transactions on systems  man  and cybernetics  vol     no     june       pp          
turney  peter          theoretical analyses of cross validation error and voting in instancebased learning  journal of experimental and theoretical artificial intelligence  jetai  
pp          
turney  peter          exploiting context when learning to classify  in proceedings of the
european conference on machine learning  vienna  austria  springer verlag  pp         
turney  peter  and michael halasz          contextual normalization applied to aircraft gas
turbine engine diagnosis  journal of applied intelligence  vol     pp          
tversky  amos          features of similarity  psychological review  vol      no     pp         
ventura  dan          on discretization as a preprocessing step for supervised learning
models  masters thesis  department of computer science  brigham young university 
ventura  dan  and tony r  martinez         an empirical comparison of discretization
methods  in proceedings of the tenth international symposium on computer and
information sciences  pp          
wasserman  philip d           advanced methods in neural computing  new york  ny  van
nostrand reinhold  pp          
wess  stefan  klaus dieter althoff and guido derwand          using k d trees to improve
the retrieval step in case based reasoning  stefan wess  klaus dieter althoff    m  m 
richter  eds    topics in case based reasoning  berlin  springer verlag  pp          
wettschereck  dietrich  and thomas g  dietterich          an experimental comparison of
nearest neighbor and nearest hyperrectangle algorithms  machine learning  vol     
no     pp       
wettschereck  dietrich  david w  aha  and takao mohri          a review and comparative
evaluation of feature weighting methods for lazy learning algorithms  technical
report aic         washington  d c   naval research laboratory  navy center for
applied research in artificial intelligence 

  

fiwilson   martinez

wilson  d  randall  and tony r  martinez          the potential of prototype styles of
generalization  in proceedings of the sixth australian joint conference on artifical
intelligence  ai     pp          
wilson  d  randall  and tony r  martinez          heterogeneous radial basis functions  in
proceedings of the international conference on neural networks  icnn     vol     pp 
          
wilson  dennis l           asymptotic properties of nearest neighbor rules using edited
data  ieee transactions on systems  man  and cybernetics  vol     no     pp          
wolpert  david h           on overfitting avoidance as bias  technical report sfi tr            santa fe  nm  the santa fe institute 
zhang  jianping          selecting typical instances in instance based learning  proceedings
of the ninth international conference on machine learning 

  

fi