journal of artificial intelligence research                 

submitted       published     

query dags  a practical paradigm for implementing
belief network inference
adnan darwiche

darwiche aub edu lb

department of mathematics
american university of beirut
po box           beirut  lebanon

gregory provan

provan risc rockwell com

rockwell science center
     camino dos rios
thousand oaks  ca      

abstract

we describe a new paradigm for implementing inference in belief networks  which consists of two steps      compiling a belief network into an arithmetic expression called a
query dag  q dag   and     answering queries using a simple evaluation algorithm 
each node of a q dag represents a numeric operation  a number  or a symbol for evidence  each leaf node of a q dag represents the answer to a network query  that is 
the probability of some event of interest  it appears that q dags can be generated using any of the standard algorithms for exact inference in belief networks   we show how
they can be generated using clustering and conditioning algorithms  the time and space
complexity of a q dag generation algorithm is no worse than the time complexity of the
inference algorithm on which it is based  the complexity of a q dag evaluation algorithm
is linear in the size of the q dag  and such inference amounts to a standard evaluation of
the arithmetic expression it represents  the intended value of q dags is in reducing the
software and hardware resources required to utilize belief networks in on line  real world
applications  the proposed framework also facilitates the development of on line inference
on different software and hardware platforms due to the simplicity of the q dag evaluation
algorithm  interestingly enough  q dags were found to serve other purposes  simple techniques for reducing q dags tend to subsume relatively complex optimization techniques
for belief network inference  such as network pruning and computation caching 

   introduction
consider designing a car to have a self diagnostic system that can alert the driver to a range
of problems  figure   shows a simplistic belief network that could provide a ranked set
of diagnoses for car troubleshooting  given input from sensors hooked up to the battery 
alternator  fuel tank and oil system 
the standard approach to building such a diagnostic system is to put this belief network 
along with inference code  onto the car s computer  see figure    we have encountered a
number of diculties when using this approach to embody belief network technology in industrial applications  first  we were asked to provide the technology on multiple platforms 
for some applications  the technology had to be implemented in ada to pass certain certification procedures  in others  it had to be implemented on domain specific hardware that
only supports very primitive programming languages  second  memory was limited to keep

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fidarwiche   provan

fuel sensor
fuel
battery

oil pressure
battery sensor

fault

oil pressure
sensor
alternator
alternator
sensor

figure    a simple belief network for car diagnosis 
the cost of a unit below a certain threshold to maintain product profitability  the dilemma
was the following  belief network algorithms are not trivial to implement  especially when optimization is crucial  and porting these algorithms to multiple platforms and languages would
have been prohibitively expensive  time consuming and demanding of qualified manpower 
to overcome these diculties  we have devised a very exible approach for implementing
belief network systems  which is based on the following observation  almost all the work
performed by standard algorithms for belief networks is independent of the specific evidence
gathered about variables  for example  if we run an algorithm with the battery sensor set
to low and then run it later with the variable set to dead  we find almost no algorithmic
difference between the two runs  that is  the algorithm will not branch differently on any
of the key decisions it makes  and the only difference between the two runs is the specific
arguments to the invoked numeric operations  therefore  one can apply a standard inference
algorithm on a network with evidence being a parameter instead of being a specific value  the
result returned by the algorithm will then be an arithmetic expression with some parameters
that depend on specific evidence  this parameterized expression is what we call a query
dag  an example of which is shown in figure    
the approach we are proposing consists of two steps  first  given a belief network  a set
of variables about which evidence may be collected  evidence variables   and a set of variables for which we need to compute probability distributions  query variables   a q dag
is compiled off line  as shown in figure    the compilation is typically done on a sophisticated software hardware platform  using a traditional belief network inference algorithm in
conjunction with the q dag compilation method  this part of the process is far and away
the most costly computationally  second  an on line system composed from the generated
q dag and an evaluator specific to the given platform is used to evaluate the q dag  given
evidence  the parameterized arithmetic expression is evaluated in a straightforward manner
using simple arithmetic operations rather than complicated belief network inference  the
   the sharing of subexpressions is what makes this a directed acyclic graph instead of a tree 

   

fia practical paradigm for implementing belief network inference

traditional approach

compiled approach

fault
variables
causal
network

o
n
l
i
n
e

sensor
values

causal network
inference
software

sensor
variables

q dag
compiler

query
dag

q dag
evaluator

o
f
f
l
i
n
e

 
n
l
i
n
e

fault
probabilities

figure    this figure compares the traditional approach to exact belief network inference
 shown on the left  with our new compiled approach  shown on the right  in the
context of diagnostic reasoning  in the traditional approach  the belief network
and sensor values are used on line to compute the probability distributions over
fault variables  in the compiled approach  the belief network  fault variables and
sensor variables are compiled off line to produce a q dag  which is then evaluated
on line using sensor values to compute the required distributions 
computational work needed to perform this on line evaluation is so straightforward that it
lends itself to easy implementations on different software and hardware platforms 
this approach shares some commonality with other methods that symbolically manipulate probability expressions  like spi  li   d ambrosio        shachter  d ambrosio   
del favero         it differs from spi on the objective of such manipulations and  hence 
on the results obtained  spi explicates the notion of an arithmetic expression to state that
belief network inference can be viewed as an expression factoring operation  this allows
results from optimization theory to be utilized in belief network inference  on the other
hand  we define an arithmetic expression to explicate and formalize the boundaries between
on line and off line inference  with the goal of identifying the minimal piece of software that
is required on line  our results are therefore oriented towards this purpose and they include 
 a  a formal definition of a q dag and its evaluator   b  a method for generating q dags
using standard inference algorithms   an algorithm need not subscribe to the inference as   

fidarwiche   provan

query variables

evidence variables

causal network

q dag compiler
off line

on line
query dag

evidence

q dag evaluator

figure    the proposed framework for implementing belief network inference 

 a 

 b 

pr a on      

pr b off  c 

pr b on  c 

a

 

 

   

    

 

pr b on a 

   

    

b

c

a

 

 

 

 

a

 

pr c on a 

on

   

on

  

off

  

off

  

  

 
  

 c on 

 
 

 
  

 c off 

figure    a belief network  a   and its corresponding query dag  b   here  c is an evidence
variable  and we are interested in the probability of variable b  
factoring view to be used for q dag generation  and  c  computational guarantees on the
size of q dags in terms of the computational guarantees of the inference algorithm used
to generate them  although the spi framework is positioned to formulate related results  it
has not been pursued in this direction 
it is important to stress the following properties of the proposed approach  first  declaring an evidence variable in the compilation process does not mean that evidence must be
collected about that variable on line this is important because some evidence values  e g  
from sensors  may be lost in practice it only means that evidence may be collected  therefore  one can declare all variables to be evidence if one wishes  second  a variable can be
declared to be both evidence and query  this allows one to perform value of information
   

fia practical paradigm for implementing belief network inference

computations to decide whether it is worth collecting evidence about a specific variable 
third  the space complexity of a q dag in terms of the number of evidence variables is no
worse than the time complexity of its underlying inference algorithm  therefore  this is not
a simple enumerate all possible cases approach  finally  the time and space complexity for
generating a q dag is no worse than the time complexity of the standard belief network
algorithm used in its generation  therefore  if a network can be solved using a standard
inference algorithm  and if the time complexity of this algorithm is no worse than its space
complexity   then we can construct a q dag for that network 
the following section explains the concept of a q dag with a concrete example and
provides formal definitions  section   is dedicated to the generation of q dags and their
computational complexity  showing that any standard belief network inference algorithm
can be used to compile a q dag as long as it meets some general conditions  section  
discusses the reduction of a q dag after it has been generated  showing that such reduction
subsumes key optimizations that are typically implemented in belief network algorithms 
section   contains a detailed example on the application of this framework to diagnostic
reasoning  finally  section   closes with some concluding remarks 

   query dags

this section starts our treatment of q dags with a concrete example  we will consider a
particular belief network  define a set of queries of interest  and then show a q dag that
can be used to answer such queries  we will not discuss how the q dag is generated  only
how it can be used  this will allow a concrete introduction to q dags and will help us
ground some of the formal definitions to follow 
the belief network we will consider is the one in figure   a   the class of queries we
are interested in is pr  b j c    that is  the probability that variable b takes some value
given some known  or unknown  value of c   figure   b  depicts a q dag for answering
such queries  which is essentially a parameterized arithmetic expression where the values of
parameters depend on the evidence obtained  this q dag will actually answer queries of
the form pr  b  c    but we can use normalization to compute pr  b j c   
first  a number of observations about the q dag in figure   b  
 the q dag has two leaf nodes labeled pr  b on   c  and pr  b off   c   these are
called query nodes because their values represent answers to the queries pr  b on   c 
and pr  b off   c  
 the q dag has two root nodes labeled  c  on   and  c  off    these are called
evidence specific nodes  esns  since their values depend on the evidence collected
about variable c on line 
according to the semantics of q dags  the value of node  v  v   is   if variable v is
observed to be v or is unknown  and   otherwise  once the values of esns are determined 
we evaluate the remaining nodes of a q dag using numeric multiplication and addition 
the numbers that get assigned to query nodes as a result of this evaluation are the answers
to queries represented by these nodes 
   algorithms based on join trees have this property 

   

fidarwiche   provan

     

     

     

pr b off  c 

pr b on  c 

pr b off  c 

     

pr b on  c 

 

 
   

     

   

 

 

 

 

  

     

   

 

 

  

  

 

 

  

 

  

 

  

 

 

  

 

 

 

 

 

 

 c on 

  

  

  

 
 c off 

 

 b 

   

    

 

  

 

 

   

    

   

     

  

 

 a 

   

    

   

    

 

 

     

 c on 

 

  

 

 
  
 
 c off 

figure    evaluating the q dag in figure   with respect to two pieces of evidence   a 
c on and  b  c off  
for example  suppose that the evidence we have is c   on   then esn  c  on   is
evaluated to   and esn  c  off   is evaluated to    the q dag in figure   b  is then
evaluated as given in figure   a   thus leading to
pr  b on   c on           

and

pr  b off   c on           
from which we conclude that pr  c   on          we can then compute the conditional
probabilities pr  b on j c on   and pr  b off j c on   using 
pr  b on j c on     pr  b on   c on   pr  c on   
pr  b off j c on     pr  b off   c on   pr  c on   
if the evidence we have is c off   however  then  c  on   evaluates to   and  c  off  
evaluates to    the q dag in figure   b  will then be evaluated as given in figure   b  
thus leading to
pr  b on   c off           
and
pr  b off   c off           
we will use the following notation for denoting variables and their values  variables
are denoted using uppercase letters  such as a  b  c   and variable values are denoted by
lowercase letters  such as a  b  c  sets of variables are denoted by boldface uppercase letters 
such as a  b  c  and their instantiations are denoted by boldface lowercase letters  such as
a  b  c  we use e to denote the set of variables about which we have evidence  therefore 
   

fia practical paradigm for implementing belief network inference

we use e to denote an instantiation of these variables that represents evidence  finally  the
family of a variable is the set containing the variable and its parents in a directed acyclic
graph 
following is the formal definition of a q dag 

definition   a q dag is a tuple  v     i   d  z   where
   v is a distinguished set of symbols  called evidence variables 
    is a symbol  called unknown value 
   i maps each variable in v into a set of symbols  called variable values  different from
 
   d is a directed acyclic graph where
  each non root node is labeled with either   or 
  each root node is labeled with either
  a number in        or
  a pair  v  v   where v is an evidence variable and v is a value

   z is a distinguished set of nodes in d  called query nodes 

evidence variables v correspond to network variables about which we expect to collect
evidence on line  for example  in figure    c is the evidence variable  each one of these
variables has a set of possible values that are captured by the function i   for example  in
figure    the evidence variable c has values on and off   the special value  is used
when the value of a variable is not known  for example  we may have a sensor variable with
values  low    medium   and  high   but then lose the sensor value during on line reasoning 
in this case  we set the sensor value to    query nodes are those representing answers to
user queries  for example  in figure    b is the query variable  and leads to query nodes
pr b on   c  and pr b off   c  
an important notion is that of evidence 

definition   for a given q dag  v     i   d  z    evidence is defined as a function e that
maps each variable v in v into the set of values i  v     fg 
when a variable v is mapped into v   i  v    then evidence tells us that v is instantiated to
value v   when v is mapped into   then evidence does not tell us anything about the value
of v  
we can now state formally how to evaluate a q dag given some evidence  but first we
need some more notation 
   numeric node  n p  denotes a node labeled with a number p          
   esn  n v  v   denotes a node labeled with  v  v   
   this is also useful in cases where a variable will be measured only if its value of information justifies
that 

   

fidarwiche   provan

   operation node  n  
       
 ni denotes a node labeled with  and having parents
n           ni  
   operation node  n          ni denotes a node labeled with   and having parents
n           ni  
the following definition tells us how to evaluate a q dag by evaluating each of its nodes 
it is a recursive definition according to which the value assigned to a node is a function of
the values assigned to its parents  the first two cases are boundary conditions  assigning
values to root nodes  the last two cases are the recursive ones 
definition   for a q dag  v     i   d  z   and evidence e   the node evaluator is defined as
a function me that maps each node in d into a number        such that 
   me  n p     p
 the value of a node labeled with a number is the number itself  
 
e  v     v or e  v      
   me  n v  v           ifotherwise
 the value of an evidence specific node depends on the available evidence  it is   if v
is consistent with the evidence and   otherwise  
   me  n  
       
 ni     me  n           me  ni  
 the value of a node labeled with  is the product of the values of its parent nodes  
   me  n          ni     me  n             me  ni  
 the value of a node labeled with   is the sum of the values of its parent nodes  
one is typically not interested in the values of all nodes in a q dag since most of these
nodes represent intermediate results that are of no interest to the user  it is the query nodes
of a q dag that represent answers to user queries and it is the values of these nodes that one
seeks when constructing a q dag  the values of these queries are captured by the notion
of a q dag output 
definition   the node evaluator me is extended to q dags as follows 
me   v     i   d  z      f n  me  n   j n   zg 
the set me   v     i   d  z    is called the q dag output 
this output is what one seeks from a q dag  each element in this output represents a
probabilistic query and its answer 
let us consider a few evaluations of the q dag shown in figure    which are shown in
figure    given evidence e  c    on   and assuming that qnode b   on   and qnode b  
off   stand for the q dag nodes labeled pr  b on   c  and pr  b off   c   respectively 
we have
me  n c  on        
me  n c  off        
me  qnode b on                                                           
me  qnode b off                                                           
   

fia practical paradigm for implementing belief network inference

meaning that pr  b on   c on           and pr  b off   c on            if instead the
evidence were e  c   off   a set of analogous computations can be done 
it is also possible that evidence tells us nothing about the value of variable c   that is 
e  c       in this case  we would have

me  n c  on        
me  n c  off        
me  qnode b on                                                          
me  qnode b off                                                          
meaning that pr  b on          and pr  b off          

    implementing a q dag evaluator

a q dag evaluator can be implemented using an event driven  forward propagation scheme 
whenever the value of a q dag node changes  one updates the value of its children  and so
on  until no possible update of values is possible  another way to implement an evaluator
is using a backward propagation scheme where one starts from a query node and updates
its value by updating the values of its parent nodes  the specifics of the application will
typically determine which method  or combination  will be more appropriate 
it is important that we stress the level of refinement enjoyed by the q dag propagation scheme and the implications of this on the eciency of query updates  propagation in
q dags is done at the arithmetic operation level  which is contrasted with propagation at
the message operation level  used by many standard algorithms   such propagation schemes
are typically optimized by keeping validity ags of messages so that only invalid messages
are recomputed when new evidence arrives  this will clearly avoid some unnecessary computations but can never avoid all unnecessary computations because a message is typically
too coarse for this purpose  for example  if only one entry in a message is invalid  the
whole message is considered invalid  recomputing such a message will lead to many unnecessary computations  this problem will be avoided in q dag propagation since validity
ags are attributed to arithmetic operations  which are the building blocks of message operations  therefore  only the necessary arithmetic operations will be recomputed in a q dag
propagation scheme  leading to a more detailed level of optimization 
we also stress that the process of evaluating and updating a q dag is done outside of
probability theory and belief network inference  this makes the development of ecient online inference software accessible to a larger group of people who may lack strong backgrounds
in these areas  

    the availability of evidence

the construction of a q dag requires the identification of query and evidence variables  this
may give an incorrect impression that we must know up front which variables are observed
and which are not  this could be problematic in     applications where one may lose a sensor
reading  thus changing the status of a variable from being observed to being unobserved 
   in fact  it appears that a background in compiler theory may be more relevant to generating an ecient
evaluator than a background in belief network theory 

   

fidarwiche   provan

a

  

  

pr b true a 
  
  

true
false

pr b true b 

 

 

pr a 

a

b

pr a true b 

a
true

pr b false b 

 

pr a false b 

 

 

 

  

 

  

 b true 

 b false 

  

 

  

  

figure    a belief network and its corresponding q dag in which variable b is declared to
be both query and evidence 
and     applications where some variable may be expensive to observe  leading to an on line
decision on whether to observe it or not  using some value of information computation  
both of these situations can be dealt with in a q dag framework  first  as we mentioned
earlier  q dags allow us to handle missing evidence through the use of the  notation which
denotes an unknown value of a variable  therefore  q dags can handle missing sensor
readings  second  a variable can be declared to be both query and evidence  this means
that we can incorporate evidence about this variable when it is available  and also compute
the probability distribution of the variable in case evidence is not available  figure   depicts a
q dag in which variable a is declared to be a query variable  while variable b is declared to
be both an evidence and a query variable  both variables have true and false as their values  
in this case  we have two esns for variable b and also two query nodes  see figure     this
q dag can be used in two ways 
   to compute the probability distributions of variables a and b when no evidence is
available about b   under this situation  the values of n b  true   and n b  false   are
set to    and we have
pr  a   true                         
pr  a   false                         
pr  b   true                          
   

fia practical paradigm for implementing belief network inference

pr  b   false                          

   to compute the probability of variable a when evidence is available about b   for
example  suppose that we observe b to be false   the value of n b  true   will then be
set to   and the value of n b  false   will be set to    and we have
pr  a   true   b   false                 
pr  a   false   b   false                 

the ability to declare a variable as both an evidence and a query variable seems to be
essential in applications where     a decision may need to be made on whether to collect
evidence about some variable b   and     making the decision requires knowing the probability
distribution of variable b   for example  suppose that we are using the following formula
 pearl        page      to compute the utility of observing variable b  
x
utility of observing  b     pr b   bje  u  b   b  
b

where u  b   b  is the utility for the decision maker of finding that variable b has value b 
suppose that u  b   true          and u  b   false          we can use the q dag to
compute the probability distribution of b and use it to evaluate utility of observing  b   
utility of observing  b                                      

which leads us to observe variable b   observing b   we find that its value is false   we can
then accommodate this evidence into the q dag and continue with our analysis 

   generating query dags

this section shows how q dags can be generated using traditional algorithms for exact
belief network inference  in particular  we will show how q dags can be generated using the
clustering  join tree  jensen  ls  algorithm  jensen  lauritzen    olesen        shachter 
andersen    szolovits        shenoy   shafer         the polytree algorithm  and cutset
conditioning  pearl        peot   shachter         we will also outline properties that must
be satisfied by other belief network algorithms in order to adapt them for generating q dags
as we propose 

    the clustering algorithm

we provide a sketch of the clustering algorithm in this section  readers interested in more
details are referred to  shachter et al         jensen et al         shenoy   shafer        
according to the clustering method  we start by 
   constructing a join tree of the given belief network  
   a join tree is a tree of clusters that satisfies the following property  the intersection of any two clusters
belongs to all clusters on the path connecting them 

   

fidarwiche   provan

   assigning the matrix of each variable in the belief network to some cluster that contains
the variable s family 
the join tree is a secondary structure on which the inference algorithm operates  we need
the following notation to state this algorithm 
  s          sn are the clusters  where each cluster corresponds to a set of variables in the
original belief network 
   i is the potential function over cluster si   which is a mapping from instantiations of
variables in si into real numbers 
  pi is the posterior probability distribution over cluster si   which is a mapping from
instantiations of variables in si into real numbers 
  mij is the message sent from cluster si to cluster sj   which is a mapping from instantiations of variables in si   sj into real numbers 
  e is the given evidence  that is  an instantiation of evidence variables e 
we also assume the standard multiplication and marginalization operations on potentials 
our goal now is to compute the potential pr  x  e  which maps each instantiation x of
variable x in the belief network into the probability pr  x  e   given this notation  we can
state the algorithm as follows 
 potential functions are initialized using
y
 i   pr x x  
x

where

  x is a variable whose matrix is assigned to cluster si 
  pr x is the matrix for variable x   a mapping from instantiations of the family of
x into conditional probabilities  and
  x is the likelihood vector for variable x   x  x  is   if x is consistent with given

evidence e and   otherwise 
 posterior distributions are computed using

pi    i

y
k

mki  

where sk are the clusters adjacent to cluster si  
 messages are computed using
x y
mij  
 i mki  
si nsj

k  j

where sk are the clusters adjacent to cluster si  
   

fia practical paradigm for implementing belief network inference

 the potential pr  x  e  is computed using
pr  x  e   

x
si nfx g

pi  

where si is a cluster to which x belongs 
these equations are used as follows  to compute the probability of a variable  we must
compute the posterior distribution of a cluster containing the variable  to compute the
posterior distribution of a cluster  we collect messages from neighboring clusters  a message
from cluster si to sj is computed by collecting messages from all clusters adjacent to si
except for sj  
this statement of the join tree algorithm is appropriate for situations where the evidence
is not changing frequently since it involves computing initial potentials each time the evidence
changes  this is not necessary in general and one can provide more optimized versions of the
algorithm  this issue  however  is irrelevant in the context of generating q dags because
updating probabilities in face of evidence changes will take place at the q dag level  which
includes its own optimization technique that we discuss later 

    generating q dags

to generate q dags using the clustering method  we have to go through two steps  first 
we have to modify the initialization of potential functions so that the join tree is quantified
using q dag nodes instead of numeric probabilities  second  we have to replace numeric
addition and multiplication in the algorithm by analogous functions that operate on q dag
nodes  in particular 
   numeric multiplication  is replaced by an operation 
 that takes q dag nodes
n          ni as arguments  constructs and returns a new node n with label  and parents
n          ni  
   numeric addition   is replaced by an operation  that takes q dag nodes n           ni
as arguments  constructs and returns a new node n with label   and parents n           ni 
therefore  instead of numeric operations  we have q dag node constructors  and instead
of returning a number as a computation result  we now return a q dag node 
before we state the q dag clustering algorithm  realize that we now do not have evidence
e  but instead we have a set of evidence variables e for which we will collect evidence 
therefore  the q dag algorithm will not compute an answer to a query pr  x  e   but instead
will compute a q dag node that evaluates to pr  x  e  under the instantiation e of variables
e 
in the following equations  potentials are mappings from variable instantiations to qdag nodes  instead of numbers   for example  the matrix for variable x will map each
instantiation of x  s family into a q dag node n p  instead of mapping it into the number
p  the q dag operations 
 and  are extended to operate on these new potentials in the
same way that  and   are extended in the clustering algorithm 
the new set of equations is 
   

fidarwiche   provan

 potential functions are initialized using
o
o
 i   n pr x   
 n e   
x

e

where

  x is a variable whose matrix is assigned to cluster si 
  n pr x   is the q dag matrix for x   a mapping from instantiations of x  s family

into q dag nodes representing conditional probabilities 
  e is an evidence variable whose matrix is assigned to cluster si  and
  n e   is the q dag likelihood vector of variable e   n e   e    n e  e   which
means that node n e   e  evaluates to   if e is consistent with given evidence
and   otherwise 
 posterior distributions are computed using
o
pi    i mki  
k

where sk are the clusters adjacent to cluster si  
 messages are computed using
m o
mij  
 i mki  
si nsj

k  j

where sk are the clusters adjacent to cluster si  
 the q dag nodes for answering queries of the form pr  x  e  are computed using
m
qnode x    
pi  
si nfx g

where si is a cluster to which x belongs 
here qnode x   is a potential that maps each instantiation x of variable x into the q dag
node qnode x   x  which evaluates to pr  x  e  for any given instantiation e of variables e 
hence  the only modifications we made to the clustering algorithm are  a  changing
the initialization of potential functions and  b  replacing multiplication and addition with
q dag constructors of multiplication and addition nodes 

    an example

we now show how the proposed q dag algorithm can be used to generate a q dag for
the belief network in figure   a  
we have only one evidence variable in this example  c   and we are interested in generating a q dag for answering queries about variable b   that is  queries of the form pr  b  e  
figure   a  shows the join tree for the belief network in figure   a   where the tables contain
the potential functions needed for the probabilistic clustering algorithm  figure   b  shows
   

fia practical paradigm for implementing belief network inference

s 

ac
a
on
off

c on
  
  

 

 a 

ab

a
c off
  

s 

s 

ac
a

 

  
a
on
off

b on
        
       

b off
        
       

ab

a
c off

c on

on

n    

n c on 

n    

n c off 

off

n    

n c on 

n    

n c off 

a
 

 b 

b on

s 

 

b off

on

n      

n      

off

n     

n     

figure    a join tree quantified with numbers  a   and with q dag nodes  b  
the join tree again  but the tables contain the potential functions needed by the q dag
clustering algorithm  note that the tables are filled with q dags instead of numbers 
we now apply the q dag algorithm  to compute the q dag nodes that will evaluate
to pr  b  e   we must compute the posterior distribution p  over cluster s  since this is a
cluster to which variable b belongs  we can then sum the distribution over variable a to
obtain what we want  to compute the distribution p  we must first compute the message
m   from cluster s  to cluster s   
the message m   is computed by summing
m the potential function    of cluster s  over
all possible values of variable c   i e   m          which leads to 
c

m    a on      n     
 n c  on      n     
 n c  off    
m   a off      n     
 n c  on      n     
 n c  off    
the posterior distribution over cluster s    p    is computed using p       
 m     which

leads to

p   a on   b on     n       
   n     
 n c  on      n     
 n c  off    
p  a on   b off     n       
   n     
 n c  on      n     
 n c  off    
p  a off   b on     n      
   n     
 n c  on      n     
 n c  off    
p  a off   b off     n      
   n     
 n c  on      n     
 n c  off     
the q dag node qnode b  for answering queries m
of the form pr  b  e  is computed by
summing the posterior p  over variable a  qnode  
p    leading to
s nfb g
qnode b on      n       
   n     
 n c  on      n     
 n c  off      
 n      
   n     
 n c  on      n     
 n c  off     
qnode b off      n       
   n     
 n c  on      n     
 n c  off      
 n      
   n     
 n c  on      n     
 n c  off      
 

   

fidarwiche   provan

which is the q dag depicted in figure   b   therefore  the result of applying the algorithm
is two q dag nodes  one will evaluate to pr  b   on   e  and the other will evaluate to
pr  b off   e  under any instantiation e of evidence variables e 

    computational complexity of q dag generation

the computational complexity of the algorithm for generating q dags is determined by
the computational complexity of the clustering algorithm  in particular  the proposed algorithm applies a  operation precisely when the clustering algorithm applies an additionoperation  similarly  it applies a 
 operation precisely when the clustering algorithm applies
a multiplication operation  therefore  if we assume that  and 
 take constant time  then
both algorithms have the same time complexity 
each application of  or 
 ends up adding a new node to the q dag  and this is the
only way a new node can be added to the q dag  moreover  the number of parents of each
added node is equal to the number of arguments that the corresponding arithmetic operation
is invoked on in the clustering algorithm  therefore  the space complexity of a q dag is
the same as the time complexity of the clustering algorithm 
in particular  this means that the space complexity of q dags in terms of the number
of evidence variables is the same as the time complexity of the clustering algorithm in those
terms  moreover  each evidence variable e will add only m evidence specific nodes to the
q dag  where m is the number of values that variable e can take  this is important to
stress because without this complexity guarantee it may be hard to distinguish between the
proposed approach and a brute force approach that builds a big table containing all possible
instantiations of evidence variables together with their corresponding distributions of query
variables 

    other generation algorithms

the polytree algorithm is a special case of the clustering algorithm as shown in  shachter
et al          therefore  the polytree algorithm can also be modified as suggested above
to compute q dags  this also means that cutset conditioning can be easily modified to
compute q dags  for each instantiation c of the cutset c  we compute a q dag node for
pr  x  c  e  using the polytree algorithm and then take the  sum of the resulting nodes 
most algorithms for exact inference in belief networks can be adapted to generate qdags  in general  an algorithm must satisfy a key condition to be adaptable for computing
q dags as we suggested above  the condition is that the behavior of the algorithm should
never depend on the specific evidence obtained  but should only depend on the variables
about which evidence is collected  that is  whether variable e is instantiated to value v 
or value v  should not affect the complexity of the algorithm  only whether variable e is
instantiated or not should matter 
most belief networks algorithms that we are aware of satisfy this property  the reason
for this seems to be the notion of probabilistic independence on which these algorithms
are based  specifically  what is read from the topology of a belief network is a relation
i  x  z  y   stating that variables x and y are independent given variables z  that is 
pr  x  y j z    pr  x j z pr  y j z 
   

fia practical paradigm for implementing belief network inference

for all instantiations x  y  z of these variables  it is possible  however  for this not to hold
for all instantiations of z but only for specific ones  most standard algorithms we are aware
of do not take advantage of this instantiation specific notion of independence   therefore 
they cannot attach any computational significance to the specific value to which a variable
is instantiated  this property of existing algorithms is what makes them easily adaptable to
the generation of q dags 

    soundness of the q dag clustering algorithm

the soundness of the proposed algorithm is stated below  the proof is given in appendix a 

theorem   suppose that qnode x   is a q dag potential generated by the q dag clustering algorithm for query variable x and evidence variables e  let e  be an instantiation
of some variables in e  and let q dag evidence e be defined as follows 
 
if evidence e  sets variable e to value e 
e  e     e   otherwise 
then

me  qnode x   x     pr  x  e   

that is  the theorem guarantees that the q dag nodes generated by the algorithm will
always evaluate to their corresponding probabilities under any partial or full instantiation
of evidence variables 

   reducing query dags

this section is focused on reducing q dags after they have been generated  the main
motivation behind this reduction is twofold  faster evaluation of q dags and less space to
store them  interestingly enough  we have observed that a few  simple reduction techniques
tend in certain cases to subsume optimization techniques that have been inuential in practical implementations of belief network inference  therefore  reducing q dags can be very
important practically 
this section is structured as follows  first  we start by discussing four simple reduction
operations in the form of rewrite rules  we then show examples in which these reductions subsume two key optimization techniques known as network pruning and computation caching 

    reductions

the goal of q dag reduction is to reduce the size of a q dag while maintaining the
arithmetic expression it represents  in describing the equivalence of arithmetic expressions 
we define the notion of q dag equivalence 
definition   two q dags are equivalent iff they have the same set of evidence specific
nodes and they have the same output for all possible q dag evidence 
   some algorithms for two level binary networks  bn   networks   and some versions of the spi algorithm
do take advantage of these independences 

   

fidarwiche   provan

q

i

 

 

 
p

 

q
q 

q

p

 

 

q 

 
q 

q 

 
q  q  

 

 

q

 

q 

q 

b  numeric
reduction

c  associative
merging

q 

q 
q 

 a  identity
elimination

q 

q 

d  commutative
merging

figure    the four main methods for q dag reduction 
figure   shows four basic reduction operations that we have experimented with 
   identity elimination  eliminates a numeric node if it is an identity element of its child
operation node 
   numeric reduction  replaces an operation node with a numeric node if all its parents
are numeric nodes 
   associative merging  eliminates an operation node using operation associativity 
   commutative merging  eliminates an operation node using operation commutativity 
these rules can be applied successively and in different order until no more applications are
possible 
we have proven that these operations are sound in  darwiche   provan         based
on an analysis of network structure and preliminary empirical results  we have observed
that many factors govern the effectives of these operations  the degree to which reduction
operations  numeric reduction in particular  can reduce the size of the q dag depends on
the topology of the given belief network and the set of evidence and query variables  for
example  if all root nodes are evidence variables of the belief network  and if all leaf nodes
are query variables  then numeric reduction will lead to little q dag reduction 
we now focus on numeric reduction  showing how it sometimes subsumes two optimization techniques that have been inuential in belief network algorithms  for both optimizations  we show examples where an unoptimized algorithm that employs numeric reduction
yields the same q dag as an optimized algorithm  the major implication is that optimizations can be done uniformly at the q dag level  freeing the underlying belief network
algorithms from such implementational complications 
the following examples assume that we are applying the polytree algorithm to singlyconnected networks 
   

fia practical paradigm for implementing belief network inference

a

a

p a 

on

  

a
a

b

  
  

  

a

b
on
off

  
  

p b on a 
  
  

on
off

p c on b 

b

 a 

p a 

p b on a 

on
off

c

a
on

 b 

figure    a simple belief network before pruning  a  and after pruning  b   the light shaded
node  a  is a query node  and the dark shaded node  b   is an evidence node 

p a on b b 

p a on b b 

 

 

 

 b on 

 

  

  

  

 

 b off 

  

  

 

 

 

 
  

 

  

  

 b on 

 b off 

  

  

 a  original q dag

 b  reduced q dag

figure     a q dag  a  and its reduction  b  

    network pruning
pruning is the process of deleting irrelevant parts of a belief network before invoking inference  consider the network in figure   a  for an example  where b is an evidence variable
and a is a query variable  one can prune node c from the network  leading to the network
in figure   b   any query of the form pr  a j b  has the same value with respect to either
network  it should be clear that working with the smaller network is preferred  in general 
pruning can lead to dramatic savings since it can reduce a multiply connected network to a
singly connected one 
   

fidarwiche   provan

if we generate a q dag for the network in figure   a  using the polytree algorithm  we
obtain the one in figure    a   this q dag corresponds to the following expression 
x
x
pr  a on   e    pr  a on   b  b pr  b j a on   pr  c j b  
c

b

if we generate a q dag for the network in figure   b   however  we obtain the one in
figure    b  which corresponds to the following expression 
x
pr  a on   e    pr  a on   b  b pr  b j a on   
b

as expected  this q dag is smaller than the q dag in figure    a   and contains a subset
of the nodes in figure    a  
the key observation  however  is that the optimized q dag in figure    b  can be
obtained from the unoptimized one in figure    a  using q dag reduction  in particular 
the nodes enclosed in dotted lines can be collapsed using numeric reduction into a single
node with value    identity elimination can then remove the resulting node  leading to the
optimized q dag in figure    b  
the more general observation  however  is that prunable nodes contribute identity elements when computing answers to queries  these contributions appear as q dag nodes
that evaluate to identity elements under all instantiations of evidence  such nodes can be
easily detected and collapsed into these identity elements using numeric reduction  identity
elimination can then remove them from the q dag  leading to the same effect as network
pruning   whether q dag reduction can replace all possible pruning operations is an open
question that is outside the scope of this paper 

    computation caching

caching computations is another inuential technique for optimizing inference in belief networks  to consider an example  suppose that we are applying the polytree algorithm to
compute pr  c  b  in the network of figure     given evidence  say b  on   the algorithm
will compute pr  c  b   on   by passing the messages shown in figure     if the evidence
changes to b off   however  an algorithm employing caching will not recompute the message b  a   which represents the causal support from a to b  pearl         since the value of
this message does not depend on the evidence on b    this kind of optimization is typically
   note  however  that q dag reduction will not reduce the computational complexity of generating a qdag  although network pruning may  for example  a multiply connected network may become singlyconnected after pruning  thereby  reducing the complexity of generating a q dag  but using q dag
reduction  we still have to generate a q dag by working with a multiply connected network 
   this can be seen by considering the following expression  which is evaluated incrementally by the polytree
algorithm through its message passes 
pr  c  e   

x
b

pr  c j b  b  b 

 

x
a

pr  b j a  pr  a   

 z

c  b 

   z  
b a
 
   

it is clear that the subexpression corresponding to the message b  a  from a to b is independent of the
evidence on b  

   

fia practical paradigm for implementing belief network inference

a
b
c

a

pr a 

on

  

a

pr b on a 
  
  

on
off

pr c on b 

b

  
  

on
off

figure     a simple belief network for demonstrating the relationship between q dag reduction and computation caching  the light shaded node  c   is a query node 
and the dark shaded node  b   is an evidence node 

a

  a 
b
b

  b 
c
c

figure     message passing when c is queried and b is observed 
implemented by caching the values of messages and by keeping track of which messages are
affected by what evidence 
now  consider the q dag corresponding to this problem which is shown in figure    a  
the nodes enclosed in dotted lines correspond to the message from a to b    these nodes do
not have evidence specific nodes in their ancestor set and  therefore  can never change values
due to evidence changes  in fact  numeric reduction will replace each one of these nodes and
its ancestors with a single node as shown in figure    b  
in general  if numeric reduction is applied to a q dag  one is guaranteed the following 
 a  if a q dag node represents a message that does not depend on evidence  that node will
not be re evaluated given evidence changes  and  b  numeric reduction will guarantee this
p pr  b a pr  a  
   more precisely  they correspond to the expression
a

   

j

fidarwiche   provan

p c on b b 

p c on b b 

 

 

 

  

 
 

 

 

 b on 

 

  

 

   
 

 b off 

 

 b off 

 b on 

  

 

  

   

 

 

 

 

cached value
  

  

  

  
  

     

  

 a  original q dag

 b  reduced q dag

figure     a q dag  a  and its reduction  b  
under any q dag evaluation method since it will replace the node and its ancestor set with
a single root node   

    optimization in belief network inference

network pruning and computation caching have proven to be very inuential in practical
implementations of belief network inference  in fact  our own experience has shown that
these optimizations typically make the difference between a usable and a non usable beliefnetwork system 
one problem with these optimizations  however  is their algorithm specific implementations although they are based on general principles  e g   taking advantage of network
topology   another problem is that they can make elegant algorithms complicated and hard
to understand  moreover  these optimizations are often hard to define succinctly  and hence
are not well documented within the community 
in contrast  belief network inference can be optimized by generating q dags using unoptimized inference algorithms  and then optimizing the generated q dag through reduction techniques  we have shown some examples of this earlier with respect to pruning and
caching optimizations  however  whether this alternate approach to optimization is always
feasible is yet to be known  a positive answer will clearly provide an algorithm independent
    note that q dags lead to a very refined caching mechanism if the q dag evaluator     caches the value
of each q dag node and     updates these cached values only when there is need to  that is  when the
value of a parent node changes   such a refined mechanism allows caching the values of messages that
depend on evidence as well 

   

fia practical paradigm for implementing belief network inference

fuel sensor
fuel
battery

oil pressure
battery sensor

fault

oil pressure
sensor
alternator
alternator
sensor

figure     a simple belief network for car diagnosis 
approach to optimizing belief network inference  which is practically important for at least
two reasons  first  q dag reduction techniques seem to be much simpler to understand
and implement since they deal with graphically represented arithmetic expressions  without
having to invoke probability or belief network theory  second  reduction operations are applicable to q dags generated by any belief network algorithm  therefore  an optimization
approach based on q dag reduction would be more systematic and accessible to a bigger
class of developers 

   a diagnosis example
this section contains a comprehensive example illustrating the application of the q dag
framework to diagnostic reasoning 
consider the car troubleshooting example depicted in figure     for this simple case
we want to determine the probability distribution for the fault node  given evidence on four
sensors  the battery   alternator   fuel  and oil sensors  each sensor provides information
about its corresponding system  the fault node defines five possible faults  normal  cloggedfuel injector  dead battery  short circuit  and broken fuel pump 
if we denote the fault variable by f   and sensor variables by e  then we want to build
a system that can compute the probability pr  f  e   for each fault f and any evidence e 
these probabilities represent an unnormalized probability distribution over the fault variable
given sensor readings  in a q dag framework  realizing this diagnostic system involves three
steps  q dag generation  reduction  and evaluation  the first two steps are accomplished
off line  while the final step is performed on line  we now discuss each one of the steps in
more detail 

    q dag generation
the first step is to generate the q dag  this is accomplished by applying the q dag
clustering algorithm with the fault as a query variable and the sensors as evidence vari   

fidarwiche   provan

p f normal e 

p f normal 
   

p f pump 
   

fuel
 normal 

p f pump e 

fuel
subtree

 pump 

battery
 normal 

battery
 pump 

alternator
 normal 

oil
alternator
 pump 

 normal 

oil
 pump 

structure sharing

figure     a partial q dag for the car example  displaying two of the five query nodes 
broken fuel pump and normal  the shaded regions are portions of the q dag
that are shared by multiple query nodes  the values of these nodes are relevant
to the value of more than one query node 
ables  the resulting q dag has five query nodes  qnode f   normal   e   qnode f  
clogged fuel injector   e   qnode f   dead battery   e   qnode f   short circuit   e   and
qnode f   broken fuel pump  e   each node evaluates to the probability of the corresponding fault under any instantiation of evidence  the probabilities constitute a differential
diagnosis that tells us which fault is most probable given certain sensor values 
figure    shows a stylized description of the q dag restricted to two of the five query
nodes  corresponding to pr  f   broken fuel pump   e  and pr  f   normal   e   the q dag
structure is symmetric for each fault value and sensor 
given that the q dag is symmetric for these possible faults  for clarity of exposition
we look at just the subset needed to evaluate node pr  f   broken fuel pump   e   figure   
shows a stylized version of the q dag produced for this node  following are some observations about this q dag  first  there is an evidence specific node for every instantiation
of sensor variables  corresponding to all forms of sensor measurements possible  second  all
other roots of the q dag are probabilities  third  one of the five parents of the query node
pr f   broken fuel pump   e  is for the prior on f   broken fuel pump   and the other four
are for the contributions of the four sensors  for example  figure    highlights  in dots  that
part of the q dag for computing the contribution of the battery sensor 

    q dag reduction

after generating a q dag  one proceeds by reducing it using graph rewrite rules  figure   
shows an example of such reduction with a q dag that is restricted to one query node
for simplicity  to give an idea of the kind of reduction that has been applied  consider the
partial q dag enclosed by dots in this figure  figure    compares this reduced q dag with
the unreduced one from which it was generated  given our goal of generating q dags that
 a  can be evaluated as eciently as possible and  b  require minimal space to store  it is
   

fia practical paradigm for implementing belief network inference

key
f s fuel sensor
b s battery sensor
a s alternator sensor
o s oil sensor

p f pump e 

 

 

p f pump 
   

 

 

 

 

 

full 

 

 

 

 

 

    esn b s      esn a s      esn a s      esn o s      esn o s 

   esn f s     esn b s 

esn f s 
empty 

 

dead 

charged 

not ok 

ok 

low 

   

normal 

figure     a partial q dag for the car example 
 
 

 

 a  reduced q dag
esn b s 

    esn b s     

dead 

charged 

 

 b  original q dag

 

 

 
esn b s 

esn b s 

 

charged 

dead 

p b s charged  p b charged 
f pump b charged  f pump 
  

 

 

  

esn b s 

 

charged 

p b s dead 
p b s charged 
p b charged 
f pump b dead 
f pump b charged  f pump 
  

  

  

 
 
p b dead 
f pump 
  

esn b s 
dead 
p b s dead 
f pump b dead 
  

 
p b dead 
f pump 
  

figure     reduced and unreduced q dags for the car diagnosis example 
important to see  even in a simple example  how q dag reduction can make a big difference
in their size 
   

fidarwiche   provan

    q dag evaluation

now that we have a reduced q dag  we can use it to compute answers to diagnostic queries 
this section presents examples of this evaluation with respect to the generated q dag 
suppose that we obtain the readings dead  normal  ok and full for the battery  oil 
alternator and fuel sensors  respectively  and let us compute the probability distribution
over the fault variable  this obtained evidence is formalized as follows 
  e  battery sensor     dead  
  e  oil sensor     normal  
  e  alternator sensor     ok  
  e  fuel sensor     full  
evidence specific nodes can now be evaluated according to definition    for example  we
have
me  n battery sensor   charged       
and
me  n battery sensor   dead        
the evaluation of evidence specific nodes is shown pictorially in figure    a   definition  
can then be used to evaluate the remaining nodes  once the values of a node s parents
are known  the value of that node can be determined  figure    b  depicts the results of
evaluating other nodes  the result of interest here is the probability         assigned to the
query node pr  fault   broken fuel pump   e  
suppose now that evidence has changed so that the value of fuel sensor is empty instead
of full  to update the probability assigned to node pr  fault   broken fuel pump   e   a brute
force method will re evaluate the whole q dag  however  if a forward propagation scheme
is used to implement the node evaluator  then only four nodes need to be re evaluated in
figure    b   those enclosed in circles  instead of thirteen  the total number of nodes   we
stress this point because this refined updating scheme  which is easy to implement in this
framework  is much harder to achieve when one attempts to embed it in standard beliefnetwork algorithms based on message passing 

   concluding remarks

we have introduced a new paradigm for implementing belief network inference that is oriented towards real world  on line applications  the proposed framework utilizes knowledge
of query and evidence variables in an application to compile a belief network into an arithmetic expression called a query dag  q dag   each node of a q dag represents a numeric
operation  a number  or a symbol that depends on available evidence  each leaf node of a
q dag represents the answer to a network query  that is  the probability of some event of
interest  inference on q dags is linear in their size and amounts to a standard evaluation
of the arithmetic expressions they represent 
a most important point to stress about the work reported here is that it is not proposing
a new algorithm for belief network inference  what we are proposing is a paradigm for
   

fia practical paradigm for implementing belief network inference

pr f pump e 
 a  evaluating esns

 

 

   
pr f pump 

 

 
 

 
 

  

esn f s 
empty 

 

 

 

 

  

 

     

esn b s 
dead 

esn f s 
full 

 

   

esn b s 
charged 

 

 

 

     

esn a s 
not ok 

   

esn a s 
ok 

 
   

 

esn o s 
low 

 

esn o s 
normal 

   

esn values

        

pr f pump e 
 b  propagating probabilities

 

   
 
esn f s 
empty 

     

    

   
pr f pump 

     

    
  

 
esn f s 
full 

  

 
esn b s 
dead 

     

  

     
esn b s 
charged 

  
   

 
esn a s 
not ok 

     

    
     
esn a s 
ok 

   
   

 

esn o s 
low 

     

   

 

esn o s 
normal 

   

esn values

figure     evaluating the q dag for the car diagnosis example given evidence for sensors 
the bar in  a  indicates the instantiation of the esns  the shaded numbers in
 b  indicate probability values that are computed by the node evaluator  the
circled operations on the left hand side of  b  are the only ones that need to be
updated if evidence for the fuel system sensor is altered  as denoted by the circled
esns 

   

fidarwiche   provan

implementing belief network inference that is orthogonal to standard inference algorithms
and is engineered to meet the demands of real world  on line applications  this class of
applications is typically demanding for the following reasons 
   it typically requires very short response time  i e   milliseconds 
   it requires software to be written in specialized languages  such as ada  c    and
assembly before it can pass certification procedures 
   it imposes severe restrictions on the available software and hardware resources in order
to keep the cost of a  unit   such as an electromechanical device  as low as possible 
to address these real world constraints  we are proposing that one compile a belief network
into a q dag as shown in figure   on and use a q dag evaluator for on line reasoning  this
brings down the required memory to that needed for storing a q dag and its evaluator  it
also brings down the required software to that needed for implementing a q dag evaluator 
which is very simple as we have seen earlier 
our proposed approach still requires a belief network algorithm to generate a q dag 
but it makes the eciency of such an algorithm less of a critical factor    for example 
we show that some standard optimizations in belief network inference  such as pruning and
caching  become less critical in a q dag framework since these optimizations tend to be
subsumed by simple q dag reduction techniques  such as numeric reduction 
the work reported in this paper can be extended in at least two ways  first  further qdag reduction techniques could be explored  some oriented towards reducing the evaluation
time of q dags  others towards minimizing the memory needed to store them  second  we
have shown that some optimization techniques that dramatically improve belief network
algorithms may become irrelevant to the size of q dags if q dag reduction is employed 
further investigation is needed to prove formal results and guarantees on the effectiveness
of q dag reduction 
we close this section by noting that the framework we proposed is also applicable to
order of magnitude  omp  belief networks  where multiplication and addition get replaced
by addition and minimization  respectively  goldszmidt        darwiche   goldszmidt 
       the omp q dag evaluator  however  is much more ecient than its probabilistic
counterpart since one may evaluate a minimization node without having to evaluate all its
parents in many cases  this can make considerable difference in the performance of a q dag
evaluator 

acknowledgements
most of the work in this paper was carried out while the first author was at rockwell science
center  special thanks to jack breese  bruce d ambrosio and to the anonymous reviewers
for their useful comments on earlier drafts of this paper 
    we have shown how clustering and conditioning algorithms can be used for q dag generation  but other
algorithms such as spi  li   d ambrosio        shachter et al         can be used as well 

   

fia practical paradigm for implementing belief network inference

appendix a  proof of theorem  

without loss of generality  we assume in this proof that all variables are declared as evidence
variables  to prove this soundness theorem  all we need to show is that each q dag potential will evaluate to its corresponding probabilistic potential under all possible evidence 
formally  for any cluster s and variables x   the matrices of which are assigned to s   we
need to show that
o
y
me   n pr x   
 n x      pr x x
   
x

x

for a given evidence e   once we establish this  we are guaranteed that qnode x   x  will
evaluate to the probability pr  x  e  because the application of 
 and  in the q dag algorithm is isomorphic to the application of  and   in the probabilistic algorithm  respectively 
to prove equation    we will extend the q dag node evaluator me to mappings in the
standard way  that is  if f is a mapping from instantiations to q dag nodes  then me  f  
is defined as follows 
me  f   x   def me  f  x   
that is  we simply apply the q dag node evaluator to the range of mapping f  
note that me  f 
 g   will then be equal to me  f  me  g    therefore 
o
me   n pr x   
 n x   
xy
 
me  n pr x   me  n x   
x
y
 
pr x me  n x    by definition of n pr x   
x

note also that by definition of n x    we have that n x   x  equals n x  x   therefore 
me  n x    x    me  n x   x  
  m
  e  n x  x  
   if e  x     x or e  x     
 
   otherwise
  x  x  
therefore 
o
y
me   n pr x   
 n x      pr x x  
x

x

references

darwiche  a     goldszmidt  m          on the relation between kappa calculus and probabilistic reasoning  in proceedings of the tenth conference on uncertainty in artificial
intelligence  uai   pp          
darwiche  a     provan  g          query dags  a practical paradigm for implementing
on line causal network inference  tech  rep         rockwell science center  thousand
oaks  ca 
   

fidarwiche   provan

goldszmidt  m          qualitative probabilities  a normative framework for commonsense
reasoning  tech  rep  r      university of california at los angeles  ph d  thesis 
jensen  f  v   lauritzen  s     olesen  k          bayesian updating in recursive graphical
models by local computation  computational statistics quarterly             
li  z     d ambrosio  b          ecient inference in bayes networks as a combinatorial
optimization problem  international journal of approximate reasoning            
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible inference  morgan kaufmann publishers  inc   san mateo  california 
peot  m  a     shachter  r  d          fusion and propagation with multiple observations
in belief networks  artificial intelligence                  
shachter  r   andersen  s     szolovits  p          global conditioning for probabilistic
inference in belief networks  in proc  tenth conference on uncertainty in ai  pp 
        seattle wa 
shachter  r   d ambrosio  b     del favero  b          symbolic probabilistic inference in
belief networks  in proc  conf  on uncertainty in ai  pp          
shenoy  p  p     shafer  g          propagating belief functions with local computations 
ieee expert               

   

fi