journal of artificial intelligence research               

submitted        published      

coactive learning
pannaga shivaswamy

pshivaswamy linkedin com

linkedin corporation
     stierlin ct
mountain view  ca        usa

thorsten joachims

tj cs cornell edu

department of computer science
cornell university
ithaca  ny        usa

abstract
we propose coactive learning as a model of interaction between a learning system and
a human user  where both have the common goal of providing results of maximum utility
to the user  interactions in the coactive learning model take the following form  at each
step  the system  e g  search engine  receives a context  e g  query  and predicts an object
 e g  ranking   the user responds by correcting the system if necessary  providing a slightly
improved  but not necessarily optimal  object as feedback  we argue that such preference
feedback can be inferred in large quantity from observable user behavior  e g   clicks in
web search   unlike the optimal feedback required in the expert model or the cardinal
valuations required for bandit learning  despite the relaxed requirements for the feedback 
we show that it is possible to adapt many existing online learning algorithms
to the coactive

framework  in particular  we provide algorithms that achieve o    t   average regret in
terms of cardinal utility  even though the learning algorithm never observes cardinal utility
values directly  we also provide an algorithm with o log t   t   average regret in the
case of  strongly convex loss functions  an extensive empirical study demonstrates the
applicability of our model and algorithms on a movie recommendation task  as well as
ranking for web search 

   introduction
in a wide range of systems in use today  the interaction between human and system takes
the following form  the user issues a command  e g  query  and receives a  possibly
structured  result in response  e g  ranking   the user then interacts with the results  e g 
clicks   thereby providing implicit feedback about the users utility function  here are three
examples of such systems and their typical interaction patterns 
web search  in response to a query  a search engine presents the ranking  a  b  c  d      
and observes that the user clicks on documents b and d 
movie recommendation  an online service recommends movie a to a user  however 
the user rents movie b after browsing the collection 
machine translation  an online machine translator is used to translate a wiki page from
language a to b  the system observes some corrections the user makes to the translated text 
c
    
ai access foundation  all rights reserved 

fishivaswamy   joachims

in all the above examples  the user provides some feedback about the results of the
system  however  the feedback is only an incremental improvement  not necessarily the
optimal result  for example  from the clicks on the web search results we can infer that the
user would have preferred the ranking  b  d  a  c       over the one we presented  however 
this is unlikely to be the best possible ranking  similarly in the recommendation example 
movie b was preferred over movie a  but there may have been even better movies that
the user did not find while browsing  and in the machine translation example  the corrected text need not be the best possible translation from language a to language b  in all
three examples  the algorithm typically receives a slightly improved result from the user as
feedback  but not necessarily the optimal prediction or any cardinal utilities  we conjecture
that many other applications fall into this schema  ranging from news filtering to personal
robotics 
in this paper  we propose coactive learning as a model of such system user interactions 
we formalize coactive learning as a general model of interaction between a learning system
and its user  define a suitable notion of regret  and validate the key modeling assumption
 namely whether observable user behavior can provide valid feedback in our model  in a
user study for web search  the new model can be viewed as a cooperative learning process
between system and user  where both parties aim to optimize utility but lack the means to
achieve this goal on their own  specifically  the  boundedly rational  user is computationally
limited in maximizing utility over the space of alternatives  while the system is limited in
how well it knows the users utility function 
the proposed online learning framework differs significantly from existing online learning
models in terms of the observed feedback  see the related works section for a comparison   a
strength of the proposed framework is that it is possible to derive a wide range of coactive
learning algorithms by adapting existing online algorithms for convex optimization  we
provide a template for coactive learning algorithms and then show several instances of
this template in this paper  and in each case  we prove that the worst case analysis of
the algorithm carries over from the conventional online learning framework to coactive
learning despite the differences between the two models in particular  in the cases of linear
utility models and convex cost functions we show o    t   regret bounds with a matching
lower bound  we also show that the regret bound can be improved with a second order
algorithm for strongly convex functions  the learning algorithms perform structured output
prediction  see bakir  hofmann  scholkopf  smola  taskar    vishwanathan        and thus
can be applied in a wide variety of problems  we study several interesting extensions of
the framework using batch updates  expected feedback  and an exponentiated learning
algorithm  finally  we provide extensive empirical evaluations of our algorithms on a movie
recommendation and a web search task  showing that the algorithms are highly efficient
and effective in practical settings 
the rest of this paper is organized as follows  we discuss related work in section    in
section   we formally introduce the coactive learning model and also motivate the model
with a real world user study  we present the linear version of our algorithm along with
several extensions in section    in section    we then detail a general schema for deriving
coactive learning algorithms and their regret bounds  in particular  we derive an exponentiated gradient algorithm in section      and we propose coactive learning algorithms for
minimizing general convex losses and  strongly convex losses in sections     and      an
 

ficoactive learning

empirical evaluation of the proposed framework and algorithms is done in section   and we
conclude in section    we include most of our proofs in the appendix 

   related works
the coactive learning model bridges the gap between two forms of feedback that have been
well studied in online learning  on one side there is the multi armed bandit model  e g  
auer  cesa bianchi  freund    schapire      b  auer  cesa bianchi    fischer      a  
where an algorithm chooses an action and observes the utility of  only  that action  on
the other side  utilities of all possible actions are revealed in the case of learning with
expert advice  e g   cesa bianchi   lugosi      a   online convex optimization  zinkevich 
      hazan  agarwal    kale        and online convex optimization in the bandit setting
 flaxman  kalai    mcmahan        are continuous relaxations of the expert and the
bandit problems respectively  our model  where information about two arms is revealed
at each iteration  the one we presented and the one we receive as feedback from the user  
sits between the expert and the bandit setting  most closely related to coactive learning
is the dueling bandits setting  yue  broder  kleinberg    joachims        yue   joachims 
       the key difference is that both arms are chosen by the algorithm in the dueling
bandits setting  whereas one of the arms is chosen by the user in the coactive learning
setting  our model allows contextual information like in contextual bandits  langford  
zhang         however  the arms in our problem are structured objects such as rankings 
a summary of how our framework compares with other existing frameworks is shown in
table    other types of feedback have also been explored in the literature  for example  in
the multi class classification problems  after the algorithm makes a prediction based on the
context  the feedback received is only whether the prediction is correct or wrong as opposed
to the actual label  crammer   gentile        kakade  shalev shwartz    tewari        
this can be seen as observing partial feedback  as opposed to the actual cardinal feedback 
in a bandit problem 
as pointed out above  coactive learning algorithms and conventional online learning
algorithms operate in different types of environments  coactive learning algorithms present
an object and observe another object as a feedback  while online convex learning algorithms
pick a vector in each step and observe the gradient at that vector as feedback  despite the
contrast between online learning and coactive learning  two of the algorithms presented
in this paper are closely related to those in the work of zinkevich        and hazan et al 
        we show that it is possible to adapt the regret bounds of these algorithms to
corresponding regrets bounds for coactive learning  at the heart of all our algorithms and
analysis is the well known idea  polyak   tsypkin        that the descent algorithms do
not necessarily need to know the gradients  but that a vector with positive inner product
with the gradient in expectation suffices 
while feedback in coactive learning takes the form of a preference  it is different from
ordinal regression and ranking  ordinal regression  e g   crammer   singer        assumes
training examples  x  y   where y is a rank  in the coactive learning model  absolute ranks
are never revealed  more closely related is learning with pairs of examples  herbrich  graepel    obermayer        freund  iyer  schapire    singer        chu   ghahramani        
since it circumvents the need for absolute ranks  only relative orderings are required  vari 

fishivaswamy   joachims

framework
bandits
experts
dueling bandits
coactive learning

algorithm
pull an arm
pull an arm
pull two arms
pull an arm

feedback
observe cardinal reward for the arm pulled
observe cardinal rewards for all the arms
observe feedback on which one is better
observe another arm which is better

table    a comparison of different online learning frameworks 

ants of such pairwise ranking algorithms have been applied to natural language processing
 haddow  arun    koehn        zhang  lei  barzilay  jaakkola    globerson        and
image annotation  weston  bengio    usunier         however  existing approaches require
an iid assumption and typically perform batch learning  finally  there is a large body of
work on ranking  see liu         these approaches are different from coactive learning as
they require training data  x  y  where y is the optimal ranking for query x  however  we
will draw upon structured prediction approaches for ranking problems in the design of our
models 
coactive learning was first proposed by shivaswamy and joachims         this paper
serves as a journal extension of that paper  adding a complete discussion of batch updates
and expected feedback  the exponentiated gradient algorithm  the o log t   t   algorithm
for  strongly convex loss functions  and a substantially extended empirical evaluation 
since then  coactive learning has been applied to intrinsically diverse retrieval  raman 
shivaswamy    joachims         learning ranking function from click feedback  raman 
joachims  shivaswamy    schnabel         optimizing social welfare  raman   joachims 
       personal robotics  jain  wojcik  joachims    saxena         pattern discovery  boley 
mampaey  kang  tokmakov    wrobel         robotic monitoring  somers   hollinger 
       and extended to allow approximate inference  goetschalckx  fern    tadepalli        

   coactive learning model
we now introduce coactive learning as a model of interaction  in rounds  between a learning
system  e g  search engine  and a human  search user  where both the human and learning
algorithm have the same goal  of obtaining good results   at each round t  the learning
algorithm observes a context xt  x  e g  a search query  and presents a structured object
yt  y  e g  a ranked list of urls   the utility of yt  y to the user for context xt  x is
described by a utility function u  xt   yt    which is unknown to the learning algorithm  as
feedback the human user returns an improved object yt  y  e g  reordered list of urls  
i e   
u  xt   yt     u  xt   yt   

   

when such an object yt exists  in fact  we will also allow violations of     when we formally
model user feedback in section     
the process by which the user generates the feedback yt can be understood as an
approximately utility maximizing action  where the user is modeled as a boundedly rational
   the improvements should be not just strict  but by a margin  this will be clear in section     

 

ficoactive learning

agent  in particular  the user selects the feedback object yt by approximately maximizing
utility over a user defined subset yt of all possible y 
yt   argmaxyy u  xt   y 

   

this approximately and boundedly rational user may employ various tools  e g   query
reformulations  browsing  to construct the subset y and to perform this search  importantly 
however  the feedback yt is typically not the optimal label which is defined as 
yt    argmaxyy u  xt   y  

   

in this way  coactive learning covers settings where the user cannot manually optimize
the argmax over the full y  e g  produce the best possible ranking in web search   or has
difficulty expressing a bandit style cardinal rating u  xt   yt   for yt in a consistent manner 
this puts our preference feedback yt in stark contrast to supervised learning approaches
which require  xt   yt    but even more importantly  our model implies that reliable preference feedback     can be derived from observable user behavior  i e   clicks   as we will
demonstrate in section     for web search  we conjecture that similar feedback strategies
exist in many application settings  e g   jain et al         boley et al         somers  
hollinger        goetschalckx et al          especially when users can be assumed to act
approximately and boundedly rational according to u  
despite the weak preference feedback  the aim of the algorithm is nevertheless to present
objects with utility close to that of the optimal yt   whenever  the algorithm presents an
object yt under context xt   we say that it suffers a regret u  xt   yt    u  xt   yt   at time
step t  formally  we consider the average regret suffered by the algorithm over t steps as
follows 
regt  

t
 x
 u  xt   yt    u  xt   yt     
t

   

t  

the goal of the learning algorithm is to minimize regt   thereby providing the human with
predictions yt of high utility  note  however  that a cardinal value of u is never observed
by the learning algorithm  but u is only revealed ordinally through preferences     
    quantifying preference feedback quality
to provide any theoretical guarantees about the regret of a learning algorithm in the coactive
setting  we need to quantify the quality of the user feedback  note that this quantification is
a tool for theoretical analysis  not a prerequisite or parameter to the algorithm  we quantify
feedback quality by how much improvement y provides in utility space  in the simplest case 
we say that user feedback is strictly  informative when the following inequality is satisfied 
u  xt   yt    u  xt   yt     u  xt   yt    u  xt   yt    

   

in the above inequality           is an unknown parameter  feedback is such that utility of
yt is higher than that of yt by a fraction  of the maximum possible utility range u  xt   yt  
u  xt   yt    the term on the right hand side in the above inequality ensures that user feedback
 

fishivaswamy   joachims

yt is not only better than yt   but also better by a margin  u  xt   yt  u  xt   yt     violations
of the above feedback model are allowed by introducing slack variables t   
u  xt   yt    u  xt   yt      u  xt   yt    u  xt   yt     t  

   

note that the t are not restricted to be positive  but can be negative as well  we refer to
the above feedback model as  informative feedback  note also that it is possible to express
feedback of any quality using     with an appropriate value of t   our regret bounds will
contain t   quantifying to what extent the  informative modeling assumption is violated 
finally  we will also consider an even weaker feedback model where a positive utility
gain is only achieved in expectation over user actions 
et  u  xt   yt    u  xt   yt       u  xt   yt    u  xt   yt     t  

   

we refer to the above feedback as expected  informative feedback  in the above equation 
the expectation is over the users choice of yt given yt under context xt  i e   under a
distribution pxt  yt  yt   which is dependent on xt   
in the rest of this paper  we use a linear model for the utility function 
u  x  y    w   x  y  

   

where w  rn is a parameter vector unknown both to the learning system and users and
   x  y  rn is a known joint feature map known to the system such that 
k x  y k    r 

   

for any x  x and y  y  note that both x and y can be structured objects 
    user study  preferences from clicks
we now validate that reliable preferences as specified in equation     can indeed be inferred
from implicit user behavior  in particular  we focus on preference feedback from clicks in
web search and draw upon data from a user study  joachims  granka  pan  hembrooke 
radlinski    gay         in this study  subjects  undergraduate students  n       were
asked to answer    identical questions    informational    navigational  using the google
search engine  all queries  result lists  and clicks were recorded  for each subject  queries
were grouped into query chains by question    on average  each query chain contained    
queries and     clicks in the result lists 
we use the following strategy to infer a ranking y from the users clicks  prepend to the
ranking y from the first query of the chain all results that the user clicked throughout the
whole query chain  to assess whether u  x  y  is indeed larger than u  x  y  as assumed in
our learning model  we measure utility in terms of a standard
measure of retrieval quality
p   r x y i  
from information retrieval  we use dcg    x  y    i   log i     where r x  y i   is the
relevance score of the i th document in ranking y  see manning  raghavan    schutze 
   strictly speaking  the value of the slack variable depends on the choice of  and the definition of utility 
however  for brevity  we do not explicitly show this dependence in the notation 
   we make a slightly different assumption in section     
   this was done manually  but can be automated with high accuracy  jones   klinkner        

 

ficumulative distribution function

coactive learning

 
   
   
   
   
   
   
   
   
   
 

normal condition
swapped condition
reversed condition
all conditions
  

  

  

  

  
 
 
dcg x ybar  dcg x y 

 

 

 

 

figure    cumulative distribution of utility differences between presented ranking y and
click feedback ranking y in terms of dcg    for three experimental conditions
and overall 
       to get ground truth relevance assessments r x  d   five independent human assessors
 students  were asked to manually rank the set of results encountered during each query
chain  the assessors were also given the true answers for navigational queries  we then
linearly normalize the resulting ranks to a relative relevance score r x  d          for each
document 
we can now evaluate whether the feedback ranking y is indeed better than the ranking
y that was originally presented  i e   dcg    x  y    dcg    x  y   figure   plots the
cumulative distribution functions  cdfs  of dcg    x  y   dcg    x  y  for three
experimental conditions  as well as the average over all conditions  all cdfs are shifted far
to the right of    showing that preference feedback from our strategy is highly accurate and
informative  focusing first on the average over all conditions  the utility difference is strictly
positive on      of all queries  and strictly negative on only       this imbalance is
significant  binomial sign test  p            among the remaining      of cases where
the dcg    difference is zero      are due to y   y  i e  click only on top   or no click  
note that a learning algorithm can easily detect those cases and may explicitly eliminate
them as feedback  overall  this shows that implicit feedback can indeed produce accurate
preferences 
what remains to be shown is whether the reliability of the feedback is affected by the
quality of the current prediction  i e   u  xt   yt    in the user study  some users actually
received results for which retrieval quality was degraded on purpose  in particular  about
one third of the subjects received googles top    results in reverse order  condition reversed  and another third received rankings with the top two positions swapped  condition
swapped   as figure   shows  we find that users provide accurate preferences across this
substantial range of retrieval quality  intuitively  a worse retrieval system may make it
harder to find good results  but it also makes an easier baseline yt to improve upon  this
intuition is formally captured in our definition of  informative feedback  the optimal value
of the  vs   trade off  however  will likely depend on many application specific factors 
like user motivation  corpus properties  and query difficulty  in the following  we therefore
present algorithms that do not require knowledge of   theoretical bounds that hold for any
value of   and experiments that explore a large range of  
 

fishivaswamy   joachims

algorithm   preference perceptron 
initialize w    
for t     to t do
observe xt
present yt  argmaxyy wt   xt   y 
obtain feedback yt
update  wt    wt    xt   yt     xt   yt  
end for

   the preference perceptron for coactive learning
in this section  we start by presenting and analyzing the most basic algorithm for the coactive learning model  which we call the preference perceptron  algorithm     the preference
perceptron maintains a weight vector wt which is initialized to    at each time step t  the
algorithm observes the context xt and presents an object y that maximizes wt   xt   y  
the algorithm then observes user feedback yt and the weight vector wt is updated in the
direction  xt   yt     xt   yt   
although the update of the preference perceptron appears similar to the standard perceptron for multi class classification problems  there are key differences  first  the standard
perceptron algorithm requires the true label y as feedback  whereas much weaker feedback
y suffices for our algorithm  second  the standard analysis of the perceptron bounds the
number of mistakes made by the algorithm based on margin and the radius of the examples 
in contrast  our analysis bounds a different regret that captures a graded notion of utility 
further  the standard perceptron mistake bound  novikoff        contains r  kwk  while
our bound in the following theorem contains rkwk where r is as defined in     
theorem   the average regret of the preference perceptron algorithm can be upper bounded 
for any          and for any w as follows 
t

  x
 rkw k
  
regt 
t  
t
 t
t  

    

proof first  consider kwt    k    we have 
wt    wt      wt  wt    wt    xt   yt     xt   yt   
    xt   yt     xt   yt       xt   yt     xt   yt  
 wt  wt    r    r  t 
on line one  we simply used our update rule from algorithm    on line two  we used the
fact that wt    xt   yt     xt   yt       from the choice of yt in algorithm   and that
k x  y k  r  further  from the update rule in algorithm    we have 
wt    w   wt  w     xt   yt     xt   yt     w
 

t
x

 u  xt   yt    u  xt   yt     

t  

 

    

ficoactive learning

we now use the fact that wt    w  kw kkwt    k  cauchy schwarz inequality   which
implies
t
x

 u  xt   yt    u  xt   yt      r t kw k 
t  

from the  informative modeling of the user feedback in      we have


t
x

 u  xt   yt    u  xt   yt    

t  

t
x


t   r t kw k 

t  

from which the claimed result follows 
the first term in the regret bound denotes the quality of feedback in terms of violation
of the  informative feedback  in particular  if the user feedback is strictly
 informative

for some   then all slack variables in      vanish and regt   o    t   
it is trivial to design algorithms  with even better regret  under strict  informative
assumption when the cardinality of the context set x is finite  one of the interesting aspects
of the above bound  theorem    and the subsequent results is that we can minimize the
regret even when the context xt is different in every step  thus   x   could be infinite and
the regret bound still holds 
we note that the bound in theorem   holds for any w and           the slacks have
to be based on the corresponding  and w  
though user feedback is modeled via  informative feedback  the algorithm itself does
not require knowledge of    plays a role only in the analysis 
so far  we have characterized user behavior in terms of deterministic feedback actions 
however  if a bound on the expected regret suffices  the weaker model of expected informative feedback from equation     is applicable 
corollary   under the expected  informative feedback model  the expected regret  over
user behavior distribution  of the preference perceptron algorithm can be upper bounded as
follows 
t

e regt   

  x   rkw k
  
t  
t

t
t  

    

the above corollary can be proved by following the argument of theorem    but taking
expectations over user feedback 
e wt    wt        e wt  wt     e  wt    xt   yt     xt   yt    
  et    xt   yt     xt   yt       xt   yt     xt   yt   
 e wt  wt      r   
in the above  e denotes expectation over all user feedback yt given yt under the context
xt   it follows that e wt    wt        t r   
 

fishivaswamy   joachims

algorithm   batch preference perceptron 
initialize w    
l 
s 
for t     to t do
observe xt
present yt  argmaxyy wl   xt   y 
obtain feedback yt
if t    s   k then
p
update  wl    wl   tj s  xj   yj     xj   yj  
l l  
st
end if
end for

applying jensens inequality on the concave function   we get 
q
 
e wt w    kw ke kwt k   kw k e wt  wt   
the corollary follows from the definition of expected  informative feedback 
    lower bound
we now show that the upper bound in theorem   cannot be improved in general with respect
to its scaling with t   in the following lemma  given any coactive learning algorithm  we
construct a sequence of examples
where  even with      feedback  the algorithm suffers

an average regret of     t   
lemma   for any coactive learning algorithm a with
 linear utility  there exist xt   objects
y and w such that regt of a in t steps is     t   
proof consider a problem where y            x    x  rt   kxk       define
the joint feature map as  x  y    yx  consider t contexts e            et such that ej has
only the j th component equal to one and all the others equal to zero  let y      
    yt be
t  
the sequence of outputs
of
a
on
contexts
e
 
 
 
 
 
e
 
construct
w
 
 y
 
 

 
t
  
y    t           yt   t     we have for this construction kw k      let the user feedback on
the tth step be yt   with these choices  the user feedback
is always  informative with     
  pt

since yt   yt   yet  the regret of the algorithm is t t    w   et   yt    w   et   yt     
   t   
    batch update
the preference perceptron as stated in algorithm   makes an update after every iteration 
in some applications  due to high volumes of feedback  it might not be possible to do an
update that frequently  for such scenarios  it is natural to consider a variant of algorithm  
that makes an update every k iterations  the algorithm simply uses wt obtained from the
  

ficoactive learning

algorithm   generic template for coactive learning algorithms
initialize w 
for t     to t do
observe xt
present yt  argmaxyy wt   xt   y 
obtain feedback yt
perform an update on wt using the gradient of w    xt   yt   xt   yt    to obtain wt    
end for
previous update until the next update  the type of updates shown in algorithm   are called
mini batch updates and have been used in distributed online optimization  dekel  giladbachrach  shamir    xiao         the steps of the batch update algorithm are shown in
algorithm    it is easy to show the following regret bound for batch updates 
lemma   the average regret of the batch preference perceptron algorithm can be upper
bounded  for any          and for any w as follows 

t
  x
 rkw k k

regt 
t  
 
t

t
t  
while this lemma implies that mini batches slow down learning by a factor equal to the
batch size  we will see in section       that empirically convergence is substantially faster 

   deriving algorithms for coactive learning
the preference perceptron and the regret it minimizes  as defined in eqn       is only
one point in the design space of different regret definitions and learning algorithms for
coactive learning  in this section  we will outline a general strategy for deriving coactive
learning algorithms from existing algorithms for online optimization  furthermore  we will
demonstrate that more general notions of regret are meaningful and feasible in coactive
learning  and derive coactive learning algorithms for general convex and  strongly convex
losses 
all coactive learning algorithms that we derive in this section follow the general template
outlined in algorithm    after initializing w    in each iteration the context xt is observed
and the algorithm presents yt by maximizing its current utility estimate represented by wt  
once the feedback yt is observed  the algorithm simply takes the gradient of w    xt   yt   
 xt   yt    and uses an update from a standard online convex optimization algorithm to
obtain wt   from wt  
in each case  an upper bound on the regret of the proposed algorithm is derived by
using the following strategy  first  we start with a notion of regret that is suited for
coactive learning  we then upper bound this regret by first reducing it to a form such
that results from a standard online convex opimization regret bound can be applied  this
gives a regret bound for the original coactive algorithm in turn  in each case  we use this
template algorithm to derive a specific algorithm  however  we still provide a self contained
proof  in the appendix  clearly pointing out where we have used the regret bound from a
corresponding online convex optimization algorithm 
  

fishivaswamy   joachims

algorithm   exponentiated preference perceptron
intialize w i  n 
   s t
for t     to t do
observe xt
present yt  argmaxyy wt   xt   y 
obtain feedback yt
i
wt  
 wti exp  i  xt   yt    i  xt   yt     zt   where zt is such that the weights add
to one 
end for

    exponentiated preference perceptron
to illustrate the generic strategy for deriving coactive learning algorithms  we first derive
an exponentiated gradient algorithm for coactive learning that can be used as an alternative
to the preference perceptron  the exponentiated gradient algorithm inherits the ability to
learn quickly for sparse weight vectors 
unlike the additive updates of the preference perceptron  the exponentiated gradient
algorithm summarized in algorithm   performs multiplicative updates  this exponentiated
algorithm is closely related to the exponentiated algorithms for classification  kivinen  
warmuth         at the start  it initializes all weights uniformly  each subsequent update
step has a rate  associated with it  the rate depends on an upper bound on the   norm
of the features  i e   k    k   s  and the time horizon t   after each multiplicative
update  the weights are normalized to sum to one  and the steps of the algorithm repeat 
since the updates are multiplicative and the weights are initially positive  wt is guaranteed
to remain in the positive orthant for this algorithm  we note that algoithm   is assumed
to know both t and s  there are standard techniques  see cesa biachi   lugosi      b 
to convert such an algorithm to not have dependence on t   however  such extensions are
not the focus of this paper 
we now provide a regret bound for algorithm    while the regret bounds for algorithm  
and algorithm   depended on the    norm of the features  the bound for the exponentiated
algorithm depends on the   norm of the features 
theorem   for any w  rn such that kw k        w     under  expected  informative feedback the average  expected  regret of the exponentiated preference perceptron can be upper bounded as 
t
  x
  log m s
s

regt 
t  
    
t
 t
  t
t  

e regt   

t
  x    log m s
s

t  
    
t
 t
  t
t  

where k x  y k   s 
  

ficoactive learning

proof we start with the regret of the coactive learning algorithm as defined in     
regt  

 

t
 x
 u  xt   yt    u  xt   yt   
t
t  
t
x

 
t

 
 
t

 u  xt   yt    u  xt   yt     

t  
t 
x

t
  x
t
t
t  

w   xt   yt  

t  





w   xt   yt  

t
  x
t
 
t

    

t  

in the above equation  we have used the definition of  informative feedback as defined in
eqn       by viewing algorithm   as an exponentiated online gradient descent algorithm  it
is easy to derive the following regret bound using techniques initially introduced by kivinen
and warmuth        

t
t
x
x

s t
 
 wt   xt   yt     xt   yt     
 u  xt   yt    u  xt   yt        log n  s t  
 
 
t  

t  

since we could not find this specific bound in the literature  a self contained proof is provided
in appendix a  in the proof  regt is first upper bounded in terms of the difference between
kl w  wt     and kl w  wt    a telescoping argument is then used to get the above result 
observing that wt    xt   yt     xt   yt        we get 
t
x
t  



s t
 u  xt   yt    u  xt   yt       log n  s t  
 
 

    

combining      and       we obtain the average regret bound  the proof of the expected
regret bound is analogous to that of the preference perceptron 
like the result in theorem    the above result  theorem    also bounds the regret in
terms of the noisein the feedback  first term  and additional terms which converge to zero
at the rate o    t    the key difference to theorem   is that the regret bound of the
exponentiated algorithm scales logarithmically with the number of features  but with the
    norm of w  which can be advantageous if the optimal w is sparse 
    convex preference perceptron
generalizing the definition of regret from eqn       we now allow that at every time step
t  there is an  unknown  convex loss function ct   r  r which determines the loss
ct  u  xt   yt    u  xt   yt    at time t based on the difference in utility between yt and the
optimal yt   the functions ct are assumed to be non increasing  non increasing assumption
on ct is based on the intuition that the loss should be higher as u  xt   yt   is farther from
u  xt   yt    further  sub derivatives of the ct s are assumed to be bounded  formally  c t    
 g     for all t and for all   r where c t    denotes the sub derivative of ct    at   the
vector w which determines the utility of yt under context xt is assumed from a closed and
  

fishivaswamy   joachims

algorithm   convex preference perceptron 
initialize w    
for t     to t do
set t   t
observe xt
present yt  argmaxyy wt   xt   y 
obtain feedback yt
update  wt    wt   t   xt   yt     xt   yt   
project  wt    arg minub ku  wt   k 
end for
bounded convex set b whose diameter is denoted as  b   in the case of convex losses  we
consider the following notion of regret 
t
t
 x
 x

cregt   
ct  u  xt   yt    u  xt   yt    
ct    
t
t
t  

    

t  

in the bound       ct     is the minimum possible convex loss since u  xt   yt    u  xt   yt   can
never be greater than zero by definition of yt   hence the above regret compares the loss of
our algorithm with the best loss that could be achieved with a convex loss  note that  for
the case ct        the above definition of regret reduces to our earlier definition of regret
in the linear case  eqn       
algorithm   minimizes the average convex loss  there are two differences between this
algorithm and algorithm    first  there is a rate t associated with the update at time
t  second  after every update  the resulting vector wt   is projected back to the set b 
algorithm   is also closely related to the online convex optimization algorithm propsed
by zinkevich         however  the online convex optimization algorithm assumes that the
gradient of the loss  ct     is observed after each iteration  our algorithm doesnt observe
the gradient directly  but only observes an improved object yt after presenting an object
yt  
our earlier regret bounds were expressed in terms of slack variables t   however  here
and in the following section  our bounds will be expressed in terms of the clipped version
of the slack variables defined as t     max    t   
theorem   for the convex preference perceptron under  informative feedback  for nonincreasing convex losses ct    with bounded sub derivative  we have  for any          and
any w  b 


t
 g x   g
 b 
 b   r 
  
cregt 
t  
  
 
t
   t
t
t
t  

    

similarly  under expected  informative feedback  we have 


t
 g x   g
 b 
 b   r 
  
e cregt   
t  
  
 
t
   t
t
t
t  
  

    

ficoactive learning

the proof for the above theorem is provided in the appendix b  the idea of the proof is
to first divide the time steps into two types depending
on the nature of the feedback  this
pt
allows us to upper bound cregt in terms of t    wt  w      xt   yt     xt   yt     this
term can further be upper bounded by following the argument from zinkevich        even
in the coactive learning framework 
from the definition of cregt in eqn        the above theorem upper bounds the
average convex loss via the minimum achievable loss and the quality of the feedback  like
the previous result  theorem     under strict
  informative feedback  the average loss approaches the best achievable loss at o    t    albeit with larger constant factors 
in the case of the linear utility bounds in theorem   and theorem    it was sufficient
to have the average of the slack variables be zero to achieve zero regret  however  in the
case of convex losses  our upper bound on regret approaches zero only when the average of
the clipped slack variables is zero 
    second order preference perceptron
for a particular class of convex functions  it turns out that we can give much stronger
regret bounds than for general convex losses  the improvement for this special class of losses
parallels improvements in online convex optimization from general convex losses  zinkevich 
      to  strongly convex losses  hazan et al         
definition   a convex function f   d  r is  strongly convex if for all points x and y
in d  the following condition is satisfied for a fixed      
f  x   f  y    f  x    x  y  


  y  x     
 

    

where f  x  denotes a sub derivative at x 
algorithm   shows the second order preference perceptron for  strongly convex losses 
like our previous algorithms  the second order preference perceptron also maintains a
weight vector wt   and the step of presenting yt based on a context xt is still the same as
in our previous algorithms  however  in addition to the weight vector  it also maintains an
additional matrix at which is constructed from the outer product of the vector  xt   yt   
 xt   yt    the update step and the projection steps now involve both at as shown in
algorithm    algorithm   is closely related to the online convex optimization algorithm
proposed by hazan et al          however  as we pointed out in the case of algorithm   
our algorithm only observes a user preference feedback after each step unlike online convex
optimization algorithms which observe gradients  it is still possible to prove a regret bound
for the  strongly convex case  and we have the following result 
theorem   for the second order preference learning algorithm  for  expected   strongly
convex  non increasing functions ct   with bounded sub derivatives  we have 
  

t
t
 x      g x   g b 
gn
 r t 
cregt 
t  
t  
 
log
    
    
 t  
t
t
 t 

t  
t  
  

t
t
 r t 
 x     g x   g b 
gn
e cregt   
t  
t  
 
log
    
    
 t  
t
t
 t 

t  

t  

  

fishivaswamy   joachims

algorithm   second order preference perceptron 
intialize w    
a   i
   g 
for t     to t do
observe xt
present yt  argmaxyy wt   xt   y 
obtain feedback yt
at  at      xt   yt     xt   yt     xt   yt     xt   yt    
update  wt    wt   a 
t   xt   yt     xt   yt   
project  wt     arg minwb  wt    w   at  wt    w 
end for
where       is an initialization parameter  as shown in algorithm   
we prove the above theorem in the appendix c  like in the proof of theorem    we divide
time steps into two types  starting with this  it is possible to upper bound cregt to such
a form that the resulting terms can be upper bounded using similar arguments as that for
online strongly convex optimization  hazan et al         
when user feedback is strictly  informative for some  and some w  b  the first two
terms of the regret bound      result in an o  logt t   scaling with t   however  there is a
linear dependence on the dimensionality of the joint feature map in the regret bound for
the second order preference perceptron algorithm 
even though it appears like we need to invert the matrix at in the second order preference perceptron  this can be avoided since the updates on at are of rank one  by the
woodbury matrix inversion lemma  we have 
     
a 
t    at      xt   yt     xt   yt     xt   yt     xt   yt       

  a 
t  

   
a 
t    xt   yt     xt   yt     xt   yt     xt   yt    at 

         xt   yt     xt   yt     a 
t    xt   yt     xt   yt   

 

thus  in practice  the second order preference perceptron can update both at and
bt in each iteration  nevertheless  the projection step to obtain wt   involves solving a
quadratically constrained quadratic program where b is a ball of fixed radius  which still
takes o n     time  hence  the second order preference perceptron is computationally more
demanding than the convex preference perceptron  as we show in the experiments  the
second order preference perceptron might be still quite useful for low noise data 

   experiments
we empirically evaluate our coactive learning algorithms on two real world datasets  the
two datasets differ in the nature of prediction and feedback  on the first dataset  the
algorithms operate on structured objects  rankings  whereas on the second dataset  atomic
items  movies  were presented and received as feedback 
  

ficoactive learning

    datasets and user feedback models
first  we provide a detailed description of the two datasets that were used in our experiments  along with this  we provide the details of the strategies that we used on each dataset
for generating user feedback 
      structured feedback  web search
our first dataset is a publicly available dataset from yahoo   chapelle   chang        for
learning to rank in web search  this dataset consists of query url feature vectors  denoted as
xqi for query q and url i   each with a relevance rating riq that ranges from zero  irrelevant 
to four  perfectly relevant   to pose ranking as a structured prediction problem  we defined
our joint feature map as follows 
w   q  y   

 
x
w  xqyi
 
log i     

    

i  

in the above equation  y denotes a ranking such that yi is the index of the url which is
placed at position i in the ranking  thus  the above measure considers the top five urls
for a query q and computes a score based on graded relevance  note that the above utility
function defined via the feature map is analogous to dcg    see  manning et al        
after replacing the relevance label with a linear prediction based on the features 
for query qt at time step t  the coactive learning algorithms present the ranking ytq
that maximizes wt   qt   y   note that this merely amounts to sorting documents by the
scores wt  xqi t   which can be done very efficiently  the utility regret in eqn       based on
p
the definition of utility w   q  y  is given by t  tt   w    qt   yqt      qt   yqt     here yqt 
denotes the optimal ranking with respect to w   which we consider to be the best least
squares fit to the relevance labels from the features using the entire dataset  we obtain
yqt  from eqn     that is  yqt    argmaxyy w   qt   y   in all our experiments  query
ordering was randomly permuted twenty times and we report average and standard error
of the results 
we used the following two user models for generating simulated user feedback in our
experiments  the first feedback model is an idealized version of feedback whereas the second
feedback is based directly on relevance labels that are available in the dataset 
 strict  informative feedback  in this model  the user is assumed to provide
strictly  informative feedback at a given  value  i e   slacks zero   given the predicted ranking yt   the user would go down the list until she found five urls such that 
when placed at the top of the list  the resulting yt satisfied the strictly  informative
feedback condition w r t  the optimal w   this model assumes that the user has
access to w hence it is an idealized feedback 
 noisy feedback at depth k  in this feedback model  given a ranking for a query 
the user would go down the list inspecting the top k urls  or all the urls if the list
is shorter  for a specified k value  five urls with the highest relevance labels  riq  
are placed at the top five locations in the user feedback  note that this produces noisy
feedback since no linear model can perfectly fit the relevance labels on this dataset 
  

fishivaswamy   joachims

      item feedback  movie recommendation
in contrast to the structured prediction problem in the previous dataset  we considered a
second dataset with atomic predictions  namely movie recommendation  in each iteration 
a movie is presented to the user  and the feedback consists of a single movie as well  we
used the movielens dataset from grouplense org which consists of a million ratings over
     movies as rated by      users  the movie ratings range from one to five 
we randomly divided users into two equally sized sets  the first set was used to obtain
a feature vector xj for each movie j using the svd embedding method for collaborative
filtering  see bell   koren        eqn         the dimensionality of the feature vectors
and the regularization parameters were chosen to optimize cross validation accuracy on the
first dataset in terms of squared error  for the second set of users  we then considered the
problem of recommending movies based on the movie features xj   this experiment setup
simulates the task of recommending movies to a new user based on movie features from old
users 
tx
for each user i in the second set  we found the best least squares approximation wi
j
to the users utility functions on the available ratings  this enabled us to impute utility
values for movies that were not explicitly
rated by this user  furthermore  it allowed us
p
   x  x    which is the average difference in
to measure regret for each user as t  tt   wi
t
t
utility between the recommended movie xt and the best available movie xt   we denote the
best available movie at time t by xt which is obtained from eqn     in this experiment 
once a user gave a particular movie as feedback  both the recommended movie and the
feedback movie were removed from the set of candidates for subsequent recommendations 
in all the experiments we report  average  regret values averaged over all      users in the
test set 
to simulate user behavior  we considered the following two feedback models on this
dataset 
 strict  informative feedback  as in the previous dataset  in this model  the user
is assumed to provide strictly  informative feedback at a given  value  i e   slacks
zero   given the predicted movie yt   the user is assumed to watch the movie if it
already has the highest rating in the remaining corpus of movies  if not  the user
picks another movie from the corpus with lowest utility that still satisfies strict informative assumption  this model again assumes that the user has access to w  
hence it is an idealized feedback 
 noisy feedback  in this feedback model  given a movie y  the user is assumed to
have access to either the actual rating of the movies  when available  or is assumed to
round the imputed rating to the nearest legal rating value  we used two sub strategies
by which the user provides feedback  in better feedback  the user provides y such
that it has the smallest rating  actual rating or rounded rating  but strictly better
rating than that of y  in best feedback  the user provides y such that it has the
highest rating  actual rating or rounded rating  in the remaining corpus  there could
be multiple movies satisfying the above criteria  and ties were broken uniformly at
random among such movies  note that this feedback model results in a noisy feedback
due to rounding of movie ratings to discrete values 
  

ficoactive learning

    preference perceptron
in the first set of experiments  we analyze the empirical performance and scaling behavior
of the basic preference perceptron algorithm and its variants 
      strong versus weak feedback
the goal of the first experiment to explore how the regret of the algorithm changed with feedback quality  to get feedback at different quality levels   we used strict  informative
feedback for various  values 
   

 

      
      
      

   

      
      
      

 

avg  util regret

avg  dcg regret

   
   
 
   

 

 

 

   
   

 
   
   
  

 

  

 

 

  

  

   
  

 

  

t

 

 

  

  

 

  

t

figure    regret based on strict  informative feedback for various  values for websearch  left  and movie recommendation  right  
figure   shows the results of this experiment for three different  values  overall  regret
is typically substantially reduced after only tens or hundreds of iterations  as expected 
the regret for        is lower compared to the regret for lower  values  note  however 
that the difference between the two curves is much smaller than a factor of ten  also note
that the differences are less prominent in the case of web search  this is because strictly
 informative feedback is also strictly  informative feedback for any     so  in our
user feedback model  we could be providing much stronger feedback than that required by
a particular  value  as expected from the theoretical bounds  since the user feedback is
based on a linear model with no noise  utility regret approaches zero in all the cases  note
that we show standard error in the plots  giving an indication of statistical significance  in
the left plots in figure    the standard errors are high at lower iterations but become lower
with more iterations  in some plots in the rest of the paper  the error bars are small and
may be difficult to visually identify 
in the rest of this paper  for strict  informative feedback  we consider       
unless we explicitly mention otherwise 
      noisy feedback
in the previous experiment  user feedback was based on actual utility values computed from
the optimal w   we next study how regret changes with noisy feedback where user behavior
  

fishivaswamy   joachims

does not follow a linear utility model  for the web search dataset  we use noisy feedback
at depths    and     and for the movie dataset we use noisy feedback with both the
better and the best variant of it 
   

 

depth   
depth   

   

better
best

 

avg  util regret

avg  util regret

   
 
   
   

 

 

 

   
 

   
   
  

 

  

 

 

  

  

   
  

 

  

t

 

 

  

  

 

  

t

figure    regret based on noisy feedback for web search  left  and movie recommendation
 right  

the results for this experiment are shown in figure    the first observation to make is
that in the case of web search  the regret values now do not converge to zero  similarly  in
the case of movie recommendation the regret values are higher than those in the previous
experiment  these results are in line with our theory which shows regret converging to
average slack variables when the user does not provide strict  informative feedback for any
  interestingly  in the case of web search the average regret is slightly higher when the
user goes to greater depth in providing feedback  this is due to the fact that the relevance
labels in this dataset are noisy  when the user maximizes  noisy  utility over a larger set of
urls  the selection of the  true  utility maximizers becomes less reliable  which degrades
user feedback quality 
in the rest of this paper  for web search we consider noisy feedback with depth     in
the case of movie recommendation  we consider the better version of the noisy feedback
unless we explicitly mention otherwise 
      batch updates
in this section  we consider the batch preference perceptron
algorithm  algorithm     its

regret bound from section     scales by a factor k under strict  informative feedback 
if the update is made only every k iterations of the algorithm  we now verify whether
empirical performance scales as suggested by the bound  for both web search and movies 
we considered both strict  informative feedback and noisy feedback  for both types
of feedback  we use the batch perceptron algorithm with various values of k and report the
resulting average regret 
the results of these experiments are shown in figure   and figure    as expected 
as the value of k becomes smaller  regret converges faster  however  we observe that the
  

ficoactive learning

   

 

k  
k     
k   
k   

   

k  
k   
k   
k   

 

avg  util regret

avg  util regret

   
 
   
   

 

 

 

   
 

   
   
  

 

  

 

 

  

  

   
  

 

  

 

 

  

t

  

 

  

t

figure    regret versus time based on batch updates with strict  informative feedback
for web search  left  and movie recommendation  right  

   
   
   
 

 

 

   

 

   

 

     
  

 

  

 

 

  

  

k  
k   
k   
k   

 

avg  util regret

avg  util regret

 

k  
k     
k   
k   

   
  

 

  

t

 

 

  

  

 

  

t

figure    regret versus time based on batch updates with noisy feedback for web search
 left  and movie recommendation  right  


empirical scaling with k is substantially better than the k factor suggested by lemma   
these results show the feasibility of using coactive learning algorithms in systems where
it might be impractical to do an update after every iteration 
      expected user feedback
the user feedback was deterministic in our experiments so far  in this sub section  we consider probabilistic feedback and study the behavior of the preference perceptron algorithm 
recall that we provided an upper bound on the expected regret for expected user feedback
in corollary   
to provide  informative feedback under expectation  we consider the following strategy 
given an object yt on context xt   the user would first generate deterministic feedback yt
  

fishivaswamy   joachims

following a strict  informative feedback model         for web search and       
for movie recommendation    in addition  we consider five randomly generated objects
as feedback  we then put uniform probability mass over the randomly generated objects
and remaining mass over the deterministic feedback such that the user feedback is still
 informative at        in expectation 
 

expct  feedback
det  feedback

   
   

expct  feedback
det  feedback

 

avg  util regret

avg  util regret

   
 
   
   

 

 

 

   
 
   
   
  

 

  

 

 

  

  

   
  

 

  

t

 

 

  

  

 

  

t

figure    expected feedback versus deterministic feedback on web search  left  and movie
recommendation  right  

the results for this experiment are shown in figure    as a reference  we also plot the
regret curve with deterministic  informative feedback with         it can be seen that
there is not much difference between deterministic and expected feedback at higher numbers
of iterations  it can also be seen that the regret converges to zero even with  informative
feedback in expectation as suggested by corollary   
      comparison with ranking svm
we now compare our algorithms against several baselines  starting with a conventional
ranking svm  joachims        that is repeatedly trained  at each iteration  the previous
qt
svm model is used to present a ranking to the user  ysvm
   the user returns a ranking
qt
 ysvm   based on strict  informative feedback in one experiment and based on noisy
qt
qt
feedback in the other  the pairs of examples  qt   ysvm
  and  qt   ysvm
  are used as training
pairs for the ranking svm  note that training a ranking svm after each iteration would be
prohibitive expensive  since it involves solving a quadratic program and cross validating the
regularization parameter c  thus  we retrained the svm whenever     more examples
were added to the training set  the first training was after the first iteration with just
one pair of examples  starting with a random yq     and the c value was fixed at     until
there were    pairs of examples  when reliable cross validation became possible  after there
were more than    pairs in the training set  the c value was obtained via five fold cross   note that  in the case of web search  our user model can provide strictly  informative where  larger
than     

  

ficoactive learning

validation  once the c value was determined  the svm was trained on all the training
examples available at that time  the same svm model was then used to present rankings
until the next retraining 
   

 

svm
pref  perceptron

   

svm
pref  perceptron

 

avg  util regret

avg  util regret

   
 
   
   

 
 
 

   
 
   
   
  

 

  

 

 

  

  

   
  

 

  

 

 

  

  

 

  

t

t

figure    preference perceptron versus ranking svm with strict  informative feedback on web search  left  and movie recommendation  right  

 

svm
pref  perceptron

   

 

   

 

avg  util regret

avg  util regret

   

 

 

   

 

   

 

     
  

 

  

 

 

  

  

   
  

 

  

svm
pref  perceptron

 

 

  

  

 

  

t

t

figure    preference perceptron versus ranking svm with noisy feedback on web search
 left  and movie recommendation  right  

the results of this experiment are shown in figure   and figure    in the case of
strict  informative feedback  the preference perceptron performed much better than
the svm for the movie recommendation  and comparably for web search  in the case of
noisy feedback  the preference perceptron performs significantly better than the svm
over most of the range on both the datasets  while it took around    minutes to run the
preference perceptron experiment  it took about    hours to run the svm experiment on
  

fishivaswamy   joachims

web dataset for each permutation of the dataset  similary  on the movie recommendation
task it took around     seconds to run the preference perceptron for each user while it took
around     seconds to run the svm for each user  these results show that the preference
perceptron can perform on par or better than svms on these tasks at a fraction of the
computational cost 
      comparison with dueling bandit
as a second baseline  we compare the preference perceptron algorithm with the dueling
bandit approach of yue and joachims         in each step  the dueling bandit algorithm
makes a comparison between a vector w and a perturbed version of it w   in a random
direction u such that w    w   u   the results produced by these two weight vectors are
assessed by the user through techniques such as interleaving  radlinski  kurup    joachims 
       providing a preference between w and w    the preference feedback determines the
update that the dueling bandits algorithm makes to w  if w is preferred  it is retained
for the next round  if w  is preferred  a small step of length  is taken in the direction of
perturbation u 
   

dueling bandit
pref  perceptron

   

   

   

   

   

   

avg  util regret

avg  util regret

   

 
   

 
   

   

   

   

   

   

   

   
  

 

  

 

 

  

  

   
  

 

  

t

dueling bandit
pref  perceptron

 

  

 

 

  

  

 

  

t

figure    preference perceptron versus dueling bandit on web search  the left plot is based
on strict  informative feedback  the right plot shows noisy feedback 
in our first experiment on web search  in each step  we first obtained two ranked lists
based on w and w    the features used to obtain these ranked lists were identical to those
used for preference perceptron  the two rankings were then interleaved  the interleaved
ranking was presented to a user  in the first experiment  the user provided strict informative feedback on the interleaved ranking  in the second experiment  the user
provided noisy feedback  depending on the feedback  we inferred which of the two rankings was preferred using the team game method proposed by radlinski et al          when
w was preferred or when there was a tie  no step was taken  when w  was preferred  a
step of length  was taken in the direction u  the regret of the dueling bandit algorithm
was measured by considering the utility of the interleaved ranking  unlike the preference
perceptron algorithm  the dueling bandit algorithm has two parameters   and   that need
  

ficoactive learning

to be tuned  we considered    values for these parameters   x  grid  and simply chose the
best parameter values of the dueling bandits algorithm in hindsight 
the results for this experiment are shown in figure    despite the advantage of setting
the parameters to best possible values  it can be seen that dueling bandit algorithm performs
significantly worse compared to the preference perceptron algorithm by orders of magnitude 
for example  the performance of the dueling bandit at around        iterations is matched
by preference perceptron at less than     iterations with both types of feedback  this is
not surprising  since the dueling bandit algorithm basically relies on random vectors to
determine the direction in which a step needs to be taken  in the coactive learning model 
the user feedback provides a  better than random  direction to guide the algorithm 
 

dueling bandit
pref  perceptron

 

 

 

 

avg  util regret

avg  util regret

 

 

 

 

 

 

 

   
  

 

 

  

  

   
  

 

  

t

dueling bandit
pref  perceptron

 

 

  

  

 

  

t

figure     preference perceptron versus dueling bandit on movie recommendation  the
left plot is based on utility values whereas the right plot shows results with
rounded values 

similarly  we also conducted a comparison with the dueling bandit algorithm on the
movie recommendation dataset  however  unlike the web search experiment  the dueling
bandit model is somewhat unnatural on this dataset in our experimental setup  since interleaving two rankings is natural whereas interleaving two items is not  we therefore consider
a different setup  two movies were obtained based on w and w  for the dueling bandit
algorithm  user feedback was to merely indicate which of these two movies has a higher
rating  in the noisy case  user feedback was based on the actual rating or the rounded rating  in the noise free case  user feedback was based on the utility values  in either case  the
utility of dueling bandit was considered to be the average utility of the two movies selected
for comparison 
the performance of the dueling bandit algorithm in this experiment is shown in figure     for the preference perceptron algorithm  regret curves for strict  informative
feedback          and better noisy feedback are also shows as reference  it can be
seen that the dueling bandit algorithm again performs substantially worse compared to the
preference perceptron algorithm 
  

fishivaswamy   joachims

    exponentiated versus additive updates
in this experiment  we compare the exponentiated algorithm  algorithm    with the additive
preference perceptron algorithm  for the exponentiated algorithm  all the components of
we must be non negative   we obtained a non negative we as follows 
 we  i


 

min     w  i  
   i  m 
max     w  im   m      i   m 

    

in the above equation   we  i denotes the ith component of we   moreover  we also modified
the joint feature map for the exponentiated algorithm as follows 
e



   x  y  i  

   x  y  i
 im
  x  y  im m      i   m

    

with the above modifications  we will have only non negative components and moreover  it is easy to verify that we   e  x  y    w   x  y   this makes the regret of the
exponentiated algorithm directly comparable with the regret of the additive algorithm 
the exponentiated algorithm has a fixed rate parameter  that inversely depends on
the time horizon t   when t is large   is small  in this situation  consider the update in
algorithm   
i
wt  
 wti exp  i  xt   yt    i  xt   yt     zt  
since   is small  we can approximate the exponential term in the above equation with
a first order approximation 
exp  i  xt   yt    i  xt   yt           i  xt   yt    i  xt   yt    
thus the exponentiated updates resemble the updates of the additive algorithm up to
a normalization factor  despite the normalization factor  we empirically observed the behavior of the two algorithms to be nearly identical  though not exact   we thus empirically
evaluated the exponentiated algorithm with a variable rate parameter t    s t at time t 
note that this is an empirical result without formal theoretical guarantees for this variable
rate 
results of this experiment are shown in figure    and figure    for strict  informative
feedback and noisy feedback respectively  it can be seen that the exponentiated algorithm tends to performs slightly better than the additive algorithm for small number of
iterations  as the time horizon becomes large  the two algorithms seem to have comparable
performance in most cases 
    minimizing convex losses
in this section  we empirically evaluate the convex preference perceptron  algorithm   
and the second order preference perceptron  algorithm    
   we put a superscript e to distinguish the joint feature map and w that we used in our experiments for
the exponentiated algorithm 

  

ficoactive learning

   

 

exponentiated
pref  perceptron

   

exponentiated
pref  perceptron

 

avg  util regret

avg  util regret

   
 
   
   

 

 

 

   
 

   
   
  

 

  

 

 

  

  

   
  

 

  

 

 

  

t

  

 

  

t

figure     exponentiated versus additive with strict  informative feedback on websearch  left  and movie recommendation  right  

   

 

exponentiated
pref  perceptron

   

exponentiated
pref  perceptron

 

avg  util regret

avg  util regret

   
 
   
   

 

 

 

   
 

   
   
  

 

  

 

 

  

  

   
  

 

  

t

 

 

  

  

 

  

t

figure     exponentiated versus additive with noisy feedback on web search  left  and
movie recommendation  right  

      convex perceptron versus second order algorithm
the regret bounds from section   show that one can get lower regret for  strongly convex
functions using a second order algorithm  while the first order convex perceptron applies
to general convex functions  in this section  we evaluate the relative performance of the
first order and the second order algorithms empirically  for this purpose  we considered the
quadratic loss c        m    where m is the largest utility value on any possible  x  y 
with any w in a convex ball of radius kw k  it can be verified this loss function is  strongly
convex  b was set to     for both the algorithms for both the datasets 
  

fishivaswamy   joachims

second order
convex perceptron

    

   

    

   

    

   

    

   

    

   

   
  

 

 

  

second order
convex perceptron

   

util regret

quad regret

    

 

  

  

   
  

 

  

 

 

  

 

  

t

  

 

  

t

figure     cumulative regret of the convex perceptron and the second order convex perceptron for web search 

    

   

second order
convex perceptron

   

    

util regret

quad  regret

     

second order
convex perceptron

     

   

   

    
  

     
   
  

 

 

  

  

   
  

 

  

t

 

 

  

  

 

  

t

figure     cumulative regret of the convex perceptron and the second order convex perceptron for movie recommendation 

in the first set of experiments  we considered strict  informative feedback  we ran
both the second order algorithm as well as the convex preference perceptron algorithm   
the  value in the second order perceptron was simply set to one  we recorded the regt
and cregt values for both the methods  note that regt corresponds to the utility
regret as defined in   
results of this experiment are shown in figure    and figure     to demonstrate
the qualitative difference between the two algorithms  we plot cumulative regret  i e  t 
regt and t  cregt   in these figures  the cumulative regret of the second order
algorithm is linear on a log scale  this shows that the convergence of the regret is indeed
  

ficoactive learning

logarithmic  compared to the much slower convergence of the convex preference perceptron 
interestingly  even the cumulative regret based on raw utility values empirically shows a
similar behavior  this is purely an empirical result  since theoretically  o log t   t   average
regret holds only for strongly convex losses and the linear loss is not strongly convex 
 

  

weak     
strong     
weak      
strong      
weak      
strong      

 

util  regret

quad regret

  

 

weak     
strong     
weak      
strong      
weak      
strong      

  

 

  

 

  

 

  

 

 

 

  

 

  

 

  

  

 

  

    
  

 

  

 

 

  

 

  



  

 

  

 

  



figure     sensitivity of the second order preference perceptron algorithm to the parameter
value  

   

  

weak     
strong     
weak      
strong      
weak      
strong      

quad  regret

   

  

  

   

  
util  regret

   

  

weak     
strong     
weak      
strong      
weak      
strong      

   

   

  

   

  

   

  

   

  
   

  

   

 

  

 

 

  

  

 

  

 

  



  

 

  

 

 

  

  

 

  

 

  



figure     sensitivity of the second order preference perceptron algorithm to the parameter
value  on movie recommendation 

in the previous experiment  we fixed the  value in the second order algorithm to one 
we now study the sensitivity of the second order algorithm to the value of this parameter 
figures    and    show regret values after a given number of iterations when  is swept
over a range of values  the dotted lines show the performance of the convex preference
perceptron for comparison  in the case of web search  there is a wide range of parameter
  

fishivaswamy   joachims

values where the performance of the algorithm is good  as the parameter  takes an extreme
value on either side  the performance of the algorithm deteriorates  the range of suitable
 values is much broader for the web search dataset than for movie recommendation  it is
interesting to note that both the algorithms performed empirically best at      among the
values that were tried 
    

     

second order
convex perceptron

     

   

second order
convex perceptron

util regret

quad regret

     
    
    
    

    
    

    
   
  

    

    
 

 

  

 

  

  

   
  

 

  

 

 

  

 

  

  

 

  

t

t

figure     strong convex versus weak convex with noisy feedback on web seach 

    

second order
convex perceptron

    

    

    

    

   

util regret

quad  regret

    

    

second order
convex perceptron

   
   

    
   

    

   

    
   
  

 

 

  

  

   
  

 

  

t

 

 

  

  

 

  

t

figure     strong convex versus weak convex with noisy feedback on movie recommendation 

we also tested the convex algorithms under noisy feedback  both regret bounds contain the slack terms on the right hand side  thus  when user feedback is not  informative
for any   the regret bounds for the second order algorithm and the first order algorithm
are both dominated by the slack variables  the empirical performance of the two algorithms under noisy feedback are shown in figures    and     in the case of web search 
the results for the second order algorithm and the first order algorithm are nearly identi  

ficoactive learning

cal  however  in the case of movie recommendation  there is still some advantage to the
second order algorithm 
in summary  the second order algorithm performs substantially superior under no noise
circumstances  in the presence of noise in the feedback  the two algorithms do not show
drastically different behaviors 

   conclusions
we proposed coactive learning as a new model of online learning with preferences that is
especially suited for implicit user feedback  unlike most supervised learning approaches 
coactive learning algorithms do not require optimal labels  but merely  noisy  feedback
that improves over the prediction  our model  where no cardinal utilities are observed 
sits between the experts and the bandits settings  and we argue that coactive learning is
applicable to a wide range of systems that aim to optimize themselves based on observable
user actions 
we provide several algorithms that provably optimize regret in the coactive learning
framework  and we empirically validate the effectiveness of the proposed framework on
web search ranking and movie recommendation datasets with simulations of both noisy and
noise free feedback  a recurring theme in this paper is that a wide variety of conventional
online learning algorithms can be converted into coactive learning algorithms  despite the
differences in the learning model itself  in the nature of feedback and in the notion of regret 
we conjecture that many other online learning algorithms could similarly be converted to
practically useful coactive learning algorithms 
the coactive learning model  the algorithms we proposed  and the ability to use weak
feedback from observable user behavior offer a wide range of opportunities for new learning
approaches to application problems ranging from natural language processing and information retrieval to robotics  there are also several opportunities for further developing
algorithms for the coactive learning model  for example  our algorithm for convex loss
minimization assume only that the gradient of the convex losses are bounded  however 
in most practical situations  the convex loss to be minimized is known apriori  it is an
interesting research direction to study whether there are algorithms that can utilize the
gradient of the loss to perform better either theoretically or empirically  another question
is whether better algorithms exist for special cases of the linear utility model  our lower
bound is based on an argument where the dimensionally of the joint feature maps grow
with the given horizon t   when the dimensionality of the joint feature map is fixed  an
interesting research question is  are there algorithms with better regret than our proposed
algorithms 

acknowledgments
this work was funded in part under nsf awards iis          iis          and iis         
this was work was done when pannaga shivaswamy was a postdoctoral associate at cornell
university  we thank peter frazier  bobby kleinberg  karthik raman  tobias schnabel and
  

fishivaswamy   joachims

yisong yue for helpful discussions  we also thank anonymous reviewers for their thoughtful
comments on an earlier version of this paper 

appendix a  proof of theorem  
proof we look at how the kl divergence between w and wt evolves 
kl w  wt    kl w  wt      

 

n
x
i  
n
x

i
wi log wt  
 wti  

wi   i  xt   yt    i  xt   yt      log zt  

i  

  w    xt   yt     xt   yt     log zt   
    
p
i
on the second line  we pulled out log zt   from the sum since n
i   w      now  consider
i
i
the last term in the above equation  denoting   xt   yt      xt   yt   by i t for brevity  we
have  by definition 
 
n
x
log zt     log
wti exp i t  
 log

i  
n
x

wti   

i

 

i

 

 

   t     t  

i  



 log     wt  t      s  
 wt  t      s    

    

on the second line we used the fact that exp x       x   x  for x     the rate  ensures
that  i       on the last line  we used the fact that log     x   x  combing      and
      we get 
 w  wt    t 

kl w  wt    kl w  wt    
  s    


adding the above inequalities  we get 
t
t
t  
x
x
kl w  wt    kl w  wt     x  
 w  wt      xt   yt     xt   yt    
 
s  

t  

t  



t  

kl w  w   
  s   t 


rearranging the above inequality  and substituting the value of  from algorithm    we
get 

t
t
x
x

s t
 
 u  xt   yt    u  xt   yt    
wt   xt   yt     xt   yt        log n  s t  
 
t  
t  


s t
   log n  s t  
 
 
  

ficoactive learning

in the above  we also used the fact that kl w  w     log n   since w  is initialized uniformly  moreover  from holders inequality  we obtained
wt   xt   yt    kwt k   k xt   yt  k   s 
the above inequality along with  informative feedback gives the claimed result 

appendix b  proof of theorem  
proof first  we divide the set of time steps into two different sets based on the nature of
the feedback 
i     t   u  xt   yt    u  xt   yt           t  t   
j     t   u  xt   yt    u  xt   yt            t  t   
for brevity we denote  xt   a    xt   b  by   a  b  in the rest of this proof  we start
by considering the following term for a single time step t 
ct  u  xt   yt    u  xt   yt     ct    

  
wt  yt   yt  

ct  u  xt   yt    u  xt   yt     ct


  

  
wt  yt   yt  
w  yt   yt   t

 ct
 ct




   

 
 w  wt    yt   yt   t   w  yt   yt   t


ct






 
 
 
  wt  yt   yt     t  w  yt   yt   g  t  i


 wt   yt   yt     t   g 
t  j 
w   y  y  

in the above inequalities  the second line follows from the fact that t  t t    and ct   
is non increasing  the third line follows from  informative feedback  eqn        the fourth
line follows since the function ct is convex   we obtain the first term in the next inequality
 in either case  since c t      g     and wt   yt   yt      from the choice of yt in the
algorithm  the second terms  in either case  is obtained by the fact that t c t  w   yt   yt   
is upper bounded by t  g  this is the step in which the clipped version of the slack variables
are needed in the proof  finally  w   yt   yt   is either positive or negative depending on
the feedback which leads to two different cases depending on whether t  i or t  j 
   since the context xt will always be clear  we suppress this in our notation for brevity 
   for any convex function f   f  y   f  x    y  x f    y  where f    y  denotes a sub derivative of f at y 

  

fishivaswamy   joachims

summing the above inequalities from   to t   we get 
t
x

ct  w   yt   yt    

t  





t
gx


g


wt   yt   yt    

t  
t
x

t
x

ct    

t  
t
x

g


t  

t  

ti

 wt  w     yt   yt    

t  

gx  
w  yt   yt  


g


t
x
t  

t   

gx  
w  yt   yt   


    

tj

p
we obtained the last line above simply by adding and and subtracting g tj w   yt   yt   
on the right side of the previous inequality  from this point  we mostly follow the proof
techniques from online convex optimization  zinkevich        
we now bound the first term on the right hand side of       for this purpose  consider
the following 
kwt    w k    kwt   t  yt   yt    w k 
  kwt  w k    t  k yt   yt  k     t  wt  w     yt   yt   

    

rearranging terms in the above equation  we get 
 
kwt  w k  
 t
 

kwt  w k  
 t

 wt  w     yt   yt    

 
t
kwt    w k    k yt   yt  k 
 t
 
 
kwt    w k     t r 
 t

where  on the last line  we used the fact that kwt    w k   kwt    w k  since the wt  
is just the projection of wt   to the convex set b  which contains the vector w    we can
bound the first term in      using the following telescoping argument 

t 
x
 
 
 
 
 
kwt  w k 
kwt    w k    t r
 t
 t
t  

t 
t
x
x
 
 
 
 
kw   w k  

kwt  w k     r 
t

  
 t  t 
t  
t  

t 
x

 
 
 

 b   

 b     r     t    
  
 t  t 
t  


t   

 b     r  t  
 
in the above  we obtained the second line by simply rearranging the terms in the expression
above 
line  we used the boundedness property of the set b  as well as the

pton the third
fact t  
 t    t     the final line follows from cancelling out terms and the fact that
t      t  
  

ficoactive learning

now  consider the third term on the right hand side of      

w   yt   yt  
 
 
 w   yt   yt     t  t  




the first inequality above follows from  informative feedback  whereas the second inequal

ity follows from the fact w   yt   yp
t      from the definition of yt   finally  the bound     
 
g
follows from the trivial fact     ti t  
to obtain the bound on the expected regret  consider the convex loss at step t conditioned on user behavior so far 


wt   yt   yt  
 et ct



  

 
et  w  yt   yt    t  
wt  yt   yt  
ct
 et ct




  
  

w  yt   yt    t
wt  yt   yt  
et ct
 et ct



 
 
 

 get  wt  yt   yt     t  w  yt   yt     t  i


get  wt   yt   yt     t    
tj
ct  w   yt   yt   



where the second line follows from the definition of expected  informative feedback and the
third line follows from jensens inequality  we obtain the last line following an argument
similar to that in the proof of theorem    the bound follows from an expected version of
     

appendix c  proof of theorem  
proof first  we divide time steps into two different sets based on the nature of feedback 

i     t   u  xt   yt    u  xt   yt           t  t   
j     t   u  xt   yt    u  xt   yt            t  t   
  

fishivaswamy   joachims

we start by considering a single time step t  we have 
ct  w   yt   yt     ct    
  

wt  yt   yt  
 

ct  w  yt   yt     ct

  
  

 
w  yt   yt   t
wt  yt   yt  
ct
 ct






 

    
 
w  yt   yt   t 
  w  wt     yt   yt   t 
 w  wt    yt   yt   t
 
ct









 


 




 
 
 
 
 



wt   yt  yt  
t  yt  

ti
  t  w  y
    w wt    yt  yt    t
g






    





 
   
   y  y  
   y  y  



w
 w
w
 

t
t

t
t t
t

  t   
 t
t  j 
 g


w   y  y  

in the above inequalities  the second line follows from the fact that t  t t    and ct    is
non increasing  the third line follows from the fact that the function ct    is non increasing
and the following inequality which follows from the definition of t   
u  xt   yt    u  xt   yt      u  xt   yt    u  xt   yt     t   
the fourth line follows by strong convexity  the last line follows from a same line of
reasoning as in the proof of theorem   
now consider the last term in both the cases 


 
 


 

 


 




 




 

 
 w  wt     yt   yt   t 





 
 
 w  wt     yt   yt  
t 
t   w  wt     yt   yt  

 

  
 


 
 
 w  wt     yt   yt  
   w   yt   yt   t  wt   yt   yt   t 
  t   




 
  

 


 
 w  wt     yt   yt  
t 
t 
t 
 

 
w  yt   yt    




  


 
 
 w  wt     yt   yt  
 
   t   
    

 


in the above equations  the second and the third lines follow from simple algebraic expansion
of the expression on the first line  the fourth line follows from the definition of  informative
feedback and the fact that wt   yt   yt       the last line follows from the fact that
w   yt   yt      from the definition of yt  
  

ficoactive learning

now  summing the terms in      and then substituting the above bound  we get 

t
x

ct  w   yt   yt    

t  

t
x

ct    

t  

   
x w   yt   yt    pt     
 w  wt     yt   yt  

t   t
g
 
g


  
t  
ti
 


 
t
 
 
x  wt  w    yt   yt     w  wt    yt   yt  
g


 

t  
p
p
 
 tt   t 
g tt   t 
gx  
 
w  yt   yt    
 

  

tj
t 
    pt       g pt   
gx

 
 
t   t
t   t

 wt  w    yt   yt   
 wt  w    yt   yt  
 
 
 

 
  

t
x

wt   yt   yt   t  
 



 



t  

 
p
t  yt  
in the above  we obtained the third inequality by adding and subtracting g tj w  y
 

 
to obtain the last line  we used the fact that        since      
we
p      finally 
   y   y  
used an argument similar to that in the proof of theorem   to bound g
w
t
t

tj

and obtained a factor of two with the sum of slacks term  from this point  we use arguments
similar to those from online convex optimization with strongly convex losses  hazan et al  
      

next  we consider  wt    w    at  wt    w   and express it interms of wt and at   

 wt    w    at  wt    w  
 
 
  wt  a 
t  yt   yt    w   at  wt  at  yt   yt    w  
 
  wt  w    at  wt  w      yt   yt    a 
t  yt   yt      wt  w    yt   yt  

  wt  w     yt   yt   yt   yt     wt  w      wt  w    at   wt  w  
 
   yt   yt    a 
t  yt   yt       w  wt    yt   yt  

rearranging terms in the above equation  we get 


 wt  w     yt   yt   yt   yt     wt  w  
 
 wt  w    at   wt  w     wt    w    at  wt    w      yt   yt    a 
t  yt   yt   
  wt  w     yt   yt   

  

fishivaswamy   joachims

we now identify that the term on the left hand side in the inequality occurs in the
expression that we would like to bound in       we therefore have 
 



t 
x

t  
t
x

 wt  w     yt   yt      wt  w     yt   yt    




 wt  w    at   wt  w     wt    w    at  wt    w      yt   yt    a 
t   byt   yt  

t  

 w   w    a   w   w    

t
x

 yt   yt    a 
t  yt   yt  

t  

 b   

n
log




 r  t 



    

p
 
in the above  we have used the fact that tt    yt   yt    a 
t  yt   yt    n log  r t      
where n is the dimens ionality of  x  y  and r is an upper bound on the norm of the joint
feature maps  i e  k x  y k    r  a proof of this fact can be found in hazan et al         

references
auer  p   cesa bianchi  n     fischer  p       a   finite time analysis of the multiarmed
bandit problem  machine learning                   
auer  p   cesa bianchi  n   freund  y     schapire  r       b   the non stochastic multiarmed bandit problem  siam journal on computing               
bakir  g  h   hofmann  t   scholkopf  b   smola  a   taskar  b     vishwanathan  s   eds   
        predicting structured data  the mit press 
bell  r  m     koren  y          scalable collaborative filtering with jointly derived neighborhood interpolation weights  in icdm 
boley  m   mampaey  m   kang  b   tokmakov  p     wrobel  s          one click mining 
interactive local pattern discovery through implicit preference and performance learning  in proceedings of the acm sigkdd workshop on interactive data exploration
and analytics  pp       
cesa bianchi  n     lugosi  g       a   prediction  learning  and games  cambridge university press 
cesa bianchi  n     lugosi  g       b   prediction  learning  and games  cambridge
university press  cambridge  uk 
chapelle  o     chang  y          yahoo  learning to rank challenge overview  jmlr proceedings track          
chu  w     ghahramani  z          preference learning with gaussian processes  in icml 
crammer  k     singer  y          pranking with ranking  in nips 
  

ficoactive learning

crammer  k     gentile  c          multiclass classification with bandit feedback using
adaptive regularization  in proceedings of the   th international conference on machine learning  icml  
dekel  o   gilad bachrach  r   shamir  o     xiao  l          optimal distributed online
prediction using mini batches  jmlr             
flaxman  a   kalai  a  t     mcmahan  h  b          online convex optimization in the
bandit setting  gradient descent without a gradient  in soda 
freund  y   iyer  r  d   schapire  r  e     singer  y          an efficient boosting algorithm
for combining preferences  journal of machine learning research            
goetschalckx  r   fern  a     tadepalli  p          coactive learning for locally optimal
problem solving   in conference of the american association for artificial intelligence
 aaai   pp           
haddow  b   arun  a     koehn  p          samplerank training for phrase based machine
translation  in proceedings of the sixth workshop on statistical machine translation 
pp          edinburgh  scotland  association for computational linguistics 
hazan  e   agarwal  a     kale  s          logarithmic regret algorithms for online convex
optimization  machine learning                   
herbrich  r   graepel  t     obermayer  k          large margin rank boundaries for
ordinal regression  in advances in large margin classifiers  mit press 
jain  a   wojcik  b   joachims  t     saxena  a          learning trajectory preferences for
manipulators via iterative improvement  in neural information processing systems
 nips   pp         
joachims  t          optimizing search engines using clickthrough data  in acm sigkdd
conference on knowledge discovery and data mining  kdd   pp         
joachims  t   granka  l   pan  b   hembrooke  h   radlinski  f     gay  g          evaluating the accuracy of implicit feedback from clicks and query reformulations in web
search  acm transactions on information systems  tois          
jones  r     klinkner  k          beyond the session timeout  automatic hierarchical
segmentation of search topics in query logs  in cikm 
kakade  s  m   shalev shwartz  s     tewari  a          efficient bandit algorithms for
online multiclass prediction  in proceedings of the   th international conference on
machine learning  icml  
kivinen  j     warmuth  m          exponentiated gradient versus gradient gradient descent for linear predictors  journal of information and computation               
langford  j     zhang  t          the epoch greedy algorithm for multi armed bandits
with side information  in nips 
liu  t  y          learning to rank for information retrieval  foundations and trends in
information retrieval    
manning  c   raghavan  p     schutze  h          introduction to information retrieval 
cambridge university press 
  

fishivaswamy   joachims

novikoff  a          on convergence proofs on perceptrons  in proceedings of the symposium
on the mathematical theory of automata  vol  xii  pp         
polyak  b     tsypkin  y          pseudogradient adaptation and training algorithms 
automatic remote control           
radlinski  f   kurup  m     joachims  t          how does clickthrough data reflect retrieval quality   in conference on information and knowledge management  cikm  
raman  k     joachims  t          learning socially optimal information systems from
egoistic users  in european conference on machine learning  ecml   pp         
raman  k   joachims  t   shivaswamy  p     schnabel  t          stable coactive learning
via perturbation  in international conference on machine learning  icml   pp 
       
raman  k   shivaswamy  p     joachims  t          online learning to diversify from
implicit feedback  in kdd 
shivaswamy  p     joachims  t          online structured prediction via coactive learning 
in icml 
somers  t     hollinger  g          coactive learning with a human expert for robotic
monitoring  in rss workshop on robotic monitoring 
weston  j   bengio  s     usunier  n          wsabie  scaling up to large vocabulary
image annotation  in proceedings of the international joint conference on artificial
intelligence  ijcai  
yue  y   broder  j   kleinberg  r     joachims  t          the k armed dueling bandits
problem  in colt 
yue  y     joachims  t          interactively optimizing information retrieval systems as
a dueling bandits problem  in icml 
zhang  y   lei  t   barzilay  r   jaakkola  t     globerson  a          steps to excellence  simple inference with refined scoring of dependency trees  in proceedings of
the   nd annual meeting of the association for computational linguistics  volume
   long papers   pp          baltimore  maryland  association for computational
linguistics 
zinkevich  m          online convex programming and generalized infinitesimal gradient
ascent  in icml 

  

fi