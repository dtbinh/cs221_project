journal artificial intelligence research                  

submitted        published      

decentralized anti coordination
multi agent learning
ludek cigler
boi faltings

ludek cigler epfl ch
boi faltings epfl ch

artificial intelligence laboratory
ecole polytechnique federale de lausanne
ch      lausanne  switzerland

abstract
achieve optimal outcome many situations  agents need choose distinct
actions one another  case notably many resource allocation problems 
single resource used one agent time  shall designer
multi agent system program identical agents behave different way 
game theoretic perspective  situations lead undesirable nash equilibria 
example consider resource allocation game two players compete exclusive
access single resource  three nash equilibria  two pure strategy ne
efficient  fair  one mixed strategy ne fair  efficient  aumanns
notion correlated equilibrium fixes problem  assumes correlation device
suggests agent action take 
however  smart coordination device might available  propose using
randomly chosen  stupid integer coordination signal  smart agents learn action
use value coordination signal 
present multi agent learning algorithm converges polynomial number
steps correlated equilibrium channel allocation game  variant resource
allocation game  show agents learn play coordination signal value
randomly chosen pure strategy nash equilibrium game  therefore  outcome
efficient correlated equilibrium  ce becomes fair number
available coordination signal values increases 

   introduction
many situations  agents coordinate actions order use limited
resource  communication networks  channel might used one agent time 
driving car  agent prefers choose road less traffic  i e  one chosen
smaller number agents  bidding one item several simultaneous
auctions  agent prefers auction less participants  usually lead
lower price  situations require agents take different decision  however 
agents identical  problem one face same 
learn behave differently everyone else 
second problem arises agents common preferences action
want take  communication networks problem  every agent prefers transmit
quiet  traffic situation  agents might prefer shorter path  order
achieve efficient allocation  necessary precisely agents stay quiet 
take longer path  achieve agents exploited 
c
    
ai access foundation  rights reserved 

ficigler   faltings

agents learn alternate  taking longer road one day  taking shorter
road next day 
central coordinator possesses complete information agents preferences
available resources easily recommend agent action take 
however  omniscient central coordinator always available  therefore 
would able use distributed scheme  consider scenarios
agents try use set resources repeatedly  use history past
interactions learn coordinate access resources future 
particular  consider problem radio channel access  problem  n agents
try transmit c non overlapping channels  fully decentralized schemes 
aloha  abramson         achieve throughput  e      i e  transmission succeeds probability  e       complex schemes  based
distributed constraint optimization  cheng  raja  xie    howitt         reach
throughput close       throughput  mean probability successful transmission given channel  however  messages necessary implement schemes
create overhead eliminates part benefits 
paper propose instead use simple signal agents observe
ergodically fluctuates  signal could common clock  radio broadcasting
specified frequency  decimal part price certain stock given time  etc 
depending stupid signal  smart agents learn take different action
value  say signal stupid  doesnt anything
game agents give meaning acting differently
value coordination signal 
lets look simplest example channel access problem  one   agents try
transmit one shared channel  model situation game normal
form  agents choose two actions  stay quiet  q  transmit  t   
one agent may transmit successfully time  agent transmits alone  receives
positive payoff  agent transmit channel  payoff    agents
try transmit channel time  transmissions fail incur
cost c 
payoff matrix game looks follows 

q


q
    
    


    
c  c

game two pure strategy nash equilibria  ne   one player stays quiet
one transmits  one mixed strategy ne  player stays
 
quiet probability c  
  two pure strategy ne efficient  maximize
social welfare  fair  one player gets full payoff  even though
game symmetric  mixed strategy ne fair  efficient  expected
payoff players   
such  nash equilibria game rather undesirable  either efficient
fair  time  seminal paper  aumann        proposed
notion correlated equilibrium fixes problem  correlated equilibrium  ce 
probability distribution joint strategy profiles game  correlation device
   

fidecentralized anti coordination multi agent learning

samples distribution recommends action agent play  probability
distribution ce agents incentive deviate recommended
action 
simple game described above  exists ce fair socially
efficient  correlation device samples two pure strategy ne probability    recommends players ne play  corresponds
authority tells player whether stay quiet transmit 
correlated equilibria several nice properties  easier find succinct
representation game  polynomial time  see  papadimitriou   roughgarden        
also  every nash equilibrium correlated equilibrium  also  convex combination
two correlated equilibria correlated equilibrium  however  smart correlation device
randomizes joint strategy profiles might always available 
possible achieve correlated equilibrium without actual correlation device 
assume game played repeatedly  agents observe history
actions taken opponents  learn predict future action  or distribution future actions  opponents  predictions need calibrated  is 
predicted probability agent play certain action aj converge
actual frequency agent plays action aj   agents always play action
best response predictions opponents actions  foster vohra       
showed case  play converges set correlated equilibria 
however  paper  foster vohra provide specific learning rule
achieve certain ce  furthermore  approach requires every agent able
observe actions every opponent  requirement met  convergence
correlated equilibrium guaranteed anymore 
paper  focus generalization simple channel allocation problem
described above  n agents always data transmit 
c channels transmit  assume n c  access channel
slotted  is  agents synchronized start transmissions
time  also  transmissions must length  one agent attempts
transmit single channel  collision occurs none transmissions
successful  unsuccessful transmission cost agent  since consume
 possibly constrained  power benefit  transmitting cost
anything 
assume agents receive binary feedback  transmitted data 
find whether transmission successful  transmit 
choose channel observe  receive information whether observed channel
free not 
described normal form game  problem several efficient  but unfair 
pure strategy nash equilibria  group c agents gets assigned channels 
remaining n c agents get stranded  fair inefficient mixed strategy ne 
agents choose transmission channels random  example resource
allocation game above  exists correlated equilibrium efficient fair 
stupid coordination signal introduced paper helps agents learn
play  potentially  different efficient outcome value  way 
reach efficient allocation still preserving level fairness 
   

ficigler   faltings

main contributions work following 
propose learning strategy agents channel allocation game that  using
minimal information  converges polynomial time randomly chosen efficient
pure strategy nash equilibrium game 
show agents observe common discrete correlation signal 
learn play efficient pure strategy ne signal value  result
correlated equilibrium increasingly fair number available signals k
increases 
experimentally evaluate sensitive algorithm player population
dynamic  i e  players leave enter system  evaluate
algorithms resistance noise  feedback players receive
coordination signal observe 
channel allocation algorithm proposed paper implemented wang 
wu  hamdi  ni        real world wireless network setting  showed
wireless devices use use actual data transmitted coordination
signal  way  able achieve     throughput gain compared random
access protocols aloha 
worth noting work  focus reaching correlated fair
outcome  provided agents willing cooperate  situations using
resources costs nothing  self interested agent could stubbornly keep using it  everyone
else better trying access resource  sometimes called
watch crazy bully strategy  littman   stone        
order prevent kind behavior  would need make sure order
use resource  agent pay cost  cost may already implicit
problem  fact wireless transmission costs energy  may imposed
external payments  recent work  cigler   faltings         show leads
equilibria rational agents indifferent accessing resource yielding 
equilibria implement allocation policy rational agents  consider
issue beyond scope paper refer reader work
deeper analysis 
rest paper organized follows  section    give basic definitions
game theory theory markov chains use throughout paper 
section    present algorithm agents use learn action possible
correlation signal value  section   prove algorithm converges
efficient correlated equilibrium polynomial time number agents channels 
show fairness resulting equilibria increases number signals k
increases section    section   highlights experiments show actual convergence
rate fairness  show algorithm performs case population
changing dynamically  section   present related work game theory
cognitive radio literature  section   concludes 
   

fidecentralized anti coordination multi agent learning

   preliminaries
section  introduce basic concepts game theory theory
markov chains going use throughout paper 
    game theory
game theory study interactions among independent  self interested agents 
agent participates game called player  player utility function
associated state world  self interested players take actions achieve
state world maximizes utility  game theory studies attempts
predict behaviour  well final outcome interactions  leyton brown
shoham        give complete introduction game theory 
basic way represent strategic interaction  game  using so called normal
form 
definition    finite  n  person normal form game tuple  n  a  u  
n set n players 
  a  a          ai set actions available player i  vector
   a    a              called action profile 
u    u    u            un    ui   r utility function player assigns
action vector certain utility  payoff  
playing game  players select strategy  pure strategy
player selects one action ai ai   vector pure strategies player  
                 n   called pure strategy profile  mixed strategy selects probability
distribution entire action space  i e   ai    mixed strategy profile
vector mixed strategies player 
definition    say mixed strategy player best response strategy
profile opponents strategy i   
ui  i     ui  i     
one basic goals game theory predict outcome strategic interaction 
outcome stable therefore  usually called equilibrium  one requirement outcome equilibrium none players incentive
change strategy  i e  players play best response strategies others 
defines perhaps important equilibrium concept  nash equilibrium 
definition    strategy profile                    n   nash equilibrium  ne 
every player i  strategy best response strategies others  
essential nash equilibria are  several disadvantages  first  may
hard find  chen deng        show finding ne ppad complete  second 
might multiple nash equilibria  shown example section    third 
efficient ne may fair one  even symmetric game  give
formal definition correlated equilibrium fixes issues 
   

ficigler   faltings

definition    given n  player game  n  a  u   correlated equilibrium tuple  v      
v tuple random variables v    v    v            vn   whose domains  
 d    d            dn    joint probability distribution v                     n  
vector mappings   di   ai   player every mapping  i   di   ai
case
x
x

 d ui     d        d             n  dn   
 d ui     d         d              n  dn    
dd

dd

    markov chains
learning algorithm propose analyze paper described randomized algorithm  randomized algorithm  steps depend value
random variable  one useful technique analyze randomized algorithms describe
execution markov chain 
markov chain random process markov property  random process
collection random variables  usually describes evolution random value
time  process markov property state  or value  next time step depends
exclusively value previous step  values past 
say process memoryless  imagine execution randomized
algorithm finite state automaton non deterministic steps  easy see
execution maps markov chain 
formal definition markov chain follows 
definition     norris        let countable set  called state
called state space  say p   i   i  measure    
i  addition total mass ii equals    call distribution 
work throughout probability space    f  p   recall random variable x
values function x   i  suppose set
  pr x   i    pr      x     i    
defines distribution  distribution x  think x modelling random
state takes value probability  
say matrix p    pij   i  j i  stochastic every row  pij   j i 
distribution 
say  xt  t  markov chain initial distribution transition matrix
p
   x  distribution  
      conditional xt   i  xt   distribution  pij   j i  independent
x    x            xt   
explicitly  conditions state that    i            it   i 
   pr x    i      i   
   pr xt     it    x    i            xt       pit it    
   

fidecentralized anti coordination multi agent learning

theorem    let set states  vector hitting probabilities ha    ha
 
               n    minimal non negative solution system linear equations

 


hi   p

p
h
 a
j         n   ij j
intuitively  hitting probability ha
probability markov chain
starts state i  ever reach states a 
one property randomized algorithms particularly interested
convergence  set states algorithm converged  define
time takes reach state set state corresponding
markov chain hitting time 
definition     norris        let  xt  t  markov chain state space i  hitting
time subset random variable h                    given
h      inf t     xt    a 
specifically  interested expected hitting time set states a  given
markov chain starts initial state x    i  denote quantity
kia   ei  h   
general  expected hitting time set states found solving
system linear equations 
theorem    vector expected hitting times k   e h      kia   i 
minimal non negative solution system linear equations

ki     p

   

kia       j
p
k
 a
ij
j
 
convergence absorbing state may guaranteed general markov chain 
calculate probability reaching absorbing state  use following theorem
 norris        
theorem    let set states  vector hitting probabilities ha    ha
 
               n    minimal non negative solution system linear equations

 


hi   p

p
h
 a
ij
j
j         n  
solving systems linear equations theorems     analytically might
difficult many markov chains though  fortunately  markov chain
one absorbing state      move state j j  use
following theorem derive upper bound expected hitting time  proved
rego        
theorem    let       
    e xt    xt   i   
    


kia   log  

   


 




ficigler   faltings

   learning algorithm
section  describe algorithm agents use learn correlated equilibrium channel allocation game 
let us denote space available correlation signals k                   k    
space available channels c                   c   assume c n  
agents channels  the opposite case easier   agent strategy   k    c
uses decide channel access time receives correlation
signal kt    kt        agent transmit signal kt   agent stores
strategy simply table 
agent adapts strategy follows 
   beginning  k  k   k    initialized uniformly random c 
is  every agent picks random channel transmit on  agent monitor
channels 
   time t 
 kt        agent tries transmit channel  kt   
otherwise  kt        agent chooses random channel mi  t  c
monitor activity 
   subsequently  agent observes outcome choice  agent transmitted
channel  observes whether transmission successful  was 
agent keep strategy unchanged  collision occurred  agent sets
 kt        probability p  probability   p  strategy remains same 
   agent transmit  observes whether channel mi  t  monitored
free  channel free  agent sets  kt      mi  t  probability   
channel free  strategy remains same 

   convergence
important property learning algorithm if  fast converge
pure strategy nash equilibrium channel allocation game every signal value 
algorithm randomized  therefore  instead analyzing worst case behavior  that may
arbitrarily bad   analyze expected number steps convergence 
    convergence c      k    
single channel single coordination signal  prove following theorem 
theorem    n agents c      k          p      expected number steps
allocation algorithm
converges
pure strategy nash equilibrium channel


 
allocation game p  p  log n  
prove convergence algorithm  useful describe execution
markov chain 
   

fidecentralized anti coordination multi agent learning

n agents compete single signal value  state markov chain
vector       n denotes agents attempting transmit  purpose
convergence proof  important many agents trying transmit 
agents  probability agents back off
everyone  therefore  describe algorithm execution using following chain 
definition    markov chain describing execution allocation algorithm
c      k          p     chain whose state time xt                n   
xt   j means j agents trying transmit time t 
transition probabilities chain look follows 
pr  xt     n  xt         
pr  xt       xt         

ij
pr  xt     j xt   i   
p    p j
j

 restart 
 absorbing 
     j

transition probabilities    agents
transmitting channel  agent attempt access it 
probability pr  xt     n  xt      equal   channel becomes
free  xt       agents spot time      everyone transmit  therefore
chain state xt     n    probability pr  xt       xt      equal one
single agent successfully transmits channel  keep transmitting
forever after  agent attempt transmit there  finally  probability
pr  xt     j xt   i  expresses fact xt    i agents transmit time t  
probability agent transmitted time keep transmitting time    
probability   p 
interested number steps take markov chain first arrive
state xt     given started state x    n   would mean agents
converged setting one transmitting  others not 
definition   defined hitting time describes quantity 
show expected value e h    hitting time state xt      and
corollary  prove theorem    following steps 
   show expected hitting time set states           lemma   
   show probability markov chain enters state   entering state
   starts state      lemma   
   finally  using law iterated expectations  combine two lemmas show
expected hitting time state   
lemma    let           expected
hitting

time set states markov
 
chain described definition   p log n  
proof  first prove
hitting time set a        slightly
expected

modified markov chain p  log n  
   

ficigler   faltings

let us define new markov chain  yt  t  following transition probabilities 
pr  yt       yt         

ij
pr  yt     j yt   i   
p    p j
j

 absorbing 
j     

note transition probabilities chain  xt  t    except
states      state   positive probability going state    state  
absorbing  clearly  expected hitting time set a        new chain
upper bound expected hitting time set          old chain 
path leads state   new chain either go state  
 so happened probability old chain   goes state   
old chain would stop state    but would one step shorter  
chain state yt   i  next state yt   drawn binomial distribution
parameters  i    p   expected next state therefore
e yt    yt   i    i   p 
 
therefore use theorem       p
derive a        
hitting time is 


l
 
 
a 
ki   log    
log
 p
p
p

upper bound kia          old chain 
lemma    probability hi markov chain defined definition   enters state  
entering state    started state      greater   p 
proof  calculating probability chain x enters state   state   equal
calculating hitting probability  i e  probability chain ever enters
given state  modified markov chain probability staying state  
pr  xt       xt           set states a  let us denote ha
probability
markov chain starting state ever enters state a  calculate probability 
use theorem    modified markov chain cannot leave neither state  
state    computing ha
    easy  since matrix system linear equations
lower triangular 
well show hi q     p     using induction  first step calculating hi
          
h     
h     
h       p   h     p   p h    p  h 
 p   p 
    p 
 
 
  p 
     p  
 p
now  induction step  derive bound hi assuming hj q     p
j   i  j   
   

fidecentralized anti coordination multi agent learning


x
ij
hi  
p    p j hj
j
j  




x
ij
p    p j q ipi     p  q h    pi h 
j
j  

  q ipi     p  q    q     p 
means matter state   markov chain starts in  enter
state   earlier state   probability least   p 
finish proof bound expected hitting time state   
use law iterated expectations 
theorem     billingsley        let x random variable satisfying e  x    
another random variable probability space 
e x    e  e x y     
i e   expected value x equal conditional expected value x given  
let h  random variable corresponding hitting time state    define
random variable z denoting number passes state   markov chain makes
reaches state    theorem    get 
e h      e e h   z   
lemma    shown expected number steps ha markov chain
reaches set states           write
e e h   z     e

  z
x

 
ha   e z ha     ha e z  

i  

lemma   know probability chain passes state  
passing   greater   p  therefore  say e z  e z    
z   random variable distributed according geometric distribution success
probability   p 


 
ha
 
 o
log n  
e h      ha e z  ha e z    
 p
p   p 
concludes proof theorem   
shown expected time convergence algorithm finite 
polynomial  probability algorithm converge finite number
steps absorbing state  following theorem shows since expected hitting
time absorbing state finite  probability   
   

ficigler   faltings

theorem    let h  hitting time state xt     markov chain
definition   
pr h  finite      
proof  markov inequality  know since h    
pr h   

e h   
 


therefore 
pr h  finite      lim pr h      lim




e h   
    


means algorithm converges almost surely nash equilibrium
channel allocation game 
    convergence c    k    
theorem     n agents c    k      expected number steps
learning algorithm

h converges toia pure strategy nash equilibrium channel allocation
 
game c  p p  log n   c  
proof  beginning  least one channel 
n agents want
 
transmit  take average p log n steps get state either    
agents transmit  lemma     call period round 
agents backed off  take average c steps
find empty channel  call period break 
channels might oscillate round break periods parallel 
worst case  whole system oscillate
two periods 


 
single channel  takes average  p oscillations two periods


one agent transmits channel  c    takes
 
average c  p steps round break channels one

h

 
 
agent transmitting  therefore  take average c  p
log
n
 
c
steps
p
system converges 
    convergence c    k  
show convergence time k      use general problem 
imagine k identical instances markov chain  know
original markov chain converges initial state absorbing state expected
time   imagine complex markov chain  every step  selects uniformly
random one k instances original markov chain  executes one step
instance  time tall k instances converge absorbing states 
extension well known coupon collectors problem  feller        
following theorem  gast        thm     shows upper bound expected number
steps k instances original markov chain converge 
   

fidecentralized anti coordination multi agent learning

theorem      gast        let k instances markov chain
known converge absorbing state expectation steps  select randomly
one markov chain instance time allow perform one step chain 
take average e tall   k log k    t k     steps k instances converge
absorbing states 
arbitrary c    k    following theorem follows theorems       
theorem     n agents c    k        p          q      expected number
steps learning algorithm converges pure strategy nash equilibrium
channel allocation game every k k




 
 
 k log k    k c
c   log n      
 p
p
aumann        shows nash equilibrium correlated equilibrium 
convex combination correlated equilibria correlated equilibrium  know
pure strategy nash equilibria algorithm converges efficient 
collisions  every channel every signal value  agent transmits  therefore 
conclude following 
theorem     learning algorithm defined section   converges expected polynomial
 
time  with respect k  c  p     p
log n   efficient correlated equilibrium
channel allocation game 

   fairness
agents decide strategy independently value coordination signal  therefore  every agent equal chance game converges equilibrium
favorable her  agent transmit resulting equilibrium given signal
value  say agent wins slot  c available channels n agents  agent
c
wins given slot probability n
 since agent transmit two channels
time  
analyse fairness algorithm converged correlated equilibrium  algorithms converge absorbing state  such aloha  
need analysed intermediate states execution  believe approach justified fact algorithm converges relatively quickly  polynomial
time 
describe number signals agent wins channel
random variable xi   variable distributed according binomial distribution
c
parameters k  n
 
measure fairness  use jain index  jain  chiu    hawe        
advantage jain index continuous  resource allocation strictly
fair higher jain index  unlike measures assign binary values 
whether least half agents access resource   also  jain index independent
population size  unlike measures standard deviation agent allocation 
   

ficigler   faltings

random variable x  jain index following 
j x   

 e x   
e x    

c
  
x distributed according binomial distribution parameters  k  n
first second moments

c
e x    k
n


 
c  
c n c
e x   k
 k

 
n
n
n
jain index
j x   

c k
 
c k    n c 

   

jain index holds     j x     allocation considered fair
j x      

n
theorem     c  k   n
c   limit limn ck     
lim j x      

n

allocation becomes fair n goes  
proof  theorem follows fact
lim j x    lim

n

n

c k
c k    n c 

limit equal    need
n c
  
n c k
lim

holds exactly k  
assume c n   

n
c



 that k grows asymptotically faster

n
c 

note

practical purposes  may need know big shall choose k given c
n   following theorem shows that 
theorem     let     
 
k 





n
   
c

j x       
proof  theorem follows straightforwardly equation   
   

fidecentralized anti coordination multi agent learning

 

convergence steps

  

 

  

 

  

 

  

  

  

  

  

  

  

c

figure    average number steps convergence n       k   n c
               n   

   experimental results
experiments  report average values     runs experiment 
error bars graphs denote interval contains true expected value
probability      provided samples follow normal distribution  error bars
missing either graph reports values obtained theoretically  jain index
constant back off scheme  confidence interval small scale graph 
    static player population
first analyze case population players remains
time 
      convergence
first  interested convergence allocation algorithm  section  
know polynomial  many steps algorithm need converge
practice 
figure   presents average number convergence steps n       k   n
increasing number available channels c                n    interestingly  convergence
takes longest time c   n   lowest convergence time c   n   
c     increases again 
happens change size signal space k  figure   shows
average number steps convergence fixed n   c varying k  theoretically 
   

ficigler   faltings

    

convergence steps

    
    
   
   
   
   
 
 

  

  

  

  

  

  

  

k

 

 

   

   

   

   

   

   

jain index

jain index

figure    average number steps convergence n       c  

   
   
k n
k   nlog n

   

   
 

  

  

  

  

   

   

   
k  
k    log n
 

   

k   n 

   
 

   

n

k             n   

   

   

 

   

n
 

k    n
  

  

  

  

   

   

   

n

 a  c    

 b  c  

n
 

figure    jain fairness index different settings c k  increasing n  

shown number convergence steps o k log k  theorem     however 
practice convergence resembles linear dependency k  algorithm
needs converge coordination signals 
      fairness

section    know k   n
c   jain fairness index converges  
n goes infinity  fast convergence  big need choose k 
depending n c  achieve reasonable bound fairness 
   

fidecentralized anti coordination multi agent learning

figure   shows jain index n increases  c     c   n  respectively 
various settings k  even though every time k   n
c  that is  k grows faster
n
c   jain index increases  as shown theorem      marked difference
various settings k  k   n
c   jain index  from equation    
j x   

n
 
 n c

   

therefore  c      jain index converges      c  
equal    n      figure   shows 

n
  

jain index

      optimizing fairness
saw fair outcome allocation algorithm agents consider game
signal value independently  however  best do 
improve fairness  agent correlates decisions different signal values 
perfectly fair solution  every agent wins  and consequently transmit 
number signal values  however  assume agents know many
agents system  therefore  agents know fair
share signal values transmit for  nevertheless  still use information
many slots already transmitted decide whether back off stop
transmitting collision occurs 
definition    strategy fit agent round t  define cardinality
number signals strategy tells agent access 


 fit     k k fit  k     
intuitively  agents whose strategies higher cardinality back off often
strategy low cardinality 
compare following variations channel allocation scheme  differ
original one probability agents back collisions 
constant scheme described section    every agent backs constant
probability p 
linear back off probability p  

 fit  
k  

exponential back off probability p  



 f  
  ki

parameter         

worst agent last case collision  agent lowest  fit   back
off  others collided  back off  greedy algorithm requires
information assume agents have 
compare fairness allocations experiments  need define jain
index actual allocation  resource allocation vector x    x    x            xn   
xi cardinality strategy used agent i  allocation x  jain index is 
p
 
n
i   xi
j x   
p
 
n n
i   xi
   

ficigler   faltings

c   n    k    log n
 
    

jain index

    
    
    
   
constant
linear
exponential
worstagent

    
    
 

  

  

  

  

   

   

   

n

figure    jain fairness index channel allocation scheme various back off probabilities  c   n    k     log  n

figure   shows average jain fairness index allocation back off probability
variations  fairness approaching   worst agent last algorithm 
worst everyone using back off probability  ratio back off
probability lowest cardinality agent highest cardinality agent decreases 
fairness increases 
shows improve fairness using different back off probabilities  nevertheless  shape fairness curve them  furthermore 
exponential back probabilities lead much longer convergence  shown figure   
c   n    convergence time linear constant back off schemes similar 
unrealistic worst agent last scheme obviously fastest  since resolves collisions
  step  unlike back off schemes 
    dynamic player population
take look performance algorithm population players
changing time  either new players join old players get replaced new ones  
analyze case errors players observe either
coordination signal channel feedback noisy 
      joining players
section  present results experiments group players joins
system later  corresponds new nodes joining wireless network  precisely 
   

fidecentralized anti coordination multi agent learning

c   n    k    log n
constant
linear
exponential
worstagent
convergence steps

 

  

 

  

 

  

 

 

  

  
n

figure    convergence steps various back off probabilities 
    players join network beginning  remaining     players
join network later  one one  new player joins network previous players
converged perfect channel allocation 
experiments two ways initializing strategy new player 
greedy either  joining players cannot observe many players already system  therefore  initial strategy tries transmit possible
slots 
polite or  players observe n  t   number players already
system time t  new player joins system  therefore  initial
strategy tries transmit slot probability n  t   
figure   shows jain index final allocation     players join later 
c      players join greedy  aggressive  start
transmitting slots  hand  polite  aggressive
enough  new player starts strategy aggressive strategies
players already system  difference new player experience
collision every slot transmits in  old players experience collision
 
n  t  slots  therefore  back less slots 
therefore  especially constant scheme  resulting allocation unfair 
either better new players  when greedy  older players  when
players polite  
phenomenon illustrated figure    compares measure called group fairness 
average throughput last     players joined network end  new
   

ficigler   faltings

c      k   nlog n  join delay   converge init population         kps
 

 

   

   

   

   

   

   

   

   

jain index

jain index

c      k   nlog n  join delay   converge init population       
 

   
   
   

   
   
   

   

   

constant
linear
worstplayer

   
 
 

  

  

  

  

  

  

  

  

constant
linear
worstplayer

   
 
 

  

  

  

  

  

n

  

  

  

  

  

n

 a  greedy

 b  polite

figure    joining players  jain index  c     k   n log  n   two graphs show
results two ways initializing strategy new player 

c      k   nlog n  join delay   converge init population       

c      k   nlog n  join delay   converge init population         kps

 

   

constant
linear
worstplayer

   

constant
linear
worstplayer

   
   

   
group fairness

group fairness

 

 
   
 

   
   
   

   
   

 
   
 

  

  

  

  

  

n

   
 

  

  

  

  

  

n

 a  greedy

 b  polite

figure    joining players  group fairness  c     k   n log  n   two graphs show
results two ways initializing strategy new player 

   

fidecentralized anti coordination multi agent learning

c   n    k    log n  join delay   converge init population       

c   n    k    log n  join delay   converge init population         kps

 

 

   

   

   

   
   

   

jain index

jain index

   

   
   
   

   
   
   

   

   

constant
linear
worstplayer

   
 
 

   

  

  

  

  

  

  

  

  

constant
linear
worstplayer

   

  

 
 

  

  

n

  

  

  

  

  

  

  

n

 a  greedy

 b  polite

figure    joining players  jain index  c   n  k     log  n   two graphs show
results two ways initializing strategy new player 

players  divided average throughput first     players join network
beginning  old players  
lets look first case players greedy  constant scheme 
ratio around      linear scheme  ratio lower  although increasing n  the
total number players  grows  worst player last scheme  ratio stays constant
interestingly  lower    means old players better
new players 
players polite  situation opposite  old players way better
new players  constant scheme  throughput ratio     
figures     show graphs c   n    here  newly joining players
worse even start transmitting every slot 
experience collision every time  because channels slots occupied   old
players experience collision probability n    hand  overall
 

fairness whole population better  channels share
agent use one channel 
difference old new players even pronounced new
players polite 
      restarting players
another scenario looked happens one old players switches
replaced new player randomly initialized strategy  say
player got restarted  wireless network  corresponds situation user
restarts router  note number players network stays same 
players forget learned start scratch 
specifically  every round  every player probability pr
restarted  restart  start strategy initialized two ways 
   

ficigler   faltings

c   n    k    log n  join delay   converge init population       

c   n    k    log n  join delay   converge init population         kps

 

    

 

constant
linear
worstplayer

   

constant
linear
worstplayer

   

    
group fairness

group fairness

   

   
    
   

   
   

    

   

   
    
 

   

  

  

  

  

  

n

 

  

  

  

  

  

n

 a  greedy

 b  polite

figure    joining players  group fairness  c   n  k     log  n   two graphs show
results two ways initializing strategy new player 

greedy assume player know n   number players system 
signal value k k chooses randomly  k  c  means
attempts transmit every slot randomly chosen channel 
polite assume player know n   k k  chooses  k  c probability
c
n    k       otherwise 
figure    shows average overall throughput n       c      k   n log  n
k   n two initialization schemes  dotted line four graphs shows
overall performance players attempt transmit randomly chosen channel
c
probability n
  baseline solution reaches  e     average throughput 
probability restart increases  average throughput decreases  players
get restarted greedy  attempt transmit every slot 
one channel available  means restarted player causes collision every slot 
therefore  surprising restart probability pr       n      
throughput virtually    every step  expectation least one player get restarted 
collision almost always 
interesting phase transition occurs pr     k   n log  n  
pr     k   n   there  performance
baseline random access scenario  that requires players know n though   similar
phase transition occurs players polite  even though resulting throughput
higher  since restarted players less aggressive 
yet another interesting  surprising  phenomenon worstplayer last scheme still achieves highest throughput  constant back scheme
better linear back off scheme  average overall throughput 
matters fast players able reach perfect allocation disruption 
worst player last scheme fastest  since resolves collision   step  con   

fidecentralized anti coordination multi agent learning

n       c      k   nlog n  kps
 

   

   

   

   

   

   

system throughput

system throughput

n       c      k   nlog n
 

   
   
   
   
   
   
 

 

   
   
   
   

constant
linear
worstplayer

   
 

  

   

  
restart probability

 

 

  

constant
linear
worstplayer
 

 a  greedy  k   n log  n

 

  

n       c      k   n  kps
 

   

   

   

   

   

   

system throughput

system throughput

n       c      k   n

   
   
   
   
   

 

  
restart probability

 b  polite  k   n log  n

 

   

 

  

   
   
   
   
   

constant
linear
worstplayer
 

  

   
 

  
restart probability

 

 

  

constant
linear
worstplayer
 

  

 c  greedy  k   n

 

  
restart probability

 d  polite  k   n

figure     restarting players  throughput  n       c    

   

 

  

ficigler   faltings

n       c   n    k   nlog n  kps
 

   

   

   

   

   

   

system throughput

system throughput

n       c   n    k   nlog n
 

   
   
   
   
   
   
 

 

   
   
   
   

constant
linear
worstplayer

   
 

  

   

  
restart probability

 

 

  

constant
linear
worstplayer
 

 a  greedy  k     log  n

  

n       c   n    k      kps
 

   

   

   

   

   

   

system throughput

system throughput

n       c   n    k    

   
   
   
   
   

 

 

  
restart probability

 b  polite  k     log  n

 

   

 

  

   
   
   
   
   

constant
linear
worstplayer
 

  

   
 

  
restart probability

 

 

  

constant
linear
worstplayer
 

  

 c  greedy  k    

 

 

  
restart probability

  

 d  polite  k    

figure     restarting players  throughput  n       c  

n
 

stant scheme back off probability p      worse  see theorem      linear scheme
slowest 
figure    shows average overall throughput c   n    k   log  n k     
substantial difference players greedy polite  since
many channels available  restarted player cause small number collisions
 in one channel n  every slot   throughput decrease much 
also  convergence time linear constant scheme
c   n    adapt disruption equally well 
      noisy feedback
far assumed players receive perfect feedback whether transmissions
successful not  could observe activity given channel perfectly 
going loosen assumption now 
   

fidecentralized anti coordination multi agent learning

n       c   n    k   nlog n
 

   

   

   

   

   

   

system throughput

system throughput

n       c      k   nlog n
 

   
   
   
   
   
   
 

 

   
   
   
   

constant
linear
worstplayer
  

   

   
 

  
noisy feedback probability

 

 

  

constant
linear
worstplayer
 

  

 a  c      k   n log  n

 

  
noisy feedback probability

 b  c  

n
 

 

  

  k     log  n

figure     noisy feedback  throughput  n     

n       c   n    k   nlog n
 

    

    

    

    

    

    

    

    

jain index

jain index

n       c      k   nlog n
 

   
    
    

    
    

    

    
constant
linear
worstplayer

    
   

   

 

  

 

  
noisy feedback probability

   

 

  

constant
linear
worstplayer

    
 

  

 a  c      k   n log  n

 

  
noisy feedback probability

 b  c  

n
 

 

  

  k     log  n

figure     noisy feedback  jain index  n     
suppose every step  every player probability pf feedback
receives wrong  is  player transmitted  learn transmission
successful not  vice versa  player observed channel 
learn channel free fact  and vice versa   context
wireless networks  corresponds interference wireless channel 
affect learning 
figure    show average overall throughput c     c   n  respectively  one channel  constant scheme better linear scheme 
adapts faster disruptions  c   n    schemes equivalent 
equally fast adapt  phase transition occurs noisy feedback probability
pf        
   

ficigler   faltings

n       c   n    k   nlog n

 

   

   

   

   

   

   

system throughput

system throughput

n       c      k   nlog n

 

   
   
   
   
   
   
 

   
   
   
   
   

constant
linear
worstplayer
 

  

   
 

  
noisy signal probability

 

 

  

constant
linear
worstplayer
 

  

 a  c      k   n log  n

 

  
noisy signal probability

 b  c  

n
 

 

  

  k     log  n

figure     noisy coordination signal  throughput  n     
figure    shows jain index allocation players receive noisy feedback 
usual  linear scheme better constant  even though throuput lower  as
shown above   overall throughput drops close    schemes
obviously almost fairness 
      noisy coordination signal
algorithm assumes players observe coordination signal every
step  signal come from  may random noise given
frequency  fm radio transmission etc  however  coordination signal might noisy 
different players observe different value  means learning would
sync  wireless networks  corresponds clock drift 
see happens case  use following experiment  every step 
every player observes correct signal  i e  one observed everyone else 
probability   ps   probability ps observes false signal  that still
taken uniformly random set          k     
overall throughput shown figure     see system able
cope fairly high level noise signal  drop throughput occurs
ps         case experiments noisy feedback  constant back off
scheme able achieve higher throughput thanks faster convergence 
jain index allocation  figure     stays almost constant 
throughput drops jain index increases  allocation random 
fair 
    generic multi agent learning algorithms
several algorithms proved converge correlated equilibrium proposed multi agent learning literature  introduction  mentioned three
learning algorithms  foster   vohra        hart   mas colell        blum   man   

fidecentralized anti coordination multi agent learning

n       c   n    k   nlog n
 

    

    

    

    

    

    

    

    

jain index

jain index

n       c      k   nlog n
 

   
    
    

    
    

    

    
constant
linear
worstplayer

    
   

   

 

  

 

  
noisy signal probability

   

 

  

constant
linear
worstplayer

    
 

  

 a  c      k   n log  n

 

  
noisy signal probability

 b  c  

n
 

 

  

  k     log  n

figure     noisy coordination signal  jain index  n     
sour         however  analysis foster vohra applicable games two
players  section  briefly recall two multi agent learning algorithms
 hart   mas colell        blum   mansour         compare performance
algorithm presented section   
two algorithms compare algorithm based notion minimizing regret agents experience adopting certain strategy  intuitively 
describe concept regret follows  imagine agent uses strategy couple
rounds game  accumulates certain payoff  would know
payoff compare payoff acquired simple alternative strategy   difference payoff strategy regret agent perceives  ex post 
choosing strategy strategy  
mean simple strategy  one class simple strategies strategies
always select action  external regret compares performance
strategy performance best single action ex post 
another class alternative strategies strategies modify strategy slightly 
every time strategy proposes play action a  alternative strategy proposes
action a     instead  internal regret defined regret strategy compared
best alternative strategy  agents adopt strategy low internal
regret  converge strategy profile close correlated equilibrium  also
shown blum   mansour        
hart mas colell        present simple multi agent learning algorithm
guaranteed converge correlated equilibrium  assume players
observe actions opponents every round game  players start
choosing actions randomly  update strategy follows  let ai
action player played round t   action aj ai   aj    ai   player calculates
difference average payoff would received played action aj
instead ai past  average payoff received far playing action ai  
mentioned above  call difference internal regret playing action ai
   

ficigler   faltings

c   n  

c  
 

  

constant backoff
hartmascolell
blummansour

constant backoff
hartmascolell
blummansour
 

convergence steps

convergence steps

  
 

  

 

  

 

  

 

  

 

  

  
n

  

  

 a  c    

 

  

  
n

 b  c  

  

  

n
 

figure     general multi agent learning algorithms  convergence rate 

instead action aj   player chooses action play round probability
proportional internal regret compared previous action ai   actions negative
regret never played  previous action ai played positive probability
way  strategy certain inertia 
hart mas colell        prove agents adopt adaptive procedure described above  empirical distribution play  the relative frequency playing
certain pure strategy profile  converges almost surely set correlated equilibria 
blum mansour        present general technique convert learning algorithm
low external regret algorithm low internal regret  idea run
multiple copies external regret algorithm  step  copy returns probability
vector playing action  probability vectors combined one joint
probability vector  player observes payoff playing action  updates
payoff beliefs external regret algorithms proportionally weight
joint probability vector  authors show players use learning
algorithm low internal regret  empirical distribution game converges close
correlated equilibrium 
one low external regret algorithms blum mansour        present
polynomial weights  pw  algorithm  there  player keeps weight
actions  every round game  updates weight proportionally loss
 negative payoff  action incurred round  actions higher weight get
chosen higher probability 
implemented two generic multi agent learning algorithms  internalregret based algorithm hart mas colell         pw algorithm blum
mansour         experiments  algorithms always converge pure strategy
nash equilibrium channel allocation game  therefore efficient allocation 
however  resulting allocation fair  subset agents size c ever
access channels 
   

fidecentralized anti coordination multi agent learning

c   n  
 

   

   

   

   

   

   

   

   

jain index

jain index

c  
 

   
   
   

   
   

   

   
constant backoff
hartmascolell
blummansour

   
 
 

   

  

  
n

  

constant backoff
hartmascolell
blummansour

   

  

 a  c    

 
 

  

  
n

 b  c  

  

  

n
 

figure     general multi agent learning algorithms  jain index 

figure    shows average number rounds algorithms take converge
stable outcome  compare performance learning algorithm section   
learning algorithm  set k      converges pure strategy
nash equilibrium game  performed     runs algorithm scenario 
error bars figure    show     confidence interval average  assuming
convergence times distributed according normal distribution 
surprisingly  generic algorithms hart mas colell        blum
mansour        cannot match convergence speed algorithm  designed specifically
problem channel allocation  generic algorithms converge pure strategy
ne  outcome unfair  jain index low  evidenced figure    
dont report confidence bounds jain index  experiments
resulting jain index same 

   related work
broadly speaking  paper interested games payoff agent receives
certain action inversely proportional number agents chose
action  achieve efficient fair outcome games  variants
problem studied several previous works 
simplest variant minority game  challet  marsili    zhang        
game  n agents simultaneously choose two actions  agents chose
action chosen minority agents receive payoff    whereas agents whose
action choice majority receive
payoff    game many pure strategy nash
equilibria  group n    agents chooses one action rest choose
action  equilibria efficient  since largest possible number agents achieve
maximum payoff  however  fair  payoff losing group agents
always    game one mixed strategy ne fair  every agent chooses
   

ficigler   faltings

action randomly  equilibrium 
hand  efficient  expected size
minority group lower n    due variance action selection 
savit  manuca  riolo        show agents receive feedback action
minority  learn coordinate better achieve efficient outcome
repeated minority game  basing agents decisions history
past iterations  cavagna        shows result achieved agents
base decisions value random coordination signal instead using
history  direct inspiration idea global coordination signal presented
paper 
ideas literature minority games recently found way
cognitive radio literature  mahonen petrova        present channel allocation
problem much ours  agents learn channel use using strategy
similar strategies minority games  difference instead preferring
action chosen minority  channel allocation problem  agent prefers channels
chosen anyone else  using approach  mahonen petrova able
achieve stable throughput     even number agents try
transmit channel increases  however  agent essentially choosing one
fixed set strategies  cannot adapt  therefore  difficult achieve
perfectly efficient channel allocation 
wang et al         implemented algorithm work actual wireless
network  setting  wireless devices able monitor activity channels 
coordination signal  used actual data packets agents send 
authors shown practice  learning algorithm  which call attachment
learning  improves throughput     random access slotted aloha protocol 
another  general variant problem  called dispersion game described
grenager  powers  shoham         dispersion game  agents choose several
actions  prefer one chosen smallest number agents 
authors define maximal dispersion outcome outcome agent move
action fewer agents  set maximal dispersion outcomes corresponds set
pure strategy nash equilibria game  propose various strategies converge
maximal dispersion outcome  different assumptions information available
agents  contrary work  individual agents dispersion games
particular preference actions chosen equilibria
achieved  therefore  issues achieving fair outcome 
verbeeck  nowe  parent  tuyls        use reinforcement learning  namely linear
reward inaction automata  learn nash equilibria common conflicting interest
games  class conflicting interest games  to channel allocation game
belongs   propose algorithm allows agents circulate various
pure strategy nash equilibria  outcome game fair  contrast
work  solution requires communication agents  requires
agents know strategies converged  addition  linear reward inaction automata
guaranteed converge pure strategy ne conflicting interest games 
may converge pure strategies 
games discussed above  including channel allocation game  form part
family potential games introduced monderer shapley         game called
   

fidecentralized anti coordination multi agent learning

potential game admits potential function  potential function defined every
strategy profile  quantifies difference payoffs agent unilaterally deviates
given strategy profile  different kinds potential functions  exact  where
difference payoffs deviating agent corresponds directly difference
potential function   ordinal  where sign potential difference
sign payoff difference  etc 
potential games several nice properties  important purestrategy nash equilibrium local maximum potential function  finite
potential games  players reach equilibria unilaterally playing best response 
matter initial strategy profile start from 
existence natural learning algorithm reach nash equilibria makes potential
games interesting candidate future research  would see kind
correlated equilibria agents converge there  use simple correlation
signal coordinate 

   conclusions
paper  proposed new approach reach efficient fair solutions multi agent
resource allocation problems  instead using centralized  smart coordination device
compute allocation  use stupid coordination signal  general random integer
k                k     priori relation problem  agents smart 
learn  value coordination signal  action take 
game theoretic perspective  ideal outcome game correlated equilibrium  results show using global coordination signal  agents learn play
convex combination pure strategy nash equilibria  correlated equilibrium 
showed learning strategy that  variant channel allocation game  converges expected polynomial number steps efficient correlated equilibrium 
proved equilibrium becomes increasingly fair k  number available
synchronization signals  increases 
confirmed fast convergence well increasing fairness increasing k
experimentally  investigated performance learning strategy case
agent population dynamic  new agents join population  learning strategy
still able learn efficient allocation  however  fairness allocation depend
greedy initial strategies new agents are  agents restart random
intervals  becomes important fast strategy converges  simple strategy
everyone backs transmitting constant probability able achieve higher
throughput sophisticated strategy back off probability depends
many slots agent already transmitting  showed experimentally
learning strategy robust noise coordination signal  well
feedback agents receive channel use  noisy scenarios  faster convergence constant back off scheme helped achieve higher throughput
fair linear back off scheme  finally  compared performance learning strategy generic multi agent learning algorithms based regret minimization  hart  
mas colell        blum   mansour         generic algorithms theoretically
proven converge distribution play close correlated equilibrium 
   

ficigler   faltings

guaranteed converge specific ce  indeed  experiments  algorithms
hart mas colell blum mansour always converged efficient unfair
pure strategy nash equilibrium channel allocation game 
learning algorithm presented paper implemented real wireless
network wang et al          shown achieves     higher throughput
random access protocols aloha 
paper  address issue whether non cooperative rational
agents would follow protocol outlined  work  cigler   faltings        
address issue show certain conditions  protocol implemented
nash equilibrium strategies infinitely repeated resource allocation game 

references
abramson  n          aloha system  another alternative computer communications  proceedings november              fall joint computer conference 
afips     fall   pp          new york  ny  usa  acm 
aumann  r          subjectivity correlation randomized strategies  journal
mathematical economics              
billingsley  p          probability measure  wiley series probability statistics 
 anniversary edition edition   wiley 
blum  a     mansour  y          algorithmic game theory  nisan  n   roughgarden 
t   tardos  e     vazirani  v   eds    algorithmic game theory  chap     cambridge
university press 
cavagna  a          irrelevance memory minority game  physical review e         
r    r     
challet  d   marsili  m     zhang  y  c          minority games  interacting agents
financial markets  oxford finance   oxford university press  new york  ny  usa 
chen  x     deng  x          settling complexity two player nash equilibrium 
       th annual ieee symposium foundations computer science  focs    
pp          ieee 
cheng  s   raja  a   xie  l     howitt  i          distributed constraint optimization algorithm dynamic load balancing wlans  ijcai    workshop distributed
constraint reasoning  dcr  
cigler  l     faltings  b          symmetric subgame perfect equilibria resource allocation   to appear  proceedings   th national conference artificial
intelligence  aaai      menlo park  ca  usa  american association artificial
intelligence 
feller  w          introduction probability theory applications  vol      rd
edition    edition   wiley 
foster  d  p     vohra  r  v          calibrated learning correlated equilibrium  games
economic behavior                 
   

fidecentralized anti coordination multi agent learning

gast  n          computing hitting times via fluid approximation  application coupon
collector problem  arxiv e prints 
grenager  t   powers  r     shoham  y          dispersion games  general definitions
specific learning results  proceedings eighteenth national conference
artificial intelligence  aaai      pp          menlo park  ca  usa  american
association artificial intelligence 
hart  s     mas colell  a          simple adaptive procedure leading correlated equilibrium  econometrica                   
jain  r  k   chiu  d  m  w     hawe  w  r          quantitative measure fairness
discrimination resource allocation shared computer systems  tech  rep   digital
equipment corporation 
leyton brown  k     shoham  y          essentials game theory  concise  multidisciplinary introduction  morgan   claypool  san rafael  ca 
littman  m     stone  p          implicit negotiation repeated games intelligent agents
viii  meyer  j  j     tambe  m   eds    intelligent agents viii  vol       lecture
notes computer science  chap      pp          springer berlin   heidelberg 
berlin  heidelberg 
mahonen  p     petrova  m          minority game cognitive radios  cooperating without cooperation  physical communication               
monderer  d     shapley  l  s          potential games  games economic behavior 
                
norris  j  r          markov chains  cambridge series statistical probabilistic
mathematics   cambridge university press 
papadimitriou  c  h     roughgarden  t          computing correlated equilibria multiplayer games  journal acm              
rego  v          naive asymptotics hitting time bounds markov chains  acta informatica                 
savit  r   manuca  r     riolo  r          adaptive competition  market efficiency 
phase transitions  physical review letters                    
verbeeck  k   nowe  a   parent  j     tuyls  k          exploring selfish reinforcement
learning repeated games stochastic rewards  autonomous agents multiagent systems                 
wang  l   wu  k   hamdi  m     ni  l  m          attachment learning multi channel
allocation distributed ofdma networks  parallel distributed systems  international conference on            

   


