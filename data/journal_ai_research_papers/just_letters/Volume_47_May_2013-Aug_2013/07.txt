journal of artificial intelligence research                  

submitted        published      

decentralized anti coordination
through multi agent learning
ludek cigler
boi faltings

ludek cigler epfl ch
boi faltings epfl ch

artificial intelligence laboratory
ecole polytechnique federale de lausanne
ch      lausanne  switzerland

abstract
to achieve an optimal outcome in many situations  agents need to choose distinct
actions from one another  this is the case notably in many resource allocation problems 
where a single resource can only be used by one agent at a time  how shall a designer of a
multi agent system program its identical agents to behave each in a different way 
from a game theoretic perspective  such situations lead to undesirable nash equilibria 
for example consider a resource allocation game in that two players compete for an exclusive
access to a single resource  it has three nash equilibria  the two pure strategy ne are
efficient  but not fair  the one mixed strategy ne is fair  but not efficient  aumanns
notion of correlated equilibrium fixes this problem  it assumes a correlation device that
suggests each agent an action to take 
however  such a smart coordination device might not be available  we propose using
a randomly chosen  stupid integer coordination signal  smart agents learn which action
they should use for each value of the coordination signal 
we present a multi agent learning algorithm that converges in polynomial number of
steps to a correlated equilibrium of a channel allocation game  a variant of the resource
allocation game  we show that the agents learn to play for each coordination signal value
a randomly chosen pure strategy nash equilibrium of the game  therefore  the outcome
is an efficient correlated equilibrium  this ce becomes more fair as the number of the
available coordination signal values increases 

   introduction
in many situations  agents have to coordinate their actions in order to use some limited
resource  in communication networks  a channel might be used by only one agent at a time 
when driving a car  an agent prefers to choose a road with less traffic  i e  the one chosen
by a smaller number of other agents  when bidding for one item in several simultaneous
auctions  an agent prefers the auction with less participants  because this will usually lead
to a lower price  such situations require agents to take each a different decision  however 
all the agents are identical  and the problem each one of them face is the same  how can
they learn to behave differently from everyone else 
second problem arises when agents have common preferences over which action they
want to take  in the communication networks problem  every agent prefers to transmit over
being quiet  in the traffic situation  agents might all prefer the shorter path  but in order
to achieve an efficient allocation  it is necessary precisely for some agents to stay quiet  to
take the longer path  how can we achieve that those agents are not exploited  how can
c
    
ai access foundation  all rights reserved 

ficigler   faltings

the agents learn to alternate  taking the longer road on one day  while taking the shorter
road the next day 
a central coordinator who possesses complete information about agents preferences
and about the available resources can easily recommend each agent which action to take 
however  such an omniscient central coordinator is not always available  therefore  we
would like to be able to use a distributed scheme  we consider scenarios where the same
agents try to use the same set of resources repeatedly  and can use the history of the past
interactions to learn to coordinate the access to the resources in the future 
in particular  consider the problem of radio channel access  in this problem  n agents
try to transmit over c non overlapping channels  fully decentralized schemes  such as
aloha  abramson         can only achieve a throughput of  e       i e  any transmission only succeeds with probability  e        more complex schemes  such as those based
on distributed constraint optimization  cheng  raja  xie    howitt         can reach a
throughput close to       by throughput  we mean the probability of successful transmission on a given channel  however  the messages necessary to implement these schemes
create an overhead that eliminates part of their benefits 
in this paper we propose instead to use a simple signal that all agents can observe and
that ergodically fluctuates  this signal could be a common clock  radio broadcasting on
a specified frequency  the decimal part of a price of a certain stock at a given time  etc 
depending on this stupid signal  smart agents can then learn to take a different action
for each of its value  we say that the signal is stupid  because it doesnt have anything to
do with the game  it is the agents who give it a meaning themselves by acting differently
for each value of the coordination signal 
lets look at the simplest example of a channel access problem  one where   agents try
to transmit over one shared channel  we can model this situation as a game in normal
form  agents can choose between two actions  to stay quiet  q  or to transmit  t    only
one agent may transmit successfully at a time  if an agent transmits alone  she receives a
positive payoff  if an agent does not transmit on the channel  her payoff is    if both agents
try to transmit on the channel at the same time  their transmissions fail and they incur a
cost c 
the payoff matrix of the game looks as follows 

q
t

q
    
    

t
    
c  c

such a game has two pure strategy nash equilibria  ne   where one player stays quiet
and the other one transmits  it has also one mixed strategy ne  where each player stays
 
quiet with probability c  
  the two pure strategy ne are efficient  in that they maximize
the social welfare  but they are not fair  only one player gets the full payoff  even though
the game is symmetric  the mixed strategy ne is fair  but not efficient  the expected
payoff of both players is   
as such  the nash equilibria of the game are rather undesirable  they are either efficient
or fair  but not both at the same time  in his seminal paper  aumann        proposed the
notion of correlated equilibrium that fixes this problem  a correlated equilibrium  ce  is
a probability distribution over the joint strategy profiles in the game  a correlation device
   

fidecentralized anti coordination through multi agent learning

samples this distribution and recommends an action for each agent to play  the probability
distribution is a ce if agents do not have an incentive to deviate from the recommended
action 
in the simple game described above  there exists a ce that is both fair and socially
efficient  the correlation device samples from each of the two pure strategy ne with probability    and then recommends the players which ne they should play  this corresponds
to an authority that tells each player whether to stay quiet or transmit 
correlated equilibria have several nice properties  they are easier to find  for a succinct
representation of a game  in polynomial time  see  papadimitriou   roughgarden        
also  every nash equilibrium is a correlated equilibrium  also  any convex combination of
two correlated equilibria is a correlated equilibrium  however  a smart correlation device
that randomizes over joint strategy profiles might not always be available 
it is possible to achieve a correlated equilibrium without the actual correlation device 
assume that the game is played repeatedly  and that agents can observe the history of
actions taken by their opponents  they can learn to predict the future action  or a distribution of future actions  of the opponents  these predictions need to be calibrated  that is 
the predicted probability that an agent i will play a certain action aj should converge to
the actual frequency with which agent i plays action aj   agents always play an action that
is the best response to their predictions of opponents actions  foster and vohra       
showed that in such a case  the play converges to a set of correlated equilibria 
however  in their paper  foster and vohra did not provide a specific learning rule to
achieve a certain ce  furthermore  their approach requires that every agent were able to
observe actions of every other opponent  if this requirement is not met  convergence to a
correlated equilibrium is not guaranteed anymore 
in this paper  we focus on a generalization of the simple channel allocation problem
described above  there are n agents who always have some data to transmit  and there
are c channels over which they can transmit  we assume that n  c  access to a channel
is slotted  that is  all agents are synchronized so that they start transmissions at the same
time  also  all transmissions must have the same length  if more than one agent attempts
to transmit over a single channel  a collision occurs and none of the transmissions are
successful  an unsuccessful transmission has a cost for the agent  since it has to consume
some of its  possibly constrained  power for no benefit  not transmitting does not cost
anything 
we assume that agents only receive binary feedback  if they transmitted some data 
they find out whether their transmission was successful  if they did not transmit  they can
choose some channel to observe  they receive information whether the observed channel
was free or not 
when described as a normal form game  this problem has several efficient  but unfair 
pure strategy nash equilibria  where a group of c agents gets assigned all the channels  the
remaining n  c agents get stranded  it has also a fair but inefficient mixed strategy ne 
where agents choose the transmission channels at random  as in the example of a resource
allocation game above  there exists a correlated equilibrium that is efficient and fair 
the stupid coordination signal introduced in this paper helps the agents to learn to
play a  potentially  different efficient outcome for each of its value  this way  they can
reach an efficient allocation while still preserving some level of fairness 
   

ficigler   faltings

the main contributions of this work are the following 
 we propose a learning strategy for agents in the channel allocation game that  using
minimal information  converges in polynomial time to a randomly chosen efficient
pure strategy nash equilibrium of the game 
 we show that when the agents observe a common discrete correlation signal  they
learn to play such an efficient pure strategy ne for each signal value  the result is
a correlated equilibrium that is increasingly fair as the number of available signals k
increases 
 we experimentally evaluate how sensitive the algorithm is to a player population
that is dynamic  i e  when players leave and enter the system  we also evaluate
the algorithms resistance to noise  be it in the feedback players receive or in the
coordination signal they observe 
the channel allocation algorithm proposed in this paper has been implemented by wang 
wu  hamdi  and ni        in a real world wireless network setting  they showed how the
wireless devices can use all use the actual data which are being transmitted as a coordination
signal  that way  they were able to achieve     throughput gain compared to random
access protocols such as aloha 
it is worth noting that in this work  our focus is on reaching a correlated and fair
outcome  provided that the agents are willing to cooperate  in situations where using
resources costs nothing  a self interested agent could stubbornly keep using it  everyone
else will then be better off not trying to access the resource  this is sometimes called the
watch out i am crazy or bully strategy  littman   stone        
in order to prevent this kind of behavior  we would need to make sure that in order to
use a resource  an agent has to pay some cost  such a cost may already be implicit in the
problem  such as the fact that wireless transmission costs energy  or it may be imposed by
external payments  in our recent work  cigler   faltings         we show how this leads to
equilibria where rational agents are indifferent between accessing the resource and yielding 
and how these equilibria implement our allocation policy for rational agents  we consider
this issue to be beyond the scope of this paper and refer the reader to our other work for a
deeper analysis 
the rest of the paper is organized as follows  in section    we give some basic definitions
from game theory and the theory of markov chains which we will use throughout the paper 
in section    we present the algorithm agents use to learn an action for each possible
correlation signal value  in section   we prove that such an algorithm converges to an
efficient correlated equilibrium in polynomial time in the number of agents and channels 
we show that the fairness of the resulting equilibria increases as the number of signals k
increases in section    section   highlights experiments that show the actual convergence
rate and fairness  we also show how the algorithm performs in case the population is
changing dynamically  in section   we present some related work from game theory and
cognitive radio literature  and section   concludes 
   

fidecentralized anti coordination through multi agent learning

   preliminaries
in this section  we will introduce some basic concepts of game theory and of the theory of
markov chains that we are going to use throughout the paper 
    game theory
game theory is the study of interactions among independent  self interested agents  an
agent who participates in a game is called a player  each player has a utility function
associated with each state of the world  self interested players take actions so as to achieve
a state of the world that maximizes their utility  game theory studies and attempts to
predict the behaviour  as well as the final outcome of such interactions  leyton brown and
shoham        give a more complete introduction to game theory 
the basic way to represent a strategic interaction  game  is using the so called normal
form 
definition    a finite  n  person normal form game is a tuple  n  a  u   where
 n is a set of n players 
 a   a   a          an   where ai is a set of actions available to player i  each vector
a    a    a            an    a is called an action profile 
 u    u    u            un    where ui   a  r is a utility function for player i that assigns
each action vector a certain utility  payoff  
when playing a game  players have to select their strategy  a pure strategy i for
player i selects only one action ai  ai   a vector of pure strategies for each player   
                 n   is called a pure strategy profile  a mixed strategy selects a probability
distribution over the entire action space  i e  i   ai    a mixed strategy profile is a
vector of mixed strategies for each player 
definition    we say that a mixed strategy i of player i is a best response to the strategy
profile of the opponents i if for any strategy i   
ui  i   i    ui  i    i  
one of the basic goals of game theory is to predict an outcome of a strategic interaction 
such outcome should be stable  therefore  it is usually called an equilibrium  one requirement for an outcome to be an equilibrium is that none of the players has an incentive to
change their strategy  i e  all players play their best response to the strategies of the others 
this defines perhaps the most important equilibrium concept  the nash equilibrium 
definition    a strategy profile                     n   is a nash equilibrium  ne  if for
every player i  her strategy i is a best response to the strategies of the others i  
as essential as nash equilibria are  they have several disadvantages  first  they may
be hard to find  chen and deng        show that finding ne is ppad complete  second 
there might be multiple nash equilibria  as shown in the example in section    third  the
most efficient ne may not be the most fair one  even in a symmetric game  we give the
formal definition of the correlated equilibrium which fixes some of these issues 
   

ficigler   faltings

definition    given an n  player game  n  a  u   a correlated equilibrium is a tuple  v      
where v is a tuple of random variables v    v    v            vn   whose domains are d  
 d    d            dn     is a joint probability distribution over v                      n   is a
vector of mappings i   di   ai   and for each player i and every mapping  i   di   ai it
is the case that
x
x

 d ui     d        d             n  dn    
 d ui     d         d              n  dn    
dd

dd

    markov chains
the learning algorithm we propose and analyze in this paper can be described as a randomized algorithm  in a randomized algorithm  some of its steps depend on the value of a
random variable  one useful technique to analyze randomized algorithms is to describe its
execution as a markov chain 
a markov chain is a random process with the markov property  a random process is a
collection of random variables  usually it describes the evolution of some random value over
time  a process has a markov property if its state  or value  in the next time step depends
exclusively on its value in the previous step  and not on the values further in the past 
we can say that the process is memoryless  if we imagine the execution of a randomized
algorithm as a finite state automaton with non deterministic steps  it is easy to see how its
execution maps to a markov chain 
the formal definition of a markov chain is as follows 
definition     norris        let i be a countable set  each i  i is called a state and i
is called the state space  we say that p   i   i  i  is a measure on i if    i    for
all i  i  if in addition the total mass ii i equals    then we call  a distribution  we
work throughout with a probability space    f  p   recall that a random variable x with
values in i is a function x     i  suppose we set
i   pr x   i    pr        x     i    
then  defines a distribution  the distribution of x  we think of x as modelling a random
state that takes value i with probability i  
we say that a matrix p    pij   i  j  i  is stochastic if every row  pij   j  i  is a
distribution 
we say that  xt  t  is a markov chain with initial distribution  and a transition matrix
p if
   x  has distribution  
   for t     conditional on xt   i  xt   has distribution  pij   j  i  and is independent
of x    x            xt   
more explicitly  the conditions state that  for t    and i            it    i 
   pr x    i      i   
   pr xt     it    x    i            xt   it     pit it    
   

fidecentralized anti coordination through multi agent learning

theorem    let a be a set of states  the vector of hitting probabilities ha    ha
i   i 
               n    is the minimal non negative solution to the system of linear equations

 
for i  a
a
hi   p
a for i 
p
h
 a
j         n   ij j
intuitively  the hitting probability ha
i is the probability that when the markov chain
starts in state i  it will ever reach some of the states in a 
one property of randomized algorithms that we are particularly interested in is its
convergence  if we have a set of states a where the algorithm has converged  we can define
the time it takes to reach any state in the set a from any other state of the corresponding
markov chain as the hitting time 
definition     norris        let  xt  t  be a markov chain with state space i  the hitting
time of a subset a  i is a random variable h a                       given by
h a      inf t      xt     a 
specifically  we are interested in the expected hitting time of a set of states a  given that
the markov chain starts in an initial state x    i  we will denote this quantity
kia   ei  h a   
in general  the expected hitting time of a set of states a can be found by solving a
system of linear equations 
theorem    the vector of expected hitting times k a   e h a      kia   i  i  is the
minimal non negative solution to the system of linear equations
 a
ki     p
for i  a
   
a for i 
kia       j a
p
k
 a
ij
j
 
convergence to an absorbing state may not be guaranteed for a general markov chain 
to calculate the probability of reaching an absorbing state  we can use the following theorem
 norris        
theorem    let a be a set of states  the vector of hitting probabilities ha    ha
i   i 
               n    is the minimal non negative solution to the system of linear equations

 
for i  a
a
hi   p
a for i 
p
h
 a
ij
j
j         n  
solving the systems of linear equations in theorems   and   analytically might be
difficult for many markov chains though  fortunately  when the markov chain has only
one absorbing state i      and it can only move from state i to j if i  j  we can use
the following theorem to derive an upper bound on the expected hitting time  proved by
rego        
theorem    let a        if
i      e xt    xt   i   
for some       then


kia   log i  

   


 

i


ficigler   faltings

   learning algorithm
in this section  we describe the algorithm that the agents use to learn a correlated equilibrium of the channel allocation game 
let us denote the space of available correlation signals k                   k      and the
space of available channels c                   c   assume that c  n   that is there are more
agents than channels  the opposite case is easier   an agent i has a strategy fi   k     c
that she uses to decide which channel she will access at time t when she receives a correlation
signal kt   when fi  kt        the agent does not transmit at all for signal kt   the agent stores
its strategy simply as a table 
each agent adapts her strategy as follows 
   in the beginning  for each k   k  fi  k    is initialized uniformly at random from c 
that is  every agent picks a random channel to transmit on  and no agent will monitor
other channels 
   at time t 
 if fi  kt        the agent tries to transmit on channel fi  kt   
 if otherwise fi  kt        the agent chooses a random channel mi  t   c that she
will monitor for activity 
   subsequently  the agent observes the outcome of its choice  if the agent transmitted
on some channel  she observes whether the transmission was successful  if it was 
the agent will keep her strategy unchanged  if a collision occurred  the agent sets
fi  kt        with probability p  with probability    p  the strategy remains the same 
   if the agent did not transmit  she observes whether the channel mi  t  she monitored
was free  if that channel was free  the agent sets fi  kt      mi  t  with probability   
if the channel was not free  the strategy fi remains the same 

   convergence
an important property of the learning algorithm is if  and how fast it can converge to a
pure strategy nash equilibrium of the channel allocation game for every signal value  the
algorithm is randomized  therefore  instead of analyzing its worst case behavior  that may
be arbitrarily bad   we will analyze its expected number of steps before convergence 
    convergence for c      k    
for single channel and single coordination signal  we prove the following theorem 
theorem    for n agents and c      k          p      the expected number of steps
before the allocation algorithm
converges
to a pure strategy nash equilibrium of the channel


 
allocation game is o p  p  log n  
to prove the convergence of the algorithm  it is useful to describe its execution as a
markov chain 
   

fidecentralized anti coordination through multi agent learning

when n agents compete for a single signal value  a state of the markov chain is a
vector from       n that denotes which agents are attempting to transmit  for the purpose
of the convergence proof  it is only important how many agents are trying to transmit  not
which agents  this is because the probability with which the agents back off is the same
for everyone  therefore  we can describe the algorithm execution using the following chain 
definition    a markov chain describing the execution of the allocation algorithm for
c      k          p     is a chain whose state at time t is xt                 n    where
xt   j means that j agents are trying to transmit at time t 
the transition probabilities of this chain look as follows 
pr  xt     n  xt         
pr  xt       xt         
 
i ij
pr  xt     j xt   i   
p     p j
j

 restart 
 absorbing 
i      j  i

all the other transition probabilities are    this is because when there are some agents
transmitting on some channel  no other agent will attempt to access it 
the probability pr  xt     n  xt      is equal to   because once the channel becomes
free  xt       agents will spot this and time t      everyone will transmit  therefore the
chain will be in state xt     n    the probability pr  xt       xt      is equal to one
because once a single agent successfully transmits on the channel  she will keep transmitting
forever after  and no other agent will attempt to transmit there  finally  the probability
pr  xt     j xt   i  expresses the fact that when xt   i  i agents transmit at time t   the
probability that an agent who transmitted at time t will keep transmitting at time t    
with probability    p 
we are interested in the number of steps it will take this markov chain to first arrive
at state xt     given that it started in state x    n   this would mean that the agents
converged to a setting where only one of them is transmitting  and the others are not 
definition   defined the hitting time which describes this quantity 
we will show the expected value e h    of the hitting time of state xt      and by
corollary  prove theorem    in the following steps 
   we show the expected hitting time for a set of states a           lemma   
   we then show probability that the markov chain enters state   before entering state
   when it starts from state i      lemma   
   finally  using the law of iterated expectations  we combine the two lemmas to show
the expected hitting time of state   
lemma    let a           the expected
hitting

 time of the set of states a in the markov
 
chain described in definition   is o p log n  
proof  we will first prove that
hitting time of a set a        in a slightly
 the expected

modified markov chain is o p  log n  
   

ficigler   faltings

let us define a new markov chain  yt  t  with the following transition probabilities 
pr  yt       yt         
 
i ij
pr  yt     j yt   i   
p     p j
j

 absorbing 
j     i   

note that the transition probabilities are the same as in the chain  xt  t    except for
states   and    from state   there is a positive probability of going into state    and state  
is now absorbing  clearly  the expected hitting time of the set a        in the new chain
is an upper bound on the expected hitting time of set a          in the old chain  this is
because any path that leads into state   in the new chain either does not go through state  
 so it happened with the same probability in the old chain   or goes through state    so in
the old chain it would stop in state    but it would be one step shorter  
if the chain is in state yt   i  the next state yt   is drawn from a binomial distribution
with parameters  i     p   the expected next state is therefore
e yt    yt   i    i    p 
 
we can therefore use the theorem   with      p
to derive that for a         the
hitting time is 


l
m  
 
a 
ki   log   i    o
log i
 p
p
p

that is also an upper bound on kia for a          in the old chain 
lemma    the probability hi that the markov chain defined in definition   enters state  
before entering state    when started in any state i      is greater than    p 
proof  calculating the probability that the chain x enters state   before state   is equal
to calculating the hitting probability  i e  the probability that the chain ever enters a
given state  for a modified markov chain where the probability of staying in state   is
pr  xt       xt           for a set of states a  let us denote ha
i the probability that the
markov chain starting in state i ever enters some state in a  to calculate this probability 
we can use theorem    for the modified markov chain that cannot leave neither state   nor
state    computing ha
i for a     is easy  since the matrix of the system of linear equations
is lower triangular 
well show that hi  q      p for i     using induction  the first step is calculating hi
for i            
h     
h     
h        p   h     p    p h    p  h 
 p    p 
     p 
 
 
    p 
       p  
 p
now  in the induction step  derive a bound on hi by assuming hj  q      p for all
j   i  j    
   

fidecentralized anti coordination through multi agent learning

i  
x
i ij
hi  
p     p j hj
j
j  



i  
x
i ij
p     p j q  ipi      p  q  h     pi h 
j
j  

  q  ipi      p  q      q      p 
this means that no matter which state i    the markov chain starts in  it will enter
into state   earlier than into state   with probability at least    p 
we can now finish the proof of the bound on the expected hitting time of state    we
will use the law of iterated expectations 
theorem     billingsley        let x be a random variable satisfying e  x      and y
another random variable on the same probability space  then
e x    e  e x y     
i e   the expected value of x is equal to the conditional expected value of x given y  
let h  be the random variable corresponding to the hitting time of state    define a
random variable z denoting the number of passes through state   our markov chain makes
before it reaches state    from theorem    we get 
e h      e e h   z   
in lemma    we have shown the expected number of steps ha before the markov chain
reaches the set of states a           we can write
e e h   z     e

  z
x

 
ha   e z  ha     ha  e z  

i  

from lemma   we know that the probability that the chain passes through state  
before passing through   is greater than    p  therefore  we can say that e z   e z    
where z   is a random variable distributed according to a geometric distribution with success
probability    p  then


 
ha
 
 o
log n  
e h      ha  e z   ha  e z    
 p
p    p 
this concludes the proof of theorem   
we have just shown that the expected time for convergence of our algorithm is finite 
and polynomial  what is the probability that the algorithm converge in a finite number of
steps to an absorbing state  the following theorem shows that since the expected hitting
time of the absorbing state is finite  this probability is   
   

ficigler   faltings

theorem    let h  be the hitting time of the state xt     in the markov chain from
definition    then
pr h  is finite      
proof  from the markov inequality  we know that since h     
pr h     

e h   
 


therefore 
pr h  is finite       lim pr h         lim




e h   
    


this means that our algorithm converges almost surely to a nash equilibrium of the
channel allocation game 
    convergence for c     k    
theorem     for n agents and c     k      the expected number of steps before the
learning algorithm

h converges toia pure strategy nash equilibrium of the channel allocation
 
game is o c  p p  log n   c  
proof  in the beginning  in at least one channel 
 there can be at most n agents who want
 
to transmit  it will take on average o p log n steps to get to a state when either   or  
agents transmit  lemma     we will call this period a round 
if all the agents backed off  it will take them on average at most c steps before some of
them find an empty channel  we call this period a break 
the channels might oscillate between the round and break periods in parallel  but
in the worst case  the whole system will oscillate
these two periods 
 between

 
for a single channel  it takes on average o  p oscillations between these two periods
before there
 is only
 one agent who transmits in that channel  for c     it takes on
 
average o c  p steps between round and break before all channels have only one

h
i
 
 
agent transmitting  therefore  it will take on average o c  p
log
n
 
c
steps before
p
the system converges 
    convergence for c     k   
to show what is the convergence time when k      we will use a more general problem 
imagine that there are k identical instances of the same markov chain  we know that the
original markov chain converges from any initial state to an absorbing state in expected
time t   now imagine a more complex markov chain  in every step  it selects uniformly at
random one of the k instances of the original markov chain  and executes one step of that
instance  what is the time tall before all k instances converge to their absorbing states 
this is an extension of the well known coupon collectors problem  feller         the
following theorem  gast        thm     shows an upper bound on the expected number of
steps after which all the k instances of the original markov chain converge 
   

fidecentralized anti coordination through multi agent learning

theorem      gast        let there be k instances of the same markov chain that is
known to converge to an absorbing state in expectation in t steps  if we select randomly
one markov chain instance at a time and allow it to perform one step of the chain  it will
take on average e tall    t k log k    t k     steps before all k instances converge to their
absorbing states 
for arbitrary c     k     the following theorem follows from theorems    and    
theorem     for n agents and c     k         p          q      the expected number
of steps before the learning algorithm converges to a pure strategy nash equilibrium of the
channel allocation game for every k  k is




 
 
o  k log k    k c
c   log n      
 p
p
aumann        shows that any nash equilibrium is a correlated equilibrium  and any
convex combination of correlated equilibria is a correlated equilibrium  we also know that
all the pure strategy nash equilibria that the algorithm converges to are efficient  there are
no collisions  and in every channel for every signal value  some agent transmits  therefore 
we conclude the following 
theorem     the learning algorithm defined in section   converges in expected polynomial
 
time  with respect to k  c  p     p
and log n   to an efficient correlated equilibrium of the
channel allocation game 

   fairness
agents decide their strategy independently for each value of the coordination signal  therefore  every agent has an equal chance that the game converges to an equilibrium that is
favorable to her  if the agent can transmit in the resulting equilibrium for a given signal
value  we say that the agent wins the slot  for c available channels and n agents  an agent
c
wins a given slot with probability n
 since no agent can transmit in two channels at the
same time  
we analyse the fairness of our algorithm after it has converged to a correlated equilibrium  while algorithms which do not converge to an absorbing state  such as aloha  
need to be analysed for all the intermediate states of their execution  we believe that our approach is justified by the fact that our algorithm converges relatively quickly  in polynomial
time 
we can describe the number of signals for which an agent i wins some channel as a
random variable xi   this variable is distributed according to a binomial distribution with
c
parameters k  n
 
as a measure of fairness  we use the jain index  jain  chiu    hawe         the
advantage of jain index is that it is continuous  so that a resource allocation that is strictly
more fair has higher jain index  unlike measures which only assign binary values  such as
whether at least half of the agents access some resource   also  jain index is independent of
the population size  unlike measures such as the standard deviation of the agent allocation 
   

ficigler   faltings

for a random variable x  the jain index is the following 
j x   

 e x   
e x    

c
   its
when x is distributed according to a binomial distribution with parameters  k  n
first and second moments are

c
e x    k 
n


  
c  
c n c
e x   k
 k 

 
n
n
n
so the jain index is
j x   

c k
 
c  k    n  c 

   

for the jain index it holds that     j x      an allocation is considered fair if
j x      

n
theorem     for any c  if k    n
c   that is the limit limn  ck      then
lim j x      

n 

so the allocation becomes fair as n goes to  
proof  the theorem follows from the fact that
lim j x    lim

n 

n 

c k
c  k    n  c 

for this limit to be equal to    we need
n c
  
n  c  k
lim

that holds exactly when k   
that we assume that c  n   

n
c



 that is k grows asymptotically faster than

n
c 

note

for practical purposes  we may also need to know how big shall we choose k given c
and n   the following theorem shows that 
theorem     let       if
 
k 





n
   
c

then j x        
proof  the theorem follows straightforwardly from equation   
   

fidecentralized anti coordination through multi agent learning

 

convergence steps

  

 

  

 

  

 

  

  

  

  

  

  

  

c

figure    average number of steps to convergence for n       k   n and c 
               n   

   experimental results
in all our experiments  we report average values over     runs of the same experiment 
error bars in the graphs denote the interval which contains the true expected value with
probability      provided that the samples follow normal distribution  the error bars
are missing either when the graph reports values obtained theoretically  jain index for the
constant back off scheme  or the confidence interval was too small for the scale of the graph 
    static player population
we will first analyze the case when the population of the players remains the same all the
time 
      convergence
first  we are interested in the convergence of our allocation algorithm  from section  
we know that it is polynomial  how many steps does the algorithm need to converge in
practice 
figure   presents the average number of convergence steps for n       k   n and
increasing number of available channels c                 n    interestingly  the convergence
takes the longest time when c   n   the lowest convergence time is for c   n    and for
c     it increases again 
what happens when we change the size of the signal space k  figure   shows the
average number of steps to convergence for fixed n   c and varying k  theoretically  we
   

ficigler   faltings

    

convergence steps

    
    
   
   
   
   
 
 

  

  

  

  

  

  

  

k

 

 

   

   

   

   

   

   

jain index

jain index

figure    average number of steps to convergence for n       c  

   
   
k n
k   nlog n

   

   
 

  

  

  

  

   

   

   
k  
k    log n
 

   

k   n 

   
 

   

n

and k              n   

   

   

 

   

n
 

k    n
  

  

  

  

   

   

   

n

 a  c    

 b  c  

n
 

figure    jain fairness index for different settings of c and k  for increasing n  

have shown that the number convergence steps is o k log k  in theorem     however  in
practice the convergence resembles linear dependency on k  this is because the algorithm
needs to converge for all the coordination signals 
      fairness

from section    we know that when k    n
c   the jain fairness index converges to   as
n goes to infinity  but how fast is this convergence  how big do we need to choose k 
depending on n and c  to achieve a reasonable bound on fairness 
   

fidecentralized anti coordination through multi agent learning

figure   shows the jain index as n increases  for c     and c   n  respectively  for
various settings of k  even though every time when k    n
c  that is  k grows faster
n
than c   the jain index increases  as shown in theorem      there is a marked difference
between the various settings of k  when k   n
c   the jain index is  from equation    
j x   

n
 
 n  c

   

therefore  for c      the jain index converges to      and for c  
equal to    for all n      just as figure   shows 

n
  

the jain index is

      optimizing fairness
we saw how fair the outcome of the allocation algorithm is when agents consider the game
for each signal value independently  however  is it the best we can do  can we further
improve the fairness  when each agent correlates her decisions for different signal values 
in a perfectly fair solution  every agent wins  and consequently can transmit  for the
same number of signal values  however  we assume that agents do not know how many
other agents there are in the system  therefore  the agents do not know what is their fair
share of signal values to transmit for  nevertheless  they can still use the information in
how many slots they already transmitted to decide whether they should back off and stop
transmitting when a collision occurs 
definition    for a strategy fit of an agent i in round t  we define its cardinality as the
number of signals for which this strategy tells the agent to access 
fi
 fi
 fit     fi k  k fit  k      fi
intuitively  agents whose strategies have higher cardinality should back off more often
than those with a strategy with low cardinality 
we compare the following variations of the channel allocation scheme  that differ from
the original one only in the probability with which agents back off on collisions 
constant the scheme described in section    every agent backs off with the same constant
probability p 
linear the back off probability is p  

 fit  
k  

exponential the back off probability is p   



 f t  
  ki

for some parameter          

worst agent last in case of a collision  the agent who has the lowest  fit   does not back
off  the others who collided  do back off  this is a greedy algorithm that requires
more information than what we assume that the agents have 
to compare the fairness of the allocations in experiments  we need to define the jain
index of an actual allocation  a resource allocation is a vector x    x    x            xn    where
xi is the cardinality of the strategy used by agent i  for an allocation x  its jain index is 
p
 
n
i   xi
j x   
p
 
n n
i   xi
   

ficigler   faltings

c   n    k    log n
 
    

jain index

    
    
    
   
constant
linear
exponential
worstagent

    
    
 

  

  

  

  

   

   

   

n

figure    jain fairness index of the channel allocation scheme for various back off probabilities  c   n    k     log  n

figure   shows the average jain fairness index of an allocation for the back off probability
variations  the fairness is approaching   for the worst agent last algorithm  it is the
worst if everyone is using the same back off probability  as the ratio between the back off
probability of the lowest cardinality agent and the highest cardinality agent decreases  the
fairness increases 
this shows that we can improve fairness by using different back off probabilities  nevertheless  the shape of the fairness curve is the same for all of them  furthermore  the
exponential back off probabilities lead to much longer convergence  as shown on figure   
for c   n    the convergence time for the linear and constant back off schemes is similar 
the unrealistic worst agent last scheme is obviously the fastest  since it resolves collisions
in   step  unlike the other back off schemes 
    dynamic player population
now we will take a look at the performance of our algorithm when the population of players
is changing over time  either new players join or old players get replaced by new ones  
we will also analyze the case when there are errors in what the players observe  either
coordination signal or channel feedback is noisy 
      joining players
in this section  we will present the results of experiments where a group of players joins the
system later  this corresponds to new nodes joining a wireless network  more precisely 
   

fidecentralized anti coordination through multi agent learning

c   n    k    log n
constant
linear
exponential
worstagent
convergence steps

 

  

 

  

 

  

 

 

  

  
n

figure    convergence steps for various back off probabilities 
    of the players join the network from the beginning  the remaining     of the players
join the network later  one by one  a new player joins the network after the previous players
have converged to a perfect channel allocation 
we experiments with two ways of initializing a strategy of a new player 
greedy either  the joining players cannot observe how many other players there are already in the system  therefore  their initial strategy tries to transmit in all possible
slots 
polite or  players do observe n  t   the number of other players who are already in the
system at time t  when the new player joins the system  therefore  their initial
strategy tries to transmit in a slot only with probability n  t   
figure   shows the jain index of the final allocation when     of the players join later 
for c      when the players who join are greedy  they are very aggressive  they start
transmitting in all slots  on the other hand  if they are polite  they are not aggressive
enough  a new player starts with a strategy that is as aggressive as the strategies of the
players who are already in the system  the difference is that the new player will experience
a collision in every slot she transmits in  the old players will only experience a collision in
 
n  t  of their slots  therefore  they will back off in less slots 
therefore  especially for the constant scheme  the resulting allocation is very unfair 
either it is better for the new players  when they are greedy  or to the older players  when
the players are polite  
this phenomenon is illustrated in figure    it compares a measure called group fairness 
the average throughput of the last     of players who joined the network at the end  new
   

ficigler   faltings

c      k   nlog n  join delay   converge init population         kps
 

 

   

   

   

   

   

   

   

   

jain index

jain index

c      k   nlog n  join delay   converge init population       
 

   
   
   

   
   
   

   

   

constant
linear
worstplayer

   
 
 

  

  

  

  

  

  

  

  

constant
linear
worstplayer

   
 
 

  

  

  

  

  

n

  

  

  

  

  

n

 a  greedy

 b  polite

figure    joining players  jain index  c     and k   n log  n   the two graphs show the
results for the two ways of initializing the strategy of a new player 

c      k   nlog n  join delay   converge init population       

c      k   nlog n  join delay   converge init population         kps

 

   

constant
linear
worstplayer

   

constant
linear
worstplayer

   
   

   
group fairness

group fairness

 

 
   
 

   
   
   

   
   

 
   
 

  

  

  

  

  

n

   
 

  

  

  

  

  

n

 a  greedy

 b  polite

figure    joining players  group fairness  c     and k   n log  n   the two graphs show
the results for the two ways of initializing the strategy of a new player 

   

fidecentralized anti coordination through multi agent learning

c   n    k    log n  join delay   converge init population       

c   n    k    log n  join delay   converge init population         kps

 

 

   

   

   

   
   

   

jain index

jain index

   

   
   
   

   
   
   

   

   

constant
linear
worstplayer

   
 
 

   

  

  

  

  

  

  

  

  

constant
linear
worstplayer

   

  

 
 

  

  

n

  

  

  

  

  

  

  

n

 a  greedy

 b  polite

figure    joining players  jain index  c   n  and k     log  n   the two graphs show the
results for the two ways of initializing the strategy of a new player 

players  divided by the average throughput of the first     of players who join the network
at the beginning  old players  
lets look first at the case when the players are greedy  for the constant scheme  this
ratio is around      for the linear scheme  this ratio is lower  although increasing as n  the
total number of players  grows  for the worst player last scheme  the ratio stays constant
and interestingly  it is lower than    which means that old players are better off than
new players 
when players are polite  this situation is opposite  old players are way better off than
new players  for the constant scheme  the throughput ratio is about     
figures   and   show the same graphs for c   n    here  the newly joining players
are worse off even when they start transmitting in every slot  this is because while they
experience a collision every time  because all channels in all slots are occupied   the old
players only experience a collision with a probability n    on the other hand  the overall
 

fairness of the whole population is better  because there are more channels to share and no
agent can use more than one channel 
the difference between the old and new players is even more pronounced when the new
players are polite 
      restarting players
another scenario we looked at was what happens when one of the old players switches off
and is replaced with a new player with a randomly initialized strategy  we say that such a
player got restarted  in a wireless network  this corresponds to a situation when a user
restarts their router  note that the number of players in the network stays the same  it is
just that some of the players forget what they have learned and start from scratch 
specifically  in every round  for every player there is a probability pr that she will be
restarted  after restart  she will start with a strategy that can be initialized in two ways 
   

ficigler   faltings

c   n    k    log n  join delay   converge init population       

c   n    k    log n  join delay   converge init population         kps

 

    

 

constant
linear
worstplayer

   

constant
linear
worstplayer

   

    
group fairness

group fairness

   

   
    
   

   
   

    

   

   
    
 

   

  

  

  

  

  

n

 

  

  

  

  

  

n

 a  greedy

 b  polite

figure    joining players  group fairness  c   n  and k     log  n   the two graphs show
the results for the two ways of initializing the strategy of a new player 

greedy assume that the player does not know n   the number of players in the system 
then for each signal value k  k she chooses randomly fi  k   c  that means that
she attempts to transmit in every slot on a randomly chosen channel 
polite assume the player does know n   for k  k  she chooses fi  k   c with probability
c
n   and fi  k       otherwise 
figure    shows the average overall throughput when n       c      and k   n log  n
or k   n for the two initialization schemes  a dotted line in all the four graphs shows the
overall performance when players attempt to transmit in a randomly chosen channel with
c
probability n
  this baseline solution reaches  e      average throughput 
as the probability of restart increases  the average throughput decreases  when players
get restarted and they are greedy  they attempt to transmit in every slot  if there is only
one channel available  this means that such a restarted player causes a collision in every slot 
therefore  it is not surprising that when the restart probability pr       and n       the
throughput is virtually    in every step  in expectation at least one player will get restarted 
so there will be a collision almost always 
there is an interesting phase transition that occurs when pr      for k   n log  n  
and when pr      for k   n   there  the performance is about the same as in the
baseline random access scenario  that requires the players to know n though   similar
phase transition occurs when players are polite  even though the resulting throughput is
higher  since the restarted players are less aggressive 
yet another interesting  but not at all surprising  phenomenon is that while the worstplayer last scheme still achieves the highest throughput  the constant back off scheme is
better than the linear back off scheme  this is because for the average overall throughput 
it only matters how fast are the players able to reach a perfect allocation after a disruption 
the worst player last scheme is the fastest  since it resolves a collision in   step  the con   

fidecentralized anti coordination through multi agent learning

n       c      k   nlog n  kps
 

   

   

   

   

   

   

system throughput

system throughput

n       c      k   nlog n
 

   
   
   
   
   
   
 

 

   
   
   
   

constant
linear
worstplayer

   
 

  

   

  
restart probability

 

 

  

constant
linear
worstplayer
 

 a  greedy  k   n log  n

 

  

n       c      k   n  kps
 

   

   

   

   

   

   

system throughput

system throughput

n       c      k   n

   
   
   
   
   

 

  
restart probability

 b  polite  k   n log  n

 

   

 

  

   
   
   
   
   

constant
linear
worstplayer
 

  

   
 

  
restart probability

 

 

  

constant
linear
worstplayer
 

  

 c  greedy  k   n

 

  
restart probability

 d  polite  k   n

figure     restarting players  throughput  n       c    

   

 

  

ficigler   faltings

n       c   n    k   nlog n  kps
 

   

   

   

   

   

   

system throughput

system throughput

n       c   n    k   nlog n
 

   
   
   
   
   
   
 

 

   
   
   
   

constant
linear
worstplayer

   
 

  

   

  
restart probability

 

 

  

constant
linear
worstplayer
 

 a  greedy  k     log  n

  

n       c   n    k      kps
 

   

   

   

   

   

   

system throughput

system throughput

n       c   n    k    

   
   
   
   
   

 

 

  
restart probability

 b  polite  k     log  n

 

   

 

  

   
   
   
   
   

constant
linear
worstplayer
 

  

   
 

  
restart probability

 

 

  

constant
linear
worstplayer
 

  

 c  greedy  k    

 

 

  
restart probability

  

 d  polite  k    

figure     restarting players  throughput  n       c  

n
 

stant scheme with back off probability p      is worse  see theorem      the linear scheme
is the slowest 
figure    shows the average overall throughput for c   n    and k   log  n or k     
there is no substantial difference between when players are greedy or polite  since there are
so many channels available  a restarted player will only cause a small number of collisions
 in one channel out of n  in every slot   so the throughput will not decrease too much 
also  the convergence time for linear and constant scheme is about the same when
c   n    so they both adapt to the disruption equally well 
      noisy feedback
so far we assumed that players receive perfect feedback about whether their transmissions
were successful or not  they could also observe the activity on a given channel perfectly 
we are going to loosen this assumption now 
   

fidecentralized anti coordination through multi agent learning

n       c   n    k   nlog n
 

   

   

   

   

   

   

system throughput

system throughput

n       c      k   nlog n
 

   
   
   
   
   
   
 

 

   
   
   
   

constant
linear
worstplayer
  

   

   
 

  
noisy feedback probability

 

 

  

constant
linear
worstplayer
 

  

 a  c      k   n log  n

 

  
noisy feedback probability

 b  c  

n
 

 

  

  k     log  n

figure     noisy feedback  throughput  n     

n       c   n    k   nlog n
 

    

    

    

    

    

    

    

    

jain index

jain index

n       c      k   nlog n
 

   
    
    

    
    

    

    
constant
linear
worstplayer

    
   

   

 

  

 

  
noisy feedback probability

   

 

  

constant
linear
worstplayer

    
 

  

 a  c      k   n log  n

 

  
noisy feedback probability

 b  c  

n
 

 

  

  k     log  n

figure     noisy feedback  jain index  n     
suppose that in every step  every player has a probability pf that the feedback she
receives was wrong  that is  if the player transmitted  she will learn that the transmission
was successful when it was not  and vice versa  if the player observed some channel  she
will learn that the channel was free when in fact it was not  and vice versa   in the context
of wireless networks  this corresponds to an interference on the wireless channel 
how does this affect the learning 
in figure    we show the average overall throughput when c     and c   n  respectively  for one channel  the constant scheme is better than the linear scheme  because it
adapts faster to disruptions  for c   n    both schemes are equivalent  because they are
equally fast to adapt  a phase transition occurs when the noisy feedback probability is
about pf        
   

ficigler   faltings

n       c   n    k   nlog n

 

   

   

   

   

   

   

system throughput

system throughput

n       c      k   nlog n

 

   
   
   
   
   
   
 

   
   
   
   
   

constant
linear
worstplayer
 

  

   
 

  
noisy signal probability

 

 

  

constant
linear
worstplayer
 

  

 a  c      k   n log  n

 

  
noisy signal probability

 b  c  

n
 

 

  

  k     log  n

figure     noisy coordination signal  throughput  n     
figure    shows the jain index of the allocation when players receive noisy feedback  as
usual  the linear scheme is better than the constant  even though its throuput is lower  as
we have shown above   only when the overall throughput drops close to    all the schemes
obviously have almost the same fairness 
      noisy coordination signal
our algorithm assumes that all players can observe the same coordination signal in every
step  but where does this signal come from  it may be some random noise on a given
frequency  an fm radio transmission etc  however  the coordination signal might be noisy 
and different players can observe a different value  this means that their learning would be
out of sync  in the wireless networks  this corresponds to clock drift 
to see what happens in such a case  we use the following experiment  in every step 
every player observes the correct signal  i e  the one that is observed by everyone else  with
probability    ps   with probability ps she observes some other false signal  that is still
taken uniformly at random from the set          k      
the overall throughput is shown in figure     we can see that the system is able to
cope with a fairly high level of noise in the signal  and the drop in throughput only occurs
as ps         as was the case for experiments with noisy feedback  the constant back off
scheme is able to achieve a higher throughput thanks to its faster convergence 
the jain index of the allocation  figure     stays almost constant  only when the
throughput drops the jain index increases  when the allocation is more random  it is also
more fair 
    generic multi agent learning algorithms
several algorithms that are proved to converge to a correlated equilibrium have been proposed in the multi agent learning literature  in the introduction  we have mentioned three
such learning algorithms  foster   vohra        hart   mas colell        blum   man   

fidecentralized anti coordination through multi agent learning

n       c   n    k   nlog n
 

    

    

    

    

    

    

    

    

jain index

jain index

n       c      k   nlog n
 

   
    
    

    
    

    

    
constant
linear
worstplayer

    
   

   

 

  

 

  
noisy signal probability

   

 

  

constant
linear
worstplayer

    
 

  

 a  c      k   n log  n

 

  
noisy signal probability

 b  c  

n
 

 

  

  k     log  n

figure     noisy coordination signal  jain index  n     
sour         however  the analysis of foster and vohra was only applicable to games of two
players  in this section  we will briefly recall the other two multi agent learning algorithms
 hart   mas colell        blum   mansour         and compare their performance with
our algorithm presented in section   
the two algorithms we will compare our algorithm to are based on the notion of minimizing regret the agents experience from adopting a certain strategy  intuitively  we can
describe the concept of regret as follows  imagine that an agent uses strategy  in a couple
of rounds of the game  and accumulates a certain payoff  we would like to know how does
this payoff compare to a payoff acquired by some simple alternative strategy    the difference in the payoff between the strategy  and  is the regret the agent perceives  ex post 
for choosing strategy  over strategy   
what do we mean by simple strategy  one class of simple strategies are strategies
that always select the same action  the external regret compares the performance of the
strategy  to the performance of the best single action ex post 
another class of alternative strategies are strategies that modify strategy  slightly 
every time the strategy  proposes to play action a  the alternative strategy  proposes
action a     a instead  the internal regret is defined as the regret of strategy  compared to
the best such alternative strategy  when all the agents adopt a strategy with low internal
regret  they converge to a strategy profile that is close to a correlated equilibrium  also
shown in blum   mansour        
hart and mas colell        present a simple multi agent learning algorithm that is
guaranteed to converge to a correlated equilibrium  they assume that the players can
observe the actions of all their opponents in every round of the game  players start by
choosing their actions randomly  then they update their strategy as follows  let ai be the
action that player i played in round t   for each action aj  ai   aj    ai   player i calculates
the difference between the average payoff she would have received had she played action aj
instead of ai in the past  and the average payoff she received so far while playing action ai  
as we mentioned above  we can call this difference the internal regret of playing action ai
   

ficigler   faltings

c   n  

c  
 

  

constant backoff
hartmascolell
blummansour

constant backoff
hartmascolell
blummansour
 

convergence steps

convergence steps

  
 

  

 

  

 

  

 

  

 

  

  
n

  

  

 a  c    

 

  

  
n

 b  c  

  

  

n
 

figure     general multi agent learning algorithms  convergence rate 

instead of action aj   the player then chooses the action to play in round t with probability
proportional to its internal regret compared to the previous action ai   actions with negative
regret are never played  the previous action ai is played with positive probability  this
way  the strategy has a certain inertia 
hart and mas colell        prove that if the agents adopt the adaptive procedure described above  the empirical distribution of the play  the relative frequency of playing a
certain pure strategy profile  converges almost surely to the set of correlated equilibria 
blum and mansour        present a general technique to convert any learning algorithm
with low external regret to an algorithm with a low internal regret  the idea is to run
multiple copies of the external regret algorithm  in each step  each copy returns a probability
vector of playing each action  these probability vectors are then combined into one joint
probability vector  when the player observes the payoff of playing each action  she updates
the payoff beliefs of each external regret algorithms proportionally to the weight they had in
the joint probability vector  the authors then show that when the players all use a learning
algorithm with low internal regret  the empirical distribution of the game converges close
to a correlated equilibrium 
one of the low external regret algorithms that blum and mansour        present is
the polynomial weights  pw  algorithm  there  a player keeps a weight for each of her
actions  in every round of the game  she updates the weight proportionally to the loss
 negative payoff  that action incurred in that round  actions with higher weight get then
chosen with a higher probability 
we have implemented the two generic multi agent learning algorithms  the internalregret based algorithm of hart and mas colell         and the pw algorithm of blum and
mansour         in all our experiments  both algorithms always converge to a pure strategy
nash equilibrium of the channel allocation game  and therefore to an efficient allocation 
however  the resulting allocation is not fair  as only a subset of agents of size c can ever
access the channels 
   

fidecentralized anti coordination through multi agent learning

c   n  
 

   

   

   

   

   

   

   

   

jain index

jain index

c  
 

   
   
   

   
   

   

   
constant backoff
hartmascolell
blummansour

   
 
 

   

  

  
n

  

constant backoff
hartmascolell
blummansour

   

  

 a  c    

 
 

  

  
n

 b  c  

  

  

n
 

figure     general multi agent learning algorithms  jain index 

figure    shows the average number of rounds the algorithms take to converge to a
stable outcome  we compare their performance with our learning algorithm from section   
for our learning algorithm  we set k      so that it also only converges to a pure strategy
nash equilibrium of the game  we performed     runs of each algorithm for each scenario 
the error bars in figure    show the     confidence interval of the average  assuming that
the convergence times are distributed according to a normal distribution 
not surprisingly  the generic algorithms of hart and mas colell        and blum and
mansour        cannot match the convergence speed of our algorithm  designed specifically
for the problem of channel allocation  as the generic algorithms converge to a pure strategy
ne  the outcome is very unfair  and the jain index is very low  as evidenced by figure    
we dont report the confidence bounds for the jain index  as in all of the experiments the
resulting jain index was the same 

   related work
broadly speaking  in this paper we are interested in games where the payoff an agent receives
from a certain action is inversely proportional to the number of other agents who chose the
same action  how can we achieve efficient and fair outcome in such games  variants of this
problem have been studied in several previous works 
the simplest such variant is the minority game  challet  marsili    zhang         in
this game  n agents have to simultaneously choose between two actions  agents who chose
an action that was chosen by a minority of agents receive a payoff of    whereas agents whose
action choice was in majority receive
 a payoff of    this game has many pure strategy nash
equilibria  where some group of n    agents chooses one action and the rest choose the
other action  such equilibria are efficient  since the largest possible number of agents achieve
the maximum payoff  however  they are not fair  the payoff to the losing group of agents
is always    this game has also one mixed strategy ne that is fair  every agent chooses its
   

ficigler   faltings

action randomly  this equilibrium 
 on the other hand  is not efficient  the expected size of
the minority group is lower than n    due to variance of the action selection 
savit  manuca  and riolo        show that if the agents receive feedback on which action
was in the minority  they can learn to coordinate better to achieve a more efficient outcome
in a repeated minority game  they do this by basing the agents decisions on the history
of past iterations  cavagna        shows that the same result can be achieved when agents
base their decisions on the value of some random coordination signal instead of using the
history  this is a direct inspiration for the idea of global coordination signal presented in
this paper 
the ideas from the literature on minority games have recently found their way into
the cognitive radio literature  mahonen and petrova        present a channel allocation
problem much like ours  the agents learn which channel they should use using a strategy
similar to the strategies for minority games  the difference is that instead of preferring the
action chosen by the minority  in the channel allocation problem  an agent prefers channels
which were not chosen by anyone else  using this approach  mahonen and petrova are able
to achieve a stable throughput of about     even when the number of agents who try to
transmit over a channel increases  however  each agent is essentially choosing one out of a
fixed set of strategies  that they cannot adapt  therefore  it is very difficult to achieve a
perfectly efficient channel allocation 
wang et al         have implemented the algorithm from this work in an actual wireless
network  in their setting  wireless devices are able to monitor the activity on all the channels 
as a coordination signal  they have used the actual data packets that the agents send  the
authors have shown that in practice  the learning algorithm  which they call attachment
learning  improves the throughput     over the random access slotted aloha protocol 
another  more general variant of our problem  called dispersion game was described by
grenager  powers  and shoham         in a dispersion game  agents can choose from several
actions  and they prefer the one that was chosen by the smallest number of agents  the
authors define a maximal dispersion outcome as an outcome where no agent can move to an
action with fewer agents  the set of maximal dispersion outcomes corresponds to the set of
pure strategy nash equilibria of the game  they propose various strategies to converge to
a maximal dispersion outcome  with different assumptions on the information available to
the agents  on the contrary with our work  the individual agents in the dispersion games
do not have any particular preference for the actions chosen or the equilibria which are
achieved  therefore  there are no issues with achieving a fair outcome 
verbeeck  nowe  parent  and tuyls        use reinforcement learning  namely linear
reward inaction automata  to learn nash equilibria in common and conflicting interest
games  for the class of conflicting interest games  to which our channel allocation game
belongs   they propose an algorithm that allows the agents to circulate between various
pure strategy nash equilibria  so that the outcome of the game is fair  in contrast with
our work  their solution requires more communication between agents  and it requires the
agents to know when the strategies converged  in addition  linear reward inaction automata
are not guaranteed to converge to a pure strategy ne in conflicting interest games  they
may only converge to pure strategies 
all the games discussed above  including the channel allocation game  form part of the
family of potential games introduced by monderer and shapley         a game is called a
   

fidecentralized anti coordination through multi agent learning

potential game if it admits a potential function  a potential function is defined for every
strategy profile  and quantifies the difference in payoffs when an agent unilaterally deviates
from a given strategy profile  there are different kinds of potential functions  exact  where
the difference in payoffs to the deviating agent corresponds directly to the difference in
potential function   ordinal  where just the sign of the potential difference is the same as
the sign of the payoff difference  etc 
potential games have several nice properties  the most important is that any purestrategy nash equilibrium is just a local maximum of the potential function  for finite
potential games  players can reach these equilibria by unilaterally playing the best response 
no matter what initial strategy profile they start from 
the existence of a natural learning algorithm to reach nash equilibria makes potential
games an interesting candidate for our future research  we would like to see to which kind of
correlated equilibria can the agents converge there  and if they can use a simple correlation
signal to coordinate 

   conclusions
in this paper  we proposed a new approach to reach efficient and fair solutions in multi agent
resource allocation problems  instead of using a centralized  smart coordination device to
compute the allocation  we use a stupid coordination signal  in general a random integer
k                 k      that has no a priori relation to the problem  agents then are smart 
they learn  for each value of the coordination signal  which action they should take 
from a game theoretic perspective  the ideal outcome of the game is a correlated equilibrium  our results show that using a global coordination signal  agents can learn to play
a convex combination of pure strategy nash equilibria  that is a correlated equilibrium 
we showed a learning strategy that  for a variant of a channel allocation game  converges in expected polynomial number of steps to an efficient correlated equilibrium  we
also proved that this equilibrium becomes increasingly fair as k  the number of available
synchronization signals  increases 
we confirmed both the fast convergence as well as increasing fairness with increasing k
experimentally  we also investigated the performance of our learning strategy in case the
agent population is dynamic  when new agents join the population  our learning strategy is
still able to learn an efficient allocation  however  the fairness of this allocation will depend
on how greedy the initial strategies of the new agents are  when agents restart at random
intervals  it becomes more important how fast a strategy converges  a simple strategy where
everyone backs off from transmitting with a constant probability is able to achieve higher
throughput than a more sophisticated strategy where the back off probability depends on in
how many slots an agent is already transmitting  we also showed experimentally that the
learning strategy is robust against noise in both the coordination signal  as well as in the
feedback the agents receive about channel use  in both of these noisy scenarios  faster convergence of the constant back off scheme helped achieve a higher throughput than the more
fair linear back off scheme  finally  we compared the performance of out learning strategy with the generic multi agent learning algorithms based on regret minimization  hart  
mas colell        blum   mansour         while these generic algorithms are theoretically
proven to converge to a distribution of play which is close to a correlated equilibrium  they
   

ficigler   faltings

are not guaranteed to converge to a specific ce  indeed  in our experiments  the algorithms
of hart and mas colell and blum and mansour always converged to the efficient but unfair
pure strategy nash equilibrium of the channel allocation game 
the learning algorithm presented in this paper has been implemented in a real wireless
network by wang et al          who have shown that it achieves     higher throughput
than random access protocols such as aloha 
in this paper  we did not address the issue of whether non cooperative but rational
agents would follow the protocol we outlined  in our other work  cigler   faltings         we
address this issue and show that under certain conditions  the protocol can be implemented
in nash equilibrium strategies of the infinitely repeated resource allocation game 

references
abramson  n          the aloha system  another alternative for computer communications  in proceedings of the november              fall joint computer conference 
afips     fall   pp          new york  ny  usa  acm 
aumann  r          subjectivity and correlation in randomized strategies  journal of
mathematical economics              
billingsley  p          probability and measure  wiley series in probability and statistics 
 anniversary edition edition   wiley 
blum  a     mansour  y          algorithmic game theory  in nisan  n   roughgarden 
t   tardos  e     vazirani  v   eds    algorithmic game theory  chap     cambridge
university press 
cavagna  a          irrelevance of memory in the minority game  physical review e         
r    r     
challet  d   marsili  m     zhang  y  c          minority games  interacting agents in
financial markets  oxford finance   oxford university press  new york  ny  usa 
chen  x     deng  x          settling the complexity of two player nash equilibrium  in
       th annual ieee symposium on foundations of computer science  focs    
pp          ieee 
cheng  s   raja  a   xie  l     howitt  i          a distributed constraint optimization algorithm for dynamic load balancing in wlans  in the ijcai    workshop on distributed
constraint reasoning  dcr  
cigler  l     faltings  b          symmetric subgame perfect equilibria for resource allocation  in  to appear  proceedings of the   th national conference on artificial
intelligence  aaai      menlo park  ca  usa  american association for artificial
intelligence 
feller  w          an introduction to probability theory and its applications  vol      rd
edition    edition   wiley 
foster  d  p     vohra  r  v          calibrated learning and correlated equilibrium  games
and economic behavior                 
   

fidecentralized anti coordination through multi agent learning

gast  n          computing hitting times via fluid approximation  application to the coupon
collector problem  arxiv e prints 
grenager  t   powers  r     shoham  y          dispersion games  general definitions and
some specific learning results  in proceedings of the eighteenth national conference
on artificial intelligence  aaai      pp          menlo park  ca  usa  american
association for artificial intelligence 
hart  s     mas colell  a          a simple adaptive procedure leading to correlated equilibrium  econometrica                   
jain  r  k   chiu  d  m  w     hawe  w  r          a quantitative measure of fairness and
discrimination for resource allocation in shared computer systems  tech  rep   digital
equipment corporation 
leyton brown  k     shoham  y          essentials of game theory  a concise  multidisciplinary introduction  morgan   claypool  san rafael  ca 
littman  m     stone  p          implicit negotiation in repeated games intelligent agents
viii  in meyer  j  j     tambe  m   eds    intelligent agents viii  vol       of lecture
notes in computer science  chap      pp          springer berlin   heidelberg 
berlin  heidelberg 
mahonen  p     petrova  m          minority game for cognitive radios  cooperating without cooperation  physical communication               
monderer  d     shapley  l  s          potential games  games and economic behavior 
                 
norris  j  r          markov chains  cambridge series in statistical and probabilistic
mathematics   cambridge university press 
papadimitriou  c  h     roughgarden  t          computing correlated equilibria in multiplayer games  journal of the acm              
rego  v          naive asymptotics for hitting time bounds in markov chains  acta informatica                 
savit  r   manuca  r     riolo  r          adaptive competition  market efficiency  and
phase transitions  physical review letters                    
verbeeck  k   nowe  a   parent  j     tuyls  k          exploring selfish reinforcement
learning in repeated games with stochastic rewards  autonomous agents and multiagent systems                 
wang  l   wu  k   hamdi  m     ni  l  m          attachment learning for multi channel
allocation in distributed ofdma networks  parallel and distributed systems  international conference on            

   

fi