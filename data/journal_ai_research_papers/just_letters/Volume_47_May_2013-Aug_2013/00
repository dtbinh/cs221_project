journal artificial intelligence research               

submitted          published        

feature subset selection algorithm automatic
recommendation method
guangtao wang
qinbao song
heli sun
xueying zhang

gt wang stu xjtu edu cn
qbsong mail xjtu edu cn
hlsun mail xjtu edu cn
zhangxueying     stu xjtu edu cn

department computer science   technology 
xian jiaotong university          china

baowen xu
yuming zhou

bwxu nju edu cn
zhouyuming nju edu cn

department computer science   technology 
nanjing university  china

abstract
many feature subset selection  fss  algorithms proposed 
appropriate given feature selection problem  time  far
rarely good way choose appropriate fss algorithms problem hand  thus 
fss algorithm automatic recommendation important practically useful 
paper  meta learning based fss algorithm automatic recommendation method
presented  proposed method first identifies data sets similar
one hand k  nearest neighbor classification algorithm  distances among
data sets calculated based commonly used data set characteristics  then 
ranks candidate fss algorithms according performance similar
data sets  chooses algorithms best performance appropriate ones 
performance candidate fss algorithms evaluated multi criteria metric
takes account classification accuracy selected features 
runtime feature selection number selected features  proposed
recommendation method extensively tested     real world data sets    wellknown frequently used different fss algorithms five representative classifiers 
results show effectiveness proposed fss algorithm recommendation method 

   introduction
feature subset selection  fss  plays important role fields data mining
machine learning  good fss algorithm effectively remove irrelevant redundant
features take account feature interaction  leads insight
understanding data  improves performance learner enhancing
generalization capacity interpretability learning model  pudil  novovicova 
somol    vrnata      a  pudil  novovicova  somol    vrnata      b  molina  belanche 
  nebot        guyon   elisseeff        saeys  inza    larranaga        liu   yu       
liu  motoda  setiono    zhao        
c
    
ai access foundation  rights reserved 

fiwang  song  sun  zhang  xu   zhou

although large number fss algorithms proposed  single
algorithm performs uniformly well feature selection problems  experiments
 hall        zhao   liu        confirmed could exist significant differences
performance  e g   classification accuracy  among different fss algorithms given
data set  means given data set  fss algorithms outperform others 
raises practical important question  fss algorithms
picked given data set  common solution apply candidate fss algorithms given data set  choose one best performance crossvalidation strategy  however  solution quite time consuming especially highdimensional data  brodley        
purpose addressing problem efficient way  paper  fss
algorithm automatic recommendation method proposed  assumption underlying
proposed method performance fss algorithm data set related
characteristics data set  rationality assumption demonstrated
follows 
   generally  new fss algorithm proposed  performance needs extensively evaluated least real world data sets  however  published fss algorithms
rarely tested identical group data sets  hall        zhao   liu        yu  
liu        dash   liu        kononenko         is  two algorithms 
usually tested different data  implies performance fss
algorithm biases data sets 
   time  famous nfl  no free lunch   wolpert        theory tells us
that  particular data set  different algorithms different data conditioned performance  performance differences vary data sets 
evidences imply relationship performance
fss algorithm characteristics data sets  paper  intend explore
relationship utilize automatically recommend appropriate fss algorithm s 
given data set  recommendation process viewed specific application
meta learning  vilalta   drissi        brazdil  carrier  soares    vilalta       
used recommend algorithms classification problems  ali   smith        king 
feng    sutherland        brazdil  soares    da costa        kalousis  gama    hilario 
      smith miles        song  wang    wang        
model relationship  three fundamental issues considered  i 
features  often referred meta features  used characterize data set  ii 
evaluate performance fss algorithm identify applicable one s 
given data set  iii  recommend fss algorithm s  new data set 
paper  meta features  frequently used meta learning  vilalta  
drissi        ali   smith        king et al         brazdil et al         castiello  castellano 
  fanelli         employed characterize data sets  time  multi criteria
metric  takes account classification accuracy classifier
fss algorithm runtime feature selection number selected features 
used evaluate performance fss algorithm  meanwhile  k  nn  k  nearest
neighbor  based method proposed recommend fss algorithm s  new data set 
 

fisubset selection algorithm automatic recommendation

proposed fss algorithm recommendation method extensively tested
    real world data sets    well known frequently used different fss algorithms
five representative classifiers  results show effectiveness proposed recommendation method 
rest paper organized follows  section   introduces preliminaries 
section   describes proposed fss algorithm recommendation method  section   provides experimental results  section   conducts sensitivity analysis number
nearest data sets recommendation results  finally  section   summarizes
work draws conclusions 

   preliminaries
section  first describe meta features used characterize data sets  then 
introduce multi criteria evaluation metric used measure performance fss
algorithms 
    meta features data sets
proposed fss algorithm recommendation method based relationship
performance fss algorithms meta features data sets 
recommendation viewed data mining problem  performance
fss algorithms meta features target function input variables 
respectively  due ubiquity garbage in  garbage  lee  lu  ling    ko       
field data mining  selection meta features crucial proposed
fss recommendation method 
meta features measures extracted data sets used
uniformly characterize different data sets  underlying properties reflected 
meta features conveniently efficiently calculated  related
performance machine learning algorithms  castiello et al         
   years research study improve meta features proposed
statlog project  michie  spiegelhalter    taylor         number meta features
employed characterize data sets  brazdil et al         castiello et al        
michie et al         engels   theusinger        gama   brazdil        lindner   studer 
      sohn         demonstrated working well modeling relationship
characteristics data sets performance  e g   classification accuracy 
learning algorithms  ali   smith        king et al         brazdil et al         kalousis et al  
      smith miles         meta features characterize data sets themselves 
connection learning algorithms types  use model
relationship data sets fss algorithms 
commonly used meta features established focusing following three
aspects data set  i  general properties  ii  statistic based properties  iii  informationtheoretic based properties  castiello et al          table    shows details 
   order compute information theoretic features  data sets continuous valued features 
needed  well known mdl  minimum description length  method fayyad   irani criterion
used discretize continuous values 

 

fiwang  song  sun  zhang  xu   zhou

category
general properties

statistical based properties

information theoretic properties

notation

f


 x   
skew x 
kurt x 
h c norm
h x norm
i c  x 
i c  x max
enattr
n sratio

measure description
number instances
number features
number target concept values
data set dimensionality    i f
mean absolute linear correlation coefficient possible pairs features
mean skewness
mean kurtosis
normalized class entropy
mean normalized feature entropy
mean mutual information class attribute
maximum mutual information class attribute
equivalent number features  enattr   h c  m i c  x 
noise signal ratio  n sratio    h x  i c  x   m i c  x 

table    meta features used characterize data set
    multi criteria metric fss algorithm evaluation
section  first  classical metrics evaluating performance fss algorithm
introduced  then  analyzing user requirements practice application  based
metrics  new user oriented multi criteria metric proposed fss algorithm
evaluation combining metrics together 
      classical performance metrics
evaluating performance fss algorithm  following three metrics
extensively used feature selection literature  i  classification accuracy   ii  runtime
feature selection  iii  number selected features 
   classification accuracy  acc  classifier selected features used
measure well selected features describe classification problem 
given data set  different feature subsets generally result different classification
accuracies  thus  reasonable feature subset higher classification accuracy stronger capability depicting classification problem  classification
accuracy reflects ability fss algorithm identifying salient features
learning 
   runtime  t  feature selection measures efficiency fss algorithm
picking useful features  viewed metric measure cost feature
selection  longer runtime  higher expenditure feature selection 
   number selected features  n  measures simplicity feature selection
results  classification accuracies two fss algorithms similar  usually
favor algorithm fewer features 
feature subset selection aims improve performance learning algorithms
usually measured classification accuracy  fss algorithms higher classification accuracy favor  however  mean runtime
number selected features could ignored  explained following two
considerations 
   suppose two different fss algorithms ai aj   given data set d 
classification accuracy ai slightly greater aj  
 

fisubset selection algorithm automatic recommendation

runtime ai number features selected ai much greater
aj   aj often chosen 
   usually  prefer use algorithms higher accuracy longer runtime 
lower accuracy shorter runtime  therefore  need tradeoff classification accuracy runtime feature selection the number selected
features  example  real time systems  impossible choose algorithm
high time consumption even classification accuracy high 
therefore  necessary allow users making user oriented performance evaluation
different fss algorithms  purpose  needed address problem
integrate classification accuracy runtime feature selection number
selected features obtain unified metric  paper  resort multi criteria
metric explore problem  underlying reason lies multi criteria metric
successfully used evaluate data mining algorithms considering positive
properties  e g  classification accuracy  negative ones  e g  runtime number
selected features  simultaneously  nakhaeizadeh   schnabl              
comparing two algorithms  besides metrics used evaluate performance 
ratio metric values used  example  suppose a  a  two
different fss algorithms  a  better a  terms classification accuracy  i e  
acc    acc      ratio acc   acc      used show a  better a  well 
contrary  negative metrics runtime feature selection number
selected features  corresponding ratio     means better algorithm 
actually  multi criteria metric adjusted ratio ratios  arr   brazdil et al         
combines classification accuracy runtime together unified metric 
proposed evaluate performance learning algorithm  extend arr integrating runtime feature selection number selected features  new
multi criteria metric earr  extened arr  proposed  following discussion 
show new metric earr inclusive  flexible  easy understand 
      multi criteria metric earr
let dset    d    d      dn   set n data sets  aset    a    a       
set fss algorithms  suppose accji classification accuracy classifier fss
algorithm ai data set dj        j n    tji nji denote runtime
number selected features fss algorithm ai data set dj   respectively 
earr ai aj dk defined
k
earrd
ai  aj  

accki  acckj
    log  tki  tkj     log  nki  nkj  

      j m    k n   

   

user predefined parameters denote relative importance
runtime feature selection number selected features  respectively 
computation metric earr based ratios classical fss algorithm performance metrics  classification accuracy  runtime feature selection
   acc  acc  corresponding classification accuracies algorithms a  a    respectively 

 

fiwang  song  sun  zhang  xu   zhou

number selected features  definition know earr evaluates
fss algorithm comparing another algorithm  reasonable since
objective assert algorithm good comparing another one instead
focusing performance  example  suppose classifier    
classification accuracy data set  get confused whether classifier good
not  however  compare another classifier obtain     classification
accuracy data set  definitely say first classifier good
compared second one 
noted that  practice  runtime difference two different fss algorithms
usually quite great  meanwhile  high dimensional data sets  difference
number selected features two different fss algorithms great well  thus 
ratio runtime ratio number selected features usually much
wide ranges classification accuracy  simple ratio runtime
simple ratio number selected features employed  would dominate
value earr  ratio classification accuracy drowned  order
avoid situation  common logarithm  i e   logarithm base     ratio
runtime common logarithm ratio number selected features
employed 
parameters represent amount classification accuracy user
willing trade    times speedup reduction runtime feature selection the
number selected features  respectively  allows users choose algorithms
shorter runtime less features acceptable accuracy  illustrated
following example  suppose accki            acckj   runtime algorithm ai
given data set    times aj  i e   tki      tkj    number selected
features algorithm ai    times aj  i e   nki      nkj    then  according
dk
 
 
k
eq       earr
ai  aj      earr aj  ai              case  aj outperforms
ai   user prefers fast algorithms less features  aj his her choice 
k
value earr varies around    value earr
ai  aj greater  or equal
k
to  smaller than  earr
aj  ai indicates ai better  or equal to 
worse than  aj  
eq      directly used evaluate performance two different fss algorithms 
comparing multiple fss algorithms  performance algorithm ai aset
given data set evaluated metric earr
ai defined follows 

earrd
ai  

 
 


x

earrd
ai  aj  

   

j  j  i

equation shows earr fss algorithm ai arithmetic
mean earrd
ai  aj ai algorithm aj d  is  performance
fss algorithm ai aset evaluated based comparisons algorithms
 aset  ai     larger value earr  better corresponding fss algorithm
given data set d 
   since log  x y    log  y x            

 

fisubset selection algorithm automatic recommendation

   fss algorithm recommendation method
section  first give framework fss algorithm recommendation  then 
introduce nearest neighbor based recommendation method detail 
    framework
based assumption relationship performance fss
algorithm given data set data set characteristics  aka meta features  
proposed recommendation method firstly constructs meta knowledge database consisting
data set meta features fss algorithm performance  that  help
meta knowledge database  k nn based method used model relationship
recommend appropriate fss algorithms new data set 
therefore  proposed recommendation method consists two parts  meta knowledge
database construction fss algorithm recommendation  fig    shows details 
meta knowledge database construction
feature selection
algorithms

historical
data sets

performance metric
aquirement
meta features
extraction

performance metrics

meta features

fss algorithm recommendation

new data set
recommended
fss algorithms

metaknowledge
database
performance
metrics

meta features

meta features
extraction

meta features

nearest data sets
identification

top r algorithms
recommendation

ranks

fss algorithms
ranking

nearest
data sets

metric
collection

performance
metrics

figure    framework feature subset selection algorithm recommendation
   meta knowledge database construction
mentioned previously  meta knowledge database consists meta features
set historical data sets performance group fss algorithms them 
foundation proposed recommendation method  effectiveness
recommendations depends heavily database 
meta knowledge database constructed following three steps  firstly 
meta features table   extracted historical data set module metafeatures extraction  then  candidate fss algorithm applied historical data
set  classification accuracy  runtime feature selection number selected
features recorded  corresponding value performance metric earr
calculated  accomplished module performance metric calculation  finally 
data set  tuple  composed meta features values
performance metric earr candidate fss algorithms  obtained added
knowledge database 
   fss algorithm recommendation
 

fiwang  song  sun  zhang  xu   zhou

based introduction first part meta knowledge database construction
presented above  learning target meta knowledge data set earr values
instead appropriate fss algorithm  case  demonstrated
researchers usually resort instance based k  nn  nearest neighbors  methods
variations  brazdil et al               algorithm recommendation  thus  k  nn
based fss algorithm recommendation procedure proposed 
recommending fss algorithms new data set  firstly  meta features
data set extracted  then  distance new data set historical
data set calculated according meta features  that  k nearest data sets
identified  earr values candidate fss algorithms k data sets
retrieved meta knowledge database  finally  candidate fss algorithms
ranked according earr values  algorithm highest earr
achieves top rank  one second highest earr gets second rank 
forth  top r algorithms recommended 
    recommendation method
recommend appropriate fss algorithms new data set dnew based k nearest
data sets  two foundational issues solved  i  identify k nearest
data sets  ii  recommend appropriate fss algorithms based k data
sets 
   k nearest data sets identification
k nearest data sets dnew identified calculating distance dnew
historical data set based meta features  smaller distance 
similar corresponding data set dnew  
order effectively calculate distance two data sets  l  norm distance
 atkeson  moore    schaal        adopted since easy understand calculate 
ability measuring similarity two data sets demonstrated
brazdil et al         
let    fi     fi       fi h   meta features data set di   fi p
value pth feature h length meta features  l  norm distance
data sets di dj formulated
dist di   dj     kfi fj k   

h
x

 fi p fj p   

   

p  

worth noting ranges different meta features quite different  example 
meta features introduced table    value normalized class entropy varies
     number instances millions  thus  meta features
different ranges directly used calculate distance two data sets  metafeatures large range would dominate distance  meta features small
range ignored  order avoid problem      standardized method  eq 
     employed make meta features range        
fi p min  f p  
 
max  f p   min  f p  
 

   

fisubset selection algorithm automatic recommendation

fi p    p h  value pth meta feature data set di   min  f p  
max  f p   denote minimum maximum values pth meta feature historical
data sets  respectively 
   fss algorithm recommendation
getting k nearest data sets dnew   performance candidate fss
algorithms dnew evaluated according k nearest data sets  then 
algorithms best performance recommended 

let dknn    d    d      dk   k nearest data sets dnew earr aij
performance metric fss algorithm ai data set dj dknn    j k  
performance ai new data set dnew evaluated
knn
earrd
ai

 

k
x

j


earraij  

j   dj

 

k
x
dt     dj   dist dnew   dj   
 

   

t  

j  

eq      indicates performance fss algorithm ai dnew evaluated
performance dknn dnew   data set dj dknn   smaller distance
dj dnew   similar two data sets  means two data
sets dp dq   dp   dq data set dp similar dnew   earr
ai dp important evaluating performance ai dnew   thus 
weighted average  takes account relative importance data set dknn
rather treating data set equally  employed  moreover  domain machine
learning  reciprocal distance usually used measure similarity 
k
p
j   dj     dt   used weight earr ai dj dknn  
t  

according earr candidate fss algorithm aset dnew   rank
candidate fss algorithms obtained  greater earr  higher
rank  then  top r  e g   r     paper  fss algorithms picked
appropriate ones new data set dnew  
procedure fssalgorithmrecommendation shows pseudo code recommendation 
time complexity  recommendation procedure consists two parts  first part
 lines       k nearest data sets given new data set identified  firstly 
meta features f extracted function metafeatureextraction    then 
k nearest historical data sets identified function neighborrecognition   based
distance f meta features historical data set di   suppose
p number instances q number features given data set
d  time complexity function metafeatureextraction   o p   q   function
neighborrecognition    time complexity o n  depends number
historical data sets n  consequently  time complexity first part o p  q  o n  
second part  lines       r fss algorithms recommended data set
d  since weights earrs k nearest data sets obtained directly 
time complexity two steps o     time complexity estimating ranking
earrs algorithms aset o k m    o m log m    k preassigned
number nearest data sets number candidate fss algorithms 
 

fiwang  song  sun  zhang  xu   zhou

procedure fssalgorithmrecommendation
inputs  
  new given data set 
dset    d    d      dn    historical data sets 
aset    a    a         candidate fss algorithms 
metadatabase     fi   earrsi     n  earrs
meta features earrs aset di   respectively 
k   predefined number nearest neighbors 
r   number recommended fss algorithms 
output  recalgs   recommended fss algorithms
  part    recognition k nearest data sets
  f   metafeatureextraction  d    extract meta features
  metafeatureset    f    f      fn     meta feature data set dset
  neighbors   neighborrecognition  k  f  metafeatureset  
  part    appropriate fss algorithm recommendation
  weightset   calculate weight data set neighbors   see eq     
  earrset   corresponding earrs data set neighbors metadatabase 
  estimate earr fss algorithm aset according weightset earrset

eq      rank algorithms aset based earrs 
  recalgs   top r fss algorithms 
  return recalgs 

sum up  time complexity recommendation procedure o p   q    o n   
o km  o mlog m    practice  data set needs conduct feature selection 
number instances p and or number features q much greater
number nearest data sets k number candidate fss algorithms
m  major time consumption recommendation procedure determined
first part 

   experimental results analysis
section  experimentally evaluate proposed feature subset selection  fss 
algorithm recommendation method recommending algorithms benchmark data
sets 
    benchmark data sets
    extensively used real world data sets  come different areas computer 
image  life  biology  physical text     employed experiment  sizes
data sets vary          instances  numbers features
        
statistical summary data sets shown table   terms number
instances  denoted i   number features  denoted f  number target
concepts  denoted t  
   data sets available http   archive ics uci edu ml datasets html  http   
featureselection asu edu datasets php  http   sci s ugr es keel datasets php  http   www 
upo es eps bigs datasets html  http   tunedit org repo data  respectively 

  

fisubset selection algorithm automatic recommendation

data id
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

data name
ada agnostic
ada prior
anneal
anneal orig
ar  p        
arrhythmia
audiology
autos
balance scale
breast cancer
breast w
bridges version 
bridges version 
car
cll sub             
cmc
colic
colic orig
colon
credit a
credit g
cylinder bands
dermatology
diabetes
ecml  x     
ecoli
embryonaldataset c
eucalyptus
flags
gcm test
gina agnostic
gina prior
gina prior 
glass
grub damage
heart c
heart h
heart statlog
hepatitis
hypothyroid
ionosphere
iris
kdd ipums la    small
kdd ipums la    small
kdd ipums la    small
kdd japanesevowels test
kdd japanesevowels train
kdd synthetic control
kr vs kp
labor
leukemia
leukemia  c
leukemia test   x    
leukemia train   x    
lung cancer
lymph
lymphoma  x      classes
lymphoma  x       classes


    
    
   
   
   
   
   
   
   
   
   
   
   
    
   
    
   
   
  
   
    
   
   
   
  
   
  
     
   
  
    
    
    
   
   
   
   
   
   
    
   
   
    
    
    
    
    
   
    
  
  
  
  
  
  
   
  
  

f
  
  
  
  
   
   
  
  
 
  
  
  
  
 
    
  
  
  
    
  
  
  
  
 
     
 
    
   
  
     
   
   
   
  
 
  
  
  
  
  
  
 
  
  
  
  
  
  
  
  
    
    
    
    
  
  
    
    


 
 
 
 
  
  
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
  
 
  
 
 
  
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
  

data id
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

data name
lymphoma  x      classes
mfeat fourier
mfeat morphological
mfeat pixel
mfeat zernike
molecular biology promoters
monks problems   test
monks problems   train
monks problems   test
monks problems   train
monks problems   test
monks problems   train
mushroom
oh  wc
oh   wc
oh   wc
oh  wc
pasture
pendigits
pie  p         
postoperative patient data
primary tumor
segment
shuttle landing control
sick
smk can             
solar flare  
solar flare  
sonar
soybean
spectf test
spectf train
spectrometer
spect test
spect train
splice
sponge
squash stored
squash unstored
sylva agnostic
sylva prior
tox             
tr   wc
tr   wc
tr   wc
tr   wc
tr   wc
tr   wc
trains
vehicle
vote
vowel
wap wc
waveform     
white clover
wine
zoo


  
    
    
    
    
   
   
   
   
   
   
   
    
    
    
   
   
  
     
   
  
   
    
  
    
   
   
    
   
   
   
  
   
   
  
    
  
  
  
     
     
   
   
   
   
   
   
   
  
   
   
   
    
    
  
   
   

f
    
  
 
   
  
  
 
 
 
 
 
 
  
    
    
    
    
  
  
    
 
  
  
 
  
    
  
  
  
  
  
  
   
  
  
  
  
  
  
   
   
    
    
    
    
     
    
    
  
  
  
  
    
  
  
  
  


 
  
  
  
  
 
 
 
 
 
 
 
 
  
  
  
  
 
  
  
 
  
 
 
 
 
 
 
 
  
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
 
 
 
  
  
 
 
 
 

table    statistical summary     data sets

    experimental setup
order evaluate performance proposed fss algorithm recommendation
method  verify whether proposed method potentially useful practice 
confirm reproducibility experiments  set experimental study follows 
      fss algorithms
fss algorithms grouped two broad categories  wrapper filter  molina et al  
      kohavi   john         wrapper method uses error rate classification
algorithm evaluation function measure feature subset  evaluation
function filter method independent classification algorithm  accuracy
wrapper method usually high  however  generality result limited 
computational complexity high  comparison  filter method generality 
computational complexity low  due fact wrapper method
computationally expensive  dash   liu         filter method usually good choice
  

fiwang  song  sun  zhang  xu   zhou

number features large  thus  focus filter method
experiment 
number filter based fss algorithms proposed handle feature selection
problems  algorithms significantly distinguished i  search method used
generate feature subset evaluated  ii  evaluation measures used assess
feature subset  liu   yu        de souza        dash   liu        pudil  novovicova 
  kittler        
order guarantee generality experimental results  twelve well known
latest search methods four representative evaluation measures employed 
brief introduction search methods evaluation measures follows 
   search methods
i  sequential forward search  sfs   starting empty set  sequentially add
feature results highest value objective function current
feature subset 
ii  sequential backward search  sbs   starting full set  sequentially eliminate
feature results smallest decrease value objective function
current feature subset 
iii  bi direction search  bis   parallel implementation sfs sbs  searches
feature subset space two directions 
iv  genetic search  gs   randomized search method performs using simple
genetic algorithm  goldberg         genetic algorithm finds feature subset
maximize special output function using techniques inspired natural evolution 
v  linear search  ls   extension bestfirst search  gutlein  frank  hall    karwath        searches space feature subsets greedy hill climbing
augmented backtracking facility 
vi  rank search  rs   battiti         uses feature evaluator  such gain ratio 
rank features  feature evaluator specified  forward selection
search used generate ranking list 
vii  scatter search  ss   garcia lopez  garcia torres  melian batista  moreno perez 
  moreno vega         method performs scatter search feature
subset space  starts population many significant diverse feature
subsets  stops assessment criteria higher given threshold
improvement longer 
viii  stepwise search  sws   kohavi   john         variation forward search 
step search process  new feature added  test performed
check features eliminated without significant reduction
output function 
ix  tabu search  ts   hedar  wang    fukushima         proposed combinatorial optimization problems  adaptive memory responsive exploration
combining local search process anti cycling memory based rules avoid
trapping local optimal solutions 
  

fisubset selection algorithm automatic recommendation

x  interactive search  zhao   liu         traverses feature subset space
maximizing target function taking consideration interaction among
features 
xi  fcbf search  yu   liu         evaluates features via relevance redundancy analysis  uses analysis results guideline choose features 
xii  ranker  kononenko        kira   rendell        liu   setiono         evaluates
feature individually ranks features values evaluation
metrics 
   evaluation measures
i  consistency  liu   setiono        zhao   liu         kind measure evaluates worth feature subset level consistency target concept
instances projected onto feature subset  consistency
feature subset never lower full feature set 
ii  dependency  hall        yu   liu         kind measure evaluates worth
subset features considering individual predictive ability feature
along degree redundancy among features  fss methods based
kind measure assume good feature subsets contain features closely
correlated target concept  uncorrelated other 
iii  distance  kira   rendell        kononenko         kind measure proposed based assumption distance instances different target
concepts greater target concepts 
iv  probabilistic significance  zhou   dillon        liu   setiono         measure
evaluates worth feature calculating probabilistic significance
two way function  i e   association feature target concept 
good feature significant association target concept 
pay attention that  besides four evaluation measures 
another basic kind measure  information based measure  liu   yu        de souza       
dash   liu         contemplated experiments  reason demonstrated follow  information based measure usually conjunction ranker
search method  thus  fss algorithms based kind measure usually provide
rank list features instead telling us features relevant learning target  case  preassign particular thresholds fss algorithms pick
relevant features  however  effective method set thresholds
acknowledged default threshold fss algorithms  moreover  unfair
conclude information measure based fss algorithms assigned threshold
appropriate comparing fss algorithms  therefore  kind
fss algorithm employed experiments 
based search methods evaluation measures introduced above     different
fss algorithms obtained  table   shows brief introduction fss algorithms 
algorithms available data mining toolkit weka   hall  frank 
   http   www cs waikato ac nz ml weka 

  

fiwang  song  sun  zhang  xu   zhou

holmes  pfahringer  reutemann    witten         search method interact
implemented based weka source codes available online   
id
 
 
 
 
 
 
 
 
 
  
  

search method
bestfirst   sequential forward search
bestfirst   sequential backward search
bestfirst   bi direction search
genetic search
linear search
rank search
scatter search
stepwise search
tabu search
interactive search
bestfirst   sequential forward search

evaluation measure
dependency
dependency
dependency
dependency
dependency
dependency
dependency
dependency
dependency
dependency
consistency

notation
cfs sfs
cfs sbs
cfs bis
cfs gs
cfs ls
cfs rs
cfs ss
cfs sws
cfs ts
interact d
cons sfs

id
  
  
  
  
  
  
  
  
  
  
  

search method
bestfirst   sequential backward search
bestfirst   bi direction search
genetic search
linear search
rank search
scatter search
stepwise search
interactive search
fcbfsearch
ranker
ranker

evaluation measure
consistency
consistency
consistency
consistency
consistency
consistency
consistency
consistency
dependency
distance
probabilistic significance

notation
cons sbs
cons bis
cons gs
cons ls
cons rs
cons ss
cons sws
interact c
fcbf
relief f
signific

table    introduction    fss algorithms
noted algorithms require particular settings certain parameters  purpose allowing researchers confirm results  introduce
parameter settings fss algorithms  as  fss algorithms interact d
interact c  parameter  c contribution threshold  used identify
irrelevant features  set threshold        suggested zhao liu        
fss algorithm fcbf  set relevance threshold su  symmetric uncertainty  value bn  log n cth ranked feature suggested yu liu         fss
algorithm relief f  set significance threshold      used robnik sikonja
kononenko         fss algorithm signific  threshold  statistical significance level   used identify irrelevant features  set commonly used value
     experiment  fss algorithms conducted weka environment
default setting s  
      classification algorithms
since actual relevant features real world data sets usually known advance 
impracticable directly evaluate fss algorithm selected features  classification
accuracy extensively used metric evaluating performance fss algorithms 
plays important role proposed performance metric earr assessing
different fss algorithms 
however  different classification algorithms different biases  fss algorithm may
suitable classification algorithms others  de souza         fact
affects performance evaluation fss algorithms 
mind  order demonstrate proposed fss algorithm recommendation method limited particular classification algorithm  five representative
classification algorithms based different hypotheses employed  bayes based
naive bayes  john   langley        bayes network  friedman  geiger    goldszmidt 
       information gain based c     quinlan         rule based part  frank   witten 
       instance based ib   aha  kibler    albert         respectively 
although naive bayes bayes net bayes based classification algorithms 
quite different since naive bayes proposed based hypoth   http   www public asu edu huanliu interact interactsoftware html

  

fisubset selection algorithm automatic recommendation

esis features conditional independent  john   langley         bayes net
takes account feature interaction  friedman et al         
      measures evaluate recommendation method
fss algorithm recommendation application meta learning  far know 
unified measures evaluate performance meta learning methods 
order assess proposed fss algorithm recommendation method  two measures 
recommendation hit ratio recommendation performance ratio  defined 
let given data set arec fss algorithm recommended recommendation method d  two measures introduced follows 
   recommendation hit ratio
intuitive evaluation criterion whether recommended fss algorithm arec meets
users requirements  is  whether arec optimal fss algorithm d  performance arec significant difference optimal fss algorithm 
suppose aopt represents optimal fss algorithm d  asetopt denotes fss
algorithm set algorithm significant difference aopt  of course
includes aopt well   then  measure named recommendation hit defined assess
whether recommended algorithm arec effective d 
definition    recommendation hit   fss algorithm arec recommended data
set d  recommendation hit hit arec   d  defined
 
   arec asetopt
hit arec   d   
 
   
   otherwise
hit arec   d      means recommendation effective since recommended
fss algorithm arec one algorithms asetopt d  hit arec   d      indicates
recommended fss algorithm arec member asetopt   i e   arec significantly
worse optimal fss algorithm aopt d  thus recommendation bad 
definition   know recommendation hit hit arec   d  used evaluate
recommendation method single data set  thus  extended recommendation
hit ratio evaluate recommendation set data sets  defined follows 
definition   recommendation hit ratio
g

  x
hit ratio arec    
hit arec   di   
g

   

i  

g number historical data sets  e g   g       experiment 
definition   represents percentage data sets appropriate fss algorithms effectively recommended recommendation method  larger value 
better recommendation method 
   recommendation performance ratio
recommendation hit ratio reveals whether appropriate fss algorithm
recommended given data set  cannot tell us margin recommended
algorithm best one  thus  new measure  recommendation performance ratio
recommendation  defined 
  

fiwang  song  sun  zhang  xu   zhou

definition    recommendation performance ratio   let earrrec earropt
performance recommended fss algorithm arec optimal fss algorithm d 
respectively  then  recommendation performance ratio  rpr  arec defined
rpr arec   d    earrrec  eaaropt  

   

definition  best performance earr opt employed benchmark  without
benchmark  hard determine recommended algorithms good not 
example  suppose earr arec       earr opt        
recommendation effective since earr arec close earr opt   however 
recommendation poor earr opt        
rpr ratio earr recommended fss algorithm optimal one 
measures close recommended fss algorithm optimal one  reveals
relative performance recommended fss algorithm  value varies     
larger value rpr  closer performance recommended fss algorithm
optimal one  recommended algorithm optimal one
rpr     
      values parameters
paper  multi criteria metric earr proposed evaluate performance
fss algorithm  proposed metric earr  two parameters established
users express requirements algorithm performance 
experiment  presenting results  two representative value pairs parameters used follows 
            setting represents situation classification
accuracy important  higher classification accuracy selected features 
better corresponding fss algorithms 
              setting represents situation user tolerate
accuracy attenuation favor fss algorithms shorter runtime fewer
selected features  experiment  set     quite different
       allows us explore impact two parameters
recommendation results 
moreover  order explore parameters affect recommended fss
algorithms terms classification accuracy  runtime number selected features 
different parameters settings provided  specifically  values vary  
    increase    
    experimental process
order make sure soundness experimental conclusion guarantee
experiments reported reproducible  part  introduce four crucial processes
used experiments  i  meta knowledge database construction  ii  optimal
fss algorithm set identification given data set  iii  recommendation method validation
iv  sensitivity analysis number nearest data sets recommendations 
   meta knowledge database construction
  

fisubset selection algorithm automatic recommendation

procedure performanceevaluation
inputs   data   given data set  i e  one     data sets 
learner   given classification algorithm  i e   one  naive bayes  c     part 
ib  bayes network  
fssalgset    fssalg     fssalg       fssalg       set    fss
algorithms 
output  earrset    earr     earr       earr       earrs    fss
algorithms data 
       folds      
        
 
earr     
     
 
randomized order data 
 
generate folds bins data 
 
j     folds
 
testdata   bin j   
 
traindata   data  testdata 
  
numberlist   null   runtimelist   null   accuracylist   null  
  
k       
  
 subset  runtime    apply fssalg k traindata 
  
number    subset   
  
redtestdata   reduce testdata according selected subset 
  
redtraindata   reduce traindata according selected subset 
  
classifier   learner  redtraindata  
  
accuracy   apply classifier redtestdata 
  
numberlist  k     number   runtimelist  k     runtime  accuracylist  k    

accuracy 
  
  
  

k       
earr   earrcompution accuracylist  runtimelist  numberlist  k   
  compute earr fssalg k jth bin pass according eqs         
earr k   earr k   earr 

       
  
earr   earr   m folds   
   return earrset 

data set di          i  extract meta features   ii  calculate
earrs    candidate fss algorithms stratified     fold cross validation
strategy  kohavi         iii  combine meta features earr fss
algorithm together form tuple  finally added meta knowledge database 
since extraction meta features combination meta features
earrs straightforward  present calculation earrs  procedure performanceevaluation shows details 
   optimal fss algorithm set identification
optimal fss algorithm set given data set di consists optimal fss algorithm data set algorithms significant performance difference
optimal one di  
optimal fss algorithm set given data set di obtained via non parametric
friedman test        followed holm procedure test        performance 
  

fiwang  song  sun  zhang  xu   zhou

estimated     cross validation strategy     fss algorithms  result
friedman test shows significant performance difference among
   fss algorithms     fss algorithms added optimal fss algorithm set 
otherwise  fss algorithm highest performance viewed optimal one
added optimal fss algorithm set  then  holm procedure test performed
identify algorithms rest    fss algorithms  algorithms
significant performance differences optimal one added optimal fss
algorithm set 
reason non parametric test employed lies difficult
performance values follow normal distribution satisfy variance homogeneous
condition 
note optimal fss algorithm sets different settings parameters
different  since values two parameters directly affect required performance
values 
   recommendation method validation
leave one out strategy used empirically evaluate proposed fss algorithm recommendation method follows  data set di        
viewed test data  i  identify k nearest data sets training data  
 d      di    di       d      excluding di   ii  calculate performance    candidate fss algorithms according eq      based k nearest data sets value
k determined standard cross validation strategy  recommend top three
di   iii  evaluate recommendations measures introduced section       
   sensitivity analysis number nearest data sets recommendations
order explore effect number nearest data sets recommendations provide users empirical method choose value  data set 
possible numbers nearest data sets tested  is  identifying k
nearest data sets given data set  k set   number historical data
sets minus    e g       experiment  
    results analysis
section  present recommendation results terms recommended fss algorithms  hit ratio performance ratio   respectively  due space limit
paper  list recommendations  instead present results two
significantly different pairs   i e                               
afterward  provide experimental results influence user oriented
parameters recommendations terms classification accuracy  runtime 
number selected features  respectively 
      recommended algorithms hit ratio
figs               show first recommended fss algorithms     data sets
classification algorithms naive bayes  c     part  ib  bayes network
used  respectively 
  

fisubset selection algorithm automatic recommendation

figure  two sub figures corresponding recommendation results
                             respectively  sub figure 
denote correctly incorrectly recommended algorithms  respectively 

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a          
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b              

figure    fss algorithms recommended     data sets naive bayes used

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a          
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b              

figure    fss algorithms recommended     data sets c    used
figures  observe that 
   five classifiers  proposed method effectively recommend appropriate
fss algorithms     data sets 
case              number data sets  whose appropriate fss
algorithms correctly recommended          naive bayes         
c             part          ib           bayes
  

fiwang  song  sun  zhang  xu   zhou

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a          
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b              

figure    fss algorithms recommended     data sets part used

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a          
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b              

figure    fss algorithms recommended     data sets ib  used

network  respectively  states recommendation method effective
classification accuracy important 
case                  number data sets  whose appropriate fss
algorithms correctly recommended          naive bayes         
c             part          ib           bayes
network  respectively  indicates recommendation method works well
tradeoff required among classification accuracy  runtime  number
selected features 
   distribution recommended fss algorithms     data sets different
different parameters settings  distribution relatively uniform             
  

fisubset selection algorithm automatic recommendation

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a          
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b              

figure    fss algorithms recommended     data sets bayes network used

seriously biased algorithm  e g     th fss algorithm     
            
phenomenon similar five classifiers  explained follows 
fss algorithms best classification accuracy distribute     data sets
uniformly  thus  case             users favor accurate classifiers 
distribution recommended fss algorithms relatively uniform well  however 
exist fss algorithms run faster  e g     th algorithm signific 
select fewer features  e g    th algorithm cfs sws    th algorithm conssws    th algorithm signific      data sets  reason 
case                 users prefer fss algorithms less runtime
fewer features  distribution fss algorithms best performance
    data sets biased algorithms  recommended fss algorithms 
     th fss algorithm performs well        data sets classifiers
                 seems fss algorithm generally wellperformed fss algorithm adopted fss tasks need
fss algorithm recommendation  unfortunately  case    th fss
algorithm still failing perform well quarter     data sets 
yet  recommendation method distinguish data sets effectively
recommend appropriate fss algorithms them  indicates recommendation
method still necessary case 
compared              know case due larger
values explained follows     fss algorithms  although
classification accuracies classifier features selected different 
differences usually bounded  meanwhile  eq      know  
set greater bound value  value earr dominated
runtime the number selected features  means set
relatively large value  algorithm lower time complexity algorithm
chooses smaller number features recommended  classification
  

fiwang  song  sun  zhang  xu   zhou

accuracy selected features ignored  however  know  one
important targets feature selection improve performance learning
algorithms  unreasonable ignore classification accuracy focus
speed simplicity fss algorithm 
thus  real applications  values set limit classification accuracies  generally    bounded  accmax accmin   accmin  
accmax accmin denote maximum minimum classification accuracies  respectively 
parameter setting
        

            


recommendation
alg st
alg nd
alg rd
top  
alg st
alg nd
alg rd
top  

naive bayes
     
     
     
     
     
     
     
     

c   
     
     
     
     
     
     
     
     

part
     
     
     
     
     
     
     
     

ib 
     
     
     
     
     
     
     
     

bayes network
     
     
     
     
     
     
     
     

algx denotes x  th algorithm ranking list recommended top   means top three
algorithms recommended 

table    hit ratio     recommendations five classifiers different settings
    
table   shows hit ratio recommended fss algorithms five classifiers 
observe that 
   single fss algorithm recommended  hit ratio first recommended algorithm alg st highest  value        least       
five classifiers  thus  alg st first choice 
   top three algorithms recommended  hit ratio      least
        indicates confidence top three algorithms including
appropriate one high  reason top three algorithms
recommended  moreover  proposed recommendation method reduced
number candidate algorithms three  users pick one fits
his her specific requirement them 
      recommendation performance ratio
figs      show recommendation performance ratio rpr first recommended
fss algorithm five classifiers                             
respectively  two figures observe that  data sets two
settings   rprs recommended fss algorithms greater    
     matter classifier used  indicates
fss algorithms recommended proposed method close optimal one 
table   shows average rprs     data sets five classifiers
different settings       table  classifier  columns rec def
shows rpr value corresponding recommended fss algorithms default fss
algorithms  respectively  default fss algorithm frequent best one
    data sets classifier 
  

fisubset selection algorithm automatic recommendation

rpr    

naive bayes
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

c   
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

part
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

ib 
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

bayes network
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

figure    rpr   st recommended fss algorithm             five
classifiers
rpr    

naive bayes
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

c   
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

part
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

ib 
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

bayes network
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

figure    rpr   st recommended fss algorithm                
five classifiers

  

fiwang  song  sun  zhang  xu   zhou

observe average rprs range                    
                                     respectively  moreover 
average rpr recommended fss algorithms surpasses default fss
algorithms five different classifiers  means proposed recommendation
method works well greatly fits users performance requirement 
parameter setting
        
            

naive bayes
rec
def
           
           

c   
rec
def
     
     
     
     

part
rec
def
           
           

ib 
rec
def
     
     
     
     

bayes network
rec
def
     
     
     
     

table    average rpr         data sets five classifiers

data id
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  


nb
      
      
      
      
      
     
      
      
     
      
      
      
      
      
      
      
      
      
      
     
     
      
     
      
      
      
      
      
     
      
      
      
      
     
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

c   
      
      
      
      
      
     
      
      
      
      
     
      
      
      
      
      
      
      
      
      
    
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
     
      
      
     
      

part
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
     
     
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
     
      
      
      
      
      
      
      
      
      
      
      
     
      
      

ib 
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
     
      
      
     
     
      
      
      
      
      
      
      
      
      
      
     
      
      

bnet
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
     
      
     
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
     
     
      
      
      
      
      
      
      
      
      
      
      
     
      
     
     
      
      
      
      
      
      
      
      

data id
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
average

nb
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
    
      
      
      
      
      
      
      
      
      
      
      
      
      
      

c   
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

part
      
      
      
      
      
      
      
      
      
      
     
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      

ib 
      
     
      
      
    
      
      
     
      
      
      
      
      
      
      
      
      
      
      
     
     
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

bnet
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
     
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

nb bnet denote naive bayes bayes network  respectively 

table    recommendation time     data sets five classifiers  in second  

  

fisubset selection algorithm automatic recommendation

      recommendation time
recommending fss algorithms feature selection problem  recommendation
time contributed meta features extraction  k nearest data sets identification 
candidate algorithm ranking according performance k data sets 
three recommendation time contributors  candidate algorithm ranking
related parameters performance metric earr 
however  computation performance earr whatever values
are  means recommendation time independent specific settings
  thus  present recommendation time              table  
shows details 
table   observe given data set  recommendation time differences
five classifiers small  reason recommendation time mainly
contributed extraction meta features  relation classifiers 
consistent time complexity analysis section      observe
data sets  recommendation time less     second  average value
    data sets around      second five classifiers  much faster
conventional cross validation method 
      impact parameters
figs                   show impact settings classification
accuracy  runtime feature selection  number selected features  hit ratio
rpr value  respectively 
naivebayes

c   

part

average accuracy

     

average accuracy

ib 

bayes network

     

    
     
    
     
    
     
    

    
     
    
     
    
     
    
     

     
 

 

                                                

                                                

 

figure    classification accuracies five classifiers recommended fss algorithms different values
fig    shows classification accuracies five classifiers different values
  observe that  increase either   classification
accuracies five classifiers recommended fss algorithms decrease 
increase indicates users much prefer faster fss algorithms
fss algorithms get less features  thus  proportion classification accuracy
performance decreased  means ranks fss algorithms run faster
and or get less features improved corresponding fss algorithms finally
selected 
  

fiwang  song  sun  zhang  xu   zhou

c   

part

ib 

bayes network

    

average runtime  ms 

average runtime  ms 

naivebayes
    
    
    
    
    
    
    
    
   
   
   
   

    
    
    
    
    
    
    
   
   
   

 

 

                                                

                                                

 

figure     runtime fss algorithms recommended five classifiers different
values
fig     shows runtime fss algorithms recommended five classifiers
different values five classifiers  observe that 
   increase   average runtime recommended fss algorithms
classifier decreases  note larger value means users favor faster fss algorithms  thus  indicates users performance requirement met since faster fss
algorithms recommended 
   increase   average runtime recommended fss algorithms increases
well  proposed recommendation method  appropriate
fss algorithms given data set recommended based nearest data sets 
moreover  experiment  half  i e           data sets 
negative correlation number selected features runtime
   fss algorithms  thus  data sets kind negative correlation 
possible nearest neighbors given data set negative correlation 
therefore  larger means longer runtime  another possible reason larger
value means users favor fss algorithms choose fewer features  order
get fewer features  fss algorithms need consume relatively time 
c   

part

average number features

average number features

naivebayes

   
  
  
  
  
  
  
 

                                                

ib 

bayes network

   
   
  
  
  
  
 

                                                

 

figure     number features selected fss algorithms recommended
five classifiers different values
fig     shows number features selected fss algorithms recommended
five classifiers different values   observe that 
  

fisubset selection algorithm automatic recommendation

   increase   average number selected features increases well 
proposed recommendation method  appropriate fss algorithms
given data set recommended based nearest data sets  moreover 
experiment  half  i e           data sets  negative correlation number selected features runtime    fss algorithms 
thus  data sets kind negative correlation  possible
nearest neighbors given data set negative correlation  therefore  larger
means features  another possible reason larger value means users
favor faster fss algorithms  possible shorter computation time obtained
via filter less features features remained 
note exception  is  average number selected features
c    decreases value small  however  decrement comes quite
small range  i e            
   increase   average number features selected recommended
fss algorithm decreases  note larger value means users favor fss algorithms
get fewer features  thus  indicates users requirement met since
fss algorithms get fewer features recommended 
naivebayes

c   

part

average hit ratio    

average hit ratio    

ib 

bayes network

   

   
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  

  
 

 

                                                

                                                

 

figure     average hit ratio fss algorithms recommended five classifiers
different values
c   

part
   

    

    

average rpr    

average rpr    

naivebayes
   

  
    
  
    

ib 

bayes network

  
    
  
    
  

  
 

 

                                                

                                                

 

figure     average rpr fss algorithms recommended five classifiers
different values
figs        show average hit ratio rpr recommended fss algorithms
different values five classifiers 
  

fiwang  song  sun  zhang  xu   zhou

observe that  average hit ratio falls intervals               
                    average rpr varies intervals         
                           change   hit
ratio rpr recommended fss algorithms vary well  however  change
intervals fall relative small interval lower bound stands fairly high level 
minimum average hit ratio        minimum average rpr
        indicates proposed fss algorithm recommendation method
general application works well different settings  

   sensitivity analysis number nearest data sets
recommendation results
section  analyze number nearest data sets affects recommendation performance  based experimental results  provide guidelines
selecting appropriate number nearest data sets practice 
    experimental method
generally  different numbers nearest data sets  i e   k  result different recommendations  thus  recommending fss algorithms feature selection problem 
appropriate k value important 
k value results higher recommendation performance preferred  however 
recommendation performance difference two different k values sometimes might
random significant  thus  order identify appropriate k value
alternatives  first determine whether performance differences among
statistically significant  non parametric statistical test  friedman test followed
holm procedure test suggested demsar         used purpose 
experiment  conducted fss algorithm recommendation possible k
values  i e              data sets  identifying appropriate k
values  non parametric statistical test conducted follows 
firstly  friedman test performed     recommendation performance
significance level       null hypothesis     k values perform equivalently
well proposed recommendation method     data sets 
friedman test rejects null hypothesis  is  exists significant difference
among     k values  choose one recommendation best
performance reference  that  holm procedure test performed find
k values recommendation performance significant difference
reference  identified k values including reference appropriate
numbers nearest data sets 
    results analysis
fig     shows number nearest data sets  i e   k  affects performance
recommendation method different settings   denotes
k recommendation performance significantly worse others
significance level       observe that 
  

fisubset selection algorithm automatic recommendation

naive bayes

c   

part

ib 

bayes network

inappropriate number neighbors

 
     

rpr

    
     
    
     
    
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

                              
                                  

number nearest data sets

 a          
naive bayes

c   

part

ib 

bayes network

inappropriate number neighbors

 

rpr

     
    
     
    
     
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

                              
                                  

number nearest data sets

 b              

figure     number nearest data sets vs  rpr
          fig     a    five classifiers  rpr varies
different k values  specifically  rpr fluctuant k smaller    
relatively flat middle part  decreases k larger    except
c     however  increment c    small            might due
c    picks useful features build tree itself  impact feature
selection methods less  moreover  difference among accuracies c   
data sets relatively small  performance metric earr used evaluate
different fss algorithms depends classification accuracy        thus 
rpr c    relatively stable different values k 
   case          fig     b    variation rpr different
       five classifiers  rpr first decreases fluctuations 
increases  finally decreases slowly steadily  could due that 
parameters set relatively large value  such     experiment  
runtime   number features selected by  fss algorithm play
important role evaluating performance fss algorithm  thus 
given data set  fss algorithms lower time complexity  or smaller number
selected features  possibly higher ranked larger rpr  therefore 
increasing k  algorithms possibly recommended  meanwhile 
data sets  algorithms either real appropriate algorithms
larger rpr  rpr averaged data sets relatively stable increasing
k 
  

fiwang  song  sun  zhang  xu   zhou

   comparing cases                found appears
k      former k      latter  emerges k     
former  means cannot choose k values falling ranges 
time  found peak values rpr         appear
range           one peak value ranges       except c    
means set k         number data sets  better recommendation
performance obtained 

   conclusion
paper  presented fss algorithm recommendation method aim
support automatic selection appropriate fss algorithms new feature selection
problem number candidates 
proposed recommendation method consists meta knowledge database construction algorithm recommendation  former obtains meta features performance candidate fss algorithms  latter models relationship
meta features fss algorithm performance based k  nn method recommends appropriate algorithms feature selection problem built model 
thoroughly tested recommendation method     real world data sets    
different fss algorithms  five representative classification algorithms two typical
users performance requirements  experimental results show recommendation
method effective 
conducted sensitivity analysis explore number nearest
data sets  k  impacts fss algorithm recommendation  suggest set k    
    number historical data sets 
paper  utilized well known commonly used meta features
characterize different data sets  meta features informative 
informative meta features  still open questions  knowledge 
still exist effective method answer questions  thus  future
work  plan explore measure information meta features
whether informative meta features lead improvements fss algorithm recommendation 

acknowledgements
work supported national natural science foundation china grant
         

references
aha  d  w   kibler  d     albert  m  k          instance based learning algorithms  machine learning              
ali  s     smith  k  a          learning algorithm selection classification  applied
soft computing                
  

fisubset selection algorithm automatic recommendation

atkeson  c  g   moore  a  w     schaal  s          locally weighted learning  artificial
intelligence review               
battiti  r          using mutual information selecting features supervised neural net
learning  ieee transactions neural networks                
brazdil  p   carrier  c   soares  c     vilalta  r          metalearning  applications data
mining  springer 
brazdil  p  b   soares  c     da costa  j  p          ranking learning algorithms  using ibl
meta learning accuracy time results  machine learning                 
brodley  c  e          addressing selective superiority problem  automatic algorithm model class selection  proceedings tenth international conference
machine learning  pp        citeseer 
castiello  c   castellano  g     fanelli  a          meta data  characterization input
features meta learning  modeling decisions artificial intelligence         
dash  m     liu  h          feature selection classification  intelligent data analysis 
              
dash  m     liu  h          consistency based search feature selection  artificial intelligence                    
de souza  j  t          feature selection general hybrid algorithm  ph d  thesis 
university ottawa 
demsar  j          statistical comparisons classifiers multiple data sets  journal
machine learning research         
engels  r     theusinger  c          using data metric preprocessing advice data
mining applications  
frank  e     witten  i  h          generating accurate rule sets without global optimization 
proceedings   th international conference machine learning  pp         
morgan kaufmann  san francisco  ca 
friedman  m          use ranks avoid assumption normality implicit
analysis variance  journal american statistical association           
       
friedman  n   geiger  d     goldszmidt  m          bayesian network classifiers  machine
learning                 
gama  j     brazdil  p          characterization classification algorithms  progress
artificial intelligence         
garcia lopez  f   garcia torres  m   melian batista  b   moreno perez  j  a     morenovega  j  m          solving feature subset selection problem parallel scatter
search  european journal operational research                  
goldberg  d  e          genetic algorithms search  optimization  machine learning 
addison wesley professional 
  

fiwang  song  sun  zhang  xu   zhou

gutlein  m   frank  e   hall  m     karwath  a          large scale attribute selection
using wrappers  proceedings ieee symposium computational intelligence
data mining  pp          ieee 
guyon  i     elisseeff  a          introduction variable feature selection 
journal machine learning research              
hall  m   frank  e   holmes  g   pfahringer  b   reutemann  p     witten  i         
weka data mining software  update  acm sigkdd explorations newsletter         
     
hall  m  a          correlation based feature selection machine learning  ph d  thesis 
university waikato 
hedar  a  r   wang  j     fukushima  m          tabu search attribute reduction
rough set theory  soft computing a fusion foundations  methodologies
applications                 
hommel  g          stagewise rejective multiple test procedure based modified
bonferroni test  biometrika                 
john  g  h     langley  p          estimating continuous distributions bayesian classifiers  proceedings eleventh conference uncertainty artificial intelligence 
vol     pp          citeseer 
kalousis  a   gama  j     hilario  m          data algorithms  understanding
inductive performance  machine learning                 
king  r  d   feng  c     sutherland  a          statlog  comparison classification algorithms large real world problems  applied artificial intelligence                
kira  k     rendell  l          practical approach feature selection  proceedings ninth international workshop machine learning  pp          morgan
kaufmann publishers inc 
kohavi  r          study cross validation bootstrap accuracy estimation
model selection  international joint conference artificial intelligence  vol     
pp            citeseer 
kohavi  r     john  g          wrappers feature subset selection  artificial intelligence 
               
kononenko  i          estimating attributes  analysis extensions relief  proceedings european conference machine learning machine learning  pp 
        springer verlag new york 
lee  m   lu  h   ling  t     ko  y          cleansing data mining warehousing 
proceedings   th international conference database expert systems
applications  pp          springer 
lindner  g     studer  r          ast  support algorithm selection cbr approach  principles data mining knowledge discovery         
liu  h   motoda  h   setiono  r     zhao  z          feature selection  ever evolving
frontier data mining  fourth workshop feature selection data
mining  pp       citeseer 
  

fisubset selection algorithm automatic recommendation

liu  h     setiono  r          chi   feature selection discretization numeric attributes  proceedings seventh international conference tools artificial intelligence  pp          ieee 
liu  h     setiono  r          probabilistic approach feature selection a filter solution  
pp          citeseer 
liu  h     yu  l          toward integrating feature selection algorithms classification
clustering  ieee transactions knowledge data engineering             
    
michie  d   spiegelhalter  d  j     taylor  c  c         
statistical classification  

machine learning  neural

molina  l  c   belanche  l     nebot  a          feature selection algorithms  survey
experimental evaluation  proceedings ieee international conference data
mining  pp          ieee 
nakhaeizadeh  g     schnabl  a          development multi criteria metrics evaluation data mining algorithms  proceedings  rd international conference
knowledge discovery data mining  pp       
nakhaeizadeh  g     schnabl  a          towards personalization algorithms evaluation data mining  proceedings  th international conference knowledge
discovery data mining  pp         
pudil  p   novovicova  j     kittler  j          floating search methods feature selection 
pattern recognition letters                    
pudil  p   novovicova  j   somol  p     vrnata  r       a   conceptual base feature
selection consulting system  kybernetika                 
pudil  p   novovicova  j   somol  p     vrnata  r       b   feature selection expertuser
oriented approach  advances pattern recognition         
quinlan  j  r          c     programs machine learning  morgan kaufmann 
robnik sikonja  m     kononenko  i          theoretical empirical analysis relieff
rrelieff  machine learning               
saeys  y   inza  i     larranaga  p          review feature selection techniques
bioinformatics  bioinformatics                    
smith miles  k  a          cross disciplinary perspectives meta learning algorithm
selection  acm computing surveys              
sohn  s  y          meta analysis classification algorithms pattern recognition  ieee
transactions pattern analysis machine intelligence                    
song  q  b   wang  g  t     wang  c          automatic recommendation classification
algorithms based data set characteristics  pattern recognition                   
vilalta  r     drissi  y          perspective view survey meta learning  artificial
intelligence review               
  

fiwang  song  sun  zhang  xu   zhou

wolpert  d  h          supervised learning no free lunch theorems  proceedings
 th online world conference soft computing industrial applications  pp 
      citeseer 
yu  l     liu  h          feature selection high dimensional data  fast correlationbased filter solution  proceedings twentieth international conference
machine leaning  vol      pp         
zhao  z     liu  h          searching interacting features  proceedings   th
international joint conference artifical intelligence  pp            morgan kaufmann publishers inc 
zhou  x     dillon  t          heuristic statistical feature selection criterion inductive machine learning real world  proceedings ieee international
conference systems  man  cybernetics  vol     pp          ieee 

  


