journal of artificial intelligence research               

submitted          published        

a feature subset selection algorithm automatic
recommendation method
guangtao wang
qinbao song
heli sun
xueying zhang

gt wang stu xjtu edu cn
qbsong mail xjtu edu cn
hlsun mail xjtu edu cn
zhangxueying     stu xjtu edu cn

department of computer science   technology 
xian jiaotong university          china

baowen xu
yuming zhou

bwxu nju edu cn
zhouyuming nju edu cn

department of computer science   technology 
nanjing university  china

abstract
many feature subset selection  fss  algorithms have been proposed  but not all of them
are appropriate for a given feature selection problem  at the same time  so far there is
rarely a good way to choose appropriate fss algorithms for the problem at hand  thus 
fss algorithm automatic recommendation is very important and practically useful  in
this paper  a meta learning based fss algorithm automatic recommendation method is
presented  the proposed method first identifies the data sets that are most similar to the
one at hand by the k  nearest neighbor classification algorithm  and the distances among
these data sets are calculated based on the commonly used data set characteristics  then 
it ranks all the candidate fss algorithms according to their performance on these similar
data sets  and chooses the algorithms with best performance as the appropriate ones 
the performance of the candidate fss algorithms is evaluated by a multi criteria metric
that takes into account not only the classification accuracy over the selected features  but
also the runtime of feature selection and the number of selected features  the proposed
recommendation method is extensively tested on     real world data sets with    wellknown and frequently used different fss algorithms for five representative classifiers  the
results show the effectiveness of our proposed fss algorithm recommendation method 

   introduction
feature subset selection  fss  plays an important role in the fields of data mining and
machine learning  a good fss algorithm can effectively remove irrelevant and redundant
features and take into account feature interaction  this not only leads up to an insight
understanding of the data  but also improves the performance of a learner by enhancing the
generalization capacity and the interpretability of the learning model  pudil  novovicova 
somol    vrnata      a  pudil  novovicova  somol    vrnata      b  molina  belanche 
  nebot        guyon   elisseeff        saeys  inza    larranaga        liu   yu       
liu  motoda  setiono    zhao        
c
    
ai access foundation  all rights reserved 

fiwang  song  sun  zhang  xu   zhou

although a large number of fss algorithms have been proposed  there is no single
algorithm which performs uniformly well on all feature selection problems  experiments
 hall        zhao   liu        have confirmed that there could exist significant differences
of performance  e g   classification accuracy  among different fss algorithms over a given
data set  that means for a given data set  some fss algorithms outperform others 
this raises a practical and very important question  which fss algorithms should be
picked up for a given data set  the common solution is to apply all candidate fss algorithms to the given data set  and choose one with the best performance by the crossvalidation strategy  however  this solution is quite time consuming especially for highdimensional data  brodley        
for the purpose of addressing this problem in a more efficient way  in this paper  an fss
algorithm automatic recommendation method is proposed  the assumption underlying our
proposed method is that the performance of an fss algorithm on a data set is related to
the characteristics of the data set  the rationality of this assumption can be demonstrated
as follows 
   generally  when a new fss algorithm is proposed  its performance needs to be extensively evaluated at least on real world data sets  however  the published fss algorithms
are rarely tested on the identical group of data sets  hall        zhao   liu        yu  
liu        dash   liu        kononenko         that is  for any two algorithms  they
are usually tested on the different data  this implies that the performance of an fss
algorithm biases to some data sets 
   at the same time  the famous nfl  no free lunch   wolpert        theory tells us
that  for a particular data set  different algorithms have different data conditioned performance  and the performance differences vary with data sets 
the above evidences imply that there is a relationship between the performance of an
fss algorithm and the characteristics of data sets  in this paper  we intend to explore this
relationship and utilize it to automatically recommend appropriate fss algorithm s  for
a given data set  the recommendation process can be viewed as a specific application of
meta learning  vilalta   drissi        brazdil  carrier  soares    vilalta        that has
been used to recommend algorithms for classification problems  ali   smith        king 
feng    sutherland        brazdil  soares    da costa        kalousis  gama    hilario 
      smith miles        song  wang    wang        
to model this relationship  there are three fundamental issues to be considered  i  which
features  often are referred to as meta features  are used to characterize a data set  ii  how
to evaluate the performance of an fss algorithm and identify the applicable one s  for a
given data set  iii  how to recommend fss algorithm s  for a new data set 
in this paper  the meta features  which are frequently used in meta learning  vilalta  
drissi        ali   smith        king et al         brazdil et al         castiello  castellano 
  fanelli         are employed to characterize data sets  at the same time  a multi criteria
metric  which takes into account not only the classification accuracy of a classifier with an
fss algorithm but also the runtime of feature selection and the number of selected features 
is used to evaluate the performance of the fss algorithm  meanwhile  a k  nn  k  nearest
neighbor  based method is proposed to recommend fss algorithm s  for a new data set 
 

fisubset selection algorithm automatic recommendation

our proposed fss algorithm recommendation method has been extensively tested on
    real world data sets with    well known and frequently used different fss algorithms
for five representative classifiers  the results show the effectiveness of our proposed recommendation method 
the rest of this paper is organized as follows  section   introduces the preliminaries 
section   describes our proposed fss algorithm recommendation method  section   provides the experimental results  section   conducts the sensitivity analysis of the number
of the nearest data sets on the recommendation results  finally  section   summarizes the
work and draws some conclusions 

   preliminaries
in this section  we first describe the meta features used to characterize data sets  then 
we introduce the multi criteria evaluation metric used to measure the performance of fss
algorithms 
    meta features of data sets
our proposed fss algorithm recommendation method is based on the relationship between
the performance of fss algorithms and the meta features of data sets 
the recommendation can be viewed as a data mining problem  where the performance
of fss algorithms and the meta features are the target function and the input variables 
respectively  due to the ubiquity of garbage in  garbage out  lee  lu  ling    ko       
in the field of data mining  the selection of the meta features is crucial for our proposed
fss recommendation method 
the meta features are measures that are extracted from data sets and can be used to
uniformly characterize different data sets  where the underlying properties are reflected  the
meta features should be not only conveniently and efficiently calculated  but also related to
the performance of machine learning algorithms  castiello et al         
there has been    years of research to study and improve on the meta features proposed
in the statlog project  michie  spiegelhalter    taylor         a number of meta features
have been employed to characterize data sets  brazdil et al         castiello et al        
michie et al         engels   theusinger        gama   brazdil        lindner   studer 
      sohn         and have been demonstrated working well in modeling the relationship
between the characteristics of data sets and the performance  e g   classification accuracy  of
learning algorithms  ali   smith        king et al         brazdil et al         kalousis et al  
      smith miles         as these meta features do characterize data sets themselves  and
have no connection with learning algorithms and their types  so we use them to model the
relationship between data sets and fss algorithms 
the most commonly used meta features are established focusing on the following three
aspects of a data set  i  general properties  ii  statistic based properties  and iii  informationtheoretic based properties  castiello et al          table    shows the details 
   in order to compute the information theoretic features  for data sets with continuous valued features  if
needed  the well known mdl  minimum description length  method with fayyad   irani criterion was
used to discretize the continuous values 

 

fiwang  song  sun  zhang  xu   zhou

category
general properties

statistical based properties

information theoretic properties

notation
i
f
t
d
 x  y  
skew x 
kurt x 
h c norm
h x norm
m i c  x 
m i c  x max
enattr
n sratio

measure description
number of instances
number of features
number of target concept values
data set dimensionality  d   i f
mean absolute linear correlation coefficient of all possible pairs of features
mean skewness
mean kurtosis
normalized class entropy
mean normalized feature entropy
mean mutual information of class and attribute
maximum mutual information of class and attribute
equivalent number of features  enattr   h c  m i c  x 
noise signal ratio  n sratio    h x   m i c  x   m i c  x 

table    meta features used to characterize a data set
    multi criteria metric for fss algorithm evaluation
in this section  first  the classical metrics evaluating the performance of fss algorithm are
introduced  then  by analyzing the user requirements in practice application  based on
these metrics  a new and user oriented multi criteria metric is proposed for fss algorithm
evaluation by combining these metrics together 
      classical performance metrics
when evaluating the performance of an fss algorithm  the following three metrics are
extensively used in feature selection literature  i  classification accuracy   ii  runtime of
feature selection  and iii  number of selected features 
   the classification accuracy  acc  of a classifier with the selected features can be used to
measure how well the selected features describe a classification problem  this is because
for a given data set  different feature subsets generally result in different classification
accuracies  thus  it is reasonable that the feature subset with higher classification accuracy has stronger capability of depicting the classification problem  the classification
accuracy also reflects the ability of an fss algorithm in identifying the salient features
for learning 
   the runtime  t  of feature selection measures the efficiency of an fss algorithm for
picking up the useful features  it is also viewed as a metric to measure the cost of feature
selection  the longer the runtime  the higher the expenditure of feature selection 
   the number of selected features  n  measures the simplicity of the feature selection
results  if the classification accuracies with two fss algorithms are similar  we usually
favor the algorithm with fewer features 
feature subset selection aims to improve the performance of learning algorithms which
usually is measured with classification accuracy  the fss algorithms with higher classification accuracy are in favor  however  this does not mean that the runtime and the
number of selected features could be ignored  this can be explained by the following two
considerations 
   suppose there are two different fss algorithms ai and aj   and a given data set d  if
the classification accuracy with ai on d is slightly greater than that with aj   but the
 

fisubset selection algorithm automatic recommendation

runtime of ai and the number of features selected by ai are much greater than those of
aj   then aj is often chosen 
   usually  we do not prefer to use the algorithms with higher accuracy but longer runtime 
so is those with lower accuracy but shorter runtime  therefore  we need a tradeoff between classification accuracy and the runtime of feature selection the number of selected
features  for example  in real time systems  it is impossible to choose the algorithm with
high time consumption even if its classification accuracy is high 
therefore  it is necessary to allow users making a user oriented performance evaluation
for different fss algorithms  for this purpose  it is needed to address the problem of how
to integrate classification accuracy with the runtime of feature selection and the number of
selected features to obtain a unified metric  in this paper  we resort to the multi criteria
metric to explore this problem  the underlying reason lies that the multi criteria metric
has been successfully used to evaluate data mining algorithms by considering the positive
properties  e g  classification accuracy  and the negative ones  e g  runtime and number of
selected features  simultaneously  nakhaeizadeh   schnabl              
when comparing two algorithms  besides the metrics used to evaluate their performance 
the ratio of the metric values can also be used  for example  suppose a  and a  are two
different fss algorithms  if a  is better than a  in terms of classification accuracy  i e  
acc    acc      then ratio acc   acc      can be used to show a  is better than a  as well 
on the contrary  for the negative metrics runtime of feature selection and number of of the
selected features  the corresponding ratio     means a better algorithm 
actually  a multi criteria metric adjusted ratio of ratios  arr   brazdil et al         
which combines classification accuracy and runtime together as a unified metric  has been
proposed to evaluate the performance of a learning algorithm  we extend arr by integrating it with the runtime of feature selection and the number of selected features  so a new
multi criteria metric earr  extened arr  is proposed  in the following discussion  we will
show that the new metric earr is more inclusive  very flexible  and easy to understand 
      multi criteria metric earr
let dset    d    d         dn   be a set of n data sets  and aset    a    a         am   be a
set of m fss algorithms  suppose accji is the classification accuracy of a classifier with fss
algorithm ai on data set dj     i  m      j  n    and tji and nji denote the runtime
and the number of selected features of fss algorithm ai on data set dj   respectively  then
the earr of ai to aj over dk is defined as
k
earrd
ai  aj  

accki  acckj
      log  tki  tkj       log  nki  nkj  

    i    j  m     k  n   

   

where  and  are the user predefined parameters which denote the relative importance of
the runtime of feature selection and the number of selected features  respectively 
the computation of the metric earr is based on the ratios of the classical fss algorithm performance metrics  the classification accuracy  the runtime of feature selection and
   where acc  and acc  are the corresponding classification accuracies of algorithms a  and a    respectively 

 

fiwang  song  sun  zhang  xu   zhou

the number of selected features  from the definition we can know that earr evaluates an
fss algorithm by comparing it with another algorithm  this is reasonable since it is more
objective to assert an algorithm is good or not by comparing it with another one instead of
just focusing on its own performance  for example  suppose there is a classifier with    
classification accuracy on a data set  we will get confused on whether the classifier is good
or not  however  if we compare it with another classifier that can obtain     classification
accuracy on the same data set  then we can definitely say that the first classifier is not good
compared with the second one 
noted that  in practice  the runtime difference between two different fss algorithms
usually can be quite great  meanwhile  for high dimensional data sets  the difference of the
number of selected features for two different fss algorithms can be great as well  thus 
the ratio of runtime and the ratio of the number of selected features usually have much
more wide ranges than that of the classification accuracy  if the simple ratio of runtime and
the simple ratio of the number of selected features are employed  they would dominate the
value of earr  and the ratio of the classification accuracy will be drowned  in order to
avoid this situation  the common logarithm  i e   the logarithm with base     of the ratio
of runtime and the common logarithm of the ratio of the number of selected features are
employed 
the parameters  and  represent the amount of classification accuracy that a user is
willing to trade for a    times speedup reduction on the runtime of feature selection the
number of selected features  respectively  this allows users to choose the algorithms with
shorter runtime and less features but acceptable accuracy  this can be illustrated by the
following example  suppose that accki              acckj   the runtime of algorithm ai on
a given data set is    times of that of aj  i e   tki       tkj    and the number of selected
features of algorithm ai is    times of that of aj  i e   nki       nkj    then  according to
dk
 
 
k
eq       earr d
ai  aj      and earr aj  ai              in this case  aj outperforms
ai   if a user prefers fast algorithms with less features  aj will be his her choice 
k
the value of earr varies around    the value of earr d
ai  aj is greater than  or equal
k
to  or smaller than  that of earr d
aj  ai indicates that ai is better than  or equal to  or
worse than  aj  
eq      can be directly used to evaluate the performance of two different fss algorithms 
when comparing multiple fss algorithms  the performance of any algorithm ai  aset on
a given data set d can be evaluated by the metric earr d
ai defined as follows 

earrd
ai  

 
m  

m
x

earrd
ai  aj  

   

j  j  i

this equation shows that the earr of an fss algorithm ai on d is the arithmetic
mean of the earrd
ai  aj of ai to other algorithm aj on d  that is  the performance of any
fss algorithm ai  aset is evaluated based on the comparisons with other algorithms in
 aset  ai     the larger the value of earr  the better the corresponding fss algorithm
on the given data set d 
   since log  x y     log  y x  and           

 

fisubset selection algorithm automatic recommendation

   fss algorithm recommendation method
in this section  we first give the framework of the fss algorithm recommendation  then 
we introduce the nearest neighbor based recommendation method in detail 
    framework
based on the assumption that there is a relationship between the performance of an fss
algorithm on a given data set and the data set characteristics  aka meta features   our
proposed recommendation method firstly constructs a meta knowledge database consisting
of data set meta features and fss algorithm performance  after that  with the help of
the meta knowledge database  a k nn based method is used to model this relationship and
recommend appropriate fss algorithms for a new data set 
therefore  our proposed recommendation method consists of two parts  meta knowledge
database construction and fss algorithm recommendation  fig    shows the details 
meta knowledge database construction
feature selection
algorithms

historical
data sets

performance metric
aquirement
meta features
extraction

performance metrics

meta features

fss algorithm recommendation

new data set
recommended
fss algorithms

metaknowledge
database
performance
metrics

meta features

meta features
extraction

meta features

nearest data sets
identification

top r algorithms
recommendation

ranks

fss algorithms
ranking

nearest
data sets

metric
collection

performance
metrics

figure    framework of feature subset selection algorithm recommendation
   meta knowledge database construction
as mentioned previously  the meta knowledge database consists of the meta features of
a set of historical data sets and the performance of a group of fss algorithms on them 
it is the foundation of our proposed recommendation method  and the effectiveness of the
recommendations depends heavily on this database 
the meta knowledge database is constructed by the following three steps  firstly  the
meta features in table   are extracted from each historical data set by the module metafeatures extraction  then  each candidate fss algorithm is applied on each historical data
set  the classification accuracy  the runtime of feature selection and the number of selected
features are recorded  and the corresponding value of the performance metric earr is
calculated  this is accomplished by the module performance metric calculation  finally 
for each data set  a tuple  which is composed of the meta features and the values of the
performance metric earr for all the candidate fss algorithms  is obtained and added into
the knowledge database 
   fss algorithm recommendation
 

fiwang  song  sun  zhang  xu   zhou

based on the introduction of the first part meta knowledge database construction
we presented above  the learning target of the meta knowledge data is a set of earr values
instead of an appropriate fss algorithm  in this case  it has been demonstrated that the
researchers usually resort to the instance based or k  nn  nearest neighbors  methods or
their variations  brazdil et al               for algorithm recommendation  thus  a k  nn
based fss algorithm recommendation procedure is proposed 
when recommending fss algorithms for a new data set  firstly  the meta features of
this data set are extracted  then  the distance between the new data set and each historical
data set is calculated according to the meta features  after that  its k nearest data sets
are identified  and the earr values of the candidate fss algorithms on these k data sets
are retrieved from the meta knowledge database  finally  all the candidate fss algorithms
are ranked according to these earr values  where the algorithm with the highest earr
achieves the top rank  the one with the second highest earr gets second rank  and so
forth  and the top r algorithms are recommended 
    recommendation method
to recommend appropriate fss algorithms for a new data set dnew based on its k nearest
data sets  there are two foundational issues to be solved  i  how to identify its k nearest
data sets  and ii  how to recommend appropriate fss algorithms based on these k data
sets 
   k nearest data sets identification
the k nearest data sets of dnew are identified by calculating the distance between dnew
and each historical data set based on their meta features  the smaller the distance  the
more similar the corresponding data set to dnew  
in order to effectively calculate the distance between two data sets  the l  norm distance
 atkeson  moore    schaal        is adopted since it is easy to understand and calculate 
and its ability in measuring the similarity between two data sets has been demonstrated by
brazdil et al         
let fi    fi     fi          fi h   be the meta features of data set di   where fi p is the
value of pth feature of fi and h is the length of the meta features  the l  norm distance
between data sets di and dj can be formulated as
dist di   dj     kfi  fj k   

h
x

 fi p  fj p   

   

p  

it is worth noting that the ranges of different meta features are quite different  for example 
of the meta features introduced in table    the value of normalized class entropy varies from
  to    while the number of instances can be millions  thus  if these meta features with
different ranges are directly used to calculate the distance between two data sets  the metafeatures with large range would dominate the distance  and the meta features with small
range will be ignored  in order to avoid this problem  the     standardized method  eq 
     is employed to make all the meta features have the same range        
fi p  min  f p  
 
max  f p    min  f p  
 

   

fisubset selection algorithm automatic recommendation

where fi p     p  h  is the value of the pth meta feature of data set di   min  f p   and
max  f p   denote the minimum and maximum values of the pth meta feature over historical
data sets  respectively 
   fss algorithm recommendation
once getting the k nearest data sets of dnew   the performance of the candidate fss
algorithms on dnew is evaluated according to those on the k nearest data sets  then  the
algorithms with the best performance are recommended 
d
let dknn    d    d         dk   be the k nearest data sets of dnew and earr aij be the
performance metric of the fss algorithm ai on data set dj  dknn     j  k   then the
performance of ai on the new data set dnew can be evaluated by
knn
earrd
ai

 

k
x

j 

d
earraij  

where j   dj

 

k
x
dt     dj   dist dnew   dj   
 

   

t  

j  

eq      indicates that the performance of the fss algorithm ai on dnew is evaluated by
its performance on the dknn of dnew   for a data set dj  dknn   the smaller the distance
dj between itself and dnew   the more similar the two data sets  this means for two data
sets dp and dq   if dp   dq then the data set dp is more similar to dnew   so the earr
of ai on dp is more important for evaluating the performance of ai on dnew   thus  the
weighted average  which takes into account the relative importance of each data set in dknn
rather than treating each data set equally  is employed  moreover  in the domain of machine
learning  the reciprocal of the distance usually is used to measure the similarity  so the
k
p
j   dj     dt   is used as the weight of the earr of ai on dj  dknn  
t  

according to the earr of each candidate fss algorithm in aset on dnew   a rank of
these candidate fss algorithms can be obtained  the greater the earr  the higher the
rank  then  the top r  e g   r     in this paper  fss algorithms are picked up as the
appropriate ones for the new data set dnew  
procedure fssalgorithmrecommendation shows the pseudo code of the recommendation 
time complexity  the recommendation procedure consists of two parts  in the first part
 lines       the k nearest data sets of the given new data set d are identified  firstly 
the meta features f of d are extracted by function metafeatureextraction    then  the
k nearest historical data sets are identified by function neighborrecognition   based on
the distance between f and the meta features fi of each historical data set di   suppose
that p is the number of instances and q is the number of features in the given data set
d  the time complexity of function metafeatureextraction   is o p   q   for function
neighborrecognition    the time complexity is o n  which depends on the number of the
historical data sets n  consequently  the time complexity of the first part is o p  q  o n  
in the second part  lines       the r fss algorithms are recommended for the data set
d  since the weights and earrs of the k nearest data sets can be obtained directly  the
time complexity of these two steps is o     the time complexity for estimating and ranking
the earrs of the algorithms in aset is o k  m    o m  log m    where k is the preassigned
number of the nearest data sets and m is the number of the candidate fss algorithms 
 

fiwang  song  sun  zhang  xu   zhou

procedure fssalgorithmrecommendation
inputs  
d   a new given data set 
dset    d    d         dn    historical data sets 
aset    a    a         am    candidate fss algorithms 
metadatabase     fi   earrsi      i  n  where fi and earrs i are the
meta features and the earrs of aset on di   respectively 
k   the predefined number of the nearest neighbors 
r   the number of recommended fss algorithms 
output  recalgs   recommended fss algorithms for d
  part    recognition of the k nearest data sets for d
  f   metafeatureextraction  d    extract meta features from d
  metafeatureset    f    f         fn     meta feature of each data set in dset
  neighbors   neighborrecognition  k  f  metafeatureset  
  part    appropriate fss algorithm recommendation
  weightset   calculate the weight for each data set in neighbors   see eq     
  earrset   the corresponding earrs for each data set in neighbors from metadatabase 
  estimate the earr of each fss algorithm  aset on d according to weightset and earrset

by eq      and rank the algorithms in aset based on these earrs 
  recalgs   top r fss algorithms 
  return recalgs 

to sum up  the time complexity of the recommendation procedure is o p   q    o n   
o km  o mlog m    in practice  for a data set d that needs to conduct feature selection 
the number of the instances p and or the number of the features q in d are much greater
than the number of the nearest data sets k and the number of the candidate fss algorithms
m  so the major time consumption of this recommendation procedure is determined by the
first part 

   experimental results and analysis
in this section  we experimentally evaluate the proposed feature subset selection  fss 
algorithm recommendation method by recommending algorithms over the benchmark data
sets 
    benchmark data sets
    extensively used real world data sets  which come from different areas such as computer 
image  life  biology  physical and text     are employed in our experiment  the sizes of
these data sets vary from    to       instances  and the numbers of their features are
between   and       
the statistical summary of these data sets is shown in table   in terms of the number
of instances  denoted as i   the number of features  denoted as f  and the number of target
concepts  denoted as t  
   these data sets are available from http   archive ics uci edu ml datasets html  http   
featureselection asu edu datasets php  http   sci s ugr es keel datasets php  http   www 
upo es eps bigs datasets html  and http   tunedit org repo data  respectively 

  

fisubset selection algorithm automatic recommendation

data id
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

data name
ada agnostic
ada prior
anneal
anneal orig
ar  p        
arrhythmia
audiology
autos
balance scale
breast cancer
breast w
bridges version 
bridges version 
car
cll sub             
cmc
colic
colic orig
colon
credit a
credit g
cylinder bands
dermatology
diabetes
ecml  x     
ecoli
embryonaldataset c
eucalyptus
flags
gcm test
gina agnostic
gina prior
gina prior 
glass
grub damage
heart c
heart h
heart statlog
hepatitis
hypothyroid
ionosphere
iris
kdd ipums la    small
kdd ipums la    small
kdd ipums la    small
kdd japanesevowels test
kdd japanesevowels train
kdd synthetic control
kr vs kp
labor
leukemia
leukemia  c
leukemia test   x    
leukemia train   x    
lung cancer
lymph
lymphoma  x      classes
lymphoma  x       classes

i
    
    
   
   
   
   
   
   
   
   
   
   
   
    
   
    
   
   
  
   
    
   
   
   
  
   
  
     
   
  
    
    
    
   
   
   
   
   
   
    
   
   
    
    
    
    
    
   
    
  
  
  
  
  
  
   
  
  

f
  
  
  
  
   
   
  
  
 
  
  
  
  
 
    
  
  
  
    
  
  
  
  
 
     
 
    
   
  
     
   
   
   
  
 
  
  
  
  
  
  
 
  
  
  
  
  
  
  
  
    
    
    
    
  
  
    
    

t
 
 
 
 
  
  
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
  
 
  
 
 
  
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
  

data id
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

data name
lymphoma  x      classes
mfeat fourier
mfeat morphological
mfeat pixel
mfeat zernike
molecular biology promoters
monks problems   test
monks problems   train
monks problems   test
monks problems   train
monks problems   test
monks problems   train
mushroom
oh  wc
oh   wc
oh   wc
oh  wc
pasture
pendigits
pie  p         
postoperative patient data
primary tumor
segment
shuttle landing control
sick
smk can             
solar flare  
solar flare  
sonar
soybean
spectf test
spectf train
spectrometer
spect test
spect train
splice
sponge
squash stored
squash unstored
sylva agnostic
sylva prior
tox             
tr   wc
tr   wc
tr   wc
tr   wc
tr   wc
tr   wc
trains
vehicle
vote
vowel
wap wc
waveform     
white clover
wine
zoo

i
  
    
    
    
    
   
   
   
   
   
   
   
    
    
    
   
   
  
     
   
  
   
    
  
    
   
   
    
   
   
   
  
   
   
  
    
  
  
  
     
     
   
   
   
   
   
   
   
  
   
   
   
    
    
  
   
   

f
    
  
 
   
  
  
 
 
 
 
 
 
  
    
    
    
    
  
  
    
 
  
  
 
  
    
  
  
  
  
  
  
   
  
  
  
  
  
  
   
   
    
    
    
    
     
    
    
  
  
  
  
    
  
  
  
  

t
 
  
  
  
  
 
 
 
 
 
 
 
 
  
  
  
  
 
  
  
 
  
 
 
 
 
 
 
 
  
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
 
 
 
  
  
 
 
 
 

table    statistical summary of the     data sets

    experimental setup
in order to evaluate the performance of the proposed fss algorithm recommendation
method  further verify whether the proposed method is potentially useful in practice  and
confirm the reproducibility of our experiments  we set the experimental study as follows 
      fss algorithms
fss algorithms can be grouped into two broad categories  wrapper and filter  molina et al  
      kohavi   john         the wrapper method uses the error rate of the classification
algorithm as the evaluation function to measure a feature subset  while the evaluation
function of the filter method is independent of the classification algorithm  the accuracy
of the wrapper method is usually high  however  the generality of the result is limited 
and the computational complexity is high  in comparison  filter method is of generality 
and the computational complexity is low  due to the fact that the wrapper method is
computationally expensive  dash   liu         the filter method is usually a good choice
  

fiwang  song  sun  zhang  xu   zhou

when the number of features is very large  thus  we focus on the filter method in our
experiment 
a number of filter based fss algorithms have been proposed to handle feature selection
problems  these algorithms can be significantly distinguished by i  the search method used
to generate the feature subset being evaluated  and ii  the evaluation measures used to assess
the feature subset  liu   yu        de souza        dash   liu        pudil  novovicova 
  kittler        
in order to guarantee the generality of our experimental results  twelve well known or
the latest search methods and four representative evaluation measures are employed  the
brief introduction of these search methods and evaluation measures is as follows 
   search methods
i  sequential forward search  sfs   starting from the empty set  sequentially add the
feature which results in the highest value of objective function into the current
feature subset 
ii  sequential backward search  sbs   starting from the full set  sequentially eliminate
the feature which results in smallest or no decrease in the value of objective function
from the current feature subset 
iii  bi direction search  bis   a parallel implementation of sfs and sbs  it searches
the feature subset space in two directions 
iv  genetic search  gs   a randomized search method which performs using a simple
genetic algorithm  goldberg         the genetic algorithm finds the feature subset
to maximize special output function using techniques inspired by natural evolution 
v  linear search  ls   an extension of bestfirst search  gutlein  frank  hall    karwath        which searches the space of feature subsets by greedy hill climbing
augmented with a backtracking facility 
vi  rank search  rs   battiti         it uses a feature evaluator  such as gain ratio 
to rank all the features  after a feature evaluator is specified  a forward selection
search is used to generate a ranking list 
vii  scatter search  ss   garcia lopez  garcia torres  melian batista  moreno perez 
  moreno vega         this method performs a scatter search through the feature
subset space  it starts with a population of many significant and diverse feature
subsets  and stops when the assessment criteria is higher than a given threshold or
does not have improvement any longer 
viii  stepwise search  sws   kohavi   john         a variation of the forward search 
at each step in the search process  after a new feature is added  a test is performed
to check if some features can be eliminated without significant reduction in the
output function 
ix  tabu search  ts   hedar  wang    fukushima         it is proposed for combinatorial optimization problems  it is an adaptive memory and responsive exploration
by combining a local search process with anti cycling memory based rules to avoid
trapping in local optimal solutions 
  

fisubset selection algorithm automatic recommendation

x  interactive search  zhao   liu         it traverses the feature subset space for
maximizing the target function while taking consideration the interaction among
features 
xi  fcbf search  yu   liu         it evaluates features via the relevance and redundancy analysis  and uses the analysis results as guideline to choose features 
xii  ranker  kononenko        kira   rendell        liu   setiono         it evaluates
each feature individually and ranks the features by the values of their evaluation
metrics 
   evaluation measures
i  consistency  liu   setiono        zhao   liu         this kind of measure evaluates the worth of a feature subset by the level of consistency in the target concept
when the instances are projected onto the feature subset  the consistency of any
feature subset can never be lower than that of the full feature set 
ii  dependency  hall        yu   liu         this kind of measure evaluates the worth
of a subset of features by considering the individual predictive ability of each feature
along with the degree of redundancy among these features  the fss methods based
on this kind of measure assume that good feature subsets contain features closely
correlated with the target concept  but uncorrelated with each other 
iii  distance  kira   rendell        kononenko         this kind of measure is proposed based on the assumption that the distance of instances from different target
concepts is greater than that from same target concepts 
iv  probabilistic significance  zhou   dillon        liu   setiono         this measure
evaluates the worth of a feature by calculating the probabilistic significance as a
two way function  i e   the association between feature and the target concept  a
good feature should have significant association with the target concept 
we should pay attention to that  besides the above four evaluation measures  there is
another basic kind of measure  information based measure  liu   yu        de souza       
dash   liu         which is not contemplated in the experiments  the reason is demonstrated as follow  the information based measure is usually in conjunction with ranker
search method  thus  the fss algorithms based on this kind of measure usually provide a
rank list of the features instead of telling us which features are relevant to the learning target  in this case  we should preassign particular thresholds for these fss algorithms to pick
up the relevant features  however  there is not any effective method to set the thresholds
or any acknowledged default threshold for these fss algorithms  moreover  it is unfair to
conclude that these information measure based fss algorithms with any assigned threshold
are not appropriate when comparing to the other fss algorithms  therefore  this kind of
fss algorithm is not employed in our experiments 
based on the search methods and the evaluation measures introduced above     different
fss algorithms are obtained  table   shows the brief introduction of these fss algorithms 
where all these algorithms are available in the data mining toolkit weka   hall  frank 
   http   www cs waikato ac nz ml weka 

  

fiwang  song  sun  zhang  xu   zhou

holmes  pfahringer  reutemann    witten         and the search method interact is
implemented based on weka and its source codes are available online   
id
 
 
 
 
 
 
 
 
 
  
  

search method
bestfirst   sequential forward search
bestfirst   sequential backward search
bestfirst   bi direction search
genetic search
linear search
rank search
scatter search
stepwise search
tabu search
interactive search
bestfirst   sequential forward search

evaluation measure
dependency
dependency
dependency
dependency
dependency
dependency
dependency
dependency
dependency
dependency
consistency

notation
cfs sfs
cfs sbs
cfs bis
cfs gs
cfs ls
cfs rs
cfs ss
cfs sws
cfs ts
interact d
cons sfs

id
  
  
  
  
  
  
  
  
  
  
  

search method
bestfirst   sequential backward search
bestfirst   bi direction search
genetic search
linear search
rank search
scatter search
stepwise search
interactive search
fcbfsearch
ranker
ranker

evaluation measure
consistency
consistency
consistency
consistency
consistency
consistency
consistency
consistency
dependency
distance
probabilistic significance

notation
cons sbs
cons bis
cons gs
cons ls
cons rs
cons ss
cons sws
interact c
fcbf
relief f
signific

table    introduction of the    fss algorithms
it is noted that some of these algorithms require particular settings of certain parameters  for the purpose of allowing other researchers to confirm our results  we introduce the
parameter settings of these fss algorithms  such as  for fss algorithms interact d
and interact c  there is a parameter  c contribution threshold  used to identify the
irrelevant features  we set this threshold as        suggested by zhao and liu         for
fss algorithm fcbf  we set the relevance threshold to be the su  symmetric uncertainty  value of the bn  log n cth ranked feature suggested by yu and liu         for fss
algorithm relief f  we set the significance threshold to      used by robnik sikonja and
kononenko         for fss algorithm signific  there is a threshold  statistical significance level   used to identify the irrelevant features  we set  as the commonly used value
     in our experiment  the other fss algorithms are conducted in the weka environment
with the default setting s  
      classification algorithms
since the actual relevant features of real world data sets are usually not known in advance  it
is impracticable to directly evaluate an fss algorithm by the selected features  classification
accuracy is an extensively used metric for evaluating the performance of fss algorithms 
and also plays an important role in our proposed performance metric earr for assessing
different fss algorithms 
however  different classification algorithms have different biases  an fss algorithm may
be more suitable for some classification algorithms than others  de souza         this fact
affects the performance evaluation of fss algorithms 
with this in mind  in order to demonstrate that our proposed fss algorithm recommendation method is not limited to any particular classification algorithm  five representative
classification algorithms based on different hypotheses are employed  they are bayes based
naive bayes  john   langley        and bayes network  friedman  geiger    goldszmidt 
       information gain based c     quinlan         rule based part  frank   witten 
       and instance based ib   aha  kibler    albert         respectively 
although naive bayes and bayes net are both bayes based classification algorithms 
they are quite different from each other since naive bayes is proposed based on the hypoth   http   www public asu edu huanliu interact interactsoftware html

  

fisubset selection algorithm automatic recommendation

esis that the features are conditional independent  john   langley         while bayes net
takes into account the feature interaction  friedman et al         
      measures to evaluate the recommendation method
fss algorithm recommendation is an application of meta learning  so far as we know 
there are no unified measures to evaluate the performance of the meta learning methods 
in order to assess our proposed fss algorithm recommendation method  two measures 
recommendation hit ratio and recommendation performance ratio  are defined 
let d be a given data set and arec be an fss algorithm recommended by the recommendation method for d  these two measures can be introduced as follows 
   recommendation hit ratio
an intuitive evaluation criterion is whether the recommended fss algorithm arec meets
users requirements  that is  whether arec is the optimal fss algorithm for d  or the performance of arec on d has no significant difference with that of the optimal fss algorithm 
suppose aopt represents the optimal fss algorithm for d  and asetopt denotes the fss
algorithm set in which each algorithm has no significant difference with aopt  of course it
includes aopt as well   then  a measure named recommendation hit can be defined to assess
whether the recommended algorithm arec is effective on d 
definition    recommendation hit   if an fss algorithm arec is recommended to a data
set d  then the recommendation hit hit arec   d  is defined as
 
   if arec  asetopt
hit arec   d   
 
   
   otherwise
where hit arec   d      means the recommendation is effective since the recommended
fss algorithm arec is one of the algorithms in asetopt for d  while hit arec   d      indicates
the recommended fss algorithm arec is not a member of asetopt   i e   arec is significantly
worse than the optimal fss algorithm aopt on d  thus the recommendation is bad 
from definition   we know that the recommendation hit hit arec   d  is used to evaluate
the recommendation method for a single data set  thus  it is extended as recommendation
hit ratio to evaluate the recommendation for a set of data sets  and is defined as follows 
definition   recommendation hit ratio
g

  x
hit ratio arec    
hit arec   di   
g

   

i  

where g is the number of the historical data sets  e g   g       in our experiment 
definition   represents the percentage of data sets on which the appropriate fss algorithms are effectively recommended by our recommendation method  the larger this value 
the better the recommendation method 
   recommendation performance ratio
the recommendation hit ratio reveals that whether or not an appropriate fss algorithm
is recommended for a given data set  but it cannot tell us the margin of the recommended
algorithm to the best one  thus  a new measure  the recommendation performance ratio
for a recommendation  is defined 
  

fiwang  song  sun  zhang  xu   zhou

definition    recommendation performance ratio   let earrrec and earropt be the
performance of the recommended fss algorithm arec and the optimal fss algorithm on d 
respectively  then  the recommendation performance ratio  rpr  for arec is defined as
rpr arec   d    earrrec  eaaropt  

   

in this definition  the best performance earr opt is employed as a benchmark  without
the benchmark  it is hard to determine the recommended algorithms are good or not 
for example  suppose the earr of arec on d is       if earr opt         then the
recommendation is effective since earr of arec is very close to earr opt   however  the
recommendation is poor if earr opt        
rpr is the ratio of earr of a recommended fss algorithm to that of the optimal one 
it measures how close the recommended fss algorithm to the optimal one  and reveals the
relative performance of the recommended fss algorithm  its value varies from   to    and
the larger the value of rpr  the closer the performance of the recommended fss algorithm
to that of the optimal one  the recommended algorithm is the optimal one if and only if
rpr     
      values of the parameters  and 
in this paper  a multi criteria metric earr is proposed to evaluate the performance of an
fss algorithm  for the proposed metric earr  two parameters  and  are established
for users to express their requirements on algorithm performance 
in the experiment  when presenting the results  two representative value pairs of parameters  and  are used as follows 
        and       this setting represents the situation where the classification
accuracy is most important  the higher the classification accuracy over the selected features 
the better the corresponding fss algorithms 
         and        this setting represents the situation where the user can tolerate
an accuracy attenuation and favor the fss algorithms with shorter runtime and fewer
selected features  in the experiment  both  and  are set to     that is quite different
from          this allows us can explore the impact of these two parameters on the
recommendation results 
moreover  in order to explore how parameters  and  affect the recommended fss
algorithms in terms of classification accuracy  runtime and the number of selected features 
different parameters settings are provided  specifically  the values of  and  vary from  
to     with an increase of    
    experimental process
in order to make sure the soundness of our experimental conclusion and guarantee the
experiments reported being reproducible  in this part  we introduce the four crucial processes
used in our experiments  they are i  meta knowledge database construction  ii  optimal
fss algorithm set identification for a given data set  iii  recommendation method validation
and iv  sensitivity analysis of the number of the nearest data sets on recommendations 
   meta knowledge database construction
  

fisubset selection algorithm automatic recommendation

procedure performanceevaluation
inputs   data   a given data set  i e  one of the     data sets 
learner   a given classification algorithm  i e   one of  naive bayes  c     part 
ib  or bayes network  
fssalgset    fssalg     fssalg          fssalg       the set of the    fss
algorithms 
output  earrset    earr     earr          earr       the earrs of the    fss
algorithms on data 
  m      folds      
  for i     to    do
 
earr i     
  for i     to m do
 
randomized order from data 
 
generate folds bins from data 
 
for j     to folds do
 
testdata   bin j   
 
traindata   data  testdata 
  
numberlist   null   runtimelist   null   accuracylist   null  
  
for k     to    do
  
 subset  runtime    apply fssalg k on traindata 
  
number    subset   
  
redtestdata   reduce testdata according to selected subset 
  
redtraindata   reduce traindata according to selected subset 
  
classifier   learner  redtraindata  
  
accuracy   apply classifier to redtestdata 
  
numberlist  k     number   runtimelist  k     runtime  accuracylist  k    

accuracy 
  
  
  

for k     to    do
earr   earrcompution accuracylist  runtimelist  numberlist  k   
  compute earr of fssalg k on jth bin of pass i according eqs      and    
earr k   earr k   earr 

   for i    to    do
  
earr i   earr i   m folds   
   return earrset 

for each data set di     i        we i  extract its meta features fi   ii  calculate the
earrs for the    candidate fss algorithms with the stratified     fold cross validation
strategy  kohavi         and iii  combine the meta features fi and the earr of each fss
algorithm together to form a tuple  which is finally added to the meta knowledge database 
since the extraction of meta features and the combination of the meta features and the
earrs are straightforward  we just present the calculation of earrs  procedure performanceevaluation shows the details 
   optimal fss algorithm set identification
the optimal fss algorithm set for a given data set di consists of the optimal fss algorithm for this data set and those algorithms that have no significant performance difference
with the optimal one on di  
the optimal fss algorithm set for a given data set di is obtained via a non parametric
friedman test        followed by a holm procedure test        on the performance  which
  

fiwang  song  sun  zhang  xu   zhou

is estimated by the     cross validation strategy  of the    fss algorithms  if the result
of the friedman test shows that there is no significant performance difference among the
   fss algorithms  these    fss algorithms are added to the optimal fss algorithm set 
otherwise  the fss algorithm with the highest performance is viewed as the optimal one
and added to the optimal fss algorithm set  then  the holm procedure test is performed
to identify the algorithms from the rest    fss algorithms  the algorithms that have no
significant performance differences with the optimal one are added into the optimal fss
algorithm set 
the reason why the non parametric test is employed lies in that it is difficult for the
performance values to follow the normal distribution and satisfy variance homogeneous
condition 
note that the optimal fss algorithm sets for different settings of parameters  and  are
different  since the values of these two parameters directly affect the required performance
values 
   recommendation method validation
the leave one out strategy is used to empirically evaluate our proposed fss algorithm recommendation method as follows  for each data set di     i       that
is viewed as the test data  i  identify its k nearest data sets from the training data  
 d         di    di          d      excluding di   ii  calculate the performance of the    candidate fss algorithms according to eq      based on the k nearest data sets where the value
of k is determined by the standard cross validation strategy  and recommend the top three
to di   and iii  evaluate the recommendations by the measures introduced in section       
   sensitivity analysis of the number of the nearest data sets on recommendations
in order to explore the effect of the number of the nearest data sets on the recommendations and provide users an empirical method to choose its value  for each data set  all
the possible numbers of the nearest data sets are tested  that is  when identifying the k
nearest data sets for a given data set  k is set from   to the number of the historical data
sets minus    e g       in this experiment  
    results and analysis
in this section  we present the recommendation results in terms of recommended fss algorithms  hit ratio and performance ratio   respectively  due to the space limit of the
paper  we do not list all the recommendations  but instead present the results under two
significantly different pairs of  and   i e                and                  
afterward  we also provide the experimental results of the influence of the user oriented
parameters  and  on recommendations in terms of classification accuracy  runtime  and
the number of selected features  respectively 
      recommended algorithms and hit ratio
figs             and   show the first recommended fss algorithms for the     data sets
when the classification algorithms naive bayes  c     part  ib  and bayes network are
used  respectively 
  

fisubset selection algorithm automatic recommendation

in each figure  there are two sub figures corresponding to the recommendation results
for              and                   respectively  in each sub figure   and 
denote the correctly and incorrectly recommended algorithms  respectively 

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a            
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b                

figure    fss algorithms recommended for the     data sets when naive bayes is used

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a            
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b                

figure    fss algorithms recommended for the     data sets when c    is used
from these figures  we observe that 
   for all the five classifiers  the proposed method can effectively recommend appropriate
fss algorithms for most of the     data sets 
in the case of               the number of data sets  whose appropriate fss
algorithms are correctly recommended  is     out of     for naive bayes      out of    
for c         out of     for part      out of     for ib   and     out of     for bayes
  

fiwang  song  sun  zhang  xu   zhou

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a            
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b                

figure    fss algorithms recommended for the     data sets when part is used

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a            
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b                

figure    fss algorithms recommended for the     data sets when ib  is used

network  respectively  this states that the recommendation method is effective when
classification accuracy is most important 
in the case of                   the number of data sets  whose appropriate fss
algorithms are correctly recommended  is     out of     for naive bayes      out of    
for c         out of     for part      out of     for ib   and     out of     for bayes
network  respectively  this indicates that the recommendation method also works well
when tradeoff is required among classification accuracy  runtime  and the number of
selected features 
   the distribution of the recommended fss algorithms for the     data sets is different for
different parameters settings  the distribution is relatively uniform for              
  

fisubset selection algorithm automatic recommendation

fss algorithm id

correctly recommended algorithm

incorrectly recommended algorithm

  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

                                  
                                  

  

                                  
                                  

  

data set id

fss algorithm id

 a            
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  

data set id

 b                

figure    fss algorithms recommended for the     data sets when bayes network is used

while it is seriously biased to some algorithm  e g   the   th fss algorithm  for    
             
this phenomenon is similar for all the five classifiers  this can be explained as follows 
the fss algorithms with the best classification accuracy distribute on the     data sets
uniformly  thus  in the case of              where users favor accurate classifiers  the
distribution of the recommended fss algorithms is relatively uniform as well  however 
there exist some fss algorithms that run faster  e g   the   th algorithm signific 
or select fewer features  e g   the  th algorithm cfs sws  the   th algorithm conssws  and the   th algorithm signific  on most of the     data sets  for this reason  in
the case of                  where users prefer the fss algorithms with less runtime
and fewer features  the distribution of the fss algorithms with the best performance on
the     data sets is biased to some algorithms  so is the recommended fss algorithms 
   the   th fss algorithm performs well on about    out of     data sets for all classifiers
when                   it seems that this fss algorithm is a generally wellperformed fss algorithm that can be adopted by all fss tasks and there is no need
for fss algorithm recommendation  unfortunately  this is not the case  the   th fss
algorithm is still failing to perform well over about a quarter of the     data sets 
yet  our recommendation method can distinguish these data sets and further effectively
recommend appropriate fss algorithms for them  this indicates our recommendation
method is still necessary in this case 
compared with               we can know that this case is due to the larger  and
 values and can be explained as follows  for all the    fss algorithms  although the
classification accuracies of a classifier over the features selected by them are different 
the differences are usually bounded  meanwhile  from eq      we know that when  
is set to be greater than the bound value  the value of earr will be dominated by
the runtime the number of selected features  this means that if  or  is set to be
a relatively large value  the algorithm with a lower time complexity or the algorithm
that chooses smaller number of features will be recommended  and the classification
  

fiwang  song  sun  zhang  xu   zhou

accuracy over the selected features will be ignored  however  as we know  one of the
most important targets of feature selection is to improve the performance of learning
algorithms  so it is unreasonable to ignore the classification accuracy and just focus on
the speed and the simplicity of an fss algorithm 
thus  in real applications  the values of  and  should be set under the limit of classification accuracies  generally  the   should be bounded by  accmax  accmin   accmin  
where accmax and accmin denote the maximum and the minimum classification accuracies  respectively 
parameter setting
          

              


recommendation
alg st
alg nd
alg rd
top  
alg st
alg nd
alg rd
top  

naive bayes
     
     
     
     
     
     
     
     

c   
     
     
     
     
     
     
     
     

part
     
     
     
     
     
     
     
     

ib 
     
     
     
     
     
     
     
     

bayes network
     
     
     
     
     
     
     
     

algx denotes only the x  th algorithm in the ranking list is recommended while top   means the top three
algorithms are recommended 

table    hit ratio     of the recommendations for the five classifiers under different settings
of     
table   shows the hit ratio of the recommended fss algorithms for the five classifiers 
from it we can observe that 
   if a single fss algorithm is recommended  the hit ratio of the first recommended algorithm alg st is the highest  its value is up to        and at least is        for all the
five classifiers  thus  alg st should be the first choice 
   if the top three algorithms are recommended  the hit ratio is up to      and at least
is         that indicates that the confidence of the top three algorithms including an
appropriate one is very high  this is the reason why only the top three algorithms
are recommended  moreover  the proposed recommendation method has reduced the
number of candidate algorithms to three  users can further pick up the one that fits
his her specific requirement from them 
      recommendation performance ratio
figs    and   show the recommendation performance ratio rpr of the first recommended
fss algorithm for the five classifiers with              and                  
respectively  from these two figures we can observe that  for most data sets and the two
settings of  and   the rprs of the recommended fss algorithms are greater than    
and some of them are up to      no matter which classifier is used  this indicates that the
fss algorithms recommended by our proposed method are very close to the optimal one 
table   shows the average rprs over the     data sets for the five classifiers under
different settings of       in this table  for each classifier  columns rec and def
shows the rpr value corresponding to the recommended fss algorithms and default fss
algorithms  respectively  where the default fss algorithm is the most frequent best one on
the     data sets under the classifier 
  

fisubset selection algorithm automatic recommendation

rpr    

naive bayes
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

c   
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

part
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

ib 
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

bayes network
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

figure    rpr of the   st recommended fss algorithm with              for the five
classifiers
rpr    

naive bayes
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

c   
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

part
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

ib 
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

rpr    

bayes network
   
  
  
  
  
  
  

 

 
 

 
 

 
 

 
 

                                                                                                                                                                      
                                                                                                                                                                      

data set id

figure    rpr of the   st recommended fss algorithm with                  for the
five classifiers

  

fiwang  song  sun  zhang  xu   zhou

from it we observe that the average rprs range from        to       for       
       and from        to        for                   respectively  moreover 
the average rpr of the recommended fss algorithms surpasses that of the default fss
algorithms for all the five different classifiers  this means our proposed recommendation
method works very well and greatly fits users performance requirement 
parameter setting
          
              

naive bayes
rec
def
           
           

c   
rec
def
     
     
     
     

part
rec
def
           
           

ib 
rec
def
     
     
     
     

bayes network
rec
def
     
     
     
     

table    average rpr     over     data sets for the five classifiers

data id
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  


nb
      
      
      
      
      
     
      
      
     
      
      
      
      
      
      
      
      
      
      
     
     
      
     
      
      
      
      
      
     
      
      
      
      
     
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

c   
      
      
      
      
      
     
      
      
      
      
     
      
      
      
      
      
      
      
      
      
    
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
     
      
      
     
      

part
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
     
     
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
     
      
      
      
      
      
      
      
      
      
      
      
     
      
      

ib 
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
     
      
      
     
     
      
      
      
      
      
      
      
      
      
      
     
      
      

bnet
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
     
      
     
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
     
     
      
      
      
      
      
      
      
      
      
      
      
     
      
     
     
      
      
      
      
      
      
      
      

data id
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
average

nb
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
    
      
      
      
      
      
      
      
      
      
      
      
      
      
      

c   
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

part
      
      
      
      
      
      
      
      
      
      
     
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      

ib 
      
     
      
      
    
      
      
     
      
      
      
      
      
      
      
      
      
      
      
     
     
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

bnet
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
     
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

nb and bnet denote naive bayes and bayes network  respectively 

table    recommendation time over     data sets for the five classifiers  in second  

  

fisubset selection algorithm automatic recommendation

      recommendation time
when recommending fss algorithms for a feature selection problem  the recommendation
time is contributed by meta features extraction  k nearest data sets identification  and the
candidate algorithm ranking according to their performance on the k data sets 
of these three recommendation time contributors  only the candidate algorithm ranking
is related with the parameters  and  of the performance metric earr 
however  the computation of performance earr is the same whatever the values of 
and  are  this means recommendation time is independent of the specific settings of 
and   thus  we just present the recommendation time with               and table  
shows the details 
from table   we observe that for a given data set  the recommendation time differences
for the five classifiers are small  the reason is that the recommendation time is mainly
contributed by the extraction of meta features  which has no relation with classifiers  this
is consistent with the time complexity analysis in section      we also observe that for most
of the data sets  the recommendation time is less than     second  and its average value on
the     data sets is around      second for each of the five classifiers  this is much faster
than the conventional cross validation method 
      impact of the parameters  and 
figs                and    show the impact of the settings of  and  on the classification
accuracy  the runtime of feature selection  the number of selected features  the hit ratio
and the rpr value  respectively 
naivebayes

c   

part

average accuracy

     

average accuracy

ib 

bayes network

     

    
     
    
     
    
     
    

    
     
    
     
    
     
    
     

     
 

 

                                                

                                                

 

figure    classification accuracies of the five classifiers with the recommended fss algorithms under different values of  and 
fig    shows the classification accuracies of the five classifiers under the different values
of  and   from it we observe that  with the increase of either  or   the classification
accuracies of the five classifiers with the recommended fss algorithms decrease  this is
because the increase of  or  indicates that users much prefer faster fss algorithms or the
fss algorithms that can get less features  thus  the proportion of classification accuracy
in performance is decreased  this means the ranks of the fss algorithms that run faster
and or get less features are improved and the corresponding fss algorithms are finally
selected 
  

fiwang  song  sun  zhang  xu   zhou

c   

part

ib 

bayes network

    

average runtime  ms 

average runtime  ms 

naivebayes
    
    
    
    
    
    
    
    
   
   
   
   

    
    
    
    
    
    
    
   
   
   

 

 

                                                

                                                

 

figure     runtime of the fss algorithms recommended to the five classifiers under different
values of  and 
fig     shows the runtime of the fss algorithms that recommended to the five classifiers
under the different values of  and  for the five classifiers  from it we observe that 
   with the increase of   the average runtime of the recommended fss algorithms for
each classifier decreases  note a larger value of  means users favor faster fss algorithms  thus  this indicates that users performance requirement is met since faster fss
algorithms were recommended 
   with the increase of   the average runtime of the recommended fss algorithms increases
as well  this is because in our proposed recommendation method  the appropriate
fss algorithms for a given data set are recommended based on its nearest data sets 
moreover  in the experiment  for more than half  i e       of the     data sets  there is
a negative correlation between the number of selected features and the runtime of the
   fss algorithms  thus  the more data sets with this kind of negative correlation  the
more possible the nearest neighbors of a given data set have the negative correlation 
therefore  a larger  means longer runtime  another possible reason is that a larger
value of  means users favor the fss algorithms that choose fewer features  and in order
to get fewer features  the fss algorithms need to consume relatively more time 
c   

part

average number of features

average number of features

naivebayes

   
  
  
  
  
  
  
 

                                                

ib 

bayes network

   
   
  
  
  
  
 

                                                

 

figure     number of features selected by the fss algorithms that recommended to the
five classifiers under different values of  and 
fig     shows the number of features selected by the fss algorithms that recommended
to the five classifiers under different values of  and   from it we observe that 
  

fisubset selection algorithm automatic recommendation

   with the increase of   the average number of selected features increases as well  this
is because in our proposed recommendation method  the appropriate fss algorithms
for a given data set are recommended based on its nearest data sets  moreover  in the
experiment  for more than half  i e       of the     data sets  there is a negative correlation between the number of selected features and the runtime of the    fss algorithms 
thus  the more data sets with this kind of negative correlation  the more possible the
nearest neighbors of a given data set have the negative correlation  therefore  a larger
 means more features  another possible reason is that a larger value of  means users
favor faster fss algorithms  it is possible that shorter computation time can be obtained
via filter out less features so more features are remained 
note that there is an exception  that is  the average number of selected features for
c    decreases when the value of  is small  however  the decrement comes up in a quite
small range of   i e            
   with the increase of   the average number of features selected by the recommended
fss algorithm decreases  note a larger value of  means users favor the fss algorithms
that can get fewer features  thus  this indicates that users requirement is met since the
fss algorithms that can get fewer features were recommended 
naivebayes

c   

part

average hit ratio    

average hit ratio    

ib 

bayes network

   

   
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  

  
 

 

                                                

                                                

 

figure     average hit ratio of the fss algorithms that recommended to the five classifiers
under different values of  and 
c   

part
   

    

    

average rpr    

average rpr    

naivebayes
   

  
    
  
    

ib 

bayes network

  
    
  
    
  

  
 

 

                                                

                                                

 

figure     average rpr of the fss algorithms that recommended to the five classifiers
under different values of  and 
figs     and    show the average hit ratio and rpr of the recommended fss algorithms
under different values of  and  for the five classifiers 
  

fiwang  song  sun  zhang  xu   zhou

from them we observe that  the average hit ratio falls in the intervals               
under  and                   under   the average rpr varies in the intervals         
        under  and                  under   with the change of the  and   the hit
ratio and rpr of the recommended fss algorithms vary as well  however  the change
intervals fall in a relative small interval and the lower bound stands at a fairly high level 
the minimum average hit ratio is up to        and the minimum average rpr is up
to         this indicates that the proposed fss algorithm recommendation method has
general application and works well for different settings of  and  

   sensitivity analysis of the number of nearest data sets on
recommendation results
in this section  we analyze how the number of the nearest data sets affects the recommendation performance  based on the experimental results  we provide some guidelines for
selecting the appropriate number of nearest data sets in practice 
    experimental method
generally  different numbers of the nearest data sets  i e   k  will result in different recommendations  thus  when recommending fss algorithms to a feature selection problem  an
appropriate k value is very important 
the k value that results in higher recommendation performance is preferred  however 
the recommendation performance difference under two different k values sometimes might
be random and not significant  thus  in order to identify an appropriate k value from
alternatives  we should first determine whether or not the performance differences among
them are statistically significant  non parametric statistical test  friedman test followed by
holm procedure test as suggested by demsar         can be used for this purpose 
in the experiment  we conducted fss algorithm recommendation with all possible k
values  i e   from   to      over the     data sets  when identifying the appropriate k
values  the non parametric statistical test is conducted as follows 
firstly  the friedman test is performed over the     recommendation performance at
the significance level       its null hypothesis is that the     k values perform equivalently
well in the proposed recommendation method over the     data sets 
if the friedman test rejects the null hypothesis  that is  there exists significant difference
among these     k values  then we choose one under which the recommendation has the best
performance as the reference  after that  the holm procedure test is performed to find out
the k values under which the recommendation performance has no significant difference with
that of the reference  the identified k values including the reference are the appropriate
numbers of the nearest data sets 
    results analysis
fig     shows how the number of the nearest data sets  i e   k  affects the performance
of the recommendation method under different settings of  and   where  denotes the
k under which the recommendation performance is significantly worse than others at the
significance level of       from it we observe that 
  

fisubset selection algorithm automatic recommendation

naive bayes

c   

part

ib 

bayes network

inappropriate number of neighbors

 
     

rpr

    
     
    
     
    
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

                              
                                  

number of nearest data sets

 a            
naive bayes

c   

part

ib 

bayes network

inappropriate number of neighbors

 

rpr

     
    
     
    
     
 

 
 

 
 

 
 

 
 

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

                              
                                  

number of nearest data sets

 b                

figure     number of the nearest data sets vs  rpr
   when          fig     a    for each of the five classifiers  the rpr varies with
different k values  specifically  the rpr is fluctuant when k is smaller than     while it
is relatively flat in the middle part  and it decreases when k is larger than    except for
c     however  the increment of c    is very small            this might be due to that
c    picks up useful features to build the tree by itself  so the impact of other feature
selection methods is less  moreover  the difference among accuracies of c    on most
data sets is relatively small  while the performance metric earr that used to evaluate
different fss algorithms depends only on classification accuracy when          thus 
the rpr of c    is relatively stable for the different values of k 
   in the case of            fig     b    the variation of rpr is different from that of
         for each of the five classifiers  the rpr first decreases with fluctuations  then
increases  and finally decreases slowly and steadily  this could be due to that  when the
parameters  and  are set to be a relatively large value  such as     in our experiment  
the runtime of   or the number of features selected by  an fss algorithm will play a
more important role in evaluating the performance of the fss algorithm  thus  for a
given data set  the fss algorithms with lower time complexity  or the smaller number of
selected features  will be more possibly higher ranked and have larger rpr  therefore 
with the increasing of k  these algorithms are more possibly recommended  meanwhile 
for most data sets  these algorithms are either the real appropriate algorithms or with
larger rpr  so the rpr averaged over all data sets is relatively stable with the increasing
of k 
  

fiwang  song  sun  zhang  xu   zhou

   comparing the cases of         and            we found that  appears when
k      for the former and k      for the latter  while it emerges again when k      for
the former  this means we cannot choose the k values falling into these ranges  at the
same time  we also found that the peak values of rpr for           appear in the
range of           which is also one of the peak value ranges for         except c    
this means if we set k to     to     of the number of data sets  better recommendation
performance can be obtained 

   conclusion
in this paper  we have presented an fss algorithm recommendation method with the aim
to support the automatic selection of appropriate fss algorithms for a new feature selection
problem from a number of candidates 
the proposed recommendation method consists of meta knowledge database construction and algorithm recommendation  the former obtains the meta features and the performance of all the candidate fss algorithms  while the latter models the relationship between
the meta features and the fss algorithm performance based on a k  nn method and recommends appropriate algorithms for a feature selection problem with the built up model 
we have thoroughly tested the recommendation method with     real world data sets    
different fss algorithms  and five representative classification algorithms under two typical
users performance requirements  the experimental results show that our recommendation
method is effective 
we have also conducted a sensitivity analysis to explore how the number of the nearest
data sets  k  impacts the fss algorithm recommendation  and suggest to set k as the    
to     of the number of the historical data sets 
in this paper  we have utilized the well known and commonly used meta features to
characterize different data sets  which meta features are informative  and are there
any other more informative meta features  are still open questions  to our knowledge 
there still does not exist any effective method to answer these questions  thus  for future
work  we plan to explore further that how to measure the information of the meta features
and whether there are some more informative meta features that can lead to further improvements for fss algorithm recommendation 

acknowledgements
this work is supported by the national natural science foundation of china under grant
         

references
aha  d  w   kibler  d     albert  m  k          instance based learning algorithms  machine learning              
ali  s     smith  k  a          on learning algorithm selection for classification  applied
soft computing                
  

fisubset selection algorithm automatic recommendation

atkeson  c  g   moore  a  w     schaal  s          locally weighted learning  artificial
intelligence review               
battiti  r          using mutual information for selecting features in supervised neural net
learning  ieee transactions on neural networks                
brazdil  p   carrier  c   soares  c     vilalta  r          metalearning  applications to data
mining  springer 
brazdil  p  b   soares  c     da costa  j  p          ranking learning algorithms  using ibl
and meta learning on accuracy and time results  machine learning                 
brodley  c  e          addressing the selective superiority problem  automatic algorithm model class selection  in proceedings of the tenth international conference
on machine learning  pp        citeseer 
castiello  c   castellano  g     fanelli  a          meta data  characterization of input
features for meta learning  modeling decisions for artificial intelligence         
dash  m     liu  h          feature selection for classification  intelligent data analysis 
              
dash  m     liu  h          consistency based search in feature selection  artificial intelligence                    
de souza  j  t          feature selection with a general hybrid algorithm  ph d  thesis 
university of ottawa 
demsar  j          statistical comparisons of classifiers over multiple data sets  journal of
machine learning research         
engels  r     theusinger  c          using a data metric for preprocessing advice for data
mining applications  
frank  e     witten  i  h          generating accurate rule sets without global optimization 
in proceedings of the   th international conference on machine learning  pp         
morgan kaufmann  san francisco  ca 
friedman  m          the use of ranks to avoid the assumption of normality implicit in
the analysis of variance  journal of the american statistical association           
       
friedman  n   geiger  d     goldszmidt  m          bayesian network classifiers  machine
learning                 
gama  j     brazdil  p          characterization of classification algorithms  progress in
artificial intelligence         
garcia lopez  f   garcia torres  m   melian batista  b   moreno perez  j  a     morenovega  j  m          solving feature subset selection problem by a parallel scatter
search  european journal of operational research                  
goldberg  d  e          genetic algorithms in search  optimization  and machine learning 
addison wesley professional 
  

fiwang  song  sun  zhang  xu   zhou

gutlein  m   frank  e   hall  m     karwath  a          large scale attribute selection
using wrappers  in proceedings of ieee symposium on computational intelligence
and data mining  pp          ieee 
guyon  i     elisseeff  a          an introduction to variable and feature selection  the
journal of machine learning research              
hall  m   frank  e   holmes  g   pfahringer  b   reutemann  p     witten  i          the
weka data mining software  an update  acm sigkdd explorations newsletter         
     
hall  m  a          correlation based feature selection for machine learning  ph d  thesis 
the university of waikato 
hedar  a  r   wang  j     fukushima  m          tabu search for attribute reduction
in rough set theory  soft computing a fusion of foundations  methodologies and
applications                 
hommel  g          a stagewise rejective multiple test procedure based on a modified
bonferroni test  biometrika                 
john  g  h     langley  p          estimating continuous distributions in bayesian classifiers  in proceedings of the eleventh conference on uncertainty in artificial intelligence 
vol     pp          citeseer 
kalousis  a   gama  j     hilario  m          on data and algorithms  understanding
inductive performance  machine learning                 
king  r  d   feng  c     sutherland  a          statlog  comparison of classification algorithms on large real world problems  applied artificial intelligence                
kira  k     rendell  l          a practical approach to feature selection  in proceedings of the ninth international workshop on machine learning  pp          morgan
kaufmann publishers inc 
kohavi  r          a study of cross validation and bootstrap for accuracy estimation and
model selection  in international joint conference on artificial intelligence  vol     
pp            citeseer 
kohavi  r     john  g          wrappers for feature subset selection  artificial intelligence 
               
kononenko  i          estimating attributes  analysis and extensions of relief  in proceedings of the european conference on machine learning on machine learning  pp 
        springer verlag new york 
lee  m   lu  h   ling  t     ko  y          cleansing data for mining and warehousing 
in proceedings of the   th international conference on database and expert systems
applications  pp          springer 
lindner  g     studer  r          ast  support for algorithm selection with a cbr approach  principles of data mining and knowledge discovery         
liu  h   motoda  h   setiono  r     zhao  z          feature selection  an ever evolving
frontier in data mining  in the fourth workshop on feature selection in data
mining  pp       citeseer 
  

fisubset selection algorithm automatic recommendation

liu  h     setiono  r          chi   feature selection and discretization of numeric attributes  in proceedings of the seventh international conference on tools with artificial intelligence  pp          ieee 
liu  h     setiono  r          a probabilistic approach to feature selection a filter solution  
pp          citeseer 
liu  h     yu  l          toward integrating feature selection algorithms for classification
and clustering  ieee transactions on knowledge and data engineering             
    
michie  d   spiegelhalter  d  j     taylor  c  c         
statistical classification  

machine learning  neural and

molina  l  c   belanche  l     nebot  a          feature selection algorithms  a survey and
experimental evaluation  in proceedings of ieee international conference on data
mining  pp          ieee 
nakhaeizadeh  g     schnabl  a          development of multi criteria metrics for evaluation of data mining algorithms  in proceedings of the  rd international conference
on knowledge discovery and data mining  pp       
nakhaeizadeh  g     schnabl  a          towards the personalization of algorithms evaluation in data mining  in proceedings of the  th international conference on knowledge
discovery and data mining  pp         
pudil  p   novovicova  j     kittler  j          floating search methods in feature selection 
pattern recognition letters                    
pudil  p   novovicova  j   somol  p     vrnata  r       a   conceptual base of feature
selection consulting system  kybernetika                 
pudil  p   novovicova  j   somol  p     vrnata  r       b   feature selection expertuser
oriented approach  advances in pattern recognition         
quinlan  j  r          c     programs for machine learning  morgan kaufmann 
robnik sikonja  m     kononenko  i          theoretical and empirical analysis of relieff
and rrelieff  machine learning               
saeys  y   inza  i     larranaga  p          a review of feature selection techniques in
bioinformatics  bioinformatics                    
smith miles  k  a          cross disciplinary perspectives on meta learning for algorithm
selection  acm computing surveys              
sohn  s  y          meta analysis of classification algorithms for pattern recognition  ieee
transactions on pattern analysis and machine intelligence                    
song  q  b   wang  g  t     wang  c          automatic recommendation of classification
algorithms based on data set characteristics  pattern recognition                   
vilalta  r     drissi  y          a perspective view and survey of meta learning  artificial
intelligence review               
  

fiwang  song  sun  zhang  xu   zhou

wolpert  d  h          the supervised learning no free lunch theorems  in proceedings
of  th online world conference on soft computing in industrial applications  pp 
      citeseer 
yu  l     liu  h          feature selection for high dimensional data  a fast correlationbased filter solution  in proceedings of the twentieth international conference on
machine leaning  vol      pp         
zhao  z     liu  h          searching for interacting features  in proceedings of the   th
international joint conference on artifical intelligence  pp            morgan kaufmann publishers inc 
zhou  x     dillon  t          a heuristic statistical feature selection criterion for inductive machine learning in the real world  in proceedings of the ieee international
conference on systems  man  and cybernetics  vol     pp          ieee 

  

fi