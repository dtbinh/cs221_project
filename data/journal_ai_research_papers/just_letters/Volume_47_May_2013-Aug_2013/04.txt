journal of artificial intelligence research                  

submitted        published      

framing image description as a ranking task 
data  models and evaluation metrics
micah hodosh
peter young
julia hockenmaier

mhodosh  illinois edu
pyoung  illinois edu
juliahmr illinois edu

department of computer science
university of illinois
urbana  il        usa

abstract
the ability to associate images with natural language sentences that describe what
is depicted in them is a hallmark of image understanding  and a prerequisite for applications such as sentence based image search  in analogy to image search  we propose to
frame sentence based image annotation as the task of ranking a given pool of captions 
we introduce a new benchmark collection for sentence based image description and search 
consisting of       images that are each paired with five different captions which provide
clear descriptions of the salient entities and events  we introduce a number of systems
that perform quite well on this task  even though they are only based on features that
can be obtained with minimal supervision  our results clearly indicate the importance of
training on multiple captions per image  and of capturing syntactic  word order based  and
semantic features of these captions  we also perform an in depth comparison of human
and automatic evaluation metrics for this task  and propose strategies for collecting human
judgments cheaply and on a very large scale  allowing us to augment our collection with
additional relevance judgments of which captions describe which image  our analysis shows
that metrics that consider the ranked list of results for each query image or sentence are
significantly more robust than metrics that are based on a single response per query  moreover  our study suggests that the evaluation of ranking based image description systems
may be fully automated 

   introduction
the ability to automatically describe the entities  events and scenes depicted in an image
is possibly the most ambitious test of image understanding  any advances on this task
have significant practical implications  since there are billions of images on the web and in
personal photo collections  our ability to efficiently access the wealth of information they
contain is hampered by limitations of standard image search engines  which must rely on
text that appears near the image  datta  joshi  li    wang        popescu  tsikrika   
kludas         there has been a lot of work on the multi label classification problem of
associating images with individual words or tags  see  e g   blei   jordan        barnard 
duygulu  forsyth  freitas  blei    jordan        feng   lapata        deschacht   moens 
      lavrenko  manmatha    jeon        makadia  pavlovic    kumar        weston 
bengio    usunier         but the much harder problem of automatically associating images
with complete sentences that describe them has only recently begun to attract attention 

     ai access foundation  all rights reserved 

fihodosh  young   hockenmaier

    related work
although a few approaches have framed sentence based image description as the task of mapping images to sentences written by people  farhadi  hejrati  sadeghi  young  rashtchian 
hockenmaier    forsyth        ordonez  kulkarni    berg         most research in this
area has focused on the task of automatically generating novel captions  kulkarni  premraj 
dhar  li  choi  berg    berg        yang  teo  daume iii    aloimonos        li  kulkarni  berg  berg    choi        mitchell  dodge  goyal  yamaguchi  stratos  han  mensch 
berg  berg    daume iii        kuznetsova  ordonez  berg  berg    choi        gupta 
verma    jawahar         we argue in this paper that framing image description as a natural language generation problem introduces a number of linguistic difficulties that detract
attention from the underlying image understanding problem we wish to address  since any
sentence based image description or retrieval system requires the ability to associate images
with captions that describe what is depicted in them  we argue it is important to evaluate
this mapping between images and sentences independently of the generation aspect  research on caption generation has also ignored the image search task  which is arguably of
much greater practical importance 
all of the systems cited above are either evaluated on a data set that our group released
in earlier work  rashtchian  young  hodosh    hockenmaier         or on the sbu captioned photo dataset  ordonez et al          our data set consists of       images from the
pascal voc      object recognition challenge that are each annotated with five descriptive captions which we purposely collected for this task  the sbu data set consists of one
million images and captions harvested from flickr  gupta et al         is the only system to
use grubinger  clough  mller  and deselaerss        iapr tc    data set  which consists
of        images paired with longer descriptions 
although details differ  most models rely on existing detectors to define and map images
to an explicit meaning representation language consisting of a fixed number of scenes  objects
 or stuff   their attributes and spatial relations  farhadi et al         kulkarni et al        
li et al         yang et al         ordonez et al         mitchell et al          but it is
unclear how well these detector based approaches generalize  the models evaluated on our
pascal voc      data set  farhadi et al         kulkarni et al         li et al        
yang et al         mitchell et al         all rely on detectors that may have been trained on
images contained in this corpus  and kuznetsova et al         select a test set of       images
from the sbu data set for which their detectors work well  moreover  among the systems
evaluated on our pascal voc      data set  only kulkarni et al          li et al         
li et al         and mitchell et al s        results may be directly comparable  since different
research groups report different evaluation metrics and use a different parts of the data set
as test or training data  the evaluation of generation systems is generally well known to be
difficult  see  e g   dale   white        reiter   belz         and typically requires expensive
human judgments that have to consider the quality of both content selection  what is being
described  and surface realization  the fluency of the generated text   these syntactic and
pragmatic issues confound the purely semantic question of whether the image is correctly
described by the caption 

   

fiframing image description as a ranking task

    our approach
in this paper  we focus on the task of associating images with sentences drawn from a large 
predefined pool of image descriptions  these descriptions are not generated automatically
or harvested from the web  feng   lapata        ordonez et al          but are written
by people who were asked to describe them  we argue that evaluating the ability to select
or rank  rather than generate  appropriate captions for an image is the most direct test
of the fundamental semantic question of how well we can associate images with sentences
that describe them well  framing image description as a ranking task also has a number
of additional advantages  first  it allows us to handle sentence based image annotation
and search in a unified framework  allowing us to evaluate whether advances in one task
carry over to the other  second  framing image description as a ranking problem greatly
simplifies evaluation  by establishing a parallel between description and retrieval  we can
use the same metrics to evaluate both tasks  moreover  we show that the rank of the
original caption  which is easily determined automatically  leads to metrics that correlate
highly with systems rankings obtained from human judgments  even if they underestimate
actual performance  we also show that standard automatic metrics such as bleu  papineni 
roukos  ward    zhu        or rouge  lin        that have also been used to evaluate
caption generation systems show poor correlation with human judgments  leading us to
believe that the evaluation of caption generation system should not be automated  we also
perform a large scale human evaluation  but since the sentences in our data set are image
descriptions written by people  we only need to collect purely semantic judgments of whether
they describe the images the system associated them with  and since these judgments are
independent of the task  we can use them to evaluate both image description and retrieval
systems  since we collect these judgments over image caption pairs in our publicly available
data set  we also establish a common benchmark that enables a direct comparison of different
systems  we believe that this is another advantage over the caption generation task  since
there are many possible ways to describe an image  generation systems are at liberty to be
more or less specific about what they describe in an image  this makes a direct comparison
of independently obtained judgments about the quality of two different systems very difficult 
since one system may be aiming to solve a much harder task than the other  and implies
that unless system outputs for a common benchmark collection of images were made publicly
available  there cannot be any shared  objective evaluation that would allow the community
to measure progress on this difficult problem  but since caption generation systems also
need to be able to determine how well a caption describes an image  our data set could
potentially be used to evaluate their semantic component 
    contributions and outline of this paper
in section    we discuss the need for a new data set for image description and introduce a
new  high quality  data set for image description which will enable the community to compare
different systems against the same benchmark  our pascal voc      data set of      
images  rashtchian et al         has been used by a number of image description systems
 farhadi et al         kulkarni et al         li et al         yang et al         mitchell et al  
      gupta et al          but has a number of shortcomings that limit its usefulness  first 
its domain is relatively limited  and the captions are relatively simple  second  since its
   

fihodosh  young   hockenmaier

images are drawn from the data used for the pascal voc      object classes challenge 
it is difficult to guarantee a fair evaluation of description systems which rely on off the shelf
object detectors  e g   felzenszwalb  mcallester    ramanan        on this data set  since
it may not be possible to identify which images these detectors have been trained on  the
experiments in this paper are therefore based on a larger  and more diverse  data set of      
images  unlike other data sets that pair images with sentences that are merely related to
the image  feng   lapata        ordonez et al          each image in our data sets are
paired with five different captions that were purposely written to describe the image 
in section    we describe our own image description systems  because image description
is such a novel task  it remains largely unknown what kind of model  and what kind of
visual and linguistic features it requires  instead of a unidirectional mapping from images
to sentences that is common to current caption generation systems  we map both images
and sentences into the same space  this allows us to apply our system to image search
by retrieving the images that are closest to a query sentence  and to image description
by annotating images with those sentences that are closest to it  the technique we use 
kernel canonical correlation analysis  kcca  bach   jordan         has already been
successfully used to associate images  hardoon  szedmak    shawe taylor        hwang
  grauman        hardoon  saunders  szedmak    shawe taylor        or image regions
 socher   li        with individual words or sets of tags  while canonical correlation
analysis  hotelling        has also been used to associate images with related wikipedia
articles from ten different categories  rasiwasia  pereira  coviello  doyle  lanckriet  levy 
  vasconcelos         however  the performance of these techniques on the much more
stringent task of associating images with sentences that describe what is depicted in them
has not been evaluated  we compare a number of text kernels that capture different linguistic
features  our experimental results  discussed in section    demonstrate the importance of
robust textual representations that consider the semantic similarity of words  and hence take
the linguistic diversity of the different captions associated with each image into account  our
visual features are relatively simple  a number of image description systems  farhadi et al  
      kulkarni et al         li et al         yang et al         kuznetsova et al         largely
rely on trained detectors  e g  to obtain an explicit intermediate meaning representation of
the depicted objects  scenes and events  but this approach would ultimately require separate
detectors  and hence labeled training data  for each term or phrase in the chosen meaning
representation language  we show here that image features that capture only low level
perceptual properties can in fact work surprisingly well on our larger data set for which no
in domain detectors are available 
in section    we consider the question of evaluation  and use a number of different metrics
to compare our systems  since we focus on the problem of learning an appropriate mapping
between images and captions  we follow standard machine learning practice and evaluate
the ability of this function to generalize to unseen examples  hence  we separate the pool of
captions and images used for testing from those used to train our systems  we first consider
metrics for the quality of a single image caption pair  and compare automatically computed
scores with detailed human judgments  we then examine metrics that evaluate the ranked
lists returned by our systems  our analysis reveals that  at the current level of performance 
differences between models may not become apparent if only a single caption per image is
considered  as is commonly done for caption generation systems  but even if two models
   

fiframing image description as a ranking task

are equally likely to fail to return a suitable caption as the first result  we should still prefer
the one that is more likely to rank good captions higher than the other  since it arguably
provides a better approximation of the semantic space in which images are near captions
that describe them well  since the test pool contains a single gold item for each query  we
first consider metrics that are based on the rank and recall of this gold item  we then show
that simpler  binary judgments of image descriptions that are good approximations of more
fine grained human judgments can be collected on a very large scale via crowdsourcing  we
augment the test pool of our data set with these relevance judgments  in the hope that
this will add to its usefulness as a community resource and benchmark  these judgments
show that the actual performance of our systems is higher than the recall of the gold item
indicates  however  a comparison of the system rankings obtained via different metrics also
suggests that differences in the rank or recall of the gold item correlate very highly with
difference in performance according to the binary relevance judgments 

   a new data set for image description
we have used crowdsourcing to collect descriptive captions for a large number of images of
people and animals  mostly dogs   before describing our data set and annotation methodology  we discuss what kind of captions are most useful for image description  and motivate
the need to create new data sets for this task 
    what do we mean by image description 
since automatic image description is a relatively novel task  it is worth reflecting what it
means to describe images  and what we wish to say about an image  there is in fact a
substantial body of work on image description related to image libraries  jaimes  jaimes 
  chang        shatford        that is useful to revisit for our purpose  we argue that
out of the three different kinds of image descriptions that are commonly distinguished  one
type  the so called conceptual descriptions  is of most relevance to the image understanding we aim to achieve with automatic captioning  conceptual image descriptions identify
what is depicted in the image  and while they may be abstract  e g   concerning the mood
a picture may convey   image understanding is mostly interested in concrete descriptions
of the depicted scene and entities  their attributes and relations  as well as the events they
participate in  because they focus on what is actually in the image  conceptual descriptions
differ from so called non visual descriptions  which provide additional background information that cannot be obtained from the image alone  e g  about the situation  time or location
in which the image was taken  perceptual descriptions capture low level visual properties
of images  e g   whether it is a photograph or a drawing  or what colors or shapes dominate  are of little interest to us  unless they link these properties explicitly to the depicted
entities  among concrete conceptual descriptions  a further distinction can be drawn between specific descriptions  which may identify people and locations by their names  and
generic descriptions  which may  e g   describe a person as a woman or a skateboarder  and
the scene as a city street or a room   with the exception of iconic entities that should be
recognized as such  e g   well known public figures or landmark locations such as the eiffel
tower  we argue that image understanding should focus on the information captured by

   

fihodosh  young   hockenmaier

bbc captions
 feng and lapata      

consumption
has soared as
the real price of
drink has fallen

amd destroys
central vision

sbu captioned photo dataset  flickr 
 ordonez et al       

at the downers grove
i don t chew up the couch
train station  our condo and pee in the kitchen
building is in the
mama 
background   on our
way to the ag store in
chicago 

iapr tc   data set
 grubinger et al       

a blue and white airplane is standing on a grey airport 
a man and red cones are standing in front of it and two
red dressed hostesses and two passengers are directly
on the stairs in front of the airplane  a brown landscape
with high dark brown mountains with snow covered
summits and a light grey sky in the background 

figure    other data sets of images and captions
generic descriptions  this leaves the question of where to obtain a data set of images paired
with suitable descriptions to train automatic description systems on 
    the need for new data sets
while there is no dearth of images that are associated with text available online  we argue
that most of this text is not suitable for our task  some work  notably in the natural language
processing community  has focused on images in news articles  feng   lapata              
however  images are often only used to illustrate stories  and have little direct connection to
the text  figure    left   furthermore  even when captions describe the depicted event  they
tend to focus on the information that cannot be obtained from the image itself  similarly 
when people provide captions for the images they upload on websites such as flickr  figure   
center   they often describe the situation that the images were taken in  rather than what
is actually depicted in the image  that is  these captions often provide non visual or overly
specific information  e g   by naming people appearing in the image or the location where the
image was taken   there is a simple reason why people do not typically provide the kinds
of generic conceptual descriptions that are of most use for our purposes  gricean maxims of
relevance and quantity  grice        entail that image captions that are written for people
usually provide precisely the kind of information that could not be obtained from the image
itself  and thus tend to bear only a tenuous relation to what is actually depicted  or  to
state it more succinctly  captions are usually written to be seen along with the images they
accompany  and users may not wish to bore other readers with the obvious 
ordonez et al         harvested images and their captions from flickr to create the
sbu captioned photo dataset  but had to discard the vast majority of images because
their captions were not actually descriptive  further analysis of a random sample of    
images of their final data set revealed that the majority          of their captions describe
information that cannot be obtained from the image itself  e g   by naming the people or
locations appearing in the image   while a substantial fraction          only describe a small
detail of the image or are otherwise just commentary about the image  examples of these
issues are shown in figure    center   this makes their data set less useful for the kind
of image understanding we are interested in  unless they refer to specific entities one may
actually wish to identify  e g   celebrities or famous landmarks that appear in the image  
proper nouns are of little help in learning about visual properties of entity types unless one
   

fiframing image description as a ranking task

our data set of       flickr images with   crowd sourced captions
a man is doing tricks on a bicycle on ramps in front of a crowd 
a man on a bike executes a jump as part of a competition while the crowd watches 
a man rides a yellow bike over a ramp while others watch 
bike rider jumping obstacles 
bmx biker jumps off of ramp 
a group of people sit at a table in front of a large building 
people are drinking and walking in front of a brick building 
people are enjoying drinks at a table outside a large brick building 
two people are seated at a table with drinks 
two people are sitting at an outdoor cafe in front of an old building 

figure    our data set of images paired with generic conceptual descriptions
can infer what kind of entity they refer to   the iapr tc    data set  grubinger et al  
       which consists of        photographs is potentially more useful for our purposes  since
it contains descriptions of what can be recognized in an image without any prior information
or extra knowledge  however  the descriptions  which consist often of multiple sentences
or sentence fragments  have a tendency to be lengthy  average length       words  and
overly detailed  instead of focusing on the salient aspects of the photograph  for example 
in the photo of an airplane in figure    right   the two hostesses are barely visible but
nevertheless described in detail 
    our data sets
since the kinds of captions that are normally provided for images do not describe the images
themselves  we have collected our own data sets of images and captions  the captions
are obtained by using the crowdsourcing service provided by amazon mechanical turk
to annotate each image with five descriptive captions  by asking people to describe the
people  objects  scenes and activities that are shown in a picture without giving them any
further information about the context in which the picture was taken  we were able to
obtain conceptual descriptions that focus only on the information that can be obtained
from the image alone  our annotation process and quality control are described in detail in
rashtchian et al        s paper  we have annotated two different data sets in this manner 
      the pascal voc      data set
the first data set we produced is relatively small  and consists of only       images randomly selected from the training and validation set of the pascal      object recognition
challenge  everingham  gool  williams  winn    zisserman         it has been used by a
large number of image description systems  farhadi et al         kulkarni et al         li
et al         yang et al         mitchell et al         gupta et al          but since almost
all of these systems  the only exception being gupta et al         rely on detectors trained
   the data set of ordonez et al         also differs significantly in content from ours  while our collection
focuses on images of eventualities  i e  people or animals doing something  the majority of ordonez et
al s images          do not depict people or animals  e g   still lifes  landscape shots  

   

fihodosh  young   hockenmaier

on images from the same data set  felzenszwalb et al          it is unclear how well these
approaches would generalize to other domains where no labeled data to train detectors is
available  the captions in the pascal data set are also relatively simple  for example 
since the data set contains many pictures that do not depict or focus on people doing something      of the captions do not contain any verb  and an additional     of the captions
contain only the common static verbs sit  stand  wear  or look 
      the flickr  k data set
for the work reported in this paper we therefore collected a larger  more diverse data set
consisting of       images from the flickr com website  unlike the more static pascal
images  the images in this data set focus on people or animals  mainly dogs  performing
some action  examples from this data set are shown in figure    the images were chosen
from six different flickr groups   and tend not to contain any well known people or locations 
but were manually selected to depict a variety of scenes and situations  in order to avoid
ungrammatical captions  we only allowed workers from the united states who had passed
a brief spelling and grammar test we devised to annotate our images  because we were
interested in conceptual descriptions  annotators were asked to write sentences that describe
the depicted scenes  situations  events and entities  people  animals  other objects   we
collected multiple captions for each image because there is a considerable degree of variance
in the way many images can be described  as a consequence  the captions of the same
images are often not direct paraphrases of each other  the same entity or event or situation
can be described in multiple ways  man vs  bike rider  doing tricks vs  jumping   and
while everybody mentions the bike rider  not everybody mentions the crowd or the ramp 
the more dynamic nature of the images is also reflected in how they are being described 
captions in this data set have an average length of      words  compared to      words
in the pascal data set  and while     of the pascal captions contain no verb other
than sit  stand  wear  or look  only     of the captions for the flickr  k set contain no
verb  and an additional     contain only these common verbs  our data sets  the flickr
training test development splits and human relevance judgments used for evaluation of the
test items  section    are publicly available   the online appendix to this paper contains
our instructions to the workers  including the qualification test they had to pass before being
allowed to complete our tasks 

   systems for sentence based image description
since image description requires the ability to associate images and sentences  all image
description systems can be viewed in terms of an affinity function f  i  s  which measures the
degree of association between images and sentences  we will evaluate our ability to compute
such affinity functions by measuring performance on two tasks that depend directly on them 
given a candidate pool of sentences scand and a candidate pool of images icand   sentencebased image retrieval aims to find the image i  icand that maximizes f  i  sq   for a query
sentence sq  scand   conversely  image annotation aims to find the sentence s  scand that
   these groups were called strangers   wild child  kids in action   dogs in action  read the rules  
outdoor activities  action photography  flickr social  two or more people in the photo 
   http   nlp cs illinois edu hockenmaiergroup data html

   

fiframing image description as a ranking task

maximizes f  iq   s  for a query image iq  icand   in both cases  f  i  s  should of course be
maximized for image sentence pairs in which the sentence describes the image well 
image search 
image annotation 

i   arg maxiicand f  i  sq  
s

   

  arg maxsscand f  iq   s 

this formulation is completely general  although we will  for evaluation purposes  define
scand as the set of captions originally written for the images in icand   this does not have to
be the case  and scand could also  for example  be defined implicitly via a caption generation
system  in order to evaluate how well f generalizes to unseen examples  we will evaluate our
system on test pools itest and stest that are drawn from the same domain but are disjoint
from the training data dtrain    itrain   strain   and development data ddev    idev   sdev   
the challenge in defining f lies in the fact that images and sentences are drawn from two
different spaces  i and s  in this paper  we present two different kinds of image description
systems  one is based on nearest neighbor search  nn   the other uses a technique called
kernel canonical correlation analysis  kcca  bach   jordan        hardoon et al         
both rely on a set of known image sentence pairs dtrain    hi  si  
    nearest neighbor search for image description
nearest neighbor based systems use unimodal text and image similarity functions directly
to first find the image sentence pair in the training corpus dtrain that contains the closest
item to the query  and then score the items in the other space by their similarity to the
other item in this pair 
image retrieval  fnn  i  sq     fi  inn   i 

for hinn   snn i   arg max fs  sq   st      
hit  st idtrain

image annotation  fnn  iq   s    fs  snn   s  for hinn   snn i   arg max fi  iq   it  
hit  st idtrain

despite their simplicity  such nearest neighbor systems are non trivial baselines  for the
task of annotating images with tags or keywords  methods which annotate unseen images
with the tags of their nearest neighbors among training images are known to achieve competitive performance  makadia et al          and similar methods have recently been proposed
for image description  ordonez et al          since the task we address here does not allow
us to return items from the training data  but requires us to rerank a pool of unseen captions
or images  our nearest neighbor search requires two similarity functions  all of our nearestneighbor systems use the same image representation as our kcca based systems  described
in section      our main nearest neighbor system  nn  nn idf
f     treats the five captions
associated with each training image as a single document  it then reweights each token by
its inverse document frequency  idf  w   and defines the similarity of two sentences as the
f  measure  harmonic mean of precision and recall  computed over their idf reweighted
bag of words representation  if dtrain  w  is the subset of training images in whose captions
word w appears at least once  the inverse document frequency  idf  of w is defined as
 dtrain  
w   log  dtrain
 w       idf reweighting is potentially helpful for our task  since words that
describe fewer images may be particularly discriminative between captions 
   

fihodosh  young   hockenmaier

in the appendix  we provide results for nn systems that use the same text representation
as two of our kcca systems 
    kernel canonical correlation analysis for image description
most of the systems we present are based on a technique called kernel canonical correlation
analysis  bach   jordan        hardoon et al          we first provide a brief introduction 
and then explain how we apply it to our task 
      kernel canonical correlation analysis  kcca 
kcca is an extension of canonical correlation analysis  hotelling         which takes
training data consisting of pairs of corresponding items hxi   yi i drawn from two different
feature spaces  xi  x   yi  y   and finds maximally correlated linear projections x and
y of both sets of items into a newly induced common space z  since linear projections of
the raw features may not capture the patterns that are necessary to explain the pairing of
the data  kcca implicitly maps the original items into higher order spaces x   and y   via
kernel functions kx   hx  xi    x  xj  i  which compute the dot product of two data points
xi and xj in a higher dimensional space x   without requiring the explicit computation of
the mapping x   kcca then operates on the two resulting kernel matrices kx  i  j   
hx  xi    x  xj  i and ky  i  j    hy  yi    y  yj  i which evaluate the kernel functions on
pairwise combinations of items in the training data  it returns two sets of projection weights 
 and     which maximize the correlation between the two  projected  kernel matrices 
          arg max q
 

  kx ky 
   k x      kx      k y       ky  

   

this can be cast as a generalized eigenproblem  kx  i   ky  ky  i   kx       
and solved by partial gram schmidt orthogonalization  hardoon et al         socher   li 
       the regularization parameter  penalizes the size of possible solutions  and is used
to avoid overfitting  which arises when the matrices are invertible 
one disadvantage of kcca is that it requires the two kernel matrices of the training
data to be kept in memory during training  this becomes prohibitive with very large data
sets  but does not cause any problems here  since our training data consists of only      
items  see section      
      using kcca to associate images and sentences
kcca has been successfully used to associate images  hardoon et al         hwang  
grauman        hardoon et al         or image regions  socher   li        with individual
words or sets of tags  in our case  the two original spaces x   i and y   s correspond to
images and sentences that describe them  images i  i are first mapped to vectors ki  i 
whose elements ki  i  t    ki  it   i  evaluate the image kernel function ki on i and the t th
image in dtrain   similarly  sentences s  s are mapped to vectors ks  s  that evaluate the
sentence kernel function ks on s and the sentences in dtrain   the learned projection weights
        then map ki  i  and ks  s  into our induced space z  in which we expect images
to appear near sentences that describe them well  in a kcca based image annotation or
   

fiframing image description as a ranking task

search system  we therefore define f as the cosine similarity  sim  of points in this new space 
fkcca  i  s    sim ki  i   ks  s  

   

we now describe the image and text kernels used by our kcca systems 
    image kernels
in contrast to much of the work done on image description  which assumes the existence of
a large number of preexisting detectors  the image representations used in this paper are
very basic  in that they rely only on three different kinds of low level pixel based perceptual
features that capture color  texture  varma   zisserman        and shape information in
the form of sift descriptors  lowe        vedaldi   fulkerson         we believe that
this establishes an important baseline  and leave the question of how more complex image
representations affect performance to future work  we use two different kinds of kernels 
a histogram kernel k histo   which represents each image as a single histogram of feature
values and computes the similarity of two images as the intersection of their histograms 
and a pyramid kernel k py  lazebnik  schmid    ponce         which represents each image
as a pyramid of nested regions  and computes the similarity of two images in terms of the
intersection of the histograms of corresponding regions  in both cases  we compute a separate
kernel for each of the three types of image features and average their result 
      the histogram kernel  k histo  
each image xi is represented as a histogram hi of discrete valued features  such that hi  v 
is the fraction of pixels in xi with value v  the similarity of two images xi and xj is defined
as the intersection of their histograms  i e  the percentage of pixels that can be mapped
onto a pixel with the same feature value in the other image 
k xi   xj    

v
x

min hi  v   hj  v  

   

v  

we combine three kernels based on different kinds of visual features  kc captures color 
represented by the three cielab coordinates  kt captures texture  represented by descriptors which capture edge information at different orientations centered on the pixel  varma
  zisserman         ks is based on sift descriptors  which capture edge and shape information in a manner that is invariant to changes in rotation and illumination  and have been
shown to be distinct across possible objects of an image lowe        vedaldi   fulkerson 
      we use     color words      texture words and     sift words  obtained in an unsupervised fashion by k means clustering on       points of     images from the pascal
     data set  everingham et al          our final histogram kernel k histo is the average
of the responses of the three kernels kchisto   kthisto   kshisto   taken to the pth power 

p
  x
k histo  xi   xj    
kfhisto  xi   xj  
   
 
f  c s t 

   

fihodosh  young   hockenmaier

      the pyramid kernel k py
the spatial pyramid kernel  lazebnik et al         is a generalization of the histogram kernel
that captures similarities not just at a global  but also at a local level  each image xi is
represented at multiple levels of scale l  l             such that each level partitions the
image into a smaller and smaller grid of cl    l   l cells  c       c       c         and
each cell c is represented as a histogram hic   the similarity of images xi and xj at level l 
iijl   is in turn defined as the sum of the histogram similarities of their corresponding cells
 l        cl at this level 
iijl

 

cl x
v
x

min hic  v   hjc  v  

   

c  l v  

although similarities at level l subsume those at a more fine grained level l      iijl 
iijl      similarities that hold at a more fine grained level are deemed more important  since
they indicate a greater local similarity  the pyramid kernel therefore proceeds from the
most fine grained  l   l  down to the coarsest  whole image  scale  l       and weights the
 
similarities first encountered at level l  iijl  iijl     by  ll
 

k

py

 xi   xj    

iijl

 

l 
x
l  

 
 i l  iijl    
 ll ij

   

l

 

    x  
i  
il
 l ij
 ll   ij
l  

we again compute three separate pyramid kernels kcpy   ktpy   kspy based on the same
color  texture and sift features as described above  and combine them into a single pyramid
kernel k py   as in equation   
    basic text kernels
we examine three different basic text kernels  a bag of words  bow  kernel  hwang and
graumans        tagrank kernel  and a truncated string kernel  tri  
      the bag of words kernel  bow 
since bag of words representations have been successfully used for other tasks involving text
and images  e g   grangier   bengio        hardoon et al          we include a basic bag
of words kernel  which ignores word order and represents each caption simply as a vector
of word frequencies  the bow kernel function is defined as the cosine similarity of the
corresponding bag of words vectors  we either merge the five captions of each training item
into a single document  bow    or reduce each training item to a single  arbitrarily chosen 
caption  bow    a words frequency can also be reweighted by its idf score  as in the
 dtrain  
nearest neighbor approach  the idf weight of a word w is defined as w   log  dtrain
 w      
where dtrain  w  is the subset of training images in whose captions word w appears at least

   

fiframing image description as a ranking task

once  we found the square root of w  bow 
idf score w  bow idf   



idf  

to give better results than the standard

      the tag rank kernel  tagrank 
hwang and grauman        apply kcca to keyword based image annotation and retrieval 
they focus on a data set where each image is paired with a list of tags ranked by their importance  and propose a new kernel for this kind of data  this so called tag rank kernel
 tagrank  is a variant of the bag of words kernel that aims to capture the relative importance of tags by reweighting them according to their position in this list  although hwang
and grauman do not evaluate the ability of their system to associate images with entire
sentences  they also consider another data set in which the lists of tags correspond to the
words of descriptive captions  and argue that the linear order of words in these captions also
reflects the relative importance of the corresponding objects in the image  so that words
that appear at the beginning of the sentence describe more salient aspects of the image 
in the tagrank kernel  each sentence is represented as two vectors   a and  r  in  a  the
weight of each word is based on its absolute position  so that the first words in each sentence
are always assigned a high weight  in this absolute tag rank representation  each caption
s is mapped to a vector  a     a           a  v      where  v   is the size of the vocabulary   a i 
depends on the absolute position pi of wi in s  if wi occurs multiple times in s  pi is averaged
over all its positions   if wi does not occur in s   a i       otherwise 
 a i   

 
log       pi  

   

in  r  the weight of a word depends on how its current position compares to the distribution of positions it occupies in the training data  the intuition behind this relative rank
representation is that words should have a higher weight when they occur earlier in the sentence than usual  here  each caption s is mapped to a vector  r     r           r v    of relative
tag ranks  again  when wi does not appear in s   r i       otherwise wi s relative tag rank
 r i  indicates what percent of its occurrences in the training data appear after position pi  
defining
p nik as the number of times word wi appears in position k in the training data  and
ni   k nik as the total frequency of wi in the training data 
ppi
 r i      

k   nik

    

ni

the final kernel kt is given by the average of two   kernels computed over  r and  a   and
  are normalization terms  
 



 
v
v
 
  x   ri  k    rj  k   
  x   ai  k    aj  k   
kt  xi   xj    
exp
  exp
    
 
 
 ri  k     rj  k 
  
 ai  k     aj  k 
k  

k  

since each image in our training data is associated with multiple  independently generated captions  we evaluate the kernel separately on each sentence pair and average the
response  instead of treating the multiple sentences as a single document 
   

fihodosh  young   hockenmaier

the tagrank kernel is relatively sensitive to overall sentence length  especially in cases
where the subject is preceded by multiple adjectives or other modifiers  a very large brown
dog vs  a dog    in english  the absolute tag rank will generally assign very high weights
to the subjects of sentences  lower weight to verbs  and even lower weight to objects or scene
descriptions  which tend to follow the main verb  the relative tag rank may not downweight
verbs  objects and scene descriptions as much  as long as they are always used in similar
positions in the sentence  
      the trigram kernel  tri 
since bag of words representations ignore which words appear close to each other in the
sentence  they lose important information  an image of a small child with red hair playing
with a large brown dog on white carpet looks quite different from one of a small white dog
playing with a large red ball on brown grass  although both descriptions share the majority
of their words  to capture this information  we define a trigram kernel as a truncated variant
of string kernels  shawe taylor   cristianini        that considers not just how many single
words two captions share  but also how many short sequences  pairs and triples  of words
occur in both 
a word sequence w   w     wk is an ordered list of words  a sentence s   s     sn contains
w  w  s  as long as the words in w appear in s in the order specified by w  that is  the
sentence a large white dog runs and catches a red ball on the beach  when lemmatized 
contains both the subject verb object triple dog catch ball and the subject verb location
triple dog run beach  formally  every substring  i  j    si    sj in s that starts with si   w   
ends in sj   wk   and contains w is considered a match between s and w  ms w is the set of
all substrings in s that match the sequence w 
ms w     i  j    w   w     wk  si    sj   w    si   wk   sj  

    

when w is restricted to individual words  k       string kernels are identical to the standard
bow kernel 
a match between strings s and s  is a pair of substrings  i  j   s and  i    j      s  that
both match the same word sequence w  standard string kernels k s  s    weight matches
 
 
by a factor  ji     j i     that depends on an adjustable parameter  and the respective
length of the matching substrings 
k s  s     

x

x

x

 

 

 ji     j i    

    

w  i j ms w  i   j    ms   w

in order to distinguish between the length of the matching subsequence  l w   and the
length of the gaps in  i  j  and  i    j      we replace  by two parameters m   g   and reformulate
this as 
k s  s     

x

x

x

 

 

i     l w 
 l w 
 ji     j
m
g

    

w  i j ms w  i   j    ms   w

we found that a gap score of g      which means that gaps are not penalized  and a
match score of m       perform best on our task 
   

fiframing image description as a ranking task

although string kernels are generally defined over sequences of arbitrary length  k    
we found that allowing longer sequences did not seem to impact performance on our task but
incurred a significant computational cost  intuitively  word pairs and triplets represent most
of the linguistic information we need to capture beyond the bow representation  since they
include head modifier dependencies such as large dog vs  small dog and subject verb object
dependencies such as child play dog vs  dog play ball  we therefore consider only sequences
up to length k     with w restricted to sequences of length k    and ms w    ms w    this
yields the following trigram kernel  tri  
ktri  s  s     

x

ms w ms   w  l w 
m

    

w k 

to deal with differences in sentence length  we normalize the kernel response between
two examples by the geometric mean of the two example responses with themselves 
since the trigram kernel also captures sequences that are merely coincidental  such as
large white red  it may seem advantageous to use richer syntactic representations such
as dependency tree kernels  moschitti  pighin    basili         which only consider word
tuples that correspond to syntactic dependencies  however  such kernels are significantly
more expensive to compute  and initial experiments indicated that they may not perform as
well as the trigram kernel  we believe that this is due to the fact that our image captions
contain little syntactic variation  and that hence surface word order may be sufficient to
differentiate e g  between the agent of an action  whose mention will be the subject of the
sentence  and other participants or entities  whose mentions will appear after the verb  
on the other hand  many of our image captions contain a lot of syntactic ambiguity  e g 
multiple prepositional phrases   and a vocabulary that is very distinct from what standard
parsers are trained on  it may be that we were not able to benefit from using a richer
representation simply because we were not able to recover it with sufficient accuracy 
in order to capture
the relative importance of words  we can also reweight sequences

by the idf  or idf  weight of theirqwords  with 
w defined as before  the idf weight of
j
a sequence w   wi    wj is w  
idf weighted trigram kernel ktriidf
k i wk   the
 tri 



idf  

is therefore

ktriidf  s  s     

x

w ms w ms   w  l w 
m

    

w k 

    extending the trigram kernel with lexical similarities
one obvious shortcoming of the basic text kernels is that they require exact matches between
words  and cannot account for the fact that the same situation  event  or entity can be
described in a variety of ways  see figure   for examples   one way of capturing this
linguistic diversity is through lexical similarities which allow us to define partial matches
between words based on their semantic relatedness  lexical similarity have found success in
other tasks  e g  semantic role labeling  croce  moschitti    basili         but have not been
fully exploited for image description  ordonez et al         define explicit equivalence classes
of synonyms and hyponyms to increase the natural language vocabulary corresponding to
each of their object detectors  e g  the word dalmatian may trigger the dog detector  
   

fihodosh  young   hockenmaier

but do not change the underlying  pre trained detectors themselves  ignoring the potential
variation of appearance between  e g   different breeds of dog  similarly  yang et al s       
generative model can produce a variety of words for each type of detected object or scene 
but given an object or scene label  the word choice itself is independent of the visual features 
we therefore also investigate the effect of incorporating different kinds of lexical similarities
into the trigram kernel that allow us to capture partial matches between words  we did
not explore the effect of incorporating lexical similarities into the tag rank kernel  since it is
unclear how they should affect the computation of ranks within a sentence 
      string kernels with lexical similarities
since standard lexical similarities sims  w  wi   do not necessarily yield valid kernel functions 
we follow bloehdorn  basili  cammisa  and moschitti        and use these similarities to
map each word w to vectors w
  s in an n  dimensional space  defined by a fixed vocabulary of
size n   each vector component w
  s  i  corresponds to the similarity of w and wi as defined
by s  
    

w
  s  i    sims  w  wi  

we then define the corresponding word kernel function s  w  w     which captures the partial
match of words w and w  according to s   as the cosine of the angle between w
  s and w
  s   
s  w  w      cos  
ws   w
  s   

    

s may only be defined over a subset of the vocabulary  the similarity of words outside
of its vocabulary is defined by the identify function  as in the standard string kernel 
the similarity of sequences w and w  of length l is defined as the product of the word
kernels over the corresponding pairs of sequence elements wi   wi   
s  w  w     

l
y

s  wi   wi   

    

i  

if s  w     w   s  w    w       l w      l w   is the set of sequences that have a non zero
match with w  the string kernel ks with similarity s is 
ks  s  s     

x

x

ms w ms   w   l w 
s  w    w 
m

    

w w  s  w 


idf
 
to obtain
 the idf weighted version of this kernel  ks  s  s    the inner term is multiplied by w w   
x x p
ks  s  s     
w w  ms w ms   w  l w 
s  w    w 
    
m
w w  s  w 

in our experiments  we use the trigram variants of these kernels  and restrict w again to
sequences of length k    
we consider three different kinds of lexical similarities  the wordnet based lin similarity
 lin         lin    a distributional similarity metric  d    and a novel alignment based
   

fiframing image description as a ranking task

similarity metric  a    which takes advantage of the fact that each image is associated with
five independently generated captions  all metrics are computed on our training corpus 
distributional similarity is also computed on the british national corpus  bnc consortium 
       both corpora are lemmatized  and stop words are removed before similarities are
computed  since almost any pair of words will have a non zero similarity  the word kernel
matrices are very dense  but since most of these similarities are very close to zero  they have
very little effect on the resulting kernel  we therefore zero out entries smaller than      in
the alignment based kernel a and less than      in any distributional kernel dc  
      the lin similarity kernel  lin  
lins        similarity relies on the hypernym hyponym relations in wordnet  fellbaum 
      as well as corpus statistics  wordnet is a directed graph in which the nodes  synsets 
represent word senses and the edges indicate is a relations  a parent sense  e g   dog    is a
hypernym of its children  e g   poodle  or dachshund     kernels based on lins similarity
have been found to perform well on tasks such as text categorization  bloehdorn et al         
but with the exception of farhadi et al          who incorporate lins similarity into their
model  but do not evaluate what benefit they obtain from it  wordnets hypernym hyponym
relations have only been used superficially for associating images and text  weston et al  
      ordonez et al         gupta et al          the lin similarity of two word senses si   sj
is defined as
  log p  lcs si   sj   
simlin  si   sj    
    
log p  si     log p  sj  
lcs s    s    refers to the lowest common subsumer of s  and s  in wordnet  i e  the most
specific synset that is an ancestor  hypernym  of both s  and s    p  s  is the probability that
a randomly drawn word is an instance of synset s or any of its descendants  hyponyms   we
use our training data to estimate p  s   and follow bloehdorn et al         in assigning each
word w its most frequent  first  noun sense sw in wordnet      hence  we represent each
word w with wordnet sense s as a vector w
  lin of lin similarities over its hypernyms h sw   
  log f  si   

 log f  s   log f  si   
w
  lin  i   
 


 

si  h s 
sw   si
otherwise

    

      distributional similarity  dc  
distributional similarity metrics are based on the observation that words that are similar to
each other tend to appear in similar contexts  jurafsky   martin         the components of
w
  dc are the non negative pointwise mutual information scores  pmi  of w and wi   computed
on the corpus c 


pc  w  wi  
    
w
  dc  i    max    log 
pc  w pc  wi  
pc  w  is the probability that a random sentence in c contains w  and pc  w  wi   is the
probability that a random sentence in c contains both w and wi   we compute two variants
   

fihodosh  young   hockenmaier

of the same metric  dic is computed on the image captions in our training corpus  and is
defined over the cooccurrences of the       words that appear at least   times in this corpus 
while dbnc uses the british national corpus  bnc consortium         and is defined for
the       words that appear at least   times in both corpora  but considers their pmi scores
against the         words that appear at least   times in the bnc 
      alignment based similarity  a  
we also propose a novel  alignment based  similarity metric  a    which takes advantage
of the fact that each image is associated with five independently generated captions  and
is specifically designed to capture how likely two words are to describe the same event
or entity in our data set  we borrow the concept of alignment from machine translation
 brown  pietra  pietra    mercer         but instead of aligning the words of sentences in two
different languages  we align pairs of captions that describe the same image  this results in a
similarity metric that has better coverage on our data set than wordnet based metrics  and
is much more specific than distributional similarities which capture broad topical relatedness
rather than semantic equivalence  instead of aligning complete captions  we have found it
beneficial to align nouns and verbs independently of each other  and to ignore all other parts
of speech  we create two versions of the training corpus  one consisting of only the nouns
of each caption  and another one consisting only of the verbs of each caption  we then
use giza    och   ney        to train ibm alignment models     brown et al        
over all pairs of noun or verb captions of the same image to obtain two sets of translation
probabilities  one over nouns  pn   w   and one over verbs  pv   w    finally  we combine
the noun and verb translation probabilities as a sum weighted by the relative frequency with
which the word w was tagged as a noun  pn  w   or verb  pv  w   in the training corpus 
the ith entry in wa is therefore 
w
  a  i    pn  wi  w pn  w    pv  wi  w pv  w 

    

we define the noun and verb vocabulary as follows  words that appear at least   times
as a noun  and are tagged as a noun in at least     of their occurrences  are considered
nouns  but since verbs are more polysemous than nouns  leading to broader translation
probabilities  and are often mistagged as nouns in our domain  we only include those words
as verbs that are tagged as verbs at least    times  and in at least     of their occurrences 
this results in      noun and     verb lemmas  including    that can be nouns or verbs 
we use the opennlp pos tagger before lemmatization 
      comparing the similarity metrics  figure   
figure   illustrates the different similarity metrics  using the words rider and swim as
examples  while distributional similarities are high for words that are topically related
 e g   swim and pool    the alignment similarity tends to be high for words that can be used
to describe the same entity  usually synonyms or hyper hyponyms  or activity such as swim
or paddle  distributional similarities that are obtained from the image captions are very
specific to our domain  the bnc similarities are much broader and help overcome data
sparsity  although the bnc has relatively low coverage of the kinds of sports that occur in
our data set  the lin similarity associates swim with hypernyms such as sport and activity 
   

fiframing image description as a ranking task

comparing similarity metrics  the five words most similar to rider and swim
alignment
strain
wi
a

wi

rider

biker
bicyclist
cyclist
bmx
bicycler

    
    
    
    
    

bike
dirt
motocross
motorcycle
ride

    
    
    
    
    

ride
horse
race
bike
jockey

swim

retrieve
paddle
dive
come
wade

    
    
    
    
    

pool
trunk
water
dive
goggles

    
    
    
    
    

fish
water
sea
pool
beach

corpus
w

distributional
strain
bnc
dic wi
dbnc

lin
strain
wi

lin

    
    
    
    
    

traveler
cyclist
bicyclist
horseman
jockey

    
    
    
    
    

    
    
    
    
    

bathe
sport
football
activity
soccer

    
    
    
    
    

figure    a comparison of lexical similarities for the noun rider and the verb swim
or other kinds of sport such as football or soccer  this makes it the least suitable similarity
for our task  see also section       for experimental results   since these terms should not be
considered similar for our purposes of identifying the different ways in which visually similar
events or entities can be described 
      combining different similarities
combining the different distributional and the alignment based similarities allows us to
capture the different strengths of each method  we define an averaged similarity which
captures aspects of the distributional similarities computed over both corpora 
dbnc  w  w      dic  w  w   
    
 
for every distributional kernel d  w  w     we also define a variant d a  w  w    which
incorporates alignment based similarities by taking the maximum of either kernel  
dbnc ic  w  w     

d a  w  w      max a  w  w     a  w  w    

    

   evaluation procedures and metrics for image description
in order to evaluate scoring functions f  i  s  for image caption pairs  we need to evaluate
their ability to associate previously unseen images and captions with each other  in analogy
to caption generation systems  we first examine metrics that aim to measure the quality of
a single image description pair  section       here  we focus on the image annotation task 
and restrict our attention to the first caption returned for each test item  and a subset of
our systems  we collect graded human judgments from small number of native speakers of
american english  and investigate whether these expert judgments can be approximated
   this operation may not preserve the positive definiteness of the matrix required to be a valid kernel  but
this simply means we effectively use  plain  cca with this representation 

   

fihodosh  young   hockenmaier

with automatically computed bleu  papineni et al         or rouge  lin   hovy       
scores  or with simpler crowdsourced human judgments that can be collected on a much
larger scale  in section      we consider approaches to evaluation that aim to measure the
quality of the ranked list of image caption pairs returned by each system  and allow us to
evaluate a large number of systems  for reasons of space  we focus most of our discussion
again on only a subset of our systems  and refer the interested reader to appendix b for
complete results  since the candidate pool contains one sentence or image that was originally
associated with the query image or sentence  we first compare systems by the rank and recall
of this original item  these metrics can be computed automatically  but should only be
considered lower bounds on actual performance  since each image may be associated with a
number of captions that describe it well or perhaps with only minor errors  we then show
that the crowdsourced human judgments can be mapped to binary relevance judgments that
correlate well with the more fine grained expert judgments  and consider metrics based on
these relevance judgments 
    experimental setup
we now describe the data  the tasks  and the systems we evaluate in our experiments 
      the data
since the pascal      data set contains only a total of       images  we perform our
experiments exclusively on the flickr  k set  we split       images from this corpus  see
section      into three disjoint sets  the training data dtrain   hitrain   strain i consists of
      images  each associated with five captions  whereas the test and development data 
dtest and ddev   each consist of       images associated with one  arbitrarily chosen  caption 
all captions are preprocessed by spellchecking with linux spell  normalizing compound words
 e g   t shirt  t shirt  and tee shirt  t shirt   stop word removal  and lemmatization 
      the tasks
we evaluate our systems on two tasks  sentence based image annotation  or description 
and sentence based image search  for image search  the task is to return a ranked list of the
      images in itest for each of the captions  queries  in stest   image annotation is defined
analogously as a retrieval problem  the task is to return a ranked list of the       captions
in stest for each of the       test  query  images in itest   in both cases  the ranked lists are
produced independently for each of the       possible queries 
      the systems
we have a total of    different systems  each of which uses either a nearest neighbor approach
or kcca  paired with a different combination of image and text representations  but for
the purposes of discussing different evaluation metrics  we will focus on only a small number
of these systems  the best performing nearest neighbor based system  nn  nn idf
f     and a
small number of kcca based systems with with different text kernels  bow  and bow 
both use the simple bag of words kernel  tagrank uses
hwang and graumans       

idf
kernel  tri  uses the trigram kernel  and tri sem  tri a dbnc ic in appendix b  uses the
   

fiframing image description as a ranking task



idf reweighted trigram kernel with all distributional and the alignment based similarities 
with the exception of bow   where we have arbitrarily selected a single caption for each
training image  all other models use all five captions for the training images  for bow  
we merge them into a single document  in all other cases  we follow moschitti        and
sum the kernel responses over the cross product of sentences before normalization  all of
these systems  including nn  use the pyramid kernel as their image representation  for the
large scale evaluations in section      the scores of all models are given in appendix b 
all our systems use hardoon et al s        kcca implementation  which allows us to
vary the regularization parameter   we also vary n  the number of dimensions  largest
eigenvalues  in the learned projection the allowable values for these parameters were based
on early exploratory experiments  in the experiments reported in this paper   is sampled
from   possible values                   and n is chosen from    possible values in the range
of             there are two additional parameters that are fixed in advance for each text
image kernel pair  the image kernels are either squared or cubed  and the text kernels are
regularized by multiplying the values on the diagonal by a factor d in the range of         
for each kernel and for each of the two tasks  image annotation and search   we then use
the development set to pick five settings of n and  that maximize the recall of the original
item as the first result  five settings that maximize its recall among the first five results 
and five settings that maximize its recall among the first ten results  yielding a total of   
different models for each pair of kernels and each task  for each query image  annotation 
or caption  search  in the test set  each of these    models returns a ranking of all       test
items  sentences or images   to combine these    rankings  we use borda counts  van erp  
schomaker         a simple  deterministic method for rank aggregation  with n items to be
ranked  each system assigns a score of n  r to the item it ranks in position r       n    
and the final rank of each item is determined by the sum of its scores across all systems 
we break ties between items by the median of their ranks across all models 
    metrics for the quality of individual image caption pairs
before we consider metrics that consider the quality of the ranked list of results  section      
we first examine metrics that measure the quality of individual image caption pairs 
      human evaluation with graded expert judgments
expert scores the decision of how well a caption describes an image ultimately requires
human judgment  for the caption generation task  a number of different evaluation schemes
have been proposed for image description  ordonez et al         presented judges with a
caption produced by their model and asked them to make a forced choice between a random
image and the image the caption was produced for  and kuznetsova et al         asked judges
to choose between captions from two of their models for a given test image  such forced
choice tasks may give a clear ranking of models  but cannot be compared across different
experiments unless the output of each system is made publicly available  one advantage
of framing image description as a ranking task is that different systems can be compared
directly on the same test pool  forced choice evaluations also do not directly measure the
quality of the captions  following common practice in natural language generation  yang
et al         and kulkarni et al         evaluated captions on a graded scale for relevance
   

fihodosh  young   hockenmaier

    describes the image
without any errors
 score     

the selected caption    
    describes the image
with minor errors
 score     

    is somewhat
related to the image
 score     

    is unrelated
to the image
 score     

a girl wearing a
yellow shirt and
sunglasses smiles 

a man climbs
up a sheer wall
of ice 

a miami basketball
player dribbles by an
arizona state player 

a group of people
walking a city street in
warm weather 

a boy jumps into
the blue pool
water 

a dog in a grassy field 
looking up 

basketball players in
action 

a man riding a motor
bike kicks up dirt 

dogs pulling
a sled in a
sled race 

two little girls
practice martial
arts 

a snowboarder in the
air over a snowy
mountain 

a child jumping
on a tennis court 

a boy in a blue life
jacket jumps into the
water 

a black dog with
a purple collar
running 

figure    our    rating scale for the fine grained expert judgments  with actual examples
returned by our best model  tri sem 
and readability  while li et al         added a creativity score  and mitchell et al        
compared systems based on whether the captions describe the main aspects of the images 
introduce objects in an appropriate order  are semantically correct  and seemed to have been
written by a human 
since the captions in our test pool are all produced by people  we do not need to evaluate
their linguistic quality  and can focus on their semantic correctness  in order to obtain a
fine grained assessment of description quality  we asked three different judges to score imagecaption pairs returned by our systems on a graded scale from   to    the judges were   
adult native speakers of american english  mostly recruited from among the local graduate
student population  in contrast to the anonymous crowdsourcing based evaluation described
in section        we will refer to them as experts  the rating scale is illustrated in figure  
with actual examples returned by our models  a score of   means that the caption describes
the image perfectly  without any mistakes   a score of   that the caption almost describes
the image  minor mistakes are allowed  e g  in the number of entities   whereas a score of
  indicates that the caption only describes some aspects of the image  but could not be
used as its description  and a score of   indicates that the caption bears no relation to the
image  the online appendix to this paper contains our annotation guidelines  annotators
took on average ten minutes per    image caption pairs  and all image caption pairs were
judged independently by three different annotators  inter annotator agreement  measured
as krippendorffs          is high            artstein   poesio         the final score of
each image caption pair was obtained by averaging the three individual scores  since this
is the most time consuming evaluation  we only judged the highest ranked caption for each
test image on the annotation task  and only focused on the subset of our models described
above  to gauge the difficulty of this task on our data set  we also include a random
baseline  since we only evaluate a single caption for each image  we are interested in the
percentage of images for which a suitable caption was returned  we therefore show each
models cumulative distribution of test items with scores at or above thresholds ranging
   

fiframing image description as a ranking task

quality of first caption  image annotation 
cumulative distribution of expert scores     x 
                                      
       
       
bow 
       
bow 
   
    
tagrank    
    

   
   
   
    
    

   
   
    
    
    

   
    
    
    
    

   
    
    
    
    

   
    
    
    
    

tri sem     

    

    

    

    

    

random
nn

    

table    cumulative distribution of expert judgments on our    scale  figure     indicating
what percentage of image caption pairs are judged to be at or above a given score  scores
are averaged over three judges  superscripts indicate statistically significant difference to
tri sem     p          p           p        
from     to      each threshold can be interpreted as a more or less strict mapping of the
fine grained scores into binary relevance judgments  in order to assess whether the difference
between models at any given threshold reaches statistical significance  we use mcnemars
significance test  a paired  non parametric test that has been advocated for the evaluation
of binary classifiers  dietterich         given the output of models a and b on the same
set of items  mcnemars test considers only the items on which a and bs output differ  the
discordant pairs of output  to test the null hypothesis that both outputs are drawn from
the same underlying population  among these discordant pairs  it compares the proportion
of items for which model a is successful but model b is not with the proportion of items for
which model b is successful but model a is not  in our results tables   superscripts indicate
whether the difference between a model and tri sem is statistically significant     p      
   p           p         
expert results  table    we first interpret the expert scores as binary relevance
judgments  and therefore show their cumulative distribution for different thresholds in  we
see very clear differences between the random baseline  nn  and the kcca models at all
thresholds  the differences between nn and the random model  as well as between any
kcca model and nn are highly significant  p          at any threshold  while the random
baseline returns a perfect caption for      of the images  and a good caption  assuming
a threshold of        for      of the images  our best kcca model  tri sem  returns
a perfect caption for       and a good caption for       of the images  however  the
differences among the kcca models are more subtle  and may only become apparent at
lower thresholds  there is no significant difference between bow  and tagrank at any
threshold  but they are both significantly better than bow   p          at thresholds of
     and above  tri sem outperforms all other models  but the differences to bow  and
tagrank only reach statistical significance when the threshold of what is considered a
suitable caption is lowered to either       p         or      p          or to       p        
or to       p           this lack of statistical significance can be partially explained by
the fact that mcnemars test has relatively low power when the percentage of items on
which the two models are successful is very low  as is the case for the higher thresholds here 

   

fihodosh  young   hockenmaier

we will show in sections       and       below that there is a very significant difference
between tri sem and these two models on image annotation once we extend the analysis
beyond the highest ranked caption  this shows that evaluations that are only based on a
single caption returned per image may fail to uncover significant differences between models
that become apparent once multiple results are considered  it may also be important to
consider performance on both annotation and retrieval  on the image retrieval task  we
will see that tri sem significantly outperforms all other models even when only the first
result is considered  table   also reveals another artefact of mcnemars test  since it is
not based on absolute differences in performance but on the number of discordant pairs 
the difference between bow  and tri sem at thresholds      and     is considered less
significant than that between bow  and tri sem at the same thresholds  even though
bow s scores are lower than bow s  in table    we present the systems average expert
scores  and use fishers randomization test to determine statistical significance  according
to this evaluation  tri sem is very significantly better than all other models  p         in
all cases   but since the average score of tri sem is only       this difference is not reflected
at the higher thresholds in the cumulative distribution shown in table   
      automatic evaluation with bleu and rouge
since human judgments are expensive and time consuming to collect  we now examine how
well they can be approximated by bleu  papineni et al         and rouge  lin        
two standard metrics for machine translation and summarization 
bleu and rouge scores bleu and rouge scores can be computed automatically
from a number of reference captions  and have been used to evaluate a number of caption
generation systems  kulkarni et al         ordonez et al         li et al         kuznetsova
et al         yang et al         gupta et al          although it is unclear how well they
correlate with human judgments on this task 
given a caption s and an image i that is associated with a set of reference captions ri  
the bleu score of a proposed image caption pair  i  s  is based on the n gram precision of
s against ri   while rouge is based on the corresponding n gram recall  as is common for
image description  we only consider unigram based scores  only      of all possible imagecaption pairs in the test have a non zero bigram based bleu   score  but       set have
a non zero bleu   score   we also ignore bleus brevity penalty  since our data set has
relatively little variation in sentence length  and we would like to avoid penalizing short  but
generic captions that include few details but are otherwise correct  hence  if cs  w  is the
number of times word w occurs in s 
p

bleu i  s   
rouge i  s 

 

min cs  w  maxrri cr  w  
p
ws cs  w 
p
p
min cs  w  cr  w  
rri
p wrp
rr
wr cr  w 
ws

    

i

both reference and candidate captions are preprocessed  we first tokenize the sentences with
the opennlp  tools  then we break up hyphenated words  stripping out non alphanumeric
   http   opennlp apache org

   

fiframing image description as a ranking task

avg  score of first caption
 image annotation 
expert

bleu


rouge

bow 
bow 
tagrank

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

tri sem

    

    

    

random
nn



table    comparison of averaged scores according to the   point expert evaluation  figure     bleu and rouge  using all five test captions as reference  superscripts indicate
statistically significant difference to tri sem      p          p           p        
and hyphen characters  and converting all words to lower case  following the work of lin
        we use a stemmer  porter        and remove stopwords before compute rouge
scores  we compute the bleu and rouge score of a system as the average bleu or
rouge scores of all items in the test set  
we use fishers randomization test  fisher        smucker  allan    carterette       
to assess the statistical significance of the difference between models  this is a paired 
sampling based test that evaluates the null hypothesis that the results of models a and
b are produced by the same underlying distribution  in each sample  the scores that a
and b assign to each test item are randomly reassigned to the two models  and p values
are obtained by comparing the actual difference between a and bs performance to the
fraction of samples with equal or greater difference between the models  we sample        
reassignments of the entire test set 
bleu and rouge results  table    table   shows the average bleu and rouge scores
of the highest ranked caption pairs returned by each image annotation systems  computed
against a reference pool consisting of the five original captions for each test image  including
the caption that was randomly selected to be part of the candidate pool   these scores
lead to the same broad conclusions as the average expert scores  all metrics find very clear
differences  p           between the random baseline and any of the other models  as
well as between nn and any of the kcca models  and none find any significant difference
between bow  and tagrank  tri sem outperforms the other kcca models according
to all metrics  but both the expert evaluation and rouge find a much larger difference to
bow   experts  p          rouge  p          than to tagrank  experts  p         
rouge  p           bleu only finds a significant difference to tagrank  p          but
not to bow   p          which indicates bleu may be less well suited to identify more
subtle differences between systems 
agreement of bleu and rouge with expert scores since it is difficult to measure
directly how well the bleu and rouge scores agree with the expert judgments  we consider
   a systems bleu score is usually computed at the corpus level  but since we are only dealing with
unigram scores and evaluate all systems on sentences from the same corpus  the averaged sentence level
bleu scores of our systems we report are almost identical  r          to their corpus level bleu scores 

   

fihodosh  young   hockenmaier

a number of different relevance thresholds for each type of score  b   r   and e    and turn
them into binary relevance judgments  this allows us to use cohens         to measure
the agreement between the corresponding binarized scores  since bleu and rouge both
require a set of reference captions for each test image  we compare four different ways of
defining the set of reference captions  for detailed scores  see tables   and   in the appendix  
since our data set contains multiple descriptions for each image  we first use all five
captions as reference  in this setting  bleu reaches the best agreement           against
e       with b       or against e      with b       however  such high bleu
scores are generally only obtained when the system proposes the original caption  rouge
has much lower agreement           against the expert scores  obtained at r      vs 
e      or e       or r      against e       since other data sets may have
only one caption per image  we also evaluate against a reference corpus that consists only
of the single caption in the test pool  in this case  both metrics reach again the highest
agreement against an expert threshold of e        bleu           rouge           
with thresholds of b       and r       we conclude that neither bleu nor rouge are
useful in this scenario  since they require such high thresholds that they only capture how
often the system returned the reference caption 
when bleu and rouge are used to evaluate caption generation systems  we cannot
assume that the generated caption is identical to one of the reference captions  we therefore
examine to what extent bleu and rouge scores agree with human judgments when the
candidate pool contains human generated captions  but is disjoint from the reference captions  we first use a reference corpus of four captions per image  excluding the caption we
use in the candidate pool  in this case  all three metrics show significantly lower agreement
with human judgments than when the candidate pool contains the reference caption  bleu
reaches only          with b      against e       and rouge reaches only        
 with r      against e        to simulate the case where only a single caption per
image is available  we also evaluate against a reference corpus consisting of only one of these
four captions  in this case  agreement with human judgments is even lower  bleu reaches
         and rouge reaches          these results suggest that bleu and rouge are
not appropriate metrics when the pool of candidate captions does not contain the reference
captions  and lead us to question their usefulness for the evaluation of caption generation
systems  this is consistent with the findings of reiter and belz         who have studied
bleu and rouge scores to evaluate natural language generation systems  and concluded
that they may be useful as metrics of fluency  but are poor measures of content quality 
    metrics for the large scale evaluation of image description systems
metrics that only consider the first caption returned for each image cannot capture the fact
that a better model should score good captions higher than most other captions  even if fails
to consider them the best possible caption  since our systems return a ranked list of results
for each item  we now examine metrics that allow us to evaluate the quality of this list 
in contrast to the human evaluations described in section     above  we now also evaluate
our image retrieval systems  we first consider metrics that can be computed automatically 
recall and median rank of the item  image or sentence  that was originally associated with
the query sentence or image  section         we then show how to use crowdsourcing to

   

fiframing image description as a ranking task

performance  rank of the original item
r k  percentage of queries with the original item among top x responses 
median r  median rank of the original item
r  

image annotation
r   r    median r

bow 
bow 
tagrank
tri 

   
   
   
   
   

   
    
    
    
    

   
    
    
    
    

tri sem

   

    

    

nn

     
    
    
    
    
    

r  

image retrieval
r   r    median r

   
   
   
   
   

   
    
    
    
    

   
    
    
    
    

   

    

    

     
    
    
    
    
    

table    model performance as measured by the rank of the original image or caption   
correct response   r k  percentage of queries for which the correct response was among
the first x results  median r  median position of the correct response in the ranked list of
results  superscripts indicate statistically significant difference to tri sem     p       
   p        
collect a very large number of human judgments  section         and use these relevance
judgments to define two additional metrics  the rate of success  which is akin to recall 
and r precision  an established information retrieval metric  section         although these
metrics allow us to evaluate all of our systems  we will focus our discussion on the small set
of systems considered so far  and refer the interested reader to section b of the appendix
for the scores of all systems 
      recall and median rank of the original item
one advantage of our ranking framework is that the position of the original caption or image
among the complete list of       test items can be determined automatically  since a better
system should  on average  assign a higher rank to the original items than a worse system 
we can use their ranks to define a number of different evaluation metrics 
recall  r k  and median rank scores since each query is only associated with a
single gold result  we need not be concerned with precision  however  recall at position k
 r k   i e  the percentage of test queries for which a model returns the original item among
the top k results  is a useful indicator of performance  especially in the context of search 
where a user may be satisfied if the first k results contain a single relevant item  we focus on
k             r    r    r      since this is a binary metric  for each query  the gold item
is either found among the top k results or not   we use again mcnemars test to identify
statistically significant differences between models  conversely  the median rank indicates
the k at which a system has a recall of      i e  the number of results one would have
to consider in order to find the original item for half the queries   here  we use fishers
randomization to identify significant differences between models 
recall  r k  and median rank results  table    the results in table   confirm our
earlier observation that the nn baseline is clearly beaten by all kcca models  p        
for all metrics and models  except for r   search  where the difference to bow  has a
   

fihodosh  young   hockenmaier

p value of p          since the r   annotation scores are based on the same image caption
pairs as the expert scores in table    we can compare them directly  the difference between
the r   and expert scores  even at the strictest threshold of     for the experts  indicates
that measures which capture how often the original caption was returned should be viewed
as a lower bound on actual performance  while tri sem returns the original caption first
for      of the images  our human judges found that these captions describe       of the
images without any errors  this discrepancy is even larger for bow        vs        and
tagrank       vs         as a consequence  the automatically computed r   scores
indicate erroneously that there is a statistically significant difference between the quality
of the first captions returned by tri sem and those returned by bow  or tagrank  even
though these differences are not significant according to the human evaluation  however 
metrics that are only based on the first caption may fail to identify differences between
models that become very apparent under all other metrics  for example  r   reveals
no significant difference between tri  and tri sem on the annotation task  although their
difference is highly significant according to all other metrics  in section        we present the
results of a large scale human evaluation which confirm that the actual differences between
tri sem and tri  on annotation can only be identified when more than the first caption is
taken into account 
table    in section b provides recall and median rank scores for all models 
      collecting binary relevance judgments on a large scale
in order to perform a human evaluation of a system that goes beyond measuring the quality
of the highest ranked result  we would have to obtain relevance judgments for all imagecaption pairs among the top k results for each query  since we have two tasks  and a
total of    different systems  this set consists of         distinct image caption pairs for
k       rendering an exhaustive evaluation on the four point scale described in section      
infeasible  we therefore needed to reduce the total number of judgments needed  and to
define a simpler annotation task that could be completed in less time  crowdsourcing
platforms such as amazon mechanical turk offer new possibilities for evaluation because
they enable us to collect a large number of human judgments rapidly and inexpensively 
and a number of researchers have evaluated caption generation systems on mechanical turk
 ordonez et al         yang et al         kuznetsova et al         kulkarni et al         li
et al          but these experiments have not been performed at the scale of our analysis  and
have also not evaluated how well crowdsourced judgments for this task approximate what
can be obtained from a smaller pool of judges that can be given more detailed instructions 
we examine here whether crowdsourcing allows us to collect reliable relevance judgments
for a large scale evaluation of all of our image description systems 
the crowdsourcing task we presented workers with images that were paired with ten
different captions  and asked them to indicate  via checkboxes  which of the captions describe
the image  we adapted the guidelines developed for the fine grained annotation such that
a caption that describes the image with minor errors  corresponding to a score of   on our
  point scale  would still be permitted to receive a positive score  these guidelines can also
be found in the online appendix to this paper  each individual task consisted of six different
images  each paired with ten captions  and included a copy of the guidelines  we accessed
   

fiframing image description as a ranking task

amazon mechanical turk through a service provided by crowdflower com  which makes it
easy to include control items for quality control  one of the six images in each task was
such a control item  which we generated by taking random images from the development
set  using between one and three of their original captions as correct responses  and adding
another nine to seven randomly selected captions  which we verified manually that they
did not describe the image  as incorrect responses  we only used workers who judged    
of their control items correctly  each image caption pair was annotated by three different
annotators  at a total cost of       and the final score of each image caption pair was
computed as the average number of positive judgments it received 
filtering unlikely image caption pairs in order to reduce the number of annotations
needed  we devised a filter based on bleu scores  papineni et al         to filter out imagecaption pairs whose caption is so dissimilar from the five captions originally written for the
image that it is highly unlikely it describes the image  we found that a filter based on
unigram bleu   scores in combination with the stemming and stop word removal that is
standardly done by lins        rouge script  bleupre   proved particularly effective  a
threshold of bleupre       filters out       of all possible              image caption
pairs in our test set  but eliminates only      of the pairs with an expert score of      or
greater  and      of the pairs with an expert score of   or greater  a slightly higher cutoff
of bleupre       would filter out       of all image caption pairs  but discard       of all
image caption pairs with an expert score of       and      of all image caption pairs with
an expert score of     among the         image caption pairs that we actually wished
to obtain judgments for  the      filter eliminates        reducing the number of pairs we
needed to annotate to         since our setup required us to pair each image with a number
of captions that was a multiple of     we also annotated an additional        image caption
pairs that had been filtered out  allowing us to evaluate the performance of our filter  for
      of these filtered out pairs  all mechanical turk judges decided that the caption did not
describe the image  and for       of them  the majority of annotators thought so  we also
found that standard bleu   without preprocessing is not a very effective filter  a threshold
of bleu        misses      of the good captions  with an expert score of          while
only filtering out     of the entire data set  whereas a threshold of bleu        filters out
    of the entire data set  but misses       of the good captions 
agreement of crowdsourced and expert judgments we again use cohens  to
measure the agreement between the crowdsourced and the expert judgments  table    in the
appendix   the best agreement is obtained between crowdsourced scores with a threshold
of      or above  i e  at least two of the three judges think the caption describes the image 
and expert scores with a threshold of       one expert thinks the caption describes the
image perfectly and the other two agree or think it describes the image with only minor
errors  or two experts think it describes the image perfectly and the other one thinks it is
at least related   at          this is a significantly better approximation to the expert
scores than was possible with either bleu or rouge  we also examine the precision  recall
and f scores that these approximate relevance judgments achieve when compared against
relevance judgments obtained from binarizing expert judgments  table            of all
items with a perfect expert score  and       of all items with an almost perfect expert
score of      are identified  and at least       of the items that pass this threshold have an
   

fihodosh  young   hockenmaier

expert score of     or greater  i e  the majority of experts agreed that the caption describes
the image perfectly or with minor errors   using a threshold of      adds       suitable
image caption pairs to the       test images paired with their original caption  among the
      test captions      still describe only a single image      describe two test images     
three  and     describe four or more images  among the       test images      have only
a single  i e  the original  caption      have two possible captions      have three possible
captions  and     have four or more captions 
      large scale evaluation with relevance judgments
the crowdsourced relevance judgments allow us to define two new metrics  the rate of
success  s k  and r precision  we believe r precision to be the more reliable indicator
of overall performance  since it summarizes the human judgments in a single number that
does not depend on an arbitrary cutoff  we therefore use it in section       for an in depth
analysis of the impact of the different linguistic features our models incorporate  the s k
rate of success scores are motivated by the fact that search engines commonly return multiple
results at once  since users may be satisfied as long as these results contain at least one
relevant item  s k scores provide a more direct measure of utility for hypothetical users 
rate of success  s k  scores the rate of success metric  s k  is analogous to the
recall based r k scores used in table    and is intended to measure the utility of our
system for a hypothetical user  it indicates the percentage of test items for which at least
one relevant result is found among the highest ranked k results  following the analysis in
section        an image caption pair is considered relevant if the majority of the judges say
that the caption describes the image 
rate of success results  table    table   confirms again that nn performs clearly
worse than any of the kcca models  the differences between tri sem and the other
models shown in table   are highly statistically significant  p          for all metrics except
for the s   annotation scores  where  in agreement with the expert scores from table    only
the differences to nn and bow  are significant  it is unclear why the quality of the first
caption that tri sem returns for annotation is not significantly better than those returned
by the other models  since it outperforms them on all other metrics  the s k scores in
table   indicate that tri sem returns a relevant caption among the top    responses for
      of the images  and a relevant image for       of the captions  a comparison with
the expert scores in table   shows that all s   annotation scores lie between expert scores
with a threshold of      and      while a comparison with the r k results in table   shows
that the s   scores are at least twice as high as the corresponding r   scores  that is  the
highest ranked response is just as often a relevant item that was not originally associated
with the query as it is the original gold item itself 
r precision scores given the crowdsourced relevance judgments  each test image may
now be associated with multiple relevant captions  and each test caption may have been
deemed relevant for multiple images besides the one it was originally written for  when
queries have a variable number of relevant answers  the performance of retrieval systems is
commonly measured in terms of r precision  manning  raghavan    schtze         unlike
the s k scores  this metric does not depend on an arbitrary cutoff  but summarizes the
   

fiframing image description as a ranking task

rate of success  s k 
 percentage of items with relevant response among top x results 
image annotation
s  
s  
s   

image retrieval
s  
s  
s   

bow 
bow 
tagrank
tri 

   
    
    
    
    

    
    
    
    
    

    
    
    
    
    

   
    
    
    
    

    
    
    
    
    

    
    
    
    
    

tri sem

    

    

    

    

    

    

nn

table    the rate of success  s k  indicates the percentage of test items for which the top x
results contain at least one relevant response  superscripts indicate statistically significant
difference to tri sem     p          p           p       
r precision
annotation
nn



search


total

bow 
bow 
tagrank
tri 

   
    
    
    
    

   
   
    
    
    

   
    
    
    
    

tri sem

    

    

    

table    model performance as measured by r precision  with statistically significant differences to tri sem     p          p           p      
performance of each system in a single number  allowing us to rank models according to
their overall performance  see section       below   and while the s k scores measure only
whether at least one of the relevant items is ranked highly  r precision requires all relevant
items to be ranked highly  it is therefore a better indicator of the quality of the mapping
between images and sentences  since a better mapping should prefer all relevant captions or
images over any irrelevant caption or image 
the r precision of system s on a query qi with ri known relevant items in the test data
is defined as its precision at rank ri  i e  the percentage of relevant items among the top ri
responses returned by s   the r precision of s is obtained by averaging over all test queries 
we again use fishers randomization test to assess whether the differences between models
reaches statistical significance 
r precision results  table    table   gives the r precision of the model types that were
used when collecting expert judgments  section         we see that the nearest neighbor
baseline is again very clearly below all kcca models  p           r precision indicates that
there is little difference between bow   bow  and tagrank in terms of their overall performance  although tagrank and tri  outperform bow  slightly on search  p          
the only statistically significant difference among these three models is that between bow 
and tri  on search  p          in contrast to the human evaluation that considered only
   

fihodosh  young   hockenmaier

r precision

tri 
ann  search

 idf
ann  search

ann 

 align
search

 align idf
ann  search

tri 

    

    

    ii

    

    aaa

    aa

    a

    aaa ii

 dbnc
 dic
 dbnc ic

    dd
    dd
    dd

    dd
    ddd
    ddd

    
    
    d

    ddd
    ddd
    ddd

    
    
    aa

    a
    
    dd

    
    
    

    a
    
    

table    the effect of adding idf weighting  i   alignment based similarities  a  and distributional similarities  d  to the tri  model  the bolded scores indicate tri   top left  and
tri sem  tri   align idf dbnc ic   bottom right   superscripts indicate statistically
significant differences that result from the addition of the corresponding feature  x   p      
xx         xxx   p         dc   distributional similarities computed over corpus c  the
bnc  our training corpus of image captions  ic   or both 
the first result  table     tri sem clearly outperforms all other models on both annotation
and retrieval  for all differences p           table    in appendix b shows scores for all
models 
      measuring the impact of linguistic features  table   
the results presented so far indicate clearly that tri sem outperforms the simpler tri 
model  but have not considered the impact of the individual text features that distinguish
the two models  since r precision summarizes the performance of each system in a single
number  it allows us to easily perform this analysis 
using r precision for model comparison table   shows the results of an ablation
study which compares the r precision of tri  and tri sem with that of other trigrambased kcca models that use a subset of tri sems additional features  the basic tri 
model yields the bolded scores shown in the top left corner  tri sems scores are given in
the bottom right corner  the top row contains models that do not capture any distributional
similarities  while each of the bottom three rows corresponds to the addition of one kind
of distributional similarity  computed on the bnc  on the image captions in our training
corpus  or on both corpora  to the corresponding model in the top column  the first column
contains models that do not capture any idf reweighting or alignment based similarities 
the second column corresponds to the addition of idf reweighting to models in the first
column  while the third column adds alignment based similarities to the models in the first
column  the last column adds both idf reweighting and alignment based similarities  and
these scores should be compared to both the second and third column  superscripts indicate that the addition of a particular feature leads to a statistically significant improvement
over the model that does not include this feature but is otherwise identical  that is  d
superscripts show that the addition of a distributional similarity metric leads to a significant improvement over the model in the top cell of the same column  the i superscripts
indicate that the addition of idf reweighting leads to a significant improvement over the
corresponding model without idf reweighting in the immediately preceding cell in the same

   

fiframing image description as a ranking task

row  the a superscripts in the third column show that the addition of the alignment based
similarity leads to a significant improvement over the model without idf reweighting shown
in the first column of the same row  and a superscripts in the fifth column show that the
addition of the alignment based similarity to the model with idf reweighting shown in the
second column of the same row leads to a significant improvement 
the impact of idf weighting  distributional and alignment based similarities
while idf weighting is almost always beneficial  the improvements obtained by adding
idf weighting to a given text kernel reach statistical significance  indicated by i superscripts in table    in only two cases  the performance of the basic tri  model on image
annotation  and the performance of the alignment based tri  model on image search  by
contrast  adding lexical similarities leads almost always to a significant or highly significant
improvement  distributional similarities  d superscripts  are very beneficial for the basic
tri  model on both tasks  and help the idf weighted tri  model on image search  distributional similarities computed on both corpora also significantly improve the performance
of the alignment based tri  model that does not incorporate idf weighting  adding them
to the alignment based tri  model without idf weighting leads to further improvement
on search  while not helping or slightly decreasing performance on annotation  albeit not
significantly so   the improvements on search only reach statistical significance when the
similarities computed over both corpora are added  conversely  adding alignment based
similarities to the non idf weighted tri  model with distributional similarities from both
corpora leads to a significant improvement on annotation  finally  the top cell of the last
column shows that adding alignment based similarities to the idf weighted tri  model
leads to a significant improvement on both tasks  although the impact on search is even
greater  comparing this models performance to the alignment based tri  model without
idf weighting shows that in this case  idf weighting only helps on search  the bottom
cells of this column show that adding alignment based similarities to models that already
use idf weighting and distributional similarities  or adding idf weighting to models with
distributional and alignment based similarities generally lead to minor improvements 
table   shows only whether the difference in performance obtained by the addition of
one kind of feature reaches statistical significance  but it is worth noting that any model
that captures lexical similarities of any kind is significantly better than the basic tri 
model on both tasks  p       search  p          annotation   while idf reweighting by
itself only leads to a significant improvement on the annotation task  p          moreover 
the difference between tri sem       search       annotation  and the basic tri  kernel
with idf reweighting       search       annotation  are highly significant  p        search 
p          annotation  
the impact of lins similarity not shown in table   is the performance of tri lin  
the model which augments the trigram kernel with lins        wordnet based similarity 
tri sem does not include lins similarity  since we found during development that tri lin
performed similarly to or worse than the basic tri  model on the automatic r k and median
rank scores  this is also reflected in tri lin s r precision scores of      for annotation  tri  
      and      for search  tri          lins similarity may simply be too coarse for our
purposes  as shown in table    the hypernym relations in wordnet lead it to associate terms
such as swimming and football with each other  but even though these are semantically
   

fihodosh  young   hockenmaier

correlation of system rankings
between s k and r k
annotation


  
  
   

    
    
    

    
    
    

correlation of system rankings
between r precision

search


    
    
    

 a  s k vs  r k

    
    
    

annotation


r  
r  
r   
median rank

    
    
    
     

    
    
    
     

search


    
    
    
     

    
    
    
     

 b  r precision vs  r k and median rank

table    correlation  spearmans  and kendalls    of system rankings obtained from
human metrics  s k and r precision  and automated scores  r k and median rank 
related by the fact that they are both different kinds of sports or activities  they are visually
very dissimilar  and should not be considered related by our systems 
      can human evaluations be approximated by automatic techniques 
r precision and the s k scores require human judgements  and therefore cannot be applied
to datasets where these judgements have not yet been collected or whose scale may prohibit
ever creating a definitive set of judgements  however  if the evaluation is intended to measure
relative progress on image description rather than absolute performance  our automatic
metrics may be a sufficient approximation  since they yield a similar ranking of systems to
r precision and the s k scores  table   a  shows the correlations between the rankings of
all of our nn and kcca systems  n       obtained from the s k scores and those obtained
from the corresponding r k scores  table   b  shows the correlations between r precision
and our automatic metrics  we report two rank correlation coefficients  spearmans  and
kendalls    we first observe that system rankings obtained via r   do not correlate highly
with either r precision or s   based rankings  on the other hand  we also observe that
r    r     and the median rank scores correlate well with r precision and that r   and
r    correlate well with their corresponding s k metrics  this suggests that rankingbased metrics are significantly more robust than metrics that consider only the quality of
the first result  moreover  these results indicate that our framework  in which systems are
expected to rank a pool of images or sentences written by people  may enable a large scale 
fully automated  evaluation of image description systems that does not require an equally
large scale effort to collect human judgments 

   summary of contributions and conclusions
in this paper  we have proposed to frame image description as the task of selecting or ranking
descriptions among a large pool of descriptions provided by people  because this framework
provides a direct test of the purely semantic aspects of image description and does not need
to be concerned with the difficulties involved in the automatic generation of syntactically
correct and pragmatically appropriate sentences  we have also introduced a new data set of
images paired with multiple captions  and have used this data set to evaluate a number of
nearest neighbor and kcca based models on sentence based image annotation as well as on
   

fiframing image description as a ranking task

the converse task of sentence based image search  our experiments indicate the importance
of capturing lexical similarities  finally  we have performed an in depth analysis of different
evaluation metrics for image description 
    the advantages of framing image description as a ranking task
one of our main motivations for framing image description as a ranking rather than a
generation problem was the question of an objective  comparable evaluation of our ability
to understand what is depicted in images  in order to make progress on this challenging
task  it is important to define tasks and evaluation metrics that allow for an objective
comparison of different approaches  we have argued that the task of ranking a pool of
captions written by people is attractive for a number of reasons  first  results obtained on
the same data set can be compared directly  second  human evaluation is easier than for
generated captions since it needs to only focus on factual correctness of the description rather
than grammaticality  fluency  or creativity  third  statistically significant differences between
systems may not become apparent when only a single caption per image is considered  and
finally  ranking makes it possible to automate evaluation  e g  by considering the position of
the original caption  moreover  framing image description as a ranking task also establishes
clear parallels to image retrieval  allowing the same metrics to be used for both tasks 
    our data set
our flickr  k data set of over       images  each paired with five crowdsourced captions  is
a unique resource for image description  although it is much smaller than the sbu corpus
 ordonez et al          we believe that the generic conceptual descriptions in our corpus are
more useful for image understanding than the original flickr captions in the sbu data set 
the data set that is perhaps most similar to ours is the iapr data set  grubinger et al  
       but the captions in our corpus are shorter  and focus on the salient aspects of the
image  and while we focus on images of people and animals  the iapr data set covers a
slightly different domain  including city pictures and landscape shots which typically do not
depict or focus on people  a distinct advantage of our corpus is that it pairs each image
with multiple  independently written captions  our results indicate that using more than a
single caption at training time leads to a significant increase in performance  we have also
shown how to use these multiple captions to define an alignment based lexical similarity that
may be more useful for image description than standard distributional or wordnet based
similarities 
    our models
this paper is the first to apply kernel canonical correlation analysis  kcca  to sentencebased image description  our results show that kcca significantly outperforms nearest
neighbor based approaches on our data set of       training images and       test images
 although these may scale up better to very large data sets such as ordonez et al s       
sbu corpus  where the memory requirements to train kcca may be prohibitive   one
advantage of kcca based approaches over other image description systems that are geared
specifically towards caption generation is that they can not only be applied to image de 

   

fihodosh  young   hockenmaier

scription  but also to image retrieval  and our results indicate that performance on both
tasks is fairly similar 
an important difference between our approach taken in this paper and most other image
description systems is that all the features used by the models presented here can be computed with minimal supervision  the only feature that relies on a supervised classifier is the
alignment based similarity  which uses a pos tagger to identify nouns and verbs  despite
the simplicity of the underlying features  our models achieve relatively high performance 
considering the difficulty of the task  although there is only a      chance that a randomly
chosen test caption will describe a test image well  fine grained human judgments reveal
that for image annotation  the first caption returned by our best kcca system is a good
description for     of the test images  furthermore  our large scale evaluation shows that
with our best system  there is an almost     chance that a suitable image or caption will be
returned among the first ten results  our results indicate that there are two main reasons
for this high performance  the availability of multiple captions for each image at training
time  and the use of robust text representations that capture lexical similarities rather than
requiring strict equality between words  however  it is also clear that this task remains
far from being solved  and we leave the question of how kcca may benefit from models
that rely on richer visual or linguistic features such as detector responses or rich syntactic
analyses for future work 
    evaluating ranking based image description systems
the main advantage of framing image description as a ranking problem is that it allows for a
direct comparison of different approaches  since they can be evaluated on the same data set 
it also makes it possible to borrow established evaluation metrics from information retrieval 
and to use the same metrics and data sets for sentence based image annotation and image
search 
on the one hand  we have shown that crowdsourcing can be used to collect a large number
of binary judgments of image caption pairs for a relatively low price  and that these crowdsourced judgments correlate well with more fine grained judgments  being able to collect
human judgments on a large scale is particularly important for retrieval based approaches
to image description  since the number of relevance judgments that need to be collected
for a test collection may be significantly larger than the number of judgments commonly
used to evaluate a single caption generation system  however  our experiments on image
annotation have provided an example where human judgments of a first caption returned for
each test image did not reveal differences between systems that become apparent when more
results are taken into account  our fine grained evaluation also indicates that evaluations
that are based on a single result may require a potentially much larger number of test items
in order to reveal robust statistically significant differences  among the human evaluation
metrics we have compared  we believe that r precision computed over the crowdsourced
relevance judgments is the most robust  r precision is a standard metric for evaluating
ranked retrieval results when items have a varying number of relevant responses  and since
it yields a single score  it also makes it particularly easy to compare systems  however  the
s k scores  which measure the percentage of items for which the top k responses contain a
relevant result  are perhaps a more direct measure of how useful a system may be in prac 

   

fiframing image description as a ranking task

tice  we will release the crowdsourced relevance judgments we have collected in order to
enable others to evaluate their image description system on our data  we hope that this will
establish a benchmark that can be used for a direct and fair large scale comparison between
an arbitrary number of image description systems 
on the other hand  we have also shown that our framework in which systems are evaluated on their ability to rank a pool of images or sentences may make it possible to perform
a fully automated evaluation  contrary to current practice  our analysis indicates clearly
that standard metrics such as bleu or rouge are not very reliable indicators for how
well captions describe images  even if bleu with rouge style preprocessing can be used
as an effective filter of implausible image caption pairs  although we only consider humangenerated captions  we stipulate that similar observations may hold for automatically generated captions  since similar criticisms about bleus appropriateness for generation and
machine translation evaluation are well known  reiter   belz        callison burch  osborne    koehn         however  in a ranking based framework each test query is associated
with a gold response that it was originally associated with  and our results indicate that
metrics based on this rank of the gold item lead to very similar conclusions as human judgments  this suggest that the evaluation of the ranking based image description task can be
automated  and performed on a potentially much larger scale than we have examined here 
    implications for the evaluation of caption generation systems
image description can  and should  also be treated as a problem for the natural language
generation community  but automatically generating captions that are indistinguishable
from captions written by people  an evaluation criterion used by mitchell et al        
for their comparison of caption generation systems  requires much more than the ability
to provide factually correct information about the image  we believe that the linguistic
issues that need to be solved in a generation setting need to be evaluated separately from
ability to decide whether a given caption describes an image  it is unclear that the kinds of
evaluations performed by e g  mitchell et al  could ever be automated  since the question of
how natural an automatically produced caption seems may always require human judgment 
but human experiments are expensive  and since each system generates its own captions 
such judgments have to be collected anew for each system and experiment  since there is
no consensus on what constitutes a good image description  independently obtained human
assessments of different caption generation systems should not be compared directly  this
means that a direct comparison of systems  e g  as performed by mitchell et al   is typically
only possible within one research group  since there is no common data set for which different
system outputs are publicly available  although automatic scores such as bleu and rouge
may still be useful for caption generation as measures of fluency  reiter   belz         we
have shown that they are not reliable metrics for how well a caption describes an image 
especially when the candidate pool is disjoint from the reference captions  this suggests
that the evaluation of the syntactic and pragmatic aspects of the caption generation task
should not be automated  and may have to rely on human judgments  however  it may
be possible to use the framework proposed in this paper to evaluate the semantic affinity
functions f  i  s  that are implicitly used in caption generations systems 

   

fihodosh  young   hockenmaier

acknowledgments
we gratefully acknowledge support for this project from the national science foundation
through iis medium grant          career award          and cns         ci p 

appendix a  agreement between approximate metrics and expert
human judgments
tables   and   use cohens kappa    to measure the agreement between bleu and rouge
scores and expert judgments  we have selected a few thresholds that yield optimal results 
table     a  shows the agreement between the crowdsourced judgments and the expert
judgments  since the best agreement to the expert scores is obtained with the crowdsourced
judgments using a threshold of      table     b  measures precision and recall of the resulting binary relevance judgments against binarized expert judgments obtained with varying
thresholds 

appendix b  performance of all systems
the following tables give results for all models  in section   of the body of the paper  nn
idf
corresponds to nn idf
f    while tri sem corresponds to tri a dbnc ic  
r k and median rank scores table    gives the recall and median rank of the original
item  section        for all of our models 
agreement between expert and bleu rouge scores  cohens  
case    scand  sref
  reference captions test image  scand  sref   r   
expert
e

   

bleu b
   
   

   

rouge r
   
   

    
   
   
   
   

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

  reference caption test image  scand   sref   r   gold  
expert
e

    

bleu
         

    

    
   
   
   
   

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

rouge
         
    
    
    
    
    

    
    
    
    
    

table    agreement  cohens   between binarized expert and bleu rouge scores when
the pool of candidate captions contains each test images reference caption s  

   

fiframing image description as a ranking task

agreement between expert and bleu rouge scores  cohens  
case    scand   sref
  reference captions  test image  r   
expert
e

    

bleu
         

    

rouge
         

    
   
   
   
   

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

  reference caption test image  r   other  
expert
e

    

bleu
         

    

rouge
         

    
   
   
   
   

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

table    agreement  cohens   between binarized expert and bleu rouge scores when
the pool of candidate captions may not contain each test images reference caption s  

agreement between expert
and lay scores  cohens  
expert
e
    
   
   
   
   

    
    
    
    
    
    

lay vs  expert
relevance judgments  l        

lay l
         
    
    
    
    
    

    
    
    
    
    

 a  agreement  cohens   between relevance judgments obtained from expert
scores  relevance   score  e   and lay
scores  relevance   score  l  

e

precision

recall

f 

    
   
   
   
   
   

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

 b  precision  recall  and f  scores of binarized lay scores  l         against
binarized expert scores with varying
thresholds e  

table     comparing the relevance judgments obtained from the lay scores against those
obtained from expert scores

   

fihodosh  young   hockenmaier

s k and r precision scores table    gives the s k success rate  section        and
r precision scores  section        for all of our models  based on the crowdsourced human
judgments  section        

   

fiframing image description as a ranking task

performance of all models  automatic evaluation 
 r k  percentage of queries with original item in top x results
median r  median rank of original item 
r  

image annotation
r   r    median r

r  

image search
r   r    median r

nn f 
nn idf
f 
nn bow 
nn tri best 

   
   
   
   

   
   
   
   

   
   
   
   

     
     
     
     

   
   
   
   

   
   
   
   

   
   
   
   

     
     
     
     

bow 
tri 

   
   

    
    

    
    

    
    

   
   

    
    

    
    

    
    

bow histo
bow 
bow idf

bow  idf
tagrank

   
   
   
   
   

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

   
   
   
   
   

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

tri histo
tri 

   
   

    
    

    
    

    
    

   
   

    
    

    
    

    
    

tri lin
tri dbnc
tri dic
tri dbnc ic

   
   
   
   

    
    
    
    

    
    
    
    

    
    
    
    

   
   
   
   

    
    
    
    

    
    
    
    

    
    
    
    

tri a
tri a dbnc
tri a dic
tri a dbnc ic

   
   
   
   

    
    
    
    

    
    
    
    

    
    
    
    

   
   
   
   

    
    
    
    

    
    
    
    

    
    
    
    

   

    

    

    

   

    

    

    

   
   
   

    
    
    

    
    
    

    
    
    

   
   
   

    
    
    

    
    
    

    
    
    

   
   
   

    
    
    

    
    
    

    
    
    

   
   
   

    
    
    

    
    
    

    
    
    

idf histo
tri a d
bnc ic

   

    

    

    

   

    

    

    

idf
a dbnc ic

   

    

    

    

   

    

    

    



tri 

idf



idf
tri 
dbnc
idf
tri 
dic
idf
tri dbnc ic


idf
tri a

idf
tri 
a dbnc
idf
tri a d
ic



tri 

table     performance of all models  measured as the percentage of test items for which the
original item was returned among the top      or    results 
as well as as the median rank

idf
idf
of the original item  in section    nn f    nn  tri a dbnc ic   tri sem 

   

fihodosh  young   hockenmaier

performance of all models  human evaluation 
s k  percentage of items with relevant response among top x results
r prec  r precision computed over relevant responses
s  
nn f 
nn idf
f 
nn bow 
nn tri best 

image annotation
s   s    r prec 

s  

image search
s   s    r prec 

   
   
   
   

    
    
    
    

    
    
    
    

   
   
   
   

   
   
   
   

    
    
    
    

    
    
    
    

   
   
   
   

bow 
tri 

    
    

    
    

    
    

    
    

    
    

    
    

    
    

   
   

bow histo
bow 
bow idf

bow  idf
tagrank

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

   
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

   
    
    
    
    

tri histo
tri 

    
    

    
    

    
    

   
    

    
    

    
    

    
    

    
    

tri lin
tri dbnc
tri dic
tri dbnc ic
tri a
tri a dbnc
tri a dic
tri a dbnc ic

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    

    

    

    

    

    

    

    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    
    
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

tri 
tri 
tri 
tri 
tri 
tri 
tri 
tri 
tri 


idf

idf
d
bnc
idf
d
ic
idf
dbnc ic

idf
a

idf
a d
 bnc
idf
a dic

idf histo
a dbnc ic

idf
a dbnc ic

table     performance of all models  measured as the percentage of test items for which
they return an item that was deemed relevant according to the crowdsourced judgments
among the top      or    results 
and as r precision computed over these judgments  in

idf
idf
section    nn f    nn  tri a dbnc ic   tri sem 

   

fiframing image description as a ranking task

references
artstein  r     poesio  m          inter coder agreement for computational linguistics 
computational linguistics                 
bach  f  r     jordan  m  i          kernel independent component analysis  journal of
machine learning research         
barnard  k   duygulu  p   forsyth  d   freitas  n  d   blei  d  m     jordan  m  i         
matching words and pictures  journal of machine learning research              
blei  d  m     jordan  m  i          modeling annotated data  in sigir       proceedings of
the   th annual international acm sigir conference on research and development
in information retrieval  pp          toronto  ontario  canada 
bloehdorn  s   basili  r   cammisa  m     moschitti  a          semantic kernels for text
classification based on topological measures of feature similarity  in proceedings of the
 th ieee international conference on data mining  icdm        pp          hong
kong  china 
bnc consortium         the british national corpus  version    bnc xml edition  
http   www natcorp ox ac uk 
brown  p  f   pietra  v  j  d   pietra  s  a  d     mercer  r  l          the mathematics
of statistical machine translation  parameter estimation  computational linguistics 
               
callison burch  c   osborne  m     koehn  p          re evaluation the role of bleu in
machine translation research  in proceedings of the   th conference of the european chapter of the association for computational linguistics  eacl   pp         
trento  italy 
cohen  j          a coefficient of agreement for nominal scales  educational and psychological measurement               
croce  d   moschitti  a     basili  r          structured lexical similarity via convolution
kernels on dependency trees  in proceedings of the      conference on empirical
methods in natural language processing  emnlp   pp            edinburgh  uk 
dale  r     white  m   eds            workshop on shared tasks and comparative evaluation in natural language generation  position papers  arlington  va  usa 
datta  r   joshi  d   li  j     wang  j  z          image retrieval  ideas  influences  and
trends of the new age  acm computing surveys                  
deschacht  k     moens  m  f          text analysis for automatic image annotation  in
proceedings of the   th annual meeting of the association of computational linguistics  acl   pp            prague  czech republic 
dietterich  t  g          approximate statistical tests for comparing supervised classification
learning algorithms  neural computation                   
everingham  m   gool  l  v   williams  c   winn  j     zisserman  a          the
pascal visual object classes challenge       voc      results  http   www 
pascal network org challenges voc voc     workshop  
   

fihodosh  young   hockenmaier

farhadi  a   hejrati  m   sadeghi  m  a   young  p   rashtchian  c   hockenmaier  j    
forsyth  d          every picture tells a story  generating sentences from images  in
proceedings of the european conference on computer vision  eccv   part iv  pp 
      heraklion  greece 
fellbaum  c          wordnet  an electronic lexical database  bradford books 
felzenszwalb  p   mcallester  d     ramanan  d          a discriminatively trained  multiscale  deformable part model  in proceedings of the      ieee conference on computer vision and pattern recognition  cvpr   pp      anchorage  ak  usa 
feng  y     lapata  m          automatic image annotation using auxiliary text information  in proceedings of the   th annual meeting of the association for computational
linguistics  human language technologies  acl     hlt   pp          columbus 
oh  usa 
feng  y     lapata  m          how many words is a picture worth  automatic caption generation for news images  in proceedings of the   th annual meeting of the association
for computational linguistics  acl   pp            uppsala  sweden 
fisher  r  a          the design of experiments  olyver and boyd  edinburgh  uk 
grangier  d     bengio  s          a discriminative kernel based approach to rank images
from text queries  ieee transactions on pattern analysis and machine intelligence 
             
grice  h  p          logic and conversation  in davidson  d     harman  g  h   eds    the
logic of grammar  pp        dickenson publishing co   encino  ca  usa 
grubinger  m   clough  p   mller  h     deselaers  t          the iapr benchmark  a new
evaluation resource for visual information systems  in ontoimage       workshop on
language resources for content based image retrieval during lrec       pp       
genoa  italy 
gupta  a   verma  y     jawahar  c          choosing linguistics over vision to describe
images  in proceedings of the twenty sixth aaai conference on artificial intelligence 
toronto  ontario  canada 
hardoon  d  r   saunders  c   szedmak  s     shawe taylor  j          a correlation approach for automatic image annotation  in li  x   zaane  o  r     li  z  h   eds   
advanced data mining and applications  vol       of lecture notes in computer science  pp          springer berlin heidelberg 
hardoon  d  r   szedmak  s  r     shawe taylor  j  r          canonical correlation
analysis  an overview with application to learning methods  neural computation     
         
hotelling  h          relations between two sets of variates  biometrika                   
hwang  s     grauman  k          learning the relative importance of objects from tagged
images for retrieval and cross modal search  international journal of computer vision 
                

   

fiframing image description as a ranking task

jaimes  a   jaimes  r     chang  s  f          a conceptual framework for indexing visual
information at multiple levels  in internet imaging       vol       of proceedings of
spie  pp       san jose  ca  usa 
jurafsky  d     martin  j  h          speech and language processing   nd edition   prentice
hall 
krippendorff  k          content analysis  an introduction to its methodology  sage 
kulkarni  g   premraj  v   dhar  s   li  s   choi  y   berg  a  c     berg  t  l         
baby talk  understanding and generating simple image descriptions  in proceedings
of the      ieee conference on computer vision and pattern recognition  cvpr  
pp           
kuznetsova  p   ordonez  v   berg  a   berg  t     choi  y          collective generation
of natural image descriptions  in proceedings of the   th annual meeting of the association for computational linguistics  volume    long papers   pp          jeju
island  korea 
lavrenko  v   manmatha  r     jeon  j          a model for learning the semantics of pictures  in thrun  s   saul  l     schlkopf  b   eds    advances in neural information
processing systems     cambridge  ma  usa 
lazebnik  s   schmid  c     ponce  j          spatial pyramid matching  in s  dickinson 
a  leonardis  b  s     tarr  m   eds    object categorization  computer and human
vision perspectives  chap      pp          cambridge university press 
li  s   kulkarni  g   berg  t  l   berg  a  c     choi  y          composing simple image descriptions using web scale n grams  in proceedings of the fifteenth conference
on computational natural language learning  conll   pp          portland  or 
usa 
lin  c  y          rouge  a package for automatic evaluation of summaries  in mariefrancine moens  s  s   ed    text summarization branches out  proceedings of the
acl    workshop  pp        barcelona  spain 
lin  c  y     hovy  e  h          automatic evaluation of summaries using n gram cooccurrence statistics  in proceedings of the      human language technology conference of the north american chapter of the association for computational linguistics
 hlt naacl   pp        edmonton  ab  canada 
lin  d          an information theoretic definition of similarity  in proceedings of the fifteenth international conference on machine learning  icml   pp          madison 
wi  usa 
lowe  d  g          distinctive image features from scale invariant keypoints  internationa
journal of computer vision                
makadia  a   pavlovic  v     kumar  s          baselines for image annotation  international
journal of computer vision                
manning  c  d   raghavan  p     schtze  h          introduction to information retrieval 
cambridge university press 

   

fihodosh  young   hockenmaier

mitchell  m   dodge  j   goyal  a   yamaguchi  k   stratos  k   han  x   mensch  a   berg 
a   berg  t     daume iii  h          midge  generating image descriptions from
computer vision detections  in proceedings of the   th conference of the european
chapter of the association for computational linguistics  eacl   pp          avignon  france 
moschitti  a          syntactic and semantic kernels for short text pair categorization  in
proceedings of the   th conference of the european chapter of the association for
computational linguistics  eacl   pp          athens  greece 
moschitti  a   pighin  d     basili  r          tree kernels for semantic role labeling 
computational linguistics                 
och  f  j     ney  h          a systematic comparison of various statistical alignment
models  computational linguistics               
ordonez  v   kulkarni  g     berg  t  l          im text  describing images using   million
captioned photographs  in advances in neural information processing systems    
pp           
papineni  k   roukos  s   ward  t     zhu  w  j          bleu  a method for automatic
evaluation of machine translation  in proceedings of   th annual meeting of the association for computational linguistics  acl   pp          philadelphia  pa  usa 
popescu  a   tsikrika  t     kludas  j          overview of the wikipedia retrieval task at
imageclef       in clef  notebook papers labs workshops   padua  italy 
porter  m  f          an algorithm for suffix stripping  program                 
rashtchian  c   young  p   hodosh  m     hockenmaier  j          collecting image annotations using amazons mechanical turk  in naacl workshop on creating speech
and language data with amazons mechanical turk  pp          los angeles  ca 
usa 
rasiwasia  n   pereira  j  c   coviello  e   doyle  g   lanckriet  g  r   levy  r     vasconcelos  n          a new approach to cross modal multimedia retrieval  in proceedings
of the international conference on multimedia  mm   pp          new york  ny 
usa 
reiter  e     belz  a          an investigation into the validity of some metrics for automatically evaluating natural language generation systems  computational linguistics 
               
shatford  s          analyzing the subject of a picture  a theoretical approach  cataloging
  classification quarterly          
shawe taylor  j     cristianini  n          kernel methods for pattern analysis  cambridge
university press 
smucker  m  d   allan  j     carterette  b          a comparison of statistical significance
tests for information retrieval evaluation  in proceedings of the sixteenth acm conference on information and knowledge management  cikm   pp          lisbon 
portugal 

   

fiframing image description as a ranking task

socher  r     li  f  f          connecting modalities  semi supervised segmentation and
annotation of images using unaligned text corpora  in proceedings of the      ieee
conference on computer vision and pattern recognition  cvpr   pp          san
francisco  ca  usa 
van erp  m     schomaker  l          variants of the borda count method for combining
ranked classifier hypotheses  in proceedings of the seventh international workshop on
frontiers in handwriting recognition  iwfhr   pp          nijmegen  netherlands 
varma  m     zisserman  a          a statistical approach to texture classification from
single images  international journal of computer vision           
vedaldi  a     fulkerson  b          vlfeat  an open and portable library of computer
vision algorithms  http   www vlfeat org  
weston  j   bengio  s     usunier  n          large scale image annotation  learning to rank
with joint word image embeddings  machine learning               
yang  y   teo  c   daume iii  h     aloimonos  y          corpus guided sentence generation of natural images  in proceedings of the      conference on empirical methods
in natural language processing  emnlp   pp          edinburgh  uk 

   

fi