journal artificial intelligence research                  

submitted        published      

learning document level semantic properties
free text annotations
s r k  branavan
harr chen
jacob eisenstein
regina barzilay

branavan   csail   mit  edu
harr   csail   mit  edu
jacobe   csail   mit  edu
regina   csail   mit  edu

computer science artificial intelligence laboratory
massachusetts institute technology
   massachusetts avenue  cambridge      

abstract
paper presents new method inferring semantic properties documents leveraging free text keyphrase annotations  annotations becoming increasingly abundant due
recent dramatic growth semi structured  user generated online content  one especially
relevant domain product reviews  often annotated authors pros cons
keyphrases real bargain good value  annotations representative
underlying semantic properties  however  unlike expert annotations  noisy  lay authors
may use different labels denote property  labels may missing  learn
using noisy annotations  find hidden paraphrase structure clusters keyphrases 
paraphrase structure linked latent topic model review texts  enabling system predict properties unannotated documents effectively aggregate semantic
properties multiple reviews  approach implemented hierarchical bayesian model
joint inference  find joint inference increases robustness keyphrase clustering
encourages latent topics correlate semantically meaningful properties  multiple evaluations demonstrate model substantially outperforms alternative approaches summarizing
single multiple documents set semantically salient keyphrases 

   introduction
identifying document level semantic properties implied text core problem natural
language understanding  example  given text restaurant review  would useful
extract semantic level characterization authors reaction specific aspects restaurant  food service quality  see figure     learning based approaches dramatically
increased scope robustness semantic processing  typically dependent
large expert annotated datasets  costly produce  zaenen        
propose use alternative source annotations learning  free text keyphrases produced novice users  example  consider lists pros cons often accompany
reviews products services  end user annotations increasingly prevalent online 
grow organically keep pace subjects interest socio cultural trends  beyond
pragmatic considerations  free text annotations appealing linguistic standpoint
capture intuitive semantic judgments non specialist language users  many real world
datasets  annotations created documents original author  providing direct window
semantic judgments motivated document text 

c
    
ai access foundation  rights reserved 

fib ranavan   c hen   e isenstein     barzilay

pros cons  great nutritional value
    combines all  amazing product  quick friendly service  cleanliness  great nutrition    
pros cons  bit pricey  healthy
    awesome place go health conscious  really great low calorie dishes
publish calories fat grams per serving 

figure    excerpts online restaurant reviews pros cons phrase lists  reviews assert
restaurant serves healthy food  use different keyphrases  additionally 
first review discusses restaurants good service  annotated
keyphrases 

major obstacle computational use free text annotations inherently noisy fixed vocabulary  explicit relationship annotation keyphrases 
guarantee relevant semantic properties document annotated  example 
pros cons annotations accompanying restaurant reviews figure    underlying
semantic idea expressed different ways keyphrases great nutritional value
healthy  additionally  first review discusses quality service  annotated such 
contrast  expert annotations would replace synonymous keyphrases single canonical label  would fully label semantic properties described text  expert annotations
typically used supervised learning methods  demonstrate paper  traditional
supervised approaches perform poorly free text annotations used instead clean  expert
annotations 
paper demonstrates new approach handling free text annotation context
hidden topic analysis document text  show regularities text clarify noise
annotations example  although great nutritional value healthy different
surface forms  text documents annotated two keyphrases likely
similar  modeling relationship document text annotations large dataset 
possible induce clustering annotation keyphrases help overcome
problem inconsistency  model addresses problem incompleteness novice
annotators fail label relevant semantic topics estimating topics predicted
document text alone 
central approach idea document text associated annotations reflect
single underlying set semantic properties  text  semantic properties correspond
induced hidden topics similar growing body work latent topic models 
latent dirichlet allocation  lda  blei  ng    jordan         however  unlike existing work topic
modeling  tie hidden topics text clusters observed keyphrases  connection
motivated idea text associated annotations grounded shared set
semantic properties  modeling properties directly  ensure inferred hidden
topics semantically meaningful  clustering free text annotations robust
noise 
approach takes form hierarchical bayesian framework  includes lda style
component word text generated mixture multinomials  addition  incorporate similarity matrix across universe annotation keyphrases 

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

constructed based orthographic distributional features keyphrases  model
matrix generated underlying clustering keyphrases  keyphrases
clustered together likely produce high similarity scores  generate words
document  model two distributions semantic properties one governed annotation
keyphrases clusters  background distribution cover properties mentioned
annotations  latent topic word drawn mixture two distributions 
learning model parameters noisily labeled training set  apply model unlabeled
data 
build system extracts semantic properties reviews products services 
system uses training corpus includes user created free text annotations pros cons
review  training yields two outputs  clustering keyphrases semantic properties 
topic model capable inducing semantic properties unlabeled text  clustering
annotation keyphrases relevant applications content based information retrieval 
allowing users retrieve documents semantically relevant annotations even surface
forms differ query term  topic model used infer semantic properties
unlabeled text 
topic model used perform multi document summarization  capturing key
semantic properties multiple reviews  unlike traditional extraction based approaches multidocument summarization  induced topic model abstracts text review representation capturing relevant semantic properties  enables comparison reviews even
use superficially different terminology describe set semantic properties 
idea implemented review aggregation system extracts majority sentiment
multiple reviewers product service  example output produced system
shown figure    system applied reviews     product categories  allowing users
navigate semantic properties        products based total         reviews 
effectiveness approach confirmed several evaluations 
summarization single multiple documents  compare properties inferred model expert annotations  approach yields substantially better results
alternatives research literature  particular  find learning clustering free text
annotation keyphrases essential extracting meaningful semantic properties dataset 
addition  compare induced clustering gold standard clustering produced expert
annotators  comparison shows tying clustering hidden topic model substantially
improves quality  clustering induced system coheres well clustering
produced expert annotators 
remainder paper structured follows  section   compares approach previous work topic modeling  semantic property extraction  multi document summarization 
section   describes properties free text annotations motivate approach  model
described section    method parameter estimation presented section   
section   describes implementation evaluation single document multi document
summarization systems using techniques  summarize contributions consider directions future work section    code  datasets expert annotations used paper
available online http   groups csail mit edu rbg code precis  

   

fib ranavan   c hen   e isenstein     barzilay

   related work
material presented section covers three lines related work  first  discuss work
bayesian topic modeling related technique learning free text annotations 
next  discuss state of the art methods identifying analyzing product properties
review text  finally  situate summarization work landscape prior research
multi document summarization 
    bayesian topic modeling
recent work topic modeling literature demonstrated semantically salient topics
inferred unsupervised fashion constructing generative bayesian model document text  one notable example line research latent dirichlet allocation  lda  blei
et al          lda framework  semantic topics equated latent distributions words
text  thus  document modeled mixture topics  class models
used variety language processing tasks including topic segmentation  purver  kording 
griffiths    tenenbaum         named entity resolution  bhattacharya   getoor         sentiment
ranking  titov   mcdonald      b   word sense disambiguation  boyd graber  blei    zhu 
      
method similar lda assigns latent topic indicators word
dataset  models documents mixtures topics  however  lda model unsupervised 
provide method linking latent topics external observed representations
properties interest  contrast  model exploits free text annotations dataset
ensure induced topics correspond semantically meaningful properties 
combining topics induced lda external supervision first considered blei
mcauliffe        supervised latent dirichlet allocation  slda  model  induction
hidden topics driven annotated examples provided training stage  perspective supervised learning  approach succeeds hidden topics mediate
document annotations lexical features  blei mcauliffe describe variational expectationmaximization procedure approximate maximum likelihood estimation models parameters  tested two polarity assessment tasks  slda shows improvement model
topics induced unsupervised model added features supervised
model 
key difference model slda assume access clean
supervision data training  since annotations provided algorithm free text
nature  incomplete fraught inconsistency  substantial difference input
structure motivates need model simultaneously induces hidden structure freetext annotations learns predict properties text 
    property assessment review analysis
model applied task review analysis  traditionally  task identifying properties product review texts cast extraction problem  hu   liu        liu 
hu    cheng        popescu  nguyen    etzioni         example  hu liu        employ
association mining identify noun phrases express key portions product reviews  polarity extracted phrases determined using seed set adjectives expanded via wordnet

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

relations  summary review produced extracting property phrases present verbatim
document 
property extraction refined pine  popescu et al          another system
review analysis  pine employs novel information extraction method identify noun phrases
could potentially express salient properties reviewed products  candidates
pruned using wordnet morphological cues  opinion phrases identified using set handcrafted rules applied syntactic dependencies extracted input document  semantic
orientation properties computed using relaxation labeling method finds optimal assignment polarity labels given set local constraints  empirical results demonstrate pine
outperforms hu lius system opinion extraction identifying polarity opinion words 
two feature extraction methods informed human knowledge way opinions
typically expressed reviews  hu liu         human knowledge encoded using
wordnet seed adjectives  popescu et al          opinion phrases extracted via handcrafted rules  alternative approach learn rules feature extraction annotated
data  end  property identification modeled classification framework  kim  
hovy         classifier trained using corpus free text pro con keyphrases
specified review authors  keyphrases compared sentences review
text  sentences exhibit high word overlap previously identified phrases marked pros
cons according phrase polarity  rest sentences marked negative examples 
clearly  accuracy resulting classifier depends quality automatically induced annotations  analysis free text annotations several domains shows automatically mapping even manually extracted annotation keyphrases document text difficult
task  due variability keyphrase surface realizations  see section     argue rest
paper  beneficial explicitly address difficulties inherent free text annotations 
end  work distinguished two significant ways property extraction methods described above  first  able predict properties beyond appear verbatim text 
second  approach learns semantic relationships different keyphrases  allowing
us draw direct comparisons reviews even semantic ideas expressed using
different surface forms 
working related domain web opinion mining  lu zhai        describe system
generates integrated opinion summaries  incorporate expert written articles  e g   review online magazine  user generated ordinary opinion snippets  e g   mentions
blogs   specifically  expert article assumed structured segments  collection
representative ordinary opinions aligned segment  probabilistic latent semantic analysis
 plsa  used induce clustering opinion snippets  cluster attached one
expert article segments  clusters may unaligned segment  indicating
opinions entirely unexpressed expert article  ultimately  integrated opinion summary combination single expert article multiple user generated opinion snippets
confirm supplement specific segments review 
works final goal different aim provide highly compact summary multitude user opinions identifying underlying semantic properties  rather supplementing
single expert article user opinions  specifically leverage annotations users already
provide reviews  thus obviating need expert article template opinion inte 

   

fib ranavan   c hen   e isenstein     barzilay

gration  consequently  approach suitable goal producing concise keyphrase
summarizations user reviews  particularly review taken authoritative 
work closest methodology approach review summarizer developed titov
mcdonald      a   method summarizes review selecting list phrases
express writers opinions set predefined properties  e g    food ambiance restaurant
reviews   system access numerical ratings set properties 
training set providing examples appropriate keyphrases extract  similar slda  method
uses numerical ratings bias hidden topics towards desired semantic properties  phrases
strongly associated properties via hidden topics extracted part summary 
several important differences work summarization method
titov mcdonald  method assumes predefined set properties thus cannot capture
properties outside set  moreover  consistent numerical annotations required training 
method emphasizes use free text annotations  finally  since titov mcdonalds
algorithm extractive  facilitate property comparison across multiple reviews 
    multidocument summarization
paper relates large body work multi document summarization  researchers
long noted central challenge multi document summarization identifying redundant
information input documents  radev   mckeown        carbonell   goldstein        mani
  bloedorn        barzilay  mckeown    elhadad         task crucial significance
multi document summarizers operate related documents describe facts
multiple times  fact  common assume repetition information among related sources
indicator importance  barzilay et al         radev  jing    budzikowska        nenkova 
vanderwende    mckeown         many algorithms first cluster sentences together 
extract generate sentence representatives clusters 
identification repeated information equally central approach multi document
summarization method selects properties stated plurality users  thereby eliminating rare and or erroneous opinions  key difference algorithm existing summarization systems method identifying repeated expressions single semantic property 
since existing work multi document summarization focuses topic independent
newspaper articles  redundancy identified via sentence comparison  instance  radev et al 
       compare sentences using cosine similarity corresponding word vectors  alternatively  methods compare sentences via alignment syntactic trees  barzilay et al        
marsi   krahmer         string  tree based comparison algorithms augmented
lexico semantic knowledge using resources wordnet 
approach described paper perform comparisons sentence level  instead  first abstract reviews set properties compare property overlap across
different documents  approach relates domain dependent approaches text summarization  radev   mckeown        white  korelsky  cardie  ng  pierce    wagstaff        elhadad
  mckeown         methods identify relations documents comparing
abstract representations  cases  abstract representation constructed using off the shelf
information extraction tools  template specifying types information select crafted
manually domain interest  moreover  training information extraction systems requires
corpus manually annotated relations interest  contrast  method require

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

incompleteness
property
good food
good service
good price
bad food
bad service
bad price
average

recall

precision

f score

     
     
     
     
     
     
     

     
     
     
     
     
     
     

     
     
     
     
     
     
     

inconsistency
keyphrase top keyphrase
count
coverage  
  
    
  
    
  
    
  
    
  
    
  
    
    
    

table    incompleteness inconsistency restaurant domain  six major properties prevalent reviews  incompleteness figures recall  precision  f score
author annotations  manually clustered properties  gold standard property
annotations  inconsistency measured number different keyphrase realizations
least five occurrences associated property  percentage frequency
commonly occurring keyphrases used annotate property 
averages bottom row weighted according frequency property occurrence 

manual template specification corpora annotated experts  abstract representations
induce linguistically rich extraction templates  nevertheless enable us
perform in depth comparisons across different reviews 

   analysis free text keyphrase annotations
section  explore characteristics free text annotations  aiming quantify degree
noise observed data  results analysis motivate development learning
algorithm described section   
perform investigation domain online restaurant reviews using documents downloaded popular epinions  website  users website evaluate products providing
textual description opinion  well concise lists keyphrases  pros cons 
summarizing review  pros cons keyphrases appealing source annotations online
review texts  however  contributed independently multiple users thus unlikely
clean expert annotations  analysis  focus two features free text annotations  incompleteness inconsistency  measure incompleteness quantifies degree
label omission free text annotations  inconsistency reflects variance keyphrase
vocabulary used various annotators 
test quality user generated annotations  compare expert annotations produced systematic fashion  annotation effort focused six properties
commonly mentioned review authors  specifically shown table    given
review property  task assess whether reviews text supports property 
annotations produced two judges guided standardized set instructions  contrast
author annotations website  judges conferred training session ensure consistency completeness  two judges collectively annotated     reviews     annotated
   http   www epinions com 

   

fib ranavan   c hen   e isenstein     barzilay

property  good price
relatively inexpensive  dirt cheap  relatively cheap  great price  fairly priced  well priced  reasonable
prices  cheap prices  affordable prices  reasonable cost

figure    examples many different paraphrases related property good price appear
pros cons keyphrases reviews used inconsistency analysis 

both  cohens kappa  measure inter annotator agreement ranges zero one 
     joint set  indicating high agreement  cohen         average  review text
annotated      properties 
separately  one judges standardized free text pros cons annotations
    reviews  reviews keyphrases matched six properties  standardization allows direct comparison properties judged supported reviews
text properties described reviews free text annotations  find many semantic properties judged present text user annotated average 
keyphrases expressed      relevant semantic properties per document  text expressed
     properties  gap demonstrates frequency authors omitted relevant semantic
properties review annotations 
    incompleteness
measure incompleteness  compare properties stated review authors form
pros cons stated review text  judged expert annotators 
comparison performed using precision  recall f score  setting  recall proportion
semantic properties text review author provided least one annotation
keyphrase  precision proportion keyphrases conveyed properties judged supported
text  f score harmonic mean  results comparison summarized
left half table   
incompleteness results demonstrate significant discrepancy user expert
annotations  expected  recall quite low      property occurrences stated
review text without explicitly mentioned annotations  precision scores indicate
converse true  though lesser extent keyphrases express properties
mentioned text 
interestingly  precision recall vary greatly depending specific property 
highest good food  matching intuitive notion high food quality would key salient
property restaurant  thus likely mentioned text annotations  conversely  recall good service lower users  high quality service apparently
key point summarizing review keyphrases 
    inconsistency
lack unified annotation scheme restaurant review dataset apparent across
reviewers  annotations feature        unique keyphrase surface forms set        total
keyphrase occurrences  clearly  many unique keyphrases express semantic property
figure    good price expressed ten different ways  quantify phenomenon  judges
   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

figure    cumulative occurrence counts top ten keyphrases associated good service
property  percentages total       separate keyphrase occurrences
property 

manually clustered subset keyphrases associated six previously mentioned properties  specifically      keyphrases associated six major properties chosen  accounting
      keyphrase occurrences 
use manually clustered annotations examine distributional pattern keyphrases
describe underlying property  using two different statistics  first  number
different keyphrases property gives lower bound number possible paraphrases 
second  measure often common keyphrase used annotate property 
i e   coverage keyphrase  metric gives sense diffuse keyphrases within
property are  specifically whether one single keyphrase dominates occurrences property 
note value overestimate true coverage  since considering tenth
keyphrase occurrences 
right half table   summarizes variability property paraphrases  observe
property associated numerous paraphrases  found multiple times
actual keyphrase set  importantly  frequent keyphrase accounted third
property occurrences  strongly suggesting targeting labels learning
limited approach  illustrate last point  consider property good service  whose
keyphrase realizations distributional histogram appears figure    cumulative percentage
frequencies frequent keyphrases associated property plotted  top four
keyphrases account three quarters property occurrences  even within limited
set keyphrases consider analysis  motivating need aggregate consideration
keyphrases 
next section  introduce model induces clustering among keyphrases
relating keyphrase clusters text  directly addressing characteristics data 

   

fib ranavan   c hen   e isenstein     barzilay


x

h


c

z

w













keyphrase cluster model
keyphrase cluster assignment
keyphrase similarity values
document keyphrases
document keyphrase topics
probability selecting instead
selects word topics
background word topic model
word topic assignment
language models topic
document words

dirichlet    
x  multinomial  
 
beta     x    x  
s    
beta      otherwise


   d         d k  

 
d k

 


x    k l hd
otherwise

beta    
cd n bernoulli d  
dirichlet    
 
multinomial d   cd n    
zd n
multinomial d   otherwise
k dirichlet    
wd n multinomial zd n  

figure    plate diagram model  shaded circles denote observed variables  squares
denote hyperparameters  dotted arrows indicate constructed deterministically x h  use refer small constant probability mass 

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

   model description
present generative bayesian model documents annotated free text keyphrases 
model assumes annotated document generated set underlying semantic topics 
semantic topics generate document text indexing language model  approach 
associated clusters keyphrases  way  model viewed extension
latent dirichlet allocation  blei et al          latent topics additionally biased
toward keyphrases appear training data  however  coupling flexible 
words permitted drawn topics represented keyphrase annotations 
permits model learn effectively presence incomplete annotations  still
encouraging keyphrase clustering cohere topics supported document text 
another critical aspect model desire ability use arbitrary comparisons
keyphrases  addition information surface forms  accommodate
goal  treat keyphrase surface forms generated model  rather  acquire
real valued similarity matrix across universe possible keyphrases  treat matrix
generated keyphrase clustering  representation permits use surface
distributional features keyphrase similarity  described section     
advantage hierarchical bayesian models easy change parts
model observed parts hidden  training  keyphrase annotations
observed  hidden semantic topics coupled clusters keyphrases  account
words related semantic topics  topics may associated keyphrases  test
time  model presented documents keyphrase annotations hidden 
model evaluated ability determine keyphrases applicable  based hidden
topics present document text 
judgment whether topic applies given unannotated document based probability mass assigned topic documents background topic distribution 
annotations  background topic distribution capture entirety documents
topics  task involving reviews products services  multiple topics may accompany
document  case  topic whose probability threshold  tuned development
set  predicted supported 
    keyphrase clustering
handle hidden paraphrase structure keyphrases  one component model estimates
clustering keyphrases  goal obtain clusters cluster correspond welldefined semantic topic e g   healthy good nutrition grouped single
cluster  overall joint model generative  generative model clustering could easily
integrated larger framework  approach would treat keyphrases
cluster generated parametric distribution  however  representation would
permit many powerful features assessing similarity pairs keyphrases  string
overlap keyphrase co occurrence corpus  mccallum  bellare    pereira        
reason  represent keyphrase real valued vector rather surface
form  vector given keyphrase includes similarity scores respect every observed keyphrase  the similarity scores represented figure     model similarity
scores generated cluster memberships  represented x figure     two keyphrases

   

fib ranavan   c hen   e isenstein     barzilay

lexical

cosine similarity surface forms two keyphrases  represented word frequency vectors 

co occurrence

keyphrase represented vector co occurrence values 
vector counts many times keyphrases appear documents
annotated keyphrase  example  similarity vector
good food may include entry tasty food  value
would number documents annotated good food
contain tasty food text  similarity two
keyphrases cosine similarity co occurrence vectors 

table    two sources information used compute similarity matrix experiments 
final similarity scores linear combinations two values  note cooccurrence similarity contains second order co occurrence information 

figure    surface plot keyphrase similarity matrix set restaurant reviews  computed according table    red indicates high similarity  whereas blue indicates low
similarity  diagram  keyphrases grouped according expertcreated clustering  keyphrases similar meaning close together  strong series
similarity blocks along diagonal hint information could induce
reasonable clustering 

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

clustered together  similarity score generated distribution encouraging high similarity  otherwise  distribution encouraging low similarity used  
features used producing similarity matrix given table    encompassing lexical
distributional similarity measures  implemented system takes linear combination
two data sources  weighting sources equally  resulting similarity matrix keyphrases
restaurant domain shown figure   
described next section  clustering keyphrases  model takes advantage
topic structure documents annotated keyphrases  addition information
individual keyphrases themselves  sense  differs traditional approaches paraphrase
identification  barzilay   mckeown        lin   pantel        
    document topic modeling
analysis document text based probabilistic topic models lda  blei et al  
       lda framework  word generated language model indexed
words topic assignment  thus  rather identifying single topic document  lda identifies
distribution topics  high probability topic assignments identify compact  low entropy
language models  probability mass language model topic divided among
relatively small vocabulary 
model operates similar manner  identifying topic word  denoted z
figure    however  lda learns distribution topics document  deterministically construct document specific topic distribution clusters represented
documents keyphrases figure  assigns equal probability topics
represented keyphrase annotations  small probability topics  generating
word topics way ties together clustering language models 
noted above  sometimes keyphrase annotation represent semantic
topics expressed text  reason  construct another background distribution topics  auxiliary variable c indicates whether given words topic drawn
distribution derived annotations  background model  representing c
hidden variable allows us stochastically interpolate two language models
  addition  given document likely discuss topics covered
keyphrase  account this  model allowed leave clusters empty  thus
leaving topics independent keyphrases 
    generative process
model assumes observed data generated stochastic process involving hidden
parameters  section  formally specify generative process  specification guides
inference hidden parameters based observed data  following 
l keyphrases  vector s  length l denoting pairwise similarity score
interval        every keyphrase 
document d  bag words wd length nd   nth word wd n  
   note model similarity score independent draw  clearly assumption strong  due
symmetry transitivity  models making similar assumptions independence related hidden variables
previously shown successful  for example  toutanova   johnson        

   

fib ranavan   c hen   e isenstein     barzilay

document d  set keyphrase annotations hd   includes index   document annotated keyphrase   
number clusters k  large enough encompass topics actual
clusters keyphrases  well word only topics 
observed variables generated according following process 
   draw multinomial distribution k keyphrase clusters symmetric dirichlet
prior parameter     
               l 
 a  draw  th keyphrases cluster assignment x  multinomial   
                       l          l  
 a  x    x     draw s     beta     beta        encouraging scores biased
toward values close one 
 b  x     x     draw s     beta      beta        encouraging scores biased
toward values close zero 
   k           k 
 a  draw language model k symmetric dirichlet prior parameter    
             d 
 a  draw background topic model symmetric dirichlet prior parameter    
 b  deterministically construct annotation topic model   based keyphrase cluster
assignments x observed document annotations hd   specifically  let h set
topics represented phrases hd   distribution assigns equal probability
element h  small probability mass topics  
 c  draw weighted coin beta      determine balance
annotation background topic models  
 d  n           nd  
i  draw binary auxiliary variable cd n bernoulli d    determines whether
topic word wd n drawn annotation topic model background model  
ii  draw topic assignment zd n appropriate multinomial indicated
cd n  
iii  draw word wd n multinomial zd n    is  language model indexed
words topic 
   variables subscripted zero fixed hyperparameters 
   making hard assignment zero probability topics creates problems parameter estimation 
probability     assigned topics represented keyphrase cluster memberships 

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

   parameter estimation
make predictions unseen data  need estimate parameters model  bayesian
inference  estimate distribution parameter  conditioned observed data
hyperparameters  inference intractable general case  sampling approaches allow
us approximately construct distributions parameter interest 
gibbs sampling perhaps generic straightforward sampling technique  conditional distributions computed hidden variable  given variables model 
repeatedly sampling distributions turn  possible construct markov chain
whose stationary distribution posterior model parameters  gelman  carlin  stern   
rubin         use sampling techniques natural language processing previously
investigated many researchers  including finkel  grenager  manning        goldwater 
griffiths  johnson        
present sampling equations hidden variables figure    prior
keyphrase clusters sampled based hyperprior   keyphrase cluster assignments
x  write p           mean probability conditioned variables 
p           p       p x     

  p       
p x     
 

  dirichlet      



multinomial x     

 

  dirichlet       
i      count x    i   conditional distribution derived based conjugacy
multinomial dirichlet distribution  first line follows bayes rule  second
line conditional independence cluster assignments x given keyphrase distribution  
resampling equations k derived similar manner 
p d          dirichlet d    d   
p k          dirichlet k   k    
p
     
 d i       count zn d   cn d      k i
 
count wn d   zn d   k  
 
building counts   consider cases cn d      indicating topic zn d
indeed drawn background topic model   similarly  building counts k   
consider cases word wd n drawn topic k 
resample   employ conjugacy beta prior bernoulli observation likelihoods  adding counts c prior    
p d          beta d    d   

p
count c
 
  
d n
 
n
 
      p
n count cd n     

   

fib ranavan   c hen   e isenstein     barzilay

keyphrase cluster assignments represented x  whose sampling distribution depends
  s  z  via  
p x           p x     p s   x    x     p z       c 






p x     
p s       x    x      
p zd n    
      

cd n   


  multinomial x     







beta s       x   x    
multinomial zd n      

      

cd n   

leftmost term equation prior x    next term encodes dependence
similarity matrix cluster assignments  slight abuse notation  write x   x  
denote   x    x        otherwise  third term dependence word topics
zd n topic distribution   compute final result probability expression
possible setting x    sample normalized multinomial 
word topics z sampled according topic distribution   background distribution
  observed words w  auxiliary variable c 
p zd n          p zd n       cd n  p wd n   zd n    
 
multinomial zd n    multinomial wd n   zd n  
 
multinomial zd n    multinomial wd n   zd n  

cd n     
otherwise 

x  zd n sampled computing conditional likelihood possible setting
within constant proportionality  sampling normalized multinomial 
finally  sample auxiliary variable cd n   indicates whether hidden topic zd n
drawn   c depends prior hidden topic assignments z 
p cd n          p cd n    p zd n       cd n  
 
bernoulli cd n    multinomial zd n    
 
bernoulli cd n    multinomial zd n    

cd n     
otherwise 

again  compute likelihood cd n     cd n     within constant proportionality 
sample normalized bernoulli distribution 
finally  model requires values fixed hyperparameters                 tuned
standard way based development set performance  appendix c lists hyperparameters
values used domain experiments 
one main applications model predict properties supported documents
annotated keyphrases  test time  would compute posterior estimate
unannotated test document d  since annotations present  property prediction
based text component model  estimate  use gibbs sampling
procedure  restricted zd n   stipulation cd n fixed zero zd n
always drawn   particular  treat language models known  accurately
integrate possible language models  use final      samples language models
training opposed using point estimate  topic  probability exceeds
certain threshold  topic predicted  threshold tuned independently topic
development set  empirical results present section   obtained manner 
   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

figure    summary reviews movie pirates caribbean  worlds end p r ecis 
summary based    documents  list pros cons generated automatically using system described paper  generation numerical ratings
based algorithm described snyder barzilay        

   evaluation summarization quality
model document analysis implemented p r ecis   system performs single 
multi document review summarization  goal p r ecis provide users effective access
review data via mobile devices  p r ecis contains information        products services
ranging childcare products restaurants movies  products  system
contains collection reviews downloaded consumer websites epinions  cnet 
amazon  p r ecis compresses data product short list pros cons
supported majority reviews  example summary    reviews movie
pirates caribbean  worlds end shown figure    contrast traditional multidocument summarizers  output system sequence sentences  rather list
phrases indicative product properties  summarization format follows format pros cons
summaries individual reviewers provide multiple consumer websites  moreover  brevity
summary particularly suitable presenting small screens mobile
devices 
automatically generate combined pros cons list product service  first apply
model review  model trained independently product domain  e g   movies 
using corresponding subset reviews free text annotations  annotations provide
set keyphrases contribute clusters associated product properties 
   p r ecis accessible http   groups csail mit edu rbg projects precis  

   

fib ranavan   c hen   e isenstein     barzilay

model trained  labels review set properties  since set possible properties
reviews product  comparison among reviews straightforward
property  count number reviews support it  select property part
summary supported majority reviews  set semantic properties converted
pros cons list presenting common keyphrase property 
aggregation technology applicable two scenarios  system applied unannotated reviews  inducing semantic properties document text  conforms traditional way learning based systems applied unlabeled data  however  model
valuable even individual reviews include pros cons keyphrase annotations  due
high degree paraphrasing  direct comparison keyphrases challenging  see section    
inferring clustering keyphrases  model permits comparison keyphrase annotations
semantic level 
remainder section provides set evaluations models ability capture
semantic content document text keyphrase annotations  section     describes evaluation
systems ability extract meaningful semantic summaries individual documents 
assesses quality paraphrase structure induced model  section     extends
evaluation systems ability summarize multiple review documents 
    single document evaluation
first  evaluate model respect ability reproduce annotations present individual documents  based document text  compare wide variety baselines
variations model  demonstrating appropriateness approach task  addition 
explicitly evaluate quality paraphrase structure induced model comparing
gold standard clustering keyphrases provided expert annotators 
      e xperimental etup
section  describe datasets evaluation techniques used experiments
system automatic methods  comment hyperparameters tuned
model  sampling initialized 
statistic
  reviews
avg  review length
avg  keyphrases   review

restaurants
    
     
    

cell phones
    
      
    

digital cameras
    
      
    

table    statistics datasets used evaluations
data sets evaluate system reviews three domains  restaurants  cell phones 
digital cameras  reviews downloaded epinions website  used user authored
pros cons associated reviews keyphrases  see section     statistics datasets
provided table    domains  selected     documents training 
consider two strategies constructing test data  first  consider evaluating semantic
properties inferred system expert annotations semantic properties present
document  end  use expert annotations originally described section   test

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

set   reiterate  annotations     reviews restaurant domain 
hold    development set  review texts annotated six properties according
standardized guidelines  strategy enforces consistency completeness ground truth
annotations  differentiating free text annotations 
unfortunately  ability evaluate expert annotations limited cost producing annotations  expand evaluation domains  use author written keyphrase
annotations present original reviews  annotations noisy presence
property annotation document strong evidence document supports property 
inverse necessarily true  is  lack annotation necessarily imply
respective property hold e g   review good service related keyphrase may
still praise service body document 
experiments using free text annotations  overcome pitfall restricting evaluation predictions individual properties documents annotated
property antonym  instance  evaluating prediction good service property 
select documents either annotated good service bad service related
keyphrases   reason  semantic property evaluated unique subset documents  details development test sets presented appendix a 
ensure free text annotations reliably used evaluation  compare
results produced expert annotations whenever possible  shown section        free text
evaluations produce results cohere well obtained expert annotations  suggesting
labels used reasonable proxy expert annotation evaluations 
evaluation methods first evaluation leverages expert annotations described section   
one complication expert annotations marked level semantic properties 
model makes predictions appropriateness individual keyphrases  address
representing expert annotation commonly observed keyphrase
manually annotated cluster keyphrases associated semantic property  example 
annotation semantic property good food represented common keyphrase realization  great food  evaluation checks whether keyphrase within clusters
keyphrases predicted model 
evaluation author free text annotations similar evaluation expert
annotations  case  annotation takes form individual keyphrases rather semantic
properties  noted  author generated keyphrases suffer inconsistency  obtain consistent
evaluation mapping author generated keyphrase cluster keyphrases determined
expert annotator  selecting common keyphrase realization
cluster  example  author may use keyphrase tasty  maps semantic cluster
good food  select common keyphrase realization  great food  expert
evaluation  check whether keyphrase within clusters predicted model 
model performance quantified using recall  precision  f score  computed
standard manner  based models representative keyphrase predictions compared
corresponding references  approximate randomization  yeh        noreen        used
statistical significance testing  test repeatedly performs random swaps individual results
   expert annotations available http   groups csail mit edu rbg code precis  
   determination made mapping author keyphrases properties using expert generated gold standard
clustering keyphrases  much cheaper produce expert clustering keyphrases obtain expert
annotations semantic properties every document 

   

fib ranavan   c hen   e isenstein     barzilay

candidate system  checks whether resulting performance gap remains least
large  use test valid comparing nonlinear functions random variables  f scores  unlike common methods sign test  previous work
used test include evaluations message understanding conference  chinchor  lewis   
hirschman        chinchor         recently  riezler maxwell        advocated
use evaluating machine translation systems 
parameter tuning initialization improve models convergence rate  perform two
initialization steps gibbs sampler  first  sampling done keyphrase clustering
component model  ignoring document text  second  fix clustering sample
remaining model parameters  two steps run       iterations each  full joint model
sampled         iterations  inspection parameter estimates confirms model convergence   ghz dual core desktop machine  multithreaded c   implementation model
training takes two hours dataset 
model needs provided number clusters k   set k large enough
model learn effectively development set  restaurant data set k     cell
phones digital cameras  k set        respectively  values tuned using
development set  however  found long k large enough accommodate significant number keyphrase clusters  additional account topics keyphrases 
specific value k affect models performance  hyperparameters
adjusted based development set performance  though tuning extensive 
previously mentioned  obtain document properties examining probability mass
topic distribution assigned property  probability threshold set property via
development set  optimizing maximum f score 
      r esults
section  report performance model  comparing array increasingly
sophisticated baselines model variations  first demonstrate learning clustering annotation keyphrases crucial accurate semantic prediction  next  investigate impact
paraphrasing quality model accuracy considering expert generated gold standard clustering keyphrases another comparison point  consider alternative automatically computed
sources paraphrase information 
ease comparison  results experiments shown table   table   
summary baselines model variations table   
comparison simple baselines first evaluation compares model four nave
baselines  four treat keyphrases independent  ignoring latent paraphrase structure 
random  keyphrase supported document probability one half 
results baseline computed expectation  rather actually run  baseline
expected recall      expectation select half correct
keyphrases  precision average proportion annotations test set
number possible annotations  is  test set size n properties  property
   requirement could conceivably removed modeling cluster indices drawn dirichlet
process prior 

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

random

keyphrase supported document probability one half 

keyphrase text

keyphrase supported document appears verbatim text 

keyphrase classifier

separate support vector machine classifier trained keyphrase 
positive examples documents labeled author
keyphrase  documents considered negative examples 
keyphrase supported document keyphrases classifier returns
positive prediction 

heuristic keyphrase
classifier

similar keyphrase classifier  except heuristic methods used attempt reduce noise training documents  specifically wish
remove sentences discuss keyphrases positive examples 
heuristic removes positive examples sentences
word overlap given keyphrase 

model cluster text

keyphrase supported document paraphrases appear
text  paraphrasing based models keyphrase clusters 

model cluster classifier

separate classifier trained cluster keyphrases  positive examples documents labeled author keyphrase
cluster  documents negative examples  keyphrases
cluster supported document clusters classifier returns
positive prediction  keyphrase clustering based model 

heuristic model cluster
classifier

similar model cluster classifier  except heuristic methods used reduce noise training documents  specifically wish remove
positive examples sentences discuss keyphrases
clusters  heuristic removes positive examples sentences
word overlap keyphrases given cluster 
keyphrase clustering based model 

gold cluster model

variation model clustering keyphrases fixed
expert created gold standard  text modeling parameters learned 

gold cluster text

similar model cluster text  except clustering keyphrases according expert produced gold standard 

gold cluster classifier

similar model cluster classifier  except clustering keyphrases
according expert produced gold standard 

heuristic gold cluster
classifier

similar heuristic model cluster classifier  except clustering
keyphrases according expert produced gold standard 

independent cluster model

variation model clustering keyphrases first learned
keyphrase similarity information only  separately text 
resulting independent clustering fixed text modeling parameters learned  variations key distinction full model
lack joint learning keyphrase clustering text topics 

independent cluster text

similar model cluster text  except clustering keyphrases
according independent clustering 

independent cluster
classifier

similar model cluster classifier  except clustering keyphrases
according independent clustering 

heuristic independent
cluster classifier

similar heuristic model cluster classifier  except clustering
keyphrases according independent clustering 

table    summary baselines variations model compared 
   

fib ranavan   c hen   e isenstein     barzilay

method
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  

model
random
keyphrase text
keyphrase classifier
heuristic keyphrase classifier
model cluster text
model cluster classifier
heuristic model cluster classifier
gold cluster model
gold cluster text
gold cluster classifier
heuristic gold cluster classifier
independent cluster model
independent cluster text
independent cluster classifier
heuristic independent cluster classifier

recall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

restaurants
prec  f score
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           

table    comparison property predictions made model series baselines
model variations restaurant domain  evaluated expert semantic annotations 
results divided according experiment  methods model
significantly better results using approximate randomization indicated
p       p     

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

method
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  

model
random
keyphrase text
keyphrase classif 
heur  keyphr  classif 
model cluster text
model cluster classif 
heur  model classif 
gold cluster model
gold cluster text
gold cluster classif 
heur  gold classif 
indep  cluster model
indep  cluster text
indep  cluster classif 
heur  indep  classif 

recall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

restaurants
prec  f score
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           

recall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

cell phones
prec  f score
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           

digital cameras
recall prec  f score
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 

table    comparison property predictions made model series baselines
model variations three product domains  evaluated author free text annotations  results divided according experiment  methods
model significantly better results using approximate randomization indicated
p       p      methods perform significantly better
model p      indicated  

   

fib ranavan   c hen   e isenstein     barzilay

p
ni
appears ni times  expected precision
i   mn   instance  restaurants
gold standard evaluation  six tested properties appeared total     times    
documents  yielding expected precision       
keyphrase text  keyphrase supported document appears verbatim
text  precision high recall low  model unable detect
paraphrases keyphrase text  instance  first review figure   
cleanliness would supported appears text  however  healthy would
supported  even though synonymous great nutrition appear 
keyphrase classifier   separate discriminative classifier trained keyphrase  positive examples documents labeled author keyphrase  documents considered negative examples  consequently  particular keyphrase 
documents labeled synonymous keyphrases would among negative examples 
keyphrase supported document keyphrases classifier returns positive prediction 
use support vector machines  built using svmlight  joachims        features
model  i e  word counts    partially circumvent imbalanced positive negative
data problem  tuned prediction thresholds development set maximize f score 
manner tuned thresholds model 
heuristic keyphrase classifier  baseline similar keyphrase classifier above  attempts mitigate noise inherent training data  specifically  given
positive example document may contain text unrelated given keyphrase  attempt
reduce noise removing positive examples sentences word
overlap given keyphrase  keyphrase supported document keyphrases
classifier returns positive prediction   
lines     tables     present results  using gold annotations original
authors annotations testing  model outperforms three baselines evaluations
strong statistical significance 
keyphrase text baseline fares poorly  f score random baseline three
four evaluations  expected  recall baseline usually low requires
keyphrases appear verbatim text  precision somewhat better  presence
significant number false positives indicates presence keyphrase text
necessarily reliable indicator associated semantic property 
interestingly  one domain keyphrase text perform well digital cameras 
believe prevalence specific technical terms keyphrases used
domain  zoom battery life  technical terms frequently used
review text  making recall keyphrase text substantially higher domain
evaluations 
   note classifier results reported initial publication  branavan  chen  eisenstein    barzilay       
obtained using default parameters maximum entropy classifier  tuning classifiers parameters allowed us
significantly improve performance classifier baselines 
    general  svms additional advantage able incorporate arbitrary features  sake
comparison restrict using features across methods 
    thank reviewer suggesting baseline 

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

keyphrase classifier baseline outperforms random keyphrase text baselines 
still achieves consistently lower performance model four evaluations  notably 
performance heuristic keyphrase classifier worse keyphrase classifier except one case 
alludes difficulty removing noise inherent document text 
overall  results indicate methods learn predict keyphrases without accounting intrinsic hidden structure insufficient optimal property prediction  leads us
toward extending present baselines clustering information 
important assess consistency evaluation based free text annotations  table    evaluation uses expert annotations  table     absolute scores
expert annotations dataset lower scores free text annotations  ordering performance various automatic methods across two evaluation scenarios 
consistency maintained rest experiments well  indicating purpose
relative comparison different automatic methods  method evaluating
free text annotations reasonable proxy evaluation expert generated annotations 
comparison clustering based approaches previous section demonstrates
model outperforms baselines account paraphrase structure keyphrases 
ask whether possible enhance baselines performance augmenting
keyphrase clustering induced model  specifically  introduce three systems  none
true baselines  since use information inferred model 
model cluster text  keyphrase supported document paraphrases
appears text  paraphrasing based models clustering keyphrases 
use paraphrasing information enhances recall potential cost precision  depending
quality clustering  example  assuming healthy great nutrition
clustered together  presence healthy text would indicate support great
nutrition  vice versa 
model cluster classifier  separate discriminative classifier trained cluster
keyphrases  positive examples documents labeled author keyphrase
cluster  documents negative examples  keyphrases cluster
supported document clusters classifier returns positive prediction  keyphrase
clustering based model  keyphrase classifier  use support vector machines trained word count features  tune prediction thresholds individual cluster development set 
another perspective model cluster classifier augments simplistic text modeling
portion model discriminative classifier  discriminative training often considered powerful equivalent generative approaches  mccallum et al         
leading us expect high level performance system 
heuristic model cluster classifier  method similar model cluster classifier above 
additional heuristics used reduce noise inherent training data  positive
example documents may contain text unrelated given cluster  reduce noise 
sentences word overlap clusters keyphrases removed 
keyphrases cluster supported document clusters classifier returns positive prediction  keyphrase clustering based model 
   

fib ranavan   c hen   e isenstein     barzilay

lines     tables     present results methods  expected  using clustering
keyphrases baseline methods substantially improves recall  low impact
precision  model cluster text invariably outperforms keyphrase text recall keyphrase
text improved addition clustering information  though precision worse cases 
phenomenon holds even cameras domain  keyphrase text already performs well 
however  model still significantly outperforms model cluster text evaluations 
adding clustering information classifier baseline results performance sometimes
better models  result surprising  model cluster classifier gains
benefit models robust clustering learning sophisticated classifier assigning
properties texts  resulting combined system complex model itself 
potential yield better performance  hand  using simple heuristic reduce
noise present training data consistently hurts performance classifier  possibly
due reduction amount training data 
overall  enhanced performance methods  contrast keyphrase baselines 
aligned previous observations entailment research  dagan  glickman    magnini        
confirming paraphrasing information contributes greatly improved performance semantic
inference tasks 
impact paraphrasing quality previous section demonstrates one central
claims paper  accounting paraphrase structure yields substantial improvements semantic inference using noisy keyphrase annotations  second key aspect research
idea clustering quality benefits tying clusters hidden topics document
text  evaluate claim comparing models clustering independent clustering
baseline  compare gold standard clustering produced expert human annotators  test impact clustering methods  substitute models inferred clustering
alternative examine resulting semantic inferences change  comparison
performed semantic inference mechanism model  well model cluster
text  model cluster classifier heuristic model cluster classifier baselines 
add gold standard clustering model  replace hidden variables correspond keyphrase clusters observed values set according gold standard clustering    parameters trained modeling text  model variation  gold
cluster model  predicts properties using inference mechanism original model 
baseline variations gold cluster text  gold cluster classifier heuristic gold cluster classifier
likewise derived substituting automatically computed clustering gold standard clusters 
additional clustering obtained using keyphrase similarity information  specifically  modify original model learns keyphrase clustering isolation
text  learns property language models  framework  keyphrase clustering
entirely independent review text  text modeling learned keyphrase
clustering fixed  refer modification model independent cluster model 
model treats document text mixture latent topics  reminiscent models
supervised latent dirichlet allocation  slda  blei   mcauliffe         labels acquired
performing clustering across keyphrases preprocessing step  previous experiment  introduce three new baseline variations independent cluster text  independent cluster
classifier heuristic independent cluster classifier 
    gold standard clustering created part evaluation procedure described section       

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

lines      tables     present results experiments  gold cluster model
produces f scores comparable original model  providing strong evidence clustering
induced model sufficient quality semantic inference  application expertgenerated clustering baselines  lines            yields less consistent results  overall
evaluation provides little reason believe performance would substantially improved
obtaining clustering closer gold standard 
independent cluster model consistently reduces performance respect full joint
model  supporting hypothesis joint learning gives rise better prediction  independent
clustering baselines  independent cluster text  independent cluster classifier heuristic independent cluster classifier  lines         worse counterparts use model
clustering  lines       observation leads us conclude expert annotated
clustering always improve results  independent clustering always degrades them 
supports view joint learning clustering text models important prerequisite
better property prediction 
clustering
model clusters
independent clusters

restaurants
     
     

cell phones
     
     

digital cameras
     
     

table    rand index scores models clusters  learned keyphrases text jointly  compared clusters learned keyphrase similarity  evaluation cluster quality
based gold standard clustering 

another way assessing quality automatically obtained keyphrase clustering
quantify similarity clustering produced expert annotators  purpose
use rand index  rand         measure cluster similarity  measure varies zero
one  higher scores indicating greater similarity  table   shows rand index scores
models full joint clustering  well clustering obtained independent cluster model 
every domain  joint inference produces overall clustering improves upon keyphrasesimilarity only approach  scores confirm joint inference across keyphrases
document text produces better clustering considering features keyphrases alone 
    summarizing multiple reviews
last experiment examines multi document summarization capability system 
study models ability aggregate properties across set reviews  compared baselines
aggregate directly using free text annotations 
      data e valuation
selected    restaurants  five user written reviews restaurant  ten annotators
asked annotate reviews five restaurants each  comprising    reviews per annotator 
used six salient properties annotation guidelines previous restaurant
annotation experiment  see section     constructing ground truth  label properties
supported least three five reviews 

   

fib ranavan   c hen   e isenstein     barzilay

method
model
keyphrase aggregation
model cluster aggregation
gold cluster aggregation
indep  cluster aggregation

recall
     
     
     
     
     

prec 
     
     
     
     
     

f score
     
     
     
     
     

table    comparison aggregated property predictions made model series
baselines use free text annotations  methods model significantly better results using approximate randomization indicated p      

make property predictions set reviews model baselines
presented below  automatic methods  register prediction system judges
property supported least two five reviews    recall  precision  f score
computed aggregate predictions  six salient properties marked annotators 
      aggregation pproaches
evaluation  run trained version model described section        note
keyphrases provided model  though provided baselines 
obvious baseline summarizing multiple reviews would directly aggregate
free text keyphrases  annotations presumably representative reviews semantic
properties  unlike review text  keyphrases matched directly other  first
baseline applies notion directly 
keyphrase aggregation  keyphrase supported restaurant least two five
reviews annotated verbatim keyphrase 
simple aggregation approach obvious downside requiring strict matching independently authored reviews  reason  consider extensions aggregation
approach allow annotation paraphrasing 
model cluster aggregation  keyphrase supported restaurant least two
five reviews annotated keyphrase one paraphrases  paraphrasing
according models inferred clustering 
gold cluster aggregation  model cluster aggregation  using expert generated
clustering paraphrasing 
independent cluster aggregation  model cluster aggregation  using clustering
learned keyphrase similarity paraphrasing 
    three corroborating reviews required  baseline systems produce positive predictions  leading
poor recall  results setting presented appendix b 

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

      r esults
table   compares baselines model  model outperforms annotationbased baselines  despite access keyphrase annotations  notably  keyphrase aggregation performs poorly  makes predictions  result requirement
exact keyphrase string match  before  inclusion keyphrase clusters improves performance baseline models  however  incompleteness keyphrase annotations  see
section    explains recall scores still low compared model  incorporating
document text  model obtains dramatically improved recall  cost reduced precision 
ultimately yielding significantly improved f score 
results demonstrate review summarization benefits greatly joint model
review text keyphrases  nave approaches consider keyphrases yield inferior results 
even augmented paraphrase information 

   conclusions future work
paper  shown free text keyphrase annotations provided novice users
leveraged training set document level semantic inference  free text annotations
potential vastly expand set training data available developers semantic inference
systems  however  shown  suffer lack consistency completeness 
overcome problems inducing hidden structure semantic properties  correspond
clusters keyphrases hidden topics text  approach takes form
hierarchical bayesian model  addresses text keyphrases jointly 
model implemented system successfully extracts semantic properties unannotated restaurant  cell phone  camera reviews  empirically validating approach  experiments demonstrate necessity handling paraphrase structure free text keyphrase
annotations  moreover  show better paraphrase structure learned joint framework
models document text  approach outperforms competitive baselines semantic
property extraction single multiple documents  permits aggregation across
multiple keyphrases different surface forms multi document summarization 
work extends actively growing literature document topic modeling  topic modeling paraphrasing posit hidden layer captures relationship disparate surface
forms  topic modeling  set latent distributions lexical items  paraphrasing
represented latent clustering phrases  show two latent structures linked 
resulting increased robustness semantic coherence 
see several avenues future work  first  model draws substantial power features measure keyphrase similarity  ability use arbitrary similarity metrics desirable 
however  representing individual similarity scores random variables compromise 
clearly independent  believe problem could avoided modeling generation
entire similarity matrix jointly 
related approach would treat similarity matrix across keyphrases indicator
covariance structure  model  would learn separate language models keyphrase 
keyphrases rated highly similar would constrained induce similar language
models  approach might possible gaussian process framework  rasmussen  
williams        

   

fib ranavan   c hen   e isenstein     barzilay

currently focus model identify semantic properties expressed given
document  allows us produce summary properties  however  mentioned
section    human authors give equal importance properties producing summary
pros cons  one possible extension work would explicitly model likelihood
topic annotated document  might avoid current post processing step
uses property specific thresholds compute final predictions model output 
finally  assumed semantic properties unstructured  reality 
properties related interesting ways  trivially  domain reviews would desirable
model antonyms explicitly  e g   restaurant review simultaneously labeled
good bad food  relationships properties  hierarchical structures  could
considered  suggests possible connections correlated topic model blei
lafferty        

bibliographic note
portions work previously presented conference publication  branavan et al         
current article extends work several ways  notably  development evaluation
multi document review summarization system uses semantic properties induced
method  section       detailed analysis distributional properties free text annotations
 section     expansion evaluation include additional domain sets baselines
considered original paper  section        

acknowledgments
authors acknowledge support national science foundation  nsf  career grant iis         microsoft research new faculty fellowship  u s  office naval research
 onr   quanta computer  nokia corporation  harr chen supported national defense science engineering nsf graduate fellowships  thanks michael collins  zoran
dzunic  amir globerson  aria haghighi  dina katabi  kristian kersting  terry koo  yoong keok
lee  brian milch  tahira naseem  dan roy  christina sauper  benjamin snyder  luke zettlemoyer 
journal reviewers helpful comments suggestions  thank marcia davidson
members nlp group mit help expert annotations  opinions  findings 
conclusions recommendations expressed article authors  necessarily reflect views nsf  microsoft  onr  quanta  nokia 

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

appendix a  development test set statistics
table   lists semantic properties domain number documents used
evaluating properties  noted section        gold standard evaluation
complete  testing every property document  conversely  free text evaluations
property use documents annotated property antonym
number documents differs semantic property 
domain
restaurants  gold 
restaurants

cell phones

cameras

property
properties
good food
bad food
good price
bad price
good service
bad service
good reception
bad reception
good battery life
poor battery life
good price
bad price
small
large
good price
bad price
good battery life
poor battery life
great zoom
limited zoom

development documents
  

test documents
   

  

   

  

  

  

   

  

  

  

   

  

  

  

   

  

   

  

   

  

  

table    breakdown property development test sets used evaluations section       

   

fib ranavan   c hen   e isenstein     barzilay

appendix b  additional multiple review summarization results
table    lists results multi document experiment  variation aggregation
require automatic method predict property three five reviews predict
property product  rather two presented section      baseline systems 
change causes precipitous drop recall  leading f score results substantially worse
presented section        contrast  f score model consistent across
evaluations 
method
model
keyphrase aggregation
model cluster aggregation
gold cluster aggregation
indep  cluster aggregation

recall
     
     
     
     
     

prec 
     
     
     
     
     

f score
     
     
     
     
     

table     comparison aggregated property predictions made model series
baselines use free text annotations  aggregation requires three five reviews
predict property  rather two section      methods
model significantly better results using approximate randomization indicated
p      

appendix c  hyperparameter settings
table    lists values hyperparameters           used experiments domain 
values arrived tuning development set  cases    set
        making beta     uniform distribution 
hyperparameters
 
 
 

restaurants
      
     
     

cell phones
      
      
      

cameras
      
   
     

table     values hyperparameters used domain across experiments 

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

references
barzilay  r   mckeown  k     elhadad  m          information fusion context multidocument summarization  proceedings acl  pp         
barzilay  r     mckeown  k  r          extracting paraphrases parallel corpus  proceedings acl  pp       
bhattacharya  i     getoor  l          latent dirichlet model unsupervised entity resolution 
proceedings siam international conference data mining 
blei  d  m     lafferty  j  d          correlated topic models  advances nips  pp         
blei  d  m     mcauliffe  j          supervised topic models  advances nips  pp         
blei  d  m   ng  a  y     jordan  m  i          latent dirichlet allocation  journal machine
learning research             
boyd graber  j   blei  d     zhu  x          topic model word sense disambiguation 
proceedings emnlp  pp           
branavan  s  r  k   chen  h   eisenstein  j     barzilay  r          learning document level semantic properties free text annotations  proceedings acl  pp         
carbonell  j     goldstein  j          use mmr  diversity based reranking reordering
documents producing summaries  proceedings acm sigir  pp         
chinchor  n          statistical significance muc   results  proceedings  th conference
message understanding  pp       
chinchor  n   lewis  d  d     hirschman  l          evaluating message understanding systems 
analysis third message understanding conference  muc     computational linguistics                
cohen  j          coefficient agreement nominal scales  educational psychological
measurement              
dagan  i   glickman  o     magnini  b          pascal recognising textual entailment challenge  lecture notes computer science               
elhadad  n     mckeown  k  r          towards generating patient specific summaries medical
articles  proceedings naacl workshop automatic summarization  pp       
finkel  j  r   grenager  t     manning  c          incorporating non local information information extraction systems gibbs sampling  proceedings acl  pp         
gelman  a   carlin  j  b   stern  h  s     rubin  d  b          bayesian data analysis   nd edition  
texts statistical science  chapman   hall crc 
goldwater  s   griffiths  t  l     johnson  m          contextual dependencies unsupervised
word segmentation  proceedings acl  pp         
hu  m     liu  b          mining summarizing customer reviews  proceedings sigkdd 
pp         
joachims  t          making large scale support vector machine learning practical  pp         
mit press 

   

fib ranavan   c hen   e isenstein     barzilay

kim  s  m     hovy  e          automatic identification pro con reasons online reviews 
proceedings coling acl  pp         
lin  d     pantel  p          discovery inference rules question answering  natural language
engineering               
liu  b   hu  m     cheng  j          opinion observer  analyzing comparing opinions
web  proceedings www  pp         
lu  y     zhai  c          opinion integration semi supervised topic modeling  proceedings www  pp         
mani  i     bloedorn  e          multi document summarization graph search matching 
proceedings aaai  pp         
marsi  e     krahmer  e          explorations sentence fusion  proceedings european
workshop natural language generation  pp         
mccallum  a   bellare  k     pereira  f          conditional random field discriminativelytrained finite state string edit distance  proceedings uai  pp         
nenkova  a   vanderwende  l     mckeown  k          compositional context sensitive multidocument summarizer  exploring factors influence summarization  proceedings
sigir  pp         
noreen  e          computer intensive methods testing hypotheses  introduction  john
wiley sons 
popescu  a  m   nguyen  b     etzioni  o          opine  extracting product features opinions reviews  proceedings hlt emnlp  pp         
purver  m   kording  k  p   griffiths  t  l     tenenbaum  j  b          unsupervised topic modelling multi party spoken discourse  proceedings coling acl  pp       
radev  d   jing  h     budzikowska  m          centroid based summarization multiple documents  sentence extraction  utility based evaluation user studies  proceedings
anlp naacl summarization workshop 
radev  d     mckeown  k          generating natural language summaries multiple on line
sources  computational linguistics                
rand  w  m          objective criteria evaluation clustering methods  journal
american statistical association                  
rasmussen  c  e     williams  c  k  i          gaussian processes machine learning  mit
press 
riezler  s     maxwell  j  t          pitfalls automatic evaluation significance
testing mt  proceedings acl workshop intrinsic extrinsic evaluation
measures machine translation and or summarization  pp       
snyder  b     barzilay  r          multiple aspect ranking using good grief algorithm 
proceedings naacl hlt  pp         
titov  i     mcdonald  r       a   joint model text aspect ratings sentiment summarization  proceedings acl  pp         

   

fil earning ocument l evel emantic p roperties f ree  t ext nnotations

titov  i     mcdonald  r       b   modeling online reviews multi grain topic models 
proceedings www  pp         
toutanova  k     johnson  m          bayesian lda based model semi supervised part ofspeech tagging  advances nips  pp           
white  m   korelsky  t   cardie  c   ng  v   pierce  d     wagstaff  k          multi document
summarization via information extraction  proceedings hlt  pp     
yeh  a          accurate tests statistical significance result differences  proceedings coling  pp         
zaenen  a          mark up barking wrong tree  computational linguistics                

   


