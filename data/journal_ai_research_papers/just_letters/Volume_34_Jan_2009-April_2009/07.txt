journal of artificial intelligence research                  

submitted        published      

wikipedia based semantic interpretation
for natural language processing
evgeniy gabrilovich
shaul markovitch

gabr yahoo inc com
shaulm cs technion ac il

department of computer science
technionisrael institute of technology
technion city        haifa  israel

abstract
adequate representation of natural language semantics requires access to vast amounts
of common sense and domain specific world knowledge  prior work in the field was based
on purely statistical techniques that did not make use of background knowledge  on limited
lexicographic knowledge bases such as wordnet  or on huge manual efforts such as the
cyc project  here we propose a novel method  called explicit semantic analysis  esa  
for fine grained semantic interpretation of unrestricted natural language texts  our method
represents meaning in a high dimensional space of concepts derived from wikipedia  the
largest encyclopedia in existence  we explicitly represent the meaning of any text in terms
of wikipedia based concepts  we evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural
language text  using esa results in significant improvements over the previous state of
the art in both tasks  importantly  due to the use of natural concepts  the esa model is
easy to explain to human users 

   introduction
recent proliferation of the world wide web  and common availability of inexpensive storage
media to accumulate over time enormous amounts of digital data  have contributed to the
importance of intelligent access to this data  it is the sheer amount of data available that
emphasizes the intelligent aspect of accessno one is willing to or capable of browsing
through but a very small subset of the data collection  carefully selected to satisfy ones
precise information need 
research in artificial intelligence has long aimed at endowing machines with the ability
to understand natural language  one of the core issues of this challenge is how to represent language semantics in a way that can be manipulated by computers  prior work on
semantics representation was based on purely statistical techniques  lexicographic knowledge  or elaborate endeavors to manually encode large amounts of knowledge  the simplest
approach to represent the text semantics is to treat the text as an unordered bag of words 
where the words themselves  possibly stemmed  become features of the textual object  the
sheer ease of this approach makes it a reasonable candidate for many information retrieval
tasks such as search and text categorization  baeza yates   ribeiro neto        sebastiani 
       however  this simple model can only be reasonably used when texts are fairly long 
and performs sub optimally on short texts  furthermore  it does little to address the two
main problems of natural language processing  nlp   polysemy and synonymy 
c
    
ai access foundation  all rights reserved 

figabrilovich   markovitch

latent semantic analysis  lsa   deerwester  dumais  furnas  landauer    harshman 
      is another purely statistical technique  which leverages word co occurrence information from a large unlabeled corpus of text  lsa does not use any explicit human organized
knowledge  rather  it learns its representation by applying singular value decomposition
 svd  to the words by documents co occurrence matrix  lsa is essentially a dimensionality reduction technique that identifies a number of most prominent dimensions in the data 
which are assumed to correspond to latent concepts  meanings of words and documents
are then represented in the space defined by these concepts 
lexical databases such as wordnet  fellbaum        or rogets thesaurus  roget       
encode important relations between words such as synonymy  hypernymy  and meronymy 
approaches based on such resources  budanitsky   hirst        jarmasz        map text
words into word senses  and use the latter as concepts  however  lexical resources offer
little information about the different word senses  thus making word sense disambiguation
nearly impossible to achieve  another drawback of such approaches is that creation of
lexical resources requires lexicographic expertise as well as a lot of time and effort  and consequently such resources cover only a small fragment of the language lexicon  specifically 
such resources contain few proper names  neologisms  slang  and domain specific technical
terms  furthermore  these resources have strong lexical orientation in that they predominantly contain information about individual words  but little world knowledge in general 
being inherently limited to individual words  these approaches require an extra level of
sophistication to handle longer texts  mihalcea  corley    strapparava         for example 
computing the similarity of a pair of texts amounts to comparing each word of one text to
each word of the other text 
studies in artificial intelligence have long recognized the importance of knowledge for
problem solving in general  and for natural language processing in particular  back in the
early years of ai research  buchanan and feigenbaum        formulated the knowledge as
power hypothesis  which postulated that the power of an intelligent program to perform
its task well depends primarily on the quantity and quality of knowledge it has about that
task 
when computer programs face tasks that require human level intelligence  such as natural language processing  it is only natural to use an encyclopedia to endow the machine
with the breadth of knowledge available to humans  there are  however  several obstacles
on the way to using encyclopedic knowledge  first  such knowledge is available in textual
form  and using it may require natural language understanding  a major problem in its own
right  furthermore  even language understanding may not be enough  as texts written for
humans normally assume the reader possesses a large amount of common sense knowledge 
which is omitted even from most detailed encyclopedia articles  lenat         thus  there
is a circular dependencyunderstanding encyclopedia articles requires natural language
understanding capabilities  while the latter in turn require encyclopedic knowledge  to
address this situation  lenat and his colleagues launched the cyc project  which aims to
explicitly catalog the common sense knowledge of the humankind 
we developed a new methodology that makes it possible to use an encyclopedia directly 
without the need for manually encoded common sense knowledge  observe that an encyclopedia consists of a large collection of articles  each of which provides a comprehensive
exposition focused on a single topic  thus  we view an encyclopedia as a collection of con   

fiwikipedia based semantic interpretation

cepts  corresponding to articles   each accompanied with a large body of text  the article
contents   we propose to use the high dimensional space defined by these concepts in order
to represent the meaning of natural language texts  compared to the bag of words and lsa
approaches  using these concepts allows the computer to benefit from huge amounts of world
knowledge  which is normally accessible to humans  compared to electronic dictionaries and
thesauri  our method uses knowledge resources that are over an order of magnitude larger 
and also uniformly treats texts that are arbitrarily longer than a single word  even more
importantly  our method uses the body of text that accompanies the concepts in order to
perform word sense disambiguation  as we show later  using the knowledge rich concepts
addresses both polysemy and synonymy  as we no longer manipulate mere words  we call
our method explicit semantic analysis  esa   as it uses knowledge concepts explicitly
defined and manipulated by humans 
our approach is applicable to many nlp tasks whose input is a document  or shorter
natural language utterance   and the output is a decision based on the document contents 
examples of such tasks are information retrieval  whether the document is relevant   text
categorization  whether the document belongs to a certain category   or comparing pairs of
documents to assess their similarity  
observe that documents manipulated in these tasks are given in the same form as the
encyclopedic knowledge we intend to useplain text  it is this key observation that allows us
to circumvent the obstacles we enumerated above  and use the encyclopedia directly  without
the need for deep language understanding or pre cataloged common sense knowledge  we
quantify the degree of relevance of each wikipedia concept to the input text by comparing
this text to the article associated with the concept 
let us illustrate the importance of external knowledge with a couple of examples  without using external knowledge  specifically  knowledge about financial markets   one can infer
little information from a very brief news title bernanke takes charge  however  using the
algorithm we developed for consulting wikipedia  we find that the following concepts are
highly relevant to the input  ben bernanke  federal reserve  chairman of the
federal reserve  alan greenspan  bernankes predecessor   monetarism  an economic theory of money supply and central banking   inflation and deflation  as another
example  consider the title apple patents a tablet mac  without deep knowledge of hitech industry and gadgets  one finds it hard to predict the contents of the news item  using
wikipedia  we identify the following related concepts  apple computer    mac os  the
macintosh operating system  laptop  the general name for portable computers  of which
tablet mac is a specific example   aqua  the gui of mac os x   ipod  another prominent product by apple   and apple newton  the name of apples early personal digital
assistant  
for ease of presentation  in the above examples we only showed a few concepts identified
by esa as the most relevant for the input  however  the essence of our method is representing the meaning of text as a weighted combination of all wikipedia concepts  then 
   thus  we do not consider tasks such as machine translation or natural language generation  whose output
includes a new piece of text based on the input 
   note that we correctly identify the concept representing the computer company  apple computer 
rather than the fruit  apple  

   

figabrilovich   markovitch

depending on the nature of the task at hand we either use these entire vectors of concepts 
or use a few most relevant concepts to enrich the bag of words representation 
the contributions of this paper are twofold  first  we propose a new methodology
to use wikipedia for enriching representation of natural language texts  our approach 
named explicit semantic analysis  effectively capitalizes on human knowledge encoded in
wikipedia  leveraging information that cannot be deduced solely from the input texts being
processed  second  we evaluate esa on two commonly occurring nlp tasks  namely  text
categorization and computing semantic relatedness of texts  in both tasks  using esa
resulted in significant improvements over the existing state of the art performance 
recently  esa was used by other researchers in a variety of tasks  and consistently proved
to be superior to approaches that do not explicitly used large scale repositories of human
knowledge  gurevych  mueller  and zesch        re implemented our esa approach for the
german language wikipedia  and found it to be superior for judging semantic relatedness of
words compared to a system based on the german version of wordnet  germanet   chang 
ratinov  roth  and srikumar        used esa for a text classification task without explicit
training set  learning only from the knowledge encoded in wikipedia  milne and witten
       found esa to compare favorably to approaches that are solely based on hyperlinks 
thus confirming that the wealth of textual descriptions in wikipedia is exlicitly superior to
using structural information alone 

   explicit semantic analysis
what is the meaning of the word cat  one way to interpret the word cat is via an
explicit definition  a cat is a mammal with four legs  which belongs to the feline species 
etc  another way to interpret the meaning of cat is by the strength of its association with
concepts that we know  cat relates strongly to the concepts feline and pet  somewhat
less strongly to the concepts mouse and tom   jerry  etc 
we use this latter association based method to assign semantic interpretation to words
and text fragments  we assume the availability of a vector of basic concepts  c            cn   and
we represent each text fragment t by a vector of weights  w            wn   where wi represents the
strength of association between t and ci   thus  the set of basic concepts can be viewed as a
canonical n dimensional semantic space  and the semantics of each text segment corresponds
to a point in this space  we call this weighted vector the semantic interpretation vector of
t 
such a canonical representation is very powerful  as it effectively allows us to estimate
semantic relatedness of text fragments by their distance in this space  in the following
section we describe the two main components of such a scheme  the set of basic concepts 
and the algorithm that maps text fragments into interpretation vectors 
    using wikipedia as a repository of basic concepts
to build a general semantic interpreter that can represent text meaning for a variety of
tasks  the set of basic concepts needs to satisfy the following requirements 
   it should be comprehensive enough to include concepts in a large variety of topics 
   

fiwikipedia based semantic interpretation

   it should be constantly maintained so that new concepts can be promptly added as
needed 
   since the ultimate goal is to interpret natural language  we would like the concepts
to be natural  that is  concepts recognized and used by human beings 
   each concept ci should have associated text di   so that we can determine the strength
of its affinity with each term in the language 
creating and maintaining such a set of natural concepts requires enormous effort of many
people  luckily  such a collection already exists in the form of wikipedia  which is one of the
largest knowledge repositories on the web  wikipedia is available in dozens of languages 
while its english version is the largest of all  and contains      million words in nearly
one million articles  contributed by over         volunteer editors  even though wikipedia
editors are not required to be established researchers or practitioners  the open editing approach yields remarkable quality  a recent study  giles        found wikipedia accuracy to
rival that of encyclopaedia britannica  however  britannica is about an order of magnitude
smaller  with    million words in        articles  http   store britannica com  visited
on february           
as appropriate for an encyclopedia  each article comprises a comprehensive exposition
of a single topic  consequently  we view each wikipedia article as defining a concept that
corresponds to each topic  for example  the article about artificial intelligence defines the
concept artificial intelligence  while the article about parasitic extraction in circuit
design defines the concept layout extraction   the body of the articles is critical in our
approach  as it allows us to compute the affinity between the concepts and the words of the
input texts 
an important advantage of our approach is thus the use of vast amounts of highly organized human knowledge  compared to lexical resources such as wordnet  our methodology
leverages knowledge bases that are orders of magnitude larger and more comprehensive 
importantly  the web based knowledge repositories we use in this work undergo constant
development so their breadth and depth steadily increase over time  compared to latent
semantic analysis  our methodology explicitly uses the knowledge collected and organized
by humans  our semantic analysis is explicit in the sense that we manipulate manifest concepts grounded in human cognition  rather than latent concepts used by lsa  therefore 
we call our approach explicit semantic analysis  esa  
    building a semantic interpreter
given a set of concepts  c            cn   and a set of associated documents  d            dn   we build
a sparse table t where each of the n columns corresponds to a concept  and each of the rows
s
corresponds to a word that occurs in i     n di   an entry t  i  j  in the table corresponds
to the tfidf value of term ti in document dj
t  i  j    tf  ti   dj    log

n
 
dfi

   here we use the titles of articles as a convenient way to refer to the articles  but our algorithm treats
the articles as atomic concepts 

   

figabrilovich   markovitch

where term frequency is defined as
 

tf  ti   dj    

    log count ti   dj    if count ti   dj      
 
  
otherwise

and dfi     dk   ti  dk    is the number of documents in the collection that contain the
term ti  document frequency  
finally  cosine normalization is applied to each row to disregard differences in document
length 
t  i  j 
t  i  j   ppr
 
 
l   t  i  j 
where r is the number of terms 
the semantic interpretation of a word ti is obtained as row i of table t   that is  the
meaning of a word is given by a vector of concepts paired with their tfidf scores  which
reflect the relevance of each concept to the word 
the semantic interpretation of a text fragment  ht            tk i  is the centroid of the vectors
representing the individual words  this definition allows us to partially perform word sense
disambiguation  consider  for example  the interpretation vector for the term mouse  it
has two sets of strong components  which correspond to two possible meanings  mouse  rodent  and mouse  computing   similarly  the interpretation vector of the word screen
has strong components associated with window screen and computer screen  in a text
fragment such as i purchased a mouse and a screen  summing the two interpretation vectors will boost the computer related components  effectively disambiguating both words 
table t can also be viewed as an inverted index  which maps each word into a list
of concepts in which it appears  inverted index provides for very efficient computation of
distance between interpretation vectors 
given the amount of information encoded in wikipedia  it is essential to control the
amount of noise present in its text  we do so by discarding insufficiently developed articles 
and by eliminating spurious association between articles and words  this is done by setting
to zero the weights of those concepts whose weights for a given term are too low  see
section        
    using the link structure
it is only natural for an electronic encyclopedia to provide cross references in the form of
hyperlinks  as a result  a typical wikipedia article has many more links to other entries
than articles in conventional printed encyclopedias 
this link structure can be used in a number of ways  observe that each link is associated
with an anchor text  clickable highlighted phrase   the anchor text is not always identical
to the canonical name of the target article  and different anchor texts are used to refer
to the same article in different contexts  for example  anchor texts pointing at federal
reserve include fed  u s  federal reserve board  u s  federal reserve system 
board of governors of the federal reserve  federal reserve bank  foreign reserves
and free banking era  thus  anchor texts provide alternative names  variant spellings 
and related phrases for the target concept  which we use to enrich the article text for the
target concept 
   

fiwikipedia based semantic interpretation

furthermore  inter article links often reflect important relations between concepts that
correspond to the linked articles  we explore the use of such relations for feature generation
in the next section 
      second order interpretation
knowledge concepts can be subject to many relations  including generalization  meronymy
 part of   holonymy and synonymy  as well as more specific relations such as capital of 
birthplace birthdate of etc  wikipedia is a notable example of a knowledge repository
that features such relations  which are represented by the hypertext links between wikipedia
articles 
these links encode a large amount of knowledge  which is not found in article texts 
consequently  leveraging this knowledge is likely to lead to better interpretation models  we
therefore distinguish between first order models  which only use the knowledge encoded in
wikipedia articles  and second order models  which also incorporate the knowledge encoded
in inter article links  similarly  we refer to the information obtained through inter article
links as second order information 
as a rule  the presence of a link implies some relation between the concepts it connects 
for example  the article on the united states links to washington  d c   country
capital  and north america  the continent where the country is situated   it also links
to a multitude of other concepts  which are definitely related to the source concept  albeit it
is more difficult to define those relations  these links include united states declaration
of independence  president of the united states  and elvis presley 
however  our observations reveal that the existence of a link does not always imply the
two articles are strongly related   in fact  many words and phrases in a typical wikipedia
article link to other articles just because there are entries for the corresponding concepts 
for example  the education subsection in the article on the united states has gratuitous
links to concepts high school  college  and literacy rate  therefore  in order to
use wikipedia links for semantic interpretation  it is essential to filter the linked concepts
according to their relevance to the text fragment being interpreted 
an intuitive way to incorporate concept relations is to examine a number of top scoring
concepts 
and eto boost the scores of the concepts linked from them  let esa     t   
d
   
   
w            wn be the interpretation vector of term t  we define the second level interpretation of term t as
d

   

esa     t    w            wn   
where
   

wi

   

  wi

x

 

e

   

wj

 j link cj  ci   

using      ensures that the linked concepts are taken with reduced weights  in our
experiments we used        
   the opposite is also truethe absence of a link may simply be due to an oversight  adafre and de rijke
       studied the problem of discovering missing links in wikipedia 

   

figabrilovich   markovitch

      concept generality filter
not all the new concepts identified through links are equally useful  relevance of the newly
added concepts is certainly important  but is not the only criterion  suppose that we
are given an input text google search  which additional concept is likely to be more
useful to characterize the input  nigritude ultramarine  a specially crafted meaningless
phrase used in a search engine optimization contest  or website  now suppose the input is
artificial intelligence  which concept is likely to contribute more to the representation
of this input  john mccarthy  computer scientist  or logic  we believe that in both
examples  the second concept would be more useful because it is not overly specific 
consequently  we conjecture that we should add linked concepts sparingly  taking only
those that are more general than the concepts that triggered them  but how can we judge
the generality of concepts  while this may be tricky to achieve in the general case  no pun
intended   we propose the following task oriented criterion  given two concepts ca and cb  
we compare the numbers of links pointing at them  then  we say that ca is more general
than cb if its number of incoming links is at least an order of magnitude larger  that is  if
log     inlinks ca     log     inlinks cb        
we show examples of additional concepts identified using inter article links in section        in section       we evaluate the effect of using inter article links as an additional
knowledge source  in this section we also specifically examine the effect of only using more
general linked concepts  i e   adding concepts that are more general than the concepts that
triggered them  

   using explicit semantic analysis for computing semantic
relatedness of texts
in this section we discuss the application of our semantic interpretation methodology to
automatic assessment of semantic relatedness of words and texts  
    automatic computation of semantic relatedness
how related are cat and mouse  and what about preparing a manuscript and writing an article  the ability to quantify semantic relatedness of texts underlies many fundamental tasks in computational linguistics  including word sense disambiguation  information
retrieval  word and text clustering  and error correction  budanitsky   hirst         reasoning about semantic relatedness of natural language utterances is routinely performed by
humans but remains an unsurmountable obstacle for computers  humans do not judge text
relatedness merely at the level of text words  words trigger reasoning at a much deeper
level that manipulates conceptsthe basic units of meaning that serve humans to organize
and share their knowledge  thus  humans interpret the specific wording of a document
in the much larger context of their background knowledge and experience  lacking such
elaborate resources  computers need alternative ways to represent texts and reason about
them 
explicit semantic analysis represents text as interpretation vectors in the high dimensional space of concepts  with this representation  computing semantic relatedness of texts
   preliminary results of this research have been reported by gabrilovich and markovitch      a  

   

fiwikipedia based semantic interpretation

building semantic interpreter

word 
wordi

building weighted
inverted index
wikipedia

wordn

weighted list
of concepts
   wikipedia
articles 

weighted
inverted index

using semantic interpreter

text 

semantic
interpreter

vector
comparison

relatedness
estimation

text 
weighted
vector of
wikipedia
concepts

figure    knowledge based semantic interpreter

simply amounts to comparing their vectors  vectors could be compared using a variety
of metrics  zobel   moffat         we use the cosine metric throughout the experiments
reported in this paper  figure   illustrates this process 
    implementation details
we used wikipedia snapshot as of november           after parsing the wikipedia xml
dump  we obtained     gb of text in         articles  although wikipedia has almost a
million articles  not all of them are equally useful for feature generation  some articles correspond to overly specific concepts  e g   metnal  the ninth level of the mayan underworld  
or are otherwise unlikely to be useful for subsequent text categorization  e g   specific dates
or a list of events that happened in a particular year   other articles are just too short 
so we cannot reliably classify texts onto the corresponding concepts  we developed a set
of simple heuristics for pruning the set of concepts  by discarding articles that have fewer
than     non stop words or fewer than   incoming and outgoing links  we also discard articles that describe specific dates  as well as wikipedia disambiguation pages  category pages
and the like  after the pruning          articles were left that defined concepts used for
feature generation  we processed the text of these articles by first tokenizing it  removing
stop words and rare words  occurring in fewer than   articles   and stemmed the remaining
words  this yielded         distinct terms 
   

figabrilovich   markovitch

      preprocessing of wikipedia xml dump
wikipedia data is publicly available online at http   download wikimedia org  all the
data is distributed in xml format  and several packaged versions are available  article texts 
edit history  list of page titles  interlanguage links etc  in this project  we only use the article
texts  but ignore the information on article authors and page modification history  before
building the semantic interpreter  we perform a number of operations on the distributed
xml dump 
 we simplify the original xml by removing all those fields that are not used in feature
generation  such as author ids and last modification times 
 wikipedia syntax defines a proprietary format for inter article links  whereas the name
of the article referred to is enclosed in brackets  e g    united states    we map
all articles to numeric ids  and for each article build a list of ids of the articles it refers
to  we also count the number of incoming and outgoing links for each article 
 wikipedia defines a redirection mechanism  which maps frequently used variant names
of entities into canonical names  for examples  united states of america is
mapped to united states  we resolve all such redirections during initial preprocessing 
 another frequently used mechanism is templates  which allows articles to include
frequently reused fragments of text without duplication  by including pre defined and
optionally parameterized templates on the fly  to speed up subsequent processing  we
resolve all template inclusions at the beginning 
 we also collect all anchor texts that point at each article 
this preprocessing stage yields a new xml file  which is then used for building the feature
generator 
      the effect of knowledge breadth
wikipedia is being constantly expanded with new material as volunteer editors contribute
new articles and extend the existing ones  consequently  we conjectured that such addition
of information should be beneficial for esa  as it would rely on a larger knowledge base 
to test this assumption  we also acquired a newer wikipedia snapshot as of march    
      table   presents a comparison in the amount of information between two wikipedia
snapshots we used  the number of articles shown in the table reflects the total number
of articles as of the date of the snapshot  the next table line  the number of concepts
used  reflects the number of concepts that remained after the pruning as explained in the
beginning of section     
in the following sections we will confirm that using a larger knowledge base is beneficial
for esa  by juxtaposing the results obtained with the two wikipedia snapshots  therefore 
no further dimensionality reduction is performed  and each input text fragment is represented in the space of up to         features  or         features in the case of the later
wikipedia snapshot   of course  many of the features will have zero values  so the feature
vectors are sparse 
   

fiwikipedia based semantic interpretation

combined article text
number of articles
concepts used
distinct terms

wikipedia snapshot
as of november         
    gb
       
       
       

wikipedia snapshot
as of march         
    gb
         
       
       

table    comparison of two wikipedia snapshots
      inverted index pruning
we eliminate spurious association between articles and words by setting to zero the weights
of those concepts whose weights for a given term are too low 
the algorithm for pruning the inverted index operates as follows  we first sort all the
concepts for a given word according to their tfidf weights in decreasing order  we then
scan the resulting sequence of concepts with a sliding window of length      and truncate
the sequence when the difference in scores between the first and last concepts in the window
drops below    of the highest scoring concept for this word  which is positioned first in the
sequence   this technique looks for fast drops in the concept scores  which would signify
that the concepts in the tail of the sequence are only loosely associated with the word  i e  
even though the word occurred in the articles corresponding to these concepts  it its not
truly characteristic of the article contents   we evaluated more principled approaches such
observing the values of the first and second derivatives  but the data seemed to be too
noisy for reliable estimation of derivatives  other researchers studied the use of derivatives
in similar contexts  e g   begelman  keller    smadja         and also found that the
derivative alone is not sufficient  hence they found it necessary to estimate the magnitude
of peaks by other means  consequently  we opted to use the simple and efficient metric 
the purpose of such pruning is to eliminate spurious associations between concepts and
terms  and is mainly beneficial for pruning the inverted index entries for very common
words that occur in many wikipedia articles  using the above criteria  we analyzed the
inverted index for the wikipedia version dated november           see section         for
the majority of terms  there were either fewer than     concepts with non zero weight  or
the concept term weights decreased gracefully and did not qualify for pruning  we pruned
the entries of      terms out of the total of         terms  among the terms whose concept
vector was pruned  the term link had the largest number of concepts with non zero
weight       of which we retained only     concepts         as another example  the
concept vector for the term number was pruned from        entries down to             
on the average      of concepts have been retained  the pruning rates for the second
wikipedia version  dated march           have been similar to these 
      processing time
using world knowledge requires additional computation  this extra computation includes
the  one time  preprocessing step where the semantic interpreter is built  as well as the
actual mapping of input texts into interpretation vectors  performed online  on a standard workstation  parsing the wikipedia xml dump takes about   hours  and building the
   

figabrilovich   markovitch

semantic interpreter takes less than an hour  after the semantic interpreter is built  its
throughput  i e   the generation of interpretation vectors for textual input  is several hundred words per second  in the light of the improvements computing semantic relatedness
and in text categorization accuracy that we report in sections   and    we believe that the
extra processing time is well compensated for 
    empirical evaluation of explicit semantic analysis
humans have an innate ability to judge semantic relatedness of texts  human judgements
on a reference set of text pairs can thus be considered correct by definition  a kind of gold
standard against which computer algorithms are evaluated  several studies measured
inter judge correlations and found them to be consistently high  budanitsky   hirst       
jarmasz        finkelstein  gabrilovich  matias  rivlin  solan  wolfman    ruppin      a  
r               these findings are to be expectedafter all  it is this consensus that allows
people to understand each other  consequently  our evaluation amounts to computing the
correlation of esa relatedness scores with human judgments 
to better evaluate wikipedia based semantic interpretation  we also implemented a semantic interpreter based on another large scale knowledge repositorythe open directory
project  odp  http   www dmoz org   which is the largest web directory to date  in the case
of odp  concepts ci correspond to categories of the directory  e g   top computers artificial intelligence   and text di associated with each concept is obtained by pooling
together the titles and descriptions of the urls catalogued under the corresponding category  interpretation of a text fragment amounts to computing a weighted vector of odp
concepts  ordered by their affinity to the input text  we built the odp based semantic
interpreter using an odp snapshot as of april       further implementation details can
be found in our previous work  gabrilovich   markovitch            b  
      test collections
in this work  we use two datasets that to the best of our knowledge are the largest publicly
available collections of their kind   for both test collections  we use the correlation of
computer assigned scores with human scores to assess the algorithm performance 
to assess word relatedness  we use the wordsimilarity     collection  finkelstein et al  
    a  finkelstein  gabrilovich  matias  rivlin  solan  wolfman    ruppin      b   which
contains     noun pairs representing various degrees of similarity   each pair has   
   human judgements made by individuals with university degrees having either mothertongue level or otherwise very fluent command of the english language  word pairs were
assigned relatedness scores on the scale from    totally unrelated words  to     very much
related or identical words   judgements collected for each word pair were then averaged to
   recently  zesch and gurevych        discussed automatic creation of datasets for assessing semantic
similarity  however  the focus of their work was on automatical generation of a set of sufficiently
diverse word pairs  thus relieving the humans of the need to construct word lists manually  obviously 
establishing the gold standard semantic relatedness for each word pair is still performed manually by
human judges 
   some previous studies  jarmasz   szpakowicz        suggested that the word pairs comprising this
collection might be culturally biased 

   

fiwikipedia based semantic interpretation

produce a single relatedness score   spearmans rank order correlation coefficient was used
to compare computed relatedness scores with human judgements  being non parametric 
spearmans correlation coefficient is considered to be much more robust than pearsons
linear correlation  when comparing our results to those of other studies  we have computed
the spearmans correlation coefficient with human judgments based on their raw data 
for document similarity  we used a collection of    documents from the australian
broadcasting corporations news mail service  lee  pincombe    welsh        pincombe 
       the documents were between    and     words long  and covered a variety of topics 
the judges were    students from the university of adelaide  australia  who were paid a
small fee for their work  these documents were paired in all possible ways  and each of the
      pairs has     human judgements  averaged for each pair   to neutralize the effects
of ordering  document pairs were presented in random order  and the order of documents
within each pair was randomized as well  when human judgements have been averaged for
each pair  the collection of       relatedness scores have only    distinct values  spearmans
correlation is not appropriate in this case  and therefore we used pearsons linear correlation
coefficient 
importantly  instructions for human judges in both test collections specifically directed
the participants to assess the degree of relatedness of words and texts involved  for example 
in the case of antonyms  judges were instructed to consider them as similar rather than
dissimilar 
      prior work
a number of prior studies proposed a variety of approaches to computing word similarity
using wordnet  rogets thesaurus  and lsa  table   presents the results of applying these
approaches to the wordsimilarity     test collection 
jarmasz        replicated the results of several wordnet based methods  and compared
them to a new approach based on rogets thesaurus  hirst and st onge        viewed
wordnet as a graph  and considered the length and directionality of the graph path connecting two nodes  leacock and chodorow        also used the length of the shortest graph
path  and normalized it by the maximum taxonomy depth  jiang and conrath         and
later resnik         used the notion of information content of the lowest node subsuming two given words  lin      b  proposed a computation of word similarity based on
the information theory  see  budanitsky   hirst        for a comprehensive discussion of
wordnet based approaches to computing word similarity 
according to jarmasz         rogets thesaurus has a number of advantages compared
to wordnet  including links between different parts of speech  topical groupings  and a variety of relations between word senses  consequently  the method developed by the authors
using rogets as a source of knowledge achieved much better results than wordnet based
methods  finkelstein et al       a  reported the results of computing word similarity using
   finkelstein et al       a  report inter judge agreement of      for the wordsimilarity     collection  we
have also performed our own assessment of the inter judge agreement for this dataset  following snow 
oconnor  jurafsky  and ng         we divided the human judges into two sets and averaged the numeric
judgements for each word pair among the judges in the set  thus yielding a      element long  vector of
average judgments for each set  spearmans correlation coefficient between the vectors of the two sets
was       

   

figabrilovich   markovitch

an lsa based model  deerwester et al         trained on the grolier academic american encyclopedia  recently  hughes and ramage        proposed a method for computing semantic relatedness using random graph walks  their results on the wordsimilarity    dataset are competitive with those reported by jarmasz        and finkelstein et al 
     a  
strube and ponzetto        proposed an alternative approach to computing word similarity based on wikipedia  by comparing articles in whose titles the words occur  we discuss
this approach in greater detail in section     
prior work on assessing the similarity of textual documents was based on comparing
the documents as bags of words  as well as on lsa  lee et al         compared a number
of approaches based on the bag of words representation  which used both binary and tfidf
representation of word weights and a variety of similarity measures  correlation  jaccard 
cosine  and overlap   the authors also implemented an lsa based model trained on a set
of news documents from the australian broadcasting corporation  test documents whose
similarity was computed came from the same distribution   the results of these experiments
are reported in table   
      results
to better understand how explicit semantic analysis works  let us consider similarity computation for pairs of actual phrases  for example  given two phrases scientific article and
journal publication  esa determines that the following wikipedia concepts are found
among the top    concepts for each phrase  scientific journal  nature  journal  
academic publication  science  journal   and peer review  when we compute
similarity of rna and dna  the following concepts are found to be shared among
the top    lists  transcription  genetics   gene  rna  and cell  biology   it is
the presence of identical concepts among the top concepts characterizing each phrase that
allows esa to establish their semantic similarity 
table   shows the results of applying our methodology to estimating relatedness of
individual words  with statistically significant improvements shown in bold  the values
shown in the table represent spearmans correlation between the human judgments and
the relatedness scores produced by the different methods  jarmasz        compared the
performance of   wordnet based metrics  namely  those proposed by hirst and st onge
        jiang and conrath         leacock and chodorow         lin      b   and resnik
        in table   we report the performance of the best of these metrics  namely  those
by lin      b  and resnik         in the wikirelate  paper  strube   ponzetto        
the authors report results of as many as   different method variations  and again we report
the performance of the best one  based on the metric proposed by leacock and chodorow 
      
as we can see  both esa techniques yield substantial improvements over previous state
of the art results  notably  esa also achieves much better results than another recently
introduce method based on wikipedia  strube   ponzetto         we provide a detailed
comparison of our approach with this latter work in section      table   shows the results
for computing relatedness of entire documents  in both tables  we show the statistical
significance of the difference between the performance of esa wikipedia  march         
   

fiwikipedia based semantic interpretation

algorithm

wordnet based techniques  jarmasz       
rogets thesaurus based technique  jarmasz       
lsa  finkelstein et al       a 
wikirelate   strube   ponzetto       
markovlink  hughes   ramage       
esa wikipedia  march          version 
esa wikipedia  november          version 
esa odp

spearmans
correlation with
human judgements
    
    
    
    
    
    
    
    

stat 
significance
 p value 
       
        
        
      
        


      

table    spearmans rank correlation of word relatedness scores with human judgements
on the wordsimilarity     collection
algorithm

bag of words  lee et al        
lsa  lee et al        
esa wikipedia  march          version 
esa wikipedia  november          version 
esa odp

pearsons
correlation with
human judgements
      
    
    
    
    

stat 
significance
 p value 
       
      


    

table    pearsons correlation of text relatedness scores with human judgements on lee et
al s document collection

version  and that of other algorithms  by using fishers z transformation  press  teukolsky 
vetterling    flannery        section       
on both test collections  wikipedia based semantic interpretation is superior to the
odp based one  in the word relatedness task  this superiority is statistically significant at
p          we believe that two factors contribute to this phenomenon  first  axes of a
multi dimensional interpretation space should ideally be as independent as possible  the
hierarchical organization of the open directory reflects the generalization relation between
concepts and obviously violates this independence requirement  second  to increase the
amount of training data for building the odp based semantic interpreter  we crawled all the
urls listed in the odp  this allowed us to increase the amount of textual data by several
orders of magnitude  but also brought about a non negligible amount of noise  which is
common in web pages  on the other hand  wikipedia articles are virtually noise free  and
   whenever a range of values is available  we compared esa wikipedia with the best performing method
in the range 

   

figabrilovich   markovitch

mostly qualify as standard written english  thus  the textual descriptions of wikipedia
concepts are arguably more focused than those of the odp concepts 
it is also essential to note that in both experiments  using a newer wikipedia snapshot
leads to better results  although the difference between the performance of two versions is
admittedly small  
we evaluated the effect of using second order interpretation for computing semantic
relatedness of texts  but it only yielded negligible improvements  we hypothesize that the
reason for this finding is that computing semantic relatedness essentially uses all available
wikipedia concepts  so second order interpretation can only slightly modify the weights
of existing concepts  in the next section  which describes the application of esa to text
categorization  we trim the interpretation vectors for the sake of efficiency  and only consider
a few highest scoring concepts for each input text fragment  in this scenario  secondorder interpretation does have a positive effect and actually improves the accuracy of text
categorization  section         this happens because only a few selected wikipedia concepts
are used to augment text representation  and the second order approach selectively adds
highly related concepts identified by analyzing wikipedia links 

   using explicit semantic analysis for text categorization
in this section we evaluate the benefits of using external knowledge for text categorization   
    background on text categorization
text categorization  tc  deals with assigning category labels to natural language documents  categories come from a fixed set of labels  possibly organized in a hierarchy  and
each document may be assigned one or more categories  text categorization systems are
useful in a wide variety of tasks  such as routing news and e mail to appropriate corporate
desks  identifying junk email  or correctly handling intelligence reports 
the majority of existing text classification systems represent text as a bag of words  and
use a variant of the vector space model with various weighting schemes  salton   mcgill 
       thus  the features commonly used in text classification are weighted occurrence
frequencies of individual words  state of the art systems for text categorization use a variety
of induction techniques  such as support vector machines  k nearest neighbor algorithm 
and neural networks  the bag of words  bow  method is very effective in easy to medium
difficulty categorization tasks where the category of a document can be identified by several
easily distinguishable keywords  however  its performance becomes quite limited for more
demanding tasks  such as those dealing with small categories or short documents 
there have been various attempts to extend the basic bow approach  several studies
augmented the bag of words with n grams  caropreso  matwin    sebastiani        peng
  shuurmans        mladenic        raskutti  ferra    kowalczyk        or statistical
language models  peng  schuurmans    wang         others used linguistically motivated
features based on syntactic information  such as that available from part of speech tagging
or shallow parsing  sable  mckeown    church        basili  moschitti    pazienza        
additional studies researched the use of word clustering  baker   mccallum        bekker    preliminary results of this research have been reported by gabrilovich and markovitch        

   

fiwikipedia based semantic interpretation

man        dhillon  mallela    kumar         neural networks  jo        jo   japkowicz 
      jo         as well as dimensionality reduction techniques such as lsa  deerwester
et al         hull        zelikovitz   hirsh        cai   hofmann         however  these
attempts had mostly limited success 
we believe that the bag of words approach is inherently limited  as it can only use those
pieces of information that are explicitly mentioned in the documents  and only if the same
vocabulary is consistently used throughout  the bow approach cannot generalize over
words  and consequently words in the testing document that never appeared in the training
set are necessarily ignored  nor can synonymous words that appear infrequently in training
documents be used to infer a more general principle that covers all the cases  furthermore 
considering the words as an unordered bag makes it difficult to correctly resolve the sense
of polysemous words  as they are no longer processed in their native context  most of these
shortcomings stem from the fact that the bag of words method has no access to the wealth
of world knowledge possessed by humans  and is therefore easily puzzled by facts and terms
that cannot be easily deduced from the training set 
    using esa for feature generation
we propose a solution that augments the bag of words with knowledge based features 
given a document to be classified  we would like to use esa to represent the document
text in the space of wikipedia concepts  however  text categorization is crucially different
from computing semantic relatedness  cf  section    in two important respects 
first  computing semantic relatedness is essentially a one off task  that is  given
a particular pair of text fragments  we need to quantify their relatedness with no prior
examples for this specific task  in such cases  the very words of the text fragments are likely
to be of marginal usefulness  especially when the two fragments are one word long  this
happens because all the data available to us is limited to the two input fragments  which in
most cases share few words  if at all 
on the other hand  in supervised text categorization  one is usually given a collection
of labeled text documents  from which one can induce a text categorizer  consequently 
words that occur in the training examples can serve as valuable featuresthis is how the
bag of words approach was born  as we have observed in an earlier work  gabrilovich
  markovitch            b   it is ill advised to completely replace the bag of words with
generated concepts  and instead it is advantageous to enrich the bag of words  rather  we
opt to augment the bag of words with carefully selected knowledge concepts  which become
new features of the document  we refer to this process as feature generation  because we
actually construct new document features beyond those in the bag of words 
second  enriching document representation for text categorization with all possible
wikipedia concepts is extremely expensive computationally  because a machine learning
classifier will be learned in the augmented feature space  such a representation obviously
takes a lot of storage space  and cannot be processed efficiently because of the multitude of
the concepts involved  whose number can easily reach hundreds of thousands   therefore 
in the text categorization task  we prune the interpretation vectors to only retain a number
of highest scoring concepts for each input text fragment 

   

figabrilovich   markovitch

using the multi resolution approach to feature generation we believe that considering the document as a single unit can often be misleading  its text might be too diverse
to be readily mapped to the right set of concepts  while notions mentioned only briefly may
be overlooked  instead  we partition the document into a series of non overlapping segments
 called contexts   and then generate features at this finer level  each context is mapped
into a number of wikipedia concepts in the knowledge base  and pooling these concepts
together to describe the entire document results in multi faceted classification  this way 
the resulting set of concepts represents the various aspects or sub topics covered by the
document 
potential candidates for such contexts are simple sequences of words  or more linguistically motivated chunks such as sentences or paragraphs  the optimal resolution for document segmentation can be determined automatically using a validation set  in our earlier work  gabrilovich   markovitch            b   we proposed a more principled multiresolution approach that simultaneously partitions the document at several levels of linguistic abstraction  windows of words  sentences  paragraphs  up to taking the entire document
as one big chunk   and performs feature generation at each of these levels  we rely on the
subsequent feature selection step to eliminate extraneous features  preserving only those
that genuinely characterize the document 
it is essential to emphasize that using the multi resolution approach only makes sense
when interpretation vectors are pruned to only retain a number of highest scoring concepts for each context  as explained above  this is exactly the case for text categorization 
without such pruning  producing interpretation vectors for each context and then summing
them up would be equivalent to simply multiplying the weight of each concept by a constant
factor  in order to explain why the situation is different in the presence of pruning  let us
consider an example  suppose we have a long document that only mentions a particular
topic t in its last paragraph  since this topic is not central to the document  the n topscoring concepts in the documents interpretation vector i are unlikely to cover this topic 
although t is likely to be covered by other concepts in i  those concepts have lower weight
in i and are going to be pruned  however  if we produce interpretation vectors also for
each paragraph of the document  and retain n highest scoring concepts of each  then the
concepts generated for the last paragraph will cover t   consequently  t will have representation in the joined set of concepts generated for the document  in many text categorization
tasks  documents are labeled with a particular topic even if they mention the topic briefly 
hence generating features describing such topics is very important 
feature generation feature generation is performed prior to text categorization  each
document is transformed into a series of local contexts  which are then represented as
interpretation vectors using esa  the top ten concepts of all the vectors are pooled together 
and give rise to the generated features of the document  which are added to the bag of words 
since concepts in our approach correspond to wikipedia articles  constructed features also
correspond to the articles  thus  a set of features generated for a document can be viewed
as representing a set of wikipedia articles that are most relevant to the document contents 
the constructed features are used in conjunction with the original bag of words  the
resulting set optionally undergoes feature selection  and the most discriminative features
are retained for document representation 
   

fiwikipedia based semantic interpretation

basic
features

feature
selection

selected
features

labeled
documents

feature
valuation

induction
algorithm

classifier

classifier

classified
documents

labeled
feature
vectors

training

testing
testing
documents

feature
valuation

figure    standard approach to text categorization 

feature generation
feature
construction

feature
selection

generated
features

wikipedia
labeled
documents

feature
valuation

induction
algorithm

classifier

labeled
feature
vectors

figure    induction of text classifiers using the proposed framework for feature generation 

figure   depicts the standard approach to text categorization  figure   outlines the
proposed feature generation framework  observe that the feature generation box replaces
the feature selection box framed in bold in figure   
it is essential to note that we do not use the encyclopedia to simply increase the amount
of the training data for text categorization  neither do we use it as a text corpus to collect
word co occurrence statistics  rather  we use the knowledge distilled from the encyclopedia
to enrich the representation of documents  so that a text categorizer is induced in the
augmented  knowledge rich feature space 
   

figabrilovich   markovitch

    test collections
this section gives a brief description of the test collections we used to evaluate our methodology  we provide a much more detailed description of these test collections in appendix b 
   reuters        reuters        is historically the most often used dataset in text categorization research  following common practice  we used the modapte split       training 
     testing documents  and two category sets     largest categories and    categories with
at least one training and testing example 
      newsgroups    ng   lang        is a well balanced dataset of    categories
containing      documents each 
   movie reviews  movies   pang  lee    vaithyanathan        defines a sentiment
classification task  where reviews express either positive or negative opinion about the
movies  the dataset has      documents in two categories  positive negative 
   reuters corpus volume i  rcv    lewis  yang  rose    li        has over
        documents  to speed up the experiments  we used a subset of rcv  with        training documents  dated             and       testing ones               following
brank  grobelnik  milic frayling  and mladenic         we used    topic and    industry
categories that constitute representative samples of the full groups of     and     categories 
respectively  we also randomly sampled the topic and industry categories into   sets of
   categories each   
   ohsumed  hersh  buckley  leone    hickam        is a subset of medline  which
contains         medical documents  each document contains a title  and about two thirds
          also contain an abstract  each document is labeled with an average of    mesh  
categories  out of total          following joachims         we used a subset of documents
from      that have abstracts  taking the first        documents for training and the next
       for testing  to limit the number of categories for the experiments  we randomly
generated   sets of    categories each   
using these   datasets allows us to comprehensively evaluate the performance of our
approach  specifically  comparing    newsgroups and the two reuters datasets  reuters      and reuters corpus volume     we observe that the former is substantially more
noisy since the data has been obtained from usenet newsgroups  while the reuters datasets
are significantly cleaner  the movie reviews collection presents an example of sentiment
classification  which is different from standard  topical  text categorization  finally  the
ohsumed dataset presents an example of a very comprehensive taxonomy of over       
categories  as we explain the next section  we also used this dataset to create a collection
of labeled short texts  which allowed us to quantify the performance of our method on such
texts 
short documents we also derived several datasets of short documents from the test
collections described above  recall that about one third of ohsumed documents have
titles but no abstract  and can therefore be considered short documents as is  we used
the same range of documents as defined above  but considered only those without abstracts 
this yielded       training and       testing documents  for all other datasets  we created
    the full definition of the category sets we used is available in table    see section b    
    http   www nlm nih gov mesh
    the full definition of the category sets we used is available in table    see section b    

   

fiwikipedia based semantic interpretation

a short document from each original document by taking only the title of the latter  with
the exception of movie reviews  where documents have no titles  
it should be noted  however  that substituting a title for the full document is a poor
mans way to obtain a collection of classified short documents  when documents were first
labeled with categories  the human labeller saw each document in its entirety  in particular 
a category might have been assigned to a document on the basis of facts mentioned in its
body  even though the information may well be missing from the  short  title  thus  taking
all the categories of the original documents to be genuine categories of the title is often
misleading  however  because we know of no publicly available test collections of short
documents  we decided to construct datasets as explained above  importantly  ohsumed
documents without abstracts have been classified as such by humans  working with the
ohsumed derived dataset can thus be considered a pure experiment 
    experimentation procedure
we used support vector machines   as our learning algorithm to build text categorizers  since
prior studies found svms to have the best performance for text categorization  sebastiani 
      dumais  platt  heckerman    sahami        yang   liu         following established
practice  we use the precision recall break even point  bep  to measure text categorization
performance  bep is defined in terms of the standard measures of precision and recall 
where precision is the proportion of true document category assignments among all assignments predicted by the classifier  and recall is the proportion of true document category
assignments that were also predicted by the classifier  it is obtained by either tuning the
classifier so that precision is equal to recall  or sampling several  precision  recall  points
that bracket the expected bep value and then interpolating  or extrapolating  in the event
that all the sampled points lie on the same side  
for the two reuters datasets and ohsumed we report both micro  and macro averaged
bep  since their categories differ in size significantly  micro averaged bep operates at the
document level and is primarily affected by categorization performance on larger categories 
on the other hand  macro averaged bep averages results for individual categories  and thus
small categories with few training examples have large impact on the overall performance 
for both reuters datasets  reuters       and rcv   and ohsumed we used a fixed
train test split as defined in section      and consequently used macro sign test  s test 
 yang   liu        to assess the statistical significance of differences in classifier performance  for   ng and movies we performed   fold cross validation  and used paired t test to
assess the significance  we also used the non parametric wilcoxon signed ranks test  demsar        to compare the baseline and the fg based classifiers over multiple data sets  in
the latter case  the individual measurements taken are the  micro  or macro averaged  bep
values observed on each dataset 
    we used the svm light implementation  joachims        with the default parameters  in our earlier
work on feature selection  gabrilovich   markovitch         we conducted a thorough experimentation
with a wide range of values of the c parameter  and found it not to be of any major importance for
these datasets  consequently  we leave this parameter at its default setting as well 

   

figabrilovich   markovitch

      text categorization infrastructure
we conducted the experiments using a text categorization platform of our own design and
development named hogwarts     davidov  gabrilovich    markovitch         we opted
to build a comprehensive new infrastructure for text categorization  as surprisingly few software tools are publicly available for researchers  while those that are available allow only
limited control over their operation  hogwarts facilitates full cycle text categorization
including text preprocessing  feature extraction  construction  selection and weighting  followed by actual classification with cross validation of experiments  the system currently
provides xml parsing  part of speech tagging  brill         sentence boundary detection 
stemming  porter         wordnet  fellbaum        lookup  a variety of feature selection
algorithms  and tfidf feature weighting schemes  hogwarts has over     configurable
parameters that control its modus operandi in minute detail  hogwarts interfaces with
svm  knn and c    text categorization algorithms  and computes all standard measures
of categorization performance  hogwarts was designed with a particular emphasis on
processing efficiency  and portably implemented in the ansi c   programming language
and c   standard template library  the system has built in loaders for reuters      
 reuters         rcv   lewis et al             newsgroups  lang         movie reviews
 pang et al          and ohsumed  hersh et al          while additional datasets can be
easily integrated in a modular way 
each document undergoes the following processing steps  document text is first tokenized  and title words are replicated twice to emphasize their importance  then  stop
words  numbers and mixed alphanumeric strings are removed  and the remaining words
are stemmed  the bag of words is next merged with the set of features generated for the
document by analyzing its contexts as explained in section      and rare features occurring
in fewer than   documents are removed 
since earlier studies found that most bow features are indeed useful for svm text
categorization    joachims        rogati   yang        brank et al         bekkerman 
      leopold   kindermann        lewis et al          we take the bag of words in its
entirety  with the exception of rare features removed in the previous step   the generated
features  however  undergo feature selection using the information gain criterion    finally 
feature weighting is performed using the ltc tf idf function  logarithmic term frequency
and inverse document frequency  followed by cosine normalization   salton   buckley       
debole   sebastiani        
      baseline performance of hogwarts
we now demonstrate that the performance of basic text categorization in our implementation  column baseline in table    is consistent with the state of the art as reflected in
other published studies  all using svm   on reuters        dumais et al         achieved
    hogwarts school of witchcraft and wizardry is the educational institution attended by harry potter
 rowling        
    gabrilovich and markovitch        described a class of problems where feature selection from the bag
of words actually improves svm performance 
    of course  feature selection is performed using only the training set of documents 

   

fiwikipedia based semantic interpretation

micro bep of       for    categories and       for all categories  on   ng     bekkerman
       obtained bep of        pang et al         obtained accuracy of       on movies    
the minor variations in performance are due to differences in data preprocessing in the
different systems  for example  for the movies dataset we worked with raw html files
rather than with the official tokenized version  in order to recover sentence and paragraph
structure for contextual analysis  for rcv  and ohsumed  direct comparison with published results is more difficult because we limited the category sets and the date span of
documents to speed up experimentation 
      using the feature generator
the core engine of explicit semantic analysis was implemented as explained in section     
we used the multi resolution approach to feature generation  classifying document contexts at the level of individual words  complete sentences  paragraphs  and finally the entire
document    for each context  features were generated from the    best matching concepts
produced by the feature generator 
    wikipedia based feature generation
in this section  we report the results of an experimental evaluation of our methodology 
      qualitative analysis of feature generation
we now study the process of feature generation on a number of actual examples 
feature generation per se to illustrate our approach  we show features generated for
several text fragments  whenever applicable  we provide short explanations of the generated
concepts  in most cases  the explanations are taken from wikipedia  wikipedia        
 text  wal mart supply chain goes real time
top    generated features      wal mart      sam walton      sears holdings
corporation      target corporation      albertsons      asda      rfid     
hypermarket      united food and commercial workers       chain store
selected explanations      wal mart founder      prominent competitors of walmart      a wal mart subsidiary in the uk      radio frequency identification  a
technology that wal mart uses very extensively to manage its stock      superstore
 a general concept  of which wal mart is a specific example       a labor union that
    for comparison with the results reported by bekkerman        we administered a single test run  i e  
without cross validation   taking the first     of postings in each newsgroup for training  and the rest
for testing 
    for comparison with the results reported by pang et al         we administered a single test run  i e  
without cross validation   taking the first     of the data for each opinion type for training  and the rest
for testing 
    the   ng dataset is an exception  owing to its high level of intrinsic noise that renders identification
of sentence boundaries extremely unreliable  and causes word level feature generation to produce too
many spurious classifications  consequently  for this dataset we restrict the multi resolution approach
to individual paragraphs and the entire document only 

   

figabrilovich   markovitch

has been trying to organize wal marts workers       a general concept  of which
wal mart is a specific example
 it is particularly interesting to juxtapose the features generated for fragments that
contain ambiguous words  to this end  we show features generated for two phrases
that contain the word bank in two different senses  bank of america  financial
institution  and bank of amazon  river bank   as can be readily seen  our feature generation methodology is capable of performing word sense disambiguation by
considering ambiguous words in the context of their neighbors 
 text  bank of america
top    generated features      bank      bank of america      bank of
america plaza  atlanta       bank of america plaza  dallas       mbna
 a bank holding company acquired by bank of america       visa  credit
card       bank of america tower  new york city      nasdaq      mastercard       bank of america corporate center
 text  bank of amazon
top    generated features      amazon river      amazon basin      amazon rainforest      amazon com      rainforest      atlantic ocean     
brazil      loreto region  a region in peru  located in the amazon rainforest  
    river       economy of brazil
 our method  however  is not      accurate  and in some cases it generates features
that are only somewhat relevant or even irrelevant to the input text  as an example  we show the outcome of feature generation for the title of our earlier article
 gabrilovich   markovitch         for each concept  we show a list of input words
that triggered it  the words are stemmed and sorted in the decreasing order of their
contribution  
text  overcoming the brittleness bottleneck using wikipedia  enhancing text categorization with encyclopedic knowledge
top    generated features 
   encyclopedia  encyclopedia  knowledge  wikipedia  text 
   wikipedia  wikipedia  enhance  encyclopedia  text 
   enterprise content management  category  knowledge  text  overcome  enhance 
   performance problem  bottleneck  category  enhance 
   immanuel kant  category  knowledge  overcome 
   tooth enamel  brittleness  text  enhance 
   lucid dreaming  enhance  text  knowledge  category 
   bottleneck  bottleneck 
   java programming language  category  bottleneck  enhance 
   

fiwikipedia based semantic interpretation

    transmission control protocol  category  enhance  overcome 
some of the generated features are clearly relevant to the input  such as encyclopedia 
wikipedia  and enterprise content management  others  however  are spurious 
such as tooth enamel or transmission control protocol  since the process of
feature generation relies on the bag of words for matching concepts to the input text  it
suffers from the bow shortcomings we mentioned above  section       consequently 
some features are generated because the corresponding wikipedia articles just happen
to share words with the input text  even though these words are not characteristic
of the article as a whole  as explained above  our method can successfully operate
in the presence of such extraneous features due to the use of feature selection  this
way  generated features that are not informative for predicting document categories
are filtered out  and only informative features are actually retained for learning the
classification model 
using inter article links for generating additional features in section    we
presented an algorithm that generates additional features using inter article links as relations between concepts  in what follows  we show a series of text fragments  where for
each fragment we show  a  features generated with the regular fg algorithm   b  features
generated using wikipedia links  and  c  more general features generated using links  as
we can see from the examples  the features constructed using the links are often relevant to
the input text 
 text  google search
regular feature generation      search engine      google video      google 
    google  search       google maps      google desktop      google  verb  
    google news      search engine optimization       spamdexing  search engine
spamming 
features generated using links      pagerank      adwords      adsense     
gmail      google platform      website      sergey brin      google bomb     
msn search       nigritude ultramarine  a meaningless phrase used in a search
engine optimization contest in      
more general features only      website      mozilla firefox      portable
document format      algorithm      world wide web
 text  programming tools
regular feature generation      tool      programming tool      computer
software      integrated development environment      computer aided software engineering      macromedia flash      borland      game programmer 
    c programming language       performance analysis
features generated using links      compiler      debugger      source code 
    software engineering      microsoft      revision control      scripting
language      gnu      make       linux
more general features only      microsoft      software engineering     
linux      compiler      gnu
   

figabrilovich   markovitch

      the effect of feature generation
table   shows the results of using wikipedia based feature generation  with significant
improvements  p         shown in bold  the different rows of the table correspond to
the performance on different datasets and their subsets  as defined in section      we
consistently observed larger improvements in macro averaged bep  which is dominated by
categorization effectiveness on small categories  this goes in line with our expectations
that the contribution of encyclopedic knowledge should be especially prominent for categories with few training examples  categorization performance was improved for virtually
all datasets  with notable improvements of up to       for rcv  and     for ohsumed 
using the wilcoxon test  we found that the wikipedia based classifier is significantly superior to the baseline with p       in both micro  and macro averaged cases  these results
clearly demonstrate the advantage of knowledge based feature generation 
in our prior work  gabrilovich   markovitch            b   we have also performed
feature generation for text categorization using an alternative source of knowledge  namely 
the open directory project  odp   the results of using wikipedia are competitive with
those using odp  with a slight advantage of wikipedia  observe also that wikipedia is
constantly updated by numerous volunteers around the globe  while the odp is virtually
frozen nowadays  hence  in the future we can expect to obtain further improvements by
using newer versions of wikipedia 
the effect of knowledge breadth we also examined the effect of performing feature
generation using a newer wikipedia snapshot  as explained in section        appendix a
reports the results of this experiment  which show a small but consistent improvement due
to using a larger knowledge base 
      classifying short documents
we conjectured that wikipedia based feature generation should be particularly useful for
classifying short documents 
table   presents the results of this evaluation on the datasets defined in section     
in the majority of cases  feature generation yielded greater improvement on short documents than on regular documents  notably  the improvements are particularly high for
ohsumed  where pure experimentation on short documents is possible  see section      
according to the wilcoxon test  the wikipedia based classifier is significantly superior to
the baseline with p            these findings confirm our hypothesis that encyclopedic knowledge should be particularly useful when categorizing short documents  which are
inadequately represented by the standard bag of words 
      using inter article links as concept relations
using inter article links for generating additional features  we observed further improvements in text categorization performance on short documents  as we can see in table   
in the absolute majority of cases using links to generate more general features only is a
superior strategy  as we explain in section      inter article links can be viewed as relations
between concepts represented by the articles  consequently  using these links allows us to
   

fiwikipedia based semantic interpretation

dataset

baseline
micro macro
bep bep
reuters           cat              
reuters           cat              
rcv  industry   
           
rcv  industry   a
           
rcv  industry   b
           
rcv  industry   c
           
rcv  industry   d
           
rcv  industry   e
           
rcv  topic   
           
rcv  topic   a
           
rcv  topic   b
           
rcv  topic   c
           
rcv  topic   d
           
rcv  topic   e
           
ohsumed   a
           
ohsumed   b
           
ohsumed   c
           
ohsumed   d
           
ohsumed   e
           
  ng
     
movies
     

wikipedia
micro macro
bep bep
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
     
     

improvement
micro macro
bep
bep
           
           
           
            
           
           
           
           
            
            
           
           
           
           
            
           
           
           
           
     
     

table    the effect of feature generation for long documents

   

figabrilovich   markovitch

dataset

baseline
micro macro
bep bep
reuters           cat              
reuters           cat              
rcv  industry   
           
rcv  industry   a
           
rcv  industry   b
           
rcv  industry   c
           
rcv  industry   d
           
rcv  industry   e
           
rcv  topic   
           
rcv  topic   a
           
rcv  topic   b
           
rcv  topic   c
           
rcv  topic   d
           
rcv  topic   e
           
ohsumed   a
           
ohsumed   b
           
ohsumed   c
           
ohsumed   d
           
ohsumed   e
           
  ng
     

wikipedia
micro macro
bep bep
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
     

improvement
micro
macro
bep
bep
     
     
           
           
             
             
             
            
           
     
     
           
     
     
           
           
           
             
             
             
            
             
     

table    feature generation for short documents

   

fiwikipedia based semantic interpretation

dataset

baseline

micro
bep
reuters           cat        
reuters           cat        
rcv  industry   
     
rcv  topic   
     
  ng
     
dataset
reuters           cat  
reuters           cat  
rcv  industry   
rcv  topic   
  ng







macro
bep
     
     
     
     






wikipedia

wikipedia
  links

micro macro
bep bep
           
           
           
           
     
improvement
over baseline
           
           
           
           
     

micro macro
bep bep
           
           
           
           
     
improvement
over baseline
           
           
            
           
     

wikipedia
  links
 more general
features only 
micro macro
bep
bep
           
           
           
           
     
improvement
over baseline
           
           
            
           
     

table    feature generation for short documents using inter article links
identify additional concepts related to the context being analyzed  which leads to better
representation of the context with additional relevant generated features 

   related work
this section puts our methodology in the context of related prior work 
in the past  there have been a number of attempts to represent the meaning of natural
language texts  early research in computational linguistics focused on deep natural language
understanding  and strived to represent text semantics using logical formulae  montague 
       however  this task proved to be very difficult and little progress has been made to
develop comprehensive grammars for non trivial fragments of the language  consequently 
the mainstream research effectively switched to more statistically based methods  manning
  schuetze        
although few of these studies tried to explicitly define semantic representation  their
modus operandi frequently induces a particular representation system  distributional similarity methods  lee        compute the similarity of a pair of words w  and w  by comparing
the distributions of other words given these two  e g   by comparing vectors of probabilities p  v w    and p  v w    for a large vocabulary v of words  v  v    therefore  these
techniques can be seen as representing the meaning of a word w as a vector of conditional
probabilities of other words given w  dagan  marcus  and markovitch        refined this
technique by considering co occurrence probabilities of a word with its left and right contextual neighbors  for example  the word water would be represented by a vector of its
left neighbors such as drink  pour  and clean  and the vector of right neighbors such
as molecule  level  and surface  lin      a  represented word meaning by considering syntactic roles of other words that co occur with it in a sentence  for example  the
   

figabrilovich   markovitch

semantics of the word water would be represented by a vector of triples such as  water 
obj of  drink  and  water  adj mod  clean   qiu and frei        proposed a method
for concept based query expansion  however  they expanded queries with additional words
rather than with features corresponding to semantic concepts 
latent semantic analysis is probably the most similar method in prior research  as it
does explicitly represents the meaning of a text fragment  lsa does so by manipulating
a vector of so called latent concepts  which are obtained through svd decomposition of a
word by document matrix of the training corpus  cyc  lenat        lenat  guha  pittman 
pratt    shepherd        represents semantics of words through an elaborate network of
interconnected and richly annotated concepts 
in contrast  our method represents the meaning of a piece of text as a weighted vector
of knowledge concepts  importantly  entries of this vector correspond to unambiguous
human defined concepts rather than plain words  which are often ambiguous  compared to
lsa  our approach benefits from large amounts of manually encoded human knowledge  as
opposed to defining concepts using statistical analysis of a training corpus  compared to
cyc  our approach streamlines the process of semantic interpretation that does not depend
on manual encoding of inference rules  with the exception of lsa  most prior approaches
to semantic interpretation explicitly represent semantics of individual words  and require an
extra level of sophistication to represent longer texts  conversely  our approach represents
the meaning of texts in a uniform way regardless of their length 
    semantic similarity and semantic relatedness
in this study we deal with semantic relatedness rather than semantic similarity or
semantic distance  which are also often used in the literature  in their extensive survey of
relatedness measures  budanitsky and hirst        argued that the notion of relatedness is
more general than that of similarity  as the former subsumes many different kind of specific
relations  including meronymy  antonymy  functional association  and others  they further
maintained that computational linguistics applications often require measures of relatedness
rather than the more narrowly defined measures of similarity  for example  word sense
disambiguation can use any related words from the context  and not merely similar words 
budanitsky and hirst        also argued that the notion of semantic distance might be
confusing due to the different ways it has been used in the literature 
our approach to estimating semantic relatedness of words is somewhat reminiscent of
distributional  or co occurrence  similarity  lee        dagan  lee    pereira         indeed  we compare the meanings of words by comparing the occurrence patterns across a
large collection of natural language documents  however  the compilation of these documents is not arbitrary  rather  the documents are aligned with encyclopedia articles  while
each of them is focused on a single topic  furthermore  distributional similarity methods
are inherently suitable for comparing individual words  while our method can compute
similarity of arbitrarily long texts 
prior work in the field mostly focused on semantic similarity of words  using r g
 rubenstein   goodenough        list of    word pairs and m c  miller   charles       
list of    word pairs  when only the similarity relation is considered  using lexical resources
was often successful enough  reaching the pearsons correlation of          with human
   

fiwikipedia based semantic interpretation

judgements  budanitsky   hirst        jarmasz         in this case  lexical techniques
even have a slight edge over esa wikipedia  whose correlation with human scores is      
on m c and       on r g     however  when the entire language wealth is considered
in an attempt to capture more general semantic relatedness  lexical techniques yield substantially inferior results  see table     wordnet based technique  which only consider the
generalization  is a  relation between words  achieve correlation of only          with
human judgements  budanitsky   hirst        jarmasz         jarmasz   szpakowiczs
elkb system  jarmasz        based on rogets thesaurus  roget        achieves a higher
correlation of      due to its use of a richer set if relations 
studying semantic similarity and relatedness of words is related to assessing similarity
of relations  an example of this task is to establish that word pairs carpenter wood and
mason stone are relationally similar  as the words in both pairs stand in the same relation
 profession material   state of the art results on relational similarity are based on latent
relational analysis  turney              
sahami and heilman        proposed to use the web as a source of additional knowledge
for measuring similarity of short text snippets  to this end  they defined a kernel function
that sends two snippets as queries to a search engine  and compares the bags of words for
the two sets of returned documents  a major limitation of this technique is that it is only
applicable to short texts  because sending a long text as a query to a search engine is likely
to return few or even no results at all  on the other hand  our approach is applicable to
text fragments of arbitrary length  additional studies that explored the web to gather
information for computing word similarity include  turney        and  metzler  dumais   
meek         the main difference between these works and our method is that the latter
uses a structured representation of human knowledge defined by wikipedia concepts 
the above mentioned based techniques are inherently limited to individual words  and
their adaptation for comparing longer texts requires an extra level of complexity  mihalcea
et al          in contrast  our method treats both words and texts in essentially the same
way 
strube and ponzetto        also used wikipedia for computing semantic relatedness 
however  their method  called wikirelate   is radically different from ours  given a pair of
words w  and w    wikirelate  searches for wikipedia articles  p  and p    that respectively
contain w  and w  in their titles  semantic relatedness is then computed based on various
distance measures between p  and p    these measures either rely on the texts of the
pages  or path distances within the category hierarchy of wikipedia  our approach  on the
other hand  represents each word as a weighted vector of wikipedia concepts  semantic
relatedness is then computed by comparing the two concept vectors 
thus  the differences between the two approaches are 
   wikirelate  can only process words that actually occur in titles of wikipedia articles 
esa only requires that the word appears within the text of wikipedia articles 
   wikirelate  is limited to single words while esa can compare texts of any length 
    wikirelate   strube   ponzetto        achieved relatively low scores of          on these domains 

   

figabrilovich   markovitch

   wikirelate  represents the semantics of a word by either the text of the article
associated with it  or by the node in the category hierarchy  esa has a much more
structured semantic representation consisting of a vector of wikipedia concepts 
indeed  as we have shown in section      the richer representation of esa yields much better
results 
    feature generation for text categorization
to date  quite a few attempts have been made to deviate from the orthodox bag of words
paradigm  usually with limited success  in particular  representations based on phrases
 lewis        dumais et al         fuernkranz  mitchell    riloff         named entities
 kumaran   allan         and term clustering  lewis   croft        bekkerman       
have been explored  however  none of these techniques could possibly overcome the problem
underlying the various examples we reviewed in this paperlack of world knowledge 
feature generation techniques were found useful in a variety of machine learning tasks
 markovitch   rosenstein        fawcett        matheus         these techniques search
for new features that describe the target concept better than the ones supplied with the
training instances  a number of proposed feature generation algorithms  pagallo   haussler        matheus   rendell        hu   kibler        murphy   pazzani        hirsh
  japkowicz        led to significant improvements in performance over a range of classification tasks  however  even though feature generation is an established research area in
machine learning  only a few works have applied it to text processing  kudenko   hirsh 
      mikheev        cohen        scott        scott   matwin         in contrast to our
approach  these techniques did not use any exogenous knowledge 
in our prior work  gabrilovich   markovitch            b   we assumed the external
knowledge is available in the form of a generalization hierarchy  and used the open directory
project as an example  this method  however  had a number of drawbacks  which can be
corrected by using wikipedia 
first  requiring the knowledge repository to define an is a hierarchy limits the choice of
appropriate repositories  moreover  hierarchical organization embodies only one particular
relation between the nodes  generalization   while numerous other relations  such as relatedness  meronymy holonymy and chronology  are ignored  second  large scale hierarchies
tend to be extremely unbalanced  so that the relative size of some branches is disproportionately large or small due to peculiar views of the editors  such phenomena are indeed
common in the odp  for example  the top society branch is heavily dominated by one
of its childrenreligion and spirituality  the top science branch is dominated by its
biology child  a considerable fraction of the mass of top recreation is concentrated in
pets  finally  to learn the scope of every odp concept  short textual descriptions of the
concepts were augmented by crawling the web sites cataloged in the odp  this procedure
allowed us to accumulate many gigabytes worth of textual data  but at a price  as texts
obtained from the web are often quite far from formal writing and plagued with noise 
crawling a typical web site often brings auxiliary material that has little to do with the
site theme  such as legal disclaimers  privacy statements  and help pages 
in this paper we proposed to use world knowledge encoded in wikipedia  which is arguably the largest knowledge repository on the web  compared to the odp  wikipedia
   

fiwikipedia based semantic interpretation

possesses several advantageous properties  first  its articles are much cleaner than typical
web pages  and mostly qualify as standard written english  although wikipedia offers
several orthogonal browsing interfaces  their structure is fairly shallow  and we propose to
treat wikipedia as having essentially no hierarchy  this way  mapping tex fragments onto
relevant wikipedia concepts yields truly multi faceted classification of the text  and avoids
the problem of unbalanced hierarchy branches  moreover  by not requiring the knowledge
repository to be hierarchically organized  our approach is suitable for new domains  for
which no ontology is available  finally  wikipedia articles are heavily cross linked  in a way
reminiscent of linking on the web  we conjectured that these links encode many interesting relations between the concepts  and constitute an important source of information in
addition to the article texts  we explored using inter article links in section       
      feature generation using electronic dictionaries
several studies performed feature construction using the wordnet electronic dictionary
 fellbaum        and other domain specific dictionaries  scott        scott   matwin 
      urena lopez  buenaga    gomez        wang  mckay  abbass    barlow       
bloehdorn   hotho        
scott and matwin        attempted to augment the conventional bag of words representation with additional features  using the symbolic classification system ripper  cohen 
       this study evaluated features based on syntactically   and statistically motivated
phrases  as well as on wordnet synsets      in the latter case  the system performed generalizations using the hypernym hierarchy of wordnet  and completely replaced a bag of words
with a bag of synsets  while using hypernyms allowed ripper to produce more general
and more comprehensible rules and achieved some performance gains on small classification tasks  no performance benefits could be obtained for larger tasks  which even suffered
from some degradation in classification accuracy  consistent with other published findings
 lewis        dumais et al         fuernkranz et al          the phrase based representation
also did not yield any significant performance benefits over the bag of words approach   
urena lopez et al         used wordnet in conjunction with rocchio  rocchio       
and widrow hoff  lewis  schapire  callan    papka        widrow   stearns        chapter    linear classifiers to fine tune the category vectors  wang et al         used medical
subject headings  mesh        to replace the bag of words with canonical medical terms 
bloehdorn and hotho        used a similar approach to augment reuters       documents
with wordnet synsets and ohsumed medical documents with mesh terms 
it should be noted  however  that wordnet was not originally designed to be a powerful
knowledge base  but rather a lexical database more suitable for peculiar lexicographers
needs  specifically  wordnet has the following drawbacks when used as a knowledge base
for text categorization 
    identification of syntactic phrases was performed using a noun phrase extractor built on top of a part of
speech tagger  brill        
    a synset is wordnet notion for a sense shared by a group of synonymous words 
    sebastiani        casts the use of bag of words versus phrases as utilizing lexical semantics rather than
compositional semantics  interestingly  some bag of words approaches  notably  knn  may be considered
context sensitive as they do not assume independence between either features  terms  or categories  yang
  pedersen        

   

figabrilovich   markovitch

 wordnet has a fairly small coveragefor the test collections we used in this paper  up
to     of their unique words are missing from wordnet  in particular  many proper
names  slang and domain specific technical terms are not included in wordnet  which
was designed as a general purpose dictionary 
 additional information about synsets  beyond their identity  is very limited  this is
because wordnet implements a differential rather than constructive lexical semantics
theory  so that glosses that accompany the synsets are mainly designed to distinguish
the synsets rather than provide a definition of the sense or concept  usage examples
that occasionally constitute part of the gloss serve the same purpose  without such
auxiliary information  reliable word sense disambiguation is almost impossible 
 wordnet was designed by professional linguists who are trained to recognize minute
differences in word senses  as a result  common words have far too many distinct
senses to be useful in information retrieval  mihalcea         for example  the word
make has as many as    senses as a verb alone  such fine grained distinctions
between synsets present an additional difficulty for word sense disambiguation 
both our approach and the techniques that use wordnet manipulate a collection of
concepts  however  there are a number of crucial differences  all previous studies only
performed feature generation for individual words only  our approach can handle arbitrarily long or short text fragments alike  considering words in context allows our approach
to perform word sense disambiguation  approaches using wordnet cannot achieve disambiguation because information about synsets is limited to merely a few words  while in
wikipedia concepts are associated with huge amounts of text  even for individual words 
our approach provides much more sophisticated mapping of words to concepts  through the
analysis of the large bodies of texts associated with concepts  this allows us to represent the
meaning of words  or texts  as a weighted combination of concepts  while mapping a word
in wordnet amounts to simple lookup  without any weights  furthermore  in wordnet
the senses of each word are mutually exclusive  in our approach  concepts reflect different
aspects of the input  thus yielding weighted multi faceted representation of the text 
in appendix d we illustrate the limitations of wordnet on a specific example  where
we juxtapose wordnet based and wikipedia based representation 
      using unlabeled examples
to the best of our knowledge  with the exception of the above studies that used wordnet 
there have been no attempts to date to automatically use large scale repositories of structured background knowledge for feature generation  an interesting approach to using nonstructured background knowledge was proposed by zelikovitz and hirsh         this work
uses a collection of unlabeled examples as intermediaries in comparing testing examples
with the training ones  specifically  when an unknown test instance does not appear to
resemble any labeled training instances  unlabeled examples that are similar to both may
be used as bridges  using this approach  it is possible to handle the situation where the
training and the test document have few or no words in common  the unlabeled documents
are utilized to define a cosine similarity metric  which is then used by the knn algorithm
for actual text categorization  this approach  however  suffers from efficiency problems  as
   

fiwikipedia based semantic interpretation

looking for intermediaries to compare every two documents makes it necessary to explore a
combinatorial search space 
in a subsequent paper  zelikovitz and hirsh        proposed an alternative way to use
unlabeled documents as background knowledge  in this work  unlabeled texts are pooled
together with the training documents to compute a latent semantic analysis  lsa   deerwester et al         model  lsa analyzes a large corpus of unlabeled text  and automatically
identifies so called latent concepts using singular value decomposition  the resulting
lsa metric then facilitates comparison of test documents to training documents  the addition of unlabeled documents significantly increases the amount of data on which word
co occurrence statistics is estimated  thus providing a solution to text categorization problems where training data is particularly scarce  however  subsequent studies found that
lsa can rarely improve the strong baseline established by svm  and often even results in
performance degradation  wu   gunopulos        liu  chen  zhang  ma    wu        
in contrast to lsa  which manipulates virtual concepts  our methodology relies on using
concepts identified and described by humans 

   conclusions
in this paper we proposed explicit semantic analysisa semantic interpretation methodology for natural language processing  in order to render computers with knowledge about
the world  we use wikipedia to build a semantic interpreter  which represents the meaning
of texts in a very high dimensional space of knowledge based concepts  these concepts correspond to wikipedia articles  and our methodology provides a fully automatic way to tap
into the collective knowledge of tens and hundreds of thousands of people  the conceptbased representation of text contains information that cannot be deduced from the input
text alone  and consequently supersedes the conventional bag of words representation 
we believe the most important aspects of the proposed approach are its ability to
address synonymy and polysemy  which are arguably the two most important problems
in nlp  thus  the two texts can discuss the same topic using different words  and the
conventional bag of words approach will not be able to identify this commonality  on the
other hand  the mere fact that the two texts contain the same word does not necessarily
imply that they discuss the same topic  since that word could be used in the two texts in two
different meanings  we believe that our concept based representation allows generalizations
and refinements to partially address synonymy and polysemy 
consider  for example  the following text fragment  taken from appendix c   a group
of european led astronomers has made a photograph of what appears to be a planet orbiting
another star  if so  it would be the first confirmed picture of a world beyond our solar
system  the fifth concept generated for this fragment is extrasolar planet  which is
exactly the topic of this text  even though these words are not mentioned in the input 
the other generated concepts  e g   astronomy and planetary orbit  are also highly
characteristic of astronomy related texts  such additions enrich the text representation  and
increase the chances of finding common features between texts  it is also essential to note
that  of course  not all the generated concepts need to match features of other documents 
even if some of the concepts match  we gain valuable insights about the document contents 
   

figabrilovich   markovitch

we succeeded to make automatic use of an encyclopedia without deep language understanding  specially crafted inference rules or relying on additional common sense knowledge
bases  this was made possible by applying standard text classification techniques to match
document texts with relevant wikipedia articles 
empirical evaluation confirmed the value of explicit semantic analysis for two common tasks in natural language processing  compared with the previous state of the art 
using esa results in significant improvements in automatically assessing semantic relatedness of words and texts  specifically  the correlation of computed relatedness scores with
human judgements increased from r        to       spearman  for individual words and
from r        to       pearson  for texts  in contrast to existing methods  esa offers a
uniform way for computing relatedness of both individual words and arbitrarily long text
fragments  using esa to perform feature generation for text categorization yielded consistent improvements across a diverse range of datasets  recently  the performance of the
best text categorization systems became similar  and previous work mostly achieved small
improvements  using wikipedia as a source of external knowledge allowed us to improve
the performance of text categorization across a diverse collection of datasets 
it should be noted that although a recent study  giles        found wikipedia accuracy to rival that of encyclopaedia britannica  arguably not all the wikipedia articles are
of equally high quality  on the one hand  wikipedia has the notion of featured articles
 http   en wikipedia org wiki featured article   which are considered to be the
best articles in wikipedia  as determined by wikipedias editors  currently  fewer than
     of articles achieve this status  on the other hand  many articles are incomplete  socalled stubs   or might even contain information that is incorrect or that does not represent
a consensus among the editors  yet in other cases  wikipedia content might be prone to
spamming  despite the editorial process that attempts to review recent changes  we believe
our method is not overly susceptible to such cases  as long as the majority of the content
is correct  arguably  except for outright vandalism  most spamming would likely modify
articles to contain information that is related to the topic of the article  but not important
or not essential for the majority of readers  as long as this newly added content remains
relevant to the gist of the article  our method will likely be able to correctly determine those
input texts that the article is relevant for  however  a proper evaluation of the robustness
of our method in the presence of imperfect content is beyond the scope of this article 
we believe that this research constitutes a step towards enriching natural language
processing with humans knowledge about the world  we hope that explicit semantic
analysis will also be useful for other nlp tasks beyond computing semantic relatedness
and text categorization  and we intend to investigate this in our future work  recently  we
have used esa to improve the performance of conventional information retrieval  egozi 
gabrilovich    markovitch         in that work  we augmented both queries and documents
with generated features  such that documents were indexed in the augmented space of words
and concepts  potthast  stein  and anderka        and sorg and cimiano        adapted
esa for multi lingual and cross lingual information retrieval 
in another recent study  gurevych et al         applied our methodology to computing
word similarity in german  and also to an information retrieval task that searched job
descriptions given a users description of her career interests  and found our method superior
to a wordnet based approach  importantly  this study also confirms that our method
   

fiwikipedia based semantic interpretation

can be easily adapted to languages other than english  by using the version of wikipedia
corresponding to the desired target language 
in our future work  we also intend to apply esa to word sense disambiguation  current
approaches to word sense disambiguation represent contexts that contain ambiguous words
using the bag of words augmented with part of speech information  we believe representation of such contexts can be greatly improved if we use feature generation to map such
contexts into relevant knowledge concepts  anecdotal evidence  such as the examples presented in section        implies our method has promise for improving the state of the art in
word sense disambiguation  in this work we capitalized on inter article links of wikipedia
in several ways  and in our future work we intend to investigate more elaborate techniques
for leveraging the high degree of cross linking between wikipedia articles 
the wiki technology underlying the wikipedia project is often used nowadays in a variety of open editing initiatives  these include corporate intranets that use wiki as a primary
documentation tool  as well as numerous domain specific encyclopedias on topics ranging
from mathematics to orthodox christianity    therefore  we believe our methodology can
also be used for augmenting document representation in many specialized domains  it is
also essential to note that wikipedia is available in numerous languages  while different
language versions are cross linked at the level of concepts  we believe this information
can be leveraged to use wikipedia based semantic interpretation for improving machine
translation 
this work proposes a methodology for explicit semantic analysis using wikipedia 
however  esa can also be implemented using other repositories of human knowledge that
satisfy the requirements listed in section      in section     we reported the results of
building an esa based semantic interpreter using the open directory project  gabrilovich
  markovitch            b   zesch  mueller  and gurevych        proposed to use wiktionary for computing semantic relatedness  in our future work  we intend to implement
esa using additional knowledge repositories 
finally  for readers interested in using wikipedia in their own work  the main software
deliverable of the described work is the wikipedia preprocessor  wikiprep   available online
as part of the sourceforge open source project at http   wikiprep sourceforge net 

acknowledgments
we thank michael d  lee and brandon pincombe for making available their document similarity data  we also thank deepak agarwal for advice on assessing statistical significance
of results in computing semantic relatedness  this work was partially supported by funding
from the ec sponsored muscle network of excellence 
the first authors current address is yahoo  research       mission college blvd  santa
clara  ca        usa 

    see http   en wikipedia org wiki category online encyclopedias for a longer list of examples 

   

figabrilovich   markovitch

appendix a  the effect of knowledge breadth in text categorization
in this appendix  we examine the effect of performing feature generation using a newer
wikipedia snapshot  as defined in section        as we can see from table    using the
larger amount of knowledge leads on average to greater improvements in text categorization performance  although the difference between the performance of the two versions is
admittedly small  it is consistent across datasets  a similar situation happens when assessing
the role of external knowledge for computing semantic relatedness  see section        

dataset

baseline

micro
bep
reuters           cat        
reuters           cat        
rcv  industry   
     
rcv  industry   a      
rcv  industry   b      
rcv  industry   c      
rcv  industry   d      
rcv  industry   e      
rcv  topic   
     
rcv  topic   a
     
rcv  topic   b
     
rcv  topic   c
     
rcv  topic   d
     
rcv  topic   e
     
ohsumed   a
     
ohsumed   b
     
ohsumed   c
     
ohsumed   d
     
ohsumed   e
     
  ng
     
movies
     
average

macro
bep
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

wikipedia
          
micro macro
bep bep
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
     
     

improvement
          
micro macro
bep
bep
           
           
           
            
           
           
           
           
            
            
           
           
           
           
            
           
           
           
           
     
     
             

improvement
          
micro macro
bep
bep
           
           
           
            
           
           
           
           
            
            
           
           
           
           
            
           
           
           
           
     
     
             

table    the effect of feature generation using a newer wikipedia snapshot  dated
march          

   

fiwikipedia based semantic interpretation

appendix b  test collections for text categorization
this appendix provides detailed description of the test collections we used to evaluate
knowledge based feature generation for text categorization 
b   reuters      
this data set contains one year worth of english language stories distributed over the
reuters newswire in           and is arguably the most often used test collection in
text categorization research  reuters       is a cleaned version of the earlier release named
reuters        which contained errors and duplicate documents 
the collection contains       documents  hence the name  in sgml format  of those 
      documents are categorized  i e   assigned a category label or marked as not belonging
to any category  other documents do not have an explicit classification  that is  they can
reasonably belong to some categories  judged by their content   but are not marked so  several train test splits of the collection has been defined  of which modapte  modified apte 
is the most commonly used one  the modapte split divides the collection chronologically 
and allocates the first      documents for training  and the rest      documents for testing 
the documents are labeled with     categories  there are     labels per document  with
the average of       the category distribution is extremely skewed  the largest category
 earn  has      positive examples  while    categories have only one positive example 
several category sets were defined for this collection 
    largest categories  earn  acq  money fx  grain  crude  trade  interest  ship  wheat  corn  
    categories with at least one document in the training set and one in the testing set
 yang        
 galavotti  sebastiani  and simi        used a set of     categories with at least one
training example  three categories  cottonseed  f cattle and sfr have no training
examples under the modapte split  
 the full set of     categories with at least one positive example either in the training
or in the testing set 
following common practice  we used the modapte split and two category sets     largest
categories and    categories with at least one training and testing example 
b      newsgroups    ng 
the    newsgroups collection  lang        is comprised of       postings to    usenet
newsgroups  most documents have a single label  defined as the name of the newsgroup
it was sent to  about    of documents have been cross posted  and hence have several
labels  each newsgroup contains exactly      positive examples  with the exception of
soc religion christian which contains     
some categories are quite close in scope  for example  comp sys ibm pc hardware and
comp sys mac hardware  or talk religion misc and soc religion christian  a document
   

figabrilovich   markovitch

posted to a single newsgroup may be reasonably considered appropriate for other groups too
 the author may have simply not known of other similar groups  and thus not cross posted
the message   this naturally poses additional difficulty for classification 
it should be noted that internet news postings are very informal  and therefore the documents frequently contain non standard and abbreviated words  foreign words  and proper
names  as well as a large amount of markup characters  used for attribution of authorship
or for message separation  
b   movie reviews
the movie reviews collection  pang et al         presents an example of sentiment classification  which is different from standard  topical  text categorization  the collection
contains      reviews of movies  half of which express positive sentiment  opinion  about
the movie  and half negative  the reviews were collected from the rec arts movies reviews
newsgroup  archived at the internet movie database  imdb  http   www imdb com   the
classification problem in this case is to determine the semantic orientation of the document  rather than to relate its content to one of the predefined topics  this problem is
arguably more difficult than topical text categorization  since the notion of semantic orientation is quite general  we saw this collection as an opportunity to apply feature generation
techniques to this new task 
recent works on semantic orientation include  turney   littman        turney       
pang et al            the two former studies used unsupervised learning techniques based
on latent semantic indexing  estimating semantic distance between a given document and
two reference words that represent polar opinions  namely  excellent and poor  the
latter work used classical tc techniques 
b   reuters corpus version    rcv  
rcv  is the newest corpus released by reuters  lewis et al         rose  stevenson   
whitehead         it is considerably larger than its predecessor  and contains over        
news items  dated between august          and august           the stories are labeled
with   category sets  topics  industries and regions 
 topics are most close in nature to the category set of the old reuters collection
 reuters         there are     topic codes  with      categories per document on
the average  the topics are organized in a hierarchy  and the hierarchy policy required that if a category is assigned to a document  all its ancestors in the hierarchy
should be assigned as well  as a result  as many as     of all topic assignments
    the field of genre classification  which attempts to establish the genre of document  is somewhat related
to sentiment classification  examples of possible genres are radio news transcripts and classified advertisements  the work by dewdney  vaness dykema  and macmillan        cast this problem as text
categorization  using presentation features in addition to words  their presentation features included
part of speech tags and verb tenses  as well as mean and variance statistics of sentence and word length 
punctuation usage  and the amount of whitespace characters  using support vector machines for actual
classification  the authors found that the performance due to the presentation features alone was at least
as good as that achieved with plain words  and that the combined feature set usually resulted in an
improvement of several percentage points 

   

fiwikipedia based semantic interpretation

are due to the four most general categories  ccat  ecat  gcat  and mcat  consequently  the micro averaged performance scores are dominated by these categories
 lewis et al          and macro averaging becomes of interest    the minimum code
policy required that each document was assigned at least one topic and one region
code 
 industries are more fine grained than topics  and are therefore harder for classification  these categories are also organized in a hierarchy  although the hierarchy
policy was only partially enforced for them  there are         documents labeled
with industry codes 
 region codes correspond to geographical places  and are further subdivided into countries  regional groupings and economic groupings  lewis et al         argue that
region codes might be more suitable for named entity recognition than for text categorization 
in our experiments we used topic and industry categories  due to the sheer size of the
collection  processing all the categories in each set would be unreasonably long  allowing to
conduct only few experiments  to speed up experimentation  we used a subset of the corpus
with        training documents  dated august             and      testing documents
 dated august              following the scheme introduced by brank et al          we
used    topic and    industry categories  which constitute a representative sample of the
full groups of     and     categories  respectively  we also randomly sampled the topic
and industry categories into   sets of    categories each  table   gives the full definition of
the category sets we used 
as noted by lewis et al          the original rcv  distribution contains a number of
errors  in particular  there are documents that do not conform to either minimum code or
hierarchy policy  or labeled with erratic codes  lewis et al         proposed a procedure
to correct these errors  and defined a new version of the collection  named rcv  v   as
opposed to the original distribution  referred to as rcv  v     all our experiments are
based on rcv  v  
b   ohsumed
ohsumed  hersh et al         is a subset of the medline database  which contains
        references to documents published in medical journals over the period of          
each reference contains the publication title  and about two thirds           also contain
an abstract  each document is labeled with several mesh categories  mesh         there
are over        distinct categories in the collection  with an average of    categories per
document  ohsumed is frequently used in information retrieval and text categorization
research 
following joachims         we used a subset of documents from      that have abstracts 
taking the first        documents for training and the next        for testing  to limit the
number of categories for the experiments  we randomly generated   sets of    categories
each  table   gives the full definition of the category sets we used 
    this is why micro averaged scores for topic codes are so much higher than macro averaged ones  see
section       

   

figabrilovich   markovitch

set name
topic   
topic   a
topic   b
topic   c
topic   d
topic   e
industry   

industry   a
industry   b
industry   c
industry   d
industry   e

categories comprising the set
e     gobit  e     c     e     godd  ghea  e    c     m    
gspo  c    e    gpol  m    c  
e    c    c     c     c    m    ecat  c    c     c  
m     c     g     gwea  grel  c     e     c    e     c  
c    c    gtour  c     g     gdef  e    genv  e     c  
c    c     e    gdis  c    c     gpro  c    g    c  
c     e     e    ghea  c     gdip  m     gcrim  e    gvio
i       i       i       i       i       i       i         i      
i         i         i         i       i         i         i        
i       
i       i         i         i       i       i       i       i      
i       i     
i       i       i       i       i         i       i         i      
i       i       
i       i       i       i       i       i       i       i      
i       i     
i         i       i       i         i       i       i       i      
i       i     
i       i       i       i       i         i       i       i      
i       i     

table    definition of rcv  category sets used in the experiments

appendix c  additional examples of feature generation for text
categorization
in this appendix  we list a number of additional feature generation examples 
 text  the development of t cell leukaemia following the otherwise successful treatment of three patients with x linked severe combined immune deficiency  x scid 
in gene therapy trials using haematopoietic stem cells has led to a re evaluation of
this approach  using a mouse model for gene therapy of x scid  we find that the
corrective therapeutic gene il rg itself can act as a contributor to the genesis of
t cell lymphomas  with one third of animals being affected  gene therapy trials for
x scid  which have been based on the assumption that il rg is minimally oncogenic 
may therefore pose some risk to patients 
top    generated features      leukemia      severe combined immunodeficiency      cancer      non hodgkin lymphoma      aids      icd    chapter
ii  neoplasms  chapter iii  diseases of the blood and blood forming organs 
and certain disorders involving the immune mechanism      bone marrow transplant      immunosuppressive drug      acute lymphoblastic leukemia       multiple sclerosis

selected explanations      a particular cancer type      a disease code of the icd
international statistical classification of diseases and related health problems
 text  scientific methods in biology
   

fiwikipedia based semantic interpretation

set name
ohsumed   a

ohsumed   b

ohsumed   c

ohsumed   d

ohsumed   e

categories comprising the set
 parentheses contain mesh identifiers 
b lymphocytes  d        
metabolism  inborn errors  d        
creatinine  d         hypersensitivity  d        
bone diseases  metabolic  d         fungi  d        
new england  d         biliary tract  d        
forecasting  d         radiation  d       
thymus gland  d         insurance  d        
historical geographic locations  d        
leukocytes  d         hemodynamics  d        
depression  d         clinical competence  d        
anti inflammatory agents  non steroidal  d        
cytophotometry  d         hydroxy acids  d       
endothelium  vascular  d        
contraceptives  oral  hormonal  d        
acquired immunodeficiency syndrome  d        
gram positive bacteria  d         diarrhea  d        
embolism and thrombosis  d        
health behavior  d         molecular probes  d        
bone diseases  developmental  d        
referral and consultation  d       
antineoplastic and immunosuppressive agents  d        
receptors  antigen  t cell  d        
government  d         arthritis  rheumatoid  d        
animal structures  d         bandages  d        
italy  d         investigative techniques  d        
physical sciences  d         anthropology  d       
htlv blv infections  d        
hemoglobinopathies  d         vulvar diseases  d        
polycyclic hydrocarbons  aromatic  d        
age factors  d         philosophy  medical  d        
antigens  cd   d        
computing methodologies  d        
islets of langerhans  d         regeneration  d       

table    definition of ohsumed category sets used in the experiments

   

figabrilovich   markovitch

top    generated features      biology      scientific classification      science      chemical biology      binomial nomenclature      nature  journal  
    social sciences      philosophy of biology      scientist       history of
biology
selected explanations      the formal method of naming species in biology
 text  with quavering voices  parents and grandparents of those killed at the world
trade center read the names of the victims in a solemn recitation today  marking the
third anniversary of the terror attacks  the ceremony is one of many planned in the
united states and around the world to honor the memory of the nearly       victims
of      
top    generated features      september          attack memorials and services      united airlines flight         aftermath of the september         
attacks      world trade center      september          attacks      oklahoma city bombing      world trade center bombing      arlington national
cemetery      world trade center site       jewish bereavement
selected explanations      one of the four flights hijacked on september          
    a terrorist attack in oklahoma city in           american military cemetery
 text  u s  intelligence cannot say conclusively that saddam hussein has weapons
of mass destruction  an information gap that is complicating white house efforts
to build support for an attack on saddams iraqi regime  the cia has advised top
administration officials to assume that iraq has some weapons of mass destruction 
but the agency has not given president bush a smoking gun  according to u s 
intelligence and administration officials 
top    generated features      iraq disarmament crisis      yellowcake forgery      senate report of pre war intelligence on iraq      iraq and weapons
of mass destruction      iraq survey group      september dossier      iraq
war      scott ritter      iraq war rationale       operation desert fox
selected explanations      falsified intelligence documents about iraqs alleged
attempt to purchase yellowcake uranium      a paper on iraqs weapons of mass
destruction published by the uk government in           un weapons inspector in
iraq       us and uk joint military campaign in iraq in     
 as another example  consider a pair of contexts that contain the word jaguar  the
first one contains this ambiguous word in the sense of a car model  and the second
onein the sense of an animal 
 text  jaguar car models
top    generated features      jaguar  car       jaguar  s type      
jaguar x type      jaguar e type      jaguar xj      daimler motor company      british leyland motor corporation      luxury vehicles      v 
engine       jaguar racing
top    generated features                      particular jaguar car models 
    a car manufacturing company that became a part of jaguar in          
   

fiwikipedia based semantic interpretation

another vehicle manufacturing company that merged with jaguar      an internal
combustion engine used in some jaguar car models       a formula one team
used by jaguar to promote its brand name
 text  jaguar  panthera onca 
top    generated features      jaguar      felidae      black panther 
    leopard      puma      tiger      panthera hybrid      cave lion     
american lion       kinkajou
top    generated features      a family that include lions  tigers  jaguars 
and other related feline species       another carnivore mammal
we also show here a number of examples for generating features using inter article links 
 text  artificial intelligence
regular feature generation      artificial intelligence      a i   film      
mit computer science and artificial intelligence laboratory      artificial
life      strong ai      swarm intelligence      computer science      frame
problem      cognitive science       carl hewitt
features generated using links      robot      john mccarthy  computer scientist       artificial consciousness      marvin minsky      planner programming language      actor model  a model of concurrent computation formulated
by carl hewitt and his colleagues       logic      scientific community metaphor 
    natural language processing       lisp programming language
more general features only      robot      massachusetts institute of technology      psychology      consciousness      lisp programming language
 text  a group of european led astronomers has made a photograph of what appears
to be a planet orbiting another star  if so  it would be the first confirmed picture of a
world beyond our solar system 
regular feature generation      planet      solar system      astronomy     
planetary orbit      extrasolar planet      pluto      jupiter      neptune     
minor planet       mars
features generated using links      asteroid      earth      oort cloud  a
postulated cloud of comets       comet      sun      saturn      moon      mercury
 planet       asteroid belt       orbital period
more general features only      earth      moon      asteroid      sun     
national aeronautics and space administration
 text  nearly    percent of americans say they are careful about what they eat  and
even more say diet is essential to good health  according to a new nationwide health
poll in which obesity ranked second among the biggest health concerns 
regular feature generation      veganism      vegetarianism      obesity     
atkins nutritional approach      binge eating disorder      dick gregory     
nutrition      super size me      health insurance       eating disorder
   

figabrilovich   markovitch

selected explanations      a philosophy of avoiding animal derived food      an
american nutritionist      a documentary film about an individual who eats only
mcdonalds fast food for one full month 
features generated using links      raw food diet      diabetes mellitus     
healthy eating      body mass index      omega   fatty acid  an important nutritional component       dieting      milk      united states         hypertension 
     egg  food 
more general features only      united states      diabetes mellitus      cancer      food      mcdonalds

appendix d  comparing knowledge sources for feature generation 
wikipedia versus wordnet
to demonstrate the shortcomings of wordnet as a source for knowledge based feature generation  we juxtapose wordnet based and wikipedia based feature generation for a sample
sentence examined in section        we repeat the wikipedia context classifications for
readers convenience  
we used wordnet version      in what follows  wordnet synsets are denoted with curly
braces  and noun and verb synsets are followed by their immediate hypernym  more general
synset   if applicable 
 text  wal mart supply chain goes real time
 most relevant wikipedia classifications 
 wal mart
 sam walton  wal mart founder
 sears holdings corporation  target corporation  albertsons  prominent competitors of wal mart
 rfid  radio frequency identification  a technology that wal mart uses very
extensively to manage its stock
 hypermarket  superstore  a general concept of which wal mart is a specific
example
 united food and commercial workers  a labor union that has been trying
to organize wal marts workers
 wordnet  
  wal mart         word not found in wordnet 
  supply    indefinite quantity    supply    economic process    provision 
providing  supply supplying    activity  
    this classification is quite interesting  as the issue discussed in the input text fragment is very characteristic of the american life style 

   

fiwikipedia based semantic interpretation

  supply provide  render  furnish    give    issue  supply    distribute  
 provide  supply ply  cater    give    add  append  supply    state  say 
tell  
  chain  concatenation    series    chain  chemical chain    unit  building
block    chain    ligament    chain    business  concern  business concern 
business organization    chain    restraint  constraint    chain    linear
unit    range  mountain range  range of mountains  chain  mountain chain 
chain of maountains    geological formation  geology  formation    iron  irons 
chain  chains    shackle  bond  hamper  trammel  trammels    chain  string 
strand    necklace  
  chain    fasten  fix  secure  
  go  spell  tour  turn    shift  work shift  duty period    crack  fling  go  pass 
whirl  offer    attempt  effort  endeavor  endeavour  try    go  go game  
 board game  
  travel  move  go  locomote    go  proceed  move    act  move       more
verb senses omitted for brevity 
  real number  real    real  spanish coin   
  real  existent    real  no less than what is stated     real  true    real  not to
be taken lightly     real  tangible    actual  genuine  literal  real    real  economics     substantial  real  material    real  of property     veridical  real  
 real  founded on practical matters   
  very  really  real  rattling  
  time  clip    case  instance  example    time    time period  period 
period of time  amount of time    time    moment  minute  second  instant    time    abstraction    clock time  time    reading  meter reading    fourth dimension  time    dimension    time    experience    meter 
time    rhythmicity    prison term  sentence  time    term  
  clock  time    quantify  measure    time    schedule    time  
 determine  shape  influence  regulate    time    adjust  set  
evidently  wordnet classifications are overly general and diverse because context words
cannot be properly disambiguated  furthermore  owing to lack of proper names  wordnet
cannot possibly provide the wealth of information encoded in wikipedia  which easily overcomes the drawbacks of wordnet  the methodology we proposed does not suffer from the
above shortcomings 

   

figabrilovich   markovitch

references
adafre  s  f     de rijke  m          discovering missing links in wikipedia  in proceedings
of the workshop on link discovery  issues  approaches and applications  linkkdd       pp       
baeza yates  r     ribeiro neto  b          modern information retrieval  addison wesley 
new york  ny 
baker  d     mccallum  a  k          distributional clustering of words for text classification  in croft  b   moffat  a   van rijsbergen  c  j   wilkinson  r     zobel  j   eds   
proceedings of the   st acm international conference on research and development
in information retrieval  pp         melbourne  au  acm press  new york  us 
basili  r   moschitti  a     pazienza  m  t          language sensitive text classification 
in proceedings of riao      th international conference recherche dinformation
assistee par ordinateur  pp          paris  france 
begelman  g   keller  p     smadja  f          automated tag clustering  improving search
and exploration in the tag space  in proceedings of the collaborative web tagging
workshop  in conjunction with the   th international world wide web conference 
edinburgh  scotland 
bekkerman  r          distributional clustering of words for text categorization  masters
thesis  technion 
bloehdorn  s     hotho  a          boosting for text classification with semantic features 
in proceedings of the msw      workshop at the   th acm sigkdd conference
on knowledge discovery and data mining  pp       
brank  j   grobelnik  m   milic frayling  n     mladenic  d          interaction of feature
selection methods and linear classification models  in workshop on text learning
held at icml      
brill  e          transformation based error driven learning and natural language processing  a case study in part of speech tagging  computational linguistics         
       
buchanan  b  g     feigenbaum  e          forward  in davis  r     lenat  d   eds   
knowledge based systems in artificial intelligence  mcgraw hill 
budanitsky  a     hirst  g          evaluating wordnet based measures of lexical semantic
relatedness  computational linguistics               
cai  l     hofmann  t          text categorization by boosting automatically extracted
concepts  in proceedings of the   th international conference on research and development in information retrieval  pp         
caropreso  m  f   matwin  s     sebastiani  f          a learner independent evaluation of
the usefulness of statistical phrases for automated text categorization  in chin  a  g 
 ed    text databases and document management  theory and practice  pp        
idea group publishing  hershey  us 
   

fiwikipedia based semantic interpretation

chang  m  w   ratinov  l   roth  d     srikumar  v          importance of semantic
representation  dataless classification  in proceedings of the   rd aaai conference
on artificial intelligence  pp         
cohen  w  w          fast effective rule induction  in proceedings of the   th international
conference on machine learning  icml      pp         
cohen  w  w          automatically extracting features for concept learning from the web 
in proceedings of the   th international conference on machine learning 
dagan  i   lee  l     pereira  f  c  n          similarity based models of word cooccurrence
probabilities  machine learning                
dagan  i   marcus  s     markovitch  s          contextual word similarity and estimation
from sparse data  computer speech and language                
davidov  d   gabrilovich  e     markovitch  s          parameterized generation of labeled
datasets for text categorization based on a hierarchical directory  in proceedings of
the   th acm international conference on research and development in information
retrieval  pp         
debole  f     sebastiani  f          supervised term weighting for automated text categorization  in proceedings of sac       th acm symposium on applied computing 
pp         
deerwester  s   dumais  s   furnas  g   landauer  t     harshman  r          indexing by
latent semantic analysis  journal of the american society for information science 
               
demsar  j          statistical comparison of classifiers over multiple data sets  journal of
machine learning research         
dewdney  n   vaness dykema  c     macmillan  r          the form is the substance 
classification of genres in text  in workshop on hlt and km held at acl      
dhillon  i   mallela  s     kumar  r          a divisive information theoretic feature clustering algorithm for text classification  journal of machine learning research    
         
dumais  s   platt  j   heckerman  d     sahami  m          inductive learning algorithms
and representations for text categorization  in proceedings of the  th acm international conference on information and knowledge management  pp         
egozi  o   gabrilovich  e     markovitch  s          concept based feature generation and
selection for information retrieval  in aaai   
fawcett  t          feature discovery for problem solving systems  ph d  thesis  umass 
fellbaum  c   ed            wordnet  an electronic lexical database  mit press  cambridge  ma 
finkelstein  l   gabrilovich  e   matias  y   rivlin  e   solan  z   wolfman  g     ruppin 
e       a   placing search in context  the concept revisited  acm transactions on
information systems                 
   

figabrilovich   markovitch

finkelstein  l   gabrilovich  e   matias  y   rivlin  e   solan  z   wolfman  g     ruppin 
e       b   wordsimilarity     test collection  
fuernkranz  j   mitchell  t     riloff  e          a case study in using linguistic phrases
for text categorization on the www  in sahami  m   ed    learning for text categorization  proceedings of the      aaai icml workshop  pp       aaai press 
madison  wisconsin 
gabrilovich  e     markovitch  s          text categorization with many redundant features 
using aggressive feature selection to make svms competitive with c     in proceedings
of the   st international conference on machine learning  pp         
gabrilovich  e     markovitch  s          feature generation for text categorization using world knowledge  in proceedings of the   th international joint conference on
artificial intelligence  pp            edinburgh  scotand 
gabrilovich  e     markovitch  s          overcoming the brittleness bottleneck using
wikipedia  enhancing text categorization with encyclopedic knowledge  in proceedings of the   st national conference on artificial intelligence  pp           
gabrilovich  e     markovitch  s       a   computing semantic relatedness using wikipediabased explicit semantic analysis  in proceedings of the   th international joint conference on artificial intelligence  pp           
gabrilovich  e     markovitch  s       b   harnessing the expertise of        human editors  knowledge based feature generation for text categorization  journal of machine
learning research              
galavotti  l   sebastiani  f     simi  m          experiments on the use of feature selection
and negative evidence in automated text categorization  in borbinha  j     baker  t 
 eds    proceedings of ecdl      th european conference on research and advanced
technology for digital libraries  pp        lisbon  portugal 
giles  j          internet encyclopaedias go head to head  nature              
gurevych  i   mueller  c     zesch  t          what to be   electronic career guidance
based on semantic relatedness  in proceedings of the   th annual meeting of the
association for computational linguistics 
hersh  w   buckley  c   leone  t     hickam  d          ohsumed  an interactive
retrieval evaluation and new large test collection for research  in proceedings of the
  th acm international conference on research and development in information
retrieval  pp         
hirsh  h     japkowicz  n          bootstrapping training data representations for inductive
learning  a case study in molecular biology  in proceedings of the twelfth national
conference on artificial intelligence  pp         
hirst  g     st onge  d          lexical chains as representations of context for the detection
and correction of malapropisms  in wordnet  an electronic lexical database  pp 
        mit press  cambridge  ma 
hu  y  j     kibler  d          a wrapper approach for constructive induction  in the
thirteenth national conference on artificial intelligence  pp       
   

fiwikipedia based semantic interpretation

hughes  t     ramage  d          lexical semantic relatedness with random graph walks  in
proceedings of the conference on empirical methods in natural language processing
 emnlp  
hull  d  a          improving text retrieval for the routing problem using latent semantic
indexing  in croft  w  b     van rijsbergen  c  j   eds    proceedings of the   th acm
international conference on research and development in information retrieval  pp 
        dublin  ireland  springer verlag  heidelberg  germany 
jarmasz  m          rogets thesaurus as a lexical resource for natural language processing 
masters thesis  university of ottawa 
jarmasz  m     szpakowicz  s          rogets thesaurus and semantic similarity  in
proceedings of the international conference on recent advances in natural language
processing  pp         
jiang  j  j     conrath  d  w          semantic similarity based on corpus statistics and
lexical taxonomy  in proceedings of the   th international conference on research on
computational linguistics  pp       
jo  t          neurotextcategorizer  a new model of neural network for text categorization 
in proceedings of the international conference of neural information processing  pp 
        taejon  south korea 
jo  t          dynamic document organization using text categorization and text clustering  ph d  thesis  university of ottawa 
jo  t     japkowicz  n          text clustering using ntso  in proceedings of the international joint conference on neural networks  pp         
joachims  t          text categorization with support vector machines  learning with many
relevant features  in proceedings of the european conference on machine learning 
pp         
joachims  t          making large scale svm learning practical  in schoelkopf  b   burges 
c     smola  a   eds    advances in kernel methods  support vector learning  pp 
        the mit press 
kudenko  d     hirsh  h          feature generation for sequence categorization  in proceedings of the   th conference of the american association for artificial intelligence 
pp         
kumaran  g     allan  j          text classification and named entities for new event
detection  in proceedings of the   th acm international conference on research and
development in information retrieval  pp         
lang  k          newsweeder  learning to filter netnews  in proceedings of the   th international conference on machine learning  pp         
leacock  c     chodorow  m          combining local context and wordnet similarity for
word sense identification  in wordnet  an electronic lexical database  pp         
mit press  cambridge  ma 
lee  l          measures of distributional similarity  in proceedings of the   th annual
meeting of the acl  pp       
   

figabrilovich   markovitch

lee  m  d   pincombe  b     welsh  m          a comparison of machine measures of text
document similarity with human judgments  in   th annual meeting of the cognitive
science society  cogsci       pp           
lenat  d  b          cyc  a large scale investment in knowledge infrastructure  communications of the acm          
lenat  d  b          from      to       common sense and the mind of hal  in hals
legacy  pp          the mit press 
lenat  d  b   guha  r  v   pittman  k   pratt  d     shepherd  m          cyc  towards
programs with common sense  communications of the acm         
leopold  e     kindermann  j          text categorization with support vector machines 
how to represent texts in input space  machine learning             
lewis  d  d          an evaluation of phrasal and clustered representations on a text
categorization task  in proceedings of the   th acm international conference on
research and development in information retrieval  pp       
lewis  d  d     croft  w  b          term clustering of syntactic phrases  in proceedings of
the   th acm international conference on research and development in information
retrieval  pp         
lewis  d  d   schapire  r  e   callan  j  p     papka  r          training algorithms for
linear text classifiers  in proceedings of the   th acm international conference on
research and development in information retrieval  pp         
lewis  d  d   yang  y   rose  t     li  f          rcv   a new benchmark collection for
text categorization research  journal of machine learning research            
lin  d       a   automatic retrieval and clustering of similar words  in proceedings of the
  th international conference on computational linguistics and   th annual meeting
of the association for computational linguistics  pp         
lin  d       b   an information theoretic definition of word similarity  in proceedings of
the   th international conference on machine learning  pp         
liu  t   chen  z   zhang  b   ma  w  y     wu  g          improving text classification
using local latent semantic indexing  in icdm    pp         
manning  c  d     schuetze  h          foundations of statistical natural language processing  the mit press 
markovitch  s     rosenstein  d          feature generation using general constructor
functions  machine learning               
matheus  c  j          the need for constructive induction  in birnbaum  l     collins  g 
 eds    proceedings of the eighth international workshop on machine learning  pp 
       
matheus  c  j     rendell  l  a          constructive induction on decision trees  in
proceedings of the   th international conference on artificial intelligence  pp     
    
   

fiwikipedia based semantic interpretation

mesh         medical subject headings  mesh  
http   www nlm nih gov mesh 

national library of medicine 

metzler  d   dumais  s     meek  c          similarity measures for short segments of text 
in proceedings of the   th european conference on information retrieval  pp       
mihalcea  r          turning wordnet into an information retrieval resource  systematic
polysemy and conversion to hierarchical codes  international journal of pattern recognition and artificial intelligence  ijprai                  
mihalcea  r   corley  c     strapparava  c          corpus based and knowledge based
measures of text semantic similarity  in aaai    pp         
mikheev  a          feature lattices and maximum entropy models  in proceedings of the
  th international conference on computational linguistics  pp         
miller  g  a     charles  w  g          contextual correlates of semantic similarity  language and cognitive processes             
milne  d     witten  i          an effective  low cost measure of semantic relatedness
obtained from wikipedia links  in proceedings of the aaai    workshop on wikipedia
and artificial intelligence  in conjunction with   rd aaai conference on artificial
intelligence 
mladenic  d          turning yahoo into an automatic web page classifier  in proceedings
of   th european conference on artificial intelligence  pp         
montague  r          the proper treatment of quantification in ordinary english  in
hintikka  j   moravcsik  j     suppes  p   eds    approaches to natural language  pp 
        reidel  dordrecht 
murphy  p  m     pazzani  m  j          id  of    constructive induction of m of n concepts for discriminators in decision trees  in proceedings of the  th international
conference on machine learning  pp          morgan kaufmann 
pagallo  g     haussler  d          boolean feature discovery in empirical learning  machine
learning              
pang  b   lee  l     vaithyanathan  s          thumbs up  sentiment classification using
machine learning techniques  in proceedings of the conference on empirical methods
in natural language processing  pp       
peng  f   schuurmans  d     wang  s          augmenting naive bayes classifiers with
statistical language models  information retrieval                  
peng  f     shuurmans  d          combining naive bayes and n gram language models
for text classification  in proceedings of the   th european conference on information
retrieval research  ecir      pp         
pincombe  b          comparison of human and latent semantic analysis  lsa  judgements
of pairwise document similarities for a news corpus  tech  rep  dsto rr       information sciences laboratory  defence science and technology organization  department of defense  australian government 
porter  m          an algorithm for suffix stripping  program                 
   

figabrilovich   markovitch

potthast  m   stein  b     anderka  m          a wikipedia based multilingual retrieval
model  in european conference on information retrieval 
press  w  h   teukolsky  s  a   vetterling  w  t     flannery  b  p          numerical
recipes in c  the art of scientific computing  cambridge university press 
qiu  y     frei  h          concept based query expansion  in proceedings of the acm
international conference on research and development in information retrieval 
raskutti  b   ferra  h     kowalczyk  a          second order features for maximizing
text classification performance  in de raedt  l     flach  p   eds    proceedings of
the european conference on machine learning  ecml   lecture notes in artificial
intelligence  lnai        pp          springer verlag 
resnik  p          semantic similarity in a taxonomy  an information based measure and
its application to problems of ambiguity in natural language  journal of artificial
intelligence research            
reuters         reuters       text categorization test collection  distribution      reuters 
daviddlewis com resources testcollections reuters      
rocchio  j  j          relevance feedback in information retrieval  in the smart retrieval
system  experiments in automatic document processing  pp          prentice hall 
rogati  m     yang  y          high performing feature selection for text classification  in
proceedings of the international conference on information and knowledge management  cikm     pp         
roget  p          rogets thesaurus of english words and phrases  longman group ltd 
rose  t   stevenson  m     whitehead  m          the reuters corpus volume  from
yesterdays news to tomorrows language resources  in proceedings of the third international conference on language resources and evaluation  pp      
rowling  j          harry potter and the philosophers stone  bloomsbury 
rubenstein  h     goodenough  j  b          contextual correlates of synonymy  communications of the acm                 
sable  c   mckeown  k     church  k  w          nlp found helpful  at least for one
text categorization task   in conference on empirical methods in natural language
processing  pp         
sahami  m     heilman  t          a web based kernel function for measuring the similarity
of short text snippets  in www    pp          acm press 
salton  g     buckley  c          term weighting approaches in automatic text retrieval 
information processing and management                 
salton  g     mcgill  m         
mcgraw hill 

an introduction to modern information retrieval 

scott  s          feature engineering for a symbolic approach to text classification  masters
thesis  u  ottawa 
scott  s     matwin  s          feature engineering for text classification  in proceedings
of the   th international conference on machine learning  pp         
   

fiwikipedia based semantic interpretation

sebastiani  f          machine learning in automated text categorization  acm computing
surveys              
snow  r   oconnor  b   jurafsky  d     ng  a  y          cheap and fast   but is it good 
evaluating non expert annotations for natural language tasks  in proceedings of the
conference on empirical methods in natural language processing 
sorg  p     cimiano  p          cross lingual information retrieval with explicit semantic
analysis  in working notes for the clef workshop 
strube  m     ponzetto  s  p          wikirelate  computing semantic relatedness using
wikipedia  in aaai    pp            boston  ma 
turney  p          thumbs up or thumbs down  semantic orientation applied to unsupervised classification of reviews  in proceedings of the   th annual meeting of the
association of computational linguistics  pp         
turney  p          measuring semantic similarity by latent relational analysis  in proceedings
of the nineteenth international joint conference on artificial intelligence  ijcai     
pp            edinburgh  scotland 
turney  p          similarity of semantic relations  computational linguistics             
    
turney  p     littman  m  l          unsupervised learning of semantic orientation from
a hundred billion word corpus  tech  rep  erb       national research council
canada 
turney  p  d          mining the web for synonyms  pmi ir versus lsa on toefl  in
proceedings of the twelfth european conference on machine learning  pp         
urena lopez  a   buenaga  m     gomez  j  m          integrating linguistic resources in
tc through wsd  computers and the humanities             
wang  b  b   mckay  r   abbass  h  a     barlow  m          a comparative study for
domain ontology guided feature extraction  in proceedings of the   th australian
computer science conference  ascs        pp       
widrow  b     stearns  s          adaptive signal processing  prentice hall 
wikipedia         wikipedia  the free encyclopedia   http   en wikipedia org 
wu  h     gunopulos  d          evaluating the utility of statistical phrases and latent
semantic indexing for text classification  in ieee international conference on data
mining  pp         
yang  y          a study on thresholding strategies for text categorization  in proceedings
of the   th international conference on research and development in information
retrieval  pp         
yang  y     liu  x          a re examination of text categorization methods  in proceedings
of the   nd international conference on research and development in information
retrieval  pp       
yang  y     pedersen  j          a comparative study on feature selection in text categorization  in proceedings of the   th international conference on machine learning 
pp         
   

figabrilovich   markovitch

zelikovitz  s     hirsh  h          improving short text classification using unlabeled background knowledge to assess document similarity  in proceedings of the   th international conference on machine learning  pp           
zelikovitz  s     hirsh  h          using lsi for text classification in the presence of
background text  in proceedings of the conference on information and knowledge
management  pp         
zesch  t     gurevych  i          automatically creating datasets for measures of semantic
relatedness  in proceedings of the acl workshop on linguistic distances  pp       
sydney  australia 
zesch  t   mueller  c     gurevych  i          using wiktionary for computing semantic
relatedness  in proceedings of the   rd aaai conference on artificial intelligence 
pp         
zobel  j     moffat  a          exploring the similarity space  acm sigir forum         
     

   

fi