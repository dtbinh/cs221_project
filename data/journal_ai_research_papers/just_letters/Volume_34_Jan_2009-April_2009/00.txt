journal of artificial intelligence research               

submitted        published     

interactive policy learning through
confidence based autonomy
sonia chernova
manuela veloso

soniac cs cmu edu
veloso cs cmu edu

computer science dept 
carnegie mellon university
pittsburgh  pa usa

abstract
we present confidence based autonomy  cba   an interactive algorithm for policy
learning from demonstration  the cba algorithm consists of two components which take
advantage of the complimentary abilities of humans and computer agents  the first component  confident execution  enables the agent to identify states in which demonstration
is required  to request a demonstration from the human teacher and to learn a policy based
on the acquired data  the algorithm selects demonstrations based on a measure of action
selection confidence  and our results show that using confident execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by
a human teacher  the second algorithmic component  corrective demonstration  enables
the teacher to correct any mistakes made by the agent through additional demonstrations
in order to improve the policy and future task performance  cba and its individual components are compared and evaluated in a complex simulated driving domain  the complete
cba algorithm results in the best overall learning performance  successfully reproducing
the behavior of the teacher while balancing the tradeoff between number of demonstrations
and number of incorrect actions during learning 

   introduction
learning from demonstration is a growing area of artificial intelligence research that explores
techniques for programming autonomous agents by demonstrating the desired behavior or
task  in demonstration based approaches  a teacher  typically a human  shows the agent
how to perform the task  the agent records the demonstrations as sequences of stateaction pairs  from which it then learns a policy that reproduces the observed behavior 
many learning from demonstration approaches are inspired by the way humans and animals
teach each other  aiming to provide an intuitive method to transfer human task knowledge
to autonomous systems  compared to exploration based methods  demonstration learning
often reduces the learning time and eliminates the frequently difficult task of defining a
detailed reward function  smart        schaal        
in this article  we present an interactive demonstration learning algorithm  confidencebased autonomy  cba   which enables an agent to learn a policy through interaction with
a human teacher  in this learning approach  the agent begins with no initial knowledge and
learns a policy incrementally through demonstrations acquired as it practices the task  each
demonstration consists of a training point representing the correct action to be performed
in a particular state  the agents state is represented using an n dimensional feature vector
c
    
ai access foundation  all rights reserved 

fichernova   veloso

that can be composed of continuous or discrete values  the agents actions are bound to a
finite set a of action primitives  the basic actions that can be combined together to perform
the overall task  given a sequence of demonstrations  si   ai    with state si and teacherselected action ai  a  the goal is for the agent to learn to imitate the teachers behavior
by generalizing from the demonstrations and learning a policy mapping from all possible
states to actions in a 
the method for gathering demonstrations is at the heart of all demonstration learning
algorithms  cba performs this function through two algorithmic components  confident
execution  which enables the agent to select demonstrations in real time as it interacts
with the environment using automatically calculated confidence thresholds  and corrective demonstration  which enables the teacher to improve the learned policy and correct
mistakes through additional demonstrations  the complete confidence based autonomy
algorithm provides a fast and intuitive method for policy learning  incorporating shared
decision making between the learner and the teacher  in our experimental evaluation  we
highlight the strengths of both learning components and compare learning performance of
five different demonstration selection techniques  our results indicate that in a complex
domain  the confident execution algorithm reduces the number of demonstrations required
to learn the task compared to demonstration selection performed by the human teacher 
additionally  we find that the teachers ability to correct mistakes performed by the agent
is critical for optimizing policy performance 
in section    we discuss related work in learning from demonstration  we then present
an overview of the complete confidence based autonomy learning algorithm in section   
followed by detailed descriptions of the confident execution and corrective demonstration
components in sections   and    respectively  in section    we present an experimental
evaluation of the complete algorithm and its components in a complex simulated driving
domain  section   presents a summary and discussion of possible extensions to this work 

   related work
a wide variety of algorithms for policy learning from demonstration have been proposed
within the machine learning and robotics communities  within the context of reinforcement
learning  sutton   barto         demonstration has been viewed as a source of reliable
information that can be used to accelerate the learning process  a number of approaches
for taking advantage of this information have been developed  such as deriving or modifying
the reward function based on demonstrations  thomaz   breazeal        abbeel   ng 
      papudesi        atkeson   schaal         and using the demonstration experiences
to prime the agents value function or model  takahashi  hikita    asada        price  
boutilier        smart        schaal        
demonstration has also been coupled with supervised learning algorithms for policy
learning  including locally weighted regression for low level skill acquisition  grollman  
jenkins        browning  xu    veloso        smart         bayesian networks for high level
behaviors  lockerd   breazeal        inamura  inaba    inoue         and the k nearest
neighbors algorithm for fast paced games and robot navigation tasks  saunders  nehaniv 
  dautenhahn        bentivegna  ude  atkeson    cheng         a recent survey covers
 

fiinteractive policy learning through confidence based autonomy

these and other demonstration learning algorithms in detail  argall  chernova  browning 
  veloso        
in addition to policy learning from demonstration  several areas of research have also
explored algorithms for demonstration selection  within machine learning research  active
learning  blum   langley        cohn  atlas    ladner        enables a learner to query
an expert and obtain labels for unlabeled training examples  aimed at domains in which
a large quantity of data is available but labeling is expensive  active learning directs the
expert to label the more informative examples with the goal of minimizing the number of
queries  in the context of reinforcement learning  the ask for help framework enables an
agent to request advice from other agents when it is confused about what action to take 
an event characterized by relatively equal quality estimates for all possible actions in a given
state  clouse         similarly motivated techniques have been used in robotics to identify
situations in which a robot should request a demonstration from its teacher  grollman  
jenkins        lockerd   breazeal        nicolescu        inamura et al          most
closely related to our work is the dogged learning algorithm  grollman   jenkins        
a confidence based learning approach for teaching low level robotic skills  in this algorithm 
the robot indicates to the teacher its certainty in performing various elements of the task 
the teacher may then choose to provide additional demonstrations based on this feedback 
while similarly motivated  our work differs from the dogged learning algorithm in a number
of ways  most important of which are our use of classification instead of regression in policy
learning  and our algorithms ability to adjust the confidence threshold to the data instead
of using a fixed value 

   confidence based autonomy overview
the confence based autonomy algorithm enables a human user to train a task policy
through demonstration  the algorithm consists of two components 
 confident execution  ce   an algorithm that enables the agent to learn a policy based
on demonstrations obtained by regulating its autonomy and requesting help from the
teacher  demonstrations are selected based on automatically calculated classification
confidence thresholds 
 corrective demonstration  cd   an algorithm that enables the teacher to improve
the learned policy by correcting mistakes made by the agent through supplementary
demonstrations 
figure   shows the interaction between these components  using the confident execution algorithm  the agent selects states for demonstration in real time as it interacts with
the environment  targeting states that are unfamiliar or in which the current policy action is
uncertain  at each timestep  the algorithm evaluates the agents current state and actively
decides between autonomously executing the action selected by its policy and requesting
an additional demonstration from the human teacher 
we assume the underlying model of the agents task to be an mdp  the agents policy
is represented and learned using supervised learning based on training data acquired from
the demonstrations  confidence based autonomy can be combined with any supervised
 

fichernova   veloso

figure    confidence based autonomy learning process 
learning algorithm that provides a measure of confidence in its classification  the policy
is represented by classifier c   s   a  c  db   trained using state vectors si as inputs  and
actions ai as labels  for each classification query  the model returns the model selected
action a  a  action selection confidence c  and the decision boundary db with the highest
confidence for the query  e g  gaussian component for gmms  
to effectively select demonstrations  the learner must be able to autonomously identify
situations in which a demonstration will provide useful information and improve the policy 
confident execution selects between agent autonomy and a request for demonstration based
on the measure of action selection confidence c returned by the classifier  given the current
state of the learner  the algorithm queries the policy to obtain its confidence in selecting
an action for that state  and regulates its autonomy based on this confidence  the learner
executes the returned action ap if confidence c is above a threshold    which is determined
by the decision boundary of the classifier  db  confidence below this threshold indicates that
the agent is uncertain about which action to take  so it seeks help from the teacher in the
form of a demonstration  receiving an additional demonstration  ad   in a low confidence
situation improves the policy  leading to increased confidence  and therefore autonomy  in
future similar states  as more training data becomes available  the quality of the policy
improves and the autonomy of the agent increases until the entire task can be performed
without help from the teacher  in section   we compare two methods of using classification
confidence to select states for demonstration 
using the confident execution algorithm  the agent incrementally acquires demonstrations as it explores its environment  as it practices its task  the agent uses the policy it
learned up to that point to make decisions between demonstration and autonomous execution  however  by relying on the policy before learning is complete  the algorithm is likely
 

fiinteractive policy learning through confidence based autonomy

to make mistakes due to factors such as overgeneralization of the classifier or incomplete
data in some area of the state space  to address this problem this article introduces the
second algorithmic component  corrective demonstration  which allows the teacher to provide corrections for the agents mistakes  using this method  when an incorrect action is
observed  the teacher provides an additional demonstration to the agent indicating which
action should have been executed in its place  in addition to indicating that the wrong action was selected  this method also provides the algorithm with the correct action to perform
in its place  ac   the correction is therefore more informative than negative reinforcement
or punishment techniques common in other algorithms  leading the agent to learn quickly
from its mistakes 
together  confident execution and corrective demonstration form an interactive learning algorithm in which the learner and human teacher play complimentary roles  the learner
is able to identify states in which demonstration is required  in fact  our results show that the
algorithm is able to do this better than the human teacher due to differences in perception
and representation abilities  the teacher  on the other hand  possesses expert knowledge
of the overall task  which is applied to performing demonstrations and spotting execution
mistakes  this is a function the agent cannot perform on its own as it has not yet learned
the desired behavior  in this way  confidence based autonomy takes advantage of the
complimentary abilities of both human and agent  sections   and   present the confident
execution and corrective demonstration components in detail 

   confident execution algorithm
confident execution is an policy learning algorithm in which the agent must select demonstration examples  in real time  as it interacts with the environment  at each timestep  the
algorithm uses thresholds to determine whether a demonstration of the correct action in
the agents current state will provide useful information and improve the agents policy  if
demonstration is required  the agent requests help from the teacher  and updates its policy based on the resulting action label  otherwise the agent continues to perform its task
autonomously based on its policy 
there are two distinct situations in which the agent requires help from the teacher 
unfamiliar states and ambiguous states  an unfamiliar state occurs when the agent encounters a situation that is significantly different from any previously demonstrated state 
as represented by the outlying points in figure    while we do not want to demonstrate
every possible state  and therefore need our model to generalize  we would like to prevent
over generalization to truly different states 
ambiguous states occur when the agent is unable to select between multiple actions with
certainty  this situation can result when demonstrations of different actions from similar
states make accurate classification impossible  as in the region of overlapping data classes in
figure    in these cases  additional demonstrations may help to disambiguate the situation 
the goal of the confident execution algorithm is to divide the state space into regions
of high confidence  autonomous execution  and low confidence  demonstration  such that
unfamiliar and ambiguous regions fall into the low confidence areas  given a world state 
two evaluation criteria are used to select between demonstration and autonomy 
 

fichernova   veloso

 nearest neighbor distance  given d   n earestn eighbor s   the distance from the
current state to the nearest  most similar  training datapoint  the agent may act
autonomously if d is below the distance threshold dist  
 classification confidence  given c  the classification confidence of the current state 
the agent may act autonomously if the value of c is above the confidence threshold
conf  
the methods for calculating thresholds dist and conf are presented in sections     and     
in this section  we continue the discussion of the confident execution algorithm assuming
that these values are given 
algorithm   presents the details of the confident execution algorithm  we assume no
preexisting knowledge about the task  and initialize the algorithm with an empty set of
training points t   since a classifier is not initially available  threshold conf is initialized
to infinity to ensure that the agent is controlled through demonstration during the initial
learning stage  distance threshold dist is initialized to   
the main learning algorithm consists of a loop  lines        each iteration of which
represents a single timestep  the behavior of the algorithm is determined by whether the
agent is currently executing an action  if an action is in progress  the algorithm performs
no additional computation during this timestep  line      once an action is complete  the
algorithm evaluates its state to determine the next action to perform  lines       
evaluation begins by obtaining the agents current state in the environment  line    
this information is then used to calculate the nearest neighbor distance and to query the
learned classifier c to obtain policy action ap and confidence c  these values are then
compared to the confidence and distance thresholds to decide between demonstration and
autonomy  line     if similar states have previously been observed  and the learned model is
confident in its selection  the algorithm finishes the timestep by initiating the autonomous

figure    outlying points and regions of overlapping data classes represent unfamiliar and
ambiguous state regions  respectively 

 

fiinteractive policy learning through confidence based autonomy

algorithm   confident execution algorithm
   t    
   conf  inf
   dist   
   while true do
  
if actioncomplete then
  
s  getsensordata  
  
d   nearestneighbor s 
  
 ap   c  db   c s 
  
if c   conf and d   dist then
   
executeaction ap  
   
else
   
requestdemonstration  
   
ad  getteacheraction  
   
if ad    n u ll then
   
t  t    s  ad   
   
c  updateclassifier t  
   
 conf   dist    updatethresholds  
   
executeaction ad  
   
else
   
  do nothing

execution of the policy selected action ap  line      otherwise it initiates a request for
teacher demonstration  lines        
the agent requests a demonstration by pausing and indicating to the teacher that a
demonstration is required  note that we assume the domain allows the agent to pause
execution  following a demonstration request  the algorithm checks whether a demonstration has been performed  lines         if the teachers response is available  a new training
datapoint consisting of the current state and the corresponding demonstrated action ad is
added to the training set  line      the model classifier is then retrained  and the threshold
values updated  before executing the teacher selected action  lines        
if the teachers response is not immediately available  the timestep terminates and the
whole process is repeated at the next iteration  the agent again senses its state  performs
the threshold comparison and checks for a demonstration  this non blocking mechanism
enables the agent to wait for a demonstration from the teacher without losing awareness
of its surroundings  in cases where the agents environment is dynamic  maintaining up
to date information is important as the state may change in the time between the initial
request and the demonstration  associating the action label with the agents most recent
state  the one the teacher is most likely responding to  is therefore critical to learning an
accurate model  additionally  changes in the environment can result in the agent attaining
a high confidence state without any actions of its own  in these cases  autonomous execution
of the task is automatically resumed  in summary  once a demonstration request is made 
no further actions are taken by the agent until either a demonstration is received from the
teacher  or changes in the environment result in a high confidence state 
 

fichernova   veloso

using this approach  confident execution enables the agent to incrementally acquire
demonstrations representing the desired behavior  as more datapoints are acquired  fewer
states distant from the training data are encountered  the performance and classification
confidence improve  and the autonomy of the agent increases  task learning is complete
once the agent is able to repeatedly perform the desired behavior without requesting demonstrations  in the following sections we present the methods for calculating the distance and
confidence thresholds 
    distance threshold
the purpose of the distance threshold is to evaluate the similarity between the agents
current state and previous demonstrations  our evaluation metric uses the nearest neighbor
distance  defined as a the euclidian distance between a query and the closest point in the
dataset  for each agent state query  we obtain its nearest neighbor distance representing the
most similar previously demonstrated state  this value is then compared to the distance
threshold dist  
the value of the distance threshold dist is calculated as a function of the average nearest
neighbor distance across the dataset of demonstrations  evaluating the average similarity
between states provides the algorithm with a domain independent method for detecting
outliers  points unusually far from previously encountered states  for trials in this article 
the value of dist was set to three times the average nearest neighbor distance across the
dataset 
an alternate method for detecting outliers would be to use classification confidence and
request demonstrations in low confidence states  however  situations can arise in which
confidence is not directly correlated with state similarity  for example  for many classifiers
a set of datapoints encircling an empty region  similar to the shape of a donut  would result
in the highest classification confidence being associated with the empty center region far
from previous demonstrations  distance provides a reliable prediction of similarity  even in
these cases 
    confidence threshold
the confidence threshold is used to select regions of uncertainty in which points from
multiple classes overlap  from the agents perspective  points in these regions represent
demonstrations of two distinct actions from states that appear similar  and are difficult
to distinguish based on the sensor data  this problem frequently arises in demonstration
learning for a number of reasons  such as the teachers inability to demonstrate the task
consistently  noise in the sensor readings  or an inconsistency between the agents and
teachers sensing abilities  we would like to set the confidence threshold to a value that
prevents either model from classifying the overlapping region with high confidence    in the
following section we will discuss the use and limitations of a single fixed threshold value 
we then present an algorithm for using multiple adjustable thresholds in section       
   see section     for further discussion of these data regions 

 

fiinteractive policy learning through confidence based autonomy

 a 

 b 

 c 

figure    examples of fixed threshold failure cases   a  fully separable data classes with an
overly conservative threshold value  b  overlapping data classes with an overly
general threshold value  c  data classes with different distributions and common
threshold value

      single fixed threshold
a single  fixed confidence threshold value provides a simple mechanism to approximate the
high confidence regions of the state space  previous algorithms utilizing a classification confidence threshold for behavior arbitration have all used a manually selected single threshold
value  inamura et al         lockerd   breazeal        grollman   jenkins         however  choosing an appropriate value can be difficult for a constantly changing dataset and
model  figure   presents examples of three frequently encountered problems 
figure   a  presents a case in which two action classes are distinct and fully separable  a
model trained on this dataset is able to classify the points with complete accuracy  without
misclassifications  however  the current threshold value classifies only     of the points
with high confidence  marking the remaining     of the points as uncertain  in this case 
a lower threshold value would be preferred that would allow the model to generalize more
freely  the resulting larger high confidence region would reduce the number of redundant
demonstrations without increasing the classification error rate of either data class 
figure   b  presents an example of the opposite case  in which a stricter threshold value
would be preferred  in this example the data classes overlap  resulting in a middle region
in which points cannot be classified with high accuracy  a higher threshold value would
prevent the classification of points in this region into either data class  initiating instead a
request for demonstration that would allow the teacher to disambiguate the situation 
figure   c  presents a case in which the datapoints of the two data classes have very
different distributions  while the fixed threshold value is appropriate for the left class     
of the points in the right class are labeled as low confidence 
classification of complex multi class data depends upon multiple decision boundaries 
using the same value for all decision boundaries can exacerbate the problems highlighted
above  as a single value often cannot be found that constrains model classification in some
areas while allowing generalization in others  the resulting effect is that the agent requests
too many demonstrations about things it already knows  and too few demonstrations about
unlearned behavior  to address this problem  we present an algorithm for calculating a
unique threshold value for each decision boundary 
 

fichernova   veloso

 a 

 b 

 c 

figure    autonomy threshold calculation   a  example dataset  with highlighted overlapping region  b  learned decision boundary  misclassified points marked with
confidence values  c  learned threshold values for each data class  a low confidence region containing most of the overlapping points remains in the center 

      multiple adjustable thresholds
in this section  we contribute an algorithm for calculating a confidence threshold for each
decision boundary  customized to its unique distribution of points  in our analysis  we
assume that we are able to query the classifier and obtain a confidence score representing
the likelihood that a particular input belongs within a specified decision boundary 
the algorithm begins by dividing the dataset into a training and test set and training the
classifier c  the resulting learned model is used to classify the withheld test set  for which
the correct action labels are known  the algorithm then calculates a unique confidence
threshold for each decision boundary based on the confidence scores of misclassified points 
given the confidence scores of a set of points mistakenly classified by a decision boundary 
we assume that future classifications with confidences at or below these values are likely to
be misclassifications as well  the threshold is therefore calculated as a function of these
confidence scores 
specifically  we define a classified point as the tuple  o  a  am   c   where o is the original
observation  a is the demonstrated action label  am is the model selected action  and c
is the model action confidence  let mi     o  ai   am   c  am    ai   be the set of all points
mistakenly classified by decision boundary i  the confidence threshold
pvalue is set to the
mi

c

average classification confidence of the misclassified points  conf i    mi     we take the
average to avoid overfitting to noisy data  other values  based on the maximum or standard
deviation  can be used if a more conservative estimate is required  a threshold value of  
indicates that no misclassifications occurred and the model is able to generalize freely 
figure   presents an example of the threshold calculation process  figure   a  presents
a small sample dataset  the rectangular box in the figure highlights a region of the state
space in which points from both classes overlap  figure   b  shows the learned decision
boundary  in this case a svm  separating our two data classes  six misclassified points are
marked with the  mis  classification confidences returned by the model  misclassified points
on each side of the decision boundary will be used to calculate the respective confidence
thresholds  figure   c  shows the confidence threshold lines and values based on the above
  

fiinteractive policy learning through confidence based autonomy

 a 

 b 

 c 

figure    multiple adjustable thresholds applied to the failure cases shown in figure   

calculations  the resulting low confidence region in the middle of the image captures most
of the noisy datapoints 
given this multi threshold approach  classification of new points is performed by first
selecting the action class with the highest confidence for the query  the comparison on
line   of algorithm   is then performed using the threshold of the decision boundary with
the highest confidence for the query  using this method  the threshold value of the most
likely decision boundary to represent the point is used to decide between demonstration
and autonomy 
figure   shows how the example failure cases discussed in section       are addressed
by the multi thresholded approach  customizing the threshold value to each unique data
distribution enables the algorithm to correctly classify      of the points in figures   a 
and  c   since there are no misclassifications  the model generalizes freely in these examples 
for the dataset in figure   b   in which perfect classification is not possible  the confidence
thresholds are set such that the overlapping region falls into a low confidence area  this
example uses a gaussian mixture model  in which the elliptical confidence gradient around
the mean results in a large low confidence area even far from the overlapping region  other
classification methods  such as support vector machines  do not have this drawback 
the presented multi threshold approach is algorithm independent  and figure   presents
classification results of four different classification methods  gaussian mixture models  random forests  rf   support vector machine with a quadratic kernel  and svm with a radial
basis function  rbf  kernel  the table below summarizes the classification performance of
each algorithm and lists the threshold values for each of the models 
algorithm
gmm
rf
svm quad 
svm rbf

correct misclas  unclass 
                 
                 
                 
                 

thresholds
             
              
                
               

table    classifier comparison 

  

fichernova   veloso

 a  gaussian mixture model

 b  random forest

 c  svm  quadratic 

 d  svm  rbf 

figure    classification of dataset into high and low confidence regions using different classification methods 

   corrective teacher demonstration
the presented confident execution algorithm enables the agent to identify unfamiliar and
ambiguous states and prevents autonomous execution in these situations  however  states
in which an incorrect action is selected with high confidence for autonomous execution
still occur  typically due to over generalization of the classifier  in this article we present
the corrective demonstration algorithm which  coupled with confident execution  enables
the teacher to correct mistakes made by the agent  algorithm   combines corrective
demonstration  lines denoted by   with confident execution and presents the complete
confidence based autonomy algorithm 
the corrective demonstration technique comes into play each time the agent executes
an autonomous action  as an action is selected for autonomous execution  the algorithm
records the agents state that led to this decision and saves this value within the variable sc
 line      during the execution of an autonomously selected action  the algorithm checks
for a teacher demonstration at every timestep  lines         if a corrective demonstration is
made  a new training datapoint consisting of the recorded demonstration state sc and the
corrective action ac is added to the training set  line      the classifier and thresholds are
then retrained using the new information 
  

fiinteractive policy learning through confidence based autonomy

algorithm   confidence based autonomy algorithm  confident execution and corrective
demonstration
   t    
   conf  inf
   dist   
   while true do
  
s  getsensordata  
  
if actioncomplete then
  
 ap   c  db   c s 
  
d   nearestneighbor s 
  
if c   conf and d   dist then
   
executeaction ap  
   
sc  s

   
else
   
requestdemonstration  
   
ad  getteacheraction  
   
if ad    n u ll then
   
t  t    s  ad   
   
c  updateclassifier t  
   
 conf   dist    updatethresholds  
   
executeaction ad  
   
else
   
if autonomousaction then

   
ac  getteacheraction  

   
if ac    n u ll then

   
t  t    sc   ac   

   
c  updateclassifier t  

   
 conf   dist    updatethresholds  


using this algorithm  the teacher observes the autonomous execution of the agent and
corrects any incorrect actions  unlike our previous demonstration technique in which the
agent was given the next action to perform  the correction is performed with relation to
the agents previous state at which the mistake was made  for example  when observing
a driving agent approaching too close behind another car  the teacher is able to indicate
that instead of continuing to drive forward  the agent should have been merging into the
passing lane  in this way  in addition to indicating that the wrong action was performed 
corrective demonstration also provides the algorithm with the action that should have
been performed in its place  this technique is more effective than negative reinforcement 
or punishment  techniques common in other algorithms  leading the agent to learn quickly
from its mistakes 
  

fichernova   veloso

figure    screenshot of the driving simulator  the agent  the black car currently in the
center lane  drives at a fixed speed and must navigate around other cars to avoid
collisions  the road consists of five lanes  three traffic lanes and two shoulder
lanes 

   evaluation and comparison
in this section we present an evaluation and comparison of the complete confidence based
autonomy algorithm and its components in simulated car driving domain  abbeel   ng 
       shown in figure   
    domain description
in the driving domain  the agent represents a car driving on a busy highway  the
learners car travels at a fixed speed of    mph  while all other cars move in their lanes
at predetermined speeds between    and    mph  the road has three normal lanes and
a shoulder lane on both sides  the agent is allowed to drive on the shoulder to pass other
cars  but cannot go further off road  since the learner cannot change its speed  it must
navigate between other cars and use the shoulder lanes to avoid collision  the agent is
limited to three actions  remaining in the current lane  or shifting one lane to the left or
right of the current position  a    forward left right    the teacher demonstrates the task
through a keyboard interface  the simulator has a framerate of   fps and is paused during
demonstration requests 
the agents state is represented by  s    l  dl   dc   dr    state feature l is a discrete value
symbolizing the agents current lane number  the remaining three features  denoted by
the letter d  represent the distance to the nearest car in each of the three driving lanes
 left  center and right   the distance features are continuously valued in the          range 
note that the nearest car in a lane can be behind the agent  distance measurements are
corrupted by noise to create a more complex testing environment  the agents policy is
relearned each time    new demonstrations are acquired 
the driving domain presents a varied and challenging environment  if car distances were
to be discretized by rounding to the nearest integer value  the domain would contain over
        possible states  due to the complexity of the domain  the agent requires a large
  

fiinteractive policy learning through confidence based autonomy

number of demonstrations to initialize the classifier  resulting in nearly constant demonstration requests early in the training process  to simplify the task of the teacher  we add
a short     datapoint  or approximately    second  non interactive driving demonstration
session to initialize the learning process  while this learning stage is not required  it simplifies the task of the teacher for whom continuous demonstration is preferred over frequent
pauses for demonstration requests 
the performance of each learning algorithm was evaluated each time     new demonstrations were acquired  for each evaluation  the agent drove for      timesteps over a road
segment with a fixed and consistent traffic pattern  this road segment was not used for
training  instead each algorithm was trained using a randomly generated car traffic pattern 
since the algorithm aims to imitate the behavior of the expert  no true reward function
exists to evaluate the performance of a given policy  we present two domain specific evaluation metrics that capture the key characteristics of the driving task  our first evaluation
metric is the agents lane preference  or the proportion of the time the agent spends in each
lane over the course of a trial  this metric provides an estimate of the similarity in driving
styles  since the demonstrated behavior attempts to navigate the domain without collisions 
our second evaluation metric is the number of collisions caused by the agent  collisions are
measured as the percentage of the total timesteps that the agent spends in contact with
another car  always driving straight and colliding with every car in the middle lane results
in a     collision rate 
    experimental results
we present the performance evaluation and comparison of the following demonstration
selection techniques 
 t g  teacher guided  all demonstrations selected by the teacher without any confidence feedback from the algorithm and without the ability to perform retroactive
corrections
 ces  confident execution  all demonstrations selected by the agent using a single
fixed confidence threshold
 cem  confident execution  all demonstrations selected by the agent using multiple
adjustable confidence thresholds
 cd  corrective demonstration  all demonstrations selected by the teacher and performed as corrections in response to mistakes made by the agent
 cba  the complete confidence based autonomy algorithm combining confident
execution using multiple adjustable confidence thresholds with corrective demonstration
for each demonstration selection method  the underlying policy of the agent was learned
using multiple gaussian mixture models  one for each action class  chernova   veloso 
       videos of the driving task are available at www cs cmu edu soniac 
figure   presents performance results of the five algorithms with respect to the above
defined lane preference and collision metrics  we describe and discuss all elements of the
  

fichernova   veloso

figure    evaluation of the agents driving performance at     demonstration intervals for
each of the five demonstration selection methods  the bar graphs indicate the
percentage of time the agent spent in each road lane  values under each bar
indicate the percentage of collision timesteps accrued over the evaluation trial 
the teacher performance bar on the right of the figure shows the teachers driving
lane preference and collision rate over the evaluation road segment  the goal is
for each algorithm to achieve performance similar to that of the teacher 

  

fiinteractive policy learning through confidence based autonomy

figure in detail in the following sections  for each evaluation  the figure presents a bar
representing a composite graph showing the percentage of time spent by the agent in each
lane  the value above the bar indicates the number of demonstrations upon which the
evaluated policy is based  the value below the bar indicates the percentage of incurred
collisions during the evaluation 
the bar on the right of the figure shows the performance of the teacher over the evaluation road segment  this evaluation indicates that the teacher prefers to drive in the center
and left lanes  followed in preference by the left shoulder  right shoulder and right lane  the
teacher also successfully avoids all collisions  resulting in a collision rate of     the goal of
the learning algorithm is to achieve a driving lane pattern similar to that of the teacher and
also without collisions  note that  as described in the previous section  policy learning was
initialized with the same     demonstration dataset for all algorithms  this initialization
results in identical performance across all algorithms for this initial learning segment 
      t g demonstration selection
the top row in figure   summarizes the performance of the teacher guided demonstration
selection approach  in this approach  the teacher performed training by alternating between
observing the performance of the agent and selecting demonstrations that  in her opinion 
would improve driving performance  the teacher selected all training examples without
receiving feedback about action selection confidence  and without the ability to provide
corrective demonstrations for incorrect actions that were already executed by the agent 
instead  the teacher was required to anticipate what data would improve the policy  the
training process was terminated once the teacher saw no further improvement in agent
performance 
figure   shows the results of the agents performance evaluations at     demonstration
intervals throughout the learning process  the similarity in the driving lane preference
of the agent improves slowly over the course of the learning  with significant fluctuations 
for example  after     demonstrations  the agents preference is to drive on the empty left
shoulder  thereby incurring few collisions  one hundred demonstrations later  the policy
has shifted to prefer the center lane  however  the agent has not yet learned to avoid other
cars  resulting in a       collision rate  the policy stabilizes after approximately     
demonstrations  representing a driving style similar to that of the teacher  with a small
number of collisions  without confidence feedback from the agent  it is difficult for the
teacher to select an exact termination point for the learning  training continued until 
after      demonstrations  the learners policy showed little improvement  the final policy
resulted in a lane preference very similar to that of the expert  but with a      collision
rate 
      ces demonstration selection
the second row in figure   presents the results of the confident execution algorithm with
a single autonomy threshold  in this demonstration selection approach  all demonstrations
were selected by the agent and learning terminated once the agent stopped requesting
demonstrations and performed all actions autonomously  the autonomy threshold value
  

fichernova   veloso

was selected by hand and evaluated in multiple performance trials  results of the best fixed
threshold are presented 
compared to the teacher guided approach  the policy learned using the ces algorithm
stabilizes quickly  achieving performance similar to the teachers after only     demonstrations  the number of collisions is again low but persistent  even as the agent gains full
confidence and stops requesting demonstrations after      demonstrations  the final lane
preference was again similar to that of the expert  with a collision rate of      
      cem demonstration selection
the third row in figure   presents the results of the confident execution algorithm with
multiple autonomy thresholds  which were calculated using the algorithm presented in section        of all the demonstration selection methods  cem required the fewest number of
demonstrations to learn the task  completing learning after only     demonstrations  this
result indicates that the use of multiple adjustable thresholds successfully focuses demonstration selection on informative areas of the state space while greatly reducing the number
of redundant demonstrations  throughout the learning process  the number of gaussian
components within the model varied between   and     this large variation highlights the
importance of automating the threshold calculation process  since hand selecting individual
thresholds for each component would be impractical  the lane preference of the final policy
was again similar to that of the expert  however  the agent still maintained a small collision
rate of      
      cd demonstration selection
the evaluation of the first three algorithms highlights the difficulty of the driving problem 
each of the approaches was able to select demonstrations that resulted in a policy that
mimics the overall driving style of the teacher  however  all of the policies resulted in
a small number of collisions  which typically occurred when the agent merged too close
to another vehicle and touched its bumper  such mistakes are difficult to correct using
the techniques evaluated so far  even within the teacher guided demonstration selection
method  in which the human teacher has full control of the demonstration training data 
by the time the collision has been observed the incorrect decision had already been made
by the algorithm  instead  retroactive demonstration is required to correct already made
mistakes  as in the corrective demonstration algorithm 
in the fourth row of figure   we present our evaluation of demonstration selection
using only the corrective demonstration algorithm  in this approach  all demonstrations
were selected by the teacher as corrections in response to mistakes made by the agent 
behavior corrected by the teacher included collisions  as well as incorrect lane preference
 e g  always driving on the shoulder  and rapid oscillations between lanes  to enable the
teacher to accurately perform corrections  the simulation was slowed from   to   frames
per second  learning was terminated once the agent required no further corrections  as
shown in figure    the complete training process using corrective demonstration took    
demonstrations  achieving a final policy that correctly imitates the teachers driving style
with a    collision rate  in the following section  we discuss how this performance compares
to the complete cba algorithm 
  

fiinteractive policy learning through confidence based autonomy

      cba demonstration selection
the final row in figure   presents the evaluation of the complete confidence based autonomy algorithm  which combines cem with cd  using this approach  learning is complete
once the agent no longer requests demonstrations and is able to perform the driving task
without collisions  using cba the agent required a total of     demonstrations to learn
the task  successfully learning to navigate the highway without collisions 
we analyze the impact of the two cba learning components by comparing the number
and distribution of demonstrations acquired by each algorithm during the learning process 
in this section we refer to the learning components of cba as cba ce and cba cd
to differentiate from the algorithm evaluations presented in previous sections  note that
the behavior of the confident execution component is dependent upon the method used
to set the autonomy thresholds  in this evaluation we use multiple adjustable thresholds
calculated as the average value of misclassified points 
in figure   a   each datapoint along the x axis represents the number of demonstrations
requested using cba ce  top  and initiated by the teacher using cba cd  bottom  during
a     timestep interval  or approximately    seconds of simulator runtime  excluding pauses
for demonstration requests   since the first three     demonstration timesteps consist entirely of non interactive demonstration  the values for these timesteps are     and  due to
scaling  exceed the bounds of the graph  figure   b  shows how the cumulative number of
demonstrations for each component  and in total  grows with respect to training time  the
complete training process lasts approximately an hour and a half 
analysis of these graphs shows that most demonstrations occur early in the training
process  importantly  confident execution accounts for     of the total number of demon 

 a 

 b 

figure     a  timeline showing how the number of demonstrations initiated by the agent
through confident execution  top  and initiated by the teacher through corrective demonstrations  bottom  changes over the course of the training   b  the
cumulative number of demonstrations acquired by each component  and in total 
over time 

  

fichernova   veloso

strations  indicating that the agent guides most of the learning  most of these demonstration requests occur during the first few minutes of training when the agent encounters
many novel states and the classification confidence remains low  the agent requires few
corrections during this stage because many mistakes are prevented by requesting a demonstration instead of performing a low confidence action  corrective demonstration plays its
greatest role towards the end of training process  where it accounts for     of the final
    demonstrations  at this stage in the learning the agents action selection confidence is
high enough that it rarely asks for demonstrations  its policy already closely imitates the
teachers driving style but a small number of collisions remain  corrective demonstration
enables the teacher to fine tune the policy and eliminate all collisions  this result highlights
the importance of corrective demonstration  whether alone or in conjunction with another
selection technique  for optimizing policy performance 
while cba achieves similar final performance compared to the cd algorithm evaluated
in the previous section  it requires approximately     additional demonstrations to learn this
policy  the additional demonstrations can be attributed to confident execution demonstration requests that served to increase the classification confidence but did not change the
outcome of the agents action  viewed another way  these datapoints correspond to states
in which the agent would have performed the correct action even if it had not asked for a
demonstration  from this result it appears that allowing the agent to make mistakes and
correcting them after the fact  as done in the cd evaluation  may be the best demonstration
selection approach with respect to the performance metrics defined above and the overall
number of demonstrations 
however  eliminating the ability to request demonstrations and utilizing only retroactive correction has several drawbacks  namely requiring constant and full attention from the
teacher  and  most importantly  requiring our agent to make many mistakes before it learns
the correct policy  by comparison  the cba algorithm enables the agent to request demonstrations in low confidence states  thereby avoiding many incorrect actions  our original
lane preference and collision metrics do not take this difference into account as they focus
only on the final policy performance of the agent 
to evaluate the difference between these algorithms  we additionally examine the number
of collisions each agent incurs over the course of the learning  using the cd algorithm 
the agent incurs     more collisions      vs       during training than by using cba 
therefore  by allowing the agent to request demonstrations in low confidence states  the
cba algorithm requires a slightly greater number of demonstrations while greatly reducing
the number of incorrect actions performed during learning  the reduction in the number
of action errors is significant due to its importance for many learning domains  especially
robotic applications in which such errors may pose dangers to the system 
in summary  our evaluation has shown that the ability to retroactively correct mistakes
is crucial to optimizing the policy and eliminating all collisions  the best performance
was achieved by the corrective demonstration and confidence based autonomy methods 
with cd requiring fewer demonstrations but incurring a greater number of collisions during
training  the choice between cd and cba can therefore be viewed as a tradeoff between
the number of demonstrations and the frequency of undesired actions during training  in
fact  cd is a special case of cba in which the autonomy threshold is set to classify all
points with high confidence  adjusting the selectiveness of the cba autonomy thresholds
  

fiinteractive policy learning through confidence based autonomy

could  therefore  provide the user with a sliding control mechanism that effects the agents
tendency to perform autonomous actions versus demonstration requests  importantly  we
note that the overall number of demonstrations required by either approach is less than the
teacher guided method and only a tiny fraction of the overall state space 

   discussion
in this section  we discuss several promising directions for future work  as well as a number
of existing extensions to the presented learning methods 
    evaluation with non technical users
the presented demonstration learning algorithm provides a fast and intuitive method for
programming and adapting the behavior of autonomous agents  we believe that its general
representation and classifier independent approach makes cba usable for a wide range of
applications  one particular application of interest is the use of demonstration learning to
enable non technical users to program autonomous agents  we believe that cba would be
highly suitable for this application as it does not assume that the teacher has any technical
knowledge about policy learning  requiring only that the teacher be an expert at the task 
the results presented in this article were obtained using only a single teacher  one of the
authors  additional studies could evaluate algorithm usability and performance for a wider
user base  and non programmers in particular 
    representation of action choices
demonstration based learning provides a natural and intuitive interface for transferring human task knowledge to autonomous agents  however  when operating in rich environments 
agents inevitably face situations in which multiple actions are equivalently applicable  for
example  an agent that encounters an obstacle directly in its path has the option of moving
left or right to avoid it  if the surrounding space is empty  both directions are equally valid
for performing the desired task  human demonstrators faced with a choice of equivalent
actions typically do not perform demonstrations consistently  instead selecting among the
applicable actions arbitrarily each time the choice is encountered  as a result  training
data obtained by the agent lacks consistency  such that identical  or nearly identical  states
are associated with different actions  in the presented cba algorithm  such inconsistent
demonstrations would result in a persistent region of low confidence  leading the agent to
repeatedly request demonstrations within the inconsistent domain region  we have successfully extended cba to identify regions of the state space with conflicting demonstrations
and represent the choice between multiple actions explicitly within the agents policy  chernova   veloso      a  
    improvement beyond teacher performance
the policy learned by the confidence based autonomy algorithm is inherently limited by
the quality of the demonstrations provided by the human teacher  assuming that the
teacher is an expert at the task  our approach aims to imitate the behavior of the teacher 
however  in many domains teacher demonstrations may be suboptimal and limited by
  

fichernova   veloso

human ability  several demonstration learning approaches have been developed that enable
an agent to learn from its own experiences in addition to demonstrations  thereby improving
performance beyond the abilities of the teacher  stolle   atkeson        smart        
extending the cba algorithm to include similar capability remains a promising direction
for future work  possible approaches include incorporating a high level feedback  argall 
browning    veloso        or reward signal  thomaz   breazeal        from the teacher 
as well as filtering noisy or inaccurate demonstrations 
    policy use after learning
the cba algorithm considers learning to be complete once the agent is able to perform
the required behavior  repeatedly and correctly  without requesting further demonstrations
and requiring corrections  once policy learning is complete  the standard procedure for the
vast majority of policy learning algorithms is to turn off the learning process and freeze
the policy  while this approach can also be used with our algorithm  we propose that
the continuing use of the confident execution component may have long term benefits
beyond policy learning  in particular  the algorithms ability to identify anomalous states
may enable the agent to detect and notify the user of system errors and unexpected input 
while further studies are needed to evaluate this use of the algorithm  we believe that such
a mechanism would provide a useful safety feature for long term autonomous operation at
a negligible cost of performing the threshold comparison at each timestep 
    richer interaction
the presented demonstration learning approach relies on a limited form of interaction between the agent and teacher  the agent requests demonstrations from the teacher  while
the teacher responds with a single recommended action  while this level of interaction
is typical of traditional active learning approaches  it fails to take full advantage of the
vast task knowledge that the teacher possesses  we believe that extending the algorithm
to include richer interaction abilities could provide a faster and more intuitive training
method  many promising directions for future research exist in this area  for example 
developing a domain independent dialog exchange between the agent and teacher that incorporates clarification questions and high level advice could speed up learning and enable
the agent to represent the high level goals of the task  the ability to play back or rewind
demonstration sequences would additionally enable both teacher and agent to reexamine
and reevaluate past learning experiences 
    application to single robot and multi robot systems
learning from demonstration techniques have been extensively studied within the robotics
community due to their interactive nature and fast learning times  in other work  we have
shown the cba algorithm to be highly effective in learning a variety of single robot tasks
 chernova   veloso            a  
furthermore  many complex tasks require the collaboration of multiple robots  up
to now  one of the greatest challenges preventing most demonstration learning algorithms
from generalizing to multi robot domains has been the problem of limited human attention 
  

fiinteractive policy learning through confidence based autonomy

the fact that the teacher is not able to pay attention to  and interact with  all robots at
the same time  based on the cba algorithm  we have developed the first multi robot
demonstration learning system that addresses the limited human attention problem by
taking advantage of the fact that the confident execution component of cba prevents the
autonomous execution of actions in low confidence states  chernova   veloso      b   our
flexmlfd system utilizes individual instances of cba for each robot  such that each learner
acquires a unique set of demonstrations and learns an individual task policy  by preventing
autonomous execution in low confidence states  cba makes each learner robust to periods
of teacher neglect  allowing multiple robots to be taught at the same time 

   conclusion
in this article we presented confidence based autonomy  an interactive algorithm for policy
learning through demonstration  using this algorithm  an agent incrementally learns an
action policy from demonstrations acquired as it practices the task  the cba algorithm
contains two methods for obtaining demonstrations  the confident execution component
enables the agent to select demonstrations in real time as it interacts with the environment 
using confidence and distance thresholds to target states that are unfamiliar or in which
the current policy action is uncertain  the corrective demonstration component allows
the teacher to additionally perform corrective demonstrations when an incorrect action is
selected by the agent  the teacher retroactively provides demonstrations for specific error
cases instead of attempting to anticipate errors ahead of time  combined  these techniques
provide a fast and intuitive approach for policy learning  incorporating shared decision
making between the learner and the teacher 
experimentally  we used a complex simulated driving domain to compare five methods
of selecting demonstration training data  manual data selection by the teacher  confidencebased selection using a single fixed threshold  confidence based selection using multiple
automatically calculated thresholds  corrective demonstration  and confidence based selection combined with corrective demonstration  based on our evaluation  we conclude that
all confidence based methods were able to select more informative demonstrations than the
human teacher  of the single and multiple threshold approaches  the multiple adjustable
threshold technique required significantly fewer demonstrations by focusing onto regions of
uncertainty and reducing the number of redundant datapoints  the best final policy performance  however  was achieved by the corrective demonstration and complete confidencebased autonomy algorithms  both of which achieved a lane preference similar to that of the
teacher without any collisions  together  these demonstration selection algorithms represent
the tradeoff between the number of demonstrations and the frequency of undesired actions
during training  while corrective demonstration required slightly fewer demonstrations to
learn the final policy  compared to cba it resulted in a significant increase in the number
of errors made by the agent over the course of the learning process  the cba algorithm 
therefore  provides the best demonstration selection method for domains in which incorrect
actions are not desirable during the training process 
  

fichernova   veloso

acknowledgments
this research was partially sponsored by the department of the interior  national business
center under contract no  nbchd       and sri international under subcontract no 
           and by bbnt solutions under subcontract no             via prime air force
contract no  sa         c       the views and conclusions contained in this document
are those of the authors and should not be interpreted as representing the official policies 
either expressed or implied  of any sponsoring institution  the u s  government or any other
entity  additional thanks to paul rybski for making his simulation package available 

references
abbeel  p     ng  a          apprenticeship learning via inverse reinforcement learning 
in proceedings of the international conference on machine learning  new york  ny 
usa  acm press 
argall  b   chernova  s   browning  b     veloso  m          a survey of robot learning
from demonstration  robotics and autonomous systems  to appear 
argall  b   browning  b     veloso  m          learning from demonstration with the critique of a human teacher  in second annual conference on human robot interactions
 hri      arlington  virginia 
atkeson  c  g     schaal  s          robot learning from demonstration  in proceedings
of the international conference on machine learning  pp        san francisco  ca 
usa  morgan kaufmann publishers inc 
bentivegna  d  c   ude  a   atkeson  c  g     cheng  g          learning to act from
observation and practice  international journal of humanoid robotics        
blum  a  l     langley  p          selection of relevant features and examples in machine
learning  artificial intelligence                   
browning  b   xu  l     veloso  m          skill acquisition and use for a dynamicallybalancing soccer robot  in proceedings of nineteenth national conference on artificial
intelligence  pp         
chernova  s     veloso  m          confidence based policy learning from demonstration
using gaussian mixture models  in proceedings of the international conference on
autonomous agents and multiagent systems  pp     
chernova  s     veloso  m       a   learning equivalent action choices from demonstration 
in in proceedings of the international conference on intelligent robots and systems 
pp           
chernova  s     veloso  m       b   teaching collaborative multi robot tasks through
demonstration  in proceedings of the ieee ras international conference on humanoid robots 
clouse  j  a          on integrating apprentice learning and reinforcement learning  ph d 
thesis  university of massachisetts  department of computer science 
cohn  d   atlas  l     ladner  r          improving generalization with active learning 
machine learning                 
  

fiinteractive policy learning through confidence based autonomy

grollman  d     jenkins  o          dogged learning for robots  in ieee international
conference on robotics and automation  pp           
inamura  t   inaba  m     inoue  h          acquisition of probabilistic behavior decision model based on the interactive teaching method  in proceedings of the ninth
international conference on advanced robotics  pp         
lockerd  a     breazeal  c          tutelage and socially guided robot learning  in proceedings of the ieee rsj international conference on intelligent robots and systems 
pp           
nicolescu  m  n          a framework for learning from demonstration  generalization and
practice in human robot domains  ph d  thesis  university of southern california 
papudesi  v          integrating advice with reinforcement learning  masters thesis  university of texas at arlington 
price  b     boutilier  c          accelerating reinforcement learning through implicit
imitation   journal of artificial intelligence research             
saunders  j   nehaniv  c  l     dautenhahn  k          teaching robots by moulding behavior and scaffolding the environment  in proceeding of the  st acm sigchi sigart
conference on human robot interaction  pp          new york  ny  usa  acm
press 
schaal  s          learning from demonstration  in advances in neural information processing systems  pp            mit press 
smart  w  d          making reinforcement learning work on real robots  ph d  thesis 
department of computer science  brown university  providence  ri 
stolle  m     atkeson  c  g          knowledge transfer using local features  in proceedings of ieee international symposium on approximate dynamic programming and
reinforcement learning  pp       
sutton  r     barto  a          reinforcement learning  an introduction  mit press 
cambridge  ma 
takahashi  y   hikita  k     asada  m          a hierarchical multi module learning system
based on self interpretation of instructions by coach  in proceedings of robocup      
robot soccer world cup vii  pp          
thomaz  a  l     breazeal  c          reinforcement learning with human teachers  evidence of feedback and guidance with implications for learning performance  in proceedings of the twenty first conference on artificial intelligence  pp           

  

fi