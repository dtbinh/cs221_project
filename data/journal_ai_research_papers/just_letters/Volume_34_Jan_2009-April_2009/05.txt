journal of artificial intelligence research                  

submitted        published      

sentence compression as tree transduction
trevor cohn

tcohn inf ed ac uk

mirella lapata

mlap inf ed ac uk

school of informatics
university of edinburgh
   crichton street edinburgh eh    ab  uk

abstract
this paper presents a tree to tree transduction method for sentence compression  our
model is based on synchronous tree substitution grammar  a formalism that allows local
distortion of the tree topology and can thus naturally capture structural mismatches  we
describe an algorithm for decoding in this framework and show how the model can be
trained discriminatively within a large margin framework  experimental results on sentence
compression bring significant improvements over a state of the art model 

   introduction
recent years have witnessed increasing interest in text to text generation methods for many
natural language processing applications  ranging from text summarisation to question answering and machine translation  at the heart of these methods lies the ability to perform
rewriting operations  for instance  text simplification identifies which phrases or sentences
in a document will pose reading difficulty for a given user and substitutes them with simpler alternatives  carroll  minnen  pearce  canning  devlin    tait        chandrasekar  
srinivas         in question answering  questions are often paraphrased in order to achieve
more flexible matching with potential answers  lin   pantel        hermjakob  echihabi 
  marcu         another example concerns reformulating written language so as to render
it more natural sounding for speech synthesis applications  kaji  okamoto    kurohashi 
      
sentence compression is perhaps one of the most popular text to text rewriting methods 
the aim is to produce a summary of a single sentence that retains the most important
information while remaining grammatical  jing         the appeal of sentence compression
lies in its potential for summarization and more generally for document compression  e g   for
displaying text on small screens such as mobile phones or pdas  vandeghinste   pan 
       much of the current work in the literature focuses on a simplified formulation of the
compression task which does not allow any rewriting operations other than word deletion 
given an input source sentence of words x   x    x            xn   a target compression y is formed
by removing any subset of these words  knight   marcu        
despite being restricted to word deletion  the compression task remains challenging from
a modeling perspective  figure   illustrates a source sentence and its target compression
taken from one of the compression corpora used in our experiments  see section   for details  
in this case  a hypothetical compression system must apply a series of rewrite rules in order
c
    
ai access foundation  all rights reserved 

ficohn   lapata

s
s

s

s

s

s

s

vp
whnp
rb

wp

exactly what

np

vp
np

nns

whnp

vbd prp cc

records

wp

np

whnp

np

vbn

wp

nns

vbp

vbn

involved

what

records

are

involved

nns vbp

made it and which ones are

vp
vp

 a  source

vp

 b  target

figure    example of sentence compression showing the source and target trees  the bold
source nodes show the terminals that need to be removed to produce the target
string 

whnp
rb

s

whnp

wp

wp

np

s

np
np

vp



   



   

   

s
s

vp

whnp s

vp
vp

s
whnp

cc

s
s

whnp

s and

s
np

vp


   

   

figure    example transduction rules  each displayed as a pair of tree fragments  the left
 source  fragment is matched against a node in the source tree  and the matching
part is then replaced by the right  target  fragment  dotted lines denote variable
correspondences  and  denotes node deletion 

to obtain the target  e g   delete the leaf nodes exactly and and  delete the subtrees made it
and which ones  and merge the subtrees corresponding to records and are involved  more
concretely  the system must have access to rules like those shown in figure    the rules
are displayed as a pair of tree fragments where the left fragment corresponds to the source
and the right to the target  for instance  rule     states that a wh noun phrase  whnp 
consisting of an adverb  rb  and a wh pronoun  wp   e g   exactly what  can be rewritten
as just a wh pronoun  without the adverb   there are two things to note here  first 
syntactic information plays an important role  since deletion decisions are not limited to
individual words but often span larger constituents  secondly  there can be a large number
of compression rules of varying granularity and complexity  see rule     in figure    
previous solutions to the compression problem have been cast mostly in a supervised
learning setting  for unsupervised methods see clarke   lapata        hori   furui       
turner   charniak         sentence compression is often modeled in a generative framework
   

fisentence compression as tree transduction

where the aim is to estimate the joint probability p  x  y  of source sentence x having
the target compression y  knight   marcu        turner   charniak        galley  
mckeown         these approaches essentially learn rewrite rules similar to those shown
in figure   from a parsed parallel corpus and subsequently use them to find the best
compression from the set of all possible compressions for a given sentence  other approaches
model compression discriminatively as subtree deletion  riezler  king  crouch    zaenen 
      nguyen  horiguchi  shimazu    ho        mcdonald        
despite differences in formulation  existing models are specifically designed with sentence compression in mind and are not generally applicable to other tasks requiring more
complex rewrite operations such as substitutions  insertions  or reordering  a common
assumption underlying previous work is that the tree structures representing the source
sentences and their target compressions are isomorphic  i e   there exists an edge preserving
bijection between the nodes in the two trees  this assumption is valid for sentence compression but does not hold for other rewriting tasks  consequently  sentence compression
models are too restrictive  they cannot be readily adapted to other generation problems
since they are not able to handle structural and lexical divergences  a related issue concerns the deletion operations themselves which often take place without considering the
structure of the target compression  the goal is to generate a compressed string rather than
the tree representing it   without a syntax based language model  turner   charniak 
      or an explicit generation mechanism that licenses tree transformations there is no
guarantee that the compressions will have well formed syntactic structures  and it will not
be straightforward to process them for subsequent generation or analysis tasks 
in this paper we present a sentence compression model that is not deletion specific but
can account for ample rewrite operations and scales to other rewriting tasks  we formulate
the compression problem as tree to tree rewriting using a synchronous grammar  with rules
like those shown in figure     specifically  we adopt the synchronous tree substitution
grammar  stsg  formalism  eisner        which can model non isomorphic tree structures
while having efficient inference algorithms  we show how such a grammar can be induced
from a parallel corpus and propose a discriminative model for the rewriting task which
can be viewed as a weighted tree to tree transducer  our learning framework makes use of
the large margin algorithm put forward by tsochantaridis  joachims  hofmann  and altun
       which efficiently learns a prediction function to minimize a given loss function  we
also develop an appropriate algorithm that can be used in both training  i e   learning the
model weights  and decoding  i e   finding the most plausible compression under the model  
beyond sentence compression  we hope that some of the work described here might be of
relevance to other tasks involving structural matching  see the discussion in section    
the remainder of this paper is structured as follows  section   provides an overview
of related work  section   presents the stsg framework and the compression model we
employ in our experiments  section   discusses our experimental set up and section  
presents our results  discussion of future work concludes the paper 

   related work
synchronous context free grammars  scfgs  aho   ullman        are a generalization
of the context free grammar  cfg  formalism to simultaneously produce strings in two
   

ficohn   lapata

languages  they have been used extensively in syntax based statistical mt  examples
include inversion transduction grammar  wu         head transducers  alshawi  bangalore 
  douglas         hierarchical phrase based translation  chiang         and several variants
of tree transducers  yamada   knight        grael   knight        
sentence compression bears some resemblance to machine translation  instead of translating from one language into another  we are translating long sentences into shorter ones
within the same language  it is therefore not surprising that previous work has also adopted
scfgs for the compression task  specifically  knight and marcu        proposed a noisychannel formulation of sentence compression  their model consists of two components  a
language model p  y  whose role is to guarantee that the compression output is grammatical and a channel model p  x y  capturing the probability that the source sentence x is
an expansion of the target compression y  their decoding algorithm searches for the compression y which maximizes p  y p  x y   the channel model is a stochastic scfg  the
rules of which are extracted from a parsed parallel corpus and their weights estimated using
maximum likelihood  galley and mckeown        show how to obtain improved scfg
probability estimates through markovization  turner and charniak        note that scfg
rules are not expressive enough to model structurally complicated compressions as they
are restricted to trees of depth    they remedy this by supplying their synchronous grammar with a set of more general special rules  for example  they allow rules of the form
hnp npi  h np np   cc np      np   i  boxed subscripts are added to distinguish between
the two nps  
our own work formulates sentence compression in the framework of synchronous treesubstitution grammar  stsg  eisner         stsg allows to describe non isomorphic tree
pairs  the grammar rules can comprise trees of arbitrary depth  and is thus suited to textrewriting tasks which typically involve a number of local modifications to the input text 
especially if each modification can be described succinctly in terms of syntactic transformations  such as dropping an adjectival phrase or converting a passive verb phrase into active
form  stsg is a restricted version of synchronous tree adjoining grammar  stag  shieber
  schabes        without an adjunction operation  stag affords mild context sensitivity 
however at increased cost of inference  scfg and stsg are weakly equivalent  that is  their
string languages are identical but they do not produce equivalent tree pairs  for example 
in figure    rules        can be expressed as scfg rules  but rule     cannot because
both the source and target fragments are two level trees  in fact it would be impossible to
describe the trees in figure   using a scfg  our grammar rules are therefore more general
than those obtained by knight and marcu        and can account for more elaborate tree
divergences  moreover  by adopting a more expressive grammar formalism  we can naturally model syntactically complex compressions without having to specify additional rules
 as in turner   charniak        
a synchronous grammar will license a large number of compressions for a given source
tree  each grammar rule typically has a score from which the overall score of a compression y for sentence x can be derived  previous work estimates these scores generatively as
discussed above  we opt for a discriminative training procedure which allows for the incorporation of all manner of powerful features  we use the large margin technique proposed
by tsochantaridis et al          the framework is attractive in that it supports a configurable loss function  which describes the extent to which a predicted target tree differs from
   

fisentence compression as tree transduction

the reference tree  by devising suitable loss functions the model can be straightforwardly
adapted to text rewriting tasks besides sentence compression 
mcdonald        also presents a sentence compression model that uses a discriminative
large margin algorithm  the model has a rich feature set defined over compression bigrams
including parts of speech  parse trees  and dependency information  without however making explicit use of a synchronous grammar  decoding in this model amounts to finding the
combination of bigrams that maximize a scoring function defined over adjacent words in
the compression and the intervening words which were dropped  our model differs from
mcdonalds in two important respects  first  we can capture more complex tree transformations that go beyond bigram deletion  being tree based  our decoding algorithm is
better able to preserve the grammaticality of the compressed output  second  the treebased representation allows greater modeling flexibility  e g   by defining a wide range of
loss functions over the tree or its string yield  in contrast  mcdonald can only define loss
functions over the final compression 
although the bulk of research on sentence compression relies on parallel corpora for
modeling purposes  a few approaches use no training data at all or a small amount  an
example is in the work of hori and furui         who propose a model for automatically
transcribed spoken text  their method scores candidate compressions using a language
model combined with a significance score  indicating whether a word is topical or not  
and a score representing the speech recognizers confidence in transcribing a given word
correctly  despite being conceptually simple and knowledge lean  their model operates at
the word level  since it does not take syntax into account  it has no means of deleting
constituents spanning several subtrees  e g   relative clauses   clarke and lapata       
show that such unsupervised models can be greatly improved when linguistically motivated
constraints are used during decoding 

   problem formulation
as mentioned earlier  we formulate sentence compression as a tree to tree rewriting problem
using a weighted synchronous grammar coupled with a large margin training process  our
model learns from a parallel corpus of input  uncompressed  and output  compressed  pairs
 x    y              xn   yn   to predict a target labeled tree y from a source labeled tree x  we
capture the dependency between x and y as a weighted stsg which we define in the
following section  section     discusses how we extract such a grammar from a parallel
corpus  each rule has a score  as does each ngram in the output tree  from which the
overall score of a compression y for sentence x can be derived  we introduce our scoring
function in section     and explain our training algorithm in section      in this framework
decoding amounts to finding the best target tree licensed by the grammar given a source
tree  we present a chart based decoding algorithm in section     
    synchronous grammar
a synchronous grammar defines a space of valid source and target tree pairs  much as a
regular grammar defines a space of valid trees  synchronous grammars can be treated as tree
transducers by reasoning over the space of possible sister trees for a given tree  that is  all
the trees which can be produced alongside the given tree  this is essentially a transducer
   

ficohn   lapata

algorithm   generative process for creating a pair of trees 
initialize source tree  x   rs
initialize target tree  y   rt
initialize stack of frontier nodes  f     rs   rt   
for all node pairs   vs   vt    f do
choose a rule hvs   vt i  h    i
rewrite node vs in x as 
rewrite node vt in y as 
for all variables  u   do
find aligned child nodes   cs   ct    under vs and vt corresponding to u
push  cs   ct   on to f
end for
end for
x and y are now complete

which takes a tree as input and produces a tree as output  the grammar rules specify
the steps taken by the transducer in recursively mapping tree fragments of the input tree
into fragments in the target tree  from the many families of synchronous grammars  see
section     we elect to use a synchronous tree substitution grammar  stsg   this is one
of the simpler formalisms  and consequently has efficient inference algorithms  while still
being complex enough to model a rich suite of tree edit operations 
a stsg is a   tuple  g    ns   nt   s   t   p  rs   rt   where n are the non terminals
and  are the terminals  with the subscripts s and t indicating source and target respectively  p are the productions and rs  ns and rt  nt are the distinguished root symbols 
each production is a rewrite rule for two aligned non terminals x  ns and y  nt in the
source and target 
hx  y i  h    i
   
where  and  are elementary trees rooted with the symbols x and y respectively  note
that a synchronous context free grammar  scfg  limits  and  to one level elementary
trees  but is otherwise identical to a stsg  which imposes no such limits  non terminal
leaves of the elementary trees are referred to as frontier nodes or variables  these are the
points of recursion in the transductive process  a one to one alignment between the frontier
nodes in  and  is specified by   the alignment can represent deletion  or insertion  by
aligning a node with the special  symbol  which indicates that the node is not present in
the other tree  only nodes in  can be aligned to   which allows for subtrees to be deleted
during transduction  we disallow the converse   aligned nodes in   as these would license
unlimited insertion in the target tree  independently of the source tree  this capability
would be of limited use for sentence compression  while also increasing the complexity of
inference 
the grammar productions can be used in a generative setting to produce pairs of trees 
or in a transductive setting to produce a target tree when given a source tree  algorithms  
and   present pseudo code for both processes  the generative process  algorithm    starts
with the two root symbols and applies a production which rewrites the symbols as the
productions elementary trees  these elementary trees might contain frontier nodes  in
   

fisentence compression as tree transduction

algorithm   the transduction of a source tree into a target tree 
require  complete source tree  x  with root node labeled rs
initialize target tree  y   rt
initialize stack of frontier nodes  f     root x   rt   
for all node pairs   vs   vt    f do
choose a rule hvs   vt i  h    i where  matches the sub tree rooted at vs in x
rewrite vt as  in y
for all variables  u   do
find aligned child nodes   cs   ct    under vs and vt corresponding to u
push  cs   ct   on to f
end for
end for
y is now complete

which case the aligned pairs of frontier nodes are pushed on to the stack  and later rewritten
using another production  the process continues in a recursive fashion until the stack is
empty  there are no frontier nodes remaining   at which point the two trees are complete 
the sequence of rewrite rules are referred to as a derivation  from which the source and
target tree can be recovered deterministically 
our model uses a stsg in a transductive setting  where the source tree is given and it
is only the target tree that is generated  this necessitates a different rewriting process  as
shown in algorithm    we start with the source tree  and rt   the target root symbol  which
is aligned to the root node of the source  denoted root x   then we choose a production to
rewrite the pair of aligned non terminals such that the productions source side    matches
the source tree  the target symbol is then rewritten using   for each variable in  the
matching node in the source and its corresponding leaf node in the target tree are pushed
on to the stack for later processing   the process repeats until the stack is empty  and
therefore the source tree has been covered  we now have a complete target tree  as before
we use the term derivation to refer to this sequence of production applications  the target
string is the yield of the target tree  given by reading the non terminals from the tree in a
left to right manner 
let us consider again the compression example from figure    the tree editing rules from
figure   are encoded as stsg productions in figure    see rules          production     
reproduces tree pair     from figure    production     tree pair      and so on  the notation
in figure    primarily for space reasons  uses brackets      to indicate constituent boundaries 
brackets surround a constituents non terminal and its child nodes  which can each be
terminals  non terminals or bracketed subtrees  the boxed indices are short hand notation
for the alignment    for example  in rule     they specify that the two wp non terminals
are aligned and the rb node occurs only in the source tree  i e   heads a deleted subtree   the grammar rules allow for differences in non terminal category between the source
and target  as seen in rules         they also allow arbitrarily deep elementary trees 
   special care must be taken for  aligned variables  nodes in  which are  aligned signify that the source
sub tree below this point can be deleted without affecting the target tree  for this reason we can safely
ignore source nodes deleted in this manner 

   

ficohn   lapata

   
   
   
   
   
   
   
   
   
    
    
    

hwhnp  whnpi
hs  npi
hs  vpi
hs  vpi
hs  si
hwp  wpi
hnp  npi
hnns  nnsi
hvp  vpi
hvbp  vbpi
hvp  vpi
hvbn  vbni

rules which perform major tree edits
 h whnp rb  wp       whnp wp    i
 h s np   vp     np   i
 h s np  vp      vp   i
 h s whnp  s      vp   i
 h s  s whnp   s      cc and  s       s whnp    s np   vp     i
rules which preserve the tree structure
 h wp what    wp what i
 h np nns       np nns    i
 h nns records    nns records i
 h vp vbp   vp       vp vbp   vp    i
 h vbp are    vbp are i
 h vp vbn       vp vbn    i
 h vbn involved    vbn involved i

figure    the rules in a synchronous tree substitution grammar  stsg  capable of generating the sentence pair from figure    equivalently  this grammar defines a
transducer which can convert the source tree  figure   a   into the target tree
 figure   b    each rule rewrites a pair of non terminals into a pair of subtrees 
shown in bracketed notation 

as evidenced by rule     which is has trees of depth two  rules         complete the
toy grammar which describes the tree pair from figure    these rules copy parts of the
source tree into the target  be they terminals  e g   rule      or internal nodes with children
 e g   rule      
figure   shows how this grammar can be used to transduce the source tree into the
target tree from figure    the first few steps of the derivation are also shown graphically in figure    we start with the source tree  and seek to transduce its root symbol
into the target root symbol  denoted s s  the first rule to be applied is rule     in figure    its source side      s  s whnp s   cc and  s   matches the root of source tree
and it has the requisite target category  y   s  the matching part of the source tree
is rewritten using the rules target elementary tree      s whnp  s np vp    the three
three variables are now annotated to reflect the category transformations required for each
node  whnp whnp  s np and s vp  the process now continues for the leftmost of these
nodes  labeled whnp whnp  rule      from figure    is then applied  which deletes the
nodes left child  shown as rb   and retains its right child  the subsequent rule completes
the transduction of the whnp node by matching the string exactly  the algorithm continues to visit each variable node and finishes when there are no variable nodes remaining 
resulting in the desired target tree 
    grammar
the previous section outlined the stsg formalism we employ in our sentence compression
model  save one important detail  the grammar itself  for example  we could obtain a
   

fisentence compression as tree transduction

 s s  s  whnp exactly what   s  np records   vp made it   
 cc and   s  whnp which   s  np ones   vp are involved    
 
 s  whnp whnp  rb exactly   wp what    s  s np  np records   vp made it  
 s vp  whnp which   s  np ones   vp are involved     
 
 s  whnp  wp wp what    s  s np  np records   vp made it  
 s vp  whnp which   s  np ones   vp are involved     
 
 s  whnp  wp what    s  s np  np records   vp  vbd made   np  prp it     
 s vp  whnp which   s  np ones   vp  vbp are   vp  vbn involved       
 
 s  whnp  wp what    s  np  nns nns records  
 s vp  whnp which   s  np ones   vp are involved     
 
 s  whnp  wp what    s  np  nns records   
 s vp  whnp which   s  np ones   vp are involved    
 
 s  whnp what   s  np records   s vp  np ones   vp are involved    
 
 s  whnp what   s  np records   vp vp  vp  vbp are   vp  vbn involved      
 
 s  whnp what   s  np records   vp  vbp vbp are   vp vp  vbn involved     
    s  whnp what   s  np records   vp  vbp are   vp vp  vbn involved     
    s  whnp what   s  np records   vp  vbp are   vp  vbn vbn involved     
    s  whnp  wp what    s  np  nns records    vp  vbp are   vp  vbn involved     

figure    derivation of example sentence pair from figure    each line shows a rewrite step 
denoted i where the subscript i identifies which rule was used  the frontier
nodes are shown in bold with x y indicating that symbol x must be transduced
into y in subsequent steps  for the sake of clarity  some internal nodes have been
omitted 

synchronous grammar by hand  automatically from a corpus  or by some combination  our
only requirement is that the grammar allows the source trees in the training set to be
transduced into their corresponding target trees  for maximum generality  we devised an
automatic method to extract a grammar from a parsed  word aligned parallel compression
corpus  the method maps the word alignment into a constituent level alignment between
nodes in the source and target trees  pairs of aligned subtrees are next generalized to create
tree fragments  elementary trees  which form the rules of the grammar 
the first step of the algorithm is to find the constituent alignment  which we define as
the set of source and target constituent pairs whose yields are aligned to one another under
the word alignment  we base our approach on the alignment template method  och   ney 
       which uses word alignments to define alignments between ngrams  called phrases in
the smt literature   this method finds pairs of ngrams where at least one word in one
of the ngrams is aligned to a word in the other  but no word in either ngram is aligned to
a word outside the other ngram  in addition  we require that these ngrams are syntactic
constituents  more formally  we define constituent alignment as 
c     vs   vt      s  t   a  s  y  vs    t  y  vt   

   

   s  t   a   s  y  vs   y t  y  vt     
where vs and vt are source and target tree nodes  subtrees   a     s  t   is the set of word
alignments  pairs of word indices   y    returns the yield span for a subtree  the minimum
and maximum word index in its yield  and y is the exclusive or operator  figure   shows
   

ficohn   lapata

s

s

s

s
s

s
vp

whnp

np

rb

wp

nns

exactly

what

records

vp
np

whnp

vbd prp cc
made

it

and

wp
which

np

vp

nns vbp
ones

are

vbn
involved

s

s

s

s
s
vp

whnp

np

rb

wp

nns

exactly

what

records

whnp

s
np

made

np

vp
whnp

vbd prp cc
it

and

wp
which

np
are

vbn
involved

s

s

s

s
s

np

rb

wp

nns

exactly

what

records

whnp

s
vp

whnp

whnp

vbd prp cc
made

it

and

wp
which

np

vp

nns vbp
ones

s

wp np vp

vp
np

vp

vp

nns vbp
ones

s

are

vbn
involved

figure    graphical depiction of the first two steps of the derivation in figure    the source
tree is shown on the left and the partial target tree on the right  variable nodes
are shown in bold face and dotted lines show their alignment 

the word alignment and the constituent alignments that are licensed for the sentence pair
from figure   
the next step is to generalize the aligned subtree pairs by replacing aligned child subtrees
with variable nodes  for example  in figure   when we consider the pair of aligned subtrees
 s which ones are involved  and  vp are involved   we could extract the rule 
hs vpi  h s  whnp  wp which    s  np  nns ones   vp  vbp are   vp  vbn involved       
 vp  vbp are   vp  vbn involved   i

   

however  this rule is very specific and consequently will not be very useful in a transduction
model  in order for it to be applied  we must see the full s subtree  which is highly unlikely
to occur in another sentence  ideally  we should generalize the rule so as to match many
more source trees  and thereby allow transduction of previously unseen structures  in the
example  the node pairs labeled  vp    vp      vbp  vbp    vp    vp    and  vbn  vbn 
can all be generalized as these nodes are aligned constituents  subscripts added to distinguish
   

fisentence compression as tree transduction

s
s

s
s

s
vp

whnp

np

np

vp
whnp np

vp

rb
wp nns vbd prp cc wp nns vbp vbn
exactly what records made it
and which ones are involved
s
s
vp
whnp np
vp
wp nns vbp vbn
what records are involved

figure    tree pair with word alignments shown as a binary matrix  a dark square indicates
an alignment between the words on its row and column  the overlaid rectangles
show constituent alignments which are inferred from the word alignment 

between the two vp nodes   in addition  the nodes whnp  wp  np and nns in the source
are unaligned  and therefore can be generalized using  alignment to signify deletion  if we
were to perform all possible generalizations for the above example   we would produce the
rule 
hs vpi  h s whnp  s      vp   i
   
there are many other possible rules which can be extracted by applying different legal
combinations of the generalizations  there are    in total for this example  
algorithm   shows how the minimial  most general  rules are extracted   this results
in the minimal set of synchronous rules which can describe each tree pair   these rules are
minimal in the sense that they cannot be made smaller  e g   by replacing a subtree with
a variable  while still honoring the word alignment  figure   shows the resulting minimal
set of synchronous rules for the example from figure    as can be seen from the example 
many of the rules extracted are overly general  ideally  we would extract every rule with
every legal combination of generalizations  however this leads to a massive number of rules
 exponential in the size of the source tree  we address this problem by allowing a limited
number of generalizations to be skipped in the extraction process  this is equivalent to
altering lines   and   in algorithm   to first make a non deterministic decision whether to
match or ignore the match and continue descending the source tree  the recursion depth
limits the number of matches that can be ignored in this way  for example  if we allow one
   where some generalizations are mutually exclusive  we take the highest match in the trees 
   the non deterministic matching step in line   allows the matching of all options individually  this
is implemented as a mutually recursive function which replicates the algorithm state to process each
different match 
   algorithm   is an extension of galley  hopkins  knight  and marcus        technique for extracting a
scfg from a word aligned corpus consisting of  tree  string  pairs 

   

ficohn   lapata

algorithm   extract x  y  a   extracts minimal rules from constituent aligned trees
require  source tree  x  target tree  y  and constituent alignment  a
   initialize source and target sides of rule     x     y
   initialize frontier alignment    
   for all nodes vs    top down do
  
if vs is null aligned then
  
  vs    
  
delete children of a
  
else if vs is aligned to some target node s  then
  
choose target node  vt
 non deterministic choice 
  
call extract vs   vt   a 
   
  vs   vt  
   
delete children of vs
   
delete children of vt
   
end if
    end for
    emit rule hroot    root  i  h    i
level of recursion when extracting rules from the  s  vp  pair from figure    we get the
additional rules 
hs vpi  h s  whnp wp    s      vp   i
hs vpi  h s whnp   s np  vp       vp   i
while at two levels of recursion  we also get 
hs vpi  h s  whnp  wp which   s      vp   i
hs vpi  h s  whnp  wp which    s np  vp       vp   i
hs vpi  h s whnp   s  np nns    vp       vp   i
hs vpi  h s whnp   s np   vp vbd   vp         vbd   vbd    i
compared to rule     we can see that the specialized rules above add useful structure
and lexicalisation  but are still sufficiently abstract to generalize to new sentences  unlike
rule      the number of rules is exponential in the recursion depth  but with fixed a depth
it is polynomial in the size of the source tree fragment  we set the recursion depth to a
small number  one or two  in our experiments 
there is no guarantee that the induced rules will have good coverage on unseen trees 
tree fragments containing previously unseen terminals or non terminals  or even an unseen
sequence of children for a parent non terminal  cannot be matched by any grammar productions  in this case the transduction algorithm  algorithm    will fail as it has no way
of covering the source tree  however  the problem can be easily remedied by adding new
rules to the grammar to allow the source tree to be fully covered   for each node in the
   there are alternative  equally valid  techniques for improving coverage which simplify the syntax trees 
for example  this can be done explicitly by binarizing large productions  e g   petrov  barrett  thibaux 
  klein        or implicitly with a markov grammar over grammar productions  e g   collins        

   

fisentence compression as tree transduction

hs si
hwhnp whnpi
hwp wpi
hs npi
hnp npi
hnns nnsi
hs vpi
hs vpi
hvp vpi
hvbp vbpi
hvp vpi
hvbn vbni














h s  s whnp   s     cc  s       s whnp    s np   vp     i
h whnp rb  wp       whnp wp    i
h wp what    wp what i
h s np   vp     np   i
h np nns       np nns    i
h nns records    nns records i
h s whnp  s      vp   i
h s np  vp      vp   i
h vp vbp   vp       vp vbp   vp    i
h vbp are    vbp are i
h vp vbn       vp vbn    i
h vbn involved    vbn involved i

figure    the minimal set of stsg rules extracted from the aligned trees in figure   

source tree  a rule is created to copy that node and its child nodes into the target tree  for
example  if we see the fragment  np dt jj nn  in the source tree  we add the rule 

hnp npi  h np dt   jj   nn       np dt   jj   nn    i

with these rules  each source node is copied into the target tree  and therefore the transduction algorithm can trivially recreate the original tree  of course  the other grammar
rules can work in conjunction with the copying rules to produce other target trees 
while the copy rules solve the coverage problem on unseen data  they do not solve the
related problem of under compression  this occurs when there are unseen cfg productions
in the source tree and therefore the only applicable grammar rules are copy rules  which
copy all child nodes into the target  none of the child subtrees can be deleted unless the
parent node can itself deleted by a higher level rule  in which case all the children are
deleted  clearly  it would add considerable modelling flexibility to be able to delete some 
but not all  of the children  for this reason  we add explicit deletion rules for each source
cfg production which allow subsets of the child nodes to be deleted in a linguistically
plausible manner 
the deletion rules attempt to preserve the most important child nodes  we measure
importance using the head finding heuristic from collins parser  appendix a  collins 
       collins method finds the single head child of a cfg production using hand coded
tables for each non terminal type  as we desire a set of child nodes  we run the algorithm to
find all matches rather than stopping after the first match  the order in which each match
is found is used as a ranking of the importance of each child  the ordered list of child nodes
is then used to create synchronous rules which retain head    heads             all heads 
   

ficohn   lapata

for the fragment  np dt jj nn   the heads are found in the following order  nn  dt 
jj   therefore we create rules to retain children  nn    dt  nn  and  dt  jj  nn  
hnp npi  h np dt  jj  nn       np nn    i
hnp nni  h np dt  jj  nn      nn   i
hnp npi  h np dt   jj  nn       np dt   nn    i
hnp npi  h np dt   jj   nn       np dt   jj   nn    i
note that when only one child remains  the rule is also produced without the parent node 
as seen in the second rule above 
    linear model
while an stsg defines a transducer capable of mapping a source tree into many possible
target trees  it is of little use without some kind of weighting towards grammatical trees
which have been constructed using sensible stsg productions and which yield fluent compressed target sentences  ideally the model would define a scoring function over target
trees or strings  however we instead operate on derivations  in general  there may be many
derivations which all produce the same target tree  a situation referred to as spurious ambiguity  to fully account for spurious ambiguity would require aggregating all derivations
which produce the same target tree  this would break the polynomial time dynamic program used for inference  rendering inference problem np complete  knight         to this
end  we define a scoring function over derivations 
score d  w    h d   wi

   

where d is a derivation  consisting of a sequence of rules  w are the model parameters 
 is a vector valued feature function and the operator h  i is the inner product  the
parameters  w  are learned during training  described in section     
the feature function    is defined as 
x
x
 d   
 r  source d    
 m  source d  
   
rd

mngrams d 

where r are the rules of a derivation  ngrams d  are the ngrams in the yield of the target
tree and  is a feature function returning a vector of feature values for each rule  note that
the feature function has access to not only the rule  r  but also the source tree  source d  
as this is a conditional model and therefore doing so has no overhead in terms of modeling
assumptions or the complexity of inference 
in the second summand in      m are the ngrams in the yield of the target tree and  is
a feature function over these ngrams  traditional  weighted  synchronous grammars only
allow features which decompose with the derivation  i e   can be expressed using the first
summand in       however  this is a very limiting requirement  as the ngram features
allow the modeling of local coherence and are commonly used in the sentence compression
literature  knight   marcu        turner   charniak        galley   mckeown       
   the derivation  d  fully specifies both the source  x   source d   and the target tree  y   target d  

   

fisentence compression as tree transduction

clarke   lapata        hori   furui        mcdonald         for instance  when deleting
a sub tree with left and right siblings  it is critical to know not only that the new siblings
are in a grammatical configuration  but also that their yield still forms a coherent string 
for this reason  we allow ngram features  specifically the conditional log probability of
an ngram language model  unfortunately  this comes at a price as the ngram features
significantly increase the complexity of inference used for training and decoding 
    decoding
decoding aims to find the best target tree licensed by the grammar given a source tree 
as mentioned above  we deal with derivations in place of target trees  decoding finds the
maximizing derivation  d   of 
d  

argmax

score d  w 

   

d source d  x

where x is the  given  source tree  source d  extracts the source tree from the derivation d
and score is defined in      the maximization is performed over the space of derivations
for the given source tree  as defined by the transduction process shown in algorithm   
the maximization problem in     is solved using the chart based dynamic program
shown in algorithm    this extends earlier inference algorithms for weighted stsgs  eisner        which assume that the scoring function must decompose with the derivation 
i e   features apply to rules but not to terminal ngrams  relaxing this assumption leads to
additional complications and increased time and space complexity  this is equivalent to using as our grammar the intersection between the original grammar and an ngram language
model  as explained by chiang        in the context of string transduction with an scfg 
the algorithm defines a chart  c  to record the best scoring  partial  target tree for each
source node vs and with root non terminal t  the back pointers  b  record the maximizing
rule and store pointers to the child chart cells filling each variable in the rule  the chart is
also indexed by the n    terminals at the left and right edges of the target trees yield to
allow scoring of ngram features   the terminal ngrams provide sufficient context to evaluate
ngram features overlapping the cells boundary when the chart cell is combined in another
rule application  this is the operation performed by the boundary ngrams function on line
     this is best illustrated with an example  using trigram features  n      if a node were
rewritten as  np the fast car  then we must store the ngram context  the fast  fast car  in
its chart entry  similarly  vp skidded to a halt  would have ngram context  skidded to  a
halt   when applying a parent rule  s np vp  which rewrites these two trees as adjacent
siblings we need to find the ngrams on the boundary between the np and vp  these can
be easily retrieved from the two chart cells contexts  we combine the right edge of the np
context  fast car  with the left edge of the vp context  skidded to  to get the two trigrams
fast car skidded and car skidded to  the other trigrams  the fast car  skidded to
a and to a halt  will have already been evaluated in the child chart cells  the new
combined s chart cell is now given the context  the fast  a halt  by taking the left and right
   strictly speaking  only the terminals on the right edge are required for a compression model which would
create the target string in a left to right manner  however  our algorithm is more general in that it
allows reordering rules such as hpp ppi  h pp in   np       pp np   in    i  such rules are required for
most other text rewriting tasks besides sentence compression 

   

ficohn   lapata

algorithm   exact chart based decoding algorithm 
require  complete source tree  x  with root node labeled rs
   let c v  t  l   r be a chart representing the score of the best derivation transducing the
tree rooted at v to a tree with root category t and ngram context l
   let b v  t  l    p  x  nt  l  be the corresponding back pointers  each consisting of
a production and the source node  target category and ngram context for each of the
productions variables
   initialize chart  c         
   initialize back pointers  b         none
   for all source nodes  vs  x  bottom up do
  
for all rules  r   hvs   y i  h    i where  matches the sub tree rooted at vs do
  
let m be the target ngrams wholly contained in 
  
let features vector     r  x     m  x 
  
let l be an empty ngram context
   
let score  q   
   
for all variables  u   do
   
find source child node  cu   under vs corresponding to u
   
let tu be the non terminal for target child node under  corresponding to u
   
choose child chart entry  qu   c cu   tu   lu  
 non deterministic choice of lu  
   
let m  boundary ngrams r  lu  
   
update features        m  x 
   
update ngram context  l  merge ngram context l  lu  
   
update score  q  q   qu
   
end for
   
update score  q  q   h  wi
   
if q   c vs   y  l  then
   
update chart  c vs   y  l   q
   
update back pointers  b vs   y  l    r    cu   tu   lu  u  
   
end if
   
end for
    end for
    find best root chart entry  l  argmaxl c root x   rt   l 
    create derivation  d  by traversing back pointers from b root x   rt   l  

edges of the two child cells  this merging process is performed by the merge ngram context
function on line     finally we add artificial root node to the target tree with n    artificial
start terminals and one end terminal  this allows the ngram features to be applied over
boundary ngrams at the beginning and end of the target string 
the decoding algorithm processes the source tree in a post order traversal  finding the
set of possible trees and their ngram contexts for each source node and inserting these into
the chart  the rules which match the node are processed in lines      the feature vector 
  is calculated on the rule and the ngrams therein  line     and for ngrams bordering child
cells filling the rules variables  line      note that the feature vector only includes those
features specific to the rule and the boundary ngrams  but not those wholly contained in
   

fisentence compression as tree transduction

the child cell  for this reason the score is the sum of the scores for each child cell  line
    and the feature vector and the model weights  line      the new ngram context  l  is
calculated by combining the rules frontier and the ngram contexts of the child cells  line
     finally the chart entry for this node is updated if the score betters the previous value
 lines       
when choosing the child chart cell entry in line     there can be many different entries
each with a different ngram context  lu   this affects the ngram features    and consequently
the ngram context  l  and the score  q  for the rule  the non determinism means that every
combination of child chart entries are chosen for each variable  and these combinations are
then evaluated and inserted into the chart  the number of combinations is the product of
the number of child chart entries for each variable  this can be bounded by o  tt    n  v  
where  tt   is the size of the target lexicon and v is the number of variables  therefore the
asymptotic time complexity of decoding is the o sr tt    n  v   where s are the number
of source nodes and r is the number of matching rules for each node  this high complexity
clearly makes exact decoding infeasible  especially so when either n or v are large 
we adopt a popular approach in syntax inspired machine translation to address this
problem  chiang         firstly  we use a beam search  which limits the number of different
ngram contexts stored in each chart cell to a constant  w   this changes the base in
the complexity term  leading to an improved o srw v   but which is still exponential in
the number of variables  in addition  we use chiangs cube pruning heuristic to further
limit the number of combinations  cube pruning uses a heuristic scoring function which
approximates the conditional log probability from a ngram language model with the logprobability from a unigram model   this allows us to visit the combinations in best first
order under the heuristic scoring function until the beam is filled the beam is then rescored
using the correct scoring function  this can be done cheaply in o w v   time  leading to an
overall time complexity of decoding to o srw v    we refer the interested reader to the
work of chiang        for further details 
    training
we now turn to the problem of how derivations are scored in our model  for a given source
tree  the space of sister target trees implied by the synchronous grammar is often very large 
and the majority of these trees are ungrammatical or poor compressions  it is the job of
the training algorithm to find weights such that the reference target trees have high scores
and the many other target trees licensed by the grammar are given lower scores 
as explained in section     we define a scoring function over derivations  this function
was given in     and      and is reproduced below 
f  d  w   

argmax hw   d i

   

d source d  x

equation     finds the best scoring derivation  d  for a given source  x  under a linear model 
recall that y is a derivation which generates the source tree x and a target tree  the goal
   we use the conditional log probability of an ngram language model as our only ngram feature  in order to
use other ngram features  such as binary identity features for specific ngrams  it would first be advisable to
construct an approximation which decomposes with the derivation for use in the cube pruning heuristic 

   

ficohn   lapata

of the training procedure is to find a parameter vector w which satisfies the condition 
i  d   source d    xi  d    di   hw   di     d i   

   

where xi   di are the ith training source tree and reference derivation  this condition states
that for all training instances the reference derivation is at least as high scoring as any
other derivations  ideally  we would also like to know the extent to which a predicted target
tree differs from the reference tree  for example  a compression that differs from the gold
standard with respect to one or two words should be treated differently from a compression
that bears no resemblance to it  another important factor is the length of the compression 
compressions whose length is similar to the gold standard should be be preferable to longer
or shorter output  a loss function  yi   y  quantifies the accuracy of prediction y with
respect to the true output value yi  
there are a plethora of different discriminative training frameworks which can optimize
a linear model  possibilities include perceptron training  collins         log linear optimisation of the conditional log likelihood  berger  pietra    pietra        and large margin
methods  we base our training on tsochantaridis et al s        framework for learning
support vector machines  svms  over structured output spaces  using the svmstruct implementation   the framework supports a configurable loss function which is particularly
appealing in the context of sentence compression and more generally text to text generation  it also has an efficient training algorithm and powerful regularization  the latter is
is critical for discriminative models with large numbers of features  which would otherwise
over fit the training sample at the expense of generalization accuracy  we briefly summarize
the approach below  for a more detailed description we refer the interested reader to the
work of tsochantaridis et al         
traditionally svms learn a linear classifier that separates two or more classes with the
largest possible margin  analogously  structured svms attempt to separate the correct
structure from all other structures with a large margin  the learning objective for the
structured svm uses the soft margin formulation which allows for errors in the training set
via the slack variables  i  
n

 
cx
min   w     
 i   i   
w   
n

    

i  

i  d   source d    xi  y    di   hw   di     d i   di   d   i
the slack variables  i   are introduced here for each training example  xi and c is a constant
that controls the trade off between training error minimization and margin maximization 
note that slack variables are combined with the loss incurred in each of the linear constraints  this means that a high loss output must be separated by a larger margin than
a low loss output  or have a much larger slack variable to satisfy the constraint  alternatively  the loss function can be used to rescale the slack parameters  in which case the
constraints in      are replaced with hw   di     d i      dii  d    margin rescaling is
theoretically less desirable as it is not scale invariant  and therefore requires the tuning of
an additional hyperparameter compared to slack rescaling  however  empirical results show
   http   svmlight joachims org svm struct html

   

fisentence compression as tree transduction

little difference between the two rescaling methods  tsochantaridis et al          we use
margin rescaling for the practical reason that it can be approximated more accurately than
can slack rescaling by our chart based inference method 
the optimization problem in      is approximated using an algorithm proposed by
tsochantaridis et al          the algorithm finds a small set of constraints from the fullsized optimization problem that ensures a sufficiently accurate solution  specifically  it
constructs a nested sequence of successively tighter relaxation of the original problem using
a  polynomial time  cutting plane algorithm  for each training instance  the algorithm
keeps track of the selected constraints defining the current relaxation  iterating through
the training examples  it proceeds by finding the output that most radically violates a
constraint  in our case  the optimization crucially relies on finding the derivation which is
both high scoring and has high loss compared to the gold standard  this requires finding
the maximizer of 
h d     d   d   hw   di     d i

    

the search for the maximizer of h d  in      can be performed by the decoding algorithm presented in section     with some extensions  firstly  by expanding      to
h d     d   d   h di    wi   h d   wi we can see that the second term is constant with
respect to d  and thus does not influence the search  the decoding algorithm maximizes
the last term  so all that remains is to include the loss function into the search process 
loss functions
which decompose
with the rules or target ngrams in the derivation 
p
p

   r   

 d
 d   d   
nngrams d  n  d   n   can be easily integrated into the
rd r
decoding algorithm  this is done by adding the partial loss  r  d   r    n  d   n  to each
rules score in line    of algorithm    the ngrams are recovered from the ngram contexts in
the same manner used to evaluate the ngram features  
however  many of our loss functions do not decompose with the rules or the ngrams  in
order to calculate these losses the chart must be stratified by the loss functions arguments
 joachims         for example  unigram precision measures the ratio of correctly predicted
tokens to total predicted tokens and therefore its loss arguments are the pair of counts 
 t p  f p    for true and false positives  they are initialized to        and are then updated
for each rule used in a derivation  this equates to checking whether each target terminal is in
the reference string and incrementing the relevant value  the chart is extended  stratified 
to store the loss arguments in the same way that ngram contexts are stored for decoding 
this means that a rule accessing a child chart cell can get multiple entries  each with
different loss argument values as well as multiple ngram contexts  line    in algorithm
    the loss argument for a rule application is calculated from the rule itself and the loss
arguments of its children  this is then stored in the chart and the back pointer list  lines
     in algorithm     although this loss can only be evaluated correctly for complete
derivations  we also evaluate the loss on partial derivations as part of the cube pruning
heuristic  losses with a large space of argument values will be more coarsely approximated
by the beam search  which prunes the number of chart entries to a constant size  for this
reason  we have focused mainly on simple loss functions which have a relatively small space
of argument values  and also use a wide beam during the search      unique items or    
items  whichever comes first  
   

ficohn   lapata

algorithm   find the gold standard derivation for a pair of trees  i e   alignment  
require  source tree  x  and target tree  y
   let c vs   vt    r be a chart representing the maximum number of rules used to align
nodes vs  x and vt  y
   let b vs   vt     p  x  y  be the corresponding back pointers  consisting of a production
and a pair aligned nodes for each of the productions variables
   initialize chart  c       
   initialize back pointers  b       none
   for all source nodes  vs  x  bottom up do
  
for all rules  r   hvs   y i  h    i where  matches the sub tree rooted at vs do
  
for all target nodes  vt  y  matching  do
  
let rule count  j   
  
for all variables  u   do
   
find aligned child nodes   cs   ct    under vs and vt corresponding to u
   
update rule count  j  j   c cs   ct  
   
end for
   
if n greater than previous value in chart then
   
update chart  c vs   vt    j
   
update back pointers  b vs   vt     r    cs   ct  u  
   
end if
   
end for
   
end for
    end for
    if c root x   root y       then
   
success  create derivation by traversing back pointers from b root x   root y  
    end if
in our discussion so far we have assumed that we are given a gold standard derivation  yi
glossing over the issue of how to find it  spurious ambiguity in the grammar means that
there are often many derivations linking the source and target  none of which are clearly
correct  we select the derivation using the maximum number of rules  each of which will be
small  and therefore should provide maximum generality    this is found using algorithm   
a chart based dynamic program similar to the alignment algorithm for inverse transduction
grammars  wu         the algorithm has time complexity o s   r  where s is the size of
the larger of the two trees and r is the number of rules which can match a node 
    loss functions
the training algorithm described above is highly modular and in theory can support a wide
range of loss functions  there is no widely accepted evaluation metric for text compression  a zero one loss would be straightforward to define but inappropriate for our problem 
    we also experimented with other heuristics  including choosing the derivation at random and selecting
the derivation with the maximum or minimum score under the model  all using the same search algorithm
but with a different objective   of these  only the maximum scoring derivation was competitive with the
maximum rules heuristic 

   

fisentence compression as tree transduction

as it would always penalize target derivations that differ even slightly from the reference
derivation  ideally  we would like a loss with a wider scoring range that can discriminate
between derivations that differ from the reference  some of these may be good compressions whereas others may be entirely ungrammatical  for this reason we have developed
a range of loss functions which draw inspiration from various metrics used for evaluating
text to text rewriting tasks such as summarization and machine translation 
loss functions are defined over derivations and can look at any item accessible including
tokens  ngrams and cfg rules  our first class of loss functions calculates the hamming
distance between unordered bags of items  it measures the number of predicted items that
did not appear in the reference  along with a penalty for short output 
hamming  d   d    f p   max  l   t p   f p      

    

where t p and f p are the number of true and false positives  respectively  when comparing
the predicted target  dt   with the reference  dt   and l is the length of the reference  we
include the second term to penalize overly short output as otherwise predicting very little
or nothing would incur no penalty 
we have created three instantiations of the loss function in      over     tokens 
   ngrams  n      and    cfg productions  in each case  the loss argument space is
quadratic in the size of the source tree  our hamming ngram loss is an attempt at defining
a loss function similar to bleu  papineni  roukos  ward    zhu         the latter is
defined over documents rather than individual sentences  and is thus not directly applicable
to our problem  now  since these losses all operate on unordered bags they may reward
erroneous predictions  for example  a permutation of the reference tokens will have zero
token loss  this is less of a problem for the cfg and ngram losses whose items overlap 
thereby encoding a partial order  another problem with the loss functions just described is
that they do not penalize multiply predicting an item that occurred only once in the reference  this could be a problem for function words which are common in most sentences 
therefore we developed two additional loss functions which take multiple predictions into
account  the first measures the edit distance  the number of insertions and deletions 
between the predicted and the reference compressions  both as bags of tokens  in contrast
to the previous loss functions  it requires the true positive counts to be clipped to the
number of occurrences of each type in the reference  the edit distance is given by 
x
edit  d   d    p   r   
min pi   qi  
    
i

where p and q denote the number of target tokens in the predicted tree  target d   and
reference  y   target d    respectively  and pi and qi are the counts for type i  the loss
arguments for the edit distance consist of a vector of counts for each item type in the
reference   pi   i   the space of possible values is exponential in the size of the source tree 
compared to quadratic for the hamming losses  consequently  we expect beam search to
result in many more search errors when using the edit distance loss 
our last loss function is the f  measure  a harmonic mean between precision and recall 
measured over bags of tokens  as with the edit distance  its calculation requires the counts
to be clipped to the number of occurrences of each terminal type in the reference  we
   

ficohn   lapata

ref 
pred 

 s  whnp  wp what    s  np  nns records    vp  vbp are   vp  vbn involved     
 s  whnp  wp what    s  np  nns ones    vp  vbp are   vbn involved    
loss
token hamming
  gram hamming
cfg hamming
edit distance
f 

arguments
t p      f p    
t p      f p    
t p      f p    
p                  
p                  

value
   
    
   
 
   

table    loss arguments and values for the example predicted and reference compressions 
note that loss values should not be compared between different loss functions 
these values are purely illustrative 

therefore use the same loss arguments for its calculation  the f  loss is given by 
f   d   d      
p

min p  q  

   precision  recall
precision   recall
p

    

min p  q  

where precision   i p i i and recall   i q i i   as f  shares the same arguments
with the edit distance loss  it also has the same exponential space of loss argument values
and will consequently be subject to severe pruning during the beam search used in training 
to illustrate the above loss functions  we present an example in table    here  the
prediction  pred  and reference  ref  have the same length    tokens   identical syntactic
structure  but differ by one word  ones versus records   correspondingly  there are three
correct tokens and one incorrect  which forms the arguments for the token hamming loss 
resulting in a loss of      the ngram loss is measured for n    and the start and end of the
string are padded with special symbols to allow evaluation of the boundary ngrams  the
cfg loss records only one incorrect cfg production  the preterminal  nns ones   from the
total of nine productions  the last two losses use the same arguments  a vector with values
for the counts of each reference type  the first four cells correspond to what  records  are
and involved  the last cell records all other types  for the example  the edit distance is two
 one deletion and one insertion  while the f  loss is      precision and recall are both      

   features
our feature space is defined over source trees  x  and target derivations  d  we devised two
broad classes of features  applying to grammar rules and to ngrams of target terminals  we
defined only a single ngram feature  the conditional log probability of a trigram language
model  this was trained on the bnc      million words  using the sri language modeling
toolkit  stolcke         with modified kneser ney smoothing 
for each rule hx y i  h    i  we extract features according to the templates detailed
below  our templates give rise to binary indicator features  except where explicitly stated 
these features perform a boolean test  returning value   when the test succeeds and  
otherwise  an example rule and its corresponding features are shown in table   
   

fisentence compression as tree transduction

type  whether the rule was extracted from the training set  created as a copy rule and or
created as a delete rule  this allows the model to learn a preference for each of the
three sources of grammar rules  see row type in table   
root  the root categories of the source  x  and target  y   and their conjunction  x  y
 see rows root in table    
identity  the source side    target side    and the full rule          this allows the
model to learn weights on individual rules or those sharing an elementary tree  another feature checks if the rules source and target elementary trees are identical     
 see rows identity in table    
unlexicalised identity  the identity feature templates above are replicated for unlexicalised elementary trees  i e   with the terminals removed from their frontiers  see
rows unlexid in table    
rule count  this feature is always    allowing the model to count the number of rules
used in a derivation  see row rule count in table    
word count  counts the number of terminals in   allowing a global preference for
shorter or longer output  additionally  we record the number of terminals in the
source tree  which can be used with the target terminal count to find the number of
deleted terminals  see rows word count in table    
yield  these features compare the terminal yield of the source  y     and target  y    
the first feature checks the identity of two sequences  y     y     we use identity
features for each terminal in both yields  and for each terminal only in the source  see
rows yield in table     we also replicate these feature templates for the sequence of
non terminals on the frontier  pre terminals or variable non terminals  
length  records the difference in the lengths of the frontiers of  and   and whether
the targets frontier is shorter than that of the source  see rows length in table    
the features listed above are defined for all the rules in the grammar  this includes
the copy and delete rules  as described in section      which were added to address the
problem of unseen words or productions in the source trees at test time  many of these
rules can not be applied to the training set  but will receive some weight because they share
features with rules that can be used in training  however  in training the model learns to
disprefer these coverage rules as they are unnecessary to model the training set  which can
be described perfectly using the extracted transduction rules  our dual use of the training
set for grammar extraction and parameter estimation results in a bias against the coverage
rules  the bias could be addressed by extracting the grammar from a separate corpus  in
which case the coverage rules would then be useful in modeling both the training set and the
testing sets  however  this solution has its own problems  namely that many of the target
trees in the training may not longer be reachable  this bias and its possible solutions is an
interesting research problem and deserves further work 
   

ficohn   lapata

rule  hnp nnsi  h np cd  adjp   nns activists     nns activists i
type
type   training set
 
root
x   np
 
root
y   nns
 
root
x   np  y   nns
 
identity
    np cd adjp  nns activists  
 
identity
    nns activists 
 
identity     np cd  adjp   nns activists        nns activists 
 
unlexid 
unlex      np cd adjp nns 
 
unlexid 
unlex     nns
 
unlexid 
unlex      np cd  adjp  nns      nns
 
rule count

 
word count
target terminals
 
word count
source terminals   
yield
source    activists   target    activists 
 
yield
terminal activists in both source and target
 
yield
non terms  source    cd  adjp  nns   target    nns 
 
yield
non terminal cd in source and not target
 
yield
non terminal adjp in source and not target
 
yield
non terminal nns in both source and target
 
length
difference in length
 
length
target shorter
 
table    features instantiated for the synchronous rule shown above  only features with
non zero values are displayed   the number of source terminals is calculated using
the source tree at the time the rule is applied 

   experimental set up
in this section we present our experimental set up for assessing the performance of the
sentence compression model described above  we give details of the corpora used  briefly
introduce mcdonalds        model used for comparison with our approach  and explain
how system output was evaluated 
    corpora
we evaluated our system on three publicly available corpora  the first is the ziff davis
corpus  a popular choice in the sentence compression literature  the corpus originates
from a collection of news articles on computer products  it was created automatically by
matching sentences that occur in an article with sentences that occur in an abstract  knight
  marcu         the other two corpora   were created manually  annotators were asked to
produce target compressions by deleting extraneous words from the source without changing
the word order  clarke   lapata         one corpus was sampled from written sources 
    available from http   homepages inf ed ac uk s        data  

   

fisentence compression as tree transduction

corpus
clspoken
clwritten
ziff davis

articles
  
  


sentences
    
    
    

training
   
   
    

development
  
  
  

testing
   
   
  

table    sizes of the various corpora  measured in articles or sentence pairs  the data split
into training  development and testing sets is measured in sentence pairs 

the british national corpus  bnc  and the american news text corpus  whereas the other
was created from manually transcribed broadcast news stories  we will henceforth refer
to these two corpora as clwritten and clspoken  respectively  the sizes of these three
corpora are shown in table   
these three corpora pose different challenges to a hypothetical sentence compression
system  firstly  they are representative of different domains and text genres  secondly 
they have different compression requirements  the ziff davis corpus is more aggressively
compressed in comparison to clspoken and clwritten  clarke   lapata         as clspoken is a speech corpus  it often contains incomplete and ungrammatical utterances and
speech artefacts such as disfluencies  false starts and hesitations  its utterances have varying lengths  some are very wordy whereas others cannot be reduced any further  this means
that a compression system should leave some sentences uncompressed  finally  we should
note the clwritten has on average longer sentences than ziff davis or clspoken  parsers
are more likely to make mistakes on long sentences which could potentially be problematic
for syntax based systems like the one presented here 
although our model is capable of performing any editing operation  such as reordering
or substitution  it will not learn to do so from the training corpora  these corpora contain
only deletions  and therefore the model will not learn transduction rules encoding  e g  
reordering  instead the rules encode only the deleting and inserting terminals and restructuring internal nodes of the syntax tree  however  the model is capable general text
rewriting  and given the appropriate training set will learn to perform these additional
edits  this is demonstrated by our recent results from adapting the model to abstractive
compression  cohn   lapata         where any edit is permitted  not just deletion 
our experiments on clspoken and clwritten followed clarke and lapatas        partition of training  test  and development sets  the partition sizes are shown in table    in
the case of the ziff davis corpus  knight and marcu        had not defined a development
set  therefore we randomly selected  and held out     sentence pairs from their training
set to form our development set 
    comparison with state of the art
we evaluated our results against mcdonalds        discriminative model  in this approach 
sentence compression is formalized as a classification task  pairs of words from the source
sentence are classified as being adjacent or not in the target compression  let x   x            xn
denote a source sentence with a target compression y   y            ym where each yi occurs
in x  the function l yi             n   maps word yi the target to the index of the word in
   

ficohn   lapata

the source  subject to the constraint that l yi     l yi       mcdonald defines the score of
a compression y for a sentence x as the dot product between a high dimensional feature
representation  f   over bigrams and a corresponding weight vector  w 
score x  y  w   

m
x

hw  f  x  l yj     l yj   i

    

i  

decoding in this framework amounts to finding the combination of bigrams that maximize
the scoring function in       the maximization is solved using a semi markov viterbi
algorithm  mcdonald        
the model parameters are estimated using the margin infused relaxed algorithm
 mira crammer   singer         a discriminative large margin online learning technique 
mcdonald        uses a similar loss function to our hamming loss  see       but without
an explicit length penalty  this loss function counts the number of words falsely retained or
dropped in the predicted target relative to the reference  mcdonald employs a rich feature
set defined over words  parts of speech  phrase structure trees  and dependencies  these are
gathered over adjacent words in the compression and the words which were dropped 
clarke and lapata        reformulate mcdonalds        model in the context of integer
linear programming  ilp  and augment it with constraints ensuring that the compressed
output is grammatically and semantically well formed  for example  if the target sentence
has negation  this must be included in the compression  if the source verb has a subject 
this must also be retained in the compression  they generate and solve an ilp for every
source sentence using the branch and bound algorithm  since they obtain performance
improvements over mcdonalds model on several corpora  we also use it for comparison
against our model 
to summarize  we believe that mcdonalds        model is a good basis for comparison
for several reasons  first  it is has good performance  and can be treated as a state of theart model  secondly  it is similar to our model in many respects  its training algorithm
and feature space  but differs in one very important respect  compression is performed
on strings and not trees  mcdonalds system does make use of syntax trees  but only
peripherally via the feature set  in contrast  the syntax tree is an integral part of our
model 
    evaluation
in line with previous work we assessed our models output by eliciting human judgments 
following knight and marcu         we conducted two separate experiments  in the first
experiment participants were presented with a source sentence and its target compression
and asked to rate how well the compression preserved the most important information from
the source sentence  in the second experiment  they were asked to rate the grammaticality
of the compressed outputs  in both cases they used a five point rating scale where a high
number indicates better performance  we randomly selected    sentences from the test
portion of each corpus  these sentences were compressed automatically by our system and
mcdonalds        system  we also included gold standard compressions  our materials
thus consisted of                source target sentences  a latin square design ensured
that subjects did not see two different compressions of the same sentence  we collected
   

fisentence compression as tree transduction

ratings from    unpaid volunteers  all self reported native english speakers  both studies
were conducted over the internet using webexp    a software package for running internetbased experiments 
we also report results using f  computed over grammatical relations  riezler et al  
       although f  conflates grammaticality and importance into a single score  it nevertheless has been shown to correlate reliably with human judgments  clarke   lapata 
       furthermore  it can be usefully employed during development for feature engineering and parameter optimization experiments  we measured f  over directed and labeled
dependency relations  for all models the compressed output was parsed using the rasp
dependency parser  briscoe   carroll         note that we could extract dependencies directly from the output of our model since it generates trees in addition to strings  however 
we refrained from doing this in order to compare all models on an equal footing 

   results
the framework presented in section   is quite flexible  depending on the grammar extraction strategy  choice of features  and loss function  different classes of models can be derived 
before presenting our results on the test set we discuss the specific model employed in our
experiments and explain how its parameters were instantiated 
    model selection
all our parameter tuning and model selection experiments were conducted on the development set of the clspoken corpus  we obtained syntactic analyses for source and target
sentences with bikels        parser  the corpus was automatically aligned using an algorithm which finds the set of deletions which transform the source into the target  this is
equivalent to the minimum edit distance script when only deletion operations are permitted 
as expected  the predicted parse trees contained a number of errors  although we did
not have gold standard trees with which to quantify this error or its effect on prediction
output  we did notice  however  that errors in the source trees in the test set did not always
negatively affect the performance of the model  in many instances the model was able to
recover from these errors and still produce good output compressions  of these recoveries 
most cases involved either deleting the erroneous structure or entirely preserving it  while
this often resulted in a poor output tree  the string yield was acceptable in most cases  less
commonly  the model corrected the errors in the source using tree transformation rules 
these rules were acquired from the training set where there were errors in the source tree
but not in the test tree  for example  one transformation allows a prepositional phrase to
be moved from a high vp attachment to an object np attachment 
we obtained a synchronous tree substitution grammar from the clspoken corpus using
the method described in section      we extracted all maximally general synchronous rules 
these were complemented with specified rules allowing recursion up to one ancestor for any
given node    grammar rules were represented by the features described in section   
an important parameter in our modeling framework is the choice of loss function  we
    see http   www webexp info  
    rules were pruned so as to have no more than   variables and    nodes 

   

ficohn   lapata

losses
hamming  tokens 
hamming  ngram 
hamming  cfg 
edit distance
f 
reference

rating
    
    
    
    
    
    

std  dev
    
    
    
    
    
    

table    mean ratings on system output  clspoken development set  while using different
loss functions 

evaluated the loss functions presented in section     as follows  we performed a grid search
for the hyper parameters  a regularization parameter and a feature scaling parameter  which
balances the magnitude of the feature vectors with the scale of the loss function    which
minimized the relevant loss on the development set  and used the corresponding system
output  the gold standard derivation was selected using the maximum number of rules
heuristic  as described in section      the beam was limited to     unique items or     items
in total  the grammar was filtered to allow no more than    target elementary trees for
every source elementary tree 
we next asked two human judges to rate on a scale of   to   the systems compressions
when optimized for the different loss functions  to get an idea of the quality of the output
we also included human authored reference compressions  sentences given high numbers
were both grammatical and preserved the most important information  the mean ratings
are shown in table    as can be seen the differences among the losses are not very large 
and the standard deviation is high  the hamming loss over tokens performed best with a
mean rating of       closely followed by the edit distance         we chose the former over
the latter as it is less coarsely approximated during search  all subsequent experiments
report results using the token based hamming loss 
we also wanted to investigate how the synchronous grammar influences performance 
the default system described above used general rules together with specialized rules where
the recursion depth was limited to one  we also experimented with a grammar that uses
specialised rules with a maximum recursion depth of two and a grammar that uses solely the
maximally general rules  in table   we report the average compression rate  relations based
f  and the hamming loss over tokens for these different grammars  we see that adding
the specified rules allows for better f   and loss  despite the fact that the search space
remains the same  we observe a slight degradation in performance moving to depth   
rules  this is probably due to the increase in spurious ambiguity affecting search quality 
and also allowing greater overfitting of the training data  the number of transduction rules
in the grammar also grows substantially with the increased depth  from        for the
maximally general extraction technique to        and        for specified rules with depth
    we found that setting the regularization parameter c        and the scaling parameter to   generally
yields good performance across loss functions 

   

fisentence compression as tree transduction

model
max general rules
depth    specified rules
depth    specified rules
max rules
max scoring
unigram lm
bigram lm
trigram lm
all features
only rule features
only token features

compression rate
     
     
     
     
     
     
     
     
     
     
     

relations f 
     
     
     
     
     
     
     
     
     
     
     

loss
   
   
   
   
   
   
   
   
   
   
   

table    parameter exploration and feature ablation studies  clspoken development set  
the default system is shown with an asterisk 

   and     respectively  the growth in grammar size is exponential in the specification
depth and therefore only small values should be used 
we also inspected the rules obtained with the maximally general extraction technique
to better assess how our rules differ from those obtained from a vanilla scfg  see knight  
marcu         many of these rules       have deeper structure and therefore would not be
licensed by an scfg  this is due to structural divergences between the source and target
syntax trees in the training set  a further     of the rules describe a change of syntactic
category  x    y    and therefore only the remaining     of the rules would be allowable in
knight and marcus transducer  the proportion of scfg rules decreases substantially as
the rule specification depth is increased 
recall from section     that our scoring function is defined over derivations rather than
target trees or strings  and that we treat the derivation using the maximum number of rules
as the gold standard derivation  as a sanity check  we also experimented with selecting the
derivation with the maximum score under the model  the results in table   indicate that
the latter strategy is not as effective as selecting the derivation with the maximum number
of rules  again we conjecture this is due to overfitting  as the training data is used to
extract the grammar  the derivations with the maximum score may consist of rules with
rare features which model the data well but do not generalize to unseen instances 
finally  we conducted a feature ablation study to assess which features are more useful
to our task  we were particularly interested to see if the ngram features would bring
any benefit  especially since they increase computational complexity during decoding and
training  we experimented with a unigram  bigram  and trigram language model  note that
the unigram language model is not as computationally expensive as the other two models
because there is no need to record ngram contexts in the chart  as shown in table    the
unigram language model is substantially worse than the bigram and trigram which deliver
similar performances  we also examined the impact of the other features by grouping them
into two broad classes  those defined over rules and those defined over tokens  our aim was
to see whether the underlying grammar  represented by rule based features  contributes
   

ficohn   lapata

to better compression output  the results in table   reveal that the two feature groups
perform comparably  however  the model using only token based features tends to compress
less  these features are highly lexicalized  and the model is not able to generalize well on
unseen data  in conclusion  the full feature set does better on all counts than the two
ablation sets  with a better compression rate 
the results reported have all been measured over string output  this was done by first
stripping the tree structure from the compression output  reparsing  extracting dependency
relations and finally comparing to the dependency relations in the reference  however  we
may wish to measure the quality of the trees themselves  not just their string yield  a
simple way to measure this   would be to extract dependency relations directly from the
phrase structure tree output    compared to dependencies extracted from the predicted
parses using bikels        parser on the output string  we observe that the relation f 
score increases uniformly for all tasks  by between       and       absolute  therefore
the systems tree output better encodes the syntactic dependencies than the tree resulting
from re parsing the string output  if the system is part of a nlp pipeline  and its output
is destined for down stream processing  then having an accurate syntax tree is extremely
important  this is also true for related tasks where the desired output is a tree  e g  
semantic parsing 

   model comparison
in this section we present our results on the test set using the best performing model from
the previous section  this model uses a grammar with unlexicalized and lexicalized rules
 recursion depth     a hamming loss based on tokens  and all the features from section   
the model was trained separately on each corpus  training portion   we first discuss our
results using relations f  and then move on to the human study 
table   illustrates the performance of our model  transducer   on clspoken  clwritten  and ziff davis  we also report results on the same corpora using mcdonalds       
model  mcdonald  and the improved version  clarke ilp  put forward by clarke and
lapata         we also present the compression rate for each system and the reference gold
standard  in all cases our tree transducer model outperforms mcdonalds original model
and the improved ilp based version 
nevertheless  it may be argued that our model has an unfair advantage here since it
tends to compress less than the other models  and is therefore less likely to make many
mistakes  to ensure that this is not the case  we created a version of our model with a
compression rate similar to mcdonald  this can be done relatively straightforwardly
by manipulating the length penalty of the hamming loss  the smaller the penalty the
more words the model will tend to drop  therefore  we varied the length penalty  and
hyper parameters  on the development set in order to obtain a compression rate similar to
    we could alternatively measure other tree metrics  such as tree edit distance  however  the standard
measures used in parser evaluation  e g   evalb  would not be suitable  as they assume that the parse
yield is fixed  in our case the reference target string is often different to the systems output 
    we extract dependency relations with the conversion tool from the conll      shared task  available
at http   nlp cs lth se pennconverter  

   

fisentence compression as tree transduction

model
transducer 
transducer 
mcdonald
clarke ilp
reference

clspoken
compression rate
     
     
     
     
     

relations f 
     
     
     
     


model
transducer 
transducer 
mcdonald
clarke ilp
reference

clwritten
compression rate
     
     
     
     
     

relations f 
     
     
     
     


model
transducer 
mcdonald
clarke ilp
reference

ziff davis
compression rate
     
     
     
     

relations f 
     
     
     


table    results on clspoken  clwritten  and ziff davis corpus  testing set   compression
rate and relations based f  

mcdonald    this model was then applied to the test set and its performance is shown
in table   as transducer   we refrained from doing this on ziff davis  since our original
transducer obtained a compression rate comparable to mcdonald        vs          as
can be seen  transducer  yields a better f  on clspoken and clwritten  the differences
in f  are statistically significant using the the wilcoxon test  p          transducer 
numerically outperforms mcdonald on ziff davis  however the difference is not significant
 the ziff davis test set consists solely of    sentences  
we next consider the results of our judgment elicitation study which assesses in more
detail the quality of the generated compressions  recall that our participants judge compressed output on two dimensions  grammaticality and importance  we compared the
output of our system  transducer  on clspoken and clwritten and transducer  on
ziff davis  against the output of mcdonald        and the reference gold standard  table  
illustrates examples of the compressions our participants saw 
    we matched the compression rate of mcdonald by scaling the length penalty by      and      for the
clwritten and clspoken corpora  respectively  another way to control the compression rate would be
to modify our chart based decoder in a fashion similar to mcdonald         however  we leave this to
future work 

   

ficohn   lapata

s  i just wish my parents and my other teachers could be like this teacher  so
we could communicate 
m  i wish my teachers could be like this teacher 
t  i wish my teachers could be like this  so we could communicate 
r  i wish my parents and other teachers could be like this  so we could
communicate 
s  the treasury is refusing to fund a further phase of the city technology
colleges 
m  the treasury is refusing to fund a further colleges 
t  the treasury is refusing to fund the city technology colleges 
r  the treasury is refusing to fund further the city technology colleges 
s  apparel makers use them to design clothes and to quickly produce and
deliver the best selling garments 
m  apparel makers use them to design clothes and to produce and deliver the
best selling garments 
t  apparel makers use them to design clothes 
r  apparel makers use them to design clothes 
s  earlier this week  in a conference call with analysts  the bank said it boosted
credit card reserves by      million 
m  earlier said credit card reserves by      million 
t  in a conference call with analysts  the bank boosted card reserves by     
million 
r  in a conference call with analysts the bank said it boosted credit card
reserves by      million 
table    compression examples from clspoken  clwritten  and ziff davis  s  source sentence  m  mcdonald        t  transducer  r  reference gold standard 

table   shows the mean ratings   for each system  and the reference  on clspoken 
clwritten  and ziff davis  we carried out an analysis of variance  anova  to examine
the effect of system type  mcdonald  transducer  reference  on the compression ratings  the anova revealed a reliable effect on all three corpora  we used post hoc tukey
tests to examine whether the mean ratings for each system differed significantly  p         
on the clspoken corpus the transducer is perceived as significantly better than mcdonald  both in terms of grammaticality and importance  we obtain the same result for
the clwritten corpus  the two systems achieve similar performances on ziff davis  the
grammaticality and importance score do not differ significantly   ziff davis seems to be
a less challenging corpus than clspoken or clwritten and less likely to highlight differences among systems  for example  turner and charniak        present several variants of
the noisy channel model  all of which achieve compressions of similar quality on ziff davis
 grammaticality ratings varied by only      and informativeness ratings      in their
human evaluation   in most cases the transducer and mcdonald yield significantly
    all statistical tests reported subsequently were done using the mean ratings 

   

fisentence compression as tree transduction

model
transducer
mcdonald
reference

clspoken
grammaticality
    
    
    

importance
    
    
    

model
transducer
mcdonald
reference

clwritten
grammaticality
    
    
    

importance
    
    
    

model
transducer
mcdonald
reference

ziff davis
grammaticality
    
    
    

importance
    
    
    

table    mean ratings on compression output elicited by humans     sig  diff  from mcdonald               sig  diff  from reference            using post hoc tukey
tests 

worse performance than the reference  save one exception  on the clspoken corpus  there
is no significant difference between the transducer and the gold standard 
these results indicate that our highly expressive framework is a good model for sentence compression  under several experimental conditions  across different domains  we
obtain better performance than previous work  importantly  the model described here is
not compression specific  it could be easily adapted to other tasks  corpora or languages  for
which syntactic analysis tools are available   being supervised  the model learns to fit the
compression rate of the training data  in this sense  it is somewhat inflexible as it cannot
easily adapt to a specific rate given by a user or imposed by an application  e g   when
displaying text on small screens   nevertheless  compression rate can be indirectly manipulated by adopting loss functions that encourage or discourage compression or directly
during decoding by stratifying the chart for length  mcdonald        

   conclusions
in this paper we have formulated sentence compression as a tree to tree rewriting task   
we developed a system that licenses the space of all possible rewrites using a tree substitution grammar  each grammar rule is assigned a weight which is learned discriminatively
within a large margin model  tsochantaridis et al          a specialized algorithm is used to
learn the model weights and find the best scoring compression under the model  we argue
    the source code is freely available from http   homepages inf ed ac uk tcohn t  

   

ficohn   lapata

that the proposed framework is appealing for several reasons  the synchronous grammar
provides expressive power to capture rewrite operations that go beyond word deletion such
as reordering  changes in non terminal categories and lexical substitution  since it is not
deletion specific  the model could be ported to other rewriting tasks  see cohn   lapata 
      for an example  without the overhead of devising new algorithms for decoding or
training  moreover  the discriminative nature of the learning algorithm allows for the incorporation of all manner of powerful features  the rich feature space in conjunction with
the choice of an appropriate loss function afford greater flexibility in fitting the empirical
data for different domains or tasks 
we evaluated our model on three compression corpora  clspoken  clwritten  and ziffdavis  and showed that in most cases it yields results superior to state of the art  mcdonald         our experiments were also designed to assess several aspects of the proposed
framework such as the complexity of the synchronous grammar  the choice of loss function 
the effect of various features  and the quality of the generated tree output  we observed
performance improvements by allowing maximally general grammar rules to be specified
once  producing larger and more lexicalized rules  this concurs with galley and mckeown
       who also find that lexicalization yields better compression output  the choice of
loss function appears to have less of an effect  we devised three classes of loss functions
based on hamming distance  edit distance and f  score  overall  the simple token based
hamming loss achieved the best results  we conjecture that this is due to its simplicity  it
can be evaluated more precisely than many of the other loss functions and isnt affected by
poor parser output  our feature ablation study revealed that ngram features are beneficial 
mirroring a similar finding in the machine translation literature  chiang         finally  we
found that the trees created by our generation algorithm are more accurate compared to the
output of a parser applied to the string output  this augurs well for use in a cascaded nlp
pipeline  where other systems use the compression output as input for further processing 
and can potentially make better use of the system output 
future extensions are many and varied  an obvious extension concerns porting the
framework to other rewriting applications such as document summarization  daume iii  
marcu        or machine translation  chiang         initial work  cohn   lapata       
shows that the tree to tree transduction model presented here can be easily adapted to a
sentence abstraction task where compression takes place using rewrite operations that are
not restricted to word deletion  examples include substitution  reordering  and insertion 
other future directions involve more detailed feature engineering  including source conditioned features and ngram features besides the language model  more research is needed
to establish suitable loss functions for compression and other rewriting tasks  in particular
it should be interesting to experiment with loss functions that incorporate a wider range
of linguistic features beyond parts of speech  examples include losses based on parse trees
and semantic similarity  finally  the experiments presented in this work use a grammar
acquired from the training corpus  however  there is nothing inherent in our formalization
that restricts us to this particular grammar  we therefore plan to investigate the potential of our method with unsupervised or semi supervised grammar induction techniques for
other rewriting tasks including paraphrase generation and machine translation 
   

fisentence compression as tree transduction

acknowledgments
we are grateful to philip blunsom for insightful comments and suggestions and to the
anonymous referees whose feedback helped to substantially improve the present paper 
special thanks to james clarke for sharing his implementations of clarke and lapatas
       and mcdonalds        models with us  we acknowledge the support of epsrc
 grants gr t         and gr t           this work has made use of the resources
provided by the edinburgh compute and data facility  ecdf   the ecdf is partially
supported by the edikt initiative  a preliminary version of this work was published in the
proceedings of emnlp conll      

references
aho  a  v     ullman  j  d          syntax directed translations and the pushdown assembler  journal of computer and system sciences          
alshawi  h   bangalore  s     douglas  s          learning dependency translation models
as collections of finite state head transducers  computational linguistics            
   
berger  a  l   pietra  s  a  d     pietra  v  j  d          a maximum entropy approach
to natural language processing  computational linguistics               
bikel  d          design of a multi lingual  parallel processing statistical parsing engine 
in proceedings of the  nd international conference on human language technology
research  pp        san diego  ca 
briscoe  e  j     carroll  j          robust accurate statistical annotation of general text  in
proceedings of the third international conference on language resources and evaluation  pp            las palmas  gran canaria 
carroll  j   minnen  g   pearce  d   canning  y   devlin  s     tait  j          simplifying
text for language impaired readers  in proceedings of the  th conference of the european chapter of the association for computational linguistics  pp          bergen 
norway 
chandrasekar  r     srinivas  c  d  b          motivations and methods for text simplification  in proceedings of the   th international conference on computational
linguistics  pp            copenhagen  danemark 
chiang  d          hierarchical phrase based translation  computational linguistics         
       
clarke  j     lapata  m          models for sentence compression  a comparison across
domains  training requirements and evaluation measures  in proceedings of the   st
international conference on computational linguistics and   th annual meeting of
the association for computational linguistics  pp          sydney  australia 
clarke  j     lapata  m          global inference for sentence compression  an integer linear
programming approach  journal of artificial intelligence research             
   

ficohn   lapata

cohn  t     lapata  m          sentence compression beyond word deletion  in proceedings of the   nd international conference on computational linguistics  pp         
manchester  uk 
collins  m          discriminative training methods for hidden markov models  theory and
experiments with perceptron algorithms  in proceedings of the      conference on
empirical methods in natural language processing  pp      morristown  nj 
collins  m  j          head driven statistical models for natural language parsing  ph d 
thesis  university of pennsylvania  philadelphia  pa 
crammer  k     singer  y          ultraconservative online algorithms for multiclass problems  machine learning            
daume iii  h     marcu  d          a noisy channel model for document compression 
in proceedings of the   th annual meeting of thev association for computational
linguistics  pp          philadelphia  pa 
eisner  j          learning non isomorphic tree mappings for machine translation  in the
companion volume to the proceedings of   st annual meeting of the association for
computational linguistics  pp          sapporo  japan 
galley  m   hopkins  m   knight  k     marcu  d          whats in a translation rule  
in proceedings of the      human language technology conference of the north
american chapter of the association for computational linguistics  pp         
boston  ma 
galley  m     mckeown  k          lexicalized markov grammars for sentence compression 
in proceedings of human language technologies       the conference of the north
american chapter of the association for computational linguistics  pp         
rochester  ny 
grael  j     knight  k          training tree transducers  in proceedings of the      human
language technology conference of the north american chapter of the association
for computational linguistics  pp          boston  ma 
hermjakob  u   echihabi  a     marcu  d          natural language based reformulation
resource and wide exploitation for question answering  in proceedings of   th text
retrieval conference  gaithersburg  md 
hori  c     furui  s          speech summarization  an approach through word extraction
and a method for evaluation  ieice transactions on information and systems  e  d          
jing  h          sentence reduction for automatic text summarization  in proceedings of
the  th applied natural language processing conference  pp          seattle  wa 
joachims  t          a support vector method for multivariate performance measures  in
proceedings of the   nd international conference on machine learning  pp         
bonn  germany 
kaji  n   okamoto  m     kurohashi  s          paraphrasing predicates from written
language to spoken language using the web  in proceedings of the      human language technology conference of the north american chapter of the association for
computational linguistics  pp          boston  ma 
   

fisentence compression as tree transduction

knight  k          decoding complexity in word replacement translation models  computational linguistics                 
knight  k     marcu  d          summarization beyond sentence extraction  a probabilistic
approach to sentence compression  artificial intelligence                 
lin  d     pantel  p          discovery of inference rules for question answering  natural
language engineering                
mcdonald  r          discriminative sentence compression with soft syntactic constraints 
in proceedings of the   th conference of the european chapter of the association for
computational linguistics  pp          trento  italy 
nguyen  m  l   horiguchi  s   shimazu  a     ho  b  t          example based sentence
reduction using the hidden markov model  acm transactions on asian language
information processing                
och  f  j     ney  h          the alignment template approach to statistical machine
translation  computational linguistics                 
papineni  k   roukos  s   ward  t     zhu  w  j          bleu  a method for automatic
evaluation of machine translation  in proceedings of the   th annual meeting of thev
association for computational linguistics  pp          philadelphia  pa 
petrov  s   barrett  l   thibaux  r     klein  d          learning accurate  compact  and
interpretable tree annotation  in proceedings of the   st international conference on
computational linguistics and   th annual meeting of the association for computational linguistics  pp          sydney  australia 
riezler  s   king  t  h   crouch  r     zaenen  a          statistical sentence condensation
using ambiguity packing and stochastic disambiguation methods for lexical functional
grammar  in proceedings of the      human language technology conference of
the north american chapter of the association for computational linguistics  pp 
        edmonton  canada 
shieber  s     schabes  y          synchronous tree adjoining grammars  in proceedings of the   th international conference on computational linguistics  pp         
helsinki  finland 
stolcke  a          srilm  an extensible language modeling toolkit  in proceedings of the
international conference on spoken language processing  denver  co 
tsochantaridis  i   joachims  t   hofmann  t     altun  y          large margin methods
for structured and interdependent output variables  journal of machine learning
research              
turner  j     charniak  e          supervised and unsupervised learning for sentence
compression  in proceedings of the   rd annual meeting of the association for computational linguistics  pp          ann arbor  mi 
vandeghinste  v     pan  y          sentence compression for automated subtitling  a
hybrid approach  in text summarization branches out  proceedings of the acl   
workshop  pp        barcelona  spain 
   

ficohn   lapata

wu  d          stochastic inversion transduction grammars and bilingual parsing of parallel
corpora  computational linguistics                 
yamada  k     knight  k          a syntax based statistical translation model  in proceedings of the   th annual meeting of the association for computational linguistics 
pp          toulouse  france 

   

fi