journal of artificial intelligence research               

submitted        published      

a heuristic search approach to planning
with continuous resources in stochastic domains
nicolas meuleau

nicolas f meuleau nasa gov

nasa ames research center
mail stop      
moffet field  ca             usa

emmanuel benazera

ebenazer laas fr

laas cnrs  universite de toulouse
   av  du colonel roche
      toulouse cedex    france

ronen i  brafman

brafman cs bgu ac il

department of computer science
ben gurion university
beer sheva        israel

eric a  hansen

hansen cse msstate edu

department of computer science and engineering
mississippi state university
mississippi state  ms        usa

mausam

mausam cs washington edu

department of computer science and engineering
university of washington
seattle  wa            usa

abstract
we consider the problem of optimal planning in stochastic domains with resource constraints 
where the resources are continuous and the choice of action at each step depends on resource availability  we introduce the hao  algorithm  a generalization of the ao  algorithm that performs
search in a hybrid state space that is modeled using both discrete and continuous state variables  where the continuous variables represent monotonic resources  like other heuristic search
algorithms  hao  leverages knowledge of the start state and an admissible heuristic to focus
computational effort on those parts of the state space that could be reached from the start state
by following an optimal policy  we show that this approach is especially effective when resource
constraints limit how much of the state space is reachable  experimental results demonstrate
its effectiveness in the domain that motivates our research  automated planning for planetary
exploration rovers 

   introduction
many nasa planetary exploration missions rely on rovers  mobile robots that carry a suite of
scientific instruments for use in characterizing planetary surfaces and transmitting information back
to earth  because of difficulties in communicating with devices on distant planets  direct human
control of rovers by tele operation is infeasible  and rovers must be able to act autonomously for
substantial periods of time  for example  the mars exploration rovers  mer   aka  spirit and
opportunity  are designed to communicate with the ground only twice per martian day 
autonomous control of planetary exploration rovers presents many challenges for research in
automated planning  progress has been made in meeting some of these challenges  for example  the
planning software developed for the mars sojourner and mer rovers has contributed significantly

c
    
ai access foundation  all rights reserved 

fimeuleau  benazera  brafman  hansen   mausam

to the success of these missions  bresina  jonsson  morris    rajan         but many important
challenges must still be addressed to achieve the more ambitious goals of future missions  bresina 
dearden  meuleau  ramakrishnan  smith    washington        
among these challenges is the problem of plan execution in uncertain environments  on planetary
surfaces such as mars  there is uncertainty about the terrain  meteorological conditions  and the state
of the rover itself  position  battery charge  solar panels  component wear  etc   in turn  this leads
to uncertainty about the outcome of the rovers actions  much of this uncertainty is about resource
consumption  for example  factors such as slope and terrain affect speed of movement and rate of
power consumption  making it difficult to predict with certainty how long it will take for a rover
to travel between two points  or how much power it will consume in doing so  because of limits
on critical resources such as time and battery power  rover plans are currently very conservative
and based on worst case estimates of time and resource usage  in addition  instructions sent to
planetary rovers are in the form of a sequential plan for attaining a single goal  e g   photographing
an interesting rock   if an action has an unintended outcome that causes a plan to fail  the rover
stops and waits for further instructions  it makes no attempt to recover or achieve an alternative
goal  this can result in under utilized resources and missed science opportunities 
over the past decade  there has been a great deal of research on how to generate conditional
plans in domains with uncertain action outcomes  much of this work is formalized in the framework
of markov decision processes  puterman        boutilier  dean    hanks         however  as
bresina et al         point out  important aspects of the rover planning problem are not adequately
handled by traditional planning algorithms  including algorithms for markov decision processes  in
particular  most traditional planners assume a discrete state space and a small discrete number of
action outcomes  but in automated planning for planetary exploration rovers  critical resources such
as time and battery power are continuous  and most of the uncertainty in the domain results from
the effect of actions on these variables  this requires a conditional planner that can branch not
only on discrete action outcomes  but on the availability of continuous resources  and such a planner
must be able to reason about continuous as well as discrete state variables 
closely related to the challenges of uncertain plan execution and continuous resources is the
challenge of over subscription planning  the rovers of future missions will have much improved
capabilities  whereas the current mer rovers require an average of three days to visit a single rock 
progress in areas such as automatic instrument placement will allow rovers to visit multiple rocks
and perform a large number of scientific observations in a single communication cycle  pedersen 
smith  deans  sargent  kunz  lees    rajagopalan         moreover  communication cycles will
lengthen substantially in more distant missions to the moons of jupiter and saturn  requiring longer
periods of autonomous behavior  as a result  space scientists of future missions are expected to
specify a large number of science goals at once  and often this will present what is known as an oversubscription planning problem  this refers to a problem in which it is infeasible to achieve all goals 
and the objective is to achieve the best subset of goals within resource constraints  smith         in
the case of the rover  there will be multiple locations the rover could reach  and many experiments
the rover could conduct  most combinations of which are infeasible due to resource constraints  the
planner must select a feasible subset of these that maximizes expected science return  when action
outcomes  including resource consumption  are stochastic  a plan that maximizes expected science
return will be a conditional plan that prescribes different courses of action based on the results of
previous actions  including resource availability 
in this paper  we present an implemented planning algorithm that handles all of these problems
together  uncertain action outcomes  limited continuous resources  and over subscription planning 
we formalize the rover planning problem as a hybrid state markov decision process  that is  a markov
decision process  mdp  with both discrete and continuous state variables  and we use the continuous
variables to represent resources  the planning algorithm we introduce is a heuristic search algorithm
called hao   for hybrid state ao   it is a generalization of the classic ao  heuristic search algorithm  nilsson        pearl         whereas ao  searches in discrete state spaces  hao  solves

  

fihao 

planning problems in hybrid domains with both discrete and continuous state variables  to handle
hybrid domains  hao  builds on earlier work on dynamic programming algorithms for continuous
and hybrid state mdps  in particular  the work of feng et al         
generalizing and or graph search for hybrid state spaces poses a complex challenge  and we
only consider a special case of the problem  in particular  continuous variables are used to represent
monotonic resources  the search is for the best conditional plan that allows branching not only on
the values of the discrete variables  but on the availability of these resources  and does not violate a
resource constraint 
it is well known that heuristic search can be more efficient than dynamic programming because it
uses reachability analysis guided by a heuristic to focus computation on the relevant parts of the state
space  we show that for problems with resource constraints  including over subscription planning
problems  heuristic search is especially effective because resource constraints can significantly limit
reachability  unlike dynamic programming  a systematic forward search algorithm such as ao  keeps
track of the trajectory from the start state to each reachable state  and thus it can check whether the
trajectory is feasible or violates a resource constraint  by pruning infeasible trajectories  a heuristic
search algorithm can dramatically reduce the number of states that must be considered to find an
optimal policy  this is particularly important in our domain where the discrete state space is huge
 exponential in the number of goals   and yet the portion reachable from any initial state is relatively
small  due to resource constraints 

   problem formulation and background
we start with a formal definition of the planning problem we are tackling  it is a special case of
a hybrid state markov decision process  and so we first define this model  then we discuss how
to include resource constraints and formalize over subscription planning in this model  finally we
review a class of dynamic programming algorithms for solving hybrid state mdps  since some of
these algorithmic techniques will be incorporated in the heuristic search algorithm we develop in
section   
    hybrid state markov decision process
a hybrid state markov decision process  or hybrid state mdp  is a factored markov decision process
that has both discrete and continuous state variables  we define it as a tuple  n  x  a  p  r   where
n is a discrete state variable  x    x    x         xd   is a set of continuous state variables  a is a set
of actions  p is a stochastic state transition model  and r is a reward function  we describe these
elements in more detail below  a hybrid state mdp is sometimes referred to as simply a hybrid
mdp  the term hybrid does not refer to the dynamics of the model  which are discrete  another
term for a hybrid state mdp  which originates in the markov chain literature  is a general state
mdp 
although a hybrid state mdp can have multiple discrete variables  this plays no role in the algorithms described in this paper  and so  for notational convenience  we model the discrete component
of the state space as a single variable n   our focus is on the continuous component  we assume
n the
domain of each continuous variable xi  x is a closed interval of the real line  and so x   i xi
is the hypercube over which the continuous variables are defined  the state set s of a hybrid state
mdp is the set of all possible assignments of values to the state variables  in particular  a hybrid
state s  s is a pair  n  x  where n  n is the value of the discrete variable  and x    xi   is a vector
of values of the continuous variables 
state transitions occur as a result of actions  and the process evolves according to markovian
state transition probabilities pr s    s  a   where s    n  x  denotes the state before action a and
s     n    x    denotes the state after action a  also called the arrival state  these probabilities can be
decomposed into 

  

fimeuleau  benazera  brafman  hansen   mausam

 the discrete marginals pr n   n  x  a   for all  n  x  a  

pr n   n  x  a      
r
 the continuous conditionals pr x   n  x  a  n     for all  n  x  a  n     x  x pr x   n  x  a  n   dx   
  
p

n  n

we assume the reward associated with a transition is a function of the arrival state only  and let
rn  x  denote the reward associated with a transition to state  n  x   more complex dependencies
are possible  but this is sufficient for the goal based domain models we consider in this paper 
    resource constraints and over subscription planning
to model the rover planning problem  we consider a special type of mdp in which the objective
is to optimize expected cumulative reward subject to resource constraints  we make the following
assumptions 
 there is an initial allocation of one or more non replenishable resources 
 each action has some minimum positive consumption of at least one resource  and
 once resources are exhausted  no further action can be taken 
one way to model an mdp with resource constraints is to formulate it as a constrained mdp 
a model that has been widely studied in the operations research community  altman         in
this model  each action a incurs a transition dependent resource cost  cai  s  s     for each resource
i  given an initial allocation of resources and an initial state  linear programming is used to find
the best feasible policy  which may be a randomized policy  although a constrained mdp models
resource consumption  it does not include resources in the state space  as a result  a policy cannot
be conditioned upon resource availability  this is not a problem if resource consumption is either
deterministic or unobservable  but it is not a good fit for the rover domain  in which resource
consumption is stochastic and observable  and the rover should take different actions depending on
current resource availability 
we adopt a different approach to modeling resource constraints in which resources are included
in the state description  although this increases the size of the state space  it allows decisions to be
made based on resource availability  and it allows a stochastic model of resource consumption  since
resources in the rover domain are continuous  we use the continuous variables of a hybrid state mdp
to represent resources  note that the duration of actions is one of the biggest sources of uncertainty
in our rover problems  and we model time as one of the continuous resources  resource constraints
are represented in the form of executability constraints on actions  where an  x  denotes the set of
actions executable in state  n  x   an action cannot be executed in a state that does not satisfy its
minimum resource requirements 
having discussed how to incorporate resource consumption and resource constraints in a hybridstate mdp  we next discuss how to formalize over subscription planning  in our rover planning
problem  scientists provide the planner with a set of goals they would like the rover to achieve 
where each goal corresponds to a scientific task such as taking a picture of a rock or performing an
analysis of a soil sample  the scientists also specify a utility or reward for each goal  usually only
a subset of these goals is feasible under resource constraints  and the problem is to find a feasible
plan that maximizes expected utility  over subscription planning for planetary exploration rovers
has been considered by smith        and van den briel et al         for deterministic domains 
we consider over subscription planning in stochastic domains  especially domains with stochastic
resource consumption  this requires construction of conditional plans in which the selection of goals
to achieve can change depending on resource availability 
in over subscription planning  the utility associated with each goal can be achieved only once  no
additional utility is achieved for repeating the task  therefore  the discrete state must include a set
of boolean variables to keep track of the set of goals achieved so far by the rover  with one boolean
  

fihao 

variable for each goal  keeping track of already achieved goals ensures a markovian reward structure 
since achievement of a goal is rewarded only if it was not achieved in the past  however  it also
significantly increases the size of the discrete state space  maintaining history information to ensure a
markovian reward structure is a simple example of planning with non markovian rewards  thiebaux 
gretton  slaney  price    kabanza        
    optimality equation
the rover planning problem we consider is a special case of a finite horizon hybrid state mdp in
which termination occurs after an indefinite number of steps  the bellman optimality equation for
this problem takes the following form 
vn  x 

 

vn  x 

 

  when  n  x  is a terminal state  otherwise 
 

z
x
max
pr n    n  x  a 
pr x    n  x  a  n     rn   x      vn   x     dx   

aan  x 

n  n

   

x 

we define a terminal state as a state in which no actions are eligible to execute  that is  an  x     
we use terminal states to model various conditions for plan termination  this includes the situation
in which all goals have been achieved  the situation in which resources have been exhausted  and the
situation in which an action results in some error condition that requires executing a safe sequence
by the rover and terminating plan execution  in addition to terminal states  we assume an explicit
initial state denoted  n    x    
assuming that resources are limited and non replenishable  and that every action consumes
some resource  and the amount consumed is greater than or equal to some positive quantity c   plan
execution will terminate after a finite number of steps  the maximum number of steps is bounded by
the initial resource allocation divided by c  the minimal resource consumption per step  the actual
number of steps is usually much less and indefinite  because resource consumption is stochastic and
because the choice of action influences resource consumption  because the number of steps it takes
for a plan to terminate is bounded but indefinite  we call this a bounded horizon mdp in contrast
to a finite horizon mdp  however  we note that any bounded horizon mdp can be converted to a
finite horizon mdp by specifying a horizon that is equal to the maximum number of plan steps  and
introducing a no op action that is taken in any terminal state 
note that there is usually a difference between the number of plan steps and the time a plan takes
to execute  since we model time as one of the continuous resources  the time it takes to execute a
plan step is both state and action dependent  and stochastic 
given a hybrid state mdp with a set of terminal states and an initial state  n    x     the objective
is to find a policy      n  x   a  that maximizes expected cumulative reward  specifically  an
optimal policy has a value function that satisfies the optimality equation given by equation      in
our rover domain  cumulative reward is equal to the sum of rewards for the goals achieved before
reaching a terminal state and there is no direct incentive to save resources  an optimal solution saves
resources only if this allows achieving more goals  however  our framework is general enough to
allow reasoning about both the cost and the availability of resources  for example  an incentive for
conserving resources could be modeled by specifying a reward that is proportional to the amount of
resources left unused upon entering the terminal state  note that our framework allows reasoning
about both the cost and availability of resources without needing to formulate this as a problem of
multi objective optimization  and we stay in a standard decision theoretic framework 
    dynamic programming for continuous state and hybrid state mdps
because the planning problem we consider is a finite horizon hybrid state mdp  it can be solved
by any algorithm for solving finite horizon hybrid state mdps  most algorithms for solving hybridstate  and continuous state  mdps rely on some form of approximation  a widely used approach is
  

fimeuleau  benazera  brafman  hansen   mausam

figure    value function in the initial state of a simple rover problem  optimal expected return as
a function of two continuous variables  time and energy remaining  

to discretize the continuous state space into a finite number of grid points and solve the resulting
finite state mdp using dynamic programming and interpolation  rust        munos   moore 
       another approach is parametric function approximation  a function associated with the
dynamic programming problem  such as the value function or policy function  is approximated
by a smooth function of k unknown parameters  in general  parametric function approximation is
faster than grid based approximation  but has the drawback that it may fail to converge  or may
converge to an incorrect solution  parametric function approximation is used by other algorithms for
solving continuous state mdps besides dynamic programming  reinforcement learning algorithms
use artificial neural networks as function approximators  bertsekas   tsitsiklis         an approach
to solving mdps called approximate linear programming has been extended to allow continuous as
well as discrete state variables  kveton  hauskrecht    guestrin        
we review another approach to solving hybrid state  or continuous state  mdps that assumes
the problem has special structure that can be exploited by the dynamic
programming algorithm 
r
the structure assumed by this approach ensures that the convolution x  pr x    n  x  a  n    rn   x    
vn   x    dx  in equation     can be computed exactly in finite time  and the value function computed
by dynamic programming is piecewise constant or piecewise linear  the initial idea for this approach
is due to the work of boyan and littman         who describe a class of mdps called time dependent
mdps  in which transitions take place along a single  irreversible continuous dimension  they
describe a dynamic programming algorithm for computing an exact piecewise linear value function
when the transition probabilities are discrete and rewards are piecewise linear  feng et al        
extend this approach to continuous state spaces of more than one dimension  and consider mdps with
discrete transition probabilities and two types of reward models  piecewise constant and piecewise
linear  li and littman        further extend the approach to allow transition probabilities that are
piecewise constant  instead of discrete  although this extension requires some approximation in the
dynamic programming algorithm 
the problem structure exploited by these algorithms is characteristic of the mars rover domain
and other over subscription planning problems  figure   shows the optimal value functions from
the initial state of a typical mars rover problem as a function of two continuous variables  the
time and energy remaining  bresina et al          the value functions feature a set of humps and
plateaus  each of them representing a region of the state space where similar goals are pursued by
the optimal policy  the sharpness of a hump or plateau reflects uncertainty about achieving the
goal s   constraints that impose minimal resource levels before attempting some actions introduce

  

fihao 

sharp cuts in the regions  plateau regions where the expected reward is nearly constant represent
regions of the state space where the optimal policy is the same  and the probability distribution over
future histories induced by this optimal policy is nearly constant 
the structure in such a value function can be exploited by partitioning the continuous state
space into a finite number of hyper rectangular regions   a region is a hyper rectangle if it is the
cartesian product of intervals at each dimension   in each hyper rectangle  the value function is
either constant  for a piecewise constant function  or linear  for a piecewise linear function   the
resolution of the hyper rectangular partitioning is adjusted to fit the value function  large hyperrectangles are used to represent large plateaus  small hyper rectangles are used to represent regions
of the state space where a finer discretization of the value function is useful  such as the edges of
plateaus and the curved hump where there is more time and energy available  a natural choice of
data structures for rectangular partitioning of a continuous space is kd trees  friedman  bentley 
  finkel         although other choices are possible  figures   and    in section     show value
functions for the initial state of a simple rover planning problem  created by a piecewise constant
partitioning of the continuous state space 
the continuous state domains of the transition and reward functions are similarly partitioned into
hyper rectangles  the reward function of each action has the same piecewise constant  or piecewiselinear  representation as the value function  the transition function partitions the state space into
regions for which the set of outcomes of an action and the probability distribution over the set of
outcomes are identical  following boyan and littman         both relative and absolute transitions
are supported  a relative outcome can be viewed as shifting a region by a constant   that is  for
any two states x and y in the same region  the transition probabilitiesp r x   x  a  and p r y    y  a 
are defined in term of the probability of   such that     x   x     y    y   an absolute outcome
maps all states in a region to a single state  that is  for any two states x and y in the same region 
p r x   x  a    p r x   y  a   we can view a relative outcome as a pair    p   where p is the probability
of that outcome  and we can view an absolute outcome as a pair  x    p   this assumes there is
only a finite number of non zero probabilities  i e   the probability distribution is discretized  which
means that for any state and action  a finite set of states can be reached with non zero probability 
this representation guarantees that a dynamic programming update of a piecewise constant value
function results in another piecewise constant value function  feng et al         show that for such
transition functions and for any finite horizon  there exists a partition of the continuous space into
hyper rectangles over which the optimal value function is piecewise constant or linear 
the restriction to discrete transition functions is a strong one  and often means the transition
function must be approximated  for example  rover power consumption is normally distributed 
and thus must be discretized   since the amount of power available must be non negative  our
implementation truncates any negative part of the normal distribution and renormalizes   any continuous transition function can be approximated by an appropriately fine discretization  and feng et
al         argue that this provides an attractive alternative to function approximation approaches in
that it approximates the model but then solves the approximate model exactly  rather than finding
an approximate value function for the original model   for this reason  we will sometimes refer
to finding optimal policies and value functions  even when the model has been approximated   to
avoid discretizing the transition function  li and littman        describe an algorithm that allows
piecewise constant transition functions  in exchange for some approximation in the dynamic programming algorithm  marecki et al        describe a different approach to this class of problems in
which probability distributions over resource consumptions are represented with phase type distributions and a dynamic programming algorithm exploits this representation  although we use the
work of feng et al         in our implementation  the heuristic search algorithm we develop in the
next section could use any of these or some other approach to representing and computing value
functions and policies for a hybrid state mdp 

  

fimeuleau  benazera  brafman  hansen   mausam

   heuristic search in a hybrid state space
in this section  we present the primary contribution of this paper  an approach to solving a special
class of hybrid state mdps using a novel generalization of the heuristic search algorithm ao   in
particular  we describe a generalization of this algorithm for solving hybrid state mdps in which
the continuous variables represent monotonic and constrained resources and the acyclic plan found
by the search algorithm allows branching on the availability of these resources 
the motivation for using heuristic search is the potentially huge size of the state space  which
makes dynamic programming infeasible  one reason for this size is the existence of continuous
variables  but even if we only consider the discrete component of the state space  the size of the
state space is exponential in the number of discrete variables  as is well known  ao  can be very
effective in solving planning problems that have a large state space because it only considers states
that are reachable from an initial state  and it uses an informative heuristic function to focus on
states that are reachable in the course of executing a good plan  as a result  ao  can often find an
optimal plan by exploring a small fraction of the entire state space 
we begin this section with a review of the standard ao  algorithm  then we consider how
to generalize ao  to search in a hybrid state space and discuss the properties of the generalized
algorithm  as well as its most efficient implementations 
    ao 
recall that ao  is an algorithm for and or graph search problems  nilsson        pearl        
such graphs arise in problems where there are choices  the or components   and each choice can
have multiple consequences  the and component   as is the case in planning under uncertainty 
hansen and zilberstein        show how and or graph search techniques can be used in solving
mdps 
following nilsson        and hansen and zilberstein         we define an and or graph as
a hypergraph  instead of arcs that connect pairs of nodes as in an ordinary graph  a hypergraph
has hyperarcs  or k connectors  that connect a node to a set of k successor nodes  when an mdp is
represented by a hypergraph  each node corresponds to a state  the root node corresponds to the start
state  and the leaf nodes correspond to terminal states  thus we often use the word state to refer to
the corresponding node in the hypergraph representing an mdp  a k connector corresponds to an
action that transforms a state into one of k possible successor states  with a probability attached to
each successor such that the probabilities sum to one  in this paper  we assume the and or graph
is acyclic  which is consistent with our assumption that the underlying mdp has a bounded horizon 
in and or graph search  a solution takes the form of an acyclic subgraph called a solution
graph  which is defined as follows 
 the start node belongs to a solution graph 
 for every non terminal node in a solution graph  exactly one outgoing k connector  corresponding to an action  is part of the solution graph and each of its successor nodes also belongs to
the solution graph 
 every directed path in the solution graph terminates at a terminal node 
a solution graph that maximizes expected cumulative reward is found by solving the following
system of equations 

  if s is a terminal
state  otherwise 
p

v   s   
   
 
 
  
maxaa s 
p
 
s s r s  s  a   r s     v  s     
where v   s  denotes the expected value of an optimal solution for state s  and v  is called the
optimal evaluation function  or value function in mdp terminology   note that this is identical to
  

fihao 

the optimality equation for hybrid state mdps defined in equation      if the latter is restricted to
a discrete state space  in keeping with the convention in the literature on mdps  we treat this as a
value maximization problem even though ao  is usually formalized as solving a cost minimization
problem 
for state space search problems that are formalized as and or graphs  an optimal solution
graph can be found using the heuristic search algorithm ao   nilsson        pearl         like other
heuristic search algorithms  the advantage of ao  over dynamic programming is that it can find an
optimal solution for a particular starting state without evaluating all problem states  therefore  a
graph is not usually supplied explicitly to the search algorithm  an implicit graph  g  is specified
implicitly by a start node or start state s and a successor function that generates the successors
states for any state action pair  the search algorithm constructs an explicit graph  g    that initially
consists only of the start state  a tip or leaf state of the explicit graph is said to be terminal if
it is a goal state  or some other state in which no action can be taken   otherwise  it is said to be
nonterminal  a nonterminal tip state can be expanded by adding to the explicit graph its outgoing
k connectors  one for each action  and any successor states not already in the explicit graph 
ao  solves a state space search problem by gradually building a solution graph  beginning from
the start state  a partial solution graph is defined similarly to a solution graph  with the difference
that tip states of a partial solution graph may be nonterminal states of the implicit and or graph 
a partial solution graph is defined as follows 
 the start state belongs to a partial solution graph 
 for every non tip state in a partial solution graph  exactly one outgoing k connector  corresponding to an action  is part of the partial solution graph and each of its successor states also
belongs to the partial solution graph 
 every directed path in a partial solution graph terminates at a tip state of the explicit graph 
the value of a partial solution graph is defined similarly to the value of a solution graph  the
difference is that if a tip state of a partial solution graph is nonterminal  it does not have a value
that can be propagated backwards  instead  we assume there is an admissible heuristic estimate
h s  of the maximal value solution graph for state s  a heuristic evaluation function h is said to
be admissible if h s   v   s  for every state s  we can recursively calculate an admissible heuristic
estimate v  s  of the optimal value of any state s in the explicit graph as follows 

   if s is a terminal state 
v  s   
a nonterminal tip state 

 h s  if s isp
 
 
 
maxaa s 
s  s p r s  s  a   r s     v  s    otherwise 

   

the best partial solution graph can be determined at any time by propagating heuristic estimates
from the tip states of the explicit graph to the start state  if we mark the action that maximizes
the value of each state  the best partial solution graph can be determined by starting at the root of
the graph and selecting the best  i e   marked  action for each reachable state 
table   outlines the ao  algorithm for finding an optimal solution graph in an acyclic and or
graph  it interleaves forward expansion of the best partial solution with a value update step that
updates estimated state values and the best partial solution  in the simplest version of ao   the
values of the expanded state and all of its ancestor states in the explicit graph are updated  but in
fact  the only ancestor states that need to be re evaluated are those from which the expanded state
can be reached by taking marked actions  i e   by choosing the best action for each state   thus 
the parenthetical remark in step   b i of table   indicates that a parent s  of state s is not added
to z unless both the estimated value of state s has changed and state s can be reached from state
s  by choosing the best action for state s    ao  terminates when the policy expansion step does not

  

fimeuleau  benazera  brafman  hansen   mausam

   the explicit graph g  initially consists of the start state s   
   while the best solution graph has some nonterminal tip state 
 a  expand best partial solution  expand some nonterminal tip state s of the best partial
solution graph and add any new successor states to g    for each new state s  added to
g  by expanding s  if s  is a terminal state then v  s          else v  s       h s    
 b  update state values and mark best actions 
i  create a set z that contains the expanded state and all of its ancestors in the explicit
graph along marked action arcs   i e   only include ancestor states from which the
expanded state can be reached by following the current best solution  
ii  repeat the following steps until z is empty 
a  remove from z a state s such that no descendant of s in g  occurs in z 
p
b  set v  s     maxaa s  s  p r s   s  a   r s      v  s     and mark the best action
for s   when determining the best action resolve ties arbitrarily  but give preference to the currently marked action  
 c  identify the best solution graph and all nonterminal states on its fringe
   return an optimal solution graph 
table    ao  algorithm 
find any nonterminal states on the fringe of the best solution graph  at this point  the best solution
graph is an optimal solution 
following the literature on and or graph search  we have so far referred to the solution found
by ao  as a solution graph  but in the following  when ao  is used to solve an mdp  we sometimes
follow the literature on mdps in referring to a solution as a policy  we also sometimes refer to it as
a policy graph  to indicate that a policy is represented in the form of a graph 
    hybrid state ao 
we now consider how to generalize ao  to solve a bounded horizon hybrid state mdp  the challenge
we face in applying ao  to this problem is the challenge of performing state space search in a hybrid
state space 
the solution we adopt is to search in an aggregate state space that is represented by an and or
graph in which there is a node for each distinct value of the discrete component of the state  in other
words  each node of the and or graph represents a region of the continuous state space in which
the discrete value is the same  given this partition of the continuous state space  we use and or
graph search techniques to solve the mdp for those parts of the state space that are reachable from
the start state under the best policy 
however  and or graph search techniques must be modified in important ways to allow search
in a hybrid state space that is represented in this way  in particular  there is no longer a correspondence between the nodes of the and or graph and individual states  each node now corresponds
to a continuous region of the state space  and different actions may be optimal for different hybrid states associated with the same search node  in the case of rover planning  for example  the
best action is likely to depend on how much energy or time is remaining  and energy and time are
continuous state variables 
to address this problem and still find an optimal solution  we attach to each search node a set of
functions  of the continuous variables  that make it possible to associate different values  heuristics 
and actions with different hybrid states that map to the same search node  as before  the explicit

  

fihao 

search graph consists of all nodes and edges of the and or graph that have been generated so far 
and describes all the states that have been considered so far by the search algorithm  the difference
is that we use a more complex state representation in which a set of continuous functions allows
representation and reasoning about the continuous part of the state space associated with a search
node 
we begin by describing this more complex node data structure  and then we describe the hao 
algorithm 
      data structures
each node n of the explicit and or graph g  consists of the following 
 the value of the discrete state variable 
 pointers to its parents and children in the explicit graph and the policy graph 
 openn             the open list  for each x  x  openn  x  indicates whether  n  x  is on
the frontier of the explicit graph  i e   generated but not yet expanded 
 closedn             the closed list  for each x  x  closedn  x  indicates whether  n  x 
is in the interior of the explicit graph  i e   already expanded 
note that  for all  n  x   openn  x   closedn  x       a state cannot be both open and
closed   there can be parts of the continuous state space associated with a node that are
neither open nor closed  until the explicit graph contains a trajectory from the start state
to a particular hybrid state  that hybrid state is not considered generated  even if the search
node to which it corresponds has been generated  such states are neither open nor closed  in
addition  only non terminal states can be open or closed  note that we do not refer to open
or closed nodes  instead  we refer to the hybrid states associated with nodes as being open or
closed 
 hn     the heuristic function  for each x  x  hn  x  is a heuristic estimate of the optimal
expected cumulative reward from state  n  x  
 vn     the value function  for any open state  n  x   vn  x    hn  x   for any closed state
 n  x   vn  x  is obtained by backing up the values of its successor states  as in equation     
 n     a  the policy  note that it is defined for closed states only 
 reachablen             for each x  x  reachablen  x  indicates whether  n  x  is reachable
by executing the current best policy beginning from the start state  n    x    
we assume that these various continuous functions  which represent information about the hybrid states associated with a search node  partition the state space associated with a node into a
discrete number of regions  and associate a distinct value or action with each region  given such
a partitioning  the hao  algorithm expands and evaluates these regions of the hybrid state space 
instead of individual hybrid states  the finiteness of the partition is important in order to ensure
that the search frontier can be extended by a finite number of expansions  and to ensure that hao 
can terminate after a finite number of steps  in our implementation of hao   described in section    we use the piecewise constant partitioning of a continuous state space proposed by feng et
al          however  any method of discrete partitioning could be used  provided that the condition
above holds  for example  li and littman        describe an alternative method of partitioning 
note that two forms of state space partitioning are used in our algorithm  first  the hybrid state
space is partitioned into a finite number of regions  one for each discrete state  where each of these

  

fimeuleau  benazera  brafman  hansen   mausam

regions corresponds to a node of the and or graph  second  the continuous state space associated with a particular node is further partitioned into smaller regions based on a piecewise constant
representation of a continuous function  such as the one used by feng et al         
in addition to this more complex representation of the nodes of an and or graph  our algorithm
requires a more complex definition of the the best  partial  solution  in standard ao   the oneto one correspondence between nodes and individual states means that a solution or policy can
be represented entirely by a graph  called the  partial  solution graph  in which a single action is
associated with each node  in the hao  algorithm  a continuum of states is associated with each
node  and different actions may be optimal for different regions of the state space associated with a
particular node  for the hao  algorithm  a  partial  solution graph is a sub graph of the explicit
graph that is defined as follows 
 the start node belongs to a solution graph 
 for every non tip node in a solution graph  one or more outgoing k connectors are part of the
solution graph  one for each action that is optimal for some hybrid state associated with the
node  and each of their successor nodes also belongs to the solution graph 
 every directed path in the solution graph terminates at a tip node of the explicit graph 
the key difference in this definition is that there may be more than one optimal action associated
with a node  since different actions may be optimal for different hybrid states associated with the
node  a policy is represented not only by a solution graph  but by the continuous functions n    
and reachablen      in particular  a  partial  policy  specifies an action for each reachable region of
the continuous state space  the best  partial  policy is the one that satisfies the following optimality
equation 
vn  x 

 

vn  x 

  hn  x  when  n  x  is a nonterminal open state 
 

z
x
 
max
pr n    n  x  a 
pr x    n  x  a  n     rn   x      vn   x     dx   

vn  x 

  when  n  x  is a terminal state 

aan  x 

n  n

   

x 

note that this optimality equation is only satisfied for regions of the state space that are reachable
from the start state   n    x    by following an optimal policy 
      algorithm
table   gives a high level summary of the hao  algorithm  in outline  it is the same as the ao 
algorithm  and consists of iteration of the same three steps  solution  or policy  expansion  use of
dynamic programming to update the current value function and policy  and analysis of reachability
to identify the frontier of the solution that is eligible for expansion  in detail  it is modified in several
important ways to allow search of a hybrid state space  in the following  we discuss the modifications
to each of these three steps 
policy expansion all nodes of the current solution graph are identified and one or more open
regions associated with these nodes are selected for expansion  that is  one or more regions of the
hybrid state space in the intersection of open and reachable is chosen for expansion  all actions
applicable to the states in these open regions are simulated  and the results of these actions are added
to the explicit graph  in some cases  this means adding a new node to the and or graph  in other
cases  it simply involves marking one or more regions of the continuous state space associated with
an existing node as open  more specifically  when an action leads to a new node  this node is added to
the explicit graph  and all states corresponding to this node that are reachable from the expanded
region s  after the action under consideration are marked as open  when an action leads to an
  

fihao 

   the explicit graph g  initially consists of the start node and corresponding start state  n    x    
marked as open and reachable 
   while reachablen  x   openn  x  is non empty for some  n  x  
 a  expand best partial solution  expand one or more region s  of open states on the frontier
of the explicit state space that is reachable by following the best partial policy  add new
successor states to g    in some cases  this requires adding a new node to the and or
graph  in other cases  it simply involves marking one or more regions of the continuous
state space associated with an existing node as open  states in the expanded region s 
are marked as closed 
 b  update state values and mark best actions 
i  create a set z that contains the node s  associated with the just expanded regions
of states and all ancestor nodes in the explicit graph along marked action arcs 
ii  decompose the part of the explicit and or graph that consists of nodes in z into
strongly connected components 
iii  repeat the following steps until z is empty 
a  remove from z a set of nodes such that     they all belong to the same connected
component  and     no descendant of these nodes occurs in z 
b  for every node n in this connected component and for all states  n  x  in any
expanded region of node n  set
vn  x    
 
max
aan  x 

x

pr n    n  x  a 

z


pr x    n  x  a  n     rn   x      vn   x     dx   

x 

n  n

and mark the best action   when determining the best action resolve ties arbitrarily  but give preference to the currently marked action   repeat until there
is no longer a change of value for any of these nodes 
 c  identify the best solution graph and all nonterminal states on its frontier  this step
updates reachablen  x  
   return an optimal policy 
table    hao  algorithm 
existing node  any region s  of markov states in this node that is both reachable from the expanded
region s  and not marked as closed  is marked open  expanded regions of the state space are marked
as closed  thus  different regions associated with the same node can be opened and expanded at
different times  this process is illustrated in figure    in this figure  nodes corresponding to a
distinct value for the discrete state are represented as rectangles  and circular connectors represent
actions  for each node  we see how many distinct continuous regions exist  for each such region we
see whether it is closed  c  or open  o   and whether it is reachable from the initial state  r 
when executing the current best policy  opt   for instance  in figure   a   node at start  has
a single region marked closed and reachable  and node lost has two regions  the smallest  open and
reachable  and the largest  closed and unreachable 
dynamic programming as in standard ao   the value of any newly expanded node n must
be updated by computing a bellman backup based on the value functions of the children of n

  

fimeuleau  benazera  brafman  hansen   mausam

at start 

at loc  
o

at start 
c

c

r

r

navigate
 start  loc  

at loc  

opt

c

r

navigate
 start  loc  

opt

r

navigate
 loc   loc  

lost
o

c

lost
o

r

o

c

c

r

at loc  

panoramic
camera

o

 a  before expansion

panoramic
camera

 b  after expansion

figure    expanding a region of the state space   a  before expansion  the nodes at start  
at loc   and lost have been previously created  the unique region in at loc   is the
next region to be expanded   b  after expansion  the action navigate loc   loc   that
can be applied in the expanded region has been added to the graph  this action can lead
either to the preexisting node lost  or to the new node at loc    the expanded region  in
at loc     as well as the continuous regions reachable from there  in lost and at loc    
are highlighted in a dotted framed  following expansion  the expanded region is closed 
discrete state at loc   has been added to the graph and all its reachable regions are
open  additionally  new open regions have been added to node lost 

in the explicit graph  for each expanded region of the state space associated with node n  each
action is evaluated  the best action is selected  and the corresponding continuous value function
is associated with the region  the continuous state value function is computed by evaluating the
continuous integral in equation      we can use any method for computing this integral  in our
implementation  we use the dynamic programming algorithm of feng et al          as reviewed
in section      they show that the continuous integral over x  can be computed exactly  as long as
the transition and reward functions satisfy certain conditions  note that  with some hybrid state
dynamic programming techniques such as feng et al          dynamic programming backups may
increase the number of pieces of the value function attached to the updated regions  figure   a   
once the expanded regions of the continuous state space associated with a node n are reevaluated  the new values must be propagated backward in the explicit graph  the backward
propagation stops at nodes where the value function is not modified  or at the root node  the
standard ao  algorithm  summarized in figure    assumes that the and or graph in which it
searches is acyclic  there are extensions of ao  for searching in and or graphs that contain
cycles  one line of research is concerned with how to find acyclic solutions in and or graphs
that contain cycles  jimenez   torras         another generalization of ao   called lao   allows
solutions to contain cycles or loops in order to specify policies for infinite horizon mdps  hansen
  zilberstein        

  

fihao 

at start 

at loc  
c c c

at start 
c

c

r

r

navigate
 start  loc  

at loc  

opt

c c c

r r r

navigate
 loc   loc  

lost
o

o

c

opt

c

r

at loc  
o

opt

r r r

navigate
 loc   loc  

opt

navigate
 start  loc  

at loc  

panoramic
camera

o
r

 a  dynamic programming

lost
o

o

c

r

r

r

c

panoramic
camera

 b  reachability analysis

figure    dynamic programming and reachability analysis  figure   continued    a  dynamic programming  the optimal policy has been reevaluated and navigate loc   loc   appears
optimal in some continuous states of at loc    node at loc   is represented with a finer
partition of the continuous state space to illustrate the fact that the backup increased the
number of pieces of the value function associated with the expanded region   b  reachability analysis  the newly created region of at loc   becomes reachable  as well as the
regions of lost that can be reached through navigate loc   loc   

given our assumption that every action has positive resource consumption  there can be no
loops in the state space of our problem because the resources available decrease at each step  but
surprisingly  there can be loops in the and or graph  this is possible because the and or
graph represents a projection of the state space onto a smaller space that consists of only the
discrete component of the state  for example  it is possible for the rover to return to the same
site it has visited before  the rover is not actually in the same state  since it has fewer resources
available  but the and or graph represents a projection of the state space that does not include
the continuous aspects of the state  such as resources  and this means the rover can visit a state that
projects to the same node of the and or graph as a state it visited earlier  as shown in figure   
as a result  there can be loops in the and or graph  and even loops in the part of the and or
graph that corresponds to a solution  but in a sense  these are phantom loops that can only
appear in the projected state space  and not in the real state space 
nevertheless we must modify the dynamic programming  dp  algorithm to deal with these loops 
because there are no loops in the real state space  we know that the exact value function can be
updated by a finite number of backups performed in the correct order  with one backup performed
for any state that can be visited along a path from the start state to the expanded node s   but
because multiple states can map to the same and or graph node  the continuous region of the
state space associated with a particular node may need to be evaluated more than once  to identify
the and or graph nodes that need to be evaluated more than once  we use the following two step
algorithm 

  

fimeuleau  benazera  brafman  hansen   mausam

at start 

at location  
energy     

at location  
energy     

at location  

at start 
energy      
at location  
energy     

at location  
energy     

at location  

figure    phantom loops in hao   solid boxes represent markov states  dashed boxes represent
search nodes  that is  the projection of markov states on the discrete components  arrows
represent possible state transition  bold arrows show an instance of phantom loop in the
search space 

first  we consider the part of the and or graph that consists of ancestor nodes of the just
expanded node s   this is the set z of nodes identified at the beginning of the dp step  we
decompose this part of the graph into strongly connected components  the graph of strongly
connected components is acyclic and can be used to prescribe the order of backups in almost the
same way as in the standard ao  algorithm  in particular  the nodes in a particular component are
not backed up until all nodes in its descendant components have been backed up  note that in the
case of an acyclic graph  every strongly connected component has a single node  it is only possible
for a connected component to have more than one node if there are loops in the and or graph 
if there are loops in the and or graph  the primary change in the dp step of the algorithm
occurs when it is time to perform backups on the nodes in a connected component with more than one
node  in this case  all nodes in the connected component are evaluated  then  they are repeatedly
re evaluated until the value functions of these nodes converge  that is  until there is no change in
the values of any of the nodes  because there are no loops in the real state space  convergence is
guaranteed to occur after a finite number of steps  typically  it occurs after a very small number
of steps  an advantage of decomposing the and or graph into connected components is that it
identifies loops and localizes their effect to a small number of nodes  in experiments in our test
domain  most nodes of the graph need to be evaluated just once during the dp step  and only a
small number of nodes  and often none  need to be evaluated more than once 
note that decomposition of the nodes in z into connected components is a method for improving
the efficiency of the dynamic programming step  and is not required for its correctness  the alternative of repeatedly updating all nodes in z until all their values converge is also correct  although
it is likely to result in many useless updates of already converged nodes 
analysis of reachability change in the value function can lead to change in the optimal policy 
and  thus  to change in which states are visited by the best policy  this  in turn  can affect which
open regions of the state space are eligible to be expanded  in this final step  hao  identifies the
best  partial  policy and recomputes reachablen for all nodes and states in the explicit graph  as
follows  see figure   b    for each node n in the best  partial  solution graph  consider each of its
parents n  in the solution graph  and all the actions a that can lead from one of the parents to n 
then reachablen  x  is the support of pn  x   where
x z
pn  x   
reachablen   x    pr n   n    x    a  pr x   n    x    a  n dx   
   
 n   a n

x

  

fihao 

that is  reachablen  x     x  x   pn  x        in equation      n is the set of pairs  n    a  where
a is the best action in n  for some reachable resource level 
n     n    a   n  a   x  x  pn   x       n   x    a  pr n   n    x  a        
it is clear that we can restrict our attention to state action pairs in n   only 
by performing this reachability analysis  hao  identifies the frontier of the state space that is
eligible for expansion  hao  terminates when this frontier is empty  that is  when it does not find
any hybrid states in the intersection of reachable and open 
    convergence and error bounds
we next consider some of the theoretical properties of hao   first  under reasonable assumptions 
we prove that hao  converges to an optimal policy after a finite number of steps  then we discuss
how to use hao  to find sub optimal policies with error bounds 
the proof of convergence after a finite number of steps depends  among other things  on the
assumption that a hybrid state mdp has a finite branching factor  in our implementation  this
means that for any region of the state space that can be represented by a hyper rectangle  the set
of successor regions after an action can be represented by a finite set of hyper rectangles  from
this assumption and the assumption that the number of actions is finite  it follows that for every
assignment n to the discrete variables  the set
 x  n  x is reachable from the initial state using some fixed sequence of actions 
is the union of a finite number of open or closed hyper rectangles  this assumption can be viewed
as a generalization of the assumption of a finite branching factor in a discrete and or graph upon
which the finite convergence proof of ao  depends 
theorem   if the heuristic functions hn are admissible  optimistic   all actions have positive resource consumptions  both continuous backups and action application are computable exactly in finite
time  and the branching factor is finite  then 
   at each step of hao   vn  x  is an upper bound on the optimal expected return in  n  x   for
all  n  x  expanded by hao  
   hao  terminates after a finite number of steps 
   after termination  vn  x  is equal to the optimal expected return in  n  x   for all  n  x  reachable
under an optimal policy  i e   reachablen  x      
proof      the proof is by induction  every state  n  x  is assigned an initial heuristic estimate 
and vn  x    hn  x   vn  x  by the admissibility of the heuristic evaluation function  we make the
inductive hypothesis that at some point in the algorithm  vn  x   vn  x  for every state  n  x   if a
backup is performed for any state  n  x  
 

z
x
vn  x   
max
pr n    n  x  a 
pr x    n  x  a  n     rn   x      vn   x     dx 
aan  x 

x 

n  n

 


max
aan  x 

x
n  n

 

z

pr n   n  x  a 

 

 

 

pr x   n  x  a  n    rn   x    
x 

  vn  x   
where the last equality restates the bellman optimality equation 

  

vn   x     dx 



fimeuleau  benazera  brafman  hansen   mausam

    because each action has positive  bounded from below  resource consumption  and resources
are finite and non replenishable  the complete implicit and or graph must be finite  for the same
reason  this graph can be turned into a finite graph without loops  along any directed loop in
this graph  the amount of maximal available resources must decrease by some  which is a positive
lower bound on the amount of resources consumed by an action  each node in this graph may be
expanded a number of times that is bounded by the number of its ancestor   each time a new
ancestor is discovered  it may lead to an update in the set of reachable regions for this node  
moreover  finite branching factor implies that the number of regions considered within each node
is bounded  because there are finite ways of reaching this node  each of which contributes a finite
number of hyper rectangles   thus  overall  the number of regions considered is finite  and the
processing required for each region expansion is finite  because action application and backups are
computed in finite time   this leads to the desired conclusion 
    the search algorithm terminates when the policy for the start state  n    x    is complete 
that is  when it does not lead to any unexpanded states  for every state  n  x  that is reachable
by following this policy  it is contradictory to suppose vn  x    vn  x  since that implies a complete
policy that is better than optimal  by the bellman optimality equation of equation      we know
that vn  x   vn  x  for every state in this complete policy  therefore  vn  x    vn  x   
hao  not only converges to an optimal solution  stopping the algorithm early allows a flexible
trade off between solution quality and computation time  if we assume that  in each state  there
is a done action that terminates execution with zero reward  in a rover problem  we would then
start a safe sequence   then we can evaluate the current policy at each step of the algorithm by
assuming that execution ends each time we reach a leaf of the policy graph  under this assumption 
the error of the current policy at each step of the algorithm can be bounded  we show this by
using a decomposition of the value function described by chakrabarti et al        and hansen and
zilberstein         we note that at any point in the algorithm  the value function can be decomposed
into two parts  gn  x  and hn  x   such that
gn  x 

 

gn  x 

 

  when  n  x  is an open state  on the fringe of the greedy policy  otherwise 
z
x
 

pr n   n  x  a  
pr x    n  x  a   n     rn  x    gn   x     dx   

   

x 

n  n

and
hn  x 

  hn  x  when  n  x  is an open state  on the fringe of the greedy policy  otherwise 
z
x
hn  x   
pr n    n  x  a  
pr x    n  x  a   n    hn   x   dx   
   
n  n

x 

where a is the action that maximizes the right hand side of equation      note that vn  x   
gn  x    hn  x   we use this decomposition of the value function to bound the error of the best policy
found so far  as follows 
theorem   at each step of the hao  algorithm  the error of the current best policy is bounded by
hn   x    
proof  for any state  n  x  in the explicit search space  a lower bound on its optimal value is given
by gn  x   which is the value that can be achieved by the current policy when the done action is
executed at all fringe states  and an upper bound is given by vn  x    gn  x    hn  x   as established
in theorem    it follows that hn   x    bounds the difference between the optimal value and the
current admissible value of any state  n  x   including the initial state  n    x    
note that the error bound for the initial state is hn   x      hn   x    at the start of the algorithm 
it decreases with the progress of the algorithm  and hn   x        when hao  converges to an optimal
solution 
  

fihao 

    heuristic function
the heuristic function hn focuses the search on reachable states that are most likely to be useful 
the more informative the heuristic  the more scalable the search algorithm  in our implementation
of hao  for the rover planning problem  which is described in detail in the next section  we used
the simple admissible heuristic function which assigns to each node the sum of all rewards associated
with goals that have not been achieved so far  note that this heuristic function only depends on the
discrete component of the state  and not on the continuous variables  that is  the function hn  x 
is constant over all values of x  it is obvious that this heuristic is admissible  since it represents
the maximum additional reward that could be achieved by continuing plan execution  although it
is not obvious that a heuristic this simple could be useful  the experimental results we present in
section   show that it is  we considered an additional  more informed heuristic function that solved
a relaxed  suitably discretized  version of the planning problem  however  taking into account the
time required to compute this heuristic estimate  the simpler heuristic performed better 
    expansion policy
hao  works correctly and converges to an optimal solution no matter which continuous region s 
of which node s  are expanded in each iteration  step   a   but the quality of the solution may
improve more quickly by using some heuristics to choose which region s  on the fringe to expand
next 
one simple strategy is to select a node and expand all continuous regions of this node that
are open and reachable  in a preliminary implementation  we expanded  the open regions of  the
node that is most likely to be reached using the current policy  changes in the value of these
states will have the greatest effect on the value of earlier nodes  implementing this strategy requires
performing the additional work involved in maintaining the probability associated with each state 
if such probabilities are available  one could also focus on expanding the most promising node  that
is  the node where the integral of hn  x  times the probability over all values of x is the highest  as
described by mausam  benazera  brafman  meuleau  and hansen        
hansen and zilberstein        observed that  in the case of lao   the algorithm is more efficient
if we expand several nodes in the fringe before performing dynamic programming in the explicit
graph  this is because the cost of performing the update of a node largely dominates the cost of
expanding a node  if we expand only one node of the fringe at each iteration  we might have to
perform more dp backups than if we expand several nodes with common ancestors before proceeding
to dp  in the limit  we might want to expand all nodes of the fringe at each algorithm iteration 
indeed  this variant of lao  proved the most efficient  hansen   zilberstein        
in the case of lao   updates are expensive because of the loops in the implicit graph  in hao  
the update of a region induces a call to the hybrid dynamic programming module for each open
region of the node  therefore  the same technique is likely to produce the same benefit 
pursuing this idea  we allowed our algorithm to expand all nodes in the fringe and all their
descendants up to a fixed depth at each iteration  we defined a parameter  called the expansion
horizon and denoted k  to represent  loosely speaking  the number of times the whole fringe is
expanded at each iteration  when k      hao  expands all open and reachable regions of all
nodes in the fringe before recomputing the optimal policy  when k      it expands all regions in
the fringe and all their children before updating the policy  at k     it also consider the grandchildren of regions in the fringe  and so on  when k tends to infinity  the algorithm essentially
performs an exhaustive search  it first expands the graph of all reachable nodes  then performs one
pass of  hybrid  dynamic programming in this graph to determine the optimal policy  by balancing
node expansion and update  the expansion horizon allows tuning the algorithm behavior from an
exhaustive search to a more traditional heuristic search  our experiments showed that a value of k
between   and    is optimal to solve our hardest benchmark problems  see section    

  

fimeuleau  benazera  brafman  hansen   mausam

start

obspt 

unsafe

c 
obs
pt 

featureless
c 

w 
w 

w 

obs
pt 

obspt 

audience

demo

label

  waypoint
name

  rock
  ip   champ

obspt 
far

  science cam 

figure    the k  rover  top left  was developed at the jet propulsion laboratory and nasa ames
research center as a prototype for the mer rovers  it is used to test advanced rover
software  including automated planners of the rovers activities  right  topological map
of the      is demo problem  arrows labeled ip   champ represent the opportunity
to deploy the arm against a rock  instrument placement  and take a picture of it with
the champ camera  arrows labeled science cam represent the opportunity to take a
remote picture of a rock with the science camera 

    updating multiple regions
the expansion policies described above are based on expanding all open regions of one or several
nodes simultaneously  they allow leveraging hybrid state dynamic programming techniques such as
those of feng et al         and li and littman         these techniques may compute in a single
iteration piecewise constant and linear value functions that cover a large range of continuous states 
possibly the whole space of possible values  in particular  they can back up in one iteration all
continuous states included between given bounds 
therefore  when several open regions of the same node are expanded at the same iteration of
hao   we can update all of them simultaneously by backing up a subset of continuous states that
includes all these regions  for instance  one may record lower bounds and upper bounds on each
continuous variable over the expanded regions  and then compute a value function that covers the
hyper rectangle between these bounds 
this modification of the algorithm does not impact convergence  as long as the value of all
expanded regions is computed  the convergence proof holds  however  execution time may be adversely affected if the expanded regions are a proper subset of the region of continuous states that is

  

fihao 

 a  value function vn     for the initial node  the
first plateau corresponds to analyzing r   the second plateau to analyzing r   and the third plateau
to analyzing both r  and r  

 b  the policy n     for the starting
node shows the partitions of the resource space where different actions
are optimal  dark  no action  grey 
navigation to r   light  analysis of
r  

figure     a  optimal value function for the initial state of the simple rover problem over all possible
values for the continuous resources  time and energy remaining   the value function is
partitioned into      pieces   b  optimal policy for the same set of states 

backed up  in that case  the values of states that are not open or not reachable is uselessly computed 
which deviates from a pure heuristic search algorithm 
however  this modification may also be beneficial because it avoids some redundant computation 
hybrid state dynamic programming techniques manipulate pieces of value functions  thus  if several
expanded regions are included in the same piece of the value function  their value is computed only
once  in practice  this benefit may outweigh the cost of evaluating useless regions  moreover  cost
is further reduced by storing the value functions associated with each node of the graph  so that
computed values of irrelevant regions are saved in case these regions become eligible for expansion
 i e   open and reachable  later  thus  this variant of hao  fully exploits hybrid state dynamic
programming techniques 

   experimental evaluation
in this section  we describe the performance of hao  in solving planning problems for a simulated
planetary exploration rover with two monotonic and continuous valued resources  time and battery
power  section     uses a simple toy example of this problem to illustrate the basic steps of
the hao  algorithm  section     tests the performance of the algorithm using a realistic  real size
nasa simulation of a rover and analyzes the results of the experiments  the simulation uses a
model of the k  rover  see figure    developed for the intelligent systems  is  demo at nasa
ames research center in october       pedersen et al          this is a complex real size model of
the k  rover that uses command names understandable by the rovers execution language  so that
the plans produced by our algorithm can be directly executed by the rover  for the experiments
reported in section      we did not simplify this nasa simulation model in any way 

  

fimeuleau  benazera  brafman  hansen   mausam

figure    first iteration of hao  on the toy problem  the explicit graph is marked by dim edges
and the solution graph is marked by thick edges  tip nodes         and   are shown with
constant heuristic functions and expanded nodes      and   are shown with backed up
value functions 

in the planning problem we consider  an autonomous rover must navigate in a planar graph
representing its surroundings and the authorized navigation paths  and schedule observations to
be performed on different rocks situated at different locations  only a subset of its observational
goals can be achieved in a single run due to limited resources  therefore  this is an oversubscribed
planning problem  it is also a problem of planning under uncertainty since each action has uncertain
positive resource consumptions and a probability of failing 
a significant amount of uncertainty in the domain comes from the tracking mechanism used by
the rover  tracking is the process by which the rover recognizes a rock based on certain features in
its camera image that are associated with the rock  during mission operations  a problem instance
containing a fixed set of locations  paths  and rocks is built from the last panoramic camera image
sent by the rover  each logical rock in this problem instance corresponds to a real rock  and
the rover must associate the two on the basis of features that can be detected by its instruments 
including its camera  as the rover moves and its camera image changes  the rover must keep track
of how those features of the image evolve  this process is uncertain and subject to faults that result
in losing track of a rock  in practice  tracking is modeled in the following way 
 in order to perform a measurement on a rock  the rover must be tracking this rock 
 to navigate along a path  it must be tracking one of the rocks that enables following this path 
the set of rocks that enable each path is part of the problem definition given to the planner 
 the decision to start tracking a rock must be made before the rover begins to move  once
the rover starts moving  it may keep track of a rock already being tracked or voluntarily stop
tracking it  but it cannot acquire a new rock that was not tracked initially 

  

fihao 

figure    second iteration of hao  on the toy problem 
 the rover may randomly lose track of some rocks while navigating along a path  the probability of losing track of a rock depends on the rock and the path followed  it is part of the
problem definition given to the planner 
 there is no way to reacquire a rock whose track has been lost  intentionally or by accident 
 the number of rocks tracked strongly influences the duration and resource consumption of
navigate actions  the higher the number of rocks tracked  the more costly it is to navigate
along a path  this is because the rover has to stop regularly to check and record the aspect
of each rock being tracked  this creates an incentive to limit the number of rocks tracked by
the rover given the set of goals it has chosen and the path it intends to follow 
so  the rover initially selects a set of rocks to track and tries to keep this set as small as possible
given its goals  once it starts moving  it may lose track of some rocks  and this may cause it to
reconsider the set of goals it will pursue and the route to get to the corresponding rocks  it can
also purposely stop tracking a rock when this is no longer necessary given the goals that are left to
achieve 
our implementation of hao  uses the dynamic programming algorithm developed by feng et
al         and summarized in section     in order to perform backups in a hybrid state space  and
partitions the continuous state space associated with a node into piecewise constant regions  it uses
multiple region updates as described in section      an upper bound on the each resource over
all expanded regions is computed  and all states included between these bounds and the minimal
possible resource levels are updated 
in our experiments  we use the variant of the hao  algorithm described in section      where a
parameter k sets the number of times the whole fringe is expanded at each iteration of hao   this
allows the behavior of the algorithm to be tuned from an exhaustive search to a heuristic search  we
used an expansion horizon of k     for the simple example in section     and a default expansion
horizon of k     for the larger examples in section      section       describes experiments with
different expansion horizons 

  

fimeuleau  benazera  brafman  hansen   mausam

figure    third iteration of hao  on the toy problem 
our implementation of hao  uses the simple heuristic described in section      augmented with
a small amount of domain knowledge  the value hn  x  of a state  n  x  is essentially equal to the
sum of the utilities of all goals not yet achieved in n  however  if the rover has already moved and a
certain rock is not being tracked in state n  then all goals requiring this rock to be tracked are not
included in the sum  this reflects the fact that  once the rover has moved  it cannot start tracking a
rock any more  and thus all goals that require this rock to be tracked are unreachable  the resulting
heuristic is admissible  i e   it never underestimates the value of a state   and it is straightforward to
compute  note that it does not depend on the current resource levels  so that the functions hn  x 
are constant over all values of x 
    example
we begin with a very simple example of the rover planning problem in order to illustrate the steps
of the algorithm  we solve this example using the same implementation of hao  that we use to
solve the more realistic examples considered in section     
in this example  the targets are two rocks  r  and r   positioned at locations l  and l  
respectively  the rovers initial location is l   and there is a direct path between l  and l  
analyzing rock r  yields a reward of    and analyzing rock r  yields a reward of     the rovers
action set is simplified  notably  it features a single action pic rx  to represents all the steps of
analyzing rock rx  and the stop tracking actions have been removed 
figure   shows the optimal value function and the optimal policy found by hao  for the starting
discrete state  and resources ranging over the whole space of possible values  figures      and  
show the step by step process by which hao  solves this problem  using an expansion horizon of
k      hao  solves this problem in three iterations  as follows 
 iteration    as shown in figure    hao  expands nodes      and   and computes a heuristic
function for the new tip nodes         and    the backup step yields value function estimates
for nodes      and    hao  then identifies the best solution graph and a new fringe node   

  

fihao 

 a       pieces 

 b      pieces 

 c      pieces 

figure     optimal value functions for the initial state of the simple rover problem with increasing initial resource levels  from left to right   the optimal return appears as a three
dimensional function carved into the reachable space of the heuristic function 
problem
name
rover 
rover 
rover 
rover 

rover
locations
 
 
 
  

paths

goals

fluents

actions

  
  
  
  

 
 
 
 

  
  
  
  

  
  
  
  

discrete
states
 approx  
       
        
        
        

reachable
discrete
states
   
    
     
     

explicit
graph

optimal
policy

longest
branch

   
    
    
    

  
  
  
  

  
  
  
  

table    size of benchmark rover problems 
 iteration    as shown in figure    hao  expands nodes         and     starting with
previous fringe node    and computes heuristic functions for the new tip nodes        and    
the heuristic value for node    is zero because  in this state  the rover has lost track of r 
and has already analyzed r   the backup step improves the accuracy of the value function in
several nodes  node    is the only new fringe node since    is a terminal node 
 iteration    as shown in figure    hao  expands node    and node     the search ends
after this iteration because there is no more open node in the optimal solution graph 
for comparison  figure    shows how the value function found by hao  varies with different initial
resource levels  in these figures  unreachable states are assigned a large constant heuristic value  so
that the value function for reachable states appears as carved in the plateau of the heuristic 
    performance
now  we describe hao s performance in solving four much larger rover planning problems using the
nasa simulation model  the characteristics of these problems are displayed in tables    columns
two to six show the size of the problems in terms of rover locations  paths  and goals  they also show
the total number of fluents  boolean state variables  and actions in each problem  columns seven
to ten report on the size of the discrete state space  the total number of discrete states is two raised
to the power of the number of fluents  although this is a huge state space  only a limited number
of states can be reached from the start state  depending on the initial resource levels  the eighth
column in table   shows the number of reachable discrete states if the initial time and energy levels
are set to their maximum value   the maximum initial resource levels are based on the scenario of
the      is demo and represent several hours of rover activity   it shows that simple reachability
  

fimeuleau  benazera  brafman  hansen   mausam

   

   
   
   
   
   
 

reachable
created
expanded
in optimal policy

   
number of discrete states

   
number of discrete states

   

reachable
created
expanded
in optimal policy

   
   
   
   
   

 

      

      
      
initial energy

      

 

      

 

    

    
    
initial time

    

     

    

     

    

     

    

     

 a  rover 
    

reachable
created
expanded
in optimal policy

    

number of discrete states

number of discrete states

    

    
    
    
    
 

 

      

      
      
initial energy

      

    
    
    
    
 

      

reachable
created
expanded
in optimal policy

    

 

    

    
    
initial time

 b  rover 
     

reachable
created
expanded
in optimal policy

     

number of discrete states

number of discrete states

     

     
     
    
 

 

      

             
initial energy

      

     
     
     
    
 

      

reachable
created
expanded
in optimal policy

 

    

    
    
initial time

 c  rover 
     

reachable
created
expanded
in optimal policy

     

number of discrete states

number of discrete states

     

     
     
    
 

 

      

             
initial energy

      

     
     
     
    
 

      

reachable
created
expanded
in optimal policy

 

    

    
    
initial time

 d  rover 
figure     number of nodes created and expanded by hao  vs  number of reachable discrete states 
the graphs in the left column are obtained by fixing the initial time to its maximum value
and varying the initial energy  the graphs in the right column are obtained by fixing the
initial energy to its maximum value and varying the initial time  results obtained with
k     
  

fihao 

analysis based on resource availability makes a huge difference  this is partly due to the fact that
our planning domain  which is very close to the k  execution language  does not allow many fluents
to be true simultaneously  columns nine and ten show the number of discrete states in the explicit
graph and in the optimal policy  more precisely  the former is the number of nodes created by hao  
that is  a subset of the reachable discrete states  the number of reachable discrete states  and thus
the size of the graph to explore  may seem small compared to other discrete combinatorial problems
solved by ai techniques  but each iteration  a continuous approximation of the two dimensional
backup is necessary to evaluate the hybrid state space associated with the graph  finally  the last
column of table   shows the length of the longest branch in the optimal policy when the initial
resource levels are set to their maximum value 
the largest of the four instances  that is  rover   is exactly the problem of the october     
is demo  this is considered a very large rover problem  for example  it is much larger than the
problems faced by the mer rovers that never visit more than one rock in a single planning cycle 
      efficiency of pruning
in a first set of simulations  we try to evaluate the efficiency of heuristic pruning in hao   that is 
the portion of the discrete search space that is spared from exploration through the use of admissible
heuristics  for this purpose  we compare the number of discrete states that are reachable for a given
resource level with the number of nodes created and expanded by hao   we also consider the
number of nodes in the optimal policy found by the algorithm 
results for the four benchmark problems are presented in figure     these curves are obtained
by fixing one resource to its maximum possible value and varying the other from   to its maximum 
therefore  they represent problems where mostly one resource is constraining  these result show 
notably  that a single resource is enough to constrain the reachability of the state space significantly 
not surprisingly  problems become larger as the initial resources increase  because more discrete
states become reachable  despite the simplicity of the heuristic used  hao  is able to by pass
a significant part of the search space  moreover  the bigger the problem  the more leverage the
algorithm can take from the simple heuristic 
these results are quite encouraging  but the number of nodes created and expanded does not
always reflect search time  therefore  we examine the time it takes for hao  to produce solutions 
      search time
figure    shows hao  search time for the same set of experiments  these curves do not exhibit the
same monotonicity and  instead  appear to show a significant amount of noise  it is surprising that
search time does not always increase with an increase in the initial levels of resource  although the
search space is bigger  this shows that search complexity does not depend on the size of the search
space alone  other factors must explain complexity peaks as observed in figure    
because the number of nodes created and expanded by the algorithm does not contain such noise 
the reason for the peaks of computation time must be the time spent in dynamic programming
backups  moreover  search time appears closely related to the complexity of the optimal policy 
figure    shows the number of nodes and branches in the policy found by the algorithm  as well as
the number of goals pursued by this policy  it shows that   i  in some cases  increasing the initial
resource level eliminates the need for branching and reduces the size of the optimal solution   ii  the
size of the optimal policy and  secondarily  its number of branches  explains most of the peaks in the
search time curves  therefore  the question is  why does a large solution graph induce a long time
spent in backups  there are two possible answers to this question  because the backups take longer
and or because more backups are performed  the first explanation is pretty intuitive  when the
policy graph contains many branches leading to different combinations of goals  the value functions
contain many humps and plateaus  and therefore many pieces  which impacts the complexity of
dynamic programming backups  however  we do not have at this time any empirical evidence to
  

fi  

  

  

  

  

  

  

  

search time  s 

search time  s 

meuleau  benazera  brafman  hansen   mausam

  
 
 

  
 
 

 

 

 

 

 

 

      

      
      
initial energy

      

 

      

 

    

    
    
initial time

    

     

    
    
initial time

    

     

   

   

   

   

   

   

   

search time  s 

search time  s 

 a  rover 
   

   
  
  

   
  
  

  

  

  

  

 

 

      

      
      
initial energy

      

 

      

 

    

     

     

     
search time  s 

search time  s 

 b  rover 
     

     
     
    
 

     
     
    

 

      

             
initial energy

      

 

      

 

    

    
    
initial time

    

     

 

    

    
    
initial time

    

     

     

     

     

     
search time  s 

search time  s 

 c  rover 

     

    

 

     

    

 

      

             
initial energy

      

 

      

 d  rover 
figure     hao  search time  the graphs in the left column are obtained by fixing the initial time
to its maximum value  and the graphs in the right column are obtained by fixing the
initial energy to its maximum  results obtained with k     

  

fihao 

confirm this hypothesis  conversely  we observe that the peak of figure    comes with an increase
of the number of backups  more work is required to explain this 
      expansion horizon
the results of section       show that hao  can leverage even a simple admissible heuristic to prune
a large portion of the search space  but it does not necessarily follow that hao  can outperform an
exhaustive search algorithm that creates a graph of all reachable states  and then executes one pass
of dynamic programming in this graph to find the optimal policy  although hao  expands a smaller
graph than such an exhaustive search  it must evaluate the graph more often  in section      we
introduced a parameter k for expansion horizon in order to allow adjustment of a trade off between
the time spent expanding nodes and the time spent evaluating nodes  we now study the influence
of this parameter on the algorithm 
figure    shows the number of nodes created and expanded by hao  as a function of the
expansion horizon for the four benchmark problem instances  not surprisingly  the algorithm creates
and expands more nodes as the expansion horizon increases  essentially  it behaves more like an
exhaustive search as k is increased  for the two smallest problem instances  and for large enough
values of k  the number of visited states levels off when the total number of reachable states is
reached  for the two largest problem instances  we had to interrupt the experiments once k reached
   because search time became too long 
figure    shows the effect of the expansion horizon on the search time of hao   for the smallest
problem instance  rover    hao  does not have a clear advantage over an exhaustive search  with
k        even though it explores fewer nodes  but for the three larger problem instances  hao 
has a clear advantage  for the rover  problem instance  the search time of hao  levels off after
k       indicating the limit of reachable states has been reached  however  the duration of such
an exhaustive search is several times longer than for hao  with smaller settings of k  the benefits
of hao  are clearer for the two largest problem instances  as k is increased  the algorithm is
quickly overwhelmed by the combinatorial explosion in the size of the search space  and simulations
eventually need to be interrupted because search time becomes too long  for these same problem
instances and smaller settings of k  hao  is able to efficiently find optimal solutions 
overall  our results show that there is a clear benefit to using admissible heuristics to prune the
search space  although the expansion horizon must be adjusted appropriately in order for hao  to
achieve a favorable trade off between node expansion time and node evaluation time 

   conclusion
we introduced a heuristic search approach to finding optimal conditional plans in domains characterized by continuous state variables that represent limited  consumable resources  the hao 
algorithm is a variant of the ao  algorithm that  to the best of our knowledge  is the first algorithm to deal with all of the following  limited continuous resources  uncertain action outcomes  and
over subscription planning  we tested hao  in a realistic nasa simulation of a planetary rover 
a complex domain of practical importance  and our results demonstrate its effectiveness in solving
problems that are too large to be solved by the straightforward application of dynamic programming  it is effective because heuristic search can exploit resource constraints  as well as an admissible
heuristic  in order to limit the reachable state space 
in our implementation  the hao  algorithm is integrated with the dynamic programming algorithm of feng et al          however hao  can be integrated with other dynamic programming
algorithms for solving hybrid state mdps  the feng et al  algorithm finds optimal policies under
the limiting assumptions that transition probabilities are discrete  and rewards are either piecewiseconstant or piecewise linear  more recently developed dynamic programming algorithms for hybridstate mdps make less restrictive assumptions  and also have the potential to improve computational

  

fimeuleau  benazera  brafman  hansen   mausam

  

 

  

 

  

 

 

 

      

             
initial energy

      

  

 
      

 

nodes
branches
goals

 

  

 

  

 

  

 

 

 

    

    
    
initial time

    

number of branches and goals

 

  

number of nodes

  
number of nodes

 

nodes
branches
goals

number of branches and goals

  

 
     

 a  rover 

  

 

  

 

  

 

 

 

      

             
initial energy

      

  

 
      

 

nodes
branches
goals

 

  

 

  

 

  

 

 

 

    

    
    
initial time

    

number of branches and goals

 

  

number of nodes

nodes
branches
goals

  
number of nodes

 
number of branches and goals

  

 
     

 b  rover 

  

 

  

 

  

 

 

 

      

             
initial energy

      

  

 
      

 

nodes
branches
goals

 

  

 

  

 

  

 

 

 

    

    
    
initial time

    

number of branches and goals

 

  

number of nodes

nodes
branches
goals

  
number of nodes

 
number of branches and goals

  

 
     

 c  rover 

  

 

  

 

  

 

 

 

      

             
initial energy

      

  

 
      

 

nodes
branches
goals

 

  

 

  

 

  

 

 

 

    

    
    
initial time

    

number of branches and goals

 

   

number of nodes

nodes
branches
goals

  
number of nodes

 
number of branches and goals

   

 
     

 d  rover 
figure     complexity of the optimal policy  number of nodes  branches  and goals in the optimal
policy in the same setting as figure    

  

fihao 

   

number of discrete states

   
number of discrete states

    

created
expanded

   
   
   
   
   
 

 

 

  
  
  
expansion horizon

    
    
    
    
 

  

created
expanded

    

 

 

  

 a  rover 
     

number of discrete states

number of discrete states

  

created
expanded

     

     
    
    
    
    
 

  

 b  rover 
     

created
expanded

     

  
  
expansion horizon

     
     
    
    
    
    

 

 

  
  
expansion horizon

 

  

 c  rover 

 

 

  
  
expansion horizon

  

 d  rover 

figure     influence of the expansion horizon on the number of nodes visited by the algorithm 
efficiency  li   littman        marecki et al          integrating hao  with one of these algorithms
could improve performance further 
there are several other interesting directions in which this work could be extended  in developing hao   we made the assumptions that every action consumes some resource and resources are
non replenishable  without these assumptions  the same state could be revisited and an optimal
plan could have loops as well as branches  generalizing our approach to allow plans with loops 
which seems necessary to handle replenishable resources  requires generalizing the heuristic search
algorithm lao  to solve hybrid mdps  hansen   zilberstein         another possible extension is
to allow continuous action variables in addition to continuous state variables  finally  our heuristic
search approach could be combined with other approaches to improving scalability  such as hierarchical decomposition  meuleau   brafman         this would allow it to handle the even larger state
spaces that result when the number of goals in an over subscription planning problem is increased 
acknowledgments
this work was funded by the nasa intelligent systems program  grant nra         eric hansen
was supported in part by a nasa summer faculty fellowship and by funding from the mississippi
space grant consortium  this work was performed while emmanuel benazera was working at
nasa ames research center and ronen brafman was visiting nasa ames research center  both
as consultants for the research institute for advanced computer science  ronen brafman was
supported in part by the lynn and william frankel center for computer science  the paul ivanier
center for robotics and production management  and isf grant          nicolas meuleau is a
consultant of carnegie mellon university at nasa ames research center 

  

fimeuleau  benazera  brafman  hansen   mausam

  

    
    
    

  

search time  s 

search time  s 

  

  
  

    
   
   
   

  
 

    

   
 

 

  
  
  
expansion horizon

 

  

 

 

  

 a  rover 

  

     
     
search time  s 

     
search time  s 

  

 b  rover 

     

     
     
    
 

  
  
expansion horizon

     
     
     
    

 

 

  
  
expansion horizon

 

  

 c  rover 

 

 

  
  
expansion horizon

  

 d  rover 

figure     influence of the expansion horizon on overall search time 

references
altman  e          constrained markov decision processes  chapman and hall crc 
bertsekas  d     tsitsiklis  j          neural dynamic programming  athena scientific  belmont 
ma 
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions
and computational leverage  journal of artificial intelligence research          
boyan  j     littman  m          exact solutions to time dependent mdps  in advances in neural
information processing systems     pp      mit press  cambridge 
bresina  j   dearden  r   meuleau  n   ramakrishnan  s   smith  d     washington  r         
planning under continuous time and resource uncertainty  a challenge for ai  in proceedings
of the eighteenth conference on uncertainty in artificial intelligence  pp       
bresina  j   jonsson  a   morris  p     rajan  k          activity planning for the mars exploration
rovers  in proceedings of the fifteenth international conference on automated planning and
scheduling  pp       
chakrabarti  p   ghose  s     desarkar  s          admissibility of ao  when heuristics overestimate  aritificial intelligence            
feng  z   dearden  r   meuleau  n     washington  r          dynamic programming for structured continuous markov decision problems  in proceedings of the twentieth conference on
uncertainty in artificial intelligence  pp         
  

fihao 

friedman  j   bentley  j     finkel  r          an algorithm for finding best matches in logarithmic
expected time  acm trans  mathematical software               
hansen  e     zilberstein  s          lao   a heuristic search algorithm that finds solutions with
loops  artificial intelligence            
jimenez  p     torras  c          an efficient algorithm for searching implicit and or graphs with
cycles  artificial intelligence           
kveton  b   hauskrecht  m     guestrin  c          solving factored mdps with hybrid state and
action variables  journal of artificial intelligence research             
li  l     littman  m          lazy approximation for solving continuous finite horizon mdps  in
proceedings of the twentieth national conference on artificial intelligence  pp           
marecki  j   koenig  s     tambe  m          a fast analytical algorithm for solving markov decision
processes with real valued resources  in proceedings of the   th international joint conference
on artificial intelligence  ijcai     pp           
mausam  benazera  e   brafman  r   meuleau  n     hansen  e          planning with continuous resources in stochastic domains  in proceedings of the nineteenth international joint
conference on artificial intelligence  pp            professional book center  denver  co 
meuleau  n     brafman  r          hierarchical heuristic forward search in stochastic domains  in
proceedings of the   th international joint conference on artificial intelligence  ijcai     
pp           
munos  r     moore  a          variable resolution discretization in optimal control  machine
learning                   
nilsson  n          principles of artificial intelligence  tioga publishing company  palo alto  ca 
pearl  j          heuristics  intelligent search strategies for computer problem solving  addisonwesley 
pedersen  l   smith  d   deans  m   sargent  r   kunz  c   lees  d     rajagopalan  s         
mission planning and target tracking for autonomous instrument placement  in proceedings
of the      ieee aerospace conference   big sky  montana 
puterman  m          markov decision processes  discrete stochastic dynamic programming 
wiley  new york  ny 
rust  j          using randomization to break the curse of dimensionality  econimetrica         
    
smith  d          choosing objectives in over subscription planning  in proceedings of the fourteenth
international conference on automated planning and scheduling  pp         
thiebaux  s   gretton  c   slaney  j   price  d     kabanza  f          decision theoretic planning
with non markovian rewards  journal of artificial intelligence research           
van den briel  m   sanchez  r   do  m     kambhampati  s          effective approaches for partial
satisfation  over subscription  planning  in proceedings of the nineteenth national conference
on artificial intelligence  pp         

  

fi