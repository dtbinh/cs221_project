journal of artificial intelligence research                  

submitted        published      

learning document level semantic properties
from free text annotations
s r k  branavan
harr chen
jacob eisenstein
regina barzilay

branavan   csail   mit  edu
harr   csail   mit  edu
jacobe   csail   mit  edu
regina   csail   mit  edu

computer science and artificial intelligence laboratory
massachusetts institute of technology
   massachusetts avenue  cambridge ma      

abstract
this paper presents a new method for inferring the semantic properties of documents by leveraging free text keyphrase annotations  such annotations are becoming increasingly abundant due
to the recent dramatic growth in semi structured  user generated online content  one especially
relevant domain is product reviews  which are often annotated by their authors with pros cons
keyphrases such as a real bargain or good value  these annotations are representative of the
underlying semantic properties  however  unlike expert annotations  they are noisy  lay authors
may use different labels to denote the same property  and some labels may be missing  to learn
using such noisy annotations  we find a hidden paraphrase structure which clusters the keyphrases 
the paraphrase structure is linked with a latent topic model of the review texts  enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic
properties of multiple reviews  our approach is implemented as a hierarchical bayesian model with
joint inference  we find that joint inference increases the robustness of the keyphrase clustering and
encourages the latent topics to correlate with semantically meaningful properties  multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing
single and multiple documents into a set of semantically salient keyphrases 

   introduction
identifying the document level semantic properties implied by a text is a core problem in natural
language understanding  for example  given the text of a restaurant review  it would be useful to
extract a semantic level characterization of the authors reaction to specific aspects of the restaurant  such as food and service quality  see figure     learning based approaches have dramatically
increased the scope and robustness of such semantic processing  but they are typically dependent on
large expert annotated datasets  which are costly to produce  zaenen        
we propose to use an alternative source of annotations for learning  free text keyphrases produced by novice users  as an example  consider the lists of pros and cons that often accompany
reviews of products and services  such end user annotations are increasingly prevalent online  and
they grow organically to keep pace with subjects of interest and socio cultural trends  beyond such
pragmatic considerations  free text annotations are appealing from a linguistic standpoint because
they capture the intuitive semantic judgments of non specialist language users  in many real world
datasets  these annotations are created by the documents original author  providing a direct window
into the semantic judgments that motivated the document text 

c
    
ai access foundation  all rights reserved 

fib ranavan   c hen   e isenstein     barzilay

pros cons  great nutritional value
    combines it all  an amazing product  quick and friendly service  cleanliness  great nutrition    
pros cons  a bit pricey  healthy
    is an awesome place to go if you are health conscious  they have some really great low calorie dishes
and they publish the calories and fat grams per serving 

figure    excerpts from online restaurant reviews with pros cons phrase lists  both reviews assert
that the restaurant serves healthy food  but use different keyphrases  additionally  the
first review discusses the restaurants good service  but is not annotated as such in its
keyphrases 

the major obstacle to the computational use of such free text annotations is that they are inherently noisy  there is no fixed vocabulary  no explicit relationship between annotation keyphrases 
and no guarantee that all relevant semantic properties of a document will be annotated  for example 
in the pros cons annotations accompanying the restaurant reviews in figure    the same underlying
semantic idea is expressed in different ways through the keyphrases great nutritional value and
healthy  additionally  the first review discusses quality of service  but is not annotated as such 
in contrast  expert annotations would replace synonymous keyphrases with a single canonical label  and would fully label all semantic properties described in the text  such expert annotations
are typically used in supervised learning methods  as we will demonstrate in the paper  traditional
supervised approaches perform poorly when free text annotations are used instead of clean  expert
annotations 
this paper demonstrates a new approach for handling free text annotation in the context of a
hidden topic analysis of the document text  we show that regularities in the text can clarify noise
in the annotations  for example  although great nutritional value and healthy have different
surface forms  the text in documents that are annotated by these two keyphrases will likely be
similar  by modeling the relationship between document text and annotations over a large dataset 
it is possible to induce a clustering over the annotation keyphrases that can help to overcome the
problem of inconsistency  our model also addresses the problem of incompleteness  when novice
annotators fail to label relevant semantic topics  by estimating which topics are predicted by the
document text alone 
central to this approach is the idea that both document text and the associated annotations reflect
a single underlying set of semantic properties  in the text  the semantic properties correspond to the
induced hidden topics  this is similar to the growing body of work on latent topic models  such as
latent dirichlet allocation  lda  blei  ng    jordan         however  unlike existing work on topic
modeling  we tie hidden topics in the text with clusters of observed keyphrases  this connection is
motivated by the idea that both the text and its associated annotations are grounded in a shared set
of semantic properties  by modeling these properties directly  we ensure that the inferred hidden
topics are semantically meaningful  and that the clustering over free text annotations is robust to
noise 
our approach takes the form of a hierarchical bayesian framework  and includes an lda style
component in which each word in the text is generated from a mixture of multinomials  in addition  we also incorporate a similarity matrix across the universe of annotation keyphrases  which is

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

constructed based on the orthographic and distributional features of the keyphrases  we model this
matrix as being generated from an underlying clustering over the keyphrases  such that keyphrases
that are clustered together are likely to produce high similarity scores  to generate the words in each
document  we model two distributions over semantic properties  one governed by the annotation
keyphrases and their clusters  and a background distribution to cover properties not mentioned in the
annotations  the latent topic for each word is drawn from a mixture of these two distributions  after
learning model parameters from a noisily labeled training set  we can apply the model to unlabeled
data 
we build a system that extracts semantic properties from reviews of products and services  this
system uses as training corpus that includes user created free text annotations of the pros and cons
in each review  training yields two outputs  a clustering of keyphrases into semantic properties  and
a topic model that is capable of inducing the semantic properties of unlabeled text  the clustering
of annotation keyphrases is relevant for applications such as content based information retrieval 
allowing users to retrieve documents with semantically relevant annotations even if their surface
forms differ from the query term  the topic model can be used to infer the semantic properties of
unlabeled text 
the topic model can also be used to perform multi document summarization  capturing the key
semantic properties of multiple reviews  unlike traditional extraction based approaches to multidocument summarization  our induced topic model abstracts the text of each review into a representation capturing the relevant semantic properties  this enables comparison between reviews even
when they use superficially different terminology to describe the same set of semantic properties 
this idea is implemented in a review aggregation system that extracts the majority sentiment of
multiple reviewers for each product or service  an example of the output produced by this system
is shown in figure    this system is applied to reviews in     product categories  allowing users
to navigate the semantic properties of        products based on a total of         reviews  the
effectiveness of our approach is confirmed by several evaluations 
for the summarization of both single and multiple documents  we compare the properties inferred by our model with expert annotations  our approach yields substantially better results than
alternatives from the research literature  in particular  we find that learning a clustering of free text
annotation keyphrases is essential to extracting meaningful semantic properties from our dataset 
in addition  we compare the induced clustering with a gold standard clustering produced by expert
annotators  the comparison shows that tying the clustering to the hidden topic model substantially
improves its quality  and that the clustering induced by our system coheres well with the clustering
produced by expert annotators 
the remainder of the paper is structured as follows  section   compares our approach with previous work on topic modeling  semantic property extraction  and multi document summarization 
section   describes the properties of free text annotations that motivate our approach  the model
itself is described in section    and a method for parameter estimation is presented in section   
section   describes the implementation and evaluation of single document and multi document
summarization systems using these techniques  we summarize our contributions and consider directions for future work in section    the code  datasets and expert annotations used in this paper
are available online at http   groups csail mit edu rbg code precis  

   

fib ranavan   c hen   e isenstein     barzilay

   related work
the material presented in this section covers three lines of related work  first  we discuss work
on bayesian topic modeling that is related to our technique for learning from free text annotations 
next  we discuss state of the art methods for identifying and analyzing product properties from
the review text  finally  we situate our summarization work in the landscape of prior research on
multi document summarization 
    bayesian topic modeling
recent work in the topic modeling literature has demonstrated that semantically salient topics can
be inferred in an unsupervised fashion by constructing a generative bayesian model of the document text  one notable example of this line of research is latent dirichlet allocation  lda  blei
et al          in the lda framework  semantic topics are equated to latent distributions of words
in a text  thus  each document is modeled as a mixture of topics  this class of models has been
used for a variety of language processing tasks including topic segmentation  purver  kording 
griffiths    tenenbaum         named entity resolution  bhattacharya   getoor         sentiment
ranking  titov   mcdonald      b   and word sense disambiguation  boyd graber  blei    zhu 
      
our method is similar to lda in that it assigns latent topic indicators to each word in the
dataset  and models documents as mixtures of topics  however  the lda model is unsupervised 
and does not provide a method for linking the latent topics to external observed representations of
the properties of interest  in contrast  our model exploits the free text annotations in our dataset to
ensure that the induced topics correspond to semantically meaningful properties 
combining topics induced by lda with external supervision was first considered by blei and
mcauliffe        in their supervised latent dirichlet allocation  slda  model  the induction of
the hidden topics is driven by annotated examples provided during the training stage  from the perspective of supervised learning  this approach succeeds because the hidden topics mediate between
document annotations and lexical features  blei and mcauliffe describe a variational expectationmaximization procedure for approximate maximum likelihood estimation of the models parameters  when tested on two polarity assessment tasks  slda shows improvement over a model in
which topics where induced by an unsupervised model and then added as features to a supervised
model 
the key difference between our model and slda is that we do not assume access to clean
supervision data during training  since the annotations provided to our algorithm are free text in
nature  they are incomplete and fraught with inconsistency  this substantial difference in input
structure motivates the need for a model that simultaneously induces the hidden structure in freetext annotations and learns to predict properties from text 
    property assessment for review analysis
our model is applied to the task of review analysis  traditionally  the task of identifying the properties of a product from review texts has been cast as an extraction problem  hu   liu        liu 
hu    cheng        popescu  nguyen    etzioni         for example  hu and liu        employ
association mining to identify noun phrases that express key portions of product reviews  the polarity of the extracted phrases is determined using a seed set of adjectives expanded via wordnet

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

relations  a summary of a review is produced by extracting all property phrases present verbatim in
the document 
property extraction was further refined in o pine  popescu et al          another system for
review analysis  o pine employs a novel information extraction method to identify noun phrases
that could potentially express the salient properties of reviewed products  these candidates are then
pruned using wordnet and morphological cues  opinion phrases are identified using a set of handcrafted rules applied to syntactic dependencies extracted from the input document  the semantic
orientation of properties is computed using a relaxation labeling method that finds the optimal assignment of polarity labels given a set of local constraints  empirical results demonstrate that o pine
outperforms hu and lius system in both opinion extraction and in identifying the polarity of opinion words 
these two feature extraction methods are informed by human knowledge about the way opinions
are typically expressed in reviews  for hu and liu         human knowledge is encoded using
wordnet and the seed adjectives  for popescu et al          opinion phrases are extracted via handcrafted rules  an alternative approach is to learn the rules for feature extraction from annotated
data  to this end  property identification can be modeled in a classification framework  kim  
hovy         a classifier is trained using a corpus in which free text pro and con keyphrases are
specified by the review authors  these keyphrases are compared against sentences in the review
text  sentences that exhibit high word overlap with previously identified phrases are marked as pros
or cons according to the phrase polarity  the rest of the sentences are marked as negative examples 
clearly  the accuracy of the resulting classifier depends on the quality of the automatically induced annotations  our analysis of free text annotations in several domains shows that automatically mapping from even manually extracted annotation keyphrases to a document text is a difficult
task  due to variability in keyphrase surface realizations  see section     as we argue in the rest of
this paper  it is beneficial to explicitly address the difficulties inherent in free text annotations  to
this end  our work is distinguished in two significant ways from the property extraction methods described above  first  we are able to predict properties beyond those that appear verbatim in the text 
second  our approach also learns the semantic relationships between different keyphrases  allowing
us to draw direct comparisons between reviews even when the semantic ideas are expressed using
different surface forms 
working in the related domain of web opinion mining  lu and zhai        describe a system
that generates integrated opinion summaries  which incorporate expert written articles  e g   a review from an online magazine  and user generated ordinary opinion snippets  e g   mentions in
blogs   specifically  the expert article is assumed to be structured into segments  and a collection of
representative ordinary opinions is aligned to each segment  probabilistic latent semantic analysis
 plsa  is used to induce a clustering of opinion snippets  where each cluster is attached to one
of the expert article segments  some clusters may also be unaligned to any segment  indicating
opinions that are entirely unexpressed in the expert article  ultimately  the integrated opinion summary is this combination of a single expert article with multiple user generated opinion snippets that
confirm or supplement specific segments of the review 
our works final goal is different  we aim to provide a highly compact summary of a multitude of user opinions by identifying the underlying semantic properties  rather than supplementing
a single expert article with user opinions  we specifically leverage annotations that users already
provide in their reviews  thus obviating the need for an expert article as a template for opinion inte 

   

fib ranavan   c hen   e isenstein     barzilay

gration  consequently  our approach is more suitable for the goal of producing concise keyphrase
summarizations of user reviews  particularly when no review can be taken as authoritative 
the work closest in methodology to our approach is a review summarizer developed by titov
and mcdonald      a   their method summarizes a review by selecting a list of phrases that
express writers opinions in a set of predefined properties  e g    food and ambiance for restaurant
reviews   the system has access to numerical ratings in the same set of properties  but there is no
training set providing examples of appropriate keyphrases to extract  similar to slda  their method
uses the numerical ratings to bias the hidden topics towards the desired semantic properties  phrases
that are strongly associated with properties via hidden topics are extracted as part of a summary 
there are several important differences between our work and the summarization method of
titov and mcdonald  their method assumes a predefined set of properties and thus cannot capture
properties outside of that set  moreover  consistent numerical annotations are required for training 
while our method emphasizes the use of free text annotations  finally  since titov and mcdonalds
algorithm is extractive  it does not facilitate property comparison across multiple reviews 
    multidocument summarization
this paper also relates to a large body of work in multi document summarization  researchers
have long noted that a central challenge of multi document summarization is identifying redundant
information over input documents  radev   mckeown        carbonell   goldstein        mani
  bloedorn        barzilay  mckeown    elhadad         this task is of crucial significance
because multi document summarizers operate over related documents that describe the same facts
multiple times  in fact  it is common to assume that repetition of information among related sources
is an indicator of its importance  barzilay et al         radev  jing    budzikowska        nenkova 
vanderwende    mckeown         many of these algorithms first cluster sentences together  and
then extract or generate sentence representatives for the clusters 
identification of repeated information is equally central in our approach  our multi document
summarization method only selects properties that are stated by a plurality of users  thereby eliminating rare and or erroneous opinions  the key difference between our algorithm and existing summarization systems is the method for identifying repeated expressions of a single semantic property 
since most of the existing work on multi document summarization focuses on topic independent
newspaper articles  redundancy is identified via sentence comparison  for instance  radev et al 
       compare sentences using cosine similarity between corresponding word vectors  alternatively  some methods compare sentences via alignment of their syntactic trees  barzilay et al        
marsi   krahmer         both string  and tree based comparison algorithms are augmented with
lexico semantic knowledge using resources such as wordnet 
the approach described in this paper does not perform comparisons at the sentence level  instead  we first abstract reviews into a set of properties and then compare property overlap across
different documents  this approach relates to domain dependent approaches for text summarization  radev   mckeown        white  korelsky  cardie  ng  pierce    wagstaff        elhadad
  mckeown         these methods identify the relations between documents by comparing their
abstract representations  in these cases  the abstract representation is constructed using off the shelf
information extraction tools  a template specifying what types of information to select is crafted
manually for a domain of interest  moreover  the training of information extraction systems requires
a corpus manually annotated with the relations of interest  in contrast  our method does not require

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

incompleteness
property
good food
good service
good price
bad food
bad service
bad price
average

recall

precision

f score

     
     
     
     
     
     
     

     
     
     
     
     
     
     

     
     
     
     
     
     
     

inconsistency
keyphrase top keyphrase
count
coverage  
  
    
  
    
  
    
  
    
  
    
  
    
    
    

table    incompleteness and inconsistency in the restaurant domain  for six major properties prevalent in the reviews  the incompleteness figures are the recall  precision  and f score of the
author annotations  manually clustered into properties  against the gold standard property
annotations  inconsistency is measured by the number of different keyphrase realizations
with at least five occurrences associated with each property  and the percentage frequency
with which the most commonly occurring keyphrases is used to annotate a property  the
averages in the bottom row are weighted according to frequency of property occurrence 

manual template specification or corpora annotated by experts  while the abstract representations
that we induce are not as linguistically rich as extraction templates  they nevertheless enable us to
perform in depth comparisons across different reviews 

   analysis of free text keyphrase annotations
in this section  we explore the characteristics of free text annotations  aiming to quantify the degree
of noise observed in this data  the results of this analysis motivate the development of the learning
algorithm described in section   
we perform this investigation in the domain of online restaurant reviews using documents downloaded from the popular epinions  website  users of this website evaluate products by providing
both a textual description of their opinion  as well as concise lists of keyphrases  pros and cons 
summarizing the review  pros cons keyphrases are an appealing source of annotations for online
review texts  however  they are contributed independently by multiple users and are thus unlikely
to be as clean as expert annotations  in our analysis  we focus on two features of free text annotations  incompleteness and inconsistency  the measure of incompleteness quantifies the degree of
label omission in free text annotations  while inconsistency reflects the variance of the keyphrase
vocabulary used by various annotators 
to test the quality of these user generated annotations  we compare them against expert annotations produced in a more systematic fashion  this annotation effort focused on six properties
that were commonly mentioned by the review authors  specifically those shown in table    given
a review and a property  the task is to assess whether the reviews text supports the property  these
annotations were produced by two judges guided by a standardized set of instructions  in contrast
to author annotations from the website  the judges conferred during a training session to ensure consistency and completeness  the two judges collectively annotated     reviews  with    annotated
   http   www epinions com 

   

fib ranavan   c hen   e isenstein     barzilay

property  good price
relatively inexpensive  dirt cheap  relatively cheap  great price  fairly priced  well priced  very reasonable
prices  cheap prices  affordable prices  reasonable cost

figure    examples of the many different paraphrases related to the property good price that appear
in the pros cons keyphrases of reviews used for our inconsistency analysis 

by both  cohens kappa  a measure of inter annotator agreement that ranges from zero to one  is
     on this joint set  indicating high agreement  cohen         on average  each review text was
annotated with      properties 
separately  one of the judges also standardized the free text pros cons annotations for the same
    reviews  each reviews keyphrases were matched to the same six properties  this standardization allows for direct comparison between the properties judged to be supported by a reviews
text and the properties described in the same reviews free text annotations  we find that many semantic properties that were judged to be present in the text were not user annotated  on average 
the keyphrases expressed      relevant semantic properties per document  while the text expressed
     properties  this gap demonstrates the frequency with which authors omitted relevant semantic
properties from their review annotations 
    incompleteness
to measure incompleteness  we compare the properties stated by review authors in the form of
pros and cons against those stated only in the review text  as judged by expert annotators  this
comparison is performed using precision  recall and f score  in this setting  recall is the proportion
of semantic properties in the text for which the review author also provided at least one annotation
keyphrase  precision is the proportion of keyphrases that conveyed properties judged to be supported
by the text  and f score is their harmonic mean  the results of the comparison are summarized in
the left half of table   
these incompleteness results demonstrate the significant discrepancy between user and expert
annotations  as expected  recall is quite low  more than     of property occurrences are stated in
the review text without being explicitly mentioned in the annotations  the precision scores indicate
that the converse is also true  though to a lesser extent  some keyphrases will express properties
not mentioned in text 
interestingly  precision and recall vary greatly depending on the specific property  they are
highest for good food  matching the intuitive notion that high food quality would be a key salient
property of a restaurant  and thus more likely to be mentioned in both text and annotations  conversely  the recall for good service is lower  for most users  high quality of service is apparently
not a key point when summarizing a review with keyphrases 
    inconsistency
the lack of a unified annotation scheme in the restaurant review dataset is apparent  across all
reviewers  the annotations feature        unique keyphrase surface forms over a set of        total
keyphrase occurrences  clearly  many unique keyphrases express the same semantic property  in
figure    good price is expressed in ten different ways  to quantify this phenomenon  the judges
   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

figure    cumulative occurrence counts for the top ten keyphrases associated with the good service
property  the percentages are out of a total of       separate keyphrase occurrences for
this property 

manually clustered a subset of the keyphrases associated with the six previously mentioned properties  specifically      keyphrases associated with the six major properties were chosen  accounting
for       of all keyphrase occurrences 
we use these manually clustered annotations to examine the distributional pattern of keyphrases
that describe the same underlying property  using two different statistics  first  the number of
different keyphrases for each property gives a lower bound on the number of possible paraphrases 
second  we measure how often the most common keyphrase is used to annotate each property 
i e   the coverage of that keyphrase  this metric gives a sense of how diffuse the keyphrases within
a property are  and specifically whether one single keyphrase dominates occurrences of the property 
note that this value is an overestimate of the true coverage  since we are only considering a tenth of
all keyphrase occurrences 
the right half of table   summarizes the variability of property paraphrases  observe that each
property is associated with numerous paraphrases  all of which were found multiple times in the
actual keyphrase set  most importantly  the most frequent keyphrase accounted for only about a third
of all property occurrences  strongly suggesting that targeting only these labels for learning is a very
limited approach  to further illustrate this last point  consider the property of good service  whose
keyphrase realizations distributional histogram appears in figure    the cumulative percentage
frequencies of the most frequent keyphrases associated with this property are plotted  the top four
keyphrases here account for only three quarters of all property occurrences  even within the limited
set of keyphrases we consider in this analysis  motivating the need for aggregate consideration of
keyphrases 
in the next section  we introduce a model that induces a clustering among keyphrases while
relating keyphrase clusters to the text  directly addressing these characteristics of the data 

   

fib ranavan   c hen   e isenstein     barzilay


x
s
h


c

z

w













keyphrase cluster model
keyphrase cluster assignment
keyphrase similarity values
document keyphrases
document keyphrase topics
probability of selecting  instead of 
selects between  and  for word topics
background word topic model
word topic assignment
language models of each topic
document words

  dirichlet    
x   multinomial  
 
beta     if x    x  
s     
beta      otherwise
t

d    d         d k  

 
where d k 

 


if x    k for any l  hd
otherwise

d  beta    
cd n  bernoulli d  
d  dirichlet    
 
multinomial d   if cd n    
zd n 
multinomial d   otherwise
k  dirichlet    
wd n  multinomial zd n  

figure    the plate diagram for our model  shaded circles denote observed variables  and squares
denote hyperparameters  the dotted arrows indicate that  is constructed deterministically from x and h  we use  to refer to a small constant probability mass 

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

   model description
we present a generative bayesian model for documents annotated with free text keyphrases  our
model assumes that each annotated document is generated from a set of underlying semantic topics 
semantic topics generate the document text by indexing a language model  in our approach  they are
also associated with clusters of keyphrases  in this way  the model can be viewed as an extension
of latent dirichlet allocation  blei et al          where the latent topics are additionally biased
toward the keyphrases that appear in the training data  however  this coupling is flexible  as some
words are permitted to be drawn from topics that are not represented by the keyphrase annotations 
this permits the model to learn effectively in the presence of incomplete annotations  while still
encouraging the keyphrase clustering to cohere with the topics supported by the document text 
another critical aspect of our model is that we desire the ability to use arbitrary comparisons
between keyphrases  in addition to information about their surface forms  to accommodate this
goal  we do not treat the keyphrase surface forms as generated from the model  rather  we acquire
a real valued similarity matrix across the universe of possible keyphrases  and treat this matrix
as generated from the keyphrase clustering  this representation permits the use of surface and
distributional features for keyphrase similarity  as described in section     
an advantage of hierarchical bayesian models is that it is easy to change which parts of the
model are observed and which parts are hidden  during training  the keyphrase annotations are
observed  so that the hidden semantic topics are coupled with clusters of keyphrases  to account for
words not related to semantic topics  some topics may not have any associated keyphrases  at test
time  the model is presented with documents for which the keyphrase annotations are hidden  the
model is evaluated on its ability to determine which keyphrases are applicable  based on the hidden
topics present in the document text 
the judgment of whether a topic applies to a given unannotated document is based on the probability mass assigned to that topic in the documents background topic distribution  because there
are no annotations  the background topic distribution should capture the entirety of the documents
topics  for the task involving reviews of products and services  multiple topics may accompany each
document  in this case  each topic whose probability is above a threshold  tuned on the development
set  is predicted as being supported 
    keyphrase clustering
to handle the hidden paraphrase structure of the keyphrases  one component of the model estimates
a clustering over keyphrases  the goal is to obtain clusters where each cluster correspond to a welldefined semantic topic  e g   both healthy and good nutrition should be grouped into a single
cluster  because our overall joint model is generative  a generative model for clustering could easily
be integrated into the larger framework  such an approach would treat all of the keyphrases in each
cluster as being generated from a parametric distribution  however  this representation would not
permit many powerful features for assessing the similarity of pairs of keyphrases  such as string
overlap or keyphrase co occurrence in a corpus  mccallum  bellare    pereira        
for this reason  we represent each keyphrase as a real valued vector rather than as its surface
form  the vector for a given keyphrase includes the similarity scores with respect to every other observed keyphrase  the similarity scores are represented by s in figure     we model these similarity
scores as generated by the cluster memberships  represented by x in figure     if two keyphrases

   

fib ranavan   c hen   e isenstein     barzilay

lexical

the cosine similarity between the surface forms of two keyphrases  represented as word frequency vectors 

co occurrence

each keyphrase is represented as a vector of co occurrence values  this
vector counts how many times other keyphrases appear in documents
annotated with this keyphrase  for example  the similarity vector for
good food may include an entry for very tasty food  the value of
which would be the number of documents annotated with good food
that contain very tasty food in their text  the similarity between two
keyphrases is then the cosine similarity of their co occurrence vectors 

table    the two sources of information used to compute the similarity matrix for our experiments 
the final similarity scores are linear combinations of these two values  note that cooccurrence similarity contains second order co occurrence information 

figure    a surface plot of the keyphrase similarity matrix from a set of restaurant reviews  computed according to table    red indicates high similarity  whereas blue indicates low
similarity  in this diagram  the keyphrases have been grouped according to an expertcreated clustering  so keyphrases of similar meaning are close together  the strong series
of similarity blocks along the diagonal hint at how this information could induce a
reasonable clustering 

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

are clustered together  their similarity score is generated from a distribution encouraging high similarity  otherwise  a distribution encouraging low similarity is used  
the features used for producing the similarity matrix are given in table    encompassing lexical
and distributional similarity measures  our implemented system takes a linear combination of these
two data sources  weighting both sources equally  the resulting similarity matrix for keyphrases
from the restaurant domain is shown in figure   
as described in the next section  when clustering keyphrases  our model takes advantage of the
topic structure of documents annotated with those keyphrases  in addition to information about the
individual keyphrases themselves  in this sense  it differs from traditional approaches for paraphrase
identification  barzilay   mckeown        lin   pantel        
    document topic modeling
our analysis of the document text is based on probabilistic topic models such as lda  blei et al  
       in the lda framework  each word is generated from a language model that is indexed by the
words topic assignment  thus  rather than identifying a single topic for a document  lda identifies
a distribution over topics  high probability topic assignments will identify compact  low entropy
language models  so that the probability mass of the language model for each topic is divided among
a relatively small vocabulary 
our model operates in a similar manner  identifying a topic for each word  denoted by z in
figure    however  where lda learns a distribution over topics for each document  we deterministically construct a document specific topic distribution from the clusters represented by the
documents keyphrases  this is  in the figure   assigns equal probability to all topics that are
represented in the keyphrase annotations  and very small probability to other topics  generating the
word topics in this way ties together the clustering and language models 
as noted above  sometimes the keyphrase annotation does not represent all of the semantic
topics that are expressed in the text  for this reason  we also construct another background distribution  over topics  the auxiliary variable c indicates whether a given words topic is drawn
from the distribution derived from annotations  or from the background model  representing c as
a hidden variable allows us to stochastically interpolate between the two language models  and
  in addition  any given document will most likely also discuss topics that are not covered by
any keyphrase  to account for this  the model is allowed to leave some of the clusters empty  thus
leaving some of the topics to be independent of all the keyphrases 
    generative process
our model assumes that all observed data is generated through a stochastic process involving hidden
parameters  in this section  we formally specify this generative process  this specification guides
inference of the hidden parameters based on observed data  which are the following 
 for each of the l keyphrases  a vector s  of length l denoting a pairwise similarity score in
the interval        to every other keyphrase 
 for each document d  its bag of words wd of length nd   the nth word of d is wd n  
   note that while we model each similarity score as an independent draw  clearly this assumption is too strong  due to
symmetry and transitivity  models making similar assumptions about the independence of related hidden variables
have previously been shown to be successful  for example  toutanova   johnson        

   

fib ranavan   c hen   e isenstein     barzilay

 for each document d  a set of keyphrase annotations hd   which includes index   if the document was annotated with keyphrase   
 the number of clusters k  which should be large enough to encompass topics with actual
clusters of keyphrases  as well as word only topics 
these observed variables are generated according to the following process 
   draw a multinomial distribution  over the k keyphrase clusters from a symmetric dirichlet
prior with parameter     
   for             l 
 a  draw the  th keyphrases cluster assignment x  from multinomial   
   for                     l          l  
 a  if x    x     draw s     from beta      beta        encouraging scores to be biased
toward values close to one 
 b  if x     x     draw s     from beta       beta        encouraging scores to be biased
toward values close to zero 
   for k           k 
 a  draw language model k from a symmetric dirichlet prior with parameter    
   for d           d 
 a  draw a background topic model d from a symmetric dirichlet prior with parameter    
 b  deterministically construct an annotation topic model d   based on keyphrase cluster
assignments x and observed document annotations hd   specifically  let h be the set of
topics represented by phrases in hd   distribution d assigns equal probability to each
element of h  and a very small probability mass to other topics  
 c  draw a weighted coin d from beta      which will determine the balance between
annotation d and background topic models d  
 d  for n           nd  
i  draw a binary auxiliary variable cd n from bernoulli d    which determines whether
the topic of the word wd n is drawn from the annotation topic model d or the background model d  
ii  draw a topic assignment zd n from the appropriate multinomial as indicated by
cd n  
iii  draw word wd n from multinomial zd n    that is  the language model indexed by
the words topic 
   variables subscripted with zero are fixed hyperparameters 
   making a hard assignment of zero probability to the other topics creates problems for parameter estimation  a
probability of     was assigned to all topics not represented by the keyphrase cluster memberships 

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

   parameter estimation
to make predictions on unseen data  we need to estimate the parameters of the model  in bayesian
inference  we estimate the distribution for each parameter  conditioned on the observed data and
hyperparameters  such inference is intractable in the general case  but sampling approaches allow
us to approximately construct distributions for each parameter of interest 
gibbs sampling is perhaps the most generic and straightforward sampling technique  conditional distributions are computed for each hidden variable  given all the other variables in the model 
by repeatedly sampling from these distributions in turn  it is possible to construct a markov chain
whose stationary distribution is the posterior of the model parameters  gelman  carlin  stern   
rubin         the use of sampling techniques in natural language processing has been previously
investigated by many researchers  including finkel  grenager  and manning        and goldwater 
griffiths  and johnson        
we now present sampling equations for each of the hidden variables in figure    the prior over
keyphrase clusters  is sampled based on the hyperprior   and the keyphrase cluster assignments
x  we write p           to mean the probability conditioned on all the other variables 
p            p       p x     
y
  p       
p x     
 

  dirichlet      

y

multinomial x     

 

  dirichlet        
where i  is     count x    i   this conditional distribution is derived based on the conjugacy of
the multinomial to the dirichlet distribution  the first line follows from bayes rule  and the second
line from the conditional independence of cluster assignments x given keyphrase distribution  
resampling equations for d and k can be derived in a similar manner 
p d           dirichlet d    d   
p k           dirichlet k   k    
p
     
where  d i       count zn d   i  cn d      and k i
 
d count wn d   i  zn d   k   in
 
building the counts for i   we consider only cases in which cn d      indicating that the topic zn d
is indeed drawn from the background topic model d   similarly  when building the counts for k   
we consider only cases in which the word wd n is drawn from topic k 
to resample   we employ the conjugacy of the beta prior to the bernoulli observation likelihoods  adding counts of c to the prior    
p d           beta d    d   

 p
count c
 
  
d n
 
n
 
where d       p
n count cd n     

   

fib ranavan   c hen   e isenstein     barzilay

the keyphrase cluster assignments are represented by x  whose sampling distribution depends
on   s  and z  via  
p x            p x     p s   x    x     p z       c 



d y
y
y
 p x      
p s       x    x       
p zd n   d  
      

d cd n   


  multinomial x      

y



d y
y
beta s       x   x     
multinomial zd n   d    

      

d cd n   

the leftmost term of the above equation is the prior on x    the next term encodes the dependence
of the similarity matrix s on the cluster assignments  with slight abuse of notation  we write x   x  
to denote   if x    x     and    otherwise  the third term is the dependence of the word topics
zd n on the topic distribution d   we compute the final result of this probability expression for each
possible setting of x    and then sample from the normalized multinomial 
the word topics z are sampled according to the topic distribution d   the background distribution
d   the observed words w  and the auxiliary variable c 
p zd n           p zd n     d   cd n  p wd n   zd n    
 
multinomial zd n   d  multinomial wd n   zd n  
 
multinomial zd n   d  multinomial wd n   zd n  

if cd n     
otherwise 

as with x  each zd n is sampled by computing the conditional likelihood of each possible setting
within a constant of proportionality  and then sampling from the normalized multinomial 
finally  we sample the auxiliary variable cd n   which indicates whether the hidden topic zd n is
drawn from d or d   c depends on its prior  and the hidden topic assignments z 
p cd n           p cd n   d  p zd n   d   d   cd n  
 
bernoulli cd n   d  multinomial zd n   d  
 
bernoulli cd n   d  multinomial zd n   d  

if cd n     
otherwise 

again  we compute the likelihood of cd n     and cd n     within a constant of proportionality  and
then sample from the normalized bernoulli distribution 
finally  our model requires values for fixed hyperparameters             and     which are tuned
in the standard way based on development set performance  appendix c lists the hyperparameters
values used for each domain in our experiments 
one of the main applications of our model is to predict the properties supported by documents
that are not annotated with keyphrases  at test time  we would like to compute a posterior estimate
of d for an unannotated test document d  since annotations are not present  property prediction is
based only on the text component of the model  for this estimate  we use the same gibbs sampling
procedure  restricted to zd n and d   with the stipulation that cd n is fixed at zero so that zd n is
always drawn from d   in particular  we treat the language models as known  to more accurately
integrate over all possible language models  we use the final      samples of the language models
from training as opposed to using a point estimate  for each topic  if its probability in d exceeds a
certain threshold  that topic is predicted  this threshold is tuned independently for each topic on a
development set  the empirical results we present in section   are obtained in this manner 
   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

figure    summary of reviews for the movie pirates of the caribbean  at worlds end on p r ecis 
this summary is based on    documents  the list of pros and cons are generated automatically using the system described in this paper  the generation of numerical ratings is
based on the algorithm described in snyder and barzilay        

   evaluation of summarization quality
our model for document analysis is implemented in p r ecis   a system that performs single  and
multi document review summarization  the goal of p r ecis is to provide users with effective access
to review data via mobile devices  p r ecis contains information about        products and services
ranging from childcare products to restaurants and movies  for each of these products  the system
contains a collection of reviews downloaded from consumer websites such as epinions  cnet 
and amazon  p r ecis compresses data for each product into a short list of pros and cons that
are supported by the majority of reviews  an example of a summary of    reviews for the movie
pirates of the caribbean  at worlds end is shown in figure    in contrast to traditional multidocument summarizers  the output of the system is not a sequence of sentences  but rather a list of
phrases indicative of product properties  this summarization format follows the format of pros cons
summaries that individual reviewers provide on multiple consumer websites  moreover  the brevity
of the summary is particularly suitable for presenting on small screens such as those of mobile
devices 
to automatically generate the combined pros cons list for a product or service  we first apply our
model to each review  the model is trained independently for each product domain  e g   movies 
using a corresponding subset of reviews with free text annotations  these annotations also provide
a set of keyphrases that contribute to the clusters associated with product properties  once the
   p r ecis is accessible at http   groups csail mit edu rbg projects precis  

   

fib ranavan   c hen   e isenstein     barzilay

model is trained  it labels each review with a set of properties  since the set of possible properties
is the same for all reviews of a product  the comparison among reviews is straightforward  for
each property  we count the number of reviews that support it  and select the property as part of a
summary if it is supported by the majority of the reviews  the set of semantic properties is converted
into a pros cons list by presenting the most common keyphrase for each property 
this aggregation technology is applicable in two scenarios  the system can be applied to unannotated reviews  inducing semantic properties from the document text  this conforms to the traditional way in which learning based systems are applied to unlabeled data  however  our model
is valuable even when individual reviews do include pros cons keyphrase annotations  due to the
high degree of paraphrasing  direct comparison of keyphrases is challenging  see section     by
inferring a clustering over keyphrases  our model permits comparison of keyphrase annotations on
a more semantic level 
the remainder of this section provides a set of evaluations of our models ability to capture the
semantic content of document text and keyphrase annotations  section     describes an evaluation
of our systems ability to extract meaningful semantic summaries from individual documents  and
also assesses the quality of the paraphrase structure induced by our model  section     extends this
evaluation to our systems ability to summarize multiple review documents 
    single document evaluation
first  we evaluate our model with respect to its ability to reproduce the annotations present in individual documents  based on the document text  we compare against a wide variety of baselines and
variations of our model  demonstrating the appropriateness of our approach to this task  in addition 
we explicitly evaluate the quality of the paraphrase structure induced by our model by comparing
against a gold standard clustering of keyphrases provided by expert annotators 
      e xperimental s etup
in this section  we describe the datasets and evaluation techniques used for experiments with our
system and other automatic methods  we also comment on how hyperparameters are tuned for our
model  and how sampling is initialized 
statistic
  of reviews
avg  review length
avg  keyphrases   review

restaurants
    
     
    

cell phones
    
      
    

digital cameras
    
      
    

table    statistics of the datasets used in our evaluations
data sets we evaluate our system on reviews from three domains  restaurants  cell phones  and
digital cameras  these reviews were downloaded from the epinions website  we used user authored
pros and cons associated with reviews as keyphrases  see section     statistics for the datasets are
provided in table    for each of the domains  we selected     of the documents for training 
we consider two strategies for constructing test data  first  we consider evaluating the semantic
properties inferred by our system against expert annotations of the semantic properties present in
each document  to this end  we use the expert annotations originally described in section   as a test

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

set   to reiterate  these were annotations of     reviews in the restaurant domain  of which we now
hold out    as a development set  the review texts were annotated with six properties according to
standardized guidelines  this strategy enforces consistency and completeness in the ground truth
annotations  differentiating them from free text annotations 
unfortunately  our ability to evaluate against expert annotations is limited by the cost of producing such annotations  to expand evaluation to other domains  we use the author written keyphrase
annotations that are present in the original reviews  such annotations are noisy  while the presence
of a property annotation on a document is strong evidence that the document supports the property 
the inverse is not necessarily true  that is  the lack of an annotation does not necessarily imply that
its respective property does not hold  e g   a review with no good service related keyphrase may
still praise the service in the body of the document 
for experiments using free text annotations  we overcome this pitfall by restricting the evaluation of predictions of individual properties to only those documents that are annotated with that
property or its antonym  for instance  when evaluating the prediction of the good service property 
we will only select documents which are either annotated with good service or bad service related
keyphrases   for this reason  each semantic property is evaluated against a unique subset of documents  the details of these development and test sets are presented in appendix a 
to ensure that free text annotations can be reliably used for evaluation  we compare with the
results produced on expert annotations whenever possible  as shown in section        the free text
evaluations produce results that cohere well with those obtained on expert annotations  suggesting
that such labels can be used as a reasonable proxy for expert annotation evaluations 
evaluation methods our first evaluation leverages the expert annotations described in section   
one complication is that expert annotations are marked on the level of semantic properties  while
the model makes predictions about the appropriateness of individual keyphrases  we address this
by representing each expert annotation with the most commonly observed keyphrase from the
manually annotated cluster of keyphrases associated with the semantic property  for example  an
annotation of the semantic property good food is represented with its most common keyphrase realization  great food  our evaluation then checks whether this keyphrase is within any of the clusters
of keyphrases predicted by the model 
the evaluation against author free text annotations is similar to the evaluation against expert
annotations  in this case  the annotation takes the form of individual keyphrases rather than semantic
properties  as noted  author generated keyphrases suffer from inconsistency  we obtain a consistent
evaluation by mapping the author generated keyphrase to a cluster of keyphrases as a determined
by the expert annotator  and then again selecting the most common keyphrase realization of the
cluster  for example  the author may use the keyphrase tasty  which maps to the semantic cluster
good food  we then select the most common keyphrase realization  great food  as in the expert
evaluation  we check whether this keyphrase is within any of the clusters predicted by the model 
model performance is quantified using recall  precision  and f score  these are computed in
the standard manner  based on the models representative keyphrase predictions compared against
the corresponding references  approximate randomization  yeh        noreen        is used for
statistical significance testing  this test repeatedly performs random swaps of individual results
   the expert annotations are available at http   groups csail mit edu rbg code precis  
   this determination is made by mapping author keyphrases to properties using an expert generated gold standard
clustering of keyphrases  it is much cheaper to produce an expert clustering of keyphrases than to obtain expert
annotations of the semantic properties in every document 

   

fib ranavan   c hen   e isenstein     barzilay

from each candidate system  and checks whether the resulting performance gap remains at least
as large  we use this test because it is valid for comparing nonlinear functions of random variables  such as f scores  unlike other common methods such as the sign test  previous work that
used this test include evaluations at the message understanding conference  chinchor  lewis   
hirschman        chinchor         more recently  riezler and maxwell        advocated for its
use in evaluating machine translation systems 
parameter tuning and initialization to improve the models convergence rate  we perform two
initialization steps for the gibbs sampler  first  sampling is done only on the keyphrase clustering
component of the model  ignoring document text  second  we fix this clustering and sample the
remaining model parameters  these two steps are run for       iterations each  the full joint model
is then sampled for         iterations  inspection of the parameter estimates confirms model convergence  on a  ghz dual core desktop machine  a multithreaded c   implementation of model
training takes about two hours for each dataset 
our model needs to be provided with the number of clusters k   we set k large enough for the
model to learn effectively on the development set  for the restaurant data we set k to     for cell
phones and digital cameras  k was set to    and     respectively  these values were tuned using the
development set  however  we found that as long as k was large enough to accommodate a significant number of keyphrase clusters  and a few additional to account for topics with no keyphrases 
the specific value of k does not affect the models performance  all other hyperparameters were
adjusted based on development set performance  though tuning was not extensive 
as previously mentioned  we obtain document properties by examining the probability mass of
the topic distribution assigned to each property  a probability threshold is set for each property via
the development set  optimizing for maximum f score 
      r esults
in this section  we report the performance of our model  comparing it with an array of increasingly
sophisticated baselines and model variations  we first demonstrate that learning a clustering of annotation keyphrases is crucial for accurate semantic prediction  next  we investigate the impact of
paraphrasing quality on model accuracy by considering the expert generated gold standard clustering of keyphrases as another comparison point  we also consider alternative automatically computed
sources of paraphrase information 
for ease of comparison  the results of all the experiments are shown in table   and table    with
a summary of the baselines and model variations in table   
comparison against simple baselines our first evaluation compares our model to four nave
baselines  all four treat keyphrases as independent  ignoring their latent paraphrase structure 
 random  each keyphrase is supported by a document with probability of one half  the
results of this baseline are computed in expectation  rather than actually run  this baseline
is expected to have a recall of      because in expectation it will select half of the correct
keyphrases  its precision is the average proportion of annotations in the test set against the
number of possible annotations  that is  in a test set of size n with m properties  if property
   this requirement could conceivably be removed by modeling the cluster indices as being drawn from a dirichlet
process prior 

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

random

each keyphrase is supported by a document with probability of one half 

keyphrase in text

a keyphrase is supported by a document if it appears verbatim in the text 

keyphrase classifier

a separate support vector machine classifier is trained for each keyphrase 
positive examples are documents that are labeled by the author with the
keyphrase  all other documents are considered to be negative examples  a
keyphrase is supported by a document if that keyphrases classifier returns a
positive prediction 

heuristic keyphrase
classifier

similar to keyphrase classifier  except heuristic methods are used in an attempt to reduce noise from the training documents  specifically we wish to
remove sentences that discuss other keyphrases from the positive examples 
the heuristic removes from the positive examples all sentences that have no
word overlap with the given keyphrase 

model cluster in text

a keyphrase is supported by a document if it or any of its paraphrases appear
in the text  paraphrasing is based on our models keyphrase clusters 

model cluster classifier

a separate classifier is trained for each cluster of keyphrases  positive examples are documents that are labeled by the author with any keyphrase from
the cluster  all other documents are negative examples  all keyphrases of
a cluster are supported by a document if that clusters classifier returns a
positive prediction  keyphrase clustering is based on our model 

heuristic model cluster
classifier

similar to model cluster classifier  except heuristic methods are used to reduce noise from the training documents  specifically we wish to remove
from the positive examples sentences that discuss keyphrases from other
clusters  the heuristic removes from the positive examples all sentences
that have no word overlap with any of the keyphrases from the given cluster 
keyphrase clustering is based on our model 

gold cluster model

a variation of our model where the clustering of keyphrases is fixed to an
expert created gold standard  only the text modeling parameters are learned 

gold cluster in text

similar to model cluster in text  except the clustering of keyphrases is according to the expert produced gold standard 

gold cluster classifier

similar to model cluster classifier  except the clustering of keyphrases is
according to the expert produced gold standard 

heuristic gold cluster
classifier

similar to heuristic model cluster classifier  except the clustering of
keyphrases is according to the expert produced gold standard 

independent cluster model

a variation of our model where the clustering of keyphrases is first learned
from keyphrase similarity information only  separately from the text  the
resulting independent clustering is then fixed while the text modeling parameters are learned  this variations key distinction from our full model is
the lack of joint learning of keyphrase clustering and text topics 

independent cluster in text

similar to model cluster in text  except that the clustering of keyphrases is
according to the independent clustering 

independent cluster
classifier

similar to model cluster classifier  except that the clustering of keyphrases
is according to the independent clustering 

heuristic independent
cluster classifier

similar to heuristic model cluster classifier  except the clustering of
keyphrases is according to the independent clustering 

table    a summary of the baselines and variations against which our model is compared 
   

fib ranavan   c hen   e isenstein     barzilay

method
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  

our model
random
keyphrase in text
keyphrase classifier
heuristic keyphrase classifier
model cluster in text
model cluster classifier
heuristic model cluster classifier
gold cluster model
gold cluster in text
gold cluster classifier
heuristic gold cluster classifier
independent cluster model
independent cluster in text
independent cluster classifier
heuristic independent cluster classifier

recall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

restaurants
prec  f score
           
            
            
            
            
            
           
            
           
            
            
            
            
            
            
            

table    comparison of the property predictions made by our model and a series of baselines and
model variations in the restaurant domain  evaluated against expert semantic annotations 
the results are divided according to experiment  the methods against which our model
has significantly better results using approximate randomization are indicated with  for
p        and  for p      

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

method
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  

our model
random
keyphrase in text
keyphrase classif 
heur  keyphr  classif 
model cluster in text
model cluster classif 
heur  model classif 
gold cluster model
gold cluster in text
gold cluster classif 
heur  gold classif 
indep  cluster model
indep  cluster in text
indep  cluster classif 
heur  indep  classif 

recall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

restaurants
prec  f score
           
            
            
            
            
            
            
            
            
            
            
            
            
            
           
            

recall
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

cell phones
prec  f score
           
            
            
           
            
            
           
            
           
            
           
            
           
            
            
            

digital cameras
recall prec  f score
                 
                  
                  
                  
                  
                  
                 
                 
                  
                  
                 
                  
                  
                  
                  
                  

table    comparison of the property predictions made by our model and a series of baselines and
model variations in three product domains  as evaluated against author free text annotations  the results are divided according to experiment  the methods against which our
model has significantly better results using approximate randomization are indicated with
 for p        and  for p       methods which perform significantly better than our
model with p       are indicated with  

   

fib ranavan   c hen   e isenstein     barzilay

p
ni
i appears ni times  then expected precision is m
i   mn   for instance  for the restaurants
gold standard evaluation  the six tested properties appeared a total of     times over    
documents  yielding an expected precision of       
 keyphrase in text  a keyphrase is supported by a document if it appears verbatim in the
text  precision should be high while recall will be low  because the model is unable to detect
paraphrases of the keyphrase in the text  for instance  for the first review from figure   
cleanliness would be supported because it appears in the text  however  healthy would
not be supported  even though the synonymous great nutrition does appear 
 keyphrase classifier   a separate discriminative classifier is trained for each keyphrase  positive examples are documents that are labeled by the author with the keyphrase  all other documents are considered to be negative examples  consequently  for any particular keyphrase 
documents labeled with synonymous keyphrases would be among the negative examples  a
keyphrase is supported by a document if that keyphrases classifier returns a positive prediction 
we use support vector machines  built using svmlight  joachims        with the same features
as our model  i e  word counts    to partially circumvent the imbalanced positive negative
data problem  we tuned prediction thresholds on a development set to maximize f score  in
the same manner that we tuned thresholds for our model 
 heuristic keyphrase classifier  this baseline is similar to keyphrase classifier above  but attempts to mitigate some of the noise inherent in the training data  specifically  any given
positive example document may contain text unrelated to the given keyphrase  we attempt
to reduce this noise by removing from the positive examples all sentences that have no word
overlap with the given keyphrase  a keyphrase is supported by a document if that keyphrases
classifier returns a positive prediction   
lines     of tables   and   present these results  using both gold annotations and the original
authors annotations for testing  our model outperforms these three baselines in all evaluations with
strong statistical significance 
the keyphrase in text baseline fares poorly  its f score is below the random baseline in three
of the four evaluations  as expected  the recall of this baseline is usually low because it requires
keyphrases to appear verbatim in the text  the precision is somewhat better  but the presence of
a significant number of false positives indicates that the presence of a keyphrase in the text is not
necessarily a reliable indicator of the associated semantic property 
interestingly  one domain in which keyphrase in text does perform well is digital cameras  we
believe that this is because of the prevalence of specific technical terms in the keyphrases used in
this domain  such as zoom and battery life  such technical terms are also frequently used in the
review text  making the recall of keyphrase in text substantially higher in this domain than in the
other evaluations 
   note that the classifier results reported in the initial publication  branavan  chen  eisenstein    barzilay        were
obtained using the default parameters of a maximum entropy classifier  tuning the classifiers parameters allowed us
to significantly improve performance of all classifier baselines 
    in general  svms have the additional advantage of being able to incorporate arbitrary features  but for the sake of
comparison we restrict ourselves to using the same features across all methods 
    we thank a reviewer for suggesting this baseline 

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

the keyphrase classifier baseline outperforms the random and keyphrase in text baselines  but
still achieves consistently lower performance than our model in all four evaluations  notably  the
performance of heuristic keyphrase classifier is worse than keyphrase classifier except in one case 
this alludes to the difficulty of removing the noise inherent in the document text 
overall  these results indicate that methods which learn and predict keyphrases without accounting for their intrinsic hidden structure are insufficient for optimal property prediction  this leads us
toward extending the present baselines with clustering information 
it is important to assess the consistency of the evaluation based on free text annotations  table    with the evaluation that uses expert annotations  table     while the absolute scores on the
expert annotations dataset are lower than the scores with free text annotations  the ordering of performance between the various automatic methods is the same across the two evaluation scenarios 
this consistency is maintained in the rest of our experiments as well  indicating that for the purpose
of relative comparison between the different automatic methods  our method of evaluating with
free text annotations is a reasonable proxy for evaluation on expert generated annotations 
comparison against clustering based approaches the previous section demonstrates that our
model outperforms baselines that do not account for the paraphrase structure of keyphrases  we
now ask whether it is possible to enhance the baselines performance by augmenting them with the
keyphrase clustering induced by our model  specifically  we introduce three more systems  none of
which are true baselines  since they all use information inferred by our model 
 model cluster in text  a keyphrase is supported by a document if it or any of its paraphrases
appears in the text  paraphrasing is based on our models clustering of the keyphrases  the
use of paraphrasing information enhances recall at the potential cost of precision  depending
on the quality of the clustering  for example  assuming healthy and great nutrition are
clustered together  the presence of healthy in the text would also indicate support for great
nutrition  and vice versa 
 model cluster classifier  a separate discriminative classifier is trained for each cluster of
keyphrases  positive examples are documents that are labeled by the author with any keyphrase
from the cluster  all other documents are negative examples  all keyphrases of a cluster are
supported by a document if that clusters classifier returns a positive prediction  keyphrase
clustering is based on our model  as with keyphrase classifier  we use support vector machines trained on word count features  and we tune the prediction thresholds for each individual cluster on a development set 
another perspective on model cluster classifier is that it augments the simplistic text modeling
portion of our model with a discriminative classifier  discriminative training is often considered to be more powerful than equivalent generative approaches  mccallum et al         
leading us to expect a high level of performance from this system 
 heuristic model cluster classifier  this method is similar to model cluster classifier above 
but with additional heuristics used to reduce the noise inherent in the training data  positive
example documents may contain text unrelated to the given cluster  to reduce this noise 
sentences that have no word overlap with any of the clusters keyphrases are removed  all
keyphrases of a cluster are supported by a document if that clusters classifier returns a positive prediction  keyphrase clustering is based on our model 
   

fib ranavan   c hen   e isenstein     barzilay

lines     of tables   and   present results for these methods  as expected  using a clustering
of keyphrases with the baseline methods substantially improves their recall  with low impact on
precision  model cluster in text invariably outperforms keyphrase in text  the recall of keyphrase in
text is improved by the addition of clustering information  though precision is worse in some cases 
this phenomenon holds even in the cameras domain  where keyphrase in text already performs well 
however  our model still significantly outperforms model cluster in text in all evaluations 
adding clustering information to the classifier baseline results in performance that is sometimes
better than our models  this result is not surprising  because model cluster classifier gains the
benefit of our models robust clustering while learning a more sophisticated classifier for assigning
properties to texts  the resulting combined system is more complex than our model by itself  but
has the potential to yield better performance  on the other hand  using a simple heuristic to reduce
the noise present in the training data consistently hurts the performance of the classifier  possibly
due to the reduction in the amount of training data 
overall  the enhanced performance of these methods  in contrast to the keyphrase baselines  is
aligned with previous observations in entailment research  dagan  glickman    magnini        
confirming that paraphrasing information contributes greatly to improved performance in semantic
inference tasks 
the impact of paraphrasing quality the previous section demonstrates one of the central
claims of this paper  accounting for paraphrase structure yields substantial improvements in semantic inference when using noisy keyphrase annotations  a second key aspect of our research is
the idea that clustering quality benefits from tying the clusters to hidden topics in the document
text  we evaluate this claim by comparing our models clustering against an independent clustering
baseline  we also compare against a gold standard clustering produced by expert human annotators  to test the impact of these clustering methods  we substitute the models inferred clustering
with each alternative and examine how the resulting semantic inferences change  this comparison
is performed for the semantic inference mechanism of our model  as well as for the model cluster
in text  model cluster classifier and heuristic model cluster classifier baselines 
to add a gold standard clustering to our model  we replace the hidden variables that correspond to keyphrase clusters with observed values that are set according to the gold standard clustering    the only parameters that are trained are those for modeling text  this model variation  gold
cluster model  predicts properties using the same inference mechanism as the original model  the
baseline variations gold cluster in text  gold cluster classifier and heuristic gold cluster classifier are
likewise derived by substituting the automatically computed clustering with gold standard clusters 
an additional clustering is obtained using only the keyphrase similarity information  specifically  we modify our original model so that it learns the keyphrase clustering in isolation from the
text  and only then learns the property language models  in this framework  the keyphrase clustering
is entirely independent of the review text  because the text modeling is learned with the keyphrase
clustering fixed  we refer to this modification of the model as independent cluster model  because
our model treats the document text as a mixture of latent topics  this is reminiscent of models such
as supervised latent dirichlet allocation  slda  blei   mcauliffe         with the labels acquired
by performing a clustering across keyphrases as a preprocessing step  as in the previous experiment  we introduce three new baseline variations  independent cluster in text  independent cluster
classifier and heuristic independent cluster classifier 
    the gold standard clustering was created as part of the evaluation procedure described in section       

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

lines      of tables   and   present the results of these experiments  the gold cluster model
produces f scores comparable to our original model  providing strong evidence that the clustering
induced by our model is of sufficient quality for semantic inference  the application of the expertgenerated clustering to the baselines  lines        and     yields less consistent results  but overall
this evaluation provides little reason to believe that performance would be substantially improved
by obtaining a clustering that was closer to the gold standard 
the independent cluster model consistently reduces performance with respect to the full joint
model  supporting our hypothesis that joint learning gives rise to better prediction  the independent
clustering baselines  independent cluster in text  independent cluster classifier and heuristic independent cluster classifier  lines    to      are also worse than their counterparts that use the model
clustering  lines   to     this observation leads us to conclude that while the expert annotated
clustering does not always improve results  the independent clustering always degrades them  this
supports our view that joint learning of clustering and text models is an important prerequisite for
better property prediction 
clustering
model clusters
independent clusters

restaurants
     
     

cell phones
     
     

digital cameras
     
     

table    rand index scores of our models clusters  learned from keyphrases and text jointly  compared against clusters learned only from keyphrase similarity  evaluation of cluster quality
is based on the gold standard clustering 

another way of assessing the quality of each automatically obtained keyphrase clustering is
to quantify its similarity to the clustering produced by the expert annotators  for this purpose we
use the rand index  rand         a measure of cluster similarity  this measure varies from zero
to one  with higher scores indicating greater similarity  table   shows the rand index scores for
our models full joint clustering  as well as the clustering obtained from independent cluster model 
in every domain  joint inference produces an overall clustering that improves upon the keyphrasesimilarity only approach  these scores again confirm that joint inference across keyphrases and
document text produces a better clustering than considering features of the keyphrases alone 
    summarizing multiple reviews
our last experiment examines the multi document summarization capability of our system  we
study our models ability to aggregate properties across a set of reviews  compared to baselines that
aggregate by directly using the free text annotations 
      data and e valuation
we selected    restaurants  with five user written reviews for each restaurant  ten annotators were
asked to annotate the reviews for five restaurants each  comprising    reviews per annotator  they
used the same six salient properties and the same annotation guidelines as in the previous restaurant
annotation experiment  see section     in constructing the ground truth  we label properties that are
supported in at least three of the five reviews 

   

fib ranavan   c hen   e isenstein     barzilay

method
our model
keyphrase aggregation
model cluster aggregation
gold cluster aggregation
indep  cluster aggregation

recall
     
     
     
     
     

prec 
     
     
     
     
     

f score
     
      
      
      
      

table    comparison of the aggregated property predictions made by our model and a series of
baselines that use free text annotations  the methods against which our model has significantly better results using approximate randomization are indicated with  for p       

we make property predictions on the same set of reviews with our model and the baselines
presented below  for the automatic methods  we register a prediction if the system judges the
property to be supported on at least two of the five reviews    the recall  precision  and f score are
computed over these aggregate predictions  against the six salient properties marked by annotators 
      aggregation a pproaches
in this evaluation  we run the trained version of our model as described in section        note that
keyphrases are not provided to our model  though they are provided to the baselines 
the most obvious baseline for summarizing multiple reviews would be to directly aggregate
their free text keyphrases  these annotations are presumably representative of the reviews semantic
properties  and unlike the review text  keyphrases can be matched directly with each other  our first
baseline applies this notion directly 
 keyphrase aggregation  a keyphrase is supported for a restaurant if at least two out of its five
reviews are annotated verbatim with that keyphrase 
this simple aggregation approach has the obvious downside of requiring very strict matching between independently authored reviews  for that reason  we consider extensions to this aggregation
approach that allow for annotation paraphrasing 
 model cluster aggregation  a keyphrase is supported for a restaurant if at least two out of
its five reviews are annotated with that keyphrase or one of its paraphrases  paraphrasing is
according to our models inferred clustering 
 gold cluster aggregation  same as model cluster aggregation  but using the expert generated
clustering for paraphrasing 
 independent cluster aggregation  same as model cluster aggregation  but using the clustering
learned only from keyphrase similarity for paraphrasing 
    when three corroborating reviews are required  the baseline systems produce very few positive predictions  leading
to poor recall  results for this setting are presented in appendix b 

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

      r esults
table   compares the baselines against our model  our model outperforms all of the annotationbased baselines  despite not having access to the keyphrase annotations  notably  keyphrase aggregation performs very poorly  because it makes very few predictions  as a result of its requirement
of exact keyphrase string match  as before  the inclusion of keyphrase clusters improves the performance of the baseline models  however  the incompleteness of the keyphrase annotations  see
section    explains why the recall scores are still low compared to our model  by incorporating
document text  our model obtains dramatically improved recall  at the cost of reduced precision 
ultimately yielding a significantly improved f score 
these results demonstrate that review summarization benefits greatly from our joint model of the
review text and keyphrases  nave approaches that consider only keyphrases yield inferior results 
even when augmented with paraphrase information 

   conclusions and future work
in this paper  we have shown how free text keyphrase annotations provided by novice users can
be leveraged as a training set for document level semantic inference  free text annotations have
the potential to vastly expand the set of training data available to developers of semantic inference
systems  however  as we have shown  they suffer from lack of consistency and completeness  we
overcome these problems by inducing a hidden structure of semantic properties  which correspond
both to clusters of keyphrases and hidden topics in the text  our approach takes the form of a
hierarchical bayesian model  which addresses both the text and keyphrases jointly 
our model is implemented in a system that successfully extracts semantic properties of unannotated restaurant  cell phone  and camera reviews  empirically validating our approach  our experiments demonstrate the necessity of handling the paraphrase structure of free text keyphrase
annotations  moreover  they show that a better paraphrase structure is learned in a joint framework
that also models the document text  our approach outperforms competitive baselines for semantic
property extraction from both single and multiple documents  it also permits aggregation across
multiple keyphrases with different surface forms for multi document summarization 
this work extends an actively growing literature on document topic modeling  both topic modeling and paraphrasing posit a hidden layer that captures the relationship between disparate surface
forms  in topic modeling  there is a set of latent distributions over lexical items  while paraphrasing
is represented by a latent clustering over phrases  we show these two latent structures can be linked 
resulting in increased robustness and semantic coherence 
we see several avenues of future work  first  our model draws substantial power from features that measure keyphrase similarity  this ability to use arbitrary similarity metrics is desirable 
however  representing individual similarity scores as random variables is a compromise  as they are
clearly not independent  we believe that this problem could be avoided by modeling the generation
of the entire similarity matrix jointly 
a related approach would be to treat the similarity matrix across keyphrases as an indicator of
covariance structure  in such a model  we would learn separate language models for each keyphrase 
but keyphrases that are rated as highly similar would be constrained to induce similar language
models  such an approach might be possible in a gaussian process framework  rasmussen  
williams        

   

fib ranavan   c hen   e isenstein     barzilay

currently the focus of our model is to identify the semantic properties expressed in a given
document  which allows us to produce a summary of those properties  however  as mentioned in
section    human authors do not give equal importance to all properties when producing a summary
of pros and cons  one possible extension of this work would be to explicitly model the likelihood
of each topic being annotated in a document  we might then avoid the current post processing step
that uses property specific thresholds to compute final predictions from the model output 
finally  we have assumed that the semantic properties themselves are unstructured  in reality 
properties are related in interesting ways  trivially  in the domain of reviews it would be desirable
to model antonyms explicitly  e g   no restaurant review should be simultaneously labeled as having
good and bad food  other relationships between properties  such as hierarchical structures  could
also be considered  this suggests possible connections to the correlated topic model of blei and
lafferty        

bibliographic note
portions of this work were previously presented in a conference publication  branavan et al         
the current article extends this work in several ways  most notably  the development and evaluation
of a multi document review summarization system that uses semantic properties induced by our
method  section       a detailed analysis of the distributional properties of free text annotations
 section     and an expansion of the evaluation to include an additional domain and sets of baselines
not considered in the original paper  section        

acknowledgments
the authors acknowledge the support of national science foundation  nsf  career grant iis         the microsoft research new faculty fellowship  the u s  office of naval research
 onr   quanta computer  and nokia corporation  harr chen is supported by the national defense science and engineering and nsf graduate fellowships  thanks to michael collins  zoran
dzunic  amir globerson  aria haghighi  dina katabi  kristian kersting  terry koo  yoong keok
lee  brian milch  tahira naseem  dan roy  christina sauper  benjamin snyder  luke zettlemoyer 
and the journal reviewers for helpful comments and suggestions  we also thank marcia davidson
and members of the nlp group at mit for help with expert annotations  any opinions  findings 
conclusions or recommendations expressed in this article are those of the authors  and do not necessarily reflect the views of nsf  microsoft  onr  quanta  or nokia 

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

appendix a  development and test set statistics
table   lists the semantic properties for each domain and the number of documents that are used
for evaluating each of these properties  as noted in section        the gold standard evaluation is
complete  testing every property with each document  conversely  the free text evaluations for each
property only use documents that are annotated with the property or its antonym  this is why the
number of documents differs for each semantic property 
domain
restaurants  gold 
restaurants

cell phones

cameras

property
all properties
good food
bad food
good price
bad price
good service
bad service
good reception
bad reception
good battery life
poor battery life
good price
bad price
small
large
good price
bad price
good battery life
poor battery life
great zoom
limited zoom

development documents
  

test documents
   

  

   

  

  

  

   

  

  

  

   

  

  

  

   

  

   

  

   

  

  

table    breakdown by property for the development and test sets used for the evaluations in section       

   

fib ranavan   c hen   e isenstein     barzilay

appendix b  additional multiple review summarization results
table    lists results of the multi document experiment  with a variation on the aggregation 
we require each automatic method to predict a property for three of five reviews to predict that
property for the product  rather than two as presented in section      for the baseline systems  this
change causes a precipitous drop in recall  leading to f score results that are substantially worse
than those presented in section        in contrast  the f score for our model is consistent across
both evaluations 
method
our model
keyphrase aggregation
model cluster aggregation
gold cluster aggregation
indep  cluster aggregation

recall
     
     
     
     
     

prec 
     
     
     
     
     

f score
     
      
      
      
      

table     comparison of the aggregated property predictions made by our model and a series of
baselines that only use free text annotations  aggregation requires three of five reviews
to predict a property  rather than two as in section      the methods against which our
model has significantly better results using approximate randomization are indicated with
 for p       

appendix c  hyperparameter settings
table    lists the values of hyperparameters         and   used in all experiments for each domain 
these values were arrived at through tuning on the development set  in all cases    was set to
        making beta     the uniform distribution 
hyperparameters
 
 
 

restaurants
      
     
     

cell phones
      
      
      

cameras
      
   
     

table     values of the hyperparameters used for each domain across all experiments 

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

references
barzilay  r   mckeown  k     elhadad  m          information fusion in the context of multidocument summarization  in proceedings of acl  pp         
barzilay  r     mckeown  k  r          extracting paraphrases from a parallel corpus  in proceedings of acl  pp       
bhattacharya  i     getoor  l          a latent dirichlet model for unsupervised entity resolution 
in proceedings of the siam international conference on data mining 
blei  d  m     lafferty  j  d          correlated topic models  in advances in nips  pp         
blei  d  m     mcauliffe  j          supervised topic models  in advances in nips  pp         
blei  d  m   ng  a  y     jordan  m  i          latent dirichlet allocation  journal of machine
learning research             
boyd graber  j   blei  d     zhu  x          a topic model for word sense disambiguation  in
proceedings of emnlp  pp           
branavan  s  r  k   chen  h   eisenstein  j     barzilay  r          learning document level semantic properties from free text annotations  in proceedings of acl  pp         
carbonell  j     goldstein  j          the use of mmr  diversity based reranking for reordering
documents and producing summaries  in proceedings of acm sigir  pp         
chinchor  n          statistical significance of muc   results  in proceedings of the  th conference
on message understanding  pp       
chinchor  n   lewis  d  d     hirschman  l          evaluating message understanding systems 
an analysis of the third message understanding conference  muc     computational linguistics                
cohen  j          a coefficient of agreement for nominal scales  educational and psychological
measurement              
dagan  i   glickman  o     magnini  b          the pascal recognising textual entailment challenge  lecture notes in computer science               
elhadad  n     mckeown  k  r          towards generating patient specific summaries of medical
articles  in proceedings of naacl workshop on automatic summarization  pp       
finkel  j  r   grenager  t     manning  c          incorporating non local information into information extraction systems by gibbs sampling  in proceedings of acl  pp         
gelman  a   carlin  j  b   stern  h  s     rubin  d  b          bayesian data analysis   nd edition  
texts in statistical science  chapman   hall crc 
goldwater  s   griffiths  t  l     johnson  m          contextual dependencies in unsupervised
word segmentation  in proceedings of acl  pp         
hu  m     liu  b          mining and summarizing customer reviews  in proceedings of sigkdd 
pp         
joachims  t          making large scale support vector machine learning practical  pp         
mit press 

   

fib ranavan   c hen   e isenstein     barzilay

kim  s  m     hovy  e          automatic identification of pro and con reasons in online reviews 
in proceedings of coling acl  pp         
lin  d     pantel  p          discovery of inference rules for question answering  natural language
engineering               
liu  b   hu  m     cheng  j          opinion observer  analyzing and comparing opinions on the
web  in proceedings of www  pp         
lu  y     zhai  c          opinion integration through semi supervised topic modeling  in proceedings of www  pp         
mani  i     bloedorn  e          multi document summarization by graph search and matching  in
proceedings of aaai  pp         
marsi  e     krahmer  e          explorations in sentence fusion  in proceedings of the european
workshop on natural language generation  pp         
mccallum  a   bellare  k     pereira  f          a conditional random field for discriminativelytrained finite state string edit distance  in proceedings of uai  pp         
nenkova  a   vanderwende  l     mckeown  k          a compositional context sensitive multidocument summarizer  exploring the factors that influence summarization  in proceedings of
sigir  pp         
noreen  e          computer intensive methods for testing hypotheses  an introduction  john
wiley and sons 
popescu  a  m   nguyen  b     etzioni  o          opine  extracting product features and opinions from reviews  in proceedings of hlt emnlp  pp         
purver  m   kording  k  p   griffiths  t  l     tenenbaum  j  b          unsupervised topic modelling for multi party spoken discourse  in proceedings of coling acl  pp       
radev  d   jing  h     budzikowska  m          centroid based summarization of multiple documents  sentence extraction  utility based evaluation and user studies  in proceedings of
anlp naacl summarization workshop 
radev  d     mckeown  k          generating natural language summaries from multiple on line
sources  computational linguistics                
rand  w  m          objective criteria for the evaluation of clustering methods  journal of the
american statistical association                  
rasmussen  c  e     williams  c  k  i          gaussian processes for machine learning  mit
press 
riezler  s     maxwell  j  t          on some pitfalls in automatic evaluation and significance
testing for mt  in proceedings of the acl workshop on intrinsic and extrinsic evaluation
measures for machine translation and or summarization  pp       
snyder  b     barzilay  r          multiple aspect ranking using the good grief algorithm  in
proceedings of naacl hlt  pp         
titov  i     mcdonald  r       a   a joint model of text and aspect ratings for sentiment summarization  in proceedings of acl  pp         

   

fil earning d ocument l evel s emantic p roperties from f ree  t ext a nnotations

titov  i     mcdonald  r       b   modeling online reviews with multi grain topic models  in
proceedings of www  pp         
toutanova  k     johnson  m          a bayesian lda based model for semi supervised part ofspeech tagging  in advances in nips  pp           
white  m   korelsky  t   cardie  c   ng  v   pierce  d     wagstaff  k          multi document
summarization via information extraction  in proceedings of hlt  pp     
yeh  a          more accurate tests for the statistical significance of result differences  in proceedings of coling  pp         
zaenen  a          mark up barking up the wrong tree  computational linguistics                

   

fi