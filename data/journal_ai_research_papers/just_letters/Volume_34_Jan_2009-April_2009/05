journal artificial intelligence research                  

submitted        published      

sentence compression tree transduction
trevor cohn

tcohn inf ed ac uk

mirella lapata

mlap inf ed ac uk

school informatics
university edinburgh
   crichton street edinburgh eh    ab  uk

abstract
paper presents tree to tree transduction method sentence compression 
model based synchronous tree substitution grammar  formalism allows local
distortion tree topology thus naturally capture structural mismatches 
describe algorithm decoding framework show model
trained discriminatively within large margin framework  experimental results sentence
compression bring significant improvements state of the art model 

   introduction
recent years witnessed increasing interest text to text generation methods many
natural language processing applications  ranging text summarisation question answering machine translation  heart methods lies ability perform
rewriting operations  instance  text simplification identifies phrases sentences
document pose reading difficulty given user substitutes simpler alternatives  carroll  minnen  pearce  canning  devlin    tait        chandrasekar  
srinivas         question answering  questions often paraphrased order achieve
flexible matching potential answers  lin   pantel        hermjakob  echihabi 
  marcu         another example concerns reformulating written language render
natural sounding speech synthesis applications  kaji  okamoto    kurohashi 
      
sentence compression perhaps one popular text to text rewriting methods 
aim produce summary single sentence retains important
information remaining grammatical  jing         appeal sentence compression
lies potential summarization generally document compression  e g  
displaying text small screens mobile phones pdas  vandeghinste   pan 
       much current work literature focuses simplified formulation
compression task allow rewriting operations word deletion 
given input source sentence words x   x    x            xn   target compression formed
removing subset words  knight   marcu        
despite restricted word deletion  compression task remains challenging
modeling perspective  figure   illustrates source sentence target compression
taken one compression corpora used experiments  see section   details  
case  hypothetical compression system must apply series rewrite rules order
c
    
ai access foundation  rights reserved 

ficohn   lapata














vp
whnp
rb

wp

exactly

np

vp
np

nns

whnp

vbd prp cc

records

wp

np

whnp

np

vbn

wp

nns

vbp

vbn

involved



records



involved

nns vbp

made ones

vp
vp

 a  source

vp

 b  target

figure    example sentence compression showing source target trees  bold
source nodes show terminals need removed produce target
string 

whnp
rb



whnp

wp

wp

np



np
np

vp



   



   

   




vp

whnp

vp
vp


whnp

cc




whnp




np

vp


   

   

figure    example transduction rules  displayed pair tree fragments  left
 source  fragment matched node source tree  matching
part replaced right  target  fragment  dotted lines denote variable
correspondences  denotes node deletion 

obtain target  e g   delete leaf nodes exactly and  delete subtrees made
ones  merge subtrees corresponding records involved 
concretely  system must access rules shown figure    rules
displayed pair tree fragments left fragment corresponds source
right target  instance  rule     states wh noun phrase  whnp 
consisting adverb  rb  wh pronoun  wp   e g   exactly what  rewritten
wh pronoun  without adverb   two things note here  first 
syntactic information plays important role  since deletion decisions limited
individual words often span larger constituents  secondly  large number
compression rules varying granularity complexity  see rule     figure    
previous solutions compression problem cast mostly supervised
learning setting  for unsupervised methods see clarke   lapata        hori   furui       
turner   charniak         sentence compression often modeled generative framework
   

fisentence compression tree transduction

aim estimate joint probability p  x  y  source sentence x
target compression  knight   marcu        turner   charniak        galley  
mckeown         approaches essentially learn rewrite rules similar shown
figure   parsed parallel corpus subsequently use find best
compression set possible compressions given sentence  approaches
model compression discriminatively subtree deletion  riezler  king  crouch    zaenen 
      nguyen  horiguchi  shimazu    ho        mcdonald        
despite differences formulation  existing models specifically designed sentence compression mind generally applicable tasks requiring
complex rewrite operations substitutions  insertions  reordering  common
assumption underlying previous work tree structures representing source
sentences target compressions isomorphic  i e   exists edge preserving
bijection nodes two trees  assumption valid sentence compression hold rewriting tasks  consequently  sentence compression
models restrictive  cannot readily adapted generation problems
since able handle structural lexical divergences  related issue concerns deletion operations often take place without considering
structure target compression  the goal generate compressed string rather
tree representing it   without syntax based language model  turner   charniak 
      explicit generation mechanism licenses tree transformations
guarantee compressions well formed syntactic structures 
straightforward process subsequent generation analysis tasks 
paper present sentence compression model deletion specific
account ample rewrite operations scales rewriting tasks  formulate
compression problem tree to tree rewriting using synchronous grammar  with rules
shown figure     specifically  adopt synchronous tree substitution
grammar  stsg  formalism  eisner        model non isomorphic tree structures
efficient inference algorithms  show grammar induced
parallel corpus propose discriminative model rewriting task
viewed weighted tree to tree transducer  learning framework makes use
large margin algorithm put forward tsochantaridis  joachims  hofmann  altun
       efficiently learns prediction function minimize given loss function 
develop appropriate algorithm used training  i e   learning
model weights  decoding  i e   finding plausible compression model  
beyond sentence compression  hope work described might
relevance tasks involving structural matching  see discussion section    
remainder paper structured follows  section   provides overview
related work  section   presents stsg framework compression model
employ experiments  section   discusses experimental set up section  
presents results  discussion future work concludes paper 

   related work
synchronous context free grammars  scfgs  aho   ullman        generalization
context free grammar  cfg  formalism simultaneously produce strings two
   

ficohn   lapata

languages  used extensively syntax based statistical mt  examples
include inversion transduction grammar  wu         head transducers  alshawi  bangalore 
  douglas         hierarchical phrase based translation  chiang         several variants
tree transducers  yamada   knight        grael   knight        
sentence compression bears resemblance machine translation  instead translating one language another  translating long sentences shorter ones
within language  therefore surprising previous work adopted
scfgs compression task  specifically  knight marcu        proposed noisychannel formulation sentence compression  model consists two components 
language model p  y  whose role guarantee compression output grammatical channel model p  x y  capturing probability source sentence x
expansion target compression y  decoding algorithm searches compression maximizes p  y p  x y   channel model stochastic scfg 
rules extracted parsed parallel corpus weights estimated using
maximum likelihood  galley mckeown        show obtain improved scfg
probability estimates markovization  turner charniak        note scfg
rules expressive enough model structurally complicated compressions
restricted trees depth    remedy supplying synchronous grammar set general special rules  example  allow rules form
hnp npi h np np   cc np      np    boxed subscripts added distinguish
two nps  
work formulates sentence compression framework synchronous treesubstitution grammar  stsg  eisner         stsg allows describe non isomorphic tree
pairs  the grammar rules comprise trees arbitrary depth  thus suited textrewriting tasks typically involve number local modifications input text 
especially modification described succinctly terms syntactic transformations  dropping adjectival phrase converting passive verb phrase active
form  stsg restricted version synchronous tree adjoining grammar  stag  shieber
  schabes        without adjunction operation  stag affords mild context sensitivity 
however increased cost inference  scfg stsg weakly equivalent  is 
string languages identical produce equivalent tree pairs  example 
figure    rules        expressed scfg rules  rule     cannot
source target fragments two level trees  fact would impossible
describe trees figure   using scfg  grammar rules therefore general
obtained knight marcu        account elaborate tree
divergences  moreover  adopting expressive grammar formalism  naturally model syntactically complex compressions without specify additional rules
 as turner   charniak        
synchronous grammar license large number compressions given source
tree  grammar rule typically score overall score compression sentence x derived  previous work estimates scores generatively
discussed above  opt discriminative training procedure allows incorporation manner powerful features  use large margin technique proposed
tsochantaridis et al          framework attractive supports configurable loss function  describes extent predicted target tree differs
   

fisentence compression tree transduction

reference tree  devising suitable loss functions model straightforwardly
adapted text rewriting tasks besides sentence compression 
mcdonald        presents sentence compression model uses discriminative
large margin algorithm  model rich feature set defined compression bigrams
including parts speech  parse trees  dependency information  without however making explicit use synchronous grammar  decoding model amounts finding
combination bigrams maximize scoring function defined adjacent words
compression intervening words dropped  model differs
mcdonalds two important respects  first  capture complex tree transformations go beyond bigram deletion  tree based  decoding algorithm
better able preserve grammaticality compressed output  second  treebased representation allows greater modeling flexibility  e g   defining wide range
loss functions tree string yield  contrast  mcdonald define loss
functions final compression 
although bulk research sentence compression relies parallel corpora
modeling purposes  approaches use training data small amount 
example work hori furui         propose model automatically
transcribed spoken text  method scores candidate compressions using language
model combined significance score  indicating whether word topical not  
score representing speech recognizers confidence transcribing given word
correctly  despite conceptually simple knowledge lean  model operates
word level  since take syntax account  means deleting
constituents spanning several subtrees  e g   relative clauses   clarke lapata       
show unsupervised models greatly improved linguistically motivated
constraints used decoding 

   problem formulation
mentioned earlier  formulate sentence compression tree to tree rewriting problem
using weighted synchronous grammar coupled large margin training process 
model learns parallel corpus input  uncompressed  output  compressed  pairs
 x    y              xn   yn   predict target labeled tree source labeled tree x 
capture dependency x weighted stsg define
following section  section     discusses extract grammar parallel
corpus  rule score  ngram output tree 
overall score compression sentence x derived  introduce scoring
function section     explain training algorithm section      framework
decoding amounts finding best target tree licensed grammar given source
tree  present chart based decoding algorithm section     
    synchronous grammar
synchronous grammar defines space valid source target tree pairs  much
regular grammar defines space valid trees  synchronous grammars treated tree
transducers reasoning space possible sister trees given tree  is 
trees produced alongside given tree  essentially transducer
   

ficohn   lapata

algorithm   generative process creating pair trees 
initialize source tree  x   rs
initialize target tree    rt
initialize stack frontier nodes  f     rs   rt   
node pairs   vs   vt   f
choose rule hvs   vt h   
rewrite node vs x
rewrite node vt
variables  u
find aligned child nodes   cs   ct    vs vt corresponding u
push  cs   ct   f
end
end
x complete

takes tree input produces tree output  grammar rules specify
steps taken transducer recursively mapping tree fragments input tree
fragments target tree  many families synchronous grammars  see
section     elect use synchronous tree substitution grammar  stsg   one
simpler formalisms  consequently efficient inference algorithms  still
complex enough model rich suite tree edit operations 
stsg   tuple  g    ns   nt       p  rs   rt   n non terminals
terminals  subscripts indicating source target respectively  p productions rs ns rt nt distinguished root symbols 
production rewrite rule two aligned non terminals x ns nt
source target 
hx  h   
   
elementary trees rooted symbols x respectively  note
synchronous context free grammar  scfg  limits one level elementary
trees  otherwise identical stsg  imposes limits  non terminal
leaves elementary trees referred frontier nodes variables 
points recursion transductive process  one to one alignment frontier
nodes specified   alignment represent deletion  or insertion 
aligning node special symbol  indicates node present
tree  nodes aligned   allows subtrees deleted
transduction  disallow converse   aligned nodes   would license
unlimited insertion target tree  independently source tree  capability
would limited use sentence compression  increasing complexity
inference 
grammar productions used generative setting produce pairs trees 
transductive setting produce target tree given source tree  algorithms  
  present pseudo code processes  generative process  algorithm    starts
two root symbols applies production rewrites symbols
productions elementary trees  elementary trees might contain frontier nodes 
   

fisentence compression tree transduction

algorithm   transduction source tree target tree 
require  complete source tree  x  root node labeled rs
initialize target tree    rt
initialize stack frontier nodes  f     root x   rt   
node pairs   vs   vt   f
choose rule hvs   vt h    matches sub tree rooted vs x
rewrite vt
variables  u
find aligned child nodes   cs   ct    vs vt corresponding u
push  cs   ct   f
end
end
complete

case aligned pairs frontier nodes pushed stack  later rewritten
using another production  process continues recursive fashion stack
empty frontier nodes remaining   point two trees complete 
sequence rewrite rules referred derivation  source
target tree recovered deterministically 
model uses stsg transductive setting  source tree given
target tree generated  necessitates different rewriting process 
shown algorithm    start source tree  rt   target root symbol 
aligned root node source  denoted root x   choose production
rewrite pair aligned non terminals productions source side    matches
source tree  target symbol rewritten using   variable
matching node source corresponding leaf node target tree pushed
stack later processing   process repeats stack empty 
therefore source tree covered  complete target tree 
use term derivation refer sequence production applications  target
string yield target tree  given reading non terminals tree
left right manner 
let us consider compression example figure    tree editing rules
figure   encoded stsg productions figure    see rules          production     
reproduces tree pair     figure    production     tree pair      on  notation
figure    primarily space reasons  uses brackets      indicate constituent boundaries 
brackets surround constituents non terminal child nodes 
terminals  non terminals bracketed subtrees  boxed indices short hand notation
alignment    example  rule     specify two wp non terminals
aligned rb node occurs source tree  i e   heads deleted subtree   grammar rules allow differences non terminal category source
target  seen rules         allow arbitrarily deep elementary trees 
   special care must taken aligned variables  nodes  aligned signify source
sub tree point deleted without affecting target tree  reason safely
ignore source nodes deleted manner 

   

ficohn   lapata

   
   
   
   
   
   
   
   
   
    
    
    

hwhnp  whnpi
hs  npi
hs  vpi
hs  vpi
hs  si
hwp  wpi
hnp  npi
hnns  nnsi
hvp  vpi
hvbp  vbpi
hvp  vpi
hvbn  vbni

rules perform major tree edits
h whnp rb wp       whnp wp    i
h s np   vp    np  
h s np vp      vp  
h s whnp      vp  
h s  s whnp        cc and        s whnp    s np   vp     i
rules preserve tree structure
h wp what    wp what i
h np nns       np nns    i
h nns records    nns records i
h vp vbp   vp       vp vbp   vp    i
h vbp are    vbp are i
h vp vbn       vp vbn    i
h vbn involved    vbn involved i

figure    rules synchronous tree substitution grammar  stsg  capable generating sentence pair figure    equivalently  grammar defines
transducer convert source tree  figure   a   target tree
 figure   b    rule rewrites pair non terminals pair subtrees 
shown bracketed notation 

evidenced rule     trees depth two  rules         complete
toy grammar describes tree pair figure    rules copy parts
source tree target  terminals  e g   rule      internal nodes children
 e g   rule      
figure   shows grammar used transduce source tree
target tree figure    first steps derivation shown graphically figure    start source tree  seek transduce root symbol
target root symbol  denoted s s  first rule applied rule     figure    source side     s  s whnp s   cc and  s   matches root source tree
requisite target category    s  matching part source tree
rewritten using rules target elementary tree     s whnp  s np vp    three
three variables annotated reflect category transformations required
node  whnp whnp  s np s vp  process continues leftmost
nodes  labeled whnp whnp  rule      from figure    applied  deletes
nodes left child  shown rb   retains right child  subsequent rule completes
transduction whnp node matching string exactly  algorithm continues visit variable node finishes variable nodes remaining 
resulting desired target tree 
    grammar
previous section outlined stsg formalism employ sentence compression
model  save one important detail  grammar itself  example  could obtain
   

fisentence compression tree transduction

 s s  s  whnp exactly what   s  np records   vp made it   
 cc and   s  whnp which   s  np ones   vp involved    
 
 s  whnp whnp  rb exactly   wp what    s  s np  np records   vp made it  
 s vp  whnp which   s  np ones   vp involved     
 
 s  whnp  wp wp what    s  s np  np records   vp made it  
 s vp  whnp which   s  np ones   vp involved     
 
 s  whnp  wp what    s  s np  np records   vp  vbd made   np  prp it     
 s vp  whnp which   s  np ones   vp  vbp are   vp  vbn involved       
 
 s  whnp  wp what    s  np  nns nns records  
 s vp  whnp which   s  np ones   vp involved     
 
 s  whnp  wp what    s  np  nns records   
 s vp  whnp which   s  np ones   vp involved    
 
 s  whnp what   s  np records   s vp  np ones   vp involved    
 
 s  whnp what   s  np records   vp vp  vp  vbp are   vp  vbn involved      
 
 s  whnp what   s  np records   vp  vbp vbp are   vp vp  vbn involved     
    s  whnp what   s  np records   vp  vbp are   vp vp  vbn involved     
    s  whnp what   s  np records   vp  vbp are   vp  vbn vbn involved     
    s  whnp  wp what    s  np  nns records    vp  vbp are   vp  vbn involved     

figure    derivation example sentence pair figure    line shows rewrite step 
denoted subscript identifies rule used  frontier
nodes shown bold x y indicating symbol x must transduced
subsequent steps  sake clarity  internal nodes
omitted 

synchronous grammar hand  automatically corpus  combination 
requirement grammar allows source trees training set
transduced corresponding target trees  maximum generality  devised
automatic method extract grammar parsed  word aligned parallel compression
corpus  method maps word alignment constituent level alignment
nodes source target trees  pairs aligned subtrees next generalized create
tree fragments  elementary trees  form rules grammar 
first step algorithm find constituent alignment  define
set source target constituent pairs whose yields aligned one another
word alignment  base approach alignment template method  och   ney 
       uses word alignments define alignments ngrams  called phrases
smt literature   method finds pairs ngrams least one word one
ngrams aligned word other  word either ngram aligned
word outside ngram  addition  require ngrams syntactic
constituents  formally  define constituent alignment as 
c     vs   vt      s  t   vs    vt   

   

   s  t   s  vs    vt     
vs vt source target tree nodes  subtrees       s  t   set word
alignments  pairs word indices      returns yield span subtree  the minimum
maximum word index yield  exclusive or operator  figure   shows
   

ficohn   lapata











vp

whnp

np

rb

wp

nns

exactly



records

vp
np

whnp

vbd prp cc
made





wp


np

vp

nns vbp
ones



vbn
involved









vp

whnp

np

rb

wp

nns

exactly



records

whnp


np

made

np

vp
whnp

vbd prp cc




wp


np


vbn
involved










np

rb

wp

nns

exactly



records

whnp


vp

whnp

whnp

vbd prp cc
made





wp


np

vp

nns vbp
ones



wp np vp

vp
np

vp

vp

nns vbp
ones





vbn
involved

figure    graphical depiction first two steps derivation figure    source
tree shown left partial target tree right  variable nodes
shown bold face dotted lines show alignment 

word alignment constituent alignments licensed sentence pair
figure   
next step generalize aligned subtree pairs replacing aligned child subtrees
variable nodes  example  figure   consider pair aligned subtrees
 s ones involved   vp involved   could extract rule 
hs vpi h s  whnp  wp which    s  np  nns ones   vp  vbp are   vp  vbn involved       
 vp  vbp are   vp  vbn involved   i

   

however  rule specific consequently useful transduction
model  order applied  must see full subtree  highly unlikely
occur another sentence  ideally  generalize rule match many
source trees  thereby allow transduction previously unseen structures 
example  node pairs labeled  vp    vp      vbp  vbp    vp    vp     vbn  vbn 
generalized nodes aligned constituents  subscripts added distinguish
   

fisentence compression tree transduction








vp

whnp

np

np

vp
whnp np

vp

rb
wp nns vbd prp cc wp nns vbp vbn
exactly records made
ones involved


vp
whnp np
vp
wp nns vbp vbn
records involved

figure    tree pair word alignments shown binary matrix  dark square indicates
alignment words row column  overlaid rectangles
show constituent alignments inferred word alignment 

two vp nodes   addition  nodes whnp  wp  np nns source
unaligned  therefore generalized using  alignment signify deletion 
perform possible generalizations example   would produce
rule 
hs vpi h s whnp      vp  
   
many possible rules extracted applying different legal
combinations generalizations  there    total example  
algorithm   shows minimial  most general  rules extracted   results
minimal set synchronous rules describe tree pair   rules
minimal sense cannot made smaller  e g   replacing subtree
variable  still honoring word alignment  figure   shows resulting minimal
set synchronous rules example figure    seen example 
many rules extracted overly general  ideally  would extract every rule
every legal combination generalizations  however leads massive number rules
exponential size source tree  address problem allowing limited
number generalizations skipped extraction process  equivalent
altering lines     algorithm   first make non deterministic decision whether
match ignore match continue descending source tree  recursion depth
limits number matches ignored way  example  allow one
   generalizations mutually exclusive  take highest match trees 
   non deterministic matching step line   allows matching options individually 
implemented mutually recursive function replicates algorithm state process
different match 
   algorithm   extension galley  hopkins  knight  marcus        technique extracting
scfg word aligned corpus consisting  tree  string  pairs 

   

ficohn   lapata

algorithm   extract x  y  a   extracts minimal rules constituent aligned trees
require  source tree  x  target tree  y  constituent alignment 
   initialize source target sides rule    x   
   initialize frontier alignment   
   nodes vs   top down
  
vs null aligned
  
 vs    
  
delete children
  
else vs aligned target node s 
  
choose target node  vt
 non deterministic choice 
  
call extract vs   vt   a 
   
 vs   vt  
   
delete children vs
   
delete children vt
   
end
    end
    emit rule hroot    root  i h   
level recursion extracting rules  s  vp  pair figure    get
additional rules 
hs vpi h s  whnp wp        vp  
hs vpi h s whnp  s np vp       vp  
two levels recursion  get 
hs vpi h s  whnp  wp which        vp  
hs vpi h s  whnp  wp which    s np vp       vp  
hs vpi h s whnp  s  np nns   vp       vp  
hs vpi h s whnp  s np  vp vbd   vp         vbd   vbd    i
compared rule     see specialized rules add useful structure
lexicalisation  still sufficiently abstract generalize new sentences  unlike
rule      number rules exponential recursion depth  fixed depth
polynomial size source tree fragment  set recursion depth
small number  one two  experiments 
guarantee induced rules good coverage unseen trees 
tree fragments containing previously unseen terminals non terminals  even unseen
sequence children parent non terminal  cannot matched grammar productions  case transduction algorithm  algorithm    fail way
covering source tree  however  problem easily remedied adding new
rules grammar allow source tree fully covered   node
   alternative  equally valid  techniques improving coverage simplify syntax trees 
example  done explicitly binarizing large productions  e g   petrov  barrett  thibaux 
  klein        implicitly markov grammar grammar productions  e g   collins        

   

fisentence compression tree transduction

hs si
hwhnp whnpi
hwp wpi
hs npi
hnp npi
hnns nnsi
hs vpi
hs vpi
hvp vpi
hvbp vbpi
hvp vpi
hvbn vbni














h s  s whnp       cc       s whnp    s np   vp     i
h whnp rb wp       whnp wp    i
h wp what    wp what i
h s np   vp    np  
h np nns       np nns    i
h nns records    nns records i
h s whnp      vp  
h s np vp      vp  
h vp vbp   vp       vp vbp   vp    i
h vbp are    vbp are i
h vp vbn       vp vbn    i
h vbn involved    vbn involved i

figure    minimal set stsg rules extracted aligned trees figure   

source tree  rule created copy node child nodes target tree 
example  see fragment  np dt jj nn  source tree  add rule 

hnp npi h np dt   jj   nn       np dt   jj   nn    i

rules  source node copied target tree  therefore transduction algorithm trivially recreate original tree  course  grammar
rules work conjunction copying rules produce target trees 
copy rules solve coverage problem unseen data  solve
related problem under compression  occurs unseen cfg productions
source tree therefore applicable grammar rules copy rules 
copy child nodes target  none child subtrees deleted unless
parent node deleted higher level rule  case children
deleted  clearly  would add considerable modelling flexibility able delete some 
all  children  reason  add explicit deletion rules source
cfg production allow subsets child nodes deleted linguistically
plausible manner 
deletion rules attempt preserve important child nodes  measure
importance using head finding heuristic collins parser  appendix a  collins 
       collins method finds single head child cfg production using hand coded
tables non terminal type  desire set child nodes  run algorithm
find matches rather stopping first match  order match
found used ranking importance child  ordered list child nodes
used create synchronous rules retain head    heads             heads 
   

ficohn   lapata

fragment  np dt jj nn   heads found following order  nn  dt 
jj   therefore create rules retain children  nn    dt  nn   dt  jj  nn  
hnp npi h np dt jj nn       np nn    i
hnp nni h np dt jj nn      nn  
hnp npi h np dt   jj nn       np dt   nn    i
hnp npi h np dt   jj   nn       np dt   jj   nn    i
note one child remains  rule produced without parent node 
seen second rule above 
    linear model
stsg defines transducer capable mapping source tree many possible
target trees  little use without kind weighting towards grammatical trees
constructed using sensible stsg productions yield fluent compressed target sentences  ideally model would define scoring function target
trees strings  however instead operate derivations  general  may many
derivations produce target tree  situation referred spurious ambiguity  fully account spurious ambiguity would require aggregating derivations
produce target tree  would break polynomial time dynamic program used inference  rendering inference problem np complete  knight        
end  define scoring function derivations 
score d  w    h d   wi

   

derivation  consisting sequence rules  w model parameters 
vector valued feature function operator h  inner product 
parameters  w  learned training  described section     
feature function    defined as 
x
x
 d   
 r  source d    
 m  source d  
   
rd

mngrams d 

r rules derivation  ngrams d  ngrams yield target
tree feature function returning vector feature values rule  note
feature function access rule  r  source tree  source d  
conditional model therefore overhead terms modeling
assumptions complexity inference 
second summand      ngrams yield target tree
feature function ngrams  traditional  weighted  synchronous grammars
allow features decompose derivation  i e   expressed using first
summand       however  limiting requirement  ngram features
allow modeling local coherence commonly used sentence compression
literature  knight   marcu        turner   charniak        galley   mckeown       
   derivation  d  fully specifies source  x   source d   target tree    target d  

   

fisentence compression tree transduction

clarke   lapata        hori   furui        mcdonald         instance  deleting
sub tree left right siblings  critical know new siblings
grammatical configuration  yield still forms coherent string 
reason  allow ngram features  specifically conditional log probability
ngram language model  unfortunately  comes price ngram features
significantly increase complexity inference used training decoding 
    decoding
decoding aims find best target tree licensed grammar given source tree 
mentioned above  deal derivations place target trees  decoding finds
maximizing derivation    of 
 

argmax

score d  w 

   

d source d  x

x  given  source tree  source d  extracts source tree derivation
score defined      maximization performed space derivations
given source tree  defined transduction process shown algorithm   
maximization problem     solved using chart based dynamic program
shown algorithm    extends earlier inference algorithms weighted stsgs  eisner        assume scoring function must decompose derivation 
i e   features apply rules terminal ngrams  relaxing assumption leads
additional complications increased time space complexity  equivalent using grammar intersection original grammar ngram language
model  explained chiang        context string transduction scfg 
algorithm defines chart  c  record best scoring  partial  target tree
source node vs root non terminal t  back pointers  b  record maximizing
rule store pointers child chart cells filling variable rule  chart
indexed n   terminals left right edges target trees yield
allow scoring ngram features   terminal ngrams provide sufficient context evaluate
ngram features overlapping cells boundary chart cell combined another
rule application  this operation performed boundary ngrams function line
     best illustrated example  using trigram features  n      node
rewritten  np fast car  must store ngram context  the fast  fast car 
chart entry  similarly  vp skidded halt  would ngram context  skidded to 
halt   applying parent rule  s np vp  rewrites two trees adjacent
siblings need find ngrams boundary np vp 
easily retrieved two chart cells contexts  combine right edge np
context  fast car  left edge vp context  skidded to  get two trigrams
fast car skidded car skidded to  trigrams fast car  skidded
halt already evaluated child chart cells  new
combined chart cell given context  the fast  halt  taking left right
   strictly speaking  terminals right edge required compression model would
create target string left to right manner  however  algorithm general
allows reordering rules hpp ppi h pp   np       pp np      i  rules required
text rewriting tasks besides sentence compression 

   

ficohn   lapata

algorithm   exact chart based decoding algorithm 
require  complete source tree  x  root node labeled rs
   let c v  t  l  r chart representing score best derivation transducing
tree rooted v tree root category ngram context l
   let b v  t  l   p  x nt l  corresponding back pointers  consisting
production source node  target category ngram context
productions variables
   initialize chart  c        
   initialize back pointers  b         none
   source nodes  vs x  bottom up
  
rules  r   hvs   h    matches sub tree rooted vs
  
let target ngrams wholly contained
  
let features vector   r  x     m  x 
  
let l empty ngram context
   
let score  q  
   
variables  u
   
find source child node  cu   vs corresponding u
   
let tu non terminal target child node corresponding u
   
choose child chart entry  qu   c cu   tu   lu  
 non deterministic choice lu  
   
let boundary ngrams r  lu  
   
update features     m  x 
   
update ngram context  l merge ngram context l  lu  
   
update score  q q   qu
   
end
   
update score  q q   h  wi
   
q   c vs   y  l 
   
update chart  c vs   y  l  q
   
update back pointers  b vs   y  l   r    cu   tu   lu  u  
   
end
   
end
    end
    find best root chart entry  l argmaxl c root x   rt   l 
    create derivation  d  traversing back pointers b root x   rt   l  

edges two child cells  merging process performed merge ngram context
function line     finally add artificial root node target tree n   artificial
start terminals one end terminal  allows ngram features applied
boundary ngrams beginning end target string 
decoding algorithm processes source tree post order traversal  finding
set possible trees ngram contexts source node inserting
chart  rules match node processed lines      feature vector 
  calculated rule ngrams therein  line     ngrams bordering child
cells filling rules variables  line      note feature vector includes
features specific rule boundary ngrams  wholly contained
   

fisentence compression tree transduction

child cell  reason score sum scores child cell  line
    feature vector model weights  line      new ngram context  l 
calculated combining rules frontier ngram contexts child cells  line
     finally chart entry node updated score betters previous value
 lines       
choosing child chart cell entry line     many different entries
different ngram context  lu   affects ngram features    consequently
ngram context  l  score  q  rule  non determinism means every
combination child chart entries chosen variable  combinations
evaluated inserted chart  number combinations product
number child chart entries variable  bounded o  tt    n  v  
 tt   size target lexicon v number variables  therefore
asymptotic time complexity decoding o sr tt    n  v   number
source nodes r number matching rules node  high complexity
clearly makes exact decoding infeasible  especially either n v large 
adopt popular approach syntax inspired machine translation address
problem  chiang         firstly  use beam search  limits number different
ngram contexts stored chart cell constant  w   changes base
complexity term  leading improved o srw v   still exponential
number variables  addition  use chiangs cube pruning heuristic
limit number combinations  cube pruning uses heuristic scoring function
approximates conditional log probability ngram language model logprobability unigram model   allows us visit combinations best first
order heuristic scoring function beam filled the beam rescored
using correct scoring function  done cheaply o w v   time  leading
overall time complexity decoding o srw v    refer interested reader
work chiang        details 
    training
turn problem derivations scored model  given source
tree  space sister target trees implied synchronous grammar often large 
majority trees ungrammatical poor compressions  job
training algorithm find weights reference target trees high scores
many target trees licensed grammar given lower scores 
explained section     define scoring function derivations  function
given          reproduced below 
f  d  w   

argmax hw   d i

   

d source d  x

equation     finds best scoring derivation  d  given source  x  linear model 
recall derivation generates source tree x target tree  goal
   use conditional log probability ngram language model ngram feature  order
use ngram features  binary identity features specific ngrams  would first advisable
construct approximation decomposes derivation use cube pruning heuristic 

   

ficohn   lapata

training procedure find parameter vector w satisfies condition 
i    source d    xi    di   hw   di    d i  

   

xi   di ith training source tree reference derivation  condition states
training instances reference derivation least high scoring
derivations  ideally  would know extent predicted target
tree differs reference tree  example  compression differs gold
standard respect one two words treated differently compression
bears resemblance it  another important factor length compression 
compressions whose length similar gold standard preferable longer
shorter output  loss function  yi   y  quantifies accuracy prediction
respect true output value yi  
plethora different discriminative training frameworks optimize
linear model  possibilities include perceptron training  collins         log linear optimisation conditional log likelihood  berger  pietra    pietra        large margin
methods  base training tsochantaridis et al s        framework learning
support vector machines  svms  structured output spaces  using svmstruct implementation   framework supports configurable loss function particularly
appealing context sentence compression generally text to text generation  efficient training algorithm powerful regularization  latter
critical discriminative models large numbers features  would otherwise
over fit training sample expense generalization accuracy  briefly summarize
approach below  detailed description refer interested reader
work tsochantaridis et al         
traditionally svms learn linear classifier separates two classes
largest possible margin  analogously  structured svms attempt separate correct
structure structures large margin  learning objective
structured svm uses soft margin formulation allows errors training set
via slack variables   
n

 
cx
min   w     
   
w   
n

    

i  

i    source d    xi    di   hw   di    d i  di   d 
slack variables    introduced training example  xi c constant
controls trade off training error minimization margin maximization 
note slack variables combined loss incurred linear constraints  means high loss output must separated larger margin
low loss output  much larger slack variable satisfy constraint  alternatively  loss function used rescale slack parameters  case
constraints      replaced hw   di    d i    dii  d    margin rescaling
theoretically less desirable scale invariant  therefore requires tuning
additional hyperparameter compared slack rescaling  however  empirical results show
   http   svmlight joachims org svm struct html

   

fisentence compression tree transduction

little difference two rescaling methods  tsochantaridis et al          use
margin rescaling practical reason approximated accurately
slack rescaling chart based inference method 
optimization problem      approximated using algorithm proposed
tsochantaridis et al          algorithm finds small set constraints fullsized optimization problem ensures sufficiently accurate solution  specifically 
constructs nested sequence successively tighter relaxation original problem using
 polynomial time  cutting plane algorithm  training instance  algorithm
keeps track selected constraints defining current relaxation  iterating
training examples  proceeds finding output radically violates
constraint  case  optimization crucially relies finding derivation
high scoring high loss compared gold standard  requires finding
maximizer of 
h d     d   d  hw   di    d i

    

search maximizer h d       performed decoding algorithm presented section     extensions  firstly  expanding     
h d     d   d  h di    wi   h d   wi see second term constant
respect d  thus influence search  decoding algorithm maximizes
last term  remains include loss function search process 
loss functions
decompose
rules target ngrams derivation 
p
p

  r   

 d
 d   d   
nngrams d  n  d   n   easily integrated
rd r
decoding algorithm  done adding partial loss  r  d   r    n  d   n 
rules score line    algorithm    the ngrams recovered ngram contexts
manner used evaluate ngram features  
however  many loss functions decompose rules ngrams 
order calculate losses chart must stratified loss functions arguments
 joachims         example  unigram precision measures ratio correctly predicted
tokens total predicted tokens therefore loss arguments pair counts 
 t p  f p    true false positives  initialized        updated
rule used derivation  equates checking whether target terminal
reference string incrementing relevant value  chart extended  stratified 
store loss arguments way ngram contexts stored decoding 
means rule accessing child chart cell get multiple entries 
different loss argument values well multiple ngram contexts  line    algorithm
    loss argument rule application calculated rule loss
arguments children  stored chart back pointer list  lines
     algorithm     although loss evaluated correctly complete
derivations  evaluate loss partial derivations part cube pruning
heuristic  losses large space argument values coarsely approximated
beam search  prunes number chart entries constant size 
reason  focused mainly simple loss functions relatively small space
argument values  use wide beam search      unique items    
items  whichever comes first  
   

ficohn   lapata

algorithm   find gold standard derivation pair trees  i e   alignment  
require  source tree  x  target tree 
   let c vs   vt   r chart representing maximum number rules used align
nodes vs x vt
   let b vs   vt    p  x y  corresponding back pointers  consisting production
pair aligned nodes productions variables
   initialize chart  c      
   initialize back pointers  b       none
   source nodes  vs x  bottom up
  
rules  r   hvs   h    matches sub tree rooted vs
  
target nodes  vt y  matching
  
let rule count  j  
  
variables  u
   
find aligned child nodes   cs   ct    vs vt corresponding u
   
update rule count  j j   c cs   ct  
   
end
   
n greater previous value chart
   
update chart  c vs   vt   j
   
update back pointers  b vs   vt    r    cs   ct  u  
   
end
   
end
   
end
    end
    c root x   root y     
   
success  create derivation traversing back pointers b root x   root y  
    end
discussion far assumed given gold standard derivation  yi
glossing issue find it  spurious ambiguity grammar means
often many derivations linking source target  none clearly
correct  select derivation using maximum number rules 
small  therefore provide maximum generality    found using algorithm   
chart based dynamic program similar alignment algorithm inverse transduction
grammars  wu         algorithm time complexity o s   r  size
larger two trees r number rules match node 
    loss functions
training algorithm described highly modular theory support wide
range loss functions  widely accepted evaluation metric text compression  zero one loss would straightforward define inappropriate problem 
    experimented heuristics  including choosing derivation random selecting
derivation maximum minimum score model  all using search algorithm
different objective   these  maximum scoring derivation competitive
maximum rules heuristic 

   

fisentence compression tree transduction

would always penalize target derivations differ even slightly reference
derivation  ideally  would loss wider scoring range discriminate
derivations differ reference  may good compressions whereas others may entirely ungrammatical  reason developed
range loss functions draw inspiration various metrics used evaluating
text to text rewriting tasks summarization machine translation 
loss functions defined derivations look item accessible including
tokens  ngrams cfg rules  first class loss functions calculates hamming
distance unordered bags items  measures number predicted items
appear reference  along penalty short output 
hamming  d   d    f p   max  l  t p   f p      

    

p f p number true false positives  respectively  comparing
predicted target  dt   reference  dt   l length reference 
include second term penalize overly short output otherwise predicting little
nothing would incur penalty 
created three instantiations loss function      over     tokens 
   ngrams  n        cfg productions  case  loss argument space
quadratic size source tree  hamming ngram loss attempt defining
loss function similar bleu  papineni  roukos  ward    zhu         latter
defined documents rather individual sentences  thus directly applicable
problem  now  since losses operate unordered bags may reward
erroneous predictions  example  permutation reference tokens zero
token loss  less problem cfg ngram losses whose items overlap 
thereby encoding partial order  another problem loss functions described
penalize multiply predicting item occurred reference  could problem function words common sentences 
therefore developed two additional loss functions take multiple predictions
account  first measures edit distance number insertions deletions
predicted reference compressions  bags of tokens  contrast
previous loss functions  requires true positive counts clipped
number occurrences type reference  edit distance given by 
x
edit  d   d    p   r  
min pi   qi  
    


p q denote number target tokens predicted tree  target d  
reference    target d    respectively  pi qi counts type i  loss
arguments edit distance consist vector counts item type
reference   pi   i   space possible values exponential size source tree 
compared quadratic hamming losses  consequently  expect beam search
result many search errors using edit distance loss 
last loss function f  measure  harmonic mean precision recall 
measured bags of tokens  edit distance  calculation requires counts
clipped number occurrences terminal type reference 
   

ficohn   lapata

ref 
pred 

 s  whnp  wp what    s  np  nns records    vp  vbp are   vp  vbn involved     
 s  whnp  wp what    s  np  nns ones    vp  vbp are   vbn involved    
loss
token hamming
  gram hamming
cfg hamming
edit distance
f 

arguments
p      f p    
p      f p    
p      f p    
p                  
p                  

value
   
    
   
 
   

table    loss arguments values example predicted reference compressions 
note loss values compared different loss functions 
values purely illustrative 

therefore use loss arguments calculation  f  loss given by 
f   d   d     
p

min p  q  

  precision recall
precision   recall
p

    

min p  q  

precision   p recall   q   f  shares arguments
edit distance loss  exponential space loss argument values
consequently subject severe pruning beam search used training 
illustrate loss functions  present example table    here 
prediction  pred  reference  ref  length    tokens   identical syntactic
structure  differ one word  ones versus records   correspondingly  three
correct tokens one incorrect  forms arguments token hamming loss 
resulting loss      ngram loss measured n   start end
string padded special symbols allow evaluation boundary ngrams 
cfg loss records one incorrect cfg production  the preterminal  nns ones  
total nine productions  last two losses use arguments  vector values
counts reference type  first four cells correspond what  records 
involved  last cell records types  example  edit distance two
 one deletion one insertion  f  loss      precision recall      

   features
feature space defined source trees  x  target derivations  d  devised two
broad classes features  applying grammar rules ngrams target terminals 
defined single ngram feature  conditional log probability trigram language
model  trained bnc      million words  using sri language modeling
toolkit  stolcke         modified kneser ney smoothing 
rule hx y h    i  extract features according templates detailed
below  templates give rise binary indicator features  except explicitly stated 
features perform boolean test  returning value   test succeeds  
otherwise  example rule corresponding features shown table   
   

fisentence compression tree transduction

type  whether rule extracted training set  created copy rule and or
created delete rule  allows model learn preference
three sources grammar rules  see row type table   
root  root categories source  x  target    conjunction  x
 see rows root table    
identity  source side    target side    full rule          allows
model learn weights individual rules sharing elementary tree  another feature checks rules source target elementary trees identical   
 see rows identity table    
unlexicalised identity  identity feature templates replicated unlexicalised elementary trees  i e   terminals removed frontiers  see
rows unlexid table    
rule count  feature always    allowing model count number rules
used derivation  see row rule count table    
word count  counts number terminals   allowing global preference
shorter longer output  additionally  record number terminals
source tree  used target terminal count find number
deleted terminals  see rows word count table    
yield  features compare terminal yield source      target     
first feature checks identity two sequences         use identity
features terminal yields  terminal source  see
rows yield table     replicate feature templates sequence
non terminals frontier  pre terminals variable non terminals  
length  records difference lengths frontiers   whether
targets frontier shorter source  see rows length table    
features listed defined rules grammar  includes
copy delete rules  described section      added address
problem unseen words productions source trees test time  many
rules applied training set  receive weight share
features rules used training  however  training model learns
disprefer coverage rules unnecessary model training set 
described perfectly using extracted transduction rules  dual use training
set grammar extraction parameter estimation results bias coverage
rules  bias could addressed extracting grammar separate corpus 
case coverage rules would useful modeling training set
testing sets  however  solution problems  namely many target
trees training may longer reachable  bias possible solutions
interesting research problem deserves work 
   

ficohn   lapata

rule  hnp nnsi h np cd adjp  nns activists     nns activists i
type
type   training set
 
root
x   np
 
root
  nns
 
root
x   np   nns
 
identity
   np cd adjp  nns activists  
 
identity
   nns activists 
 
identity    np cd adjp  nns activists      nns activists 
 
unlexid 
unlex     np cd adjp nns 
 
unlexid 
unlex    nns
 
unlexid 
unlex     np cd adjp nns    nns
 
rule count

 
word count
target terminals
 
word count
source terminals  
yield
source    activists  target    activists 
 
yield
terminal activists source target
 
yield
non terms  source    cd  adjp  nns  target    nns 
 
yield
non terminal cd source target
 
yield
non terminal adjp source target
 
yield
non terminal nns source target
 
length
difference length
 
length
target shorter
 
table    features instantiated synchronous rule shown above  features
non zero values displayed  number source terminals calculated using
source tree time rule applied 

   experimental set up
section present experimental set up assessing performance
sentence compression model described above  give details corpora used  briefly
introduce mcdonalds        model used comparison approach  explain
system output evaluated 
    corpora
evaluated system three publicly available corpora  first ziff davis
corpus  popular choice sentence compression literature  corpus originates
collection news articles computer products  created automatically
matching sentences occur article sentences occur abstract  knight
  marcu         two corpora   created manually  annotators asked
produce target compressions deleting extraneous words source without changing
word order  clarke   lapata         one corpus sampled written sources 
    available http   homepages inf ed ac uk s        data  

   

fisentence compression tree transduction

corpus
clspoken
clwritten
ziff davis

articles
  
  


sentences
    
    
    

training
   
   
    

development
  
  
  

testing
   
   
  

table    sizes various corpora  measured articles sentence pairs  data split
training  development testing sets measured sentence pairs 

british national corpus  bnc  american news text corpus  whereas
created manually transcribed broadcast news stories  henceforth refer
two corpora clwritten clspoken  respectively  sizes three
corpora shown table   
three corpora pose different challenges hypothetical sentence compression
system  firstly  representative different domains text genres  secondly 
different compression requirements  ziff davis corpus aggressively
compressed comparison clspoken clwritten  clarke   lapata         clspoken speech corpus  often contains incomplete ungrammatical utterances
speech artefacts disfluencies  false starts hesitations  utterances varying lengths  wordy whereas others cannot reduced further  means
compression system leave sentences uncompressed  finally 
note clwritten average longer sentences ziff davis clspoken  parsers
likely make mistakes long sentences could potentially problematic
syntax based systems one presented here 
although model capable performing editing operation  reordering
substitution  learn training corpora  corpora contain
deletions  therefore model learn transduction rules encoding  e g  
reordering  instead rules encode deleting inserting terminals restructuring internal nodes syntax tree  however  model capable general text
rewriting  given appropriate training set learn perform additional
edits  demonstrated recent results adapting model abstractive
compression  cohn   lapata         edit permitted  deletion 
experiments clspoken clwritten followed clarke lapatas        partition training  test  development sets  partition sizes shown table   
case ziff davis corpus  knight marcu        defined development
set  therefore randomly selected  and held out     sentence pairs training
set form development set 
    comparison state of the art
evaluated results mcdonalds        discriminative model  approach 
sentence compression formalized classification task  pairs words source
sentence classified adjacent target compression  let x   x            xn
denote source sentence target compression   y            ym yi occurs
x  function l yi            n   maps word yi target index word
   

ficohn   lapata

source  subject constraint l yi     l yi       mcdonald defines score
compression sentence x dot product high dimensional feature
representation  f   bigrams corresponding weight vector  w 
score x  y  w   


x

hw  f  x  l yj     l yj   i

    

i  

decoding framework amounts finding combination bigrams maximize
scoring function       maximization solved using semi markov viterbi
algorithm  mcdonald        
model parameters estimated using margin infused relaxed algorithm
 mira crammer   singer         discriminative large margin online learning technique 
mcdonald        uses similar loss function hamming loss  see       without
explicit length penalty  loss function counts number words falsely retained
dropped predicted target relative reference  mcdonald employs rich feature
set defined words  parts speech  phrase structure trees  dependencies 
gathered adjacent words compression words dropped 
clarke lapata        reformulate mcdonalds        model context integer
linear programming  ilp  augment constraints ensuring compressed
output grammatically semantically well formed  example  target sentence
negation  must included compression  source verb subject 
must retained compression  generate solve ilp every
source sentence using branch and bound algorithm  since obtain performance
improvements mcdonalds model several corpora  use comparison
model 
summarize  believe mcdonalds        model good basis comparison
several reasons  first  good performance  treated state of theart model  secondly  similar model many respects training algorithm
feature space differs one important respect  compression performed
strings trees  mcdonalds system make use syntax trees 
peripherally via feature set  contrast  syntax tree integral part
model 
    evaluation
line previous work assessed models output eliciting human judgments 
following knight marcu         conducted two separate experiments  first
experiment participants presented source sentence target compression
asked rate well compression preserved important information
source sentence  second experiment  asked rate grammaticality
compressed outputs  cases used five point rating scale high
number indicates better performance  randomly selected    sentences test
portion corpus  sentences compressed automatically system
mcdonalds        system  included gold standard compressions  materials
thus consisted              source target sentences  latin square design ensured
subjects see two different compressions sentence  collected
   

fisentence compression tree transduction

ratings    unpaid volunteers  self reported native english speakers  studies
conducted internet using webexp    software package running internetbased experiments 
report results using f  computed grammatical relations  riezler et al  
       although f  conflates grammaticality importance single score  nevertheless shown correlate reliably human judgments  clarke   lapata 
       furthermore  usefully employed development feature engineering parameter optimization experiments  measured f  directed labeled
dependency relations  models compressed output parsed using rasp
dependency parser  briscoe   carroll         note could extract dependencies directly output model since generates trees addition strings  however 
refrained order compare models equal footing 

   results
framework presented section   quite flexible  depending grammar extraction strategy  choice features  loss function  different classes models derived 
presenting results test set discuss specific model employed
experiments explain parameters instantiated 
    model selection
parameter tuning model selection experiments conducted development set clspoken corpus  obtained syntactic analyses source target
sentences bikels        parser  corpus automatically aligned using algorithm finds set deletions transform source target 
equivalent minimum edit distance script deletion operations permitted 
expected  predicted parse trees contained number errors  although
gold standard trees quantify error effect prediction
output  notice  however  errors source trees test set always
negatively affect performance model  many instances model able
recover errors still produce good output compressions  recoveries 
cases involved either deleting erroneous structure entirely preserving it 
often resulted poor output tree  string yield acceptable cases  less
commonly  model corrected errors source using tree transformation rules 
rules acquired training set errors source tree
test tree  example  one transformation allows prepositional phrase
moved high vp attachment object np attachment 
obtained synchronous tree substitution grammar clspoken corpus using
method described section      extracted maximally general synchronous rules 
complemented specified rules allowing recursion one ancestor
given node    grammar rules represented features described section   
important parameter modeling framework choice loss function 
    see http   www webexp info  
    rules pruned   variables    nodes 

   

ficohn   lapata

losses
hamming  tokens 
hamming  ngram 
hamming  cfg 
edit distance
f 
reference

rating
    
    
    
    
    
    

std  dev
    
    
    
    
    
    

table    mean ratings system output  clspoken development set  using different
loss functions 

evaluated loss functions presented section     follows  performed grid search
hyper parameters  a regularization parameter feature scaling parameter 
balances magnitude feature vectors scale loss function   
minimized relevant loss development set  used corresponding system
output  gold standard derivation selected using maximum number rules
heuristic  described section      beam limited     unique items     items
total  grammar filtered allow    target elementary trees
every source elementary tree 
next asked two human judges rate scale     systems compressions
optimized different loss functions  get idea quality output
included human authored reference compressions  sentences given high numbers
grammatical preserved important information  mean ratings
shown table    seen differences among losses large 
standard deviation high  hamming loss tokens performed best
mean rating       closely followed edit distance         chose former
latter less coarsely approximated search  subsequent experiments
report results using token based hamming loss 
wanted investigate synchronous grammar influences performance 
default system described used general rules together specialized rules
recursion depth limited one  experimented grammar uses
specialised rules maximum recursion depth two grammar uses solely
maximally general rules  table   report average compression rate  relations based
f  hamming loss tokens different grammars  see adding
specified rules allows better f   and loss  despite fact search space
remains same  observe slight degradation performance moving depth  
rules  probably due increase spurious ambiguity affecting search quality 
allowing greater overfitting training data  number transduction rules
grammar grows substantially increased depth       
maximally general extraction technique               specified rules depth
    found setting regularization parameter c        scaling parameter   generally
yields good performance across loss functions 

   

fisentence compression tree transduction

model
max general rules
depth   specified rules
depth   specified rules
max rules
max scoring
unigram lm
bigram lm
trigram lm
features
rule features
token features

compression rate
     
     
     
     
     
     
     
     
     
     
     

relations f 
     
     
     
     
     
     
     
     
     
     
     

loss
   
   
   
   
   
   
   
   
   
   
   

table    parameter exploration feature ablation studies  clspoken development set  
default system shown asterisk 

     respectively  growth grammar size exponential specification
depth therefore small values used 
inspected rules obtained maximally general extraction technique
better assess rules differ obtained vanilla scfg  see knight  
marcu         many rules       deeper structure therefore would
licensed scfg  due structural divergences source target
syntax trees training set      rules describe change syntactic
category  x       therefore remaining     rules would allowable
knight marcus transducer  proportion scfg rules decreases substantially
rule specification depth increased 
recall section     scoring function defined derivations rather
target trees strings  treat derivation using maximum number rules
gold standard derivation  sanity check  experimented selecting
derivation maximum score model  results table   indicate
latter strategy effective selecting derivation maximum number
rules  conjecture due overfitting  training data used
extract grammar  derivations maximum score may consist rules
rare features model data well generalize unseen instances 
finally  conducted feature ablation study assess features useful
task  particularly interested see ngram features would bring
benefit  especially since increase computational complexity decoding
training  experimented unigram  bigram  trigram language model  note
unigram language model computationally expensive two models
need record ngram contexts chart  shown table   
unigram language model substantially worse bigram trigram deliver
similar performances  examined impact features grouping
two broad classes  defined rules defined tokens  aim
see whether underlying grammar  represented rule based features  contributes
   

ficohn   lapata

better compression output  results table   reveal two feature groups
perform comparably  however  model using token based features tends compress
less  features highly lexicalized  model able generalize well
unseen data  conclusion  full feature set better counts two
ablation sets  better compression rate 
results reported measured string output  done first
stripping tree structure compression output  reparsing  extracting dependency
relations finally comparing dependency relations reference  however 
may wish measure quality trees themselves  string yield 
simple way measure this   would extract dependency relations directly
phrase structure tree output    compared dependencies extracted predicted
parses using bikels        parser output string  observe relation f 
score increases uniformly tasks              absolute  therefore
systems tree output better encodes syntactic dependencies tree resulting
re parsing string output  system part nlp pipeline  output
destined down stream processing  accurate syntax tree extremely
important  true related tasks desired output tree  e g  
semantic parsing 

   model comparison
section present results test set using best performing model
previous section  model uses grammar unlexicalized lexicalized rules
 recursion depth     hamming loss based tokens  features section   
model trained separately corpus  training portion   first discuss
results using relations f  move human study 
table   illustrates performance model  transducer   clspoken  clwritten  ziff davis  report results corpora using mcdonalds       
model  mcdonald  improved version  clarke ilp  put forward clarke
lapata         present compression rate system reference gold
standard  cases tree transducer model outperforms mcdonalds original model
improved ilp based version 
nevertheless  may argued model unfair advantage since
tends compress less models  therefore less likely make many
mistakes  ensure case  created version model
compression rate similar mcdonald  done relatively straightforwardly
manipulating length penalty hamming loss  smaller penalty
words model tend drop  therefore  varied length penalty  and
hyper parameters  development set order obtain compression rate similar
    could alternatively measure tree metrics  tree edit distance  however  standard
measures used parser evaluation  e g   evalb  would suitable  assume parse
yield fixed  case reference target string often different systems output 
    extract dependency relations conversion tool conll      shared task  available
http   nlp cs lth se pennconverter  

   

fisentence compression tree transduction

model
transducer 
transducer 
mcdonald
clarke ilp
reference

clspoken
compression rate
     
     
     
     
     

relations f 
     
     
     
     


model
transducer 
transducer 
mcdonald
clarke ilp
reference

clwritten
compression rate
     
     
     
     
     

relations f 
     
     
     
     


model
transducer 
mcdonald
clarke ilp
reference

ziff davis
compression rate
     
     
     
     

relations f 
     
     
     


table    results clspoken  clwritten  ziff davis corpus  testing set   compression
rate relations based f  

mcdonald    model applied test set performance shown
table   transducer   refrained ziff davis  since original
transducer obtained compression rate comparable mcdonald        vs         
seen  transducer  yields better f  clspoken clwritten  differences
f  statistically significant using wilcoxon test  p          transducer 
numerically outperforms mcdonald ziff davis  however difference significant
 the ziff davis test set consists solely    sentences  
next consider results judgment elicitation study assesses
detail quality generated compressions  recall participants judge compressed output two dimensions  grammaticality importance  compared
output system  transducer  clspoken clwritten transducer 
ziff davis  output mcdonald        reference gold standard  table  
illustrates examples compressions participants saw 
    matched compression rate mcdonald scaling length penalty          
clwritten clspoken corpora  respectively  another way control compression rate would
modify chart based decoder fashion similar mcdonald         however  leave
future work 

   

ficohn   lapata

s  wish parents teachers could teacher 
could communicate 
m  wish teachers could teacher 
t  wish teachers could this  could communicate 
r  wish parents teachers could this  could
communicate 
s  treasury refusing fund phase city technology
colleges 
m  treasury refusing fund colleges 
t  treasury refusing fund city technology colleges 
r  treasury refusing fund city technology colleges 
s  apparel makers use design clothes quickly produce
deliver best selling garments 
m  apparel makers use design clothes produce deliver
best selling garments 
t  apparel makers use design clothes 
r  apparel makers use design clothes 
s  earlier week  conference call analysts  bank said boosted
credit card reserves      million 
m  earlier said credit card reserves      million 
t  conference call analysts  bank boosted card reserves     
million 
r  conference call analysts bank said boosted credit card
reserves      million 
table    compression examples clspoken  clwritten  ziff davis  s  source sentence  m  mcdonald        t  transducer  r  reference gold standard 

table   shows mean ratings   system  and reference  clspoken 
clwritten  ziff davis  carried analysis variance  anova  examine
effect system type  mcdonald  transducer  reference  compression ratings  anova revealed reliable effect three corpora  used post hoc tukey
tests examine whether mean ratings system differed significantly  p         
clspoken corpus transducer perceived significantly better mcdonald  terms grammaticality importance  obtain result
clwritten corpus  two systems achieve similar performances ziff davis  the
grammaticality importance score differ significantly   ziff davis seems
less challenging corpus clspoken clwritten less likely highlight differences among systems  example  turner charniak        present several variants
noisy channel model  achieve compressions similar quality ziff davis
 grammaticality ratings varied      informativeness ratings     
human evaluation   cases transducer mcdonald yield significantly
    statistical tests reported subsequently done using mean ratings 

   

fisentence compression tree transduction

model
transducer
mcdonald
reference

clspoken
grammaticality
    
    
    

importance
    
    
    

model
transducer
mcdonald
reference

clwritten
grammaticality
    
    
    

importance
    
    
    

model
transducer
mcdonald
reference

ziff davis
grammaticality
    
    
    

importance
    
    
    

table    mean ratings compression output elicited humans     sig  diff  mcdonald              sig  diff  reference            using post hoc tukey
tests 

worse performance reference  save one exception  clspoken corpus 
significant difference transducer gold standard 
results indicate highly expressive framework good model sentence compression  several experimental conditions  across different domains 
obtain better performance previous work  importantly  model described
compression specific  could easily adapted tasks  corpora languages  for
syntactic analysis tools available   supervised  model learns fit
compression rate training data  sense  somewhat inflexible cannot
easily adapt specific rate given user imposed application  e g  
displaying text small screens   nevertheless  compression rate indirectly manipulated adopting loss functions encourage discourage compression directly
decoding stratifying chart length  mcdonald        

   conclusions
paper formulated sentence compression tree to tree rewriting task   
developed system licenses space possible rewrites using tree substitution grammar  grammar rule assigned weight learned discriminatively
within large margin model  tsochantaridis et al          specialized algorithm used
learn model weights find best scoring compression model  argue
    source code freely available http   homepages inf ed ac uk tcohn t  

   

ficohn   lapata

proposed framework appealing several reasons  synchronous grammar
provides expressive power capture rewrite operations go beyond word deletion
reordering  changes non terminal categories lexical substitution  since
deletion specific  model could ported rewriting tasks  see cohn   lapata 
      example  without overhead devising new algorithms decoding
training  moreover  discriminative nature learning algorithm allows incorporation manner powerful features  rich feature space conjunction
choice appropriate loss function afford greater flexibility fitting empirical
data different domains tasks 
evaluated model three compression corpora  clspoken  clwritten  ziffdavis  showed cases yields results superior state of the art  mcdonald         experiments designed assess several aspects proposed
framework complexity synchronous grammar  choice loss function 
effect various features  quality generated tree output  observed
performance improvements allowing maximally general grammar rules specified
once  producing larger lexicalized rules  concurs galley mckeown
       find lexicalization yields better compression output  choice
loss function appears less effect  devised three classes loss functions
based hamming distance  edit distance f  score  overall  simple token based
hamming loss achieved best results  conjecture due simplicity
evaluated precisely many loss functions isnt affected
poor parser output  feature ablation study revealed ngram features beneficial 
mirroring similar finding machine translation literature  chiang         finally 
found trees created generation algorithm accurate compared
output parser applied string output  augurs well use cascaded nlp
pipeline  systems use compression output input processing 
potentially make better use system output 
future extensions many varied  obvious extension concerns porting
framework rewriting applications document summarization  daume iii  
marcu        machine translation  chiang         initial work  cohn   lapata       
shows tree to tree transduction model presented easily adapted
sentence abstraction task compression takes place using rewrite operations
restricted word deletion  examples include substitution  reordering  insertion 
future directions involve detailed feature engineering  including source conditioned features ngram features besides language model  research needed
establish suitable loss functions compression rewriting tasks  particular
interesting experiment loss functions incorporate wider range
linguistic features beyond parts speech  examples include losses based parse trees
semantic similarity  finally  experiments presented work use grammar
acquired training corpus  however  nothing inherent formalization
restricts us particular grammar  therefore plan investigate potential method unsupervised semi supervised grammar induction techniques
rewriting tasks including paraphrase generation machine translation 
   

fisentence compression tree transduction

acknowledgments
grateful philip blunsom insightful comments suggestions
anonymous referees whose feedback helped substantially improve present paper 
special thanks james clarke sharing implementations clarke lapatas
       mcdonalds        models us  acknowledge support epsrc
 grants gr t         gr t           work made use resources
provided edinburgh compute data facility  ecdf   ecdf partially
supported edikt initiative  preliminary version work published
proceedings emnlp conll      

references
aho  a  v     ullman  j  d          syntax directed translations pushdown assembler  journal computer system sciences          
alshawi  h   bangalore  s     douglas  s          learning dependency translation models
collections finite state head transducers  computational linguistics            
   
berger  a  l   pietra  s  a  d     pietra  v  j  d          maximum entropy approach
natural language processing  computational linguistics               
bikel  d          design multi lingual  parallel processing statistical parsing engine 
proceedings  nd international conference human language technology
research  pp        san diego  ca 
briscoe  e  j     carroll  j          robust accurate statistical annotation general text 
proceedings third international conference language resources evaluation  pp            las palmas  gran canaria 
carroll  j   minnen  g   pearce  d   canning  y   devlin  s     tait  j          simplifying
text language impaired readers  proceedings  th conference european chapter association computational linguistics  pp          bergen 
norway 
chandrasekar  r     srinivas  c  d  b          motivations methods text simplification  proceedings   th international conference computational
linguistics  pp            copenhagen  danemark 
chiang  d          hierarchical phrase based translation  computational linguistics         
       
clarke  j     lapata  m          models sentence compression  comparison across
domains  training requirements evaluation measures  proceedings   st
international conference computational linguistics   th annual meeting
association computational linguistics  pp          sydney  australia 
clarke  j     lapata  m          global inference sentence compression  integer linear
programming approach  journal artificial intelligence research             
   

ficohn   lapata

cohn  t     lapata  m          sentence compression beyond word deletion  proceedings   nd international conference computational linguistics  pp         
manchester  uk 
collins  m          discriminative training methods hidden markov models  theory
experiments perceptron algorithms  proceedings      conference
empirical methods natural language processing  pp      morristown  nj 
collins  m  j          head driven statistical models natural language parsing  ph d 
thesis  university pennsylvania  philadelphia  pa 
crammer  k     singer  y          ultraconservative online algorithms multiclass problems  machine learning            
daume iii  h     marcu  d          noisy channel model document compression 
proceedings   th annual meeting thev association computational
linguistics  pp          philadelphia  pa 
eisner  j          learning non isomorphic tree mappings machine translation 
companion volume proceedings   st annual meeting association
computational linguistics  pp          sapporo  japan 
galley  m   hopkins  m   knight  k     marcu  d          whats translation rule  
proceedings      human language technology conference north
american chapter association computational linguistics  pp         
boston  ma 
galley  m     mckeown  k          lexicalized markov grammars sentence compression 
proceedings human language technologies       conference north
american chapter association computational linguistics  pp         
rochester  ny 
grael  j     knight  k          training tree transducers  proceedings      human
language technology conference north american chapter association
computational linguistics  pp          boston  ma 
hermjakob  u   echihabi  a     marcu  d          natural language based reformulation
resource wide exploitation question answering  proceedings   th text
retrieval conference  gaithersburg  md 
hori  c     furui  s          speech summarization  approach word extraction
method evaluation  ieice transactions information systems  e  d          
jing  h          sentence reduction automatic text summarization  proceedings
 th applied natural language processing conference  pp          seattle  wa 
joachims  t          support vector method multivariate performance measures 
proceedings   nd international conference machine learning  pp         
bonn  germany 
kaji  n   okamoto  m     kurohashi  s          paraphrasing predicates written
language spoken language using web  proceedings      human language technology conference north american chapter association
computational linguistics  pp          boston  ma 
   

fisentence compression tree transduction

knight  k          decoding complexity word replacement translation models  computational linguistics                 
knight  k     marcu  d          summarization beyond sentence extraction  probabilistic
approach sentence compression  artificial intelligence                 
lin  d     pantel  p          discovery inference rules question answering  natural
language engineering                
mcdonald  r          discriminative sentence compression soft syntactic constraints 
proceedings   th conference european chapter association
computational linguistics  pp          trento  italy 
nguyen  m  l   horiguchi  s   shimazu  a     ho  b  t          example based sentence
reduction using hidden markov model  acm transactions asian language
information processing                
och  f  j     ney  h          alignment template approach statistical machine
translation  computational linguistics                 
papineni  k   roukos  s   ward  t     zhu  w  j          bleu  method automatic
evaluation machine translation  proceedings   th annual meeting thev
association computational linguistics  pp          philadelphia  pa 
petrov  s   barrett  l   thibaux  r     klein  d          learning accurate  compact 
interpretable tree annotation  proceedings   st international conference
computational linguistics   th annual meeting association computational linguistics  pp          sydney  australia 
riezler  s   king  t  h   crouch  r     zaenen  a          statistical sentence condensation
using ambiguity packing stochastic disambiguation methods lexical functional
grammar  proceedings      human language technology conference
north american chapter association computational linguistics  pp 
        edmonton  canada 
shieber  s     schabes  y          synchronous tree adjoining grammars  proceedings   th international conference computational linguistics  pp         
helsinki  finland 
stolcke  a          srilm extensible language modeling toolkit  proceedings
international conference spoken language processing  denver  co 
tsochantaridis  i   joachims  t   hofmann  t     altun  y          large margin methods
structured interdependent output variables  journal machine learning
research              
turner  j     charniak  e          supervised unsupervised learning sentence
compression  proceedings   rd annual meeting association computational linguistics  pp          ann arbor  mi 
vandeghinste  v     pan  y          sentence compression automated subtitling 
hybrid approach  text summarization branches out  proceedings acl   
workshop  pp        barcelona  spain 
   

ficohn   lapata

wu  d          stochastic inversion transduction grammars bilingual parsing parallel
corpora  computational linguistics                 
yamada  k     knight  k          syntax based statistical translation model  proceedings   th annual meeting association computational linguistics 
pp          toulouse  france 

   


