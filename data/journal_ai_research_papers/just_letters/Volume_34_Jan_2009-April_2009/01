journal artificial intelligence research               

submitted        published      

heuristic search approach planning
continuous resources stochastic domains
nicolas meuleau

nicolas f meuleau nasa gov

nasa ames research center
mail stop      
moffet field  ca             usa

emmanuel benazera

ebenazer laas fr

laas cnrs  universite de toulouse
   av  du colonel roche
      toulouse cedex    france

ronen i  brafman

brafman cs bgu ac il

department computer science
ben gurion university
beer sheva        israel

eric a  hansen

hansen cse msstate edu

department computer science engineering
mississippi state university
mississippi state  ms        usa

mausam

mausam cs washington edu

department computer science engineering
university washington
seattle  wa            usa

abstract
consider problem optimal planning stochastic domains resource constraints 
resources continuous choice action step depends resource availability  introduce hao  algorithm  generalization ao  algorithm performs
search hybrid state space modeled using discrete continuous state variables  continuous variables represent monotonic resources  heuristic search
algorithms  hao  leverages knowledge start state admissible heuristic focus
computational effort parts state space could reached start state
following optimal policy  show approach especially effective resource
constraints limit much state space reachable  experimental results demonstrate
effectiveness domain motivates research  automated planning planetary
exploration rovers 

   introduction
many nasa planetary exploration missions rely rovers mobile robots carry suite
scientific instruments use characterizing planetary surfaces transmitting information back
earth  difficulties communicating devices distant planets  direct human
control rovers tele operation infeasible  rovers must able act autonomously
substantial periods time  example  mars exploration rovers  mer   aka  spirit
opportunity  designed communicate ground twice per martian day 
autonomous control planetary exploration rovers presents many challenges research
automated planning  progress made meeting challenges  example 
planning software developed mars sojourner mer rovers contributed significantly

c
    
ai access foundation  rights reserved 

fimeuleau  benazera  brafman  hansen   mausam

success missions  bresina  jonsson  morris    rajan         many important
challenges must still addressed achieve ambitious goals future missions  bresina 
dearden  meuleau  ramakrishnan  smith    washington        
among challenges problem plan execution uncertain environments  planetary
surfaces mars  uncertainty terrain  meteorological conditions  state
rover  position  battery charge  solar panels  component wear  etc   turn  leads
uncertainty outcome rovers actions  much uncertainty resource
consumption  example  factors slope terrain affect speed movement rate
power consumption  making difficult predict certainty long take rover
travel two points  much power consume so  limits
critical resources time battery power  rover plans currently conservative
based worst case estimates time resource usage  addition  instructions sent
planetary rovers form sequential plan attaining single goal  e g   photographing
interesting rock   action unintended outcome causes plan fail  rover
stops waits instructions  makes attempt recover achieve alternative
goal  result under utilized resources missed science opportunities 
past decade  great deal research generate conditional
plans domains uncertain action outcomes  much work formalized framework
markov decision processes  puterman        boutilier  dean    hanks         however 
bresina et al         point out  important aspects rover planning problem adequately
handled traditional planning algorithms  including algorithms markov decision processes 
particular  traditional planners assume discrete state space small discrete number
action outcomes  automated planning planetary exploration rovers  critical resources
time battery power continuous  uncertainty domain results
effect actions variables  requires conditional planner branch
discrete action outcomes  availability continuous resources  planner
must able reason continuous well discrete state variables 
closely related challenges uncertain plan execution continuous resources
challenge over subscription planning  rovers future missions much improved
capabilities  whereas current mer rovers require average three days visit single rock 
progress areas automatic instrument placement allow rovers visit multiple rocks
perform large number scientific observations single communication cycle  pedersen 
smith  deans  sargent  kunz  lees    rajagopalan         moreover  communication cycles
lengthen substantially distant missions moons jupiter saturn  requiring longer
periods autonomous behavior  result  space scientists future missions expected
specify large number science goals once  often present known oversubscription planning problem  refers problem infeasible achieve goals 
objective achieve best subset goals within resource constraints  smith        
case rover  multiple locations rover could reach  many experiments
rover could conduct  combinations infeasible due resource constraints 
planner must select feasible subset maximizes expected science return  action
outcomes  including resource consumption  stochastic  plan maximizes expected science
return conditional plan prescribes different courses action based results
previous actions  including resource availability 
paper  present implemented planning algorithm handles problems
together  uncertain action outcomes  limited continuous resources  over subscription planning 
formalize rover planning problem hybrid state markov decision process  is  markov
decision process  mdp  discrete continuous state variables  use continuous
variables represent resources  planning algorithm introduce heuristic search algorithm
called hao   hybrid state ao   generalization classic ao  heuristic search algorithm  nilsson        pearl         whereas ao  searches discrete state spaces  hao  solves

  

fihao 

planning problems hybrid domains discrete continuous state variables  handle
hybrid domains  hao  builds earlier work dynamic programming algorithms continuous
hybrid state mdps  particular  work feng et al         
generalizing and or graph search hybrid state spaces poses complex challenge 
consider special case problem  particular  continuous variables used represent
monotonic resources  search best conditional plan allows branching
values discrete variables  availability resources  violate
resource constraint 
well known heuristic search efficient dynamic programming
uses reachability analysis guided heuristic focus computation relevant parts state
space  show problems resource constraints  including over subscription planning
problems  heuristic search especially effective resource constraints significantly limit
reachability  unlike dynamic programming  systematic forward search algorithm ao  keeps
track trajectory start state reachable state  thus check whether
trajectory feasible violates resource constraint  pruning infeasible trajectories  heuristic
search algorithm dramatically reduce number states must considered find
optimal policy  particularly important domain discrete state space huge
 exponential number goals   yet portion reachable initial state relatively
small  due resource constraints 

   problem formulation background
start formal definition planning problem tackling  special case
hybrid state markov decision process  first define model  discuss
include resource constraints formalize over subscription planning model  finally
review class dynamic programming algorithms solving hybrid state mdps  since
algorithmic techniques incorporated heuristic search algorithm develop
section   
    hybrid state markov decision process
hybrid state markov decision process  hybrid state mdp  factored markov decision process
discrete continuous state variables  define tuple  n  x  a  p  r  
n discrete state variable  x    x    x         xd   set continuous state variables  set
actions  p stochastic state transition model  r reward function  describe
elements detail below  hybrid state mdp sometimes referred simply hybrid
mdp  term hybrid refer dynamics model  discrete  another
term hybrid state mdp  originates markov chain literature  general state
mdp 
although hybrid state mdp multiple discrete variables  plays role algorithms described paper  so  notational convenience  model discrete component
state space single variable n   focus continuous component  assume
n
domain continuous variable xi x closed interval real line  x   xi
hypercube continuous variables defined  state set hybrid state
mdp set possible assignments values state variables  particular  hybrid
state pair  n  x  n n value discrete variable  x    xi   vector
values continuous variables 
state transitions occur result actions  process evolves according markovian
state transition probabilities pr s    s  a      n  x  denotes state action
s     n    x    denotes state action a  called arrival state  probabilities
decomposed into 

  

fimeuleau  benazera  brafman  hansen   mausam

discrete marginals pr n   n  x  a    n  x  a  

pr n   n  x  a      
r
continuous conditionals pr x   n  x  a  n      n  x  a  n     x  x pr x   n  x  a  n   dx   
  
p

n  n

assume reward associated transition function arrival state only  let
rn  x  denote reward associated transition state  n  x   complex dependencies
possible  sufficient goal based domain models consider paper 
    resource constraints over subscription planning
model rover planning problem  consider special type mdp objective
optimize expected cumulative reward subject resource constraints  make following
assumptions 
initial allocation one non replenishable resources 
action minimum positive consumption least one resource 
resources exhausted  action taken 
one way model mdp resource constraints formulate constrained mdp 
model widely studied operations research community  altman        
model  action incurs transition dependent resource cost  cai  s  s     resource
i  given initial allocation resources initial state  linear programming used find
best feasible policy  may randomized policy  although constrained mdp models
resource consumption  include resources state space  result  policy cannot
conditioned upon resource availability  problem resource consumption either
deterministic unobservable  good fit rover domain  resource
consumption stochastic observable  rover take different actions depending
current resource availability 
adopt different approach modeling resource constraints resources included
state description  although increases size state space  allows decisions
made based resource availability  allows stochastic model resource consumption  since
resources rover domain continuous  use continuous variables hybrid state mdp
represent resources  note duration actions one biggest sources uncertainty
rover problems  model time one continuous resources  resource constraints
represented form executability constraints actions   x  denotes set
actions executable state  n  x   action cannot executed state satisfy
minimum resource requirements 
discussed incorporate resource consumption resource constraints hybridstate mdp  next discuss formalize over subscription planning  rover planning
problem  scientists provide planner set goals would rover achieve 
goal corresponds scientific task taking picture rock performing
analysis soil sample  scientists specify utility reward goal  usually
subset goals feasible resource constraints  problem find feasible
plan maximizes expected utility  over subscription planning planetary exploration rovers
considered smith        van den briel et al         deterministic domains 
consider over subscription planning stochastic domains  especially domains stochastic
resource consumption  requires construction conditional plans selection goals
achieve change depending resource availability 
over subscription planning  utility associated goal achieved once 
additional utility achieved repeating task  therefore  discrete state must include set
boolean variables keep track set goals achieved far rover  one boolean
  

fihao 

variable goal  keeping track already achieved goals ensures markovian reward structure 
since achievement goal rewarded achieved past  however 
significantly increases size discrete state space  maintaining history information ensure
markovian reward structure simple example planning non markovian rewards  thiebaux 
gretton  slaney  price    kabanza        
    optimality equation
rover planning problem consider special case finite horizon hybrid state mdp
termination occurs indefinite number steps  bellman optimality equation
problem takes following form 
vn  x 

 

vn  x 

 

   n  x  terminal state  otherwise 
 

z
x
max
pr n    n  x  a 
pr x    n  x  a  n     rn   x      vn   x     dx   

aan  x 

n  n

   

x 

define terminal state state actions eligible execute  is   x     
use terminal states model various conditions plan termination  includes situation
goals achieved  situation resources exhausted 
situation action results error condition requires executing safe sequence
rover terminating plan execution  addition terminal states  assume explicit
initial state denoted  n    x    
assuming resources limited non replenishable  every action consumes
resource  and amount consumed greater equal positive quantity c   plan
execution terminate finite number steps  maximum number steps bounded
initial resource allocation divided c  minimal resource consumption per step  actual
number steps usually much less indefinite  resource consumption stochastic
choice action influences resource consumption  number steps takes
plan terminate bounded indefinite  call bounded horizon mdp contrast
finite horizon mdp  however  note bounded horizon mdp converted
finite horizon mdp specifying horizon equal maximum number plan steps 
introducing no op action taken terminal state 
note usually difference number plan steps time plan takes
execute  since model time one continuous resources  time takes execute
plan step state action dependent  stochastic 
given hybrid state mdp set terminal states initial state  n    x     objective
find policy     n x  a  maximizes expected cumulative reward  specifically 
optimal policy value function satisfies optimality equation given equation     
rover domain  cumulative reward equal sum rewards goals achieved
reaching terminal state direct incentive save resources  optimal solution saves
resources allows achieving goals  however  framework general enough
allow reasoning cost availability resources  example  incentive
conserving resources could modeled specifying reward proportional amount
resources left unused upon entering terminal state  note framework allows reasoning
cost availability resources without needing formulate problem
multi objective optimization  stay standard decision theoretic framework 
    dynamic programming continuous state hybrid state mdps
planning problem consider finite horizon hybrid state mdp  solved
algorithm solving finite horizon hybrid state mdps  algorithms solving hybridstate  and continuous state  mdps rely form approximation  widely used approach
  

fimeuleau  benazera  brafman  hansen   mausam

figure    value function initial state simple rover problem  optimal expected return
function two continuous variables  time energy remaining  

discretize continuous state space finite number grid points solve resulting
finite state mdp using dynamic programming interpolation  rust        munos   moore 
       another approach parametric function approximation  function associated
dynamic programming problem value function policy function approximated
smooth function k unknown parameters  general  parametric function approximation
faster grid based approximation  drawback may fail converge  may
converge incorrect solution  parametric function approximation used algorithms
solving continuous state mdps besides dynamic programming  reinforcement learning algorithms
use artificial neural networks function approximators  bertsekas   tsitsiklis         approach
solving mdps called approximate linear programming extended allow continuous
well discrete state variables  kveton  hauskrecht    guestrin        
review another approach solving hybrid state  or continuous state  mdps assumes
problem special structure exploited dynamic
programming algorithm 
r
structure assumed approach ensures convolution x  pr x    n  x  a  n    rn   x    
vn   x    dx  equation     computed exactly finite time  value function computed
dynamic programming piecewise constant piecewise linear  initial idea approach
due work boyan littman         describe class mdps called time dependent
mdps  transitions take place along single  irreversible continuous dimension 
describe dynamic programming algorithm computing exact piecewise linear value function
transition probabilities discrete rewards piecewise linear  feng et al        
extend approach continuous state spaces one dimension  consider mdps
discrete transition probabilities two types reward models  piecewise constant piecewise
linear  li littman        extend approach allow transition probabilities
piecewise constant  instead discrete  although extension requires approximation
dynamic programming algorithm 
problem structure exploited algorithms characteristic mars rover domain
over subscription planning problems  figure   shows optimal value functions
initial state typical mars rover problem function two continuous variables 
time energy remaining  bresina et al          value functions feature set humps
plateaus  representing region state space similar goals pursued
optimal policy  sharpness hump plateau reflects uncertainty achieving
goal s   constraints impose minimal resource levels attempting actions introduce

  

fihao 

sharp cuts regions  plateau regions expected reward nearly constant represent
regions state space optimal policy same  probability distribution
future histories induced optimal policy nearly constant 
structure value function exploited partitioning continuous state
space finite number hyper rectangular regions   a region hyper rectangle
cartesian product intervals dimension   hyper rectangle  value function
either constant  for piecewise constant function  linear  for piecewise linear function  
resolution hyper rectangular partitioning adjusted fit value function  large hyperrectangles used represent large plateaus  small hyper rectangles used represent regions
state space finer discretization value function useful  edges
plateaus curved hump time energy available  natural choice
data structures rectangular partitioning continuous space kd trees  friedman  bentley 
  finkel         although choices possible  figures      section     show value
functions initial state simple rover planning problem  created piecewise constant
partitioning continuous state space 
continuous state domains transition reward functions similarly partitioned
hyper rectangles  reward function action piecewise constant  or piecewiselinear  representation value function  transition function partitions state space
regions set outcomes action probability distribution set
outcomes identical  following boyan littman         relative absolute transitions
supported  relative outcome viewed shifting region constant   is 
two states x region  transition probabilitiesp r x   x  a  p r y    y  a 
defined term probability      x  x     y   y   absolute outcome
maps states region single state  is  two states x region 
p r x   x  a    p r x   y  a   view relative outcome pair    p   p probability
outcome  view absolute outcome pair  x    p   assumes
finite number non zero probabilities  i e   probability distribution discretized 
means state action  finite set states reached non zero probability 
representation guarantees dynamic programming update piecewise constant value
function results another piecewise constant value function  feng et al         show
transition functions finite horizon  exists partition continuous space
hyper rectangles optimal value function piecewise constant linear 
restriction discrete transition functions strong one  often means transition
function must approximated  example  rover power consumption normally distributed 
thus must discretized   since amount power available must non negative 
implementation truncates negative part normal distribution renormalizes   continuous transition function approximated appropriately fine discretization  feng et
al         argue provides attractive alternative function approximation approaches
approximates model solves approximate model exactly  rather finding
approximate value function original model   for reason  sometimes refer
finding optimal policies value functions  even model approximated  
avoid discretizing transition function  li littman        describe algorithm allows
piecewise constant transition functions  exchange approximation dynamic programming algorithm  marecki et al        describe different approach class problems
probability distributions resource consumptions represented phase type distributions dynamic programming algorithm exploits representation  although use
work feng et al         implementation  heuristic search algorithm develop
next section could use approach representing computing value
functions policies hybrid state mdp 

  

fimeuleau  benazera  brafman  hansen   mausam

   heuristic search hybrid state space
section  present primary contribution paper  approach solving special
class hybrid state mdps using novel generalization heuristic search algorithm ao  
particular  describe generalization algorithm solving hybrid state mdps
continuous variables represent monotonic constrained resources acyclic plan found
search algorithm allows branching availability resources 
motivation using heuristic search potentially huge size state space 
makes dynamic programming infeasible  one reason size existence continuous
variables  even consider discrete component state space  size
state space exponential number discrete variables  well known  ao 
effective solving planning problems large state space considers states
reachable initial state  uses informative heuristic function focus
states reachable course executing good plan  result  ao  often find
optimal plan exploring small fraction entire state space 
begin section review standard ao  algorithm  consider
generalize ao  search hybrid state space discuss properties generalized
algorithm  well efficient implementations 
    ao 
recall ao  algorithm and or graph search problems  nilsson        pearl        
graphs arise problems choices  the components   choice
multiple consequences  the component   case planning uncertainty 
hansen zilberstein        show and or graph search techniques used solving
mdps 
following nilsson        hansen zilberstein         define and or graph
hypergraph  instead arcs connect pairs nodes ordinary graph  hypergraph
hyperarcs  k connectors  connect node set k successor nodes  mdp
represented hypergraph  node corresponds state  root node corresponds start
state  leaf nodes correspond terminal states  thus often use word state refer
corresponding node hypergraph representing mdp  k connector corresponds
action transforms state one k possible successor states  probability attached
successor probabilities sum one  paper  assume and or graph
acyclic  consistent assumption underlying mdp bounded horizon 
and or graph search  solution takes form acyclic subgraph called solution
graph  defined follows 
start node belongs solution graph 
every non terminal node solution graph  exactly one outgoing k connector  corresponding action  part solution graph successor nodes belongs
solution graph 
every directed path solution graph terminates terminal node 
solution graph maximizes expected cumulative reward found solving following
system equations 

  terminal
state  otherwise 
p

v  s   
   
 
 
 
maxaa s 
p
 
r s  s  a   r s     v  s     
v  s  denotes expected value optimal solution state s  v called
optimal evaluation function  or value function mdp terminology   note identical
  

fihao 

optimality equation hybrid state mdps defined equation      latter restricted
discrete state space  keeping convention literature mdps  treat
value maximization problem even though ao  usually formalized solving cost minimization
problem 
state space search problems formalized and or graphs  optimal solution
graph found using heuristic search algorithm ao   nilsson        pearl        
heuristic search algorithms  advantage ao  dynamic programming find
optimal solution particular starting state without evaluating problem states  therefore 
graph usually supplied explicitly search algorithm  implicit graph  g  specified
implicitly start node start state successor function generates successors
states state action pair  search algorithm constructs explicit graph  g    initially
consists start state  tip leaf state explicit graph said terminal
goal state  or state action taken   otherwise  said
nonterminal  nonterminal tip state expanded adding explicit graph outgoing
k connectors  one action  successor states already explicit graph 
ao  solves state space search problem gradually building solution graph  beginning
start state  partial solution graph defined similarly solution graph  difference
tip states partial solution graph may nonterminal states implicit and or graph 
partial solution graph defined follows 
start state belongs partial solution graph 
every non tip state partial solution graph  exactly one outgoing k connector  corresponding action  part partial solution graph successor states
belongs partial solution graph 
every directed path partial solution graph terminates tip state explicit graph 
value partial solution graph defined similarly value solution graph 
difference tip state partial solution graph nonterminal  value
propagated backwards  instead  assume admissible heuristic estimate
h s  maximal value solution graph state s  heuristic evaluation function h said
admissible h s  v  s  every state s  recursively calculate admissible heuristic
estimate v  s  optimal value state explicit graph follows 

  terminal state 
v  s   
nonterminal tip state 

h s  isp
 
 
 
maxaa s 
s  p r s  s  a   r s     v  s    otherwise 

   

best partial solution graph determined time propagating heuristic estimates
tip states explicit graph start state  mark action maximizes
value state  best partial solution graph determined starting root
graph selecting best  i e   marked  action reachable state 
table   outlines ao  algorithm finding optimal solution graph acyclic and or
graph  interleaves forward expansion best partial solution value update step
updates estimated state values best partial solution  simplest version ao  
values expanded state ancestor states explicit graph updated 
fact  ancestor states need re evaluated expanded state
reached taking marked actions  i e   choosing best action state   thus 
parenthetical remark step   b i table   indicates parent s  state added
z unless estimated value state changed state reached state
s  choosing best action state s    ao  terminates policy expansion step

  

fimeuleau  benazera  brafman  hansen   mausam

   explicit graph g  initially consists start state s   
   best solution graph nonterminal tip state 
 a  expand best partial solution  expand nonterminal tip state best partial
solution graph add new successor states g    new state s  added
g  expanding s  s  terminal state v  s          else v  s       h s    
 b  update state values mark best actions 
i  create set z contains expanded state ancestors explicit
graph along marked action arcs   i e   include ancestor states
expanded state reached following current best solution  
ii  repeat following steps z empty 
a  remove z state descendant g  occurs z 
p
b  set v  s     maxaa s  s  p r s   s  a   r s      v  s     mark best action
s   when determining best action resolve ties arbitrarily  give preference currently marked action  
 c  identify best solution graph nonterminal states fringe
   return optimal solution graph 
table    ao  algorithm 
find nonterminal states fringe best solution graph  point  best solution
graph optimal solution 
following literature and or graph search  far referred solution found
ao  solution graph  following  ao  used solve mdp  sometimes
follow literature mdps referring solution policy  sometimes refer
policy graph  indicate policy represented form graph 
    hybrid state ao 
consider generalize ao  solve bounded horizon hybrid state mdp  challenge
face applying ao  problem challenge performing state space search hybrid
state space 
solution adopt search aggregate state space represented and or
graph node distinct value discrete component state 
words  node and or graph represents region continuous state space
discrete value same  given partition continuous state space  use and or
graph search techniques solve mdp parts state space reachable
start state best policy 
however  and or graph search techniques must modified important ways allow search
hybrid state space represented way  particular  longer correspondence nodes and or graph individual states  node corresponds
continuous region state space  different actions may optimal different hybrid states associated search node  case rover planning  example 
best action likely depend much energy time remaining  energy time
continuous state variables 
address problem still find optimal solution  attach search node set
functions  of continuous variables  make possible associate different values  heuristics 
actions different hybrid states map search node  before  explicit

  

fihao 

search graph consists nodes edges and or graph generated far 
describes states considered far search algorithm  difference
use complex state representation set continuous functions allows
representation reasoning continuous part state space associated search
node 
begin describing complex node data structure  describe hao 
algorithm 
      data structures
node n explicit and or graph g  consists following 
value discrete state variable 
pointers parents children explicit graph policy graph 
openn            open list  x x  openn  x  indicates whether  n  x 
frontier explicit graph  i e   generated yet expanded 
closedn            closed list  x x  closedn  x  indicates whether  n  x 
interior explicit graph  i e   already expanded 
note that   n  x   openn  x  closedn  x       a state cannot open
closed   parts continuous state space associated node
neither open closed  explicit graph contains trajectory start state
particular hybrid state  hybrid state considered generated  even search
node corresponds generated  states neither open closed 
addition  non terminal states open closed  note refer open
closed nodes  instead  refer hybrid states associated nodes open
closed 
hn     heuristic function  x x  hn  x  heuristic estimate optimal
expected cumulative reward state  n  x  
vn     value function  open state  n  x   vn  x    hn  x   closed state
 n  x   vn  x  obtained backing values successor states  equation     
n    a  policy  note defined closed states only 
reachablen            x x  reachablen  x  indicates whether  n  x  reachable
executing current best policy beginning start state  n    x    
assume various continuous functions  represent information hybrid states associated search node  partition state space associated node
discrete number regions  associate distinct value action region  given
partitioning  hao  algorithm expands evaluates regions hybrid state space 
instead individual hybrid states  finiteness partition important order ensure
search frontier extended finite number expansions  ensure hao 
terminate finite number steps  implementation hao   described section    use piecewise constant partitioning continuous state space proposed feng et
al          however  method discrete partitioning could used  provided condition
holds  example  li littman        describe alternative method partitioning 
note two forms state space partitioning used algorithm  first  hybrid state
space partitioned finite number regions  one discrete state 

  

fimeuleau  benazera  brafman  hansen   mausam

regions corresponds node and or graph  second  continuous state space associated particular node partitioned smaller regions based piecewise constant
representation continuous function  one used feng et al         
addition complex representation nodes and or graph  algorithm
requires complex definition best  partial  solution  standard ao   oneto one correspondence nodes individual states means solution policy
represented entirely graph  called  partial  solution graph  single action
associated node  hao  algorithm  continuum states associated
node  different actions may optimal different regions state space associated
particular node  hao  algorithm   partial  solution graph sub graph explicit
graph defined follows 
start node belongs solution graph 
every non tip node solution graph  one outgoing k connectors part
solution graph  one action optimal hybrid state associated
node  successor nodes belongs solution graph 
every directed path solution graph terminates tip node explicit graph 
key difference definition may one optimal action associated
node  since different actions may optimal different hybrid states associated
node  policy represented solution graph  continuous functions n    
reachablen      particular   partial  policy specifies action reachable region
continuous state space  best  partial  policy one satisfies following optimality
equation 
vn  x 

 

vn  x 

  hn  x   n  x  nonterminal open state 
 

z
x
 
max
pr n    n  x  a 
pr x    n  x  a  n     rn   x      vn   x     dx   

vn  x 

   n  x  terminal state 

aan  x 

n  n

   

x 

note optimality equation satisfied regions state space reachable
start state   n    x    following optimal policy 
      algorithm
table   gives high level summary hao  algorithm  outline  ao 
algorithm  consists iteration three steps  solution  or policy  expansion  use
dynamic programming update current value function policy  analysis reachability
identify frontier solution eligible expansion  detail  modified several
important ways allow search hybrid state space  following  discuss modifications
three steps 
policy expansion nodes current solution graph identified one open
regions associated nodes selected expansion  is  one regions
hybrid state space intersection open reachable chosen expansion  actions
applicable states open regions simulated  results actions added
explicit graph  cases  means adding new node and or graph 
cases  simply involves marking one regions continuous state space associated
existing node open  specifically  action leads new node  node added
explicit graph  states corresponding node reachable expanded
region s  action consideration marked open  action leads
  

fihao 

   explicit graph g  initially consists start node corresponding start state  n    x    
marked open reachable 
   reachablen  x  openn  x  non empty  n  x  
 a  expand best partial solution  expand one region s  open states frontier
explicit state space reachable following best partial policy  add new
successor states g    cases  requires adding new node and or
graph  cases  simply involves marking one regions continuous
state space associated existing node open  states expanded region s 
marked closed 
 b  update state values mark best actions 
i  create set z contains node s  associated expanded regions
states ancestor nodes explicit graph along marked action arcs 
ii  decompose part explicit and or graph consists nodes z
strongly connected components 
iii  repeat following steps z empty 
a  remove z set nodes     belong connected
component      descendant nodes occurs z 
b  every node n connected component states  n  x 
expanded region node n  set
vn  x    
 
max
aan  x 

x

pr n    n  x  a 

z


pr x    n  x  a  n     rn   x      vn   x     dx   

x 

n  n

mark best action   when determining best action resolve ties arbitrarily  give preference currently marked action   repeat
longer change value nodes 
 c  identify best solution graph nonterminal states frontier  step
updates reachablen  x  
   return optimal policy 
table    hao  algorithm 
existing node  region s  markov states node reachable expanded
region s  marked closed  marked open  expanded regions state space marked
closed  thus  different regions associated node opened expanded
different times  process illustrated figure    figure  nodes corresponding
distinct value discrete state represented rectangles  circular connectors represent
actions  node  see many distinct continuous regions exist  region
see whether closed  c  open  o   whether reachable initial state  r 
executing current best policy  opt   instance  figure   a   node at start 
single region marked closed reachable  node lost two regions  smallest  open
reachable  largest  closed unreachable 
dynamic programming standard ao   value newly expanded node n must
updated computing bellman backup based value functions children n

  

fimeuleau  benazera  brafman  hansen   mausam

at start 

at loc  


at start 
c

c

r

r

navigate
 start  loc  

at loc  

opt

c

r

navigate
 start  loc  

opt

r

navigate
 loc   loc  

lost


c

lost


r



c

c

r

at loc  

panoramic
camera



 a  expansion

panoramic
camera

 b  expansion

figure    expanding region state space   a  expansion  nodes at start  
at loc   lost previously created  unique region at loc  
next region expanded   b  expansion  action navigate loc   loc  
applied expanded region added graph  action lead
either preexisting node lost  new node at loc    expanded region  in
at loc     well continuous regions reachable  in lost at loc    
highlighted dotted framed  following expansion  expanded region closed 
discrete state at loc   added graph reachable regions
open  additionally  new open regions added node lost 

explicit graph  expanded region state space associated node n 
action evaluated  best action selected  corresponding continuous value function
associated region  continuous state value function computed evaluating
continuous integral equation      use method computing integral 
implementation  use dynamic programming algorithm feng et al          reviewed
section      show continuous integral x  computed exactly  long
transition reward functions satisfy certain conditions  note that  hybrid state
dynamic programming techniques feng et al          dynamic programming backups may
increase number pieces value function attached updated regions  figure   a   
expanded regions continuous state space associated node n reevaluated  new values must propagated backward explicit graph  backward
propagation stops nodes value function modified  root node 
standard ao  algorithm  summarized figure    assumes and or graph
searches acyclic  extensions ao  searching and or graphs contain
cycles  one line research concerned find acyclic solutions and or graphs
contain cycles  jimenez   torras         another generalization ao   called lao   allows
solutions contain cycles loops order specify policies infinite horizon mdps  hansen
  zilberstein        

  

fihao 

at start 

at loc  
c c c

at start 
c

c

r

r

navigate
 start  loc  

at loc  

opt

c c c

r r r

navigate
 loc   loc  

lost




c

opt

c

r

at loc  


opt

r r r

navigate
 loc   loc  

opt

navigate
 start  loc  

at loc  

panoramic
camera


r

 a  dynamic programming

lost




c

r

r

r

c

panoramic
camera

 b  reachability analysis

figure    dynamic programming reachability analysis  figure   continued    a  dynamic programming  optimal policy reevaluated navigate loc   loc   appears
optimal continuous states at loc    node at loc   represented finer
partition continuous state space illustrate fact backup increased
number pieces value function associated expanded region   b  reachability analysis  newly created region at loc   becomes reachable  well
regions lost reached navigate loc   loc   

given assumption every action positive resource consumption 
loops state space problem resources available decrease step 
surprisingly  loops and or graph  possible and or
graph represents projection state space onto smaller space consists
discrete component state  example  possible rover return
site visited before  rover actually state  since fewer resources
available  and or graph represents projection state space include
continuous aspects state  resources  means rover visit state
projects node and or graph state visited earlier  shown figure   
result  loops and or graph  even loops part and or
graph corresponds solution  sense  phantom loops
appear projected state space  real state space 
nevertheless must modify dynamic programming  dp  algorithm deal loops 
loops real state space  know exact value function
updated finite number backups performed correct order  one backup performed
state visited along path start state expanded node s  
multiple states map and or graph node  continuous region
state space associated particular node may need evaluated once  identify
and or graph nodes need evaluated once  use following two step
algorithm 

  

fimeuleau  benazera  brafman  hansen   mausam

at start 

at location  
energy     

at location  
energy     

at location  

at start 
energy      
at location  
energy     

at location  
energy     

at location  

figure    phantom loops hao   solid boxes represent markov states  dashed boxes represent
search nodes  is  projection markov states discrete components  arrows
represent possible state transition  bold arrows show instance phantom loop
search space 

first  consider part and or graph consists ancestor nodes
expanded node s   set z nodes identified beginning dp step 
decompose part graph strongly connected components  graph strongly
connected components acyclic used prescribe order backups almost
way standard ao  algorithm  particular  nodes particular component
backed nodes descendant components backed up  note
case acyclic graph  every strongly connected component single node  possible
connected component one node loops and or graph 
loops and or graph  primary change dp step algorithm
occurs time perform backups nodes connected component one
node  case  nodes connected component evaluated  then  repeatedly
re evaluated value functions nodes converge  is  change
values nodes  loops real state space  convergence
guaranteed occur finite number steps  typically  occurs small number
steps  advantage decomposing and or graph connected components
identifies loops localizes effect small number nodes  experiments test
domain  nodes graph need evaluated dp step 
small number nodes  and often none  need evaluated once 
note decomposition nodes z connected components method improving
efficiency dynamic programming step  required correctness  alternative repeatedly updating nodes z values converge correct  although
likely result many useless updates already converged nodes 
analysis reachability change value function lead change optimal policy 
and  thus  change states visited best policy  this  turn  affect
open regions state space eligible expanded  final step  hao  identifies
best  partial  policy recomputes reachablen nodes states explicit graph 
follows  see figure   b    node n best  partial  solution graph  consider
parents n  solution graph  actions lead one parents n 
reachablen  x  support pn  x  
x z
pn  x   
reachablen   x    pr n   n    x    a  pr x   n    x    a  n dx   
   
 n   a n

x

  

fihao 

is  reachablen  x     x x   pn  x        equation      n set pairs  n    a 
best action n  reachable resource level 
n     n    a  n   x x  pn   x       n   x    a  pr n   n    x  a        
clear restrict attention state action pairs n   only 
performing reachability analysis  hao  identifies frontier state space
eligible expansion  hao  terminates frontier empty  is  find
hybrid states intersection reachable open 
    convergence error bounds
next consider theoretical properties hao   first  reasonable assumptions 
prove hao  converges optimal policy finite number steps  discuss
use hao  find sub optimal policies error bounds 
proof convergence finite number steps depends  among things 
assumption hybrid state mdp finite branching factor  implementation 
means region state space represented hyper rectangle  set
successor regions action represented finite set hyper rectangles 
assumption assumption number actions finite  follows every
assignment n discrete variables  set
 x  n  x is reachable initial state using fixed sequence actions 
union finite number open closed hyper rectangles  assumption viewed
generalization assumption finite branching factor discrete and or graph upon
finite convergence proof ao  depends 
theorem   heuristic functions hn admissible  optimistic   actions positive resource consumptions  continuous backups action application computable exactly finite
time  branching factor finite  then 
   step hao   vn  x  upper bound optimal expected return  n  x  
 n  x  expanded hao  
   hao  terminates finite number steps 
   termination  vn  x  equal optimal expected return  n  x    n  x  reachable
optimal policy  i e   reachablen  x      
proof      proof induction  every state  n  x  assigned initial heuristic estimate 
vn  x    hn  x  vn  x  admissibility heuristic evaluation function  make
inductive hypothesis point algorithm  vn  x  vn  x  every state  n  x  
backup performed state  n  x  
 

z
x
vn  x   
max
pr n    n  x  a 
pr x    n  x  a  n     rn   x      vn   x     dx 
aan  x 

x 

n  n

 


max
aan  x 

x
n  n

 

z

pr n   n  x  a 

 

 

 

pr x   n  x  a  n    rn   x    
x 

  vn  x   
last equality restates bellman optimality equation 

  

vn   x     dx 



fimeuleau  benazera  brafman  hansen   mausam

    action positive  bounded below  resource consumption  resources
finite non replenishable  complete implicit and or graph must finite 
reason  graph turned finite graph without loops  along directed loop
graph  amount maximal available resources must decrease positive
lower bound amount resources consumed action  node graph may
expanded number times bounded number ancestor   each time new
ancestor discovered  may lead update set reachable regions node  
moreover  finite branching factor implies number regions considered within node
bounded  because finite ways reaching node  contributes finite
number hyper rectangles   thus  overall  number regions considered finite 
processing required region expansion finite  because action application backups
computed finite time   leads desired conclusion 
    search algorithm terminates policy start state  n    x    complete 
is  lead unexpanded states  every state  n  x  reachable
following policy  contradictory suppose vn  x    vn  x  since implies complete
policy better optimal  bellman optimality equation equation      know
vn  x  vn  x  every state complete policy  therefore  vn  x    vn  x  
hao  converges optimal solution  stopping algorithm early allows flexible
trade off solution quality computation time  assume that  state 
done action terminates execution zero reward  in rover problem  would
start safe sequence   evaluate current policy step algorithm
assuming execution ends time reach leaf policy graph  assumption 
error current policy step algorithm bounded  show
using decomposition value function described chakrabarti et al        hansen
zilberstein         note point algorithm  value function decomposed
two parts  gn  x  hn  x  
gn  x 

 

gn  x 

 

   n  x  open state  fringe greedy policy  otherwise 
z
x
 

pr n   n  x   
pr x    n  x    n     rn  x    gn   x     dx   

   

x 

n  n


hn  x 

  hn  x   n  x  open state  fringe greedy policy  otherwise 
z
x
hn  x   
pr n    n  x   
pr x    n  x    n    hn   x   dx   
   
n  n

x 

action maximizes right hand side equation      note vn  x   
gn  x    hn  x   use decomposition value function bound error best policy
found far  follows 
theorem   step hao  algorithm  error current best policy bounded
hn   x    
proof  state  n  x  explicit search space  lower bound optimal value given
gn  x   value achieved current policy done action
executed fringe states  upper bound given vn  x    gn  x    hn  x   established
theorem    follows hn   x    bounds difference optimal value
current admissible value state  n  x   including initial state  n    x    
note error bound initial state hn   x      hn   x    start algorithm 
decreases progress algorithm  hn   x        hao  converges optimal
solution 
  

fihao 

    heuristic function
heuristic function hn focuses search reachable states likely useful 
informative heuristic  scalable search algorithm  implementation
hao  rover planning problem  described detail next section  used
simple admissible heuristic function assigns node sum rewards associated
goals achieved far  note heuristic function depends
discrete component state  continuous variables  is  function hn  x 
constant values x  obvious heuristic admissible  since represents
maximum additional reward could achieved continuing plan execution  although
obvious heuristic simple could useful  experimental results present
section   show is  considered additional  informed heuristic function solved
relaxed  suitably discretized  version planning problem  however  taking account
time required compute heuristic estimate  simpler heuristic performed better 
    expansion policy
hao  works correctly converges optimal solution matter continuous region s 
node s  expanded iteration  step   a   quality solution may
improve quickly using heuristics choose region s  fringe expand
next 
one simple strategy select node expand continuous regions node
open reachable  preliminary implementation  expanded  the open regions of 
node likely reached using current policy  changes value
states greatest effect value earlier nodes  implementing strategy requires
performing additional work involved maintaining probability associated state 
probabilities available  one could focus expanding promising node 
is  node integral hn  x  times probability values x highest 
described mausam  benazera  brafman  meuleau  hansen        
hansen zilberstein        observed that  case lao   algorithm efficient
expand several nodes fringe performing dynamic programming explicit
graph  cost performing update node largely dominates cost
expanding node  expand one node fringe iteration  might
perform dp backups expand several nodes common ancestors proceeding
dp  limit  might want expand nodes fringe algorithm iteration 
indeed  variant lao  proved efficient  hansen   zilberstein        
case lao   updates expensive loops implicit graph  hao  
update region induces call hybrid dynamic programming module open
region node  therefore  technique likely produce benefit 
pursuing idea  allowed algorithm expand nodes fringe
descendants fixed depth iteration  defined parameter  called expansion
horizon denoted k  represent  loosely speaking  number times whole fringe
expanded iteration  k      hao  expands open reachable regions
nodes fringe recomputing optimal policy  k      expands regions
fringe children updating policy  k     consider grandchildren regions fringe  on  k tends infinity  algorithm essentially
performs exhaustive search  first expands graph reachable nodes  performs one
pass  hybrid  dynamic programming graph determine optimal policy  balancing
node expansion update  expansion horizon allows tuning algorithm behavior
exhaustive search traditional heuristic search  experiments showed value k
     optimal solve hardest benchmark problems  see section    

  

fimeuleau  benazera  brafman  hansen   mausam

start

obspt 

unsafe

c 
obs
pt 

featureless
c 

w 
w 

w 

obs
pt 

obspt 

audience

demo

label

  waypoint
name

  rock
  ip   champ

obspt 
far

  science cam 

figure    k  rover  top left  developed jet propulsion laboratory nasa ames
research center prototype mer rovers  used test advanced rover
software  including automated planners rovers activities  right  topological map
     demo problem  arrows labeled ip   champ represent opportunity
deploy arm rock  instrument placement  take picture
champ camera  arrows labeled science cam represent opportunity take
remote picture rock science camera 

    updating multiple regions
expansion policies described based expanding open regions one several
nodes simultaneously  allow leveraging hybrid state dynamic programming techniques
feng et al         li littman         techniques may compute single
iteration piecewise constant linear value functions cover large range continuous states 
possibly whole space possible values  particular  back one iteration
continuous states included given bounds 
therefore  several open regions node expanded iteration
hao   update simultaneously backing up subset continuous states
includes regions  instance  one may record lower bounds upper bounds
continuous variable expanded regions  compute value function covers
hyper rectangle bounds 
modification algorithm impact convergence  long value
expanded regions computed  convergence proof holds  however  execution time may adversely affected expanded regions proper subset region continuous states

  

fihao 

 a  value function vn     initial node 
first plateau corresponds analyzing r   second plateau analyzing r   third plateau
analyzing r  r  

 b  policy n     starting
node shows partitions resource space different actions
optimal  dark  action  grey 
navigation r   light  analysis
r  

figure     a  optimal value function initial state simple rover problem possible
values continuous resources  time energy remaining   value function
partitioned      pieces   b  optimal policy set states 

backed up  case  values states open reachable uselessly computed 
deviates pure heuristic search algorithm 
however  modification may beneficial avoids redundant computation 
hybrid state dynamic programming techniques manipulate pieces value functions  thus  several
expanded regions included piece value function  value computed
once  practice  benefit may outweigh cost evaluating useless regions  moreover  cost
reduced storing value functions associated node graph 
computed values irrelevant regions saved case regions become eligible expansion
 i e   open reachable  later  thus  variant hao  fully exploits hybrid state dynamic
programming techniques 

   experimental evaluation
section  describe performance hao  solving planning problems simulated
planetary exploration rover two monotonic continuous valued resources  time battery
power  section     uses simple toy example problem illustrate basic steps
hao  algorithm  section     tests performance algorithm using realistic  real size
nasa simulation rover analyzes results experiments  simulation uses
model k  rover  see figure    developed intelligent systems  is  demo nasa
ames research center october       pedersen et al          complex real size model
k  rover uses command names understandable rovers execution language 
plans produced algorithm directly executed rover  experiments
reported section      simplify nasa simulation model way 

  

fimeuleau  benazera  brafman  hansen   mausam

figure    first iteration hao  toy problem  explicit graph marked dim edges
solution graph marked thick edges  tip nodes           shown
constant heuristic functions expanded nodes        shown backed
value functions 

planning problem consider  autonomous rover must navigate planar graph
representing surroundings authorized navigation paths  schedule observations
performed different rocks situated different locations  subset observational
goals achieved single run due limited resources  therefore  oversubscribed
planning problem  problem planning uncertainty since action uncertain
positive resource consumptions probability failing 
significant amount uncertainty domain comes tracking mechanism used
rover  tracking process rover recognizes rock based certain features
camera image associated rock  mission operations  problem instance
containing fixed set locations  paths  rocks built last panoramic camera image
sent rover  logical rock problem instance corresponds real rock 
rover must associate two basis features detected instruments 
including camera  rover moves camera image changes  rover must keep track
features image evolve  process uncertain subject faults result
losing track rock  practice  tracking modeled following way 
order perform measurement rock  rover must tracking rock 
navigate along path  must tracking one rocks enables following path 
set rocks enable path part problem definition given planner 
decision start tracking rock must made rover begins move 
rover starts moving  may keep track rock already tracked voluntarily stop
tracking it  cannot acquire new rock tracked initially 

  

fihao 

figure    second iteration hao  toy problem 
rover may randomly lose track rocks navigating along path  probability losing track rock depends rock path followed  part
problem definition given planner 
way reacquire rock whose track lost  intentionally accident 
number rocks tracked strongly influences duration resource consumption
navigate actions  higher number rocks tracked  costly navigate
along path  rover stop regularly check record aspect
rock tracked  creates incentive limit number rocks tracked
rover given set goals chosen path intends follow 
so  rover initially selects set rocks track tries keep set small possible
given goals  starts moving  may lose track rocks  may cause
reconsider set goals pursue route get corresponding rocks 
purposely stop tracking rock longer necessary given goals left
achieve 
implementation hao  uses dynamic programming algorithm developed feng et
al         summarized section     order perform backups hybrid state space 
partitions continuous state space associated node piecewise constant regions  uses
multiple region updates described section      upper bound resource
expanded regions computed  states included bounds minimal
possible resource levels updated 
experiments  use variant hao  algorithm described section     
parameter k sets number times whole fringe expanded iteration hao  
allows behavior algorithm tuned exhaustive search heuristic search 
used expansion horizon k     simple example section     default expansion
horizon k     larger examples section      section       describes experiments
different expansion horizons 

  

fimeuleau  benazera  brafman  hansen   mausam

figure    third iteration hao  toy problem 
implementation hao  uses simple heuristic described section      augmented
small amount domain knowledge  value hn  x  state  n  x  essentially equal
sum utilities goals yet achieved n  however  rover already moved
certain rock tracked state n  goals requiring rock tracked
included sum  reflects fact that  rover moved  cannot start tracking
rock more  thus goals require rock tracked unreachable  resulting
heuristic admissible  i e   never underestimates value state   straightforward
compute  note depend current resource levels  functions hn  x 
constant values x 
    example
begin simple example rover planning problem order illustrate steps
algorithm  solve example using implementation hao  use
solve realistic examples considered section     
example  targets two rocks  r  r   positioned locations l  l  
respectively  rovers initial location l   direct path l  l  
analyzing rock r  yields reward    analyzing rock r  yields reward     rovers
action set simplified  notably  features single action pic rx  represents steps
analyzing rock rx  stop tracking actions removed 
figure   shows optimal value function optimal policy found hao  starting
discrete state  resources ranging whole space possible values  figures       
show step by step process hao  solves problem  using expansion horizon
k      hao  solves problem three iterations  follows 
iteration    shown figure    hao  expands nodes        computes heuristic
function new tip nodes            backup step yields value function estimates
nodes         hao  identifies best solution graph new fringe node   

  

fihao 

 a       pieces 

 b      pieces 

 c      pieces 

figure     optimal value functions initial state simple rover problem increasing initial resource levels  from left right   optimal return appears three
dimensional function carved reachable space heuristic function 
problem
name
rover 
rover 
rover 
rover 

rover
locations
 
 
 
  

paths

goals

fluents

actions

  
  
  
  

 
 
 
 

  
  
  
  

  
  
  
  

discrete
states
 approx  
       
        
        
        

reachable
discrete
states
   
    
     
     

explicit
graph

optimal
policy

longest
branch

   
    
    
    

  
  
  
  

  
  
  
  

table    size benchmark rover problems 
iteration    shown figure    hao  expands nodes             starting
previous fringe node    computes heuristic functions new tip nodes           
heuristic value node    zero because  state  rover lost track r 
already analyzed r   backup step improves accuracy value function
several nodes  node    new fringe node since    terminal node 
iteration    shown figure    hao  expands node    node     search ends
iteration open node optimal solution graph 
comparison  figure    shows value function found hao  varies different initial
resource levels  figures  unreachable states assigned large constant heuristic value 
value function reachable states appears carved plateau heuristic 
    performance
now  describe hao s performance solving four much larger rover planning problems using
nasa simulation model  characteristics problems displayed tables    columns
two six show size problems terms rover locations  paths  goals  show
total number fluents  boolean state variables  actions problem  columns seven
ten report size discrete state space  total number discrete states two raised
power number fluents  although huge state space  limited number
states reached start state  depending initial resource levels  eighth
column table   shows number reachable discrete states initial time energy levels
set maximum value   the maximum initial resource levels based scenario
     demo represent several hours rover activity   shows simple reachability
  

fimeuleau  benazera  brafman  hansen   mausam

   

   
   
   
   
   
 

reachable
created
expanded
optimal policy

   
number discrete states

   
number discrete states

   

reachable
created
expanded
optimal policy

   
   
   
   
   

 

      

      
      
initial energy

      

 

      

 

    

    
    
initial time

    

     

    

     

    

     

    

     

 a  rover 
    

reachable
created
expanded
optimal policy

    

number discrete states

number discrete states

    

    
    
    
    
 

 

      

      
      
initial energy

      

    
    
    
    
 

      

reachable
created
expanded
optimal policy

    

 

    

    
    
initial time

 b  rover 
     

reachable
created
expanded
optimal policy

     

number discrete states

number discrete states

     

     
     
    
 

 

      

             
initial energy

      

     
     
     
    
 

      

reachable
created
expanded
optimal policy

 

    

    
    
initial time

 c  rover 
     

reachable
created
expanded
optimal policy

     

number discrete states

number discrete states

     

     
     
    
 

 

      

             
initial energy

      

     
     
     
    
 

      

reachable
created
expanded
optimal policy

 

    

    
    
initial time

 d  rover 
figure     number nodes created expanded hao  vs  number reachable discrete states 
graphs left column obtained fixing initial time maximum value
varying initial energy  graphs right column obtained fixing
initial energy maximum value varying initial time  results obtained
k     
  

fihao 

analysis based resource availability makes huge difference  partly due fact
planning domain  close k  execution language  allow many fluents
true simultaneously  columns nine ten show number discrete states explicit
graph optimal policy  precisely  former number nodes created hao  
is  subset reachable discrete states  number reachable discrete states  thus
size graph explore  may seem small compared discrete combinatorial problems
solved ai techniques  iteration  continuous approximation two dimensional
backup necessary evaluate hybrid state space associated graph  finally  last
column table   shows length longest branch optimal policy initial
resource levels set maximum value 
largest four instances  that is  rover   exactly problem october     
demo  considered large rover problem  example  much larger
problems faced mer rovers never visit one rock single planning cycle 
      efficiency pruning
first set simulations  try evaluate efficiency heuristic pruning hao   is 
portion discrete search space spared exploration use admissible
heuristics  purpose  compare number discrete states reachable given
resource level number nodes created expanded hao   consider
number nodes optimal policy found algorithm 
results four benchmark problems presented figure     curves obtained
fixing one resource maximum possible value varying   maximum 
therefore  represent problems mostly one resource constraining  result show 
notably  single resource enough constrain reachability state space significantly 
surprisingly  problems become larger initial resources increase  discrete
states become reachable  despite simplicity heuristic used  hao  able by pass
significant part search space  moreover  bigger problem  leverage
algorithm take simple heuristic 
results quite encouraging  number nodes created expanded
always reflect search time  therefore  examine time takes hao  produce solutions 
      search time
figure    shows hao  search time set experiments  curves exhibit
monotonicity and  instead  appear show significant amount noise  surprising
search time always increase increase initial levels resource  although
search space bigger  shows search complexity depend size search
space alone  factors must explain complexity peaks observed figure    
number nodes created expanded algorithm contain noise 
reason peaks computation time must time spent dynamic programming
backups  moreover  search time appears closely related complexity optimal policy 
figure    shows number nodes branches policy found algorithm  well
number goals pursued policy  shows that   i  cases  increasing initial
resource level eliminates need branching reduces size optimal solution   ii 
size optimal policy and  secondarily  number branches  explains peaks
search time curves  therefore  question is  large solution graph induce long time
spent backups  two possible answers question  backups take longer
and or backups performed  first explanation pretty intuitive 
policy graph contains many branches leading different combinations goals  value functions
contain many humps plateaus  therefore many pieces  impacts complexity
dynamic programming backups  however  time empirical evidence
  

fi  

  

  

  

  

  

  

  

search time  s 

search time  s 

meuleau  benazera  brafman  hansen   mausam

  
 
 

  
 
 

 

 

 

 

 

 

      

      
      
initial energy

      

 

      

 

    

    
    
initial time

    

     

    
    
initial time

    

     

   

   

   

   

   

   

   

search time  s 

search time  s 

 a  rover 
   

   
  
  

   
  
  

  

  

  

  

 

 

      

      
      
initial energy

      

 

      

 

    

     

     

     
search time  s 

search time  s 

 b  rover 
     

     
     
    
 

     
     
    

 

      

             
initial energy

      

 

      

 

    

    
    
initial time

    

     

 

    

    
    
initial time

    

     

     

     

     

     
search time  s 

search time  s 

 c  rover 

     

    

 

     

    

 

      

             
initial energy

      

 

      

 d  rover 
figure     hao  search time  graphs left column obtained fixing initial time
maximum value  graphs right column obtained fixing
initial energy maximum  results obtained k     

  

fihao 

confirm hypothesis  conversely  observe peak figure    comes increase
number backups  work required explain this 
      expansion horizon
results section       show hao  leverage even simple admissible heuristic prune
large portion search space  necessarily follow hao  outperform
exhaustive search algorithm creates graph reachable states  executes one pass
dynamic programming graph find optimal policy  although hao  expands smaller
graph exhaustive search  must evaluate graph often  section     
introduced parameter k expansion horizon order allow adjustment trade off
time spent expanding nodes time spent evaluating nodes  study influence
parameter algorithm 
figure    shows number nodes created expanded hao  function
expansion horizon four benchmark problem instances  surprisingly  algorithm creates
expands nodes expansion horizon increases  essentially  behaves
exhaustive search k increased  two smallest problem instances  large enough
values k  number visited states levels total number reachable states
reached  two largest problem instances  interrupt experiments k reached
   search time became long 
figure    shows effect expansion horizon search time hao   smallest
problem instance  rover    hao  clear advantage exhaustive search  with
k        even though explores fewer nodes  three larger problem instances  hao 
clear advantage  rover  problem instance  search time hao  levels
k       indicating limit reachable states reached  however  duration
exhaustive search several times longer hao  smaller settings k  benefits
hao  clearer two largest problem instances  k increased  algorithm
quickly overwhelmed combinatorial explosion size search space  simulations
eventually need interrupted search time becomes long  problem
instances smaller settings k  hao  able efficiently find optimal solutions 
overall  results show clear benefit using admissible heuristics prune
search space  although expansion horizon must adjusted appropriately order hao 
achieve favorable trade off node expansion time node evaluation time 

   conclusion
introduced heuristic search approach finding optimal conditional plans domains characterized continuous state variables represent limited  consumable resources  hao 
algorithm variant ao  algorithm that  best knowledge  first algorithm deal following  limited continuous resources  uncertain action outcomes 
over subscription planning  tested hao  realistic nasa simulation planetary rover 
complex domain practical importance  results demonstrate effectiveness solving
problems large solved straightforward application dynamic programming  effective heuristic search exploit resource constraints  well admissible
heuristic  order limit reachable state space 
implementation  hao  algorithm integrated dynamic programming algorithm feng et al          however hao  integrated dynamic programming
algorithms solving hybrid state mdps  feng et al  algorithm finds optimal policies
limiting assumptions transition probabilities discrete  rewards either piecewiseconstant piecewise linear  recently developed dynamic programming algorithms hybridstate mdps make less restrictive assumptions  potential improve computational

  

fimeuleau  benazera  brafman  hansen   mausam

  

 

  

 

  

 

 

 

      

             
initial energy

      

  

 
      

 

nodes
branches
goals

 

  

 

  

 

  

 

 

 

    

    
    
initial time

    

number branches goals

 

  

number nodes

  
number nodes

 

nodes
branches
goals

number branches goals

  

 
     

 a  rover 

  

 

  

 

  

 

 

 

      

             
initial energy

      

  

 
      

 

nodes
branches
goals

 

  

 

  

 

  

 

 

 

    

    
    
initial time

    

number branches goals

 

  

number nodes

nodes
branches
goals

  
number nodes

 
number branches goals

  

 
     

 b  rover 

  

 

  

 

  

 

 

 

      

             
initial energy

      

  

 
      

 

nodes
branches
goals

 

  

 

  

 

  

 

 

 

    

    
    
initial time

    

number branches goals

 

  

number nodes

nodes
branches
goals

  
number nodes

 
number branches goals

  

 
     

 c  rover 

  

 

  

 

  

 

 

 

      

             
initial energy

      

  

 
      

 

nodes
branches
goals

 

  

 

  

 

  

 

 

 

    

    
    
initial time

    

number branches goals

 

   

number nodes

nodes
branches
goals

  
number nodes

 
number branches goals

   

 
     

 d  rover 
figure     complexity optimal policy  number nodes  branches  goals optimal
policy setting figure    

  

fihao 

   

number discrete states

   
number discrete states

    

created
expanded

   
   
   
   
   
 

 

 

  
  
  
expansion horizon

    
    
    
    
 

  

created
expanded

    

 

 

  

 a  rover 
     

number discrete states

number discrete states

  

created
expanded

     

     
    
    
    
    
 

  

 b  rover 
     

created
expanded

     

  
  
expansion horizon

     
     
    
    
    
    

 

 

  
  
expansion horizon

 

  

 c  rover 

 

 

  
  
expansion horizon

  

 d  rover 

figure     influence expansion horizon number nodes visited algorithm 
efficiency  li   littman        marecki et al          integrating hao  one algorithms
could improve performance further 
several interesting directions work could extended  developing hao   made assumptions every action consumes resource resources
non replenishable  without assumptions  state could revisited optimal
plan could loops well branches  generalizing approach allow plans loops 
seems necessary handle replenishable resources  requires generalizing heuristic search
algorithm lao  solve hybrid mdps  hansen   zilberstein         another possible extension
allow continuous action variables addition continuous state variables  finally  heuristic
search approach could combined approaches improving scalability  hierarchical decomposition  meuleau   brafman         would allow handle even larger state
spaces result number goals over subscription planning problem increased 
acknowledgments
work funded nasa intelligent systems program  grant nra         eric hansen
supported part nasa summer faculty fellowship funding mississippi
space grant consortium  work performed emmanuel benazera working
nasa ames research center ronen brafman visiting nasa ames research center 
consultants research institute advanced computer science  ronen brafman
supported part lynn william frankel center computer science  paul ivanier
center robotics production management  isf grant          nicolas meuleau
consultant carnegie mellon university nasa ames research center 

  

fimeuleau  benazera  brafman  hansen   mausam

  

    
    
    

  

search time  s 

search time  s 

  

  
  

    
   
   
   

  
 

    

   
 

 

  
  
  
expansion horizon

 

  

 

 

  

 a  rover 

  

     
     
search time  s 

     
search time  s 

  

 b  rover 

     

     
     
    
 

  
  
expansion horizon

     
     
     
    

 

 

  
  
expansion horizon

 

  

 c  rover 

 

 

  
  
expansion horizon

  

 d  rover 

figure     influence expansion horizon overall search time 

references
altman  e          constrained markov decision processes  chapman hall crc 
bertsekas  d     tsitsiklis  j          neural dynamic programming  athena scientific  belmont 
ma 
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions
computational leverage  journal artificial intelligence research          
boyan  j     littman  m          exact solutions time dependent mdps  advances neural
information processing systems     pp      mit press  cambridge 
bresina  j   dearden  r   meuleau  n   ramakrishnan  s   smith  d     washington  r         
planning continuous time resource uncertainty  challenge ai  proceedings
eighteenth conference uncertainty artificial intelligence  pp       
bresina  j   jonsson  a   morris  p     rajan  k          activity planning mars exploration
rovers  proceedings fifteenth international conference automated planning
scheduling  pp       
chakrabarti  p   ghose  s     desarkar  s          admissibility ao  heuristics overestimate  aritificial intelligence            
feng  z   dearden  r   meuleau  n     washington  r          dynamic programming structured continuous markov decision problems  proceedings twentieth conference
uncertainty artificial intelligence  pp         
  

fihao 

friedman  j   bentley  j     finkel  r          algorithm finding best matches logarithmic
expected time  acm trans  mathematical software               
hansen  e     zilberstein  s          lao   heuristic search algorithm finds solutions
loops  artificial intelligence            
jimenez  p     torras  c          efficient algorithm searching implicit and or graphs
cycles  artificial intelligence           
kveton  b   hauskrecht  m     guestrin  c          solving factored mdps hybrid state
action variables  journal artificial intelligence research             
li  l     littman  m          lazy approximation solving continuous finite horizon mdps 
proceedings twentieth national conference artificial intelligence  pp           
marecki  j   koenig  s     tambe  m          fast analytical algorithm solving markov decision
processes real valued resources  proceedings   th international joint conference
artificial intelligence  ijcai     pp           
mausam  benazera  e   brafman  r   meuleau  n     hansen  e          planning continuous resources stochastic domains  proceedings nineteenth international joint
conference artificial intelligence  pp            professional book center  denver  co 
meuleau  n     brafman  r          hierarchical heuristic forward search stochastic domains 
proceedings   th international joint conference artificial intelligence  ijcai     
pp           
munos  r     moore  a          variable resolution discretization optimal control  machine
learning                   
nilsson  n          principles artificial intelligence  tioga publishing company  palo alto  ca 
pearl  j          heuristics  intelligent search strategies computer problem solving  addisonwesley 
pedersen  l   smith  d   deans  m   sargent  r   kunz  c   lees  d     rajagopalan  s         
mission planning target tracking autonomous instrument placement  proceedings
     ieee aerospace conference   big sky  montana 
puterman  m          markov decision processes  discrete stochastic dynamic programming 
wiley  new york  ny 
rust  j          using randomization break curse dimensionality  econimetrica         
    
smith  d          choosing objectives over subscription planning  proceedings fourteenth
international conference automated planning scheduling  pp         
thiebaux  s   gretton  c   slaney  j   price  d     kabanza  f          decision theoretic planning
non markovian rewards  journal artificial intelligence research           
van den briel  m   sanchez  r   do  m     kambhampati  s          effective approaches partial
satisfation  over subscription  planning  proceedings nineteenth national conference
artificial intelligence  pp         

  


