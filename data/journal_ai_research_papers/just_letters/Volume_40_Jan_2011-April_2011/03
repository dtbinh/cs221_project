journal artificial intelligence research                 

submitted        published      

narrowing modeling gap 
cluster ranking approach coreference resolution
altaf rahman
vincent ng

altaf hlt utdallas edu
vince hlt utdallas edu

human language technology research institute
university texas dallas
    west campbell road  mail station ec  
richardson  tx            u s a 

abstract
traditional learning based coreference resolvers operate training mention pair
model determining whether two mentions coreferent not  though conceptually
simple easy understand  mention pair model linguistically rather unappealing
lags far behind heuristic based coreference models proposed pre statistical
nlp era terms sophistication  two independent lines recent research attempted improve mention pair model  one acquiring mention ranking model
rank preceding mentions given anaphor  training entity mention
model determine whether preceding cluster coreferent given mention 
propose cluster ranking approach coreference resolution  combines strengths
mention ranking model entity mention model  therefore theoretically
appealing models  addition  seek improve cluster rankers
via two extensions      lexicalization     incorporating knowledge anaphoricity
jointly modeling anaphoricity determination coreference resolution  experimental results ace data sets demonstrate superior performance cluster rankers
competing approaches well effectiveness two extensions 

   introduction
noun phrase  np  coreference resolution task identifying nps  or mentions
ace terminology    text dialogue refer real world entity concept 
computational perspective  coreference clustering task  goal partitioning
set mentions coreference clusters cluster contains
mentions co referring  mathematical perspective  coreference relation
equivalence relation defined pair mentions  satisfies reflexivity  symmetry 
transitivity  following previous work coreference resolution  use term
anaphoric describe mention part coreference chain head
chain  given anaphoric mention mk   antecedent mk mention coreferent
mk precedes associated text  set candidate antecedents mk
consists mentions precede mk   
   precisely  mention instance reference entity real world  article 
treat terms mention noun phrase synonymous use interchangeably 
   note definitions somewhat overloaded  linguistically  anaphor noun phrase
depends antecedent semantic interpretation  hence  barack obama anaphoric
definition formal definition 
c
    
ai access foundation  rights reserved 

firahman   ng

research focus computational coreference resolution exhibited gradual shift
heuristic based approaches machine learning approaches past decade  shift
attributed part advent statistical natural language processing  nlp 
era  part public availability coreference annotated corpora produced
result muc   muc   conferences series ace evaluations  one
influential machine learning approaches coreference resolution classificationbased approach  coreference recast binary classification task  e g   aone  
bennett        mccarthy   lehnert         specifically  classifier trained
coreference annotated data used determine whether pair mentions co referring
not  however  pairwise classifications produced classifier  which commonly known mention pair model  may satisfy transitivity property inherent
coreference relation  since possible model classify  a b  coreferent 
 b c  coreferent   a c  coreferent  result  separate clustering mechanism needed coordinate possibly contradictory pairwise classification decisions
construct partition given mentions 
mention pair model significantly influenced learning based coreference research
past fifteen years  fact  many recently published coreference papers
still based classical learning based coreference model  e g   bengtson   roth       
stoyanov  gilbert  cardie    riloff         despite popularity  model least
two major weaknesses  first  since candidate antecedent mention resolved
 henceforth active mention  considered independently others  model
determines good candidate antecedent relative active mention 
good candidate antecedent relative candidates  words  fails
answer critical question candidate antecedent probable  second 
limitations expressiveness  information extracted two mentions
alone may sufficient making informed coreference decision  especially
candidate antecedent pronoun  which semantically empty  mention lacks
descriptive information gender  e g   clinton  
recently  coreference researchers investigated alternative models coreference
aim address aforementioned weaknesses mention pair model  address
first weakness  researchers proposed mention ranking model  model determines candidate antecedent probable given active mention imposing
ranking candidate antecedents  e g   denis   baldridge      b        iida  inui 
  matsumoto         ranking arguably natural formulation coreference resolution classification  ranker allows candidate antecedents considered
simultaneously therefore directly captures competition among them  another desirable consequence exists natural resolution strategy ranking approach 
mention resolved candidate antecedent highest rank  contrasts
classification based approaches  many clustering algorithms employed
co ordinate pairwise coreference decisions  because unclear one best  
address second weakness  researchers proposed entity mention coreference
model  e g   luo  ittycheriah  jing  kambhatla    roukos        yang  su  zhou    tan 
      yang  su  lang  tan    li         unlike mention pair model  entity mention
model trained determine whether active mention belongs preceding  possibly
partially formed  coreference cluster  hence  employ cluster level features  i e   fea   

fia cluster ranking approach coreference resolution

tures defined subset mentions preceding cluster   makes
expressive mention pair model 
entity mention model mention ranking model conceptually simple
extensions mention pair model  born nearly ten years mention pair
model proposed  particular  contributions under estimated 
paved new way thinking supervised modeling coreference represents
significant departure mention pair counterpart  many years
learning based coreference model nlp researchers  proposal two models
facilitated part advances statistical modeling natural languages  statistical
nlp models evolved capturing local information global information 
employing classification based models ranking based models  context
coreference resolution  entity mention model enables us compute features based
variable number mentions  mention ranking model enables us rank variable
number candidate antecedents  nevertheless  neither models addresses
weaknesses mention pair model satisfactorily  mention ranking model allows
candidate antecedents ranked compared simultaneously  enable
use cluster level features  hand  entity mention model employ
cluster level features  allow candidates considered simultaneously 
motivated part observation  propose learning based approach coreference resolution theoretically appealing mention ranking model
entity mention model  cluster ranking approach  specifically  recast coreference problem determining set preceding coreference clusters
best link active mention using learned cluster ranking model  essence 
cluster ranking model combines strengths mention ranking model entitymention model  addresses weaknesses associated mention pair model 
cluster ranking model appears conceptually simple natural extension entity mention model mention ranking model  believe
simplicity stems primarily choice presentation concepts easiest
reader understand  particular  note mental processes involved
design cluster ranking model means simple way model
presented  requires analysis strengths weaknesses existing
approaches learning based coreference resolution connection them 
formulation view entity mention model mention ranking
model addressing two complementary weaknesses mention pair model  believe
significance cluster ranking model lies bridging two rather independent
lines learning based coreference research going past years 
one involving entity mention model mention ranking model 
addition  seek improve cluster ranking model two sources linguistic knowledge  first  propose exploit knowledge anaphoricity  i e   knowledge
whether mention anaphoric not   anaphoricity determination means new
problem  neither use anaphoricity information improve coreference resolution  innovation lies way learn knowledge anaphoricity  specifically 
previous work typically adopted pipeline coreference architecture 
anaphoricity determination performed prior coreference resolution resulting
information used prevent coreference system resolving mentions de   

firahman   ng

termined non anaphoric  for overview  see work poesio  uryupina  vieira 
alexandrov kabadjov    goulart         propose model jointly learning anaphoricity determination coreference resolution  note major weakness pipeline
architecture lies fact errors anaphoricity determination could propagated
coreference resolver  possibly leading deterioration coreference performance
 ng   cardie      a   joint model potential solution error propagation
problem 
second  examine kind linguistic features exploited majority
existing supervised coreference resolvers  word pairs composed strings  or
head nouns  active mention one preceding mentions  intuitively 
word pairs contain useful information  example  may help improve precision
model  allowing learner learn moderate probability
anaphoric  contrary taken phrase contrary never
anaphoric  may help improve recall  allowing learner determine 
instance  airline carrier coreferent  hence  offer convenient
means attack one major problems coreference research  identifying coreferent
common nouns lexically dissimilar semantically related  note
extremely easy compute  even so called cheap features stringmatching grammatical features  yang  zhou  su    tan         majority
existing supervised coreference systems unlexicalized hence exploiting
them  somewhat unexpectedly  however  researchers lexicalize coreference
models employing word pairs features  e g   luo et al         daume iii   marcu       
bengtson   roth         feature analysis experiments indicate lexical features
best marginally useful  instance  luo et al  daume iii marcu report
leaving lexical features feature ablation experiments causes ace value
drop          respectively  previous attempts lexicalization merely
append word pairs conventional coreference feature set  goal investigate
whether make better use lexical features learning based coreference resolution 
sum up  propose cluster ranking approach coreference resolution joint
model exploiting anaphoricity information  investigate role lexicalization
learning based coreference resolution  besides empirically demonstrating clusterranking model significantly outperforms competing approaches ace      coreference
data set  two extensions model  namely lexicalization joint modeling 
effective improving performance  believe work makes four contributions
coreference resolution 
narrowing modeling gap  machine learning approaches coreference resolution received lot attention since mid     s  mention pair model
heavily influenced learning based coreference research decade  yet
model lags far behind heuristic based coreference models proposed     s
    s terms sophistication  particular  notion ranking traced back
centering algorithms  for information  see books mitkov        walker  joshi 
  prince         idea behind ranking preceding clusters  in heuristic manner 
found lappin leasss        influential paper pronoun resolution 
cluster ranking model completely close gap simplicity machine
learning approaches sophistication heuristic approaches coreference resolu   

fia cluster ranking approach coreference resolution

tion  believe represents important step towards narrowing gap  another
important gap cluster ranking model helps bridge two independent lines
learning based coreference research going past years  one
involving entity mention model mention ranking model 
promoting use ranking models  mention ranking model
empirically shown outperform mention pair model  denis   baldridge      b        
former received much attention among coreference researchers should 
particular  mention pair model continues popularly used investigated
past years mention ranking model  believe lack excitement
ranking based approaches coreference resolution attributed least part
lack theoretical understanding ranking  previous work ranking based coreference
resolution employed ranking algorithms essentially black box  without opening
black box  could difficult researchers appreciate subtle difference
ranking classification  attempt promote use ranking based models 
provide brief history use ranking coreference resolution  section    
tease apart differences classification ranking showing constrained
optimization problem support vector machine  svm  attempts solve classificationbased ranking based coreference models  section    
gaining better understanding existing learning based coreference models 
recall lexicalization one two linguistic knowledge sources propose
use improve cluster ranking model  note lexicalization applied
cluster ranking model  essentially learning based coreference models  however 
mentioned before  vast majority existing coreference resolvers unlexicalized 
fact  mention ranking model shown improve mention pair model
unlexicalized feature set  attempt gain additional insights behavior
different learning based coreference models  compare performance lexicalized
feature set  furthermore  analyze via experiments involving feature ablation
data source adaptability  well report performance resolving different types
anaphoric expressions 
providing implementation cluster ranking model  stimulate
research ranking based approaches coreference resolution  facilitate use
coreference information high level nlp applications  make software implements cluster ranking model publicly available  
rest article organized follows  section   provides overview use
ranking coreference resolution  section   describes baseline coreference models 
mention pair model  entity mention model  mention ranking model 
discuss cluster ranking approach joint model anaphoricity determination
coreference resolution section    section   provides details lexicalize
coreference models  present evaluation results experimental analyses different
aspects coreference models section   section    respectively  finally 
conclude section   
   software available http   www hlt utdallas edu   altaf cherrypicker  

   

firahman   ng

   ranking approaches coreference resolution  bit history
ranking theoretically empirically better formulation learning based coreference resolution classification  mention ranking model popularly
used investigated mention pair counterpart since proposed  promote
ranking based coreference models  set stage discussion learningbased coreference models next section  provide section brief history
use ranking heuristic based learning based coreference resolution 
broader sense  many heuristic anaphora coreference resolvers rankingbased  example  find antecedent anaphoric pronoun  hobbss        seminal syntax based resolution algorithm considers sentences given text reverse
order  starting sentence pronoun resides searching potential
antecedents corresponding parse trees left to right  breadth first manner
obeys binding agreement constraints  hence  keep searching beginning
text reached  i e   stop even algorithm proposes antecedent  
obtain ranking candidate antecedents pronoun consideration  rank candidate determined order proposed
algorithm  fact  rank antecedent obtained via method commonly
known hobbss distance  used linguistic feature statistical
pronoun resolvers  e g   ge  hale    charniak        charniak   elsner         general 
search based resolution algorithms hobbss consider candidate antecedents particular order  typically  propose first candidate satisfies linguistic constraints
antecedent 
strictly speaking  however  may want consider heuristic resolution algorithm
ranking based algorithm considers candidate antecedents simultaneously 
example assigning rank score candidate selecting highest ranked
highest scored candidate antecedent  even stricter definition
ranking  still many heuristic resolvers ranking based  resolvers
typically assign rank score candidate antecedent based number factors 
knowledge sources  propose one highest rank score
antecedent  e g   carbonell   brown        cardie   wagstaff         factor belongs
one two types  constraints preferences  mitkov         constraints must satisfied
two mentions posited coreferent  examples constraints include gender
number agreement  binding constraints  semantic compatibility  preferences indicate likelihood candidate antecedent  preference factors measure
compatibility anaphor candidate  e g   syntactic parallelism favors candidates grammatical role anaphor   preference factors
computed based candidate only  typically capturing salience candidate 
constraint preference manually assigned weight indicating importance 
instance  gender disagreement typically assigned weight   indicating
candidate anaphor must agree gender  whereas preference factors typically
finite weight  score candidate obtained summing weights
factors associated candidate 
ranking based resolution algorithms assign score candidate antecedent  rather  simply impose ranking candidates based salience 
   

fia cluster ranking approach coreference resolution

perhaps representative family algorithms employ salience rank candidates centering algorithms  for descriptions specific centering algorithms  see
work grosz  joshi    weinstein              walker et al         mitkov        
salience mention  typically estimated using grammatical role  used rank
forward looking centers 
work related lappin leass         whose goal perform pronoun resolution assigning anaphoric pronoun highest ranked preceding
cluster  therefore heuristic cluster ranking model  many heuristic based
resolvers  lappin leasss algorithm identifies highest ranked preceding cluster
active mention first applying set linguistic constraints filter candidate antecedents grammatically incompatible active mention  ranking
preceding clusters  contain mentions survive filtering process  using
salience factors  examples salience factors include sentence recency  whether preceding cluster contains mention appears sentence currently processed  
subject emphasis  whether cluster contains mention subject position   existential emphasis  whether cluster contains mention predicate nominal
existential construction   accusative emphasis  whether cluster contains mention
appears verbal complement accusative case   salience factor associated
manually assigned weight indicates importance relative factors 
score cluster sum weights salience factors applicable
cluster  lappin leasss paper widely read paper pronoun resolution 
cluster ranking aspect algorithm rarely emphasized  fact 
aware recent work learning based coreference resolution establishes
connection entity mention model lappin leasss algorithm 
despite conceptual similarities  cluster ranking model lappin leasss
       algorithm differ several respects  first  lappin leass tackle pronoun resolution rather full coreference task  second  apply linguistic constraints
filter incompatible candidate antecedents  resolution strategy learned without applying hand coded constraints separate filtering step  third  attempt
compute salience preceding cluster respect active mention  attempt
determine compatibility cluster active mention  using factors
determine salience lexical grammatical compatibility  instance 
finally  algorithm heuristic based  weights associated salience
factor encoded manually rather learned  unlike system 
first paper learning based coreference resolution written connolly  burger 
day        published year lappin leasss        paper 
contrary common expectation  coreference model paper proposes rankingbased model  influential mention pair model  main idea behind connolly et
al s approach convert problem ranking n candidate antecedents set
pairwise ranking problems  involves ranking exactly two candidates 
rank two candidates  classifier trained using training set instance
corresponds active mention well two candidate antecedents possesses
class value indicates two candidates better  idea certainly ahead
time  embodied many advanced ranking algorithms developed
machine learning information retrieval communities past years 
   

firahman   ng

later re invented almost time  independently  yang et al        
iida  inui  takamura  matsumoto         refer twin candidate model
tournament model  respectively  name twin candidate model motivated
fact model considers two candidates time  whereas name tournament
model assigned ranking two candidates viewed tournament
 with higher ranked candidate winning tournament  candidate wins
largest number tournaments chosen antecedent active mention 
bit history rarely mentioned literature  reveals three somewhat interesting
perhaps surprising facts  first  ranking first applied train coreference models
much earlier people typically think  second  despite first learning based
coreference model  connolly et al s ranking based model theoretically appealing
classification based mention pair model  later shown yang et al 
iida et al   empirically better well  finally  despite theoretical empirical
superiority  connolly et al s model largely ignored nlp community received
attention re invented nearly decade later  time period
mention pair counterpart essentially dominated learning based coreference research  
conclude section making important observation distinction classification ranking applies discriminative models generative models 
generative models try capture true conditional probability event  context coreference resolution  probability mention particular
antecedent referring particular entity  i e   preceding cluster   since probabilities normalize  similar ranking objective  system trying raise
probability mention refers correct antecedent entity expense
probabilities refers other  thus  antecedent version generative
coreference model proposed ge et al         resembles mention ranking model 
entity version proposed haghighi klein        similar spirit
cluster ranking model 

   baseline coreference models
section  describe three coreference models serve baselines 
mention pair model  entity mention model  mention ranking model  illustrative purposes  use text segment shown figure    mention
segment annotated  m cid
mid   mid mention id cid id cluster
belongs  see  mentions partitioned four sets  barack
obama  his  one cluster  remaining mentions cluster 

   may possible  and perhaps crucial  determine mention pair model received
lot attention connolly et al s model  since days academic papers
could accessed easily electronic form  speculate publication venue played role 
connolly et al s work published new methods language processing conference     
 and later book chapter        whereas mention pair model introduced aone
bennetts        paper mccarthy lehnerts        paper  appeared proceedings
two comparatively higher profile ai conferences  acl      ijcai      

   

fia cluster ranking approach coreference resolution

 barack obama    nominated  hillary rodham clinton      his    secretary state     monday     
 he       

figure    illustrative example
    mention pair model
noted before  mention pair model classifier decides whether
active mention mk coreferent candidate antecedent mj   instance i mj   mk  
represents mj mk   implementation  instance consists    features shown
table    features largely employed state of the art learning based
coreference systems  e g   soon  ng    lim        ng   cardie      b  bengtson   roth 
       computed automatically  seen  features divided four
blocks  first two blocks consist features describe properties mj mk  
respectively  last two blocks features describe relationship mj
mk   classification associated training instance either positive negative 
depending whether mj mk coreferent 
one training instance created pair mentions  negative instances
would significantly outnumber positives  yielding skewed class distribution
typically adverse effect model training  result  subset mention
pairs generated training  following soon et al          create     positive
instance anaphoric mention mk closest antecedent mj       negative
instance mk paired intervening mentions  mj     mj             mk   
running example shown figure    three training instances generated he 
i monday  he   i secretary state  he   i his  he   first two instances
labeled negative  last one labeled positive  train mention pair
model  use svm learning algorithm svmlight package  joachims         
mentioned introduction  previous work learning based coreference
resolution typically treats underlying machine learner simply black box tool 
choose provide reader overview svms  learner employing
work  note self contained overview  means comprehensive
introduction maximum margin learning  goal provide reader
details believe needed understand difference classification
ranking perhaps appreciate importance ranking  
begin with  assume given data set consisting positively labeled
points  class value     negatively labeled points  class
   since svmlight assumes real valued features  cannot operate features multiple discrete values
directly  hence  need convert features shown table   equivalent set features
used directly svmlight   uniformity  perform conversion feature
table    rather multi valued features  follows  create one binary valued feature
svmlight feature value pair derived feature set table    example 
pronoun   two values  n  derive two binary valued features  pronoun   y
pronoun   n  one value   value   instance 
   overview theory maximum margin learning  refer reader burgess       
tutorial 

   

firahman   ng

features describing mj   candidate antecedent
  pronoun  
mj pronoun  else n
  subject  
mj subject  else n
  nested  
mj nested np  else n
features describing mk   mention resolved
  number  
singular plural  determined using lexicon
male  female  neuter  unknown  determined using list
  gender  
common first names
mk pronoun  else n
  pronoun  
  nested  
mk nested np  else n
  semclass  
semantic class mk   one person  location  organization  date  time  money  percent  object  others  determined using wordnet  fellbaum        stanford ne recognizer  finkel  grenager    manning       
  animacy  
mk determined human animal wordnet ne
recognizer  else n
nominative case mk pronoun  else na  e g  
   pro type  
feature value
features describing relationship mj   candidate antecedent mk  
mention resolved
   head match
c mentions head noun  else
   str match
c mentions string  else
   substr match
c one mention substring other  else
c mentions pronominal string  else
   pro str match
   pn str match
c mentions proper names string  else
   nonpro str match c two mentions non pronominal
string  else
   modifier match
c mentions modifiers  na one
dont modifier  else
c mentions pronominal either pronoun
   pro type match
different respect case  na least one
pronominal  else
   number
c mentions agree number  disagree  na
number one mentions cannot determined
   gender
c mentions agree gender  disagree  na gender
one mentions cannot determined
   agreement
c mentions agree gender number  disagree
number gender  else na
   animacy
c mentions match animacy  dont  na
animacy one mentions cannot determined
c mentions pronouns  neither pronouns  else na
   pronouns
   proper nounsc mentions proper nouns  neither proper nouns 
else na
   maximalnp
c two mentions maximial np projection  else
   span
c neither mention spans other  else
   indefinite
c mk indefinite np appositive relationship 
else
   appositive
c mentions appositive relationship  else
   copular
c mentions copular construction  else

   

fia cluster ranking approach coreference resolution

features describing relationship mj   candidate antecedent mk  
mention resolved  continued previous page 
   semclass
c mentions semantic class  where set
semantic classes considered enumerated description
semclass   feature   dont  na semantic class
information one mentions cannot determined
   alias
c one mention abbreviation acronym other  else

   distance
binned values sentence distance mentions
additional features describing relationship mj   candidate antecedent
mk   mention resolved
   number
concatenation number   feature values mj mk  
e g   mj clinton mk they  feature value singularplural  since mj singular mk plural
   gender
concatenation gender   feature values mj mk
   pronoun
concatenation pronoun   feature values mj mk
   nested
concatenation nested   feature values mj mk
   semclass
concatenation semclass   feature values mj mk
   animacy
concatenation animacy   feature values mj mk
concatenation pro type   feature values mj mk
   pro type

table    feature set coreference resolution  non relational features describe mention
cases take value yes no  relational features describe relationship
two mentions indicate whether compatible  incompatible
applicable 
value    used classification mode  svm learner aims learn hyperplane
 i e   linear classifier  separates positive points negative points 
one hyperplane achieves zero training error  learner choose
hyperplane maximizes margin separation  i e   distance
hyperplane training example closest it   larger margin proven
provide better generalization unseen data  vapnik         formally  maximum
margin hyperplane defined w x b      x feature vector representing
arbitrary data point  w  a weight vector  b  a scalar  parameters
learned solving following constrained optimization problem 
optimization problem    hard margin svm classification
arg min
subject

 
kwk 
 

yi  w xi b    

  n 

yi         class i th training point xi   note data point
xi   exactly one linear constraint optimization problem ensures xi
correctly classified  particular  using value   right side inequality
   

firahman   ng

constraint ensures certain distance  i e   margin  xi hyperplane 
shown margin inversely proportional length weight vector 
hence  minimizing length weight vector equivalent maximizing margin 
resulting svm classifier known hard margin svm  margin hard
data point correct side hyperplane 
however  cases data set linearly separable  hyperplane
perfectly separate positives negatives  result 
constrained optimization problem solution  instead asking svm
learner give return solution  solve relaxed version problem
consider hyperplanes produce non zero training errors potential solutions 
words  modify linear constraints associated data point
training errors allowed  however  modify linear constraints
leave objective function is  learner search maximum margin
hyperplane regardless training error produces  since training error correlates
positively generalization error  crucial objective function take
consideration training error hyperplane large margin low training
error found  however  non trivial maximize margin minimize
training error simultaneously  since training error typically increases maximize
margin  result  need find trade off two criteria  resulting
objective function linear combination margin size training error 
formally  find optimal hyperplane solving following constrained optimization
problem 
optimization problem    soft margin svm classification
arg min

x
 
kwk    c

 


subject
yi  w xi b        n 
before  yi         class i th training point xi   c regularization
parameter balances training error margin size  finally  non negative slack
variable represents degree misclassification xi   particular      
data point wrong side hyperplane  svm allows data points
appear wrong side hyperplane  known soft margin svm 
given optimization problem  rely training algorithm employed svmlight
finding optimal hyperplane 
training  resulting svm classifier used clustering algorithm identify
antecedent mention test text  specifically  active mention compared
turn preceding mention  pair  test instance created training
presented svm classifier  returns value indicates likelihood
two mentions coreferent  mention pairs class values   considered
coreferent  otherwise pair considered coreferent  following soon et al         
apply closest first linking regime antecedent selection  given active mention mk  
   

fia cluster ranking approach coreference resolution

select antecedent closest preceding mention classified coreferent
mk   mk classified coreferent preceding mention  considered
non anaphoric  i e   antecedent selected mk   
    entity mention model
unlike mention pair model  entity mention model classifier decides whether
active mention mk belongs partial coreference cluster cj precedes mk  
training instance  i cj   mk    represents cj mk   features instance
divided two types      features describe mk  i e  shown second block
table         cluster level features  describe relationship cj
mk   cluster level feature created feature employed mention pair
model applying logical predicate  example  given number feature  i e   feature
    table     determines whether two mentions agree number  apply
predicate create cluster level feature value yes mk agrees
number mentions cj otherwise  motivated previous work  luo
et al         culotta  wick    mccallum        yang et al          create cluster level
features mention pair features using four commonly used logical predicates  none 
most false  most true  all  specifically  feature x shown last two
blocks table    first convert x equivalent set binary valued features
multi valued  then  resulting binary valued feature xb   create four binaryvalued cluster level features      none xb true xb false mk
mention cj       most false xb true xb true mk less half
 but least one  mentions cj       most true xb true xb true
mk least half  but all  mentions cj       all xb true xb
true mk mention cj   hence  xb   exactly one four
cluster level features evaluates true  
following yang et al          create     positive instance anaphoric mention
mk preceding cluster cj belongs      negative instance mk
paired preceding cluster whose last mention appears mk closest
antecedent  i e   last mention cj    consider running example  three
training instances generated he  i  monday   he   i  secretary state   he  
i  barack obama  his   he   first two instances labeled negative 
last one labeled positive  mention pair model  train
entity mention model using svm learner 
since entity mention model classifier  use svmlight classification
mode  resulting constrained optimization problem essentially optimization problem    except training example xi represents active mention
one preceding clusters rather two mentions 
   note cluster level feature represented probabilistic feature  specifically  recall
four logical predicates partitions       interval  predicate evaluates true given
cluster level feature depends probability obtained computation feature  instead
applying logical predicates convert probability one four discrete values 
simply use probability value cluster level feature  however  choose employ
probabilistic representation  preliminary experiments indicated using probabilistic features
yielded slightly worse results using logical features 

   

firahman   ng

training  resulting classifier used identify preceding cluster mention
test text  specifically  mentions processed left to right manner 
active mention mk   test instance created mk preceding clusters
formed far  test instances presented classifier  finally  adopt
closest first clustering regime  linking mk closest preceding cluster classified
coreferent mk   mk classified coreferent preceding cluster 
considered non anaphoric  note partial clusters preceding mk formed
incrementally based predictions classifier first k   mentions 
gold standard coreference information used formation 
    mention ranking model
noted before  ranking model imposes ranking candidate antecedents
active mention mk   train ranking model  use svm ranker learning algorithm
joachimss        svmlight package 
mention pair model  training instance i mj   mk   represents mk
preceding mention mj   fact  features represent instance method
creating training instances identical employed mention pair model 
difference lies labeling training instances  assuming sk set
training instances created anaphoric mention mk   rank value i mj   mk   sk
rank mj among competing candidate antecedents    mj closest
antecedent mk     otherwise   exemplify  consider running example 
mention pair model  three training instances generated he  i monday 
he   i secretary state  he   i his  he   third instance rank value   
remaining two rank value   
first glance  seems training set generated learning mentionranking model  identical one learning mention pair model  instance
represents two mentions labeled one two possible values  since previous work
ranking based coreference resolution attempt clarify difference
two  believe could difficult reader appreciate idea using
ranking coreference resolution 
let us first describe difference classification ranking high level 
beginning training sets employed mention ranking model mentionpair model  difference label associated instance training
mention ranking model rank value  whereas label associated instance
training mention pair model class value  specifically  since ranking svm
learns rank set candidate antecedents  relative ranks two candidates 
rather absolute rank candidate  matter training process 
words  point view ranking svm  training set instance   
rank value   instance    rank value   functionally equivalent one
   rank value       rank value    assuming remaining
instances generated anaphor two training sets identical
rank value      
   larger rank value implies better rank svmlight  

   

fia cluster ranking approach coreference resolution

next  take closer look ranker training process  denote training set
created described   addition  assume instance
denoted  xjk   yjk    xjk feature vector created anaphoric mention
mk candidate antecedent mj   yjk rank value  training ranker 
svm ranker learning algorithm derives training set original training set
follows  specifically  every pair training instances  xik   yik    xjk   yjk  
yik    yjk   create new training instance  xijk   yijk     xijk   xik xjk  
yijk           xik larger rank value xjk  and   otherwise   way 
creation resembles connolly et al s        pairwise ranking approach saw
section    convert ranking problem pairwise classification problem  
goal ranker learning algorithm  then  find hyperplane minimizes
number misclassifications   note since yijk          class value
instance depends relative ranks two candidate antecedents 
absolute rank values 
given conversion ranking problem pairwise classification problem 
constrained optimization problem svm ranker learning algorithm attempts
solve  described below  similar optimization problem   
optimization problem    soft margin svm ranking
x
 
ijk
arg min kwk    c
 
subject
yijk  w  xik xjk   b    ijk  
ijk non negative slack variable represents degree misclassification
xijk   c regularization parameter balances training error margin size 
two points deserve mention  first  optimization problem equivalent one
classification svm pairwise difference feature vectors xik xjk   result 
training algorithm used solve optimization problem   applicable
optimization problem  second  number linear inequality constraints
generated document optimization problems training mention pair
model entity mention model quadratic number mentions d 
number constraints generated ranking svm cubic number mentions 
since instance represents three  rather two  mentions 
training  mention ranking model applied rank candidate antecedents
active mention test text follows  given active mention mk   follow denis
baldridge        use independently trained classifier determine whether mk
non anaphoric  so  mk resolved  otherwise  create test instances mk
pairing preceding mentions  test instances presented
ranker  computes rank value instance taking dot product
   main difference training set employed connolly et al s approach
  instance formed taking difference feature vectors two instances   whereas
connolly et al s training set  instance formed concatenating feature vectors two
instances  

   

firahman   ng

instance vector weight vector  preceding mention assigned largest
value ranker selected antecedent mk   ties broken preferring
antecedent closest distance mk  
anaphoricity classifier used resolution step trained using publicly available
implementation   maximum entropy  maxent  modeling  instance corresponds
mention represented    features deemed useful distinguishing
anaphoric non anaphoric mentions  see table   details   linguistically 
features broadly divided three types  string matching  grammatical 
semantic  either relational feature  compares mention one
preceding mentions  non relational feature  encodes certain linguistic property
mention whose anaphoricity determined  e g   np type  number  definiteness  

   coreference cluster ranking
section  describe cluster ranking approach np coreference  noted
before  approach aims combine strengths entity mention model
mention ranking model 
    training applying cluster ranker
ease exposition  describe subsection train apply clusterranking model used pipeline architecture  anaphoricity determination
performed prior coreference resolution  next subsection  show
two tasks learned jointly 
recall cluster ranking model ranks set preceding clusters active
mention mk   since cluster ranking model hybrid mention ranking model
entity mention model  way trained applied hybrid
two  particular  instance representation employed cluster ranking model
identical used entity mention model  training instance i cj   mk  
represents preceding cluster cj anaphoric mention mk consists clusterlevel features formed predicates  unlike entity mention model  however 
cluster ranking model      training instance created anaphoric mention mk
preceding clusters      since training model ranking clusters 
assignment rank values training instances similar mention ranking
model  specifically  rank value training instance i cj   mk   created mk
rank cj among competing clusters    mk belongs cj     otherwise 
train cluster ranking model  use svm learner ranking mode  resulting
constrained optimization problem essentially optimization problem
   except training example xijk represents active mention mk two
preceding clusters  ci cj   rather two preceding mentions 
applying learned cluster ranker test text similar applying mentionranking model  specifically  mentions processed left to right manner 
active mention mk   first apply independently trained classifier determine mk
non anaphoric  so  mk resolved  otherwise  create test instances mk
    see http   homepages inf ed ac uk s        maxent toolkit html 

   

fia cluster ranking approach coreference resolution

feature type
lexical

feature
str match

head match

grammatical
 np type 

uppercase
definite
demonstrative
indefinite
quantified
article

grammatical
 np
property 
relationship

pronoun
proper noun
bare singular
bare plural
embedded
appositive
prednom
number

contains pn
grammatical
 syntactic
pattern 

n
 n
pn
pn n
adj n
num n
ne
sing n

semantic

alias

description
exists mention mj preceding mk that 
discarding determiners  mj mk string  else
n 
exists mention mj preceding mk mj
mk head  else n 
mk entirely uppercase  else n 
mk starts the  else n 
mk starts demonstrative this  that 
these  those  else n 
mk starts an  else n 
mk starts quantifiers every  some  all 
most  many  much  few  none  else n 
definite mk definite np  quantified mk quantified np  else indefinite 
mk pronoun  else n 
mk proper noun  else n 
mk singular start article  else n 
mk plural start article  else n 
mk prenominal modifier  else n 
mk first two mentions appositive
construction  else n 
mk first two mentions predicate nominal
construction  else n 
singular mk singular number  plural mk plural
number  unknown number information cannot
determined 
mk proper noun contains proper noun  else
n 
mk starts followed exactly one common
noun  else n 
mk starts followed exactly two common
nouns  else n 
mk starts followed exactly proper noun 
else n 
mk starts followed exactly proper noun
common noun  else n 
mk starts followed exactly adjective
common noun  else n 
mk starts followed exactly cardinal
common noun  else n 
mk starts followed exactly named entity 
else n 
mk starts followed singular np containing proper noun  else n 
exists mention mj preceding mk mj
mk aliases  else n 

table    feature set anaphoricity determination  instance represents single mention 
mk   characterized    features 
   

firahman   ng

pairing preceding clusters  test instances presented
ranker  mk linked cluster assigned highest value ranker  ties
broken preferring cluster whose last mention closest distance mk   note
partial clusters preceding mk formed incrementally based predictions
ranker first k   mentions 
    joint anaphoricity determination coreference resolution
cluster ranker described used determine preceding cluster
anaphoric mention linked to  cannot used determine whether mention anaphoric not  reason simple  training instances generated
anaphoric mentions  hence  jointly learn anaphoricity determination coreference
resolution  must train ranker using instances generated anaphoric
non anaphoric mentions 
specifically  training ranker  provide active mention option
start new cluster creating additional instance     contains features solely
describe active mention  i e   features shown second block table        
highest rank value among competing clusters  i e      non anaphoric
lowest rank value  i e      otherwise  main advantage jointly learning two tasks
allows ranking model evaluate possible options active mention
 i e   whether resolve it  so  preceding cluster best  simultaneously 
essentially method applied jointly learn two tasks mentionranking model 
training  resulting cluster ranker processes mentions test text
left to right manner  active mention mk   create test instances pairing
preceding clusters  allow possibility mk non anaphoric 
create additional test instance contains features solely describe active
mention  similar training step above   test instances
presented ranker  additional test instance assigned highest rank value
ranker  mk classified non anaphoric resolved  otherwise 
mk linked cluster highest rank  ties broken preferring
antecedent closest mk   before  partial clusters preceding mk formed
incrementally based predictions ranker first k   mentions 
finally  note model jointly learning anaphoricity determination coreference resolution different recent attempts perform joint inference anaphoricity
determination coreference resolution using integer linear programming  ilp  
anaphoricity classifier coreference classifier trained independently other 
ilp applied postprocessing step jointly infer anaphoricity coreference decisions consistent  e g   denis   baldridge      a  
joint inference different joint learning approach  allows two tasks
learned jointly independently 

   lexicalization coreference resolution
next  investigate role lexicalization  i e   use word pairs linguistic features 
learning based coreference resolution  motivation behind investigation two   

fia cluster ranking approach coreference resolution

fold  first  lexical features easy compute yet under investigated
coreference resolution  particular  attempts made employ
train mention pair model  e g   luo et al         daume iii   marcu       
bengtson   roth         contrast  want determine whether improve
performance cluster ranking model  second  mention pair model
mention ranking model compared respect non lexical feature set
 denis   baldridge      b         clear perform relative
trained lexical features  desire answer question 
allow us gain additional insights strengths weaknesses
learning based coreference models 
recall introduction previous attempts lexicalizing mention pair
model show lexical features best marginally useful  hence  one goals
determine whether make better use lexical features learning based
coreference resolver  particular  unlike aforementioned attempts lexicalization 
simply append word pairs conventional coreference feature set consisting
string matching  grammatical  semantic  distance  i e   proximity based  features  e g  
feature set shown table     investigate model exploits lexical features
combination small subset conventional coreference features 
would allow us better understanding significance conventional
features  example  features encode agreement gender  number  semantic
class two mentions employed virtually learning based coreference resolver 
never question whether better alternatives features  could
build lexicalized coreference model without commonly used features
observe performance deterioration  would imply conventional features
replaceable  prototypical way building learning based coreference
system 
question is  small subset conventional features use
combination lexical features  mentioned above  since one advantages
lexical features extremely easy compute  desire conventional
features easy compute  especially require dictionary
compute  see  choose use two features  alias feature
distance feature  see features       table     rely off the shelf named
entity  ne  recognizer compute ne types 
note  however  usefulness lexical features could limited part data
sparseness  many word pairs appear training data may appear test
data  employing conventional features described  e g   distance 
help alleviate problem  seek improve generalizability introducing
two types features  semi lexical unseen features  henceforth refer
feature set comprises two types features  lexical features  alias feature 
distance feature lexical feature set  addition  refer feature
set shown table   conventional feature set 
first describe lexical feature set training mention pair model
mention ranking model  section       that  show create cluster level
features feature set training entity mention model cluster ranking
   

firahman   ng

model  well issues training joint model anaphoricity determination coreference resolution  section      
    lexical feature set
unlike previous work lexicalizing learning based coreference models  lexical feature
set consists four types features  lexical features  semi lexical features  unseen features 
well two conventional features  namely  alias distance  
compute features  preprocess training text randomly replacing    
nominal mentions  i e   common nouns  label unseen  mention mk
replaced unseen  mentions string mk replaced
unseen  test text preprocessed differently  simply replace mentions whose
strings seen training data unseen  hence  artificially creating unseen
labels training text allow learner learn handle unseen words
test text  potentially improving generalizability 
preprocessing  compute features instance  assuming
training mention pair model mention ranking model  instance corresponds
two mentions  mj mk   mj precedes mk text  features
divided four groups  unseen  lexical  semi lexical  conventional  describing
features  two points deserve mention  first  least one mj mk unseen 
lexical  semi lexical  conventional features created them  since features
involving unseen mention likely misleading learner sense
may yield incorrect generalizations training set  second  since use svm
training testing  instance contain number features  unless otherwise
stated  feature value   
unseen feature  mj mk unseen  determine whether
string  so  create unseen same feature  otherwise  create unseendiff feature  one unseen  feature created 
lexical feature  create lexical feature mj mk   ordered
pair consisting heads mentions  pronoun common noun  head
assumed last word mention     proper noun  head taken
entire noun phrase 
semi lexical features  features aim improve generalizability  specifically 
exactly one mj mk tagged ne stanford ne recognizer  finkel et al  
       create semi lexical feature identical lexical feature described above 
except ne replaced ne label  i e   person  location  organization  
mentions nes  check whether string  so  create
feature  ne  same   ne  replaced corresponding ne label  otherwise 
check whether ne tag word subset match  i e   whether
    see evaluation section  mention extractor trained extract base nps  hence 
heuristic extracting head nouns arguably overly simplistic  applied
recursive nps  e g   nps contain prepositional phrases   phrases likely
make mistakes  however  desire better extraction accuracy  extract head nouns
syntactic parsers provide head information  collinss        parser 

   

fia cluster ranking approach coreference resolution

word tokens one mention appear others list tokens   so  create feature
 ne  subsame   ne  replaced ne label  otherwise  create feature
concatenation ne labels two mentions 
conventional features  improve generalizability  incorporate two easy to
compute features conventional feature set  alias distance 
    feature generation
lexical feature set training mention pair model mentionranking model  describe two extensions feature set needed
    train entity mention model cluster ranking model      perform joint
learning anaphoricity determination coreference resolution 
first extension concerns generation cluster level features entity mention
model cluster level model  recall section     create cluster level features given conventional feature set  first convert feature employed
mention pair model equivalent set binary valued features  create
cluster level feature resulting binary valued features  hand 
given lexical feature set  method producing cluster level features applicable two conventional features  i e   alias distance   appear
conventional feature set  unseen  lexical  semi lexical feature  create
feature active mention mention preceding cluster  described
section         value feature number times appears instance  encoding feature values frequency rather binary values allows us capture
cluster level information shallow manner 
second extension concerns generation features representing additional
instance created training joint version mention ranking model
cluster ranking model  recall section     conventional feature set
used  represented additional instance using features computed solely
active mention  hand  given lexical feature set  longer
use method representing additional instance  feature
lexical feature set computed solely active mention  result 
represent additional instance using one feature  null x  x head
active mention  help learner learn x likely non anaphoric 

   evaluation
evaluation driven following questions  focusing     comparison among
different learning based coreference models      effect lexicalization
models  specifically 
learning based coreference models  namely  mention pair model 
entity mention model  mention ranking model  cluster ranking model 
compare other 
    strictly speaking  resulting feature cluster level feature  computed active
mention one mentions preceding cluster 

   

firahman   ng

joint modeling anaphoricity determination coreference resolution offer
benefits pipeline architecture  anaphoricity performed prior
coreference resolution 
lexicalized coreference models perform better unlexicalized counterparts 
rest section  first describe experimental setup  section      
show performance four models  including effect lexicalization
joint modeling whenever applicable  three different feature sets  section      
    experimental setup
begin providing details data sets  automatic mention extraction
method  scoring programs 
      corpus
use ace      coreference corpus released ldc  consists    
training documents used official ace evaluation    ensure diversity  corpus
created selecting documents six different sources  broadcast news  bn   broadcast
conversations  bc   newswire  nw   webblog  wb   usenet  un   conversational
telephone speech  cts   number documents belonging source shown
table   
data set
  documents

bn
   

bc
  

nw
   

wl
   

un
  

cts
  

table    statistics ace      corpus

      mention extraction
evaluate coreference model using system mentions  extract system mentions
test text  trained mention extractor training texts  following florian
et al          recast mention extraction sequence labeling task  assign
token test text label indicates whether begins mention  inside
mention  outside mention  hence  learn extractor  create one training
instance token training text derive class value  one b  i  o 
annotated data  instance represents wi   token consideration 
consists    linguistic features  many modeled systems bikel 
schwartz  weischedel        florian et al          described below 
lexical     

tokens window     wi            wi     

capitalization      determine whether wi isallcap  isinitcap  iscapperiod 
isalllower 
    since participate ace       access official test set 

   

fia cluster ranking approach coreference resolution

morphological      wi prefixes suffixes length one  two  three  four 
grammatical      part of speech  pos  tag wi obtained using stanford loglinear pos tagger  toutanova  klein  manning    singer        
semantic      named entity  ne  tag wi obtained using stanford crf based
ne recognizer  finkel et al         
dictionaries      employ eight dictionary based features indicate presence
absence wi particular dictionary  eight dictionaries contain pronouns    
entries   common words words names       k   person names      k  
person titles honorifics        vehicle words        location names     k   company
names      k   nouns extracted wordnet hyponyms person     k  
employ crf       c   implementation conditional random fields  training
mention detector training set  overall  detector achieves f measure
           recall       precision  test set  extracted mentions used
system mentions coreference experiments 
      scoring programs
score output coreference model  employ two scoring programs  b   bagga
  baldwin           ceaf    luo         address inherent weaknesses
muc scoring program  vilain  burger  aberdeen  connolly    hirschman          
b  ceaf score response  i e   system generated  partition  r  key
 i e   gold standard  partition  k  report coreference performance terms recall 
precision  f measure  b  first computes recall precision mention  mk  
follows 
recall mk    

 rmk kmk  
 rmk kmk  
  precision mk    
 
 kmk  
 rmk  

rmk coreference cluster containing mk r  kmk coreference cluster
containing mk k  computes overall recall  resp  precision  averaging
per mention recall  resp  precision  scores 
hand  ceaf first constructs optimal one to one mapping
clusters key partition response partition  specifically  assume
k    k    k            km   set clusters key partition  r    r    r            rn  
set clusters response partition  compute recall  ceaf first computes
score cluster  ki   k follows 
score ki      ki rj   
    available http   crfpp sourceforge net
    ceaf two versions     ceaf    ceaf  two versions differ similarity two
aligned clusters computed  refer reader luos        paper details     ceaf chosen
commonly used version ceaf 
    briefly  muc scoring program suffers two often cited weaknesses  first  link based measure 
reward successful identification singleton clusters  since mentions clusters
linked mentions  second  tends under penalize partitions overly large clusters 
see work bagga baldwin         luo         recasens hovy        details 

   

firahman   ng

rj cluster ki mapped optimal one to one mapping 
constructed efficiently using kuhn munkres algorithm  kuhn         note
ki mapped cluster r  score ki        ceaf computes recall
summing score cluster k dividing sum number mentions
k  precision computed manner  except reverse roles
k r 
complication arises b  used score response partition containing system
mentions  recall b  constructs mapping mentions response
key  hence  response generated using gold standard mentions 
every mention response mapped mention key vice versa 
words  twinless  i e   unmapped  mentions  stoyanov et al         
case system mentions used  original description b 
specify twinless mentions scored  bagga   baldwin         address
problem  set per mention recall precision twinless mention zero 
regardless whether mention appears key response  note ceaf
compare partitions twinless mentions without modification  since operates
aligning clusters  mentions 
additionally  apply preprocessing step response partition scoring it 
remove twinless system mentions singletons  reason
simple  since coreference resolver successfully identified mentions singletons 
penalized  removing allows us avoid penalty  note
remove twinless  as opposed all  system mentions singletons  allows
us reward resolver successful identification singleton mentions twins 
hand  retain     twinless system mentions non singletons  as
resolver penalized identifying spurious coreference relations      twinless
mentions key partition  as want ensure resolver makes correct
coreference non coreference decisions them    
    results
showing results learning based coreference models  let us consider head
match baseline  commonly used heuristic baseline coreference resolution 
posits two mentions coreferent head nouns match  head nouns
determined described section      head proper noun string entire
mention  whereas head pronoun common noun last word mention 
since one goals examine effect lexicalization coreference model 
head match baseline provide information well one simplest
kinds string matching  results baseline  shown row   table    expressed
terms recall  r   precision  p   f measure  f  obtained via b  ceaf 
see table    baseline achieves f measure scores           according
b  ceaf  respectively 
    addition method described here  number methods proposed address
mapping problem  refer reader work enrique  gonzalo  artiles  verdejo        
stoyanov et al          cai strube        details 

   

fia cluster ranking approach coreference resolution

next  train evaluate learning based coreference models using five fold cross
validation  data set si shown table    partition documents si
five folds approximately equal size  si            si    train coreference model
four folds use generate coreference chains documents remaining
fold  repeating step five times fold used test fold exactly once 
that  apply b  ceaf entire set automatically coreference annotated
documents obtain scores table    discuss results learningbased coreference models obtained used combination three feature sets 
conventional feature set  section         lexical feature set  section        
combined feature set  composed features conventional lexical
 section        
      results using conventional features
gauge performance cluster ranking model  employ baselines mentionpair model  entity mention model  mention ranking model 
mention pair baseline  train first learning based baseline  mentionpair model  using svm learning algorithm implemented svmlight package   
see row   table    mention pair model achieves f measure scores
      b          ceaf   represent statistically significant improvement     
     f measure corresponding results head match baseline   
entity mention baseline  next  train second learning based baseline 
entity mention model  using svm learner  see row   table   
baseline achieves f measure scores       b          ceaf   represent small
statistically significant improvements mention pair model  significant performance difference perhaps particularly surprising given improved expressiveness
entity mention model mention pair model 
mention ranking baseline  third baseline mention ranking model 
trained using ranker learning algorithm svmlight   identify non anaphoric mentions  employ two methods  first method  follow denis baldridge       
adopt pipeline architecture  train maxent classifier anaphoricity determination independently mention ranker training set using    features
described section      apply resulting classifier test text filter nonanaphoric mentions prior coreference resolution  results pipeline mention ranker
shown row   table    see  ranker achieves f measure scores     
 b          ceaf   yielding significant performance deterioration comparison
entity mention baseline 
second method  perform anaphoricity determination jointly coreference
resolution using method described section      discussed joint learning
method context cluster ranking  easy see method
equally applicable mention ranking model  results mention ranker using
    subsequent uses svm learner  set parameters default values 
particular  employ linear kernel obtain results article 
    statistical significance results article obtained using paired t test  p        

   

firahman   ng

r
    

b 
p
    

ceaf
p
f
         

 

coreference model
head match

f
    

r
    

 
 
 
 
 
 

using conventional feature set
mention pair model
              
entity mention model
              
mention ranking model  pipeline 
              
mention ranking model  joint 
              
cluster ranking model  pipeline 
              
cluster ranking model  joint 
              

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

 
 
  
  
  
  

using lexical feature set
mention pair model
              
entity mention model
              
mention ranking model  pipeline 
              
mention ranking model  joint 
              
cluster ranking model  pipeline 
              
cluster ranking model  joint 
              

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

  
  
  
  
  
  

using combined
mention pair model
    
entity mention model
    
mention ranking model  pipeline 
    
mention ranking model  joint 
    
cluster ranking model  pipeline 
    
cluster ranking model  joint 
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

feature
    
    
    
    
    
    

set
    
    
    
    
    
    

table    five fold cross validation coreference results obtained using b  ceaf 
best f measure achieved feature set scoring program combination boldfaced 

joint architecture shown row   table    see  ranker achieves fmeasure scores       b          ceaf   represent significant improvements
entity mention model pipeline counterpart  results
demonstrate superiority joint mention ranking model entity mention model 
substantiate hypothesis joint modeling offers benefits pipeline modeling 
cluster ranking model  finally  evaluate cluster ranking model 
mention ranking baselines  employ pipeline architecture joint architecture anaphoricity determination  results shown rows     table   
respectively  two architectures  see  pipeline architecture yields fmeasure scores       b          ceaf   represent significant improvement
mention ranker adopting pipeline architecture  joint architecture 
cluster ranker achieves f measure scores       b          ceaf   rep   

fia cluster ranking approach coreference resolution

resents significant improvement mention ranker adopting joint architecture 
best baselines  taken together  results demonstrate superiority
cluster ranker mention ranker  finally  fact joint cluster ranker performs
significantly better pipeline counterpart provides empirical support
benefits joint modeling pipeline modeling 
      results using lexical features
next  evaluate learning based coreference models using lexical features  results
shown rows     table    comparison results obtained using conventional features  see different trend  joint mention ranking model replaces
cluster ranking model best performing model  moreover  improvement
second best performing model  entity mention model according b 
pipeline mention ranking model according ceaf  statistically significant regardless
scoring program used  closer examination results reveals employing
lexical rather conventional features substantially improves performance
mention ranking model  comparison unlexicalized joint mention ranking model
 row     f measure scores lexicalized joint mention ranking model  row     rise
      b          ceaf   increase f measure attributed primarily
substantial rise recall  even though large increase ceaf precision 
besides joint mention ranking model  mention pair model entity mention
model benefit substantially conventional features replaced lexical features  see f measure scores increase       b          ceaf 
mention pair model        b          ceaf  entity mention model 
gains f measure two models attributed large increases
recall precision  hand  joint cluster ranking model always
improve replace conventional features lexical features  fact 
performance difference cluster ranking model entity mention model
statistically indistinguishable  finally  see benefits jointly learning anaphoricity
determination coreference resolution again  joint version mentionranking model used rather pipeline version  compare rows        
f measure scores rise significantly       b          ceaf   similarly clusterranking model  joint version improves pipeline version significantly       b   
      ceaf  f measure 
overall  results somewhat unexpected  recall lexical features
knowledge lean  consisting lexical  semi lexical  unseen features  well
two conventional features  particular  employ conventional coreference
features encode agreement gender number  implies many existing
implementations mention pair model  entity mention model  mentionranking model  unlexicalized rely heavily conventional features 
making effective use labeled data  perhaps importantly  results indicate
coreference models perform well  and fact better  even without conventional
coreference features  since lexical computed extremely easily 
readily applied languages  another advantage feature set 
hand  interesting see versions cluster ranking model exhibit
   

firahman   ng

less dramatic changes performance replace conventional features
lexical features 
      results using combined features
since conventional features lexical features represent two fairly different sources
knowledge  examine whether improve coreference models combining
two feature sets  results coreference models using combined features
shown rows      table    results exhibit essentially trend
obtained conventional features  joint cluster ranking model performing
best mention pair model performing worst  fact  joint cluster ranking
model yields significantly better performance used combined features
conventional features lexical features alone  similarly pipeline
cluster ranking model  achieves significantly better performance combined
features conventional lexical features  results seem suggest
cluster ranking model able exploit potentially different sources information
provided two feature sets improve performance  addition  demonstrate
benefits joint modeling  mention ranking model  joint version improves
pipeline version significantly       b          ceaf  f measure 
cluster ranking model  joint version improves pipeline counterpart significantly
      b          ceaf  f measure 
remaining coreference models exhibit drop performance combined
features used lieu lexical features  results seem suggest
cluster ranking model offers robust performance face changes underlying feature set coreference models  feature selection  issue
under explored coreference resolution  may crucial employ coreference models    perhaps importantly  despite fact conventional features
lexical features represent two fairly different sources information 
cluster ranking model unable exploit potentially richer amount information
contained combined feature set  hence  virtually linguistic features
recently developed supervised coreference resolution evaluated using
mention pair model  see  example  work strube  rapp    muller        ji 
westbrook    grishman        ponzetto   strube         utility features may
better demonstrated using cluster ranking model 
natural question is  joint cluster ranking model compare existing
coreference systems  since participate ace evaluations 
access official test sets compare model ace
participating coreference systems  comparison complicated fact
existing coreference systems evaluated different data sets  including two
muc data sets  muc          muc          various ace data sets  e g   ace   
ace       ace       ace        well different partitions given data set 
knowledge  coreference model evaluated test
data haghighi kleins        unsupervised coreference model  model
    fact  ng cardie      b   strube muller         ponzetto strube        show
mention pair model improved using feature selection 

   

fia cluster ranking approach coreference resolution

recently shown surpass performance stoyanov et al s        system 
one best existing implementations mention pair model  test
data  haghighi kleins model achieves b  f measure       achieves
b  f measure         results provide suggestive evidence cluster ranking
model achieves performance comparable one best existing coreference
models 
nevertheless  caution results allow one claim anything
fact model compares favorably haghighi kleins        model 
instance  one cannot claim model better achieves level
performance without using labeled data  reasons     mentions
used two models coreference process extracted differently    
linguistic features employed two models way features computed
different other  since previous work shown linguistic
preprocessing steps considerable impact performance resolver  barbu
  mitkov        stoyanov et al          possible one model employed features
mentions model currently using  results would different 
hence  one fairly compare two coreference models  evaluated
set mentions  rather set documents  given access
set knowledge sources  essentially way compare various
learning based coreference models article 

   experimental analyses
attempt gain insights different aspects coreference models 
conduct additional experiments analyses  rather report five fold cross validation
results  section report results one fold  i e   fold designate test
set  use remaining four folds solely training 
    improving classification based coreference models
given generally poorer performance classification based coreference models  natural question is  improved  answer question  investigate whether
models improved employing different clustering algorithm different
learning algorithm  reasons decision focus two dimensions 
first  noted introduction  one weaknesses models
clear clustering algorithm offers best performance  given observation 
examine whether improve models replacing soon et al s       
closest first linking regime best first linking strategy  shown
offer better performance mention pair model muc data sets  ng   cardie 
    b   second  discussed end section    may able achieve
advantage ranking classification based models employing learning algorithm
optimizes conditional probabilities instead     decisions  motivated observation  examine whether improve classification based models training
using maxent  employs likelihood based loss function  note maxent one
    note haghighi klein report ceaf scores paper 

   

firahman   ng

popular learning algorithms training coreference models  see  example 
morton        kehler  appelt  taylor    simma        ponzetto   strube        denis  
baldridge        finkel   manning        ng        
evaluate two modifications  apply isolation combination
two classification based models  i e   mention pair model entity mention
model  trained using three different feature sets  i e   conventional  lexical  combined   train maxent based coreference models using yasmet    
follow ng cardies      b  implementation best first clustering algorithm 
specifically  among candidate antecedents preceding clusters classified
coreferent active mention mk   best first clustering links mk likely one 
maxent model  pair classified coreferent classification value
     likely antecedent preceding cluster mk one
highest probability coreference mk   svmlight  trained model  pair
classified coreferent classification value    likely
antecedent preceding cluster mk one positive classification
value 
table   presents b  ceaf results two classification based coreference models
trained using two learning algorithms  i e   svm maxent  used
combination two clustering algorithms  i e   closest first clustering best first
clustering   study choice clustering algorithm impacts performance 
compare results closest first clustering best first clustering table  
combination learning algorithm  feature set  coreference model  scoring
program  instance  comparing rows     table   enables us examine
two clustering algorithms better mention pair model trained
conventional feature set two learners  overall  see fairly consistent
trend  best first clustering yields results slightly worse obtained using
closest first clustering  regardless choice clustering algorithm  learning
algorithm  feature set  scoring program  first glance  results seem
contradictory ng cardie      b   demonstrate superiority bestfirst clustering closest first clustering coreference resolution  speculate
contradictory results attributed two reasons  first  best first clustering
experiments  still employed soon et al s        training instance selection method 
created positive training instance anaphoric mention closest
antecedent preceding cluster  unlike ng cardie  claim proposed bestfirst clustering successful  however  different method training instance selection
would needed  particular  propose use confident antecedent 
rather closest antecedent  generate positive instances anaphoric mention 
second  ng cardie demonstrate success best first clustering muc data
sets  possible success may carry ace data sets  additional
experiments needed determine reason  however 
    see http   www fjoch com yasmet html  reason yasmet chosen provides
capability rank  allows us compare results maxent trained classification models
ranking models  see work ravichandran  hovy  och        discussion differences
training two types maxent models 

   

fia cluster ranking approach coreference resolution

coreference model

r

svm
p

f

r

maxent
p
f

 
 
 
 

b  results using conventional feature
mention pair model  closest first 
              
mention pair model  best first 
              
entity mention model  closest first                
entity mention model  best first 
              

set
    
    
    
    

    
    
    
    

    
    
    
    

 
 
 
 

b  results using lexical feature set
mention pair model  closest first 
              
mention pair model  best first 
              
entity mention model  closest first                
entity mention model  best first 
              

    
    
    
    

    
    
    
    

    
    
    
    

combined feature set
              
    
              
    
                   
              
    

    
    
    
    

    
    
    
    

  
  
  
  

ceaf results using conventional feature set
mention pair model  closest first 
              
    
mention pair model  best first 
              
    
entity mention model  closest first                
    
entity mention model  best first 
              
    

    
    
    
    

    
    
    
    

  
  
  
  

ceaf results using
mention pair model  closest first 
mention pair model  best first 
entity mention model  closest first 
entity mention model  best first 

    
    
    
    

    
    
    
    

  
  
  
  

ceaf results using combined
mention pair model  closest first 
         
mention pair model  best first 
         
entity mention model  closest first           
entity mention model  best first 
         

    
    
    
    

    
    
    
    

 
  
  
  

b  results using
mention pair model  closest first 
mention pair model  best first 
entity mention model  closest first 
entity mention model  best first 

 a  b  results

lexical feature set
              
    
              
    
                   
              
    
feature
    
    
    
    

set
    
    
    
    

 b  ceaf  results

table    svm vs  maxent results classification based coreference models  one fold
b  ceaf scores obtained training coreference models using svm maxent 
best f measure achieved feature set scoring program combination boldfaced 
   

firahman   ng

next  examine whether minimizing likelihood based loss via maxent training instead
svms classification loss would enable us achieve advantage ranking
 and hence leads better performance   compare two columns table   
see  conventional feature set used  maxent outperforms svm  regardless
choice clustering algorithm  scoring program  coreference model 
hand  lexical features combined features used  svm outperforms
maxent consistently  overall  mixed results seem suggest whether maxent
offers better performance svm extent dependent underlying feature
set 
    performance maximum entropy based ranking models
prior work suggests maxent based ranking may provide better gains svmbased ranking  since generate reliable confidence values dynamically adjust
relative ranks according baseline results  e g   ji  rudin    grishman         determine whether case coreference resolution  conduct experiments
train ranking based coreference models using ranker learning algorithm yasmet 
b  ceaf results mention ranking model cluster ranking model
trained using maxent combination three different feature sets  i e   conventional 
lexical  combined  shown maxent column table    comparison 
show corresponding results obtained via svm based ranking table
 see svm column   comparing two columns  see mixed results    
experiments involve ranking models  maxent based ranking outperforms svm based
ranking six them  words  results suggest coreference task 
svm based ranking generally better maxent based ranking 
    accuracy anaphoricity determination
section      saw joint ranking model always performs significantly better
pipeline counterpart  words  joint modeling coreference anaphoricity
improves coreference resolution  natural question is  joint modeling improve
anaphoricity determination 
answer question  measure accuracy anaphoricity information resulting pipeline modeling joint modeling  recall pipeline modeling  rely
output anaphoricity classifier trained independently coreference
system uses anaphoricity information  see section       accuracy classifier test set shown acc column row   table    addition 
show table recall  r   precision  p   f measure  f  identifying anaphoric
mentions  see  classifier achieves accuracy      f measure score
     
hand  joint modeling  compute accuracy anaphoricity
determination output joint coreference model  specifically  given output
joint model  determine mentions resolved preceding antecedent
not  assuming mention resolved anaphoric one
resolved non anaphoric  compute accuracy anaphoricity determination
   

fia cluster ranking approach coreference resolution

coreference model

r

svm
p

f

r

maxent
p
f

 
 
 
 

b  results using conventional feature
mention ranking model  pipeline 
              
mention ranking model  joint 
              
cluster ranking model  pipeline 
              
cluster ranking model  joint 
              

set
    
    
    
    

    
    
    
    

    
    
    
    

 
 
 
 

b  results using lexical feature set
mention ranking model  pipeline 
              
mention ranking model  joint 
              
cluster ranking model  pipeline 
              
cluster ranking model  joint 
              

    
    
    
    

    
    
    
    

    
    
    
    

combined feature set
              
    
              
    
              
    
                   

    
    
    
    

    
    
    
    

conventional feature set
              
    
              
    
              
    
                   

    
    
    
    

    
    
    
    

 
  
  
  

b  results using
mention ranking model  pipeline 
mention ranking model  joint 
cluster ranking model  pipeline 
cluster ranking model  joint 

 a  b  results

  
  
  
  

ceaf results using
mention ranking model  pipeline 
mention ranking model  joint 
cluster ranking model  pipeline 
cluster ranking model  joint 

  
  
  
  

ceaf results using
mention ranking model  pipeline 
mention ranking model  joint 
cluster ranking model  pipeline 
cluster ranking model  joint 

lexical feature set
              
    
                   
              
    
              
    

    
    
    
    

    
    
    
    

  
  
  
  

ceaf results
mention ranking model  pipeline 
mention ranking model  joint 
cluster ranking model  pipeline 
cluster ranking model  joint 

combined feature set
              
              
              
              

    
    
    
    

    
    
    
    

    
    
    
    

 b  ceaf results

table    svm vs  maxent results ranking basd coreference models  one fold b 
ceaf scores obtained training coreference models using svm maxent  best
f measure achieved feature set scoring program combination boldfaced 
   

firahman   ng

 
 
 
 
 
 
 

source anaphoricity information
anaphoricity classifier
mention ranking  conventional 
cluster ranking  conventional 
mention ranking  lexical 
cluster ranking  lexical 
mention ranking  combined 
cluster ranking  combined 

acc
    
    
    
    
    
    
    

r
    
    
    
    
    
    
    

p
    
    
    
    
    
    
    

f
    
    
    
    
    
    
    

table    anaphoricity determination results 

well precision  recall  f measure identifying anaphoric mentions  since
performance numbers derived output joint model  compute
two joint ranking models  i e   mention ranking model
cluster ranking model  used combination three coreference feature
sets  i e   conventional  lexical  combined   results six sets performance
numbers  shown rows    table    see  accuracies range
           f measure scores range           
comparison results anaphoricity classifier shown row    see
joint modeling improves performance anaphoricity determination except two
cases  namely  mention ranking conventional mention ranking combined 
words  two cases  joint modeling benefits coreference resolution anaphoricity determination  seems counter intuitive one achieve better coreference
performance lower accuracy determining anaphoricity  difficult
see reason  joint model trained maximize pairwise ranking accuracy 
presumably correlates coreference performance  whereas anaphoricity classifier trained maximize accuracy determining anaphoricity mention 
may always correlation coreference performance  words 
improvements anaphoricity accuracy generally necessarily imply corresponding
improvements clustering level coreference accuracy 
finally  important bear mind conclusions drawn regarding
pipeline joint modeling based results anaphoricity classifier trained
   features  possible different conclusions could drawn trained
anaphoricity classifier different set features  therefore  interesting future direction
would improve anaphoricity classifier employing additional features 
proposed uryupina         may able derive sophisticated features
harnessing recent advances lexical semantics research  specifically using methods
phrase clustering  e g   lin   wu         lexical chain discovery  e g   morris   hirst 
       paraphrase discovery  see survey papers androutsopoulos   malakasiotis 
      madnani   dorr        
   

fia cluster ranking approach coreference resolution

    joint inference versus joint learning mention pair model
mentioned end section      joint modeling anaphoricity determination
coreference resolution fundamentally different joint inference two tasks 
recall joint inference using ilp  anaphoricity classifier coreference classifier
trained independently other  ilp applied postprocessing step
jointly infer anaphoricity coreference decisions consistent
 e g   denis   baldridge      a   subsection  investigate joint learning
compares joint inference anaphoricity determination coreference resolution 
let us begin overview ilp approach proposed denis baldridge
     a  joint inference anaphoricity determination coreference resolution 
ilp approach motivated observation output anaphoricity model
coreference model given document satisfy certain constraints 
instance  coreference model determines mention mk coreferent
mentions associated text  anaphoricity model determine
mk non anaphoric  practice  however  since two models trained independently
other  constraints cannot enforced 
denis baldridge      a  provide ilp framework jointly determining anaphoricity coreference decisions given set mentions based probabilities provided
anaphoricity model pa mention pair coreference model pc  
resulting joint decisions satisfy desired constraints respecting much possible probabilistic decisions made independently trained pa pc   specifically  ilp program composed objective function optimized subject
set linear constraints  created test text follows  let
set mentions d  p set mention pairs formed  i e   p  
  mj   mk     mj   mk m  j   k    ilp program set indicator variables 
case  one binary valued variable anaphoricity decision coreference
decision made ilp solver  following denis baldridges notation  use
yk denote anaphoricity decision mention mk   xhj ki denote coreference
decision involving mentions mj mk   addition  variable associated
assignment cost  specifically  let cc
hj ki   log pc  mj   mk    cost setting xhj ki
c
   chj ki   log   pc  mj   mk    complementary cost setting xhj ki   
similarly define cost associated yk   letting ca
k   log pa  mk   
 

log  

p
 m
  


complementary
cost setting
cost setting yk    ca

k
k
yk    given costs  aim optimize following objective function 
min

x

c
cc
hj ki xhj ki   chj ki    xhj ki    

x


ca
k yk   ck    yk  

mk

 mj  mk  p

subject set manually specified linear constraints  denis baldridge specify four
types constraints      indicator variable take value          mj
mk coreferent  xhj ki      mk anaphoric  yk          mk anaphoric  yk     
must coreferent preceding mention mj       mk non anaphoric 
cannot coreferent mention 
two points deserve mention  first  minimizing objective function  since
assignment cost expressed negative logarithm value  second  since transitivity
   

firahman   ng

guaranteed constraints     use closest link clustering algorithm
put two mentions posited coreferent cluster  note
best link clustering strategy applicable here  since binary decision assigned
pair mentions ilp solver  use lp solve     publicly available ilp solver 
solve program 
b  ceaf results performing joint inference outputs anaphoricity
model mention pair model using ilp shown joint inference column
tables  a  b  respectively  rows correspond results obtained training
coreference models different feature sets  since one goals compare joint
inference joint learning  show joint learning column results
joint mention ranking model  anaphoricity determination coreference resolution
learned joint fashion  note reason using mention ranking model
 rather cluster ranking model  joint model want ensure
fair comparison joint learning joint inference much possible  chosen
cluster ranking model joint model  difference joint learning results
joint inference results could caused increased expressiveness
cluster ranking model  finally  better understand whether mention pair model
benefits joint inference using ilp  show inference column relevant
mention pair model results table    output model postprocessed
inference mechanism 
table    see joint learning results substantially better
joint inference results  except one case  conventional ceaf   two achieve
comparable performance  previous work roth        roth yih       
suggested often effective learn simple local models use complicated
integration strategies make sure constraints output satisfied learn
models satisfy constraints directly  results imply true
coreference task 
comparing joint inference inference results table    see
mention pair model benefit application ilp  fact  performance
deteriorates ilp used  results inconsistent reported denis
baldridge      a   show joint inference using ilp improve mentionpair model  speculate inconsistency accures fact denis
baldridge evaluate ilp approach true mentions  i e   gold standard mentions  
evaluate system mentions  additional experiments needed determine
reason  however 
    data source adaptability
one may argue since train test model documents data
source  i e   model trained documents bc tested documents
    finkel manning        show formulate linear constraints ilp solver outputs
coreference decisions satisfy transitivity  however  since number additional constraints needed
guarantee transitivity grows cubically number mentions previous work shows
additional constraints yield substantial performance improvements applied
system mentions  ng         decided employ experiments 
    available http   lpsolve sourceforge net 

   

fia cluster ranking approach coreference resolution

 
 
 

feature set
conventional
lexical
combined

joint learning
r
p
f
              
              
              

joint inference
r
p
f
              
              
              


r
    
    
    

inference
p
f
         
         
         


r
    
    
    

inference
p
f
         
         
         

 a  b  results

 
 
 

feature set
conventional
lexical
combined

joint learning
r
p
f
              
              
              

joint inference
r
p
f
              
              
              

 b  ceaf results

table    joint learning vs  joint inference results  joint modeling results obtained
using mention ranking model  joint inference results obtained applying ilp
anaphoricity classifier mention pair model  inference results produced
mention pair model  coreference models trained using maxent 

bc  example   surprising lexicalization helps  since word pairs
training set likely found test set training test texts
data source  examine whether models employ lexical features
suffer trained tested different data sources  perform set data
source adaptability experiments  apply coreference model trained
lexical features documents one data source documents data sources 
here  show results obtained using mention ranking model  primarily
yielded best performance lexical features among learning based coreference
models  comparison  show data source adaptability results obtained using
mention ranking model trained  non lexical  conventional feature set 
b  ceaf f measure scores experiments shown tables  a
 b  left half right half table contain lexicalized mention ranking
model results unlexicalized mention ranking model results  respectively  row
corresponds data source model trained  except last two rows 
explain shortly  column corresponds test set particular data
source 
answer question whether performance coreference model employs
lexical features deteriorate trained tested different data sources 
look diagonal entries left half tables  a  b  contain
results obtained lexicalized mention ranking model trained tested
documents source  model indeed performs worse trained
tested documents different sources  diagonal entry contain
highest score among entries column  see left half
two tables  large extent correct  four six diagonal entries contain
highest scores respective columns according scoring programs  provides
   

firahman   ng

lexical features
pp
pp test
train ppp
p
bc
bn
cts
nw
un
wl
maxmin
std  dev 

conventional features

bc

bn

cts nw un

wl

bc

bn

cts nw un

wl

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

 a  b  results

lexical features
pp
pp test
train ppp
p
bc
bn
cts
nw
un
wl
maxmin
std  dev 

conventional features

bc

bn

cts nw un

wl

bc

bn

cts nw un

wl

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

    
    
    
    
    
    
   
    

 b  ceaf results

table    results data source adaptability  row shows results obtained training
mention ranking model data set shown first column row  column
corresponds test set particular data source  best result obtained test set
two coreference models boldfaced 

suggestive evidence answer question affirmative  nevertheless  look
right half two tables  show results obtained using unlexicalized
mention ranking model  see similar  perhaps weaker  trend  according ceaf 
four six diagonal entries contain highest scores respective columns 
according b    two six diagonal entries exhibit trend  hence  fact
model performs worse trained tested different data sources cannot
attributed solely lexicalization 
perhaps informative question is  lexicalized models trained different data
sources exhibit varied performance given test set  composed documents
source  unlexicalized models trained different data sources  affirmative
answer question provide empirical support hypothesis lexicalized
model fits data trained unlexicalized counterpart  answer
question  compute column two models     difference
highest lowest scores  see maxmin row       standard
deviation six scores corresponding column  see std  dev  row  
   

fia cluster ranking approach coreference resolution

compare corresponding columns two coreference models  see except
bn  lexicalized model exhibit varied performance given test set
unlexicalized model according scoring programs  regardless whether
measuring variation using maxmin standard deviation 
    feature analysis
subsection  analyze effects linguistic features performance
coreference models  given large number models trained three
feature sets  feasible us analyze features model feature
set  since cluster ranking model  used combined feature set  yields
best performance  analyze features  addition  since lexical features
yielded good performance mention ranking model  would informative see
lexical features greatest contribution performance  result 
perform feature analysis two model feature set combinations 
although identified two particular model feature set combinations  actually
total    model feature set combinations  recall except row    row
table   shows aggregated result six data sets  trained one model
data set  words  two combinations selected above 
six learned models  reduce number models need analyze yet maximize
insights gain  choose analyze models trained data sets
two fairly different domains  newswire  nw  broadcast news  bn  
next question is  analyze features  apply backward elimination feature selection algorithm  see survey paper blum   langley        
starts full feature set removes iteration feature whose removal
yields best system performance  despite greedy nature  algorithm runs time
quadratic number features  making computationally expensive run
feature sets  reduce computational cost  divide features feature types
apply backward elimination eliminate one feature type per iteration 
features grouped follows  lexical feature set  divide features
five types      unseen features      lexical features      semi lexical features      distance      alias  words  division corresponds roughly one described
section      except put two conventional features two different groups 
since linguistically one positional feature semantic feature 
combined feature set  divide features seven groups  first four
identical division lexical features above  remaining features 
divide string matching features  comprise features      table   
grammatical features  comprise features                           
semantic features  comprise features            note alias  semantic feature lexical feature set  combined semantic features
conventional feature set form semantic feature type 
results shown tables       specifically  tables   a   b show b 
ceaf f measure scores feature analysis experiments involving mention ranking
model  using lexical feature set nw data set  table  first row shows
system would perform class features removed  remove least
   

fi    
    
    
    

    
    
    
    

    
    
    

    
    

ee
n
u
ns


lia



ist

ce

i l
ex
ic
al
se


le
xi
ca
l

rahman   ng

    

    
    
    
    

    
    
    
    

    
    
    

    
    

ee
n
u
ns


lia



ist

ce

le
xi
ca
l

se


i l
ex
ic
al

 a  b  results

    

 b  ceaf results

table     feature analysis results  in terms f measure scores  mention ranking
model using lexical features nw data set  feature types used train
model  b  ceaf f measure scores            respectively 

important feature class  i e   feature class whose removal yields best performance  
next row shows adjusted system would perform without remaining
class  according scoring programs  removing unseen features yields least
drop performance  note caption full feature set  b  score
     ceaf score        fact  two scorers agree lexical
semi lexical features important unseen  alias  distance features 
nevertheless  results suggest five feature types important  since best
performance achieved using full feature set 
tables   a   b show b  ceaf f measure scores feature analysis
experiments involving cluster ranking model  using combined feature set nw
data set  recall combined feature set  seven types features 
see  two scorers agree completely order features removed 
particular  important features lexical semi lexical features 
whereas least important features present lexical feature
set  namely  grammatical  string matching  semantic features  suggests
lexical features general important non lexical features
used combination  somewhat surprising  non lexical features
commonly used features coreference resolution  whereas lexical features
comparatively much less investigated coreference researchers  nevertheless  unlike
saw table     feature types appear relevant  table   a  see
   

fi    
    
    
    


ica
l



ch

g


tic
se

    
    
    

g
ra


    
    
    
    
    


ist

ce

u
ns

ee
n

i l
ex
ica
l
    
    
    
    
    
    

st
rin

    
    
    
    
    
    

se


le
xi
ca
l

cluster ranking approach coreference resolution

    
    

    

    
    
    
    


ica
l



ch

g


tic
se

    
    
    

g
ra


    
    
    
    
    


ist

ce

u
ns

ee
n

i l
ex
ica
l
    
    
    
    
    
    

st
rin

    
    
    
    
    
    

se


le
xi
ca
l

 a  b  results

    
    

    

 b  ceaf results

table     feature analysis results  in terms f measure  cluster ranking model
using combined features nw data set  feature types used train
model  b  ceaf f measure scores            respectively 

best b  f measure score       achieved using lexical features 
represents      absolute gain f measure model trained seven
feature types  suggesting learning based coreference model could improved via feature
selection 
next  investigate whether similar trends observed models trained
different source  broadcast news  specifically  show tables   a   b b 
ceaf f measure scores feature analysis experiments involving mentionranking model  using lexical feature set bn data set  table    
see two scorers agree completely order features
removed  fact  similar observed table     on nw data set  
scorers determine lexical semi lexical features important 
whereas distance alias features least important  although five feature
types appear relevant according scorers 
finally  show tables   a   b b  ceaf f measure scores feature
analysis experiments involving cluster ranking model  using combined feature set
   

fi    
    
    
    

    
    
    
    

    
    
    

    
    


lia


ee
n
u
ns


ist

ce

i l
ex
ica
l
se


le
xi
ca
l

rahman   ng

    

    
    
    
    

    
    
    
    

    
    
    

    
    


lia


ee
n
u
ns


ist

ce

i l
ex
ica
l
se


le
xi
ca
l

 a  b  results

    

 b  ceaf results

table     feature analysis results  in terms f measure  mention ranking model
using lexical features bn data set  feature types used train
model  b  ceaf f measure scores            respectively 

bn data set  tables        two scorers agree completely order
features removed  far feature contribution concerned  two
tables resemble tables   a   b  cases  lexical  semi lexical  unseen
features important  string matching grammatical features
least important  semantic distance features middle  case 
however  seven feature types seem relevant  best performance achieved
using full feature set according scorers  perhaps interestingly  numbers
column generally increasing move column  means
feature type becomes progressively less useful remove feature types 
suggests interactions different feature types non trivial
feature type may useful presence another feature type 
summary  results two data sets  nw bn  two scoring programs demonstrate     general feature types crucial overall performance     
little investigated lexical features contribute overall performance
commonly used conventional features 
    resolution performance
gain additional insights results  analyze behavior coreference
models different types anaphoric expressions trained different
feature sets  specifically  partition mentions different resolution classes 
   

fiat
ica
l





ch
    
    
    

g


ist

ce


tic
    
    
    
    

g
ra


    
    
    
    
    

se


u
ns

ee
n

i l
ex
ica
l
    
    
    
    
    
    

st
rin

    
    
    
    
    
    

se


le
xi
ca
l

cluster ranking approach coreference resolution

    
    

    


ica
l





ch
    
    
    

g


ist

ce


tic
    
    
    
    

g
ra


    
    
    
    
    

se


u
ns

ee
n

i l
ex
ica
l
    
    
    
    
    
    

st
rin

    
    
    
    
    
    

se


le
xi
ca
l

 a  b  results

    
    

    

 b  ceaf results

table     feature analysis results  in terms f measure  cluster ranking model
using combined features bn data set  feature types used train
model  b  ceaf f measure scores            respectively 

previous work focused mainly three rather coarse grained resolution classes  namely 
pronouns  proper nouns  common nouns   follow stoyanov et al         subdivide
class three fine grained classes  worth mentioning none stoyanov et
al s classes corresponds non anaphoric expressions  since believe non anaphoric
expressions play important role analysis performance coreference
model  propose three additional classes correspond non anaphoric pronouns 
non anaphoric proper nouns  non anaphoric common nouns  finally  certain
types anaphoric pronouns  e g   wh pronouns  fall stoyanov et
al s pronoun categories  fill gap  create another category serves
default category anaphoric pronouns covered stoyanov et al s classes 
results    resolution classes  discussed detail 
proper nouns  four classes defined proper nouns      e  proper noun assigned
exact string match class preceding mention two
coreferent string      p  proper noun assigned partial string
match class preceding mention two coreferent
   

firahman   ng

content words common      n  proper noun assigned string match class
preceding mention two coreferent content
words common      na  proper noun assigned non anaphor class
coreferent preceding mention 
common nouns  four analogous resolution classes defined mentions whose head
common noun      e      p      n      na 
pronouns  three pronoun classes            st  nd person pronouns      
g   gendered  rd person pronouns  e g   she        u   ungendered  rd person pronouns 
     oa  anaphoric pronouns belong                       na 
non anaphoric pronouns 
next  score resolution class  unlike stoyanov et al          use modified
version muc scorer  employ b    reasons muc scorer    
reward singleton clusters      inflate systems performance clusters
overly large  compute score class c  process mentions test text
left to right manner  mention encountered  check whether belongs c 
so  use coreference model decide resolve it  otherwise  use oracle
make correct resolution decision    so end mistakes attributed
incorrect resolution mentions c  thus allowing us directly measure
impact overall performance   test documents processed  compute
b  f measure score mentions belong c 
performance resolution class  aggregated test sets six data
sources way before  shown table     provides nice diagnosis
strengths weaknesses coreference model used combination
feature set  show table percentage mentions belonging
class name class  abbreviate name model follows  hm
corresponds head match baseline  whereas mp  em  mr  cr denote mentionpair model  entity mention model  mention ranking model  cluster ranking
model  respectively  ranking model two versions  pipeline version  denoted
p  joint version  denoted j  
points deserve mention  recall table   conventional features
used  joint mention ranking model performs better mention pair model
entity mention model  comparing row   rows     table    
see improvements attributed primarily better handling one proper
    oracle determines mention anaphoric antecedents cluster
 because model previously made mistake   employ following heuristic select
antecedent resolve mention to  try resolve closest preceding antecedent
belong class c  antecedent exists  resolve closest preceding antecedent
belongs class c  reason behind heuristics preference preceding antecedent
belong class c simple  since resolving mention using oracle  want choose
antecedent allows us maximize overall score  resolving mention antecedent
belong c likely yield better score resolving antecedent
belongs c  since former resolved using oracle latter not  heuristic
applies trying use oracle resolve mention preceding cluster  first attempt
resolve closest preceding cluster containing mention belong c 
antecedent exists  resolve closest preceding cluster containing mention belongs c 

   

fia cluster ranking approach coreference resolution

proper nouns
p
n
na
   
   
    

e
   

common nouns
p
n
na
   
   
    

class
 

e
    

 

hm

    

    

    

    

    

 
 
 
 
 
 

mp
em
mr p
mr j
cr p
cr j

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

using
    
    
    
    
    
    

 
 
  
  
  
  

mp
em
mr p
mr j
cr p
cr j

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

using lexical feature set
                        
                        
                        
                        
                        
                        

  
  
  
  
  
  

mp
em
mr p
mr j
cr p
cr j

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

using
         
         
         
         
         
         

pronouns
u 
oa
   
   

   
    

g 
   

    

    

    

    

    

conventional feature set
                        
                        
                        
                        
                        
                        

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    

    

    

combined feature set
              
              
              
              
              
              

na
   

table     b  f measure scores different resolution classes 
noun class  e  three classes correspond non anaphoric mentions  na  
results indicate important take account non anaphoric mentions
analyzing performance coreference model  time  see
joint mention ranking model resolve type e common nouns well
mention pair model entity mention model  also  results rows     indicate
joint cluster ranking model better joint mention ranking model due
better handling type e common nouns  non anaphoric common nouns 
well anaphoric pronouns 
next  recall table   lexical features used lieu conventional features  mention pair model  entity mention model  joint mentionranking model exhibit significant improvements performance  mention pair
model entity mention model  improvements stem primarily better handling three proper noun classes  e p na   two common noun classes  e na   nonanaphoric pronouns  compare rows     well rows     table      joint
mention ranking model  hand  improvements accrue better handling
two proper noun classes  p n   two common classes  e na   anaphoric pronouns 
   

firahman   ng

seen rows       joint cluster ranking model show
overall improvement switch conventional lexical features  compare rows  
     resulting models behave differently  specifically  using lexical features 
model gets worse handling one proper noun class  e  one common noun class  e  
better handling another proper noun class  n   two common noun classes  p n  
one anaphoric pronoun class        non anaphoric pronouns 
finally  recall combined features used lieu lexical features 
cluster ranking model show deterioration performance  mention pair
model entity mention model  deterioration performance attributed
poorer handling two proper noun classes  e na   two common noun classes  e na  
non anaphoric pronouns  although better handling one proper noun
class  n  anaphoric pronouns  compare rows      well rows     
table      overall  poorer handling anaphoricity appears major factor responsible
performance deterioration  joint mention ranking model  reasons
performance deterioration slightly different  comparing rows        see poorer
handling two proper noun classes  p n   three common noun classes  p n na  
anaphoric pronouns  although better handling non anaphoric proper nouns
pronouns  mentioned before  two versions cluster ranking model improve
trained combined features  however  improvements stem
improvements classes  compare rows       well rows        
instance  replacing lexical features combined features joint
cluster ranking model  see improvements two proper noun classes  e na   one common
noun class  e   several pronoun classes      g  u    performance drops another
proper noun class  p   three common noun classes  p n na   two pronoun classes
 oa na  
overall  results provide us additional insights strengths weaknesses learning based coreference model well directions future work  particular  even two models yield similar overall performance  quite different
resolution class level  since single coreference model outperforms
others resolution classes  may beneficial apply ensemble approach 
anaphor belonging particular resolution class resolved model offers
best performance class 

   conclusions
mitkov        p       puts it  coreference resolution difficult  intractable
problem  researchers making steady progress improving machine learning approaches problem past fifteen years  progress slow  however 
despite deficiencies  mention pair model widely thought learningbased coreference model almost decade  entity mention model mentionranking model emerged mention pair model dominated learning based
coreference research nearly ten years  although two models conceptually simple  represent significant departure mention pair model new way
thinking alternative models coreference designed  cluster ranking
model advances learning based coreference research theoretically combining
   

fia cluster ranking approach coreference resolution

strengths two models  thereby addressing two commonly cited weaknesses
mention pair model  bridges gap two independent lines learningbased coreference research one concerning entity mention model
mention ranking model going past years  narrows modeling gap sophistication rule based coreference models
simplicity learning based coreference models  empirically  shown using
ace      coreference data set cluster ranking model acquired jointly learning
anaphoricity determination coreference resolution surpasses performance several
competing approaches  including mention pair model  entity mention model 
mention ranking model  perhaps equally importantly  cluster ranking model
model considered profitably exploit information provided two
fairly different sources information  conventional features lexical features 
ranking natural formulation coreference resolution classification 
ranking based coreference models popularly used influential
mention pair model  one goals article promote application ranking
techniques coreference resolution  specifically  attempted clarify difference classification based ranking based coreference models showing constrained
optimization problem svm learner needs solve type models  hoping
help reader appreciate importance ranking coreference resolution  addition  provided ample empirical evidence ranking based models
superior classification based models coreference resolution 
another contribution work lies empirical demonstration benefits
lexicalizing learning based coreference models  previous work showed lexicalization provides marginal benefits coreference model  showed lexicalization significantly improve mention pair model  entity mention model 
mention ranking model  point approach even surpass performance
cluster ranking model  interestingly  showed models benefit lexicalization conventional coreference features used  challenges
common belief prototypical set linguistic features  e g   gender
number agreement  must used constructing learning based coreference systems 
addition  feature analysis experiments indicated conventional features contributed less overall performance rarely studied lexical features joint
cluster ranking coreference model two types features used combination 
finally  examined performance coreference model resolving mentions
belonging different resolution classes  found even two models achieve similar
overall performance  quite different resolution class level  overall 
results provide us additional insights strengths weaknesses learningbased coreference model well promising directions future research 

bibliographic note
portions work previously presented conference publication  rahman  
ng         current article extends work several ways  notably     
overview literature ranking approaches coreference resolution  section        
detailed explanation difference classification ranking  section        
   

firahman   ng

investigation issues lexicalizing coreference models  section         in depth
analysis different aspects coreference system  section    

acknowledgments
authors acknowledge support national science foundation  nsf  grant iis         thank three anonymous reviewers insightful comments unanimously recommending article publication jair  opinions  findings  conclusions recommendations expressed article authors
necessarily reflect views official policies  either expressed implied  nsf 

references
androutsopoulos  i     malakasiotis  p          survey paraphrasing textual
entailment methods  journal artificial intelligence research              
aone  c     bennett  s  w          evaluating automated manual acquisition
anaphora resolution strategies  proceedings   rd annual meeting
association computational linguistics  acl   pp         
bagga  a     baldwin  b          algorithms scoring coreference chains  proceedings
linguistic coreference workshop first international conference
language resources evaluation  lrec   pp         
barbu  c     mitkov  r          evaluation tool rule based anaphora resolution methods  proceedings   th annual meeting association computational
linguistics  acl   pp       
bengtson  e     roth  d          understanding values features coreference
resolution  proceedings      conference empirical methods natural
language processing  emnlp   pp         
berger  a  l   della pietra  s  a     della pietra  v  j          maximum entropy
approach natural language processing  computational linguistics               
bikel  d  m   schwartz  r     weischedel  r  m          algorithm learns whats
name  machine learning  special issue natural language learning          
       
blum  a     langley  p          selection relevant features examples machine
learning  artificial intelligence                  
burges  c  j  c          tutorial support vector machines pattern recognition 
data mining knowledge discovery                
cai  j     strube  m          evaluation metrics end to end coreference resolution
systems  proceedings   th annual sigdial meeting discourse dialogue
 sigdial   pp       
carbonell  j     brown  r          anaphora resolution  multi strategy approach 
proceedings   th international conference computational linguistics  coling   pp        
   

fia cluster ranking approach coreference resolution

cardie  c     wagstaff  k          noun phrase coreference clustering  proceedings
     joint sigdat conference empirical methods natural language
processing large corpora  emnlp vlc   pp       
charniak  e     elsner  m          em works pronoun anaphora resolution  proceedings   th conference european chapter association computational linguistics  eacl   pp         
collins  m  j          head driven statistical models natural language parsing  ph d 
thesis  department computer information science  university pennsylvania 
philadelphia  pa 
connolly  d   burger  j  d     day  d  s          machine learning approach anaphoric
reference  proceedings international conference new methods language
processing  pp         
culotta  a   wick  m     mccallum  a          first order probabilistic models coreference resolution  human language technologies       conference north
american chapter association computational linguistics  proceedings
main conference  naacl hlt   pp       
daume iii  h     marcu  d          large scale exploration effective global features
joint entity detection tracking model  proceedings human language
technology conference conference empirical methods natural language
processing  hlt emnlp   pp        
denis  p     baldridge  j       a   global  joint determination anaphoricity coreference resolution using integer programming  human language technologies      
conference north american chapter association computational
linguistics  proceedings main conference  naacl hlt   pp         
denis  p     baldridge  j       b   ranking approach pronoun resolution  proceedings
twentieth international conference artificial intelligence  ijcai   pp      
     
denis  p     baldridge  j          specialized models ranking coreference resolution 
proceedings      conference empirical methods natural language
processing  emnlp   pp         
enrique  a   gonzalo  j   artiles  j     verdejo  f          comparison extrinsic clustering evaluation metrics based formal constraints  information retrieval         
       
fellbaum  c          wordnet  electronic lexical database  mit press  cambridge  ma 
finkel  j  r   grenager  t     manning  c          incorporating non local information
information extraction systems gibbs sampling  proceedings   rd annual
meeting association computational linguistics  acl   pp         
finkel  j  r     manning  c          enforcing transitivity coreference resolution 
proceedings acl     hlt short papers  companion volume   pp       
florian  r   hassan  h   ittycheriah  a   jing  h   kambhatla  n   luo  x   nicolov  n    
roukos  s          statistical model multilingual entity detection tracking 
hlt naacl       main proceedings  pp     
   

firahman   ng

ge  n   hale  j     charniak  e          statistical approach anaphora resolution 
proceedings sixth workshop large corpora  wvlc   pp         
grosz  b  j   joshi  a  k     weinstein  s          providing unified account definite noun phrases discourse  proceedings   th annual meeting
association computational linguistics  acl   pp       
grosz  b  j   joshi  a  k     weinstein  s          centering  framework modeling
local coherence discourse  computational linguistics                 
haghighi  a     klein  d          coreference resolution modular  entity centered
model  human language technologies       annual conference north
american chapter association computational linguistics  naacl hlt  
pp         
hobbs  j          resolving pronoun references  lingua             
iida  r   inui  k     matsumoto  y          capturing salience trainable cache
model zero anaphora resolution  proceedings joint conference   th
annual meeting acl  th international joint conference natural
language processing afnlp  acl ijcnlp   pp         
iida  r   inui  k   takamura  h     matsumoto  y          incorporating contextual cues
trainable models coreference resolution  proceedings eacl workshop
computational treatment anaphora 
ji  h   rudin  c     grishman  r          re ranking algorithms name tagging 
proceedings workshop computationally hard problems joint inference
speech language processing  pp       
ji  h   westbrook  d     grishman  r          using semantic relations refine coreference
decisions  proceedings human language technology conference conference
empirical methods natural language processing  hlt emnlp   pp       
joachims  t          making large scale svm learning practical  scholkopf  b   burges 
c     smola  a   eds    advances kernel methods support vector learning  pp 
      mit press  cambridge  ma 
joachims  t          optimizing search engines using clickthrough data  proceedings
eighth acm sigkdd international conference knowledge discovery
data mining  kdd   pp         
kehler  a   appelt  d   taylor  l     simma  a           non utility predicateargument frequencies pronoun interpretation  proceedings human language technology conference north american chapter association
computational linguistics  hlt naacl   pp         
kuhn  h  w          hungarian method assignment problem  naval research
logistics quarterly          
lappin  s     leass  h          algorithm pronominal anaphora resolution  computational linguistics                 
lin  d     wu  x          phrase clustering discriminative learning  proceedings
joint conference   th annual meeting acl  th international
   

fia cluster ranking approach coreference resolution

joint conference natural language processing afnlp  acl ijcnlp   pp 
         
luo  x          coreference resolution performance metrics  proceedings human
language technology conference conference empirical methods natural
language processing  hlt emnlp   pp       
luo  x   ittycheriah  a   jing  h   kambhatla  n     roukos  s          mentionsynchronous coreference resolution algorithm based bell tree  proceedings
  nd annual meeting association computational linguistics  acl  
pp         
madnani  n     dorr  b          generating phrasal sentential paraphrases  survey
data driven methods  computational linguistics                 
mccarthy  j     lehnert  w          using decision trees coreference resolution  proceedings fourteenth international conference artificial intelligence  ijcai  
pp           
mitkov  r          robust pronoun resolution limited knowledge  proceedings
  th annual meeting association computational linguistics   th
international conference computational linguistics  coling acl   pp     
    
mitkov  r          outstanding issues anaphora resolution  gelbukh  a   ed   
computational linguistics intelligent text processing  pp          springer 
mitkov  r          anaphora resolution  longman 
morris  j     hirst  g          lexical cohesion computed thesaural relations
indicator struture text  computational linguistics               
morton  t          coreference nlp applications  proceedings   th annual
meeting association computational linguistics  acl  
muc           proceedings sixth message understanding conference  muc    
morgan kaufmann  san francisco  ca 
muc           proceedings seventh message understanding conference  muc    
morgan kaufmann  san francisco  ca 
ng  v          graph cut based anaphoricity determination coreference resolution 
proceedings      conference north american chapter association
computational linguistics  human language technologies  naacl hlt   pp 
       
ng  v     cardie  c       a   identifying anaphoric non anaphoric noun phrases
improve coreference resolution  proceedings   th international conference
computational linguistics  coling   pp         
ng  v     cardie  c       b   improving machine learning approaches coreference resolution  proceedings   th annual meeting association computational
linguistics  acl   pp         
   

firahman   ng

poesio  m   uryupina  o   vieira  r   alexandrov kabadjov  m     goulart  r         
discourse new detectors definite description resolution  survey preliminary
proposal  proeedings acl workshop reference resolution 
ponzetto  s  p     strube  m          exploiting semantic role labeling  wordnet
wikipedia coreference resolution  proceedings human language technology conference conference north american chapter association
computational linguistics  hlt naacl   pp         
rahman  a     ng  v          supervised models coreference resolution  proceedings      conference empirical methods natural language processing
 emnlp   pp         
ravichandran  d   hovy  e     och  f  j          statistical qa   classifier vs  re ranker 
whats difference  proceedings acl      workshop multilingual
summarization question answering  pp       
recasens  m     hovy  e          blanc  implementing rand index coreference
resolution  natural language engineering  to appear  
roth  d          reasoning classifiers   proceedings   th european conference
machine learning  ecml   pp         
roth  d     yih  w  t          linear programming formulation global inference
natural language tasks   proceedings eighth conference computational
natural language learning  conll   pp     
soon  w  m   ng  h  t     lim  d  c  y          machine learning approach coreference
resolution noun phrases  computational linguistics                 
stoyanov  v   gilbert  n   cardie  c     riloff  e          conundrums noun phrase
coreference resolution  making sense state of the art  proceedings
joint conference   th annual meeting acl  th international
joint conference natural language processing afnlp  acl ijcnlp   pp 
       
strube  m     muller  c          machine learning approach pronoun resolution
spoken dialogue  proceedings   st annual meeting association
computational linguistics  acl   pp         
strube  m   rapp  s     muller  c          influence minimum edit distance
reference resolution  proceedings      conference empirical methods
natural language processing  emnlp   pp         
toutanova  k   klein  d   manning  c  d     singer  y          feature rich part of speech
tagging cyclic dependency network  hlt naacl       proceedings
main conference  pp         
uryupina  o          high precision identification discourse new unique noun
phrases  proceedings   st annual meeting association computational linguistics  companion volume  pp       
vapnik  v  n          nature statistical learning  springer  new york 
   

fia cluster ranking approach coreference resolution

vilain  m   burger  j   aberdeen  j   connolly  d     hirschman  l          modeltheoretic coreference scoring scheme  proceedings sixth message understanding conference  muc     pp       
walker  m   joshi  a     prince  e   eds            centering theory discourse  oxford
university press 
yang  x   su  j   lang  j   tan  c  l     li  s          entity mention model
coreference resolution inductive logic programming  proceedings   th
annual meeting association computational linguistics  human language
technologies  acl     hlt   pp         
yang  x   su  j   zhou  g     tan  c  l          np cluster based approach coreference
resolution  proceedings   th international conference computational
linguistics  coling   pages        
yang  x   zhou  g   su  j     tan  c  l          coreference resolution using competitive
learning approach  proceedings   st annual meeting association
computational linguistics  acl   pp         

   


