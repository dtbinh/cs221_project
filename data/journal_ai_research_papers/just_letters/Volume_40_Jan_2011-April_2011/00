journal artificial intelligence research               

submitted        published      

non deterministic policies
markovian decision processes
mahdi milani fard
joelle pineau

mmilan  cs mcgill ca
jpineau cs mcgill ca

reasoning learning laboratory
school computer science  mcgill university
montreal  qc  canada

abstract
markovian processes long used model stochastic environments  reinforcement learning emerged framework solve sequential planning decision making
problems environments  recent years  attempts made apply methods
reinforcement learning construct decision support systems action selection
markovian environments  although conventional methods reinforcement learning
proved useful problems concerning sequential decision making  cannot applied current form decision support systems  medical domains 
suggest policies often highly prescriptive leave little room users
input  without ability provide flexible guidelines  unlikely methods
gain ground users systems 
paper introduces new concept non deterministic policies allow flexibility users decision making process  constraining decisions remain near
optimal solutions  provide two algorithms compute non deterministic policies
discrete domains  study output running time method set
synthetic real world problems  experiment human subjects  show
humans assisted hints based non deterministic policies outperform human only
computer only agents web navigation task 

   introduction
planning decision making well studied ai community  intelligent
agents designed developed act in  interact with  variety environments  usually involves sensing environment  making decision using
intelligent inference mechanism  performing action environment  russell   norvig         often times  process involves level learning  along
decision making process  make agent efficient performing intended
goal 
reinforcement learning  rl  branch ai tries develop computational
approach solving problem learning interaction  rl process
learning dohow map situations actionsso maximize numerical
reward signal  sutton   barto         many methods developed solve
rl problem different types environments different types agents  however 
work rl focused autonomous agents robots software
agents  rl controllers thus designed issue single action time step
c
    
ai access foundation  rights reserved 

fimilani fard   pineau

executed acting agent  past years  methods developed
rl community started used sequential decision support systems  murphy 
      pineau  bellemare  rush  ghizaru    murphy        thapa  jung    wang       
hauskrecht   fraser         many systems  human makes final
decision  usability acceptance issues thus become important cases 
rl methods therefore require level adaptation used decision support
systems  adaptations main contribution paper 
medical domains among cases rl needs adaptation  although
rl framework correctly models sequential decision making complex medical scenarios  including long term treatment design  standard rl methods cannot applied
medical settings current form lack flexibility suggestions 
requirements are  course  specific medical domains and  instance  might
needed aircraft controller provides suggestions pilot 
important difference decision support system classical rl problem
stems fact decision support system  acting agent often human
being  course his her decision process  therefore  assumption
controller send one clear commanding signal acting agent
appropriate  accurate assume aspect decision making process
influenced user system 
view decision process particularly relevant two different situations 
first  many practical cases  exact model system  instead 
may noisy model built finite number interactions environment 
leads type uncertainty usually referred extrinsic uncertainty 
rl algorithms ignore uncertainty assume model perfect  however
look closely  performance optimal action based imperfect model might
statistically different next best action  bayesian approaches looked
problem providing confidence measure agents performance  mannor  simester 
sun    tsitsiklis         cases acting agent human being  use
confidence measures provide user complete set actions  might
optimal enough evidence differentiate  user
use his her expertise make final decision  methods guarantee
suggestions provided system statistically meaningful plausible 
hand  even complete knowledge system
identify optimal action  might still actions roughly equal
performance  point  decision near optimal options could left
acting agentnamely human using decision support system 
could many advantages  ranging better user experience  increased robustness
flexibly  among near optimal solutions  user select based domain
knowledge  preferences  captured system  instance 
medical diagnosis system suggests treatments  providing physician several
options might useful final decision could made based knowledge
patients medical status  preferences regarding side effects 
throughout paper address latter issue combination theoretical
empirical investigations  introduce new concept non deterministic policies
capture decision making process intended decision support systems  policies
 

finon deterministic policies markovian decision processes

involve suggesting set actions  non deterministic choice made
user  apply formulation solve problem finding near optimal policies
provide flexible suggestions user 
particular  investigate suggest several actions acting agent 
providing performance guarantees worst case analysis  section   introduces
necessary technical background material  section   defines concept non deterministic
policies related concepts  section   addresses problem providing choice
acting agent keeping near optimality guarantees performance worst case
scenario  propose two algorithms solve problems provide approximation
techniques speed computation larger domains 
methods introduced paper general enough apply decision support system observable markovian environment  empirical investigations focus
primarily sequential decision making problems clinical domains  system
provide suggestions best treatment options patients  decisions
provided sequence treatment phases  systems specifically interesting
often times  different treatment options seem provide slightly different results  therefore  providing physician several suggestions would beneficial
improving usability system performance final decision 

   definitions notations
section introduces main notions behind sequential decision making mathematical formulations used rl 
    markov decision processes
markov decision process  mdp  model system dynamics sequential decision
problems involves probabilistic uncertainty future states system  bellman 
       mdps used model interactions agent observable
markovian environment  system assumed state given time 
agent observes state performs action accordingly  system makes
transition next state agent receives reward 
formally  mdp defined   tuple  s  a  t  r    
states  set states  state usually captures complete configuration
system  state system known  future system
independent previous system transitions  means state
system sufficient statistic history system 
actions     a set actions allowed state set
actions  a s  set actions agent choose from  interacting
system state s 
transition probabilities           defines transition probabilities
system  function specifies likely end state  given
current state specific action performed agent  transition probabilities
 

fimilani fard   pineau

specified based markovian assumption  is  state system
time denoted st action time   have 
pr st    at   st   at    at            a    s      p r st    at   st   

   

focus homogeneous processes system dynamics independent
time  thus transition function stationary respect time 
def

 s  a  s      p r st     s   at   a  st   s  

   

rewards  r   r        probabilistic reward model  depending
current state system action taken  agent receive reward
drawn model  focus homogeneous processes which  again 
reward distribution change time  reward time denoted
rt   have 
rt r st     

   

depending domain  reward could deterministic stochastic  use
general stochastic model throughout paper  mean distribution
denoted r s  a  
discount factor         discount rate used calculate long term
return 
agent starts initial state s  s  time step t  action a st  
taken agent  system makes transition st    st     agent
receives immediate reward rt r st     
goal agent maximize discounted sum rewards planning
horizon h  could infinite   usually referred return  denoted d  
d 

h
x

rt  

   

t  

finite horizon case  sum taken horizon limit discount factor
set    however  infinite horizon case discount factor less
  return finite value  return process depends
stochastic transitions rewards  well actions taken agent 
often times transition structure mdp contains loop non zero probability  transition graph modeled directed acyclic graph  dag  
class mdps interesting includes multi step decision making finite horizons 
found medical domains 
    policy value function
policy way defining agents action selection respect changes
environment   probabilistic  policy mdp mapping state space
distribution action space 
         
 

   

finon deterministic policies markovian decision processes

deterministic policy policy defines single action per state  is   s 
a s   later introduce notion non deterministic policies mdps deal
sets actions 
agent interacts environment takes actions according policy 
value function policy defined expectation return given
agent acts according policy 
 
 
x
def
v  s    e d  s     e
rt  s    s     st    
   
t  

using linearity expectation  write expression recursive
form  known bellman equation  bellman        
 
 
x
x

 
 
v  s   
 s  a  r s  a   
 s  a   v  s    
   
s 

aa

value function used primary measure performance much
rl literature  are  however  ideas take risk variance
return account measure optimality  heger        sato   kobayashi        
common criteria  though  assume agent trying find policy
maximizes value function  policy referred optimal policy 
define value function state action pairs  usually referred
q function  q value  pair  definition 
 
 
x
def
   
q  s  a    e d  s  a     e
rt  s    s  a    a         st    
t  

is  q value expectation return  given agent starts state
s  takes action a  follows policy   q function satisfies bellman
equation 
x
x
q  s  a    r s  a   
 s  a  s   
 s    a   q  s    a    
   
s 

a 

rewritten as 
q  s  a    r s  a   

x

 s  a  s   v  s    

    

s 

q function often used compare optimality different actions given fixed
subsequent policy 
    planning algorithms optimality
optimal policy  denoted   defined policy maximizes value
function initial state 
  argmax v  s    


 

    

fimilani fard   pineau

shown  bellman        mdp  exists optimal deterministic policy worse policy mdp  value optimal
policy v satisfies bellman optimality equation 
 
 
x
v  s    max r s  a   
 s  a  s   v  s     
    
aa

s 

deterministic optimal policy follows this 
 
 
x

 
 
 s    argmax r s  a   
 s  a   v  s    
aa

    

s 

alternatively write equations q function 
x
q  s  a    r s  a   
 s  a  s   v  s    

    

s 

thus v  s    maxa q  s  a   s    argmaxa q  s  a  
much literature rl focused finding optimal policy  many
methods developed policy optimization  one way find optimal policy solve
bellman optimality equation use eqn    choose actions  bellman
optimality equation formulated simple linear program  bertsekas        
minv v  subject
p
v  s  r s  a    s   s  a  s   v  s    s  a 

    

represents initial distribution states  solution problem optimal value function  notice v represented matrix form
equation  known linear programs solved polynomial time  karmarkar 
       however  solving might become impractical large  or infinite  state spaces 
therefore often times methods based dynamic programming preferred linear
programming solution 

   non deterministic policies  definition motivation
begin section considering problem decision making sequential decision
support systems  recently  mdps emerged useful frameworks optimizing action
choices context medical decision support systems  schaefer  bailey  shechter   
roberts        hauskrecht   fraser        magni  quaglini  marchetti    barosi       
ernst  stan  concalves    wehenkel         given adequate mdp model  or data
source   many methods used find good action selection policy  policy
usually deterministic stochastic function  policies types face substantial
barrier terms gaining acceptance medical community  highly
prescriptive leave little room doctors input  problems are  course 
specific medical domain present application actions
executed human  cases  may preferable provide several equivalently
 

finon deterministic policies markovian decision processes

good action choices  agent pick among according
heuristics preferences 
address problem  work introduces notion non deterministic policy 
function mapping state set actions  acting agent
choose 
definition    non deterministic policy mdp  s  a  t  r    function
maps state non empty set actions denoted  s  a s  
agent choose action  s  whenever mdp state s 
definition    size non deterministic
policy   denoted     sum
p
cardinality action sets          s   
following sections discuss two scenarios non deterministic policies
useful  show used implement robust decision support
systems statistical guarantees performance 
    providing choice acting agent
even cases complete knowledge dynamics planning problem
hand  accurately calculate actions utilities  might desirable
provide user optimal choice action time step  domains 
difference utility top actions may substantial  medical
decision making  instance  difference may medically significant based
given state variables 
cases  seems natural let user decide top actions 
using his her expertise domain  results injection domain
knowledge decision making process  thus making robust practical 
decisions based facts known user incorporated automated
planning system  based preferences might change case case 
instance  doctor get several recommendations treat patient
maximize chance remission  decide medication apply considering
patients medical record  preferences regarding side effects  medical expenses 
idea providing choice user accompanied reasonable guarantees performance final decision  regardless choice made user 
notion near optimality enforced make sure actions never far
best possible option  guarantees enforced providing worst case analysis
decision process 
    handling model uncertainty
many practical cases complete knowledge system hand  instead 
may get set trajectories collected system according specific policy 
cases  may given chance choose policy  in on line active rl  
cases may access data fixed policy  medical
trials  particular  data usually collected according randomized policy  fixed ahead
time consultation clinical researchers 
 

fimilani fard   pineau

given set sample trajectories  either build model domain  in modelbased approaches  directly estimate utility different actions  with model free approaches   however models estimates always accurate
observe finite amount data  many cases  data may sparse incomplete
uniquely identify best option  is  difference performance measure
different actions statistically significant 
cases might useful let user decide final choice
actions enough evidence differentiate 
comes assumption user identify best choice among
recommended  task therefore provide user small set actions
almost surely include optimal one 
paper focus problem providing flexible policies nearoptimal performance  using non deterministic policies handling model uncertainty remains interesting future work 

   near optimal non deterministic policies
often times  beneficial provide user decision support system set
near optimal solutions  mdps  would suggest set near optimal actions
user let user make decision among proposed actions  notion
near optimality therefore set possible policies consistent
proposed actions  is  matter action chosen among proposed
options state  final performance close optimal policy 
constraint suggests worst case analysis decision making process  therefore 
opt guarantee performance action selection consistent non deterministic
policy putting near optimality constraint worst case selection actions
user 
definition     worst case  value state action pair  s  a  according nondeterministic policy mdp    s  a  t  r    given recursive definition 

x

 

   
qm  s  a    r s  a   
 s  a    min qm  s      
    
a   s   

s 

worst case expected return allowed set actions 
definition    define  worst case  value state according non  s   be 
deterministic policy   denoted vm
min q
 s  a  

    

a s 

calculate value non deterministic policy  construct evaluation mdp 
     s  a    r    t     a    r    r 
theorem    negated value non deterministic policy equal
optimal policy evaluation mdp 

q
 s  a    qm    s  a  

 

    

finon deterministic policies markovian decision processes

proof  show qm   satisfies bellman optimality equation    
negated values satisfy eqn     
x
qm    s  a    r   s  a   
 s  a  s    max
qm    s    a   
    
 
 


s 

qm    s  a    r   s  a 

x

 s  a  s    max
qm    s    a   
 
 

    

 s  a  s    min qm    s    a    

    



s 

qm    s  a    r s  a   

x
s 

a   s   


equivalent eqn    q
 s  a    qm    s  a  

means policy evaluation non deterministic policy achieved
method finds optimal policy mdp 
definition    non deterministic policy said augmented state action pair
 s  a   denoted        s  a   satisfies 
 
 s    
s    
   s     
    
 s     a   s    s 
policy achieved number augmentations policy     say
includes    
definition    non deterministic policy said non augmentable according
constraint satisfies   state action pair  s  a      s  a 
satisfy  
paper working constraints particular property 
policy satisfy   policy includes satisfy  
refer constraints monotonic  one constraint  optimality 
discussed next section 
     optimal non deterministic policies
definition    non deterministic policy mdp said  optimal 
        have   


vm
 s      vm
 s   s 

    

thought constraint space non deterministic policies  set
ensure worst case expected return within range optimal value 
theorem     optimality constraint monotonic 

   mdp literature   optimality defined additive constraint  q
qm    kearns
  singh         derivations analogous case  chose multiplicative constraint
cleaner derivations 

 

fimilani fard   pineau

proof  suppose  optimal  augmentation        s  a   have 

x
 
 
     
q
 s 
a 
 
r s 
a 
 


 s 
a 

 
min
q
 s
 

 


s 

a     s   


x
 
     
 s  a    min qm  s    
r s  a   
s 

a   s   

q
 s  a  
implies 
 



vm
 s  vm
 s  

 optimal  means    optimal either value function
decrease policy augmentation 
intuitively  follows fact adding options cannot increase
minimum utility former worst case choice still available augmentation 
definition    conservative  optimal non deterministic policy mdp
policy non augmentable according following constraint 
x



r s  a   
 s  a  s       vm
 s        vm
 s    s  
    
s 

constraint indicates add actions policy whose reward plus
     future optimal return within sub optimal margin  ensures
non deterministic policy  optimal using inequality 
x


q
 s  a  s       vm
 s     
    
 s  a  r s  a   
s 

instead solving eqn    using inequality constraint eqn     applying eqn   
guarantees non deterministic policy  optimal may still augmentable
according eqn     hence name conservative 
shown conservative policy unique 
two different conservative policies  union would conservative 
violates assumption non augmentable according eqn    
definition    non augmentable  optimal non deterministic policy mdp
policy non augmentable according constraint eqn    
non deterministic policy adding actions violates nearoptimality constraint worst case performance  search  optimal policies 
non augmentable one locally maximal size  means although policy
might largest among  optimal policies  cannot add actions
without removing actions  hence locally maximal reference 
non augmentable  optimal policy includes conservative policy 
always add conservative policy policy remain within bound 
  

finon deterministic policies markovian decision processes

however  non augmentable  optimal policies necessarily unique 
locally maximal size 
remainder section  focus problem searching space
non augmentable  optimal policies  maximize criteria  specifically 
aim find non deterministic policies give acting agent options staying
within acceptable sub optimal margin 
present example clarifies concepts introduced far  simplify
presentation example  assume deterministic transitions  however  concepts
apply well probabilistic mdp  figure   shows example mdp  labels
arcs show action names corresponding rewards shown parentheses 
assume             figure   shows optimal policy mdp 
conservative  optimal non deterministic policy mdp shown figure   

figure    example mdp

figure    optimal policy

figure    conservative  optimal policy

figure    two non augmentable  optimal policies

figure   includes two possible non augmentable  optimal policies  although policies figure    optimal  union  optimal  due fact
adding option one states removes possibility adding options
  

fimilani fard   pineau

states  illustrates local changes policy always appropriate
searching space  optimal policies 
    optimization criteria
formalize problem finding  optimal non deterministic policy terms
optimization problem  several optimization criteria formulated 
still complying  optimality constraint 
maximizing size policy  according criterion  seek nonaugmentable  optimal policies biggest overall size  def     provides
options agent still keeping  optimal guarantees  algorithms
proposed later sections use optimization criterion  notice solution
optimization problem non augmentable according  optimal constraint 
maximizes overall size policy 
variant this  try maximize sum log size action
sets 
x
log   s   
    
ss

enforces even distribution choice action set  however 
using basic case maximizing overall size easier optimization
problem 
maximizing margin  aim maximize margin non deterministic
policy  
max    

    



where 

     min
ss

min



q s  a  q s     
 

a s  a   s 
 

    

optimization criterion useful one wants find clear separation
good bad actions state 
minimizing uncertainty  learn models data
uncertainty optimal action state  use variance estimation value function  mannor  simester  sun    tsitsiklis        along
z test get confidence level comparisons find probability
wrong order comparing actions according values  let q
value true model q empirical estimate based dataset
d  aim minimize uncertainty non deterministic policy  
min    


    

where 

     max
ss

max

a s  a   s 
 

  




p r q s  a    q s  a    d
 

    

finon deterministic policies markovian decision processes

notice last two criteria defined space  optimal policies 
non augmentable ones 
following sections provide algorithms solve first optimization problem
mentioned above  aims maximize size policy  focus criterion
seems appropriate medical decision support systems  desirable
acceptability system find policies provide much choice possible
acting agent  developing algorithms address two optimization criteria
remains interesting open problem 
    maximal  optimal policy
exact computational complexity finding maximal  optimal policy yet known 
problem certainly np  one find value non deterministic policy
polynomial time solving evaluation mdp linear program  suspect
problem np complete  yet find reduction known np complete
problem 
order find largest  optimal policy  present two algorithms  first present
mixed integer program  mip  formulation problem  present search algorithm uses monotonic property  optimal constraint  mip method
useful general theoretical formulation problem  search algorithm
potential extensions heuristics 
      mixed integer programming solution
recall formulate problem finding optimal deterministic policy
mdp simple linear program  bertsekas        
minv v  subject
p
v  s  r s  a    s   s  a  s   v  s    s  a 

    

thought initial distribution states  solution
problem optimal value function  v    similarly  computed v using
eqn     problem searching optimal non deterministic policy according
size criterion rewritten mixed integer program  
maxv   t v    vmax vmin  ets ea    subject
v  s      v  s 

p
 s  a     

p
 
 
v  s  r s  a    s   s  a   v  s     vmax     s  a   s  a 

    

overloading notation define binary matrix representing policy 
 s  a     s     otherwise  define vmax   rmax      
vmin   rmin        es column vectors   appropriate dimensions 
first set constraints makes sure stay within optimal return 
   note mip  unlike standard lp mdps  choice affect solution cases
tie size  

  

fimilani fard   pineau

second set constraints ensures least one action selected per state  third
set ensures state action pairs chosen policy  bellman
constraint holds  otherwise  constant vmax makes constraint trivial  notice
solution problem maximizes    result non augmentable 
theorem    solution mixed integer program eqn    non augmentable
according  optimality constraint 
proof  first  notice solution  optimal  due first set constraints
 worst case  value function  show non augmentable  counter argument 
suppose could add state action pair solution   still staying suboptimal margin  adding pair  objective function increased  vmax vmin   
bigger possible decrease v term  thus objective
improved  conflicts solution 
use mip solver solve problem  note however
make use monotonic nature constraints  general purpose mip solver could
end searching space possible non deterministic policies  would
require running time exponential number state action pairs  o   s  a      
      heuristic search
alternatively  develop heuristic search algorithm find maximal  optimal policy 
make use monotonic property  optimal policies narrow
search  start computing conservative policy  augment arrive
non augmentable policy  make use fact policy  optimal 
neither policy includes it  thus cut search tree
point 
table    heuristic search algorithm find  optimal policies maximum size
function getoptimal   startindex   

startindex  s  a 
 s  a  pi

   s    v      s  a       v
  getoptimal      s  a         
g       g o  
 
end
end
end
return
algorithm presented table   one sided recursive depth first search algorithm
searches space plausible non deterministic policies maximize function
g    assume ordering set state action pairs  pi    
  

finon deterministic policies markovian decision processes

  sj   ak     ordering chosen according heuristic along mechanism
cut parts search space  v optimal value function function
v returns value non deterministic policy calculated solving
corresponding evaluation mdp 
make call function passing conservative policy
starting first state action pair  getoptimal m        
asymptotic running time algorithm o   s  a  d  tm   tg    
maximum size  optimal policy minus size conservative policy  tm
time solve original mdp  polynomial relevant parameters   tg time
calculate function g  although worst case running time still exponential
number state action pairs  run time much less search space sufficiently
small   a  term due fact check possible augmentations
state  note algorithm searches space  optimal policies rather
non augmentable ones  set function g         algorithm
return biggest non augmentable  optimal policy 
search improved using heuristics order state action pairs
prune search  one start search policy rather
conservative policy  potentially useful constraints
problem 
      directed acyclic transition graphs
one way narrow search add action maximum value
state s  ignore rest actions adding top action result values
 optimality bound 
 
     

s  argmax q  s  a   
a s 
 

modified algorithm follows 
table    modified heuristic search algorithm augmentation rule eqn    
function getoptimal    

 s     a s 
argmaxa s 
q  s  a 
 
v      s  a       v
  getoptimal      s  a    
g       g o  
 
end
end
end
return

  

    

fimilani fard   pineau

algorithm table   leads running time o  s d  tm   tg     however
guarantee see non augmentable policies  due fact
adding action  order values might change  transition structure mdp
contains loop non zero probability  transition graph directed acyclic  i e  dag  
heuristic produce optimal result cutting search time 
theorem    mdps dag transition structure  algorithm table   generate non augmentable  optimal policies would generated full search 
proof  prove this  first notice sort dag topological sort  therefore  arrange states levels  state make transitions states
future level  easy see adding actions state non deterministic
policy change worst case value past levels  effect
q values current level future level 
given non augmentable  optimal policy generated full search 
sequence augmentations generated policy  permutation sequence
would create policy intermediate polices  optimal  rearrange sequence add actions reverse order level 
point mentioned above  q value actions point added
change target policy realized  therefore actions q values
minimum value must policy  otherwise add them  conflicts
target policy non augmentable  since actions certain q value
must added  add order  therefore target policy realized
rule eqn    
transition structure dag  one might partial evaluation
augmented policy approximate value adding actions  possibly
backups rather using original q values  offers possibility trading off
computation time better solutions 

   empirical results
evaluate framework proposed algorithms  first test mip search
formulations mdps created randomly  test search algorithm real world
treatment design scenario  finally  conduct experiment computer aided web
navigation task human subjects assess usefulness non deterministic policies
assisting human decision making 
    random mdps
first experiment  aim study non deterministic policies change
value two algorithms compare terms running time  begin 
generated random mdps   states   actions  transitions deterministic
 chosen uniformly random  rewards random values      except
one states reward    one actions  set       mip
method implemented matlab cplex 
  

finon deterministic policies markovian decision processes

  

      

      

      

figure    mip solution different values                        labels
edges action indices  followed corresponding immediate rewards 
figure   shows solution mip defined eqn    particular randomly
generated mdp  see size non deterministic policy increases performance threshold relaxed  see even small values several
actions included policy state  course result q values
close other  property typical many medical scenarios different
treatments provide slightly different results 
compare running time mip solver search algorithm  constructed
random mdps described state action pairs  figure   shows running
time averaged    different random mdps   states  assuming         which
allows several solutions   expected  algorithms running time exponential
number state action pairs  note exponential scale time axis  
running time search algorithm bigger constant factor  possibly due naive
implementation   smaller exponent base  results faster asymptotic
running time  even exponential running time  one still use search algorithm
solve problems hundred state action pairs  sufficient
many practical domains  including real world medical decision scenarios shown
next section 
observe effect choice running time algorithms  fix
size random mdps   states   actions state  change
  

fimilani fard   pineau

figure    running time mip search algorithm function number
state action pairs        
value measure running time algorithms     trials  figure   shows
average running time algorithms different values   expected 
search algorithm go deeper search tree optimality threshold relaxed
running time thus increase  running time mip method 
hand  remains relativity constant exhaustively searches space possible
non deterministic policies  results representative relative behaviour
two approaches range problems 

figure    running time mip search algorithm function     states
  actions  many actions included policy        

    medical decision making
demonstrate non deterministic policies used presented medical
domain  tested full search algorithm mdp constructed medical decisionmaking task involving real patient data  data collected part large       
patients  multi step randomized clinical trial  designed investigate comparative effectiveness different treatments provided sequentially patients suffering depression
 fava et al          goal find treatment plan maximizes chance
  

finon deterministic policies markovian decision processes

remission  dataset includes large number measured outcomes  current
experiment  focus numerical score called quick inventory depressive symptomatology  qids   used study assess levels depression  including
patients achieved remission   purposes experiment  discretize
qids scores  which range       uniformly quartiles  assume this 
along treatment step  up   steps allowed   completely describe patients state  note underlying transition graph treated dag 
study limited four steps treatment action choices change steps 
   actions  treatments  total  reward   given patient achieves
remission  at step  reward   given otherwise  transition reward
models estimated empirically medical database using frequentist approach 
table    policy running time full search algorithm medical problem 
      

       

      

  

     

    

   

   

ct
ser
bup
cit bus

ct
ser

ct

ct

  qids     

cit bup
cit ct

cit bup
cit ct

cit bup

cit bup

ven
cit bus
ct

ven
cit bus

ven

ven

   qids     

   qids   

ct
cit ct

ct
cit ct

ct
cit ct

ct

time  seconds 
    qids    

table   shows non deterministic policy obtained state second
step trial  each acronym refers specific treatment   computed using
search algorithm  assuming different values   although problem tractable
mip formulation      state action pairs   full search space  optimal policies
still possible  table   shows running time algorithm  expected 
increases relax threshold   here  use heuristics  however 
underlying transition graph dag  could use heuristic discussed previous
section  eqn     get policies even faster 
interesting question set priori  practice  doctor may use
full table guideline  using smaller values he she wants rely
decision support system  larger values relying his her assessments 
believe particular presentation non deterministic policies could used
accepted clinicians  excessively prescriptive keeps physician
patient decision cycle  contrast traditional notion policies
reinforcement learning  often leaves place physicians intervention 
  

fimilani fard   pineau

    human subject interaction
finally  conduct experiment assess usefulness non deterministic policies
human subjects  ideally  would conduct experiments medical settings
physicians  studies costly difficult conduct given require
participation many medical professionals  therefore study non deterministic policies
easier domain constructing web based game played computer
human  either jointly separately  
game defined follows  user given target word asked navigate
around pages wikipedia visit pages contain target word  user
click word page  system uses google search wiki website
clicked word keyword current page  the choice keyword
discussed later   randomly chooses one top eight search results moves
page  process mimics hyperlink structure web  extending
hyperlink structure wiki make target words easily reachable  
user given ten attempts asked reach many pages target word
possible  similar game used another work infer semantic distances
concepts  west  pineau    precup         game  however  designed way
computer model provide results similar human player thus enable us
assess effectiveness computer aided decisions non deterministic policies 
construct task cd version wikipedia  schools wikipedia        
structured manageable version wikipedia intended use schools  test
approach need build mdp model task  done using empirical data
follows  first  use latent dirichlet allocation  lda  using gibbs sampling  griffiths
  steyvers        divide pages wikipedia    topics  topic corresponds
state mdp  lda algorithm identifies topic set keywords
occur often pages topic  define sets keywords
action     actions totals  corresponding    keywords   randomly navigate
around wiki using protocol described  with computer player
clicks lda keywords  collect         transitions  use observed data build
transition reward model mdp  the reward   hit   otherwise  
specific choices lda parameter number states actions
mdp made way best policy provided model comparable
performance human player 
using amazon mechanical turk  mturk         consider three experimental conditions task  one experiment  given target  computer chooses word
 uniformly random  set keywords  the action  comes optimal
policy mdp model  another experiment  human subjects choose click
word without help  finally  test domain human users
computer highlights  hints  words come non deterministic policy
       record time taken process number times target word
observed  number hits   table   summarizes average outcomes experiment
four target words  we used seven target words  could collect enough data
them   include p value t test comparing results human
agents without hints  computer score averaged      runs 
  

finon deterministic policies markovian decision processes

table    comparison different agents web navigation task  t test
number hits human player uses hints one not 
target

computer

human

human hint

t test

marriage

     hits

     hits
    seconds
    subjects 

     hits
   seconds
    subjects 

     

     hits
   seconds
    subjects 

     hits
   seconds
    subjects 

     

     hits
   seconds
    subjects 

     hits
   seconds
    subjects 

     

     hits
   seconds
    subjects 

     hits
   seconds
     subjects 

    

      runs 
military

     hits
      runs 

book

     hits
      runs 

animal

     hits
      runs 

first three target words  performance computer agent close
human user  observe providing hints user results statistically significant
increase number hits  fact see computer aided human outperforms
computer human agents  shows non deterministic policies
provide means inject human domain knowledge computer models way
final outcome superior decision making solely performed one party 
last word  computer model working poorly  judging low hit rate  thus 
surprising see hints provide much help human agent
case  as seen non significant p value   observe general speedup
 for three targets  time taken agent choose click words 
shows usefulness non deterministic policies accelerating human
subjects decision making process 

   discussion
paper introduces new concept non deterministic policies potential use
decision support systems based markovian processes  context  investigate
assumption decision making system return single optimal action
relaxed  instead return set near optimal actions 
non deterministic policies inherently different stochastic policies  stochastic
policies assume randomized action selection strategy specific probabilities 
whereas non deterministic policies impose constraint  thus use bestcase worst case analysis non deterministic policies highlight different scenarios
human user 
  

fimilani fard   pineau

benefits non deterministic policies sequential decision making two fold 
first  several actions difference performance negligible 
report actions near optimal options  instance  medical setting 
difference outcome two treatment options might medically
significant  case  may beneficial provide near optimal options 
makes system robust user friendly  medical decision making
process  instance  physician make final decision among near optimal
options based side effects burden  patients preferences  expense  criteria
captured model used decision support system  key constraint 
however  make sure regardless final choice actions  performance
executed policy always bounded near optimal  framework  property
maintained  optimality guarantee worst case scenario 
another potential use non deterministic action sets markovian decision processes capture uncertainties optimality actions  often times  amount
data models constructed sufficient clearly identify single optimal
action  forced chose one action optimal one  might high
chance making wrong decision  however  given chance provide set
possibly optimal actions  ensure include promising options
cutting obviously bad ones  setting  task trim action set much
possible providing guarantee optimal action still among top
possible options 
solve first problem  paper introduces two algorithms find flexible nearoptimal policies  first derive exact solution mip formulation find maximal
 optimal policy  mip solution is  however  computationally expensive
scale large domains  describe search algorithm solve problem
less computational cost  algorithm fast enough applied real world medical
domains  show use heuristics search algorithm find solution
dag structures even faster  heuristic search provide approximate solutions
general case 
another way scale problem larger domains approximate solution
mip program relaxing constraints  one relax constraints
allow non integral solutions penalize objective values away     
study approximation methods remains interesting direction future work 
idea non deterministic policies introduces wide range new problems
research topics  section    discuss idea near optimal non deterministic policies
address problem finding one largest action set  mentioned 
optimization criteria might useful decision support systems 
include maximizing decision margin  the margin worst selected action
best one selected   alternatively minimizing uncertainty wrong selection 
formalizing problems mip formulation  incorporating heuristic
search  might prove useful 
evidenced human interaction experiments  non deterministic policies substantially improve outcome planning decision making tasks human
user assisted robust computer generated plan  allowing several suggestions
step provides effective way incorporating domain knowledge human side
  

finon deterministic policies markovian decision processes

decision making process  medical domains physicians domain knowledge
often hard capture computer model  collaborative model decision making
non deterministic policies could offer powerful framework selecting effective 
clinically acceptable  treatment strategies 

acknowledgments
authors wish thank a  john rush  duke nus graduate medical school   susan
a  murphy  university michigan   doina precup  mcgill university  helpful
discussions regarding work  funding provided national institutes health
 grant r   da        nserc discovery grant program 

references
bellman  r          dynamic programming  princeton university press 
bertsekas  d          dynamic programming optimal control  vol    athena scientific 
ernst  d   stan  g  b   concalves  j     wehenkel  l          clinical data based optimal
sti strategies hiv  reinforcement learning approach  proceedings
fifteenth machine learning conference belgium netherlands  benelearn  
pp       
fava  m   rush  a   trivedi  m   nierenberg  a   thase  m   sackeim  h   quitkin  f   wisniewski  s   lavori  p   rosenbaum  j     kupfer  d          background rationale
sequenced treatment alternatives relieve depression  star  d  study  psychiatric clinics north america                 
griffiths  t  l     steyvers  m          finding scientific topics  proceedings national
academy sciences       suppl               
hauskrecht  m     fraser  h          planning treatment ischemic heart disease
partially observable markov decision processes  artificial intelligence medicine 
               
heger  m          consideration risk reinforcement learning  proceedings
eleventh international conference machine learning  icml   pp         
karmarkar  n          new polynomial time algorithm linear programming  combinatorica                
kearns  m     singh  s          near optimal reinforcement learning polynomial time 
machine learning     
magni  p   quaglini  s   marchetti  m     barosi  g          deciding intervene 
markov decision process approach  international journal medical informatics 
               
mannor  s   simester  d   sun  p     tsitsiklis  j  n          bias variance value
function estimation  proceedings twenty first international conference
machine learning  icml   pp         
  

fimilani fard   pineau

mannor  s   simester  d   sun  p     tsitsiklis  j  n          bias variance approximation value function estimates  management science                 
mturk         amazon mechanical turk  http   www mturk com  
murphy  s  a          experimental design development adaptive treatment
strategies  statistics medicine                    
pineau  j   bellemare  m  g   rush  a  j   ghizaru  a     murphy  s  a          constructing evidence based treatment strategies using methods computer science  drug
alcohol dependence      supplement     s   s   
russell  s  j     norvig  p          artificial intelligence  modern approach  second
edition   prentice hall 
sato  m     kobayashi  s          variance penalized reinforcement learning risk averse
asset allocation  proceedings second international conference intelligent
data engineering automated learning  data mining  financial engineering 
intelligent agents  pp          springer verlag 
schaefer  a   bailey  m   shechter  s     roberts  m          handbook operations
research   management science applications health care  chap  medical decisions
using markov decision processes  kluwer academic publishers 
schools wikipedia        
wikipedia org  

       wikipedia selection schools 

http   schools 

sutton  r  s     barto  a  g          reinforcement learning  introduction  adaptive
computation machine learning   mit press 
thapa  d   jung  i     wang  g          agent based decision support system using reinforcement learning emergency circumstances  lecture notes computer
science            
west  r   pineau  j     precup  d          wikispeedia  online game inferring
semantic distances concepts  proceedings twenty first international
jont conference artifical intelligence  ijcai   pp            san francisco  ca 
usa  morgan kaufmann publishers inc 

  


