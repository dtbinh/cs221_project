journal of artificial intelligence research                  

submitted        published      

scaling up heuristic planning with relational decision trees
tomas de la rosa
sergio jimenez
raquel fuentetaja
daniel borrajo

trosa   inf  uc   m   es
sjimenez   inf  uc   m   es
rfuentet   inf  uc   m   es
dborrajo   ia   uc   m   es

departamento de informatica
universidad carlos iii de madrid
av  universidad     leganes  madrid  spain

abstract
current evaluation functions for heuristic planning are expensive to compute  in numerous
planning problems these functions provide good guidance to the solution  so they are worth the
expense  however  when evaluation functions are misguiding or when planning problems are large
enough  lots of node evaluations must be computed  which severely limits the scalability of heuristic planners  in this paper  we present a novel solution for reducing node evaluations in heuristic
planning based on machine learning  particularly  we define the task of learning search control for
heuristic planning as a relational classification task  and we use an off the shelf relational classification tool to address this learning task  our relational classification task captures the preferred action
to select in the different planning contexts of a specific planning domain  these planning contexts
are defined by the set of helpful actions of the current state  the goals remaining to be achieved  and
the static predicates of the planning task  this paper shows two methods for guiding the search of
a heuristic planner with the learned classifiers  the first one consists of using the resulting classifier as an action policy  the second one consists of applying the classifier to generate lookahead
states within a best first search algorithm  experiments over a variety of domains reveal that our
heuristic planner using the learned classifiers solves larger problems than state of the art planners 

   introduction
during the last few years  state space heuristic search planning has achieved significant results and
has become one of the most popular paradigms for automated planning  however  heuristic search
planners suffer from strong scalability limitations  even well studied domains like blocksworld
become challenging for these planners when the number of blocks is relatively large  usually  statespace heuristic search planners are based on action grounding  which makes the state space to be
explored very large when the number of objects and or action parameters is large enough  moreover 
domain independent heuristics are expensive to compute  in domains where these heuristics are
more misleading  heuristic planners spend most of their planning time computing useless node
evaluations  even with the best current domain independent heuristic functions in the literature 
forward chaining heuristic planners currently have to visit too many nodes  which takes considerable
time  especially due to the time required to compute those heuristic functions 
these problems entail strong limitations on the application of heuristic planners to real problems  for instance  logistics applications need to handle hundreds of objects together with hundreds
of vehicles and locations  florez  garca  torralba  linares  garca olaya    borrajo         current heuristic search planners exhaust the computational resources before solving a problem in a
real logistics application 
c
    
ai access foundation  all rights reserved 

fid e la rosa   j imenez   f uentetaja   b orrajo

a classic approach for dealing with planning scalability issues is assisting the search engines
of planners with domain specific control knowledge  dck   examples of planning systems that
benefit from this knowledge are tlp lan  bacchus   kabanza         talp lanner  doherty
  kvarnstrom        and shop   nau  au  ilghami  kuter  murdock  wu    yaman        
nevertheless  hand coding dck is a complex task because it implies expertise in both  the planning domain and the search algorithm of the planning system  in recent years there has been a
renewed interest in using machine learning  ml  to automatically extract dck  zimmerman and
kambhampati        made a comprehensive survey of ml for defining dck  as shown in the first
learning for planning competition held in       learning track   this renewed interest is specially
targeted at heuristic planners 
this paper presents an approach for learning dck for planning by building domain dependent
relational decision trees from examples of good quality solutions of a forward chaining heuristic
planner  these decision trees are built with an off the shelf relational classification tool and capture which is the best action to take for each possible decision of the planner in a given domain 
the resulting decision trees can be used either as a policy to solve planning problems directly or
to generate lookahead states within a best first search  bfs  algorithm  both techniques allow
the planner to avoid state evaluations  which helps in the objective of improving scalability  the
approach has been implemented in a system we have called roller  this work is an improvement
of a previous one  de la rosa  jimenez    borrajo         alternatively  a roller version for
repairing relaxed plans  de la rosa  jimenez  garca duran  fernandez  garca olaya    borrajo 
      competed in the learning track of the  th international planning competition  ipc  held in
      roller improvements presented in this article are mainly a result of lessons learned from
the competition  which will be discussed later 
the paper is organized as follows  section   introduces the issues that need to be considered
when designing a learning system for heuristic planning  they will help us to clarify which decisions we made in the development of our approach  section   describes the roller system in
detail  section   presents the experimental results obtained in a variety of benchmarks  section  
discusses the improvements of the roller system compared to the previous version of the system 
section   revises the related work on learning dck for heuristic planning  finally  the last section
discusses some conclusions and future work 

   common issues in learning domain specific control knowledge
when designing an ml process for the automatic acquisition of dck  one must consider some
common issues  among others 
   the representation of the learned knowledge  predicate logic is a common language to
represent planning dck because planning tasks are usually defined in this language  however  other representation languages have been used aiming to make the learning of dck
more effective  for instance  languages for describing object classes such as the concept
language  martin   geffner        or taxonomic syntax  mcallester   givan        have
been shown to provide a useful learning bias for different domains 
another representation issue is the selection of the feature space  i e   the set of instance
features used for representing the learned knowledge and for training the system    the feature
space should be able to capture the key knowledge of the domain  traditionally  the feature
   

fis caling up h euristic p lanning with r elational d ecision t rees

space consisted only of predicates for describing the current state and the goals of the planning
task  the feature space can be enriched with extra predicates  called metapredicates  which
capture extra useful information of the planning context such as the applicable operators or
the pending goals  veloso  carbonell  perez  borrajo  fink    blythe         recently  works
on learning dck for heuristic planners define metapredicates to capture information about
the planning context of a heuristic planner  including for example  predicates which capture
the actions in the relaxed plan of a given state  yoon  fern    givan        
   the learning algorithms  inductive logic programming  ilp   muggleton   de raedt 
      deals with the development of inductive techniques which learn a given target concept
from examples described in predicate logic  because planning tasks are normally represented
in predicate logic  ilp algorithms are quite suitable for dck learning  moreover  in recent
years  ilp has broadened its scope to cover the whole spectrum of ml tasks such as regression  clustering and association analysis  extending the classical propositional ml algorithms
to the relational framework  consequently  ilp algorithms have been used by heuristic planners to capture dck in different forms such as decision rules to select actions in the different
planning context or regression rules to obtain better node evaluations  yoon et al         
   the generation of training examples  the success of ml algorithms depends directly on
the quality of the training examples used  when learning planning dck  these examples
are extracted from the experience collected from solving training problems  which should
be representative of different tasks across the domain  therefore  the quality of the training
examples will depend on the variety of the problems used for training and the quality of the
solutions to these problems  traditionally  these training problems are obtained by random
generators provided with some parameters to tune problems difficulty  in this way  one has
to find  for each domain  which kind of problems makes the learning algorithm generalize
useful dck 
   use of the learned dck  decisions made for each of these three issues affect the quality of
the learned dck  some representation schemes may not be expressive enough to capture effective dck for a given domain  the learning algorithm may not be able to acquire the useful
dck within reasonable time and memory requirements  or the set of training problems may
lack significant examples of the key knowledge  in all these situations  a direct use of the
learned dck will not improve the scalability of the planner  and could even decrease its performance  an effective way of dealing with this problem in heuristic planners is integrating
the learned dck within robust strategies such as a best first search  yoon et al         or
combining it with domain independent heuristic functions  roger   helmert        

   the roller system
this section describes how the general scheme for learning dck is instantiated in the roller
system  first  it describes the dck representation followed by roller  second  it explains the
learning algorithm used by roller  third  it depicts how roller collects good quality training
examples and finally  it shows different approaches for scaling up heuristic planning algorithms with
the learned dck 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

    the representation of the learned knowledge  helpful contexts in heuristic planning
we present our approach following the notation specified by the planning domain definition language  pddl  for typed strips tasks  accordingly  the definition of a planning domain d comprises the definition of 
 a hierarchy of types 
 a set of typed constants  cd   representing the objects present in all tasks for the domain  this
set can be empty 
 a set of predicate symbols  p  each one with its corresponding arity and the type of its
arguments 
 a set of operators o  whose arguments are typed variables 
variables are declared directly when defining each operator argument  so they are local to the
operator definition  we will call po the set of atomic formulas that can be generated using the
defined predicates p  the variables defined as arguments of the operator o  o  and the general
constants cd   then  each operator o  o is defined by three sets  pre o   po   the operator
preconditions  add o   po   the positive effects  and del o   po   the negative effects of the
operator 
a planning task  for the domain d is a tuple   c   s    g   where c is a set of typed
constants representing the objects which are particular to the task  s  is the set of ground atomic
formulas describing the initial state and g is the set of ground atomic formulas describing the goals 
given the total set of constants c   c  cd   the task  defines a finite state space s and a finite set
a of instantiated operators over o  a state s  s is a set of ground atomic formulas representing the
facts that are true in s  states are described following the closed world assumption  an instantiated
operator or action a  a is an operator where each variable has been replaced by a constant in c of
the same type  thus  a is the set of all actions a that can be generated using the set of constants c
and the set of operators o  under this definition  solving a planning task  implies finding a plan 
as the sequence of actions  a            an    ai  a that transforms the initial state into a state in which
the goals are achieved 
the planning contexts defined by roller rely on the concepts of relaxed plan heuristic and
helpful actions  both introduced by the ff planner  hoffmann   nebel         the relaxed plan
heuristic returns an integer for each evaluated node  which is the number of actions in a solution to
the relaxed planning task   from that node    is a simplification of the original task in which the
deletes of actions are ignored  the idea of delete relaxation for computing heuristics in planning
was first introduced by mcdermott        and by bonet  loerincs and geffner        
the relaxed plan is extracted from a relaxed planning graph  which is a sequence of facts and
actions layers  f    a            at   ft    the first fact layer contains all facts in the initial state  then
each action layer contains the set of all applicable actions given the previous fact layer  each fact
layer contains the set of all positive effects of all actions appearing in the previous layers  the
process finishes when all the goals are in a fact layer  or when two consecutive facts layers have the
same facts  in the last case  the relaxed problems have no solution and the relaxed plan heuristic
returns infinity 
   

fis caling up h euristic p lanning with r elational d ecision t rees

once the relaxed planning graph is built  the solution is extracted in a backwards process  each
goal appearing for the first time in fact layer i is assigned to the set of goals of that layer  gi   then 
from the last set of goals  gt   to the second set of goals  g    and for each goal in each goals set 
an action is selected which generates the goal and whose layer index is minimal  afterwards  each
precondition of that action  i e  a subgoal  is included in the goals set corresponding to the first
layer where this fact appears  when the process is finished  the set of selected actions comprises the
relaxed plan 
according to the extraction process  the ff planner marks as helpful actions the set of actions
in the first layer a  of the relaxed planning graph which can achieve any of the subgoals of the next
fact layer  i e  in the goals set g    in other words  helpful actions are those applicable actions which
generate facts that are top level goals of the problems or required by any action of the relaxed plan 
formally  the set of helpful actions of a given state s is defined as 
helpful  s     a  a    add a   g      
the ff planner uses helpful actions in the search as a pruning technique  because they are considered as the only candidates for being selected during the search  given that each state generates
its own particular set of helpful actions  we claim that the helpful actions  together with the remaining goals and the static literals of the planning task  encode a helpful context related to each state 
the helpful actions and the remaining target goals relate actions that are more likely to be applied
with the goals that need to be achieved  these relations arise because helpful actions and target
goals often share some arguments  problem objects   additionally  the static predicates express
facts that characterize objects of the planning task  identifying these objects is also relevant since
they may be shared arguments for helpful actions and or target goals 
definition   the helpful context for a state s is defined as
h s     helpful  s   target s   static s  
where target s   g describes the set of goals not achieved in the state s  target s    g  s
and static s  is the set of literals that always hold in the planning task  they are defined in the
initial state and are present at every state given that they can not be changed by any action  thus 
static s     p  s    a  a   p  add a   p  del a   
the helpful context is an alternative representation to the tuple  state  goals  applied action  
traditionally used when learning dck for planning  helpful contexts present some advantages for
improving the scalability of heuristic planners 
 in most domains  the set of helpful actions contains the actions most likely to be applied and
focusing reasoning on them has been shown to be a good strategy 
 the set of helpful actions is normally smaller than the set of non static literals in the state
 i e   s  static s    thus  the process of matching learned dck within the search obtains
the benefits of using a more compact representation 
 the number of helpful actions normally decreases when the search has fewer goals left 
therefore  the matching process will become faster when the search is advancing towards
the goals 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

    the learning algorithm  learning generalized policies with relational decision trees
roller implements a two step learning process for building dck from a collection of examples
from different helpful contexts 

   learning the operator classifier  roller builds a classifier to choose the best operator in the
different helpful contexts 
   learning the binding classifiers  for each operator in the domain  roller builds a classifier
to choose the best binding  instantiation of the operator  in the different helpful contexts 
the learning process is split into these two steps to build dck with off the shelf learning
tools  each planning action may have different number of arguments and arguments of different types  e g  actions switch on instrument satellite  and turn to satellite 
direction direction  from the satellite domain  which hinders the definition of the target
classes  this two step decision process is also clearer from the decision making point of view  it
helps users to understand the generated dck better by focusing on either the decision of which operator to apply or which bindings to use for a given selected operator  both the learning algorithm
and the set of learning examples are the same for the two learning steps  figure   shows an overview
of the learning process of the roller system 

roller learner
training
problems

   operator classifier
example
generator

pddl
domain

op  examples

relational
classification

bind  examples

tool

   binding classifiers

   

language bias

figure    overview of the roller learning process 

      l earning r elational d ecision t ress
a classic approach to assist decision making consists of gathering a significant set of previous decisions and building a decision tree that generalizes them  the leaves of the resulting tree contain
the classes  decisions to make   and the internal nodes contain the conditions that lead to those decisions  the most common way to build these trees is following the top down induction of decision
trees  tdidt  algorithm  quinlan         this algorithm builds the tree by repeatedly splitting
the set of training examples according to the conditions that minimize the entropy in the examples  traditionally  training examples are described in an attribute value representation  therefore 
conditions of the decision trees represent tests over the value of a given attribute of the examples 
nevertheless  this attribute value approach is not suitable for representing decisions if we want to
keep the predicate logic representation  a better approach is to represent decisions relationally  for
instance  a given action is chosen to reach certain goals in a given context if they share some arguments  recently  new algorithms for building relational decision trees from examples described as
   

fis caling up h euristic p lanning with r elational d ecision t rees

predicate logic facts have been developed  these new relational learning algorithms are similar to
the propositional ones  except that     condition nodes in the tree do not refer to attribute values  but
to logic queries about relational facts holding in the training examples and      these logic queries
can share variables with condition nodes placed above in the decision tree  the learning algorithm
is a greedy search process  since the space of potential relational decision trees is usually huge  this
search is normally biased according to a specification of syntactic restrictions called language bias 
this specification contains the target concept  the predicates that can appear in the condition nodes
of the trees and some learning specific knowledge such as type information  or input and output
variables of predicates 
in this paper we use the tool tilde  blockeel   de raedt        for learning the operator
and binding classifiers  this tool implements a relational version of the tdidt algorithm  although
any other off the shelf tool for learning relational classifiers could have been used  such as pro gol  muggleton        or ribl  emde   wettschereck         each of these different learning
algorithms would provide different results  since they explore the classifiers space differently  the
study of the pros and cons of the different algorithms is beyond the scope of the paper  for a
comprehensive explanation of current relational learning approaches please refer to the work by de
raedt        
      l earning the o perator c lassifier
the inputs to learning the operator classifier are 
 training examples  examples are represented in a prolog like syntax and consist of the
operator selected  the class  together with the helpful context  the background knowledge in
terms of relational learning  in which it was selected  in particular  an example contains 
 class  we use the predicate of arity   selected to encode the operator chosen in the
context  this predicate is the target concept of this learning step  its first argument holds
the example identifier that links the rest of the example predicates  the second argument
is the problem identifier  which links the static predicates shared by all examples coming
from the same planning problem  the third argument is the example class  i e   the name
of the selected operator in the helpful context 
 helpful predicates  they are predicates to express the helpful actions contained in the
helpful context  the predicate symbol of these predicates is helpful ai where ai
is the name of an instantiated action  the arguments are the example and problem
identifier together with the parameters of action ai   as it is an instantiated action  its
parameters are constants 
 target goal predicates  they represent the predicates that appear in the goals and do
not hold in the current state  these predicates have the form target goal gi where
gi are the domain predicates  each predicate also contains the example and problem
identifiers 
 static predicates  they represent the static predicates of a given problem  these predicates are shared by all the training examples that belong to the same planning problem 
they have the form static fact fi where fi are the domain predicates that do not
appear in the effects of any domain action  they have as arguments the problem identifier and the corresponding arguments of each domain predicate 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

figure   shows one learning example with id tr   e  consisting of a selection of the operator switch on and its associated helpful context  this example is used for building the
operator classifier for the satellite domain 
  example tr   e  from problem tr  
selected tr   e  tr   switch on  
helpful turn to tr   e  tr   satellite  groundstation  star   
helpful turn to tr   e  tr   satellite  phenomenon  star   
helpful turn to tr   e  tr   satellite  phenomenon  star   
helpful turn to tr   e  tr   satellite  phenomenon  star   
helpful switch on tr   e  tr   instrument  satellite   
target goal have image tr   e  tr   phenomenon  infrared   
target goal have image tr   e  tr   phenomenon  infrared   
target goal have image tr   e  tr   phenomenon  spectrograph   
  static predicates of problem
static fact calibration target tr   instrument  groundstation   
static fact supports tr   instrument  spectrograph   
static fact supports tr   instrument  infrared   
static fact on board tr   instrument  satellite   

figure    knowledge base corresponding to an example from the satellite domain  the example
has the id tr   e   which links all example predicates  it was obtained solving the
training problem tr   which links the rest of examples for the same problem  the
selected operator in this helpful context is switch on  which corresponds to one of the
helpful actions encoded in the helpful predicates of the example 

 language bias  this bias specifies constraints over the arguments of the predicates in the
training examples  we do not assume any domain specific constraint  given that our learning
technique is domain independent  so  this bias only contains restrictions over argument types
and restrictions which ensure that identifier variables are not added as new variables in the
classifier generation  this bias is automatically extracted from the pddl domain definitions
and consists of a declaration of the predicates used in the learning example and their argument
types  figure   shows the language bias specified for learning the operator classifier for the
satellite domain 
the resulting relational decision tree represents a set of disjoint rules for action selection that
can be used to provide advice to the planner  the internal nodes of the tree contain the set of conditions related to the helpful context under which the advice can be provided  the leaf nodes contain
the corresponding advice  in this case  the operator to select and the number of examples covered
by the rule  the operator to select is the one which has been selected more often in the training
examples covered by the rule  the operator classifiers learned by roller also advise on nonhelpful actions  given a state  non helpful actions are the subset of applicable actions in the state
that are not considered as helpful actions  certainly  these actions are not part of the helpful contexts defined  however  the learned operator classifiers indicate the name of the operator to select
regardless of whether it was helpful or not  figure   shows the operator tree learned for the satellite
   

fis caling up h euristic p lanning with r elational d ecision t rees

       the target concept    predict selected  idexample  idproblem  operator   
type selected idex idprob class   
classes  turn to switch on switch off calibrate take image   
       the helpful context      predicates for the helpful actions
rmode helpful turn to  idexample  idproblem   s    d    d    
type helpful turn to idex idprob satellite direction direction   
rmode helpful switch on  idexample  idproblem   i    s    
type helpful switch on idex idprob instrument satellite   
rmode helpful switch off  idexample  idproblem   i    s    
type helpful switch off idex idprob instrument satellite   
rmode helpful calibrate  idexample  idproblem   s    i    d    
type helpful calibrate idex idprob satellite instrument direction   
rmode helpful take image  idexample  idproblem   s    d    i    m    
type helpful take image idex idprob satellite direction instrument mode   
  predicates for the target goals
rmode target goal pointing  idexample  idproblem   s    d    
type target goal pointing idex idprob satellite direction   
rmode target goal have image  idexample  idproblem   d    m    
type target goal have image idex idprob direction mode   
  predicates for the static facts
rmode static fact on board  idproblem   i    s    
type static fact on board idprob instrument satellite   
rmode static fact supports  idproblem   i    m    
type static fact supports idprob instrument mode   
rmode static fact calibration target  idproblem   i    d    
type static fact calibration target idprob instrument direction   

figure    language bias for learning the operator classifier of the satellite domain  it is automatically generated from the pddl definition  rmode predicates indicate those which can
be used in the tree  type predicates indicate types for each particular rmode 

domain  in learned decision trees each branch is denoted by the symbols      yes no   where
yes indicates the next node for positive answers to the current question and no indicates the next
node for negative answers  in the figure  the first branch states that when there is a calibrate
action in the set of helpful actions  the recommendation  in square brackets  is choosing that action
 i e  calibrate   in addition  the branch indicates that the recommended action has occurred   
times in the training examples  moreover  each leaf node has information  in double square brack   

fid e la rosa   j imenez   f uentetaja   b orrajo

ets  about the number of times each type of action has been selected in the training examples covered
by the rule in the current branch  thus  in our case  the action calibrate has been selected   
out of a total of    times  and other operators have never been selected  the second branch says
that if there is no calibrate helpful action  but there is a take image one  the planner selected
to take image     out of     times  if there are no helpful calibrate or take image actions but there is a helpful switch on action  switch on is the recommendation  that has been
selected    out of    times  other tree branches are interpreted similarly 
selected  a  b  c 
helpful calibrate a b  d  e  f   
   yes  calibrate         turn to     switch on     switch off     
 
calibrate      take image      
   no  helpful take image a b  g  h  i  j   
   yes  take image          turn to     switch on     switch off     
 
calibrate     take image        
   no  helpful switch on a b  k  l   
   yes  switch on         turn to      switch on      
 
switch off     calibrate     
 
take image      
   no   turn to          turn to       switch on     
switch off     calibrate     
take image      

figure    relational decision tree learned for the operator selection in the satellite domain  internal
nodes  with   ending  have queries to helpful contexts  leaf nodes  in brackets  have
the class and the number of observed examples for each operator 

      l earning the b inding c lassifiers
at the second learning step  a relational decision tree is built for each domain operator o  o  these
trees indicate the bindings to select for o in the different helpful contexts  the inputs for learning
the binding classifier of operator o are 
 training examples  these consist exclusively of the helpful contexts where operator o was
selected  together with the applicable instantiations of o in these contexts  note that for a
given helpful context  the applicable instantiations of o may include both helpful and nonhelpful actions  helpful contexts are coded exactly as in the previous learning step  the
applicable instantiations of o are represented with the selected o predicate  this predicate is the target concept of the second learning step and its arguments are the example
and problem identifiers  the instantiated arguments of the applicable action and the example class  selected or rejected   the purpose of this predicate is to distinguish between
good and bad bindings for the operator  figure   shows a piece of the knowledge base
for building the binding tree corresponding to the action switch on from the satellite domain  this example  with id tr   e    resulted in the selection of the action instantiation
   

fis caling up h euristic p lanning with r elational d ecision t rees

switch on instrument  satellite    the action switch on instrument  
satellite   was also applicable but it was rejected by the planner 
 language bias  the bias for learning binding trees is the same as the bias for learning the
operator tree  except that it includes the definition of the selected o predicate  as in the
previous learning step  the language bias for learning a binding tree is also automatically extracted from the pddl domain definition  figure   shows part of the language bias specified
for learning the binding tree for the action switch on from the satellite domain 

  example tr   e   from problem tr  
selected switch on tr   e   tr   instrument  satellite  rejected  
selected switch on tr   e   tr   instrument  satellite  selected  
helpful switch on tr   e   tr   instrument  satellite   
helpful switch on tr   e   tr   instrument  satellite   
helpful turn to tr   e   tr   satellite  star  star   
helpful turn to tr   e   tr   satellite  star  star   
helpful turn to tr   e   tr   satellite  phenomenon  star   
helpful turn to tr   e   tr   satellite  phenomenon  star   
target goal have image tr   e   tr   phenomenon  spectrograph   
target goal have image tr   e   tr   phenomenon  spectrograph   
target goal have image tr   e   tr   star  image   
  static predicates of problem
static fact calibration target tr   instrument  star   
static fact calibration target tr   instrument  star   
static fact supports tr   instrument  image   
static fact supports tr   instrument  spectrograph   
static fact supports tr   instrument  image   
static fact supports tr   instrument  image   
static fact on board tr   instrument  satellite   
static fact on board tr   instrument  satellite   

figure    knowledge base corresponding to the example tr   e   obtained by solving the training problem tr   from the satellite domain 
the result of this second learning step is a relational decision tree to for each uninstantiated
operator o  o  to consists of the set of disjoint rules for the binding selection of o  figure  
shows an example of the binding tree tswitch on built for operator switch on from the satellite
domain  according to this tree  the first branch states that when there is a helpful action which is
a switch on of instrument c in satellite d  these switch on bindings  c  d  were selected
by the planner     out of     times  note that binding trees learned by roller also advise on
non helpful actions  frequently  the selected o predicate matches with tree queries that refer to
helpful o predicates  in these cases  the no branch of the query may cover bindings of non helpful
actions for this operator 
for the other binding trees of the satellite domain we refer the reader to the online appendix
of this article  where we include the learned dck for the domains used in the experimental section 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

       the target concept    predict selected switch on  idexample  idproblem  inst   sat   class   
type selected switch on idex idprob instrument satellite class   
classes  selected rejected   
       the helpful context       the same as in the operator classification
   

figure    part of the language bias for learning the binding tree for the switch on action from
the satellite domain 

selected switch on  a  b  c  d  e 
helpful switch on a b c d   
   yes   selected          selected       rejected       
   no   rejected         selected     rejected       

figure    relational decision tree learned for the bindings selection of the switch on action from
the satellite domain 

in many cases  decision trees are somewhat more complex that the one shown in figure    for
instance  the turn to binding tree has    nodes and includes several queries about target goals  e g  
asking if there is a pending image at the new pointed direction  and others about static facts  e g  
asking if the new pointed direction is a calibration target  
    generation of training examples
roller training examples are instances of decisions made when solving training problems  in order
to characterize a variety of good solutions  these decisions should consider different alternatives for
solving each individual problem  at a given search tree node  state   the alternatives come from the
possibility of choosing different operators or of having different bindings for a single operator  in
both cases assuming the alternative will lead to equally good solutions 

regarding binding decisions  if actions from some alternative solutions are ignored  they are
tagged as rejected and consequently they introduce noise in the learning process  for instance 
consider the problem of figure   from the satellite domain in which a satellite  with a calibrated
instrument  must turn to directions d  d  and d  in order to take images there  in this planning context  the three turn to actions are helpful actions and regarding only one solution makes
learning consider one action as selected and the other two actions as rejected  however  the learned
knowledge should always recommend a helpful turn to action towards a direction where the
satellite  with the corresponding calibrated instrument  has to take an image  to learn such kind of
knowledge  roller should consider the three turn to actions as selected because the three actions correspond to selectable actions for learning the correct knowledge in this particular planning
   

fis caling up h euristic p lanning with r elational d ecision t rees

context  if most of these actions are marked as rejected the learner will consider selecting turn to
in the described context as a bad choice 
takeimage d 

s 

s 

turnto d  d  

s 

takeimage d 

turnto d  d  
takeimage d 
turnto d  d  

s 

turnto d  d  
s 

s 

turnto d  d  
s  

   

s 

turnto d  d  
s 

   

   

figure    solution path alternatives in a simplified satellite problem 
regarding operator decisions  complete training with a full catalogue of different solutions can
confuse the learning process  for instance  consider the example problem of figure   where the
goal is to take an image at direction d   before applying calibrate action in s    it is necessary
to switch on the instrument t and to turn the satellite to d   the calibration target direction   these
two actions are helpful in so and generate two different solution paths  in fact  they are commutative 
generalizing operator selection from these kinds of helpful contexts is difficult when the training
examples contain examples of both types  i e  examples where the switch on action is situated
before the turn to action and vice versa   this is caused by the fact that for the same helpful
context there are different operators to choose from and all of them are equally good choices 
s 
turnto d  d  

switchon t

s 

s 

switchon t

calibrate t

s 

turnto d  d  

s 

takeimage d 

g

turnto d  d  
s 

figure    solution path alternatives in a simplified satellite problem 
roller follows a commitment approach for the generation of training examples      generation of solutions  given a training problem  roller performs an exhaustive search to obtain
multiple best cost solutions  taking into account the alternatives of different binding choices     
selection of solutions  roller selects a subset of solutions from the set of best cost solutions in
order to reproduce a particular preference for the operator alternatives      extraction of examples from solutions  roller encodes the selected subset of solutions as examples for the required
learning  operator classification or binding classification  the following sections detail how roller
proceeds at each of these three steps 

   

g

fid e la rosa   j imenez   f uentetaja   b orrajo

      g eneration of s olutions
roller solves each training problem using a best first branch and bound  bfs bnb  algorithm
that extracts multiple good quality solutions  if the search space has not been explored exhaustively
within a time bound  the problem is discarded and no examples are generated from it  therefore 
training problems need to be sufficiently small  in addition  training problems need to be representative enough to generalize the dck which assists roller when solving future problems in the
same domain 
the bfs bnb search is completed without pruning repeated states  in practice  many repeated
states are generated by changing the order among actions of different solution paths  thus  pruning
repeated states would involve tagging actions leading to these solutions as rejected bindings  which
is in fact not true  in addition  the bfs bnb algorithm prunes according to the a evaluation
function f  n    g n    h n   where g n  is the node cost  in this work we use plan length as the
cost function  and h n  is the ff heuristic  the safe way to prune the search space is by using an
admissible heuristic  however  existing admissible heuristics will not allow roller to complete
an exhaustive search in problems of reasonable size  in practice  using the ff heuristic produces
few overestimations which introduces negligible noise into the learning process  at the end of the
search  the bfs bnb algorithm returns the set of solutions with the best cost  these solutions are
used to tag the nodes in the search tree that belong to any of the solutions with the label on solution 

      s electing s olutions
from the set of best cost solutions found  roller selects the subset of solutions that will be used
for generating training examples  since it is difficult to develop domain independent criteria for systematically selecting solutions that reproduce the same operator selection in a particular context  we
have defined an approach which  heuristically  prefers some actions over others  these preferences
are 
 least commitment preference  prefer actions that generate more alternatives of different solution paths 
 difficulty preference  prefer actions that reach the goals or sub goals which are most difficult
to achieve  in the example of figure   having instrument t switched on is only achievable by
one action  on the other hand  pointing to direction d  is considered easier since it can be
achieved with actions turn to d  d   and turn to d  d   
given      a            an   a best cost plan for a planning task  we compute these preferences with
functions depending on each action 
commitment  ai       a    a   successors ai    on solution a      
where the function successors ai   returns all applicable actions in state si   and function on solution a 
verifies whether an action is tagged as being part of a best cost plan 
difficulty  ai    

min

 
  supporters l   

ladd ai  

where the function supporters l     a  a   l  add a   returns the set of actions that achieve the
literal l 
   

fis caling up h euristic p lanning with r elational d ecision t rees

solutions are ranked according to these preferences  the ranking for each solution     
a            an is computed as the weighted sum of the action preferences  as follows 
ranking         

x
i       n 

 n  i 
  ai    
n

where n is the plan length and  is one of commitment or difficulty   this sum is weighted to
give more importance to the preferences in the first actions of the plan  the first action preference
value is multiplied by    the second by  n     n  and so on  otherwise  several alternatives  i e  
commutative actions in different positions within a plan  would lead to the same ranking value  we
compute the ranking for all best cost solutions using commitment   ties in this ranking are broken
with the ranking computed with difficulty   the subset of solutions with the best ranking values is
the subset of solutions selected for generating training examples 
      e xtracting e xamples from s olutions
takes the subset of solutions selected at the previous step and generates training examples  when generating examples for the operator classification  roller takes solution plans     
 a    a         an   which correspond to the sequence of state transitions  s    s         sn   and generates
one learning example for each pair   si   ai     consisting of h si   and the class  i e   the operator
name of action ai      see learning example shown in figure   
when generating examples for the binding classification of operator o  roller only considers
pairs   si   ai     where ai   matches operator o  a learning example generated from the pair
  si   ai     for the binding selection of the operator o consists of h si   and the classes of all
applicable actions in si that match o  including ai     applicable actions with the on solution label
belong to the selected class and all other applicable actions to the rejected class  moreover  actions
belonging to solutions not in the top ranking are still marked as selected even though they are not
nodes from which an example is generated  see learning example shown in figure   
roller

    use of the learned knowledge  planning with relational decision trees
this section details how to make heuristic planning benefit from our dck  beginning with how
we build action orderings with the learned dck  then  it explains two different search strategies
to exploit these orderings      the application of the dck as a generalized action policy  depthfirst h context policy algorithm  and     the use of the dck to generate lookahead states within a
best first search  bfs  guided by the ff heuristic  h context policy lookahead bfs algorithm  
      o rdering actions with r elational d ecision t rees
given a state s  the expression app s  denotes the set of actions applicable in s  the learned dck
provides an ordering for app s   the ordering is built by matching each action a  app s  first with
the operator classifier and then with the corresponding binding classifier  figure    shows in detail
the algorithm for ordering applicable actions with relational decision trees 
the algorithm divides the set of applicable actions into two subsets  the helpful actions  and
the non helpful actions  then  it matches the helpful context of the state  i e   h s   with the tree
for the operator classification  this matching provides a leaf node that contains the list of operators
sorted by the number of examples covered by the leaf during the training phase  see the operator
   

fid e la rosa   j imenez   f uentetaja   b orrajo

dt filter sort  a h t  sorted list of applicable actions
a  list of actions
h  helpful context
t  decision trees

selected actions   
ha   helpful actions a  h 
non ha   a   ha
leaf node   classify operators tree t  h 
for each a in ha do
priority a    leaf node operator value leaf node  a 
if priority a      then
 selected a  rejected a     classify bindings tree t  h  a 
selected a 
selection ratio a   selected a  rejected a 
priority a    priority a    selection ratio a 
selected actions   selected actions  a 
max ha priority   maxaselected actions priority a 
for each a in non ha do
priority a    leaf node operator value leaf node a 
if priority a    max ha priority then
 selected a  rejected a     classify bindings tree t  h  a 
selected a 
selection ratio a   selected a  rejected a 
priority a    priority a    selection ratio a 
selected actions   selected actions  a 
return sort selected actions  priority 

figure     algorithm for ordering actions using relational decision trees 
classification tree in figure     the number of examples covered gives an operator ordering that
can be used to prefer actions during the search  the algorithm uses this number to initialize the
priority value for each helpful action  taking the value of the corresponding operator  the algorithm
keeps only helpful actions that have at least one matching example  for each of these actions  the
algorithm matches the action with its corresponding binding classification tree  the resulting leaf
of the binding tree returns two values  the number of times the ground action was selected  and the
number of times it was rejected in the training phase  we define the selection ratio for the ground
action as 
selected a 
selection ratio a   
selected a    rejected a 
this ratio represents the proportion of good bindings covered by a particular leaf of the binding
tree  when the denominator is zero  the selection ratio is assumed to be zero  the priority of the
action is updated by adding this selection ratio  thus  the final priority for an action is higher for
   

fis caling up h euristic p lanning with r elational d ecision t rees

actions with operators for which the operator classification tree provides higher values  i e  they
have been selected more often in the training examples  since the selection ratio remains between  
and    adding up to this number can be considered as a method for breaking ties in the initial priority
value  using the information in the binding classification tree 
the priority for non helpful actions is computed in a similar way except that  in this case  the
algorithm only considers actions whose initial priority  the value provided by the operator classification tree   is higher than the maximum priority of the helpful actions  in this manner  we capture
useful non helpful actions  ff follows a heuristic criterion to classify an action as helpful  although
this heuristic has been shown to be very useful  the case may arise in which the most useful action at
a particular moment is not classified as helpful  decision trees capture this information  given that
they can recommend choosing a non helpful action  the described method takes advantage of this
fact and defines a way of using such information when applying the learned knowledge  an alternative approach would be to extend the planning context with a new meta predicate for non helpful
actions  however  it does not pay off in a variety of problems and domains because it means significantly larger contexts  which causes more expensive matching  finally  the selected actions are
sorted in order of decreasing priority values  the sorted list of actions is the output of the algorithm 
      t he h c ontext p olicy a lgorithm
the helpful context action policy algorithm moves forward  applying at each state the best action
according to the dck  the pseudo code of the algorithm is shown in figure     the algorithm
maintains an ordered open list  the open list contains the states to be expanded which are extracted
in order  once extracted  each state is evaluated using the ff heuristic  thus  we evaluate upon
extraction and not when nodes are included in the open list  the evaluation provides the heuristic
value for the state  h  and the set of helpful actions ha  which are needed to generate the helpful
context  the heuristic value is only used for      continuing the search when the state is a recognized
dead end  h      and     goal checking  h       then  the helpful context is generated  subsequently  the algorithm obtains the set aa of actions applicable in the state and sorts them using the
decision trees  as shown above in the algorithm in figure      the result is aa   aa  a sorted list
of applicable actions  the algorithm inserts the successors generated by actions in aa  at the beginning of the open list preserving their ordering  function push ordered list in open  
furthermore  to make the algorithm complete and more robust  successors generated by applicable
actions that are not in aa  are included in a secondary list called delayed list  the delayed list is
only used when the open list is empty  in that case  only one node of the delayed list is moved to
the open list and then  the algorithm continues extracting nodes from the open list 
in this algorithm  each node maintains a pointer to each parent in order to recover the solution
once it has been found  also  each node maintains its g value  i e  the length of the path from the
initial state up to the node  the function push ordered list in open only inserts in the
open list those candidates that      are not repeated states  or     are repeated states with lower g
value than the previous one  otherwise  repeated states are pruned  this type of pruning guarantees
that we maintain for each node the shortest solution found 
in other words  the proposed search algorithm is a depth first search with delayed successors 
the benefit of this algorithm is that it exploits the best action selection when the policy is per   

fid e la rosa   j imenez   f uentetaja   b orrajo

depth first h context policy  i  g  t    plan
i  initial state
g  goals
t   decision trees

open list    i  
delayed list    
while open list     do
n   pop open list 
 h  ha    evaluate n  g    compute ff heuristic  
if h    then   recognized dead end  
continue
if h     then   goal state  
return path i  n 
h   helpful context ha  g  n 
aa   applicable actions n 
aa   dt filter sort aa  h  t  
candidates   generate successors n  aa 
open list   push ordered list in open candidates open list 
delayed candidates   generate successors n  aa   aa 
delayed list   push ordered list delayed candidates  delayed list 
while open list    and delayed list     do
open list     pop delayed list   
return fail

figure     a depth first algorithm with a sorting strategy given by the dck 
fect  and the action ordering when is not  particularly  perfect dck will be directly applied in a
backtrack free search and inaccurate dck will force the search algorithm to backtrack 
      t he h c ontext p olicy in a l ookahead s trategy
in many domains the learned dck may contain flaws  the helpful context may not be expressive
enough to capture good decisions  the learning algorithm may not be able to generalize well or
the training examples may not be representative enough  in these cases  a direct application of the
learned dck  without backtracking  may not allow the planner to reach the goals of the problem 
poor quality in the learned dck can be balanced with a guide of a different nature such as
a domain independent heuristic  a successful example is the obtusewedge system  yoon et al  
      that combined a learned generalized policy with the ff heuristic  obtusewedge exploited
the learned policy to synthesize lookahead states within a lookahead strategy  lookahead states
   with perfect policy we refer to a policy that leads directly to a goal state  our policies are not guaranteed to be perfect
given that they are generated by inductive learning 

   

fis caling up h euristic p lanning with r elational d ecision t rees

were first applied in heuristic planning by the yahsp planner  vidal         they are intermediate
states that are frequently closer to a goal state than the direct descendants of the current state  these
intermediate states are added to the list of nodes to be expanded so that they can be used within
different search algorithms  when the learned policy contains flaws  lookahead states synthesized
with the policy may not provide good guidance for the search  however  if these lookahead states
are included in a complete search algorithm that also considers ordinary successors  the search
process becomes more robust  in general  the use of lookahead states in a forward state space
search slightly increases the branching factor of the search process  but overall  as shown by the
yahsp planner at ipc      and in the experiments included in the yahsp paper  vidal         the
approach seems to improve the performance significantly 
figure    shows a generic algorithm for using lookahead states generated from a policy during
the search  this algorithm is a weighted best first search  bfs   with the only modification being that one or more lookahead states are inserted into the open list when expanding a node  as
in weighted bfs  nodes to be expanded are maintained in an open list ordered by the evaluation
function f  n      h n    g n   apart from the usual arguments of bfs  the algorithm receives
the policy  p   and the horizon  the horizon represents the maximum number of policy steps that
are applied for generating the lookahead states  in the experiments  we will use this algorithm with
the ff heuristic as h n  

h context policy lookahead bfs  i g t  horizon   plan
i  initial state
g  goals
t   decision trees  policy 
horizon  horizon
open list   
add to open i 
while open list     do
n   pop open list 
if goal state n  g 
return path i  n 
add to open lookahead successors n  g  t  horizon 
add to open standard successors n 
return fail

figure     a generic lookahead bfs algorithm 
the heuristic evaluation  h n   the g value  g n   and the set of helpful actions  are also saved
at each node when the node is evaluated  the function add to open state  evaluates the
state and inserts it in the open list  which is ordered in increasing values of the evaluation function 
f  n   this function also prunes repeated states  following the strategy described for the depthfirst h context policy algorithm  only repeated states with higher g n  than the existent one are
   

fid e la rosa   j imenez   f uentetaja   b orrajo

pruned  the function add to open standard successors n  calls add to open for
each successor of the node n  the function add to open lookahead successors is explained below 
we have adapted the generic lookahead bfs algorithm to our learned dck  our particular
instantiation of the function add to open lookahead successors is shown in figure    
in our case  lookahead states are generated by iteratively applying the first action in the action ordering provided by the dck  the inputs to the algorithm are the current state  the problem goals  the
decision trees and the horizon  first  our algorithm generates the helpful context and the applicable
actions  the helpful actions  n ha  are recovered from the node  then  the algorithm sorts the applicable actions using the decision trees  as previously shown in the algorithm in figure      after
that  the successor generated by the first action is inserted in the open list  and there is a recursive
call with this successor and the horizon decremented by one  the function add to open returns
true when its argument has been added to the open list and false otherwise  in fact  it returns
false in only two cases      the state is a repeated state with g value higher than the g value of
the existent state  or     the state is a recognized dead end  when the ordered list becomes empty 
the lookahead state can not be generated and the initial node is returned  the same occurs when
the horizon is zero  the described implementation is similar to the lookahead strategy approach
followed by obtusewedge  but instead we perform the lookahead generation using helpful contexts
and relational decision trees 
on the other hand  in the described h context policy lookahead bfs algorithm the search is
perfomed over the set of applicable actions of each node  however  in many domains the use of
helpful actions has shown to be a very good heuristic  one possible way of prioritizing helpful
actions over non helpful actions is to include in the open list only those successors given by helpful
actions  and to include the remaining successors in a secondary list  we have implemented this idea
following the same strategy used in the depth first h context policy algorithm  when the open list
becomes empty only one node is passed from the secondary list to the open list  and the search
continues  the algorithm is still complete given that we do not prune any successor  when helpful
actions are good enough  this strategy can save many heuristic evaluations  in the experiments
we will compare this strategy with the previous one  our intuition is that the adequacy of each
strategy depends directly on the quality of the helpful actions  the quality of the learned dck  and
the accuracy of the heuristic for each particular domain 
another technique for prioritizing helpful actions in bfs was implemented in yahsp  vidal 
      which inserts two consecutive instances of each node in the open list  these nodes have
equal f  n  since they represent the same state  the first one contains only the helpful actions  and
therefore  when expanded  it only generates successors resulting from these actions  the second
contains only non helpful actions  called rescue actions  in this way  all the successors with lower
f  n  than the parent node in the sub tree generated by helpful actions are expanded before any
successor resulting from non helpful actions 
we have performed some preliminary experiments  obtaining similar results for the two described methods for prioritizing helpful actions in bfs  the use of a secondary list for non helpful
actions  and the use of rescue nodes  for this reason  we only include results of the first technique
in the experimental section  we call this algorithm h context policy lookahead bfs ha 
   when the state is repeated but with a g value smaller than the existent one  add to open does not re evaluate but
instead takes the heuristic evaluation from the existent state 

   

fis caling up h euristic p lanning with r elational d ecision t rees

add to open lookahead successors  n g t  horizon   state
n  node  state 
g  goals
t   decision trees  policy 
horizon  horizon
if horizon     then
return n
h   helpful context n ha  g  n 
aa   applicable actions n 
aa    dt filter sort aa  h  t  
while aa      do
a   pop aa   
n    generate successor n  a 
added   add to open n   
if added then
if goal state n    g 
return n 
return add to open lookahead successors n    g  t   horizon    
return n

figure     algorithm for generating lookahead states from decision trees 

   experimental results
in this section we evaluate the performance of the roller system  the evaluation is carried out
over a variety of domains belonging to diverse ipcs  four domains come from the learning track of
ipc       gold miner  matching blocksworld  parking and thoughtful   the rest of the domains
 blocksworld  depots  satellite  rovers  storage and tpp  were selected from among the domains
of the sequential tracks from ipc between      and      because they presented different structure
and difficulty  and because they have available random problem generators  so that we can automatically build training sets for learning dck  for each domain  we complete a training phase in which
roller learns the corresponding dck and a testing phase in which we evaluate the scalability and
quality of the solutions found by roller with the learned dck  next  we detail the experimental
results obtained at each of these two phases  moreover  for each of the domains we give particular
details about training and test sets  the learned dck and the observed roller performance 
    training phase
for each domain  we built a training set of thirty randomly generated problems  the size and
structure of these problems is further discussed in the particular details given for each domain  as
explained in section      roller generates its training examples solving the problems from the
   

fid e la rosa   j imenez   f uentetaja   b orrajo

training set with a bfs bnb search  we set a time bound of    seconds to solve each training
problem  discarding those that are not exhaustively explored in this time bound  then  roller
generates the training examples from the solutions found and builds the corresponding decision
trees with the tilde system  blockeel   de raedt        
to evaluate the efficiency of roller at the training phase we computed the following metrics 
the time needed for solving the training problems  the number of training examples generated in
this process  the time spent by tilde in learning the decision trees and the number of leaves of the
operator selection tree  this last number gives a clue about the size of the learned dck  table  
shows the results obtained for each domain 
domain
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp

training
time  s 
     
     
      
     
     
     
    
     
     
     

learning
examples
    
   
   
   
   
    
    
   
   
   

learning
time  s 
    
    
   
    
   
    
    
   
     
    

tree
leaves
  
  
 
  
  
  
 
 
  
 

table    experimental results of the training process  training and learning times are shown  as
well as the number of training examples  and complexity of generated trees  number of
leaves  

achieves shorter learning times  fourth column in table    than the state of the art
systems for learning generalized policies  martin   geffner        yoon et al          particularly 
while these systems implement ad hoc learning algorithms that sometimes require hours in order to
obtain good policies  our approach only needs seconds to learn the dck for a given domain  this
fact makes our approach more suitable for architectures that need on line planning and learning processes  however  these learning times are not constant for different domains  because they depend
on the number of training examples  in our work  this number is given by the amount of different
solutions for the training problems   on the size of the training examples  in our work this size is
given by the number and arity of predicates and actions in the planning domain  and how training
examples are structured  i e   whether examples are easily separated by learning or not 
roller

    testing phase
in the testing phase roller attempts to solve  for each domain  a set of thirty test problems  these
problems are taken from the evaluation set of the corresponding ipc  when this evaluation set
contains more problems  these thirty problems are the thirty hardest ones  the depots domain is
an exception with twenty two problems  because the evaluation set for this domain at ipc     
   

fis caling up h euristic p lanning with r elational d ecision t rees

only contained those twenty two problems  three experiments are made for the testing phase  the
first one evaluates rollers performance when dck is learned with all solutions of the training
problems or with the ranked solution approach  the second one evaluates the usefulness of the
learned dck and the third one compares roller with state of the art planners  for each experiment we evaluate two different dimensions of the solutions found by roller  the scalability and
the quality  all testing experiments were done using a     ghz processor with a time bound of    
seconds  and  gb of memory bound 
      s olution r anking e valuation
this experiment evaluates the effect of selecting solutions following the approach described in section      the roller configurations for this evaluation are 
 top ranked solutions  the depth first h context policy algorithm using dck learned
with the sub set of the top ranked solutions  we use this search algorithm  since its performance depends more on the quality of the learned dck than that of the other algorithms
using dck 
 all solutions  the depth first h context policy algorithm using dck learned with all solutions obtained by the bfs bnb algorithm 
table   shows the number of problems solved by each configuration  also with the time and plan
length average computed over the problems solved by both configurations  the number in brackets
in the first column is the number of problems solved in common  the top ranked solutions configuration solved thirty more problems than all solutions configuration  mainly due to the difference
of    problems in the matching blocksworld domain 
domains
blocksworld     
depots     
gold miner     
matching bw    
parking     
rovers     
satellite     
storage     
thoughtful     
tpp     
total

top ranked solutions
solved
time length
  
    
     
  
    
     
  
    
    
  


  
    
     
  
    
     
  
     
     
  
    
   
  
    
     
  
    
     
   



all solutions
solved
time length
  
    
     
  
    
     
  
    
    
 


  
    
    
  
     
     
  
     
     
  
    
   
  
    
     
  
    
     
   



table    problems solved and time and plan length average for the evaluation on ranking solution
heuristic 

the effect of selecting solutions varies across domains  for instance  it is quite important regarding the plan quality for blocksworld  depots and rovers  in the satellite domain the top ranked
solutions allow roller to solve two more problems while maintaining similar time and plan length
       seconds was the time bound established at the learning track of ipc      

   

fid e la rosa   j imenez   f uentetaja   b orrajo

average  in the gold miner domain  selecting solutions is irrelevant because there are few equally
good solutions per problem  i e   the goal is always the single fact of having the gold  and fairly
most of them are top ranked ones  the parking domain does not benefit from selecting solutions 
considering the overall results  we think that selecting solutions is a useful heuristic for improving
the dck quality in many domains  in the remaining evaluations we will refer to dck used by
roller as the decision trees learned with the top ranked solutions 
      dck u sefulness e valuation
as shown in ipc learning track results  dck may degrade the performance of the base planner 
when the dck is incorrect  with this in mind  we designed this experiment to measure the performance of roller algorithms comparing them with versions without dck  we made two versions
for the non learning algorithms  the first one is an empty configuration where there is no decision
tree given to the algorithm  thus no ordering is computed for the helpful actions  and the second one
is the systematic configuration  where the ordering is supplied by the ff heuristic instead 
the roller configurations used for the comparisons are 
 roller  the depth first h context policy algorithm with the dck learned at the training
phase 
 roller bfs  the h context policy lookahead bfs algorithm with the dck learned at
the training phase  this configuration uses the horizon h        we choose this value on the
basis of empirical evaluations 
 roller bfs ha  a modified version of roller   bfs where only helpful actions are considered as immediate successors  the lookahead states are generated as in the original version  using also the same horizon 
these three algorithms have their equivalent version for the empty configuration 
 df ha  depth first helpful actions   an empty dck for roller corresponds to a depthfirst algorithm over the helpful actions  as in the original algorithm  non helpful actions are
placed in the delayed list 
 bfs  an empty dck for roller   bfs does not generate lookahead states  i e   the algorithm add to open lookahead successors in figure      therefore  the algorithm becomes
the standard best first search 
 bfs ha  a modified version of bfs where only helpful actions are considered  non helpful
actions are placed in a delayed list 
previous configurations also have a systematic version  in this case action ordering is computed
with the ff heuristic 
 gr ha  greedy helpful actions   this algorithm corresponds to a greedy search over the
helpful actions  for each node  helpful immediate successors are sorted with the ff heuristic 
non helpful nodes go to the delayed list 
 lh bfs  lookahead bfs   a bfs with lookahead states  the function dt filter sort is
replaced by a function that computes the ordering using the ff heuristic 
   

fis caling up h euristic p lanning with r elational d ecision t rees

 lh bfs ha  a modified version of lh   bfs where only helpful actions are considered  nonhelpful actions are placed in a delayed list 
for the comparison  we computed the number of problems solved and the scores used in the
ipc      learning track to evaluate planners performance in terms of cpu time and quality  plan
length   the time score is computed as follows  for each problem i the planner receives ti  ti
points  where ti is the minimum time a participant used for solving the problem i  and ti is the
cpu time used by the planner in question  in a    problem test set a planner can receive at most   
points  the higher the score the better  the quality score is computed in the same way  just replacing
t with l  where l measures the quality in terms of plan length  in addition we compute the time
and quality averages for problems solved by all configurations  if a configuration did not solve
any problem  it is not taken into account for this measure  average measures complement scores
since they give a direct information for commonly solved problems  while scores tend to benefit
configurations that solve problems which others do not 
table   shows a summary for the results obtained in the dck usefulness evaluation  for each
configuration we compute the number of domains where the algorithm was the top performer for
each of the evaluated criteria  i e   numbers of solved problems  time and quality scores and averages   a top performer in a domain is an algorithm that has equal to or better measure than
the rest of the algorithms  in the table  each algorithm can have at most    points  the number of
evaluated domains  global section refers to overall top performers  relative section refers to the
number of domains where a configuration was equal or better than the other two configurations of
the same algorithm strategy  i e   depth first  best first  best first with helpful actions   all averages
of commonly solved problems were computed for configurations that solve more than one problem  results show that roller is very good in the number of solved problems and speed metrics 
regarding quality score  roller and roller   bfs   ha were the best performers in three domains
each  however  bfs and bfs   ha obtained better results in quality average 
global
solved problems
time score
time average
quality score
quality average
relative
solved problems
time score
time average
quality score
quality average

depth first
roller gr ha df ha
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

best first
roller bfs lh bfs
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

bfs
 
 
 
 
 
 
 
 
 
 

helpful best first
roller bfs ha lh bfs ha bfs ha
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

table    summary for dck usefulness evaluation  each column gives the number of domains
where each configuration was the top performer for a row item 

table   shows the number of solved problems for the dck usefulness evaluation  the total
row shows that each roller configuration solved more problems than the empty and systematic
versions  results for time and quality scores are reported in table   and table    of appendix a 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

detailed results for averages were considered less interesting since in many domains there are very
few common solved problems  which are the easy problems 
domains
blocksworld     
depots     
gold miner     
matching bw     
parking     
rovers     
satellite     
storage     
thoughtful    
tpp     
total

depth first
roller gr ha df ha
  
 
 
  
  
  
  
 
 
  
 
 
  
  
 
  
  
  
  
  
  
  
 
  
  
  
 
  
  
  
   
   
   

best first
roller bfs lh bfs
bfs
 
 
 
  
  
  
  
  
  
  
 
  
  
  
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
   
       

helpful best first
roller bfs ha lh bfs ha bfs ha
 
 
 
  
  
  
  
 
 
  
  
  
  
  
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   
   
   

table    problems solved for the dck usefulness evaluation 

      t ime p erformance c omparison
this experiment evaluates the scalability of the roller system  compared to state of the art planners  for the comparison  we have chosen lama  richter   westphal         the winner of the
sequential track of the past ipc  and ff  which in the last ipc has shown to be still competitive  we
used the three roller configurations explained in the previous evaluation  the configuration for
other planners are 
 ff  running the enforced hill climbing  ehc  algorithm with helpful actions together with
a complete bfs in case ehc fails     though this planner dates from      we include it
in the evaluation because  as shown by the results of ipc       it is still competitive with
other state of the art planners  besides  this planner is extensively used in other planning and
learning systems 
 lama first  the winner of the classical track of ipc       in this configuration lama
is modified to stop when it finds the first solution  in this way  comparison is fair because
the rest of configurations do not implement anytime behavior  i e   the continuous solution
refinement until reaching the time bound   the anytime behavior of lama is compared later
with the roller performance in the next section 
table   shows the number of problems solved together with the speed score  these results
give an overall view of the performance of the different planners  roller solves as many or more
problems than any other configuration in   of    domains and achieves the top speed score in
seven domains  the second best score belongs to roller   bfs   ha  which solves as many or more
problems than other planners in six domains  lama first is fairly competitive  since it solves seven
problems less than roller and    more problems than roller   bfs   ha  in both cases lama first
achieves a lower speed score 
   this planner is actually metric ff running strips domains  we consider this implementation an adequate baseline
for comparison because roller was implemented over this code rather than over the original ff in order to extend
our approach to other planning models 

   

fis caling up h euristic p lanning with r elational d ecision t rees

domain  problems 
blocksworld     
depots     
gold miner     
matching bw     
parking     
rovers     
satellite     
storage     
thoughtful    
tpp     
total

roller
solved
score
  
     
  
     
  
     
  
     
  
     
  
     
  
     
  
     
  
     
  
     
   
      

roller bfs
solved
score
 
    
  
     
  
    
  
    
  
     
  
     
  
     
  
     
  
     
  
     
          

roller bfs ha
solved
score
 
    
  
     
  
    
  
    
  
     
  
     
  
     
  
     
  
     
  
     
   
      

ff
solved
 
  
  
 
  
  
  
  
  
  
   

score
    
    
    
    
    
    
    
     
    
    
     

lama first
solved
score
  
    
  
    
  
     
  
     
  
    
  
     
  
     
  
    
  
     
  
    
   
      

table    problems solved and speed score of the five configurations 

table   shows the average time for the five configurations when addressing the subset of problems solved by all configurations  the first column shows in parenthesis the number of commonly
solved problems  these results are closely related to those shown in table    roller achieves
the best average time in eight out of ten domains  we also observe that different configurations are
good in particular domains and even more so in particular problems  for instance  in the thoughtful
domain there were only four problems solved by all the configurations 
domain  problems 
blocksworld    
depots     
gold miner     
matching bw    
parking     
rovers     
satellite     
storage     
thoughtful   
tpp     

roller
    
    
    
    
    
    
    
     
    
    

roller bfs
     
     
     
     
    
     
    
    
     
    

roller bfs r
     
    
    
     
    
    
    
    
    
    

ff
    
    
     
     
     
     
    
     
    

lama first
      
     
    
    
      
    
    
    
    
    

table    planning time averages in the problems solved by all the configurations 

      q uality p erformance c omparison
this experiment compares the quality of the first solutions found and the solutions found by the
anytime behavior  in the anytime configuration  planners exhaust the time bound trying to improve
incrementally the best solution found  three roller algorithms are modified to a configuration
where the best solution found so far is used as an upper bound in order to prune all nodes that
exceed this plan length  the anytime behavior is the regular configuration for lama  ff does
not have anytime behavior  but it will be included in the anytime comparison as well as a base for
comparing quality improvements of other planners 
table   shows the quality scores for the first solution and for the last solution found by the
anytime configurations  the anytime column for each planner shows the score variation and reveals
whether or not the planner was able to make relative improvements of the first solutions  the relative
   

fid e la rosa   j imenez   f uentetaja   b orrajo

domain
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp
total

roller
first anytime
     
     
    
    
     
     
    
    
     
     
     
     
     
     
     
     
    
    
     
     
      
      

roller bfs
first anytime
    
    
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      

roller bfs ha
first
anytime
    
    
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      

ff
first
    
     
     
    
     
     
     
     
     
     
      

relative
    
     
     
    
     
     
     
     
     
     
      

lama
first anytime
    
    
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      

table    quality scores for the first solution and the anytime configuration of evaluated planners 

for ff shows the score of its solutions compared to the solutions given by the anytime configuration
of other planners  ff loses points in most cases because the others were able to improve their
solutions  the two lama configurations obtained the top score in their category  nevertheless  no
planner dominated in all the domains  furthermore  all configurations achieved the top quality score
for the first solution in at least one domain 
domain
blocksworld    
depots     
gold miner     
matching bw    
parking     
rovers     
satellite     
storage     
thoughtful   
tpp     

roller
first
anytime
      
      
      
      
     
     
      
      
     
     
      
      
     
     
     
     
      
      
     
     

roller bfs
first
anytime
      
      
     
     
     
     
     
     
     
     
      
      
     
     
     
     
      
      
     
     

roller bfs ha
first
anytime
      
      
     
     
     
     
     
     
     
     
      
      
     
     
     
     
      
      
     
     

ff
first

     
     
     
     
     
     
     
      
     

relative

     
     
     
     
     
     
     
      
     

lama
first
anytime
      
      
     
     
     
     
     
     
     
     
      
     
     
     
     
     
      
      
     
     

table    quality averages for the first solution and the anytime configuration of evaluated planners 

table   shows the plan length average for the problems solved by all configurations  the first
column shows the average for the first solutions and the anytime column gives the average for
the last solutions of the anytime configuration  the commonly solved problems are the same as
those reported in table    although ff is the planner that solved fewer problems  it achieves the
best average plan length in seven domains  plan length averages reveal that roller is not able
to find first solutions of good quality for most domains  roller   bfs and roller   bfs   ha find
better quality solutions than roller  and in several domains  their averages are competitive with
lama   roller   bfs and roller   bfs   ha show a better quality performance mainly due to the
combination of learned dck with a domain independent heuristic within the bfs algorithm 
in the following subsections we discuss particular details for each of the domains  we give a
very brief description of the domain together with information about training and test sets used in
the experimental evaluation  for each domain  we also analyze the learned dck and the obtained
   

fis caling up h euristic p lanning with r elational d ecision t rees

results in order to give a fine grained interpretation of the observed performance  further details on
these domains can be found at the ipc web site  
      b locksworld d etails
problems in this domain are concerned with configuring towers of blocks using a robotic arm 
the training set used for the experiments consisted of  ten eight block problems  ten nine block
problems and ten ten block problems  the test set consisted of the    largest typed problems from
ipc       which have from    to    blocks 
blocksworld domain
   
roller
roller bfs
roller bfs ha
lama first

percentage solved

  

  

  

  

 
   

 

  
cpu time

   

    

figure     percentage of solved problems when increasing time for evaluating the scalability performance in the blocksworld domain 
although this domain is one of the oldest benchmarks in automated planning  it is still challenging for state of the art heuristic planners  blocksworld presents strong interaction among goals
that current heuristics fail to capture  in particular  achieving a goal in this domain may undo
previously satisfied goals  therefore  it is crucial to achieve goals in a specific order  the dck
learned by roller gives a total order of the domain actions in different contexts capturing this key
knowledge  which lets roller achieve impressive scalability results while producing good quality
solution plans  roller configurations are considerably better than non learning configurations 
particularly  roller solved the thirty problems of the set while df   ha and gr   ha did not solve
any problem  roller is also quite good when compared to state of the art planners  in figure   
we can observe that roller performs two orders of magnitude faster than lama  the x axis of
the figure represents the cpu time in logarithmic scale and the y axis represents the percentage of
solved problems in a particular time  moreover  roller obtained the best quality score in the first
solution and anytime evaluations  in addition  the average plan length of common problems is fairly
close to the best average  obtained by roller   bfs and roller   bfs   ha  bfs algorithms do not
   http   idm lab org wiki icaps index php main competitions

   

fid e la rosa   j imenez   f uentetaja   b orrajo

scale well in this domain because they are partially guided by the ff heuristic  which considerably
underestimates the distance to the goals  similarly  lookahead states generated by the policy are
discarded because they fail to escape plateaus generated by this heuristic function 
when analyzing the learned operator tree we found explanations for the good performance of
roller in the blocksworld domain  the operator tree is clearly split in two parts  the first part contains decisions to take when the arm is holding a block  in this situation  the tree captures when to
stack or put down a block  the second part contains decisions to take when the arm is empty  in
this case the tree captures when to unstack or pick up a block  in this second part of the tree  if
the current state of the search matches the logical query helpful unstack block  block   
it means that the tower of blocks under block  is not well arranged  i e   block  or at least
one block beneath block  is not well placed  therefore  the set of helpful actions compactly
encodes the useful concept of a bad tower  this kind of knowledge was manually defined in
previous works in order to learn good policies for blocksworld  one approach consisted of including recursive definitions of new predicates  such as the support predicates above x y  and
inplace x   khardon         another alternative involved changing the representation language  for instance the concept language  martin   geffner        or the taxonomic syntax  yoon 
fern    givan         the kleene star operator of the taxonomic syntax  i e   the operator for defining recursion  was discarded in a subsequent work  yoon et al         and the above predicate was
used instead  rollers ability to recognize bad towers without extra predicates arises because any
misplaced block in a tower makes the unstack action of the top block helpful  since it is always
part of the relaxed plan when the arm is empty 
due to the extraordinary performance of roller in this domain  we built an extra test set
to clarify whether or not the trend observed in the roller configuration would hold for larger
problems  with this aim  we randomly generated    problems distributed in sub sets of            
       and     blocks with   problems for each sub set  roller solved the    problems in this
extra test set with a time average of      seconds per problem and spending at most       to solve a
problem  obviously  problems became more difficult for roller as the number of blocks increase 
      d epots d etails
this domain is a combination of a transportation domain and the blocksworld domain  where there
are crates instead of blocks and hoists instead of the robot arm  the problems consist of trucks
transporting crates around depots and distributors  using hoists  crates can be stacked onto pallets
or on top of other crates at their final destination  in this domain  the    training problems are
different combinations of   or   locations  depots and distributors     or   trucks    or   pallets per
location    hoist per location and from   to   crates to be placed in different configurations  for
the testing phase we have used the    problems of the ipc      set  the hardest problem has   
locations    or   pallets and   or   hoists     trucks and    crates 
roller and roller   bfs improve the performance of the non learning strategies  but the three
configurations of bfs helpful action solved the same    problems  roller is able to solve   
problems  achieving the best speed score  however  the high average plan length indicates that the
policy is not producing good quality plans  roller   bfs   ha obtains the second best speed score
with more competitive plan lengths  figure    shows the percentage of solved problems while
   as explained in section     logic queries in roller present the example and problem ids  in this case these ids are
ignored for simplicity given that they are not needed for matching the current helpful context 

   

fis caling up h euristic p lanning with r elational d ecision t rees

increasing the cpu time  in logarithmic scale   in the anytime configuration  roller   bfs   ha is
able to refine its solutions  achieving a quality average similar to lama 
depots domain
   
roller
roller bfs
roller bfs ha
ff
lama first

percentage solved

  

  

  

  

 
    

   

 

  

   

    

cpu time

figure     percentage of solved problem when increasing time for evaluating the scalability performance in the depots domain 

the dck learned in this domain provides inaccurate advice for large planning contexts  for
instance  roller makes some mistakes when deciding which crate to unload when several crates
are loaded in a truck  the reason for the inaccurate dck is that training problems are not large
enough to gain this knowledge  in addition  adding more crates to these problems makes it unfeasible for them to be solved with bfs bnb  nevertheless  this limitation of the learned dck is
not very evident  the depots domain is undirected  i e   all actions are reversible   so it has no
dead ends  therefore  mistakes made by the dck are fixed with additional actions  which leads
to worse quality plans  besides  since first solutions are rapidly found  roller configurations can
spend time refining solutions  this is the reason for the great improvement in the plan average for
roller   bfs   ha  
      g old  m iner d etails
the objective of this domain is to navigate in a grid of cells until reaching a cell containing gold 
some of the cells are occupied by rocks that can be cleared using bombs or a laser  in this domain
the training set consists of     problems with      cells     problems with      cells  and   
problems with      cells  this domain was part of the learning track in ipc      so we have used
the same test set used in the competition  this set has problems ranging from      up to      cells 
problems in the gold miner domain are not solvable with helpful actions alone  this explains
the difference in the number of solved problems between roller  roller   bfs   ha and their nonlearning counterpart  in general terms  this domain is trivial for roller  roller   bfs   ha  they
solved all the test problems in less than    seconds per problem  and lama  nevertheless  ff scales   

fid e la rosa   j imenez   f uentetaja   b orrajo

up poorly  in this domain essential actions for picking up bombs are frequently not considered
helpful actions  because the relaxed problem is solvable using the laser  consequently  ff fails
to solve most problems with ehc and it requires an additional bfs search  figure    shows the
percentage of solved problems while increasing the cpu time  regarding the anytime evaluation 
all tested configurations improved the first solution found in many problems 
gold miner domain
   

percentage solved

  

  

  

  
roller
roller bfs
roller bfs ha
ff
lama first
 
    

   

 

  

   

    

cpu time

figure     percentage of solved problems when increasing time for evaluating the scalability performance in the gold miner domain 
in this domain  the operator tree succeeds in capturing the key knowledge  in the initial states 
the bombs and the laser are in the same cell  so the robot needs to decide which of them to pick up 
the operator tree for this domain matches the logical query candidate pickup laser cell 
with a higher ratio for operator pickup bomb than for operator pickup laser  this operator
preference allows roller to avoid dead ends when the laser destroys the gold  on the other hand 
situations where the laser is required  i e   to destroy hard rocks  are reached as a second choice
of the policy  this fact implies some backtracking for roller  but the additional evaluated nodes
do not significantly affect the overall performance  the preference of the pickup bomb over the
pickup laser action is an example of selecting non helpful actions 
      m atching b locksworld d etails
this domain is a version of blocksworld designed to analyze limitations of the relaxed plan heuristic 
in this version blocks are polarized  either positive or negative  there are also two polarized robot
arms  furthermore  when a block is placed  stack or put down actions  with an arm of different
polarity  the block becomes damaged and no block can be placed on top of it  however  picking up
or unstacking a block with the wrong polarity seems to be harmless  this fact makes recognizing
dead ends a difficult task for the ff heuristic  particularly  in the relaxed task blocks are never
damaged  thus  both the relaxed plan  and consequently the set of helpful actions  and the heuristic
estimation are wrong  the training set used in this domain consists of fifteen   blocks problems and
   

fis caling up h euristic p lanning with r elational d ecision t rees

fifteen   blocks problems  we used an even number of blocks to keep the problems balanced  i e  
half of the blocks of each polarity   for the testing phase we used the test set from the learning track
of ipc       this set has problems ranging from    to    blocks 
df   ha and gr   ha did not solve any problem  because these problems are not solvable with
helpful actions alone  the learned dck recommended some useful non helpful actions  thus
roller was able to solve    problems  policy configurations perform better than systematic strategies  but are fairly similar to not using a lookahead strategy  this fact reveals that the learned dck
is not effective enough to pay off the effort of building lookahead states  lama is the planner that
solves the most problems  figure    shows the percentage of solved problems while increasing the
cpu time 
matching bw domain
   
roller
roller bfs
roller bfs ha
ff
lama first

percentage solved

  

  

  

  

 
   

 

  
cpu time

   

    

figure     percentage of solved problems when increasing time for evaluating the scalability performance in the matching blocksworld domain 

roller solved problems by evaluating a considerable number of nodes above the plan length 
which means that the dck learned for this domain is not accurate  when analyzing the training
examples we find many solution plans that do not satisfy the key knowledge of the domain  robot
arms should unstack or pick up blocks of the same polarity   specifically  when the robot is handling
a top block  i e   a block with no other blocks above it in the goal state  then the polarity of the robot
arm becomes meaningless  this effect is unavoidable because the shortest plans involve managing
top blocks in a more efficient way while ignoring the polarities  these examples include noise in
the learning and make generalization very complex 

      parking d etails
this domain involves parking cars on a street with n curb locations where cars can be double
parked  but not triple parked  the goal is to move from one configuration of parked cars to another
by driving cars from one curb location to another  in this domain the training set consists of  fifteen
   

fid e la rosa   j imenez   f uentetaja   b orrajo

problems with six cars and four curbs and fifteen problems with eight cars and five curbs  for testing
we used the test set from the learning track of ipc       the hardest problem in this set has    cars
and    curbs 
the three roller configurations solve all problems and perform significantly better than nonlearning strategies  in addition  the three roller configurations outperform ff and lama with a
difference of more than one order of magnitude  this is the reason for lama and ff low speed
scores  roller configurations are also consistently better than systematic and empty configurations  figure    shows the percentage of solved problems while increasing the cpu time  on
the other hand  the three roller configurations did not achieve first solutions of suficient quality  however  these solutions are refined in the anytime evaluation  especially by roller   bfs   ha 
which achieves the top quality score and has a plan length average fairly similar to lama 
parking domain
   
roller
roller bfs
roller bfs ha
ff
lama first

percentage solved

  

  

  

  

 
   

 

  
cpu time

   

    

figure     percentage of solved problems when increasing time for evaluating the scalability performance in the parking domain 

the learned dck in this domain was quite effective  roller rarely backtracked   the operator tree perfectly classifies the move car to curb action at the first tree node  asking if it is
considered a helpful action  besides  the binding tree for this operator selects the right car by asking
about the target goal and rejecting other candidates  these two decisions guide the planner to place
a car in the right position whenever possible  as a result  a large number of nodes are not evaluated 
which explains the scalability difference with ff and lama 
       rovers d etails
this domain is a simplification of the tasks performed by the autonomous exploration vehicles sent
to mars  the tasks consist of navigating the rovers  collecting soils and rocks samples  and taking
images of different objectives  in this domain the training set consists of  ten problems with one
rover  four waypoints  two objectives and one camera  ten problems with an additional camera  and
   

fis caling up h euristic p lanning with r elational d ecision t rees

ten problems with an additional rover  problems in the test set are the thirty largest problems from
the ipc      set  i e   problems    to      the largest problem in this set has    rovers and    
waypoints 
dck strategies are faster than systematic and empty strategies  but differences are not significant since all configurations solved most of the problems  on the one hand helpful actions in the
rovers domain are quite good but on the other hand the test set does not have problems which are
big enough to generate differences among approaches  regarding planner comparison  roller
achieves the top performance score and scales significantly better than ff  and solves two problems
less than lama  figure    shows the percentage of solved problems when increasing the cpu time 
regarding the anytime evaluation  all planners are able to refine their first solutions  lama gets the
top quality score and the best plan length after refining solutions 
rovers domain
   
roller
roller bfs
roller bfs ha
ff
lama first

percentage solved

  

  

  

  

 
    

   

 

  

   

    

cpu time

figure     percentage of solved problems when increasing time for evaluating the scalability performance in the rovers domain 

in this domain  roller learned imperfect dck  but it manages to achieve good scalability
results  dck is imperfect partially because actions for communicating rock  soil or image analysis
can be applied in any order among them  therefore  the preferences for ranking and selecting
solutions fail to discriminate among these actions which confuse the learning algorithm  since
these actions could be applied in any order  the dck mistakes seem to be harmless at planning
time 
       s atellite d etails
this domain comprises a set of satellites with different instruments  which can operate in different
formats  modes   the tasks consist of managing the instruments for taking images of certain targets
in particular modes  in this domain the training set consist of thirty problems with one satellite  two
instruments  five modes and five observations  problems in the test set are the thirty largest problems
   

fid e la rosa   j imenez   f uentetaja   b orrajo

from the ipc       i e   problems   to      the largest problem in this set has    satellites    modes
and     observations 
the three roller configurations improved the number of solved problems of their non learning
counterpart  in addition  roller and roller   bfs   ha solved the    problems in the set  two more
than lama and eight more than ff  figure    shows the percentage of solved problems when
increasing the cpu time  roller and roller   bfs   ha achieve good quality solutions and are
able to refine them in the anytime evaluation  achieving plan lengths similar to lama 
satellite domain
   
roller
roller bfs
roller bfs ha
ff
lama first

percentage solved

  

  

  

  

 
    

   

 

  

   

    

cpu time

figure     percentage of solved problems when increasing time for evaluating the scalability performance in the satellite domain 

the learned dck captures the key knowledge of the satellite domain  the trees shown in
figure   and figure   are part of the learned dck with fewer training examples  in this domain
both roller and roller   bfs   ha perform quite similarly  the reason is that the ff heuristic is
also quite accurate in the domain  thus  the deepest lookahead state generated by the learned policy
is frequently selected by the heuristic in the bfs search 
       s torage d etails
this domain is concerned with the storage of a set of crates taking into account the spatial configuration of a depot  the domain tasks comprise using hoists to move crates from containers to a
particular area in the depot  the training set consists of    problems with   depot    container   
hoist and different combinations of   or   crates and from   up to   areas inside the depot  for the
test set we used the    problems from the ipc      set  the largest problem in this domain has  
depots with   areas each    hoist and    crates 
the first    problems are trivially solved by all configurations  then  problem difficulty increases quickly when the number of problem objects increases  the bfs solved    problems  one
more than any dck strategy  meaning that dck lookahead strategies do not pay off  this domain
   

fis caling up h euristic p lanning with r elational d ecision t rees

is also hard for ff and lama  figure    shows the percentage of solved problems when increasing
the cpu time 
storage domain
   
roller
roller bfs
roller bfs ha
ff
lama first

percentage solved

  

  

  

  

 
    

   

 

  

   

    

cpu time

figure     percentage of solved problems when increasing time for evaluating the scalability performance in the storage domain 

although dck is not effective  we found interesting properties in it  the learned operator tree
is compact and succeeds in selecting the go in action which is normally not marked as a helpful
action 
       t houghtful d etails
this domain models a version of the solitaire card game  where all cards are visible and one can
turn each card from the talon rather than   cards at a time  as in the original version  the goal of
the game is to place all cards in ascending order in their corresponding suit stacks  home deck  
there is no available random problem generator for this domain  therefore  we used the bootstrap
problem distribution given in the learning track of ipc       this set contains problems for the
four suits  having up to card seven for each suit  for the test phase we used the    problems from
the test distribution of the learning of ipc       the largest problem in this domain has the full set
of a standard card game 
roller only solves    problems  three fewer than gr   ha   however  roller   bfs and roller bfs   ha are better in the number of solved problems than non learning approaches  in this domain 
the use of dck for lookahead construction combined with the ff heuristic makes the search process
more robust against policy mistakes  roller   bfs   ha solves     three more than lama  figure   
shows the percentage of solved problems when increasing the cpu time 
the bfs bnb algorithm for generating training examples is only able to solve    out of   
problems from the bootstrap problem distribution  we believe that a different bootstrap distribution with smaller problems would generate more accurate dck  additionally  even though dck
   

fid e la rosa   j imenez   f uentetaja   b orrajo

thoughtful domain
   
roller
roller bfs
roller bfs ha
ff
lama first

percentage solved

  

  

  

  

 
   

 

  
cpu time

   

    

figure     percentage of solved problems when increasing time for evaluating the scalability performance in the thoughtful domain 

lookahead strategies achieve good results  learning accurate decision trees is more complex when
there are many more classes     operators in this particular domain  and many more arguments in
the predicates of the background knowledge  up to   parameters in operator col to home and  
parameters in operator col to home b  
       tpp d etails
tpp stands for traveling purchase problem  which is a generalization of the traveling salesman
problem  tasks in the domain consist in selecting a subset of markets to satisfy the demand for
a set of goods  the selection of markets should try to optimize the routing and the purchasing
costs of the goods  in the strips version  the graph that connects markets has equal costs for all
arcs  nevertheless  the domain is still interesting because it is difficult for planners to scale when
increasing the number of goods  markets and trucks  the training set consists of thirty problems
with a number of goods  trucks and depots varying from one to three and with load levels of five
and six  the test set consists of the thirty problems used for planner evaluation at ipc       the
largest problem in this set has    goods    trucks    markets with a load level of six 
roller   gr   ha and df   ha solved the    problems in the test set  but roller performs faster
than the other two  achieving similar plan lengths  besides  roller outperforms the rest of the
planners and it is two orders of magnitude faster than ff  the main reason is the overwhelming
branching factor of the large problems together with the fact that ff heuristic falls into big plateaus
in this domain  greedy  depth first  approaches perform better because they avoid the effect of these
plateaus  additionally  roller achieved competitive quality scores and average plan length in the
first solution and the anytime evaluation  roller   bfs and roller   bfs   ha got very bad results

   

fis caling up h euristic p lanning with r elational d ecision t rees

for this domain because of the imprecision of the ff heuristic  figure    shows the percentage of
solved problems while increasing the cpu time 
tpp domain
   

percentage solved

  

  

  

  
roller
roller bfs
roller bfs ha
ff
lama first
 
    

   

 

  

   

    

cpu time

figure     percentage of solved problems when increasing time for evaluating the scalability performance in the tpp domain 
the learned dck is compact and useful for reducing the number of evaluations  as shown by
roller performance  for instance  the drive binding tree recognizes perfectly when a truck in
market a does not need to go to a market b because there is already a truck in b handling the goods
of that market  in these situations  the state with the truck in b has a helpful action drive  meaning
the truck b has something to deliver 

   lessons learned from the ipc
ipc      included a specific track for planning systems that benefit from learning  thirteen systems
took part in this track including a previous version of roller  de la rosa et al         that achieved
the  th position  this version was an upgrade of the original roller system  de la rosa et al  
       the first version proposed the ehc sorted algorithm as an alternative to the h context policy  but it was not effective in many domains  the competing version tried to recommend ordering
for applying actions from the relaxed plans  this idea  although initially appealing  was not a good
choice because its usefulness strongly depends on the fact that the relaxed plan contains the right actions  after the competition we completed an analysis of the roller performance to diagnose and
strengthen its weak points  the system resulting from these improvements is the roller version
described in this article  one example of the roller improvements is the results obtained at the
thoughtful and matching blocksworld domains  at ipc       roller failed to solve all the problems from the thoughtful domain and it only solved two problems from the matching blocksworld 
as reported in section    the current version of roller solves    and    problems respectively in
these domains  in addition  the current version of roller outperforms lama and ff in the park   

fid e la rosa   j imenez   f uentetaja   b orrajo

ing domain by one order of magnitude  the improvements of roller overcome limitations of the
version submitted to ipc      in three aspects 
 robustness to wrong dck  issues discussed in section   are all decisions that introduce biases in the learning process making learning of dck a complex task  in fact  no competitor at
ipc      was able to learn useful dck for all the domains  furthermore  in many domains
the learned dck damaged the performance of the baseline planner  this was the case of
roller   as we described in the paper  we have strengthened roller against wrong dck
by proposing two versions of a modified bfs algorithm that combine the learned dck with
a numerical heuristic  the combination of dck and the heuristic makes the planning process more robust to imperfect and or incorrectly learned knowledge  a similar approach was
followed by the winner of the best learner award  o btuse w edge  yoon et al         
 efficiency of the baseline  the overall competition winner was p bp  gerevini  saetti    vallati        a portfolio of state of the art planners that learns which planner and settings are
the best ones for a given planning domain  as a result  the performance of this competitor
was never worse than the performance of a state of the art planner  at ipc      the baseline performance of roller was far from being competitive with state of the art planners
because roller algorithms were coded in lisp  to overcome this weakness we optimized
the implementation of roller using c code that outperformed our ipc      results in all
domains 
 definition of significant training sets  training examples are extracted from the experience
collected while solving problems of a training set  therefore  the quality of the training
examples depends on the quality of the problems used for training  at ipc      the training
problems were fixed by the organizers and  in many domains  they were too large for the
roller system to extract useful dck  in this paper we have created training problems using
random generators to build useful training sets for the roller system for each domain 
 selection of training examples  relational classifiers induce a set of rules trees that model
regularities in the training data  for the case of forward state space search planning not all
best cost solutions for a problem may be used as training data  because this leads to alternatives that will confuse the learner  to avoid this  training data should be cleaned before being
used by the learning algorithm  the ranking and solution selection proposed in this article is
an option to give the learner training data with clearer regularities 
additionally  roller performed poorly in the sokoban and n puzzle domains  traditionally 
useful dck for these domains has the form of numeric functions  such as the manhattan distance 
which provides a lower bound for the solution length  in general  action policies are inaccurate in
these domains  because they lack knowledge about the trajectory to the goals  currently  we are still
unable to learn useful dck for roller in these domains  a possible future direction is to introduce
not only goals but subgoals  e g  landmarks  in the helpful context with the aim of capturing some
of this knowledge 

   related work
our approach is strongly inspired by the way prodigy  veloso et al         models dck  in the
prodigy architecture  the action selection is a two step process  first  prodigy selects the uninstan   

fis caling up h euristic p lanning with r elational d ecision t rees

tiated operator to apply  and second  it selects the bindings for that operator  both selections can
be guided by dck in the form of control rules  leckie   zukerman        minton         we
have returned to this idea of the two step action selection because it allows us to define the learning
of planning dck as a standard classification task and therefore to solve this learning task with an
off the shelf classification technique such as relational decision trees  nevertheless  roller does
not need to distinguish among different kinds of nodes as prodigy does  because roller performs
a standard forward heuristic search in the state space where all the search nodes are of the same
type 
relational decision trees have been previously used to learn action policies in the context of
relational reinforcement learning  rrl   dzeroski  de raedt    blockeel         in comparison
with the dck learned by roller  rrl action policies present two limitations when solving planning problems  first  in rrl the learned knowledge is targeted to a given set of goals  therefore
rrl cannot directly generalize the learned knowledge for different goals within a given domain 
second  since training examples in rrl consist of explicit representations of the states  rrl needs
to add extra background knowledge to learn effective policies in domains with recursive predicates
such as blocksworld 
previous works on learning generalized policies  martin   geffner        yoon et al        
succeed in addressing these two limitations of rrl  first  they introduce planning goals in the
training examples  in this way the learned policy applies for any set of goals in the domain  second 
they change the representation language of the dck from predicate logic to concept language  this
language makes capturing decisions related to recursive concepts easier  alternatively  roller
captures effective dck in domains like blocksworld without varying the representation language 
roller implicitly encodes states in terms of the set of helpful actions of the state  as a result 
roller can benefit directly from off the shelf relational classifiers that work in predicate logic 
this fact makes learning times shorter and the resulting policies easier to read 
recently  other techniques have also been developed to improve the performance of heuristic
planners 
 learning macro actions  botea  enzenberger  muller    schaeffer        coles   smith 
      are the combination of two or more operators that are considered as new domain operators in order to reduce the search tree depth  however  this benefit decreases with the number
of new macro actions added because they enlarge the branching factor of the search tree causing the utility problem  minton         other approaches overcome this problem  applying
filters that decide on the applicability of the macro actions  newton  levine  fox    long 
       two versions of this work participated in the learning track of ipc       obtaining
third and fourth place  one advantage of macro actions is that the learned knowledge can
be exploited by any planner  thus  approaches which learn generalized policies could also
benefit from macro actions  nevertheless  as far as we know  this combination has not been
tried for improving heuristic planners 
 learning domain specific heuristic functions  in this approach  yoon  fern    givan       
xu  fern    yoon         a state generalized heuristic function is obtained from examples
of solution plans  the main drawback of learning domain specific heuristic functions is that
the result of the learning algorithm is difficult to understand by humans which makes the
verification of the learned knowledge difficult  on the other hand  the learned knowledge is
easy to combine with existing domain independent heuristics  a slightly different approach
   

fid e la rosa   j imenez   f uentetaja   b orrajo

consists of learning a ranking function for greedy search algorithms  xu  fern    yoon 
             at each step of a greedy search  the current node is expanded and the child node
with the highest rank is selected to be the current node  in this case  the ranking function is
iteratively estimated in an attempt to cover a set of solution plans with the greedy algorithm 
 learning task decomposition  this approach learns how to divide planning tasks of a given
domain into smaller subtasks that are easier to solve  techniques for reachability analysis
and landmark extraction  hoffmann  porteous    sebastia        are able to compute intermediate states that must be reached before satisfying the goals  however  it is not clear
how to systematically exploit this knowledge to build good problem decompositions  vidal et al         consider this as an optimization problem and use a specialized optimization
algorithm to discover good decompositions 
in general  any system that learns planning dck has to deal with ambiguity in the training
examples  because a given planning state may present many good actions  trying to learn dck
that selects one action over other  inherently equal  is a complex learning problem  to cope with
ambiguous training data roller created a function that ranks solutions with the aim of learning
from the same kind of solutions  a different approach is followed by xu et al         who generate
training examples from partially ordered plans 

   conclusions and future work
we have presented a new technique for reducing the number of node evaluations in heuristic planning based on learning and exploiting generalized policies  our technique defines the process of
learning generalized policies as a two step classification and builds domain specific relational decision trees that capture the action to be selected in the different planning contexts  in this work 
planning contexts are specified by the helpful actions of the state  the pending goals and the static
predicates of the problem  finally  we have explained how to exploit the learned policies to solve
classical planning problems  applying them directly or combining them with a domain independent
heuristic in a lookahead strategy for the bfs algorithm  this work contributes to the state of the art
of learning based planning in three ways 
   representation  we propose a new encoding for generalized policies that is able to capture
efficient dck using predicate logic  as opposed to previous works that represent generalized
policies in predicate logic  khardon         our representation does not need extra background knowledge  support predicates  to learn efficient policies for the blocksworld domain 
besides  encoding states with the set of helpful actions is frequently more compact and furthermore  this set normally decreases when the search has fewer goals left  thus  the process
of matching dck becomes faster when the search advances towards the goals 
   learning  we have defined the task of learning a generalized policy as a two step standard
classification task  thus  we can learn the generalized policy with an off the shelf tool for
building relational classifiers  results in this paper are obtained with the tilde system  blockeel   de raedt         but any other tool for learning relational classifiers could have been
used  because of this  advances in relational classification can be applied in a straightforward
manner in roller to learn faster and better planning dck 
   

fis caling up h euristic p lanning with r elational d ecision t rees

   planning  we have explained how to extract an action ordering from an h context policy
and we have shown how to use this ordering to reduce node evaluations      in the algorithm depth first h context policy that allows a direct application of the h context policies 
and     in the h context policy lookahead bfs  which combines the policy with a domainindependent heuristic within a bfs algorithm  in addition  we have included a modified
version of this algorithm  roller   bfs   ha  that only considers helpful successors in order
to reduce the number of evaluations in domains where helpful actions are good 
experimental results show that our approach improved the scalability of the baseline heuristic
planners ff and lama  winner of ipc       over a variety of ipc domains  this effect is more
evident in domains where the learned dck presents good quality  e g  blocksworld and parking 
in these domains the direct application of the learned dck saves large amounts of node evaluations
achieving impressive scalability performance  moreover  using the learned dck in combination
with a domain independent heuristic in a bfs algorithm achieves good quality solutions  when
the quality of the learned dck is poor  planning with the direct application of the policy fails to
solve many problems  mainly the largest ones which are more difficult to solve without a reasonable
guide  unfortunately  the only current mechanism for quantifying the quality of the learned dck
is evaluating it against a set of test problems  therefore  a good compromise solution is combining
the learned dck with domain independent heuristics 
in some domains  the dck learned by roller presents poor quality because the helpful context
is not able to represent concepts that are necessary in order to discriminate between good and bad
actions  this problem frequently arises when the arguments of the good action do not correspond
to the problem goals or the static predicates  we plan to study refinements to our definition of
the helpful context to achieve good dck in such domains  one possible direction is extending
the helpful context with subgoal information such as landmarks  hoffmann et al         of the
relaxed plan  moreover  the use of decision trees introduces an important bias in the learning step 
algorithms for tree learning only insert a new query in the tree if doing so produces a significant
information gain  however  in some domains this information gain can only be obtained by the
conjunction of two or more queries  finally  we are currently providing the learner with a fixed
distribution of training examples  in the near future  we plan to explore how the learner can generate
the most convenient distribution of training examples according to a target planning task as proposed
by fuentetaja and borrajo        

acknowledgments
this work has been partially supported by the spanish miciin project tin           c      and
the regional cam uc m project ccg   uc m tic      

   

fid e la rosa   j imenez   f uentetaja   b orrajo

appendix a  dck usefulness results

domains
blocksworld     
depots     
gold miner     
matching bw     
parking     
rovers     
satellite     
storage     
thoughtful    
tpp     
total

deph first
roller gr ha df ha
     
    
    
     
    
    
     
    
    
     
    
    
     
    
    
           
    
     
    
    
     
    
    
           
    
                 
                  

best first
roller bfs lh bfs
bfs
    
    
    
     
    
    
    
    
    
    
    
    
     
    
    
     
    
    
     
    
    
           
    
     
    
    
     
    
    
                  

helpful best first
roller bfs ha lh bfs ha bfs ha
    
    
    
     
    
    
    
    
    
    
    
    
     
    
    
     
     
    
     
    
    
     
    
    
     
    
    
     
    
    
      
     
     

table    problems solved for the dck usefulness evaluation 

domains
blocksworld     
depots     
gold miner     
matching bw     
parking     
rovers     
satellite     
storage     
thoughtful    
tpp     
total

deph first
roller
gr ha df ha
     
    
    
    
    
    
     
    
    
     
    
    
     
    
    
     
           
     
     
    
     
    
    
    
     
    
     
           
                   

best first
roller bfs
lh bfs
    
    
     
     
     
     
     
    
     
    
     
     
     
     
     
     
     
     
     
     
             

bfs
    
     
     
     
    
     
     
     
     
    
      

helpful best first
roller bfs ha lh bfs ha
bfs ha
    
    
    
     
     
     
     
    
    
     
    
     
     
    
    
     
     
     
     
     
     
     
    
    
     
     
     
     
     
     
      
             

table     quality scores for the dck usefulness evaluation 

references
bacchus  f     kabanza  f          using temporal logics to express search control knowledge for
planning  artificial intelligence                   
biba  j   saveant  p   schoenauer  m     vidal  v          an evolutionary metaheuristic based on
state decomposition for domain independent satisficing planning  in proceedings of the   th
international conference on automated planning and scheduling  icaps    toronto  on 
canada  aaai press 
blockeel  h     de raedt  l          top down induction of first order logical decision trees 
artificial intelligence                   
   

fis caling up h euristic p lanning with r elational d ecision t rees

bonet  b   loerincs  g     geffner  h          a robust and fast action selection mechanism
for planning  in proceedings of the american association for the advancement of artificial
intelligence conference  aaai   pp          mit press 
botea  a   enzenberger  m   muller  m     schaeffer  j          macro ff  improving ai planning
with automatically learned macro operators  journal of artificial intelligence research     
       
coles  a     smith  a          marvin  a heuristic search planner with online macro action learning  journal of artificial intelligence research             
de la rosa  t   jimenez  s     borrajo  d          learning relational decision trees for guiding heuristic planning  in international conference on automated planning and scheduling
 icaps  
de la rosa  t   jimenez  s   garca duran  r   fernandez  f   garca olaya  a     borrajo  d 
        three relational learning approaches for lookahead heuristic planning  in working
notes of icaps      workshop on planning and learning  pp       
de raedt  l          logical and relational learning  springer  berlin heidelberg 
doherty  p     kvarnstrom  j          talplanner  a temporal logic based planner  ai magazine 
             
dzeroski  s   de raedt  l     blockeel  h          relational reinforcement learning  in international workshop on ilp  pp       
emde  w     wettschereck  d          relational instance based learning  in proceedings of the
  th conference on machine learning  pp         
florez  j  e   garca  j   torralba  a   linares  c   garca olaya  a     borrajo  d          timiplan  an application to solve multimodal transportation problems  in proceedings of spark 
scheduling and planning applications workshop  icaps   
fuentetaja  r     borrajo  d          improving control knowledge acquisition for planning by
active learning  in ecml  berlin  germany  vol        pp         
gerevini  a   saetti  a     vallati  m          an automatically configurable portfolio based planner
with macro actions  pbp  in proceedings of the   th international conference on automated
planning and scheduling  pp         thessaloniki  greece 
hoffmann  j     nebel  b          the ff planning system  fast plan generation through heuristic
search  journal of artificial intelligence research             
hoffmann  j   porteous  j     sebastia  l          ordered landmarks in planning  journal of
artificial intelligence research     
khardon  r          learning action strategies for planning domains  artificial intelligence      
       
   

fid e la rosa   j imenez   f uentetaja   b orrajo

leckie  c     zukerman  i          inductive learning of search control rules for planning  artificial
intelligence                
martin  m     geffner  h          learning generalized policies in planning using concept languages  in international conference on artificial intelligence planning systems  aips   
martin  m     geffner  h          learning generalized policies from planning examples using
concept languages  appl  intell          
mcallester  d     givan  r          taxonomic syntax for first order inference  journal of the acm 
           
mcdermott  d          a heuristic estimator for means ends analysis in planning  in proceedings
of the  rd conference on artificial intelligence planning systems  aips   pp          aaai
press 
minton  s          quantitative results concerning the utility of explanation based learning  artif 
intell                   
muggleton  s          inverse entailment and progol  new generation computing             
muggleton  s     de raedt  l          inductive logic programming  theory and methods  journal
of logic programming             
nau  d   au  t  c   ilghami  o   kuter  u   murdock  w   wu  d     yaman  f          shop   an
htn planning system  journal of artificial intelligence research             
newton  m  a  h   levine  j   fox  m     long  d          learning macro actions for arbitrary
planners and domains  in proceedings of the   th international conference on automated
planning and scheduling  icaps  
quinlan  j          induction of decision trees  machine learning           
richter  s     westphal  m          the lama planner  guiding cost based anytime planning
with landmarks  journal of artificial intelligence research             
roger  g     helmert  m          the more  the merrier  combining heuristic estimators for satisficing planning  in proceedings of the   th international conference on automated planning
and scheduling  icaps   pp         
veloso  m   carbonell  j   perez  a   borrajo  d   fink  e     blythe  j          integrating planning
and learning  the prodigy architecture  jetai              
vidal  v          a lookahead strategy for heuristic search planning  in proceedings of the   th
international conference on automated planning and scheduling  icaps        whistler 
british columbia  canada  pp         
xu  y   fern  a     yoon  s  w          discriminative learning of beam search heuristics for
planning  in ijcai       proceedings of the   th ijcai  pp           
   

fis caling up h euristic p lanning with r elational d ecision t rees

xu  y   fern  a     yoon  s          learning linear ranking functions for beam search with
application to planning  journal of machine learning research               
xu  y   fern  a     yoon  s          iterative learning of weighted rule sets for greedy search 
in proceedings of the   th international conference on automated planning and scheduling
 icaps  toronto  canada 
yoon  s   fern  a     givan  r          learning heuristic functions from relaxed plans  in proceedings of the   th international conference on automated planning and scheduling  icaps  
yoon  s   fern  a     givan  r          using learned policies in heuristic search planning  in
proceedings of the   th ijcai 
yoon  s   fern  a     givan  r          learning control knowledge for forward search planning 
j  mach  learn  res             
zimmerman  t     kambhampati  s          learning assisted automated planning  looking back 
taking stock  going forward  ai magazine             

   

fi