journal of articial intelligence research                  

submitted        published      

multiagent learning in large anonymous games
ian a  kash

kash seas harvard edu

center for research on computation and society
harvard university

eric j  friedman

ejf   cornell edu

department of operations research
and information engineering
cornell university

joseph y  halpern

halpern cs cornell edu

department of computer science
cornell university

abstract
in large systems  it is important for agents to learn to act eectively  but sophisticated
multi agent learning algorithms generally do not scale  an alternative approach is to nd
restricted classes of games where simple  ecient algorithms converge  it is shown that
stage learning eciently converges to nash equilibria in large anonymous games if bestreply dynamics converge  two features are identied that improve convergence  first 
rather than making learning more dicult  more agents are actually benecial in many
settings  second  providing agents with statistical information about the behavior of others
can signicantly reduce the number of observations needed 

   introduction
designers of distributed systems are frequently unable to determine how an agent in the
system should behave  because optimal behavior depends on the users preferences and
the actions of others  a natural approach is to have agents use a learning algorithm 
many multiagent learning algorithms have been proposed including simple strategy update
procedures such as ctitious play  fudenberg   levine         multiagent versions of qlearning  watkins   dayan         and no regret algorithms  cesa bianchi   lugosi        
our goal in this work is to help the designers of distributed systems understand when
learning is practical  as we discuss in section    existing algorithms are generally unsuitable
for large distributed systems  in a distributed system  each agent has a limited view of the
actions of other agents  algorithms that require knowing  for example  the strategy chosen
by every agent cannot be implemented  furthermore  the size of distributed systems requires
fast convergence  users may use the system for short periods of time and conditions in the
system change over time  so a practical algorithm for a system with thousands or millions
of users needs to have a convergence rate that is sublinear in the number of agents  existing
algorithms tend to provide performance guarantees that are polynomial or even exponential 
finally  the large number of agents in the system guarantees that there will be noise  agents
will make mistakes and will behave in unexpectedly  even if no agent changes his strategy 
there can still be noise in agent payos  for example  a gossip protocol will match dierent

c
    
ai access foundation  all rights reserved 

fikash  friedman    halpern

agents from round to round  congestion in the underlying network may eect message delays
between agents  a learning algorithm needs to be robust to this noise 
while nding an algorithm that satises these requirements for arbitrary games may
be dicult  distributed systems have characteristics that make the problem easier  first 
they involve a large number of agents  having more agents may seem to make learning
harderafter all  there are more possible interactions  however  it has the advantage that
the outcome of an action typically depends only weakly on what other agents do  this
makes outcomes robust to noise  having a large number of agents also make it less useful
for an agent to try to inuence others  it becomes a better policy to try to learn an optimal
response  in contrast  with a small number of agents  an agent can attempt to guide learning
agents into an outcome that is benecial for him 
second  distributed systems are often anonymous  it does not matter who does something  but rather how many agents do it  for example  when there is congestion on a link 
the experience of a single agent does not depend on who is sending the packets  but on how
many are being sent  anonymous games have a long history in the economics literature
 e g   blonski        and have been a subject of recent interest in the computer science
literature  daskalakis   papadimitriou        gradwohl   reingold        
finally  and perhaps most importantly  in a distributed system the system designer
controls the game agents are playing  this gives us a somewhat dierent perspective than
most work  which takes the game as given  we do not need to solve the hard problem
of nding an ecient algorithm for all games  instead  we can nd algorithms that work
eciently for interesting classes of games  where for us interesting means the type of
games a system designer might wish agents to play  such games should be well behaved 
since it would be strange to design a system where an agents decisions can inuence other
agents in pathological ways 
in section    we show that stage learning  friedman   shenker        is robust  implementable with minimal information  and converges eciently for an interesting class of
games  in this algorithm  agents divide the rounds of the game into a series of stages  in
each stage  the agent uses a xed strategy except that he occasionally explores  at the end
of a stage  the agent chooses as his strategy for the next stage whatever strategy had the
highest average reward in the current stage  we prove that  under appropriate conditions  a
large system of stage learners will follow  approximate  best reply dynamics  despite errors
and exploration 
for games where best reply dynamics converge  our theorem guarantees that learners
will play an approximate nash equilibrium  in contrast to previous results  where the convergence guarantee scales poorly with the number of agents  our theorem guarantees convergence in a nite amount of time with an innite number of agents  while the assumption
that best reply dynamics converge is a strong one  many interesting games converge under best reply dynamics  including dominance solvable games  games with monotone best
replies  and max solvable games  nisan  schapira    zohar         the class of max solvable
games in particular includes many important games such as transmission control protocol
 tcp  congestion control  interdomain routing with the border gateway protocol  bgp  
cost sharing games  and stable roommates games  nisan  schapira  valiant    zohar        
   in this paper  we consider best reply dynamics where all agents update their strategy at the same time 
some other results about best reply dynamics assume agents update their strategy one at a time 

   

fimultiagent learning in large anonymous games

marden  arslan  and shamma      a  have observed that convergence of best reply dynamics is often a property of games that humans design  although their observation was for a
slightly dierent notion of best reply dynamics   moreover  convergence of best reply dynamics is a weaker assumption than a common assumption made in the mechanism design
literature  that the games of interest have dominant strategies  each agent has a strategy
that is optimal no matter what other agents do  
simulation results  presented in section    show that convergence is fast in practice  a
system with thousands of agents can converge in a few thousand rounds  furthermore  we
identify two factors that determine the rate and quality of convergence  one is the number
of agents  having more agents makes the noise in the system more consistent so agents can
learn using fewer observations  the other is giving agents statistical information about the
behavior of other agents  this can speed convergence by an order of magnitude  indeed 
even noisy statistical information about agent behavior  which should be relatively easy to
obtain and disseminate  can signicantly improve performance 
while our theoretical results are limited to stage learning  they provide intuition about
why other well behaved learning algorithms should also converge  our simulations  which
include two other learning algorithms  bear this out  furthermore  to demonstrate the
applicability of stage learning in more realistic settings  we simulate the results of learning
in a scrip system  kash  friedman    halpern         our results demonstrate that stage
learning is robust to factors such as churn  agents joining and leaving the system  and
asynchrony  agents using stages of dierent lengths   however  stage learning is not robust
to all changes  we include simulations of games with a small number of agents  games
that are not anonymous  and games that are not continuous  these games violate the
assumptions of our theoretical results  our simulations show that  in these games  stage
learning converges very slowly or not at all 
finally  not all participants in a system will necessarily behave as expected  for learning
to be useful in a real system  it needs to be robust to such behavior  in section    we show
that the continuity of utility functions is a key property that makes stage learning robust
to byzantine behavior by a small fraction of agents 

   related work
one approach to learning to play games is to generalize reinforcement learning algorithms
such as q learning  watkins   dayan         one nice feature of this approach is that it
can handle games with state  which is important in distributed systems  in q learning  an
agent associates a value with each state action pair  when he chooses action  in state  
he updates the value      based on the reward he received and the best value he can
achieve in the resulting state   max          when generalizing to multiple agents  
and  become vectors of the state and action of every agent and the max is replaced by a
prediction of the behavior of other agents  dierent algorithms use dierent predictions 
for example  nash q uses a nash equilibrium calculation  hu   wellman         see the
work of shoham  powers  and grenager        for a survey 
unfortunately  these algorithms converge too slowly for a large distributed system  the
algorithm needs to experience each possible action prole many times to guarantee convergence  so  with  agents and  strategies  the naive convergence time is      even with
   

fikash  friedman    halpern

a better representation for anonymous games  the convergence time is still      typically
     there is also a more fundamental problem with this approach  it assumes information that an agent is unlikely to have  in order to know which value to update  the
agent must learn the action chosen by every other agent  in practice  an agent will learn
something about the actions of the agents with whom he directly interacts  but is unlikely
to gain much information about the actions of other agents 
another approach is no regret learning  where agents choose a strategy for each round
that guarantees that the regret of their choices will be low  hart and mas colell       
present such a learning procedure that converges to a correlated equilibrium   given knowledge of what the payos of every action would have been in each round  they also provide a variant of their algorithm that requires only information about the agents actual
payos  hart   mas colell         however  to guarantee convergence to within  of a
correlated equilibrium requires     log    still too slow for large systems  furthermore  the convergence guarantee is that the distribution of play converges to equilibrium 
the strategies of individual learners will not converge  many other no regret algorithms
exist  blum   mansour         in section    we use the exp  algorithm  auer  cesabianchi  freund    schapire         they can achieve even better convergence in restricted
settings  for example  blum  even dar  and ligett        showed that in routing games
a continuum of no regret learners will approximate nash equilibrium in a nite amount of
time  jafari  greenwald  gondek  and ercal        showed that no regret learners converge
to nash equilibrium in dominance solvable  constant sum  and general sum      games 
foster and young        use a stage learning procedure that converges to nash equilibrium for two player games  germano and lugosi        showed that it converges for generic
 player games  games where best replies are unique   young        uses a similar algorithm without explicit stages that also converges for generic  player games  rather than
selecting best replies  in these algorithms agents choose new actions randomly when not in
equilibrium  unfortunately  these algorithms involve searching the whole strategy space 
so their convergence time is exponential  another algorithm that uses stages to provide a
stable learning environment is the esrl algorithm for coordinated exploration  verbeeck 
nowe  parent    tuyls        
marden  arslan  and shamma      b  and marden  young  arslan  and shamma       
use an algorithm with experimentation and best replies but without explicit stages that
converges for weakly acyclic games  where best reply dynamics converge when agents move
one at a time  rather than moving all at once  as we assume here  convergence is based
on the existence of a sequence of exploration moves that lead to equilibrium  with 
agents who explore with probability   this analysis gives a convergence time of       
furthermore  the guarantee requires  to be suciently small that agents essentially explore
one at a time  so  needs to be      
adlakha  johari  weintraub  and goldsmith        have independently given conditions
for the existence of an oblivious equilibrium  or mean eld equilibrium  in stochastic
games  just as in our model they require that the game be large  anonymous  and continuous  in an oblivious equilibrium  each player reacts only to the average states and
   correlated equilibrium is a more general solution concept than nash equilibrium  see osborne   rubenstein         every nash equilibrium is a correlated equilibrium  but there may be correlated equilibria
that are not nash equilibria 

   

fimultiagent learning in large anonymous games

strategies of other players rather than their exact values  however  this model assumes that
a players payo depends only on the state of other players and not their actions  adlakha
and johari        consider stochastic games with strategic complementarities and show that
mean eld equilibria exist  best reply dynamics converge  and myopic learning dynamics
 which require only knowledge of the aggregate states of other players  can nd them 
there is a long history of work examining simple learning procedures such as ctitious
play  fudenberg   levine         where each agent makes a best response assuming that
each other players strategy is characterized by the empirical frequency of his observed
moves  in contrast to algorithms with convergence guarantees for general games  these algorithms fail to converge in many games  but for classes of games where they do converge 
they tend to do so rapidly  however  most work in this area assumes that the actions of
agents are observed by all agents  agents know the payo matrix  and payos are deterministic  a recent approach in this tradition is based on the win or learn fast principle 
which has limited convergence guarantees but often performs well in practice  bowling  
veloso         hopkins        showed that many such procedures converge in symmetric
games with an innite number of learners  although his results provide no guarantees about
the rate of convergence 
there is also a body of empirical work on the convergence of learning algorithms in
multiagent settings  q learning has had empirical success in pricing games  tesauro  
kephart          player cooperative games  claus   boutilier         and grid world
games  bowling         greenwald at al         showed that a number of algorithms 
including stage learning  converge in a variety of simple games  marden et al         found
that their algorithm converged must faster in a congestion game than the theoretical analysis
would suggest  our theorem suggests an explanation for these empirical observations  bestreply dynamics converge in all these games  while our theorem applies directly only to
stage learning  it provides intuition as to why algorithms that learn quickly enough and
change their behavior slowly enough rapidly converge to nash equilibrium in practice 

   theoretical results
in this section we present the theoretical analysis of our model  we then provide support
from simulations in the following section 
    large anonymous games
we are interested in anonymous games with countably many agents  assuming that there
are countably many agents simplies the proofs  it is straightforward to extend our results
to games with a large nite number of agents  our model is adapted from that of blonski         formally  a large anonymous game is characterized by a tuple           pr  
  is the countably innite set of agents 
  is a nite set of actions from which each agent can choose  for simplicity  we assume
that each agent can choose from the same set of actions  
     the set of probability distributions over   has two useful interpretations  the
rst is as the set of mixed actions  for    we will abuse notation and denote the
   

fikash  friedman    halpern

mixed action that is  with probability   as   in each round each agent chooses one
of these mixed actions  the second interpretation of      is as the fraction of
agents choosing each action     this is important for our notion of anonymity 
which says an agents utility should depend only on how many agents choose each
action rather than who chooses it 
              is the set of  mixed  action proles  i e  which action each
agent chooses   given the mixed action of every agent  we want to know the fraction
of agents that end up choosing action   for     let      denote the probability
with which agent  plays  according to        we can then express the fraction
of agents in  that choose action  as lim               if this limit exists  if
the limit exists for all actions     let      give the value of the limit for each
  the proles  that we use are all determined by a simple random process  for such
proles   the strong law of large numbers  slln  guarantees that with probability
   is well dened  thus it will typically be well dened  using similar limits  for us
to talk about the fraction of agents who do something 
    is the nite set of payos that agents can receive 
 pr             denotes the distribution over payos that results when the
agent performs action  and other agents follow action prole   we use a probability
distribution over payos rather than a payo to model the fact that agent payos may
change even if no agent changes his strategy  the expected utility of an agent who
performs

 mixed action  when other agents follow action distribution  is       

    pr      our denition of pr in terms of    rather than 
ensures the game is anonymous  we further require that pr  and thus   be lipschitz
continuous   for deniteness  we use the l  norm as our notion of distance when
specifying continuity  the l  distance between two vectors is the sum of the absolute
values of the dierences in each component   note that this formulation assumes all
agents share a common utility function  this assumption can be relaxed to allow
agents to have a nite number of types  which we show in appendix a 
an example of a large anonymous game is one where  in each round  each agent plays a
two player game against an opponent chosen at random  such random matching games are
common in the literature  e g   hopkins         and the meaning of an opponent chosen
at random can be made formal  boylan         in such a game   is the set of actions of
the two player game and  is the set of payos of the game  once every agent chooses an
action  the distribution over opponent actions is characterized by some       let  
denote the payo for the agent if he plays  and the other agent plays    then the utility
of mixed action  given distribution  is

      
        
   

   lipschitz continuity imposes the additional constraint that there is some constant  such that  pr    
pr             for all  and    intuitively  this ensures that the distribution of outcomes does
not change too fast  this is a standard assumption that is easily seen to hold in the games that have
typically been considered in the literature 

   

fimultiagent learning in large anonymous games

    best reply dynamics
given a game  and an action distribution   a natural goal for an agent is to play the
action that maximizes his expected utility with respect to   argmax       we call
such an action a best reply to   in a practical amount of time  an agent may have diculty
determining which of two actions with close expected utilities is better  so we will allow
agents to choose actions that are close to best replies  if  is a best reply to   then  is
an  best reply to  if                 there may be more than one  best reply  we
denote the set of  best replies abr     
we do not have a single agent looking for a best reply  every agent is trying to nd a one
at the same time  if agents start o with some action distribution     after they all nd a
best reply there will be a new action distribution     we assume that            agents
choose their initial strategy uniformly at random   but our results apply to any distribution
used to determine the initial strategy  we say that a sequence                 is an  bestreply sequence if the support of    is a subset of abr      that is    gives positive
probability only to approximate best replies to    a  best reply sequence converges if
there exists some  such that for all            note that this is a particularly
strong notion of convergence because we require the  to converge in nite time and not
merely in the limit  a game may have innitely many best reply sequences  so we say that
approximate best reply dynamics converge if there exists some      such that every  bestreply sequence converges  the limit distribution  determines a mixed strategy that is an
 nash equilibrium  i e  the support of  is a subset of       
our main result shows that learners can successfully learn in large anonymous games
where approximate best reply dynamics converge  the number of stages needed to converge
is determined by the number of best replies needed before the sequence converges  it is possible to design games that have long best reply sequences  but in practice most games have
short sequences  one condition that guarantees this is if   and all the degenerate action
distributions     i e   distributions that assign probability   to some     have unique
best replies  in this case  there can be at most  best replies before equilibrium is reached 
because we have assumed that all agents have the same utility function  furthermore  in
such games the distinction between  best replies and best replies is irrelevant  for suciently small   a  best reply is a best reply  it is not hard to show that the property that
degenerate strategies have unique best replies is generic  it holds for almost every game 
    stage learners
an agent who wants to nd a best reply may not know the set of payos    the mapping
from actions to distributions over payos pr  or the action distribution   and  indeed  
may be changing over time   so he will have to use some type of learning algorithm to learn
it  our approach is to divide the play of the game into a sequence of stages  in each stage 
the agent almost always plays some xed action   but also explores other actions  at the
end of the stage  he chooses a new  for the next stage based on what he has learned  an
important feature of this approach is that agents maintain their actions for the entire stage 
so each stage provides a stable environment in which agents can learn  to simplify our
results  we specify a way of exploring and learning within a stage  originally described in
friedman   shenker         but our results should generalize to any reasonable learning
   

fikash  friedman    halpern

algorithm used to learn within a stage   we discuss what is reasonable in section     in
this section  we show that  given a suitable parameter  at each stage most agents will have
learned a best reply to the environment of that stage 
given a game   in each round  agent  needs to select a mixed action     our
agents use strategies that we denote    for     where           and         
        thus  with    an agent almost always plays   but with probability  explores
other strategies uniformly at random  thus far we have not specied what information an
agent can use to choose    dierent games may provide dierent information  all that
we require is that an agent know all of his previous actions and his previous payos  more
precisely  for all      he knows his action      which is determined by     and his
payos      which is determined by pr         where  is the action distribution for
round    note that we do not assume that the agent knows     using this information  we
can express the average value of an action over the previous         rounds  the length
of a stage    let                            be 
the set of recent rounds in
which  was played by   then the average value is                           
if            and   otherwise  while we need the value of  only at times that are
multiples of    for convenience we dene it for arbitrary times  
we say that an agent is an  stage learner if he chooses his actions as follows  if      
 is chosen at random from         if  is a nonzero multiple of             where
       argmax          otherwise           thus  within a stage  his mixed
action is xed  at the end of a stage he updates it to use the action with the highest average
value during the previous stage 
the evolution of a game played by stage learners is not deterministic  each agent chooses
a random    and the sequence of     and     he observes is also random  however  with
a countably innite set of agents  we can use the slln to make statements about the overall
behavior of the game  let          a run of the game consists of a sequence of triples
           the slln guarantees that with probability   the fraction of agents who choose
a strategy  in  is      similarly  the fraction of agents who chose  in  that receive
payo  will be pr        with probability   
to make our notion of a stage precise  we refer to the sequence of tuples
                                            as stage  of the run  during stage  there
is a stationary action distribution that we denote    if          and   abr      
then we say that agent  has learned an  best reply during stage  of the run  as the following lemma shows  for suciently small   most agents will learn an  best reply 
lemma      for all large anonymous games   action proles  approximations       and
probabilities of error       there exists an      such that for     and all   if all
agents are  stage learners  then at least a     fraction of agents will learn an  best reply
during stage  
proof   sketch  on average  an agent using strategy  plays action        times during
a stage and plays all other actions         times each  for  large  the realized number
of times played will be close to the expectation value with high probability  thus  if  is
suciently large  then the average payo from each action will be exponentially close to the
   the use of the exponent   is arbitrary  we require only that the expected number of times a strategy is
explored increases as  decreases 

   

fimultiagent learning in large anonymous games

true expected value  via a standard hoeding bound on sums of i i d  random variables   and
thus each learner will correctly identify an action with approximately the highest expected
payo with probability at least      by the slln  at least a     fraction of agents will
learn an  best reply  a detailed version of this proof in a more general setting can be found
in the work by friedman and shenker        
    convergence theorem
thus far we have dened large anonymous games where approximate best reply dynamics
converge  if all agents in the game are  stage learners  then the sequence               of
action distributions in a run of the game is not a best reply sequence  but it is close  the
action used by most agents most of the time in each  is the action used in  for some
approximate best reply sequence 
in order to prove this  we need to dene close  our denition is based on the error rate
 and exploration rate  that introduces noise into    intuitively  distribution  is close to
 if  by changing the strategies of an  fraction of agents and having all agents explore an 
fraction of the time  we can go from an action prole with corresponding action distribution
 to one with corresponding distribution   note that this denition will not be symmetric 
in this denition   identies what  pure  action each agent is using that leads to   
allows an  fraction of agents to use some other action  and  incorporates the fact that
each agent is exploring  so each strategy is an   the agent usually plays  but explores
with probability   
denition      action distribution  is      close to  if there exist      and    such
that 
     and      
      for all    
          this allows an  fraction of agents in  to play a dierent strategy
from   
 for some     if        then        
the use of  in the nal requirement ensures that if two distributions are      close
then they are also        close for all    and     as an example of the asymmetry of
this denition   is       close to   but the reverse is not true  while      closeness is a
useful distance measure for our analysis  it is an unnatural notion of distance for specifying
the continuity of   where we used the l  norm  the following simple lemma shows that
this distinction is unimportant  if  is suciently      close to  then it is close according
to the l  measure as well 
lemma      if  is      close to   then 
            
proof  since  is      close to   there exist      and  as in denition      consider the
distributions         and      we can view these three distributions as vectors  and
calculate their l  distances  by denition                       because
an  fraction of agents explore  thus by the triangle inequality  the l  distance between 
and  is at most        
   

fikash  friedman    halpern

we have assumed that approximate best reply sequences of  converge  but during
a run of the game agents will actually be learning approximate best replies to    the
following lemma shows that this distinction does not matter if  and  are suciently close 
lemma      for all  there exists a  such that if  is      close to               and
       then abr       
   abr     
proof  let  be the maximum of the lipschitz constants for all       one constant for each
  and          then for all  that are      close to  and all              

                 by lemma     
let  
  abr     and   argmax        then                  combining
this with the above gives                    thus  
  abr    
  
lemmas     and     give requirements on       in the statement of the theorem  we call
      acceptable if they satisfy the requirements of both lemmas for    and all  best reply
sequences converge in  
theorem      let  be a large anonymous game where approximate best reply dynamics
converge and let      be  acceptable for   if all agents are  stage learners then  for all
runs  there exists an  best reply sequence               such that in stage  at least a    
fraction will learn a best reply to  with probability   
proof         both are the uniform distribution   so   is      close to   assume  is
     close to   by lemma     at least a     fraction will learn a    best reply to   
by lemma      this is a  best reply to    thus    will be      close to     
theorem     guarantees that after a nite number of stages  agents will be close to
an approximate nash equilibrium prole  specically   will be      close to an  nash
equilibrium prole    note that this means that  is actually an    nash equilibrium for
a larger   that depends on     and the lipschitz constant  
our three requirements for a practical learning algorithm were that it require minimal
information  converge quickly in a large system  and be robust to noise  stage learning
requires only that an agent know his own payos  so the rst condition is satised  theorem     shows that it satises the other two requirements  convergence is guaranteed in a
nite number of stages  while the number of stages depends on the game  in section     we
argued that in many cases it will be quite small  finally  robustness comes from tolerating
an  fraction of errors  while in our proofs we assumed these errors were due to learning 
the analysis is the same if some of this noise is from other sources such as churn or agents
making errors  we discuss this issue more in section   

   simulation results
in this section  we discuss experimental results that demonstrate the practicality of learning
in large anonymous games  theorem     guarantees convergence for a suciently small
exploration probability   but decreasing  also increases    the length of a stage  our
rst set of experiments shows that the necessary values of  and  are quite reasonable in
practice  while our theorem applies only to stage learning  the analysis provides intuition as
   

fimultiagent learning in large anonymous games

to why a reasonable algorithm that changes slowly enough that other learners have a chance
to learn best replies should converge as well  to demonstrate this  we also implemented two
other learning algorithms  which also quickly converged 
our theoretical results make two signicant predictions about factors that inuence
the rate of convergence  lemma     tells us that the length of a stage is determined by
the number of times each strategy needs to be explored to get an accurate estimate of its
value  thus  the amount of information provided by each observation has a large eect
on the rate of convergence  for example  in a random matching game  an agents payo
provides information about the strategy of one other agent  on the other hand  if he
receives his expected payo for being matched  a single observation provides information
about the entire distribution of strategies  in the latter case the agent can learn with many
fewer observations  a related prediction is that having more agents will lead to faster
convergence  particularly in games where payos are determined by the average behavior of
other agents  because variance in payos due to exploration and mistakes decreases as the
number of agents increases  our experimental results illustrate both of these phenomena 
the game used in our rst set of experiments  like many simple games used to test
learning algorithms  is symmetric  hopkins        showed that many learning algorithms
are well behaved in symmetric games with large populations  to demonstrate that our
main results are due to something other than symmetry  we also tested stage learning on
an asymmetric game  and observed convergence even with a small population 
to explore the applicability of stage learning in a more practical setting that violates a
number of the assumptions of our theorem  we implemented a variant of stage learning for
a game based on a scrip system  kash et al          to demonstrate the applicability of
this approach to real systems  we included experiments where there is churn  agents leaving
and being replaced by new agents  and agents learning at dierent rates 
finally  we give examples of games that are not large  not anonymous  and not continuous  and provide simulations showing that stage learners learn far more slowly in these
games than in those that satisfy the hypotheses of theorem      or do not learn to play
equilibrium at all  these examples demonstrate that these assumptions are essential for
our results 
    a contribution game
in our rst set of experiments  agents play a contribution game  also called a diamondtype search model in the work by milgrom   roberts         in the contribution game 
two agents choose strategies from   to     indicating how much eort they contribute to a
collective enterprise  the value to an agent depends on how much he contributes  as well
as how much the other agent contributes  if he contributes  and the contribution of the
other agent is   then his utility is             in each round of our game  each agent is
paired with a random agent and they play the contribution game  in this game  best reply
dynamics converge within   stages from any starting distribution 
we implemented three learning algorithms to run on this game  our implementation of
stage learners is as described in section      with          rather than taking the length of
stage  to be       we set         to have suciently long stages for this value of   rather
than decreasing  until stages are long enough  our second algorithm is based on that of

   

fikash  friedman    halpern

 
  agents
   agents
    agents

distance from equilibrium

 

 

 

 

 

 

 

 

 

 

 

time

 
 

x   

figure    stage learners with random matching 
   
  agents
   agents
    agents

distance from equilibrium

 

   

 

   

 

   

 

 

 

 
time

 

 
 

x   

figure    hart and mas colell with random matching 
hart and mas colell         with improvements suggested by greenwald  friedman  and
shenker         this algorithm takes parameters  and   the exploration probability  
we used       and          our nal learning algorithm is exp   auer et al         
we set   the exploration probability  to       this algorithm requires that payos be
normalized to lie in         since a few choices of strategies lead to very large negative
payos  a naive normalization leads to almost every payo being close to    for better
performance  we normalized payos such that most payos fell into the range        and any
that were outside were set to   or   as appropriate 
the results of these three algorithms are shown in figures       and    each curve
shows the distance from equilibrium as a function of the number of rounds of a population
of agents of a given size using a given learning algorithm  the results were averaged over
ten runs  since the payos for nearby strategies are close  we want our notion of distance
to take into account that agents playing   are closer to equilibrium     thanthose playing
zero  therefore  we consider the expected distance of  from equilibrium 
       
to determine   we counted the number of times each action was taken over the length of
   

fimultiagent learning in large anonymous games

 
  agents
   agents
    agents

   

distance from equilibrium

 
   
 
   
 
   
 
   

 

 

 

 

 

time

 
 

x   

figure    exp  with random matching 
 
  agents
   agents
    agents

distance from equilibrium

 

 

 

 

 

 

 

 

 

 
time

 

 
 

x   

figure    stage learning with average based payos 
a stage  so in practice the distance will never be zero due to mistakes and exploration  for
ease of presentation  the graph shows only populations of size up to      similar results
were obtained for populations up to      agents 
for stage learning  increasing the population size has a dramatic impact  with two
agents  mistakes and best replies to the results of these mistakes cause behavior to be quite
chaotic  with ten agents  agents successfully learn  although mistakes and suboptimal
strategies are quite frequent  with one hundred agents  all the agents converge quickly to
near equilibrium strategies and signicant mistakes are rare 
despite a lack of theoretical guarantees  our other two algorithms also converge  although
somewhat more slowly  the long run performance of exp  is similar to stage learning 
hart and mas colells algorithm only has asymptotic convergence guarantees  and tends
to converge slowly in practice if tuned for tight convergence  so to get it to converge in a
reasonable amount of time we tuned the parameters to accept somewhat weaker convergence
 although for the particular game shown here the dierence in convergence is not dramatic  

   

fikash  friedman    halpern

   
  agents
   agents
    agents

   

distance from equilibrium

   
   
 
   
   
   
   
 

 

   

 

   
time

 

   
 

x   

figure    stage learners in a congestion game 
convergence of stage learning in the random matching game takes approximately       
rounds  which is too slow for many applications  if a system design requires this type of
matching  this makes learning problematic  however  the results of figure   suggest that
the learning could be done much faster if the system designer could supply agents with
more information  this suggests that collecting statistical information about the behavior
of agents may be a critical feature for ensuring fast convergence  to model such a scenario 
consider a related game where  rather than being matched against a random opponent  all
agents contribute to the same project and their reward is based on the average contribution
of the other agents  the results of stage learning in this game are shown in figure    with
so much more information available to agents from each observation  we were able to cut
the length of a stage by a factor of     the number of stages needed to reach equilibrium
remained essentially the same  convergence was tighter as well  mistakes were rare and
almost all of the distance from equilibrium is due to exploration 
    a congestion game
for a dierent game  we tested the performance of stage learners in a congestion game 
this game models a situation where two agents share a network link  they gain utility
proportional to their transmission rate over the link  but are penalized based on the resulting
congestion they experience  the game is asymmetric because the two dierent types of
agents place dierent values on transmission rate  the game is described in detail by
greenwald  friedman  and shenker         who showed that no regret learners are able to
nd the equilibrium of this game  an extension of our theoretical results to games with
multiple types is presenting in appendix a 
figure   shows that stage learners were able to learn very quickly in this game  using
stages of length     even though they were being randomly matched against a player of
the other type  because the dierent types of agents had dierent equilibrium strategies 
the distance measure we use is to treat the observed distribution of strategies and the
equilibrium distribution as vectors and compute their l  distance 

   

fimultiagent learning in large anonymous games

 

fraction playing equilibrium strategy

   
   
   
   
   
   
   

capacity  
capacity  
capacity  

   
   
 

 

 

  
number of stages

  

  

figure    stage learners in a tcp like game 
    a tcp like game
in the previous example  we considered a random matching game where two agents of
dierent types share a link  we now consider a game where a large number of agents share
several links  this game is a variant of the congestion control game studied in nisan et al  
      
there are three types of agents using a network  each agent chooses an integer rate at
which to transmit between   and     links in the network have a maximum average rate at
which agents can transmit  if this is exceeded they share the capacity evenly among agents 
an agents utility is his overall transmission rate through the network minus a penalty for
trac that was dropped due to congestion  if an agent attempts to transmit at a rate of 
and has an actual rate of  the penalty is          
all agents share a link with an average capacity of    one third of agents are further
constrained by sharing a link with an average capacity of   and another third share a link
with average capacity of    this game has the unique equilibrium where agents in the rst
third choose a rate of    agents in the second third choose a rate of    and agents in the
nal third choose a rate of    so that the overall average rate is     this results in a game
where best reply dynamics converge in ve stages from a uniform starting distribution 
figure   shows the results for    learners     of each type  with          and         
averaged over ten runs  agents constrained by an average capacity of two quickly learn their
equilibrium strategy  followed by those with an average capacity of four  agents constrained
by an average capacity of ve learn their equilibrium strategy  but have a sawtooth pattern
where a small fraction alternately plays    rather than    this is because  with exploration 
it is actually optimal for a small number of agents to play     once a noticeable fraction
does so    is uniquely optimal  this demonstrates that  strictly speaking  this game does
not satisfy our continuity requirement  in equilibrium  the demand for bandwidth is exactly
equal to the supply  thus  small changes in the demand of other agents due to exploration
can have a large eect on the amount that can actually be demanded and thus on the payos
   this penalty is not used in the work by nisan et al          using it avoids the tie breaking issues they
consider 

   

fikash  friedman    halpern

 

fraction playing equilibrium strategy

   
   
   
   
   
   
   
type  
type  
type  

   
   
 

 

 

  
number of stages

  

  

figure    stage learners in random tcp like games 
of various strategies  however  the structure of the game is such that play still tends to
remain close to the equilibrium in terms of the rates agents choose 
in addition to the specic parameters mentioned above  we also ran     simulations
where each of the three capacities was a randomly chosen integer between   and     figure  
shows that  on average  the results were similar  all three types of agents share a common
constraint  type   and type   each have an additional constraint  unsurprisingly  since these
two types are symmetric their results are almost identical  all three types demonstrate the
sawtooth behavior  with type   doing so in more runs due to examples like figure   where
having fewer constraints gives agents more exibility  this primarily comes from runs where
type   and type   have constraints that are larger than the overall constraint  i e  only the
overall constraint matters   thus all three types have the ability to benet from resources
not demanded when other agents explore 
    a scrip system game
our motivation for this work is to help the designers of distributed systems understand
when learning is practical  in order to demonstrate how stage learning could be applied in
such a setting  we tested a variant of stage learners in the model of a scrip system used by
kash et al          in the model  agents pay other agents to provide them service and in
turn provide service themselves to earn money to pay for future service  agents may place
dierent values on receiving service     incur dierent costs to provide service     discount
future utility at dierent rates     and have dierent availabilities to provide service    
we used a single type of agent with parameters                                average
amount of money per agent       and stages of     rounds per agent  only one agent
makes a request each round  
this model is not a large anonymous game because whether an agent should provide
service depends on how much money he currently has  thus  stage learning as specied does
not work  because it does not take into account the current state of the  stochastic  game 
despite this  we can still implement a variant of stage learning  x a strategy during each
stage and then at the end of the stage use an algorithm designed for this game to determine
   

fimultiagent learning in large anonymous games

   
   agents
    agents

   

distance from equilibrium

   
 
   
   
   
   
 
   

   

   

   

 

   

   

   

   

time

 
 

x   

figure    stage learners in a scrip system 
   
   agents
    agents

   

distance from equilibrium

   
 
   
   
   
   
 
   

   

   

   

 

   
time

   

   

   

 
 

x   

figure    a scrip system with churn 
a new strategy that is a best reply to what the agent observed  our algorithm works by
estimating the agents probabilities of making a request and being chosen as a volunteer
in each round  and then uses these probabilities to compute an optimal policy  figure  
shows that this is quite eective  the distance measure used is based on directly measuring
the distance of the agents chosen  threshold  strategy from the equilibrium strategy  since
unlike the previous games it is impossible to directly infer the agents strategy in each round
solely from his decision whether or not to volunteer  note that the number of rounds has
been normalized based on the number of agents in figure   and later gures  stages actually
lasted ten times as long with     agents 
real systems do not have a static population of learning agents  to demonstrate the
robustness of stage learning to churn  we replaced ten percent of the agents with new agents
with randomly chosen initial strategies at the end of each period  as figure   shows  this
has essentially no eect on convergence 

   

fikash  friedman    halpern

   
   agents
    agents

   

distance from equilibrium

   
 
   
   
   
   
 
   

   

   

   

 

   
time

   

   

   

 
 

x   

figure     a scrip system with dierent stage lengths 
finally  in a real system it is often unreasonable to expect all agents to be able to update
their strategies at the same time  figure    shows that having half the agents use stages of
    rounds per agent rather than     did not have a signicant eect on convergence  
    learning counterexamples
at rst glance  theorem     may seem trivial  in a game where best reply dynamics are
guaranteed to converge  it seems obvious that agents who attempt to nd best replies
should successfully nd them and reach equilibrium  however  as we show in this section 
this fact alone is not sucient  in particular  all three of the key features of the games we
studythat they are large  anonymous  and continuousare required for the theorem to
hold 
first  if the game has only a small number of agents  a mistake made by a single
agent could be quite important  to the point where learning essentially has to start over 
so  while our results can be converted into results about the probability that none of a
nite number of agents will make a mistake in a given stage  the expected time to reach
equilibrium following this algorithm can be signicantly longer than the best reply dynamics
would suggest  the following is an example of a game where the number of best replies
needed to reach equilibrium is approximately the number of strategies  but our experimental
results show that the number of stages needed by stage learners to nd the equilibrium is
signicantly longer   we conjecture that in fact the learning time is exponentially longer  
in contrast  theorem     guarantees that  for games satisfying our requirements  the number
of stages needed is equal to the number of best replies 
consider a game with three agents  where   the set of actions  is                   the
utility functions of the agents are symmetric  the rst agents utility function is given by
the following table 
   in general we expect that small variations in stage lengths will not aect convergence  however large
enough dierences can result in non nash convergence  see the work by greenwald et al         for some
simulations and analysis 

   

fimultiagent learning in large anonymous games

actions
       
      
         
         
         
         
      
      
      
      

payo
 
 
 
 
 
 
 
 
 
 

conditions
if     and either      or     
if     and      and either      or     

if        
if         and    
if    

agents learning best replies can be viewed as climbing a ladder  the best reply to
       is                    until agents reach         which is a nash equilibrium 
however  when a mistake is made  agents essentially start over  to see how this works 
suppose that agents are at           and for the next stage one makes a mistake and they
select            this leads to the best reply sequence                                  at which
point agents can begin climbing again  the somewhat complicated structure of payos
near   ensures that agents begin climbing again from arbitrary patterns of mistakes  in a
typical run       stages of best replies are needed to reach equilibrium  one stage with the
initial randomly chosen strategies  one stage where all three agents switch to strategy   
and  stages of climbing  the exact number of stages can vary if two or more agents choose
the same initial strategy  but can never be greater than      
the following table gives the number of rounds  averaged over ten runs  for stage learners
in this game to rst reach equilibrium  as the number of strategies varies  the length of a
stage is               with exploration probability         
 rounds to reach 
 
   
 
    
  
    
  
    
  
    
  
     
  
     
  
     
with       stage learners typically require      stages  with an occasional error raising
the average slightly  with  between   and     a majority of runs feature at least one agent
making a mistake  so the number of stages required is closer to    with       and up 
there are many opportunities for agents to make a mistake  so the number of stages required
on average is in the range of   to    thus learning is slower than best reply dynamics 
and the disparity grows as the number of strategies increases 
a small modication of this example shows the problems that arise in games that are
not anonymous  in a non anonymous game with a large number of agents  payos can
depend entirely on the actions of a small number of agents  for example  we can split the
set  of agents into three disjoint sets          and     and choose agents              
   

fikash  friedman    halpern

 
   
   

fraction playing  

   
   
   
   agents
    agents
     agents

   
   
   
   
 

 

 

  
number of stages

  

  

figure     stage learners in a discontinuous game 
and        again  each agent chooses an action in                the payos of agents      
and   are determined as above  everyone in   gets the same payo as    everyone in  
gets the same payo as    and everyone in   gets the same payo as    again  convergence
to equilibrium will be signicantly slower than with best reply dynamics 
finally  consider the following game  which is large and anonymous  but does not satisfy
the continuity requirement  the set of actions is            and each agent always receives
a payo in                if an agent chooses action    his payo is always    pr            
if he chooses action    his payo is    if every other agent chooses action       if every
other agent chooses action    and   otherwise  pr                  pr                  and
pr            for                     
in this game  suppose approximate best reply dynamics start at             each action
is chosen by half of the agents   as they have not coordinated  the unique approximate best
reply for all agents is action    so after one best reply  the action distribution will be        
since agents have now coordinated  another round of approximate best replies leads to the
equilibrium         if the agents are stage learners  after the rst stage they will learn an
approximate best reply to             exploration does not change the action prole in this
case   so most will adopt the mixed action     playing   with probability     and   with
probability   thus  even if no agents make a mistake  the action distribution for the next
stage will have at least an  fraction playing action    thus the unique approximate best
reply will be action    stage learners will be stuck at    and never reach the equilibrium
of   
figure    shows the fraction of times strategy   was played during each stage  averaged
over ten runs  for          and      agents         and           with ten agents 
some initial mistakes are made  but after stage    strategy   was played about      of
the time in all runs  which corresponds to the fraction of time we expect to see it simply
from exploration  with     agents we see another sawtooth pattern where most agents
are stuck playing    but in alternating rounds a small fraction plays    this happens
because  in rounds where all are playing    a small fraction are lucky and explore   when
no other agents explore  as a result  they adopt strategy   for the next stage  however 
most do not  so in the following stage agents return to all playing    such oscillating
   

fimultiagent learning in large anonymous games

behavior has been observed in other learning contexts  for example among competing myopic
pricebots  kephart  hanson    greenwald         with      agents  such lucky agents are
quite rare  so essentially all agents are constantly stuck playing   

   learning with byzantine agents
in practice  learning algorithms need to be robust to the presence of other agents not
following the algorithm  we have seen that stage learning in large anonymous games is
robust to agents who do not learn and instead follow some xed strategy in each stage 
in the analysis  these agents can simply be treated as agents who made a mistake in the
previous stage  however  an agent need not follow some xed strategy  an agent attempting
to interfere with the learning of other for malicious reasons or personal gain will likely adapt
his strategy over time  however  as we show in this section  stage learning can also handle
such manipulation in large anonymous games 
gradwohl and reingold        examined several classes of games and introduced the
notion of a stable equilibrium as one in which a change of strategy by a small fraction
of agents only has a small eect on the payo of other agents  their denition is for
games with a nite number of agents  but it can easily be adapted to our notion of a large
anonymous game  we take this notion a step further and characterize the game  rather
than an equilibrium  as stable if every strategy is stable 
denition      a large anonymous game  is      stable if for all        such that
       and all                   
one class of games they consider is  continuous games   continuity is essentially a
version of lipschitz continuity for nite games  so it easy to show that large anonymous
games are stable  where the amount of manipulation that can be tolerated depends on the
lipschitz constants for agents utility functions 
lemma      for all large anonymous games   there exists a constant  such that for
all    is        stable
proof  for all        is lipschitz continuous with a constant  such that      
               take    max    then for all  and  such that      
   
                  
      
 
 

    

  

gradwohl and reingold        show that stable equilibria have several nice properties 
if a small fraction of agents deviate  payos for the other agent will not decrease very much
relative to equilibrium  additionally  following those strategies will still be an approximate
   

fikash  friedman    halpern

equilibrium despite the deviation  finally  this means that the strategies still constitute
an approximate equilibrium even if asynchronous play causes the strategies of a fraction of
agents to be revealed to others 
we show that  if the game is stable  then learning is also robust to the actions of a small
fraction of byzantine agents  the following lemma adapts lemma     to show that  in each
stage  agents can learn approximate best replies despite the actions of byzantine agents 
thus agents can successfully reach equilibrium  as shown by theorem     
to state the lemma  we need to dene the actions of a byzantine agent  if there were
 and
no byzantine agents  then in stage  there would be some stationary strategy 

corresponding fraction of agents choosing each action    a  fraction of byzantine agents
can change their actions arbitrarily each round  but doing so will have no eect on the
actions of the other agents  thus  the byzantine agents can cause the observed fraction of
agents choosing each strategy in round  to be any  such that           we refer
to a sequence                   such that this condition holds for each  as a consistent
sequence  when we say that agents learn an  best reply during stage   we mean that the
   the actions that the players
strategy that they learn is an approximate best reply to 
would have used had there been no byzantine players  not the actual action prole  which
includes the strategies used by the byzantine players 
lemma      for all large anonymous games   action distributions     approximations
      probabilities of error       and fractions of agents         there exists an     
such that for       all   and all consistent sequences                     if all agents are
 stage learners  then at least a     fraction of agents will learn an  best reply during
stage  despite a  fraction of byzantine agents 
proof  consider an agent  and round  in stage   if all agents were stage learners then
the action distribution would be        however  the byzantine agents have changed it
such that         fix an action   by lemma     
                  

 

  
 
 

this means that byzantine agents can adjust an agents expected estimate of the value
of an action by at most     let  be a best reply to    the action used by stage learners
during stage    in each round  of stage  
                


 
 

for any action  that is not an  best reply 
                                                   


 
 
 
 
 

thus  regardless of the actions of the  fraction of byzantine agents  agent s expected
estimate of the value of  exceeds his expected estimate of the value of  by at least    
using hoeding bounds as before  for suciently large   s estimates will be exponentially
close to these expectations  so with probability at least      he will not select as best any
action that is not an  best reply  by the slln  this means that at least a     fraction
of agents will learn an  best reply 
   

fimultiagent learning in large anonymous games

thus  as lemma     shows  not only can stage learners learn despite some agents learning incorrect values  they can also tolerate a suciently small number of agents behaving
arbitrarily 

   discussion
while our results show that a natural learning algorithm can learn eciently in an interesting class of games  there are many further issues that merit exploration 
    other learning algorithms
our theorem assumes that agents use a simple rule for learning within each stage  they
average the value of payos received  however  there are certainly other rules for estimating the value of an action  any of these can be used as long as the rule guarantees that
errors can be made arbitrarily rare given sucient time  it is also not necessary to restrict
agents to stage learning  stage learning guarantees a stationary environment for a period
of time  but such strict behavior may not be needed or practical  other approaches  such
as exponentially discounting the weight of observations  greenwald et al         marden
et al         or win or learn fast  bowling   veloso        allow an algorithm to focus
its learning on recent observations and provide a stable environment in which other agents
can learn 
    other update rules
in addition to using dierent algorithms to estimate the values of actions  a learner could
also change the way he uses those values to update his behavior  for example  rather than
basing his new strategy on only the last stage  he could base it on the entire history of
stages and use a rule in the spirit of ctitious play  since there are games where ctitious
play converges but best reply dynamics do not  this could extend our results to another
interesting class of games  as long as the errors in each period do not accumulate over time 
another possibility is to update probabilistically or use a tolerance to determine whether
to update  see  e g   foster   young        hart   mas colell         this could allow
convergence in games where best reply dynamics oscillate or decrease the fraction of agents
who make mistakes once the system reaches equilibrium 
    model assumptions
our model makes several unrealistic assumptions  most notably that there are countably
many agents who all share the same utility function  essentially the same results holds
with a large  nite number of agents  adding a few more error terms  in particular  since
there is always a small probability that every agent makes a mistake at the same time  we
can prove only that no more than a     fraction of the agents make errors in most rounds 
and that agents spend most of their time playing equilibrium strategies 
we have also implicitly assumed that the set of agents is xed  as figure   shows 
we could easily allow for churn  a natural strategy for newly arriving agents is to pick
a random  to use in the next stage  if all agents do this  it follows that convergence is
unaected  we can treat the new agents as part of the  fraction that made a mistake in the
   

fikash  friedman    halpern

last stage  furthermore  this tells us that newly arriving agents catch up very quickly 
after a single stage  new agents are guaranteed to have learned a best reply with probability
at least     
finally  we have assumed that all agents have the same utility function  our results can
easily be extended to include a nite number of dierent types of agents  each with their
own utility function  since the slln can be applied to each type of agent  this extension
is discussed in appendix a  we believe that our results hold even if the set of possible
types is innite  this can happen  for example  if an agents utility depends on a valuation
drawn from some interval  however  some care is needed to dene best reply sequences in
this case 
    state
one common feature of distributed systems not addressed in the theoretical portion of this
work is state  as we saw with the scrip system in section      an agents current state is
often an important factor in choosing an optimal action 
in principle  we could extend our framework to games with state  in each stage each
agent chooses a policy to usually follow and explores other actions with probability   each
agent could then use some o policy algorithm  one where the agent can learn without
controlling the sequence of observations  see kaelbling  littman    moore       for examples  to learn an optimal policy to use in the next stage  one major problem with
this approach is that standard algorithms learn too slowly for our purposes  for example  q learning  watkins   dayan        typically needs to observe each state action pair
hundreds of times in practice  the low exploration probability means that the expected
  rounds needed to explore each pair even once is large  ecient learning requires
more specialized algorithms that can make better use of the structure of a problem  however  the use of specialized algorithms makes providing a general guarantee of convergence
more dicult  another problem is that  even if an agent explores each action for each of
his possible local states  the payo he receives will depend on the states of the other agents 
and thus the actions they chose  we need some property of the game to guarantee that this
distribution of states is in some sense well behaved  adlakha and joharis        work on
mean eld equilibria gives one such condition  in this setting  the use of publicly available
statistics might provide a solution to these problems 
    mixed equilibria
another restriction of our results is that our agents only learn pure strategies  one way
to address this is to discretize the mixed strategy space  see  e g   foster   young        
if one of the resulting strategies is suciently close to an equilibrium strategy and bestreply dynamics converge with the discretized strategies  then we expect agents to converge
to a near equilibrium distribution of strategies  we have had empirical success using this
approach to learn to play rock paper scissors 

   

fimultiagent learning in large anonymous games

   conclusion
learning in distributed systems requires algorithms that are scalable to thousands of agents
and can be implemented with minimal information about the actions of other agents  most
general purpose multiagent learning algorithms fail one or both of these requirements  we
have shown here that stage learning can be an ecient solution in large anonymous games
where approximate best reply dynamics lead to approximate pure strategy nash equilibria 
many interesting classes of games have this property  and it is frequently found in designed
games  in contrast to previous work  the time to convergence guaranteed by the theorem
does not increase with the number of agents  if system designers can nd an appropriate
game satisfying these properties on which to base their systems  they can be condent that
nodes can eciently learn appropriate behavior 
our results also highlight two factors that aid convergence  first  having more learners often improves performance  with more learners  the noise introduced into payos by
exploration and mistakes becomes more consistent  second  having more information typically improves performance  publicly available statistics about the observed behavior of
agents can allow an agent to learn eectively while making fewer local observations  our
simulations demonstrate the eects of these two factors  as well how our results generalize
to situations with other learning algorithms  churn  asynchrony  and byzantine behavior 
    acknowledgments
most of the work was done while ik was at cornell university  ef  ik  and jh are supported
in part by nsf grant itr          jh is also supported in part by nsf grant iis        
and by afosr grants fa               and fa                ef is also supported in
part by nsf grant cdi         

appendix a  multiple types
in this section  we extend our denition of a large anonymous game to settings where
agents may have dierent utility functions  to do so  we introduce the notion of a type 
agents utilities may depend on their type and the fraction of each type taking each action 
as our results rely on the strong law of large numbers  we restrict the set of types to
be nite  formally  a large anonymous game with types is characterized by a tuple   
           pr   we dene       and  as before  for the remaining terms 
  is a nite set of agent types 
       is a function mapping each agent to his type 
 as before     is the set of probability distributions over   and can be viewed as
the set of mixed actions available to an agent  but now  to describe the fraction of
agents of each type choosing each action  we must use element of     
 pr            determines the distribution over payos that results when
an agent of type  performs action  and other agents follow action prole   the
expected utility of an agent of type  who performs mixed action  when other agents

   

fikash  friedman    halpern



follow action distribution  is               pr       as before  we
further require that pr  and thus   be lipschitz continuous 
the revised denitions of an  best reply  an  nash equilibrium  an  best reply sequence  convergence of approximate best reply dynamics  and      close follow naturally
from the revised denitions of  and   lemma     now applies to each type of agent separately  and shows that all but a small fraction of each type will learn an approximate best
reply in each stage  lemma     and lemma     hold given the revised denitions of  and
  thus theorem      which combines these  also still holds 

references
adlakha  s     johari  r          mean eld equilibrium in dynamic games with complementarities  in ieee conference on decision and control  cdc  
adlakha  s   johari  r   weintraub  g  y     goldsmith  a          mean eld analysis
for large population stochastic games  in ieee conference on decision and control
 cdc  
auer  p   cesa bianchi  n   freund  y     schapire  r  e          the nonstochastic multiarmed bandit problem  siam journal on computing               
blonski  m          equilibrium characterization in large anonymous games  tech  rep  
u  mannheim 
blum  a   even dar  e     ligett  k          routing without regret  on convergence
to nash equilibria of regret minimizing algorithms in routing games  in   th acm
symp  on principles of distributed computing  podc   pp       
blum  a     mansour  y          learning  regret minimization  and equilibria  in nisan 
n   roughgarden  t   tardos  e     vazirani  v   eds    algorithmic game theory 
pp         cambridge university press 
bowling  m  h          convergence problems of general sum multiagent reinforcement
learning  in   th int  conf  on machine learning  icml        pp       
bowling  m  h     veloso  m  m          rational and convergent learning in stochastic
games  in   th int  joint conference on articial intelligence  ijcai        pp 
         
boylan  r  t          laws of large numbers for dynamical systems with randomly matched
indviduals  journal of economic theory             
cesa bianchi  n     lugosi  g          prediction  learning and games  cambridge university press 
claus  c     boutilier  c          the dynamics of reinforcement learning in cooperative
multiagent systems  in aaai    workshop on multiagent learning  pp         
daskalakis  c     papadimitriou  c  h          computing equilibria in anonymous games 
in   th annual ieee symposium on foundations of computer science  focs       
pp       

   

fimultiagent learning in large anonymous games

foster  d  p     young  p          regret testing  learning to play nash equilibrium without
knowing you have an opponent  theoretical economics            
friedman  e  j     shenker  s          learning and implementation on the internet  tech 
rep   cornell university 
fudenberg  d     levine  d          theory of learning in games  mit press 
germano  f     lugosi  g          global nash convergence of foster and youngs regret
testing  games and economic behavior                 
gradwohl  r     reingold  o          fault tolerance in large games  in proc   th acm
conference on electronic commerce  ec        pp         
greenwald  a   friedman  e  j     shenker  s          learning in networks contexts 
experimental results from simulations  games and economic behavior              
    
hart  s     mas colell  a          a simple adaptive procedure leading to correlated equilibrium  econometrica                   
hart  s     mas colell  a          a reinforecement learning procedure leading to correlated
equilibrium  in debreu  g   neuefeind  w     trockel  w   eds    economic essays 
pp          springer 
hopkins  e          learning  matching  and aggregation  games and economic behavior 
          
hu  j     wellman  m  p          nash q learning for general sum stochastic games  journal
of machine learning research              
jafari  a   greenwald  a  r   gondek  d     ercal  g          on no regret learning 
ctitious play  and nash equilibrium  in proc  eighteenth international conference on
machine learning  icml   pp         
kaelbling  l  p   littman  m  l     moore  a  p          reinforcement learning  a survey 
j  artif  intell  res   jair             
kash  i  a   friedman  e  j     halpern  j  y          optimizing scrip systems  eciency 
crashes  hoarders and altruists  in eighth acm conference on electronic commerce
 ec        pp         
kephart  j  o   hanson  j  e     greenwald  a  r          dynamic pricing by software
agents  computer networks                 
marden  j  r   arslan  g     shamma  j  s       a   connections between cooperative
control and potential games  in proc       european control conference  ecc  
marden  j  r   arslan  g     shamma  j  s       b   regret based dynamics  convergence in
weakly acyclic games  in  th int  joint conf  on autonomous agents and multiagent
systems  aamas   pp       
marden  j  r   young  h  p   arslan  g     shamma  j  s          payo based dynamics
for multi player weakly acyclic games  siam journal on control and optimization 
               

   

fikash  friedman    halpern

milgrom  p     roberts  j          rationalizability  learning  and equilibrium in games
with strategic complement  arities  econometrica                   
nisan  n   schapira  m   valiant  g     zohar  a          best response mechanisms  in
proc  second syposium on innovations in computer science  ics   to appear 
nisan  n   schapira  m     zohar  a          asynchronous best reply dynamics  in
proc   th international workshop on internet and netwrok economics  wine   pp 
       
osborne  m     rubenstein  a          a course in game theory  mit press 
shoham  y   powers  r     grenager  t          multi agent reinforcement learning  a
critical survey  tech  rep   stanford 
tesauro  g     kephart  j  o          pricing in agent economies using multi agent qlearning  autonomous agents and multi agent systems                
verbeeck  k   nowe  a   parent  j     tuyls  k          exploring selsh reinforcement
learning in repeated games with stochastic rewards  journal of autonomous agents
and multi agent systems             
watkins  c  j     dayan  p          technical note q learning  machine learning    
       
young  h  p          learning by trial and error  games and economic behavior         
       

   

fi