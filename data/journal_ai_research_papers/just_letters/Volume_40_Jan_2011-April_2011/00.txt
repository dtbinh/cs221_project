journal of artificial intelligence research               

submitted        published      

non deterministic policies in
markovian decision processes
mahdi milani fard
joelle pineau

mmilan  cs mcgill ca
jpineau cs mcgill ca

reasoning and learning laboratory
school of computer science  mcgill university
montreal  qc  canada

abstract
markovian processes have long been used to model stochastic environments  reinforcement learning has emerged as a framework to solve sequential planning and decision making
problems in such environments  in recent years  attempts were made to apply methods
from reinforcement learning to construct decision support systems for action selection in
markovian environments  although conventional methods in reinforcement learning have
proved to be useful in problems concerning sequential decision making  they cannot be applied in their current form to decision support systems  such as those in medical domains 
as they suggest policies that are often highly prescriptive and leave little room for the users
input  without the ability to provide flexible guidelines  it is unlikely that these methods
can gain ground with users of such systems 
this paper introduces the new concept of non deterministic policies to allow more flexibility in the users decision making process  while constraining decisions to remain near
optimal solutions  we provide two algorithms to compute non deterministic policies in
discrete domains  we study the output and running time of these method on a set of
synthetic and real world problems  in an experiment with human subjects  we show that
humans assisted by hints based on non deterministic policies outperform both human only
and computer only agents in a web navigation task 

   introduction
planning and decision making have been well studied in the ai community  intelligent
agents have been designed and developed to act in  and interact with  a variety of environments  this usually involves sensing the environment  making a decision using some
intelligent inference mechanism  and then performing an action on the environment  russell   norvig         often times  this process involves some level of learning  along with
the decision making process  to make the agent more efficient in performing the intended
goal 
reinforcement learning  rl  is a branch of ai that tries to develop a computational
approach to solving the problem of learning through interaction  rl is the process of
learning what to dohow to map situations to actionsso as to maximize a numerical
reward signal  sutton   barto         many methods have been developed to solve the
rl problem with different types of environments and different types of agents  however 
most of the work in rl has focused on autonomous agents such as robots or software
agents  the rl controllers are thus designed to issue a single action at each time step
c
    
ai access foundation  all rights reserved 

fimilani fard   pineau

which will be executed by the acting agent  in the past few years  methods developed by
the rl community have started to be used in sequential decision support systems  murphy 
      pineau  bellemare  rush  ghizaru    murphy        thapa  jung    wang       
hauskrecht   fraser         in many of these systems  a human being makes the final
decision  usability and acceptance issues thus become important in these cases  most
rl methods therefore require some level of adaptation to be used with decision support
systems  such adaptations are the main contribution of this paper 
medical domains are among the cases for which rl needs further adaptation  although
the rl framework correctly models sequential decision making in complex medical scenarios  including long term treatment design  standard rl methods cannot be applied to
medical settings in their current form as they lack flexibility in their suggestions  such
requirements are  of course  not specific to medical domains and  for instance  might be
needed in an aircraft controller that provides suggestions to a pilot 
the important difference between decision support system and the classical rl problem
stems from the fact that in the decision support system  the acting agent is often a human
being  which of course has his her own decision process  therefore  the assumption that
the controller should only send one clear commanding signal to the acting agent is not
appropriate  it is more accurate to assume that some aspect of the decision making process
will be influenced by the user of the system 
this view of the decision process is particularly relevant in two different situations 
first  in many practical cases  we do not have an exact model of the system  instead  we
may have a noisy model built from a finite number of interactions with the environment 
this leads to a type of uncertainty that is usually referred to as extrinsic uncertainty  most
rl algorithms ignore this uncertainty and assume that the model is perfect  however if we
look closely  the performance of the optimal action based on an imperfect model might not
be statistically different from the next best action  bayesian approaches have looked at this
problem by providing confidence measure over the agents performance  mannor  simester 
sun    tsitsiklis         in cases where the acting agent is a human being  we can use these
confidence measures to provide the user with a complete set of actions  each of which might
be the optimal and for which we do not have enough evidence to differentiate  the user
can then use his her expertise to make the final decision  such methods should guarantee
that the suggestions provided by the system are statistically meaningful and plausible 
on the other hand  even when we do have complete knowledge of the system and we can
identify the optimal action  there might still be other actions which are roughly equal in
performance  at this point  the decision between these near optimal options could be left to
the acting agentnamely the human being that is using the decision support system  this
could have many advantages  ranging from better user experience  to increased robustness
and flexibly  among the near optimal solutions  the user can select based on further domain
knowledge  or any other preferences  that are not captured by the system  for instance  in
a medical diagnosis system that suggests treatments  providing the physician with several
options might be useful as the final decision could be made based on further knowledge of
the patients medical status  or preferences regarding side effects 
throughout this paper we address the latter issue through a combination of theoretical
and empirical investigations  we introduce the new concept of non deterministic policies
to capture a decision making process intended for decision support systems  such policies
 

finon deterministic policies in markovian decision processes

involve suggesting a set of actions  from which a non deterministic choice is made by the
user  we apply this formulation to solve the problem of finding near optimal policies to
provide flexible suggestions to the user 
in particular  we investigate how we can suggest several actions to the acting agent 
while providing performance guarantees with a worst case analysis  section   introduces the
necessary technical background material  section   defines the concept of non deterministic
policies and related concepts  section   addresses the problem of providing choice to the
acting agent while keeping near optimality guarantees on the performance of the worst case
scenario  we propose two algorithms to solve these problems and provide approximation
techniques to speed up the computation for larger domains 
methods introduced in this paper are general enough to apply to any decision support system in an observable markovian environment  our empirical investigations focus
primarily on sequential decision making problems in clinical domains  where the system
should provide suggestions on the best treatment options for patients  these decisions are
provided over a sequence of treatment phases  these systems are specifically interesting
because often times  different treatment options seem to provide only slightly different results  therefore  providing the physician with several suggestions would be beneficial in
improving the usability of the system and the performance of the final decision 

   definitions and notations
this section introduces the main notions behind sequential decision making and the mathematical formulations used in rl 
    markov decision processes
a markov decision process  mdp  is a model of system dynamics in sequential decision
problems that involves probabilistic uncertainty about future states of the system  bellman 
       mdps are used to model the interactions between an agent and an observable
markovian environment  the system is assumed to be in a state at any given time  the
agent observes the state and performs an action accordingly  the system then makes a
transition to the next state and the agent receives some reward 
formally  an mdp is defined by the   tuple  s  a  t  r    
 states  s is the set of states  the state usually captures the complete configuration
of the system  once the state of the system is known  the future of the system is
independent from all previous system transitions  this means that the state of the
system is a sufficient statistic of the history of the system 
 actions  a   s   a is the set of actions allowed in each state where a is the set
of all actions  a s  is the set of actions the agent can choose from  while interacting
with the system in state s 
 transition probabilities  t   s  a  s         defines the transition probabilities
of the system  this function specifies how likely it is to end up at any state  given the
current state and a specific action performed by the agent  transition probabilities
 

fimilani fard   pineau

are specified based on the markovian assumption  that is  if the state of the system
at time t is denoted by st and the action at that time is at   then we have 
pr st    at   st   at    at            a    s      p r st    at   st   

   

we focus on homogeneous processes in which the system dynamics are independent
of the time  thus the transition function is stationary with respect to time 
def

t  s  a  s      p r st     s   at   a  st   s  

   

 rewards  r   s  a  r         is the probabilistic reward model  depending on
the current state of the system and the action taken  the agent will receive a reward
drawn from this model  we focus on homogeneous processes in which  again  the
reward distribution does not change over time  if the reward at time t is denoted by
rt   then we have 
rt  r st   at   

   

depending on the domain  the reward could be deterministic or stochastic  we use
the general stochastic model throughout this paper  the mean of this distribution is
denoted by r s  a  
 discount factor           is the discount rate used to calculate the long term
return 
the agent starts in an initial state s   s  at each time step t  an action at  a st   is
taken by the agent  the system then makes a transition to st    t  st   at   and the agent
receives an immediate reward rt  r st   at   
the goal of the agent is to maximize the discounted sum of rewards over the planning
horizon h  could be infinite   this is usually referred to as the return  denoted by d  
d 

h
x

 t rt  

   

t  

in the finite horizon case  this sum is taken up to the horizon limit and the discount factor
can be set to    however  in the infinite horizon case the discount factor should be less
than   so that the return has finite value  the return on the process depends on both the
stochastic transitions and rewards  as well as the actions taken by the agent 
often times the transition structure of the mdp contains no loop with non zero probability  such a transition graph can be modeled by a directed acyclic graph  dag   this
class of mdps is interesting as it includes multi step decision making in finite horizons  such
as those found in medical domains 
    policy and value function
a policy is a way of defining the agents action selection with respect to the changes in the
environment  a  probabilistic  policy on an mdp is a mapping from the state space to a
distribution over the action space 
   s  a         
 

   

finon deterministic policies in markovian decision processes

a deterministic policy is a policy that defines a single action per state  that is   s  
a s   we will later introduce the notion of non deterministic policies on mdps to deal with
sets of actions 
the agent interacts with the environment and takes actions according to the policy  the
value function of the policy is defined to be the expectation of the return given that the
agent acts according to that policy 
 
 
x
def
v   s    e d  s     e
 t rt  s    s  at    st    
   
t  

using the linearity of the expectation  we can write the above expression in a recursive
form  known as the bellman equation  bellman        
 
 
x
x

 
  
v  s   
 s  a  r s  a   
t  s  a  s  v  s    
   
s  s

aa

the value function has been used as the primary measure of performance in much of
the rl literature  there are  however  some ideas that take the risk or the variance of the
return into account as a measure of optimality  heger        sato   kobayashi         the
more common criteria  though  is to assume that the agent is trying to find a policy that
maximizes the value function  such a policy is referred to as the optimal policy 
we can also define the value function over the state action pairs  this is usually referred
to as the q function  or the q value  of that pair  by definition 
 
 
x
def
   
q  s  a    e d  s  a     e
 t rt  s    s  a    a  t      at    st    
t  

that is  the q value is the expectation of the return  given that the agent starts in state
s  takes action a  and then follows policy   the q function also satisfies the bellman
equation 
x
x
q  s  a    r s  a   
t  s  a  s   
 s    a   q  s    a    
   
s  s

a  a

which can be rewritten as 
q  s  a    r s  a   

x

t  s  a  s   v   s    

    

s  s

the q function is often used to compare the optimality of different actions given a fixed
subsequent policy 
    planning algorithms and optimality
the optimal policy  denoted by     is defined to be the policy that maximizes the value
function at the initial state 
    argmax v   s    


 

    

fimilani fard   pineau

it has been shown  bellman        that for any mdp  there exists an optimal deterministic policy that is no worse than any other policy for that mdp  the value of the optimal
policy v  satisfies the bellman optimality equation 
 
 
x
v   s    max r s  a   
t  s  a  s   v   s     
    
aa

s  s

the deterministic optimal policy follows from this 
 
 
x

 
  
  s    argmax r s  a   
t  s  a  s  v  s    
aa

    

s  s

alternatively we can write these equations with the q function 
x
q  s  a    r s  a   
t  s  a  s   v   s    

    

s  s

thus v   s    maxa q  s  a  and    s    argmaxa q  s  a  
much of the literature in rl has focused on finding the optimal policy  there are many
methods developed for policy optimization  one way to find the optimal policy is to solve
the bellman optimality equation and then use eqn    to choose the actions  the bellman
optimality equation can be formulated as a simple linear program  bertsekas        
minv t v  subject to
p
v  s   r s  a     s  t  s  a  s   v  s    s  a 

    

where  represents an initial distribution over the states  the solution to the above problem is the optimal value function  notice that v is represented in matrix form in this
equation  it is known that linear programs can be solved in polynomial time  karmarkar 
       however  solving them might become impractical in large  or infinite  state spaces 
therefore often times methods based on dynamic programming are preferred to the linear
programming solution 

   non deterministic policies  definition and motivation
we begin this section by considering the problem of decision making in sequential decision
support systems  recently  mdps have emerged as useful frameworks for optimizing action
choices in the context of medical decision support systems  schaefer  bailey  shechter   
roberts        hauskrecht   fraser        magni  quaglini  marchetti    barosi       
ernst  stan  concalves    wehenkel         given an adequate mdp model  or data
source   many methods can be used to find a good action selection policy  this policy is
usually a deterministic or stochastic function  but policies of these types face a substantial
barrier in terms of gaining acceptance from the medical community  because they are highly
prescriptive and leave little room for the doctors input  these problems are  of course  not
specific to the medical domain and are present in any application where the actions are
executed by a human  in such cases  it may be preferable to provide several equivalently
 

finon deterministic policies in markovian decision processes

good action choices  so that the agent can pick among those according to his or her own
heuristics and preferences 
to address this problem  this work introduces the notion of a non deterministic policy 
which is a function mapping each state to a set of actions  from which the acting agent can
choose 
definition    a non deterministic policy  on an mdp  s  a  t  r    is a function
that maps each state s  s to a non empty set of actions denoted by  s   a s   the
agent can choose to do any action a   s  whenever the mdp is in state s 
definition    the size of a non deterministic
policy   denoted by     is the sum of the
p
cardinality of the action sets in        s   s   
in the following sections we discuss two scenarios in which non deterministic policies
can be useful  we show how they can be used to implement more robust decision support
systems with statistical guarantees of performance 
    providing choice to the acting agent
even in cases where we have complete knowledge of the dynamics of the planning problem
at hand  and when we can accurately calculate actions utilities  it might not be desirable to
provide the user with only the optimal choice of action at each time step  in some domains 
the difference between the utility of the top few actions may not be substantial  in medical
decision making  for instance  this difference may not be medically significant based on the
given state variables 
in such cases  it seems natural to let the user decide between the top few actions 
using his her own expertise in the domain  this results in a further injection of domain
knowledge in the decision making process  thus making it more robust and practical  such
decisions can be based on facts known to the user that are not incorporated in the automated
planning system  it can also be based on preferences that might change from case to case 
for instance  a doctor can get several recommendations as to how to treat a patient to
maximize the chance of remission  but then decide what medication to apply considering
also the patients medical record  preferences regarding side effects  or medical expenses 
this idea of providing choice to the user should be accompanied by reasonable guarantees on the performance of the final decision  regardless of the choice made by the user  a
notion of near optimality should be enforced to make sure the actions are never far from
the best possible option  such guarantees are enforced by providing a worst case analysis
on the decision process 
    handling model uncertainty
in many practical cases we do not have complete knowledge of the system at hand  instead 
we may get a set of trajectories collected from the system according to some specific policy 
in some cases  we may be given the chance to choose this policy  in on line and active rl  
and in other cases we may have access only to data from some fixed policy  in medical
trials  in particular  data is usually collected according to a randomized policy  fixed ahead
of time through consultation with the clinical researchers 
 

fimilani fard   pineau

given a set of sample trajectories  we can either build a model of the domain  in modelbased approaches  or directly estimate the utility of different actions  with model free approaches   however these models and estimates are not always accurate when we only
observe a finite amount of data  in many cases  the data may be too sparse and incomplete
to uniquely identify the best option  that is  the difference in the performance measure of
different actions is not statistically significant 
there are other cases where it might be useful to let the user decide on the final choice
between those actions for which we do not have enough evidence to differentiate  this
comes with the assumption that the user can identify the best choice among those that are
recommended  the task is therefore to provide the user with a small set of actions that will
almost surely include the optimal one 
in this paper we only focus on the problem of providing flexible policies with nearoptimal performance  using non deterministic policies for handling model uncertainty remains an interesting future work 

   near optimal non deterministic policies
often times  it is beneficial to provide the user of a decision support system with a set of
near optimal solutions  with mdps  this would be to suggest a set of near optimal actions
to the user and let the user make a decision among the proposed actions  the notion of
near optimality should therefore be on the set of all possible policies that are consistent
with the proposed actions  that is  no matter which action is chosen among the proposed
options at each state  the final performance should be close to that of the optimal policy 
such constraint suggests a worst case analysis of the decision making process  therefore  we
opt to guarantee the performance of any action selection consistent with a non deterministic
policy by putting the near optimality constraint on the worst case selection of actions by
the user 
definition    the  worst case  value of a state action pair  s  a  according to a nondeterministic policy  on an mdp m    s  a  t  r    is given by the recursive definition 

x

 

   
qm  s  a    r s  a    
t  s  a  s   min qm  s   a    
    
a   s   

s  s

which is the worst case expected return under the allowed set of actions 
definition    we define the  worst case  value of a state s according to a non  s   to be 
deterministic policy   denoted by vm
min q
m  s  a  

    

a s 

to calculate the value of a non deterministic policy  we construct an evaluation mdp 
m      s  a    r    t     where a     and r    r 
theorem    the negated value of the non deterministic policy  is equal to that of the
optimal policy on the evaluation mdp 

q
m  s  a    qm    s  a  

 

    

finon deterministic policies in markovian decision processes

proof  we show that if qm   satisfies the bellman optimality equation on m     then the
negated values satisfy eqn    on m  
x
qm    s  a    r   s  a    
t  s  a  s    max
qm    s    a   
    
 
 
a a

s  s

 qm    s  a    r   s  a   

x

t  s  a  s    max
qm    s    a   
 
 

    

t  s  a  s    min qm    s    a    

    

a a

s  s

 qm    s  a    r s  a    

x
s  s

a   s   


which is equivalent to eqn    for q
m  s  a    qm    s  a  

this means that policy evaluation for a non deterministic policy can be achieved by any
method that finds the optimal policy on an mdp 
definition    a non deterministic policy  is said to be augmented with state action pair
 s  a   denoted by         s  a   if it satisfies 
 
 s    
s     s
   s     
    
 s      a   s    s 
if a policy  can be achieved by a number of augmentations from a policy     we say
that  includes    
definition    a non deterministic policy  is said to be non augmentable according to
a constraint  if and only if  satisfies   and for any state action pair  s  a       s  a 
does not satisfy  
in this paper we will be working with constraints that have this particular property 
if a policy  does not satisfy   any policy that includes  does not satisfy   we will
refer to such constraints as being monotonic  one such constraint is  optimality  which is
discussed in the next section 
     optimal non deterministic policies
definition    a non deterministic policy  on an mdp m is said to be  optimal  with
          if we have   


vm
 s        vm
 s   s  s 

    

this can be thought of as a constraint  on the space of non deterministic policies  set
to ensure that the worst case expected return is within some range of the optimal value 
theorem    the  optimality constraint is monotonic 

   in some of the mdp literature   optimality is defined as an additive constraint  q
m  qm     kearns
  singh         the derivations will be analogous in that case  we chose the multiplicative constraint
as it has cleaner derivations 

 

fimilani fard   pineau

proof  suppose  is not  optimal  then for any augmentation         s  a   we have 

x
 
 
     
q
 s 
a 
 
r s 
a 
 

t
 s 
a 
s
 
min
q
 s
 
a
 
m
m
s  s

a     s   


x
 
     
t  s  a  s   min qm  s   a  
 r s  a    
s  s

a   s   

 q
m  s  a  
which implies 
 



vm
 s   vm
 s  

as  was not  optimal  this means that   will not be  optimal either as the value function
can only decrease with the policy augmentation 
more intuitively  it follows from the fact that adding more options cannot increase the
minimum utility as the former worst case choice is still available after the augmentation 
definition    a conservative  optimal non deterministic policy  on an mdp m is a
policy that is non augmentable according to the following constraint 
x



r s  a    
t  s  a  s        vm
 s          vm
 s   a   s  
    
s 

this constraint indicates that we only add those actions to the policy whose reward plus
      of the future optimal return is within the sub optimal margin  this ensures that the
non deterministic policy is  optimal by using the inequality 
x


q
t  s  a  s        vm
 s     
    
m  s  a   r s  a    
s 

instead of solving eqn    and using the inequality constraint in eqn     applying eqn   
guarantees that the non deterministic policy is  optimal while it may still be augmentable
according to eqn     hence the name conservative 
it can also be shown that the conservative policy is unique  this is because if there were
two different conservative policies  then the union of them would be conservative  which
violates the assumption that they are non augmentable according to eqn    
definition    a non augmentable  optimal non deterministic policy  on an mdp
m is a policy that is non augmentable according to the constraint in eqn    
this is a non deterministic policy for which adding more actions violates the nearoptimality constraint of the worst case performance  in a search for  optimal policies 
a non augmentable one has a locally maximal size  this means that although the policy
might not be the largest among the  optimal policies  we cannot add any more actions to
it without removing other actions  hence the locally maximal reference 
any non augmentable  optimal policy includes the conservative policy  this is because
we can always add the conservative policy to any policy and remain within the  bound 
  

finon deterministic policies in markovian decision processes

however  non augmentable  optimal policies are not necessarily unique  as they have only
locally maximal size 
in the remainder of this section  we focus on the problem of searching over the space
of non augmentable  optimal policies  such as to maximize some criteria  specifically  we
aim to find non deterministic policies that give the acting agent more options while staying
within an acceptable sub optimal margin 
we now present an example that clarifies the concepts introduced so far  to simplify the
presentation of the example  we assume deterministic transitions  however  the concepts
apply as well to any probabilistic mdp  figure   shows an example mdp  the labels on
the arcs show action names and the corresponding rewards are shown in the parentheses 
we assume      and          figure   shows the optimal policy of this mdp  the
conservative  optimal non deterministic policy of this mdp is shown in figure   

figure    example mdp

figure    optimal policy

figure    conservative  optimal policy

figure    two non augmentable  optimal policies

figure   includes two possible non augmentable  optimal policies  although both policies in figure   are  optimal  the union of these is not  optimal  this is due to the fact
that adding an option to one of the states removes the possibility of adding options to other
  

fimilani fard   pineau

states  which illustrates why local changes to the policy are not always appropriate when
searching in the space of  optimal policies 
    optimization criteria
we formalize the problem of finding an  optimal non deterministic policy in terms of an
optimization problem  there are several optimization criteria that can be formulated  while
still complying with the  optimality constraint 
 maximizing the size of the policy  according to this criterion  we seek nonaugmentable  optimal policies that have the biggest overall size  def     this provides
more options to the agent while still keeping the  optimal guarantees  the algorithms
proposed in later sections use this optimization criterion  notice that the solution to
this optimization problem is non augmentable according to the  optimal constraint 
because it maximizes the overall size of the policy 
as a variant of this  we can try to maximize the sum of the log of the size of the action
sets 
x
log   s   
    
ss

this enforces a more even distribution of choice on the action set  however  we will be
using the basic case of maximizing the overall size as it will be an easier optimization
problem 
 maximizing the margin  we can aim to maximize the margin of a non deterministic
policy  
max m    

    



where 

m      min
ss

min



q s  a   q s  a    
 

a s  a   s 
 

    

this optimization criterion is useful when one wants to find a clear separation between
the good and bad actions in each state 
 minimizing the uncertainty  if we learn the models from data we will have some
uncertainty about the optimal action in each state  we can use some variance estimation on the value function  mannor  simester  sun    tsitsiklis        along with
a z test to get some confidence level on our comparisons and find the probability of
having the wrong order when comparing actions according to their values  let q be
the value of the true model and q be our empirical estimate based on some dataset
d  we aim to minimize the uncertainty of a non deterministic policy  
min m    


    

where 

m      max
ss

max

a s  a   s 
 

  



 
p r q s  a    q s  a    d
 

    

finon deterministic policies in markovian decision processes

notice that the last two criteria can be defined both in the space of all  optimal policies 
or only the non augmentable ones 
in the following sections we provide algorithms to solve the first optimization problem
mentioned above  which aims to maximize the size of the policy  we focus on this criterion
as it seems most appropriate for medical decision support systems  where it is desirable
for the acceptability of the system to find policies that provide as much choice as possible
for the acting agent  developing algorithms to address the other two optimization criteria
remains an interesting open problem 
    maximal  optimal policy
the exact computational complexity of finding a maximal  optimal policy is not yet known 
the problem is certainly np  as one can find the value of the non deterministic policy in
polynomial time by solving the evaluation mdp with a linear program  we suspect that the
problem is np complete  but we have yet to find a reduction from a known np complete
problem 
in order to find the largest  optimal policy  we present two algorithms  we first present
a mixed integer program  mip  formulation of the problem  and then present a search algorithm that uses the monotonic property of the  optimal constraint  while the mip method
is useful as a general and theoretical formulation of the problem  the search algorithm has
potential for further extensions with heuristics 
      mixed integer programming solution
recall that we can formulate the problem of finding the optimal deterministic policy of an
mdp as a simple linear program  bertsekas        
minv t v  subject to
p
v  s   r s  a     s  t  s  a  s   v  s    s  a 

    

where  can be thought of as the initial distribution over states  the solution to the
above problem is the optimal value function  v     similarly  having computed v  using
eqn     the problem of searching for an optimal non deterministic policy according to the
size criterion can be rewritten as a mixed integer program  
maxv   t v    vmax  vmin  ets ea    subject to
v  s        v   s 
s
p
 s  a     
s
p a
 
 
v  s   r s  a     s  t  s  a  s  v  s     vmax      s  a   s  a 

    

here we are overloading the notation  to define a binary matrix representing the policy 
where  s  a  is   if a   s   and   otherwise  we define vmax   rmax        and
vmin   rmin         the es are column vectors of   with the appropriate dimensions 
the first set of constraints makes sure that we stay within  of the optimal return  the
   note that in this mip  unlike the standard lp for mdps  the choice of  can affect the solution in cases
where there is a tie in the size of  

  

fimilani fard   pineau

second set of constraints ensures that at least one action is selected per state  the third
set ensures that for those state action pairs that are chosen in any policy  the bellman
constraint holds  and otherwise  the constant vmax makes the constraint trivial  notice
that the solution to the above problem maximizes    and the result is non augmentable 
theorem    the solution to the mixed integer program of eqn    is non augmentable
according to  optimality constraint 
proof  first  notice that the solution is  optimal  due to the first set of constraints on the
 worst case  value function  to show that it is non augmentable  as a counter argument 
suppose that we could add a state action pair to the solution   while still staying in  suboptimal margin  by adding that pair  the objective function is increased by  vmax  vmin   
which is bigger than any possible decrease in the t v term  and thus the objective is
improved  which conflicts with  being the solution 
we can use any mip solver to solve the above problem  note however that we do not
make use of the monotonic nature of the constraints  a general purpose mip solver could
end up searching in the space of all the possible non deterministic policies  which would
require a running time exponential in the number of state action pairs  o   s  a      
      heuristic search
alternatively  we develop a heuristic search algorithm to find a maximal  optimal policy 
we can make use of the monotonic property of the  optimal policies to narrow down the
search  we start by computing the conservative policy  we then augment it until we arrive
at a non augmentable policy  we also make use of the fact that if a policy is not  optimal 
neither is any other policy that includes it  and thus we can cut the search tree at this
point 
table    heuristic search algorithm to find  optimal policies with maximum size
function getoptimal   startindex   
o  
for i  startindex to  s  a  do
 s  a   pi
if a 
   s    v      s  a         v  then
   getoptimal      s  a   i       
if g       g o   then
o   
end
end
end
return o
the algorithm presented in table   is a one sided recursive depth first search algorithm
that searches in the space of plausible non deterministic policies to maximize a function
g    here we assume that there is an ordering on the set of state action pairs  pi    
  

finon deterministic policies in markovian decision processes

  sj   ak     this ordering can be chosen according to some heuristic along with a mechanism
to cut down some parts of the search space  v  is the optimal value function and the function
v returns the value of the non deterministic policy that can be calculated by solving the
corresponding evaluation mdp 
we should make a call to the above function passing in the conservative policy m and
starting from the first state action pair  getoptimal m        
the asymptotic running time of the above algorithm is o   s  a  d  tm   tg     where d is
the maximum size of an  optimal policy minus the size of the conservative policy  tm is the
time to solve the original mdp  polynomial in the relevant parameters   and tg is the time
to calculate the function g  although the worst case running time is still exponential in the
number of state action pairs  the run time is much less when the search space is sufficiently
small  the  a  term is due to the fact that we check all possible augmentations for each
state  note that this algorithm searches in the space of all  optimal policies rather than
only the non augmentable ones  if we set the function g         then the algorithm will
return the biggest non augmentable  optimal policy 
this search can be further improved by using heuristics to order the state action pairs
and prune the search  one can also start the search from any other policy rather than the
conservative policy  this can be potentially useful if we have further constraints on the
problem 
      directed acyclic transition graphs
one way to narrow down the search is to only add the action that has the maximum value
for any state s  and ignore the rest of actions if adding the top action will result in values
out of the  optimality bound 
 
      

s  argmax q  s  a   
a s 
 

the modified algorithm will be as follows 
table    modified heuristic search algorithm with augmentation rule of eqn    
function getoptimal    
o  
for s  s where  s     a s  do
a  argmaxa s 
q  s  a 
 
if v      s  a         v  then
   getoptimal      s  a    
if g       g o   then
o   
end
end
end
return o

  

    

fimilani fard   pineau

the algorithm in table   leads to a running time of o  s d  tm   tg     however this does
not guarantee that we see all non augmentable policies  this is due to the fact that after
adding an action  the order of values might change  if the transition structure of the mdp
contains no loop with non zero probability  transition graph is directed acyclic  i e  dag  
then this heuristic will produce the optimal result while cutting down the search time 
theorem    for mdps with dag transition structure  the algorithm of table   will generate all non augmentable  optimal policies that would be generated with a full search 
proof  to prove this  first notice that we can sort the dag with a topological sort  therefore  we can arrange the states in levels  having each state only make transitions to states
at a future level  it is easy to see that adding actions to a state for a non deterministic
policy can only change the worst case value of past levels  it will not have any effect on the
q values at the current level or any future level 
now given any non augmentable  optimal policy generated with a full search  there is
a sequence of augmentations that generated that policy  any permutation of that sequence
would create the same policy and all the intermediate polices are  optimal  now we rearrange that sequence such that we add actions in the reverse order of the level  by the
point mentioned above  the q value of actions at the point where they are being added will
not change until the target policy is realized  therefore all the actions with q values above
the minimum value must be in the policy  or otherwise we can add them  which conflicts
with the target policy being non augmentable  since all the actions above a certain q value
must be added  we can add them in order  therefore the target policy can be realized with
the rule of eqn    
when the transition structure is not a dag  one might do a partial evaluation of the
augmented policy to approximate the value after adding the actions  possibly by doing a few
backups rather than using the original q values  this offers the possibility of trading off
computation time for better solutions 

   empirical results
to evaluate our framework and proposed algorithms  we first test both the mip and search
formulations on mdps created randomly  and then test the search algorithm on a real world
treatment design scenario  finally  we conduct an experiment on a computer aided web
navigation task with human subjects to assess the usefulness of non deterministic policies
in assisting human decision making 
    random mdps
in the first experiment  we aim to study how non deterministic policies change with the
value of  and how the two algorithms compare in terms of running time  to begin  we
generated random mdps with   states and   actions  the transitions are deterministic
 chosen uniformly at random  and the rewards are random values between   and    except
for one of the states with reward    for one of its actions   was set to       the mip
method was implemented with matlab and cplex 
  

finon deterministic policies in markovian decision processes

  

       

       

       

figure    mip solution for different values of                          the labels on the
edges are action indices  followed by the corresponding immediate rewards 
figure   shows the solution to the mip defined in eqn    for a particular randomly
generated mdp  we see that the size of the non deterministic policy increases as the performance threshold is relaxed  we can see that even with small values for  there are several
actions included in the policy for each state  this is of course a result of the q values being
close to each other  such property is typical in many medical scenarios where different
treatments provide only slightly different results 
to compare the running time of the mip solver and the search algorithm  we constructed
random mdps as described above with more state action pairs  figure   shows the running
time averaged over    different random mdps with   states  assuming          which
allows several solutions   as expected  both algorithms have a running time exponential
in the number of state action pairs  note the exponential scale on the time axis   the
running time of the search algorithm has a bigger constant factor  possibly due to our naive
implementation   but has a smaller exponent base  which results in a faster asymptotic
running time  even with the exponential running time  one can still use the search algorithm
to solve problems with a few hundred state action pairs  this is more than sufficient for
many practical domains  including real world medical decision scenarios as shown in the
next section 
to observe the effect of the choice of  on the running time our algorithms  we fix the
size of random mdps to have   states and   actions at each state  and then change the
  

fimilani fard   pineau

figure    running time of mip and the search algorithm as a function of the number of
state action pairs with         
value of  and measure the running time of the algorithms over     trials  figure   shows
the average running time of both algorithms with different values for   as expected  the
search algorithm will go deeper in the search tree as the optimality threshold is relaxed and
its running time will thus increase  the running time of the mip method  on the other
hand  remains relativity constant as it exhaustively searches in the space of all possible
non deterministic policies  these results are representative of the relative behaviour of the
two approaches over a range of problems 

figure    running time of mip and the search algorithm as a function of   with   states
and   actions  many of the actions are included in the policy with         

    medical decision making
to demonstrate how non deterministic policies can be used and presented in a medical
domain  we tested the full search algorithm on an mdp constructed for a medical decisionmaking task involving real patient data  the data was collected as part of a large       
patients  multi step randomized clinical trial  designed to investigate the comparative effectiveness of different treatments provided sequentially for patients suffering from depression
 fava et al          the goal is to find a treatment plan that maximizes the chance of
  

finon deterministic policies in markovian decision processes

remission  the dataset includes a large number of measured outcomes  for the current
experiment  we focus on a numerical score called the quick inventory of depressive symptomatology  qids   which was used in the study to assess levels of depression  including
when patients achieved remission   for the purposes of our experiment  we discretize the
qids scores  which range from   to     uniformly into quartiles  and assume that this 
along with the treatment step  up to   steps were allowed   completely describe the patients state  note that the underlying transition graph can be treated as a dag  as the
study is limited to four steps of treatment and action choices change between these steps 
there are    actions  treatments  in total  a reward of   is given if the patient achieves
remission  at any step  and a reward of   is given otherwise  the transition and reward
models were estimated empirically from the medical database using a frequentist approach 
table    policy and running time of the full search algorithm on the medical problem 
       

        

       

  

     

    

   

   

ct
ser
bup
cit bus

ct
ser

ct

ct

   qids     

cit bup
cit ct

cit bup
cit ct

cit bup

cit bup

ven
cit bus
ct

ven
cit bus

ven

ven

    qids     

    qids    

ct
cit ct

ct
cit ct

ct
cit ct

ct

time  seconds 
    qids    

table   shows the non deterministic policy obtained for each state during the second
step of the trial  each acronym refers to a specific treatment   this is computed using the
search algorithm  assuming different values of   although this problem is not tractable with
the mip formulation      state action pairs   a full search in the space of  optimal policies
is still possible  table   also shows the running time of the algorithm  which as expected 
increases as we relax the threshold   here  we did not use any heuristics  however  as the
underlying transition graph is a dag  we could use the heuristic discussed in the previous
section  eqn     to get the same policies even faster 
an interesting question is how to set  a priori  in practice  a doctor may use the
full table as a guideline  using smaller values of  when he she wants to rely more on the
decision support system  and larger values when relying more on his her own assessments 
we believe this particular presentation of non deterministic policies could be used and
accepted by clinicians  as it is not excessively prescriptive and keeps the physician and the
patient in the decision cycle  this is in contrast with the traditional notion of policies in
reinforcement learning  which often leaves no place for the physicians intervention 
  

fimilani fard   pineau

    human subject interaction
finally  we conduct an experiment to assess the usefulness of non deterministic policies with
human subjects  ideally  we would like to conduct such experiments in medical settings and
with physicians  but such studies are costly and difficult to conduct given that they require
participation of many medical professionals  we therefore study non deterministic policies
in an easier domain by constructing a web based game that can be played by any computer
and human  either jointly or separately  
the game is defined as follows  a user is given a target word and is asked to navigate
around the pages of wikipedia and visit pages that contain that target word  the user can
click on any word in a page  the system then uses a google search on the wiki website
with the clicked word and a keyword in the current page  the choice of this keyword is
discussed later   it then randomly chooses one of the top eight search results and moves
to that page  this process mimics the hyperlink structure of the web  extending over
the hyperlink structure of the wiki to make target words more easily reachable   the
user is given ten attempts and is asked to reach as many pages with the target word as
possible  a similar game was used in another work to infer the semantic distances between
concepts  west  pineau    precup         our game  however  is designed in such a way
that a computer model can provide results similar to a human player and thus enable us to
assess the effectiveness of computer aided decisions with non deterministic policies 
we construct this task on the cd version of wikipedia  schools wikipedia         which
is a structured and manageable version of wikipedia intended for use in schools  to test our
approach we also need to build an mdp model of the task  this is done using empirical data
as follows  first  we use latent dirichlet allocation  lda  using gibbs sampling  griffiths
  steyvers        to divide the pages of wikipedia into    topics  each topic corresponds
to a state in the mdp  the lda algorithm identifies each topic by a set of keywords that
occur more often in pages with that topic  we define each of these sets of keywords to be an
action     actions totals  each corresponding to    keywords   we then randomly navigate
around the wiki using the protocol described above  with a computer player that only
clicks lda keywords  and collect         transitions  we use the observed data to build
the transition and reward model of our mdp  the reward is   for each hit and   otherwise  
the specific choices for the lda parameter and the number of states and actions in the
mdp are made in such a way that the best policy provided by the model has comparable
performance with that of the human player 
using amazon mechanical turk  mturk         we consider three experimental conditions with this task  in one experiment  given a target  the computer chooses a word
 uniformly at random  from the set of keywords  the action  that comes from the optimal
policy on the mdp model  in another experiment  human subjects choose and click the
word themselves without any help  finally  we test the domain with human users while the
computer highlights  as hints  the words that come from the non deterministic policy with
        we record the time taken during the process and number of times the target word
is observed  number of hits   table   summarizes the average outcomes of this experiment
for four of the target words  we used seven target words  but could not collect enough data
for all of them   we also include the p value for the t test comparing the results for human
agents with and without the hints  the computer score is averaged over      runs 
  

finon deterministic policies in markovian decision processes

table    comparison of different agents in the web navigation task  the t test is between
the number of hits for a human player that uses hints and one that does not 
target

computer

human

human with hint

t test

marriage

     hits

     hits
    seconds
    subjects 

     hits
   seconds
    subjects 

     

     hits
   seconds
    subjects 

     hits
   seconds
    subjects 

     

     hits
   seconds
    subjects 

     hits
   seconds
    subjects 

     

     hits
   seconds
    subjects 

     hits
   seconds
     subjects 

    

      runs 
military

     hits
      runs 

book

     hits
      runs 

animal

     hits
      runs 

for the first three target words  where the performance of the computer agent is close to
a human user  we observe that providing hints to the user results in a statistically significant
increase in the number of hits  in fact we see that the computer aided human outperforms
both the computer and human agents  this shows that non deterministic policies can
provide the means to inject human domain knowledge to computer models in such a way
that the final outcome is superior to the decision making solely performed by one party 
for the last word  the computer model is working poorly  judging by its low hit rate  thus 
it is not surprising to see that the hints do not provide much help to the human agent
in this case  as seen by the non significant p value   we also observe a general speedup
 for three of the targets  in the time taken by the agent to choose and click the words 
which further shows the usefulness of non deterministic policies in accelerating the human
subjects decision making process 

   discussion
this paper introduces the new concept of non deterministic policies and their potential use
in decision support systems based on markovian processes  in this context  we investigate
how the assumption that a decision making system should return a single optimal action
can be relaxed  to instead return a set of near optimal actions 
non deterministic policies are inherently different from stochastic policies  stochastic
policies assume a randomized action selection strategy with some specific probabilities 
whereas non deterministic policies do not impose such constraint  we can thus use bestcase and worst case analysis with non deterministic policies to highlight different scenarios
with the human user 
  

fimilani fard   pineau

the benefits of non deterministic policies for sequential decision making are two fold 
first  when we have several actions for which the difference in performance are negligible 
we can report all those actions as near optimal options  for instance  in a medical setting 
the difference between the outcome of two treatment options might not be medically
significant  in that case  it may be beneficial to provide all the near optimal options 
this makes the system more robust and user friendly  in the medical decision making
process  for instance  the physician can make the final decision among the near optimal
options based on side effects burden  patients preferences  expense  or any other criteria
that is not captured by the model used in the decision support system  the key constraint 
however  is to make sure that regardless of the final choice of actions  the performance of
the executed policy is always bounded near the optimal  in our framework  this property
is maintained by an  optimality guarantee on the worst case scenario 
another potential use of the non deterministic action sets in markovian decision processes is to capture uncertainties in the optimality of actions  often times  the amount of
data from which models are constructed is not sufficient to clearly identify a single optimal
action  if we are forced to chose only one action as the optimal one  we might have a high
chance of making the wrong decision  however  if we are given the chance to provide a set
of possibly optimal actions  then we can ensure we include all the promising options while
cutting off the obviously bad ones  in this setting  the task is to trim the action set as much
as possible while providing the guarantee that the optimal action is still among the top few
possible options 
to solve the first problem  this paper introduces two algorithms to find flexible nearoptimal policies  first we derive an exact solution with a mip formulation to find a maximal
 optimal policy  the mip solution is  however  computationally expensive and does not
scale to large domains  we then describe a search algorithm to solve the same problem with
less computational cost  this algorithm is fast enough to be applied to real world medical
domains  we also show how to use heuristics in the search algorithm to find the solution for
dag structures even faster  the heuristic search can also provide approximate solutions in
the general case 
another way to scale the problem to larger domains is to approximate the solution to
the mip program by relaxing some of the constraints  one can relax the constraints to
allow non integral solutions and penalize the objective for values away from   and    the
study of such approximation methods remains an interesting direction of future work 
the idea of non deterministic policies introduces a wide range of new problems and
research topics  in section    we discuss the idea of near optimal non deterministic policies
and address the problem of finding the one with the largest action set  as mentioned  there
are other optimization criteria that might be useful with decision support systems  these
include maximizing the decision margin  the margin between the worst selected action and
the best one not selected   or alternatively minimizing the uncertainty of a wrong selection 
formalizing these problems into a mip formulation  or incorporating them into a heuristic
search  might prove to be useful 
as evidenced by our human interaction experiments  non deterministic policies can substantially improve the outcome of planning and decision making tasks in which a human
user is assisted by a robust computer generated plan  allowing several suggestions at each
step provides an effective way of incorporating domain knowledge from the human side of
  

finon deterministic policies in markovian decision processes

the decision making process  in medical domains where the physicians domain knowledge
is often hard to capture in a computer model  a collaborative model of decision making such
as non deterministic policies could offer a powerful framework for selecting effective  and
clinically acceptable  treatment strategies 

acknowledgments
the authors wish to thank a  john rush  duke nus graduate medical school   susan
a  murphy  university of michigan   and doina precup  mcgill university  for helpful
discussions regarding this work  funding was provided by the national institutes of health
 grant r   da        and the nserc discovery grant program 

references
bellman  r          dynamic programming  princeton university press 
bertsekas  d          dynamic programming and optimal control  vol    athena scientific 
ernst  d   stan  g  b   concalves  j     wehenkel  l          clinical data based optimal
sti strategies for hiv  a reinforcement learning approach  in proceedings of the
fifteenth machine learning conference of belgium and the netherlands  benelearn  
pp       
fava  m   rush  a   trivedi  m   nierenberg  a   thase  m   sackeim  h   quitkin  f   wisniewski  s   lavori  p   rosenbaum  j     kupfer  d          background and rationale
for the sequenced treatment alternatives to relieve depression  star  d  study  psychiatric clinics of north america                 
griffiths  t  l     steyvers  m          finding scientific topics  proceedings of the national
academy of sciences       suppl               
hauskrecht  m     fraser  h          planning treatment of ischemic heart disease with
partially observable markov decision processes  artificial intelligence in medicine 
               
heger  m          consideration of risk in reinforcement learning  in proceedings of the
eleventh international conference on machine learning  icml   pp         
karmarkar  n          a new polynomial time algorithm for linear programming  combinatorica                
kearns  m     singh  s          near optimal reinforcement learning in polynomial time 
machine learning     
magni  p   quaglini  s   marchetti  m     barosi  g          deciding when to intervene 
a markov decision process approach  international journal of medical informatics 
               
mannor  s   simester  d   sun  p     tsitsiklis  j  n          bias and variance in value
function estimation  in proceedings of the twenty first international conference on
machine learning  icml   pp         
  

fimilani fard   pineau

mannor  s   simester  d   sun  p     tsitsiklis  j  n          bias and variance approximation in value function estimates  management science                 
mturk         amazon mechanical turk  in http   www mturk com  
murphy  s  a          an experimental design for the development of adaptive treatment
strategies  statistics in medicine                    
pineau  j   bellemare  m  g   rush  a  j   ghizaru  a     murphy  s  a          constructing evidence based treatment strategies using methods from computer science  drug
and alcohol dependence      supplement     s    s   
russell  s  j     norvig  p          artificial intelligence  a modern approach  second
edition   prentice hall 
sato  m     kobayashi  s          variance penalized reinforcement learning for risk averse
asset allocation  in proceedings of the second international conference on intelligent
data engineering and automated learning  data mining  financial engineering  and
intelligent agents  pp          springer verlag 
schaefer  a   bailey  m   shechter  s     roberts  m          handbook of operations
research   management science applications in health care  chap  medical decisions
using markov decision processes  kluwer academic publishers 
schools wikipedia        
wikipedia org  

       wikipedia selection for schools 

in http   schools 

sutton  r  s     barto  a  g          reinforcement learning  an introduction  adaptive
computation and machine learning   the mit press 
thapa  d   jung  i     wang  g          agent based decision support system using reinforcement learning under emergency circumstances  lecture notes in computer
science            
west  r   pineau  j     precup  d          wikispeedia  an online game for inferring
semantic distances between concepts  in proceedings of the twenty first international
jont conference on artifical intelligence  ijcai   pp            san francisco  ca 
usa  morgan kaufmann publishers inc 

  

fi