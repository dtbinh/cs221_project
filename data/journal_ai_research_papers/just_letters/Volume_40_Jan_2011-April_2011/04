journal artificial intelligence research                  

submitted        published      

scaling heuristic planning relational decision trees
tomas de la rosa
sergio jimenez
raquel fuentetaja
daniel borrajo

trosa   inf  uc     es
sjimenez   inf  uc     es
rfuentet   inf  uc     es
dborrajo   ia   uc     es

departamento de informatica
universidad carlos iii de madrid
av  universidad     leganes  madrid  spain

abstract
current evaluation functions heuristic planning expensive compute  numerous
planning problems functions provide good guidance solution  worth
expense  however  evaluation functions misguiding planning problems large
enough  lots node evaluations must computed  severely limits scalability heuristic planners  paper  present novel solution reducing node evaluations heuristic
planning based machine learning  particularly  define task learning search control
heuristic planning relational classification task  use off the shelf relational classification tool address learning task  relational classification task captures preferred action
select different planning contexts specific planning domain  planning contexts
defined set helpful actions current state  goals remaining achieved 
static predicates planning task  paper shows two methods guiding search
heuristic planner learned classifiers  first one consists using resulting classifier action policy  second one consists applying classifier generate lookahead
states within best first search algorithm  experiments variety domains reveal
heuristic planner using learned classifiers solves larger problems state of the art planners 

   introduction
last years  state space heuristic search planning achieved significant results
become one popular paradigms automated planning  however  heuristic search
planners suffer strong scalability limitations  even well studied domains blocksworld
become challenging planners number blocks relatively large  usually  statespace heuristic search planners based action grounding  makes state space
explored large number objects and or action parameters large enough  moreover 
domain independent heuristics expensive compute  domains heuristics
misleading  heuristic planners spend planning time computing useless node
evaluations  even best current domain independent heuristic functions literature 
forward chaining heuristic planners currently visit many nodes  takes considerable
time  especially due time required compute heuristic functions 
problems entail strong limitations application heuristic planners real problems  instance  logistics applications need handle hundreds objects together hundreds
vehicles locations  florez  garca  torralba  linares  garca olaya    borrajo         current heuristic search planners exhaust computational resources solving problem
real logistics application 
c
    
ai access foundation  rights reserved 

fid e la rosa   j imenez   f uentetaja   b orrajo

classic approach dealing planning scalability issues assisting search engines
planners domain specific control knowledge  dck   examples planning systems
benefit knowledge tlp lan  bacchus   kabanza         talp lanner  doherty
  kvarnstrom        shop   nau  au  ilghami  kuter  murdock  wu    yaman        
nevertheless  hand coding dck complex task implies expertise both  planning domain search algorithm planning system  recent years
renewed interest using machine learning  ml  automatically extract dck  zimmerman
kambhampati        made comprehensive survey ml defining dck  shown first
learning planning competition held       learning track   renewed interest specially
targeted heuristic planners 
paper presents approach learning dck planning building domain dependent
relational decision trees examples good quality solutions forward chaining heuristic
planner  decision trees built off the shelf relational classification tool capture best action take possible decision planner given domain 
resulting decision trees used either policy solve planning problems directly
generate lookahead states within best first search  bfs  algorithm  techniques allow
planner avoid state evaluations  helps objective improving scalability 
approach implemented system called roller  work improvement
previous one  de la rosa  jimenez    borrajo         alternatively  roller version
repairing relaxed plans  de la rosa  jimenez  garca duran  fernandez  garca olaya    borrajo 
      competed learning track  th international planning competition  ipc  held
      roller improvements presented article mainly result lessons learned
competition  discussed later 
paper organized follows  section   introduces issues need considered
designing learning system heuristic planning  help us clarify decisions made development approach  section   describes roller system
detail  section   presents experimental results obtained variety benchmarks  section  
discusses improvements roller system compared previous version system 
section   revises related work learning dck heuristic planning  finally  last section
discusses conclusions future work 

   common issues learning domain specific control knowledge
designing ml process automatic acquisition dck  one must consider
common issues  among others 
   representation learned knowledge  predicate logic common language
represent planning dck planning tasks usually defined language  however  representation languages used aiming make learning dck
effective  instance  languages describing object classes concept
language  martin   geffner        taxonomic syntax  mcallester   givan       
shown provide useful learning bias different domains 
another representation issue selection feature space  i e   set instance
features used representing learned knowledge training system    feature
space able capture key knowledge domain  traditionally  feature
   

fis caling h euristic p lanning r elational ecision rees

space consisted predicates describing current state goals planning
task  feature space enriched extra predicates  called metapredicates 
capture extra useful information planning context applicable operators
pending goals  veloso  carbonell  perez  borrajo  fink    blythe         recently  works
learning dck heuristic planners define metapredicates capture information
planning context heuristic planner  including example  predicates capture
actions relaxed plan given state  yoon  fern    givan        
   learning algorithms  inductive logic programming  ilp   muggleton   de raedt 
      deals development inductive techniques learn given target concept
examples described predicate logic  planning tasks normally represented
predicate logic  ilp algorithms quite suitable dck learning  moreover  recent
years  ilp broadened scope cover whole spectrum ml tasks regression  clustering association analysis  extending classical propositional ml algorithms
relational framework  consequently  ilp algorithms used heuristic planners capture dck different forms decision rules select actions different
planning context regression rules obtain better node evaluations  yoon et al         
   generation training examples  success ml algorithms depends directly
quality training examples used  learning planning dck  examples
extracted experience collected solving training problems 
representative different tasks across domain  therefore  quality training
examples depend variety problems used training quality
solutions problems  traditionally  training problems obtained random
generators provided parameters tune problems difficulty  way  one
find  domain  kind problems makes learning algorithm generalize
useful dck 
   use learned dck  decisions made three issues affect quality
learned dck  representation schemes may expressive enough capture effective dck given domain  learning algorithm may able acquire useful
dck within reasonable time memory requirements  set training problems may
lack significant examples key knowledge  situations  direct use
learned dck improve scalability planner  could even decrease performance  effective way dealing problem heuristic planners integrating
learned dck within robust strategies best first search  yoon et al        
combining domain independent heuristic functions  roger   helmert        

   roller system
section describes general scheme learning dck instantiated roller
system  first  describes dck representation followed roller  second  explains
learning algorithm used roller  third  depicts roller collects good quality training
examples finally  shows different approaches scaling heuristic planning algorithms
learned dck 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

    representation learned knowledge  helpful contexts heuristic planning
present approach following notation specified planning domain definition language  pddl  typed strips tasks  accordingly  definition planning domain comprises definition of 
hierarchy types 
set typed constants  cd   representing objects present tasks domain 
set empty 
set predicate symbols  p  one corresponding arity type
arguments 
set operators o  whose arguments typed variables 
variables declared directly defining operator argument  local
operator definition  call po set atomic formulas generated using
defined predicates p  variables defined arguments operator o  general
constants cd   then  operator defined three sets  pre o  po   operator
preconditions  add o  po   positive effects  del o  po   negative effects
operator 
planning task domain tuple   c   s    g   c set typed
constants representing objects particular task  s  set ground atomic
formulas describing initial state g set ground atomic formulas describing goals 
given total set constants c   c cd   task defines finite state space finite set
instantiated operators o  state set ground atomic formulas representing
facts true s  states described following closed world assumption  instantiated
operator action operator variable replaced constant c
type  thus  set actions generated using set constants c
set operators o  definition  solving planning task implies finding plan
sequence actions  a               ai transforms initial state state
goals achieved 
planning contexts defined roller rely concepts relaxed plan heuristic
helpful actions  introduced planner  hoffmann   nebel         relaxed plan
heuristic returns integer evaluated node  number actions solution
relaxed planning task   node    simplification original task
deletes actions ignored  idea delete relaxation computing heuristics planning
first introduced mcdermott        bonet  loerincs geffner        
relaxed plan extracted relaxed planning graph  sequence facts
actions layers  f    a              ft    first fact layer contains facts initial state 
action layer contains set applicable actions given previous fact layer  fact
layer contains set positive effects actions appearing previous layers 
process finishes goals fact layer  two consecutive facts layers
facts  last case  relaxed problems solution relaxed plan heuristic
returns infinity 
   

fis caling h euristic p lanning r elational ecision rees

relaxed planning graph built  solution extracted backwards process 
goal appearing first time fact layer assigned set goals layer  gi   then 
last set goals  gt   second set goals  g    goal goals set 
action selected generates goal whose layer index minimal  afterwards 
precondition action  i e  subgoal  included goals set corresponding first
layer fact appears  process finished  set selected actions comprises
relaxed plan 
according extraction process  planner marks helpful actions set actions
first layer a  relaxed planning graph achieve subgoals next
fact layer  i e  goals set g    words  helpful actions applicable actions
generate facts top level goals problems required action relaxed plan 
formally  set helpful actions given state defined as 
helpful  s     a a    add a  g      
planner uses helpful actions search pruning technique  considered candidates selected search  given state generates
particular set helpful actions  claim helpful actions  together remaining goals static literals planning task  encode helpful context related state 
helpful actions remaining target goals relate actions likely applied
goals need achieved  relations arise helpful actions target
goals often share arguments  problem objects   additionally  static predicates express
facts characterize objects planning task  identifying objects relevant since
may shared arguments helpful actions and or target goals 
definition   helpful context state defined
h s     helpful  s   target s   static s  
target s  g describes set goals achieved state s  target s    g
static s  set literals always hold planning task  defined
initial state present every state given changed action  thus 
static s     p    a   p add a  p del a   
helpful context alternative representation tuple  state  goals  applied action  
traditionally used learning dck planning  helpful contexts present advantages
improving scalability heuristic planners 
domains  set helpful actions contains actions likely applied
focusing reasoning shown good strategy 
set helpful actions normally smaller set non static literals state
 i e   static s    thus  process matching learned dck within search obtains
benefits using compact representation 
number helpful actions normally decreases search fewer goals left 
therefore  matching process become faster search advancing towards
goals 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

    learning algorithm  learning generalized policies relational decision trees
roller implements two step learning process building dck collection examples
different helpful contexts 

   learning operator classifier  roller builds classifier choose best operator
different helpful contexts 
   learning binding classifiers  operator domain  roller builds classifier
choose best binding  instantiation operator  different helpful contexts 
learning process split two steps build dck off the shelf learning
tools  planning action may different number arguments arguments different types  e g  actions switch on instrument satellite  turn to satellite 
direction direction  satellite domain  hinders definition target
classes  two step decision process clearer decision making point view 
helps users understand generated dck better focusing either decision operator apply bindings use given selected operator  learning algorithm
set learning examples two learning steps  figure   shows overview
learning process roller system 

roller learner
training
problems

   operator classifier
example
generator

pddl
domain

op  examples

relational
classification

bind  examples

tool

   binding classifiers

   

language bias

figure    overview roller learning process 

      l earning r elational ecision ress
classic approach assist decision making consists gathering significant set previous decisions building decision tree generalizes them  leaves resulting tree contain
classes  decisions make   internal nodes contain conditions lead decisions  common way build trees following top down induction decision
trees  tdidt  algorithm  quinlan         algorithm builds tree repeatedly splitting
set training examples according conditions minimize entropy examples  traditionally  training examples described attribute value representation  therefore 
conditions decision trees represent tests value given attribute examples 
nevertheless  attribute value approach suitable representing decisions want
keep predicate logic representation  better approach represent decisions relationally 
instance  given action chosen reach certain goals given context share arguments  recently  new algorithms building relational decision trees examples described
   

fis caling h euristic p lanning r elational ecision rees

predicate logic facts developed  new relational learning algorithms similar
propositional ones  except     condition nodes tree refer attribute values 
logic queries relational facts holding training examples      logic queries
share variables condition nodes placed decision tree  learning algorithm
greedy search process  since space potential relational decision trees usually huge 
search normally biased according specification syntactic restrictions called language bias 
specification contains target concept  predicates appear condition nodes
trees learning specific knowledge type information  input output
variables predicates 
paper use tool tilde  blockeel   de raedt        learning operator
binding classifiers  tool implements relational version tdidt algorithm  although
off the shelf tool learning relational classifiers could used  pro gol  muggleton        ribl  emde   wettschereck         different learning
algorithms would provide different results  since explore classifiers space differently 
study pros cons different algorithms beyond scope paper 
comprehensive explanation current relational learning approaches please refer work de
raedt        
      l earning perator c lassifier
inputs learning operator classifier are 
training examples  examples represented prolog like syntax consist
operator selected  the class  together helpful context  the background knowledge
terms relational learning  selected  particular  example contains 
class  use predicate arity   selected encode operator chosen
context  predicate target concept learning step  first argument holds
example identifier links rest example predicates  second argument
problem identifier  links static predicates shared examples coming
planning problem  third argument example class  i e   name
selected operator helpful context 
helpful predicates  predicates express helpful actions contained
helpful context  predicate symbol predicates helpful ai ai
name instantiated action  arguments example problem
identifier together parameters action ai   instantiated action 
parameters constants 
target goal predicates  represent predicates appear goals
hold current state  predicates form target goal gi
gi domain predicates  predicate contains example problem
identifiers 
static predicates  represent static predicates given problem  predicates shared training examples belong planning problem 
form static fact domain predicates
appear effects domain action  arguments problem identifier corresponding arguments domain predicate 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

figure   shows one learning example id tr   e  consisting selection operator switch on associated helpful context  example used building
operator classifier satellite domain 
  example tr   e  problem tr  
selected tr   e  tr   switch on  
helpful turn to tr   e  tr   satellite  groundstation  star   
helpful turn to tr   e  tr   satellite  phenomenon  star   
helpful turn to tr   e  tr   satellite  phenomenon  star   
helpful turn to tr   e  tr   satellite  phenomenon  star   
helpful switch on tr   e  tr   instrument  satellite   
target goal image tr   e  tr   phenomenon  infrared   
target goal image tr   e  tr   phenomenon  infrared   
target goal image tr   e  tr   phenomenon  spectrograph   
  static predicates problem
static fact calibration target tr   instrument  groundstation   
static fact supports tr   instrument  spectrograph   
static fact supports tr   instrument  infrared   
static fact board tr   instrument  satellite   

figure    knowledge base corresponding example satellite domain  example
id tr   e   links example predicates  obtained solving
training problem tr   links rest examples problem 
selected operator helpful context switch on  corresponds one
helpful actions encoded helpful predicates example 

language bias  bias specifies constraints arguments predicates
training examples  assume domain specific constraint  given learning
technique domain independent  so  bias contains restrictions argument types
restrictions ensure identifier variables added new variables
classifier generation  bias automatically extracted pddl domain definitions
consists declaration predicates used learning example argument
types  figure   shows language bias specified learning operator classifier
satellite domain 
resulting relational decision tree represents set disjoint rules action selection
used provide advice planner  internal nodes tree contain set conditions related helpful context advice provided  leaf nodes contain
corresponding advice  case  operator select number examples covered
rule  operator select one selected often training
examples covered rule  operator classifiers learned roller advise nonhelpful actions  given state  non helpful actions subset applicable actions state
considered helpful actions  certainly  actions part helpful contexts defined  however  learned operator classifiers indicate name operator select
regardless whether helpful not  figure   shows operator tree learned satellite
   

fis caling h euristic p lanning r elational ecision rees

       target concept    predict selected  idexample  idproblem  operator   
type selected idex idprob class   
classes  turn to switch on switch off calibrate take image   
       helpful context      predicates helpful actions
rmode helpful turn to  idexample  idproblem   s    d    d    
type helpful turn to idex idprob satellite direction direction   
rmode helpful switch on  idexample  idproblem   i    s    
type helpful switch on idex idprob instrument satellite   
rmode helpful switch off  idexample  idproblem   i    s    
type helpful switch off idex idprob instrument satellite   
rmode helpful calibrate  idexample  idproblem   s    i    d    
type helpful calibrate idex idprob satellite instrument direction   
rmode helpful take image  idexample  idproblem   s    d    i    m    
type helpful take image idex idprob satellite direction instrument mode   
  predicates target goals
rmode target goal pointing  idexample  idproblem   s    d    
type target goal pointing idex idprob satellite direction   
rmode target goal image  idexample  idproblem   d    m    
type target goal image idex idprob direction mode   
  predicates static facts
rmode static fact board  idproblem   i    s    
type static fact board idprob instrument satellite   
rmode static fact supports  idproblem   i    m    
type static fact supports idprob instrument mode   
rmode static fact calibration target  idproblem   i    d    
type static fact calibration target idprob instrument direction   

figure    language bias learning operator classifier satellite domain  automatically generated pddl definition  rmode predicates indicate
used tree  type predicates indicate types particular rmode 

domain  learned decision trees branch denoted symbols      yes no  
yes indicates next node positive answers current question indicates next
node negative answers  figure  first branch states calibrate
action set helpful actions  recommendation  in square brackets  choosing action
 i e  calibrate   addition  branch indicates recommended action occurred   
times training examples  moreover  leaf node information  in double square brack   

fid e la rosa   j imenez   f uentetaja   b orrajo

ets  number times type action selected training examples covered
rule current branch  thus  case  action calibrate selected   
total    times  operators never selected  second branch says
calibrate helpful action  take image one  planner selected
take image         times  helpful calibrate take image actions helpful switch action  switch recommendation 
selected       times  tree branches interpreted similarly 
selected  a  b  c 
helpful calibrate a b  d  e  f   
   yes  calibrate         turn to     switch on     switch off     
 
calibrate      take image      
   no  helpful take image a b  g  h  i  j   
   yes  take image          turn to     switch on     switch off     
 
calibrate     take image        
   no  helpful switch on a b  k  l   
   yes  switch on         turn to      switch on      
 
switch off     calibrate     
 
take image      
   no   turn to          turn to       switch on     
switch off     calibrate     
take image      

figure    relational decision tree learned operator selection satellite domain  internal
nodes  with   ending  queries helpful contexts  leaf nodes  in brackets 
class number observed examples operator 

      l earning b inding c lassifiers
second learning step  relational decision tree built domain operator o 
trees indicate bindings select different helpful contexts  inputs learning
binding classifier operator are 
training examples  consist exclusively helpful contexts operator
selected  together applicable instantiations contexts  note
given helpful context  applicable instantiations may include helpful nonhelpful actions  helpful contexts coded exactly previous learning step 
applicable instantiations represented selected predicate  predicate target concept second learning step arguments example
problem identifiers  instantiated arguments applicable action example class  selected rejected   purpose predicate distinguish
good bad bindings operator  figure   shows piece knowledge base
building binding tree corresponding action switch satellite domain  example  id tr   e    resulted selection action instantiation
   

fis caling h euristic p lanning r elational ecision rees

switch on instrument  satellite    action switch on instrument  
satellite   applicable rejected planner 
language bias  bias learning binding trees bias learning
operator tree  except includes definition selected predicate 
previous learning step  language bias learning binding tree automatically extracted pddl domain definition  figure   shows part language bias specified
learning binding tree action switch satellite domain 

  example tr   e   problem tr  
selected switch on tr   e   tr   instrument  satellite  rejected  
selected switch on tr   e   tr   instrument  satellite  selected  
helpful switch on tr   e   tr   instrument  satellite   
helpful switch on tr   e   tr   instrument  satellite   
helpful turn to tr   e   tr   satellite  star  star   
helpful turn to tr   e   tr   satellite  star  star   
helpful turn to tr   e   tr   satellite  phenomenon  star   
helpful turn to tr   e   tr   satellite  phenomenon  star   
target goal image tr   e   tr   phenomenon  spectrograph   
target goal image tr   e   tr   phenomenon  spectrograph   
target goal image tr   e   tr   star  image   
  static predicates problem
static fact calibration target tr   instrument  star   
static fact calibration target tr   instrument  star   
static fact supports tr   instrument  image   
static fact supports tr   instrument  spectrograph   
static fact supports tr   instrument  image   
static fact supports tr   instrument  image   
static fact board tr   instrument  satellite   
static fact board tr   instrument  satellite   

figure    knowledge base corresponding example tr   e   obtained solving training problem tr   satellite domain 
result second learning step relational decision tree uninstantiated
operator o  consists set disjoint rules binding selection o  figure  
shows example binding tree tswitch built operator switch satellite
domain  according tree  first branch states helpful action
switch instrument c satellite d  switch bindings  c  d  selected
planner         times  note binding trees learned roller advise
non helpful actions  frequently  selected predicate matches tree queries refer
helpful predicates  cases  no branch query may cover bindings non helpful
actions operator 
binding trees satellite domain refer reader online appendix
article  include learned dck domains used experimental section 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

       target concept    predict selected switch on  idexample  idproblem  inst   sat   class   
type selected switch on idex idprob instrument satellite class   
classes  selected rejected   
       helpful context       operator classification
   

figure    part language bias learning binding tree switch action
satellite domain 

selected switch on  a  b  c  d  e 
helpful switch on a b c d   
   yes   selected          selected       rejected       
   no   rejected         selected     rejected       

figure    relational decision tree learned bindings selection switch action
satellite domain 

many cases  decision trees somewhat complex one shown figure   
instance  turn binding tree    nodes includes several queries target goals  e g  
asking pending image new pointed direction  others static facts  e g  
asking new pointed direction calibration target  
    generation training examples
roller training examples instances decisions made solving training problems  order
characterize variety good solutions  decisions consider different alternatives
solving individual problem  given search tree node  state   alternatives come
possibility choosing different operators different bindings single operator 
cases assuming alternative lead equally good solutions 

regarding binding decisions  actions alternative solutions ignored 
tagged rejected consequently introduce noise learning process  instance 
consider problem figure   satellite domain satellite  calibrated
instrument  must turn directions d  d  d  order take images there  planning context  three turn actions helpful actions regarding one solution makes
learning consider one action selected two actions rejected  however  learned
knowledge always recommend helpful turn action towards direction
satellite  with corresponding calibrated instrument  take image  learn kind
knowledge  roller consider three turn actions selected three actions correspond selectable actions learning correct knowledge particular planning
   

fis caling h euristic p lanning r elational ecision rees

context  actions marked rejected learner consider selecting turn
described context bad choice 
takeimage d 

s 

s 

turnto d  d  

s 

takeimage d 

turnto d  d  
takeimage d 
turnto d  d  

s 

turnto d  d  
s 

s 

turnto d  d  
s  

   

s 

turnto d  d  
s 

   

   

figure    solution path alternatives simplified satellite problem 
regarding operator decisions  complete training full catalogue different solutions
confuse learning process  instance  consider example problem figure  
goal take image direction d   applying calibrate action s    necessary
switch instrument turn satellite d   the calibration target direction  
two actions helpful generate two different solution paths  fact  commutative 
generalizing operator selection kinds helpful contexts difficult training
examples contain examples types  i e  examples switch on action situated
turn to action vice versa   caused fact helpful
context different operators choose equally good choices 
s 
turnto d  d  

switchon

s 

s 

switchon

calibrate

s 

turnto d  d  

s 

takeimage d 

g

turnto d  d  
s 

figure    solution path alternatives simplified satellite problem 
roller follows commitment approach generation training examples      generation solutions  given training problem  roller performs exhaustive search obtain
multiple best cost solutions  taking account alternatives different binding choices     
selection solutions  roller selects subset solutions set best cost solutions
order reproduce particular preference operator alternatives      extraction examples solutions  roller encodes selected subset solutions examples required
learning  operator classification binding classification  following sections detail roller
proceeds three steps 

   

g

fid e la rosa   j imenez   f uentetaja   b orrajo

      g eneration olutions
roller solves training problem using best first branch bound  bfs bnb  algorithm
extracts multiple good quality solutions  search space explored exhaustively
within time bound  problem discarded examples generated it  therefore 
training problems need sufficiently small  addition  training problems need representative enough generalize dck assists roller solving future problems
domain 
bfs bnb search completed without pruning repeated states  practice  many repeated
states generated changing order among actions different solution paths  thus  pruning
repeated states would involve tagging actions leading solutions rejected bindings 
fact true  addition  bfs bnb algorithm prunes according evaluation
function f  n    g n    h n   g n  node cost  in work use plan length
cost function  h n  heuristic  safe way prune search space using
admissible heuristic  however  existing admissible heuristics allow roller complete
exhaustive search problems reasonable size  practice  using heuristic produces
overestimations introduces negligible noise learning process  end
search  bfs bnb algorithm returns set solutions best cost  solutions
used tag nodes search tree belong solutions label solution 

      electing olutions
set best cost solutions found  roller selects subset solutions used
generating training examples  since difficult develop domain independent criteria systematically selecting solutions reproduce operator selection particular context 
defined approach which  heuristically  prefers actions others  preferences
are 
least commitment preference  prefer actions generate alternatives different solution paths 
difficulty preference  prefer actions reach goals sub goals difficult
achieve  example figure   instrument switched achievable
one action  hand  pointing direction d  considered easier since
achieved actions turn to d  d   turn to d  d   
given     a              best cost plan planning task  compute preferences
functions depending action 
commitment  ai       a    a  successors ai   solution a      
function successors ai   returns applicable actions state si   function solution a 
verifies whether action tagged part best cost plan 
difficulty  ai    

min

 
  supporters l   

ladd ai  

function supporters l     a   l add a   returns set actions achieve
literal l 
   

fis caling h euristic p lanning r elational ecision rees

solutions ranked according preferences  ranking solution    
a            computed weighted sum action preferences  follows 
ranking         

x
i       n 

 n i 
 ai    
n

n plan length one commitment difficulty   sum weighted
give importance preferences first actions plan  first action preference
value multiplied    second  n    n  on  otherwise  several alternatives  i e  
commutative actions different positions within plan  would lead ranking value 
compute ranking best cost solutions using commitment   ties ranking broken
ranking computed difficulty   subset solutions best ranking values
subset solutions selected generating training examples 
      e xtracting e xamples olutions
takes subset solutions selected previous step generates training examples  generating examples operator classification  roller takes solution plans    
 a    a           correspond sequence state transitions  s    s         sn   generates
one learning example pair   si   ai     consisting h si   class  i e   operator
name action ai      see learning example shown figure   
generating examples binding classification operator o  roller considers
pairs   si   ai     ai   matches operator o  learning example generated pair
  si   ai     binding selection operator consists h si   classes
applicable actions si match o  including ai     applicable actions solution label
belong selected class applicable actions rejected class  moreover  actions
belonging solutions top ranking still marked selected even though
nodes example generated  see learning example shown figure   
roller

    use learned knowledge  planning relational decision trees
section details make heuristic planning benefit dck  beginning
build action orderings learned dck  then  explains two different search strategies
exploit orderings      application dck generalized action policy  depthfirst h context policy algorithm      use dck generate lookahead states within
best first search  bfs  guided heuristic  h context policy lookahead bfs algorithm  
      rdering actions r elational ecision rees
given state s  expression app s  denotes set actions applicable s  learned dck
provides ordering app s   ordering built matching action app s  first
operator classifier corresponding binding classifier  figure    shows detail
algorithm ordering applicable actions relational decision trees 
algorithm divides set applicable actions two subsets  helpful actions 
non helpful actions  then  matches helpful context state  i e   h s   tree
operator classification  matching provides leaf node contains list operators
sorted number examples covered leaf training phase  see operator
   

fid e la rosa   j imenez   f uentetaja   b orrajo

dt filter sort  a h t  sorted list applicable actions
a  list actions
h  helpful context
t  decision trees

selected actions  
ha   helpful actions a  h 
non ha     ha
leaf node   classify operators tree t  h 
ha
priority a    leaf node operator value leaf node  a 
priority a     
 selected a  rejected a     classify bindings tree t  h  a 
selected a 
selection ratio a   selected a  rejected a 
priority a    priority a    selection ratio a 
selected actions   selected actions  a 
max ha priority   maxaselected actions priority a 
non ha
priority a    leaf node operator value leaf node a 
priority a    max ha priority
 selected a  rejected a     classify bindings tree t  h  a 
selected a 
selection ratio a   selected a  rejected a 
priority a    priority a    selection ratio a 
selected actions   selected actions  a 
return sort selected actions  priority 

figure     algorithm ordering actions using relational decision trees 
classification tree figure     number examples covered gives operator ordering
used prefer actions search  algorithm uses number initialize
priority value helpful action  taking value corresponding operator  algorithm
keeps helpful actions least one matching example  actions 
algorithm matches action corresponding binding classification tree  resulting leaf
binding tree returns two values  number times ground action selected 
number times rejected training phase  define selection ratio ground
action as 
selected a 
selection ratio a   
selected a    rejected a 
ratio represents proportion good bindings covered particular leaf binding
tree  denominator zero  selection ratio assumed zero  priority
action updated adding selection ratio  thus  final priority action higher
   

fis caling h euristic p lanning r elational ecision rees

actions operators operator classification tree provides higher values  i e 
selected often training examples  since selection ratio remains  
   adding number considered method breaking ties initial priority
value  using information binding classification tree 
priority non helpful actions computed similar way except that  case 
algorithm considers actions whose initial priority  the value provided operator classification tree   higher maximum priority helpful actions  manner  capture
useful non helpful actions  follows heuristic criterion classify action helpful  although
heuristic shown useful  case may arise useful action
particular moment classified helpful  decision trees capture information  given
recommend choosing non helpful action  described method takes advantage
fact defines way using information applying learned knowledge  alternative approach would extend planning context new meta predicate non helpful
actions  however  pay variety problems domains means significantly larger contexts  causes expensive matching  finally  selected actions
sorted order decreasing priority values  sorted list actions output algorithm 
      h c ontext p olicy lgorithm
helpful context action policy algorithm moves forward  applying state best action
according dck  pseudo code algorithm shown figure     algorithm
maintains ordered open list  open list contains states expanded extracted
order  extracted  state evaluated using heuristic  thus  evaluate upon
extraction nodes included open list  evaluation provides heuristic
value state  h  set helpful actions ha  needed generate helpful
context  heuristic value used for      continuing search state recognized
dead end  h          goal checking  h       then  helpful context generated  subsequently  algorithm obtains set aa actions applicable state sorts using
decision trees  as shown algorithm figure      result aa  aa  sorted list
applicable actions  algorithm inserts successors generated actions aa  beginning open list preserving ordering  function push ordered list in open  
furthermore  make algorithm complete robust  successors generated applicable
actions aa  included secondary list called delayed list  delayed list
used open list empty  case  one node delayed list moved
open list then  algorithm continues extracting nodes open list 
algorithm  node maintains pointer parent order recover solution
found  also  node maintains g value  i e  length path
initial state node  function push ordered list in open inserts
open list candidates that      repeated states      repeated states lower g
value previous one  otherwise  repeated states pruned  type pruning guarantees
maintain node shortest solution found 
words  proposed search algorithm depth first search delayed successors 
benefit algorithm exploits best action selection policy per   

fid e la rosa   j imenez   f uentetaja   b orrajo

depth first h context policy  i  g     plan
i  initial state
g  goals
  decision trees

open list    i  
delayed list    
open list   
n   pop open list 
 h  ha    evaluate n  g    compute heuristic  
h     recognized dead end  
continue
h       goal state  
return path i  n 
h   helpful context ha  g  n 
aa   applicable actions n 
aa   dt filter sort aa  h   
candidates   generate successors n  aa 
open list   push ordered list in open candidates open list 
delayed candidates   generate successors n  aa   aa 
delayed list   push ordered list delayed candidates  delayed list 
open list   delayed list   
open list     pop delayed list   
return fail

figure     depth first algorithm sorting strategy given dck 
fect  action ordering not  particularly  perfect dck directly applied
backtrack free search inaccurate dck force search algorithm backtrack 
      h c ontext p olicy l ookahead trategy
many domains learned dck may contain flaws  helpful context may expressive
enough capture good decisions  learning algorithm may able generalize well
training examples may representative enough  cases  direct application
learned dck  without backtracking  may allow planner reach goals problem 
poor quality learned dck balanced guide different nature
domain independent heuristic  successful example obtusewedge system  yoon et al  
      combined learned generalized policy heuristic  obtusewedge exploited
learned policy synthesize lookahead states within lookahead strategy  lookahead states
   perfect policy refer policy leads directly goal state  policies guaranteed perfect
given generated inductive learning 

   

fis caling h euristic p lanning r elational ecision rees

first applied heuristic planning yahsp planner  vidal         intermediate
states frequently closer goal state direct descendants current state 
intermediate states added list nodes expanded used within
different search algorithms  learned policy contains flaws  lookahead states synthesized
policy may provide good guidance search  however  lookahead states
included complete search algorithm considers ordinary successors  search
process becomes robust  general  use lookahead states forward state space
search slightly increases branching factor search process  overall  shown
yahsp planner ipc      experiments included yahsp paper  vidal        
approach seems improve performance significantly 
figure    shows generic algorithm using lookahead states generated policy
search  algorithm weighted best first search  bfs   modification one lookahead states inserted open list expanding node 
weighted bfs  nodes expanded maintained open list ordered evaluation
function f  n    h n    g n   apart usual arguments bfs  algorithm receives
policy  p   horizon  horizon represents maximum number policy steps
applied generating lookahead states  experiments  use algorithm
heuristic h n  

h context policy lookahead bfs  i g t  horizon   plan
i  initial state
g  goals
  decision trees  policy 
horizon  horizon
open list  
add to open i 
open list   
n   pop open list 
goal state n  g 
return path i  n 
add to open lookahead successors n  g  t  horizon 
add to open standard successors n 
return fail

figure     generic lookahead bfs algorithm 
heuristic evaluation  h n   g value  g n   set helpful actions  saved
node node evaluated  function add to open state  evaluates
state inserts open list  ordered increasing values evaluation function 
f  n   function prunes repeated states  following strategy described depthfirst h context policy algorithm  repeated states higher g n  existent one
   

fid e la rosa   j imenez   f uentetaja   b orrajo

pruned  function add to open standard successors n  calls add to open
successor node n  function add to open lookahead successors explained below 
adapted generic lookahead bfs algorithm learned dck  particular
instantiation function add to open lookahead successors shown figure    
case  lookahead states generated iteratively applying first action action ordering provided dck  inputs algorithm current state  problem goals 
decision trees horizon  first  algorithm generates helpful context applicable
actions  helpful actions  n ha  recovered node  then  algorithm sorts applicable actions using decision trees  as previously shown algorithm figure     
that  successor generated first action inserted open list  recursive
call successor horizon decremented one  function add to open returns
true argument added open list false otherwise  fact  returns
false two cases      state repeated state g value higher g value
existent state      state recognized dead end  ordered list becomes empty 
lookahead state generated initial node returned  occurs
horizon zero  described implementation similar lookahead strategy approach
followed obtusewedge  instead perform lookahead generation using helpful contexts
relational decision trees 
hand  described h context policy lookahead bfs algorithm search
perfomed set applicable actions node  however  many domains use
helpful actions shown good heuristic  one possible way prioritizing helpful
actions non helpful actions include open list successors given helpful
actions  include remaining successors secondary list  implemented idea
following strategy used depth first h context policy algorithm  open list
becomes empty one node passed secondary list open list  search
continues  algorithm still complete given prune successor  helpful
actions good enough  strategy save many heuristic evaluations  experiments
compare strategy previous one  intuition adequacy
strategy depends directly quality helpful actions  quality learned dck 
accuracy heuristic particular domain 
another technique prioritizing helpful actions bfs implemented yahsp  vidal 
      inserts two consecutive instances node open list  nodes
equal f  n  since represent state  first one contains helpful actions 
therefore  expanded  generates successors resulting actions  second
contains non helpful actions  called rescue actions  way  successors lower
f  n  parent node sub tree generated helpful actions expanded
successor resulting non helpful actions 
performed preliminary experiments  obtaining similar results two described methods prioritizing helpful actions bfs  use secondary list non helpful
actions  use rescue nodes  reason  include results first technique
experimental section  call algorithm h context policy lookahead bfs ha 
   state repeated g value smaller existent one  add to open re evaluate
instead takes heuristic evaluation existent state 

   

fis caling h euristic p lanning r elational ecision rees

add to open lookahead successors  n g t  horizon   state
n  node  state 
g  goals
  decision trees  policy 
horizon  horizon
horizon    
return n
h   helpful context n ha  g  n 
aa   applicable actions n 
aa    dt filter sort aa  h   
aa    
  pop aa   
n    generate successor n  a 
added   add to open n   
added
goal state n    g 
return n 
return add to open lookahead successors n    g    horizon   
return n

figure     algorithm generating lookahead states decision trees 

   experimental results
section evaluate performance roller system  evaluation carried
variety domains belonging diverse ipcs  four domains come learning track
ipc       gold miner  matching blocksworld  parking thoughtful   rest domains
 blocksworld  depots  satellite  rovers  storage tpp  selected among domains
sequential tracks ipc           presented different structure
difficulty  available random problem generators  automatically build training sets learning dck  domain  complete training phase
roller learns corresponding dck testing phase evaluate scalability
quality solutions found roller learned dck  next  detail experimental
results obtained two phases  moreover  domains give particular
details training test sets  learned dck observed roller performance 
    training phase
domain  built training set thirty randomly generated problems  size
structure problems discussed particular details given domain 
explained section      roller generates training examples solving problems
   

fid e la rosa   j imenez   f uentetaja   b orrajo

training set bfs bnb search  set time bound    seconds solve training
problem  discarding exhaustively explored time bound  then  roller
generates training examples solutions found builds corresponding decision
trees tilde system  blockeel   de raedt        
evaluate efficiency roller training phase computed following metrics 
time needed solving training problems  number training examples generated
process  time spent tilde learning decision trees number leaves
operator selection tree  last number gives clue size learned dck  table  
shows results obtained domain 
domain
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp

training
time  s 
     
     
      
     
     
     
    
     
     
     

learning
examples
    
   
   
   
   
    
    
   
   
   

learning
time  s 
    
    
   
    
   
    
    
   
     
    

tree
leaves
  
  
 
  
  
  
 
 
  
 

table    experimental results training process  training learning times shown 
well number training examples  complexity generated trees  number
leaves  

achieves shorter learning times  fourth column table    state of the art
systems learning generalized policies  martin   geffner        yoon et al          particularly 
systems implement ad hoc learning algorithms sometimes require hours order
obtain good policies  approach needs seconds learn dck given domain 
fact makes approach suitable architectures need on line planning learning processes  however  learning times constant different domains  depend
number training examples  in work  number given amount different
solutions training problems   size training examples  in work size
given number arity predicates actions planning domain  training
examples structured  i e   whether examples easily separated learning not 
roller

    testing phase
testing phase roller attempts solve  domain  set thirty test problems 
problems taken evaluation set corresponding ipc  evaluation set
contains problems  thirty problems thirty hardest ones  depots domain
exception twenty two problems  evaluation set domain ipc     
   

fis caling h euristic p lanning r elational ecision rees

contained twenty two problems  three experiments made testing phase 
first one evaluates rollers performance dck learned solutions training
problems ranked solution approach  second one evaluates usefulness
learned dck third one compares roller state of the art planners  experiment evaluate two different dimensions solutions found roller  scalability
quality  testing experiments done using     ghz processor time bound    
seconds   gb memory bound 
      olution r anking e valuation
experiment evaluates effect selecting solutions following approach described section      roller configurations evaluation are 
top ranked solutions  depth first h context policy algorithm using dck learned
sub set top ranked solutions  use search algorithm  since performance depends quality learned dck algorithms
using dck 
solutions  depth first h context policy algorithm using dck learned solutions obtained bfs bnb algorithm 
table   shows number problems solved configuration  time plan
length average computed problems solved configurations  number brackets
first column number problems solved common  top ranked solutions configuration solved thirty problems solutions configuration  mainly due difference
   problems matching blocksworld domain 
domains
blocksworld     
depots     
gold miner     
matching bw    
parking     
rovers     
satellite     
storage     
thoughtful     
tpp     
total

top ranked solutions
solved
time length
  
    
     
  
    
     
  
    
    
  


  
    
     
  
    
     
  
     
     
  
    
   
  
    
     
  
    
     
   



solutions
solved
time length
  
    
     
  
    
     
  
    
    
 


  
    
    
  
     
     
  
     
     
  
    
   
  
    
     
  
    
     
   



table    problems solved time plan length average evaluation ranking solution
heuristic 

effect selecting solutions varies across domains  instance  quite important regarding plan quality blocksworld  depots rovers  satellite domain top ranked
solutions allow roller solve two problems maintaining similar time plan length
       seconds time bound established learning track ipc      

   

fid e la rosa   j imenez   f uentetaja   b orrajo

average  gold miner domain  selecting solutions irrelevant equally
good solutions per problem  i e   goal always single fact gold  fairly
top ranked ones  parking domain benefit selecting solutions 
considering overall results  think selecting solutions useful heuristic improving
dck quality many domains  remaining evaluations refer dck used
roller decision trees learned top ranked solutions 
      dck u sefulness e valuation
shown ipc learning track results  dck may degrade performance base planner 
dck incorrect  mind  designed experiment measure performance roller algorithms comparing versions without dck  made two versions
non learning algorithms  first one empty configuration decision
tree given algorithm  thus ordering computed helpful actions  second one
systematic configuration  ordering supplied heuristic instead 
roller configurations used comparisons are 
roller  depth first h context policy algorithm dck learned training
phase 
roller bfs  h context policy lookahead bfs algorithm dck learned
training phase  configuration uses horizon h        choose value
basis empirical evaluations 
roller bfs ha  modified version roller   bfs helpful actions considered immediate successors  lookahead states generated original version  using horizon 
three algorithms equivalent version empty configuration 
df ha  depth first helpful actions   empty dck roller corresponds depthfirst algorithm helpful actions  original algorithm  non helpful actions
placed delayed list 
bfs  empty dck roller   bfs generate lookahead states  i e   algorithm add to open lookahead successors figure      therefore  algorithm becomes
standard best first search 
bfs ha  modified version bfs helpful actions considered  non helpful
actions placed delayed list 
previous configurations systematic version  case action ordering computed
heuristic 
gr ha  greedy helpful actions   algorithm corresponds greedy search
helpful actions  node  helpful immediate successors sorted heuristic 
non helpful nodes go delayed list 
lh bfs  lookahead bfs   bfs lookahead states  function dt filter sort
replaced function computes ordering using heuristic 
   

fis caling h euristic p lanning r elational ecision rees

lh bfs ha  modified version lh   bfs helpful actions considered  nonhelpful actions placed delayed list 
comparison  computed number problems solved scores used
ipc      learning track evaluate planners performance terms cpu time quality  plan
length   time score computed follows  problem planner receives ti  ti
points  ti minimum time participant used solving problem i  ti
cpu time used planner question     problem test set planner receive   
points  higher score better  quality score computed way  replacing
l  l measures quality terms plan length  addition compute time
quality averages problems solved configurations  configuration solve
problem  taken account measure  average measures complement scores
since give direct information commonly solved problems  scores tend benefit
configurations solve problems others not 
table   shows summary results obtained dck usefulness evaluation 
configuration compute number domains algorithm top performer
evaluated criteria  i e   numbers solved problems  time quality scores averages   top performer domain algorithm equal better measure
rest algorithms  table  algorithm    points  number
evaluated domains  global section refers overall top performers  relative section refers
number domains configuration equal better two configurations
algorithm strategy  i e   depth first  best first  best first helpful actions   averages
commonly solved problems computed configurations solve one problem  results show roller good number solved problems speed metrics 
regarding quality score  roller roller   bfs   ha best performers three domains
each  however  bfs bfs   ha obtained better results quality average 
global
solved problems
time score
time average
quality score
quality average
relative
solved problems
time score
time average
quality score
quality average

depth first
roller gr ha df ha
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

best first
roller bfs lh bfs
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

bfs
 
 
 
 
 
 
 
 
 
 

helpful best first
roller bfs ha lh bfs ha bfs ha
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

table    summary dck usefulness evaluation  column gives number domains
configuration top performer row item 

table   shows number solved problems dck usefulness evaluation  total
row shows roller configuration solved problems empty systematic
versions  results time quality scores reported table   table    appendix a 
   

fid e la rosa   j imenez   f uentetaja   b orrajo

detailed results averages considered less interesting since many domains
common solved problems  easy problems 
domains
blocksworld     
depots     
gold miner     
matching bw     
parking     
rovers     
satellite     
storage     
thoughtful    
tpp     
total

depth first
roller gr ha df ha
  
 
 
  
  
  
  
 
 
  
 
 
  
  
 
  
  
  
  
  
  
  
 
  
  
  
 
  
  
  
   
   
   

best first
roller bfs lh bfs
bfs
 
 
 
  
  
  
  
  
  
  
 
  
  
  
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
   
       

helpful best first
roller bfs ha lh bfs ha bfs ha
 
 
 
  
  
  
  
 
 
  
  
  
  
  
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   
   
   

table    problems solved dck usefulness evaluation 

      ime p erformance c omparison
experiment evaluates scalability roller system  compared state of the art planners  comparison  chosen lama  richter   westphal         winner
sequential track past ipc  ff  last ipc shown still competitive 
used three roller configurations explained previous evaluation  configuration
planners are 
ff  running enforced hill climbing  ehc  algorithm helpful actions together
complete bfs case ehc fails     though planner dates      include
evaluation because  shown results ipc       still competitive
state of the art planners  besides  planner extensively used planning
learning systems 
lama first  winner classical track ipc       configuration lama
modified stop finds first solution  way  comparison fair
rest configurations implement anytime behavior  i e   continuous solution
refinement reaching time bound   anytime behavior lama compared later
roller performance next section 
table   shows number problems solved together speed score  results
give overall view performance different planners  roller solves many
problems configuration      domains achieves top speed score
seven domains  second best score belongs roller   bfs   ha  solves many
problems planners six domains  lama first fairly competitive  since solves seven
problems less roller    problems roller   bfs   ha  cases lama first
achieves lower speed score 
   planner actually metric ff running strips domains  consider implementation adequate baseline
comparison roller implemented code rather original order extend
approach planning models 

   

fis caling h euristic p lanning r elational ecision rees

domain  problems 
blocksworld     
depots     
gold miner     
matching bw     
parking     
rovers     
satellite     
storage     
thoughtful    
tpp     
total

roller
solved
score
  
     
  
     
  
     
  
     
  
     
  
     
  
     
  
     
  
     
  
     
   
      

roller bfs
solved
score
 
    
  
     
  
    
  
    
  
     
  
     
  
     
  
     
  
     
  
     
          

roller bfs ha
solved
score
 
    
  
     
  
    
  
    
  
     
  
     
  
     
  
     
  
     
  
     
   
      


solved
 
  
  
 
  
  
  
  
  
  
   

score
    
    
    
    
    
    
    
     
    
    
     

lama first
solved
score
  
    
  
    
  
     
  
     
  
    
  
     
  
     
  
    
  
     
  
    
   
      

table    problems solved speed score five configurations 

table   shows average time five configurations addressing subset problems solved configurations  first column shows parenthesis number commonly
solved problems  results closely related shown table    roller achieves
best average time eight ten domains  observe different configurations
good particular domains even particular problems  instance  thoughtful
domain four problems solved configurations 
domain  problems 
blocksworld    
depots     
gold miner     
matching bw    
parking     
rovers     
satellite     
storage     
thoughtful   
tpp     

roller
    
    
    
    
    
    
    
     
    
    

roller bfs
     
     
     
     
    
     
    
    
     
    

roller bfs r
     
    
    
     
    
    
    
    
    
    


    
    
     
     
     
     
    
     
    

lama first
      
     
    
    
      
    
    
    
    
    

table    planning time averages problems solved configurations 

      q uality p erformance c omparison
experiment compares quality first solutions found solutions found
anytime behavior  anytime configuration  planners exhaust time bound trying improve
incrementally best solution found  three roller algorithms modified configuration
best solution found far used upper bound order prune nodes
exceed plan length  anytime behavior regular configuration lama 
anytime behavior  included anytime comparison well base
comparing quality improvements planners 
table   shows quality scores first solution last solution found
anytime configurations  anytime column planner shows score variation reveals
whether planner able make relative improvements first solutions  relative
   

fid e la rosa   j imenez   f uentetaja   b orrajo

domain
blocksworld
depots
gold miner
matching bw
parking
rovers
satellite
storage
thoughtful
tpp
total

roller
first anytime
     
     
    
    
     
     
    
    
     
     
     
     
     
     
     
     
    
    
     
     
      
      

roller bfs
first anytime
    
    
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      

roller bfs ha
first
anytime
    
    
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      


first
    
     
     
    
     
     
     
     
     
     
      

relative
    
     
     
    
     
     
     
     
     
     
      

lama
first anytime
    
    
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
      
      

table    quality scores first solution anytime configuration evaluated planners 

shows score solutions compared solutions given anytime configuration
planners  loses points cases others able improve
solutions  two lama configurations obtained top score category  nevertheless 
planner dominated domains  furthermore  configurations achieved top quality score
first solution least one domain 
domain
blocksworld    
depots     
gold miner     
matching bw    
parking     
rovers     
satellite     
storage     
thoughtful   
tpp     

roller
first
anytime
      
      
      
      
     
     
      
      
     
     
      
      
     
     
     
     
      
      
     
     

roller bfs
first
anytime
      
      
     
     
     
     
     
     
     
     
      
      
     
     
     
     
      
      
     
     

roller bfs ha
first
anytime
      
      
     
     
     
     
     
     
     
     
      
      
     
     
     
     
      
      
     
     


first

     
     
     
     
     
     
     
      
     

relative

     
     
     
     
     
     
     
      
     

lama
first
anytime
      
      
     
     
     
     
     
     
     
     
      
     
     
     
     
     
      
      
     
     

table    quality averages first solution anytime configuration evaluated planners 

table   shows plan length average problems solved configurations  first
column shows average first solutions anytime column gives average
last solutions anytime configuration  commonly solved problems
reported table    although planner solved fewer problems  achieves
best average plan length seven domains  plan length averages reveal roller able
find first solutions good quality domains  roller   bfs roller   bfs   ha find
better quality solutions roller  several domains  averages competitive
lama   roller   bfs roller   bfs   ha show better quality performance mainly due
combination learned dck domain independent heuristic within bfs algorithm 
following subsections discuss particular details domains  give
brief description domain together information training test sets used
experimental evaluation  domain  analyze learned dck obtained
   

fis caling h euristic p lanning r elational ecision rees

results order give fine grained interpretation observed performance  details
domains found ipc web site  
      b locksworld etails
problems domain concerned configuring towers blocks using robotic arm 
training set used experiments consisted of  ten eight block problems  ten nine block
problems ten ten block problems  test set consisted    largest typed problems
ipc             blocks 
blocksworld domain
   
roller
roller bfs
roller bfs ha
lama first

percentage solved

  

  

  

  

 
   

 

  
cpu time

   

    

figure     percentage solved problems increasing time evaluating scalability performance blocksworld domain 
although domain one oldest benchmarks automated planning  still challenging state of the art heuristic planners  blocksworld presents strong interaction among goals
current heuristics fail capture  particular  achieving goal domain may undo
previously satisfied goals  therefore  crucial achieve goals specific order  dck
learned roller gives total order domain actions different contexts capturing key
knowledge  lets roller achieve impressive scalability results producing good quality
solution plans  roller configurations considerably better non learning configurations 
particularly  roller solved thirty problems set df   ha gr   ha solve
problem  roller quite good compared state of the art planners  figure   
observe roller performs two orders magnitude faster lama  x axis
figure represents cpu time logarithmic scale y axis represents percentage
solved problems particular time  moreover  roller obtained best quality score first
solution anytime evaluations  addition  average plan length common problems fairly
close best average  obtained roller   bfs roller   bfs   ha  bfs algorithms
   http   idm lab org wiki icaps index php main competitions

   

fid e la rosa   j imenez   f uentetaja   b orrajo

scale well domain partially guided heuristic  considerably
underestimates distance goals  similarly  lookahead states generated policy
discarded fail escape plateaus generated heuristic function 
analyzing learned operator tree found explanations good performance
roller blocksworld domain  operator tree clearly split two parts  first part contains decisions take arm holding block  situation  tree captures
stack put down block  second part contains decisions take arm empty 
case tree captures unstack pick up block  second part tree 
current state search matches logical query helpful unstack block  block   
means tower blocks block  well arranged  i e   block  least
one block beneath block  well placed  therefore  set helpful actions compactly
encodes useful concept bad tower  kind knowledge manually defined
previous works order learn good policies blocksworld  one approach consisted including recursive definitions new predicates  support predicates above x y 
inplace x   khardon         another alternative involved changing representation language  instance concept language  martin   geffner        taxonomic syntax  yoon 
fern    givan         kleene star operator taxonomic syntax  i e   operator defining recursion  discarded subsequent work  yoon et al         predicate
used instead  rollers ability recognize bad towers without extra predicates arises
misplaced block tower makes unstack action top block helpful  since always
part relaxed plan arm empty 
due extraordinary performance roller domain  built extra test set
clarify whether trend observed roller configuration would hold larger
problems  aim  randomly generated    problems distributed sub sets            
           blocks   problems sub set  roller solved    problems
extra test set time average      seconds per problem spending       solve
problem  obviously  problems became difficult roller number blocks increase 
      epots etails
domain combination transportation domain blocksworld domain 
crates instead blocks hoists instead robot arm  problems consist trucks
transporting crates around depots distributors  using hoists  crates stacked onto pallets
top crates final destination  domain     training problems
different combinations     locations  depots distributors       trucks      pallets per
location    hoist per location     crates placed different configurations 
testing phase used    problems ipc      set  hardest problem   
locations      pallets     hoists     trucks    crates 
roller roller   bfs improve performance non learning strategies  three
configurations bfs helpful action solved    problems  roller able solve   
problems  achieving best speed score  however  high average plan length indicates
policy producing good quality plans  roller   bfs   ha obtains second best speed score
competitive plan lengths  figure    shows percentage solved problems
   explained section     logic queries roller present example problem ids  case ids
ignored simplicity given needed matching current helpful context 

   

fis caling h euristic p lanning r elational ecision rees

increasing cpu time  in logarithmic scale   anytime configuration  roller   bfs   ha
able refine solutions  achieving quality average similar lama 
depots domain
   
roller
roller bfs
roller bfs ha

lama first

percentage solved

  

  

  

  

 
    

   

 

  

   

    

cpu time

figure     percentage solved problem increasing time evaluating scalability performance depots domain 

dck learned domain provides inaccurate advice large planning contexts 
instance  roller makes mistakes deciding crate unload several crates
loaded truck  reason inaccurate dck training problems large
enough gain knowledge  addition  adding crates problems makes unfeasible solved bfs bnb  nevertheless  limitation learned dck
evident  depots domain undirected  i e   actions reversible  
dead ends  therefore  mistakes made dck fixed additional actions  leads
worse quality plans  besides  since first solutions rapidly found  roller configurations
spend time refining solutions  reason great improvement plan average
roller   bfs   ha  
      g old  m iner etails
objective domain navigate grid cells reaching cell containing gold 
cells occupied rocks cleared using bombs laser  domain
training set consists of     problems     cells     problems     cells    
problems     cells  domain part learning track ipc      used
test set used competition  set problems ranging         cells 
problems gold miner domain solvable helpful actions alone  explains
difference number solved problems roller  roller   bfs   ha nonlearning counterpart  general terms  domain trivial roller  roller   bfs   ha  they
solved test problems less    seconds per problem  lama  nevertheless  scales   

fid e la rosa   j imenez   f uentetaja   b orrajo

poorly  domain essential actions picking bombs frequently considered
helpful actions  relaxed problem solvable using laser  consequently  fails
solve problems ehc requires additional bfs search  figure    shows
percentage solved problems increasing cpu time  regarding anytime evaluation 
tested configurations improved first solution found many problems 
gold miner domain
   

percentage solved

  

  

  

  
roller
roller bfs
roller bfs ha

lama first
 
    

   

 

  

   

    

cpu time

figure     percentage solved problems increasing time evaluating scalability performance gold miner domain 
domain  operator tree succeeds capturing key knowledge  initial states 
bombs laser cell  robot needs decide pick up 
operator tree domain matches logical query candidate pickup laser cell 
higher ratio operator pickup bomb operator pickup laser  operator
preference allows roller avoid dead ends laser destroys gold  hand 
situations laser required  i e   destroy hard rocks  reached second choice
policy  fact implies backtracking roller  additional evaluated nodes
significantly affect overall performance  preference pickup bomb
pickup laser action example selecting non helpful actions 
      atching b locksworld etails
domain version blocksworld designed analyze limitations relaxed plan heuristic 
version blocks polarized  either positive negative  two polarized robot
arms  furthermore  block placed  stack put down actions  arm different
polarity  block becomes damaged block placed top it  however  picking
unstacking block wrong polarity seems harmless  fact makes recognizing
dead ends difficult task heuristic  particularly  relaxed task blocks never
damaged  thus  relaxed plan  and consequently set helpful actions  heuristic
estimation wrong  training set used domain consists fifteen   blocks problems
   

fis caling h euristic p lanning r elational ecision rees

fifteen   blocks problems  used even number blocks keep problems balanced  i e  
half blocks polarity   testing phase used test set learning track
ipc       set problems ranging       blocks 
df   ha gr   ha solve problem  problems solvable
helpful actions alone  learned dck recommended useful non helpful actions  thus
roller able solve    problems  policy configurations perform better systematic strategies  fairly similar using lookahead strategy  fact reveals learned dck
effective enough pay effort building lookahead states  lama planner
solves problems  figure    shows percentage solved problems increasing
cpu time 
matching bw domain
   
roller
roller bfs
roller bfs ha

lama first

percentage solved

  

  

  

  

 
   

 

  
cpu time

   

    

figure     percentage solved problems increasing time evaluating scalability performance matching blocksworld domain 

roller solved problems evaluating considerable number nodes plan length 
means dck learned domain accurate  analyzing training
examples find many solution plans satisfy key knowledge domain  robot
arms unstack pick up blocks polarity   specifically  robot handling
top block  i e   block blocks goal state  polarity robot
arm becomes meaningless  effect unavoidable shortest plans involve managing
top blocks efficient way ignoring polarities  examples include noise
learning make generalization complex 

      parking etails
domain involves parking cars street n curb locations cars double
parked  triple parked  goal move one configuration parked cars another
driving cars one curb location another  domain training set consists of  fifteen
   

fid e la rosa   j imenez   f uentetaja   b orrajo

problems six cars four curbs fifteen problems eight cars five curbs  testing
used test set learning track ipc       hardest problem set    cars
   curbs 
three roller configurations solve problems perform significantly better nonlearning strategies  addition  three roller configurations outperform lama
difference one order magnitude  reason lama low speed
scores  roller configurations consistently better systematic empty configurations  figure    shows percentage solved problems increasing cpu time 
hand  three roller configurations achieve first solutions suficient quality  however  solutions refined anytime evaluation  especially roller   bfs   ha 
achieves top quality score plan length average fairly similar lama 
parking domain
   
roller
roller bfs
roller bfs ha

lama first

percentage solved

  

  

  

  

 
   

 

  
cpu time

   

    

figure     percentage solved problems increasing time evaluating scalability performance parking domain 

learned dck domain quite effective  roller rarely backtracked   operator tree perfectly classifies move car to curb action first tree node  asking
considered helpful action  besides  binding tree operator selects right car asking
target goal rejecting candidates  two decisions guide planner place
car right position whenever possible  result  large number nodes evaluated 
explains scalability difference lama 
       rovers etails
domain simplification tasks performed autonomous exploration vehicles sent
mars  tasks consist navigating rovers  collecting soils rocks samples  taking
images different objectives  domain training set consists of  ten problems one
rover  four waypoints  two objectives one camera  ten problems additional camera 
   

fis caling h euristic p lanning r elational ecision rees

ten problems additional rover  problems test set thirty largest problems
ipc      set  i e   problems         largest problem set    rovers    
waypoints 
dck strategies faster systematic empty strategies  differences significant since configurations solved problems  one hand helpful actions
rovers domain quite good hand test set problems
big enough generate differences among approaches  regarding planner comparison  roller
achieves top performance score scales significantly better ff  solves two problems
less lama  figure    shows percentage solved problems increasing cpu time 
regarding anytime evaluation  planners able refine first solutions  lama gets
top quality score best plan length refining solutions 
rovers domain
   
roller
roller bfs
roller bfs ha

lama first

percentage solved

  

  

  

  

 
    

   

 

  

   

    

cpu time

figure     percentage solved problems increasing time evaluating scalability performance rovers domain 

domain  roller learned imperfect dck  manages achieve good scalability
results  dck imperfect partially actions communicating rock  soil image analysis
applied order among them  therefore  preferences ranking selecting
solutions fail discriminate among actions confuse learning algorithm  since
actions could applied order  dck mistakes seem harmless planning
time 
       atellite etails
domain comprises set satellites different instruments  operate different
formats  modes   tasks consist managing instruments taking images certain targets
particular modes  domain training set consist thirty problems one satellite  two
instruments  five modes five observations  problems test set thirty largest problems
   

fid e la rosa   j imenez   f uentetaja   b orrajo

ipc       i e   problems        largest problem set    satellites    modes
    observations 
three roller configurations improved number solved problems non learning
counterpart  addition  roller roller   bfs   ha solved    problems set  two
lama eight ff  figure    shows percentage solved problems
increasing cpu time  roller roller   bfs   ha achieve good quality solutions
able refine anytime evaluation  achieving plan lengths similar lama 
satellite domain
   
roller
roller bfs
roller bfs ha

lama first

percentage solved

  

  

  

  

 
    

   

 

  

   

    

cpu time

figure     percentage solved problems increasing time evaluating scalability performance satellite domain 

learned dck captures key knowledge satellite domain  trees shown
figure   figure   part learned dck fewer training examples  domain
roller roller   bfs   ha perform quite similarly  reason heuristic
quite accurate domain  thus  deepest lookahead state generated learned policy
frequently selected heuristic bfs search 
       torage etails
domain concerned storage set crates taking account spatial configuration depot  domain tasks comprise using hoists move crates containers
particular area depot  training set consists    problems   depot    container   
hoist different combinations     crates     areas inside depot 
test set used    problems ipc      set  largest problem domain  
depots   areas each    hoist    crates 
first    problems trivially solved configurations  then  problem difficulty increases quickly number problem objects increases  bfs solved    problems  one
dck strategy  meaning dck lookahead strategies pay off  domain
   

fis caling h euristic p lanning r elational ecision rees

hard lama  figure    shows percentage solved problems increasing
cpu time 
storage domain
   
roller
roller bfs
roller bfs ha

lama first

percentage solved

  

  

  

  

 
    

   

 

  

   

    

cpu time

figure     percentage solved problems increasing time evaluating scalability performance storage domain 

although dck effective  found interesting properties it  learned operator tree
compact succeeds selecting go in action normally marked helpful
action 
       houghtful etails
domain models version solitaire card game  cards visible one
turn card talon rather   cards time  original version  goal
game place cards ascending order corresponding suit stacks  home deck  
available random problem generator domain  therefore  used bootstrap
problem distribution given learning track ipc       set contains problems
four suits  card seven suit  test phase used    problems
test distribution learning ipc       largest problem domain full set
standard card game 
roller solves    problems  three fewer gr   ha   however  roller   bfs roller bfs   ha better number solved problems non learning approaches  domain 
use dck lookahead construction combined heuristic makes search process
robust policy mistakes  roller   bfs   ha solves     three lama  figure   
shows percentage solved problems increasing cpu time 
bfs bnb algorithm generating training examples able solve      
problems bootstrap problem distribution  believe different bootstrap distribution smaller problems would generate accurate dck  additionally  even though dck
   

fid e la rosa   j imenez   f uentetaja   b orrajo

thoughtful domain
   
roller
roller bfs
roller bfs ha

lama first

percentage solved

  

  

  

  

 
   

 

  
cpu time

   

    

figure     percentage solved problems increasing time evaluating scalability performance thoughtful domain 

lookahead strategies achieve good results  learning accurate decision trees complex
many classes     operators particular domain  many arguments
predicates background knowledge  up   parameters operator col to home  
parameters operator col to home b  
       tpp etails
tpp stands traveling purchase problem  generalization traveling salesman
problem  tasks domain consist selecting subset markets satisfy demand
set goods  selection markets try optimize routing purchasing
costs goods  strips version  graph connects markets equal costs
arcs  nevertheless  domain still interesting difficult planners scale
increasing number goods  markets trucks  training set consists thirty problems
number goods  trucks depots varying one three load levels five
six  test set consists thirty problems used planner evaluation ipc      
largest problem set    goods    trucks    markets load level six 
roller   gr   ha df   ha solved    problems test set  roller performs faster
two  achieving similar plan lengths  besides  roller outperforms rest
planners two orders magnitude faster ff  main reason overwhelming
branching factor large problems together fact heuristic falls big plateaus
domain  greedy  depth first  approaches perform better avoid effect
plateaus  additionally  roller achieved competitive quality scores average plan length
first solution anytime evaluation  roller   bfs roller   bfs   ha got bad results

   

fis caling h euristic p lanning r elational ecision rees

domain imprecision heuristic  figure    shows percentage
solved problems increasing cpu time 
tpp domain
   

percentage solved

  

  

  

  
roller
roller bfs
roller bfs ha

lama first
 
    

   

 

  

   

    

cpu time

figure     percentage solved problems increasing time evaluating scalability performance tpp domain 
learned dck compact useful reducing number evaluations  shown
roller performance  instance  drive binding tree recognizes perfectly truck
market need go market b already truck b handling goods
market  situations  state truck b helpful action drive  meaning
truck b something deliver 

   lessons learned ipc
ipc      included specific track planning systems benefit learning  thirteen systems
took part track including previous version roller  de la rosa et al         achieved
 th position  version upgrade original roller system  de la rosa et al  
       first version proposed ehc sorted algorithm alternative h context policy  effective many domains  competing version tried recommend ordering
applying actions relaxed plans  idea  although initially appealing  good
choice usefulness strongly depends fact relaxed plan contains right actions  competition completed analysis roller performance diagnose
strengthen weak points  system resulting improvements roller version
described article  one example roller improvements results obtained
thoughtful matching blocksworld domains  ipc       roller failed solve problems thoughtful domain solved two problems matching blocksworld 
reported section    current version roller solves       problems respectively
domains  addition  current version roller outperforms lama park   

fid e la rosa   j imenez   f uentetaja   b orrajo

ing domain one order magnitude  improvements roller overcome limitations
version submitted ipc      three aspects 
robustness wrong dck  issues discussed section   decisions introduce biases learning process making learning dck complex task  fact  competitor
ipc      able learn useful dck domains  furthermore  many domains
learned dck damaged performance baseline planner  case
roller   described paper  strengthened roller wrong dck
proposing two versions modified bfs algorithm combine learned dck
numerical heuristic  combination dck heuristic makes planning process robust imperfect and or incorrectly learned knowledge  similar approach
followed winner best learner award  btuse w edge  yoon et al         
efficiency baseline  overall competition winner p bp  gerevini  saetti    vallati        portfolio state of the art planners learns planner settings
best ones given planning domain  result  performance competitor
never worse performance state of the art planner  ipc      baseline performance roller far competitive state of the art planners
roller algorithms coded lisp  overcome weakness optimized
implementation roller using c code outperformed ipc      results
domains 
definition significant training sets  training examples extracted experience
collected solving problems training set  therefore  quality training
examples depends quality problems used training  ipc      training
problems fixed organizers and  many domains  large
roller system extract useful dck  paper created training problems using
random generators build useful training sets roller system domain 
selection training examples  relational classifiers induce set rules trees model
regularities training data  case forward state space search planning
best cost solutions problem may used training data  leads alternatives confuse learner  avoid this  training data cleaned
used learning algorithm  ranking solution selection proposed article
option give learner training data clearer regularities 
additionally  roller performed poorly sokoban n puzzle domains  traditionally 
useful dck domains form numeric functions  manhattan distance 
provides lower bound solution length  general  action policies inaccurate
domains  lack knowledge trajectory goals  currently  still
unable learn useful dck roller domains  possible future direction introduce
goals subgoals  e g  landmarks  helpful context aim capturing
knowledge 

   related work
approach strongly inspired way prodigy  veloso et al         models dck 
prodigy architecture  action selection two step process  first  prodigy selects uninstan   

fis caling h euristic p lanning r elational ecision rees

tiated operator apply  second  selects bindings operator  selections
guided dck form control rules  leckie   zukerman        minton        
returned idea two step action selection allows us define learning
planning dck standard classification task therefore solve learning task
off the shelf classification technique relational decision trees  nevertheless  roller
need distinguish among different kinds nodes prodigy does  roller performs
standard forward heuristic search state space search nodes
type 
relational decision trees previously used learn action policies context
relational reinforcement learning  rrl   dzeroski  de raedt    blockeel         comparison
dck learned roller  rrl action policies present two limitations solving planning problems  first  rrl learned knowledge targeted given set goals  therefore
rrl cannot directly generalize learned knowledge different goals within given domain 
second  since training examples rrl consist explicit representations states  rrl needs
add extra background knowledge learn effective policies domains recursive predicates
blocksworld 
previous works learning generalized policies  martin   geffner        yoon et al        
succeed addressing two limitations rrl  first  introduce planning goals
training examples  way learned policy applies set goals domain  second 
change representation language dck predicate logic concept language 
language makes capturing decisions related recursive concepts easier  alternatively  roller
captures effective dck domains blocksworld without varying representation language 
roller implicitly encodes states terms set helpful actions state  result 
roller benefit directly off the shelf relational classifiers work predicate logic 
fact makes learning times shorter resulting policies easier read 
recently  techniques developed improve performance heuristic
planners 
learning macro actions  botea  enzenberger  muller    schaeffer        coles   smith 
      combination two operators considered new domain operators order reduce search tree depth  however  benefit decreases number
new macro actions added enlarge branching factor search tree causing utility problem  minton         approaches overcome problem  applying
filters decide applicability macro actions  newton  levine  fox    long 
       two versions work participated learning track ipc       obtaining
third fourth place  one advantage macro actions learned knowledge
exploited planner  thus  approaches learn generalized policies could
benefit macro actions  nevertheless  far know  combination
tried improving heuristic planners 
learning domain specific heuristic functions  approach  yoon  fern    givan       
xu  fern    yoon         state generalized heuristic function obtained examples
solution plans  main drawback learning domain specific heuristic functions
result learning algorithm difficult understand humans makes
verification learned knowledge difficult  hand  learned knowledge
easy combine existing domain independent heuristics  slightly different approach
   

fid e la rosa   j imenez   f uentetaja   b orrajo

consists learning ranking function greedy search algorithms  xu  fern    yoon 
             step greedy search  current node expanded child node
highest rank selected current node  case  ranking function
iteratively estimated attempt cover set solution plans greedy algorithm 
learning task decomposition  approach learns divide planning tasks given
domain smaller subtasks easier solve  techniques reachability analysis
landmark extraction  hoffmann  porteous    sebastia        able compute intermediate states must reached satisfying goals  however  clear
systematically exploit knowledge build good problem decompositions  vidal et al         consider optimization problem use specialized optimization
algorithm discover good decompositions 
general  system learns planning dck deal ambiguity training
examples  given planning state may present many good actions  trying learn dck
selects one action other  inherently equal  complex learning problem  cope
ambiguous training data roller created function ranks solutions aim learning
kind solutions  different approach followed xu et al         generate
training examples partially ordered plans 

   conclusions future work
presented new technique reducing number node evaluations heuristic planning based learning exploiting generalized policies  technique defines process
learning generalized policies two step classification builds domain specific relational decision trees capture action selected different planning contexts  work 
planning contexts specified helpful actions state  pending goals static
predicates problem  finally  explained exploit learned policies solve
classical planning problems  applying directly combining domain independent
heuristic lookahead strategy bfs algorithm  work contributes state of the art
learning based planning three ways 
   representation  propose new encoding generalized policies able capture
efficient dck using predicate logic  opposed previous works represent generalized
policies predicate logic  khardon         representation need extra background knowledge  support predicates  learn efficient policies blocksworld domain 
besides  encoding states set helpful actions frequently compact furthermore  set normally decreases search fewer goals left  thus  process
matching dck becomes faster search advances towards goals 
   learning  defined task learning generalized policy two step standard
classification task  thus  learn generalized policy off the shelf tool
building relational classifiers  results paper obtained tilde system  blockeel   de raedt         tool learning relational classifiers could
used  this  advances relational classification applied straightforward
manner roller learn faster better planning dck 
   

fis caling h euristic p lanning r elational ecision rees

   planning  explained extract action ordering h context policy
shown use ordering reduce node evaluations      algorithm depth first h context policy allows direct application h context policies 
    h context policy lookahead bfs  combines policy domainindependent heuristic within bfs algorithm  addition  included modified
version algorithm  roller   bfs   ha  considers helpful successors order
reduce number evaluations domains helpful actions good 
experimental results show approach improved scalability baseline heuristic
planners lama  winner ipc       variety ipc domains  effect
evident domains learned dck presents good quality  e g  blocksworld parking 
domains direct application learned dck saves large amounts node evaluations
achieving impressive scalability performance  moreover  using learned dck combination
domain independent heuristic bfs algorithm achieves good quality solutions 
quality learned dck poor  planning direct application policy fails
solve many problems  mainly largest ones difficult solve without reasonable
guide  unfortunately  current mechanism quantifying quality learned dck
evaluating set test problems  therefore  good compromise solution combining
learned dck domain independent heuristics 
domains  dck learned roller presents poor quality helpful context
able represent concepts necessary order discriminate good bad
actions  problem frequently arises arguments good action correspond
problem goals static predicates  plan study refinements definition
helpful context achieve good dck domains  one possible direction extending
helpful context subgoal information landmarks  hoffmann et al        
relaxed plan  moreover  use decision trees introduces important bias learning step 
algorithms tree learning insert new query tree produces significant
information gain  however  domains information gain obtained
conjunction two queries  finally  currently providing learner fixed
distribution training examples  near future  plan explore learner generate
convenient distribution training examples according target planning task proposed
fuentetaja borrajo        

acknowledgments
work partially supported spanish miciin project tin           c     
regional cam uc m project ccg   uc m tic      

   

fid e la rosa   j imenez   f uentetaja   b orrajo

appendix a  dck usefulness results

domains
blocksworld     
depots     
gold miner     
matching bw     
parking     
rovers     
satellite     
storage     
thoughtful    
tpp     
total

deph first
roller gr ha df ha
     
    
    
     
    
    
     
    
    
     
    
    
     
    
    
           
    
     
    
    
     
    
    
           
    
                 
                  

best first
roller bfs lh bfs
bfs
    
    
    
     
    
    
    
    
    
    
    
    
     
    
    
     
    
    
     
    
    
           
    
     
    
    
     
    
    
                  

helpful best first
roller bfs ha lh bfs ha bfs ha
    
    
    
     
    
    
    
    
    
    
    
    
     
    
    
     
     
    
     
    
    
     
    
    
     
    
    
     
    
    
      
     
     

table    problems solved dck usefulness evaluation 

domains
blocksworld     
depots     
gold miner     
matching bw     
parking     
rovers     
satellite     
storage     
thoughtful    
tpp     
total

deph first
roller
gr ha df ha
     
    
    
    
    
    
     
    
    
     
    
    
     
    
    
     
           
     
     
    
     
    
    
    
     
    
     
           
                   

best first
roller bfs
lh bfs
    
    
     
     
     
     
     
    
     
    
     
     
     
     
     
     
     
     
     
     
             

bfs
    
     
     
     
    
     
     
     
     
    
      

helpful best first
roller bfs ha lh bfs ha
bfs ha
    
    
    
     
     
     
     
    
    
     
    
     
     
    
    
     
     
     
     
     
     
     
    
    
     
     
     
     
     
     
      
             

table     quality scores dck usefulness evaluation 

references
bacchus  f     kabanza  f          using temporal logics express search control knowledge
planning  artificial intelligence                   
biba  j   saveant  p   schoenauer  m     vidal  v          evolutionary metaheuristic based
state decomposition domain independent satisficing planning  proceedings   th
international conference automated planning scheduling  icaps    toronto  on 
canada  aaai press 
blockeel  h     de raedt  l          top down induction first order logical decision trees 
artificial intelligence                   
   

fis caling h euristic p lanning r elational ecision rees

bonet  b   loerincs  g     geffner  h          robust fast action selection mechanism
planning  proceedings american association advancement artificial
intelligence conference  aaai   pp          mit press 
botea  a   enzenberger  m   muller  m     schaeffer  j          macro ff  improving ai planning
automatically learned macro operators  journal artificial intelligence research     
       
coles  a     smith  a          marvin  heuristic search planner online macro action learning  journal artificial intelligence research             
de la rosa  t   jimenez  s     borrajo  d          learning relational decision trees guiding heuristic planning  international conference automated planning scheduling
 icaps  
de la rosa  t   jimenez  s   garca duran  r   fernandez  f   garca olaya  a     borrajo  d 
        three relational learning approaches lookahead heuristic planning  working
notes icaps      workshop planning learning  pp       
de raedt  l          logical relational learning  springer  berlin heidelberg 
doherty  p     kvarnstrom  j          talplanner  temporal logic based planner  ai magazine 
             
dzeroski  s   de raedt  l     blockeel  h          relational reinforcement learning  international workshop ilp  pp       
emde  w     wettschereck  d          relational instance based learning  proceedings
  th conference machine learning  pp         
florez  j  e   garca  j   torralba  a   linares  c   garca olaya  a     borrajo  d          timiplan  application solve multimodal transportation problems  proceedings spark 
scheduling planning applications workshop  icaps   
fuentetaja  r     borrajo  d          improving control knowledge acquisition planning
active learning  ecml  berlin  germany  vol        pp         
gerevini  a   saetti  a     vallati  m          automatically configurable portfolio based planner
macro actions  pbp  proceedings   th international conference automated
planning scheduling  pp         thessaloniki  greece 
hoffmann  j     nebel  b          planning system  fast plan generation heuristic
search  journal artificial intelligence research             
hoffmann  j   porteous  j     sebastia  l          ordered landmarks planning  journal
artificial intelligence research     
khardon  r          learning action strategies planning domains  artificial intelligence      
       
   

fid e la rosa   j imenez   f uentetaja   b orrajo

leckie  c     zukerman  i          inductive learning search control rules planning  artificial
intelligence                
martin  m     geffner  h          learning generalized policies planning using concept languages  international conference artificial intelligence planning systems  aips   
martin  m     geffner  h          learning generalized policies planning examples using
concept languages  appl  intell          
mcallester  d     givan  r          taxonomic syntax first order inference  journal acm 
           
mcdermott  d          heuristic estimator means ends analysis planning  proceedings
 rd conference artificial intelligence planning systems  aips   pp          aaai
press 
minton  s          quantitative results concerning utility explanation based learning  artif 
intell                   
muggleton  s          inverse entailment progol  new generation computing             
muggleton  s     de raedt  l          inductive logic programming  theory methods  journal
logic programming             
nau  d   au  t  c   ilghami  o   kuter  u   murdock  w   wu  d     yaman  f          shop  
htn planning system  journal artificial intelligence research             
newton  m  a  h   levine  j   fox  m     long  d          learning macro actions arbitrary
planners domains  proceedings   th international conference automated
planning scheduling  icaps  
quinlan  j          induction decision trees  machine learning           
richter  s     westphal  m          lama planner  guiding cost based anytime planning
landmarks  journal artificial intelligence research             
roger  g     helmert  m          more  merrier  combining heuristic estimators satisficing planning  proceedings   th international conference automated planning
scheduling  icaps   pp         
veloso  m   carbonell  j   perez  a   borrajo  d   fink  e     blythe  j          integrating planning
learning  prodigy architecture  jetai              
vidal  v          lookahead strategy heuristic search planning  proceedings   th
international conference automated planning scheduling  icaps        whistler 
british columbia  canada  pp         
xu  y   fern  a     yoon  s  w          discriminative learning beam search heuristics
planning  ijcai       proceedings   th ijcai  pp           
   

fis caling h euristic p lanning r elational ecision rees

xu  y   fern  a     yoon  s          learning linear ranking functions beam search
application planning  journal machine learning research               
xu  y   fern  a     yoon  s          iterative learning weighted rule sets greedy search 
proceedings   th international conference automated planning scheduling
 icaps  toronto  canada 
yoon  s   fern  a     givan  r          learning heuristic functions relaxed plans  proceedings   th international conference automated planning scheduling  icaps  
yoon  s   fern  a     givan  r          using learned policies heuristic search planning 
proceedings   th ijcai 
yoon  s   fern  a     givan  r          learning control knowledge forward search planning 
j  mach  learn  res             
zimmerman  t     kambhampati  s          learning assisted automated planning  looking back 
taking stock  going forward  ai magazine            

   


