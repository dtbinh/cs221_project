journal of artificial intelligence research                 

submitted       published     

accelerating reinforcement learning by composing
solutions of automatically identified subtasks
chris drummond

school of information technology and engineering
university of ottawa  ontario  canada  k n  n 

cdrummon site uottawa ca

abstract

this paper discusses a system that accelerates reinforcement learning by using transfer
from related tasks  without such transfer  even if two tasks are very similar at some
abstract level  an extensive re learning effort is required  the system achieves much of its
power by transferring parts of previously learned solutions rather than a single complete
solution  the system exploits strong features in the multi dimensional function produced
by reinforcement learning in solving a particular task  these features are stable and easy
to recognize early in the learning process  they generate a partitioning of the state space
and thus the function  the partition is represented as a graph  this is used to index and
compose functions stored in a case base to form a close approximation to the solution of
the new task  experiments demonstrate that function composition often produces more
than an order of magnitude increase in learning rate compared to a basic reinforcement
learning algorithm 

   introduction
a standard reinforcement learning algorithm  applied to a series of related tasks  could learn
each new task independently  it only requires knowledge of its present state and infrequent
numerical rewards to learn the actions necessary to bring a system to some desired goal
state  but this very paucity of knowledge results in a slow learning rate  this paper shows
how to exploit the results of prior learning to speed up the process while maintaining the
robustness of the general learning method 
the system proposed here achieves much of its power by transferring parts of previously
learned solutions  rather than a single complete solution  the solution pieces represent
knowledge about how to solve certain subtasks  we might call them macro actions  precup 
sutton    singh         with the obvious allusion to macro operators commonly found in
artificial intelligence systems  the main contribution of this work is in providing a way of
automatically identifying these macro actions and mapping them to new tasks 
this work uses syntactic methods of composition much like in symbolic planning  but the
novelty arises in that the parts being composed are multi dimensional real valued functions 
these functions are learned using reinforcement learning as part of more complex functions
associated with compound tasks  the ecacy of this approach is due to the composition
occurring at a suciently abstract level  where much of the uncertainty has been removed 
each function acts much like a funnel operator  christiansen         so although individual
actions may be highly uncertain  the overall result is largely predictable 
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fidrummond

the subtasks are identified on the basis of strong features in the multi dimensional
function that arise during reinforcement learning  the features are not  in the world  
but in the system s interaction with the world  here   strong  means that the features are
stable  i e  relatively insensitive to variations in the low level learning process  and easy
to recognize and locate accurately early in the learning process  one important aspect of
these features is that they largely dictate the shape of the function  if the features differ by
a small amount  one would expect the function to differ by a small amount 
the features generate a partitioning of the function  a popular technique in object
recognition  the snake  kass  witkin    terzopoulus        suetens  fua    hanson        
is used to produce this partition  in object recognition  the snake produces a closed curve
that lies along the boundary of an object  as defined by edges in an image  in this application  the snake groups together sets of features to define a region of the function  the
boundary of the region is a low order polygon  demarcating an individual subtask  this
is repeated until the whole function is covered  the polygons are converted into discrete
graphs  a vertex of the polygon becoming a node of the graph  merging these graphs
produces a composite graph representing the whole task 
the composite graph is used to control the transfer by accessing a case base of previously
learned functions  the case base is indexed by graphs  the relevant function is determined
by matching a subgraph of the composite graph with one acting as an index to a case  the
associated functions are transformed and composed to form a solution to the new task  this
is used to reinitialize the lower level learning process  it is not necessary for the transfer to
produce an exact solution for the new task  it is sucient that the solution is close enough
to the final solution often enough to produce an average speed up  reinforcement learning
will further refine the function and quickly remove any error 
this paper demonstrates the applicability of transfer in two different situations  in the
first  the system learns a task for a particular goal position and then the goal is moved 
although the function itself will change significantly  the partition generated on the initial
task can be used to compose the function for the new task  in the second situation considered  the system is placed in a different environment within the same domain  here  a new
partition has to be extracted to control the composition process 
this paper unifies and significantly extends previous work by the author  drummond 
             additional work has largely focussed on removing some of the limitations
inherent in the partitioning approach introduced in drummond         one limitation of
the original approach was that the snake could only extract polygons that were rectangles 
this paper relaxes this restriction  allowing it to be applied to a different environment
within the same domain and to a different task domain  although lifting this restriction
removes some desirable bias  the experiments demonstrate that none of the ecacy of the
original system is lost  further  the results are more broadly obtained on the larger set of
related tasks and in a different domain  overall  the function composition approach often
produces more than an order of magnitude increase in learning rate when compared to a
basic reinforcement learning algorithm 
the rest of the paper begins with section   giving a very high level discussion of the
approach taken  section   gives a more in depth discussion of the techniques used  sections   and   present and analyze the experimental results  subsequent sections deal with
limitations and related research 
  

fiaccelerating reinforcement learning

   an overview
the intent of this section is to appeal to the intuitions of the reader  leaving much of the
detail to later sections in the paper  the subsections that follow will demonstrate in turn 
that there are features in the function produced by reinforcement learning  that graphs
based on these features can be used to control the composition of the function pieces  that
these features are easy to detect early in the learning process  that these features exist in
multiple domains 

    features in the reinforcement learning function
this overview begins with a very high level introduction to reinforcement learning and the
function it produces  it will show that there are features in this function which can be
extracted and converted into a graphical representation 
one of the experimental test beds used is this paper is a simulated robot environment of
different configurations of interconnected rooms  the robot must learn to navigate eciently
through these rooms to reach a specified goal from any start location  figure   shows one
example with   rooms and the goal in the top right corner  the robot s actions are small
steps in any of eight directions  as indicated by the arrows  here  the location  or state 
is simply the robot s x and y coordinates  the thin lines of figure   are the walls of the
rooms  the thick lines the boundary of the state space 
  
goal

robot
  

y

  

  
  

  
  

  

  

  
x
  

  

figure    robot navigating through a series of rooms
  

fidrummond

if each action is independent of preceding actions  the task becomes one of learning
the best action in any state  the best overall action would be one that takes the robot
immediately to the goal  but this is only possible in states close to the goal  suppose
the robot is in a particular state and that the number of steps to goal from each of its
neighboring states is known  indicated by the numbered squares surrounding the robot in
figure    then a one step look ahead procedure would consider each step and select the
one that reaches the neighboring state with the shortest distance to goal  in figure   the
robot would move to the state    steps from goal  if this process is repeated  the robot will
take the shortest path to goal  in practice we must  of course  learn such values  this can
be done using some type of reinforcement learning  watkins   dayan        sutton       
which progressively improves estimates of the distance to goal from each state until they
converge to the correct values 
          

          

           o

i
           

           
           

figure    the value function obtained using reinforcement learning
the function shown in figure   is called the value function  subsequently  the term
function will mean the value function unless otherwise indicated  the function is the result
of reinforcement learning on the problem of figure    but instead of it representing the
actual distance to goal  it represents essentially an exponential decay with distance to goal 
the reasons for this will be made clear in section      the shaded areas represent large
gradients in the learned function  comparing this to the environment shown in figure    it
is apparent that these correspond to the walls of the various rooms  these are the strong
features discussed in this paper  they exist because of the extra distance for the robot
to travel around the wall to reach the inside of the next room on the path to the goal 
these features are visually readily apparent to a human  so it seems natural to use vision
processing techniques to locate them 
an edge detection technique called a snake is used to locate these features  the snake
produces a polygon  in this instance a rectangle  locating the boundary of each room  the
doorways to the room occur where the differential of the function  along the body of the
snake  is at a local minimum  the direction of the differential with respect to edges of
  

fiaccelerating reinforcement learning

the polygon  associated with the walls of the room  determines if it is an entrance or an
exit  a positive gradient into the room indicates an entrance  a positive gradient out of
the room indicates an exit  from this information  a plane graph  labeled with an  x  y 
coordinate for each node  is constructed  figure   shows one such example  for the room
at the top left corner of the state space  subsequent graphs will not show the coordinates 
nodes corresponding to the doorways are labeled  i  or  o  for in and out respectively 
their positions on the function are indicated by the dashed arrows 

    composing function pieces

this overview continues by showing how the graphs  extracted from the features in the
function learned by reinforcement learning  can be used to produce a good approximation
to the solution for a new goal position  the left hand side of figure   shows plane graphs
for all the rooms  ignore the dashed lines and circles for now   the node representing the
goal is labeled  g   a directed edge is added from  i  to  o  or  i  to  g   as appropriate 
associated with this edge is a number representing the distance between the nodes  this is
determined from the value of the function at the points of the doorways  each individual
graph is then merged with its neighbor to produce a graph for the whole problem  the right
hand side of figure    the doorway nodes have been relabeled to  d   the composite
graph represents the whole function  each individual subgraph represents a particular part
of the function  this information is stored in a case base  each subgraph is an index and
the corresponding part of the function is the case 

o

g

i

extract
graphs

g

i

merge
graphs

g

o

d

g

i
d
o

d

i
o

d

figure    graphical representation
now suppose the goal is moved from the top right corner to the top left corner of the
state space  reinforcement learning in its most basic form would be required to learn the
new function from scratch  in this work if the goal is moved  once the new goal position
  

fidrummond

is known  the node representing the goal can be relocated  the new goal position is shown
as the dashed circle in figure    the edges connecting the doorways and the goal are
changed to account for the new goal position  the dashed lines representing the new edges
replace the arrows in the same subgraph  to produce a new function  the idea is to regress
backwards from the goal along these edges  for each edge  the small subgraph containing
the edge is extracted  the extracted subgraph is used to index the case base of functions 
the retrieved function is transformed and added to the appropriate region of the state space
to form the new function 
rotate
stretch

rotate
i

g

stretch

i

figure    function composition
in this example  some of the existing subgraphs match the new configuration  the two
that do not are the subgraph originally containing the goal and the subgraph now containing
the goal  it is certainly possible to exchange these two  using an appropriate transform 
but other graphs in the case base may better match the new task  the best match for
the subgraph containing the new goal is  in fact  the subgraph for the goal in the original
problem  to fit this to the new task  the plane graph is rotated and stretched slightly in
the new x direction by changing the coordinates of its nodes  see figure    then this same
transformation is applied to the function  but for the room containing the original goal  a
case obtained when solving another task is a better match  the other three rooms use the
functions from the original problem  since changing the goal position has little effect on the
actions taken  in fact  only the height of the functions must be changed  this is simply
a multiplication by a value representing the distance to goal from the  o  doorway  this
is discussed in detail at the end of section       because the matching of the subgraphs
allows some error and asymmetric scaling may be used  the resulting function may not be
exact  but as the experiments will demonstrate  the function is often very close and further
reinforcement learning will quickly correct any error 
  

fiaccelerating reinforcement learning

the new position of the goal must be established before the graph can be modified and
function composition can occur  the system is not told that the goal has moved  rather it
discovers this by determining that it is no longer at the maximum of the existing function 
there is some uncertainty in the exact boundary of the original goal  the robot may reach
a state which it believes is part of the original goal region  but fail to detect it even if
the goal has not moved  to be reasonably certain that the goal has in fact moved  this is
required to occur ten times with no intervening occurrence of the goal being detected at
the maximum 
the system then composes a search function  by assuming a particular room contains
the goal  search functions are also produced by composing previously learned functions 
however  for the room assumed to contain the goal the function is a constant  this does
not bias the search to any particular part of the room and allows some limited learning to
encourage exploration of the room  the search function drives the robot into the room from
anywhere else in the state space  if it fails to find the goal after a fixed number of steps 
a new search function is composed with another room assumed to contain the goal  this
process is repeated until the goal has been located ten times  this ensures a good estimate
of the  center of mass  of the goal  the  center of mass  is used as the new position of
the goal node in the composite graph  requiring that the old goal or new goal positions
are sampled a fixed number of times has proven to be effective in the domains discussed in
this paper  nevertheless  it is a somewhat ad hoc procedure and will be addressed in future
work  discussed in section     

    detecting features early
in the previous section  the existing task and the new task were strongly related  the walls
and doorways were fixed and only the goal position was different  in this section  no such
relationship is assumed  the robot is faced with a brand new task and must determine
what  if any  relationship exists between the new task and any previous tasks 
the experimental testbed is again a simulated robot environment  but this time the
problem is simplified to just an inner rectangular room and an outer l shaped room  figures
  and   show two possible room configurations  again  the thin lines are the walls of the
room  the thick lines the boundary of the state space  suppose the robot had already
learned a function for the  old task  of figure    we would hope that we could adapt the
old solution to fit the closely related  new task  of figure   
the steps  in this example  are essentially those in the previous one  but now as the
learning process is started afresh  there are no features and the system must wait until they
emerge through the normal reinforcement learning process  then we can proceed much as
before  first a graph for the inner room is extracted  the best matching graph in the
case base from the old task is rotated and stretched to fit the new task  next a matching
graph for the outer l shaped room is rotated and stretched around the larger inner room 
the same transforms are then applied to the associated functions  any height adjustments
carried out and the functions composed to form an approximate solution to the new task 
in this example  the first step in the process is to locate the goal  as there is no
partition to aid the search  the initial value function is set to a mid range constant value
 see figure     this allows some limited learning which encourages the system to move
  

fidrummond

goal

robot

outer
room

inner
room

robot

inner
room

outer
room

goal

figure    the old task

figure    the new task

away from regions it has explored previously  to prevent a completely random walk through
state space  once the goal is located  the learning algorithm is reinitialized with a function
for the same goal position but no walls  see figure     if such a function does not exist in
the case base  any rough approximation could be used instead  the  no walls  function is
not used exactly as stored in the case base  the difference between the goal and the rest of
the state space is reduced by scaling the function then adding a constant  this reduces the
 bias  of the function  allowing the learning algorithm to alter it relatively easily as new
information becomes available 

figure    start function

figure    intermediate function

figure   shows the resultant function about      exploratory steps from the beginning
of the learning process  again  the large gradients associated with the walls are readily
  

fiaccelerating reinforcement learning

apparent  figure    shows the function for the new task if it had been allowed to converge
to a good solution  both functions have roughly the same form  the large gradients are
in the same position  although learning the latter took some         steps  after the  no
walls  function is introduced the features take some time to clearly emerge  the snake will
typically filter out features that are too small and not well formed  additional filtering at
the graphical level further constrains acceptable features  the total set of features must
produce a consistent composite graph  the doorways from different subgraphs must align
and the graph must overlay the complete state space  there must also be a matching case
in the case base for every subtask  many of these checks and balances will be removed when
the iterative updating technique of section     is incorporated 

figure    early function

figure     new task function

    a different task domain

the previous sections dealt with a simple robot navigation problem  this section demonstrates that these features also exist in a quite different domain  that of a two degrees of
freedom robot arm  as shown in figure     the shoulder joint can achieve any angle between  radians  the elbow joint any angle between    radians  zero is indicated by the
arrows  if the arm is straight and the shoulder joint rotated  the elbow joint will describe
the inner dotted circle  the hand the outer dotted circle  there are eight actions  small
rotations either clockwise or anti clockwise for each joint separately or together  the aim
is to learn to move the arm eciently from any initial position until the hand reaches the
goal on the perimeter of the arm s work space 
the state space  for the purposes of reinforcement learning  is the configuration space
for the arm  sometimes called the joint space  see figure      the x axis is the angle of the
shoulder joint  the y axis the elbow joint  the eight actions when mapped to actions in the
configuration space become much like the actions in the robot navigation problem  as shown
by the shaded diamond  labeled arm  in figure     to map an obstacle in the work space
to the configuration space  one must find all pairs of shoulder and elbow angles blocked by
the obstacle  the obstacles in this space become elongated to form barriers much like the
  

fidrummond

   

shoulder

hand

 

elbow angle

obstacle

  
  
  
  
  
  
  
  
  
  

g
o
a
l

       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       

arm

 

obstacle

elbow
 

  


obstacle
  
  
  
  
  
  
  
  
  
  

figure     work space

       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       

obstacle

g
o
a
l

 
shoulder angle

 

figure     configuration space

walls in the experiments of the previous sections  if this is not clear  imagine straightening
the arm in the work space and rotating it such that it intersects one of the obstacles  the
middle dotted line in figure     the arm can then be rotated at the shoulder joint with a
roughly linearly proportional rotation in the elbow joint  but in the opposite direction  such
as to keep it intersecting the obstacle  this produces the  wall  in the configuration space 
this linearity holds only for small objects not too far from the perimeter of the work space 
more complex  larger objects  would result in more complex shapes in the configuration
space  at the moment the feature extraction method is limited to these simpler shapes 
this will be discussed further in section   
the reinforcement learning function produced by this problem is shown in figure     as
before the features are shaded for clarity  the large gradient associated with the obstacle
on the left hand side of the configuration space can be clearly seen  there is a similar large
gradient associated with the obstacle on the right hand side of the configuration space 
again  these features can be used to control the composition of functions if the goal is
moved or for a different task in the same domain 

   details of the techniques used
this section will discuss in more detail the techniques used  these include  reinforcement
learning to produce the initial function  snakes to extract the features producing the graph 
and the transformation and composition of the subgraphs  and their corresponding functions  to fit the new task 

    reinforcement learning

reinforcement learning typically works by refining its estimate of expected future reward 
in goal directed tasks  such as the ones investigated here  this is equivalent to progressively
  

fiaccelerating reinforcement learning

figure     the robot arm function
improving the estimate of the distance to goal from each state  this estimate is updated
by the best local action  i e  the one moving the robot  or arm  to the new state with the
smallest estimated distance  early in the learning process  only states close to the goal are
likely to have accurate estimates of true distance  each time an action is taken  the estimate
of distance at the new state is used to update the estimate at the old state  eventually this
process will propagate back accurate estimates from the goal to all other states 
rather than directly estimating pthe distance to goal  the system uses the expected
t
discounted reward for each state e    
t    rt    the inuence of rewards  rt   are reduced
progressively the farther into the future they occur by using a  less than one  in this work 
the only reward is for reaching the goal  so the farther the state is from the goal the smaller
the value  the use of an expectation allows the actions to be stochastic  so when the robot 
or arm  takes a particular action in a particular state  the next state is not always the same 
to carry out reinforcement learning  this research uses the q learning algorithm  watkins
  dayan         this algorithm assumes the world is a discrete markov process  thus both
states and actions are discrete  for each action a in each state s  q learning maintains a
rolling average of the immediate reward r plus the maximum value of any action a  in the
next state s   see equation     the action selected in each state is usually the one with the
highest score  but to encourage exploration of the state space  this paper uses an  greedy
policy  sutton        which chooses a random action a fraction  of the time  the only
effect that function composition has on the q learning algorithm is that the initial value
for each state action pair is set to some value other than zero 
   
qts a          ff qts a   ff r   maxa qts  a  
the q function over state and action is usually referred to as the action value function 
in this paper  it is the action value function that is transformed and composed to form a
solution to the new task  the value function  discussed in previous sections and shown in
 

  

 

 

fidrummond

the figures  is the maximum value of the q function  it is used to generate the partition
and associated graphs needed to control the process 
watkins and dayan        proved that q learning will converge to the optimal value
with certain constraints on the reward and the learning rate ff  the optimal solution is produced by taking the action with the greatest value in any state  so  for goal directed tasks 
a greedy algorithm will take the shortest path to the goal  once learning is complete  the
extension to continuous spaces may be done using function approximation  the simplest
method  and the one used here  is to divide the state dimensions into intervals  the resulting action value function has cells representing the average q value of taking each action
from somewhere within a region of the state space  in off line learning  where any action
in any state can be executed  this representation has been proven to converge  gordon 
       in on line learning  where the current state is determined by the environment  this
approach is generally successful  but there exists no proof of its convergence 

    feature extraction
feature extraction uses a vision processing technique that fits a deformable model called
a snake  kass et al         to edges in an image  after initializing the snake  the process
iterates until external forces  due to the edges  balance internal forces in the snake that
promote a smooth shape  here  the external forces are due to steep gradients in the value
function  as a piecewise constant function approximator is used  a smoothed cubic b spline
is fitted to the value function and used to generate the necessary derivatives  the left hand
side of figure    is the gradient of the value function shown in figure   when extracting
features early in the learning process  the system has added a gradient around the border
to represent the state space boundary 
to locate the features  a curve is found that lies along the ridge of the hills  a local
maximum in the differential  on the right hand side of figure     the dashed lines are
contour lines for the small inner room as indicated  the bold lines  on the right hand side
of figure     are the snake at different stages of the process  the snake is first positioned
approximately in the center of the room  the innermost circle  it is then expanded until
it abuts on the base of the hills  now to simplify the exposition  we can imagine that the
snake consists of a number of individual hill climbers spread out along the line representing
the snake  indicated by the small white circles  but instead of being allowed to climb
independently  their movement relative to each other is constrained to maintain a smooth
shape  when the snake reaches the top of the ridge  it is further constrained to be polygon
  in this instance a quadrilateral   the outside dark line in figure     at this point  it
will tend to oscillate around an equilibrium position  by limiting the step size the process
can be brought into a stationary state  a more detailed mathematical treatment of this
approach is given in appendix a 
the polygon forms the  skeleton  for the graph  as shown at the top left of figure    
nodes in a graph correspond to vertices of the polygon and to the doorways and the goal 
looking at the gradient plot  the doorways are regions with a small differential between
the ridges  their locations can be determined from the magnitude of the gradient along
the boundary of the polygon  in this example  a node is added for the goal  labeled g 
and this is connected to the  in  doorway  labeled i   the polygon delimits a region of
  

fiaccelerating reinforcement learning

graph

g

polygon

i

doorway

figure     the gradient and resultant polygon  left  extracted by the snake  right 
the state space  and therefore a region of the action value function  this becomes a case
in the case base  and the corresponding graph its index  constraining the snake to be a
polygon is done for two reasons  firstly  the vertices are needed to produce nodes in the
plane graphs  which are important part of the matching process  secondly  the additional
constraint results in a more accurate fit to the boundaries of the subtask  this  in turn 
results in a more accurate solution after function composition 

      three extensions to the snake approach
this section introduces three extensions to the basic snake approach to facilitate the extraction of features 
the first extension affects the direction the snake moves when hill climbing the gradient 
in normal hill climbing  each step is taken in the direction of steepest ascent  the step size
being determined by the size of the differential  roughly  this translates into forces at points
along the body of the snake  each force points in the direction of steepest ascent locally  but
interacts with other forces through the various shape constraints  looking at the gradient
function and contour lines of figure     there is a steep slope leading to the top of each
ridge  but there is also a significant slope along each ridge away from the doorway towards
the boundary of the state space  thus the force on a single point on the body of the snake
  

fidrummond

is not directly towards the top of the ridge but turned towards its apex  as indicated by the
bold black arrow on the left hand side of figure    
snake

steepest
ascent
tangent
normal

figure     controlling forces on the snake
this force can be broken into two components with respect to the snake  a normal and
a tangential force  the latter force acts along the body of the snake  once the shape is
constrained to be a quadrilateral  this will cause the relevant side to shrink  this effect will
be partially counteracted by the force towards the top of the ridge on the adjacent side of
the quadrilateral  but the net result will be a shrinking of the two sides associated with the
ridges inwards until the forces are balanced  this will push the corner of the quadrilateral
near the doorway inwards  as indicated by the thin black arrow in figure     in an extreme
case  this might cause the snake to collapse into something close to a triangle  but the more
likely outcome will just be a degradation of the accuracy of registration of the ridges 
drummond        prevented this degradation of the accuracy by restricting the snakes
to rectangular shapes  but with the weakening of this constraint to more general polygons 
this effect again becomes a problem  this problem is addressed by removing the component
of the force tangential to the snake  then hill climbing is always in the direction of the
normal  this does not significantly restrict the motion of the snake  all that is being removed
is the component along the body of the snake  thus it mainly prevents the stretching and
shrinking of the snake due to the gradient 
the second extension controls the way the snake is expanded to reach the base of the
hills  drummond        used a ballooning force  as introduced by cohen and cohen        
but problems arose when extending the system to deal with more general shapes than
rectangles  such as the outer l shaped room in figure    the ballooning force expands the
snake in directions normal to its body  one deleterious effect of this is if the snake contacts
a sharp external corner  such as that of the inner room  the force tends to push the snake
through the corner  this can be seen in figure     the bold continuous lines are the snake 
the bold dashed lines are the ridges  if we imagine starting off with a circular snake in the
  

fiaccelerating reinforcement learning

middle of the l shaped outer room  by the time it reaches the walls of the inner room the
sides of the snake are roughly perpendicular to the ridges  thus there is little to restrain
the expansion of the snake and it passes completely through the walls of the inner room 
ridge

ballooning
force

ridge

figure     using the ballooning force
the approach adopted here is analogous to the ow of mercury  if we imagine starting
somewhere in the middle of the l shaped room and progressively adding mercury  it would
tend to fill up the lower regions of the valley first and reach the bases of the hills roughly at
the same time  the analogy of mercury is used as it has a high surface tension preventing
it from owing through small gaps in the edges associated with doorways  to increase the
effectiveness of this idea  the absolute value of the differential of the gradient is thresholded 
values above the threshold being set to one those below to zero  it is then smoothed with
a truncated gaussian  as shown in figure     smoothing and thresholding are commonly
used techniques in machine vision  tanimoto         they are typically used to remove
noise  but here the aim is to strongly blur the thresholded image  this produces bowls
associated with each room  in this example  the smoothing has almost completely obscured
the presence of the doorway  although this is generally not the case 
the snake is initialized as a small circle at the minimum of one of these bowls  this
is shown as the circle in the middle of figure     where the dashed lines are the contour
lines of this function  it then ows outwards  so as to follow the contour lines of the bowl 
the largest component of the ow being in the direction of the arrows in figure     this
is achieved by varying the force normal to the body of the snake according to the height
difference with the average height of the snake  thus points along the snake which are
higher than average tend to get pushed inwards  those lower pushed outwards  the surface
tension of the mercury is produced by various smoothing constraints on the first and second
differentials of the snake  see appendix a  
the third extension limits changes in the shape of the snake as it expands from its initial
position to reach the base of the hills  the smoothness constraints on the snake  that give
the mercury like properties  prevent the snake owing through the gaps associated with the
  

fidrummond

figure     smoothed function

figure     mercury flow

doorways  but even this proved insucient if the width of the rooms and the width of
doorways were of similar sizes  in figure     looking at the  room  on the left hand side
of the configuration space of the robot arm  the  doorway  and the  room  at the top are
of similar width  increasing the surface tension of the mercury suciently to prevent ow
through the doorways also prevents the ow to the top of the room 
the solution is to limit the amount the snake can change its shape as it grows  this
is achieved by constraining how much the second differential of the snake can change from
step to step  in figure     it is apparent that the snake takes up a good approximation
to the shape of the room some time before it reaches the ridges  if the shape can be
locked in before reaching the ridges  the problem just described can be avoided  when
the snake is initialized  the only constraint is smoothness  as the snake is expanded  this
smoothness constraint is progressively weakened and the curvature constraint progressively
strengthened  this progressively locks in the shape while still allowing the snake to make
small local adjustments to better fit the features 
the extensions  discussed in this section  either modify the traditional forces that act
on the snake or add new ones  there are also forces associated with knot spacing and drag 
how the snake moves  with each iteration  depends on the vector addition of these forces 
the sum acts to accelerate the body of the snake which has both mass and velocity  and
therefore momentum  a schematic representation of these forces is shown in figure     a
more detailed mathematical description is given in appendix a  the dashed line represents
the body of the snake  the arrows are the forces applied to one point on the body  the snake
is a parameterized function  given by f  s     x s   y s   where x s  and y s  are individual
cubic b splines giving the x and y coordinates associated with a variable s along the body
of the snake  the circles represent points equi distant in s but not necessarily in x and y 
these points are kept roughly the same euclidean distance apart in x and y due to the knot
spacing force  the momentum  although not strictly a force  encourages the point to move
  

fiaccelerating reinforcement learning

in constant direction  the drag opposes any motion  the stiffness encourages the snake to
maintain a smooth shape  the overall stiffness is reduced as the snake grows  to keep its
exibility per unit length roughly constant  and is also controlled locally to maintain its
shape 
steepest ascent

mercuryflow
momentum

knot spacing

drag
stiffness

figure     the forces on the snake
the following is an algorithmic summary of the processing of the snake 

 initialize the coecients to produce a circular snake in the middle of a room 
 iterate until the forces are roughly in equilibrium and the snake oscillates around a
stationary value 

 modify the stiffness to enforce the polygonal constraints
 iterate for a further    steps increasing the momentum and drag at each step to reduce
the oscillation to a small value 

 use the final position of the snake to form the polygon that delimits the boundary of
the room 

    transformation

this section discusses the matching process   how a subgraph is used to locate and transform
a function from the case base  the matching process first finds all subgraphs in the case base
isomorphic to the extracted subgraph and all possible isomorphic mappings between their
nodes  using a labeling algorithm  macdonald         the number of isomorphic mappings
  

fidrummond

is potentially exponential in the number of nodes  here  the graphs typically have only
a few nodes and a few symmetries  so only a few isomorphic mappings  associated with
each node of a subgraph is an  x  y  coordinate  an ane transform  equation    is found
that minimizes the distances between the coordinates of the mapped nodes for each of the
isomorphic subgraphs  the advantage of this transform is its relative exibility while having
a simple form 

x    c  x   c  y   c  y    c  x   c  y   c 
   
ideally the transformed nodes would be positioned exactly over the mapped nodes  but
this is not usually possible  even with simple rectangular shapes  the case base may not
contain a graph with exactly the same doorway positions  using a graph that is not an
exact match will introduce some error in the composed function for the new task  by
weighting some nodes more than others where the error occurs can be controlled  one aim
is to minimize the introduction of errors that affect the overall path length  however  of
equal importance is that the errors introduced be easily correctable by normal reinforcement
learning 
 

 

   

 

 

 

figure     weighting graph nodes
the left hand side of figure    shows the composite graph for the new task  the right
hand side shows the result of overlaying it with a graph from the case base  if the fit at the
doorway of the outer l shaped room is in error  the robot will tend to miss the doorway
and collide with the wall on one side  the farther the doorway is out of position  the longer
normal reinforcement learning will take to correct the error  to encourage a good fit at
the doorway  a weight of   is used  nodes adjacent to the doorway are given a weight of   
all other nodes have a weight of one  this is based on the intuition that more trajectories 
from different parts of the state space  will be pass through the region close to the doorway 
any error here is likely to have a broader effect  and take longer for normal reinforcement
  

fiaccelerating reinforcement learning

learning to correct  than in regions far from the doorway  so the fit around the inner room
is improved by sacrificing fit far from the doorway 
the exact position of the doorway in the inner room is not critical and its weight is
set to      whatever the position of the doorway  the shape of the function will be correct
inside the room as the goal is also in this room  however  the further the doorway is from
its correct position  the greater the error in the edge length  this will produce some error
in the composed function  but again the expectation is that this error will be small and
reinforcement learning will quickly correct it 
not only should the fit be good  but we would also prefer that the amount of transformation be small  all transforms produce some error and this is particularly true of
asymmetric scaling  as discussed later in this section  generally the transform produces
translation  reection  rotation  shearing and independent scaling in each dimension  in
the robot navigation domain  the distance between points in the state space is just the
normal euclidean distance  the reinforcement learning function is an exponential decay
in the distance to goal  if the transformation does not change the euclidean distance  the
transformed function should be directly applicable 

affine  similar  symmetric
   
the ane transformation is just one family in a hierarchy of transformations  at the
bottom of this hierarchy  shown in equation    are the symmetric transformations  these
solid body transformations do not change the euclidean distance  the next step up in
the hierarchy introduces scaling  equal in each dimension  this will affect the euclidean
distance but only by a multiplicative factor  thus the only change needed to the transformed
function is to scale the height  the ane transformations allow the addition of asymmetric
scaling and shear  which will distort the euclidean distance  to determine the amount
of distortion  the transformation is applied to the unit circle  the symmetric  rigid body 
transformations will not alter the circle  but the other transformations will  the symmetric
scaling transform just changes the diameter of the circle  the asymmetric scaling and shear
transformations change the circle into the ellipse  the amount of distortion of the euclidean
distance introduced by the transform can be determined by the ratio of lengths of the major
and minor axes of the ellipse 
error   sqrt
 p
wi 
x    yi      node misalignment 
fi  i
fi i

fi
 euclidean distortion 
  log  fifi rrmaj
   
minfi
 

j
r
 
r
j
maj
min
       log 
 scaling factor 
 
the error of fit of the transformed subgraph can be combined with the transformation
error using the lengths of the major and minor axes  rmaj and rmin respectively  of the
ellipse  there is a penalty for euclidean distortion from asymmetric scaling and shear 
the log factor is added directly to the error of fit as shown in equation    log factors are
used  so that the penalty functions are symmetric  there is a small penalty for symmetric
scaling  once the best matching subgraph has been found  the same transformation can be
applied to the associated function  if no isomorphic graph is found with a total error less
than      a constant function will be used as a default  where the new graph overlays the
old graph  values are assigned by using bilinear interpolation on the discrete values of the
  

fidrummond

function  where it does not  bilinear extrapolation is used  based on the closest values  in
both cases once the four values are selected  the value for the new point is calculated as
shown in equation    as the action value function is indexed by action as well as state  this
process is carried out for each action in turn  any rotation or reection in the transform
is also applied to a predefined matrix of actions  this produces the necessary mapping of
actions from the original to the new action value function 

v   c  x   c  y   c  xy   c 

   

finally  the height of the new action value function must be adjusted to account for the
change in overall distance to goal  the height of the value function at an  out  doorway is
 dg where dg is the distance to goal and  the discount factor  the value at some random
point within the room is  dg dd where dd is the distance to the doorway  the action value
function is first normalized by dividing by  dg   the height of the function at the doorway
in the original problem  it is then multiplied by  dng   where dng is the distance to the
new goal  the value of the point becomes  dng dd   scaling will also affect the height of the
function  assuming the scaling is symmetric then the new value function for anywhere in
the room will be  cdd where c is the scale factor  thus raising the function to the power of c
i e    dd  c will account for scaling  when scaling is symmetric the result is exact  assuming
distance is based on the linear combination of the two dimensions  with asymmetric scaling 
the result is not exact  but if the difference between the two scale factors is relatively small 
it is a useful approximation to use their maximum 
the following is an algorithmic summary of the whole matching process 

 sg   subgraph extracted from the new task 
 for each subgraph g acting as an index to the case base

  for each isomorphic mapping of g to sg
 find minimum weighted least squares fit of g to sg using mapping
 ane transform   coecients of least squares fit
 penalized fit   least squares error   transform penalty
 keep graph and transform with lowest penalized fit
 retrieve function associated with best graph from case base  if none use default 
 apply ane transform to function
 apply bilinear interpolation extrapolation
 adjust function height
 add new function to existing function
  

fiaccelerating reinforcement learning

    composition

this section describes function composition  how the transformation is applied successively
to the series of subgraphs extracted from the composite graph  function composition uses a
slightly modified form of dijkstra s algorithm  dijkstra        to traverse the edges between
doorway nodes  the left hand side of figure    shows the composite graph after moving
the goal in the robot navigation example of section      the right hand side shows the
graph traversed by dijkstra s algorithm 

g

d 

g

d

d 

d

gr 

gr 

d

d

d 

d

gr 
d

d

gr 

d

gr 

figure     using dijkstra s algorithm
to begin the process  the subgraph which contains the goal is extracted and the best
matching isomorphic subgraph is found  the edge lengths in the composite graph are
then updated using the scaled length of the corresponding edge in the matching isomorphic
subgraph  d  and d  in figure     as d  is less than d   the next subgraph extracted 
gr   is the one sharing the doorway node with the edge of length d   the best matching
isomorphic subgraph is found and the edge of length d  updated  the shortest path is
again determined  as d  is less than d    d  subgraph  gr  is extracted  the process is
repeated until all subgraphs have been updated  at each stage when a subgraph is matched 
the corresponding transformed function is retrieved and added to the new function in the
appropriate region 
in this example  there is only a single path to the goal from each room  often there will
be multiple paths  suppose room   had an additional doorway in the lower left corner of
the room  labeled  b  on the left hand side of figure     in addition to the original doorway
labeled  a   the graph  shown on the right hand side of figure     would result  there are
now two possible paths to the goal of lengths d  and d   if the length across room    d   is
greater than the absolute difference between d  and d   the choice of path from this room
will be determined by a decision boundary inside the room  this is produced by taking the
  

fidrummond

    
         room  
      
  
a
  
       
       
 
room    
    
       room  
   
      
   
room    
 
room  

d 
a
n 

d 

d 

n 

gr 
b

b

n 

figure     multiple paths to goal
maximum of two functions as shown in figure     one for entering by doorway  a  and
leaving by doorway  b   one for entering by doorway  b  and leaving by doorway  a  
this principle can be repeated if there are more than two paths to the goal from a given
room 
if the cross room distance  d   had been smaller than the difference  jd  d j  the decision
boundary would have to be in another room  in general  we want to find the room in which
the cross room distance is larger than the difference between the incident paths  this is
repeated for every cycle in the path graph  a cycle is detected when a node is visited twice 
indicating that it is reachable by two separate paths  let us suppose this is node n  in the
graph of figure     as dijkstra s algorithm is being used  we know that all previous nodes 
on either path  such as n  and n  are already closed  this must be true for both paths
to have reached n   all the rooms on paths up to these nodes cannot contain the decision
boundary  so it must be in either room   or    to decide which remaining room it is in  we
compare the two path lengths  if d  is longer than d    d  then the decision boundary will
be in room    otherwise it will be in room   
whichever room is selected  the decision boundary is produced from the maximum of two
functions  the heights of the two functions  when adjusted for their path lengths  determine
where the decision boundary occurs within the room  if the paths are of equal length  taking
the maximum will correctly put the decision boundary at the doorway  if there are no such
functions in the case base  functions that already include decision boundaries may be used 
this technique produces a correct decision boundary if the difference in the path lengths
entering the room is less than the difference between the heights of the function at the  out 
doorways  on the left hand side of figure    there is a room with two doorways  as path
  is significantly longer than path    the decision boundary is far to the left  the shortest
path to the goal from most of the room is via the right hand doorway  if this function is
combined with a mirror image of itself  it will produce a decision boundary in the middle
  

fiaccelerating reinforcement learning

maximum
decision
boundary

figure     combining two functions

decision
boundary

path  
path 

room

figure     decision functions

  

fidrummond

of the room  as shown on the right hand side of figure     this could be used for the new
problem shown on the left hand side of figure    where the two paths are the same length 
again the heights of the two functions can be changed to move the decision boundary  but
it cannot be moved to anywhere in the room  the decision boundary can be moved no
closer to a particular doorway than in the original function shown in figure   
decision
boundary
path  

path 

room

figure     combining decision functions

   experiments

this section compares learning curves for function composition and a simple baseline algorithm  four sets of results are presented  one for each of the two types of related task
in each of the two domains  the learning curves represent the average distance to goal as
a function of the number of actions taken during learning  the distance is averaged over
   different start positions  distributed uniformly throughout the state space  and over the
different experimental runs  to determine this distance  normal learning is stopped after a
fixed number of actions and a copy of the function learned so far is stored  one of the   
start positions is selected  learning is restarted and the number of actions needed to reach
the goal is recorded  if a trial takes      actions and has not yet reached the goal  it is
stopped and the distance to goal recorded as       the function is then reinitialized with
the stored version and another start state selected  this is repeated    times  then the
function is reinitialized once more and normal learning resumed 
the baseline algorithm and the underlying learning algorithm for the function composition system is the basic q learning algorithm  using a discrete function approximator as
discussed in section      the learning rate ff is set to      the greedy policy uses an  of    
 the best action is selected     of the time   the future discount  is     and a reward of    
is received on reaching the goal  although the state spaces for the different domains represent two quite different things   the robot s hx  yi location and the angle of the arm s two
joints   the actual representation is the same  the state space ranges between   for each
dimension  a step is      in each dimension either separately or together  giving the eight
possible actions  the actions are stochastic  a uniformly distributed random value between
      being added to each dimension of the action  in the robot navigation examples if
  

fiaccelerating reinforcement learning

the robot hits the wall  it is positioned a small distance from the wall along the direction of
its last action  this has not been implemented for the robot arm as it is a somewhat more
complex calculation  instead  if a collision with an obstacle occurs the arm is restored to
its position before taking the action 
learning begins at a randomly selected start state and continues until the goal is reached 
then a new start state is selected randomly and the process repeated  this continues until
the requisite total number of actions is achieved  speed up is calculated by dividing the
number of learning steps at one specific point on the baseline learning curve by the number
of learning steps at an equivalent point on the function composition system s learning curve 
the knee of the function composition system s curve is used  this occurs where the low
level learning algorithm is initialized with the composed function  this is compared to the
approximate position of the knee of the baseline curve 

    robot navigation  goal relocation
the first experiment investigates the time taken to correct a learned function when the goal
is relocated in the robot navigation domain  there are nine different room configurations 
as shown in figure     the number of rooms varying from three to five and there are four
different goal positions  each room has one or two doorways and one or two paths to the
goal  to initialize the case base  a function is learned for each of these configurations with
the goal in the position shown by the black square  the rooms are generated randomly 
with some constraints on the configuration of the rooms and doorways  a room can not
be too small or narrow  a doorway can not be too large  the case base also includes
functions generated for the experiments discussed in section      this was necessary to
give a sucient variety of cases to cover most of the new tasks  even with this addition 
not all subgraphs are matched  constant valued default functions are used when there is
not a match  this reduces speed up significantly  but does not eliminate it altogether 

 

 

 

 

 

 

 

 

 

figure     the different suites of rooms
  

fidrummond

once the case base is loaded  the basic q learning algorithm is rerun on each room
configuration with the goal in the position shown  after         steps the goal is moved 
this is denoted as time t on the x axis of figure     the goal is moved to one of the
three remaining corners of the state space  a task not included in the case base  learning
continues for a further         steps  at fixed intervals  learning is stopped and the average
number of steps to reach the goal is recorded  the curves in figure    are the average of
   experimental runs  three new goal positions for each of the nine room configurations 
function composition

average no  of steps to goal

q learning
q learning  no reinit 

  

  

 

 

 

  
t        t    

t   

t

t   

t    

t    

t    

t    

t    

t   no  of learning steps x     

figure     learning curves  robot navigation  goal relocation
the basic q learning algorithm  the top curve of figure     performs poorly because 
when the goal is moved  the existing function pushes the robot towards the old goal position 
a variant of the basic algorithm reinitializes the function to zero everywhere on detecting
that the goal has moved  this reinitialized q learning  the middle curve  performed much
better  but it still has to learn the new task from scratch 
the function composition system  the lowest curve  performed by far the best  the
precise position of the knee of this curve is dicult to determine due to the effect of using
default functions  if only those examples using case base functions are considered  the knee
point is very sharp at about      steps  the average number of steps to goal at      steps 
for all examples  is     the non reinitialized q learning fails to reach this value within
        steps giving a speed of over      the reinitialized q learning reaches this value
at about         steps  giving a speed up of about     function composition generally
produces accurate solutions  even if some error is introduced  further q learning quickly
refines the function towards the asymptotic value of about     after about         steps 
  

fiaccelerating reinforcement learning

normal q learning reaches an average value of    steps and then slowly refines the solution
to reach an average value of    after         steps 

    robot arm  goal relocation
the second experiment is essentially a repeat of the first experiment but in the robot arm
domain  the initial number of steps  before the goal was moved  was reduced to        
to speed up the experiments  as the arm has only two degrees of freedom  and with the
restrictions discussed in section      the number of variations is small  so only three obstacle
configurations were used  constructed by hand  with two obstacles in each  to increase the
number of experiments  to allow for greater statistical variation  each configuration was
repeated with the goal in each of three possible positions  as shown in figure     the
black diamonds represent the obstacles  the black rectangles the goal  solutions to all these
tasks were loaded into the case base  when composing a function  however  the system is
prevented from selecting a case that comes from the same goal and obstacle configuration 

 

 

 

 

 

 

 

 

 

figure     the robot arm obstacle and goal positions
the curves in figure    are the average of    experimental runs  two new goal positions
for each of the three original goal positions in the three obstacle configurations shown in
figure     there are only two learning curves  non reinitialized q learning being dropped 
as in the first experiment  the function composition system  the lower curve  performed
much better than q learning  the knee of the function composition system occurs at     
steps  the knee of q learning at        steps  giving a speed up of     in this experiment 
the case base contained subgraphs that matched for all new tasks  so default functions were
not needed  the composed functions tend to be very accurate and little further refinement
is necessary 
  

fidrummond

function composition

average no  of steps to goal

q learning

  

  

 

 

 

  
t        t    

t   

t

t   

t    

t    

t    

t    

t    

t   no  of learning steps x     

figure     learning curves  robot arm  goal relocation

    robot navigation  new environment
the third experiment investigates the time taken to learn in a new  but related  environment
in the robot navigation domain  nine different inner rooms were generated randomly  again
under some constraints  all have a single doorway  but the size and position of the room
and the location of the doorway are varied as shown in figure     to initialize the case base 
a function is learned for each of these configurations with the goal inside the small room as
indicate by the dark square  learning is then repeated on each of the room configurations
in turn  however  when composing the new function the system is prevented from selecting
a case learned from the same goal and room configuration  experimental runs for the qlearning algorithm and the function composition system are initialized with a at function
of zero and      everywhere respectively  denoted as zero on the x axis  learning continues
for         steps  to improve the statistical variation  experiments for each configuration
were repeated three times  each time with a new random seed  the curves in figure    are 
therefore  the average across    experimental runs 
the top curve is the q learning algorithm  the bottom curve the function composition
system  for these experiments  locating the goal took typically between     and      steps 
although some took      steps  the function composition system then introduces the  no
walls  function and typically a further     to      steps are taken before usable features are
generated  again  certain experimental runs took longer  this will be discussed in section
     due to these runs  the knee of the function composition system s curve occurs at       
steps  the knee of the basic q learning curve occurs at approximately        steps giving
  

fi  

  

  

 

 

 

 

 

 

 

 

 

 

  

  

  

  

  

q learning

  

  

function composition

accelerating reinforcement learning

 

 

  

figure     the single rooms

 

  

no  of learning steps x     

  

   

figure     learning curves  robot navigation  new environment

average no  of steps to goal

fidrummond

a speed up of      as in previous experiments once initialized the function is very accurate
and little further refinement is necessary  basic q learning  on reaching the knee  takes a
long time to remove the residual error 

    robot arm  new environment

  

  

  

 

 

 

 

 

  

  

 

  

  

 

  

  

q learning

  

  

function composition

figure     the different obstacle positions

  

no  of learning steps x     

figure     learning curves  robot arm  new environment
  

   

the fourth experiment is essentially the same as the third experiment except in the robot
arm domain  here  three  hand crafted  configurations of a single obstacle with the goal in
a fixed position were used  as shown in figure     to increase the statistical variation each
configuration was run five times with a different random seed  the curves in figure    are
therefore the average across    experimental runs 

average no  of steps to goal

fiaccelerating reinforcement learning

the top curve of figure    is the q learning algorithm  the bottom curve the function
composition system  the knee of the function composition system s curve occurs at about
     steps  the knee of the basic q learning algorithm at about        steps giving a speed
up of about    

   analysis of results

the experiments of the previous section have shown that function composition produces
a significant speed up across two different types of related task and across two domains 
in addition  the composed solutions tend to be very accurate and little further refinement
is required  this section begins by looking at possible concerns with the experimental
methodology that might affect the measurement of speed up  it then discusses various
properties of the task being solved that affect the speed up achieved by using function
composition 

    possible concerns with the experimental methodology

the speed up obtained using function composition is suciently large that small variations
in the experimental set up should be unlikely to affect the overall result  nevertheless  there
are a number of concerns that might be raised about the experimental methodology  some
will be  at least partially  addressed in this section  others will be the subject of future work 
the first concern might be how the estimated value of speed up is measured  the
value represents the speed up of the average of a set of learning tasks  rather than the
average of the speed up in each of the tasks  one of the diculties of estimation  with
curves for single tasks  is that the average distance to goal may oscillate up and down as
learning progresses  even though the general trend is downwards  this makes judging the
position of the knee of the curves dicult  and any estimate of speed up questionable  even
experimental runs using the same configuration  but with different random seeds  exhibit a
considerable variation  in some instances  the speed up measured on individual curves may
benefit the function composition system  in others  the baseline algorithm  nevertheless 
probably overall most of these effects will cancel out 
the second concern might be the effect on speed up of the limit of      steps when
measuring the distance to the goal  comparing two averages of values limited in this way
is sometimes misleading  gordon   segre         but this limit primarily affects only the
baseline algorithm  and was only significant when the goal was moved and the function not
reinitialized  estimation of speed up is principally concerned with comparing the position
of the knees of the different curves  here  the average distance to goal is relatively small 
so limiting the value is likely to have little effect 
the third concern might be that the value of speed up is dependent on the configuration
of the baseline algorithm  certainly  it is the experience of this author that the way the
function is initialized  and how actions are selected  can have an impact on the speed of
learning  in previous work  drummond         the function was initialized to a constant
value of       a technique termed  optimistic initial values  by sutton and barto        
tie breaking between actions of the same value was achieved by adding a small amount of
noise  circa           it was expected that this would increase exploration early on in
the learning process and speed up learning overall  however  using an initial value of zero
  

fi 

 

 

 

  

drummond

   

   

   

   

   

and a strict tie breaker  randomly selecting amongst actions with the same value  turned
out to produce a significant speed up in the baseline learning algorithm  this configuration
was used for the preceding experiments  but on one experimental run this caused serious
problems for the baseline algorithm 

  

  

  

no  of learning steps x     

figure     learning curves in a partially observable domain

  

the upper learning curve of figure    is for the baseline algorithm  for one run when
the goal was moved in the robot arm domain  as it had such a large impact on the average
learning curve  it was replaced by the lower curve  produced by repeating the experiment
with a different random seed  this very slow learning rate arises from the interaction
of the partial observability of the robot arm domain with the use of an initial value of
zero  individual cells of the function approximator straddle the obstacles allowing a  leakthrough  of value from one side of the obstacle to the other  starting with a zero value 
once an action receives some value it will remain the best action for some time  continual
update of this action will decrease the value  but it can only asymptotically approach zero 
until other actions for the same state are updated  it will always be selected as the greedy
action  this did not occur for higher initial values  it may be that in domains where there is
some degree of partial observability  small initial values are better than zero or some means
of improving exploration for very small values might be necessary 
other variations in the parameters of the baseline algorithm have not been explored
in this paper  for instance  a constant learning rate of     was used  alternatives  such
as starting with a higher rate and reducing it as learning progresses might also improve
the overall speed of the baseline algorithm  some preliminary experiments were  however 

average no  of steps to goal

fiaccelerating reinforcement learning

carried out using undiscounted reinforcement learning  the discounting being strictly unnecessary in goal directed tasks  room configuration   of figure     with the goal in the
lower right hand corner  was used as the experimental task  the discounting  discussed in
section      is turned off by setting  to    in addition  the value on reaching the goal state
is set to zero and a cost is associated with every action  this form of learning simplifies
function composition  normalization procedures needed to compensate for the value function s exponential form being no longer required  with normalization disabled  the snake
successfully partitioned the function  the most critical part of the process  however  the
baseline learner took considerably longer to learn the function than in the discounted case 
with discounting  the learner reached an average distance to goal of about    steps after
       learning steps  without discounting  the learner reached an average of     steps at
the same point in time and only an average of    steps after         learning steps  the
action value function was initialized to zero  which appears to be the standard practice in
the literature  however  the experience with initialization in the discounted case suggests
this might be the part of problem and this will be investigated in future work 
the baseline q learning algorithm used is the most basic and a more sophisticated one
would unquestionably reduce the speed up experimentally obtained  for instance  some
form of reinforcement learning using eligibility traces  singh   sutton        might be
used  for the experiments when the goal was moved  a baseline such dyna q   sutton 
      which was specifically designed to deal with changing worlds would probably be a
better reference point 
the speed up obtained  by transferring pieces of an action value function  has also not
been compared to alternatives  such as transferring pieces of a policy or transferring pieces
of a model  transferring pieces of a policy would reduce memory requirements and not
require the rescaling applied to pieces of an action value function  it does  however  have
two disadvantages  firstly  a solution can not be directly composed  as the position of
decision boundaries can not be determined  further learning would be necessary to decide
the appropriate policy for each room  secondly  the policy only indicates the best action 
the action value function orders the actions  indicating potentially useful small changes
to the policy which might improve the accuracy on a new task  transferring pieces of a
model  would require first learning a model consisting of a probability distribution function
for each action in each state  the memory requirement is considerably larger  unless the
states reachable by an action are limited beforehand  nevertheless  a model would need
less modification in a changing world  such as when the goal is moved  it also carries
more information which might speed up learning  the action value function seems a good
compromise in terms of complexity versus information content  but this would need to be
empirically validated and is the subject of future work 

    performance variation with task configuration
generally  function composition outperforms the baseline learning algorithm by an amount
dependent on the complexity of the learning problem  in the robot navigation domain
when the goal was moved  the amount of speed up increased with more rooms and fewer
paths to goal  a speed up of     against an average speed up of     was obtained on the
configurations with five rooms and a single path to goal  configurations with only three
  

fidrummond

rooms had the least speed up  but this was not only due to the relative simplicity of the
problem 

  

  

  

 

 

 

 

  

 

   

q learning

function composition

no  of learning steps x     

 

  

figure     failure in robot navigation moving goal

 

   

the top of figure    shows the average of four learning curves for the three room
configurations  the bottom of figure    shows one of the configurations that produced
these curves  not only is it one of the easiest tasks  from the experimental set  for the
baseline algorithm  but also there are no solutions in the case base for the lowest room 
there are no isomorphic subgraphs of this form  rather than not composing a solution 
the system introduces a constant value function for this room  this room represents almost
half the state space  so much additional learning is required  as the top of figure    shows 
initially there is significant speed up  further refinement reduces the advantage and for
a short while the baseline algorithm is better  but later  function composition gains the
upper hand and converges more quickly than the baseline algorithm towards the asymptotic
value 

average no  of steps to goal

fiaccelerating reinforcement learning

in the robot navigation domain when learning a new task  the amount of speed up varied
with the size of the inner room  this was primarily due to the number of actions needed
before the features emerged with sucient clarity for the snake to locate them  function
composition is most successful when the inner room is small  if a wall is long  the feature
takes more time to develop  more refinement by q learning is needed to make it apparent 
very short walls are also hard to identify  the likelihood of the robot colliding with them
is small and it takes many exploratory actions for the features to emerge clearly 
the features may be suciently clear for the snake to form a partition  yet not be well
enough defined to precisely locate the doorways  a doorway may appear to be a bit wider
than it actually is  more importantly  it may appear to be displaced from its true position 
typically  the error in the composed function is small and normal reinforcement learning
quickly eliminates it  in one of the experimental runs  configuration   in figure     the
speed up was reduced by a factor of   due to the doorway being incorrectly positioned 
the feature representing the lower wall had not completely emerged when the partition
was generated  this made the doorway appear to be almost exactly at the corner  the
algorithm  in fact  positioned the doorway just on the wrong side of the corner  this resulted
in the significantly reduced speed up  but it is unclear why reinforcement learning took
so long to correct what seems  on the surface at least  to be a local error  this will be
investigated in future work 

   limitations

limitations come in   roughly  two kinds  those arising from the overall approach and those
arising from the way it was implemented  in the former case  ways to address these limitations may be highly speculative  or impossible without abandoning some of the fundamental
ideas behind the approach  in the latter case  there is a reasonable expectation that future
work will address these limitations  the following sections will deal with these cases in
turn 

    limitations in the approach

to explore the possible limitations in the approach  this section reviews the fundamental
assumptions on which it is based 
it is a fundamental assumption that features arise in the reinforcement learning function
that qualitatively define its shape  the features used in this paper are the violation of a
smoothness assumption  that neighboring states have very similar utility values  a wall  by
preventing transitions between neighboring states  typically causes such a violation  other
things  such as actions with a significant cost  would have a similar effect  smaller  and
much more varied costs  will not generate the features required by this approach  so it offers
little in the way of speed up in these cases  if there is a mixture of large and small costs 
it is expected that the system will capture features generated by the former  initialize the
function and normal reinforcement learning will address the latter 
the smoothness assumption is less clear if the dimensions are not numeric  the neighborhood relation  used here  is a predefined distance metric over a continuous space  in
nominal  binary or mixed domains it is not obvious how such a metric would be defined 
although there is some work on such metrics for other applications  osborne   bridge 
  

fidrummond

       if the dimensions are mixed  feature location might be limited to the continuous
ones  if the dimensions are purely nominal or binary  a generalization of the snake may be
appropriate  the snake is  at an abstract level  a constrained hill climber  but whether or
not this idea would usefully generalize in this way is at present somewhat speculative 
it is a fundamental assumption that the features clearly delimit subtasks  in the domains discussed in this paper  the obstacles and walls subdivide the state space into regions
connected by small  doorways   the subtask of reaching one doorway is not greatly affected
by the subsequent subtask  in other domains this may not be the case  as the doorways
become larger  the context sensitivity increases  as long as the composed solution is reasonably accurate  reinforcement learning can easily correct the error although speed up will
be reduced  at some point however  due to a very large amount of context sensitivity  the
advantage of dividing the task into subtasks will become questionable  it would be possible
to account for some of the context dependency in the graph matching stage  looking at
larger units than subgraphs  if two adjacent subgraphs match the new problem  they might
be used as a pair  thereby including any contextual relationship between them  even if
single subgraphs were used  the context in which they appear  i e  the shape of neighboring
subgraphs  could be taken into account  in the limit  graph matching the whole task might
be used  but  as was argued in the introduction  this would considerably limit when transfer
is applicable  and thus its overall effectiveness 
it is a fundamental assumption that the absolute position of the features is unimportant  it is the shape of the delimited region that matters  to increase the likelihood of
transfer  solutions to subtasks have been subjected to a variety of transformations  in
some domains  many  if not all  of these transformations will be invalid  if actions cannot
be rotated or reected  or if many small costs affect different regions of the state space 
the effectiveness of transfer will be reduced  this would be  to some extent  addressed by
additional penalties for different transformations  but again this would limit the opportunities for transfer  which transformations are appropriate  and whether or not this can be
determined automatically from the domain  will be the subject of future research 
it is a fundamental assumption that a vision processing technique can locate these
features in a timely fashion  even in very high dimensional domains  learning in very high
dimensional domains is likely to be slow whatever technique is used  normal reinforcement
learning will take time to navigate the much larger space  slowing down the emergence of
the features  although the time taken to partition the function will increase  the frequency
with which partitioning is applicable will decrease  thus the amortized cost will rise more
slowly  further  as high dimensional spaces are generally problematical  methods such as
principal components analysis and projection pursuit  nason        can be used to reduce
dimensionality  it may prove in practice that the dimensionality which is important  and is
the focus of feature extraction  is much smaller than the actual dimensionality of the space 

    limitations in the implementation
if the assumptions of the previous section are met  it is expected that the remaining limitations are due to the present implementation  these limitations are likely to become
apparent when the system is applied to other domains  certainly other domains may differ
from those presented in this paper in a number of ways 
  

fiaccelerating reinforcement learning

a domain may differ in that the dimensionality of the space is higher than the two
dimensions of the tasks investigated in this paper  the implementation of the snake has
been updated to work in higher dimensions  the bold lines at the top of figure    are
one of the simpler tasks from the robot navigation domain  the task has been extended in
the z dimension  the snake starts out as a sphere and then expands outwards until it fills
the room  in this example  the polygonal constraint has not been used  but everything else
remains the same  figure    shows the complete partition of the task 

figure     adding a z dimension

figure     the complete  d partition

the mathematics behind the snake is not limited to three dimensions  there also seems
to be nothing in principle that would prevent other processes such as graph matching 
planning or transformation from working in higher dimensions  speed is the main problem 
this is not a problem unique to this approach and there is a large body of research addressing
this issue  for instance  although graph matching is in general np complete  there is
much active research in speeding up matching on the average or in special cases  gold  
rangarajan        galil         at present  the snake represents the principal restriction
on speed  this is an issue of great importance to the vision processing community  current
research is investigating this problem  at least in two or three dimensions  one example is
hierarchical methods  schnabel        leroy  herlin    cohen        which find solutions
for the snake at progressively finer and finer resolution scales  the results of such research
will undoubtedly be of importance here 
a domain may differ in that the value function learned might not produce features locatable by the snake with the present parameter settings  the values of the parameters
were empirically determined  using hand crafted examples from the robot navigation and
the robot arm domains  the obvious danger is that the parameters might be tuned to these
examples  to demonstrate that this is not the case  configurations for the experiments in
the robot navigation domain were generated randomly  as configurations for the robot arm
domain are more tightly constrained  the hand crafted examples were used in the experiments  nevertheless  the experiments have shown that the parameters worked successfully
for random examples in the robot navigation domain  the same parameters also work successfully in the second domain  the robot arm  the following discussion demonstrates that
  

fidrummond

they are also reasonably effective in a quite different domain  the  car on the hill   it is
anticipated that using the results of current research into snakes will automate the selection
of many parameters 
in the  car on the hill domain  moore         the task  simply stated  is to get a car
up a steep hill  figure     if the car is stationary part way up the hill  in fact anywhere
within the dotted line  then it has insucient acceleration to make it to the top  so the
car must reverse down the hill and then achieve sucient forward velocity  by accelerating
down the other side  before accelerating up the hill  the state space  for the purposes of
reinforcement learning  is defined by two dimensions  these are the position and velocity
of the car  as shown in figure     the goal is to reach the top of the hill with a small
positive or negative velocity  in this domain there are two possible actions  accelerate
forward  accelerate backwards  unlike in previous domains  there is no clear mapping of
the actions onto the state space  the state achieved on applying an action is determined by
newton s laws of motion  as the car has insucient acceleration to make it up the hill from
everywhere in the state space  a  wall  is effectively introduced  the bold line in figure    
to reach the top of the hill  the car must follow a trajectory around this  wall   the dashed
line in figure    
goal

velocity

 ve

goal

 

y

cit

lo
ve

 ve

 

position

figure     the car on the hill

position

figure     car state space

figure    shows the reinforcement learning function  it exhibits the same steep gradient
as the other domains  the important point to note is that  unlike in the other domains 
no physical object causes this gradient  it is implicit in the problem itself  yet the features
still exist  figure    shows the partition produced when applying the snake to the  car on
the hill  domain  the main difference from the previous examples is that the polygonal
constraint has not been used  when the snake initially comes to rest  the mercury force
is turned off and then the snake is allowed to find the minimum energy state  it was also
necessary to reduce the scaling of the edges  by about a factor of three quarters  to achieve
the accuracy of fit  the fit around the top left corner of the second snake  the dashed line 
  

fiaccelerating reinforcement learning

also has some problems  the snake is growing very slowly downwards and is  at present  only
stopped because it has reached the maximum number of iterations allowed  one diculty
in this example is that there is not such clear delimitation of the upper and lower regions
at the end of the feature  future work will investigate altering the stopping condition to
eliminate this problem 

figure     the steep gradient

figure     the regions extracted

a domain may differ in that the shape of various regions in the partition is more complex than can be dealt with by the present snake  fitting the snake to the task discussed in
the previous paragraphs goes some way towards mitigating that concern  nevertheless  the
randomly generated examples of section     were subject to certain constraints  configurations with narrower rooms were tried informally  but the snake did not reliably locate the
features  the configurations in section   represent the limit of the complexity of partition
the snake can produce at present  it is expected that using ideas from the large body of
already published research into snakes will go a long way towards addressing this limitation 
for complex regions  locating all the subtleties of the underlying shape may be unnecessary 
or even undesirable  the aim is to speed up low level learning  as long as the solution is
reasonably accurate  speed up should be obtained  being too sensitive to minor variations
in shape may severely limit the opportunities for transfer and thus reduce speed up overall 
a domain may differ in that changes in the environment are more complex than those
investigated in this paper  at present  the system detects that the goal has moved by
counting how often a reward is received at the old goal position  not only is this a rather
ad hoc approach  but it also does not account for other possible changes  such as paths
becoming blocked or short cuts becoming available  at present  when learning a new task
the system is restarted and is not required to determine that its present solution is no longer
applicable  in future work  the system should decide when its model of the world is no longer
correct  it should also decide what  if any  relationship there is to the existing task and
how it might be best exploited  this will allow a more complex interaction of the function
composition system with reinforcement learning  for instance  the learning of a new task for
  

fidrummond

the robot navigation domain used the relatively simple situation of two rooms  the function
composition system initialized the low level algorithm once on detecting suitable features  in
the future  to address more complex tasks  with many more rooms  an incremental approach
will be used  when a new task is being learned  the system will progressively build up a
solution by function composition as different features become apparent 
this approach also should handle any errors the system might make with feature extraction  in the experiments with these simple room configurations  the filtering discussed
in section     proved sucient to prevent problems  but in more complex tasks  it is likely
that false  doorways  will be detected  simply because the system has not explored that
region of the state space  a composed function including that extra doorway will drive the
system into that region  it should then become quickly apparent that the doorway does not
exist and a new function can be composed 

   related work
the most strongly related work is that investigating macro actions in reinforcement learning  precup  sutton and singh              propose a possible semantics for macro actions
within the framework of normal reinforcement learning  singh        uses policies  learned
to solve low level problems  as primitives for reinforcement learning at a higher level  mahadevan and connell        use reinforcement learning in behavior based robot control 
to learn a solution to a new task  all these systems require a definition for each subtask
and their interrelationships in solving the compound task  the work presented here gives
one way that macro actions can be extracted directly from the system s interaction with
its environment  without any such hand crafted definitions  it also shows how to determine
the interrelationships of these macro actions needed to solve the new task  thrun s research        does identify macro actions  by finding commonalities in multiple tasks  but
unlike the research presented here  no mapping of such actions to new tasks is proposed 
hauskrecht et al         discuss various methods of generating macro actions  parr       
develops algorithms to control the caching of policies that can be used in multiple tasks 
but in both cases  they need to be given a partitioning over the state space  it is the
automatic generation of just such a partition that has been the focus of much of the work
presented in this paper  it may well be that this approach to generating partitions and to
determining the interrelationships between partitions of related tasks will prove useful to
this other work 
another group of closely connected work is the various forms of instance based or case
based learning that have been used in conjunction with reinforcement learning  they have
been used to address a number of issues      the economical representation of the state
space      prioritizing states for updating and     dealing with hidden state  the first issue
is addressed by peng        and by tadepalli and ok        who use learned instances
combined with linear regression over a set of neighboring points  sheppard and salzberg
       also use learned instances  but they are carefully selected by a genetic algorithm  the
second issue is addressed by moore and atkeson        who keep a queue of  interesting 
instances  predecessors of those states where learning produces a large change in values 
these are updated most frequently to improve the learning rate  the third issue is addressed
by mccallum      b  who uses trees which expand the state representation to include prior
  

fiaccelerating reinforcement learning

states  removing ambiguity due to hidden states  in further work  mccallum      a  uses
a single representation to address both the hidden state problem and the general problem
of representing a large state space by using a case base of state sequences associated with
various trajectories  unlike this other research  in the work presented here the case is not
an example of the value function during learning  instead  it is the result of a complete
learning episode  so the method should be complementary to these other approaches 
this work is also related to case based planning  hammond        veloso   carbonell 
       firstly through the general connection of reinforcement learning and planning  but
it is analogous in other ways  when there is a small change to the world  such as the
goal being moved  a composite plan is modified by using sub plans extracted from other
composite plans 
last  but not least  is the connection with object recognition in vision research  suetens
et al         chin   dyer         in the work presented here  many of the methods   if not
the final application   has come from that field  the features in the reinforcement learning
function are akin to edges in an image  these are located by finding the zero crossing point
of the laplacian as introduced by marr         in the work presented here  it was proposed
that the features largely dictate the form of the function  mallat and zhong        have
shown that a function can be accurately reconstructed from a record of its steep slopes 

   conclusions

this paper described a system that transfers the results of prior learning to significantly
speed up reinforcement learning on related tasks  vision processing techniques are utilized
to extract features from the learned function  the features are then used to index a case
base and control function composition to produce a close approximation to the solution of
a new task  the experiments demonstrated that function composition often produces more
than an order of magnitude increase in learning rate compared to a basic reinforcement
learning algorithm 

acknowledgements
the author would like to thank rob holte for many useful discussions and help in preparing
this paper  this work was in part supported by scholarships from the natural sciences and
engineering research council of canada and the ontario government 

appendix a  spline representations

this appendix presents some of the underlying mathematics associated with spline representations and the snake  it is not meant to be an introduction to the subject  rather it
is added for completeness to discuss certain important aspects of the system not addressed
elsewhere in this paper  knowledge of these aspects is not necessary to understand the basic
principles of the approach discussed in this paper  but would be necessary if one wanted
to duplicate the system  more detailed explanation is given in drummond         some
specific papers that address these ideas in much greater detail are  for splines  terzopoulos 
      and for snakes  cohen   cohen        leymarie   levine        
  

fidrummond

splines are piecewise polynomials where the degree of the polynomial determines the
continuity and smoothness of the function approximation  additional smoothing constraints
can be introduced by penalty terms which reduce the size of various differentials  one way
then to view spline fitting is in the form of an energy functional such as equation   

espline f    

z

r





efit  f     esmooth  f   ds

   

here  there is an energy associated with the goodness of fit  some measure of how close
the approximating function is to the input function  this is typically the least squares
distance between the functions  there is an energy associated with the smoothness of the
function  two very commonly used smoothness controls produce the membrane and thin
plate splines by restricting the first and second differentials of the function respectively  to
fit the spline to the function  the total energy must be minimized  a necessary condition
for this is an euler lagrange differential equation such as equation    here  t controls the
tension in the spline  the resistance to stretching  and  s the stiffness  the resistance to
bending   often the error function will be based on individual data points and the left hand
side of equation   would include delta functions 
 

  

      s    f  s            s    f  s      f  s    f  s 
   s
t
in
 s
 s  s  s 

   

in this work  such splines have been used for a number of purposes  when fitting the
snake  measures of the first and second differential are needed  a two dimensional quadratic
spline is fitted to the discrete representation of the maximum q values  an  t of     is used
  s is zero  to limit overshoot  drummond        to prevent false edges  values from an
identical spline except using an  t of     are squared and then divided into the differential
values  this normalizes the differentials  so that the size of edges is not dependent on where
they occur in the function  the same type of spline is used to produce the bowls associated
with the rooms as discussed in section        here  t is     and  s is     giving roughly
gaussian smoothing  the values used to produce this function are weighted  values close
to one are given weights of      lower values a weight of    this prevents the sides of the
bowls from collapsing under smoothing 
a one dimensional cubic spline is used in locating the doorways  these are found by
steepest descent on the value of the differential along the body of the snake  this differential
contains many local minima not associated with doorways  these arise either from the
inherent noise in the process or from errors of fit in the snake  the aim is to remove the
ones not associated with doorways by smoothing and thresholding  this is achieved by
first sampling the gradient at points along the snake  the values are then normalized to lie
between zero and one  the spline has an  t of        s of       here a weighted least mean
squares fit is used  the weighting function is the inverse square of the values  preventing
the spline from being overwhelmed by large values  starting points for steepest descent are
changes in the sign of the coecients of the gradient of the spline  the initial step size
is set to slightly larger than a knot spacing and then decreased over time  when a local
minimum is found if the value exceeds a threshold  of       it is rejected 
to represent the snake  the model of the spline must be changed somewhat  the snake
itself is a one dimensional cubic spline  but the energy minimum that is being sought is
   

fiaccelerating reinforcement learning

in the differential of the qmax function  subject to other constraints  the dynamics of the
snake are defined by the euler langrange equation shown in equation   
  f   
  f  
  f 
     
 
f
 
 
 
 
  t      t    t  s   c s   s     s   tp  s   s    f  f  

   

an  c of     minimizes changes to the snake s shape as it grows  by penalizing the
difference in the second differential to the previous time step scaled by the ratio of their
lengths  an  s of     is the initial stiffness of the snake  this is reduced proportionately to
the snake s length to give the spline more degrees of freedom  a  of    and a  of    control
the momentum and the drag on the snake respectively  as in cohen and cohen         a
factor is added to the energy associated with the differential that is in the direction normal
to the body of the snake  as shown in equation    but instead of it being a constant  a
variable is used to produce the mercury model discussed in section       
 
f  f     m  f    
n  s    r   fifirqmax f  fifi    
n  s 
fi

fi

   

the energy minimization process is carried out iteratively interleaving steps for the x
and y directions  the differential of r jqmax j  for the x direction is given by equation    
a similar equation is used for the y direction 
 
  qmax
 qmax        qmax  
 
 
 
    jrq xmax j         q xmax       x
 
 y
 x y
 

 

    

the snake grows under the forces of the mercury model until it reaches an approximately
stable position  subject only to small oscillations  it is then converted into a polygon by
 where n               the
finding the corners  where the normal passes through   n   
 
coecient    is set to zero everywhere  the coecient    is set to zero at the corners and
   between them  this produces a polygon which is exible at its vertices 
to detect the features as early as possible in the learning process  as discussed in section
     the height of the gradient is scaled according to the signal to noise ratio  the noise
arises from variations in the low level learning process and the stochastic nature of the task 
both the size of the features and the noise grow with time and are somewhat normalized by
this scaling process  the idea is to collect uniformly sampled values of the function shown
in equation    for both the x and y directions and find the median of their absolute values 
the median is not strongly affected by extreme values and thus largely ignores the size of
the features  measuring only the noise of the regions in between 

references
chin  c  h     dyer  c  r          model based recognition in robot vision  computing
surveys                 
christiansen  a  d          learning to predict in uncertain continuous tasks  in proceedings
of the ninth international workshop on machine learning  pp        
   

fidrummond

cohen  l  d     cohen  i          finite element methods for active contour models and
balloons for   d and   d images  ieee transactions on pattern analysis and machine
intelligence                     
dijkstra  e  w          a note on two problems in connexion with graphs  numerische
mathematik             
drummond  c          preventing overshoot of splines with application to reinforcement
learning  computer science technical report tr        school of information technology and engineering  university of ottawa  ottawa  ontario  canada 
drummond  c          using a case base of surfaces to speed up reinforcement learning 
in proceedings of the second international conference on case based reasoning  vol 
     of lnai  pp          
drummond  c          composing functions to speed up reinforcement learning in a changing world  in proceedings of the tenth european conference on machine learning 
vol       of lnai  pp          
drummond  c          a symbol s role in learning low level control functions  ph d 
thesis  school of information technology and engineering  university of ottawa  ottawa  ontario  canada 
galil  z          ecient algorithms for finding maximum matching in graphs  acm
computing surveys                
gold  s     rangarajan  a          a graduated assignment algorithm for graph matching 
ieee transactions on pattern analysis and machine intelligence                  
gordon  g  j          stable function approximation in dynamic programming  in proceedings of the twelfth international conference of machine learning  pp          
gordon  g  j     segre  a  m          nonparametric statistical methods for experimental evaluations of speedup learning  in proceedings of the thirteenth international
conference of machine learning  pp          
hammond  k  j          case based planning  a framework for planning from experience 
the journal of cognitive science                  
hauskrecht  m   meuleau  n   boutilier  c   kaelbling  l  p     dean  t          hierarchical
solution for markov decision processes using macro actions  in proceedings of the
fourteenth conference on uncertainty in artificial intelligence  pp          
kass  m   witkin  a     terzopoulus  d          snakes  active contour models  international journal of computer vision             
leroy  b   herlin  i  l     cohen  l  d          multi resolution algorithms for active
contour models  in proceedings of the twelfth international conference on analysis
and optimization of systems  pp        
   

fiaccelerating reinforcement learning

leymarie  f     levine  m  d          tracking deformable objects in the plane using
an active contour model  ieee transactions on pattern analysis and machine
intelligence                  
macdonald  a          graphs  notes on symetries  imbeddings  decompositions  tech 
rep  electrical engineering department tr       ajm  brunel university  uxbridge 
middlesex  united kingdom 
mahadevan  s     connell  j          automatic programming of behavior based robots
using reinforcement learning  artificial intelligence              
mallat  s     zhong  s          characterization of signals from multiscale edges  ieee
transactions on pattern analysis and machine intelligence                  
marr  d          vision  a computational investigation into the human representation
and processing of visual information  w h  freeman 
mccallum  r  a       a   instance based state identification for reinforcement learning  in
advances in neural information processing systems    pp          
mccallum  r  a       b   instance based utile distinctions for reinforcement learning with
hidden state  in proceedings of the twelfth international conference on machine
learning  pp          
moore  a  w     atkeson  c  g          prioritized sweeping  reinforcement learning with
less data and less real time  machine learning              
moore  a  w          variable resolution dynamic programming  eciently learning action
maps in multivariate real valued state spaces  in proceedings of the ninth international
workshop on machine learning 
nason  g          three dimensional projection pursuit  tech  rep   department of mathematics  university of bristol  bristol  united kingdom 
osborne  h     bridge  d          similarity metrics  a formal unification of cardinal
and non cardinal similarity measures  in proceedings of the second international
conference on case based reasoning  vol       of lnai  pp          
parr  r          flexible decomposition algorithms for weakly coupled markov decision
problems  in proceedings of the fourteenth conference on uncertainty in artificial
intelligence  pp          
peng  j          ecient memory based dynamic programming  in proceedings of the
twelfth international conference of machine learning  pp          
precup  d   sutton  r  s     singh  s  p          planning with closed loop macro actions 
in working notes of the      aaai fall symposium on model directed autonomous
systems  pp        
   

fidrummond

precup  d   sutton  r  s     singh  s  p          theoretical results on reinforcement
learning with temporally abstract options  in proceedings of the tenth european
conference on machine learning  vol       of lnai  pp          
schnabel  j  a          multi scale active shape description in medical imaging  ph d 
thesis  university of london  london  united kingdom 
sheppard  j  w     salzberg  s  l          a teaching strategy for memory based control 
artificial intelligence review  special issue on lazy learning              
singh  s  p     sutton  r  s          reinforcement learning with replacing eligibility traces 
machine learning              
singh  s  p          reinforcement learning with a hierarchy of abstract models  in proceedings of the tenth national conference on artificial intelligence  pp          
suetens  p   fua  p     hanson  a          computational strategies for object recognition 
computing surveys               
sutton  r  s          integrated architectures for learning  planning  and reacting based
on approximating dynamic programming  in proceedings of the seventh international
conference on machine learning  pp          
sutton  r  s          generalization in reinforcement learning  successful examples using
sparse coarse coding  in advances in neural information processing systems    pp 
          
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
tadepalli  p     ok  d          scaling up average reward reinforcement learning by approximating the domain models and the value function  in proceedings of the thirteenth
international conference of machine learning  pp          
tanimoto  s  l          the elements of artficial intelligence  w h  freeman 
terzopoulos  d          regularization of inverse visual problems involving discontinuities 
ieee transactions on pattern analysis and machine intelligence                 
thrun  s     schwartz  a          finding structure in reinforcement learning  in advances
in neural information processing systems    pp          
veloso  m  m     carbonell  j  g          derivational analogy in prodigy  automating
case acquisition  storage and utilization  machine learning                  
watkins  c  j     dayan  p          technical note  q learning  machine learning          
        

   

fi