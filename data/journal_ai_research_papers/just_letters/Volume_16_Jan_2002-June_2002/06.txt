journal of artificial intelligence research                  

submitted       published     

efficient reinforcement learning using
recursive least squares methods
xin xu
han gen he
dewen hu

xuxin mail     net
hehangen cs hn cn
dwhu nudt edu cn

department of automatic control
national university of defense technology
changsha  hunan          p r china

abstract
the recursive least squares  rls  algorithm is one of the most well known algorithms used
in adaptive filtering  system identification and adaptive control  its popularity is mainly due to its
fast convergence speed  which is considered to be optimal in practice  in this paper  rls methods
are used to solve reinforcement learning problems  where two new reinforcement learning
algorithms using linear value function approximators are proposed and analyzed  the two
algorithms are called rls td     and fast ahc  fast adaptive heuristic critic   respectively 
rls td     can be viewed as the extension of rls td    from     to general       so it is
a multi step temporal difference  td  learning algorithm using rls methods  the convergence
with probability one and the limit of convergence of rls td     are proved for ergodic markov
chains  compared to the existing ls td     algorithm  rls td     has advantages in
computation and is more suitable for online learning  the effectiveness of rls td     is
analyzed and verified by learning prediction experiments of markov chains with a wide range of
parameter settings 
the fast ahc algorithm is derived by applying the proposed rls td     algorithm in the
critic network of the adaptive heuristic critic method  unlike conventional ahc algorithm 
fast ahc makes use of rls methods to improve the learning prediction efficiency in the critic 
learning control experiments of the cart pole balancing and the acrobot swing up problems are
conducted to compare the data efficiency of fast ahc with conventional ahc  from the
experimental results  it is shown that the data efficiency of learning control can also be improved
by using rls methods in the learning prediction process of the critic  the performance of
fast ahc is also compared with that of the ahc method using ls td      furthermore  it is
demonstrated in the experiments that different initial values of the variance matrix in rls td    
are required to get better performance not only in learning prediction but also in learning control 
the experimental results are analyzed based on the existing theoretical work on the transient
phase of forgetting factor rls methods 

   introduction
in recent years  reinforcement learning  rl  has been an active research area not only in machine
learning but also in control engineering  operations research and robotics  kaelbling et al       
bertsekas  et al        sutton and barto       lin        it is a computational approach to
     ai access foundation and morgan kaufmann publishers  all rights reserved 

fixu  he    hu

understand and automate goal directed learning and decision making  without relying on
exemplary supervision or complete models of the environment  in rl  an agent is placed in an
initial unknown environment and only receives evaluative feedback from the environment  the
feedback is called reward or reinforcement signal  the ultimate goal of rl is to learn a strategy
for selecting actions such that the expected sum of discounted rewards is maximized 
since lots of problems in the real world are sequential decision processes with delayed
evaluative feedback  the research in rl has been focused on theory and algorithms of learning to
solve the optimal control problem of markov decision processes  mdps  which provide an
elegant mathematical model for sequential decision making  in operations research  many results
have been presented to solve the optimal control problem of mdps with model information 
however  in reinforcement learning  the model information is assumed to be unknown  which is
different from the methods studied in operations research such as dynamic programming  in
dynamic programming  there are two elemental processes  which are the policy evaluation process and the policy improvement process  respectively  in rl  there are two similar processes 
one is called learning prediction and the other is called learning control  the goal of learning
control is to estimate the optimal policy or optimal value function of an mdp without knowing its
model  learning prediction aims to solve the policy evaluation problem of a stationary policy
mdp without any prior model and it can be regarded as a sub problem of learning control 
furthermore  in rl  learning prediction is different from that in supervised learning  as pointed
out by sutton         the prediction problems in supervised learning are single step prediction
problems while those in reinforcement learning are multi step prediction problems  to solve
multi step prediction problems  a learning system must predict outcomes that depend on a future
sequence of decisions  therefore  the theory and algorithms for multi step learning prediction
become an important topic in rl and much research work has been done in the literature  sutton 
      tsitsiklis and roy        
among the proposed multi step learning prediction methods  temporal difference  td 
learning  sutton        is one of the most popular methods  it was studied and applied in the early
research of machine learning  including the celebrated checkers playing program  minsky       
samuel         in       sutton presented the first formal description of temporal  difference
methods and the td     algorithm  sutton        convergence results are established for tabular
temporal difference learning algorithms where the cardinality of tunable parameters is the same
as that of the state space  sutton        watkins et al        dayan et al         jaakkola  et
al         since many real world applications have large or infinite state space  value function
approximation  vfa  methods need to be used in those cases  when combined with nonlinear
value function approximators  td     can not guarantee convergence and several results
regarding divergence have been reported in the literature  tsitsiklis and roy        for td    
with linear function approximators  also called linear td     algorithms  several convergence
proofs have been presented  dayan        showed the convergence in the mean for linear td    
algorithms with arbitrary          tsitsiklis and roy        proved the convergence for a
special class of td learning algorithms  known as td     while in tsitsiklis and roy         they
extended the early results to general linear td     case and proved the convergence with
probability one 
the above linear td     algorithms have rules for updating parameters similar to those in
gradient descent methods  however  as in gradient learning methods  a step size schedule must
be carefully designed not only to guarantee convergence but also to obtain good performance  in
   

fiefficient reinforcement learning using rls methods

addition  there is inefficient use of data that slows the convergence of the algorithms  based on
the theory of linear least squares estimation  brartke and barto        proposed two
temporal difference algorithms called the least squares td    algorithm  ls td     and the
recursive least  squares td    algorithm  rls td      respectively  ls td    and rls td   
are more efficient in a statistical sense than conventional linear td     algorithms and they
eliminate the design of step size schedules  furthermore  the convergence of ls td    and
rls td    has been provided in theory  the above two algorithms can be viewed as the
least squares versions of conventional linear td    methods  however  as has been shown in the
literature  td learning algorithms such as td     with        that update predictions based on
the estimates of multiple steps are more efficient than monte carlo methods as well as td     by
employing the mechanism of eligibility traces  which is determined by    td     algorithms
with        can extract more information from historical data  recently  a class of linear
temporal difference learning algorithms called ls td     has been proposed by boyan
             where least squares methods are employed to compute the value function estimation
of td     with        although ls td     is more efficient than td      it requires too much
computation per time step when online updates are needed and the number of state features
becomes large 
in system identification  adaptive filtering and adaptive control  the recursive least squares
 rls   young       ljung        ljung       method  commonly used to reduce the
computational burden of least squares methods  is more suitable for online estimation and control 
although rls td    makes use of rls methods  it does not employ the mechanism of
eligibility traces  based on the work of tsitsiklis and roy               boyan             and
motivated by the above ideas  a new class of temporal difference learning methods  called the
rls td     algorithm  is proposed and analyzed formally in this paper  rls td     is superior
to conventional linear td     algorithms in that it makes use of rls methods to improve the
learning efficiency in a statistical point of view and eliminates the step size schedules 
rls td     has the mechanism of eligibility traces and can be viewed as the extension of
rls td    from     to general       the convergence with probability   of rls td     is
proved for ergodic markov chains and the limit of convergence is also analyzed  in learning
prediction experiments for markov chains  the performance of rls td     and td     as well as
ls td     is compared  where a wide range of parameter settings is tested  in addition  the influence of the initialization parameters in rls td     is also discussed  it is observed that the
rate of convergence is influenced by the initialization of the variance matrix  which is a
phenomenon investigated theoretically in adaptive filtering  moustakides        haykin        
as will be analyzed in the following sections  there are two benefits of the extension from
rls td    to rls td      one is that the value of         will still affect the performance
of the rls based temporal difference algorithms  although for rls td      the rate of
convergence is mainly influenced by the initialization of the variance matrix  the bound of
approximation error is dominantly determined by the parameter    the smallest error bound can
be obtained for     and the worst bound is obtained for      these bounds suggest that the
value of  should be selected appropriately to obtain the best approximation error  the second
benefit is that rls td     is more suitable for online learning than ls td     since the
computation per time step is reduced from o k   to o k    where k is the number of state
features 
the adaptive heuristic critic  ahc  learning algorithm is a class of reinforcement learning
   

fixu  he    hu

methods that has an actor critic architecture and can be used to solve full reinforcement learning
or learning control problems  by applying the rls td     algorithm in the critic  the fast ahc
algorithm is proposed in this paper  using rls methods in the critic  the performance of learning
prediction in the critic is improved so that learning control problems can be solved more
efficiently  simulation experiments on the learning control of the cart pole balancing problem and
the swing up of an acrobot are conducted to verify the effectiveness of the fast ahc method  by
comparing with conventional ahc methods which use td     in the critic  it is demonstrated that
fast ahc can obtain higher data efficiency than conventional ahc methods  experiments on the
performance comparisons between ahc methods using ls td     and fast ahc are also
conducted  in the learning control experiments  it is also illustrated that the initializing constant of
the variance matrix in rls td     influences the performance of fast ahc and different values
of the constant should be selected to get better performance in different problems  the above
results are analyzed based on the theoretical work on the transient phase of rls methods 
this paper is organized as follows  in section    an introduction on the previous linear
temporal difference algorithms is presented  in section    the rls td     algorithm is proposed
and its convergence  with probability one  is proved  in section    a simulation example of the
value function prediction for absorbing markov chains is presented to illustrate the effectiveness
of the rls td     algorithm  where different parameter settings for different algorithms
including ls td     are studied  in section    the fast ahc method is proposed and the
simulation experiments on the learning control of the cart pole balancing and the acrobot are
conducted to compare fast ahc with the conventional ahc method as well as the
ls td     based ahc method  some simulation results are presented and analyzed in detail  the
last section contains concluding remarks and directions for future work 

   previous work on linear temporal difference algorithms
in this section  a brief discussion on the conventional linear td     algorithm and rls td    as
well as the ls td     algorithm will be given  first of all  some mathematical notations are
presented as follows 
consider a markov chain whose states lie in a finite or countable infinite space s  the states
of the markov chain can be indexed as       n   where n is possibly infinite  although the
algorithms and the results in this paper are applicable to markov chains with general state space 
the discussion in this paper will be restricted within the cases with a countable state space to
simplify the notation  the extension to markov chains with a general state space only requires the
translation of the matrix notation into operator notation 
let the trajectory generated by the markov chain be denoted by  xt  t         xt s  the
dynamics of the markov chain is described by a transition probability matrix p whose  i j  th
entry  denoted by pij  is the transition probability for xt   j given that xt i  for each state transition
from xt to xt    a scalar reward rt is defined  the value function of each state is defined as follows 


v  i     e   t rt x     i 

   

t   

where        is a discount factor 
in the td     algorithm  there are two basic mechanisms which are the temporal difference
   

fiefficient reinforcement learning using rls methods

and the eligibility trace  respectively  temporal differences are defined as the differences between
two successive estimations and have the following form 
 
 
 t   rt   vt   xt       vt   xt  
   
 
where xt   is the successive state of xt  v   x  denotes the estimate of the value function v x  and rt
is the reward received after the state transition from xt to xt   
the eligibility trace can be viewed as an algebraic trick to improve learning efficiency
without recording all the data of a multi step prediction process  this trick is based on the idea of
using the truncated return of a markov chain  in temporal difference learning with eligibility
traces  an n step truncated return is defined as
 
rtn   rt   rt             n   rt   n      nvt   s t   n  
   
for an absorbing markov chain whose length is t  the weighted average of truncated returns
is
t t  

rt         

  n  rtn    t t   rt

   

n   

where        is a decaying factor and rt  rt   rt             t rt is the monte carlo return at
the terminal state  in each step of the td     algorithm  the update rule of the value function
estimation is determined by the weighted average of truncated returns defined above  the
corresponding update equation is
 
 
vt   s i      t   rt  vt   s i   
   
where t is a learning factor 
the update equation     can be used only after the whole trajectory of the markov chain is
observed  to realize incremental or online learning  eligibility traces are defined for each state as
follows 

 z t   s i       
z t      s i     
 z t   s i   

if s i   s t
if s i  s t

the online td     update rule with eligibility traces is
 
 
vt      si     vt   si      t  t z t      si  

   

   

where t is the temporal difference at time step t  which is defined in     and z  s    for all s 
since the state space of a markov chain is usually large or infinite in practice  function
approximators such as neural networks are commonly used to approximate the value function 
td     algorithms with linear function approximators are the most popular and well studied ones 
consider a general linear function approximator with a fixed basis function vector

   x          x         x         n   x   t
the estimated value function can be denoted as

 
vt   x     t   x wt
   

   

fixu  he    hu

where wt   w   w   wn t is the weight vector 
the corresponding incremental weight update rule is

r
wt      wt    t  rt    t   xt     wt   t   xt  wt   z t   
r
where the eligibility trace vector z t   s       z t   s    z  t   s        z nt   s    t is defined as
r
r
z t      z t      xt  

   

    

in tsitsiklis and roy         the above linear td     algorithm is proved to converge with
probability   under certain assumptions and the limit of convergence w  is also derived  which
satisfies the following equation 
e     a  x t   w    e    b  x t       

    

where xt   xt xt   zt     t       form a markov process  e    stands for the expectation with
respect to the unique invariant distribution of  xt   and a xt  and b xt  are defined as
r
a  x t     z t   t   xt     t   xt      
    

r
b  x t     z t rt

    

to improve the efficiency of linear td   algorithms  least squares methods are used with the
linear td    algorithm  and the ls td    and rls td    algorithms are suggested  brartke and
barto         in ls td    and rls td     the following quadratic objective function is defined 
t  

j     rt    tt   tt    w    

    

t   

thus  the aim of ls td    and rls td    is to obtain a least squares estimation of the real
value function which satisfies the following bellman equation 
v   xt     e rt   xt   xt        v   xt      

    

by employing the instrumental variables approach  soderstrom and stoica         the
least squares solution of      is given as
t

t

t   

t   

w ls td            t   t   t      t         t rt  

    

where  t is the instrumental variable chosen to be uncorrelated with the input and output noises 
in rls td     recursive least squares methods are used to decrease the computational burden of ls td     the update rules of rls td    are as follows 
wt      wt   pt  t  rt    t  t      t wt           t  t      t pt  t  

    

pt      pt  pt  t   t   t      t pt         t   t      t pt  t  

    

the convergence  with probability one  of ls td    and rls td    is proved for periodic and
absorbing markov chains under certain assumptions  brartke and barto       
   

fiefficient reinforcement learning using rls methods

in boyan              ls td     is proposed by solving      directly and the model based
property of ls td     is also analyzed  however  for ls td      the computation per time step is
o k    i e   the cubic order of the state feature number  therefore the computation required by
ls td   increases very fast when k increases  which is undesirable for online learning 
in the next section  we propose the rls td     algorithm by making use of recursive
least squares methods so that the computational burden of ls td     can be reduced from o k  
to o k    we also give a rigorous mathematical analysis on the algorithm  where the convergence
 with probability    of rls td     is proved 

   the rls td     algorithm
for the markov chain discussed above  when linear function approximators are used  the
least squares estimation problem of      has the following objective function 
j 

t

t

t   

t   

 a  x t  w   b  x t  

 

    

where a  x t    r nn   b  x t    r n are defined as      and       respectively   is a euclid norm
and n is the number of basis functions 
in ls td      the least squares estimate of the weight vector w is computed according to the
following equation 
t

t

t   

t   

w ls td        at bt     a  x t        b  x t   

    

t
t
r
at      a  x t       z t   t   xt     t   xt      

    

t
t
r
bt    b  x t      z t rt

    

where

t   

t   

t   

t   

as is well known in system identification  adaptive filtering and control  rls methods are
commonly used to solve the computational and memory problems of least squares algorithms  in
the sequel  we present the rls td     algorithm based on the above idea  first  the matrix inverse lemma is given as follows 
lemma   ljung  et al         if a  r nn   b  r n    c  r  n and a is invertible  then

  a   bc       a    a   b   i   ca   b     ca  

    

pt   at 

    

let

   

fixu  he    hu

p    i

    

r
k t      pt    z t

    

where  is a positive number and i is the identity matrix 
then the weight update rules of rls td     are given by
r
r
k t      pt z t         t   xt     t   xt       pt z t  
wt      wt   k t     rt    t   xt     t   xt      wt  

pt     

 



r
r
  pt  pt z t        t   xt     t   xt       pt z t        t   xt     t   xt       pt  

    
    

    

where for the standard rls td   algorithm      for the general forgetting factor rls td  
case      
the forgetting factor        is usually used in adaptive filtering to improve the
performance of rls methods in non stationary environments  the forgetting factor rls td    
algorithm with     can be derived using similar techniques as in haykin         the detailed
derivation of rls td   is referred to appendix a 
in the follows  the descriptions of rls td     for two different kinds of markov chains are
given  first  a complete description of rls td     for ergodic markov chains is presented below 

algorithm   rls td     for ergodic markov chains

   given 
 a termination criterion for the algorithm 
 a set of basis functions    j  i      j      n  for each state i  where n is the
number of basis functions 
   initialize 
      let t   
      initialize the weight vector wt  the variance matrix pt   the initial state x  
r
      set the eligibility traces vector z      
   loop 
      for the current state xt  observe the state transition from xt to xt   and the
reward r xt  xt    
      apply equations           to update the weight vector 
      t t   
until the termination criterion is satisfied 

the rls td     algorithm for absorbing markov chains is a little different from the above
algorithm in coping with the state features of absorbing states  following is a description of
   

fiefficient reinforcement learning using rls methods

rls td     for absorbing markov chains 

algorithm   rls td     for absorbing markov chains

   given 
 a termination criterion for the algorithm 
 a set of basis functions    j  i      j      n  for each state i  where n is the
number of basis functions 
   initialize 
      let t   
      initialize the weight vector wt  the variance matrix pt   the initial state x  
r
      set the eligibility traces vector z      
   loop 
      for the current state xt 
 if xt is an absorbing state  set   xt       r xt  rt  where rt is the terminal
reward 
 otherwise  observe the state transition from xt to xt   and the reward
r xt  xt    
      apply equations           to update the weight vector 
      if xt is an absorbing state  re initialize the process by setting xt   to an initial
r
state and set the eligibility traces z t to a zero vector 
      t t   
until the termination criterion is satisfied 

in the above rls td     algorithm for absorbing markov chains  the weight updates in the
absorbing states are treated differently and the process is re initialized in absorbing states to
transform the absorbing markov chain into an equivalent ergodic markov chain  so in the
following convergence analysis  we only focus on ergodic markov chains 
under similar assumptions as in tsitsiklis and roy         we will prove that the proposed
rls td     algorithm converges with probability one 
assumption    the markov chain  xt   whose transition probability matrix is p  is ergodic  and
there is a unique distribution  that satisfies



p   t
    
with   i    for all is and is a finite or infinite vector  depending on the cardinality of s 
t

assumption    transition rewards r xt xt    satisfy

e    r     xt   xt         

    

where e     is the expectation with respect to the distribution   
assumption    the matrix                   n    r n n has full column rank  that is  the basis
   

fixu  he    hu

functions  i  i      n  are linearly independent 
assumption    for every i  i      n   the basis function  i satisfies
 

e     i   xt      

    

  t
 a  x t    is non singular for all t   
t t   
assumptions    are almost the same as those for the linear td   algorithms discussed in
tsitsiklis and roy        except that in assumption    ergodic markov chains are considered 
assumption   is specially needed for the convergence of the rls td   algorithm 
based on the above assumptions  the convergence theorem for rls td   can be given as
follows 
assumption    the matrix   p    

theorem    for a markov chain which satisfies assumptions     the asymptotic estimate found
by rls td     converges  with probability    to w  determined by      

for the proof of theorem    please refer to appendix b  the condition specified by
assumption   can be satisfied by setting p   i appropriately 
according to theorem    rls td     converges to the same solution as conventional linear
td     algorithms do  which satisfies       so the limit of convergence can be characterized by
the following theorem 
theorem    tsitsiklis and roy        let w  be the weight vector determined by      and v  be
the true value function of the markov chain  then under assumption     the following relation
holds 
   
w    v   
    
v    v  
d
d
 

where

x

d

 

x t dx         t d      t d  

for more explanations on the notations in theorem    please refer to appendix b 
as discussed by tsitsiklis and roy         the above theorem shows that the distance of the
limiting function  w  from the true value function v  is bounded and the smallest bound of
approximation error can be obtained when     for every     the bound actually deteriorates as
 decreases  the worst bound is obtained when     although this is only a bound  it strongly
suggests that higher values of  are likely to produce more accurate approximations of v  
compared to ls td    there is an additional parameter in rls td    which is the value 
for the initial variance matrix p   as was pointed out by haykin       pp       the exact value of
the initializing constant  has an insignificant effect when the data length is large enough  this
means that in the limit  the final solutions obtained by ls and rls are almost the same  for the
influence of  on the transient phase  when the positive constant  becomes large enough or goes
to infinity  the transient behavior of rls will be almost the same as that of ls methods  ljung 
       but when  is initialized with a relatively small value  the transient phases of rls and ls
will be different  in practice  it is observed that there is a variable performance of rls as a
function of the initialization of   moustakides         in some cases  rls can exhibit a
significantly faster convergence when initialized with a relatively small positive definite matrix
than when initialized with a large one  haykin       moustakides        hubing and alexander 
   

fiefficient reinforcement learning using rls methods

       a first effort toward this direction is the statistical analysis of rls for soft and exact
initialization but limits to the case that the number of iterations is less than the size of the
estimation vector  hubing and alexander         moustakides        provided a theoretical
analysis on the relation between the algorithmic performance of rls and the initialization of  
by using the settling time as the performance measure  moustakides proved that the well known
rule of initialization with a relatively small matrix is preferable for cases of high and medium
signal to noise ratio  snr   whereas for low snr  a relatively large matrix must be selected for
achieving best results  in the following learning prediction experiments of rls td    as well as
the learning control simulation of fast ahc  it is observed that the value of the initializing
constant  also plays an important role in the convergence performance  and the above theoretical
analyses provide a clue to explain our experimental results 

   learning prediction experiments on markov chains
in this section  an illustrative example is given to show the effectiveness of the proposed
rls td   algorithm  furthermore  the algorithmic performance under the influence of the
initializing constant  is studied 
the example is a finite state absorbing markov chain called the hop world problem  boyan 
       as shown in figure    the hop world problem is a    state markov chain with an
absorbing state 

figure    the hop world problem
in figure    state    is the initial state for each trajectory and state   is the absorbing state 
each non absorbing state has two possible state transitions with transition probability      each
state transition has reward   except the transition from state   to state   which has a reward of   
thus  the true value function for state i   i    is  i 
to apply linear temporal difference algorithms to the value function prediction problem  a set
of four element state features or basis functions is chosen  as shown in figure    the state
features of states        and   are  respectively                                             and the
state features of other states are obtained by linearly interpolating between these 
in our simulation  the rls td     algorithm as well as ls td   and conventional linear
td     algorithms are used to solve the above value function prediction problem without
knowing the model of the markov chain  in the experiments  a trial is defined as the period from
the initial state    to the terminal state    the performance of the algorithms is evaluated by the
averaged root mean squared  rms  error of value function predictions over all the    states  for
each parameter setting  the performance is averaged over    independent monte carlo runs 
figure   shows the learning curves of rls td   and conventional linear td   algorithms with
three different parameter settings  the parameter  is set to     for all the algorithms and the
   

fixu  he    hu

step size parameter of td   has the following form 

n    

n    
n    n

    

the above step size schedule is also studied in boyan         in our experiments  three
different settings are used  which are
 s               n         
 s               n         
 s              n           

    

different from those in boyan         the linear td   algorithms applied here are in their
online forms  which update the weights after every state transitions  so the parameter n in      is
the number of state transitions  in each run  the weights are all initialized to zeroes  in figure   
the learning curves of conventional linear td   algorithms with step size schedules  s     s  
and  s   are shown by curves     and    respectively  for each curve  the averaged rms errors of
value function predictions over all the states and    independent runs are plotted for each trial 
curve   shows the learning performance of rls td    one additional parameter for rls td  
is the initial value  of the variance matrix p   in this experiment   is set to      which is a
relatively large value  from figure    it can be concluded that by making use of rls methods 
rls td   can obtain much better performance than conventional linear td   algorithms and
eliminates the design problem of the step size schedules  other experiments for linear td   and
rls td   with different parameters  are also conducted and similar results are obtained when
the initial values  of rls td   are large and the conclusion is confirmed 

figure    performance comparison between rls td   and td  
         td      with step size parameters specified by  s    s   and  s  
 rls td      with initial variance matrix p     i
we have done demonstrative experiments to investigate the influence of  on the performance
of the rls td   algorithm  figure   shows the performance comparison between rls td  
   

fiefficient reinforcement learning using rls methods

algorithms using two different initial parameters of the variance matrix p   which are p     i and
p      i  respectively  the forgetting factor is         the performance of the suggested
algorithm is measured by the averaged rms errors of the value function prediction in the first
    trials over    independent runs and all the    states  in the experiments     settings of the
parameter  are tested  which are    n  n          
in figure    it is clearly shown that the performance of rls td   with a large initial value
of  is much better than rls td   with a small initial value of   in other experiments with
different parameter settings of  and   similar results are also obtained  we may refer this
phenomenon to the low snr case of the forgetting factor rls studied in moustakides        
for the hop world problem  the stochastic state transitions could introduce high equation
residuals a  x t  w  b  x t   in       which corresponds to the additive noise with large variance 
i e   the low snr case  as has been discussed in section    for the forgetting factor rls in low
snr cases  a relatively large initializing constant  must be selected for better results  a full
understanding of this phenomenon is yet to be found 

figure    performance comparison of rls td   with different initial value of          
the performance of rls td   with unit forgetting factor    is also tested in our
experiments  although the initial value effect in rls with    has not been discussed intensively
 moustakides        the same effects of  are observed empirically in the case of    as that in
    which is shown by figure   
in our other experiments  it is also found that when  is initialized with a small value  the
performance is sensitive to the values of  and the parameter   in this case  the convergence
speed of rls td   increases as  increases from   to    which is shown in figure   
furthermore  when  is fixed  the performance of rls td   deteriorates as  becomes smaller 
as shown in figure    

   

fixu  he    hu

figure    performance comparison of rls td   with different initial value of      

figure    learning curves of ls td   and rls td   with different      

in figure    the learning curves of rls td   with different initializing constants  are
shown and compared with that of ls td    in the experiment   is set to      from figure    it is
shown that the performance of rls td   approaches that of ls td   when  becomes large 
as is well known  when  becomes large enough  the performance of rls and ls methods will
be almost the same  figure   shows the performance comparison between ls td   and
rls td   with a large value of   the initial variance matrix for rls td   is set to    i in
every runs  where i is the identity matrix 

   

fiefficient reinforcement learning using rls methods

figure    performance comparison of ls td   and rls td   with    and large initial
value of 
based on the above experimental results  it can be concluded that the convergence speed of
rls td     is mainly influenced by the initial value  of the variance matrix and the parameter
   detailed discussions on the properties of rls td     are given as follows 
    when  is relatively large  the effect of  becomes small  if  is large enough or goes to
infinity  the performance of rls td     and ls td     will be almost the same  as was
discussed above  in such cases  the effect of  on the speed of convergence is insignificant 
which coincides with the discussion in boyan         however  as described in theorem    the
value of  still affects the ultimate error bound of value function approximation 
    when  is relatively small  it is observed that the convergence performance of
rls td   is different from that of ls td   and is influenced by the values of both  and   in
the experiments of the hop world problem  the results show that smaller values of  lead to
slower convergence  these results may be explained by the theoretical analysis on the transient
phase of the forgetting factor rls  moustakides        according to the theory in moustakides
        larger values of  are needed for better performance in the cases of low snr while
smaller  values are preferable for fast convergence in the cases of high and medium snr  so
different values of  must be selected for faster convergence of rls td     in different cases 
especially  in some cases  such as the high snr case discussed in moustakides         rls
methods with small values of  can obtain a very fast speed of convergence 
    compared to conventional linear td     algorithms  the rls td     algorithm can
obtain much better performance by making use of rls methods for value function prediction
problems  furthermore  in td      a step size schedule needs to be carefully designed to achieve
good performance  while in rls td      the initial value  of the variance matrix can be selected
according to the criterion of a large or a small value 
    for the comparison of ls td     and rls td      which one is preferable depends on
the objective  in online applications  rls td     has advantages in computational efficiency
because the computation per step for rls td     is o k   and for ls td      it is o k    where
   

fixu  he    hu

k is the number of state features  moreover  as will be seen later  rls td     can obtain better
transient convergence performance than ls td     in some cases  on the other hand  ls td    
may be preferable to rls td     in the long term convergence performance  as can be seen in
figure    and from a system identification point of view  ls td     can obtain unbiased
parameter estimates in face of white additive noises while rls td     with finite  would
possess large parameter discrepancies 

   the fast ahc algorithm and two learning control experiments
in this section  the fast ahc algorithm is proposed based on the above results on learning
prediction to solve learning control problems  two learning control experiments are conducted to
illustrate the efficiency of fast ahc 
    the fast ahc algorithm

the ultimate goal of reinforcement learning is learning control  i e   to estimate the optimal
policies or the optimal value functions of markov decision processes  mdps   until now  several
reinforcement learning control algorithms including q learning  watkins and dayan       
sarsa learning  singh  et al        and the adaptive heuristic critic  ahc  algorithm  barto 
sutton and anderson       have been proposed  among the above methods  the ahc method is
different from q learning and sarsa learning which are value function based methods  in the
ahc method  value functions and policies are separately represented while in value functionbased methods the policies are determined by the value functions directly  there are two
components in the ahc method  which are called the critic and the actor  respectively  the actor
is used to generate control actions according to the policies  the critic is used to evaluate the
policies represented by the actor and provide the actor with internal rewards without waiting for
delayed external rewards  since the objective of the critic is policy evaluation or learning
prediction  temporal difference learning methods are chosen as the critics learning algorithms 
the learning algorithm of the actor is determined by the estimation of the gradient of the policies 
in the following discussion  a detailed introduction on the ahc method is given 
figure   shows the architecture of a learning system based on the ahc method  the learning
system consists of a critic network and an actor network  the inputs of the critic network include
the external rewards and the state feedback from the environment  the internal rewards provided
by the critic network are called the temporal difference  td  signals 
as in most reinforcement learning methods  the whole system is modeled as an mdp denoted
by a tuple  s a p r  where s is the state set  a is the action set  p is the state transition probability
and r is the reward function  the policy of the mdp is defined as a function   spr a   where
pr a  is a probability distribution in the action space  the objective of the ahc method is to
estimate the optimal policy   satisfying the following equation 


j   max j    max e    t rt  
 





    

t   

where  is the discount factor and rt is the reward at time step te    stands for the expectation
with respect to the policy  and the state transition probabilities and j is the expected total
reward 

   

fiefficient reinforcement learning using rls methods

figure    the ahc learning system
the value function for a stationary policy  and the optimal value function for the optimal
policy are defined as follows 


v    s     e    t rt s     s  

    

v     s     e     t rt s    s  

    

t   


t   

according to the theory of dynamic programming  the optimal value function satisfies the
following bellman equation 
    
v     s     max  r   s  a     ev     s     
a

where r s a  is the expected reward received after taking action a in state s 
in ahc  the critic uses temporal difference learning to approximate the value function of the
current policy  when linear function approximators are used in the critic  the weight update
equation is
wt      wt    t  rt   v   s t       v   s t   z t

    

where zt is the eligibility trace defined in      
the action selection policy of the actor is determined by the current state and the value
function estimation of the critic  suppose a neural network with weight vector u  u   u    um  is
used in the actor  and the output of the actor network is

y t   f  u   st  

    

the action outputs of the actor are determined by the following gaussian probabilistic distribution 
  y  y   
p r   y t     exp  t   t  
    

t

where the mean value is given by      and the variance is given by

 t   k        exp k  v   s t   

    

in the above equation  k  and k  are positive constants and v st  is the value function es   

fixu  he    hu

timation of the critic network 
to obtain the learning rule of the actor  an estimation of the policy gradient is given as
follows 
j y t
j 
y  y t y t
  
 rt t
u
y t u
 t u

    

where rt is the internal reward or the td signal provided by the critic 

rt   rt   v   st       v   st  

    

since in the ahc method  the critic is used to estimate the value function of the actors policy
and provide the internal reinforcement using temporal difference learning algorithms  the
efficiency of temporal different learning or learning prediction will greatly influence the whole
learning systems performance  although the policy of the actor is changing  it may change
relatively slowly especially when fast convergence of learning prediction in the critic can be
realized  in the previous sections  rls td     is shown to have better data efficiency than
conventional linear td     algorithms and a very fast convergence speed can be obtained when
the initializing constant is chosen appropriately  thus  applying rls td     to the policy
evaluation in the critic network will improve the learning prediction performance of the critic and
is promising to enhance the whole systems learning control performance  based on the above
idea  a new ahc method called the fast ahc algorithm is proposed in this paper  the efficiency
of the fast ahc algorithm is verified empirically and detailed analysis of the results is given 
following is a complete description of the fast ahc algorithm 

algorithm    the fast ahc algorithm
   given  a critic neural network and an actor neural network  which are both linear in
parameters  a stop criterion for the algorithm 
   initialize the state of the mdp and the learning parameters  set t   
   while the stop criterion is not satisfied 
      according to the current state s t   compute the output of the actor network y t  

     

determine the actual action of the actor by the probability distribution given by
     
take the action y t on the mdp  and observe the state transition from s t to

s t      set reward rt   r   st   st       
     
     

     

apply the rls td     algorithm described in           to update the weights of
the critic network 
apply the following equation to update the weights of the actor network 
j
at      at    t 
    
at
where t is the learning factor of the actor 
let t t    return to   

   

fiefficient reinforcement learning using rls methods

    learning control experiments on the cart pole balancing problem
the balancing control of inverted pendulums is a typical nonlinear control problem and has been
widely studied not only in control theory but also in artificial intelligence  in the research of
artificial intelligence  the learning control of inverted pendulums is considered as a standard test
problem for machine learning methods  especially for rl algorithms  it has been studied in the
early work of michies boxes system  michie et al        and later in barto and sutton        
where the learning controllers only have two output values      n  and    n   in berenji  et
al        and lin  et al         ahc methods with continuous outputs are applied to the cart pole
balancing problem  in this paper  the cart pole balancing problem with continuous control values
is used to illustrate the effectiveness of the fast ahc method 
figure   shows a typical cart pole balancing control system  which consists of a cart moving
horizontally and a pole with one end fixed at the cart  let x denote the horizontal distance
between the center of the cart and the center of the track  where x is negative when the cart is in
the left part of the track  variable  denotes the angle of the pole from its upright position  in
degrees  and f is the amount of force  n  applied to the cart to move it towards its left or right  so
the control system has four state variables x  x         where x     are the derivatives of x and   
respectively 
in figure    the mass of the cart is m    kg  the mass of the pole is m    kg  the half pole
length is l    m  the coefficient of friction of the cart on the track is c        and the coefficient
of friction of the pole on the cart is p           the boundary constraints on the state variables
are given as follows 
    o       o
    
    m  x     m
    
the dynamics of the control system can be described by the following equations 

 p  m   m   
 m   m   g sin   cos    f   ml    sin    c sgn  x     

ml
    

 
 
    

  m   m l  ml cos 
 


f   ml      sin      cos      c sgn  x   
 x   
m  m

where g is the acceleration due to the gravity  which is    m s   the above parameters and
dynamics equations are the same as those studied in barto et al         

figure    the cart pole balancing control system
   

fixu  he    hu

in the learning control experiments of the pole balancing problem  the dynamics      is
assumed to be unknown to the learning controller  in addition to the four state variables  the only
available feedback is a failure signal that notifies the controller when a failure occurs  which
means the values of the state variables exceed the boundary constraints prescribed by inequalities
     and       it is a typical reinforcement learning problem  where the failure signal serves as the
reward  since an external reward may only be available after a long sequence of actions  the critic
in the ahc learning controller is used to provide the internal reinforcement signal to accomplish
the learning task  learning control experiments on the pole balancing problem are conducted
using conventional ahc method which uses linear td   algorithms in the critic and the
fast ahc method proposed in this paper 
to solve the continuous state space problem in reinforcement learning  a class of linear
function approximators  which is called cerebellar model articulation controller  cmac  is
used  as a neural network model based on the neuro physiological theory about human
cerebellarcmac was first proposed by albus        and has been widely used in automatic
control and function approximation  in cmac neural networks  the dependence of adjustable
parameters or weights with respect to outputs is linear  for detailed discussion on the structure of
cmac neural networks  one may refer to albus        and sutton   barto        
in the ahc and fast ahc learning controllers  two cmac neural networks with four inputs
and one output for each are used as the function approximators in the critic and the actor 
respectively  each cmac has c tilings and m partitions for every input  so the total physical
memory for each cmac network is m c  to reduce the computation and memory requirements 
a hashing technique described by the following equations is employed in our experiments   for
detailed discussion on the parameters of the cmac networks  please refer to appendix c  
a  s    

 

  a i    m i    

    

i   

f s  a s  mod k
    
in      and       s represents an input state vector  a i     a i  m  is the activated tile for
the i th element of s  k is the total number of the physical memory and f s  is the physical
memory address corresponding to the state s  which is the remainder of a s  divided by k 
in order to compare the performance of different learning algorithms  the initial parameters of
each learning controller are selected as follows  the weights of the critic are all initialized to  
and the weights of the actor are initialized to random numbers in interval          the other
parameters for the ahc and fast ahc algorithms are           k        and k          
in all the experiments  a trial is defined as the period from an initial state to a failure state and
the initial state of each trial is set to a randomly generated state near the unstable equilibrium
          with a maximum distance of       equation      is employed to simulate the dynamics
of the system using the euler method  which has a time step of     s  when a trial lasts for more
than         time steps  it is said to be successful and the learning controller is assumed to be
able to balance the pole  the reinforcement signal for the problem is defined as

    if failure occurs
rt   
    otherwise

    

the performance of the fast ahc method is tested extensively  where different parameter
settings including  and the initial variance matrix p  are chosen  in the experiments  the
   

fiefficient reinforcement learning using rls methods

forgetting factor of rls td   in the critic is set to a value that is equal to   or very close to   
the learning control experiments using conventional ahc methods are also conducted for
comparison  the performance comparisons between the two algorithms are shown in figure      
and    
in the above experiments  the initial variance matrixes of the fast ahc algorithm are all set
to p     i  the performance of fast ahc is compared with ahc for different   the numbers of
physical memories of the critic network and the actor network are chosen as    and    
respectively  for each parameter setting of the two algorithms    independent runs are tested  the
performance is evaluated according to the trial number needed to successfully balance the pole 
the learning factors for the actor networks are all set to      which is a manually optimized value
for both algorithms  in all the experiments     settings of  are tested 

figure    performance comparison between fast ahc and ahc with      

figure     performance comparison between fast ahc and ahc with      
   

fixu  he    hu

figure     performance comparison between fast ahc and ahc with      

in figure       and     the learning factors of the critic networks in ahc are chosen as
            and       respectively  it is found that when        the performance of ahc
becomes worse  for the learning factors that are greater than       the ahc algorithm may
become unstable  and even when       and        the ahc algorithm becomes unstable for
    for the time varying learning factors specified in  s    s    the performance is worse than
the above constant learning factors  so the above three settings of the learning factor  are typical
and near optimal for the ahc algorithm 
from the above experimental results  it can be concluded that by using rls td   in the
critic network  the fast ahc algorithm can obtain better performance than conventional ahc
algorithms  although fast ahc requires more computation per step than ahc  it is more
efficient than ahc in that less trials or data are needed to successfully balance the pole 
as has been discussed in the previous sections  the convergence performance of rls td  
is influenced by the initial value of the variance matrix  this is also the case in fast ahc  in the
above learning control experiments  a small value      is selected  in other experiments  when 
is set to other small values  the performance of fast ahc is satisfactory and is better than ahc 
however  when  is equal to a relatively large value  for example      or      the performance
of fast ahc deteriorates significantly  since rls td   with a large initializing constant has
similar performance as ls td    it can be deduced that the ahc method using ls td   in the
critic will also have bad performance in the cart pole balancing problem  to verify this 
experiments are conducted using fast ahc with large initializing constant  and ahc using
ls td    for each parameter setting    independent runs are tested  in the experiments  the
maximum trials for each algorithm in one run is     so that if an algorithm fails to balance the
pole within     trials  its performance is set to     when using ls td   in the ahc method 
there may be computational problems in the matrix inversion during the first few steps of learning
and two methods are tried to avoid this problem  one is the usage of td   in the first    steps of
updates  the other is that the actor is not updated in the early stage of learning until ls td   is
   

fiefficient reinforcement learning using rls methods

stable  however  similar results are found for the two methods  figure    shows the experimental
results which clearly verify that the performance of fast ahc with a large initializing constant 
is similar to ahc using ls td   and it is much worse than fast ahc with a small   a detailed
discussion of this phenomenon is provided in subsection     

figure     performance comparison of fast ahc with different initial variance
in the following figure    and figure     the variations of the pole angle  and the control
force f are plotted  where a successfully trained fast ahc learning controller is used to control
the cart pole system 

figure     variation of the pole angle

figure     variation of the control force

    learning control experiments of the acrobot

in this subsection  another learning control example  which is the swing up control of the acrobot
in minimum time  is presented  the learning control of the acrobot is a class of adaptive optimal
control problem that is more difficult than the pole balancing problem  it has been investigated in
sutton         where cmac based sarsa learning algorithms were employed to solve it and only
the case of discrete control actions was studied  in our experiments  the case of continuous actions
   

fixu  he    hu

is considered 
an acrobot moving in the vertical plane is shown in figure     where oa and ab are the first
link and the second link  respectively  the control torque is applied at point a  the goal of the
swing up control is to swing the tip b of the acrobot above the line cd which is higher than the
joint o by an amount of the length of one link 

figure     the acrobot
the dynamics of the acrobot system is described by the following equations 

       d              d 

    

          d      d       

    

d    m l c     m   l     l c      l l c   cos       i     i  

    

d     m   l c     l l c   cos       i  

    

    m  l l c      sin      m  l l c       sin       m l c    m  l    g cos               

    

     m  l c   g cos               

    

where

in the above equations  the parameters  i    i   mi   li   i i   l ci are the angle  the angle velocity 
the mass  the length  the moment of inertia and the length of the center of mass for link i  i      
respectively 
let st denote the goal state of the swing up control  since the control aim is to swing up the
acrobot in minimum time  the reward function rt is defined as
   if s   st
rt   
   else

    

in the simulation experiments  the control torque  is continuous and is bounded by    n   n  
similar to the cart pole balancing problem  cmac neural networks are applied to solve the above
   

fiefficient reinforcement learning using rls methods

learning control problem with continuous states and actions  in the cmac based actor critic
controller  the actor network and the critic network both have c   tilings and m   partitions for
each input  in the actor network  uniform coding is employed and non uniform coding is used in
the critic network  for details of the coding parameters  please refer to appendix c  the sizes of
the physical memories for the actor network and the critic network are     and     respectively 
in the cmac networks  the following hashing techniques are used   for the definition of a s  a i 
and f s   please refer to subsection      
 

a  s        a i    m i    

    

i   

f s  a s  mod k
    
in the simulation  the parameters for the acrobot are chosen as m  m   kg  i  i   kgm  
lc  lc     m  l  l   m and g    m s   the time step for simulation is     s and the time interval
for learning control is    s  the learning parameters are                    k       k       a
trial is defined as the period that starts from the stable equilibrium and ends when the goal state is
reached  after each trial  the state of the acrobot is re initialized to its stable equilibrium  for each
parameter setting    independent runs are tested  each run consists of    trials and after    th trial 
the actor network is tested by controlling the acrobot alone  i e   by setting the action variance
defined in      to zero  the performance of the algorithms is evaluated according to the steps
used by the actor networks to swing up the acrobot 
the performance comparisons between fast ahc and ahc are shown in figure       and
    in the experiments  both algorithms are tested with different  and ahc is also tested with
different learning factors of the critic networks 
from the results  it is also shown that fast ahc can achieve higher data efficiency than ahc 
however  in this example  a relatively large  is used  which is different from the previous
cart pole balancing example  in other experiments  good performance is obtained with large
initializing constant and when  is very small  the performance deteriorates significantly  thus
this problem may be referred to the low snr case in moustakides         where large values of
 are preferable for best convergence rate of rls methods 

figure     performance comparison between fast ahc and ahc with      
   

fixu  he    hu

figure     performance comparison between fast ahc and ahc with      

figure     performance comparison between fast ahc and ahc with     

the following figure    shows the performance comparison between fast ahc with a large
      and a small        value of    where   settings of the parameter  are tested for each
algorithm  the performance of ahc using ls td   is also shown  in figure     a typical curve
of the angle of the first link is plotted  where the acrobot is controlled by the actor network of the
fast ahc method        after    trials 

   

fiefficient reinforcement learning using rls methods

figure     performance comparison of fast ahc and ahc using ls td  

figure     variation of the angle of link   controlled by fast ahc after    trials 

   

analysis of the experimental results

based on the above experimental results  it can be concluded that by using the rls td  
algorithm in the critic network  the fast ahc algorithm can obtain better performance than
conventional ahc algorithms in that less trials or data are needed to converge to a near optimal
policy  as is well known  one difficulty for the applications of rl methods is their slow
convergence  especially in the cases where learning data are hard to be generated  for the
fast ahc algorithm  although more computation per step is required than conventional ahc
methods  it will not be a serious problem when the number of linear state features is small  in all
of our learning control experiments  hashing techniques are used to reduce the state features in
cmac networks so that the computation of fast ahc can be reduced to an economical amount 
nevertheless  when the state feature number is large  conventional ahc methods may be
preferable 
in the experiments  it is observed that the performance of fast ahc is affected by the
initializing constant   these results are consistent with the property of rls td   and the rls
   

fixu  he    hu

method in adaptive filtering  which has been discussed in section    in the learning control
experiments of the cart pole balancing problem  better performance of fast ahc is obtained by
using small values of   while in the learning control of the acrobot  higher data efficiency is
achieved using fast ahc with a relatively large   these two different properties of fast ahc
may be referred to the different snr cases for rls methods  moustakides        a thorough
theoretical analysis on this problem is an interesting topic for future research 
in our experiments  the performance of the ahc method using ls td   is also tested  as
has been studied in section    when the initializing constant  is large  the performance of
rls td   and ls td   does not differ much  so the performance of ahc using ls td   is
similar to that of fast ahc with large values of  
as studied in moustakides         the rls method can converge much faster than other
adaptive filtering methods if the environment is stationary and the initializing constant is selected
appropriately  in some cases  rls may converge almost instantly  this is also verified in the
learning prediction experiments of the rls td   algorithm  when applying rls td   in an
actor critic learning controller  although the policy of the actor will change over time  it can still
be assumed that the changing speed of the policy is slow when compared with the fast
convergence speed of rls td    thus good performance of learning prediction can be obtained
in the critic  moreover  since the learning prediction performance of the critic is important to the
policy learning of the actor  the improvement in learning prediction efficiency will contribute to
the whole performance improvement of the controller 
   conclusions and future work

two new reinforcement learning algorithms using rls methods  which are called rls td    
and fast ahc  respectively  are proposed in this paper  rls td     can be used to solve learning
prediction problems more efficiently than conventional linear td     algorithms  the
convergence with probability   is proved for rls td     and the limit of convergence is also
analyzed  experimental results on learning prediction problems show that the rls td    
algorithm is superior to conventional td     algorithms in data efficiency and it also eliminates
the design problem of the step sizes in linear td     algorithms  rls td     can be viewed as
the extension of rls td    from     to general        although the effect of  on the
convergence speed of rls td     may not be significant in some cases  the usage of     will
still affect the approximation error bound  thus  when there are needs for value function
estimation with high precision  large values of  are preferable to      furthermore  rlstd     is superior to ls td     in computation when the weight vector must be updated after
every observations 
since learning prediction can be viewed as a sub problem of learning control  we extend the
results in learning prediction to a learning control method called the ahc algorithm  using
rls td     in the critic network  fast ahc can achieve better performance than conventional
ahc method in data efficiency  simulation results on the learning control of the pole balancing
problem and the acrobot system confirm the above analyses 
in the experiments  it is found that the performance of rls td     as well as fast ahc is
influenced by the initializing constant  of rls methods  different values of  are needed for best
performance in different cases  this is also a well known phenomenon in rls based adaptive
   

fiefficient reinforcement learning using rls methods

filtering and the theoretical results in moustakides        provide some basis for the explanations
of our results  a complete investigation of this problem is our ongoing work 
the idea of using rls td     in the critic network may be applied to other reinforcement
learning methods with actor critic architectures  in konda and tsitsiklis         a new actor critic
algorithm using linear function approximators is proposed and the convergence under certain
conditions is proved  one condition for the convergence of this algorithm is that the convergence
rate of the critic is much faster than that of the actor  thus the application of rls td     in the
critic may be preferable in order to ensure the convergence of the algorithm  the theoretical and
empirical work on this problem deserves to be studied in the future 

acknowledgements
this work is supported by the national natural science foundation of china under grants
                   and china university key teachers fellowship  we would very much like
to thank the anonymous reviewers and associate editor michael l  littman for their insights and
constructive criticisms  which have helped improve the paper significantly 

   

fixu  he    hu

appendix a  derivation of the rls td   algorithm

for the derivation of rls td    there are two different cases  which are determined by the value
of the forgetting factor 
    rls td   with a unit forgetting factor 
since

pt   at 

    

p    i

    

r
k t      pt    z t

    

according to lemma   

pt      at   

r
r
  pt  pt z t        t   xt     t   x t       pt z t        t   x t     t   xt       pt

r
k t      pt    z t
r
r
  pt z t         t   xt     t   xt       pt z t  

    

    

wt      at   bt   
t
r
  pt       z i ri  

    

i   

r
  pt      pt  wt   z t rt  
thus

r
r
wt      pt       pt      z t   t   xt     t   x t       wt   z t rt  
r
r
  wt   pt      z t rt  z t   t   xt     t   x t      wt  

    

  wt   k t     rt    t   xt     t   xt      wt  
    rls td   with a forgetting factor   
the derivation of rls td   with a forgetting factor    is similar to the exponentially weighted
rls algorithm in haykins        pp           here we only present the results 

pt     

 



r
r
k t      pt z t         t   x t     t   x t       pt z t  

    

wt      wt   k t     rt    t   x t     t   xt      wt  

    

r
r
  pt  pt z t        t   xt     t   xt       pt z t       t   xt     t   xt       pt  

   

    

fiefficient reinforcement learning using rls methods

appendix b  proof of theorem  
to study the steady property of the markov chain defined in section    we construct a stationary
process as follows  let  xt  be a markov chain that evolves according to the transition matrix p and is
already in its steady state  which means that pr xt i     i  for all i and t  given any sample path of
the markov chain  we define

r
zt  

t

     t     x  

    

   

r

then x t    xt   xt      z t   is a stationary process  which is the same as that discussed in  tsitsiklis
and roy        
let d denote a nn diagonal matrix with diagonal entries                n   where n is
the cardinality of state space x  then lemma   can be derived as follows 
lemma     tsitsiklis and roy        under assumption      the following equations hold 
   e       xt     xt   m       t dp m    for m  

r
   e     z t  t   xt     

    



     m  t dp m   

    

m   

r
   e     z t rt   xt   xt        



     m  t dp m r

    

m   

where r  r n   whose nth component is equal to e r   xt   xt      xt   i    
according to lemma    e  a xt   and e  b xt   are well defined and finite  furthermore  e  a xt  
is negative definite  so it is invertible 
from equation      
t

t

wrls td          p      a  x t        p  w     b  x t   
t   

t   

 
 
 
  t
    p      a  x t        p  w     b  x t   
t
t t   
t
t t   
t

    

since

  t
 a  x t  
t  t
t   

    

  t
 b  x t  
t  t
t   

    

e     a  x t      lim

e    b  x t      lim
and e  a xt   is invertible 
 

lim w rls td        e     a  x t   e    b  x t      w  

t 

   

    

fixu  he    hu

thus w rls td      converges to w  with probability   

appendix c  some details of the coding structures of cmac networks
in the following discussion  the coding structures of cmac networks in the cart pole balancing
problem and the acrobot control problem are presented 
    cmac coding structures in the cart pole balancing problem
in the cmac networks  the state variables have the following boundaries 

      o     o    

       deg  s     deg  s 

x              
x        
for the critic network  c   and m    the hashing technique specified in equations      and     
is employed and the total memory size is    
for the actor network  c   and m    the hashing technique specified in equations      and     
is employed and the total memory size is     
    cmac coding structures in the acrobot swing up problem
in the simulation  the angles are bounded by        and the angular velocities are bounded by

                            the tiling numbers of the actor and the critic both are equal to  
 c     the total memory sizes for the critic and the actor are    and      respectively  in the actor
network  each tiling partitions the range of each input into   equal intervals  m     in the critic
network  the partitions for each input are non uniform  which are given by

                                 

                                     

                                

                               

   

fiefficient reinforcement learning using rls methods

references
albus j s         a new approach to manipulator control  the cerebellar model articulation
controller  cmac   journal of dynamic systems  measurement  and control                 
barto a g   sutton r s     anderson c w          neuronlike adaptive elements that can solve
difficult learning control problems  ieee transactions on system  man  and cybernetics    
        
bertsekas d p    tsitsiklis j n          neurodynamic programming  belmont  mass   athena
scientific 
berenji h r    khedkar p          learning and tuning fuzzy logic controllers through reinforcements  ieee trans on neural networks                
boyan  j         least squares temporal difference learning  in bratko  i   and dzeroski  s   eds  
machine learning  proceedings of the sixteenth international conference  icml  
boyan  j         technical update  least squares temporal difference learning  machine learning 
special issue on reinforcement learning  to appear 
brartke  s j    barto a          linear least squares algorithms for temporal difference learning 
machine learning            
dayan p         the convergence of td   for general   machine learning             
dayan p     sejnowski t j          td   converges with probability    machine learning     
        
eleftheriou e    falconer d d          tracking properties and steady state performance of rls
adaptive filter algorithms  ieee transactions on acoustics  speech  and signal processing     
          
eweda e    macchi  o          convergence of the rls and lms adaptive filters  ieee trans 
circuits and systems              
haykin s          adaptive filter theory   rd edition  englewood cliffs  nj  prentice hall 
hubing n e    alexander s t          statistical analysis of the soft constrained initialization of
rls algorithms  in proc  of the ieee international conference on acoustics  speech and
signal processing 
jaakkola t   jordan m i     singh s p          on the convergence of stochastic iterative dynamic
programming algorithms  neural computation                  
kaelbling l p   littman m l     moore a w          reinforcement learning  a survey  journal
of artificial intelligence research             
konda v r    tsitsiklis j n          actor critic algorithms  in neural information processing
systems        mit press 
   

fixu  he    hu

lin l j          self improving reactive agents based reinforcement learning  planning and
teaching  machine learning                  
lin c t    lee c s g          reinforcement structure parameter learning for neural networkbased fuzzy logic control system  ieee transactions on fuzzy system              
ljung l    soderstron t          theory and practice of recursive identification  mit press 
ljung l          analysis of recursive stochastic algorithm  ieee  transactions on automatic
control          
michie d    chambers r a          boxes  an experiment in adaptive control  machine
intelligence    dale e  and michie d   eds   edinburgh  oliver and boyd          
minsky m l          theory of neural analog reinforcement systems and its application to the
brain model problem  ph d  thesis  princeton university 
moustakides g v          study of the transient phase of the forgetting factor rls  ieee trans 
on signal processing                    
samuel a l          some studies in machine learning using game of checkers  ibm journal on
research and development             
singh  s p   jaakkola t   littman m l     szepesvari c          convergence results for singlestep on policy reinforcement learning algorithms  machine learning              
sutton r    barto a          reinforcement learning  an introduction  cambridge ma  mit
press 
sutton r          learning to predict by the method of temporal differences  machine learning 
           
tsitsiklis j n          asynchronous stochastic approximation and q learning  machine learning 
            
tsitsiklis j n    roy b v          feature based methods for large scale dynamic programming 
neural computation                  
tsitsiklis j n    roy b v          an analysis of temporal difference learning with function
approximation  ieee transactions on automatic control                 
watkins c j c h    dayan p          q learning  machine learning             
young p          recursive estimation and time series analysis  springer verlag 

   

fi