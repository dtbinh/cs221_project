journal artificial intelligence research                 

submitted        published      

smote  synthetic minority over sampling technique
nitesh v  chawla

chawla csee usf edu

department computer science engineering  enb    
university south florida
     e  fowler ave 
tampa  fl             usa

kevin w  bowyer

kwb cse nd edu

department computer science engineering
    fitzpatrick hall
university notre dame
notre dame         usa

lawrence o  hall

hall csee usf edu

department computer science engineering  enb    
university south florida
     e  fowler ave 
tampa  fl             usa

w  philip kegelmeyer

wpk california sandia gov

sandia national laboratories
biosystems research department  p o  box      ms     
livermore  ca              usa

abstract
approach construction classiers imbalanced datasets described 
dataset imbalanced classication categories approximately equally represented  often real world data sets predominately composed normal examples
small percentage abnormal interesting examples  case
cost misclassifying abnormal  interesting  example normal example
often much higher cost reverse error  under sampling majority  normal  class proposed good means increasing sensitivity classier
minority class  paper shows combination method over sampling
minority  abnormal  class under sampling majority  normal  class achieve
better classier performance  in roc space  under sampling majority class 
paper shows combination method over sampling minority class
under sampling majority class achieve better classier performance  in roc
space  varying loss ratios ripper class priors naive bayes  method
over sampling minority class involves creating synthetic minority class examples 
experiments performed using c     ripper naive bayes classier  method
evaluated using area receiver operating characteristic curve  auc 
roc convex hull strategy 

   introduction
dataset imbalanced classes approximately equally represented  imbalance
order       prevalent fraud detection imbalance        
c
    
ai access foundation morgan kaufmann publishers  rights reserved 

fichawla  bowyer  hall   kegelmeyer

  reported applications  provost   fawcett        
attempts deal imbalanced datasets domains fraudulent telephone calls
 fawcett   provost         telecommunications management  ezawa  singh    norton 
       text classication  lewis   catlett        dumais  platt  heckerman    sahami 
      mladenic   grobelnik        lewis   ringuette        cohen      a  detection
oil spills satellite images  kubat  holte    matwin        
performance machine learning algorithms typically evaluated using predictive
accuracy  however  appropriate data imbalanced and or costs
dierent errors vary markedly  example  consider classication pixels mammogram images possibly cancerous  woods  doss  bowyer  solka  priebe    kegelmeyer 
       typical mammography dataset might contain     normal pixels    abnormal
pixels  simple default strategy guessing majority class would give predictive accuracy      however  nature application requires fairly high rate correct
detection minority class allows small error rate majority class
order achieve this  simple predictive accuracy clearly appropriate situations  receiver operating characteristic  roc  curve standard technique
summarizing classier performance range tradeos true positive false
positive error rates  swets         area curve  auc  accepted traditional performance metric roc curve  duda  hart    stork        bradley        lee 
       roc convex hull used robust method identifying potentially
optimal classiers  provost   fawcett         line passes point convex
hull  line slope passing another point
larger true positive  tp  intercept  thus  classier point optimal
distribution assumptions tandem slope 
machine learning community addressed issue class imbalance two ways 
one assign distinct costs training examples  pazzani  merz  murphy  ali  hume   
brunk        domingos         re sample original dataset  either oversampling minority class and or under sampling majority class  kubat   matwin 
      japkowicz        lewis   catlett        ling   li         approach  chawla 
bowyer  hall    kegelmeyer        blends under sampling majority class
special form over sampling minority class  experiments various datasets
c    decision tree classier  quinlan         ripper  cohen      b   naive bayes
classier show approach improves previous re sampling  modifying loss
ratio  class priors approaches  using either auc roc convex hull 
section   gives overview performance measures  section   reviews
closely related work dealing imbalanced datasets  section   presents details
approach  section   presents experimental results comparing approach
re sampling approaches  section   discusses results suggests directions future
work 

   performance measures
performance machine learning algorithms typically evaluated confusion matrix
illustrated figure    for   class problem   columns predicted class
rows actual class  confusion matrix  n number negative examples
   

fismote

predicted
negative

predicted
positive

actual
negative

tn

fp

actual
positive

fn

tp

figure    confusion matrix
correctly classied  true negatives   f p number negative examples incorrectly
classied positive  false positives   f n number positive examples incorrectly
classied negative  false negatives  p number positive examples correctly
classied  true positives  
predictive accuracy performance measure generally associated machine learning algorithms dened accuracy    t p   n    t p   f p   n   f n   
context balanced datasets equal error costs  reasonable use error rate
performance metric  error rate   accuracy  presence imbalanced datasets
unequal error costs  appropriate use roc curve similar
techniques  ling   li        drummond   holte        provost   fawcett        bradley 
      turney        
roc curves thought representing family best decision boundaries
relative costs tp fp  roc curve x axis represents  f p   f p  t n  f p  
y axis represents  t p   p  t p  f n    ideal point roc curve would
         positive examples classied correctly negative examples
misclassied positive  one way roc curve swept manipulating
balance training samples class training set  figure   shows illustration 
line   x represents scenario randomly guessing class  area roc
curve  auc  useful metric classier performance independent decision
criterion selected prior probabilities  auc comparison establish dominance
relationship classiers  roc curves intersecting  total auc
average comparison models  lee         however  specic cost class
distributions  classier maximum auc may fact suboptimal  hence 
compute roc convex hulls  since points lying roc convex hull
potentially optimal  provost  fawcett    kohavi        provost   fawcett        

   previous work  imbalanced datasets
kubat matwin        selectively under sampled majority class keeping
original population minority class  used geometric mean performance measure classier  related single point roc curve 
minority examples divided four categories  noise overlapping positive class decision region  borderline samples  redundant samples safe samples 
borderline examples detected using tomek links concept  tomek         another
   

fichawla  bowyer  hall   kegelmeyer

roc

          

   
ideal point

percent
true

y x

positive
increased undersampling
majority class moves
operating point
upper right
original data set

 

percent false positive

   

figure    illustration sweeping roc curve under sampling  increased
under sampling majority  negative  class move performance
lower left point upper right 

related work proposed shrink system classies overlapping region minority  positive  majority  negative  classes positive  searches best positive
region  kubat et al         
japkowicz        discussed eect imbalance dataset  evaluated three
strategies  under sampling  resampling recognition based induction scheme  focus
sampling approaches  experimented articial  d data order easily
measure construct concept complexity  two resampling methods considered 
random resampling consisted resampling smaller class random consisted
many samples majority class focused resampling consisted resampling
minority examples occurred boundary minority
majority classes  random under sampling considered  involved under sampling
majority class samples random numbers matched number minority
class samples  focused under sampling involved under sampling majority class samples
lying away  noted sampling approaches eective 
observed using sophisticated sampling techniques give clear advantage
domain considered  japkowicz        
one approach particularly relevant work ling li        
combined over sampling minority class under sampling majority
class  used lift analysis instead accuracy measure classiers performance 
proposed test examples ranked condence measure lift used
evaluation criteria  lift curve similar roc curve  tailored
   

fismote

marketing analysis problem  ling   li         one experiment  under sampled
majority class noted best lift index obtained classes equally
represented  ling   li         another experiment  over sampled positive
 minority  examples replacement match number negative  majority  examples
number positive examples  over sampling under sampling combination
provide signicant improvement lift index  however  approach oversampling diers theirs 
solberg solberg        considered problem imbalanced data sets oil slick
classication sar imagery  used over sampling under sampling techniques
improve classication oil slicks  training data distribution    oil
slicks       look alikes  giving prior probability      look alikes  imbalance
would lead learner  without appropriate loss functions methodology modify
priors  classify almost look alikes correctly expense misclassifying many
oil slick samples  solberg   solberg         overcome imbalance problem 
over sampled  with replacement      samples oil slick  randomly sampled
    samples non oil slick class create new dataset equal probabilities 
learned classier tree balanced data set achieved     error rate
oil slicks leave one out method error estimation  look alikes achieved
error rate     solberg   solberg        
another approach similar work domingos         compares
metacost approach majority under sampling minority over sampling 
nds metacost improves either  under sampling preferable minority over sampling  error based classiers made cost sensitive  probability
class example estimated  examples relabeled optimally
respect misclassication costs  relabeling examples expands decision
space creates new samples classier may learn  domingos        
feed forward neural network trained imbalanced dataset may learn discriminate enough classes  derouin  brown  fausett    schneider        
authors proposed learning rate neural network adapted statistics
class representation data  calculated attention factor proportion
samples presented neural network training  learning rate network
elements adjusted based attention factor  experimented articially
generated training set real world training set  multiple  more two 
classes  compared approach replicating minority class samples
balance data set used training  classication accuracy minority class
improved 
lewis catlett        examined heterogeneous uncertainty sampling supervised
learning  method useful training samples uncertain classes  training
samples labeled incrementally two phases uncertain instances passed
next phase  modied c    include loss ratio determining class
values leaves  class values determined comparison probability
threshold lr  lr       lr loss ratio  lewis   catlett        
information retrieval  ir  domain  dumais et al         mladenic   grobelnik 
      lewis   ringuette        cohen      a  faces problem class imbalance
dataset  document web page converted bag of words representation 
   

fichawla  bowyer  hall   kegelmeyer

is  feature vector reecting occurrences words page constructed  usually 
instances interesting category text categorization  overrepresentation negative class information retrieval problems cause problems
evaluating classiers performances  since error rate good metric skewed
datasets  classication performance algorithms information retrieval usually
measured precision recall 
recall  

tp
tp   fn

precision  

tp
tp   fp

mladenic grobelnik        proposed feature subset selection approach deal
imbalanced class distribution ir domain  experimented various
feature selection methods  found odds ratio  van rijsbergen  harper    porter 
      combined naive bayes classier performs best domain  odds
ratio probabilistic measure used rank documents according relevance
positive class  minority class   information gain word  hand 
pay attention particular target class  computed per word class 
imbalanced text dataset  assuming        negative class   features
associated negative class  odds ratio incorporates target class information
metric giving better results compared information gain text categorization 
provost fawcett        introduced roc convex hull method estimate
classier performance imbalanced datasets  note problems unequal
class distribution unequal error costs related little work done
address either problem  provost   fawcett         roc convex hull method 
roc space used separate classication performance class cost distribution
information 
summarize literature  under sampling majority class enables better classiers
built over sampling minority class  combination two done
previous work lead classiers outperform built utilizing undersampling  however  over sampling minority class done sampling
replacement original data  approach uses dierent method over sampling 

   smote  synthetic minority over sampling technique
    minority over sampling replacement
previous research  ling   li        japkowicz        discussed over sampling
replacement noted doesnt signicantly improve minority class recognition 
interpret underlying eect terms decision regions feature space  essentially 
minority class over sampled increasing amounts  eect identify similar
specic regions feature space decision region minority class 
eect decision trees understood plots figure   
   

fismote

 attributes      data original mammography dataset

 attributes      data original mammography dataset
   

   

   

   

   

attribute  

attribute  

   

   

   

   

   

   

  

  

 

 

 

 

 

 

  

  

  

 

  

 

 

 

 

attribute  

 

 

 

 

attribute  

 a 

 b 

 attributes      data original mammography dataset
   

attribute  

   

   

  

 

 

 

 

 

 

attribute  

 

 

 

 c 
figure    a  decision region three minority class samples  shown    reside
building decision tree  decision region indicated solid line
rectangle  b  zoomed in view chosen minority class samples
dataset  small solid line rectangles show decision regions result oversampling minority class replication  c  zoomed in view chosen
minority class samples dataset  dashed lines show decision region
over sampling minority class synthetic generation 

   

fichawla  bowyer  hall   kegelmeyer

data plot figure   extracted mammography dataset   woods
et al          minority class samples shown   majority class samples
shown plot  figure   a   region indicated solid line rectangle
majority class decision region  nevertheless  contains three minority class samples
shown   false negatives  replicate minority class  decision region
minority class becomes specic cause new splits decision tree 
lead terminal nodes  leaves  learning algorithm tries learn
specic regions minority class  essence  overtting  replication minority
class cause decision boundary spread majority class region  thus 
figure   b   three samples previously majority class decision region
specic decision regions 
    smote
propose over sampling approach minority class over sampled creating synthetic examples rather over sampling replacement  approach
inspired technique proved successful handwritten character recognition  ha
  bunke         created extra training data performing certain operations
real data  case  operations rotation skew natural ways perturb
training data  generate synthetic examples less application specic manner 
operating feature space rather data space  minority class over sampled
taking minority class sample introducing synthetic examples along line
segments joining any all k minority class nearest neighbors  depending upon
amount over sampling required  neighbors k nearest neighbors randomly
chosen  implementation currently uses nearest neighbors  instance 
amount over sampling needed       two neighbors nearest neighbors chosen one sample generated direction each  synthetic samples
generated following way  take dierence feature vector  sample 
consideration nearest neighbor  multiply dierence random number
     add feature vector consideration  causes
selection random point along line segment two specic features 
approach eectively forces decision region minority class become general 
algorithm smote   next page  pseudo code smote  table     shows
example calculation random synthetic samples  amount over sampling
parameter system  series roc curves generated dierent
populations roc analysis performed 
synthetic examples cause classier create larger less specic decision
regions shown dashed lines figure   c   rather smaller specic
regions  general regions learned minority class samples rather
subsumed majority class samples around them  eect decision trees generalize better  figures     compare minority over sampling
replacement smote  experiments conducted mammography dataset 
      examples majority class     examples minority class
originally  approximately      examples majority class     examples
   data available usf intelligent systems lab  http   morden csee usf edu chawla 

   

fismote

minority class training set used    fold cross validation  minority class
over sampled                             original size  graphs
show tree sizes minority over sampling replacement higher degrees
replication much greater smote  minority class recognition
minority over sampling replacement technique higher degrees replication isnt
good smote 
algorithm smote  t  n  k 
input  number minority class samples   amount smote n    number nearest
neighbors k
output   n        synthetic minority class samples
     n less       randomize minority class samples random
percent smoted   
   n      
  
randomize minority class samples
  
   n     
  
n      
   endif
   n    int  n        amount smote assumed integral multiples
      
   k   number nearest neighbors
   numattrs   number attributes
    sample        array original minority class samples
    newindex  keeps count number synthetic samples generated  initialized  
    synthetic        array synthetic samples
  compute k nearest neighbors minority class sample only   
     
   
compute k nearest neighbors i  save indices nnarray
   
populate n   i  nnarray 
    endfor
populate n  i  nnarray    function generate synthetic samples   
    n    
   
choose random number   k  call nn  step chooses one
k nearest neighbors i 
   
attr   numattrs
   
compute  dif   sample nnarray nn   attr  sample i  attr 
   
compute  gap   random number    
   
synthetic newindex  attr    sample i  attr    gap dif
   
endfor
   
newindex  
   
n   n  
    endwhile
    return   end populate   
end pseudo code 

   

fichawla  bowyer  hall   kegelmeyer

consider sample       let       nearest neighbor 
      sample k nearest neighbors identied 
      one k nearest neighbors 
let 
f        f        f      f        
f        f        f      f        
new samples generated
 f  f             rand               
rand      generates random number     
table    example generation synthetic examples  smote  

pruned decision tree size vs degree minority oversampling
   

   

decisiion tree size  number nodes 

   

   

   

   

   
synthetic data
replicated data
   

   

  

  

 

  

   

   

   
   
   
   
degree minority oversampling

   

   

   

figure    comparison decision tree sizes replicated over sampling smote
mammography dataset

   

fismote

  minority correct vs degree minority oversampling
  

 minority correct

  

  

  

synthetic data
replicated data

  

  
 

  

   

   

   

   

   

   

   

   

   

degree minority oversampling

figure    comparison   minority correct replicated over sampling smote
mammography dataset

    under sampling smote combination
majority class under sampled randomly removing samples majority class
population minority class becomes specied percentage majority class 
forces learner experience varying degrees under sampling higher degrees
under sampling minority class larger presence training set  describing
experiments  terminology under sample majority class
      would mean modied dataset contain twice many elements
minority class majority class  is  minority class    samples
majority class     samples under sample majority       majority
class would end    samples  applying combination under sampling
over sampling  initial bias learner towards negative  majority  class reversed
favor positive  minority  class  classiers learned dataset perturbed
smoting minority class under sampling majority class 

   experiments
used three dierent machine learning algorithms experiments  figure   provides
overview experiments 
   c     compared various combinations smote under sampling plain
under sampling using c    release    quinlan        base classier 
   

fichawla  bowyer  hall   kegelmeyer

smote
undersampling 

c   

loss ratio
modify costs majority minority
varied           
classes changing priors 

ripper

naive bayes

rocs generated smote  undersampling
loss ratio comparisons  performance
evaluated auc roc convex hull 

rocs generated comparison
smote under sampling using c    
smote using c    naive bayes 
performance evaluated auc roc convex hull 

figure    experiments overview

   ripper  compared various combinations smote under sampling
plain under sampling using ripper  cohen      b  base classier 
varied rippers loss ratio  cohen   singer        lewis   catlett           
       as means varying misclassication cost  compared eect
variation combination smote under sampling  reducing loss
ratio           able build set rules minority class 
   naive bayes classifier  naive bayes classier  made cost sensitive
varying priors minority class  varied priors minority
class      times majority class compared c   s smote
under sampling combination 

dierent learning algorithms allowed smote compared methods
handle misclassication costs directly   fp  tp averaged    fold
cross validation runs data combinations  minority class examples
over sampled calculating nearest neighbors generating synthetic examples 
auc calculated using trapezoidal rule  extrapolated extra point tp
       fp        roc curve  computed roc convex hull
identify optimal classiers  points lying hull potentially optimal
classiers  provost   fawcett        
   source code downloaded http   fuzzy cs uni magdeburg de borgelt software html 

   

fismote

    datasets
experimented nine dierent datasets  datasets summarized table     
datasets vary extensively size class proportions  thus oering dierent
domains smote  order increasing imbalance are 
   pima indian diabetes  blake   merz          classes     samples 
data used identify positive diabetes cases population near phoenix 
arizona  number positive class samples      good sensitivity
detection diabetes cases desirable attribute classier 
   phoneme dataset elena project    aim dataset
distinguish nasal  class    oral sounds  class       features 
class distribution       samples class         samples class   
   adult dataset  blake   merz               samples        samples
belonging minority class  dataset   continuous features   nominal
features  smote smote nc  see section      algorithms evaluated
dataset  smote  extracted continuous features generated new
dataset continuous features 
   e state data   hall  mohney    kier        consists electrotopological state
descriptors series compounds national cancer institutes yeast anticancer drug screen  e state descriptors nci yeast anticancer drug screen
generated tripos  inc  briey  series        compounds
tested series   yeast strains given concentration  test
high throughput screen one concentration results subject contamination  etc  growth inhibition yeast strain exposed given
compound  with respect growth yeast neutral solvent  measured 
activity classes either active least one single yeast strain inhibited
     inactive yeast strain inhibited     
dataset        samples       samples active compounds 
   satimage dataset  blake   merz          classes originally  chose
smallest class minority class collapsed rest classes one
done  provost et al          gave us skewed   class dataset      
majority class samples     minority class samples 
   forest cover dataset uci repository  blake   merz        
dataset   classes         samples  dataset prediction forest
cover type based cartographic variables  since system currently works binary classes extracted data two classes dataset ignored rest 
approaches work two classes  ling   li        japkowicz 
      kubat   matwin        provost   fawcett         two classes considered ponderosa pine        samples cottonwood willow      
   ftp dice ucl ac be directory pub neural nets elena databases 
   would thank steven eschrich providing dataset description us 

   

fichawla  bowyer  hall   kegelmeyer

dataset
pima
phoneme
adult
e state
satimage
forest cover
oil
mammography


majority class
   
    
     
     
    
     
   
     
      

minority class
   
    
     
    
   
    
  
   
    

table    dataset distribution
samples  nevertheless  smote technique applied multiple class problem well specifying class smote for  however  paper 
focused   classes problems  explicitly represent positive negative classes 
   oil dataset provided robert holte used paper  kubat et al  
       dataset    oil slick samples     non oil slick samples 
   mammography dataset  woods et al                samples     calcications  look predictive accuracy measure goodness classier
case  default accuracy would        every sample labeled noncalcication  but  desirable classier predict calcications
correctly 
   dataset generated exodusii data using avatar
 chawla   hall        version mustafa visualization tool    portion
crushed marked interesting rest
marked unknown  dataset size         samples       samples marked
interesting generated 
    roc creation
roc curve smote produced using c    ripper create classier
one series modied training datasets  given roc curve produced rst
over sampling minority class specied degree under sampling majority
class increasing degrees generate successive points curve  amount
under sampling identical plain under sampling  so  corresponding point
roc curve dataset represents number majority class samples  dierent
roc curves produced starting dierent levels minority over sampling  roc
curves generated varying loss ratio ripper          
varying priors minority class original distribution    times
majority class naive bayes classier 
   mustafa visualization tool developed mike glass sandia national labs 

   

fismote

phoneme roc
   

  

  

  

 tp

underc   
    smotec   
naive bayes
hull

  

  

  

  
  

  

  

  

  

  

  

  

  

   

 fp

figure    phoneme  comparison smote c     under c     naive bayes  smotec    dominates naive bayes under c    roc space  smotec    classiers potentially optimal classiers 

figures      show experimental roc curves obtained nine datasets
three classiers  roc curve plain under sampling majority class
 ling   li        japkowicz        kubat   matwin        provost   fawcett       
compared approach combining synthetic minority class over sampling  smote 
majority class under sampling  plain under sampling curve labeled under 
smote under sampling combination roc curve labeled smote  depending size relative imbalance dataset  one smote undersampling curves created  show best results smote combined
under sampling plain under sampling curve graphs  smote roc curve
c    compared roc curve obtained varying priors minority
class using naive bayes classier labeled naive bayes  smote  under 
loss ratio roc curves  generated using ripper compared  given family
roc curves  roc convex hull  provost   fawcett        generated  roc
convex hull generated using grahams algorithm  orourke         reference 
show roc curve would obtained using minority over sampling replication
figure    
point roc curve result either classier  c    ripper  learned
particular combination under sampling smote  classier  c    ripper 
learned plain under sampling  classier  ripper  learned using loss ratio
classier  naive bayes  learned dierent prior minority class  point
represents average   tp  fp     fold cross validation result  lower leftmost
point given roc curve raw dataset  without majority class under   

fichawla  bowyer  hall   kegelmeyer

phoneme roc ripper
   

  

 tp

  

  

underripper
    smoteripper
loss ratio
hull

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure    phoneme  comparison smote ripper  under ripper  modifying loss
ratio ripper  smote ripper dominates under ripper loss ratio
roc space  smote ripper classiers lie roc convex hull 

pima roc
   

  

  

  

 tp

  

  

underc   
    smotec   
naive bayes
hull

  

  

  

  

  
  

  

  

  

  

  

  

  

  

   

 fp

figure    pima indians diabetes  comparison smote c     under c     naive
bayes  naive bayes dominates smote c    roc space 

   

fismote

pima roc ripper
   

  

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

  
  

  

  

  

  

  

  

  

  

   

 fp

figure     pima indians diabetes  comparison smote ripper  under ripper 
modifying loss ratio ripper  smote ripper dominates under ripper
loss ratio roc space 

sampling minority class over sampling  minority class over sampled     
                              majority class under sampled          
                                                                                
              amount majority class under sampling minority class oversampling depended dataset size class proportions  instance  consider
roc curves figure    mammography dataset  three curves one
plain majority class under sampling range under sampling varied
         dierent intervals  one combination smote majority class
under sampling  one naive bayes one roc convex hull curve  roc
curve shown figure    minority class over sampled       point
smote roc curves represents combination  synthetic  over sampling undersampling  amount under sampling follows range plain under sampling 
better understanding roc graphs  shown dierent sets roc curves
one datasets appendix a 
dataset  smote lesser degree datasets
due structural nature dataset  dataset structural
neighborhood already established mesh geometry  smote lead creating
neighbors surface  and hence interesting   since looking
feature space physics variables structural information 
roc curves show trend increase amount under sampling coupled
over sampling  minority classication accuracy increases  course expense
majority class errors  almost roc curves  smote approach dom   

fichawla  bowyer  hall   kegelmeyer

satimage roc
   

  

  

  
underc   
    smotec   
naive bayes
hull

 tp

  

  

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     satimage  comparison smote c     under c     naive bayes 
roc curves naive bayes smote c    show overlap  however 
higher tps points smote c    lie roc convex hull 

satimage roc ripper
   

  

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     satimage  comparison smote ripper  under ripper  modifying loss
ratio ripper  smote ripper dominates roc space  roc convex
hull mostly constructed points smote ripper 

   

fismote

covtype roc
   

  

  

  

 tp

  
underc   
    smotec   
naive bayes
hull

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     forest cover  comparison smote c     under c     naive bayes 
smote c    under c    roc curves close other  however  points smote c    roc curve lie roc convex
hull  thus establishing dominance 

inates  adhering denition roc convex hull  potentially optimal
classiers ones generated smote 
    auc calculation
area roc curve  auc  calculated using form trapezoid rule 
lower leftmost point given roc curve classiers performance raw data 
upper rightmost point always               curve naturally end
point  point added  necessary order aucs compared
range  fp 
aucs listed table     show datasets combined synthetic minority over sampling majority over sampling able improve plain majority
under sampling c    base classier  thus  smote approach provides
improvement correct classication data underrepresented class 
conclusion holds examination roc convex hulls  entries
missing table  smote applied amounts datasets 
amount smote less less skewed datasets  also  included aucs
ripper naive bayes  roc convex hull identies smote classiers potentially optimal compared plain under sampling treatments misclassication
costs  generally  exceptions follows  pima dataset  naive bayes dominates
smote c     oil dataset  under ripper dominates smote ripper 
dataset  smote classifier  classifier   c    ripper  under classifier roc
   

fichawla  bowyer  hall   kegelmeyer

covtype roc ripper
   

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     forest cover  comparison smote ripper  under ripper  modifying
loss ratio ripper  smote ripper shows domination roc space 
points smote ripper curve lie roc convex hull 

oil roc
   

  

  

  

 tp

  

  

  
underc   
    smotec   
naive bayes
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     oil  comparison smote c     under c     naive bayes  although 
smote c    under c    roc curves intersect points  points
smote c    curve lie roc convex hull 

   

fismote

oil roc ripper
   

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     oil  comparison smote ripper  under ripper  modifying loss ratio
ripper  under ripper smote ripper curves intersect  points
under ripper curve lie roc convex hull 

mammography roc
   

  

  

  
underc   
    smotec   
naive bayes
hull

 tp

  

  

  

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     mammography  comparison smote c     under c     naive bayes 
smote c    under c    curves intersect roc space  however 
virtue number points roc convex hull  smote c   
potentially optimal classiers 

   

fichawla  bowyer  hall   kegelmeyer

mammography roc ripper
   

  

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     mammography  comparison smote ripper  under ripper  modifying
loss ratio ripper  smote ripper dominates roc space tp       
mammography roc c   
   

  

  

  

 tp

  

  

  
    smote
    replicate
hull

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     comparison over sampling minority class examples smote oversampling minority class examples replication mammography
dataset 

   

fismote

estate roc
   

  

  

  

 tp

  

  

  
underc   
    smotec   
naive bayes
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     e state   a  comparison smote c     under c     naive bayes 
smote c    under c    curves intersect roc space  however 
smote c    potentially optimal classiers  based number
points roc convex hull 

estate roc ripper
   

  

  

  

 tp

  

  

  
underripper
    smoteripper
loss ratio
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     e state  comparison smote ripper  under ripper  modifying loss
ratio ripper  smote ripper potentially optimal classiers  based
number points roc convex hull 

   

fichawla  bowyer  hall   kegelmeyer

roc
   

  

  

  

 tp

  

  

  

underc   
    smotec   
naive bayes
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     can  comparison smote c     under c     naive bayes  smotec    under c    roc curves overlap roc space 

roc ripper
   

  

  

  

 tp

  

  

  

underripper
   smoteripper
loss ratio
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     can  comparison smote ripper  under ripper  modifying loss ratio
ripper  smote ripper under ripper roc curves overlap
roc space 

   

fismote

dataset



pima
phoneme
satimage
forest cover
oil
mammography
e state


    
    
    
    
    
    
    
    

  
smote

    

   
smote
    
    
    
    
    
    
    
    

   
smote

   
smote

   
smote

   
smote

    
    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

table    aucs  c    base classier  best highlighted bold 

curves overlap roc space  datasets  smote classifier
potentially optimal classiers approach 
    additional comparison changing decision thresholds
provost        suggested simply changing decision threshold always
considered alternative sophisticated approaches  case c    
would mean changing decision threshold leaves decision trees  example 
leaf could classify examples minority class even     training
examples leaf represent majority class  experimented setting decision
thresholds leaves c    decision tree learner                                  
                                                              experimented phoneme
dataset  figure    shows comparison smote under sampling combination
c    learning tuning bias towards minority class  graph shows
smote under sampling combination roc curve dominating entire
range values 
    additional comparison one sided selection shrink
oil dataset  followed slightly dierent line experiments obtain results
comparable  kubat et al          alleviate problem imbalanced datasets
authors proposed  a  one sided selection under sampling majority class  kubat
  matwin         b  shrink system  kubat et al          table     contains
results  kubat et al          acc  accuracy positive  minority  examples
acc accuracy negative  majority  examples  figure    shows trend
acc  acc one combination smote strategy varying degrees undersampling majority class  y axis represents accuracy x axis represents
percentage majority class under sampled  graphs indicate band
under sampling          results comparable achieved
shrink better shrink cases  table     summarizes results
smote      under sampling combination  tried combinations smote
         varying degrees under sampling achieved comparable results 
   

fichawla  bowyer  hall   kegelmeyer

phoneme  roc comparison smote c    variation decision thresholds
   

  

 tp

  
smote
varying c    decision thresholds
hull
  

  

  
  

  

  

  

  

  

  

  

  

   

 fp

figure     smote under sampling combination c    learning tuning
bias towards minority class

smote undersampling
   

  

accuracy

  

accuracy majority  negative class 
accuracy minority  positive class 

  

  

  

  

  

 

   

   

   
   
   
   
percentage undersampling majority class

   

   

figure     smote      ou  under sampling combination performance

shrink approach smote approach directly comparable  though 
see dierent data points  smote oers clear improvement one sided selection 
   

fismote

method
shrink
one sided selection

acc 
     
     

acc
     
     

table    cross validation results  kubat et al        

under sampling  
   
   
   
   
   
    
    
    
    
    
    
    
    
    
    
    

acc 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

acc
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    cross validation results smote      smote oil data set 

   

fichawla  bowyer  hall   kegelmeyer

   future work
several topics considered line research  automated adaptive
selection number nearest neighbors would valuable  dierent strategies
creating synthetic neighbors may able improve performance  also  selecting
nearest neighbors focus examples incorrectly classied may improve
performance  minority class sample could possibly majority class sample
nearest neighbor rather minority class sample  crowding likely contribute
redrawing decision surfaces favor minority class  addition
topics  following subsections discuss two possible extensions smote 
application smote information retrieval 
    smote nc
smote approach currently handle data sets nominal features 
generalized handle mixed datasets continuous nominal features  call
approach synthetic minority over sampling technique nominal continuous  smote nc  
tested approach adult dataset uci repository  smote nc
algorithm described below 
   median computation  compute median standard deviations continuous
features minority class  nominal features dier sample
potential nearest neighbors  median included euclidean distance
computation  use median penalize dierence nominal features
amount related typical dierence continuous feature values 
   nearest neighbor computation  compute euclidean distance feature
vector k nearest neighbors identied  minority class sample 
feature vectors  minority class samples  using continuous feature space 
every diering nominal feature considered feature vector
potential nearest neighbor  include median standard deviations previously
computed  euclidean distance computation  table   demonstrates example 
f          b c  let sample computing nearest
neighbors 
f          e
f          b k
so  euclidean distance f  f  would be 
eucl   sqrt                            med    med   
med median standard deviations continuous features minority class 
median term included twice feature numbers    bd    ce 
dier two feature vectors  f  f  

table    example nearest neighbor computation smote nc 

   

fismote

   populate synthetic sample  continuous features new synthetic minority
class sample created using approach smote described earlier 
nominal feature given value occuring majority k nearest neighbors 
smote nc experiments reported set smote 
except fact examine one dataset only  smote nc adult dataset
diers typical result  performs worse plain under sampling based auc 
shown figures        extracted continuous features separate eect
smote smote nc dataset  determine whether oddity
due handling nominal features  shown figure     even smote
continuous features applied adult dataset  achieve better performance
plain under sampling  minority class continuous features high
variance  so  synthetic generation minority class samples could overlapping
majority class space  thus leading false positives plain under sampling 
hypothesis supported decreased auc measure smote degrees
greater      higher degrees smote lead minority class samples
dataset  thus greater overlap majority class decision space 
adult smotenc
   

  

  

  

 tp

  
underc   
   smotencc   
naive bayes
hull

  

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     adult  comparison smote c     under c     naive bayes  smotec    under c    roc curves overlap roc space 

    smote n
potentially  smote extended nominal features smote n
nearest neighbors computed using modied version value dierence metric  stanll
  waltz        proposed cost salzberg         value dierence metric  vdm 
looks overlap feature values feature vectors  matrix dening distance
   

fichawla  bowyer  hall   kegelmeyer

adult roc ripper
   

  

  

 tp

  

  

underripper
   smoteripper
loss ratio
hull

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     adult  comparison smote ripper  under ripper  modifying loss ratio ripper  smote ripper under ripper roc curves overlap
roc space 
adult continuous  c    
   

  

 tp

  

  


   smote

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     adult continuous features  overlap smote c    underc    observed scenario well 

   

fismote

corresponding feature values feature vectors created  distance
two corresponding feature values dened follows 
 v    v     

n

c i

 

i  

c 



c i k
 
c 

   

equation  v  v  two corresponding feature values  c  total
number occurrences feature value v    c i number occurrences feature
value v  class i  similar convention applied c i c    k constant 
usually set    equation used compute matrix value dierences
nominal feature given set feature vectors  equation   gives geometric distance
xed  nite set values  cost   salzberg         cost salzbergs modied vdm
omits weight term wfa included computation stanll waltz 
eect making symmetric  distance two feature vectors given by 

 x      wx wy

n


 xi   yi  r

   

i  

r     yields manhattan distance  r     yields euclidean distance  cost  
salzberg         wx wy exemplar weights modied vdm  wy    
new example  feature vector   wx bias towards reliable examples  feature
vectors  computed ratio number uses feature vector number
correct uses feature vector  thus  accurate feature vectors wx
   smote n ignore weights equation    smote n used
classication purposes directly  however  redene weights give weight
minority class feature vectors falling closer majority class feature vectors  thus 
making minority class features appear away feature vector
consideration  since  interested forming broader accurate regions
minority class  weights might used avoid populating along neighbors fall
closer majority class  generate new minority class feature vectors  create
new set feature values taking majority vote feature vector consideration
k nearest neighbors  table     shows example creating synthetic feature vector 
let f    b c e feature vector consideration
let   nearest neighbors
f    f c g n
f    h b c n
application smote n would create following feature vector 
fs   b c n
table    example smote n

   

fichawla  bowyer  hall   kegelmeyer

    application smote information retrieval
investigating application smote information retrieval  ir   ir problems come plethora features potentially many categories  smote would
applied conjunction feature selection algorithm  transforming given
document web page bag of words format 
interesting comparison smote would combination naive bayes
odds ratio  odds ratio focuses target class  ranks documents according
relevance target positive class  smote focuses target class creating
examples class 

   summary
results show smote approach improve accuracy classiers
minority class  smote provides new approach over sampling  combination
smote under sampling performs better plain under sampling  smote
tested variety datasets  varying degrees imbalance varying amounts
data training set  thus providing diverse testbed  combination smote
under sampling performs better  based domination roc space  varying
loss ratios ripper varying class priors naive bayes classier  methods
could directly handle skewed class distribution  smote forces focused learning
introduces bias towards minority class  pima least skewed dataset
naive bayes classier perform better smote c     also  oil
dataset under ripper perform better smote ripper  dataset 
smote classifier under classifier roc curves overlap roc space 
rest datasets smote classifier performs better under classifier  loss ratio 
naive bayes  total    experiments performed  smote classifier
perform best   experiments 
interpretation synthetic minority over sampling improves performance
minority over sampling replacement fairly straightforward  consider
eect decision regions feature space minority over sampling done
replication  sampling replacement  versus introduction synthetic examples 
replication  decision region results classication decision minority
class actually become smaller specic minority samples region
replicated  opposite desired eect  method synthetic over sampling
works cause classier build larger decision regions contain nearby minority
class points  reasons may applicable smote performs better
rippers loss ratio naive bayes  methods  nonetheless  still learning
information provided dataset  albeit dierent cost information  smote
provides related minority class samples learn from  thus allowing learner carve
broader decision regions  leading coverage minority class 

acknowledgments
research partially supported united states department energy
sandia national laboratories asci views data discovery program  contract number
   

fismote

de ac     do       thank robert holte providing oil spill dataset used
paper  thank foster provost clarifying method using satimage
dataset  would thank anonymous reviewers various insightful
comments suggestions 

   

fichawla  bowyer  hall   kegelmeyer

appendix a  roc graphs oil dataset
following gures show dierent sets roc curves oil dataset  figure     a 
shows roc curves oil dataset  included main text  figure    b  shows
roc curves without roc convex hull  figure    c  shows two convex hulls 
obtained without smote  roc convex hull shown dashed lines stars
figure    c   computed including under c    naive bayes family
roc curves  roc convex hull shown solid line small circles figure    c 
computed including     smote c     under c     naive bayes family
roc curves  roc convex hull smote dominates roc convex hull without
smote  hence smote c    contributes optimal classiers 
oil

  

  

  

  

  

  

  

  

 tp

   

  

  

  

  

underc   
    smotec   
naive bayes

  

  

  

 

  

  
underc   
    smotec   
naive bayes
hull

  

 

  

  

  

  

  
 fp

  

  

  

  

 

   

 

  

  

  

  

 a 

  
 fp

  

  

  

  

 b 
oil roc convex hulls
   

  

  

  

  

 tp

 tp

oil roc
   

  
convex hull smote
convex hull without smote

  

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

 c 
figure     roc curves oil dataset   a  roc curves smote c     underc     naive bayes  roc convex hull   b  roc curves smotec     under c     naive bayes   c  roc convex hulls without
smote 

   

   

fismote

references
blake  c     merz  c         
uci repository machine learning databases
http   www ics uci edu mlearn mlrepository html  department information
computer sciences  university california  irvine 
bradley  a  p          use area roc curve evaluation
machine learning algorithms  pattern recognition                  
chawla  n   bowyer  k   hall  l     kegelmeyer  p          smote  synthetic minority
over sampling technique  international conference knowledge based computer systems  pp        national center software technology  mumbai  india 
allied press 
chawla  n     hall  l          modifying mustafa capture salient data  tech  rep 
isl        university south florida  computer science eng  dept 
cohen  w       a   learning classify english text ilp methods  proceedings  th international workshop inductive logic programming  pp      
department computer science  katholieke universiteit leuven 
cohen  w  w       b   fast eective rule induction  proc    th international conference machine learning  pp         lake tahoe  ca  morgan kaufmann 
cohen  w  w     singer  y          context sensitive learning methods text categorization  frei  h  p   harman  d   schauble  p     wilkinson  r   eds    proceedings
sigir       th acm international conference research development
information retrieval  pp         zurich  ch  acm press  new york  us 
cost  s     salzberg  s          weighted nearest neighbor algorithm learning
symbolic features  machine learning               
derouin  e   brown  j   fausett  l     schneider  m          neural network training
unequally represented classes  intellligent engineering systems artificial
neural networks  pp         new york  asme press 
domingos  p          metacost  general method making classiers cost sensitive 
proceedings fifth acm sigkdd international conference knowledge
discovery data mining  pp         san diego  ca  acm press 
drummond  c     holte  r          explicitly representing expected cost  alternative
roc representation  proceedings sixth acm sigkdd international
conference knowledge discovery data mining  pp         boston  acm 
duda  r   hart  p     stork  d          pattern classification  wiley interscience 
dumais  s   platt  j   heckerman  d     sahami  m          inductive learning algorithms representations text categorization  proceedings seventh
international conference information knowledge management   pp         
   

fichawla  bowyer  hall   kegelmeyer

ezawa  k   j   singh  m     norton  s   w          learning goal oriented bayesian
networks telecommunications risk management  proceedings international conference machine learning  icml     pp         bari  italy  morgan
kauman 
fawcett  t     provost  f          combining data mining machine learning effective user prole  proceedings  nd international conference knowledge
discovery data mining  pp      portland  or  aaai 
ha  t  m     bunke  h          o line  handwritten numeral recognition perturbation
method  pattern analysis machine intelligence               
hall  l   mohney  b     kier  l          electrotopological state  structure information
atomic level molecular graphs  journal chemical information
computer science          
japkowicz  n          class imbalance problem  signicance strategies  proceedings      international conference artificial intelligence  ic ai      
special track inductive learning las vegas  nevada 
kubat  m   holte  r     matwin  s          machine learning detection oil
spills satellite radar images  machine learning             
kubat  m     matwin  s          addressing curse imbalanced training sets  one
sided selection  proceedings fourteenth international conference machine
learning  pp         nashville  tennesse  morgan kaufmann 
lee  s          noisy replication skewed binary classication  computational statistics
data analysis     
lewis  d     catlett  j          heterogeneous uncertainity sampling supervised learning  proceedings eleventh international conference machine learning  pp 
       san francisco  ca  morgan kaufmann 
lewis  d     ringuette  m          comparison two learning algorithms text
categorization  proceedings sdair      rd annual symposium document
analysis information retrieval  pp       
ling  c     li  c          data mining direct marketing problems solutions 
proceedings fourth international conference knowledge discovery data
mining  kdd     new york  ny  aaai press 
mladenic  d     grobelnik  m          feature selection unbalanced class distribution
naive bayes  proceedings   th international conference machine
learning   pp          morgan kaufmann 
orourke  j          computational geometry c  cambridge university press  uk 
pazzani  m   merz  c   murphy  p   ali  k   hume  t     brunk  c          reducing
misclassication costs  proceedings eleventh international conference
machine learning san francisco  ca  morgan kaumann 
   

fismote

provost  f     fawcett  t          robust classication imprecise environments  machine learning               
provost  f   fawcett  t     kohavi  r          case accuracy estimation
comparing induction algorithms  proceedings fifteenth international
conference machine learning  pp         madison  wi  morgan kaumann 
quinlan  j          c     programs machine learning  morgan kaufmann  san mateo 
ca 
solberg  a     solberg  r          large scale evaluation features automatic
detection oil spills ers sar images  international geoscience remote
sensing symposium  pp           lincoln  ne 
stanll  c     waltz  d          toward memory based reasoning  communications
acm                    
swets  j          measuring accuracy diagnostic systems  science                
tomek  i          two modications cnn  ieee transactions systems  man
cybernetics            
turney  p          cost sensitive bibliography  http   ai iit nrc ca bibiliographies costsensitive html 
van rijsbergen  c   harper  d     porter  m          selection good search terms 
information processing management           
woods  k   doss  c   bowyer  k   solka  j   priebe  c     kegelmeyer  p          comparative evaluation pattern recognition techniques detection microcalcications
mammography  international journal pattern recognition artificial intelligence                 

   


