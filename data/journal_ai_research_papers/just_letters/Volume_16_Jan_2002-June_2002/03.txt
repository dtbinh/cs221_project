journal of artificial intelligence research                  

submitted       published     

improving the eciency of inductive logic programming
through the use of query packs
hendrik blockeel cs kuleuven ac be

hendrik blockeel

katholieke universiteit leuven  department of computer science
celestijnenlaan    a  b      leuven  belgium

luc dehaspe pharmadm com

luc dehaspe
pharmadm  ambachtenlaan   d  b      leuven  belgium

bart demoen
gerda janssens
jan ramon

bart demoen cs kuleuven ac be
gerda janssens cs kuleuven ac be
jan ramon cs kuleuven ac be

katholieke universiteit leuven  department of computer science
celestijnenlaan    a  b      leuven  belgium

henk vandecasteele pharmadm com

henk vandecasteele

pharmadm  ambachtenlaan   d  b      leuven  belgium

abstract

inductive logic programming  or relational learning  is a powerful paradigm for machine
learning or data mining  however  in order for ilp to become practically useful  the
eciency of ilp systems must improve substantially  to this end  the notion of a query pack
is introduced  it structures sets of similar queries  furthermore  a mechanism is described
for executing such query packs  a complexity analysis shows that considerable eciency
improvements can be achieved through the use of this query pack execution mechanism 
this claim is supported by empirical results obtained by incorporating support for query
pack execution in two existing learning systems 
   introduction

many data mining algorithms employ to some extent a generate and test approach  large
amounts of partial or complete hypotheses are generated and evaluated during the data
mining process  this evaluation usually involves testing the hypothesis on a large data set 
a process which is typically linear in the size of the data set  examples of such data mining
algorithms are apriori  agrawal et al          decision tree algorithms  quinlan      a 
breiman et al          algorithms inducing decision rules  clark   niblett         etc 
even though the search through the hypothesis space is seldom exhaustive in practical
situations  and clever branch and bound or greedy search strategies are employed  the number of hypotheses generated and evaluated by these approaches may still be huge  this is
especially true when a complex hypothesis space is used  as is often the case in inductive
logic programming  ilp   where the sheer size of the hypothesis space is an important
contribution to the high computational complexity of most ilp approaches  this computational complexity can be reduced  however  by exploiting the fact that there are many
similarities between hypotheses 

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
most ilp systems build a hypothesis one clause at a time  this search for a single clause
is what we will be concerned with in the rest of this paper  and so the word  hypothesis 
further on will usually refer to a single clause  the clause search space is typically structured
as a lattice  because clauses close to one another in the lattice are similar  the computations
involved in evaluating them will be similar as well  in other words  many of the computations
that are performed when evaluating one clause  which boils down to executing a query
consisting of the body of the clause  will have to be performed again when evaluating the
next clause  storing certain intermediate results during the computation for later use could
be a solution  e g   tabling as in the xsb prolog engine  chen   warren         but may be
infeasible in practice because of its memory requirements  it becomes more feasible if the
search is reorganised so that intermediate results are always used shortly after they have
been computed  this can be achieved to some extent by rearranging the computations  the
best way of removing the redundancy  however  seems to be to re implement the execution
strategy of the queries in such a way that as much computation as possible is effectively
shared 
in this paper we discuss a strategy for executing sets of queries  organised in so called
query packs  that avoids the redundant computations  the strategy is presented as an adaptation of the standard prolog execution mechanism  the adapted execution mechanism
has been implemented in ilprolog  a prolog system dedicated to inductive logic programming  several inductive logic programming systems have been re implemented to make use
of this dedicated engine  and using these new implementations we obtained experimental
results showing in some cases a speed up of more than an order of magnitude  thus  our
work significantly contributes to the applicability of inductive logic programming to real
world data mining tasks  in addition  we believe it may contribute to the state of the art in
query optimisation in relational databases  indeed  in the latter field there has been a lot of
work on the optimisation of individual queries or relatively small sets of queries  but much
less on the optimisation of large groups of very similar queries  which understandably did
not get much attention before the advent of data mining  optimisation of groups of queries
for relational databases seems an interesting research area now  and we believe techniques
similar to the ones proposed here might be relevant in that area 
the remainder of this paper is structured as follows  in section   we precisely describe
the ilp problem setting in which this work is set  in section   we define the notion of a
query pack and indicate how it would be executed by a standard prolog interpreter and what
computational redundancy this causes  we further describe an execution mechanism for
query packs that makes it possible to avoid the redundant computations that would arise if
all queries in the pack were run separately  and show how it can be implemented by making a
few small but significant extensions to the wam  the standard prolog execution mechanism 
in section   we describe how the query pack execution strategy can be incorporated in two
existing inductive logic programming algorithms  tilde and warmr   in section   we
present experimental results that illustrate the speed up that these systems achieve by
using the query pack execution mechanism  in section   we discuss related work and in
section   we present conclusions and some directions for future work 

   

fiimproving the efficiency of ilp through query packs
   inductive logic programming

inductive logic programming  muggleton   de raedt        is situated in the intersection
of machine learning or data mining on the one hand  and logic programming on the other
hand  it shares with the former fields the goal of finding patterns in data  patterns that can
be used to build predictive models or to gain insight in the data  with logic programming
it shares the use of clausal first order logic as a representation language for both data
and hypotheses  in the remainder of this text we will use some basic notions from logic
programming  such as literals  conjunctive queries  and variable substitutions  we will use
prolog notation throughout the paper  for an introduction to prolog and logic programming
see bratko        
inductive logic programming can be used for many different purposes  and the problem
statements found in ilp papers consequently vary  in this article we consider the so called
learning from interpretations setting  de raedt   dzeroski        de raedt         it
has been argued elsewhere that this setting  while slightly less powerful than the standard
ilp setting  it has problems with  e g   learning recursive predicates   is sucient for most
practical purposes and scales up better  blockeel et al         
we formulate the learning task in such a way that it covers a number of different problem
statements  more specifically  we consider the problem of detecting for a set of conjunctive
queries for which instantiations of certain variables each query succeeds  these variables
are called key variables  and a grounding substitution for them is called a key instantiation 
the intuition is that an example in the learning task is uniquely identified by a single key
instantiation 
the link with ilp systems that learn clauses is then as follows  the search performed
by an ilp system is directed by regularly evaluating candidate clauses  let us denote such
a candidate clause by head x  
body  x  y   where x represents a vector of variables
appearing in the head of the clause and y represents additional variables that occur in the
body  we assume that the head is a single literal and that a list of examples is given  where
each example is of the form head x   with  a substitution that grounds x   examples
may be labelled  e g   as positive or negative   but this is not essential in our setting  while
an example can be represented as a fact head x   when learning definite horn clauses  we
can also consider it just a tuple x  both notations will be used in this paper 
intuitively  when positive and negative examples are given  one wants to find a clause
that covers as many positive examples as possible  while covering few or no negatives 
whether a single example head x   is covered by the clause or not can be determined
by running the query   body x  y    in other words  evaluating a clause boils down to
running a number of queries consisting of the body of the clause  for simplicity of notation 
we will often denote a conjunctive query by just the conjunction  without the   symbol  
in some less typical ilp settings  the ilp algorithm does not search for horn clauses
but rather for general clauses  e g   claudien  de raedt   dehaspe        or for frequent
patterns that can be expressed as conjunctive queries  e g   warmr dehaspe   toivonen 
       these settings can be handled by our approach as well  all that is needed is a mapping
from hypotheses to queries that allow to evaluate these hypotheses  such a mapping is
defined by de raedt and dehaspe        for claudien  for warmr it is trivial 

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
given a set of queries s and a set of examples e   the main task is to determine which
queries q   s cover which examples e   e   we formalise this using the notion of a result
set 

definition    result set  the result set of a set of queries s in a deductive database d
for key k and example set e   is
rs  s  k  d  e  

  f k  i jqi   s and k   e and qi  succeeds in dg

similar to the learning from interpretations setting defined in  de raedt         the
problem setting can now be stated as 

given  a set of conjunctive queries s   a deductive database d  a tuple k of variables
that occur in each query in s  and an example set e
find  the result set rs  s  k  d  e    i e   find for each query q in s those ground
instantiations  of k for which k   e and q succeeds in d 
example   assume an ilp system learning a definition for grandfather   wants to evaluate the following hypotheses 

grandfather x y     parent x z   parent z y   male x  
grandfather x y     parent x z   parent z y   female x  

examples are of the form grandfather gf  gc  where gf and gc are constants  hence
each example is uniquely identified by a ground substitution of the tuple  x  y    so in the
above problem setting the set of prolog queries s equals f    parent x z   parent z y  
male x        parent x z   parent z y   female x  g and the key k equals  x  y   
given a query qi   s   finding all tuples  x  y  for which   x  y   i    r  with r the result
set as defined above  is equivalent to finding which of the grandfather x y  facts in the
example set are predicted by the clause grandfather x y     qi  

the generality of our problem setting follows from the fact that once it is known which
queries succeed for which examples  the statistics and heuristics that typical ilp systems
use can be readily obtained from this  a few examples 






discovery of frequent patterns  dehaspe   toivonen         for each query qi the
number of key instantiations for which it succeeds just needs to be counted  i e  
f req  qi     jfk j k  i    rgj with r the result set 
induction of horn clauses  muggleton        quinlan      b   the accuracy of a
clause h    qi  defined as the number of examples for which body and head hold 
divided by the number of examples for which the body holds  can be computed as
jfkj k i  r dj hgj with r the result set 
jfkj k i  rgj
induction of first order classification or regression trees  kramer        blockeel  
de raedt        blockeel et al          the class entropy or variance of the examples
covered  or not covered  by a query can be computed from the probability distribution
of the target variable  computing this distribution involves simple counts similar to
the ones above 

   

fiimproving the efficiency of ilp through query packs
after transforming the grandfather   clauses into
grandfather  x y   i     parent x z   parent z y   male x   i     
grandfather  x y   i     parent x z   parent z y   female x   i     

the result set can clearly be computed by collecting for all grounding  s where k   e the
answers to the query    grandfather k i    in section   the queries will have a literal
i   i at the end or another goal which by side effects results in collecting the result set 
in practice  it is natural to compute the result set using a double loop  one over examples
and one over queries and one has the choice as to which is the outer loop  both the  examples
in outer loop  and the  queries in outer loop  have been used in data mining systems  in
the context of decision trees  see for instance quinlan      a  and mehta et al          we
shall see further that the redundancy removal approach we propose uses the  examples in
outer loop  strategy  in both approaches however  given a query and a key instantiation  we
are interested only in whether the query succeeds for that key instantiation  this implies
that after a particular query has succeeded on an example  its execution can be stopped 
in other words  computing the result set defined above boils down to evaluating each
query on each example  where we are only interested in the existence of success for each such
evaluation  computing more than one solution for one query on one example is unnecessary 
   query packs

for simplicity  we make abstraction of the existence of keys in the following examples  what
is relevant here  is that for each query we are only interested in whether it succeeds or not 
not in finding all answer substitutions 
given the following set of queries
p x  
p x  
p x  
p x  
p x  

i     
q x a  
q x b  
q x y  
q x y  

i     
i     
t x   i     
t x   r y     i     

we can choose to evaluate them separately  since we are only interested in one   the first  
success for each query  we would evaluate in prolog the queries
once  p x  
once  p x  
once  p x  
once  p x  
once  p x  

i       
q x a   i       
q x b   i       
q x y   t x   i       
q x y   t x   r y     i       

the wrapper once   is a pruning primitive and prevents the unnecessary search for more
solutions  its definition in prolog is simply
once goal     call goal     

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
an alternative way to evaluate the queries consists in merging them into one  nested 
disjunction as in 
p x    i  

 

q x a   i  

 

q x b   i  

 

q x y   t x    i  

 

r y     i     

the set of queries can now be evaluated as a whole  the success of one branch in the
disjunctive query corresponds to the success of the corresponding individual query 
compared to the evaluation of the individual queries  the disjunctive query has both an
advantage and a disadvantage 
  all the queries have the same prefix p x   which is evaluated once in each individual
query  while in the disjunctive query  the goal p x  is evaluated only once  depending
on the evaluation cost of p    this can lead to arbitrary performance gains 
the usual prolog pruning primitives are not powerful enough to prevent all the unnecessary backtracking after a branch in the disjunctive query has succeeded  this is
explained further in example   

example   in this example the literals

contribute to the discussion 

  i have been left out  because they do not

i

p x   q x  
p x   r x  

evaluating these queries separately means evaluating
once  p x   q x    
once  p x   r x    

or equivalently
p x   q x     
p x   r x     

the corresponding disjunctive query is
p x    q x    r x   

we can now try to place a pruning primitive in the disjunctive query      at the end of
each branch results in
p x    q x       r x     

the scope of the first cut is clearly too large  after the goal q x  has succeeded  the cut
will prevent entering the second branch  it means that adding the cut in the disjunctive
query leads to a wrong result 
using once   in the disjunctive query results in
p x    once q x     once r x   

   

fiimproving the efficiency of ilp through query packs
this results in a correct query  however  both branches are still executed for every
binding that the goal p x  produces  even if both branches have succeeded already 

the combination of the advantage of the disjunctive query with the advantage of the
individual query with pruning  once or cut  results in the notion of the query pack  syntactically  a query pack looks like a disjunctive query where the   control construct is
replaced by a new control construct denoted by or  so the query pack corresponding to the
disjunctive query above is
p x    i  

or

q x a   i  

or

q x b   i  

or

q x y   t x    i  

or

r y     i    

this query pack can be represented as the tree in figure    for a query pack q such a
tree has literals or conjunctions of literals in the nodes  each path from the root to a leaf
node represents a conjunctive query q which is a member of q  denoted q   q  the or
construct is implicit in the branching points 
p x 
i  

q x a  
i  

q x b  
i  

q x c  
i  
i  

q x y   t x 
r y    
i  

r y    
i  

figure    a query pack 
the intended procedural behaviour of the or construct is that once a branch has succeeded  it is effectively pruned away from the pack during the evaluation of the query pack
on the current example  this pruning must be recursive  i e   when all branches in a subtree
of the query pack have succeeded  the whole subtree must be pruned  evaluation of the
query pack then terminates when all subtrees have been pruned or all of the remaining
queries fail for the example 
the semantics of the or construct and its ecient implementation is the subject of the
rest of this section  it should however be clear already now that in the case that all the
answers of each query are needed  pruning cannot be performed and the disjunctive query
is already sucient  i e   query packs are useful when a single success per query suces 

    ecient execution of query packs
in section        a meta interpreter is given that defines the behaviour of query packs  in
practice this meta interpreter is not useful  because in many cases the meta interpreter itself
causes more overhead than the use of query packs can compensate for  indeed  previously
reported results  demoen et al         blockeel        indicate that the overhead involved
in a high level prolog implementation destroys the eciency gain obtained by redundancy
reduction  moreover as discussed in section        the meta interpreter does not have the
desired time complexity  this shows that the desired procedural semantics of or can be

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
implemented in prolog itself  but not with the desired performance because prolog lacks
the appropriate primitives 
the conclusion is that changes are needed at the level of the prolog engine itself  this
requires an extension of the wam  warren abstract machine  which is the underlying
abstract machine for most prolog implementations  the extended wam provides the or
operator as discussed above  it permanently removes branches from the pack that do not
need to be investigated anymore  this extended wam has become the basis of a new prolog
engine dedicated to inductive logic programming  called ilprolog  this section continues
with the introduction of some basic terminology for query packs and explains at a high level
how query pack execution works  next our meta interpreter for the query pack execution
is given and finally the changes needed for the wam are clarified 
      principles of query packs  execution 

before we discuss query pack execution in detail  note the following two points      during
the pack execution  the pruning of a branch must survive backtracking      when executing
a pack we are not interested in any variable instantiations  just in whether a member of the
pack succeeds or not  in our previous description we were interested in the binding to the
variable i  since each branch can bind i to only one value   the query number   we collect
these values in practice by a side effect denoted in section     by report success 
the starting point for the query pack execution mechanism is the usual prolog execution
of a query q given a prolog program p   by backtracking prolog will generate all the
solutions for q by giving the possible instantiations  such that q succeeds in p  
a query pack consists of a conjunction of literals and a set of alternatives  where each
alternative is again a query pack  note that leaves are query packs with an empty set of
alternatives  for each query pack q  conj  q  denotes the conjunction and children q 
denotes the set of alternatives  a set of queries is then represented by a so called root query
pack  for every query pack q  there is a path of query packs starting from the root query
pack qroot and ending at the query pack itself  namely   qroot   q         qn   q    the
query packs in this path are the predecessors of q  every query pack has a set of dependent
queries  dependent queries q   let   qroot   qi         qin   q   be the path to q  then
dependent queries q    fconj  qroot     conj  qi              conj  qin     conj  q    conj  qj     
        conj  qjm     conj  ql   j   q  qj         qjm   ql   is a path from q to a leaf ql g  note
that dependent queries qroot   are actually the members of the query pack as described
earlier 

qroot is the root of the tree  conj  qroot   is
qroot   contains the   query packs which correspond to the trees

example   for the query pack in figure   
p x    the set children 

rooted at the   sons of the root of the tree  suppose that these query packs are named  from
left to right  q    q    q    and q    then conj  q    equals  q x  a   i       children q   
equals the empty set  conj  q    equals  q x  y    t x     and dependent queries q    equals
f p x    q x  y    t x    i        p x    q x  y    t x    r y      i     g 

execution of a root query pack qroot aims at finding out which queries of the set
dependent queries qroot   succeed  if a query pack is executed as if the ors were usual
disjunctions  backtracking occurs over queries that have already succeeded and too many

   

fiimproving the efficiency of ilp through query packs
 
 
 
 
 
 
 
 
 
 
  
  

execute qp  pack q  substitution   f
while    next solution  conj  q  

f

for each qchild in children q  do

f
g

g

if   execute qp  qchild        success 
children q 
children q  n fqchild g

if   children q  is an empty set  return success 

return fail 

g

figure    the query pack execution algorithm 
successes are detected  to avoid this  it should be the case that as soon as a query succeeds 
the corresponding part of the query pack should no longer be considered during backtracking  our approach realises this by reporting success of queries  and of query packs  to
predecessors in the query pack  a  non root  query pack q can be safely removed if all the
queries that depend on it  i e   all the queries in dependent queries q   succeeded once 
for a leaf q  empty set of children   success of conj  q  is sucient to remove it  for a
non leaf q  we wait until all the dependent queries report success or equivalently until all
the query packs in children q  report success 
at the start of the evaluation of a root query pack  the set of children for every query
pack in it contains all the alternatives in the given query pack  during the execution  query
packs can be removed from children sets and thus the values of the children q  change
accordingly  when due to backtracking a query pack is executed again  it might be the case
that fewer alternatives have to be considered 
the execution of a query pack q is defined by the algorithm execute qp q     figure
   which imposes additional control on the usual prolog execution 
the usual prolog execution and backtracking behaviour is modelled by the while loop
 line    which generates all possible solutions  for the conjunction in the query pack  if
no more solutions are found  fail is returned and backtracking will occur at the level of the
calling query pack 
the additional control manages the children q   for each solution   the necessary
children of q will be executed  it is important to notice that the initial set of children of a
query pack is changed destructively during the execution of this algorithm  firstly  when a
leaf is reached  success is returned  line    and the corresponding child is removed from the
query pack  line     secondly  when a query pack that initially had several children  finally
ends up with an empty set of children  line     also this query pack is removed  line    
the fact that children are destructively removed  implies that when due to backtracking
the same query pack is executed again for a different   not all of the alternatives that
were initially there  have to be executed any more  moreover  by returning success the

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
a qp   
ch   

ch   
ch   

b qp   
ch   

f

q   

g qp   
ch   

ch   
ch   

c q   

d q   

ch   
ch   

e q   

h q   

i q   

j

q   

figure    query pack numbers qp i   query numbers q i  and child numbers ch i  in our
example 
backtracking over the current query pack conjunction conj  q  is stopped  all branches
have reported success 
      a meta interpreter for query packs

the first implementation of the query pack execution algorithm is the meta interpreter
meta execute qp q   the meta interpreter uses the following labelling in its representation
of a query pack 

 query pack number all the non leaf query packs in the tree are numbered  depth
first  from left to right  qp i   

 query number each leaf is numbered  from left to right  if the original queries were
numbered sequentially  then the numbers at the leaves correspond with these  q i   

 child number for each non leaf query pack with n children  all children are numbered
from   up to n sequentially  ch i   

consider the query pack a   b   c or d or e  or f or g   h or i or j    note that the
atoms in the example could in general be arbitrary conjunctions of non ground terms  its
labelling is shown in figure   
a labelled query pack q is then represented as a prolog term as follows  with qf the
father of q  



a leaf

q is represented by the term  c  leaf  qpnbf  chnb  qnb   with c the conj  q  
the query pack number of qf   chnb the child number of q w r t  qf   and qnb
the query number of q 



a non leaf q is represented by the term  c  or cs  qpnbf  qpnb  chnb  totcs  with c the
conj  q   cs the list children q   qpnbf the query pack number of qf   qpnb the query
pack number of q  chnb the child number of q w r t  qf   and totcs the total number
of children q    the query pack number of the father of the root query pack is
assumed to be zero 

qpnbf

   

fiimproving the efficiency of ilp through query packs
the example of figure   has the following representation  as a prolog term  
 a  or   b or   c leaf          d leaf          e leaf                    
 f leaf         
 g or   h leaf          i leaf          j leaf                     
         

during the execution of the meta interpreter  solved   facts are asserted  each fact
solved qpnb  chnb  denotes that the child with number chnb from query pack with number

has succeeded  such facts are asserted when reaching a leaf and also when all children
of a query pack have succeeded  the meta interpreter only executes children for which no
solved   fact has been asserted 
note that the time complexity of this meta interpreter is not yet as desired  execution
of a query pack will always be dependent on the number of original children  instead of the
number of remaining  as yet unsuccessful  children 
qpnb

run querypack q   preprocess q  qlabeled                  
  the code for preprocessing is given in appendix a
retractall solved       
meta execute qp qlabeled  
solved         
meta execute qp  a b        
call a  
meta execute qp b  
meta execute qp or cs  qpnbf  qpnb  childnb  totcs          or  corresponds to a non leaf query pack
handlechildren cs  qpnb     
all solved qpnb     totcs  
assert solved qpnbf childnb   
meta execute qp leaf qpnbf  childnb   querynb          leaf  corresponds to the end of a query
write succeed querynb    nl 
assert solved qpnbf childnb   
handlechildren         
handlechildren  c     qpnb  childnb   not solved qpnb childnb   
once meta execute qp c    fail 
handlechildren    cs   qpnb  childnb   childnb  is childnb     
handlechildren cs  qpnb  childnb   
all solved qpnb  childnb  totcs    childnb   totcs    true
 
childnb  is childnb     
solved qpnb  childnb   
all solved qpnb  childnb   totcs 
  

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
      wam extensions

to fully exploit the potential of a query pack  shared computation and avoidance of unnecessary backtracking  changes have to be made at the level of the prolog engine itself  the
explanation assumes a wam based prolog engine  at kaci        but a short explanation
of the execution of disjunction in prolog is given first  so that it becomes more easy to see
what was newly introduced in the wam 
assume that the body of a clause to be executed is a   b c   d   e   assume also that
all predicates have several clauses  at the moment that execution has reached the first
clause of c  the choice point stack looks like figure   a   there are choice points for the
activation of a  the disjunction itself  b and c  the choice points are linked together so that
backtracking can easily pop the top most one  each choice point contains a pointer to the
next alternative to be tried  only for the disjunction choice point  this alternative pointer
is shown  it points to the beginning of the second branch of the disjunction  after all
alternatives for b and c have been exhausted  this second branch is entered and d becomes
active  this is the situation shown in figure   b   at that point  the alternative of the
disjunction choice point refers to the last alternative branch of the disjunction  finally 
once e is entered  the disjunction choice point is already popped 
a   b  c   d   e 

a   b  c   d   e 

a   b  c   d   e 

a

a

a

 

 

e

b

d

c

 a  choice points just
after entering c 

 b  choice points just
after entering d 

 c  choice points just
after entering e 

figure    illustration of execution of disjunction in the wam 
when the goal a produces a new solution  all branches of the disjunction must be tried
again  it is exactly this we want to avoid for query packs  a branch that has succeeded once 
should never be re entered  we therefore adapt the disjunction choice point to become an
or choice point which is set up to point into a data structure that contains references to
each alternative in the or disjunction  this data structure is named the pack table  figure
  a  shows the state of the execution when it has reached c  it is similar to figure   a   the
or choice point now contains the information that the first branch is being executed  as the
execution proceeds  there are two possibilities  either this first branch succeeds or it fails 
we describe the failing situation for the first branch and explain what happens on success of

   

fiimproving the efficiency of ilp through query packs
the second branch  if the first branch has no solution  backtracking updates the alternative
in the or choice point  to point to the next branch in the pack table  the situation after
the second branch is entered is shown in   b  and is again similar to   b   suppose now
that the branch with the goal d succeeds  the entry in the pack table with or alternatives
is now adapted by erasing the second alternative branch  backtracking occurs  and the next
alternative branch of the or choice point is taken  this is shown in   c  
when a produces a new solution and the or disjunction is entered again  the pack table
does no longer contain the second alternative branch and it is never re entered  the pack
table is actually arranged in such a way that entries are really removed instead of just erased
so that they cause no overhead later 
a   b  c or d or e 

a   b  c or d or e 

a   b  c or d or e 

a

a

a

or

or

or

b

d

e

c

 a  the choice points just
after entering c 

 b  the choice points just
after entering d  the first
branch did not succeed  

 c  the choice points just
after entering e  d succeeded  

figure    illustration of execution of pack disjunction on the wam 
two more issues must be explained  first  the pack table with alternatives must be
constructed at runtime every time the query pack is entered for evaluation  this is done by
emitting the necessary instructions in the beginning of the code for the query pack  as an
example  we show the code for the query pack a   b c or d or e  in figure   
finally  in the example it is clear that at the moment that all alternatives of an ordisjunction have succeeded  a can stop producing more solutions  so the computation can
be stopped  in general   with nested query packs   it means that one pack table entry of
the next higher or node can be erased and this in a recursive way  the recursive removal
of entries from the pack tables  is done by the instruction query pack prune 
we have implemented this schema in ilprolog  section   presents some measurements
in ilprolog 

    using query packs
figure   shows an algorithm that makes use of the pack execution mechanism to compute
the result set r as defined in our problem statement  the set s of queries is here typically

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
construct pack table           
call a
query pack try
    call b
call c
query pack prune
    call d
query pack prune
    call e
query pack prune

figure    abstract machine code for a   b c or d or e   
the set of all refinements of a given query  i e   it does not correspond to the whole hypothesis
space  from a query pack q containing all queries in s   a derived pack q  is constructed
by adding a report success   literal to each leaf of the pack  the  procedural  task of
report success k i  is simply to add  k  i  to the result set r  obviously a specific
ilp system not interested in the result set itself could provide its own report success  
predicate and thus avoid the overhead of explicitly building the result set  
  evaluate set of examples e   pack q  key k   f
 
q  q 
 
q
  
 
for each leaf of q  do f
 
add report success k  q  to the right of the conjunction in the leaf
 
increment q
 
g
 
c
 evaluate pack k      q    
 
compile and load c  
  
for each example e in e do f
  
evaluate pack e  
  
g
   g
figure    using query packs to compute the result set 
note that the algorithm in figure   follows the strategy of running all queries for each
single example before moving on to the next example  this could be called the  examples in
outer loop  strategy  as opposed to the  queries in outer loop  strategy used by most ilp
   in our current implementation the result set is implemented as a bit matrix indexed on queries and
examples  this implementation is practically feasible  on typical computers at the time of writing  even
when the number of queries in the pack multiplied by the number of examples is up to a billion  a bound
which holds for most current ilp applications 

   

fiimproving the efficiency of ilp through query packs
systems  the  examples in outer loop  strategy has important advantages when processing
large data sets  mainly due to the ability to process them eciently without having all data
in main memory at the same time  mehta et al         blockeel et al         

    computational complexity
we estimate the speedup factor that can be achieved using query pack execution in two
steps  first we consider one level packs  then we extend the results towards deeper packs 
lower and upper bounds on the speedup factor that can be achieved by executing a
one level pack instead of separate queries can be obtained as follows  for a pack containing
n queries qi    a  bi    let ti be the time needed to compute the first answer substitution of
qi if there are any  or to obtain failure otherwise  let ti be the part of ti spent within a
and t i the part of ti spent in bi   then ts   i  ti   t i   and tp   max ti     i t i with ts
representing the total time needed for executing all queries separately and tp the total time
needed for executing the pack  introducing c   i ti   i t i   which roughly represents the
ratio of the computational complexity in the shared part over that in the non shared part 
we have
 
c  
ts
i ti  
i ti
 
  maxi ti
   
 
tp
maxi ti   i ti
  
t 

p

p

p

p p

p
p

p

i i

now defining k as the ratio of the maximal ti over the average ti   i e 

we can rewrite equation     as

since

p

i ti
n

pmaxt  nt
i i

k

 

ts
tp

 

i i

c  

k
nc

   

  

 max ti  pi ti we know    k  n  which leads to the following bounds 
 

ts
tp



c  

c
n

  

  min c      n 

   

thus the speedup factor is bounded from above by the branching factor n and by the
ratio c of computational complexity in the shared part over the computational complexity
of the non shared part  and a maximal speedup can be attained when max ti   ti  n  or 
k       in other words when the ti for all queries are approximately equal 
for multi level packs  we can estimate the eciency gain as follows  given a query qi  
let ti be defined as above  the total time for finding   answer to qi or obtaining failure  
instead of ti and t i   we now define ti l as the time spent on level l of the pack when solving qi  
counting the root as level   and denoting the depth of the pack with d we have ti   dl   ti l  
further define ti l as the time spent on level l or deeper  ti l   dj l ti j with d the depth
of the pack   thus ti   ti       we will assume a constant branching factor b in the pack 
finally  we define tl   i ti l  n with n   bd   for simplicity  in the formulae we implicitly
assume that i always ranges from   to n with n the number of queries  unless explicitly

p

p

p

   

p

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
specified otherwise  we then have
tp

  max ti  
i

x
  t
i

i  

  max ti  
i

x
   max t
b

j   

 

i gj

i  

xt

 

 

i gj

i  

 

   

where j           b is the index of a child of the root and gj is the set of indexes of the
queries belonging to that child  now define k    maxi ti    t  and define k  as the smallest
number such that maxi gj ti    k  tj   with tj     i gj ti    b  note    k    k   b  it
then follows that
b
b
max ti    k  tj     k  bt 
   
i gj
j   
j   

p

x

x

which allows us to rewrite equation     into
tp

 k t    k  bt   

xt
i

   

i  

where the equality holds if maxi gj ti   is equal in all gj   the reasoning can be continued
up till the lowest level of the pack  yielding
tp

 k t    bk t    b k  t         bd

and finally
tp

 k  t    bk  t    b  k t         bd



 k

d   td  

 k

 



d   td  

xt

i d

   

i

  bd td

   

with all kl between   and b  we will further simplify the comparison with ts by assuming
 l   kl      the kl can then be dropped and the inequality becomes an equality  because
all maxima must be equal  
tp

  t    bt    b  t         bd   td

 

  bd td

   

 

  bd td

    

note that for ts we have
ts

  bd t    bd t    bd t         bd td

it is clear  then  that the speedup will be governed by how the bd tk terms compare to the
bk tk terms   in the worst case  where kk   b  the latter become bk   tk    we therefore
introduce rl m as follows 
m
bm tk
    
rl m   km l k

k  l b tk

p
p

the r coecients are always between    if tm dominates  and bm l  if tl strongly dominates  
for all tl equal  rl m is approximately m l 
further  similar to c in our previous analysis  define

p    b t
c   p

     b t
l

l
k

d
k l

   

k

k

k

k

    

fiimproving the efficiency of ilp through query packs
some algebra then gives

ts
tp

 

bd l cl r  l   rl   d
cl    

    

which needs to hold for all l  we can interpret this as follows  for a certain level l  cl roughly
reects the speedup gained by the fact that the part up till level l needs to be executed
only once  the r factors reect the speedup obtained within these parts because of the pack
mechanism 
this inequality holds for all l  hence we will find the best lower bound for the speedup factor by maximizing the right hand side  note that cl increases and bd l decreases
monotonically with l  it is clear that if at some point cl becomes much larger than    a
speedup factor of roughly bd l is obtained  on the other hand  if cl is smaller than    then
the behaviour of bd l cl is crucial  now 
bd l cl

tl     tl          b l t 
     b
 
td   b td          bd  l   tl  

our conclusion is similar to that for the one level pack  if for some l  cl       i e   if in
the upper part of the pack  up till level l  computations take place that are so expensive
that they dominate all computations below level l  even taking into account that the latter
are performed bd l times more often   then a speedup of bd l can be expected  if cl      
which will usually be the case for all l except those near d  the speedup can roughly be
estimated as tl  td   the maximum of all these factors will determine the actual speedup 
   adapting ilp algorithms to use query packs

in this section we discuss how the above execution method can be included in ilp algorithms  and illustrate this in more detail for two existing ilp algorithms  experimental
results concerning actual eciency improvements this yields are presented in the next section 

    refinement of a single rule
many systems for inductive logic programming use an algorithm that consists of repeatedly
refining clauses  any of these systems could in principle be rewritten to make use of a query
pack evaluation mechanism and thus achieve a significant eciency gain  we first show this
for a concrete algorithm for decision tree induction  then discuss the more general case 
      induction of decision trees

the first algorithm we discuss is tilde  blockeel   de raedt         an algorithm that
builds first order decision trees  in a first order decision tree  nodes contain literals that
together with the conjunction of the literals in the nodes above this node  i e   in a path
from the root to this node  form the query that is to be run for an example to decide which
subtree it should be sorted into  when building the tree  the literal  or conjunction of
literals  to be put in one node is chosen as follows  given the query corresponding to a path
from the root to this node  generate all refinements of this query  a refinement of a query

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
is formed by adding one or more literals to the query   evaluate these refinements on the
relevant subset of the data   computing  e g   the information gain  quinlan      a  yielded
by the refinement  choose the best refinement  and put the literals that were added to the
original clause to form this refinement in the node 
at this point it is clear that a lot of computational redundancy exists if each refinement
is evaluated separately  indeed all refinements contain exactly the same literals except those
added during this single refinement step  organising all refinements into one query pack  we
obtain a query pack that essentially has only one level  the root immediately branches into
leaves   when tilde s lookahead facility is used  blockeel   de raedt         refinements
form a lattice and the query pack may contain multiple  though usually few  levels 
note that the root of these packs may consist of a conjunction of many literals  giving
the pack a broom like form  the more literals in the root of the pack  the greater the benefit
of query pack execution is expected to be 

example   assume the node currently being refined has the following query associated with

it     circle a c  leftof a c d  above a d e   i e   the node covers all examples a
where there is a circle to the left of some other object which is itself above yet another object 
the query pack generated for this refinement could for instance be

circle a c   leftof a c d   above a d e  

triangle a f 
circle a h 
small a i 
large a j 
in a e k 
in a d l 
in a c m 
above a e n 
above a d o 
above a c p 
leftof a e q 
leftof a d r 
leftof a c s 

when evaluating this pack  all backtracking through the root of the pack  the  stick 
of the broom  will happen only once  instead of once for each refinement  in other words 
when evaluating queries one by one  for each query the prolog engine needs to search once
again for all objects c   d and e fulfilling the constraint circle a c   leftof a c d  
above a d e   when executing a pack this search is done only once 
      other algorithms based on rule refinement

as mentioned  any ilp algorithm that consists of repeatedly refining clauses could in principle be rewritten to make use of a query pack evaluation mechanism and thus achieve a
significant eciency gain  consider  e g   a rule induction system performing an a search
through a refinement lattice  such as progol  muggleton         since a imposes a certain order in which clauses will be considered for refinement  it is hard to reorganise the
computation at this level  however  when taking one node in the list of open nodes and
producing all its refinements  the evaluation of the refinements involves executing all of
them  this can be replaced by a pack execution  in which case a positive eciency gain is
guaranteed  in principle one could also perform several levels of refinement at this stage 
   i e   that subset of the original data set for which the parent query succeeded  or  in the decision tree
context  the examples sorted into the node that is being refined 

   

fiimproving the efficiency of ilp through query packs
adding all of the refinements to a  s queue  part of the eciency of a is then lost  but
the pack execution mechanism is exploited to a larger extent  which of these two effects
is dominant will depend on the application  if most of the first level refinements would
be further refined anyway at some point during the search  clearly there will be a gain in
executing a two level pack  otherwise there may be a loss of eciency  for instance  if
executing a two level pack takes x times as much time as a one level pack  it will bring an
eciency gain only if at least x of the first level refinements would afterwards be refined
themselves 

    level wise frequent pattern discovery
an alternative family of data mining algorithms scans the refinement lattice in a breadthfirst manner for queries whose frequency exceeds some user defined threshold  the bestknown instance of these level wise algorithms is the apriori method for finding frequent
item sets  agrawal et al          warmr  dehaspe   toivonen        is an ilp variant of
attribute value based apriori 
query packs in warmr correspond to hash trees of item sets in apriori  both are used
to store a subgraph of the total refinement lattice down to level n  the paths from the root
down to level n   in that subgraph correspond to frequent patterns  the paths from root
to the leaves at depth n correspond to candidates whose frequency has to be computed 
like hash trees in apriori  query packs in warmr exploit massive similarity between
candidates to make their evaluation more ecient  essentially the warmr algorithm starts
with an empty query pack and iterates between pack evaluation and pack extension  see
figure     the latter is achieved by adding all potentially frequent refinements  of all leaves
in the pack  i e   adding another level of the total refinement lattice 
   experiments

the goal of this experimental evaluation is to empirically investigate the actual speedups
that can be obtained by re implementing ilp systems so that they use the pack execution
mechanism  at this moment such re implementations exist for the tilde and warmr
systems  hence we have used these for our experiments  these re implementations are
available within the ace data mining tool  available for academic use upon request   we
attempt to quantify  a  the speedup of packs w r t  to separate execution of queries  thus
validating our complexity analysis   and  b  the total speedup that this can yield for an
ilp system 
the data sets that we have used for our experiments are the following 



the mutagenesis data set   an ilp benchmark data set  introduced to the ilp community by srinivasan et al          that consists of structural descriptions of    
molecules that are to be classified as mutagenic or not  next to the standard mutagenesis data set  we also consider versions of it where each example occurs n times 

   refinements found to be specialisations of infrequent queries cannot be frequent themselves  and are
pruned consequently 
   see http   www cs kuleuven ac be  dtai ace  

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele

circle a b 

triangle a b 

leftof a b c  above a b c  leftof a b c 

expand

circle a b 

leftof a b c 

triangle a b 

above a b c 

leftof a b c 

circle a c triangle a c  circle a c triangle a c  circle a c triangle a c 
evaluate
circle a b  triangle a b 
above a b c 

leftof a b c 

triangle a c  circle a c triangle a c 

circle a b 

expand

above a b c 
triangle a c 

triangle a b 
leftof a b c 

circle a c 

triangle a c 

leftof a c d  leftof a c d  above a c d  leftof a c d 

figure    a sequence of   query packs in warmr  refinement of the above left query
pack results in the   level pack above right  removal of queries found infrequent
during pack evaluation results in the bottom left pack  finally  another level is
added in a second query expansion step to produce the bottom right pack  this
iteration between expansion and evaluation continues until the pack is empty 
this allows us to easily generate data sets of larger size where the average example
and query complexity are constant and equal to those of the original data set 



bongard data sets   introduced in ilp by de raedt and van laer         the so called
 bongard problems  are a simplified version of problems used by bongard        for
research on pattern recognition  a number of drawings are shown containing each a
number of elementary geometrical figures  the drawings have to be classified according
to relations that hold on the figures in them  we use a bongard problem generator
to create data sets of varying size 

the experiments were run on sun workstations  a sparc ultra    at     mhz for
tilde  a sparc ultra    at     mhz for warmr  tilde and warmr were run with their
default settings  except where mentioned differently 

    tilde
we consider three different ways in which tilde can be run in its ilprolog implementation 
   no packs  the normal implementation of tilde as described by blockeel and de raedt
        where queries are generated one by one and each is evaluated on all relevant
examples  since queries are represented as terms  each evaluation of a query involves
a meta call in prolog 

   

fiimproving the efficiency of ilp through query packs
   disjoint execution of packs  a query pack is executed in which all queries in the pack
are put beside one another  i e   common parts are not shared by the queries  the
computational redundancy in executing such a pack is the same as that in executing
all queries one after another  the main difference is that in this case all queries are
compiled 
   packed execution of packs  a compiled query pack is executed where queries share as
much as possible 
the most interesting information is obtained by comparing  a  the actual query evaluation time in settings   and    this gives a view of the eciency gain obtained by the
removal of redundant computation itself  we will abbreviate this as exec in the tables  
and  b  the total execution time in settings   and    this provides an indication of how
much is gained by implementing packs in an ilp system  taking all other effects into account  re implementation of the computation of heuristics via a bit matrix  use of compiled
queries instead of meta calls  etc    or in other words  what the net effect of the whole
re implementation is  indicated as net in the tables  
in a first experiment we used bongard problems  varying     the size of the data sets 
    the complexity of the target hypothesis  and     tilde s lookahead parameter  the
complexity of the target hypothesis can be small  medium  or none  in the latter case the
examples are random  which causes tilde to grow ever larger trees in an attempt to find
a good hypothesis  the size of the final tree then typically depends on the size of the data
set  the lookahead parameter is used to control the number of levels the pack contains 
with lookahead n  packs of depth n     are generated 
table   gives an overview of results for the bongard problems  the total induction
time is reported  as well as  for pack based execution mechanisms  the time needed for
pack compilation and pack execution  note that the total time includes not only pack
compilation and execution  but also all other computations not directly related to packs
 e g   the computation of heuristics from the bitmatrix   the results can be interpreted as
follows 
first of all  the table shows that significant speedups can be obtained by using the pack
mechanism  net speedups of over a factor     are obtained  while the execution itself is up
to    times faster compared to disjoint execution 
a further observation is that for more complex target hypotheses greater speedups are
obtained  this can be explained by the broom like form of the packs in tilde  complex
target hypotheses correspond to deep trees  and refinement of a node at a lower level of
such a tree yields a pack with a long clause before the branching  which in accordance with
our previous analysis should yield a speedup closer to the branching factor b in the case
of lookahead    and more generally  closer to bl   for lookahead l  although the latter is
much harder to achieve   note that the maximum branching factor occurring in each pack
is included in the table in column bf  
finally  deeper packs also yield higher speedups  and this effect is larger for more complex
theories  this is understandable considering the following  let us call the clause that is
being refined c  with lookahead l  conjunctions of l     literals are added to the clause  in
some cases the first of these l     literals may fail immediately  which causes this branch of
the pack to have almost no execution time  while cutting away bl queries  remember that

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
la

bf

 
 
 
 

  
  
  
  

 
 
 
 

  
  
  
  

 
 
 
 

  
  
  
  

 
 
 
 

  
  
  
  

 
 
 
 

  
  
  
  

 
 
 
 

  
  
  
  

 
 
 
 

  
  
  
  

 
 
 
 

  
  
  
  

 
 
 
 

  
  
  
  

original

    
    
    
    

    
    
    
    

    
    
    
   

    
    
    
    

    
    
    
   

    
     
   
   

    
     
    
   

    
    
   
    

    
   
    
    

disjoint
packed
comp exec total comp
simple target hypothesis
     examples
    
    
    
    
    
    
    
    
    
    
    
   
    
    
    
     
    
    
    
    
     examples
    
    
    
    
    
    
    
    
    
    
    
    
    
   
    
    
    
    
    
    
     examples
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
medium complexity target hypothesis
     examples
    
    
    
    
    
   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     examples
    
    
    
    
   
    
   
    
   
    
    
          
    
    
     
    
    
    
    
     examples
    
    
   
    
    
     
   
    
   
    
    
    
    
    
    
   
   
    
   
    
no target hypothesis
     examples
    
    
    
    
    
     
                     
   
    
    
    
    
   
   
    
   
    
     examples
    
    
    
   
    
    
    
    
     
    
   
   
  
     
     
    
    
   
   
   
     examples
    
    
    
    
    
   
    
    
    
    
    
    
   
   
   
    
              
   
total

speedup

exec

net

exec

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

   

    

   

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

 

    

    

    

    

    

    

    

    

    

    

    

    

   

    

    

    

    

    

    

    

    

    

    

    

   

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

 

    

    

    

    

    

table    timings of tilde runs on the bongard data sets  la   lookahead setting  bf  
maximum branching factor  reported times  in seconds  are the total time needed
to build a tree  and the time spent on compilation respectively execution of packs 

   

fiimproving the efficiency of ilp through query packs
la original
 
 
 

    
      
    

 
 
 

    
     
   

disjoint
packed
total comp exec total comp
regression      examples
                        
                     
 
 
 
   
   
classification      examples
                        
                        
 
 
 
        

table    timings of tilde runs for mutagenesis  a
ended prematurely 

exec

speedup ratio
net
exec

          
         
         

    
    
 

    
    
    

    
    
 

    
    
   

in the table indicates that that run

according to our analysis  the speedup can in the limit approximate bl if the complexity of
clause c dominates over the complexity of the rest of the pack  such  early failing branches 
in the pack cause the actual situation to approximate closer this ideal case 
we have also run experiments on the mutagenesis data set  table     both in a regression
and a classification setting  here  query packs are much larger than for the bongard data set
 there is a higher branching factor   with a lookahead of   the largest packs had over      
queries  for these large packs a significant amount of time is spent compiling the pack  but
even then clear net speedups are obtained   a comparison of execution times turned out
infeasible because in the disjoint execution setting the pack structures consumed too much
memory 

    warmr
      used implementations

for warmr we consider the following implementations 
   no packs  the normal implementation of warmr  where queries are generated  and
for all examples the queries are evaluated one by one 
   with packs  an implementation where first all queries for one level are generated and
put into a pack  and then this pack is evaluated on each example 
      datasets

mutagenesis we used the mutagenesis dataset of     molecules  where each example is
repeated    times to make more accurate timings possible and to have a better idea of the
effect on larger datasets  we used three different language biases   small  is a language
   in one case  with a relatively small pack  the system became slower  the timings indicate that this is
not due to the compilation time  but to other changes in the implementation which for this relatively
simple problem were not compensated by the faster execution of the packs 

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele

level
 
 
 
 
 
 
 
 
 

small

mutagenesis

medium

large

queries frequent queries frequent queries frequent
 
 
  
  
  
  
  
  
   
  
    
   
  
  
   
   
    
    
   
  
   
   
  
  
   
   
  
  
    
    
  
  
 
 
  
  
 
 
 
 
 
 

table    number of queries for the mutagenesis experiment with warmr 
bias that was chosen so as to generate a limited number of refinements  i e   a relatively
small branching factor in the search lattice   this allows us to generate query packs that are
relatively deep but narrow   medium  and  large  use broader but more shallow packs 
table   summarises the number of queries and the number of frequent queries found for
each level in the different languages 

bongard we use bongard      for experiments with

warmr as this system does not
construct a theory and hence the existence of a simple theory is not expected to make much
difference 
      results

in tables      and   the execution times of warmr on mutagenesis are given  with maximal
search depth varying from   for the large language to   levels for the small language  here 
 total  is the total execution time and  exec  is the time needed to test the queries against
the examples  in table   the execution times of warmr on bongard are given 
      discussion

the execution time of warmr has a large component that is not used to evaluate queries 
this is caused by the fact that warmr needs to do a lot of administrative work  in
particular  theta subsumption tests should be done on the queries to check wether a query
is equivalent to another candidate  or if a query is a specialisation of an infrequent one 
in the propositional case  the apriori algorithm   these tests are very simple  but in the
first order case they require exponential time in the size of the queries  of course  when
using larger datasets  the relative contribution of these administrative costs will decrease
proportionally  it can be observed that at deeper levels  these costs are less for the setting
using packs  one of the causes is the fact that the no packs version also uses more memory
than the packs setting  and hence causes proportionally more memory management  
here again  the most important numbers are the speedup factors for the execution of
queries  speedup factors of query execution do not always increase with increasing depth of

   

fiimproving the efficiency of ilp through query packs

level
 
 
 
 
 
 
 
 
 

no packs
with packs ilprolog
total
exec
total
exec
    
    
    
    
    
    
    
    
     
           
    
            
     
     
                    
     
                    
     
                    
     
                    
     
                      
     

speedup ratio
net
exec
         
         
         
         
         
         
         
         
         

table    results for warmr on the mutagenesis dataset using a small language 

level
 
 
 
 
 
 

no packs
with packs ilprolog
total
exec
total
exec
    
    
    
    
      
     
     
     
                    
     
                      
     
                       
     
                        
      

speedup ratio
net
exec
         
         
         
         
         
         

table    results for warmr on the mutagenesis dataset using a medium language 

level
 
 
 

no packs
with packs ilprolog
total
exec
total
exec
    
    
    
    
      
             
     
                        
      

speedup ratio
net
exec
         
         
         

table    results for warmr on the mutagenesis dataset using a large language 

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
level
 
 
 
 
 
 
 
 
 
  

no packs
total
exec
    
    
    
    
    
    
          
           
           
            
            
            
            
table   

with packs ilprolog
total
exec
    
    
    
    
    
    
    
    
     
    
     
     
     
     
      
     
      
     
      
     

warmr

speedup ratio
net
exec
         
         
         
         
         
         
         
         
         
         

results on bongard 

the packs  in contrast to tilde where larger packs yielded higher speedups  at first sight
we found this surprising  however it becomes less so when the following observation is made 
when refining a pack into a new pack by adding a level  warmr prunes away branches that
lead only to infrequent queries  there are thus two effects when adding a level to a pack 
one is the widening of the pack at the lowest level  at least on the first few levels  a new
pack typically has more leaves than the previous one   the second is the narrowing of the
pack as a whole  because of pruning   since the speedup obtained by using packs largely
depends on the branching factor of the pack  speedup factors can be expected to decrease
when the narrowing effect is stronger than the widening at the bottom effect  this can
be seen  e g  in the small mutagenesis experiment  where at the deepest levels queries are
becoming less frequent  for the mutagenesis experiment with the medium size language 
query execution speedup factors are larger as the number of queries increases much faster 
for the mutagenesis experiment with the large language  it is the total speedup that is large 
as the language generates so many queries that the most time consuming part becomes the
administration and storage in memory  the packs version is much faster as it stores the
queries in trees  requiring significantly less memory 

    comparison with other engines
implementing a new special purpose prolog engine  different from the already existing ones 
carries a risk  given the level of sophistication of popular prolog engines  it is useful to check
whether the new engine performs comparably with these existing engines  at least for the
tasks under consideration here  the eciency gain obtained through query pack execution
should not be offset by a less ecient implementation of the engine itself 
originally the tilde and warmr systems were implemented in masterprolog 
in an attempt to allow them to run on other platforms  parts of these systems were reimplemented into a kind of  generic  prolog from which implementations for specific prolog engines  sicstus  ilprolog  can easily be derived  the low level of standardisation of
prolog made this necessary   given this situation  there are two questions to be answered 

   

fiimproving the efficiency of ilp through query packs
data set
la
bongard       
bongard       
bongard       
bongard       
bongard       
bongard       
bongard       
bongard       
bongard       
table   

masterprolog ilprolog original  ilprolog packs 
   
    
  
    
    
   
   
    
  

    
    
  
    
    
   
   
    
  

    
   
  
    
    
   
   
   
   

compared to other engines  times in seconds  for several data sets and
lookahead settings 

ilprolog

 a  does the move from masterprolog to other prolog engines inuence performance in
a negative way  and  b  does the performance loss  if any  reduce the performance improvements due to the use of packs 
tilde and warmr have been tuned for fast execution on masterprolog and ilprolog but not for sicstus  which makes a comparison with the latter unfair  therefore
we just report on the former   engines  table   shows some results  these confirm that
ilprolog is competitive with state of the art prolog engines 

    summary of experimental results
our experiments confirm that  a  query pack execution in itself is much more ecient than
executing many highly similar queries separately   b  existing ilp systems  we use tilde
and warmr as examples  can use this mechanism to their advantage  achieving significant
speedups  and c  although a new prolog engine is needed to achieve this  the current state
of development of this engine is such that with respect to execution speed it can compete
with state of the art engines  further  the experiments are consistent with our complexity
analysis of the execution time of packs 
   related work

the re implementation of tilde is related to the work by mehta et al         who were
the first to describe the  examples in outer loop  strategy for decision tree induction  the
query pack execution mechanism  here described from the prolog execution point of view 
can also be seen as a first order counterpart of apriori s mechanism for counting item sets
 agrawal et al         
other lines of work on eciency improvements for ilp involves stochastic methods
which trade a certain amount of optimality for eciency by  e g   evaluating clauses on a
sample of the data set instead of the full data set  srinivasan         exploring the clause
search space in a random fashion  srinivasan         or stochastically testing whether a

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
query succeeds on an example  sebag   rouveirol         the first of these is entirely
orthogonal to query pack execution and can easily be combined with it 
the idea of optimising sets of queries instead of individual queries has existed for a
while in the database community  the typical context considered in earlier research on
multi query optimisation  e g   sellis        was that of a database system that needs to
handle disjunctions of conjunctive queries  or of a server that may receive many queries from
different clients in a brief time interval  if several of these queries are expected to compute
the same intermediary relations  it may be more ecient to materialise these relations
instead of having them recomputed for each query  data mining provides in a sense a new
context for multi query optimisation  in which the multi query optimisation approach is at
the same time easier  the similarities among the queries are more systematic  so one need
not look for them  and more promising  given the huge number of queries that may be
generated at once  
tsur et al         describe an algorithm for ecient execution of so called query ocks
in this context  like our query pack execution mechanism  the query ock execution mechanism is inspired to some extent by apriori and is set in a deductive database setting 
the main difference between our query packs and the query ocks described by tsur et al 
       is that query packs are more hierarchically structured and the queries in a pack are
structurally less similar than the queries in a ock   a ock is represented by a single query
with placeholders for constants  and is equal to the set of all queries that can be obtained
by instantiating the placeholders to constants  flocks could not be used for the applications
we consider here  
dekeyser and paredaens        describe work on multi query optimisation in the context
of relational databases  they also consider tree like structures in which multiple queries are
combined  the main difference is that their trees are rooted in one single table from which
the queries select tuples  whereas our queries correspond to joins of multiple tables  further 
dekeyser and paredaens define a cost measure for trees as well as operators that map trees
onto semantically equivalent  but less costly  trees  whereas we have considered only the
creation of packs and an ecient top down execution mechanism for them  combining both
approaches seems an interesting topic for further research 
finally  other optimisation techniques for ilp have been proposed that exploit results
from program analysis  santos costa et al         blockeel et al         or from propositional
data mining technology  blockeel et al          these are complementary to our pack
execution optimisation  especially the approach of blockeel et al         can easily be
combined with our pack mechanism  the techniques discussed by santos costa et al 
       and blockeel et al         involve optimisations for single query execution  some of
which can to some extent be upgraded to the pack setting  this is future work 
   conclusions

there is a lot of redundancy in the computations performed by most ilp systems  in this
paper we have identified a source of redundancy and proposed a method for avoiding it 
execution of query packs  we have discussed how query pack execution can be incorporated
in ilp systems  the query pack execution mechanism has been implemented in a new
prolog system called ilprolog and dedicated to data mining tasks  and two ilp systems

   

fiimproving the efficiency of ilp through query packs
have been re implemented to make use of the mechanism  we have experimentally evaluated
these re implementations  and the results of these experiments confirm that large speedups
may be obtained in this way  we conjecture that the query pack execution mechanism can
be incorporated in other ilp systems and that similar speedups can be expected 
the problem setting in which query pack execution was introduced is very general  and
allows the technique to be used for any kind of task where many queries are to be executed
on the same data  as long as the queries can be organised in a hierarchy 
future work includes further improvements to the ilprolog engine and the implementation of techniques that will increase the suitability of the engine to handle large data sets 
in the best case one might hope to combine techniques known from database optimisation
and program analysis with our pack execution mechanism to further improve the speed of
ilp systems 

acknowledgements
hendrik blockeel is a post doctoral fellow of the fund for scientific research  fwo  of
flanders  jan ramon is funded by the flemish institute for the promotion of scientific
research in industry  iwt   henk vandecasteele was funded in part by the fwo project
g           query languages for database mining   the authors thank luc de raedt for
his inuence on this work  ashwin srinivasan for suggesting the term  query packs   the
anonymous reviewers for their useful comments  and kurt driessens for proofreading this
text  this work was motivated in part by the esprit project        aladin 
appendix a  preparing the query for the meta interpreter

note that the following preprocessor assumes that the pack of the form a   b   c or d
or e  or f or g   h or i or j   was already transformed to the form a   or   b 
or  c d e     f   g  or  h i j      
preprocess  a b   a newb  prevnode nodenr  leafnr  branchnr nodenr  leafnr       
preprocess b newb prevnode nodenr  leafnr  branchnr nodenr  leafnr   
preprocess or querys  or nquerys prevnode nodenr  branchnr length  
prevnode nodenr  leafnr  branchnr  nodenr  leafnr       
nodenr  is nodenr      
preprocessbranches querys nquerys nodenr  nodenr  leafnr  
  nodenr  leafnr  length  
preprocess a  a leaf prevnode branchnr leafnr    
prevnode nodenr  leafnr   branchnr nodenr  leafnr   leafnr  is leafnr      
preprocessbranches         nodenr leafnr branchnr  nodenr leafnr branchnr  
preprocessbranches  queryjquerys   newqueryjnewquerys  prevnode 
nodenr  leafnr  branchnr  nodenr  leafnr  length  preprocess query newquery 
prevnode nodenr  leafnr  branchnr  nodenr  leafnr   
branchnr  is branchnr     
preprocessbranches querys newquerys  prevnode 
nodenr  leafnr  branchnr   nodenr  leafnr  length  

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele

references

agrawal  r   mannila  h   srikant  r   toivonen  h     verkamo  a          fast discovery
of association rules  in fayyad  u   piatetsky shapiro  g   smyth  p     uthurusamy 
r   eds    advances in knowledge discovery and data mining  pp           the mit
press 
at kaci  h          warren s abstract machine  a tutorial reconstruction  the mit
press  cambridge  massachusetts 
http   www isg sfu ca  hak documents wam html 
blockeel  h          top down induction of first order logical decision trees  ph d  thesis 
department of computer science  katholieke universiteit leuven 
http   www cs kuleuven ac be  ml ps blockeel   phd ps gz 
blockeel  h     de raedt  l          lookahead and discretization in ilp  in proceedings
of the seventh international workshop on inductive logic programming  vol       of
lecture notes in artificial intelligence  pp         springer verlag 
blockeel  h     de raedt  l          top down induction of first order logical decision trees 
artificial intelligence                     
blockeel  h   de raedt  l   jacobs  n     demoen  b          scaling up inductive logic programming by learning from interpretations  data mining and knowledge discovery 
             
blockeel  h   de raedt  l     ramon  j          top down induction of clustering trees 
in proceedings of the   th international conference on machine learning  pp        
http   www cs kuleuven ac be  ml ps ml      ps 
blockeel  h   demoen  b   janssens  g   vandecasteele  h     van laer  w          two
advanced transformations for improving the eciency of an ilp system  in   th
international conference on inductive logic programming  work in progress reports 
pp         london  uk 
bongard  m          pattern recognition  spartan books 
bratko  i          prolog programming for artificial intelligence  addison wesley  wokingham  england   nd edition 
breiman  l   friedman  j   olshen  r     stone  c          classification and regression
trees  wadsworth  belmont 
chen  w     warren  d  s          tabled evaluation with delaying for general logic programs  journal of the acm                 http   www cs sunysb edu  sbprolog 
clark  p     niblett  t          the cn  algorithm  machine learning                 
de raedt  l          logical settings for concept learning  artificial intelligence          
    
de raedt  l     dehaspe  l          clausal discovery  machine learning             

   

fiimproving the efficiency of ilp through query packs
de raedt  l     dzeroski  s          first order jk clausal theories are pac learnable 
artificial intelligence              
de raedt  l     van laer  w          inductive constraint logic  in jantke  k  p   shinohara  t     zeugmann  t   eds    proceedings of the sixth international workshop on
algorithmic learning theory  vol      of lecture notes in artificial intelligence  pp 
       springer verlag 
dehaspe  l     toivonen  h          discovery of frequent datalog patterns  data mining
and knowledge discovery              
dekeyser  s     paredaens  j          query pack trees for multi query optimization  tech 
rep         university of antwerp  ftp   wins uia ac be pub dekeyser qpt ps 
demoen  b   janssens  g     vandecasteele  h          executing query flocks for ilp  in
etalle  s   ed    proceedings of the eleventh benelux workshop on logic programming 
pp        maastricht  the netherlands     pages 
kramer  s          structural regression trees  in proceedings of the thirteenth national
conference on artificial intelligence  pp           cambridge menlo park  aaai
press mit press 
mehta  m   agrawal  r     rissanen  j          sliq  a fast scalable classifier for data
mining  in proceedings of the fifth international conference on extending database
technology 
muggleton  s          inverse entailment and progol  new generation computing  special
issue on inductive logic programming                    
muggleton  s     de raedt  l          inductive logic programming   theory and methods 
journal of logic programming                 
quinlan  j  r       a   c     programs for machine learning  morgan kaufmann series in
machine learning  morgan kaufmann 
quinlan  j       b   foil  a midterm report  in brazdil  p   ed    proceedings of the  th
european conference on machine learning  lecture notes in artificial intelligence 
springer verlag 
santos costa  v   srinivasan  a     camacho  r          a note on two simple transformations for improving the eciency of an ilp system  in proceedings of the tenth
international conference on inductive logic programming  vol       of lecture notes
in artificial intelligence  pp           springer verlag 
sebag  m     rouveirol  c          tractable induction and classification in first order
logic via stochastic matching  in proceedings of the   th international joint conference on artificial intelligence  morgan kaufmann 
sellis  t          multiple query optimization  acm transactions on database systems 
              
srinivasan  a          a study of two sampling methods for analysing large datasets with
ilp  data mining and knowledge discovery                
srinivasan  a          a study of two probabilistic methods for searching large spaces with
ilp  tech  rep  prg tr        oxford university computing laboratory 

   

fiblockeel  dehaspe  demoen  janssens  ramon    vandecasteele
srinivasan  a   muggleton  s     king  r          comparing the use of background knowledge by inductive logic programming systems  in de raedt  l   ed    proceedings of
the fifth international workshop on inductive logic programming 
tsur  d   ullman  j   abiteboul  s   clifton  c   motwani  r   nestorov  s     rosenthal  a 
        query ocks  a generalization of association rule mining  in proceedings of
the acm sigmod international conference on management of data  sigmod     
vol       of acm sigmod record  pp        new york  acm press 

   

fi