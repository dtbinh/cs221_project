journal of artificial intelligence research                 

submitted        published      

smote  synthetic minority over sampling technique
nitesh v  chawla

chawla csee usf edu

department of computer science and engineering  enb    
university of south florida
     e  fowler ave 
tampa  fl             usa

kevin w  bowyer

kwb cse nd edu

department of computer science and engineering
    fitzpatrick hall
university of notre dame
notre dame  in        usa

lawrence o  hall

hall csee usf edu

department of computer science and engineering  enb    
university of south florida
     e  fowler ave 
tampa  fl             usa

w  philip kegelmeyer

wpk california sandia gov

sandia national laboratories
biosystems research department  p o  box      ms     
livermore  ca              usa

abstract
an approach to the construction of classiers from imbalanced datasets is described 
a dataset is imbalanced if the classication categories are not approximately equally represented  often real world data sets are predominately composed of normal examples
with only a small percentage of abnormal or interesting examples  it is also the case
that the cost of misclassifying an abnormal  interesting  example as a normal example is
often much higher than the cost of the reverse error  under sampling of the majority  normal  class has been proposed as a good means of increasing the sensitivity of a classier to
the minority class  this paper shows that a combination of our method of over sampling
the minority  abnormal  class and under sampling the majority  normal  class can achieve
better classier performance  in roc space  than only under sampling the majority class 
this paper also shows that a combination of our method of over sampling the minority class
and under sampling the majority class can achieve better classier performance  in roc
space  than varying the loss ratios in ripper or class priors in naive bayes  our method
of over sampling the minority class involves creating synthetic minority class examples 
experiments are performed using c     ripper and a naive bayes classier  the method
is evaluated using the area under the receiver operating characteristic curve  auc  and
the roc convex hull strategy 

   introduction
a dataset is imbalanced if the classes are not approximately equally represented  imbalance
on the order of     to   is prevalent in fraud detection and imbalance of up to         to
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

fichawla  bowyer  hall   kegelmeyer

  has been reported in other applications  provost   fawcett         there have been
attempts to deal with imbalanced datasets in domains such as fraudulent telephone calls
 fawcett   provost         telecommunications management  ezawa  singh    norton 
       text classication  lewis   catlett        dumais  platt  heckerman    sahami 
      mladenic   grobelnik        lewis   ringuette        cohen      a  and detection
of oil spills in satellite images  kubat  holte    matwin        
the performance of machine learning algorithms is typically evaluated using predictive
accuracy  however  this is not appropriate when the data is imbalanced and or the costs of
dierent errors vary markedly  as an example  consider the classication of pixels in mammogram images as possibly cancerous  woods  doss  bowyer  solka  priebe    kegelmeyer 
       a typical mammography dataset might contain     normal pixels and    abnormal
pixels  a simple default strategy of guessing the majority class would give a predictive accuracy of      however  the nature of the application requires a fairly high rate of correct
detection in the minority class and allows for a small error rate in the majority class in
order to achieve this  simple predictive accuracy is clearly not appropriate in such situations  the receiver operating characteristic  roc  curve is a standard technique for
summarizing classier performance over a range of tradeos between true positive and false
positive error rates  swets         the area under the curve  auc  is an accepted traditional performance metric for a roc curve  duda  hart    stork        bradley        lee 
       the roc convex hull can also be used as a robust method of identifying potentially
optimal classiers  provost   fawcett         if a line passes through a point on the convex
hull  then there is no other line with the same slope passing through another point with a
larger true positive  tp  intercept  thus  the classier at that point is optimal under any
distribution assumptions in tandem with that slope 
the machine learning community has addressed the issue of class imbalance in two ways 
one is to assign distinct costs to training examples  pazzani  merz  murphy  ali  hume   
brunk        domingos         the other is to re sample the original dataset  either by oversampling the minority class and or under sampling the majority class  kubat   matwin 
      japkowicz        lewis   catlett        ling   li         our approach  chawla 
bowyer  hall    kegelmeyer        blends under sampling of the majority class with a
special form of over sampling the minority class  experiments with various datasets and
the c    decision tree classier  quinlan         ripper  cohen      b   and a naive bayes
classier show that our approach improves over other previous re sampling  modifying loss
ratio  and class priors approaches  using either the auc or roc convex hull 
section   gives an overview of performance measures  section   reviews the most
closely related work dealing with imbalanced datasets  section   presents the details of
our approach  section   presents experimental results comparing our approach to other
re sampling approaches  section   discusses the results and suggests directions for future
work 

   performance measures
the performance of machine learning algorithms is typically evaluated by a confusion matrix
as illustrated in figure    for a   class problem   the columns are the predicted class and the
rows are the actual class  in the confusion matrix  t n is the number of negative examples
   

fismote

predicted
negative

predicted
positive

actual
negative

tn

fp

actual
positive

fn

tp

figure    confusion matrix
correctly classied  true negatives   f p is the number of negative examples incorrectly
classied as positive  false positives   f n is the number of positive examples incorrectly
classied as negative  false negatives  and t p is the number of positive examples correctly
classied  true positives  
predictive accuracy is the performance measure generally associated with machine learning algorithms and is dened as accuracy    t p   t n    t p   f p   t n   f n    in the
context of balanced datasets and equal error costs  it is reasonable to use error rate as a
performance metric  error rate is    accuracy  in the presence of imbalanced datasets
with unequal error costs  it is more appropriate to use the roc curve or other similar
techniques  ling   li        drummond   holte        provost   fawcett        bradley 
      turney        
roc curves can be thought of as representing the family of best decision boundaries for
relative costs of tp and fp  on an roc curve the x axis represents  f p   f p  t n  f p  
and the y axis represents  t p   t p  t p  f n    the ideal point on the roc curve would
be          that is all positive examples are classied correctly and no negative examples are
misclassied as positive  one way an roc curve can be swept out is by manipulating the
balance of training samples for each class in the training set  figure   shows an illustration 
the line y   x represents the scenario of randomly guessing the class  area under the roc
curve  auc  is a useful metric for classier performance as it is independent of the decision
criterion selected and prior probabilities  the auc comparison can establish a dominance
relationship between classiers  if the roc curves are intersecting  the total auc is an
average comparison between models  lee         however  for some specic cost and class
distributions  the classier having maximum auc may in fact be suboptimal  hence  we
also compute the roc convex hulls  since the points lying on the roc convex hull are
potentially optimal  provost  fawcett    kohavi        provost   fawcett        

   previous work  imbalanced datasets
kubat and matwin        selectively under sampled the majority class while keeping the
original population of the minority class  they have used the geometric mean as a performance measure for the classier  which can be related to a single point on the roc curve 
the minority examples were divided into four categories  some noise overlapping the positive class decision region  borderline samples  redundant samples and safe samples  the
borderline examples were detected using the tomek links concept  tomek         another
   

fichawla  bowyer  hall   kegelmeyer

roc

          

   
ideal point

percent
true

y x

positive
increased undersampling
of the majority class moves
the operating point to the
upper right
original data set

 

percent false positive

   

figure    illustration of sweeping out a roc curve through under sampling  increased
under sampling of the majority  negative  class will move the performance from
the lower left point to the upper right 

related work proposed the shrink system that classies an overlapping region of minority  positive  and majority  negative  classes as positive  it searches for the best positive
region  kubat et al         
japkowicz        discussed the eect of imbalance in a dataset  she evaluated three
strategies  under sampling  resampling and a recognition based induction scheme  we focus
on her sampling approaches  she experimented on articial  d data in order to easily
measure and construct concept complexity  two resampling methods were considered 
random resampling consisted of resampling the smaller class at random until it consisted
of as many samples as the majority class and focused resampling consisted of resampling
only those minority examples that occurred on the boundary between the minority and
majority classes  random under sampling was considered  which involved under sampling
the majority class samples at random until their numbers matched the number of minority
class samples  focused under sampling involved under sampling the majority class samples
lying further away  she noted that both the sampling approaches were eective  and she also
observed that using the sophisticated sampling techniques did not give any clear advantage
in the domain considered  japkowicz        
one approach that is particularly relevant to our work is that of ling and li        
they combined over sampling of the minority class with under sampling of the majority
class  they used lift analysis instead of accuracy to measure a classiers performance  they
proposed that the test examples be ranked by a condence measure and then lift be used as
the evaluation criteria  a lift curve is similar to an roc curve  but is more tailored for the
   

fismote

marketing analysis problem  ling   li         in one experiment  they under sampled the
majority class and noted that the best lift index is obtained when the classes are equally
represented  ling   li         in another experiment  they over sampled the positive
 minority  examples with replacement to match the number of negative  majority  examples
to the number of positive examples  the over sampling and under sampling combination
did not provide signicant improvement in the lift index  however  our approach to oversampling diers from theirs 
solberg and solberg        considered the problem of imbalanced data sets in oil slick
classication from sar imagery  they used over sampling and under sampling techniques
to improve the classication of oil slicks  their training data had a distribution of    oil
slicks and       look alikes  giving a prior probability of      for look alikes  this imbalance
would lead the learner  without any appropriate loss functions or a methodology to modify
priors  to classify almost all look alikes correctly at the expense of misclassifying many of
the oil slick samples  solberg   solberg         to overcome this imbalance problem  they
over sampled  with replacement      samples from the oil slick  and they randomly sampled
    samples from the non oil slick class to create a new dataset with equal probabilities 
they learned a classier tree on this balanced data set and achieved a     error rate on the
oil slicks in a leave one out method for error estimation  on the look alikes they achieved
an error rate of     solberg   solberg        
another approach that is similar to our work is that of domingos         he compares
the metacost approach to each of majority under sampling and minority over sampling 
he nds that metacost improves over either  and that under sampling is preferable to minority over sampling  error based classiers are made cost sensitive  the probability of
each class for each example is estimated  and the examples are relabeled optimally with
respect to the misclassication costs  the relabeling of the examples expands the decision
space as it creates new samples from which the classier may learn  domingos        
a feed forward neural network trained on an imbalanced dataset may not learn to discriminate enough between classes  derouin  brown  fausett    schneider         the
authors proposed that the learning rate of the neural network be adapted to the statistics
of class representation in the data  they calculated an attention factor from the proportion
of samples presented to the neural network for training  the learning rate of the network
elements was adjusted based on the attention factor  they experimented on an articially
generated training set and on a real world training set  both with multiple  more than two 
classes  they compared this to the approach of replicating the minority class samples to
balance the data set used for training  the classication accuracy on the minority class was
improved 
lewis and catlett        examined heterogeneous uncertainty sampling for supervised
learning  this method is useful for training samples with uncertain classes  the training
samples are labeled incrementally in two phases and the uncertain instances are passed on
to the next phase  they modied c    to include a loss ratio for determining the class
values at the leaves  the class values were determined by comparison with a probability
threshold of lr  lr       where lr is the loss ratio  lewis   catlett        
the information retrieval  ir  domain  dumais et al         mladenic   grobelnik 
      lewis   ringuette        cohen      a  also faces the problem of class imbalance
in the dataset  a document or web page is converted into a bag of words representation 
   

fichawla  bowyer  hall   kegelmeyer

that is  a feature vector reecting occurrences of words in the page is constructed  usually 
there are very few instances of the interesting category in text categorization  this overrepresentation of the negative class in information retrieval problems can cause problems
in evaluating classiers performances  since error rate is not a good metric for skewed
datasets  the classication performance of algorithms in information retrieval is usually
measured by precision and recall 
recall  

tp
tp   fn

precision  

tp
tp   fp

mladenic and grobelnik        proposed a feature subset selection approach to deal
with imbalanced class distribution in the ir domain  they experimented with various
feature selection methods  and found that the odds ratio  van rijsbergen  harper    porter 
      when combined with a naive bayes classier performs best in their domain  odds
ratio is a probabilistic measure used to rank documents according to their relevance to the
positive class  minority class   information gain for a word  on the other hand  does not
pay attention to a particular target class  it is computed per word for each class  in an
imbalanced text dataset  assuming    to     is the negative class   most of the features will
be associated with the negative class  odds ratio incorporates the target class information in
its metric giving better results when compared to information gain for text categorization 
provost and fawcett        introduced the roc convex hull method to estimate the
classier performance for imbalanced datasets  they note that the problems of unequal
class distribution and unequal error costs are related and that little work has been done to
address either problem  provost   fawcett         in the roc convex hull method  the
roc space is used to separate classication performance from the class and cost distribution
information 
to summarize the literature  under sampling the majority class enables better classiers
to be built than over sampling the minority class  a combination of the two as done in
previous work does not lead to classiers that outperform those built utilizing only undersampling  however  the over sampling of the minority class has been done by sampling with
replacement from the original data  our approach uses a dierent method of over sampling 

   smote  synthetic minority over sampling technique
    minority over sampling with replacement
previous research  ling   li        japkowicz        has discussed over sampling with
replacement and has noted that it doesnt signicantly improve minority class recognition 
we interpret the underlying eect in terms of decision regions in feature space  essentially 
as the minority class is over sampled by increasing amounts  the eect is to identify similar
but more specic regions in the feature space as the decision region for the minority class 
this eect for decision trees can be understood from the plots in figure   
   

fismote

 attributes      data of the original mammography dataset

 attributes      data of the original mammography dataset
   

   

   

   

   

attribute  

attribute  

   

   

   

   

   

   

  

  

 

 

 

 

 

 

  

  

  

 

  

 

 

 

 

attribute  

 

 

 

 

attribute  

 a 

 b 

 attributes      data of the original mammography dataset
   

attribute  

   

   

  

 

 

 

 

 

 

attribute  

 

 

 

 c 
figure    a  decision region in which the three minority class samples  shown by    reside
after building a decision tree  this decision region is indicated by the solid line
rectangle  b  a zoomed in view of the chosen minority class samples for the same
dataset  small solid line rectangles show the decision regions as a result of oversampling the minority class with replication  c  a zoomed in view of the chosen
minority class samples for the same dataset  dashed lines show the decision region
after over sampling the minority class with synthetic generation 

   

fichawla  bowyer  hall   kegelmeyer

the data for the plot in figure   was extracted from a mammography dataset   woods
et al          the minority class samples are shown by   and the majority class samples
are shown by o in the plot  in figure   a   the region indicated by the solid line rectangle
is a majority class decision region  nevertheless  it contains three minority class samples
shown by   as false negatives  if we replicate the minority class  the decision region for the
minority class becomes very specic and will cause new splits in the decision tree  this will
lead to more terminal nodes  leaves  as the learning algorithm tries to learn more and more
specic regions of the minority class  in essence  overtting  replication of the minority
class does not cause its decision boundary to spread into the majority class region  thus 
in figure   b   the three samples previously in the majority class decision region now have
very specic decision regions 
    smote
we propose an over sampling approach in which the minority class is over sampled by creating synthetic examples rather than by over sampling with replacement  this approach
is inspired by a technique that proved successful in handwritten character recognition  ha
  bunke         they created extra training data by performing certain operations on
real data  in their case  operations like rotation and skew were natural ways to perturb
the training data  we generate synthetic examples in a less application specic manner  by
operating in feature space rather than data space  the minority class is over sampled
by taking each minority class sample and introducing synthetic examples along the line
segments joining any all of the k minority class nearest neighbors  depending upon the
amount of over sampling required  neighbors from the k nearest neighbors are randomly
chosen  our implementation currently uses ve nearest neighbors  for instance  if the
amount of over sampling needed is       only two neighbors from the ve nearest neighbors are chosen and one sample is generated in the direction of each  synthetic samples
are generated in the following way  take the dierence between the feature vector  sample 
under consideration and its nearest neighbor  multiply this dierence by a random number
between   and    and add it to the feature vector under consideration  this causes the
selection of a random point along the line segment between two specic features  this
approach eectively forces the decision region of the minority class to become more general 
algorithm smote   on the next page  is the pseudo code for smote  table     shows
an example of calculation of random synthetic samples  the amount of over sampling
is a parameter of the system  and a series of roc curves can be generated for dierent
populations and roc analysis performed 
the synthetic examples cause the classier to create larger and less specic decision
regions as shown by the dashed lines in figure   c   rather than smaller and more specic
regions  more general regions are now learned for the minority class samples rather than
those being subsumed by the majority class samples around them  the eect is that decision trees generalize better  figures   and   compare the minority over sampling with
replacement and smote  the experiments were conducted on the mammography dataset 
there were       examples in the majority class and     examples in the minority class
originally  we have approximately      examples in the majority class and     examples
   the data is available from the usf intelligent systems lab  http   morden csee usf edu chawla 

   

fismote

in the minority class for the training set used in    fold cross validation  the minority class
was over sampled at                        and      of its original size  the graphs
show that the tree sizes for minority over sampling with replacement at higher degrees of
replication are much greater than those for smote  and the minority class recognition of
the minority over sampling with replacement technique at higher degrees of replication isnt
as good as smote 
algorithm smote  t  n  k 
input  number of minority class samples t   amount of smote n    number of nearest
neighbors k
output   n        t synthetic minority class samples
     if n is less than       randomize the minority class samples as only a random
percent of them will be smoted   
   if n      
  
then randomize the t minority class samples
  
t    n       t
  
n      
   endif
   n    int  n        the amount of smote is assumed to be in integral multiples of
      
   k   number of nearest neighbors
   numattrs   number of attributes
    sample        array for original minority class samples
    newindex  keeps a count of number of synthetic samples generated  initialized to  
    synthetic        array for synthetic samples
  compute k nearest neighbors for each minority class sample only   
    for i    to t
   
compute k nearest neighbors for i  and save the indices in the nnarray
   
populate n   i  nnarray 
    endfor
populate n  i  nnarray    function to generate the synthetic samples   
    while n    
   
choose a random number between   and k  call it nn  this step chooses one of
the k nearest neighbors of i 
   
for attr    to numattrs
   
compute  dif   sample nnarray nn   attr   sample i  attr 
   
compute  gap   random number between   and  
   
synthetic newindex  attr    sample i  attr    gap  dif
   
endfor
   
newindex  
   
n   n  
    endwhile
    return   end of populate   
end of pseudo code 

   

fichawla  bowyer  hall   kegelmeyer

consider a sample       and let       be its nearest neighbor 
      is the sample for which k nearest neighbors are being identied 
      is one of its k nearest neighbors 
let 
f        f        f      f        
f        f        f      f        
the new samples will be generated as
 f  f             rand               
rand      generates a random number between   and   
table    example of generation of synthetic examples  smote  

pruned decision tree size vs the degree of minority oversampling
   

   

decisiion tree size  number of nodes 

   

   

   

   

   
synthetic data
replicated data
   

   

  

  

 

  

   

   

   
   
   
   
degree of minority oversampling

   

   

   

figure    comparison of decision tree sizes for replicated over sampling and smote for
the mammography dataset

   

fismote

  minority correct vs the degree of minority oversampling
  

 minority correct

  

  

  

synthetic data
replicated data

  

  
 

  

   

   

   

   

   

   

   

   

   

degree of minority oversampling

figure    comparison of   minority correct for replicated over sampling and smote for
the mammography dataset

    under sampling and smote combination
the majority class is under sampled by randomly removing samples from the majority class
population until the minority class becomes some specied percentage of the majority class 
this forces the learner to experience varying degrees of under sampling and at higher degrees
of under sampling the minority class has a larger presence in the training set  in describing
our experiments  our terminology will be such that if we under sample the majority class at
      it would mean that the modied dataset will contain twice as many elements from the
minority class as from the majority class  that is  if the minority class had    samples and
the majority class had     samples and we under sample majority at       the majority
class would end up having    samples  by applying a combination of under sampling and
over sampling  the initial bias of the learner towards the negative  majority  class is reversed
in the favor of the positive  minority  class  classiers are learned on the dataset perturbed
by smoting the minority class and under sampling the majority class 

   experiments
we used three dierent machine learning algorithms for our experiments  figure   provides
an overview of our experiments 
   c     we compared various combinations of smote and under sampling with plain
under sampling using c    release    quinlan        as the base classier 
   

fichawla  bowyer  hall   kegelmeyer

smote
and undersampling 

c   

loss ratio
modify costs of majority and minority
varied from     to       
classes by changing priors 

ripper

naive bayes

rocs generated for smote  undersampling
and loss ratio comparisons  performance
evaluated with auc and roc convex hull 

rocs generated for comparison between
smote and under sampling using c     and
smote using c    and naive bayes 
performance evaluated with auc and roc convex hull 

figure    experiments overview

   ripper  we compared various combinations of smote and under sampling with
plain under sampling using ripper  cohen      b  as the base classier  we also
varied rippers loss ratio  cohen   singer        lewis   catlett        from     to
       as a means of varying misclassication cost  and compared the eect of this
variation with the combination of smote and under sampling  by reducing the loss
ratio from     to       we were able to build a set of rules for the minority class 
   naive bayes classifier  the naive bayes classier  can be made cost sensitive
by varying the priors of the minority class  we varied the priors of the minority
class from   to    times the majority class and compared with c   s smote and
under sampling combination 

these dierent learning algorithms allowed smote to be compared to some methods
that can handle misclassication costs directly   fp and  tp were averaged over    fold
cross validation runs for each of the data combinations  the minority class examples were
over sampled by calculating the ve nearest neighbors and generating synthetic examples 
the auc was calculated using the trapezoidal rule  we extrapolated an extra point of tp
       and fp        for each roc curve  we also computed the roc convex hull
to identify the optimal classiers  as the points lying on the hull are potentially optimal
classiers  provost   fawcett        
   the source code was downloaded from http   fuzzy cs uni magdeburg de borgelt software html 

   

fismote

    datasets
we experimented on nine dierent datasets  these datasets are summarized in table     
these datasets vary extensively in their size and class proportions  thus oering dierent
domains for smote  in order of increasing imbalance they are 
   the pima indian diabetes  blake   merz        has   classes and     samples  the
data is used to identify the positive diabetes cases in a population near phoenix 
arizona  the number of positive class samples is only      good sensitivity to
detection of diabetes cases will be a desirable attribute of the classier 
   the phoneme dataset is from the elena project    the aim of the dataset is to
distinguish between nasal  class    and oral sounds  class     there are   features 
the class distribution is       samples in class   and       samples in class   
   the adult dataset  blake   merz        has        samples with        samples
belonging to the minority class  this dataset has   continuous features and   nominal
features  smote and smote nc  see section      algorithms were evaluated on
this dataset  for smote  we extracted the continuous features and generated a new
dataset with only continuous features 
   the e state data   hall  mohney    kier        consists of electrotopological state
descriptors for a series of compounds from the national cancer institutes yeast anticancer drug screen  e state descriptors from the nci yeast anticancer drug screen
were generated by tripos  inc  briey  a series of about        compounds were
tested against a series of   yeast strains at a given concentration  the test was a
high throughput screen at only one concentration so the results are subject to contamination  etc  the growth inhibition of the yeast strain when exposed to the given
compound  with respect to growth of the yeast in a neutral solvent  was measured 
the activity classes are either active  at least one single yeast strain was inhibited
more than      or inactive  no yeast strain was inhibited more than      the
dataset has        samples with       samples of active compounds 
   the satimage dataset  blake   merz        has   classes originally  we chose the
smallest class as the minority class and collapsed the rest of the classes into one as
was done in  provost et al          this gave us a skewed   class dataset  with     
majority class samples and     minority class samples 
   the forest cover dataset is from the uci repository  blake   merz         this
dataset has   classes and         samples  this dataset is for the prediction of forest
cover type based on cartographic variables  since our system currently works for binary classes we extracted data for two classes from this dataset and ignored the rest 
most other approaches only work for only two classes  ling   li        japkowicz 
      kubat   matwin        provost   fawcett         the two classes we considered are ponderosa pine with        samples and cottonwood willow with      
   ftp dice ucl ac be in the directory pub neural nets elena databases 
   we would like to thank steven eschrich for providing the dataset and description to us 

   

fichawla  bowyer  hall   kegelmeyer

dataset
pima
phoneme
adult
e state
satimage
forest cover
oil
mammography
can

majority class
   
    
     
     
    
     
   
     
      

minority class
   
    
     
    
   
    
  
   
    

table    dataset distribution
samples  nevertheless  the smote technique can be applied to a multiple class problem as well by specifying what class to smote for  however  in this paper  we have
focused on   classes problems  to explicitly represent positive and negative classes 
   the oil dataset was provided by robert holte and is used in their paper  kubat et al  
       this dataset has    oil slick samples and     non oil slick samples 
   the mammography dataset  woods et al         has        samples with     calcications  if we look at predictive accuracy as a measure of goodness of the classier
for this case  the default accuracy would be        when every sample is labeled noncalcication  but  it is desirable for the classier to predict most of the calcications
correctly 
   the can dataset was generated from the can exodusii data using the avatar
 chawla   hall        version of the mustafa visualization tool    the portion of
the can being crushed was marked as very interesting and the rest of the can was
marked as unknown  a dataset of size         samples with       samples marked
as very interesting was generated 
    roc creation
a roc curve for smote is produced by using c    or ripper to create a classier for
each one of a series of modied training datasets  a given roc curve is produced by rst
over sampling the minority class to a specied degree and then under sampling the majority
class at increasing degrees to generate the successive points on the curve  the amount of
under sampling is identical to plain under sampling  so  each corresponding point on each
roc curve for a dataset represents the same number of majority class samples  dierent
roc curves are produced by starting with dierent levels of minority over sampling  roc
curves were also generated by varying the loss ratio in ripper from     to       and by
varying the priors of the minority class from the original distribution to up to    times the
majority class for a naive bayes classier 
   the mustafa visualization tool was developed by mike glass of sandia national labs 

   

fismote

phoneme roc
   

  

  

  

 tp

underc   
    smotec   
naive bayes
hull

  

  

  

  
  

  

  

  

  

  

  

  

  

   

 fp

figure    phoneme  comparison of smote c     under c     and naive bayes  smotec    dominates over naive bayes and under c    in the roc space  smotec    classiers are potentially optimal classiers 

figures   through    show the experimental roc curves obtained for the nine datasets
with the three classiers  the roc curve for plain under sampling of the majority class
 ling   li        japkowicz        kubat   matwin        provost   fawcett        is
compared with our approach of combining synthetic minority class over sampling  smote 
with majority class under sampling  the plain under sampling curve is labeled under 
and the smote and under sampling combination roc curve is labeled smote  depending on the size and relative imbalance of the dataset  one to ve smote and undersampling curves are created  we only show the best results from smote combined with
under sampling and the plain under sampling curve in the graphs  the smote roc curve
from c    is also compared with the roc curve obtained from varying the priors of minority
class using a naive bayes classier  labeled as naive bayes  smote  under  and
loss ratio roc curves  generated using ripper are also compared  for a given family
of roc curves  an roc convex hull  provost   fawcett        is generated  the roc
convex hull is generated using the grahams algorithm  orourke         for reference  we
show the roc curve that would be obtained using minority over sampling by replication
in figure    
each point on the roc curve is the result of either a classier  c    or ripper  learned
for a particular combination of under sampling and smote  a classier  c    or ripper 
learned with plain under sampling  or a classier  ripper  learned using some loss ratio or
a classier  naive bayes  learned for a dierent prior for the minority class  each point
represents the average   tp and  fp     fold cross validation result  the lower leftmost
point for a given roc curve is from the raw dataset  without any majority class under   

fichawla  bowyer  hall   kegelmeyer

phoneme roc with ripper
   

  

 tp

  

  

underripper
    smoteripper
loss ratio
hull

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure    phoneme  comparison of smote ripper  under ripper  and modifying loss
ratio in ripper  smote ripper dominates over under ripper and loss ratio
in the roc space  more smote ripper classiers lie on the roc convex hull 

pima roc
   

  

  

  

 tp

  

  

underc   
    smotec   
naive bayes
hull

  

  

  

  

  
  

  

  

  

  

  

  

  

  

   

 fp

figure    pima indians diabetes  comparison of smote c     under c     and naive
bayes  naive bayes dominates over smote c    in the roc space 

   

fismote

pima roc with ripper
   

  

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

  
  

  

  

  

  

  

  

  

  

   

 fp

figure     pima indians diabetes  comparison of smote ripper  under ripper  and
modifying loss ratio in ripper  smote ripper dominates over under ripper
and loss ratio in the roc space 

sampling or minority class over sampling  the minority class was over sampled at     
                              the majority class was under sampled at          
                                                                                
       and        the amount of majority class under sampling and minority class oversampling depended on the dataset size and class proportions  for instance  consider the
roc curves in figure    for the mammography dataset  there are three curves  one for
plain majority class under sampling in which the range of under sampling is varied between
   and       at dierent intervals  one for a combination of smote and majority class
under sampling  and one for naive bayes  and one roc convex hull curve  the roc
curve shown in figure    is for the minority class over sampled at       each point on
the smote roc curves represents a combination of  synthetic  over sampling and undersampling  the amount of under sampling follows the same range as for plain under sampling 
for a better understanding of the roc graphs  we have shown dierent sets of roc curves
for one of our datasets in appendix a 
for the can dataset  we had to smote to a lesser degree than for the other datasets
due to the structural nature of the dataset  for the can dataset there is a structural
neighborhood already established in the mesh geometry  so smote can lead to creating
neighbors which are under the surface  and hence not interesting   since we are looking at
the feature space of physics variables and not the structural information 
the roc curves show a trend that as we increase the amount of under sampling coupled
with over sampling  our minority classication accuracy increases  of course at the expense
of more majority class errors  for almost all the roc curves  the smote approach dom   

fichawla  bowyer  hall   kegelmeyer

satimage roc
   

  

  

  
underc   
    smotec   
naive bayes
hull

 tp

  

  

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     satimage  comparison of smote c     under c     and naive bayes  the
roc curves of naive bayes and smote c    show an overlap  however  at
higher tps more points from smote c    lie on the roc convex hull 

satimage roc with ripper
   

  

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     satimage  comparison of smote ripper  under ripper  and modifying loss
ratio in ripper  smote ripper dominates the roc space  the roc convex
hull is mostly constructed with points from smote ripper 

   

fismote

covtype roc
   

  

  

  

 tp

  
underc   
    smotec   
naive bayes
hull

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     forest cover  comparison of smote c     under c     and naive bayes 
smote c    and under c    roc curves are very close to each other  however  more points from the smote c    roc curve lie on the roc convex
hull  thus establishing a dominance 

inates  adhering to the denition of roc convex hull  most of the potentially optimal
classiers are the ones generated with smote 
    auc calculation
the area under the roc curve  auc  is calculated using a form of the trapezoid rule  the
lower leftmost point for a given roc curve is a classiers performance on the raw data 
the upper rightmost point is always               if the curve does not naturally end at
this point  the point is added  this is necessary in order for the aucs to be compared
over the same range of  fp 
the aucs listed in table     show that for all datasets the combined synthetic minority over sampling and majority over sampling is able to improve over plain majority
under sampling with c    as the base classier  thus  our smote approach provides
an improvement in correct classication of data in the underrepresented class  the same
conclusion holds from an examination of the roc convex hulls  some of the entries are
missing in the table  as smote was not applied at the same amounts to all datasets  the
amount of smote was less for less skewed datasets  also  we have not included aucs
for ripper naive bayes  the roc convex hull identies smote classiers to be potentially optimal as compared to plain under sampling or other treatments of misclassication
costs  generally  exceptions are as follows  for the pima dataset  naive bayes dominates
over smote c     for the oil dataset  under ripper dominates over smote ripper  for
the can dataset  smote classifier  classifier   c    or ripper  and under classifier roc
   

fichawla  bowyer  hall   kegelmeyer

covtype roc with ripper
   

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     forest cover  comparison of smote ripper  under ripper  and modifying
loss ratio in ripper  smote ripper shows a domination in the roc space 
more points from smote ripper curve lie on the roc convex hull 

oil roc
   

  

  

  

 tp

  

  

  
underc   
    smotec   
naive bayes
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     oil  comparison of smote c     under c     and naive bayes  although 
smote c    and under c    roc curves intersect at points  more points from
smote c    curve lie on the roc convex hull 

   

fismote

oil roc with ripper
   

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     oil  comparison of smote ripper  under ripper  and modifying loss ratio
in ripper  under ripper and smote ripper curves intersect  and more points
from the under ripper curve lie on the roc convex hull 

mammography roc
   

  

  

  
underc   
    smotec   
naive bayes
hull

 tp

  

  

  

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     mammography  comparison of smote c     under c     and naive bayes 
smote c    and under c    curves intersect in the roc space  however  by
virtue of number of points on the roc convex hull  smote c    has more
potentially optimal classiers 

   

fichawla  bowyer  hall   kegelmeyer

mammography roc with ripper
   

  

  

  

 tp

  
underripper
    smoteripper
loss ratio
hull

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     mammography  comparison of smote ripper  under ripper  and modifying
loss ratio in ripper  smote ripper dominates the roc space for tp       
mammography roc with c   
   

  

  

  

 tp

  

  

  
    smote
    replicate
hull

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     a comparison of over sampling minority class examples by smote and oversampling the minority class examples by replication for the mammography
dataset 

   

fismote

estate roc
   

  

  

  

 tp

  

  

  
underc   
    smotec   
naive bayes
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     e state   a  comparison of smote c     under c     and naive bayes 
smote c    and under c    curves intersect in the roc space  however 
smote c    has more potentially optimal classiers  based on the number
of points on the roc convex hull 

estate roc with ripper
   

  

  

  

 tp

  

  

  
underripper
    smoteripper
loss ratio
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     e state  comparison of smote ripper  under ripper  and modifying loss
ratio in ripper  smote ripper has more potentially optimal classiers  based
on the number of points on the roc convex hull 

   

fichawla  bowyer  hall   kegelmeyer

can roc
   

  

  

  

 tp

  

  

  

underc   
    smotec   
naive bayes
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     can  comparison of smote c     under c     and naive bayes  smotec    and under c    roc curves overlap for most of the roc space 

can roc with ripper
   

  

  

  

 tp

  

  

  

underripper
   smoteripper
loss ratio
hull

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     can  comparison of smote ripper  under ripper  and modifying loss ratio
in ripper  smote ripper and under ripper roc curves overlap for most of
the roc space 

   

fismote

dataset

under

pima
phoneme
satimage
forest cover
oil
mammography
e state
can

    
    
    
    
    
    
    
    

  
smote

    

   
smote
    
    
    
    
    
    
    
    

   
smote

   
smote

   
smote

   
smote

    
    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

table    aucs  c    as the base classier  with the best highlighted in bold 

curves overlap in the roc space  for all the other datasets  smote classifier has more
potentially optimal classiers than any other approach 
    additional comparison to changing the decision thresholds
provost        suggested that simply changing the decision threshold should always be
considered as an alternative to more sophisticated approaches  in the case of c     this
would mean changing the decision threshold at the leaves of the decision trees  for example 
a leaf could classify examples as the minority class even if more than     of the training
examples at the leaf represent the majority class  we experimented by setting the decision
thresholds at the leaves for the c    decision tree learner at                                  
                                                              we experimented on the phoneme
dataset  figure    shows the comparison of the smote and under sampling combination
against c    learning by tuning the bias towards the minority class  the graph shows that
the smote and under sampling combination roc curve is dominating over the entire
range of values 
    additional comparison to one sided selection and shrink
for the oil dataset  we also followed a slightly dierent line of experiments to obtain results
comparable to  kubat et al          to alleviate the problem of imbalanced datasets the
authors have proposed  a  one sided selection for under sampling the majority class  kubat
  matwin        and  b  the shrink system  kubat et al          table     contains the
results from  kubat et al          acc  is the accuracy on positive  minority  examples and
acc is the accuracy on the negative  majority  examples  figure    shows the trend for
acc  and acc for one combination of the smote strategy and varying degrees of undersampling of the majority class  the y axis represents the accuracy and the x axis represents
the percentage majority class under sampled  the graphs indicate that in the band of
under sampling between     and      the results are comparable to those achieved by
shrink and better than shrink in some cases  table     summarizes the results for the
smote at      and under sampling combination  we also tried combinations of smote
at          and varying degrees of under sampling and achieved comparable results  the
   

fichawla  bowyer  hall   kegelmeyer

phoneme  roc comparison between smote and c    variation of decision thresholds
   

  

 tp

  
smote
varying c    decision thresholds
hull
  

  

  
  

  

  

  

  

  

  

  

  

   

 fp

figure     smote and under sampling combination against c    learning by tuning the
bias towards the minority class

smote and undersampling
   

  

accuracy

  

accuracy on majority  negative class 
accuracy on minority  positive class 

  

  

  

  

  

 

   

   

   
   
   
   
percentage undersampling of majority class

   

   

figure     smote      ou  and under sampling combination performance

shrink approach and our smote approach are not directly comparable  though  as they
see dierent data points  smote oers no clear improvement over one sided selection 
   

fismote

method
shrink
one sided selection

acc 
     
     

acc
     
     

table    cross validation results  kubat et al        

under sampling  
   
   
   
   
   
    
    
    
    
    
    
    
    
    
    
    

acc 
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

acc
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    cross validation results for smote at      smote on the oil data set 

   

fichawla  bowyer  hall   kegelmeyer

   future work
there are several topics to be considered further in this line of research  automated adaptive
selection of the number of nearest neighbors would be valuable  dierent strategies for
creating the synthetic neighbors may be able to improve the performance  also  selecting
nearest neighbors with a focus on examples that are incorrectly classied may improve
performance  a minority class sample could possibly have a majority class sample as its
nearest neighbor rather than a minority class sample  this crowding will likely contribute
to the redrawing of the decision surfaces in favor of the minority class  in addition to
these topics  the following subsections discuss two possible extensions of smote  and an
application of smote to information retrieval 
    smote nc
while our smote approach currently does not handle data sets with all nominal features 
it was generalized to handle mixed datasets of continuous and nominal features  we call this
approach synthetic minority over sampling technique nominal continuous  smote nc  
we tested this approach on the adult dataset from the uci repository  the smote nc
algorithm is described below 
   median computation  compute the median of standard deviations of all continuous
features for the minority class  if the nominal features dier between a sample and
its potential nearest neighbors  then this median is included in the euclidean distance
computation  we use median to penalize the dierence of nominal features by an
amount that is related to the typical dierence in continuous feature values 
   nearest neighbor computation  compute the euclidean distance between the feature
vector for which k nearest neighbors are being identied  minority class sample  and
the other feature vectors  minority class samples  using the continuous feature space 
for every diering nominal feature between the considered feature vector and its
potential nearest neighbor  include the median of the standard deviations previously
computed  in the euclidean distance computation  table   demonstrates an example 
f          a b c  let this be the sample for which we are computing nearest
neighbors 
f          a d e
f          a b k
so  euclidean distance between f  and f  would be 
eucl   sqrt                            med    med   
med is the median of the standard deviations of continuous features of the minority class 
the median term is included twice for feature numbers    bd and    ce 
which dier for the two feature vectors  f  and f  

table    example of nearest neighbor computation for smote nc 

   

fismote

   populate the synthetic sample  the continuous features of the new synthetic minority
class sample are created using the same approach of smote as described earlier  the
nominal feature is given the value occuring in the majority of the k nearest neighbors 
the smote nc experiments reported here are set up the same as those with smote 
except for the fact that we examine one dataset only  smote nc with the adult dataset
diers from our typical result  it performs worse than plain under sampling based on auc 
as shown in figures    and     we extracted only continuous features to separate the eect
of smote and smote nc on this dataset  and to determine whether this oddity was
due to our handling of nominal features  as shown in figure     even smote with only
continuous features applied to the adult dataset  does not achieve any better performance
than plain under sampling  some of the minority class continuous features have a very high
variance  so  the synthetic generation of minority class samples could be overlapping with
the majority class space  thus leading to more false positives than plain under sampling 
this hypothesis is also supported by the decreased auc measure as we smote at degrees
greater than      the higher degrees of smote lead to more minority class samples in
the dataset  and thus a greater overlap with the majority class decision space 
adult smotenc
   

  

  

  

 tp

  
underc   
   smotencc   
naive bayes
hull

  

  

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     adult  comparison of smote c     under c     and naive bayes  smotec    and under c    roc curves overlap for most of the roc space 

    smote n
potentially  smote can also be extended for nominal features  smote n  with the
nearest neighbors computed using the modied version of value dierence metric  stanll
  waltz        proposed by cost and salzberg         the value dierence metric  vdm 
looks at the overlap of feature values over all feature vectors  a matrix dening the distance
   

fichawla  bowyer  hall   kegelmeyer

adult roc with ripper
   

  

  

 tp

  

  

underripper
   smoteripper
loss ratio
hull

  

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     adult  comparison of smote ripper  under ripper  and modifying loss ratio in ripper  smote ripper and under ripper roc curves overlap for most
of the roc space 
adult only continuous  c    
   

  

 tp

  

  

under
   smote

  

  

  

 

  

  

  

  

  
 fp

  

  

  

  

   

figure     adult with only continuous features  the overlap of smote c    and underc    is observed under this scenario as well 

   

fismote

between corresponding feature values for all feature vectors is created  the distance 
between two corresponding feature values is dened as follows 
 v    v     

n

c i

 

i  

c 



c i k
 
c 

   

in the above equation  v  and v  are the two corresponding feature values  c  is the total
number of occurrences of feature value v    and c i is the number of occurrences of feature
value v  for class i  a similar convention can also be applied to c i and c    k is a constant 
usually set to    this equation is used to compute the matrix of value dierences for each
nominal feature in the given set of feature vectors  equation   gives a geometric distance
on a xed  nite set of values  cost   salzberg         cost and salzbergs modied vdm
omits the weight term wfa included in the  computation by stanll and waltz  which has
an eect of making  symmetric  the distance  between two feature vectors is given by 

 x  y     wx wy

n


 xi   yi  r

   

i  

r     yields the manhattan distance  and r     yields the euclidean distance  cost  
salzberg         wx and wy are the exemplar weights in the modied vdm  wy     for a
new example  feature vector   and wx is the bias towards more reliable examples  feature
vectors  and is computed as the ratio of the number of uses of a feature vector to the number
of correct uses of the feature vector  thus  more accurate feature vectors will have wx 
   for smote n we can ignore these weights in equation    as smote n is not used for
classication purposes directly  however  we can redene these weights to give more weight
to the minority class feature vectors falling closer to the majority class feature vectors  thus 
making those minority class features appear further away from the feature vector under
consideration  since  we are more interested in forming broader but accurate regions of the
minority class  the weights might be used to avoid populating along neighbors which fall
closer to the majority class  to generate new minority class feature vectors  we can create
new set feature values by taking the majority vote of the feature vector in consideration and
its k nearest neighbors  table     shows an example of creating a synthetic feature vector 
let f    a b c d e be the feature vector under consideration
and let its   nearest neighbors be
f    a f c g n
f    h b c d n
the application of smote n would create the following feature vector 
fs   a b c d n
table    example of smote n

   

fichawla  bowyer  hall   kegelmeyer

    application of smote to information retrieval
we are investigating the application of smote to information retrieval  ir   the ir problems come with a plethora of features and potentially many categories  smote would have
to be applied in conjunction with a feature selection algorithm  after transforming the given
document or web page in a bag of words format 
an interesting comparison to smote would be the combination of naive bayes and
odds ratio  odds ratio focuses on a target class  and ranks documents according to their
relevance to the target or positive class  smote also focuses on a target class by creating
more examples of that class 

   summary
the results show that the smote approach can improve the accuracy of classiers for
a minority class  smote provides a new approach to over sampling  the combination
of smote and under sampling performs better than plain under sampling  smote was
tested on a variety of datasets  with varying degrees of imbalance and varying amounts of
data in the training set  thus providing a diverse testbed  the combination of smote and
under sampling also performs better  based on domination in the roc space  than varying
loss ratios in ripper or by varying the class priors in naive bayes classier  the methods
that could directly handle the skewed class distribution  smote forces focused learning
and introduces a bias towards the minority class  only for pima  the least skewed dataset
 does the naive bayes classier perform better than smote c     also  only for the oil
dataset does the under ripper perform better than smote ripper  for the can dataset 
smote classifier and under classifier roc curves overlap in the roc space  for all the
rest of the datasets smote classifier performs better than under classifier  loss ratio 
and naive bayes  out of a total of    experiments performed  smote classifier does not
perform the best only for   experiments 
the interpretation of why synthetic minority over sampling improves performance where
as minority over sampling with replacement does not is fairly straightforward  consider
the eect on the decision regions in feature space when minority over sampling is done
by replication  sampling with replacement  versus the introduction of synthetic examples 
with replication  the decision region that results in a classication decision for the minority
class can actually become smaller and more specic as the minority samples in the region are
replicated  this is the opposite of the desired eect  our method of synthetic over sampling
works to cause the classier to build larger decision regions that contain nearby minority
class points  the same reasons may be applicable to why smote performs better than
rippers loss ratio and naive bayes  these methods  nonetheless  are still learning from
the information provided in the dataset  albeit with dierent cost information  smote
provides more related minority class samples to learn from  thus allowing a learner to carve
broader decision regions  leading to more coverage of the minority class 

acknowledgments
this research was partially supported by the united states department of energy through
the sandia national laboratories asci views data discovery program  contract number
   

fismote

de ac     do       we thank robert holte for providing the oil spill dataset used in
their paper  we also thank foster provost for clarifying his method of using the satimage
dataset  we would also like to thank the anonymous reviewers for their various insightful
comments and suggestions 

   

fichawla  bowyer  hall   kegelmeyer

appendix a  roc graphs for oil dataset
the following gures show dierent sets of roc curves for the oil dataset  figure     a 
shows the roc curves for the oil dataset  as included in the main text  figure    b  shows
the roc curves without the roc convex hull  figure    c  shows the two convex hulls 
obtained with and without smote  the roc convex hull shown by dashed lines and stars
in figure    c   was computed by including under c    and naive bayes in the family of
roc curves  the roc convex hull shown by solid line and small circles in figure    c  was
computed by including     smote c     under c     and naive bayes in the family of
roc curves  the roc convex hull with smote dominates the roc convex hull without
smote  hence smote c    contributes more optimal classiers 
oil

  

  

  

  

  

  

  

  

 tp

   

  

  

  

  

underc   
    smotec   
naive bayes

  

  

  

 

  

  
underc   
    smotec   
naive bayes
hull

  

 

  

  

  

  

  
 fp

  

  

  

  

 

   

 

  

  

  

  

 a 

  
 fp

  

  

  

  

 b 
oil roc convex hulls
   

  

  

  

  

 tp

 tp

oil roc
   

  
convex hull with smote
convex hull without smote

  

  

  

  

 

 

  

  

  

  

  
 fp

  

  

  

  

   

 c 
figure     roc curves for the oil dataset   a  roc curves for smote c     underc     naive bayes  and their roc convex hull   b  roc curves for smotec     under c     and naive bayes   c  roc convex hulls with and without
smote 

   

   

fismote

references
blake  c     merz  c         
uci repository of machine learning databases
http   www ics uci edu mlearn mlrepository html  department of information
and computer sciences  university of california  irvine 
bradley  a  p          the use of the area under the roc curve in the evaluation of
machine learning algorithms  pattern recognition                  
chawla  n   bowyer  k   hall  l     kegelmeyer  p          smote  synthetic minority
over sampling technique  in international conference of knowledge based computer systems  pp        national center for software technology  mumbai  india 
allied press 
chawla  n     hall  l          modifying mustafa to capture salient data  tech  rep 
isl        university of south florida  computer science and eng  dept 
cohen  w       a   learning to classify english text with ilp methods  in proceedings of the  th international workshop on inductive logic programming  pp      
department of computer science  katholieke universiteit leuven 
cohen  w  w       b   fast eective rule induction  in proc    th international conference on machine learning  pp         lake tahoe  ca  morgan kaufmann 
cohen  w  w     singer  y          context sensitive learning methods for text categorization  in frei  h  p   harman  d   schauble  p     wilkinson  r   eds    proceedings
of sigir       th acm international conference on research and development in
information retrieval  pp         zurich  ch  acm press  new york  us 
cost  s     salzberg  s          a weighted nearest neighbor algorithm for learning with
symbolic features  machine learning               
derouin  e   brown  j   fausett  l     schneider  m          neural network training on
unequally represented classes  in intellligent engineering systems through artificial
neural networks  pp         new york  asme press 
domingos  p          metacost  a general method for making classiers cost sensitive 
in proceedings of the fifth acm sigkdd international conference on knowledge
discovery and data mining  pp         san diego  ca  acm press 
drummond  c     holte  r          explicitly representing expected cost  an alternative
to roc representation  in proceedings of the sixth acm sigkdd international
conference on knowledge discovery and data mining  pp         boston  acm 
duda  r   hart  p     stork  d          pattern classification  wiley interscience 
dumais  s   platt  j   heckerman  d     sahami  m          inductive learning algorithms and representations for text categorization  in proceedings of the seventh
international conference on information and knowledge management   pp         
   

fichawla  bowyer  hall   kegelmeyer

ezawa  k   j   singh  m     norton  s   w          learning goal oriented bayesian
networks for telecommunications risk management  in proceedings of the international conference on machine learning  icml     pp         bari  italy  morgan
kauman 
fawcett  t     provost  f          combining data mining and machine learning for effective user prole  in proceedings of the  nd international conference on knowledge
discovery and data mining  pp      portland  or  aaai 
ha  t  m     bunke  h          o line  handwritten numeral recognition by perturbation
method  pattern analysis and machine intelligence               
hall  l   mohney  b     kier  l          the electrotopological state  structure information
at the atomic level for molecular graphs  journal of chemical information and
computer science          
japkowicz  n          the class imbalance problem  signicance and strategies  in proceedings of the      international conference on artificial intelligence  ic ai      
special track on inductive learning las vegas  nevada 
kubat  m   holte  r     matwin  s          machine learning for the detection of oil
spills in satellite radar images  machine learning             
kubat  m     matwin  s          addressing the curse of imbalanced training sets  one
sided selection  in proceedings of the fourteenth international conference on machine
learning  pp         nashville  tennesse  morgan kaufmann 
lee  s          noisy replication in skewed binary classication  computational statistics
and data analysis     
lewis  d     catlett  j          heterogeneous uncertainity sampling for supervised learning  in proceedings of the eleventh international conference of machine learning  pp 
       san francisco  ca  morgan kaufmann 
lewis  d     ringuette  m          a comparison of two learning algorithms for text
categorization  in proceedings of sdair      rd annual symposium on document
analysis and information retrieval  pp       
ling  c     li  c          data mining for direct marketing problems and solutions  in
proceedings of the fourth international conference on knowledge discovery and data
mining  kdd     new york  ny  aaai press 
mladenic  d     grobelnik  m          feature selection for unbalanced class distribution
and naive bayes  in proceedings of the   th international conference on machine
learning   pp          morgan kaufmann 
orourke  j          computational geometry in c  cambridge university press  uk 
pazzani  m   merz  c   murphy  p   ali  k   hume  t     brunk  c          reducing
misclassication costs  in proceedings of the eleventh international conference on
machine learning san francisco  ca  morgan kaumann 
   

fismote

provost  f     fawcett  t          robust classication for imprecise environments  machine learning               
provost  f   fawcett  t     kohavi  r          the case against accuracy estimation
for comparing induction algorithms  in proceedings of the fifteenth international
conference on machine learning  pp         madison  wi  morgan kaumann 
quinlan  j          c     programs for machine learning  morgan kaufmann  san mateo 
ca 
solberg  a     solberg  r          a large scale evaluation of features for automatic
detection of oil spills in ers sar images  in international geoscience and remote
sensing symposium  pp           lincoln  ne 
stanll  c     waltz  d          toward memory based reasoning  communications of
the acm                    
swets  j          measuring the accuracy of diagnostic systems  science                
tomek  i          two modications of cnn  ieee transactions on systems  man and
cybernetics            
turney  p          cost sensitive bibliography  http   ai iit nrc ca bibiliographies costsensitive html 
van rijsbergen  c   harper  d     porter  m          the selection of good search terms 
information processing and management           
woods  k   doss  c   bowyer  k   solka  j   priebe  c     kegelmeyer  p          comparative evaluation of pattern recognition techniques for detection of microcalcications
in mammography  international journal of pattern recognition and artificial intelligence                 

   

fi