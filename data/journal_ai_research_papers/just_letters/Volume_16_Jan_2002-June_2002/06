journal artificial intelligence research                  

submitted       published     

efficient reinforcement learning using
recursive least squares methods
xin xu
han gen
dewen hu

xuxin mail     net
hehangen cs hn cn
dwhu nudt edu cn

department automatic control
national university defense technology
changsha  hunan          p r china

abstract
recursive least squares  rls  algorithm one well known algorithms used
adaptive filtering  system identification adaptive control  popularity mainly due
fast convergence speed  considered optimal practice  paper  rls methods
used solve reinforcement learning problems  two new reinforcement learning
algorithms using linear value function approximators proposed analyzed  two
algorithms called rls td    fast ahc  fast adaptive heuristic critic   respectively 
rls td    viewed extension rls td       general     
multi step temporal difference  td  learning algorithm using rls methods  convergence
probability one limit convergence rls td    proved ergodic markov
chains  compared existing ls td    algorithm  rls td    advantages
computation suitable online learning  effectiveness rls td   
analyzed verified learning prediction experiments markov chains wide range
parameter settings 
fast ahc algorithm derived applying proposed rls td    algorithm
critic network adaptive heuristic critic method  unlike conventional ahc algorithm 
fast ahc makes use rls methods improve learning prediction efficiency critic 
learning control experiments cart pole balancing acrobot swing up problems
conducted compare data efficiency fast ahc conventional ahc 
experimental results  shown data efficiency learning control improved
using rls methods learning prediction process critic  performance
fast ahc compared ahc method using ls td     furthermore 
demonstrated experiments different initial values variance matrix rls td   
required get better performance learning prediction learning control 
experimental results analyzed based existing theoretical work transient
phase forgetting factor rls methods 

   introduction
recent years  reinforcement learning  rl  active research area machine
learning control engineering  operations research robotics  kaelbling et al       
bertsekas  et al        sutton barto       lin        computational approach
     ai access foundation morgan kaufmann publishers  rights reserved 

fixu  he    hu

understand automate goal directed learning decision making  without relying
exemplary supervision complete models environment  rl  agent placed
initial unknown environment receives evaluative feedback environment 
feedback called reward reinforcement signal  ultimate goal rl learn strategy
selecting actions expected sum discounted rewards maximized 
since lots problems real world sequential decision processes delayed
evaluative feedback  research rl focused theory algorithms learning
solve optimal control problem markov decision processes  mdps  provide
elegant mathematical model sequential decision making  operations research  many results
presented solve optimal control problem mdps model information 
however  reinforcement learning  model information assumed unknown 
different methods studied operations research dynamic programming 
dynamic programming  two elemental processes  policy evaluation process policy improvement process  respectively  rl  two similar processes 
one called learning prediction called learning control  goal learning
control estimate optimal policy optimal value function mdp without knowing
model  learning prediction aims solve policy evaluation problem stationary policy
mdp without prior model regarded sub problem learning control 
furthermore  rl  learning prediction different supervised learning  pointed
sutton         prediction problems supervised learning single step prediction
problems reinforcement learning multi step prediction problems  solve
multi step prediction problems  learning system must predict outcomes depend future
sequence decisions  therefore  theory algorithms multi step learning prediction
become important topic rl much research work done literature  sutton 
      tsitsiklis roy        
among proposed multi step learning prediction methods  temporal difference  td 
learning  sutton        one popular methods  studied applied early
research machine learning  including celebrated checkers playing program  minsky       
samuel               sutton presented first formal description temporal  difference
methods td    algorithm  sutton        convergence results established tabular
temporal difference learning algorithms cardinality tunable parameters
state space  sutton        watkins et al        dayan et al         jaakkola  et
al         since many real world applications large infinite state space  value function
approximation  vfa  methods need used cases  combined nonlinear
value function approximators  td    guarantee convergence several results
regarding divergence reported literature  tsitsiklis roy        td   
linear function approximators  called linear td    algorithms  several convergence
proofs presented  dayan        showed convergence mean linear td   
algorithms arbitrary       tsitsiklis roy        proved convergence
special class td learning algorithms  known td     tsitsiklis roy        
extended early results general linear td    case proved convergence
probability one 
linear td    algorithms rules updating parameters similar
gradient descent methods  however  gradient learning methods  step size schedule must
carefully designed guarantee convergence obtain good performance 
   

fiefficient reinforcement learning using rls methods

addition  inefficient use data slows convergence algorithms  based
theory linear least squares estimation  brartke barto        proposed two
temporal difference algorithms called least squares td    algorithm  ls td    
recursive least  squares td    algorithm  rls td      respectively  ls td    rls td   
efficient statistical sense conventional linear td    algorithms
eliminate design step size schedules  furthermore  convergence ls td   
rls td    provided theory  two algorithms viewed
least squares versions conventional linear td    methods  however  shown
literature  td learning algorithms td          update predictions based
estimates multiple steps efficient monte carlo methods well td    
employing mechanism eligibility traces  determined   td    algorithms
      extract information historical data  recently  class linear
temporal difference learning algorithms called ls td    proposed boyan
             least squares methods employed compute value function estimation
td         although ls td    efficient td     requires much
computation per time step online updates needed number state features
becomes large 
system identification  adaptive filtering adaptive control  recursive least squares
 rls   young       ljung        ljung       method  commonly used reduce
computational burden least squares methods  suitable online estimation control 
although rls td    makes use rls methods  employ mechanism
eligibility traces  based work tsitsiklis roy               boyan            
motivated ideas  new class temporal difference learning methods  called
rls td    algorithm  proposed analyzed formally paper  rls td    superior
conventional linear td    algorithms makes use rls methods improve
learning efficiency statistical point view eliminates step size schedules 
rls td    mechanism eligibility traces viewed extension
rls td       general      convergence probability   rls td   
proved ergodic markov chains limit convergence analyzed  learning
prediction experiments markov chains  performance rls td    td    well
ls td    compared  wide range parameter settings tested  addition  influence initialization parameters rls td    discussed  observed
rate convergence influenced initialization variance matrix 
phenomenon investigated theoretically adaptive filtering  moustakides        haykin        
analyzed following sections  two benefits extension
rls td    rls td     one value       still affect performance
rls based temporal difference algorithms  although rls td     rate
convergence mainly influenced initialization variance matrix  bound
approximation error dominantly determined parameter   smallest error bound
obtained    worst bound obtained     bounds suggest
value selected appropriately obtain best approximation error  second
benefit rls td    suitable online learning ls td    since
computation per time step reduced o k   o k    k number state
features 
adaptive heuristic critic  ahc  learning algorithm class reinforcement learning
   

fixu  he    hu

methods actor critic architecture used solve full reinforcement learning
learning control problems  applying rls td    algorithm critic  fast ahc
algorithm proposed paper  using rls methods critic  performance learning
prediction critic improved learning control problems solved
efficiently  simulation experiments learning control cart pole balancing problem
swing up acrobot conducted verify effectiveness fast ahc method 
comparing conventional ahc methods use td    critic  demonstrated
fast ahc obtain higher data efficiency conventional ahc methods  experiments
performance comparisons ahc methods using ls td    fast ahc
conducted  learning control experiments  illustrated initializing constant
variance matrix rls td    influences performance fast ahc different values
constant selected get better performance different problems 
results analyzed based theoretical work transient phase rls methods 
paper organized follows  section    introduction previous linear
temporal difference algorithms presented  section    rls td    algorithm proposed
convergence  with probability one  proved  section    simulation example
value function prediction absorbing markov chains presented illustrate effectiveness
rls td    algorithm  different parameter settings different algorithms
including ls td    studied  section    fast ahc method proposed
simulation experiments learning control cart pole balancing acrobot
conducted compare fast ahc conventional ahc method well
ls td    based ahc method  simulation results presented analyzed detail 
last section contains concluding remarks directions future work 

   previous work linear temporal difference algorithms
section  brief discussion conventional linear td    algorithm rls td   
well ls td    algorithm given  first all  mathematical notations
presented follows 
consider markov chain whose states lie finite countable infinite space s  states
markov chain indexed       n   n possibly infinite  although
algorithms results paper applicable markov chains general state space 
discussion paper restricted within cases countable state space
simplify notation  extension markov chains general state space requires
translation matrix notation operator notation 
let trajectory generated markov chain denoted  xt  t         xt s  the
dynamics markov chain described transition probability matrix p whose  i j  th
entry  denoted pij  transition probability xt   j given xt i  state transition
xt xt    scalar reward rt defined  value function state defined follows 


v  i     e  rt x     i 

   

  

     discount factor 
td    algorithm  two basic mechanisms temporal difference
   

fiefficient reinforcement learning using rls methods

eligibility trace  respectively  temporal differences defined differences
two successive estimations following form 
 
 
  rt   vt   xt      vt   xt  
   
 
xt   successive state xt  v   x  denotes estimate value function v x  rt
reward received state transition xt xt   
eligibility trace viewed algebraic trick improve learning efficiency
without recording data multi step prediction process  trick based idea
using truncated return markov chain  temporal difference learning eligibility
traces  n step truncated return defined
 
rtn   rt   rt            n   rt   n     nvt     n  
   
absorbing markov chain whose length t  weighted average truncated returns

 

rt       

n  rtn     rt

   

n   

    decaying factor rt  rt   rt            rt monte carlo return
terminal state  step td    algorithm  update rule value function
estimation determined weighted average truncated returns defined above 
corresponding update equation
 
 
vt         rt vt     
   
learning factor 
update equation     used whole trajectory markov chain
observed  realize incremental online learning  eligibility traces defined state
follows 

z         
z         
z     

 


online td    update rule eligibility traces
 
 
vt      si     vt   si     z      si  

   

   

temporal difference time step t  defined     z  s    s 
since state space markov chain usually large infinite practice  function
approximators neural networks commonly used approximate value function 
td    algorithms linear function approximators popular well studied ones 
consider general linear function approximator fixed basis function vector

  x          x        x        n   x   t
estimated value function denoted

 
vt   x      x wt
   

   

fixu  he    hu

wt   w   w   wn t weight vector 
corresponding incremental weight update rule

r
wt      wt    rt     xt     wt   xt  wt   z   
r
eligibility trace vector z         z t      z  t          z nt      defined
r
r
z      z     xt  

   

    

tsitsiklis roy         linear td    algorithm proved converge
probability   certain assumptions limit convergence w  derived 
satisfies following equation 
e     a  x   w   e    b  x       

    

xt   xt xt   zt     t       form markov process  e    stands expectation
respect unique invariant distribution  xt   a xt  b xt  defined
r
a  x     z     xt     xt      
    

r
b  x     z rt

    

improve efficiency linear td   algorithms  least squares methods used
linear td    algorithm  ls td    rls td    algorithms suggested  brartke
barto         ls td    rls td     following quadratic objective function defined 
 

j    rt   tt tt    w    

    

  

thus  aim ls td    rls td    obtain least squares estimation real
value function satisfies following bellman equation 
v   xt     e rt   xt   xt        v   xt      

    

employing instrumental variables approach  soderstrom stoica        
least squares solution      given




  

  

w ls td                          rt  

    

instrumental variable chosen uncorrelated input output noises 
rls td     recursive least squares methods used decrease computational burden ls td     update rules rls td    follows 
wt      wt   pt  rt        wt                pt  

    

pt      pt pt        pt              pt  

    

convergence  with probability one  ls td    rls td    proved periodic
absorbing markov chains certain assumptions  brartke barto       
   

fiefficient reinforcement learning using rls methods

boyan              ls td    proposed solving      directly model based
property ls td    analyzed  however  ls td     computation per time step
o k    i e   cubic order state feature number  therefore computation required
ls td   increases fast k increases  undesirable online learning 
next section  propose rls td    algorithm making use recursive
least squares methods computational burden ls td    reduced o k  
o k    give rigorous mathematical analysis algorithm  convergence
 with probability    rls td    proved 

   rls td    algorithm
markov chain discussed above  linear function approximators used 
least squares estimation problem      following objective function 
j 





  

  

a  x  w b  x  

 

    

a  x   r nn   b  x   r n defined            respectively  euclid norm
n number basis functions 
ls td     least squares estimate weight vector w computed according
following equation 




  

  

w ls td       at bt     a  x        b  x   

    



r
    a  x      z     xt     xt      

    



r
bt   b  x     z rt

    



  

  

  

  

well known system identification  adaptive filtering control  rls methods
commonly used solve computational memory problems least squares algorithms 
sequel  present rls td    algorithm based idea  first  matrix inverse lemma given follows 
lemma   ljung  et al         r nn   b r n    c r  n invertible 

    bc           b     ca   b     ca  

    

pt   at 

    

let

   

fixu  he    hu

p   

    

r
k      pt    z

    

positive number identity matrix 
weight update rules rls td    given
r
r
k      pt z          xt     xt       pt z  
wt      wt   k     rt     xt     xt      wt  

pt     

 



r
r
  pt pt z         xt     xt       pt z          xt     xt       pt  

    
    

    

standard rls td   algorithm      general forgetting factor rls td  
case      
forgetting factor       usually used adaptive filtering improve
performance rls methods non stationary environments  forgetting factor rls td   
algorithm     derived using similar techniques haykin         detailed
derivation rls td   referred appendix a 
follows  descriptions rls td    two different kinds markov chains
given  first  complete description rls td    ergodic markov chains presented below 

algorithm   rls td    ergodic markov chains

   given 
termination criterion algorithm 
set basis functions   j  i      j      n  state i  n
number basis functions 
   initialize 
      let t   
      initialize weight vector wt  variance matrix pt   initial state x  
r
      set eligibility traces vector z      
   loop 
      current state xt  observe state transition xt xt  
reward r xt  xt    
      apply equations           update weight vector 
      t t   
termination criterion satisfied 

rls td    algorithm absorbing markov chains little different
algorithm coping state features absorbing states  following description
   

fiefficient reinforcement learning using rls methods

rls td    absorbing markov chains 

algorithm   rls td    absorbing markov chains

   given 
termination criterion algorithm 
set basis functions   j  i      j      n  state i  n
number basis functions 
   initialize 
      let t   
      initialize weight vector wt  variance matrix pt   initial state x  
r
      set eligibility traces vector z      
   loop 
      current state xt 
xt absorbing state  set  xt       r xt  rt  rt terminal
reward 
otherwise  observe state transition xt xt   reward
r xt  xt    
      apply equations           update weight vector 
      xt absorbing state  re initialize process setting xt   initial
r
state set eligibility traces z zero vector 
      t t   
termination criterion satisfied 

rls td    algorithm absorbing markov chains  weight updates
absorbing states treated differently process re initialized absorbing states
transform absorbing markov chain equivalent ergodic markov chain 
following convergence analysis  focus ergodic markov chains 
similar assumptions tsitsiklis roy         prove proposed
rls td    algorithm converges probability one 
assumption    markov chain  xt   whose transition probability matrix p  ergodic 
unique distribution satisfies



p  
    
 i    finite infinite vector  depending cardinality s 


assumption    transition rewards r xt xt    satisfy

e    r     xt   xt        

    

e     expectation respect distribution  
assumption    matrix                n   r n n full column rank  is  basis
   

fixu  he    hu

functions  i      n  linearly independent 
assumption    every  i      n   basis function satisfies
 

e       xt     

    

 
a  x    non singular t   
  
assumptions    almost linear td   algorithms discussed
tsitsiklis roy        except assumption    ergodic markov chains considered 
assumption   specially needed convergence rls td   algorithm 
based assumptions  convergence theorem rls td   given
follows 
assumption    matrix   p    

theorem    markov chain satisfies assumptions     asymptotic estimate found
rls td    converges  probability    w  determined      

proof theorem    please refer appendix b  condition specified
assumption   satisfied setting p   appropriately 
according theorem    rls td    converges solution conventional linear
td    algorithms do  satisfies       limit convergence characterized
following theorem 
theorem    tsitsiklis roy        let w  weight vector determined      v 
true value function markov chain  assumption     following relation
holds 
 
w   v  
    
v   v  


 



x



 

x dx            

explanations notations theorem    please refer appendix b 
discussed tsitsiklis roy         theorem shows distance
limiting function w  true value function v  bounded smallest bound
approximation error obtained     every     bound actually deteriorates
decreases  worst bound obtained     although bound  strongly
suggests higher values likely produce accurate approximations v  
compared ls td    additional parameter rls td    value
initial variance matrix p   pointed haykin       pp       exact value
initializing constant insignificant effect data length large enough 
means limit  final solutions obtained ls rls almost same 
influence transient phase  positive constant becomes large enough goes
infinity  transient behavior rls almost ls methods  ljung 
       initialized relatively small value  transient phases rls ls
different  practice  observed variable performance rls
function initialization  moustakides         cases  rls exhibit
significantly faster convergence initialized relatively small positive definite matrix
initialized large one  haykin       moustakides        hubing alexander 
   

fiefficient reinforcement learning using rls methods

       first effort toward direction statistical analysis rls soft exact
initialization limits case number iterations less size
estimation vector  hubing alexander         moustakides        provided theoretical
analysis relation algorithmic performance rls initialization  
using settling time performance measure  moustakides proved well known
rule initialization relatively small matrix preferable cases high medium
signal to noise ratio  snr   whereas low snr  relatively large matrix must selected
achieving best results  following learning prediction experiments rls td    well
learning control simulation fast ahc  observed value initializing
constant plays important role convergence performance  theoretical
analyses provide clue explain experimental results 

   learning prediction experiments markov chains
section  illustrative example given show effectiveness proposed
rls td   algorithm  furthermore  algorithmic performance influence
initializing constant studied 
example finite state absorbing markov chain called hop world problem  boyan 
       shown figure    hop world problem    state markov chain
absorbing state 

figure    hop world problem
figure    state    initial state trajectory state   absorbing state 
non absorbing state two possible state transitions transition probability     
state transition reward   except transition state   state   reward   
thus  true value function state   i     i 
apply linear temporal difference algorithms value function prediction problem  set
four element state features basis functions chosen  shown figure    state
features states          are  respectively                                            
state features states obtained linearly interpolating these 
simulation  rls td    algorithm well ls td   conventional linear
td    algorithms used solve value function prediction problem without
knowing model markov chain  experiments  trial defined period
initial state    terminal state    performance algorithms evaluated
averaged root mean squared  rms  error value function predictions    states 
parameter setting  performance averaged    independent monte carlo runs 
figure   shows learning curves rls td   conventional linear td   algorithms
three different parameter settings  parameter set     algorithms
   

fixu  he    hu

step size parameter td   following form 

n    

n    
n    n

    

step size schedule studied boyan         experiments  three
different settings used 
 s              n         
 s              n         
 s             n           

    

different boyan         linear td   algorithms applied
online forms  update weights every state transitions  parameter n     
number state transitions  run  weights initialized zeroes  figure   
learning curves conventional linear td   algorithms step size schedules  s     s  
 s   shown curves        respectively  curve  averaged rms errors
value function predictions states    independent runs plotted trial 
curve   shows learning performance rls td    one additional parameter rls td  
initial value variance matrix p   experiment  set     
relatively large value  figure    concluded making use rls methods 
rls td   obtain much better performance conventional linear td   algorithms
eliminates design problem step size schedules  experiments linear td  
rls td   different parameters conducted similar results obtained
initial values rls td   large conclusion confirmed 

figure    performance comparison rls td   td  
         td      step size parameters specified  s    s    s  
 rls td      initial variance matrix p     i
done demonstrative experiments investigate influence performance
rls td   algorithm  figure   shows performance comparison rls td  
   

fiefficient reinforcement learning using rls methods

algorithms using two different initial parameters variance matrix p   p     i
p      i  respectively  forgetting factor         performance suggested
algorithm measured averaged rms errors value function prediction first
    trials    independent runs    states  experiments     settings
parameter tested     n  n          
figure    clearly shown performance rls td   large initial value
much better rls td   small initial value   experiments
different parameter settings   similar results obtained  may refer
phenomenon low snr case forgetting factor rls studied moustakides        
hop world problem  stochastic state transitions could introduce high equation
residuals a  x  w b  x         corresponds additive noise large variance 
i e   low snr case  discussed section    forgetting factor rls low
snr cases  relatively large initializing constant must selected better results  full
understanding phenomenon yet found 

figure    performance comparison rls td   different initial value         
performance rls td   unit forgetting factor    tested
experiments  although initial value effect rls    discussed intensively
 moustakides        effects observed empirically case   
    shown figure   
experiments  found initialized small value 
performance sensitive values parameter   case  convergence
speed rls td   increases increases      shown figure   
furthermore  fixed  performance rls td   deteriorates becomes smaller 
shown figure    

   

fixu  he    hu

figure    performance comparison rls td   different initial value     

figure    learning curves ls td   rls td   different     

figure    learning curves rls td   different initializing constants
shown compared ls td    experiment  set      figure   
shown performance rls td   approaches ls td   becomes large 
well known  becomes large enough  performance rls ls methods
almost same  figure   shows performance comparison ls td  
rls td   large value   initial variance matrix rls td   set    i
every runs  identity matrix 

   

fiefficient reinforcement learning using rls methods

figure    performance comparison ls td   rls td      large initial
value
based experimental results  concluded convergence speed
rls td    mainly influenced initial value variance matrix parameter
  detailed discussions properties rls td    given follows 
    relatively large  effect becomes small  large enough goes
infinity  performance rls td    ls td    almost same 
discussed above  cases  effect speed convergence insignificant 
coincides discussion boyan         however  described theorem   
value still affects ultimate error bound value function approximation 
    relatively small  observed convergence performance
rls td   different ls td   influenced values  
experiments hop world problem  results show smaller values lead
slower convergence  results may explained theoretical analysis transient
phase forgetting factor rls  moustakides        according theory moustakides
        larger values needed better performance cases low snr
smaller values preferable fast convergence cases high medium snr 
different values must selected faster convergence rls td    different cases 
especially  cases  high snr case discussed moustakides         rls
methods small values obtain fast speed convergence 
    compared conventional linear td    algorithms  rls td    algorithm
obtain much better performance making use rls methods value function prediction
problems  furthermore  td     step size schedule needs carefully designed achieve
good performance  rls td     initial value variance matrix selected
according criterion large small value 
    comparison ls td    rls td     one preferable depends
objective  online applications  rls td    advantages computational efficiency
computation per step rls td    o k   ls td     o k   
   

fixu  he    hu

k number state features  moreover  seen later  rls td    obtain better
transient convergence performance ls td    cases  hand  ls td   
may preferable rls td    long term convergence performance  seen
figure    system identification point view  ls td    obtain unbiased
parameter estimates face white additive noises rls td    finite would
possess large parameter discrepancies 

   fast ahc algorithm two learning control experiments
section  fast ahc algorithm proposed based results learning
prediction solve learning control problems  two learning control experiments conducted
illustrate efficiency fast ahc 
    fast ahc algorithm

ultimate goal reinforcement learning learning control  i e   estimate optimal
policies optimal value functions markov decision processes  mdps   now  several
reinforcement learning control algorithms including q learning  watkins dayan       
sarsa learning  singh  et al        adaptive heuristic critic  ahc  algorithm  barto 
sutton anderson       proposed  among methods  ahc method
different q learning sarsa learning value function based methods 
ahc method  value functions policies separately represented value functionbased methods policies determined value functions directly  two
components ahc method  called critic actor  respectively  actor
used generate control actions according policies  critic used evaluate
policies represented actor provide actor internal rewards without waiting
delayed external rewards  since objective critic policy evaluation learning
prediction  temporal difference learning methods chosen critics learning algorithms 
learning algorithm actor determined estimation gradient policies 
following discussion  detailed introduction ahc method given 
figure   shows architecture learning system based ahc method  learning
system consists critic network actor network  inputs critic network include
external rewards state feedback environment  internal rewards provided
critic network called temporal difference  td  signals 
reinforcement learning methods  whole system modeled mdp denoted
tuple  s a p r  where state set  action set  p state transition probability
r reward function  policy mdp defined function  spr a  
pr a  probability distribution action space  objective ahc method
estimate optimal policy   satisfying following equation 


j   max j   max e   rt  
 





    

  

discount factor rt reward time step te    stands expectation
respect policy state transition probabilities j expected total
reward 

   

fiefficient reinforcement learning using rls methods

figure    ahc learning system
value function stationary policy optimal value function optimal
policy defined follows 


v       e   rt      

    

v         e    rt s     

    

  


  

according theory dynamic programming  optimal value function satisfies
following bellman equation 
    
v         max  r   s      ev         


r s a  expected reward received taking action state s 
ahc  critic uses temporal difference learning approximate value function
current policy  linear function approximators used critic  weight update
equation
wt      wt    rt   v        v     z

    

zt eligibility trace defined      
action selection policy actor determined current state value
function estimation critic  suppose neural network weight vector u  u   u    um 
used actor  output actor network

  f  u   st  

    

action outputs actor determined following gaussian probabilistic distribution 
    
p r       exp     
    



mean value given      variance given

  k        exp k  v     

    

equation  k  k  positive constants v st  value function es   

fixu  he    hu

timation critic network 
obtain learning rule actor  estimation policy gradient given
follows 
j
j

 
rt
u
u
u

    

rt internal reward td signal provided critic 

rt   rt   v   st      v   st  

    

since ahc method  critic used estimate value function actors policy
provide internal reinforcement using temporal difference learning algorithms 
efficiency temporal different learning learning prediction greatly influence whole
learning systems performance  although policy actor changing  may change
relatively slowly especially fast convergence learning prediction critic
realized  previous sections  rls td    shown better data efficiency
conventional linear td    algorithms fast convergence speed obtained
initializing constant chosen appropriately  thus  applying rls td    policy
evaluation critic network improve learning prediction performance critic
promising enhance whole systems learning control performance  based
idea  new ahc method called fast ahc algorithm proposed paper  efficiency
fast ahc algorithm verified empirically detailed analysis results given 
following complete description fast ahc algorithm 

algorithm    fast ahc algorithm
   given  critic neural network actor neural network  linear
parameters  stop criterion algorithm 
   initialize state mdp learning parameters  set t   
   stop criterion satisfied 
      according current state   compute output actor network  

     

determine actual action actor probability distribution given
     
take action mdp  observe state transition

     set reward rt   r   st   st       
     
     

     

apply rls td    algorithm described           update weights
critic network 
apply following equation update weights actor network 
j
      
    

learning factor actor 
let t t    return   

   

fiefficient reinforcement learning using rls methods

    learning control experiments cart pole balancing problem
balancing control inverted pendulums typical nonlinear control problem
widely studied control theory artificial intelligence  research
artificial intelligence  learning control inverted pendulums considered standard test
problem machine learning methods  especially rl algorithms  studied
early work michies boxes system  michie et al        later barto sutton        
learning controllers two output values      n     n   berenji  et
al        lin  et al         ahc methods continuous outputs applied cart pole
balancing problem  paper  cart pole balancing problem continuous control values
used illustrate effectiveness fast ahc method 
figure   shows typical cart pole balancing control system  consists cart moving
horizontally pole one end fixed cart  let x denote horizontal distance
center cart center track  x negative cart
left part track  variable denotes angle pole upright position  in
degrees  f amount force  n  applied cart move towards left right 
control system four state variables x  x         x     derivatives x  
respectively 
figure    mass cart m    kg  mass pole m    kg  half pole
length l    m  coefficient friction cart track c        coefficient
friction pole cart p           boundary constraints state variables
given follows 
     
    
   m x    m
    
dynamics control system described following equations 

p  m     
 m     g sin cos   f   ml    sin c sgn  x    

ml
    

 
 
    

    m l ml cos
 


f   ml      sin    cos   c sgn  x   
 x   
 m

g acceleration due gravity     m s   parameters
dynamics equations studied barto et al         

figure    cart pole balancing control system
   

fixu  he    hu

learning control experiments pole balancing problem  dynamics     
assumed unknown learning controller  addition four state variables 
available feedback failure signal notifies controller failure occurs 
means values state variables exceed boundary constraints prescribed inequalities
           typical reinforcement learning problem  failure signal serves
reward  since external reward may available long sequence actions  critic
ahc learning controller used provide internal reinforcement signal accomplish
learning task  learning control experiments pole balancing problem conducted
using conventional ahc method uses linear td   algorithms critic
fast ahc method proposed paper 
solve continuous state space problem reinforcement learning  class linear
function approximators  called cerebellar model articulation controller  cmac 
used  neural network model based neuro physiological theory human
cerebellarcmac first proposed albus        widely used automatic
control function approximation  cmac neural networks  dependence adjustable
parameters weights respect outputs linear  detailed discussion structure
cmac neural networks  one may refer albus        sutton   barto        
ahc fast ahc learning controllers  two cmac neural networks four inputs
one output used function approximators critic actor 
respectively  cmac c tilings partitions every input  total physical
memory cmac network m c  reduce computation memory requirements 
hashing technique described following equations employed experiments   for
detailed discussion parameters cmac networks  please refer appendix c  
a     

 

 a i       

    

  

f s  a s  mod k
    
           represents input state vector  a i     a i  m  activated tile
i th element s  k total number physical memory f s  physical
memory address corresponding state s  remainder a s  divided k 
order compare performance different learning algorithms  initial parameters
learning controller selected follows  weights critic initialized  
weights actor initialized random numbers interval         
parameters ahc fast ahc algorithms          k        k          
experiments  trial defined period initial state failure state
initial state trial set randomly generated state near unstable equilibrium
          maximum distance       equation      employed simulate dynamics
system using euler method  time step     s  trial lasts
        time steps  said successful learning controller assumed
able balance pole  reinforcement signal problem defined

   failure occurs
rt  
   otherwise

    

performance fast ahc method tested extensively  different parameter
settings including initial variance matrix p  chosen  experiments 
   

fiefficient reinforcement learning using rls methods

forgetting factor rls td   critic set value equal   close   
learning control experiments using conventional ahc methods conducted
comparison  performance comparisons two algorithms shown figure      
   
experiments  initial variance matrixes fast ahc algorithm set
p     i  performance fast ahc compared ahc different   numbers
physical memories critic network actor network chosen       
respectively  parameter setting two algorithms    independent runs tested 
performance evaluated according trial number needed successfully balance pole 
learning factors actor networks set      manually optimized value
algorithms  experiments     settings tested 

figure    performance comparison fast ahc ahc      

figure     performance comparison fast ahc ahc      
   

fixu  he    hu

figure     performance comparison fast ahc ahc      

figure           learning factors critic networks ahc chosen
                  respectively  found        performance ahc
becomes worse  learning factors greater       ahc algorithm may
become unstable  even              ahc algorithm becomes unstable
    time varying learning factors specified  s    s    performance worse
constant learning factors  three settings learning factor typical
near optimal ahc algorithm 
experimental results  concluded using rls td  
critic network  fast ahc algorithm obtain better performance conventional ahc
algorithms  although fast ahc requires computation per step ahc 
efficient ahc less trials data needed successfully balance pole 
discussed previous sections  convergence performance rls td  
influenced initial value variance matrix  case fast ahc 
learning control experiments  small value      selected  experiments 
set small values  performance fast ahc satisfactory better ahc 
however  equal relatively large value  example           performance
fast ahc deteriorates significantly  since rls td   large initializing constant
similar performance ls td    deduced ahc method using ls td  
critic bad performance cart pole balancing problem  verify this 
experiments conducted using fast ahc large initializing constant ahc using
ls td    parameter setting    independent runs tested  experiments 
maximum trials algorithm one run     algorithm fails balance
pole within     trials  performance set     when using ls td   ahc method 
may computational problems matrix inversion first steps learning
two methods tried avoid problem  one usage td   first    steps
updates  actor updated early stage learning ls td  
   

fiefficient reinforcement learning using rls methods

stable  however  similar results found two methods  figure    shows experimental
results clearly verify performance fast ahc large initializing constant
similar ahc using ls td   much worse fast ahc small   detailed
discussion phenomenon provided subsection     

figure     performance comparison fast ahc different initial variance
following figure    figure     variations pole angle control
force f plotted  successfully trained fast ahc learning controller used control
cart pole system 

figure     variation pole angle

figure     variation control force

    learning control experiments acrobot

subsection  another learning control example  swing up control acrobot
minimum time  presented  learning control acrobot class adaptive optimal
control problem difficult pole balancing problem  investigated
sutton         cmac based sarsa learning algorithms employed solve
case discrete control actions studied  experiments  case continuous actions
   

fixu  he    hu

considered 
acrobot moving vertical plane shown figure     oa ab first
link second link  respectively  control torque applied point a  goal
swing up control swing tip b acrobot line cd higher
joint amount length one link 

figure     acrobot
dynamics acrobot system described following equations 

       d              d 

    

               d     

    

d    m l c     m   l     l c      l l c   cos            

    

    m   l c     l l c   cos        

    

    m  l l c      sin    m  l l c       sin      m l c    m  l    g cos            

    

    m  l c   g cos            

    



equations  parameters    i   mi   li     l ci angle  angle velocity 
mass  length  moment inertia length center mass link  i      
respectively 
let st denote goal state swing up control  since control aim swing
acrobot minimum time  reward function rt defined
     st
rt  
   else

    

simulation experiments  control torque continuous bounded    n   n  
similar cart pole balancing problem  cmac neural networks applied solve
   

fiefficient reinforcement learning using rls methods

learning control problem continuous states actions  cmac based actor critic
controller  actor network critic network c   tilings m   partitions
input  actor network  uniform coding employed non uniform coding used
critic network  details coding parameters  please refer appendix c  sizes
physical memories actor network critic network         respectively 
cmac networks  following hashing techniques used   for definition a s  a i 
f s   please refer subsection      
 

a        a i      

    

  

f s  a s  mod k
    
simulation  parameters acrobot chosen m  m   kg  i  i   kgm  
lc  lc     m  l  l   m g    m s   time step simulation     s time interval
learning control    s  learning parameters                    k       k      
trial defined period starts stable equilibrium ends goal state
reached  trial  state acrobot re initialized stable equilibrium 
parameter setting    independent runs tested  run consists    trials    th trial 
actor network tested controlling acrobot alone  i e   setting action variance
defined      zero  performance algorithms evaluated according steps
used actor networks swing acrobot 
performance comparisons fast ahc ahc shown figure      
    experiments  algorithms tested different ahc tested
different learning factors critic networks 
results  shown fast ahc achieve higher data efficiency ahc 
however  example  relatively large used  different previous
cart pole balancing example  experiments  good performance obtained large
initializing constant small  performance deteriorates significantly  thus
problem may referred low snr case moustakides         large values
preferable best convergence rate rls methods 

figure     performance comparison fast ahc ahc      
   

fixu  he    hu

figure     performance comparison fast ahc ahc      

figure     performance comparison fast ahc ahc     

following figure    shows performance comparison fast ahc large
      small        value     settings parameter tested
algorithm  performance ahc using ls td   shown  figure     typical curve
angle first link plotted  acrobot controlled actor network
fast ahc method           trials 

   

fiefficient reinforcement learning using rls methods

figure     performance comparison fast ahc ahc using ls td  

figure     variation angle link   controlled fast ahc    trials 

   

analysis experimental results

based experimental results  concluded using rls td  
algorithm critic network  fast ahc algorithm obtain better performance
conventional ahc algorithms less trials data needed converge near optimal
policy  well known  one difficulty applications rl methods slow
convergence  especially cases learning data hard generated 
fast ahc algorithm  although computation per step required conventional ahc
methods  serious problem number linear state features small 
learning control experiments  hashing techniques used reduce state features
cmac networks computation fast ahc reduced economical amount 
nevertheless  state feature number large  conventional ahc methods may
preferable 
experiments  observed performance fast ahc affected
initializing constant   results consistent property rls td   rls
   

fixu  he    hu

method adaptive filtering  discussed section    learning control
experiments cart pole balancing problem  better performance fast ahc obtained
using small values   learning control acrobot  higher data efficiency
achieved using fast ahc relatively large   two different properties fast ahc
may referred different snr cases rls methods  moustakides        thorough
theoretical analysis problem interesting topic future research 
experiments  performance ahc method using ls td   tested 
studied section    initializing constant large  performance
rls td   ls td   differ much  performance ahc using ls td  
similar fast ahc large values  
studied moustakides         rls method converge much faster
adaptive filtering methods environment stationary initializing constant selected
appropriately  cases  rls may converge almost instantly  verified
learning prediction experiments rls td   algorithm  applying rls td  
actor critic learning controller  although policy actor change time  still
assumed changing speed policy slow compared fast
convergence speed rls td    thus good performance learning prediction obtained
critic  moreover  since learning prediction performance critic important
policy learning actor  improvement learning prediction efficiency contribute
whole performance improvement controller 
   conclusions future work

two new reinforcement learning algorithms using rls methods  called rls td   
fast ahc  respectively  proposed paper  rls td    used solve learning
prediction problems efficiently conventional linear td    algorithms 
convergence probability   proved rls td    limit convergence
analyzed  experimental results learning prediction problems show rls td   
algorithm superior conventional td    algorithms data efficiency eliminates
design problem step sizes linear td    algorithms  rls td    viewed
extension rls td       general       although effect
convergence speed rls td    may significant cases  usage   
still affect approximation error bound  thus  needs value function
estimation high precision  large values preferable     furthermore  rlstd    superior ls td    computation weight vector must updated
every observations 
since learning prediction viewed sub problem learning control  extend
results learning prediction learning control method called ahc algorithm  using
rls td    critic network  fast ahc achieve better performance conventional
ahc method data efficiency  simulation results learning control pole balancing
problem acrobot system confirm analyses 
experiments  found performance rls td    well fast ahc
influenced initializing constant rls methods  different values needed best
performance different cases  well known phenomenon rls based adaptive
   

fiefficient reinforcement learning using rls methods

filtering theoretical results moustakides        provide basis explanations
results  complete investigation problem ongoing work 
idea using rls td    critic network may applied reinforcement
learning methods actor critic architectures  konda tsitsiklis         new actor critic
algorithm using linear function approximators proposed convergence certain
conditions proved  one condition convergence algorithm convergence
rate critic much faster actor  thus application rls td   
critic may preferable order ensure convergence algorithm  theoretical
empirical work problem deserves studied future 

acknowledgements
work supported national natural science foundation china grants
                   china university key teachers fellowship  would much
thank anonymous reviewers associate editor michael l  littman insights
constructive criticisms  helped improve paper significantly 

   

fixu  he    hu

appendix a  derivation rls td   algorithm

derivation rls td    two different cases  determined value
forgetting factor 
    rls td   unit forgetting factor 
since

pt   at 

    

p   

    

r
k      pt    z

    

according lemma   

pt      at   

r
r
  pt pt z          xt     x       pt z          x     xt       pt

r
k      pt    z
r
r
  pt z           xt     xt       pt z  

    

    

wt      at   bt   

r
  pt      z ri  

    

  

r
  pt      pt  wt   z rt  
thus

r
r
wt      pt       pt     z     xt     x       wt   z rt  
r
r
  wt   pt      z rt z     xt     x      wt  

    

  wt   k     rt     xt     xt      wt  
    rls td   forgetting factor   
derivation rls td   forgetting factor    similar exponentially weighted
rls algorithm haykins        pp           present results 

pt     

 



r
r
k      pt z          x     x       pt z  

    

wt      wt   k     rt     x     xt      wt  

    

r
r
  pt pt z         xt     xt       pt z         xt     xt       pt  

   

    

fiefficient reinforcement learning using rls methods

appendix b  proof theorem  
study steady property markov chain defined section    construct stationary
process follows  let  xt  markov chain evolves according transition matrix p
already steady state  means pr xt i    i  t  given sample path
markov chain  define

r
zt  



      x  

    

 

r

x    xt   xt      z   stationary process  discussed  tsitsiklis
roy        
let denote nn diagonal matrix diagonal entries             n   n
cardinality state space x  lemma   derived follows 
lemma     tsitsiklis roy        assumption      following equations hold 
   e       xt     xt        dp   m  

r
   e     z   xt     

    



    dp  

    

  

r
   e     z rt   xt   xt        



    dp r

    

  

r r n   whose nth component equal e r   xt   xt      xt      
according lemma    e  a xt   e  b xt   well defined finite  furthermore  e  a xt  
negative definite  invertible 
equation      




wrls td         p     a  x        p  w    b  x   
  

  

 
 
 
 
    p     a  x        p  w    b  x   

  

  


    

since

 
a  x  

  

    

 
b  x  

  

    

e     a  x      lim

e    b  x      lim
e  a xt   invertible 
 

lim w rls td       e     a  x   e    b  x      w  



   

    

fixu  he    hu

thus w rls td     converges w  probability   

appendix c  details coding structures cmac networks
following discussion  coding structures cmac networks cart pole balancing
problem acrobot control problem presented 
    cmac coding structures cart pole balancing problem
cmac networks  state variables following boundaries 

           

      deg  s     deg  s 

x             
x       
critic network  c   m    hashing technique specified equations          
employed total memory size    
actor network  c   m    hashing technique specified equations          
employed total memory size     
    cmac coding structures acrobot swing up problem
simulation  angles bounded       angular velocities bounded

                          tiling numbers actor critic equal  
 c     total memory sizes critic actor         respectively  actor
network  tiling partitions range input   equal intervals  m     critic
network  partitions input non uniform  given

                                

                                     

                               

                               

   

fiefficient reinforcement learning using rls methods

references
albus j s         new approach manipulator control  cerebellar model articulation
controller  cmac   journal dynamic systems  measurement  control                 
barto a g   sutton r s     anderson c w          neuronlike adaptive elements solve
difficult learning control problems  ieee transactions system  man  cybernetics    
        
bertsekas d p    tsitsiklis j n          neurodynamic programming  belmont  mass   athena
scientific 
berenji h r    khedkar p          learning tuning fuzzy logic controllers reinforcements  ieee trans on neural networks                
boyan  j         least squares temporal difference learning  bratko  i   dzeroski  s   eds  
machine learning  proceedings sixteenth international conference  icml  
boyan  j         technical update  least squares temporal difference learning  machine learning 
special issue reinforcement learning  appear 
brartke  s j    barto a          linear least squares algorithms temporal difference learning 
machine learning            
dayan p         convergence td   general   machine learning             
dayan p     sejnowski t j          td   converges probability    machine learning     
        
eleftheriou e    falconer d d          tracking properties steady state performance rls
adaptive filter algorithms  ieee transactions acoustics  speech  signal processing     
          
eweda e    macchi  o          convergence rls lms adaptive filters  ieee trans 
circuits systems              
haykin s          adaptive filter theory   rd edition  englewood cliffs  nj  prentice hall 
hubing n e    alexander s t          statistical analysis soft constrained initialization
rls algorithms  proc  ieee international conference acoustics  speech
signal processing 
jaakkola t   jordan m i     singh s p          convergence stochastic iterative dynamic
programming algorithms  neural computation                  
kaelbling l p   littman m l     moore a w          reinforcement learning  survey  journal
artificial intelligence research             
konda v r    tsitsiklis j n          actor critic algorithms  neural information processing
systems        mit press 
   

fixu  he    hu

lin l j          self improving reactive agents based reinforcement learning  planning
teaching  machine learning                  
lin c t    lee c s g          reinforcement structure parameter learning neural networkbased fuzzy logic control system  ieee transactions fuzzy system              
ljung l    soderstron t          theory practice recursive identification  mit press 
ljung l          analysis recursive stochastic algorithm  ieee  transactions automatic
control          
michie d    chambers r a          boxes  experiment adaptive control  machine
intelligence    dale e  michie d   eds   edinburgh  oliver boyd          
minsky m l          theory neural analog reinforcement systems application
brain model problem  ph d  thesis  princeton university 
moustakides g v          study transient phase forgetting factor rls  ieee trans 
signal processing                    
samuel a l          studies machine learning using game checkers  ibm journal
research development             
singh  s p   jaakkola t   littman m l     szepesvari c          convergence results singlestep on policy reinforcement learning algorithms  machine learning              
sutton r    barto a          reinforcement learning  introduction  cambridge ma  mit
press 
sutton r          learning predict method temporal differences  machine learning 
           
tsitsiklis j n          asynchronous stochastic approximation q learning  machine learning 
            
tsitsiklis j n    roy b v          feature based methods large scale dynamic programming 
neural computation                  
tsitsiklis j n    roy b v          analysis temporal difference learning function
approximation  ieee transactions automatic control                 
watkins c j c h    dayan p          q learning  machine learning             
young p          recursive estimation time series analysis  springer verlag 

   


