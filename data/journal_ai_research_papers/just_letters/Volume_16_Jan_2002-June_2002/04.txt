journal of artificial intelligence research                  

submitted       published     

learning geometrically constrained hidden markov models for
robot navigation  bridging the topological geometrical gap
hagit shatkay

hagit shatkay celera com

informatics research group 
celera genomics  rockville  md      

leslie pack kaelbling

artificial intelligence laboratory
massachusetts institute of technology  cambridge  ma      

lpk ai mit edu

you will come to a place where the streets are not marked 
some windows are lighted but mostly they re darked 
a place you could sprain both your elbow and chin 
do you dare to stay out  do you dare to go in    
and if you go in  should you turn left or right   
or right and three quarters  or  maybe  not quite    
simple it s not  i m afraid you will find 
for a mind maker upper to make up his mind 

oh  the places you ll go  dr  seuss 

abstract
hidden markov models  hmms  and partially observable markov decision processes
 pomdps  provide useful tools for modeling dynamical systems  they are particularly
useful for representing the topology of environments such as road networks and oce
buildings  which are typical for robot navigation and planning  the work presented
here describes a formal framework for incorporating readily available odometric information and geometrical constraints into both the models and the algorithm that learns
them  by taking advantage of such information  learning hmms pomdps can be made
to generate better solutions and require fewer iterations  while being robust in the face
of data reduction  experimental results  obtained from both simulated and real robot
data  demonstrate the effectiveness of the approach 

  introduction

this work is concerned with robots that need to perform tasks in structured environments 
a robot moving in the environment suffers from two main limitations  its noisy sensors prevent
it from confidently knowing where it is  while its noisy effectors prevent it from knowing with
certainty where its actions will take it  we concentrate here on structured environments  which
can in turn be characterized by two main properties  such environments consist of vast uneventful and uninteresting areas  and are interspersed with relatively few interesting positions or
situations  consider for instance a robot delivering a bagel in an oce building  the interesting
situations are the doors and the intersections in the building hallways  as well as the various
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fishatkay   kaelbling

positions where the bagel might be with respect to the robot s arm  e g   the robot is holding
the bagel  puts it down  etc   most other aspects of the environment  such as the desk positions
in the oces  are inconsequential for the bagel delivery task 
a natural way to represent the combination of such an environment and the robot s interactions
with it  is as a probabilistic automaton  in which states represent interesting situations  and
edges between states represent the actions leading from one situation to another  probability
distributions over the transitions and over the possible observations the robot may perceive at
each situation model the robot s noisy effectors and sensors  respectively 
such models are formally known as pomdp  partially observable markov decision process  models  and have been proven useful for robot planning and acting under the inherent world uncertainty  simmons   koenig        nourbakhsh  powers    birchfield        cassandra  kaelbling    kurien        
despite much work on using such models  the task of learning them directly and automatically
from the data has not been widely addressed  research concerning this immediate topic to date
consists mostly of the work done by simmons and koenig      b   the assumption underlying
their work was that a human provides a rather accurate topological model of the states and
their connections  and the exact probability distributions are then learned on top of this model 
using a version of the baum welch algorithm  rabiner         another interesting approach to
the acquisition of topological models is that of thrun and bucken      a     b  thrun        
who focused on extracting deterministic topological maps from previously acquired geometricalgrid based maps  where the latter were learned directly from the data  further discussion of
related research on both the geometrical and the topological approaches  in their probabilistic
and deterministic versions  is given in the next section 
the work reported here is the first successful attempt we are aware of to learn purely probabilistictopological models  directly and completely from recorded data  without using previous humanprovided or grid based models  it is based on using weak geometric information  recorded by
the robot  to help learn the topology of the environment  and represent it as a probabilistic
model  therefore  it directly bridges the historically perceived gap between topological and
geometrical information  and addresses the claim presented in thrun s work        that the
main shortcoming of the topological approach is its failure to utilize the inherent geometry of
the learnt environment 
most robots are equipped with wheel encoders that enable an odometer to record the change in
the robot s position as it moves through the environment  this data is typically very noisy and
inaccurate  the oors in the environment are rarely smooth  the wheels of the robot are not
always aligned and neither are the motors  the mechanics is imperfect  resulting in slippage and
drift  all these effects accumulate  and if we were to mark the initial position of the robot  and
try to estimate its current position based on summing a long sequence of odometric recordings 
the resulting estimate will be incorrect  that is  the raw recorded odometric information is
not an effective tool  in and of itself  for determining the absolute location of the robot in the
environment 
while our approach is not aimed at determining absolute locations  the idea underlying it is that
this weak odometric information  despite its noise and inaccuracy  still provides geometrical cues
that can help to distinguish between different states  as well as to identify revisitation of the
same state  hence  such information enhances the ability to learn topological models  however 
   

filearning geometrically constrained hmms

the use of geometrical information requires careful treatment of geometrical constraints and
directional data  we demonstrate how the existing models and algorithms can be extended to
take advantage of the noisy odometric data and the geometrical constraints  the geometrical
information is directly incorporated into the probabilistic topological framework  producing a
significant improvement over the standard baum welch algorithm  without the need for humanprovided model 
the rest of this paper is organized as follows  section   provides a survey of previous work in
the area of learning maps for robot navigation  and briey refers to earlier work on learning
automata  section   presents the formal framework for this work  section   presents the main
aspects of our iterative learning algorithm  while section   describes the strategies for selecting
the initial point from which the iterative process begins  section   presents experimental results
obtained from both simulated and real robot data in traditionally hard to learn environments 
the experiments demonstrate that our algorithm indeed converges to better models with fewer
iterations than the standard baum welch method  and is robust in the face of data reduction 

  approaches to learning maps and models

the work presented here lies in the intersection between the theoretical area of learning computational models in particular  learning automata from data sequences and the applied area of
map acquisition for robot navigation  we concentrate here on surveying the work in the latter
area  pointing out the distinction between our approach and its predecessors  we briey review
some results from automata and computational learning theory  a more comprehensive review
of theoretical results is given by shatkay        

    modeling environments for robot navigation

in the context of maps and models for robot navigation  a distinction is usually made between two
principal kinds of maps  geometric and topological  geometric maps describe the environment
as a collection of objects or occupied positions in space  and the geometric relationships among
them  the topological framework is less concerned with the geometrical positions  and models
the world as a collection of states and their connectivity  that is  which states are reachable from
each of the other states and what actions lead from one state to the next 
we draw an additional distinction  between world centric  maps that provide an  objective 
description of the environment independent of the agent using the map  and robot centric models
which capture the interaction of a particular  subjective  agent with the environment  when
learning a map  the agent needs to take into account its own noisy sensors and actuators and try
to obtain an objectively correct map that other agents could use as well  similarly  other agents
using the map need to compensate for their own limitations in order to assess their position
according to the map  when learning a model that captures interaction  the agent acquiring the
model is the one who is also using it  hence  the noisy sensors and actuators specific to the agent
are reected in the model  a different model is likely to be needed by different agents  most
of the related work described below  especially within the geometrical framework  is centered
around learning objective maps of the world rather than agent specific models  we shall point
out in this survey the work that is concerned with the latter kind of models 
our work focuses on acquiring purely topological models  and is less concerned with learning
geometrical relationships between locations or objects  or objective maps  although geometrical
   we thank sebastian thrun for the terminology 

   

fishatkay   kaelbling

relationships do serve as an aid in our acquisition process  the concept of a state used in this
topological framework is more general than the concept of a geometrical location  since a state
can include information such as the battery level  the arm position etc  such information  which
is of great importance for planning  is non geometrical in nature and therefore cannot be readily
captured in a purely geometrical framework  the following sections provide a survey of work
done both within the geometrical framework and within the topological framework  as well as
combinations of the two approaches 

    geometric maps

geometric maps provide a description of the environment in terms of the objects placed in it
and their positions  for example  grid based maps are an instance of the geometric approach 
in a grid based map  the environment is modeled as a grid  an array   where each position in
the grid can be either vacant or occupied by some object  binary values placed in the array  
this approach can be further refined to reect uncertainty about the world  by having grid cells
contain occupancy probabilities rather than just binary values  a lot of work has been done on
learning such grid based maps for robot navigation through the use of sonar readings and their
interpretation  by moravec and elfes and others  moravec   elfes        moravec        elfes 
      asada        
an underlying assumption when learning such maps is that the robot can tell  or find out 
where it is on the grid when it obtains a sonar reading indicating an object  and therefore can
place the object correctly on the grid  a similar localization assumption  requiring the robot
to identify its geometrical location  underlies other geometric mapping techniques by leonard
et al          smith et al          thrun et al       b  and dissanayake et al          even
when an explicit grid is not part of the model  explicit localization can be hard to satisfy 
leonard et al         and smith et al         address this issue through the use of geometrical
beacons to estimate the location of the robot  in what is known as the kalman filter method  a
gaussian probability distribution is used to model the robot s possible current location  based
on observations collected up to the current point   without allowing the refinement of previous
position estimates based on later observations   research in this area has recently been extended
in two directions  leonard and feder        partition the task of learning one large map into
learning multiple smaller map sections  thus addressing the issue of computational eciency 
dissanayake et al         conduct a theoretical study of the approach and show its convergence
properties  the latter may lead to computational eciency by identifying the cases for which a
steady state solution can be readily obtained  accordingly bounding the number of steps required
by the algorithms to reach a useful solution in these cases 
work by thrun et al       a  uses a similar probabilistic approach for obtaining grid based maps 
this work is refined  thrun et al       b  to first learn the location of significant landmarks in
the environment and then fill in the details of the complete geometrical grid  based on laser range
scans  the latter work extends the approach of smith et al    by using observations obtained
both before and after a location has been visited  in order to derive a probability distribution
over possible locations  to achieve this  the authors use a forward backward procedure similar
to the one used in the baum welch algorithm  rabiner         in order to determine possible
locations from observed data  the approach resembles ours both in the use of the forwardbackward estimation procedure  and in its probabilistic basis  aiming at obtaining a maximum
likelihood map of the environment  it still significantly differs from ours both in its initial
assumptions and in its final results  the data assumed to be provided to the learner includes
   

filearning geometrically constrained hmms

both the motion model and the perceptual model of the robot  these consist of transition and
observation probabilities within the grid  both of these components are learnt by our algorithm 
although not in a grid context but in a coarser grained  topological framework  the end result of
their algorithm is a probabilistic grid based map  while ours is a probabilistic topological model 
as further explained in the next section 
in addition to being concerned only with locations  rather than with the richer notion of state 
a fundamental drawback of geometrical maps is their fine granularity and high accuracy  geometrical maps  particularly grid based ones  tend to give an accurate and detailed picture of the
environment  in cases where it is necessary for a robot to know its exact location in terms of
metric coordinates  metric maps are indeed the best choice  however  many planning tasks do
not require such fine granularity or accurate measurements  and are better facilitated through a
more abstract representation of the world  for example  if a robot needs to deliver a bagel from
oce a to oce b  all it needs to have is a map depicting the relative location of a with respect to
b  the passageways between the two oces  and perhaps a few other landmarks to help it orient
itself if it gets lost  if it has a reasonably well operating low level obstacle avoidance mechanism
to help it bypass ower pots and chairs that it might encounter on its way  such objects do
not need to be part of the environment map  just as a driver traveling between cities needs to
know neither his longitude and latitude coordinates on the globe  nor the location of the specific
houses along the way  the robot does not need to know its exact location within the building
nor the exact location of various items in the environment  in order to get from one point to
another  hence  the effort of obtaining such detailed maps is not usually justified  in addition
the maps can be very large  which makes planning even though planning is polynomial in the
size of the map inecient 

    topological maps and models

an alternative to the detailed geometric maps are the more abstract topological maps  such
maps specify the topology of important landmarks and situations  states   and routes or transitions  arcs  between them  they are concerned less with the physical location of landmarks 
and more with topological relationships between situations  typically  they are less complex and
support much more ecient planning than metric maps  topological maps are built on lowerlevel abstractions that allow the robot to move along arcs  perhaps by wall  or road following  
to recognize properties of locations  and to distinguish significant locations as states  they are
exible in allowing a more general notion of state  possibly including information about the
non geometrical aspects of the robot s situation 
there are two typical strategies for deriving topological maps  one is to learn the topological
map directly  the other is to first learn a geometric map  then to derive a topological model
from it through some process of analysis 
a nice example of the second approach is provided by thrun and bucken      a      b  thrun 
       who use occupancy grid techniques to build the initial map  this strategy is appropriate
when the primary cues for decomposition and abstraction of the map are geometric  however 
in many cases  the nodes of a topological map are defined in terms of other sensory data  e g  
labels on a door or whether or not the robot is holding a bagel   learning a geometric map first
also relies on the odometric abilities of a robot  if they are weak and the space is large  it is very
dicult to derive a consistent map 

   

fishatkay   kaelbling

in contrast  our work concentrates on learning a topological model directly  assuming that abstraction of the robot s perception and action abilities has already been done  such abstractions
were manually encoded into the lower level of our robot navigational software  as described in
section    work by pierce and kuipers        discusses an automatic method for extracting
abstract states and features from raw perceptual information 
kuipers and byun        provide a strategy for learning deterministic topological maps  it works
well in domains in which most of the noise in the robot s perception and action is abstracted
away  learning from single visits to nodes and traversals of arcs  a strong underlying assumption
for these strategies  when building the map  is that the current state can be reliably identified
based on local information  or based on distance traversed from the previous well identified
state  these methods are unable to handle situations in which long sequences of actions and
observations are necessary to disambiguate the robot s state 
mataric        provides an alternative approach for learning deterministic topological maps 
represented as distributed graphs  the learning process again relies on the assumption that the
current state can be distinguished from all other states based on local information which includes
compass and sonar readings  uncertainty is not modeled through probability distributions 
instead  matching of current readings to already existing states is not required to be exact  and
thresholds of tolerated error are set empirically  another difference from the work presented
here  is that while we learn the complete probabilistic topology of the environment  in mataric s
work the overall topology of the graph is assumed in advance to be a linear list  and additional
edges are added during the learning process  no probability distribution is associated with the
edges  and a mechanism for choosing which edge to take is determined as part of the goal seeking
process  and is not part of the model itself 
engelson and mcdermott        learn  diktiometric  maps  topological maps with metric relations between nodes  from experience  the uncertainty model they use is interval based rather
than probabilistic  and the learned representation is deterministic  ad hoc routines handle problems resulting from failures of the uncertainty representation 
we prefer to learn a combined model of the world and the robot s interaction with the world 
this allows robust planning that takes into account likelihood of error in sensing and action  the
work most closely related to ours is by koenig and simmons      b      a   who learn pomdp
models  stochastic topological models  of a robot hallway environment  they also recognize
the diculty of learning a good model without initial information  they solve the problem by
using a human provided topological map  together with further constraints on the structure
of the model  a modified version of the baum welch algorithm learns the parameters of the
model  they also developed an incremental version of baum welch that can be used on line 
their models contain very weak metric information  representing hallways as chains of one meter
segments and allowing the learning algorithm to select the most probable chain length  this
method is effective  but results in large models with size proportional to the hallways  length 
and strongly depends on the quality of the human provided initial model 

    learning automata from data

informally speaking  an automaton consists of a set of states and a set of transitions that lead
from one state to another  in the context of this work  the automaton states correspond to the
states of the modeled environments  and the transitions  to the state changes due to actions
performed in the environment  each transition of the automaton is tagged by a symbol from an
   

filearning geometrically constrained hmms

input alphabet    corresponding to the action or the input to the system that caused the state
transition  classical automata theory  e g   hopcroft   ullman        distinguishes between
deterministic and non deterministic automata  if  for each alphabet symbol ff  there is a single
edge tagged by it  going out of each state  the automaton is deterministic  otherwise  the
transition between states is not uniquely determined by the input symbol and the automaton is
non deterministic  if we augment each transition edge of a non deterministic automaton with a
probability of taking it given a certain input  ff  the resulting automaton is called probabilistic 
the basic problem of learning finite deterministic automata from given data can be roughly
described as follows  given a set of positive and a set of negative example strings  s and t
respectively  over alphabet   and a fixed number of states k  construct a minimal deterministic
finite automaton with no more than k states that accepts s and does not accept t   this problem
has been shown to be np complete  gold         despite the hardness  positive results have
been shown possible under various special settings  angluin        showed that if an oracle can
answer membership queries and provide counterexamples to conjectures about the automaton 
there is a polynomial time learning algorithm from positive and negative examples  rivest
and schapire               provide several effective methods  that under various settings  learn
deterministic automata that are correct with high probability  while the above work deals with
learning from noise free data  basye  dean and kaelbling        presented several algorithms
that  with high probability  learn input output deterministic automata  when the data observed
by the learner is corrupted by various forms of noise 
in all these cases  the learned automaton is deterministic rather than probabilistic  the basic
learning problem in the probabilistic context is to find an automaton that assigns the same
distribution as the true one to data sequences  using training data s   that was generated by
the true automaton  another form of a learning problem is that of finding a probabilistic
automaton  that assigns the maximum likelihood to the training data s   that is  an automaton
that maximizes pr s j  
abe and warmuth        show that finding a probabilistic automaton with   states  even when
a small error with respect to the true model is allowed with some probability  the probably
approximately correct  or pac  learning model   cannot be done in polynomial time with polynomial number of examples  unless np   rp  from their work arises the broadly accepted
conjecture  which has not yet been proven  that learning hidden markov models is hard even
in the pac sense  there are two ways to address this hardness  one is to restrict the class of
probabilistic models learned  while the other is to learn unrestricted hidden markov models with
good practical results but with no pac guarantees on the quality of the result 
work by ron et al                     pursues the first approach  learning restricted classes of
automata  namely  acyclic probabilistic finite automata  and probabilistic finite sux automata 
both classes are useful for various applications related to natural language processing  and can
be learned in polynomial time within the pac framework 
the second approach  which is the one predominantly taken in this work  is to learn a model that
is a member of the complete unrestricted class of hidden markov models  only weak guarantees
exist about the goodness of the model  but the learning procedure may be directed to obtain
practically good results  this approach is based on guessing an automaton  model   and using
an iterative procedure to make the automaton fit better to the training data  one algorithm
commonly used for this purpose is the baum welch algorithm  baum  petrie  soules    weiss 
       which is presented in detail by rabiner         the iterative updates of the model are
   

fishatkay   kaelbling

based on gathering sucient statistics from the data given the current automaton  and the
update procedure is guaranteed to converge to a model that locally maximizes the likelihood
function pr datajmodel   since the maximum is local  the model might not be close enough
to the true automaton by which the data was generated  and a challenging problem is to find
ways to force the algorithm into converging to higher likelihood maxima  or at least to make
it converge faster  facilitating multiple guesses of initial models  thus raising the probability
of converging to higher likelihood maxima  such an approach is the one taken in the work
presented here 
we assume  throughout this paper  that the number of states in the model we are learning is
known  this is not a very strong assumption since there are methods for learning the number of
states  regularization methods for deciding on the number of states and other model parameters 
are discussed  for instance  in vapnik s book         we do not address this issue here 
the rest of the work describes our approach to learning topological models  we use noisy
odometric information that is readily available in most robots  this geometrical information is
typically not used by topological mapping methods  we demonstrate how a topological model
and the algorithm used to learn it can be extended to directly incorporate this weak odometric
information  we further show that by doing so  we can avoid the use of human provided a priori
models and still learn stochastic environment models eciently and effectively 

  models and assumptions
this section describes the formal framework for our work  it starts by introducing the classic
hidden markov model  the model is then extended to accommodate noisy odometric information
in its most nave form  ignoring information about the robot s heading and orientation  and later
adapted to accommodate heading information 
we concentrate here on describing models and algorithms for learning hmms  rather than
pomdps  this means that the robot has no decisions to make regarding its next action at
every state  only one action can be executed at each state  in our experiments  a human operator gave the action command associated with each state to the robot when gathering the data 
note that the action is not necessarily the same one for every state  e g   the robot is told to
always turn right in state   and move forward at state    however  at each state only one action can be taken  the extension to complete pomdps  which we have implemented  is through
learning an hmm for each of the possible actions  it is straightforward although notationally
more cumbersome  thus we limit the discussion here to hmms 

    hmms   the basics
a hidden markov model consists of states  transitions  observations and probabilistic behavior 
and is formally defined as a tuple    hs  o  a  b  i  satisfying the following conditions 

 s   fs            sn    g is a finite set of n states 
 o   fo            om   g is a finite set of m possible observation values 
   

filearning geometrically constrained hmms

 a is a stochastic transition matrix  with ai j   pr qt     sj jqt   si   where    i  j  n    
nx
  
qt is the state at time t  for every state si  

j   

ai j     

ai j holds the transition probability from state si to state sj  
 b is a stochastic observation matrix  with bj k   pr vt   ok jqt   sj    where    j  n     
mx
  
   k  m      vt is the observation recorded at time t  for every state sj  
bj k     
bj k holds the probability of observing ok while being at state sj  

k  

  is a stochastic initial distribution vector  with i   pr q    si      i  n     

nx
  
i  

i     

i holds the probability of being in state si at time    when starting to record observations 
this model corresponds to a world whose actual state at any given time t  qt   s   is hidden
and not directly observable  but some observable aspects of the state  vt   o  are detected and
recorded when the state is visited at time t  an agent moves from one hidden state to the
next according to the probability distribution encoded in matrix a  the observed information
in each state is governed by the probability matrix b   although our work is concerned with
discrete observations  the extension to continuous observations is straightforward and has been
well addressed in work on hidden markov models  liporace        juang        
simply stated  the problem of learning an hmm is that of  reverse engineering  a hidden markov
model for a stochastic system from the sampled data  generated by the system  we formalize
the learning task in section      the next section extends hmms to account for geometric
information 

    adding odometry to hidden markov models

the world is composed of a finite set of states  there is a fundamental distinction in our
framework between the term state and the term location  the state of the robot does not
directly correspond to its location  a state may include other information  such as the robot s
battery level or its orientation in that location  a robot standing in the entrance to oce    
facing right is in a different state than a robot standing in the same place facing left  similarly 
a robot standing with a bagel in its arm is in a different state from the same robot being in the
same position without the bagel 
the dynamics of the world are described by state transition distributions that specify the probability of making transitions from one state to the next as a result of a certain action  there
is a finite set of observations that can be perceived in each state  the relative frequency of each
observation is described by a probability distribution and depends only on the current state 
in our model  observations are multi dimensional  an observation is a vector of values  each
chosen from a finite domain  that is  we factorize the observation associated with each state
into several components  for instance  as demonstrated in section      we view the observation
recorded by the robot when standing in an oce environment as consisting of three components 
corresponding to the three cardinal directions  front  left and right  in this example  the observation vector is thus   dimensional  it is assumed that the vector s components are conditionally
independent  given the state 
   

fishatkay   kaelbling

in addition to the above components  each state is assumed to be associated with a position in a
metric space  whenever a state transition is made  the robot records an odometry vector  which
estimates the position of the current state relative to the previous one  for the time being we assume that the odometry vector consists of readings along the x and y coordinates of a global coordinate system  and that these readings are corrupted with independent normal noise  the latter
independence assumption is not a strict one  and can be relaxed by introducing a complete covariance matrix  although we have not done this in this work  in section     we extend the odometry vector to include information about the heading of the robot  and drop the global coordinate
framework 
note that the odometric relationship characterizes a transition rather than a state and  as
described below  receives a different treatment than the observations that are associated with
states 
there are two important assumptions underlying our treatment of odometric relations between
states  first  that there is an inherent  true  odometric relation between the position of every
two states in the world  second  that when the robot moves from one state to the next  there
is a normal    mean noise around the correct expected odometric reading along each odometric
dimension  this noise reects two kinds of odometric error sources 

  the lack of precision in the discretization of the real world into states  e g  there is a rather

large area in which the robot can stand which can be regarded as  the doorway of the ai
lab   
  the lack of precision of the odometric measures recorded by the robot  due to slippage 
friction  disalignment of the wheels  imprecision of the measuring instruments  etc 

to formally introduce odometric information into the hidden markov model framework  we
define an augmented hidden markov model as a tuple    hs  o  a  b  r  i  where 

 s   fs            sn    g is a finite set of n states 
 o   qli   oi is a finite set of observation vectors of length l  the ith element of an

observation vector is chosen from the finite set oi  
 a is a stochastic transition matrix  with ai j   pr qt     sj jqt   si      i  j  n     
nx
  
qt is the state at time t  for every state si   ai j     
j   

ai j holds the transition probability from state si to state sj  
 b is an array of l stochastic observation matrices  with bi j k   pr vt  i    ok jqt   sj   
   i  l     j  n      ok   oi   vt is the observation vector at time t  vt  i  is its ith

component 
bi j k holds the probability of observing ok along the ith component of the observation
vector  while being at state sj  
 r is a relation matrix  specifying for each pair of states  si and sj   the mean and variance
of the d dimensional  odometric relation between them   ri j  m   is the mean of the mth

   for the time being we consider d to be    corresponding to  x  y  readings 

   

filearning geometrically constrained hmms

component of the relation between si and sj and    ri j  m    the variance  furthermore 
r is geometrically consistent  for each component m  the relation m  a  b     ra b  m  
must be a directed metric  satisfying the following properties for all states a  b  and c 
def

 m a  a      
 m a  b     m b  a   anti symmetry   and
 m a  c    m  a  b    m b  c   additivity    
this representation of odometric relations reects the two assumptions  previously stated 
regarding the nature of the odometric information  the  true  odometric relation between
the position of every two states is represented as the mean  the noise around the correct
expected odometric relation  accounting for both the lack of precision in the real world
discretization and the inaccuracy in measurement  is represented through the variance 

  is a stochastic initial probability vector describing the distribution of the initial state 
for simplicity it is assumed here to be of the form h                             i  implying that there
is one designated initial state  si   in which the robot is always started 

this model extends the standard hidden markov model described in section     in two ways 
 it facilitates observations that are factored into components  and represented as vectors 
these components are assumed to be conditionally independent of each other given the
state  such factorization  together with the conditional independence assumption  allows
for a simple calculation of the probability of the complete observation vector from the
probabilities of its components  it therefore results in fewer probabilistic parameters in
the learnt model than if we were to view each observation vector  consisting of a possible
combination of component values as a single  atomic  observation 

 it introduces the odometric relation matrix r and constraints over its components  using
r and the constraints over it  as explained in section    has proven useful for learning the
other model parameters  as demonstrated in section   

    handling directional data

we further extend the model to accommodate directional changes in addition to the positional
changes  there are two issues stemming from directional changes while moving in an environment  the need for non traditional distributions to model directional changes  and the need
to correct for the cumulative rotational error which severely interferes with location estimation
within a global coordinate framework  a detailed discussion of these two problems and their
solution is given in an earlier paper by the authors  shatkay   kaelbling         for the sake
of completeness  we briey review these two issues here 
      circular distributions

the robot s change in direction as it moves through the environment is expressed in terms of the
angular change with respect to its original heading  since angular measures are inherently circular  treating them as  normally distributed   and using the standard procedures for obtaining
sucient statistics from the data is not adequate  as a trivial example  if we were to average
   

fishatkay   kaelbling
y

 

 x    y  
 x    y  
 x    y  
 

     
   

 

  

 

 

 
 

x

  

figure    simple average of two angles  depicted

as vectors to the unit circle  the average angle is
formed by the dashed vector 

figure    directional data represented as angles
and as vectors on the unit circle 

the two angular readings      and        using simple average we obtain the angle      which
is far from the intuitive        as illustrated in figure   
to address the circularity issue  we use the von mises distribution  which is a circular version of
the normal distribution  to model the change in heading between two states  as explained below 
a collection of changes in heading within a two dimensional space can be represented in terms
of either cartesian or polar coordinates  using a cartesian system  n changes in headings can
be recorded as a sequence of   dimensional vectors   hx    y  i        hxn   yn i   on the unit circle 
as shown in figure    the same changes can also be represented as the corresponding angles
between the radii from the center of the unit circle and the x axis               n    respectively 
the relationship between the two representations is 
xi   cos i    yi   sin i         i  n   
the vector mean of the n points  hx  yi  is calculated as 
pn cos   
pn sin   
i
i  
i
  
i  
x 
y
 
 
n
n

   

using polar coordinates  we can express the mean vector in terms of angle    and length  a 
where  except for the case x   y      

   arctan  xy   

a    x    y      
 
 

the angle  is the mean angle  while the length a is a measure  between   and    of how
concentrated the sample angles are around   the closer a is to    the more concentrated the
sample is around the mean  which corresponds to a smaller sample variance 
intuitively  a satisfactory circular version of the normal distribution would have a mean for
which the maximum likelihood estimate is the average angle as calculated above  in a way
analogous to gauss  derivation of the normal distribution  von mises developed such a circular
version  gumbel  greenwood    durand        mardia         which is defined as follows 
definition  a circular random variable            is said to have the von mises
distribution with parameters  and   where        and       if its probability density
   

filearning geometrically constrained hmms

function is 

f       i     e cos     
 

where i     is the modified bessel function of the first kind and order   

i      

     
x
 r
         
r
 
r  

   

the parameters  and  correspond to the distribution s mean and concentration respectively 
while other circular normal distributions do exist  the von mises has the desirable estimation
procedure alluded to earlier  given a set of heading samples  angles           n   from a von mises
distribution  the maximum likelihood estimate  for  is 

   arctan  xy    

where y  x are as defined in equation   
the maximum likelihood estimate for the concentration parameter    is the  that satisfies 
n
i       max    x
i    
n i   cos i          

where i  is the modified bessel function of the first kind and order   

i      

 
x

        r    
r   r  r        

   

further information about the estimation procedure is beyond the scope of this paper and can
be found elsewhere  gumbel et al         mardia        
to conclude  we assume that the change in heading  is von mises distributed  around a mean
 with concentration parameter   this assumption is reected in the model learning procedures
as explained later in section        the change in heading h  a  b     a  b i between each pair
of states  a  b  completes the set of parameters included in the relation matrix r which was
introduced earlier in section     
      cumulative rotational error

we tend to think about an environment as consisting of landmarks fixed in a global coordinate
system and corridors or transitions connecting these landmarks  this idea underlies the typical
maps constructed and used in everyday life  however  this view of the environment may be
problematic when robots are involved 
conceptually  a robot has two levels at which it operates  the abstract level  in which it centers
itself through corridors  follows walls and avoids obstacles  and the physical level in which motors
turn the wheels as the robot moves  in the physical level many inaccuracies can manifest
themselves  wheels can be unaligned with each other resulting in a drift to the right or to the
left  one motor can be slightly faster than another resulting in similar drifts  an obstacle under
one of the wheels can cause the robot to rotate around itself slightly  or uneven oors may cause
   

fishatkay   kaelbling

 

  actual position
  recorded position

figure    a robot moving along the solid arrow  while correcting for drift in the direction of the dashed
arrow  the dotted arrow marks its recorded change in position 

the robot to slip in a certain direction  in addition  the measuring instrumentation for odometric
information may not be accurate in and of itself  at the abstract level  corrective actions are
constantly executed to overcome the physical drift and drag  for example  if the left wheel is
misaligned and drags the robot leftwards  a corrective action of moving to the right is constantly
taken in the higher level to keep the robot centered in the corridor 
the phenomena described above have a significant effect on the odometry recorded by the robot 
if such data interpreted with respect to one global framework  for example  consider the robot
depicted in figure    it drifts to the left   when moving from one state to the next  and
corrects for it by moving  to the right in order to maintain itself centered in the corridor 
let us assume that states are   meters apart along the center of the corridor  and that the center
of the corridor is aligned with the y axis of the global coordinate system  the robot steps back
and forth in the corridor from one state to the next  whenever the robot reaches a state  its
odometry reading changes by hx  y  i along the hx  y  headingi dimensions  respectively  as the
robot proceeds  the deviation with respect to the x axis becomes more and more severe  thus 
after going through several transitions  the odometric changes recorded between every pair of
states  if taken with respect to a global coordinate system  become larger and larger  similar
problems of inconsistent odometric changes recorded between pairs of states can arise along any
of the odometric dimensions  it is especially severe when such inconsistencies arise with respect
to the heading  since this can lead to mistakenly switching movement along the x and the y
axes  as well as confusion between forwards and backwards movement  when the deviation in
the heading is around    or     respectively  
in early work  shatkay   kaelbling        we assumed perpendicularity of the corridors  which
was taken advantage of while the robot collected the data  odometric readings were recorded
with respect to a global coordinate system  and the robot could re align itself with the origin after
each turn  a trajectory of odometry recorded under this perpendicularity assumption by our
robot ramona  along the x and y axes is given in figure    the sequence shown was recorded
while the robot drove repeatedly around a loop of corridors  further details about the data
gathering process are provided in section    in contrast  figure   shows a trajectory of another
sequence of odometric readings recorded by ramona  driving through the same corridors  without
using the perpendicularity assumption  the data collected under the latter setting is subjected
to cumulative rotational error 
   

filearning geometrically constrained hmms
    
    

    
    

    
   

    

   

    

   

   

   
   

   

   

   

    

                            

figure    sequence gathered by ramona  perpendicularity assumed 

   

    

figure    sequence gathered by ramona  no per 

pendicularity assumed 

such data can be handled through state relative coordinate systems  shatkay   kaelbling        
the latter implies that each state si has its own coordinate system  as shown in figure    the
origin is anchored in si   the y axis is aligned with the robot s heading in the state  denoted by
bold arrows in the figure   and the x axis is perpendicular to it  this is in contrast to a global
coordinate system which is anchored in the initial starting state  within the global coordinate
system  the relations recorded may vary greatly among multiple instances of the same transition
between the same pair of states  by using the state relative system  the recorded and learned
relationship between each pair of states  hsi   sj i  is reliable  despite the fact that it is based on
multiple transitions recorded from si to sj  
under state relative coordinate systems  the geometric relation stored in rij    which was introduced in section       is expressed for each pair of states  si and sj   with respect to the
coordinate system associated with state si  accordingly  the constraints imposed over the x and
y components of the relation matrix must be specified with respect to the explicit coordinate
system used  as explained below 
given a pair of states a and b  we denote by hx yi  a  b  the vector h ra b  x     ra b  y  i  let
us define tab to be the transformation that maps an hxa   ya i point represented with respect to
the coordinate system of state a  to the same point represented with respect to the coordinate
system of state b  hxb   yb i 
more explicitly  let ab be the mean change in heading from state a to state b  applying tab to
a vector h xyaa i results in the vector h xybb i as follows 

   

     

xb
x
x cos ab     ya sin ab  
  tab a   a
yb
ya
xa sin ab     ya cos ab  

 

 

the consistency constraints within this framework must be restated as 

 hx yi a  a    h    i 
 hx yi a  b     tba hx yi b  a    anti symmetry  
 hx yi a  c    hx yi  a  b    tba hx yi  b  c    additivity  
   

fishatkay   kaelbling
y

x
sj
si



y

x

figure    a robot in state si   faces in the y  axis direction  the relation si  sj is wrt si  s coordinate
system 

these consistency constraints are the ones that need to be enforced by our learning algorithm
which constructs the hmm  it is important to note that the transformation t itself does not
constitute a set of additional parameters that need to be learnt  rather  it is calculated in terms
of the heading change parameter     which is already an integral part of the relation matrix we
have defined in sections     and       
we have introduced the basic formal model that we use for representing environments and
the robot s interaction with them  in the following section we state the learning problem and
describe the basic algorithm for learning the model from data 

  learning hmms with odometric information

this section formalizes the learning problem for hmms  and discusses how odometric information
is incorporated into the learning algorithm  an overview of the complete algorithm is provided
in the appendix for this paper 

    the learning problem

the learning problem for hidden markov models can be generally stated as follows  given an
experience sequence e  find a hidden markov model that could have generated this sequence and
is  useful  or  close to the original  according to some criterion  an explicit common statistical
approach is to look for a model  that maximizes the likelihood of the data sequence e given
the model  formally stated  it maximizes pr ej   however  given the complicated landscape
of typical likelihood functions in a multi parameter domain  obtaining a maximum likelihood
model is not feasible  all studied practical methods  and in particular the well known baumwelch algorithm  rabiner        and references therein  can only guarantee a local maximum
likelihood model 
another way of evaluating the quality of a learned model is by comparing it to the true model 
we note that stochastic models  such as hmms  induce a probability distribution over all observation sequences of a given length  the kullback leibler  kullback   leibler        divergence
of a learned distribution from a true one is a commonly used measure for estimating how good a
   

filearning geometrically constrained hmms

learned model is  obtaining a model that minimizes this measure is a possible learning goal  the
culprit here is that in practice  when we learn a model from data  we do not have any  ground
truth  model to compare the learned model with  still  we can evaluate learning algorithms by
measuring how well they perform on data obtained from known models  it is reasonable to expect that an algorithm that learns well from data that is generated from a model we do have  will
perform well on data generated from an unknown model  assuming that the models indeed form
a suitable representation of the true generating process  we discuss the kullback leibler  kl 
divergence in more detail in section     in the context of evaluating our experimental results 
to summarize  the learning problem as we address it in this work is that of obtaining a model
by attempting to  locally  maximize the likelihood  while evaluating the results based on the
kl divergence with respect to the true underlying distribution  when such a distribution is
available 

    the learning algorithm

the learning algorithm starts from an initial model   and is given an experience sequence e 
it returns a revised model   which  locally  maximizes the likelihood p  ej   the experience
sequence e is of length t   each element  et   for    t   t       is a pair hrt   vt i  where rt is the
observed relation vector along the x  y and  dimensions  between the states qt   and qt   and vt
is the observation vector at time t 
our algorithm extends the standard baum welch algorithm to deal with the relational information and the factored observation sets  the baum welch algorithm is an expectationmaximization  em  algorithm  dempster  laird    rubin         it alternates between
 the e step of computing the state occupation and state transition probabilities   and  
at each time in the sequence given e and the current model   and
 the m step of finding a new model    that maximizes p  ej      
providing monotone convergence of the likelihood function p  ej  to a local maximum 
however  our extension introduces an additional component  namely  the relation matrix r  it
can be viewed as having two kinds of observations  state observations  as the ordinary hmm  
with the distinction that we observe integer vectors rather than integers  and transition observations  the odometry relations between states   the latter must satisfy geometrical constraints 
hence  an extension of the standard update formulae  as described below  is required 
      state occupation probabilities

following rabiner         we first compute the forward  ff  and backward  fi   matrices  fft  i 
denotes the probability density value of observing e  through et and qt   si   given   fit  i  is
the probability density of observing et   through et    given qt   si and   formally 
fft  i    pr e            et   qt   sij   
fit  i    pr et             et    jqt   si      
when some of the measurements are continuous  as is the case with r   these matrices contain
probability density values rather than probabilities 
the forward procedure for calculating the ff matrix is initialized with
  i
b if i    
ff   i       otherwise
 
   

fishatkay   kaelbling

and continued for     t  t     with
fft  j    

nx
  
i  

fft    i ai j f  rt jri j  bjt  

   

the expression f  rt jri j   denotes the density at point rt according to the distribution represented
by the means and variances in entry i  j of q
the relation matrix r  while bjt is the probability of
j
observing vector vt in state sj   that is  bt   li   bi j vt i   
the backward procedure for calculating the fi matrix is initialized with fit     j      and continued
for    t t     with
nx
  
fit  i    fit    j  ai j f  rt   jri j  bjt    
   
j   

given ff and fi   we now compute for each given time point t the state occupation and statetransition probabilities   and    the state occupation probabilities  t  i   representing the
probability of being in state si at time t given the experience sequence and the current model 
are computed as follows 
 
   
t  i    pr qt   si je      pnff t  i fit  i 
j    fft  j  fit  j  
similarly  t  i  j    the state transition probabilities from state i to state j at time t given the
experience sequence and the current model  are computed as 
t  i  j     pr qt   si   qt     sj je   
fft  i ai j bjt   f  rt   jri j  fit    j  
 
   
 
nx
   nx
  
i   j   

fft  i ai j bjt   f  rt   jri j  fit    j  

these are essentially the same formulae appearing in rabiner s tutorial  rabiner         but
they also take into account the density of the odometric relations 
in the next phase of the algorithm  the goal is to find a new model    that maximizes the likelihood conditioned on the current transition and observation probabilities  pr ej        usually 
this is simply done using maximum likelihood estimation of the probability distributions in a
and b by computing expected transition and observation frequencies  in our model we must also
compute a new relation matrix  r  under the constraint that it remain geometrically consistent 
through the rest of this section we use the notation v to denote a reestimated value  where v
denotes the current value 
      updating transition and observation parameters

the a and b matrices can be straightforwardly reestimated  ai j is the expected number of
transitions from si to sj divided by the expected number of transitions from si   and b i j k is the
expected number of times ok is observed along the ith dimension when in state sj   divided by
the expected number of times of being in sj  
pt    
pt      i  j  
t
t
  
  b i j k   t  pt v t  i  ok   t  j    
   
ai j   pt   
t   t  i 
t   t  i 
the expression c denotes an indicator function with value   if condition c is true and   otherwise 
   

filearning geometrically constrained hmms
   
p

q

 

p

   

  

  

  

  

 

 

 

 

    
  

  

  

  

 

 

 
    

q

figure    examples of two sets of normally distributed points with constrained means  in   and in  
dimensions 

      updating relation parameters

when reestimating the relation matrix  r  the geometrical constraints induce interdependencies
among the optimal mean estimates as well as between optimal variance estimates and mean
estimates  parameter estimation under this form of constraints is almost untreated in mainstream statistics  bartels        and we found no previous existing solutions to the estimation
problem addressed here  as an illustration for the issues involved in estimation under constraints
consider the following estimation problem of   normal means 
example     the data consists of two sample sets of points p   fp   p            pn g and q  
fq   q            qk g  independently drawn from two distinct normal distributions with means p   q
and variances p    q    respectively  we are asked to find maximum likelihood estimates for the
two distribution parameters  moreover  we are told that the means of the two distributions are
related  such that q    p   as illustrated in figure    if not for the latter constraint  the task
is simple  degroot         and we have 
pn p
pn
i        i    pi   p     
p   i  
p
n
n

and similarly for q and q    however  the constraint p    q requires finding a single mean   
and setting the other one to its negated value     intuitively  when choosing such a maximum
likelihood single mean  the more concentrated sample should have more effect  while the more
varied sample should be more  submissive   thus  the overall sample deviation from the means
would be minimized and the likelihood of the data maximized  therefore  there is a mutual
dependence between the estimation of the mean and the estimation of the variance 
since the samples are independently drawn  their joint likelihood function is 
  pi  p   

n
p
y
f  p  qjp   q  p    q      e p
i    p
   



yk e
j   

  qj  q   
q

p

   

 q

 

by taking the derivatives of this joint log likelihood function  with respect to p   p and q  and
equating them to    while using the constraint q    p   we obtain the following set of mutual
equations for maximum likelihood estimators 
p
p
 q  ni   pi     p  kj   qj  
p  
  q    p  
nq    kp 
pk  q      
pn  p      
i
p
i
  
 
 
p  
  q   j    j p  

n

k

   

fishatkay   kaelbling

by substituting the expressions for p and q into the expression for p   we obtain a cubic equation which is cumbersome  but still solvable  in this simple case   the solution provides a maximum likelihood estimate for the mean and variance under the constraint q    p  
 
we now proceed to the actual update of the relation matrix under constraints  for clarity  we
initially discuss only the first two geometrical constraints  and discuss the additivity constraint in
section      recall that we concentrate here on the enforcement of global constraints  appropriate
under the perpendicularity assumption  although the same idea is applied in the case of staterelative constraints 
zero distances between states and themselves are trivially enforced  by setting all the diagonal
entries in the r matrix to    with a small variance 
anti symmetry within a global coordinate system is enforced by using the data recorded along
the transition from state sj to si as well as from state si to sj when reestimating  ri j    as
demonstrated in example      the variance has to be taken into account  leading to the following
set of mutual equations 



mi j

 

  mi j     

pt   

rt m t  i j     rt  m t j i 
  m
  m
i j   
j i   

pt    t mi j    t mj i  
t    i j     j i   
pt      i  j   r  m    m     
t   t
pt    t i  j   i j  
t   t
t  

 

   
    

for the x and y dimensions   m   x  y   this amounts to a complicated but still solvable cubic
equation  however  in the more general case  when accounting for the orientation of the robot 
and also when complete additivity is enforced  we do not obtain such closed form reestimation
formulae 
to avoid these hardships  we use a lag behind update rule  the yet unupdated estimate of the
variance is used for calculating a new estimate for the mean  and this new mean estimate is
used to update the variance  using equation      thus  the mean is updated using a variance
parameter that lags behind it in the update process  and the reestimation equation     needs to
use m rather than m as follows  pt    h rt  m t  i j  rt  m t  j i  i
m     j i
m   
t  
i  
    
mi j   pt    hi jt   i j  t   j i
 
t  

m       j i
m   
i j

 

as we have shown  shatkay         this lag behind policy is an instance of generalized em  mclachlan   krishnan         the latter guarantees monotone convergence to a local maximum of the
likelihood function  even when each  maximization  step increases rather than strictly maximizes the expected likelihood of the data given the current model 
similarly  the reestimation formula for the von mises mean    and concentration    parameters
of the heading change between states si and sj is the solution to the equations 

  tx
 
 
bb  sin rt     t  i  j  i j   t j  i j i   cc
t
cc
  arctan b
b  tx
 
a
 cos rt     t  i  j  i j   t  j  i j i   
 

i j

  

 

t  

   a similar approach  termed one step late update  is taken by others applying em to highly non linear optimization problems  mclachlan   krishnan        

   

filearning geometrically constrained hmms

i   i j  
  max
i   i j  

  pt  

 
 
 i  j   cos rt      i j   
t    tp
   
t      i  j  
t   t

 

    

where i  and i  are the modified bessel functions as defined by equations   and   in section       
again  to avoid the need to solve the mutual equations  we take advantage of the lag behind strategy  updating the mean using the current estimates of the concentration parameters  i j   j i 
as follows 
pt    sin r       i  j       j  i      
t
t
i j t
j i
i j   arctan ptt  
    
    cos r       i  j       j  i      
t
t
i j t
j i
t  
and then calculating the new concentration parameters based on the newly updated mean  as
the solution to equation     through the use of lookup tables 
a possible alternative to our lag behind approach is to update the mean as though the assumption j i   i j holds  under this assumption  the variance terms in equation   cancel out  and
the mean update is independent of the variance once again  then the variances are updated as
stated in equation     without assuming any constraints over them  this approach was taken
in earlier stages of this work  shatkay   kaelbling               the lag behind strategy is
superior  both according to our experiments  and due to its being an instance of generalized em 

    enforcing additivity

note that the additivity constraint directly implies the other two geometrical constraints    thus 
enforcing it results in complete geometrical consistency  we present here the method for directly
enforcing additivity through the reestimation procedure along the x and y dimensions  for the
heading dimension we describe how complete geometrical consistency is achieved through the
projection of anti symmetric estimates onto a geometrically consistent space  as before  to
simplify the presentation  we focus on the case of global coordinate systems  the same basic
idea applies to state relative coordinate systems  but the relationship used to recover the mean
ij from individual state coordinates is more complex 
      additivity in the x  y dimensions

the main observation underlying our approach is that the additivity constraint is a result of the
fact that states can be embedded in a geometrical space  that is  assuming we have n states 
s           sn     there are points on the x   y and  axes  x            xn      y            yn                  n    
respectively  such that each state  si   is associated with the coordinates hxi   yi   i i  assuming
one global coordinate system  the mean odometric relation from state si to state sj can be
expressed as  hxj   xi   yj   yi   j   i i 
during the maximization phase of the em iteration  rather than try to maximize with respect
to n   odometric relation vectors  hxij   yij   ij i  we reparameterize the problem  specifically 
we express each odometric relation as a function of two of the n state positions  and maximize
with respect to the unconstrained  n state positions  for instance  for the x dimension  rather
than search for n   maximum likelihood estimates for xij   we use the maximization step to find
n   dimensional points  x            xn      we can then calculate xij   xj   xi   moreover  since
all we are interested in is finding the best relationships between xi and xj   we can fix one of
   f a  a    a  a     a  a g     a  a       f  a  a         a  a    a  b   b  a  g     a  b      b  a   

   

fishatkay   kaelbling

the xi  s at    e g  x        and find optimal estimates for the remaining n     state positions 
the variance reestimation remains as before  and the lag behind policy is used to eliminate the
interdependency between the update of the mean and the variance parameters 
      additive heading estimation

unfortunately  the reparameterization described above is not feasible for estimation of changes
in heading  due to the von mises distribution assumption over the heading measures  by reparameterizing ij as j   i and trying to maximize the likelihood function with respect to the 
parameters  we obtain a set of n   trigonometric equations with terms of the form cos j   sin i  
which do not enable simple solution 
as an alternative  it is possible to use the anti symmetric reestimation procedure described
earlier  followed by a perpendicular projection operator  mapping the resulting headings vector
h             ij           n    n   i     i  j  n     which does not satisfy additivity  onto a vector of
headings within an additive linear vector space  simple orthogonal projection is not satisfactory
within our setting  since it simply looks for the additive vector closest to the non additive one 
this procedure ignores the fact that some of the entries in the non additive vector are based on
a lot of observations  and are therefore more reliable  while other  less reliable ones  are based on
hardly any data at all  intuitively  we would like to keep the estimates that are well accounted
for intact  and adapt the less reliable estimates to meet the additivity constraint  more precisely 
there are heading change estimates between states that are better accounted for than others  in
the sense that the transitions between
these states have higher expected counts than transition
p
between other states  higher t t  i  j     we would like to project the non additive heading
estimates vector onto a subspace of the additive vector space  in which the vectors have the same
values as the non additive
p vector in the entries that are well accounted for  that is  those with
the highest values of t t  i  j    the diculty is that the latter subspace is not a linear vector
space  for instance  it does not satisfy closure under scalar multiplication   and the projection
operator over linear spaces cannot be applied directly  still  this set of vectors does form an
ane vector space  and we can project onto it using an algebraic technique  as explained below  
definition
a rn is an n dimensional ane space if for all vectors va a  the set of vectors 
def
a   va   fua   va jua   ag is a linear space 
hence  we can pick a vector in an ane space  va  a  and define the translation ta   a   v  
where v is a linear space  v   a   va   this translation is trivially extended for any vector
v    rn   by defining ta  v      v    va   in order to project any vector v   rn onto a  we apply
the translation ta to v and project ta  v  onto v   which results in a vector p  ta  v   in v   by
applying the inverse transform ta   to it  we obtain the projection of v on a  as demonstrated
in figure    the linear space in the figure is the two dimensional vector space fhx  yij y    xg 
and the ane space is fhx  yij y    x    g  the transform ta consists of subtracting the vector
h    i  the solid arrow corresponds to the direct projection of the vector v onto the point p  v 
of the ane space  the dotted arrows represent the projection via translation of v to ta  v   the
projection of the latter onto the linear vector space  and the inverse translation of the result 
p  ta  v    onto the ane space 
 

 

 

   many thanks to john hughes for introducing us to this technique 

   

filearning geometrically constrained hmms

 
 x  x   
 
p v 

v

 

  

 

  

 
ta  v 
p ta  v  

 x  x 

  

figure    projecting v onto the ane vector space fhx  yij y    x    g 
although the procedure for preserving additivity over headings is not formally proven to preserve monotone convergence of the likelihood function towards a local maximum  our extensive
experiments consisting of hundreds of runs have shown that monotone convergence is preserved 

  choosing an initial model

typically  in instances of the baum welch algorithm  an initial model is picked uniformly at
random from the space of all possible models  perhaps trying multiple initial models to find different local likelihood maxima  an alternative approach we have reported  shatkay   kaelbling 
      was based on clustering the accumulated odometric information using the simple k means
algorithm  duda   hart         taking the clusters to be the states in which the observations
were recorded  to obtain state and observation counts and estimate the model parameters 
if perpendicularity is assumed when collecting the data  as shown in figure    the k means
algorithm assigns the same cluster  state  to odometric readings recorded at close locations 
leading to reasonable initial models  however  when this assumption is dropped  as illustrated
in figure    the cumulative rotational error distorts the odometric location recorded within a
global coordinate system  so that the location assigned to the same state during multiple visits
varies greatly and would not be recognized as  the same  by a simple location based clustering
algorithm  to overcome this  we developed an alternative initialization heuristics  which we call
tag based initialization  it is based directly on the recorded relations between states  rather than
on states  absolute location  for clarity  the description here consists mostly of an illustrative
example  and concentrates on the case where global consistency constraints are enforced 
given a sequence of observations and odometric readings e  we begin by clustering the odometric
readings into buckets  the number of buckets is at most the number of distinct state transitions
recorded in the sequence  the goal at this stage is to have each bucket contain all the odometric
readings that are close to each other along all three dimensions 
to achieve this  we start by fixing a predetermined  small standard deviation value along the x 
y  and  dimensions  denote these standard deviation values x   y    respectively   typically
x   y    the first odometric reading is assigned to bucket   and the mean of this bucket is
set to be the value of this reading  through the rest of the process the subsequent odometric
readings are examined  if the next reading is within     standard deviations along each of the
three dimensions from the mean of some existing non empty bucket  add it to the bucket and
   

fishatkay   kaelbling

             
               

              
                

              
                

                 
                

  

  

  

  

              

                

                  

                

 

 

 

 

figure    the bucket assignment of the example sequence 
update the bucket mean accordingly  if not  assign it to an empty bucket and set the mean of
the bucket to be this reading 
intuitively  by using this heuristic each of the resulting buckets is tightly concentrated about
its mean  we note that other clustering algorithms  duda   hart        could be used at the
bucketing stage 
example     we would like to learn a   state model from a sequence of odometric readings 
hx  y  i as follows 
h       i  h         i  h         i  h          i 
h         i  h           i  h           i  h          i  
as a first stage we place these readings into buckets  suppose the standard deviation constant is
    the placement is as shown in figure    the mean value associated with each bucket is shown
as well 
 
the next stage of the algorithm is the state tagging phase  in which each odometric reading 
rt   is assigned a pair of states  si  sj   denoting the origin state  from which the transition took
place  and the destination state  to which the transition led   respectively  in conjunction  the
mean entries  ij   of the relation matrix  r  are populated 

example      cont   returning to the sequence above  the process is demonstrated in figure     we assume that the data recording starts at state    and that the odometric change
through self transitions is    with some small standard deviation  we use    here as well   this
is shown on part a of the figure 
since the first element in the sequence  h       i  is more than two standard deviations away
from the mean        and no other entry in the relation row of state   is populated  we pick  
as the next state and populate the mean        to be the same as the mean of bucket    to which
h       i belongs  to maintain geometrical consistency the mean        is set to          as
shown in part b of the figure  we now have populated   off diagonal entries  and the state
sequence is h    i  the entry        in the matrix becomes associated with bucket    and this
information is recorded for helping with tagging future odometric readings belonging to the same
bucket 
the next odometric reading  h         i  is a few standard deviations from any populated mean
in row    where   is the current believed state   hence  we pick a new state    and set the mean
       to be   the mean of bucket   to which the reading belongs  figure    c   the entry
       is recorded as associated with bucket    to preserve anti symmetry and additivity        
is set to                 is set to be the sum                  and        is set to         
   

filearning geometrically constrained hmms
a
 
 

 

b
 

 

 

       

 

 
 

       

       

 

 

 

    
           
     
    
    
      

       

       

 
 

       

 

 

       

s   

s      
bucket r           

c
 
 
 
 

 

d
 

 

 

    
      
     
           
             
      
    
    
             
      
   

 
 

               
      
    
       
           

 

 
 

       

s         

 

 

 

    
               
     
   
           
                  
               
    
    
     
             
      
   
      
               
      
      
    
              
           
     
                          
             
  
    
              
   

s         
bucket r           

bucket r           

s           
bucket r           
      s                          

figure     populating the odometric relation matrix and creating a state tagging sequence 
similarly         is updated to be the mean of bucket    causing the setting of                
                and         bucket   is associated with        
at this stage the odometric table is fully populated  as shown in part d of figure     the state
sequence at this point is  h          i  the next reading  h           i  is within one standard
deviation from        and therefore the next state is    entry        is associated with bucket   
 the bucket to which the reading was assigned   and the state sequence becomes  h             i 
the next reading  being from bucket    is associated with the relation from state   that is tagged
by bucket    namely  state    by repeating this for the last two readings  the final state transition
sequence becomes h                         i 
 
note that the process described in the above illustration was simplified  in the general case 
we need to take into account the rotational error in the data  use state relative coordinate
systems  and therefore populate the entries under the transformed anti symmetry and additivity
constraints 
 hx yi a  b     tba  hx yi b  a    
 hx yi a  c    hx yi a  b    tba  hx yi b  c   
as defined in section       
   

fishatkay   kaelbling

it is possible that by the end of the tagging algorithm  some rows or columns of the relation
matrix are still unpopulated  this happens when there is too little data to learn from or when
the number of states provided to the algorithm is too large with respect to the actual model  in
such cases we can either  trim  the model  using the number of populated rows as the number
of states  or pick random odometric readings to populate the rest of the table  improving these
estimates later  note that the first approach suggests a method for learning the number of states
in the model when this is not given  starting from a gross over estimate of the number  and truncating it to the number of populated rows in the odometric table after initialization is performed 
once the state transition sequence is obtained  the rest of the initialization algorithm is the same
as it is for k means based initialization  deriving state transition counts from the state transition
sequence  assigning the observations to the states under the assumption that the state sequence
is correct  and obtaining state transition and observation probabilities  the initialization phase
does not incur much computational overhead  and is equivalent time wise to performing one
additional iteration of the em procedure 

  experiments and results

the goal of the work described so far is to use odometry to improve the learning of topological
models  while using fewer iterations and less data  we tested our algorithm in a simple robotnavigation world  our experiments consist of running the algorithm both on data obtained
from a simulated model and on data gathered by our mobile robot  ramona  the amount of
data gathered by ramona is used here as a proof of concept but is not sucient for statistical
analysis  for the latter  we use data obtained from the simulated model  we gathered data and
used the algorithms both with and without the perpendicularity assumption  see section        
and results are provided from both settings 

    robot domain

the robot used in our experiments  ramona  is a modified rwi b   robot  it has a cylindrical
synchro drive base     ultrasonic sensors and    infrared sensors  situated evenly around its
circumference  the infrared sensors are used mostly for short range obstacle avoidance  the
ultrasonic sensors are longer ranged  and are used for obtaining  noisy  observations of the
environment  in the experiments described here  the robot follows a prescribed path through
the corridors in the oce environment of our department  thus  there is no decision making
involved  and an hmm is a sucient model  rather than a complete pomdp 
low level software  provides a level of abstraction that allows the robot to move through hallways
from intersection to intersection and to turn ninety degrees to the left or right  the software
uses sonar data to distinguish doors  openings  and intersections along the path  and to stop
the robot s current action whenever such a landmark is detected  each stop either due to the
natural termination of an action or due to a landmark detection is considered by the robot to
be a  state  
at each stop  ultrasonic data interpretation allows the robot to perceive  in each of the three
cardinal directions   front  left and right   whether there is an open space  a door  a wall  or
something unknown 
encoders on the robot s wheels allow it to estimate its pose  position and orientation  with respect to its pose at the previous intersection  after recording both the sonar based observations
   the low level software was written and maintained by james kurien 

   

filearning geometrically constrained hmms
 

 

 

 

 

 
 

 

  
  

  
  

 
 

  

  

   
     
  

  
 
  

  

 
 

 

 

   
  

     
  

  
  

  
  

  
  
  

  
  
  

  

     

     

  

 
  

  

  

  
  

  

  
  

  

figure     true model of the corridors ramona traversed  arrows represent the prescribed path direction 

figure     true model of a prescribed path
through the simulated hallway environment 

and the odometric information  the robot goes on to execute the next prescribed action  the
action command is issued manually by a human operator  of course  both the action performance and the perception routines are subject to error  the path ramona followed consists of
  connected corridors in our building  which include    states  as shown in figure    
in our simulation  we manually generated an hmm representing a prescribed path of the robot
through the complete oce environment of our department  consisting of    states  and the
associated transition  observation  and odometric distributions  the transition probabilities
reect an action failure rate of about          that is  the probability of moving from the
current state to the correct next state in the environment  under the predetermined action is
between      and       the probability of self transition is typically between      and      
some small probability  typically smaller than       is sometimes assigned to other transitions 
our experience with the real robot proves that this is a reasonable transition model  since
typically the robot moves to the next state correctly  and the only error that occurs with some
significant frequency is when it does not move at all  due to sonar interpretation indicating a
barrier when there is actually none  once the action command is repeated the robot usually
performs the action correctly  moving to the expected next state  the observation distribution
typically assigns probabilities of             to the true observation that should be perceived
by the robot at each state  and probabilities of             to other observations that might be
perceived  for example  if a door should actually be perceived  a door is typically assigned a
probability of           a wall is assigned a probability of          and an open space is assigned
a probability of about      to be perceived  the standard deviation around odometric readings
is about    of the mean 
figure    shows the hmm corresponding to the simulated hallway environment  observations
and orientation are omitted from the figure for clarity  nodes correspond to states in the
environment  while directed edges correspond to the corridors  the arrows point at the direction
in which the corridors were traversed  further interpretation of the figures is provided in the
following section 
   

fishatkay   kaelbling

    evaluation method

there are a number of different ways of evaluating the results of a model learning algorithm 
none are completely satisfactory  but they all give some insight into the utility of the results 
in this domain  there are transitions and observations that usually take place  and are therefore
more likely than the others  furthermore  the relational information gives us a rough estimate
of the metric locations of the states  to get a qualitative sense of the plausibility of a learnt
model  we can extract an essential map from the learnt model  consisting of the states  the
most likely transitions and the metric measures associated with them  and ask whether this map
corresponds to the essential map underlying the true world 
figures    and    are such essential versions of the true models  while figures    and     shown
later  are essential versions of representative learnt ones  obtained from sequences gathered
under the perpendicularity assumption   black dots represent the physical locations of states 
and each state is assigned a unique number  multiple state numbers associated with a single
location typically correspond to different orientations of the robot at that location  the larger
black circle represents the initial state  solid arrows represent the most likely non self transitions
between the states  dashed arrows represent the other transitions when their probability is    
or higher  typically  due to the predetermined path we have taken  the connectivity of the
modeled environment is low  and therefore the transitions represented by dashed arrows are
almost as likely as the most likely ones  note that the length of the arrows  within each plot  is
significant and represents the length of the corridors  drawn to scale 
it is important to note that the figures do not provide a complete representation of the models 
first  they lack observation and orientation information  we stress the fact that the figures
serve more as a visual aid than as a plot of the true model  we are looking for a good topological
model rather than a geometrical model  the figures provide a geometrical embedding of the
topological model  however  even when the geometry  as described by the relation matrix  is
different  the topology  as described by the transition and observation matrices  can still be valid 
traditionally  in simulation experiments  the learnt model is quantitatively compared to the
actual model that generated the data  each of the models induces a probability distribution
on strings of observations  the asymmetric kullback leibler divergence  kullback   leibler 
      between the two distributions is a measure of how good the learnt model is with respect
to the true model  given a true probability distribution p   fp         pn g and a learnt one
q   fq        qn g  the kl divergence of q with respect to p is 

d p jjq   

def

n
x
i  

pi log  pqi  
i

we report our results in terms of a sampled version of the kl divergence  as described by juang
and rabiner         it is based on generating sequences of sucient length    sequences of     
observations in our case  according to the distribution induced by the true model  and comparing
their log likelihood according to the learnt model with the true model log likelihood  the total
difference in log likelihood is then divided by the total number of observations  accumulated
over all the sequences  giving a number that roughly measures the difference in log likelihood
per observation  formally stated  let m  be the true model and m  a learnt one  by generating
k sequences s            sk   each of length t   from the true model  m    the sampled kl divergence 
ds is 
k
x
 log pr si jm       log pr si jm     
i
  
ds m  jjm     
 
kt
   

filearning geometrically constrained hmms
    
    

   
    

   

                      

    

    

   

    
   

     
   

   

   

   

   

     

    

figure     sequence gathered by ramona 
perpendicularity assumed 

figure     sequence generated by our simulator  perpendicularity assumed 

we ignore the odometric information when applying the kl measure  thus allowing comparison
between purely topological models that are learnt with and without odometry 

    results within a global framework

we let ramona go around the path depicted in figure    and collect a sequence of about
    observations  while assuming perpendicularity of the environment  that is  at every turning
point the angle of turn is      thus at each turn ramona realigns its odometric readings with
its initial x and y axes  figure    plots the sequence of metric coordinates  gathered in this
way  while accumulating consecutive odometric readings  projected on hx  yi  we applied the
learning algorithm to the data    times     of these runs were started from a k means based
initial model     started from a tag based initial model  and    started from a random initial
model  in addition we also ran the standard baum welch algorithm  ignoring the odometric
information     times   note that there is non determinism even when using biased initial
models  since the k means clustering starts from random seeds  and low  random noise is added
to the data in all algorithms to avoid numerical instabilities  thus multiple runs give multiple
results   we report here the results obtained using the tag based method  which is the most
appropriate initialization method in the general case  these results are contrasted with those
obtained when odometric information is not used at all  for a comparison of all four settings
the reader is referred to the complete report of this work  shatkay        
figure    shows the essential representations of typical learnt models starting from a tag based
initial model  the geometry of the learnt model strongly corresponds to that of the true environment  and most of the states  positions were learnt correctly  although the figure does
not show it  the learnt observation distributions at each state usually match well with the true
observations 
to demonstrate the effect of odometry on the quality of the learnt topological model  we contrast
the plotted models learnt using odometry with a representative topological model learnt without
   a random number between   cm and  cm is added to recorded distances that are typically several meters
long 

   

fishatkay   kaelbling
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

  
 
 
  
  

 
  

  

 

  
 
  
  

  
  

  

  

  

  

  
  

 

  
  

 

mona traversed 

 

 

  

figure     learnt model of the corridors ra  

  

figure     the topology of a model learnt
without the use of odometry 

the use of odometric information  figure    shows the topology of a typical model learnt without
the use of odometric information  in this case  the arcs represent only topological relationships 
and their length is not meaningful  the initial state is shown as a bold circle  it is clear that
the topology learnt does not match the characteristic loop topology of the true environment 

for obtaining statistically sucient information  we generated   data sequences  each of length
      using monte carlo sampling from the hidden markov model whose projection is shown in
figure     one of these sequences is depicted in figure     the figure demonstrates that the
noise model used in the simulation is indeed compatible with the noise pattern associated with
real robot data  we used four different settings of the learning algorithm 

 starting from a biased  tag based  initial model and using odometric information 
 starting from a biased  k means based  initial model and using odometric information 
 starting from an initial model picked uniformly at random  while using odometric information 
 starting from a random initial model without using odometric information  standard baumwelch  

for each sequence and each of the four algorithmic settings we ran the algorithm    times  to
keep the discussion focused  we concentrate here on the first and the last of these settings and
the reader is referred to a more extensive report  shatkay        for a complete discussion 
in all the experiments  n was set to be     which is the  correct  number of states  for generalization  it will be necessary to use cross validation or regularization methods to select model
complexity  section   also suggests one possible heuristic for obtaining an estimate of the number
of states 
figure    shows an essential version of one learnt model  obtained from the sequence shown
in figure     using tag based initialization  we note that the learnt model is not completely
   

filearning geometrically constrained hmms
  
  

  
     

  
  

  
  
  
  

 

 

 

  

 
 

        

 
     
  
  

  

   
   

  
     

  
  

     

     

  
  

  
  

  

figure     learnt model of the simulated hallway environment 
accurate with respect to the true model  however  there is an obvious correspondence between
groups of states in the learnt and true models  and most of the transitions  as well as the
observations  which are not shown  were learnt correctly  the quality of the geometry of the
learnt model in this simulated large environment varies  and the geometrical results are not as
uniformly good as was the case when learning the smaller environment from real robot data 
as the environment gets large  the global relations between remote states  which are reected
in the geometrical consistency constraints  become harder to learn  still  the topology of the
learnt model as demonstrated by our statistical experiments is good 
table   lists the kl divergence between the true and learnt model  as well as the number
of runs until convergence was reached  for each of the   sequences for both the setting that
uses odometric information under tag based initialization and the learning algorithm that does
not use odometric information  averaged over    runs per sequence  we stress that each kl
divergence measure is calculated based on new data sequences that are generated from the true
model  as described in section      the   sequences from which the models were learnt do not
participate in the testing process 
the kl divergence with respect to the true model for models learnt using odometry  is about    
times smaller than for models learnt without odometric data  the standard deviation around
the means is about     for kl distances for models learnt with odometry and     for the noodometry setting  to check the significance of our results we used the simple two sample t test 
the models learnt using odometric information have statistically significantly  p          lower
average kl divergence than the others 

seq   
with kl
odo iter  
no
kl
odo iter  

 
     
     
     
     

 
     
     
     
     

 
     
     
     
     

 
     
     
     
     

 
     
     
     
     

table    average results of two learning settings with five training sequences 
   

fishatkay   kaelbling

in addition  the number of iterations required for convergence when learning using odometric
information is roughly     times smaller than that required when ignoring such information 
again  the t test verifies the significance of this result 
under all three initialization settings  the models learnt are topologically somewhat inferior  and
this is with high statistical significance   in terms of the kl divergence  to those learnt without
enforcing additivity  reported in earlier papers  shatkay   kaelbling               this is likely
to be a result of the very strong constraints enforced during the learning process  which prevent
the algorithm from searching better areas of the learning space  and restrict it to reach poor local
maxima  the geometry looks superior in some cases  but it is not significantly better  however 
there seems to be less variability in the quality of the geometrical models across multiple runs
when additivity is enforced 
while the details of an extensive comparison between the different initialization methods are
beyond the scope of this paper  we point out that our studies of both small and large models
show that when large models and long data sequences are involved  random initialization often
results in lower kl divergence than the tag based initialization  this again has to do with the
strong bias of tag based initialization  which can lead to very peaked models compared with the
less peaked distributions associated with the true model  random initialization leads to atter
models  as the kl divergence strongly penalizes models that are much more peaked than the
true ones  randomly initialized models are often closer  in terms of this measure  to the true
models than the very peaked ones learnt from other initial models  when learning small models 
where sucient training data is available  the tag based initialization results in models that are
clearly superior to the random ones  again  the reader is referred to the complete report of this
work  shatkay        for a comparative study of all initialization methods under the various
settings 

    results within a relative framework

we applied the algorithm described in section      extended to accommodate the state relative
constraints  as listed in section         the data used was gathered by the robot from the
same environment  and generated from the same simulated model as before  figures         
however  here the data is generated without assuming perpendicularity  this means that the x
and y coordinates are not realigned after each turn with the global x and y axes  but rather 
recorded  as is   the evaluation methods stay as described above 
figure    shows the projection of the odometric readings that ramona recorded along the
x and y dimensions  while traversing this environment  for obtaining statistically sucient
information  we generated   data sequences  each of length      using monte carlo sampling
from the hidden markov model whose projection is shown in figure     one of these sequences
is depicted in figure    
figure    shows a typical model obtained by applying the algorithm enforcing the complete
geometrical consistency  to the robot data shown in figure     using tag based initialization 
we note that the rectangular geometry of the environment is preserved  although state   does
not participate in the loop  this is explained by observing the corresponding area of the true
environment as depicted in figure     consisting of the   states clustered at the bottom left
corner            and      due to the relatively large number of states that are close together in
that area of the true environment  it was not recognized that we ever returned particularly to
state   during the loop  therefore  there was only one transition recorded from state   to state
   

filearning geometrically constrained hmms
    

    

    

    

    

   

    

     

     

    

   

    

    

   

     

     

                            

   

    

figure     sequence gathered by ramona  no

figure     sequence generated by our simula 

perpendicularity assumed 

tor  no perpendicularity assumed 
  

  

  

 
  

  

 

  

 
 
 

 

  

 
 
 

 

figure     learnt model of the corridors ramona traversed  initialization is tag based 
  according to the expected transition counts calculated by the algorithm  when projecting the
angles to maintain additivity   as described in section         the angle from state   to   was
therefore compromised  allowing geometrical consistency to maintain the rectangular geometry
among the more regularly visited states 
for the purpose of quantitatively evaluating the learning algorithm we list in table   the kl
divergence between the true and learnt model  as well as the number of iterations until convergence was reached  for each of the   simulation sequences with without odometric information 
averaged over    runs per sequence  the table demonstrates that the kl divergence with respect to the true model for models learnt using odometric data  is about   times smaller than
for models learnt without it  to check the significance of our results we again use the simple
two sample t test  the models learnt using odometric information have highly statistically significantly  p          lower average kl divergence than the others  in addition  the number of
   

fishatkay   kaelbling

seq   
with kl
odo iter  
no
kl
odo iter  

 
 
 
 
 
                        
                        
                          
                             

table    average results of   learning settings with   training sequences 
iterations required for convergence when learning using odometric information is smaller than
required when ignoring such information  again  the t test verifies the significance  p         
of this result 
it is important to point out that the number of iterations  although much lower  does not automatically imply that our algorithm runs in less time than the non odometric baum welch  the
major bottleneck is caused by the need to compute within the forward backward calculations 
as described in section        the values of the normal and the von mises densities  these require the calculation of exponent terms rather than simple multiplications  slowing down each
iteration  under the current nave implementation  however  we can solve this by augmenting
the program with look up tables for obtaining the relevant values rather than calculating them 
in addition  we can take advantage of the symmetry in the relations table to cut down on the
amount of calculation required  it is also possible to use the fact that many odometric relations remain unchanged  particularly in the later iterations of the algorithm  from one iteration
to the next  and therefore values can be cached and shared between iterations rather than be
recalculated at each iteration 

    reducing the amount of data
learning hmms obviously requires visiting states and transitioning between them multiple times 
to gather sucient data for robust statistical estimation  intuitively  exploiting odometric data
can help reduce the number of visits needed for obtaining a reliable model 
to examine the inuence of reduction in the length of data sequences on the quality of the learnt
models  we took one of the   sequences and used its prefixes of length     to      the complete
sequence   in increments of      as training sequences  we ran the two algorithmic settings over
each of the   prefix sequences     times repeatedly  we then used the kl divergence as described
above to evaluate each of the resulting models with respect to the true model  for each prefix
length we averaged the kl divergence over the    runs 
the plot in figure    depicts the average kl divergence as a function of the sequence length for
each of the two settings  it demonstrates that  in terms of the kl divergence  our algorithm 
which uses odometric information  is robust in the face of data reduction   down to     data
points   in contrast  learning without the use of odometry quickly deteriorates as the amount
of data is reduced 
we note that the data sequence is twice as  wide  when odometry is used than when it is
not  that is  there is more information in each element of the sequence when odometry data is
recorded  however  the effort of recording this additional odometric information is negligible 
and is well rewarded by the fact that fewer observations and less exploration are required for
obtaining a data sequence sucient for adequate learning 
   

filearning geometrically constrained hmms
  

  

  

no odometry

kl
  

  
odometry used
 

   

   
seq  length

   

   

figure     average kl divergence as a function of sequence length 

  conclusions
odometric information  which is often readily available in the robotics domain  makes it possible
to learn hidden markov models eciently and effectively  while using shorter training sequences 
more importantly  in contrast to the traditional perception of viewing the topological and the
geometric models as two distinct types of entities  we have shown that the odometric information
can be directly incorporated into the traditional topological hmm model  while maintaining
convergence of the reestimation algorithm to a local maximum of the likelihood function 
our method uses the odometric information in two ways  we first choose an initial model 
based on the odometric information  an iterative procedure  which extends the baum welch
algorithm  is then used to learn the topological model of the environment while learning an
additional set of constrained geometric parameters  the additional set of constrained parameters constitutes an extension to the basic hmm pomdp model of transitions and observations 
even though we are primarily interested in the underlying topological model  transition and
observation probabilities   our experiments demonstrate that the use of odometric relations can
reduce the number of iterations and the amount of data required by the algorithm  and improve
the resulting model 
the initialization procedure and the enforcement of the additivity constraint over relatively
small models prove helpful both topologically and geometrically  an extensive study  shatkay 
      shows that for long data sequences  generated from large models  enforcing only antisymmetry rather than additivity  leads to better topological models  this is because in these
cases  initialization is not always good  and additivity may over constrain the learning to an
unfavorable area  learning large models may benefit from enforcing only anti symmetry during
the first few iterations  and complete additivity in later iterations  alternatively  we may use our
algorithm  enforcing additivity  to learn separate models for small portions of the environment 
combining them later into one complete model  a similar idea of combining small modelfragments into a complete map of an environments was applied  in the context of geometrical
maps  in recent work by leonard and feder        
   

fishatkay   kaelbling

the work presented here demonstrates how domain specific information and constraints can be
enforced as part of the statistical estimation process  resulting in better models  while requiring
shorter data sequences  we strongly believe that this idea can be applied in domains other than
robotics  in particular  the acquisition of hmms for use in molecular biology may greatly benefit
from exploiting geometrical  and other  constraints on molecular structures  similarly  temporal
constraints may be exploited in domains in which pomdps are appropriate for decision support 
such as air trac control and medicine 

acknowledgments
we thank sebastian thrun for his insightful comments throughout this work  john hughes and luis ortiz
for their helpful advice  anthony cassandra for his code for generating random distributions  bill smart
for sustaining ramona and jim kurien for providing the low level code for driving her  the presentation
in this paper has benefited from the comments made by the anonymous referees to whom we are grateful 
this work was done while both authors were at the computer science department at brown university 
and was supported by darpa rome labs planning initiative grant f                 by nsf grants
iri         and iri          and by the brown university graduate research fellowship 

   

filearning geometrically constrained hmms

appendix a  an overview of the odometric learning algorithm
the algorithm takes as input an experience sequence e   hr  v i  consisting of the odometric
sequence r and the observation sequence v   as defined in the beginning of section      the
number of states is also assumed to be given 
learn odometric hmm e 
  initialize matrices a  b  r
 see section   
  max change  
  while   max change    
  do calculate forward probabilities  ff
 equation   
 
calculate backward probabilities  fi
 equation   
 
calculate state occupation probabilities    equation   
 
calculate state transition probabilities     equation   
 
old a a  old b b
 
a reestimate  a 
 equation    left 
  
b reestimate  b  
 equation    right 
  
r reestimate  r  
 equations    and    
x
y
x
y
  
hr   r i reestimate r   r    equations    and    
  
max change max get max change a  old a   
get max change b  old b   
the equations referenced in step    correspond to updates under the perpendicularity assumption  where a global framework is used  see  shatkay        for update formulae within a
state relative framework 
if additivity is enforced  step    is followed by a projection of the reestimated r onto an additive
ane space  as described in section        in addition  step    is substituted by the procedure
described in section        the reader is referred again to  shatkay        for further detail 
get max change is a function that takes two matrices and returns the maximal element wise
absolute difference between them   is a constant set to denote the margin of error on changes
in parameters  when the change in parameters is  small enough   the model is regarded as
 unchanged  

   

fishatkay   kaelbling

references
abe  n     warmuth  m  k          on the computational complexity of approximating distributions by probabilistic automata  machine learning                 
angluin  d          learning regular sets from queries and counterexamples  information and
computation             
asada  m          map building for a mobile robot from sensory data  in iyengar  s  s    
elfes  a   eds    autonomous mobile robots  pp           ieee computer society press 
bartels  r          estimation in a bidirectional mixture of von mises distributions  biometrics 
            
basye  k   dean  t     kaelbling  l  p          learning dynamics  system identification for
perceptually challenged agents  artificial intelligence         
baum  l  e   petrie  t   soules  g     weiss  n          a maximization technique occurring
in the statistical analysis of probabilistic functions of markov chains  the annals of
mathematical statistics                  
cassandra  a  r   kaelbling  l  p     kurien  j  a          acting under uncertainty  discrete
bayesian models for mobile robot navigation  in proceedings of ieee rsj international
conference on intelligent robots and systems 
degroot  m  h          probability and statistics   nd edition   addison wesley 
dempster  a  p   laird  n  m     rubin  d  b          maximum likelihood from incomplete
data via the em algorithm  journal of the royal statistical society               
dissanayake  g   newman  p   clark  s   durrant whyte  h  f     csorba  m          a solution
to the simultaneous localization and map building  slam  problem  ieee transactions
on robotics and automation         
duda  r  o     hart  p  e          unsupervised learning and clustering  chap     john wiley
and sons 
elfes  a          using occupancy grids for mobile robot perception and navigation  computer 
special issue on autonomous intelligent machines                
engelson  s  p     mcdermott  d  v          error correction in mobile robot map learning 
in proceedings of the ieee international conference on robotics and automation  pp 
           nice  france 
gold  e  m          complexity of automaton identification from given data  information and
control              
gumbel  e  g   greenwood  j  a     durand  d          the circular normal distribution 
theory and tables  american statistical society journal              
hopcroft  j  e     ullman  j  d          introduction to automata theory  languages  and
computation  addison   wesley 
   

filearning geometrically constrained hmms

juang  b  h          maximum likelihood estimation for mixture multivariate stochastic observations of markov chains  at t technical journal         
juang  b  h     rabiner  l  r          a probabilistic distance measure for hidden markov
models  at t technical journal                  
koenig  s     simmons  r  g       a   passive distance learning for robot navigation  in
proceedings of the thirteenth international conference on machine learning  pp      
    
koenig  s     simmons  r  g       b   unsupervised learning of probabilistic models for robot
navigation  in proceedings of the ieee international conference on robotics and automation 
kuipers  b     byun  y  t          a robot exploration and mapping strategy based on a semantic hierarchy of spatial representations  journal of robotics and autonomous systems 
         
kullback  s     leibler  r  a          on information and suciency  annals of mathematical
statistics                
leonard  j   durrant whyte  h  f     cox  i  j          dynamic map building for an autonomous mobile robot  in iyengar  s  s     elfes  a   eds    autonomous mobile robots 
pp           ieee computer society press 
leonard  j  j     feder  h  j  s          a computationally ecient method for large scale concurrent mapping and localization  in hollerbach  j     kodischek  d   eds    proceedings
of the ninth international symposium on robotics research 
liporace  l  a          maximum likelihood estimation for multivariate observations of markov
sources  ieee transactions on information theory         
mardia  k  v          statistics of directional data  academic press 
mataric  m  j          a distributed model for mobile robot environment learning and navigation  master s thesis  mit  artificial intelligence laboratory 
mclachlan  g  j     krishnan  t          the em algorithm and extensions  john wiley  
sons 
moravec  h  p          sensor fusion in certainty grids for mobile robots  ai magazine        
      
moravec  h  p     elfes  a          high resolution maps from wide angle sonar  in proceedings
of the international conference on robotics and automation  pp          
nourbakhsh  i   powers  r     birchfield  s          dervish  an oce navigating robot  ai
magazine                
pierce  d     kuipers  b          map learning with uninterpreted sensors and effectors  artificial intelligence                    
   

fishatkay   kaelbling

rabiner  l  r          a tutorial on hidden markov models and selected applications in speech
recognition  proceedings of the ieee                  
rivest  r  l     schapire  r  e          diversity based inference of finite automata  in
proceedings of the ieee twenty eighth annual symposium on foundations of computer
science  pp         los angeles  california 
rivest  r  l     schapire  r  e          inference of finite automata using homing sequences  in
proceedings of the twenty first annual symposium on theory of computing  pp          
seattle  washington 
ron  d   singer  y     tishbi  n          learning probabilistic automata with variable memory length  in proceedings of the seventh annual workshop on computational learning
theory  pp        
ron  d   singer  y     tishbi  n          on the learnability and usage of acyclic probabilistic
finite automata  in proceedings of the eighth annual workshop on computational learning
theory  pp        
ron  d   singer  y     tishby  n          on the learnability and usage of acyclic probabilistic
finite automata  journal of computer and systems science         
shatkay  h          learning models for robot navigation  ph d  thesis  department of computer science  brown university  providence  ri 
shatkay  h     kaelbling  l  p          learning topological maps with weak local odometric
information  in proceedings of the fifteenth international joint conference on artificial
intelligence  nagoya  japan 
shatkay  h     kaelbling  l  p          heading in the right direction  in proceedings of the
fifteenth international conference on machine learning  madison  wisconsin 
simmons  r  g     koenig  s          probabilistic navigation in partially observable environments  in proceedings of the international joint conference on artificial intelligence 
smith  r   self  m     cheeseman  p          a stochastic map for uncertain spatial relationships  in iyengar  s  s     elfes  a   eds    autonomous mobile robots  pp           ieee
computer society press 
thrun  s          learning metric topological maps for indoor mobile robot navigation  ai
journal           
thrun  s     bucken  a       a   integrating grid based and topological maps for mobile robot
navigation  in proceedings of the thirteenth national conference on artificial intelligence 
pp          
thrun  s     bucken  a       b   learning maps for indoor mobile robot navigation  tech  rep 
cmu cs         school of computer science  carnegie mellon university  pittsburgh 
pa 
thrun  s   burgard  w     fox  d       a   a probabilistic approach to concurrent map acquisition and localization for mobile robots  machine learning            
   

filearning geometrically constrained hmms

thrun  s   gutmann  j  s   fox  d   burgard  w     kuipers  b  j       b   integrating topological and metric maps for mobile robot navigation  a statistical approach  in proceedings
of the fifteenth national conference on artificial intelligence  pp          
vapnik  v  n          the nature of statistical learning theory  springer 

   

fi