journal artificial intelligence research                  

submitted       published     

learning geometrically constrained hidden markov models
robot navigation  bridging topological geometrical gap
hagit shatkay

hagit shatkay celera com

informatics research group 
celera genomics  rockville  md      

leslie pack kaelbling

artificial intelligence laboratory
massachusetts institute technology  cambridge       

lpk ai mit edu

come place streets marked 
windows lighted mostly they re darked 
place could sprain elbow chin 
dare stay out  dare go in    
go in  turn left right   
right and three quarters  or  maybe  quite    
simple it s not  i m afraid find 
mind maker upper make mind 

oh  places you ll go  dr  seuss 

abstract
hidden markov models  hmms  partially observable markov decision processes
 pomdps  provide useful tools modeling dynamical systems  particularly
useful representing topology environments road networks oce
buildings  typical robot navigation planning  work presented
describes formal framework incorporating readily available odometric information geometrical constraints models algorithm learns
them  taking advantage information  learning hmms pomdps made
generate better solutions require fewer iterations  robust face
data reduction  experimental results  obtained simulated real robot
data  demonstrate effectiveness approach 

  introduction

work concerned robots need perform tasks structured environments 
robot moving environment suffers two main limitations  noisy sensors prevent
confidently knowing is  noisy effectors prevent knowing
certainty actions take it  concentrate structured environments 
turn characterized two main properties  environments consist vast uneventful uninteresting areas  interspersed relatively interesting positions
situations  consider instance robot delivering bagel oce building  interesting
situations doors intersections building hallways  well various
c      ai access foundation morgan kaufmann publishers  rights reserved 

fishatkay   kaelbling

positions bagel might respect robot s arm  e g   robot holding
bagel  puts down  etc   aspects environment  desk positions
oces  inconsequential bagel delivery task 
natural way represent combination environment robot s interactions
it  probabilistic automaton  states represent interesting situations 
edges states represent actions leading one situation another  probability
distributions transitions possible observations robot may perceive
situation model robot s noisy effectors sensors  respectively 
models formally known pomdp  partially observable markov decision process  models  proven useful robot planning acting inherent world uncertainty  simmons   koenig        nourbakhsh  powers    birchfield        cassandra  kaelbling    kurien        
despite much work using models  task learning directly automatically
data widely addressed  research concerning immediate topic date
consists mostly work done simmons koenig      b   assumption underlying
work human provides rather accurate topological model states
connections  exact probability distributions learned top model 
using version baum welch algorithm  rabiner         another interesting approach
acquisition topological models thrun bucken      a     b  thrun        
focused extracting deterministic topological maps previously acquired geometricalgrid based maps  latter learned directly data  discussion
related research geometrical topological approaches  probabilistic
deterministic versions  given next section 
work reported first successful attempt aware learn purely probabilistictopological models  directly completely recorded data  without using previous humanprovided grid based models  based using weak geometric information  recorded
robot  help learn topology environment  represent probabilistic
model  therefore  directly bridges historically perceived gap topological
geometrical information  addresses claim presented thrun s work       
main shortcoming topological approach failure utilize inherent geometry
learnt environment 
robots equipped wheel encoders enable odometer record change
robot s position moves environment  data typically noisy
inaccurate  oors environment rarely smooth  wheels robot
always aligned neither motors  mechanics imperfect  resulting slippage
drift  effects accumulate  mark initial position robot 
try estimate current position based summing long sequence odometric recordings 
resulting estimate incorrect  is  raw recorded odometric information
effective tool  itself  determining absolute location robot
environment 
approach aimed determining absolute locations  idea underlying
weak odometric information  despite noise inaccuracy  still provides geometrical cues
help distinguish different states  well identify revisitation
state  hence  information enhances ability learn topological models  however 
   

filearning geometrically constrained hmms

use geometrical information requires careful treatment geometrical constraints
directional data  demonstrate existing models algorithms extended
take advantage noisy odometric data geometrical constraints  geometrical
information directly incorporated probabilistic topological framework  producing
significant improvement standard baum welch algorithm  without need humanprovided model 
rest paper organized follows  section   provides survey previous work
area learning maps robot navigation  brie refers earlier work learning
automata  section   presents formal framework work  section   presents main
aspects iterative learning algorithm  section   describes strategies selecting
initial point iterative process begins  section   presents experimental results
obtained simulated real robot data traditionally hard to learn environments 
experiments demonstrate algorithm indeed converges better models fewer
iterations standard baum welch method  robust face data reduction 

  approaches learning maps models

work presented lies intersection theoretical area learning computational models in particular  learning automata data sequences and applied area
map acquisition robot navigation  concentrate surveying work latter
area  pointing distinction approach predecessors  brie review
results automata computational learning theory  comprehensive review
theoretical results given shatkay        

    modeling environments robot navigation

context maps models robot navigation  distinction usually made two
principal kinds maps  geometric topological  geometric maps describe environment
collection objects occupied positions space  geometric relationships among
them  topological framework less concerned geometrical positions  models
world collection states connectivity  is  states reachable
states actions lead one state next 
draw additional distinction  world centric  maps provide  objective 
description environment independent agent using map  robot centric models
capture interaction particular  subjective  agent environment 
learning map  agent needs take account noisy sensors actuators try
obtain objectively correct map agents could use well  similarly  agents
using map need compensate limitations order assess position
according map  learning model captures interaction  agent acquiring
model one using it  hence  noisy sensors actuators specific agent
ected model  different model likely needed different agents 
related work described below  especially within geometrical framework  centered
around learning objective maps world rather agent specific models  shall point
survey work concerned latter kind models 
work focuses acquiring purely topological models  less concerned learning
geometrical relationships locations objects  objective maps  although geometrical
   thank sebastian thrun terminology 

   

fishatkay   kaelbling

relationships serve aid acquisition process  concept state used
topological framework general concept geometrical location  since state
include information battery level  arm position etc  information 
great importance planning  non geometrical nature therefore cannot readily
captured purely geometrical framework  following sections provide survey work
done within geometrical framework within topological framework  well
combinations two approaches 

    geometric maps

geometric maps provide description environment terms objects placed
positions  example  grid based maps instance geometric approach 
grid based map  environment modeled grid  an array   position
grid either vacant occupied object  binary values placed array  
approach refined ect uncertainty world  grid cells
contain occupancy probabilities rather binary values  lot work done
learning grid based maps robot navigation use sonar readings
interpretation  moravec elfes others  moravec   elfes        moravec        elfes 
      asada        
underlying assumption learning maps robot tell  or find out 
grid obtains sonar reading indicating object  therefore
place object correctly grid  similar localization assumption  requiring robot
identify geometrical location  underlies geometric mapping techniques leonard
et al          smith et al          thrun et al       b  dissanayake et al          even
explicit grid part model  explicit localization hard satisfy 
leonard et al         smith et al         address issue use geometrical
beacons estimate location robot  known kalman filter method 
gaussian probability distribution used model robot s possible current location  based
observations collected current point   without allowing refinement previous
position estimates based later observations   research area recently extended
two directions  leonard feder        partition task learning one large map
learning multiple smaller map sections  thus addressing issue computational eciency 
dissanayake et al         conduct theoretical study approach show convergence
properties  latter may lead computational eciency identifying cases
steady state solution readily obtained  accordingly bounding number steps required
algorithms reach useful solution cases 
work thrun et al       a  uses similar probabilistic approach obtaining grid based maps 
work refined  thrun et al       b  first learn location significant landmarks
environment fill details complete geometrical grid  based laser range
scans  latter work extends approach smith et al    using observations obtained
location visited  order derive probability distribution
possible locations  achieve this  authors use forward backward procedure similar
one used baum welch algorithm  rabiner         order determine possible
locations observed data  approach resembles use forwardbackward estimation procedure  probabilistic basis  aiming obtaining maximum
likelihood map environment  still significantly differs initial
assumptions final results  data assumed provided learner includes
   

filearning geometrically constrained hmms

motion model perceptual model robot  consist transition
observation probabilities within grid  components learnt algorithm 
although grid context coarser grained  topological framework  end result
algorithm probabilistic grid based map  probabilistic topological model 
explained next section 
addition concerned locations  rather richer notion state 
fundamental drawback geometrical maps fine granularity high accuracy  geometrical maps  particularly grid based ones  tend give accurate detailed picture
environment  cases necessary robot know exact location terms
metric coordinates  metric maps indeed best choice  however  many planning tasks
require fine granularity accurate measurements  better facilitated
abstract representation world  example  robot needs deliver bagel
oce oce b  needs map depicting relative location respect
b  passageways two oces  perhaps landmarks help orient
gets lost  reasonably well operating low level obstacle avoidance mechanism
help bypass ower pots chairs might encounter way  objects
need part environment map  driver traveling cities needs
know neither longitude latitude coordinates globe  location specific
houses along way  robot need know exact location within building
exact location various items environment  order get one point
another  hence  effort obtaining detailed maps usually justified  addition
maps large  makes planning even though planning polynomial
size map inecient 

    topological maps models

alternative detailed geometric maps abstract topological maps 
maps specify topology important landmarks situations  states   routes transitions  arcs  them  concerned less physical location landmarks 
topological relationships situations  typically  less complex
support much ecient planning metric maps  topological maps built lowerlevel abstractions allow robot move along arcs  perhaps wall  road following  
recognize properties locations  distinguish significant locations states 
exible allowing general notion state  possibly including information
non geometrical aspects robot s situation 
two typical strategies deriving topological maps  one learn topological
map directly  first learn geometric map  derive topological model
process analysis 
nice example second approach provided thrun bucken      a      b  thrun 
       use occupancy grid techniques build initial map  strategy appropriate
primary cues decomposition abstraction map geometric  however 
many cases  nodes topological map defined terms sensory data  e g  
labels door whether robot holding bagel   learning geometric map first
relies odometric abilities robot  weak space large 
dicult derive consistent map 

   

fishatkay   kaelbling

contrast  work concentrates learning topological model directly  assuming abstraction robot s perception action abilities already done  abstractions
manually encoded lower level robot navigational software  described
section    work pierce kuipers        discusses automatic method extracting
abstract states features raw perceptual information 
kuipers byun        provide strategy learning deterministic topological maps  works
well domains noise robot s perception action abstracted
away  learning single visits nodes traversals arcs  strong underlying assumption
strategies  building map  current state reliably identified
based local information  based distance traversed previous well identified
state  methods unable handle situations long sequences actions
observations necessary disambiguate robot s state 
mataric        provides alternative approach learning deterministic topological maps 
represented distributed graphs  learning process relies assumption
current state distinguished states based local information includes
compass sonar readings  uncertainty modeled probability distributions 
instead  matching current readings already existing states required exact 
thresholds tolerated error set empirically  another difference work presented
here  learn complete probabilistic topology environment  mataric s
work overall topology graph assumed advance linear list  additional
edges added learning process  probability distribution associated
edges  mechanism choosing edge take determined part goal seeking
process  part model itself 
engelson mcdermott        learn  diktiometric  maps  topological maps metric relations nodes  experience  uncertainty model use interval based rather
probabilistic  learned representation deterministic  ad hoc routines handle problems resulting failures uncertainty representation 
prefer learn combined model world robot s interaction world 
allows robust planning takes account likelihood error sensing action 
work closely related koenig simmons      b      a   learn pomdp
models  stochastic topological models  robot hallway environment  recognize
diculty learning good model without initial information  solve problem
using human provided topological map  together constraints structure
model  modified version baum welch algorithm learns parameters
model  developed incremental version baum welch used on line 
models contain weak metric information  representing hallways chains one meter
segments allowing learning algorithm select probable chain length 
method effective  results large models size proportional hallways  length 
strongly depends quality human provided initial model 

    learning automata data

informally speaking  automaton consists set states set transitions lead
one state another  context work  automaton states correspond
states modeled environments  transitions  state changes due actions
performed environment  transition automaton tagged symbol
   

filearning geometrically constrained hmms

input alphabet    corresponding action input system caused state
transition  classical automata theory  e g   hopcroft   ullman        distinguishes
deterministic non deterministic automata  if  alphabet symbol ff  single
edge tagged it  going state  automaton deterministic  otherwise 
transition states uniquely determined input symbol automaton
non deterministic  augment transition edge non deterministic automaton
probability taking given certain input  ff  resulting automaton called probabilistic 
basic problem learning finite deterministic automata given data roughly
described follows  given set positive set negative example strings 
respectively  alphabet   fixed number states k  construct minimal deterministic
finite automaton k states accepts accept   problem
shown np complete  gold         despite hardness  positive results
shown possible various special settings  angluin        showed oracle
answer membership queries provide counterexamples conjectures automaton 
polynomial time learning algorithm positive negative examples  rivest
schapire               provide several effective methods  various settings  learn
deterministic automata correct high probability  work deals
learning noise free data  basye  dean kaelbling        presented several algorithms
that  high probability  learn input output deterministic automata  data observed
learner corrupted various forms noise 
cases  learned automaton deterministic rather probabilistic  basic
learning problem probabilistic context find automaton assigns
distribution true one data sequences  using training data   generated
true automaton  another form learning problem finding probabilistic
automaton assigns maximum likelihood training data   is  automaton
maximizes pr s j  
abe warmuth        show finding probabilistic automaton   states  even
small error respect true model allowed probability  the probably
approximately correct  pac  learning model   cannot done polynomial time polynomial number examples  unless np   rp  work arises broadly accepted
conjecture  yet proven  learning hidden markov models hard even
pac sense  two ways address hardness  one restrict class
probabilistic models learned  learn unrestricted hidden markov models
good practical results pac guarantees quality result 
work ron et al                     pursues first approach  learning restricted classes
automata  namely  acyclic probabilistic finite automata  probabilistic finite sux automata 
classes useful various applications related natural language processing 
learned polynomial time within pac framework 
second approach  one predominantly taken work  learn model
member complete unrestricted class hidden markov models  weak guarantees
exist goodness model  learning procedure may directed obtain
practically good results  approach based guessing automaton  model   using
iterative procedure make automaton fit better training data  one algorithm
commonly used purpose baum welch algorithm  baum  petrie  soules    weiss 
       presented detail rabiner         iterative updates model
   

fishatkay   kaelbling

based gathering sucient statistics data given current automaton 
update procedure guaranteed converge model locally maximizes likelihood
function pr datajmodel   since maximum local  model might close enough
true automaton data generated  challenging problem find
ways force algorithm converging higher likelihood maxima  least make
converge faster  facilitating multiple guesses initial models  thus raising probability
converging higher likelihood maxima  approach one taken work
presented here 
assume  throughout paper  number states model learning
known  strong assumption since methods learning number
states  regularization methods deciding number states model parameters 
discussed  instance  vapnik s book         address issue here 
rest work describes approach learning topological models  use noisy
odometric information readily available robots  geometrical information
typically used topological mapping methods  demonstrate topological model
algorithm used learn extended directly incorporate weak odometric
information  show so  avoid use human provided priori
models still learn stochastic environment models eciently effectively 

  models assumptions
section describes formal framework work  starts introducing classic
hidden markov model  model extended accommodate noisy odometric information
nave form  ignoring information robot s heading orientation  later
adapted accommodate heading information 
concentrate describing models algorithms learning hmms  rather
pomdps  means robot decisions make regarding next action
every state  one action executed state  experiments  human operator gave action command associated state robot gathering data 
note action necessarily one every state  e g   robot told
always turn right state   move forward state    however  state one action taken  extension complete pomdps  implemented 
learning hmm possible actions  straightforward although notationally
cumbersome  thus limit discussion hmms 

    hmms   basics
hidden markov model consists states  transitions  observations probabilistic behavior 
formally defined tuple   hs  o  a  b  i  satisfying following conditions 

  fs            sn    g finite set n states 
  fo            om   g finite set possible observation values 
   

filearning geometrically constrained hmms

stochastic transition matrix  ai j   pr qt     sj jqt   si     i  j n    
nx
  
qt state time t  every state si  

j   

ai j     

ai j holds transition probability state si state sj  
b stochastic observation matrix  bj k   pr vt   ok jqt   sj      j n     
mx
  
  k      vt observation recorded time t  every state sj  
bj k     
bj k holds probability observing ok state sj  

k  

stochastic initial distribution vector    pr q    si     n     

nx
  
i  

    

holds probability state si time    starting record observations 
model corresponds world whose actual state given time t  qt     hidden
directly observable  observable aspects state  vt   o  detected
recorded state visited time t  agent moves one hidden state
next according probability distribution encoded matrix a  observed information
state governed probability matrix b   although work concerned
discrete observations  extension continuous observations straightforward
well addressed work hidden markov models  liporace        juang        
simply stated  problem learning hmm  reverse engineering  hidden markov
model stochastic system sampled data  generated system  formalize
learning task section      next section extends hmms account geometric
information 

    adding odometry hidden markov models

world composed finite set states  fundamental distinction
framework term state term location  state robot
directly correspond location  state may include information  robot s
battery level orientation location  robot standing entrance oce    
facing right different state robot standing place facing left  similarly 
robot standing bagel arm different state robot
position without bagel 
dynamics world described state transition distributions specify probability making transitions one state next result certain action 
finite set observations perceived state  relative frequency
observation described probability distribution depends current state 
model  observations multi dimensional  observation vector values 
chosen finite domain  is  factorize observation associated state
several components  instance  demonstrated section      view observation
recorded robot standing oce environment consisting three components 
corresponding three cardinal directions  front  left right  example  observation vector thus   dimensional  assumed vector s components conditionally
independent  given state 
   

fishatkay   kaelbling

addition components  state assumed associated position
metric space  whenever state transition made  robot records odometry vector 
estimates position current state relative previous one  time assume odometry vector consists readings along x coordinates global coordinate system  readings corrupted independent normal noise  latter
independence assumption strict one  relaxed introducing complete covariance matrix  although done work  section     extend odometry vector include information heading robot  drop global coordinate
framework 
note odometric relationship characterizes transition rather state and 
described below  receives different treatment observations associated
states 
two important assumptions underlying treatment odometric relations
states  first  inherent  true  odometric relation position every
two states world  second  robot moves one state next 
normal    mean noise around correct expected odometric reading along odometric
dimension  noise ects two kinds odometric error sources 

  lack precision discretization real world states  e g  rather

large area robot stand regarded  the doorway ai
lab   
  lack precision odometric measures recorded robot  due slippage 
friction  disalignment wheels  imprecision measuring instruments  etc 

formally introduce odometric information hidden markov model framework 
define augmented hidden markov model tuple   hs  o  a  b  r  i  where 

  fs            sn    g finite set n states 
  qli   oi finite set observation vectors length l  ith element

observation vector chosen finite set oi  
stochastic transition matrix  ai j   pr qt     sj jqt   si     i  j n     
nx
  
qt state time t  every state si   ai j     
j   

ai j holds transition probability state si state sj  
b array l stochastic observation matrices  bi j k   pr vt  i    ok jqt   sj   
  l    j n      ok   oi   vt observation vector time t  vt  i  ith

component 
bi j k holds probability observing ok along ith component observation
vector  state sj  
r relation matrix  specifying pair states  si sj   mean variance
d dimensional  odometric relation them   ri j  m   mean mth

   time consider    corresponding  x  y  readings 

   

filearning geometrically constrained hmms

component relation si sj    ri j  m    variance  furthermore 
r geometrically consistent  component m  relation  a  b     ra b  m  
must directed metric  satisfying following properties states a  b  c 
def

m a  a      
m a  b     m b  a   anti symmetry  
m a  c     a  b    m b  c   additivity    
representation odometric relations ects two assumptions  previously stated 
regarding nature odometric information   true  odometric relation
position every two states represented mean  noise around correct
expected odometric relation  accounting lack precision real world
discretization inaccuracy measurement  represented variance 

stochastic initial probability vector describing distribution initial state 
simplicity assumed form h                             i  implying
one designated initial state  si   robot always started 

model extends standard hidden markov model described section     two ways 
facilitates observations factored components  represented vectors 
components assumed conditionally independent given
state  factorization  together conditional independence assumption  allows
simple calculation probability complete observation vector
probabilities components  therefore results fewer probabilistic parameters
learnt model view observation vector  consisting possible
combination component values single  atomic  observation 

introduces odometric relation matrix r constraints components  using
r constraints it  explained section    proven useful learning
model parameters  demonstrated section   

    handling directional data

extend model accommodate directional changes addition positional
changes  two issues stemming directional changes moving environment  need non traditional distributions model directional changes  need
correct cumulative rotational error severely interferes location estimation
within global coordinate framework  detailed discussion two problems
solution given earlier paper authors  shatkay   kaelbling         sake
completeness  brie review two issues here 
      circular distributions

robot s change direction moves environment expressed terms
angular change respect original heading  since angular measures inherently circular  treating  normally distributed   using standard procedures obtaining
sucient statistics data adequate  trivial example  average
   

fishatkay   kaelbling


 

 x    y  
 x    y  
 x    y  
 

     
   

 

  

 

 

 
 

x

  

figure    simple average two angles  depicted

vectors unit circle  average angle
formed dashed vector 

figure    directional data represented angles
vectors unit circle 

two angular readings             using simple average obtain angle     
far intuitive       illustrated figure   
address circularity issue  use von mises distribution  circular version
normal distribution  model change heading two states  explained below 
collection changes heading within two dimensional space represented terms
either cartesian polar coordinates  using cartesian system  n changes headings
recorded sequence   dimensional vectors   hx    y  i        hxn   yn i   unit circle 
shown figure    changes represented corresponding angles
radii center unit circle x axis               n    respectively 
relationship two representations is 
xi   cos i    yi   sin i        n   
vector mean n points  hx  yi  calculated as 
pn cos   
pn sin   

 

  
i  
x 

 
 
n
n

   

using polar coordinates  express mean vector terms angle    length  a 
 except case x        

  arctan  xy   

   x         
 
 

angle mean angle  length measure  between     
concentrated sample angles around   closer    concentrated
sample around mean  corresponds smaller sample variance 
intuitively  satisfactory circular version normal distribution would mean
maximum likelihood estimate average angle calculated above  way
analogous gauss  derivation normal distribution  von mises developed circular
version  gumbel  greenwood    durand        mardia         defined follows 
definition  circular random variable         said von mises
distribution parameters            probability density
   

filearning geometrically constrained hmms

function is 

f       i     e cos     
 

i     modified bessel function first kind order   

i      

     
x
 r
         
r
 
r  

   

parameters correspond distribution s mean concentration respectively 
circular normal distributions exist  von mises desirable estimation
procedure alluded earlier  given set heading samples  angles           n   von mises
distribution  maximum likelihood estimate is 

  arctan  xy    

y  x defined equation   
maximum likelihood estimate concentration parameter    satisfies 
n
i       max    x
i    
n i   cos i          

i  modified bessel function first kind order   

i      

 
x

        r    
r   r  r        

   

information estimation procedure beyond scope paper
found elsewhere  gumbel et al         mardia        
conclude  assume change heading von mises distributed  around mean
concentration parameter   assumption ected model learning procedures
explained later section        change heading h  a  b    a  b i pair
states  a  b  completes set parameters included relation matrix r
introduced earlier section     
      cumulative rotational error

tend think environment consisting landmarks fixed global coordinate
system corridors transitions connecting landmarks  idea underlies typical
maps constructed used everyday life  however  view environment may
problematic robots involved 
conceptually  robot two levels operates  abstract level  centers
corridors  follows walls avoids obstacles  physical level motors
turn wheels robot moves  physical level many inaccuracies manifest
themselves  wheels unaligned resulting drift right
left  one motor slightly faster another resulting similar drifts  obstacle
one wheels cause robot rotate around slightly  uneven oors may cause
   

fishatkay   kaelbling



  actual position
  recorded position

figure    robot moving along solid arrow  correcting drift direction dashed
arrow  dotted arrow marks recorded change position 

robot slip certain direction  addition  measuring instrumentation odometric
information may accurate itself  abstract level  corrective actions
constantly executed overcome physical drift drag  example  left wheel
misaligned drags robot leftwards  corrective action moving right constantly
taken higher level keep robot centered corridor 
phenomena described significant effect odometry recorded robot 
data interpreted respect one global framework  example  consider robot
depicted figure    drifts left   moving one state next 
corrects moving right order maintain centered corridor 
let us assume states   meters apart along center corridor  center
corridor aligned axis global coordinate system  robot steps back
forth corridor one state next  whenever robot reaches state 
odometry reading changes hx  y  along hx  y  headingi dimensions  respectively 
robot proceeds  deviation respect x axis becomes severe  thus 
going several transitions  odometric changes recorded every pair
states  taken respect global coordinate system  become larger larger  similar
problems inconsistent odometric changes recorded pairs states arise along
odometric dimensions  especially severe inconsistencies arise respect
heading  since lead mistakenly switching movement along x
axes  well confusion forwards backwards movement  when deviation
heading around        respectively  
early work  shatkay   kaelbling        assumed perpendicularity corridors 
taken advantage robot collected data  odometric readings recorded
respect global coordinate system  robot could re align origin
turn  trajectory odometry recorded perpendicularity assumption
robot ramona  along x axes given figure    sequence shown recorded
robot drove repeatedly around loop corridors  details data
gathering process provided section    contrast  figure   shows trajectory another
sequence odometric readings recorded ramona  driving corridors  without
using perpendicularity assumption  data collected latter setting subjected
cumulative rotational error 
   

filearning geometrically constrained hmms
    
    

    
    

    
   

    

   

    

   

   

   
   

   

   

   

    

                            

figure    sequence gathered ramona  perpendicularity assumed 

   

    

figure    sequence gathered ramona  per 

pendicularity assumed 

data handled state relative coordinate systems  shatkay   kaelbling        
latter implies state si coordinate system  shown figure   
origin anchored si   axis aligned robot s heading state  denoted
bold arrows figure   x axis perpendicular it  contrast global
coordinate system anchored initial starting state  within global coordinate
system  relations recorded may vary greatly among multiple instances transition
pair states  using state relative system  recorded learned
relationship pair states  hsi   sj i  reliable  despite fact based
multiple transitions recorded si sj  
state relative coordinate systems  geometric relation stored rij    which introduced section       expressed pair states  si sj   respect
coordinate system associated state si  accordingly  constraints imposed x
components relation matrix must specified respect explicit coordinate
system used  explained below 
given pair states b  denote hx yi  a  b  vector h ra b  x     ra b  y  i  let
us define tab transformation maps hxa   ya point represented respect
coordinate system state a  point represented respect coordinate
system state b  hxb   yb i 
explicitly  let ab mean change heading state state b  applying tab
vector h xyaa results vector h xybb follows 

   

     

xb
x
x cos ab     ya sin ab  
  tab  
yb
ya
xa sin ab     ya cos ab  

 

 

consistency constraints within framework must restated as 

hx yi a  a    h    i 
hx yi a  b     tba hx yi b  a    anti symmetry  
hx yi a  c    hx yi  a  b    tba hx yi  b  c    additivity  
   

fishatkay   kaelbling


x
sj
si





x

figure    robot state si   faces  axis direction  relation si  sj wrt si  s coordinate
system 

consistency constraints ones need enforced learning algorithm
constructs hmm  important note transformation
constitute set additional parameters need learnt  rather  calculated terms
heading change parameter    already integral part relation matrix
defined sections           
introduced basic formal model use representing environments
robot s interaction them  following section state learning problem
describe basic algorithm learning model data 

  learning hmms odometric information

section formalizes learning problem hmms  discusses odometric information
incorporated learning algorithm  overview complete algorithm provided
appendix paper 

    learning problem

learning problem hidden markov models generally stated follows  given
experience sequence e  find hidden markov model could generated sequence
 useful   close original  according criterion  explicit common statistical
approach look model maximizes likelihood data sequence e given
model  formally stated  maximizes pr ej   however  given complicated landscape
typical likelihood functions multi parameter domain  obtaining maximum likelihood
model feasible  studied practical methods  particular well known baumwelch algorithm  rabiner        references therein  guarantee local maximum
likelihood model 
another way evaluating quality learned model comparing true model 
note stochastic models  such hmms  induce probability distribution observation sequences given length  kullback leibler  kullback   leibler        divergence
learned distribution true one commonly used measure estimating good
   

filearning geometrically constrained hmms

learned model is  obtaining model minimizes measure possible learning goal 
culprit practice  learn model data   ground
truth  model compare learned model with  still  evaluate learning algorithms
measuring well perform data obtained known models  reasonable expect algorithm learns well data generated model have 
perform well data generated unknown model  assuming models indeed form
suitable representation true generating process  discuss kullback leibler  kl 
divergence detail section     context evaluating experimental results 
summarize  learning problem address work obtaining model
attempting  locally  maximize likelihood  evaluating results based
kl divergence respect true underlying distribution  distribution
available 

    learning algorithm

learning algorithm starts initial model   given experience sequence e 
returns revised model    locally  maximizes likelihood p  ej   experience
sequence e length   element  et      t       pair hrt   vt i  rt
observed relation vector along x  dimensions  states qt   qt   vt
observation vector time t 
algorithm extends standard baum welch algorithm deal relational information factored observation sets  baum welch algorithm expectationmaximization  em  algorithm  dempster  laird    rubin         alternates
e step computing state occupation state transition probabilities   
time sequence given e current model  
m step finding new model    maximizes p  ej      
providing monotone convergence likelihood function p  ej  local maximum 
however  extension introduces additional component  namely  relation matrix r 
viewed two kinds observations  state observations  as ordinary hmm  
distinction observe integer vectors rather integers  transition observations  the odometry relations states   latter must satisfy geometrical constraints 
hence  extension standard update formulae  described below  required 
      state occupation probabilities

following rabiner         first compute forward  ff  backward  fi   matrices  fft  i 
denotes probability density value observing e  et qt   si   given   fit  i 
probability density observing et   et    given qt   si   formally 
fft  i    pr e            et   qt   sij   
fit  i    pr et             et    jqt   si      
measurements continuous  as case r   matrices contain
probability density values rather probabilities 
forward procedure calculating matrix initialized
 
b    
ff   i       otherwise
 
   

fishatkay   kaelbling

continued        
fft  j    

nx
  
i  

fft    i ai j f  rt jri j  bjt  

   

expression f  rt jri j   denotes density point rt according distribution represented
means variances entry i  j q
relation matrix r  bjt probability
j
observing vector vt state sj   is  bt   li   bi j vt i   
backward procedure calculating matrix initialized fit     j      continued
  t t    
nx
  
fit  i    fit    j  ai j f  rt   jri j  bjt    
   
j   

given   compute given time point state occupation statetransition probabilities    state occupation probabilities   i   representing
probability state si time given experience sequence current model 
computed follows 
 
   
 i    pr qt   si je      pnff t  i fit  i 
j    fft  j  fit  j  
similarly   i  j    state transition probabilities state state j time given
experience sequence current model  computed as 
 i  j     pr qt   si   qt     sj je   
fft  i ai j bjt   f  rt   jri j  fit    j  
 
   
 
nx
   nx
  
i   j   

fft  i ai j bjt   f  rt   jri j  fit    j  

essentially formulae appearing rabiner s tutorial  rabiner        
take account density odometric relations 
next phase algorithm  goal find new model    maximizes likelihood conditioned current transition observation probabilities  pr ej       usually 
simply done using maximum likelihood estimation probability distributions
b computing expected transition observation frequencies  model must
compute new relation matrix  r  constraint remain geometrically consistent 
rest section use notation v denote reestimated value  v
denotes current value 
      updating transition observation parameters

b matrices straightforwardly reestimated  ai j expected number
transitions si sj divided expected number transitions si   b i j k
expected number times ok observed along ith dimension state sj   divided
expected number times sj  
pt   
pt     i  j  


  
  b i j k   t  pt v t  i  ok    j    
   
ai j   pt   
t    i 
t    i 
expression c denotes indicator function value   condition c true   otherwise 
   

filearning geometrically constrained hmms
   
p

q

 

p

   

  

  

  

  

 

 

 

 

    
  

  

  

  

 

 

 
    

q

figure    examples two sets normally distributed points constrained means     
dimensions 

      updating relation parameters

reestimating relation matrix  r  geometrical constraints induce interdependencies
among optimal mean estimates well optimal variance estimates mean
estimates  parameter estimation form constraints almost untreated mainstream statistics  bartels        found previous existing solutions estimation
problem addressed here  illustration issues involved estimation constraints
consider following estimation problem   normal means 
example     data consists two sample sets points p   fp   p            pn g q  
fq   q            qk g  independently drawn two distinct normal distributions means p   q
variances p    q    respectively  asked find maximum likelihood estimates
two distribution parameters  moreover  told means two distributions
related  q    p   illustrated figure    latter constraint  task
simple  degroot         have 
pn p
pn
      i    pi   p     
p   i  
p
n
n

similarly q q    however  constraint p    q requires finding single mean   
setting one negated value     intuitively  choosing maximum
likelihood single mean  concentrated sample effect 
varied sample  submissive   thus  overall sample deviation means
would minimized likelihood data maximized  therefore  mutual
dependence estimation mean estimation variance 
since samples independently drawn  joint likelihood function is 
  pi  p   

n
p

f  p  qjp   q  p    q      e p
i    p
   



yk e
j   

  qj  q   
q

p

   

 q

 

taking derivatives joint log likelihood function  respect p   p q 
equating    using constraint q    p   obtain following set mutual
equations maximum likelihood estimators 
p
p
 q  ni   pi     p  kj   qj  
p  
  q    p  
nq    kp 
pk  q     
pn  p     

p

  
 
 
p  
  q   j    j p  

n

k

   

fishatkay   kaelbling

substituting expressions p q expression p   obtain cubic equation cumbersome  still solvable  in simple case   solution provides maximum likelihood estimate mean variance constraint q    p  
 
proceed actual update relation matrix constraints  clarity 
initially discuss first two geometrical constraints  discuss additivity constraint
section      recall concentrate enforcement global constraints  appropriate
perpendicularity assumption  although idea applied case staterelative constraints 
zero distances states trivially enforced  setting diagonal
entries r matrix    small variance 
anti symmetry within global coordinate system enforced using data recorded along
transition state sj si well state si sj reestimating  ri j   
demonstrated example      variance taken account  leading following
set mutual equations 



mi j

 

  mi j     

pt   

rt m t  i j     rt  m t j i 
 
 
i j   
j i   

pt    t mi j    t mj i 
t    i j     j i   
pt      i  j   r  m        
t  
pt    t i  j   i j  
t  
t  

 

   
    

x dimensions   m   x  y   amounts complicated still solvable cubic
equation  however  general case  accounting orientation robot 
complete additivity enforced  obtain closed form reestimation
formulae 
avoid hardships  use lag behind update rule  yet unupdated estimate
variance used calculating new estimate mean  new mean estimate
used update variance  using equation      thus  mean updated using variance
parameter lags behind update process  reestimation equation     needs
use rather follows  pt    h rt  m t  i j  rt  m t  j i 
    j i
  
t  
 
    
mi j   pt    hi jt   i j    j i
 
t  

      j i
  
i j

 

shown  shatkay         lag behind policy instance generalized em  mclachlan   krishnan         latter guarantees monotone convergence local maximum
likelihood function  even  maximization  step increases rather strictly maximizes expected likelihood data given current model 
similarly  reestimation formula von mises mean    concentration    parameters
heading change states si sj solution equations 

  tx
 
 
bb  sin rt     t  i  j  i j   t j  i j i   cc

cc
  arctan b
b  tx
 

 cos rt     t  i  j  i j    j  i j i   
 

i j

  

 

t  

   similar approach  termed one step late update  taken others applying em highly non linear optimization problems  mclachlan   krishnan        

   

filearning geometrically constrained hmms

i   i j  
  max
i   i j  

  pt  

 
 
 i  j   cos rt      i j   
t    tp
   
    i  j  
t  

 

    

i  i  modified bessel functions defined equations     section       
again  avoid need solve mutual equations  take advantage lag behind strategy  updating mean using current estimates concentration parameters  i j   j i 
follows 
pt    sin r       i  j      j  i      


i j
j i
i j   arctan ptt  
    
    cos r       i  j      j  i      


i j
j i
t  
calculating new concentration parameters based newly updated mean 
solution equation     use lookup tables 
possible alternative lag behind approach update mean though assumption j i   i j holds  assumption  variance terms equation   cancel out 
mean update independent variance again  variances updated
stated equation     without assuming constraints them  approach taken
earlier stages work  shatkay   kaelbling               lag behind strategy
superior  according experiments  due instance generalized em 

    enforcing additivity

note additivity constraint directly implies two geometrical constraints    thus 
enforcing results complete geometrical consistency  present method directly
enforcing additivity reestimation procedure along x dimensions 
heading dimension describe complete geometrical consistency achieved
projection anti symmetric estimates onto geometrically consistent space  before 
simplify presentation  focus case global coordinate systems  basic
idea applies state relative coordinate systems  relationship used recover mean
ij individual state coordinates complex 
      additivity x  dimensions

main observation underlying approach additivity constraint result
fact states embedded geometrical space  is  assuming n states 
s           sn     points x   axes  x            xn      y            yn                  n    
respectively  state  si   associated coordinates hxi   yi   i  assuming
one global coordinate system  mean odometric relation state si state sj
expressed as  hxj   xi   yj   yi   j   i 
maximization phase em iteration  rather try maximize respect
n   odometric relation vectors  hxij   yij   ij i  reparameterize problem  specifically 
express odometric relation function two n state positions  maximize
respect unconstrained  n state positions  instance  x dimension  rather
search n   maximum likelihood estimates xij   use maximization step find
n   dimensional points  x            xn      calculate xij   xj   xi   moreover  since
interested finding best relationships xi xj   fix one
   f a  a    a  a     a  a g     a  a       f  a  a         a  a    a  b   b  a  g     a  b      b  a   

   

fishatkay   kaelbling

xi  s    e g  x        find optimal estimates remaining n     state positions 
variance reestimation remains before  lag behind policy used eliminate
interdependency update mean variance parameters 
      additive heading estimation

unfortunately  reparameterization described feasible estimation changes
heading  due von mises distribution assumption heading measures  reparameterizing ij j   trying maximize likelihood function respect
parameters  obtain set n   trigonometric equations terms form cos j   sin i  
enable simple solution 
alternative  possible use anti symmetric reestimation procedure described
earlier  followed perpendicular projection operator  mapping resulting headings vector
h             ij           n    n   i    i  j n     satisfy additivity  onto vector
headings within additive linear vector space  simple orthogonal projection satisfactory
within setting  since simply looks additive vector closest non additive one 
procedure ignores fact entries non additive vector based
lot observations  therefore reliable  other  less reliable ones  based
hardly data all  intuitively  would keep estimates well accounted
intact  adapt less reliable estimates meet additivity constraint  precisely 
heading change estimates states better accounted others 
sense transitions
states higher expected counts transition
p
states  higher  i  j     would project non additive heading
estimates vector onto subspace additive vector space  vectors
values non additive
p vector entries well accounted for  is 
highest values  i  j    diculty latter subspace linear vector
space  for instance  satisfy closure scalar multiplication   projection
operator linear spaces cannot applied directly  still  set vectors form
ane vector space  project onto using algebraic technique  explained below  
definition
rn n dimensional ane space vectors va a  set vectors 
def
  va   fua   va jua   ag linear space 
hence  pick vector ane space  va  a  define translation ta     v  
v linear space  v     va   translation trivially extended vector
v    rn   defining ta  v      v    va   order project vector v   rn onto a  apply
translation ta v project ta  v  onto v   results vector p  ta  v   v  
applying inverse transform ta   it  obtain projection v a  demonstrated
figure    linear space figure two dimensional vector space fhx  yij    xg 
ane space fhx  yij    x    g  transform ta consists subtracting vector
h    i  solid arrow corresponds direct projection vector v onto point p  v 
ane space  dotted arrows represent projection via translation v ta  v  
projection latter onto linear vector space  inverse translation result 
p  ta  v    onto ane space 
 

 

 

   many thanks john hughes introducing us technique 

   

filearning geometrically constrained hmms

 
 x  x   
 
p v 

v

 

  

 

  

 
ta  v 
p ta  v  

 x  x 

  

figure    projecting v onto ane vector space fhx  yij    x    g 
although procedure preserving additivity headings formally proven preserve monotone convergence likelihood function towards local maximum  extensive
experiments consisting hundreds runs shown monotone convergence preserved 

  choosing initial model

typically  instances baum welch algorithm  initial model picked uniformly
random space possible models  perhaps trying multiple initial models find different local likelihood maxima  alternative approach reported  shatkay   kaelbling 
      based clustering accumulated odometric information using simple k means
algorithm  duda   hart         taking clusters states observations
recorded  obtain state observation counts estimate model parameters 
perpendicularity assumed collecting data  shown figure    k means
algorithm assigns cluster  state  odometric readings recorded close locations 
leading reasonable initial models  however  assumption dropped  illustrated
figure    cumulative rotational error distorts odometric location recorded within
global coordinate system  location assigned state multiple visits
varies greatly would recognized  the same  simple location based clustering
algorithm  overcome this  developed alternative initialization heuristics  call
tag based initialization  based directly recorded relations states  rather
states  absolute location  clarity  description consists mostly illustrative
example  concentrates case global consistency constraints enforced 
given sequence observations odometric readings e  begin clustering odometric
readings buckets  number buckets number distinct state transitions
recorded sequence  goal stage bucket contain odometric
readings close along three dimensions 
achieve this  start fixing predetermined  small standard deviation value along x 
y  dimensions  denote standard deviation values x     respectively   typically
x      first odometric reading assigned bucket   mean bucket
set value reading  rest process subsequent odometric
readings examined  next reading within     standard deviations along
three dimensions mean existing non empty bucket  add bucket
   

fishatkay   kaelbling

             
               

              
                

              
                

                 
                

  

  

  

  

              

                

                  

                

 

 

 

 

figure    bucket assignment example sequence 
update bucket mean accordingly  not  assign empty bucket set mean
bucket reading 
intuitively  using heuristic resulting buckets tightly concentrated
mean  note clustering algorithms  duda   hart        could used
bucketing stage 
example     would learn   state model sequence odometric readings 
hx  y  follows 
h       i  h         i  h         i  h          i 
h         i  h           i  h           i  h          i  
first stage place readings buckets  suppose standard deviation constant
    placement shown figure    mean value associated bucket shown
well 
 
next stage algorithm state tagging phase  odometric reading 
rt   assigned pair states  si  sj   denoting origin state  from transition took
place  destination state  to transition led   respectively  conjunction 
mean entries  ij   relation matrix  r  populated 

example      cont   returning sequence above  process demonstrated figure     assume data recording starts state    odometric change
self transitions    small standard deviation  we use    well  
shown part figure 
since first element sequence  h       i  two standard deviations away
mean        entry relation row state   populated  pick  
next state populate mean        mean bucket   
h       i belongs  maintain geometrical consistency mean        set         
shown part b figure  populated   off diagonal entries  state
sequence h    i  entry        matrix becomes associated bucket   
information recorded helping tagging future odometric readings belonging
bucket 
next odometric reading  h         i  standard deviations populated mean
row    where   current believed state   hence  pick new state    set mean
         the mean bucket   to reading belongs  figure    c   entry
       recorded associated bucket    preserve anti symmetry additivity        
set                 set sum                         set         
   

filearning geometrically constrained hmms

 
 

 

b
 

 

 

       

 

 
 

       

       

 

 

 

    
           
     
    
    
      

       

       

 
 

       

 

 

       

s   

s      
bucket r           

c
 
 
 
 

 


 

 

 

    
      
     
           
             
      
    
    
             
      
   

 
 

               
      
    
       
           

 

 
 

       

s         

 

 

 

    
               
     
   
           
                  
               
    
    
     
             
      
   
      
               
      
      
    
              
           
     
                          
             
  
    
              
   

s         
bucket r           

bucket r           

s           
bucket r           
      s                          

figure     populating odometric relation matrix creating state tagging sequence 
similarly         updated mean bucket    causing setting                
                        bucket   associated        
stage odometric table fully populated  shown part figure     state
sequence point is  h          i  next reading  h           i  within one standard
deviation        therefore next state    entry        associated bucket   
 the bucket reading assigned   state sequence becomes  h             i 
next reading  bucket    associated relation state   tagged
bucket    namely  state    repeating last two readings  final state transition
sequence becomes h                         i 
 
note process described illustration simplified  general case 
need take account rotational error data  use state relative coordinate
systems  therefore populate entries transformed anti symmetry additivity
constraints 
hx yi a  b     tba  hx yi b  a    
hx yi a  c    hx yi a  b    tba  hx yi b  c   
defined section       
   

fishatkay   kaelbling

possible end tagging algorithm  rows columns relation
matrix still unpopulated  happens little data learn
number states provided algorithm large respect actual model 
cases either  trim  model  using number populated rows number
states  pick random odometric readings populate rest table  improving
estimates later  note first approach suggests method learning number states
model given  starting gross over estimate number  truncating number populated rows odometric table initialization performed 
state transition sequence obtained  rest initialization algorithm
k means based initialization  deriving state transition counts state transition
sequence  assigning observations states assumption state sequence
correct  obtaining state transition observation probabilities  initialization phase
incur much computational overhead  equivalent time wise performing one
additional iteration em procedure 

  experiments results

goal work described far use odometry improve learning topological
models  using fewer iterations less data  tested algorithm simple robotnavigation world  experiments consist running algorithm data obtained
simulated model data gathered mobile robot  ramona  amount
data gathered ramona used proof concept sucient statistical
analysis  latter  use data obtained simulated model  gathered data
used algorithms without perpendicularity assumption  see section        
results provided settings 

    robot domain

robot used experiments  ramona  modified rwi b   robot  cylindrical
synchro drive base     ultrasonic sensors    infrared sensors  situated evenly around
circumference  infrared sensors used mostly short range obstacle avoidance 
ultrasonic sensors longer ranged  used obtaining  noisy  observations
environment  experiments described here  robot follows prescribed path
corridors oce environment department  thus  decision making
involved  hmm sucient model  rather complete pomdp 
low level software  provides level abstraction allows robot move hallways
intersection intersection turn ninety degrees left right  software
uses sonar data distinguish doors  openings  intersections along path  stop
robot s current action whenever landmark detected  stop either due
natural termination action due landmark detection is considered robot
 state  
stop  ultrasonic data interpretation allows robot perceive  three
cardinal directions   front  left right   whether open space  door  wall 
something unknown 
encoders robot s wheels allow estimate pose  position orientation  respect pose previous intersection  recording sonar based observations
   low level software written maintained james kurien 

   

filearning geometrically constrained hmms
 

 

 

 

 

 
 

 

  
  

  
  

 
 

  

  

   
     
  

  
 
  

  

 
 

 

 

   
  

     
  

  
  

  
  

  
  
  

  
  
  

  

     

     

  

 
  

  

  

  
  

  

  
  

  

figure     true model corridors ramona traversed  arrows represent prescribed path direction 

figure     true model prescribed path
simulated hallway environment 

odometric information  robot goes execute next prescribed action 
action command issued manually human operator  course  action performance perception routines subject error  path ramona followed consists
  connected corridors building  include    states  shown figure    
simulation  manually generated hmm representing prescribed path robot
complete oce environment department  consisting    states 
associated transition  observation  odometric distributions  transition probabilities
ect action failure rate          is  probability moving
current state correct next state environment  predetermined action
           probability self transition typically           
small probability  typically smaller       sometimes assigned transitions 
experience real robot proves reasonable transition model  since
typically robot moves next state correctly  error occurs
significant frequency move all  due sonar interpretation indicating
barrier actually none  action command repeated robot usually
performs action correctly  moving expected next state  observation distribution
typically assigns probabilities             true observation perceived
robot state  probabilities             observations might
perceived  example  door actually perceived  door typically assigned
probability           wall assigned probability          open space assigned
probability      perceived  standard deviation around odometric readings
   mean 
figure    shows hmm corresponding simulated hallway environment  observations
orientation omitted figure clarity  nodes correspond states
environment  directed edges correspond corridors  arrows point direction
corridors traversed  interpretation figures provided
following section 
   

fishatkay   kaelbling

    evaluation method

number different ways evaluating results model learning algorithm 
none completely satisfactory  give insight utility results 
domain  transitions observations usually take place  therefore
likely others  furthermore  relational information gives us rough estimate
metric locations states  get qualitative sense plausibility learnt
model  extract essential map learnt model  consisting states 
likely transitions metric measures associated them  ask whether map
corresponds essential map underlying true world 
figures       essential versions true models  figures        shown
later  essential versions representative learnt ones  obtained sequences gathered
perpendicularity assumption   black dots represent physical locations states 
state assigned unique number  multiple state numbers associated single
location typically correspond different orientations robot location  larger
black circle represents initial state  solid arrows represent likely non self transitions
states  dashed arrows represent transitions probability    
higher  typically  due predetermined path taken  connectivity
modeled environment low  therefore transitions represented dashed arrows
almost likely likely ones  note length arrows  within plot 
significant represents length corridors  drawn scale 
important note figures provide complete representation models 
first  lack observation orientation information  stress fact figures
serve visual aid plot true model  looking good topological
model rather geometrical model  figures provide geometrical embedding
topological model  however  even geometry  described relation matrix 
different  topology  described transition observation matrices  still valid 
traditionally  simulation experiments  learnt model quantitatively compared
actual model generated data  models induces probability distribution
strings observations  asymmetric kullback leibler divergence  kullback   leibler 
      two distributions measure good learnt model respect
true model  given true probability distribution p   fp         pn g learnt one
q   fq        qn g  kl divergence q respect p is 

d p jjq   

def

n
x
i  

pi log  pqi  


report results terms sampled version kl divergence  described juang
rabiner         based generating sequences sucient length    sequences     
observations case  according distribution induced true model  comparing
log likelihood according learnt model true model log likelihood  total
difference log likelihood divided total number observations  accumulated
sequences  giving number roughly measures difference log likelihood
per observation  formally stated  let m  true model m  learnt one  generating
k sequences s            sk   length   true model  m    sampled kl divergence 
ds is 
k
x
 log pr si jm       log pr si jm     

  
ds m  jjm     
 
kt
   

filearning geometrically constrained hmms
    
    

   
    

   

                      

    

    

   

    
   

     
   

   

   

   

   

     

    

figure     sequence gathered ramona 
perpendicularity assumed 

figure     sequence generated simulator  perpendicularity assumed 

ignore odometric information applying kl measure  thus allowing comparison
purely topological models learnt without odometry 

    results within global framework

let ramona go around path depicted figure    collect sequence
    observations  assuming perpendicularity environment  is  every turning
point angle turn      thus turn ramona realigns odometric readings
initial x axes  figure    plots sequence metric coordinates  gathered
way  accumulating consecutive odometric readings  projected hx  yi  applied
learning algorithm data    times     runs started k means based
initial model     started tag based initial model     started random initial
model  addition ran standard baum welch algorithm  ignoring odometric
information     times   note non determinism even using biased initial
models  since k means clustering starts random seeds  low  random noise added
data algorithms avoid numerical instabilities  thus multiple runs give multiple
results   report results obtained using tag based method 
appropriate initialization method general case  results contrasted
obtained odometric information used all  comparison four settings
reader referred complete report work  shatkay        
figure    shows essential representations typical learnt models starting tag based
initial model  geometry learnt model strongly corresponds true environment  states  positions learnt correctly  although figure
show it  learnt observation distributions state usually match well true
observations 
demonstrate effect odometry quality learnt topological model  contrast
plotted models learnt using odometry representative topological model learnt without
   random number   cm  cm added recorded distances typically several meters
long 

   

fishatkay   kaelbling
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

  
 
 
  
  

 
  

  

 

  
 
  
  

  
  

  

  

  

  

  
  

 

  
  

 

mona traversed 

 

 

  

figure     learnt model corridors ra  

  

figure     topology model learnt
without use odometry 

use odometric information  figure    shows topology typical model learnt without
use odometric information  case  arcs represent topological relationships 
length meaningful  initial state shown bold circle  clear
topology learnt match characteristic loop topology true environment 

obtaining statistically sucient information  generated   data sequences  length
      using monte carlo sampling hidden markov model whose projection shown
figure     one sequences depicted figure     figure demonstrates
noise model used simulation indeed compatible noise pattern associated
real robot data  used four different settings learning algorithm 

starting biased  tag based  initial model using odometric information 
starting biased  k means based  initial model using odometric information 
starting initial model picked uniformly random  using odometric information 
starting random initial model without using odometric information  standard baumwelch  

sequence four algorithmic settings ran algorithm    times 
keep discussion focused  concentrate first last settings
reader referred extensive report  shatkay        complete discussion 
experiments  n set      correct  number states  generalization  necessary use cross validation regularization methods select model
complexity  section   suggests one possible heuristic obtaining estimate number
states 
figure    shows essential version one learnt model  obtained sequence shown
figure     using tag based initialization  note learnt model completely
   

filearning geometrically constrained hmms
  
  

  
     

  
  

  
  
  
  

 

 

 

  

 
 

        

 
     
  
  

  

   
   

  
     

  
  

     

     

  
  

  
  

  

figure     learnt model simulated hallway environment 
accurate respect true model  however  obvious correspondence
groups states learnt true models  transitions  as well
observations  shown  learnt correctly  quality geometry
learnt model simulated large environment varies  geometrical results
uniformly good case learning smaller environment real robot data 
environment gets large  global relations remote states  ected
geometrical consistency constraints  become harder learn  still  topology
learnt model demonstrated statistical experiments good 
table   lists kl divergence true learnt model  well number
runs convergence reached    sequences setting
uses odometric information tag based initialization learning algorithm
use odometric information  averaged    runs per sequence  stress kl
divergence measure calculated based new data sequences generated true
model  described section        sequences models learnt
participate testing process 
kl divergence respect true model models learnt using odometry     
times smaller models learnt without odometric data  standard deviation around
means     kl distances models learnt odometry     noodometry setting  check significance results used simple two sample t test 
models learnt using odometric information statistically significantly  p         lower
average kl divergence others 

seq   
kl
odo iter  

kl
odo iter  

 
     
     
     
     

 
     
     
     
     

 
     
     
     
     

 
     
     
     
     

 
     
     
     
     

table    average results two learning settings five training sequences 
   

fishatkay   kaelbling

addition  number iterations required convergence learning using odometric
information roughly     times smaller required ignoring information 
again  t test verifies significance result 
three initialization settings  models learnt topologically somewhat inferior  and
high statistical significance   terms kl divergence  learnt without
enforcing additivity  reported earlier papers  shatkay   kaelbling               likely
result strong constraints enforced learning process  prevent
algorithm searching better areas learning space  restrict reach poor local
maxima  geometry looks superior cases  significantly better  however 
seems less variability quality geometrical models across multiple runs
additivity enforced 
details extensive comparison different initialization methods
beyond scope paper  point studies small large models
show large models long data sequences involved  random initialization often
results lower kl divergence tag based initialization 
strong bias tag based initialization  lead peaked models compared
less peaked distributions associated true model  random initialization leads atter
models  kl divergence strongly penalizes models much peaked
true ones  randomly initialized models often closer  terms measure  true
models peaked ones learnt initial models  learning small models 
sucient training data available  tag based initialization results models
clearly superior random ones  again  reader referred complete report
work  shatkay        comparative study initialization methods various
settings 

    results within relative framework

applied algorithm described section      extended accommodate state relative
constraints  as listed section         data used gathered robot
environment  generated simulated model  figures         
however  data generated without assuming perpendicularity  means x
coordinates realigned turn global x axes  rather 
recorded  as is   evaluation methods stay described above 
figure    shows projection odometric readings ramona recorded along
x dimensions  traversing environment  obtaining statistically sucient
information  generated   data sequences  length      using monte carlo sampling
hidden markov model whose projection shown figure     one sequences
depicted figure    
figure    shows typical model obtained applying algorithm enforcing complete
geometrical consistency  robot data shown figure     using tag based initialization 
note rectangular geometry environment preserved  although state  
participate loop  explained observing corresponding area true
environment depicted figure     consisting   states clustered bottom left
corner                 due relatively large number states close together
area true environment  recognized ever returned particularly
state   loop  therefore  one transition recorded state   state
   

filearning geometrically constrained hmms
    

    

    

    

    

   

    

     

     

    

   

    

    

   

     

     

                            

   

    

figure     sequence gathered ramona 

figure     sequence generated simula 

perpendicularity assumed 

tor  perpendicularity assumed 
  

  

  

 
  

  

 

  

 
 
 

 

  

 
 
 

 

figure     learnt model corridors ramona traversed  initialization tag based 
  according expected transition counts calculated algorithm  projecting
angles maintain additivity   as described section         angle state    
therefore compromised  allowing geometrical consistency maintain rectangular geometry
among regularly visited states 
purpose quantitatively evaluating learning algorithm list table   kl
divergence true learnt model  well number iterations convergence reached    simulation sequences with without odometric information 
averaged    runs per sequence  table demonstrates kl divergence respect true model models learnt using odometric data    times smaller
models learnt without it  check significance results use simple
two sample t test  models learnt using odometric information highly statistically significantly  p         lower average kl divergence others  addition  number
   

fishatkay   kaelbling

seq   
kl
odo iter  

kl
odo iter  

 
 
 
 
 
                        
                        
                          
                             

table    average results   learning settings   training sequences 
iterations required convergence learning using odometric information smaller
required ignoring information  again  t test verifies significance  p         
result 
important point number iterations  although much lower  automatically imply algorithm runs less time non odometric baum welch 
major bottleneck caused need compute within forward backward calculations 
described section        values normal von mises densities  require calculation exponent terms rather simple multiplications  slowing
iteration  current nave implementation  however  solve augmenting
program look up tables obtaining relevant values rather calculating them 
addition  take advantage symmetry relations table cut
amount calculation required  possible use fact many odometric relations remain unchanged  particularly later iterations algorithm  one iteration
next  therefore values cached shared iterations rather
recalculated iteration 

    reducing amount data
learning hmms obviously requires visiting states transitioning multiple times 
gather sucient data robust statistical estimation  intuitively  exploiting odometric data
help reduce number visits needed obtaining reliable model 
examine uence reduction length data sequences quality learnt
models  took one   sequences used prefixes length          the complete
sequence   increments      training sequences  ran two algorithmic settings
  prefix sequences     times repeatedly  used kl divergence described
evaluate resulting models respect true model  prefix
length averaged kl divergence    runs 
plot figure    depicts average kl divergence function sequence length
two settings  demonstrates that  terms kl divergence  algorithm 
uses odometric information  robust face data reduction   down     data
points   contrast  learning without use odometry quickly deteriorates amount
data reduced 
note data sequence twice  wide  odometry used
not  is  information element sequence odometry data
recorded  however  effort recording additional odometric information negligible 
well rewarded fact fewer observations less exploration required
obtaining data sequence sucient adequate learning 
   

filearning geometrically constrained hmms
  

  

  

odometry

kl
  

  
odometry used
 

   

   
seq  length

   

   

figure     average kl divergence function sequence length 

  conclusions
odometric information  often readily available robotics domain  makes possible
learn hidden markov models eciently effectively  using shorter training sequences 
importantly  contrast traditional perception viewing topological
geometric models two distinct types entities  shown odometric information
directly incorporated traditional topological hmm model  maintaining
convergence reestimation algorithm local maximum likelihood function 
method uses odometric information two ways  first choose initial model 
based odometric information  iterative procedure  extends baum welch
algorithm  used learn topological model environment learning
additional set constrained geometric parameters  additional set constrained parameters constitutes extension basic hmm pomdp model transitions observations 
even though primarily interested underlying topological model  transition
observation probabilities   experiments demonstrate use odometric relations
reduce number iterations amount data required algorithm  improve
resulting model 
initialization procedure enforcement additivity constraint relatively
small models prove helpful topologically geometrically  extensive study  shatkay 
      shows long data sequences  generated large models  enforcing antisymmetry rather additivity  leads better topological models 
cases  initialization always good  additivity may over constrain learning
unfavorable area  learning large models may benefit enforcing anti symmetry
first iterations  complete additivity later iterations  alternatively  may use
algorithm  enforcing additivity  learn separate models small portions environment 
combining later one complete model  similar idea combining small modelfragments complete map environments applied  context geometrical
maps  recent work leonard feder        
   

fishatkay   kaelbling

work presented demonstrates domain specific information constraints
enforced part statistical estimation process  resulting better models  requiring
shorter data sequences  strongly believe idea applied domains
robotics  particular  acquisition hmms use molecular biology may greatly benefit
exploiting geometrical  and other  constraints molecular structures  similarly  temporal
constraints may exploited domains pomdps appropriate decision support 
air trac control medicine 

acknowledgments
thank sebastian thrun insightful comments throughout work  john hughes luis ortiz
helpful advice  anthony cassandra code generating random distributions  bill smart
sustaining ramona jim kurien providing low level code driving her  presentation
paper benefited comments made anonymous referees grateful 
work done authors computer science department brown university 
supported darpa rome labs planning initiative grant f                 nsf grants
iri         iri          brown university graduate research fellowship 

   

filearning geometrically constrained hmms

appendix a  overview odometric learning algorithm
algorithm takes input experience sequence e   hr  v i  consisting odometric
sequence r observation sequence v   defined beginning section     
number states assumed given 
learn odometric hmm e 
  initialize matrices a  b  r
 see section   
  max change  
    max change    
  calculate forward probabilities 
 equation   
 
calculate backward probabilities 
 equation   
 
calculate state occupation probabilities   equation   
 
calculate state transition probabilities     equation   
 
old a  old b b
 
reestimate  a 
 equation    left 
  
b reestimate  b  
 equation    right 
  
r reestimate  r  
 equations       
x

x

  
hr   r reestimate r   r    equations       
  
max change max get max change a  old   
get max change b  old b   
equations referenced step    correspond updates perpendicularity assumption  global framework used  see  shatkay        update formulae within
state relative framework 
additivity enforced  step    followed projection reestimated r onto additive
ane space  described section        addition  step    substituted procedure
described section        reader referred  shatkay        detail 
get max change function takes two matrices returns maximal element wise
absolute difference them  constant set denote margin error changes
parameters  change parameters  small enough   model regarded
 unchanged  

   

fishatkay   kaelbling

references
abe  n     warmuth  m  k          computational complexity approximating distributions probabilistic automata  machine learning                 
angluin  d          learning regular sets queries counterexamples  information
computation             
asada  m          map building mobile robot sensory data  iyengar  s  s    
elfes  a   eds    autonomous mobile robots  pp           ieee computer society press 
bartels  r          estimation bidirectional mixture von mises distributions  biometrics 
            
basye  k   dean  t     kaelbling  l  p          learning dynamics  system identification
perceptually challenged agents  artificial intelligence         
baum  l  e   petrie  t   soules  g     weiss  n          maximization technique occurring
statistical analysis probabilistic functions markov chains  annals
mathematical statistics                  
cassandra  a  r   kaelbling  l  p     kurien  j  a          acting uncertainty  discrete
bayesian models mobile robot navigation  proceedings ieee rsj international
conference intelligent robots systems 
degroot  m  h          probability statistics   nd edition   addison wesley 
dempster  a  p   laird  n  m     rubin  d  b          maximum likelihood incomplete
data via em algorithm  journal royal statistical society               
dissanayake  g   newman  p   clark  s   durrant whyte  h  f     csorba  m          solution
simultaneous localization map building  slam  problem  ieee transactions
robotics automation         
duda  r  o     hart  p  e          unsupervised learning clustering  chap     john wiley
sons 
elfes  a          using occupancy grids mobile robot perception navigation  computer 
special issue autonomous intelligent machines                
engelson  s  p     mcdermott  d  v          error correction mobile robot map learning 
proceedings ieee international conference robotics automation  pp 
           nice  france 
gold  e  m          complexity automaton identification given data  information
control              
gumbel  e  g   greenwood  j  a     durand  d          circular normal distribution 
theory tables  american statistical society journal              
hopcroft  j  e     ullman  j  d          introduction automata theory  languages 
computation  addison   wesley 
   

filearning geometrically constrained hmms

juang  b  h          maximum likelihood estimation mixture multivariate stochastic observations markov chains  at t technical journal         
juang  b  h     rabiner  l  r          probabilistic distance measure hidden markov
models  at t technical journal                  
koenig  s     simmons  r  g       a   passive distance learning robot navigation 
proceedings thirteenth international conference machine learning  pp      
    
koenig  s     simmons  r  g       b   unsupervised learning probabilistic models robot
navigation  proceedings ieee international conference robotics automation 
kuipers  b     byun  y  t          robot exploration mapping strategy based semantic hierarchy spatial representations  journal robotics autonomous systems 
         
kullback  s     leibler  r  a          information suciency  annals mathematical
statistics                
leonard  j   durrant whyte  h  f     cox  i  j          dynamic map building autonomous mobile robot  iyengar  s  s     elfes  a   eds    autonomous mobile robots 
pp           ieee computer society press 
leonard  j  j     feder  h  j  s          computationally ecient method large scale concurrent mapping localization  hollerbach  j     kodischek  d   eds    proceedings
ninth international symposium robotics research 
liporace  l  a          maximum likelihood estimation multivariate observations markov
sources  ieee transactions information theory         
mardia  k  v          statistics directional data  academic press 
mataric  m  j          distributed model mobile robot environment learning navigation  master s thesis  mit  artificial intelligence laboratory 
mclachlan  g  j     krishnan  t          em algorithm extensions  john wiley  
sons 
moravec  h  p          sensor fusion certainty grids mobile robots  ai magazine        
      
moravec  h  p     elfes  a          high resolution maps wide angle sonar  proceedings
international conference robotics automation  pp          
nourbakhsh  i   powers  r     birchfield  s          dervish  oce navigating robot  ai
magazine                
pierce  d     kuipers  b          map learning uninterpreted sensors effectors  artificial intelligence                    
   

fishatkay   kaelbling

rabiner  l  r          tutorial hidden markov models selected applications speech
recognition  proceedings ieee                  
rivest  r  l     schapire  r  e          diversity based inference finite automata 
proceedings ieee twenty eighth annual symposium foundations computer
science  pp         los angeles  california 
rivest  r  l     schapire  r  e          inference finite automata using homing sequences 
proceedings twenty first annual symposium theory computing  pp          
seattle  washington 
ron  d   singer  y     tishbi  n          learning probabilistic automata variable memory length  proceedings seventh annual workshop computational learning
theory  pp        
ron  d   singer  y     tishbi  n          learnability usage acyclic probabilistic
finite automata  proceedings eighth annual workshop computational learning
theory  pp        
ron  d   singer  y     tishby  n          learnability usage acyclic probabilistic
finite automata  journal computer systems science         
shatkay  h          learning models robot navigation  ph d  thesis  department computer science  brown university  providence  ri 
shatkay  h     kaelbling  l  p          learning topological maps weak local odometric
information  proceedings fifteenth international joint conference artificial
intelligence  nagoya  japan 
shatkay  h     kaelbling  l  p          heading right direction  proceedings
fifteenth international conference machine learning  madison  wisconsin 
simmons  r  g     koenig  s          probabilistic navigation partially observable environments  proceedings international joint conference artificial intelligence 
smith  r   self  m     cheeseman  p          stochastic map uncertain spatial relationships  iyengar  s  s     elfes  a   eds    autonomous mobile robots  pp           ieee
computer society press 
thrun  s          learning metric topological maps indoor mobile robot navigation  ai
journal           
thrun  s     bucken  a       a   integrating grid based topological maps mobile robot
navigation  proceedings thirteenth national conference artificial intelligence 
pp          
thrun  s     bucken  a       b   learning maps indoor mobile robot navigation  tech  rep 
cmu cs         school computer science  carnegie mellon university  pittsburgh 
pa 
thrun  s   burgard  w     fox  d       a   probabilistic approach concurrent map acquisition localization mobile robots  machine learning            
   

filearning geometrically constrained hmms

thrun  s   gutmann  j  s   fox  d   burgard  w     kuipers  b  j       b   integrating topological metric maps mobile robot navigation  statistical approach  proceedings
fifteenth national conference artificial intelligence  pp          
vapnik  v  n          nature statistical learning theory  springer 

   


