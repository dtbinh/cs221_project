journal of artificial intelligence research                

submitted        published      

automatic wordnet development for low resource
languages using cross lingual wsd
nasrin taghizadeh

nsr taghizadeh ut ac ir

school of electrical and computer engineering
college of engineering  university of tehran  tehran  iran

hesham faili

hfaili ut ac ir

school of electrical and computer engineering
college of engineering  university of tehran  tehran  iran

abstract
wordnets are an eective resource for natural language processing and information
retrieval  especially for semantic processing and meaning related tasks  so far  wordnets
have been constructed for many languages  however  the automatic development of wordnets for low resource languages has not been well studied  in this paper  an expectationmaximization algorithm is used to create high quality and large scale wordnets for poorresource languages  the proposed method benefits from possessing cross lingual word sense
disambiguation and develops a wordnet by only using a bi lingual dictionary and a monolingual corpus  the proposed method has been executed with persian language and the
resulting wordnet has been evaluated through several experiments  the results show that
the induced wordnet has a precision score of     and a recall score of     

   introduction
one of the most important projects in natural language processing over the years has been
the construction of an english wordnet  wordnet  at princeton university under the direction of george a  miller         wordnet consists of a lexical database  in which english
words are grouped into sets of cognitive synonyms called synsets  the eectiveness of wordnet in a wide range of language technology applications inspired many researchers to create
wordnets for other languages  the first attempts at this led to the construction of eurowordnet  vossen        and balkanet  tufis  cristea    stamou         eurowordnet
deals with european languages such as english  dutch  german  french  spanish  italian 
czech and estonian  while balkanet covers languages from the balkan zone  to interconnect wordnets of dierent languages  eurowordnet links synsets of each language to an
interlingual index  ili   the ili allows it to find equivalent synsets across all languages
connected to the ili 
although the first wordnet was created manually  several automatic and semi automatic
techniques have been used for developing the other wordnets  these methods are usually
divided into merge and expansion approaches  fellbaum   vossen        oliver   climent        erjavec   fiser         however  there are methods that combine the merge
and expansion models and benefit from the advantages of both approaches  prabhu  desai 
redkar  prabhugaonkar  nagvenkar    karmali        apidianaki   sagot         with
the merge approach  a small wordnet is created manually  which contains high level and
c
    
ai access foundation  all rights reserved 

fitaghizadeh   faili

basic concepts  next  this small wordnet is developed using automatic and semi automatic
techniques  in this process  mono lingual resources and language specific properties are
employed  wordnets created in this manner later are mapped onto either the wordnet or
ili  when using the expansion approach  a multilingual wordnet is constructed by translating words inside the synsets of the wordnet  or other existing wordnets  into the target
language using multi lingual resources  therefore the structure of the original wordnet is
preserved and the words are translated 
among the dierent methods proposed for wordnet construction  few of them are applicable to low resource languages  methods that follow the merge approach are labourintensive and time consuming  moreover  they need to have vast knowledge about the language and also require many resources  which is the main obstacle of low resource languages
  so it makes this approach inapplicable for them in practice  on the other hand  methods
that follow the expansion approach usually adopt wordnet structure and find the correct
translation of the associated words with the wordnet synsets in the target language  in
this process  multilingual resources such as comparable corpora  kaji   watanabe        
parallel corpora  oliver   climent        kazakov   shahid        fiser        diab        
thesaurus  gunawan   saputra         machine translators  saveski   trajkovski       
and multiple bi lingual machine readable dictionaries  atserias  climent  farreres  rigau 
  rodrguez        patanakul   charnyote        bond  isahara  kanzaki    uchimoto 
      lam  al tarouti    kalita        are used  which causes a bottleneck for low resource
languages 
taking a deeper look at the expansion based methods  each synset from the wordnet
is kept and words associated with it are translated into the target language  a bi lingual
dictionary is usually employed and english words inside the wordnet synsets are translated 
since dictionaries do not translate word sense to word sense  but rather word to word 
translations are ambiguous and should be disambiguated  looking more carefully  after
translating english words inside a wordnet synset  a set of candidate words in the target
language is obtained  some of these are equivalent to the other senses of the english words
and should thus be omitted  methods that following the expansion approach rank the
candidate words and omit low rated ones from the candidate sets  if the task of scoring
candidate words for the wordnet synsets is considered to be an optimization problem   sub optimal values can be found using algorithms such as expectation maximization  montazery
  faili         the proposed method is an extension of this work for low resource languages 
in this paper  the problem of automatically constructing large scale and high quality
wordnets for low resource languages is studied  between the two major approaches  merge
and expansion  the first one is not suitable  because it requires vast knowledge about the
target language and also many language resources  so the preferred approach is to utilize wordnets in other languages by adopting their structure and translating their content 
finding the correct senses of the target language words is an ai complete problem  mallery 
       that is  by analogy to the np completeness in the complexity theory  it is a problem
whose diculty is equivalent to solving the central problems of ai  navigli         in this
paper  an iterative optimization method based on cross lingual wsd is proposed to find
the local optimum of the problem in a reasonable time  the main idea is to iteratively
improve the estimation of the probability of selecting wordnet synsets for the words of the
target language  additionally  the proposed method needs few resources and so it is suitable
  

fiautomatic wordnet development for low resource languages

for poor resource languages  to investigate the performance of the proposed method  persian has been selected as a poor resource language and the resulting wordnet is examined
through conducting several experiments 
the roadmap of the paper is as follows  section   presents related works  section  
explains the wordnet construction problem and the proposed formulation  section   presents
a case study of the persian language and error analysis  and conclusions are given and future
works suggested in the last section  section   

   related work
in this section  some automatic methods for constructing wordnets are reviewed that are
based on the expansion approach  the main stage of the expansion based methods is finding
a set of words that lexicalizes the concept captured by a given synset of an existing wordnet
in another language  all candidate words are usually extracted by a dictionary and a scoring
system is utilized to find the correct words 
in the work of kaji and watanabe         the gloss information in wordnet has been
used for the automatic construction of a japanese wordnet  given an english synset  it
calculates a score for each of its japanese translation candidates according to the gloss
appended to the synset  the score is defined as the sum of correlations between the translation candidates and the associated words that appear in the gloss  a pair of words are
deemed associated if the amount of mutual information between them is above a predefined threshold  since the availability of bi lingual corpora is limited  an iterative approach
has been proposed for calculating pair wise correlations 
another study on creating a wordnet by automatically expanding wordnet describes a
romanian wordnet  in the work of barbu and barbu mititelu         in order to identify
the romanian words corresponding to a wordnet synset  several heuristics have been proposed  according to the first heuristic  words related to a synset share a common meaning 
therefore  the intersection of translations of words associated with the wordnet synsets
is considered  the second heuristic states that a synset and its hypernym share the same
meaning  therefore  the intersection of word translations from a given wordnet synset
and its hypernym is selected as a romanian synset  according to the third heuristic  those
translations that have the same domain label are selected for a given wordnet synset  by
the fourth heuristic  a romanian word is selected if english translations of words based on
its definition have maximum similarity with words in the gloss of the given synset 
in the research conducted by patanakul and charnyote         a semi automatic expanding approach has been presented to construct a thai wordnet  candidates for links
between thai words and wordnet synsets have been derived from wordnet and its translations  to rank these links     criteria are used that have been categorized into three groups 
monosemic  polysemic  and structural criteria  monosemic criteria focus on english words
that have only one meaning and assume that such english words have only one synset in
the wordnet  polysemic criteria focus on english words that have multiple meanings  and
believe that such english words have multiple synsets in the wordnet  structural criteria
focus on structural relations among synsets with respect to the wordnet     
another idea for creating wordnet is to use a word aligned parallel corpus with n languages  annotate each word with a lexical sense tag that consists of the n tuple of aligned
  

fitaghizadeh   faili

words  as a result  all occurrences of a given word in the text for language l are considered
to have the same sense  provided they are tagged with the same multi lingual synset  however  this kind of corpus is not easily available in most languages  in the research  which was
conducted by oliver and climent         two strategies for the automatic construction of
these corpora are proposed   i  by the machine translation of sense tagged corpora  and  ii 
by the automatic sense tagging of bi lingual word aligned corpora  the results for spanish
language showed that the first strategy works better than the second  this suggests that
lexical selection errors made by the machine translation systems are less important than
the sense tagging errors 
in the babelnet project  which was undertaken by navigli and ponzetto            a   a
very large multi lingual semantic network was constructed  in this project  original wordnet
was used as its lexicographic resource as well as wikipedia pages in dierent languages for
its encyclopedic knowledge  first a mapping between the english wikipedia pages and
the synsets in the original wordnet was established  given a wikipedia page w and its
mapping  a babel synset was created using the wordnet synset s  page w  all inter language
links  which are translation of w to the other languages  in this project  the coverage of the
resulting network has been analyzed by comparing it with the gold standard wordnets in
terms of synset coverage  word coverage  and synset extra coverage  the results show that
the synset coverage varies for dierent languages from     for italian to     for french 
in the work of bond and foster         an open multi lingual wordnet for more than
eighty languages was developed  in this project  a common interface for accessing multiple
wordnets was created through gathering existing freely available wordnets of dierent languages and automatically linking them to the wordnet  next  the wordnets were extended
using the unicode common locale data repository  ucldr  and wiktionary  to rank
candidate links between wordnet synsets and wiktionary  several similarity measures were
employed  the results show that the precision score was         when measured on sense 
an arabic wordnet was created that follows the eurowordnet methodology of manually
encoding a set of base concepts while maximizing compatibility across arabic and english
wordnets  black  elkateb    vossen        elkateb  black  rodrguez  alkhalifa  vossen 
pease    fellbaum         next  in the project  which was performed by rodrquez et al 
        a machine learning algorithm was employed for extending the arabic wordnet and
augmenting formal specification to the senses of its synsets  in order to associate arabic
words with the wordnet synsets  a bayesian network with four layers was proposed  four
layers respectively represent  arabic words  the corresponding english translation of these
arabic words in the first layer  all the synsets of the english words in the second layer  and
other wordnet synsets linked to the synsets in layer three  a set of candidates word synset
is built with pairs  x  y   where x is an arabic word and y is a wordnet synset in the third
layer of the bayesian network that has a non null probability and so there is a path from
x to y  the score of each link is calculated with the posterior probability of y  given the
evidence provided by the network  only the tuples that score over a threshold are selected
for inclusion in the final set of candidates word synset  the best results of the method
proposed in this study noted a score of     precision 
in the work of boudabous et al          an arabic wordnet was enriched via adding
semantic relations between synsets  the method consisted of two main phases  the first
phase consisted of defining morpho lexical patterns using a study corpora extracted from
  

fiautomatic wordnet development for low resource languages

the arabic wikipedia  the second phase consisted of using morpho lexical patterns  defined
in the previous phase  in order to extract new semantic relations from the arabic wikipedia 
extracted relations were validated  then added to the arabic wordnet data base 
piasecki et al         proposed an algorithm for automatically expanding the polish
wordnet  this method uses heterogeneous knowledge sources  which are extracted from a
large corpus  and combines them based on a weighted voting scheme  this method extracts
potential instances of lexicon semantic relations from a corpus and measures the semantic
similarity of lexical units  it analyzes the eect of using dierent knowledge resources on
the performance of the algorithm  due to the high accuracy of the results  this approach
can be said to be a good basis for semi automatic methods of constructing wordnets using
human knowledge to correct the output of the automatic approaches 
lam et al         proposed an automatic method for constructing wordnet synsets that
uses the publicly available wordnets  a machine translator and bi lingual dictionaries  for
this purpose  each synset of an existing wordnet is translated into the target language 
then a ranking method is applied to the resulting translation candidates to find the best
translations  to generate candidate synsets  three approaches were proposed  the first one
directly translates synsets in wordnet into the target language  the second one uses intermediate wordnets to handle ambiguities in synset translations  in the case of dictionaries
being available  in addition to the wordnets in the intermediate languages  a third approach
can be used  the experimental results showed that the resulting wordnets have a coverage
of                    and     for karbi  arabic  assamese  dimasa and vietnamese
languages  respectively 
in the project  which was conducted by hasanuzzaman et al          a method for
constructing a tempo wordnet was suggested  according to this method  the wordnet
was augmented with temporal information by following a two step process  in the first
step  synsets of the wordnet are classified as atemporal or temporal  next  all synsets are
associated with past  present and future probabilities  the obtained tempo wordnet can
be used in time related applications 
in the work of shamsfard         a semi automated method was proposed for developing a persian lexical ontology called farsnet  about       verbs and       nouns were
gathered manually to make the wordnets core  after that  two heuristics and a word sense
disambiguation  wsd  method were used to find the most likely related persian synsets 
a practical evaluation of the proposed automatic method used in this studt shows a score
of     correctness and covers about       entries on wordnet  the extension of this work
 shamsfard  hesabi  fadaei  mansoory  famian  bagherbeigi  fekri  monshizadeh    assi 
    a   is known as being the first published persian wordnet  farsnet  which contains
about        persian words and covers about       wordnet synsets 
in the research  which was performed by montazery and faili         an automatic
approach for persian wordnet construction based on the wordnet has been introduced 
the proposed approach uses two mono lingual corpora for english and persian  and a bilingual dictionary in order to construct mapping between wordnet synsets and persian
words using two dierent methods  some links were selected directly by using heuristics
that recognize these links as unambiguous  other types of links are ambiguous  in which a
scoring method is used to select the appropriate synset  the practical evaluation of the links
for     randomly selected persian words shows about       quality in terms of accuracy 
  

fitaghizadeh   faili

by augmenting the persian wordnet with unambiguous words  the total accuracy of the
automatically extracted persian wordnet becomes       

   iterative method for wordnet construction
to construct a multi lingual wordnet  several methods have been presented  however  few
of them have paid attention to low resource languages  creating a wordnet from scratch for
such languages is a time consuming and expensive process  instead  new wordnets could be
developed by adopting the structure of existing wordnets in other languages  usually wordnet  and translating the words associated with their synsets into the target language  one
important advantage of this approach is that the resulting wordnet is aligned to the wordnet and the ili  and thus is interesting for contrastive semantic analysis and is particularly
useful in multi lingual tasks such as multi lingual information retrieval  dini  peters  liebwald  schweighofer  mommers    voermans        otegi  arregi  ansa    agirre       
and multi lingual semantic web  buitelaar   cimiano         the main assumption on
which one can develop a wordnet using the expansion approach is that most of the concepts
and semantic relations are common among dierent languages  therefore  language specific
concepts and relations may not be covered in the resulting wordnet 
in general  and regardless of the approach taken  the main step toward constructing a
complete wordnet is to generate synonym sets  in this section  an automatic method for
extracting synsets for languages with limited resources is proposed  the proposed method
follows the expansion approach  at the start  wordnet is initialized with wordnet synsets 
for every wordnet synset s  all translations of english words inside s are extracted from bilingual dictionary and links between translation words and wordnet synsets are established 
since dictionaries translate word to word  not word sense to word sense  translations are
ambiguous  therefore  the task is to score links and find incorrect ones  we consider these
scores to be the probability of selecting each candidate synset for each word in the target
language 
in this paper  the task of finding correct the translation of words associated with the
wordnet synsets is regarded as an optimization problem  if a sensed tagged corpus similar
to the english semcor  landes  leacock    tengi        exists in the target language  the
problem of creating wordnet is converted to the maximum likelihood estimation  mle  
the english semcor corpus is a sense tagged corpus created at princeton university by the
wordnet project research team  the corpus consists of a subset of the brown corpus and
contains about         words  in semcor all the words are pos tagged and more than
        content words are sense tagged with reference to the wordnet lexical database 
since such resources may not exist  we use a word sense disambiguation method to find
correct sense of each word in a raw corpus  as shown in the research  which was conducted
by mallery         wsd is an ai complete problem whose diculty is equivalent to solving
the central problems of ai  this class of problems is analogous with np complete problems
in complexity theory  which are classified as being the most dicult problems  the proposed
idea is to use an iterative algorithm that finds the local optima of the problem with few
iterations in a reasonable time  our work can be regarded as an extension of the work
which was performed by montazery and faili         the proposed method adopts this
  

fiautomatic wordnet development for low resource languages

mono lingual
corpus

extract
unique words

bi lingual
dictionary

w

extract english
translations

wordnet

 w  e 

extract
wordnet
synsets

 w  s 

em algorithm
 w  s  p 

synsets in
the target
language

deleting
low rated links

figure    the overview of the proposed approach for constructing wordnet

work for low resource languages  and our method additionally attempts to solve its major
drawbacks 
the idea proposed in the work of montazery and faili        for wordnet construction 
is to use a bi lingual dictionary as well as a raw corpus  first  for each farsi word in the
corpus  all translations are extracted from the bi lingual dictionary  next  all synsets of
the english translations are considered as the candidate synsets for the farsi word  a
score is calculated for each pair of farsi words and wordnet synsets using the expectationmaximization  em  algorithm  in the expectation step  they use a relative based wsd
method  pmi   in which the co occurrence frequency of pairs of words in the farsi language
have been used to disambiguate words of a corpus  experimental results showed that the
precision of this method varies for dierent pos tags  the highest precision is shown for
adjectives which is        next for adverbs  which is        and the lowest precision is for
nouns at       
the major drawbacks of the above method are that calculating the co occurrence between each pair of words in the target language usually requires a large corpus  which may
not be easily found in low resource languages  this is important because the quality of the
resulting wordnet highly depends on the co occurrence values  as a result  we propose to
change the expectation step of the pmi based algorithm so that the wsd procedure can be
performed without needing an additional corpus or any other language resources  figure  
represents an overview of the proposed method  next  in the experimental analysis  we will
re implement this work as the baseline and compare the proposed method with it 
em is an iterative algorithm for finding the maximum likelihood parameters of a statistical model in cases where the equations cannot be directly solved  these models typically
consist of latent variables in addition to unknown parameters and known data observations 
that is  either there are missing values among the data  or the model can be formulated
more simply by assuming the existence of additional unobserved data points  the basic
idea of the em is as follows 
   if we have the actual sucient statistics for the data  we can compute the parameter
values that maximize the likelihood of the data  this is just the problem of learning
a probabilistic model from complete data 
  

fitaghizadeh   faili

maximization step

initial values

parameters w s

sense tagged corpus

expectation step

figure    expectation maximization algorithm for wordnet construction
   if we actually succeed in learning the model parameters  we could then compute a
probability distribution over the values of the missing attributes 
in the case of our problem  the em algorithm should find the probability of mapping each
word in the target language to each of its candidate synsets  if a candidate synset represents
a correct sense for a word in the target language  it is expected that this sense occurs in a
corpus containing that word  so the observed data is the words of a corpus in the target
language  the unseen part of each data is the wordnet sense tag of the words 
th em algorithm switches between two stages     finding an approximate distribution
of missing data given the parameters  and    finding better parameters given the approximation  the first step is known as the expectation or e step  while the second step is
called the maximization or m step  figure   represents an overview of the em algorithm
used for learning words connected to the wordnet synsets  next  details of each step in the
proposed algorithm are presented 
    e step
similar to the work of montazery and faili         for each word in the target language 
w  and each a wordnet synset  s  w s is defined as the probability of choosing wordnet
synset s for word w  p  s w   in other words  the number of times that word w appears in
a large corpus with sense s divided by total number of appearance w  that is 
w  s  
w  

w s         


w s     

   

   

s

at this step  current values of parameters w s are used to label the corpus with sense
tags  for each word w appearing in the corpus  an appropriate sense among the candidate
wordnet synsets should be chosen  to do this task  an unsupervised cross lingual word
sense disambiguation  wsd  could be employed  wsd algorithms aim to resolve word
ambiguity without the use of annotated corpora  unsupervised wsd is a well studied task
in the literature  among these  two categories of knowledge based algorithms have gained
popularity  overlap  and graph based methods  the former owns its success to the simple
intuition that underlies that family of algorithms  while the diusion of the latter started
growing after the development of semantic networks  basile  caputo    semeraro        
  

fiautomatic wordnet development for low resource languages

within the graph based framework for wsd  a graph is built from a lexical knowledge
base  usually wordnet  representing all possible senses of the word sequence that is being
disambiguated  graph nodes correspond to word senses  whereas edges represent dependencies between senses  these dependencies include hypernymy  synonymy  antonymy  etc 
next  the graph structure is analyzed to determine the importance of each node  finding
the right sense for each word in the sequence amounts to identifying the most important
node among the set of graph nodes representing its candidate senses  the main challenge of
the graph based wsd methods is how to create the graph  especially which dependencies
should be chosen as the graphs edges  and which connectivity measure should be used to
score the nodes of the graph 
in the research  which was conducted by navigli and lapata         a comprehensive
study on unsupervised graph based wsd was conducted  they evaluated a wide range of
local and global measures of graph connectivity with the aim of isolating those that are
particularly suited for this task  local measures include degree  page rank  hits  kpp
and betweenness  whereas global measures consist of compactness  graph entropy  and edge
density  their results indicate that local measures yield a better performance than global
ones  the best local measures are degree and pagerank 
for the task of wordnet development  we adapt a graph based wsd method as presented
in work of navigli and lapata         for the problem of the sense labelling of the corpus
using the current parameters w s   it is assumed that the true sense of each word in the
corpus is determined through senses of other words in the same sentence  for every sentence
of the corpus  the following procedure is executed 
 for each word w in the sentence  candidate wordnet synsets are picked  and one
terminal node for each synset s in the graph is created  this set of terminal nodes is
called vw  
 for each terminal node v  a depth first search  dfs  on the wordnet graph is performed  every time a node v   vw  w   w   along a path of length  l is encountered 
all intermediate nodes and edges on the path from v to v  are added to the graph  l
is a parameter of the algorithm and usually takes small values such as      or   
 terminal nodes of the graph are scored according to their degree as follows  for node
v  vw  
deg v 
c v   
 
   
maxuvw  deg u  
where deg v  is the number of edges terminating in v in graph g    v  e  
deg v       u  v   e   u  v  v    

   

relations chosen as the graphs edges consist of all the lexical and semantic relations
defined in wordnet in addition to the gloss relation  a pair of synsets s and s is connected
via a gloss relation if an unambiguous word w  s occurs in the gloss of s  the word
w must be unambiguous  otherwise  s should have been connected with the appropriate
sense of w  navigli   lapata         to use gloss relation in the wsd procedure  sense
disambiguated glosses of the wordnet are utilized  semantically tagged glosses         in
  

fitaghizadeh   faili

which word forms from the glosses in wordnets synsets are manually linked to the contextappropriate sense in wordnet  therefore  gloss relation is established between s and s   if
s appears as the correct sense of any word in the gloss of the s  
the time complexity of calculating a degree measure is less than pagerank  and its
performance has been shown to be better  so in the last step of the wsd procedure  a
degree measure is preferred for scoring nodes of the graph  to illustrate the steps of the
wsd procedure  we provide an example in the next section 
      wsd of a persian sentence
in order to better understand wsd procedure  an example is presented  consider the following persian sentence which means workers with thirty years of service become retired 

 

  z 
punc

  jpa
 
 
 j  pak
 ak   y
 g  k   a a     x
    iy
yk  
   z      z      z      z     z    z     z     z     z  
verb

adj

noun

noun noun num

noun

prep

noun

preposition  number and punctuation tags are not involved in the wordnet and so are
     retired in the above sentence  according to the aryanignored  consider the word j pak
 
pour dictionary  this word has three translations  emeritus  pensionary  retired  according
to the wordnet      the first translation has one noun synset and one adjective synset 
the second one has two noun synsets  and the third one has eleven verb synsets and one
adjective synset  since this word can be a noun or an adjective in a persian corpus  verb
synsets are ignored  the definitions of the other synsets are as follows 
             noun person  emeritus     a professor or minister who is retired from
assigned duties 
             adj all  emeritus     honorably retired from assigned duties and retaining your title along with the additional title emeritus as in professor emeritus 
             noun person  pensioner    pensionary     the beneficiary of a pension
fund 
             noun person  hireling    pensionary     a person who works only for
money 
             adj all  retired     no longer active in your work or profession 

 

     retired consists of these five
therefore  the candidate set for the persian word j pak
synsets  in general  each of these synsets could be the correct sense in the above sentence 
however  the pos tag of this word in the given sentence can come to our aid during the
wsd procedure in order to filter some synsets  indeed in the wsd procedure  only those
  

fiautomatic wordnet development for low resource languages

table    persian words and their candidate synsets 
persian word

 
yjpa

   x
a
k   a

 g 
iy
 
j  pak
 y  

pos
noun
noun
noun
noun

noun
adjective
verb

translations
employee 
worker 
member
relieve  own  have
year
background 
antecedent 
history 
record  service
work  job  activity 
profession
retired  emeritus 
wind  grow  lapse 
branch  become  be

candidate synsets

selected synset

correct

  

workern 

 

 
 
  

have n
yearn 
record n

 
 
 

  

job n

 

 
  

retired a
growv 

 
 

synsets which have the same pos as the given pos in the sentence should be involved 
     retired has an adjective pos in the above sentence  only adjective
since the word j pak
 
synsets are involved in the graphs construction  following the above steps for the other
words of the sentence leads to finding the candidate synsets of each word that should be
accounted for in the wsd graph  table   represents persian words  their translations  and
the number of candidate synsets regarding the pos tag of the persian words  all of these
candidate synsets represent the terminal nodes of the wsd graph  as figure   shows  the
candidate synsets of each persian word of the given sentence have been grouped in a dotted
box 
in the next step  a dfs algorithm is run for each terminal node on the wordnet graph
with the length being at most three  upon finding a path from one terminal node to another 
all intermediate nodes and edges are added to the wsd graph  part of the wsd graph is
shown in figure    each word in this graph is associated with a pos  which is denoted with
a subscript  n stands for noun  v for verb  a for adjective  and r for adverb  the superscript
denotes the sense number associated with that word in wordnet      this graph has three
  
 
  become and a year and the
separate components  one component for each word y k
other component for remaining words  this means no word in the given sentence indicates
the sense of these words 
after the construction of the wsd graph  the correct sense of each persian word should
be determined  to do this  the synset with the most degree among the candidate set of
     retired 
each word is chosen as the correct synset for that word  consider the word j pak
 
 
in the wsd graph of figure    the node retireda has a degree of one  whereas the node
emeritus a has a degree of zero  so the selected sense for this word is retired a   using the
degree measure  the selected sense for each word of the given sentence is determined  which
is represented in the bold box  table   summarizes the steps taken in the wsd procedure
of the given sentence  as the last column shows  the selected sense for all of the words is

   become 
correct except for k  a background and y
  

fitaghizadeh   faili

  
j  pak
retired a

workn 

workn 

record n

move n

wind n

historyn 
photographyn 

unf ortunate n

grown 

ancendent n
relative n

job  
n
job n

wind n

be n

   

processorn 

have n

wind n

person n

job n

employee n
   

   

decade n

yearn 

period n

yearn 

season n

yearn 

a

workern 

 
yjpa

job n

   


   x

 g 
iy

activityn 

traveln 

 

yk  

occupation n employment n

service n

k   a

prof ession n

emeritus a

   

  
  jpa
   
 
 j  pak
 g  k   a a     x
 ak   y
    iy
figure    part of wsd graph for the sentence yk
    m step
in the maximization step  a new estimation of the models parameters should be calculated
based on the sense tagged corpus that resulted from the expectation step  similar to the
work of montazery and faili         on iteration j  the new value for parameter w s   which
denotes the probability of assigning a sense tag s to the word w  is equal to averaging the
conditional probability p  s j    over dierent occurrences of w in the corpus  where j 
is the set of all parameters w s on iteration j     in formal notation 
n
p  si  w n   j   
i  
w
 w s
 s
i
i
j
w s
 
 
   
n  w 
j
where w s
denotes the value of w s on iteration j  w n presents sequence of corpus words
and n  w  is number of occurrence of w in w n  
in each iteration of the em algorithm  the likelihood of the data given the new parameter
values is at least as great as the likelihood given the old ones  so em behaves similar to
the gradient descent  at each step  it adjusts the parameter values so as to improve the
likelihood of the data  it follows that em converges to a set of parameter values that
locally maximizes the likelihood 
the proposed em method is repeated until the changes in the probability of selecting
a candidate synset for a word in the target language becomes negligible  so  at the end
of each iteration  the maximum change of probabilities is computed  if this value is less
than t  the algorithm stops  after execution of the em algorithm  all links with a score of
below the threshold tremove  w s  tremove   will be deleted from the wordnet  also in each

  

fiautomatic wordnet development for low resource languages

 

     retired per iteration 
table    assigned probabilities for word  j pak
synset id
noun         
adjective         
noun         
noun         
adjective         
entropy

correct
 
 
 
 
 

itr   
   
   
   
   
   
      

itr   
       
       
       
       
       
      

itr    
       
       
       
       
       
      

itr   
       
 
       
       
       
      

itr   
       
 
       
       
       
      

itr   
       
 
       
       
       
      

iteration  those links with a current score below t are ignored and the corresponding senses
are not presented in the graphs construction and the wsd procedure  at the end  those
words in the target language that are mapped onto the same synset in the wordnet make
synsets of the resulting wordnet 
to better follow the process of updating probabilities of each word per iteration  an
example is presented here  for demonstrating the probability adjustment in each iteration 
     retired  in the expectation step  all words of the corpus
consider again the word  j  pak
 
should be disambiguated  next in the maximization step  the new value of the probabilities
  
is computed  table   represents the probabilities of synsets assigned to the word  j  pak
 
 retired in each iteration  the first and the second columns show the synset id and the
correction of synsets for the specified word  respectively  the following columns represent
the probability values of the first five iterations  values less than       were considered to
be    this table shows that the probabilities start out uniformly  then in each iteration 
the probability of correct synsets increases and the probability of incorrect synsets or those
that are not frequent enough in the corpus decreases or does not change  indeed  if the
     retired in the corpus  which are tagged with a
number of occurrences of word  j  pak
 
specific wordnet sense in iteration i are the same as the iteration i     the probability of
     retired does not change in the iteration i  if this
that sense of the given word  j  pak
 
value becomes greater  that probability increases and so if this value becomes smaller  that
probability decreases  in this particular example  after five iterations  the synset achieving
the highest probability is the correct synset  in iteration three  the probability of the word
     retired being assigned to the second synset goes down to    e    which is below
 j   pak
the threshold  so in the next iterations  this synset is not considered in wsd procedure
and its probability will be zero  the last row of the table presents the entropy value in
respect to the iteration  the steady decrease in entropy indicates that in each iteration 
the distinction between candidates synsets for each word becomes more clear  which leads
to identification of the correct synsets  the subject of analysis of the entropy for each word
per iteration is discussed later in section       

   case study  persian language
in this section  the proposed method for automatic wordnet construction is applied to
persian as a low resource language  in the following subsections  the experimental setup
and evaluation methods are described  after that  the results are presented 
  

fitaghizadeh   faili

    experimental setup and data
in this section  the required resources and setup of the experiments are explained    to
construct a wordnet for the persian language  the bijankhan persian corpus  has been
used  this collection has been gathered from daily news and common texts  in which
all documents are categorized into dierent subjects such as political  cultural and so on 
bijankhan contains about ten million manually tagged words with a tag set containing    
fine grained persian pos tags  oroumchian  tasharofi  amiri  hojjat    raja        
although pos tags are not explicitly used in the proposed method  to get better wsd
results  one can use pos tags to prune synsets along with the other tags from the candidate
set of each word as explained in section        as a result  in the wsd procedure  just
those synsets with the same pos tag as the word of the corpus are taken part  in wordnet 
four categories of tags are included  noun  verb  adverb and adjective  thus the words of
the corpus with other tags such as pronoun and preposition are ignored 
bijankhan is a large corpus  most low resource languages may not have such a large
corpus  in order to evaluate the behaviour of the proposed method when the corpus size is
limited  a part of the bijankhan has been picked for training persian wordnet  so both the
pmi based and the graph based method have been conducted using this part  this part
includes nearly     of the total size of the corpus  the remaining     has been used in
the testing phase in which coverage of the wordnet over the corpus was evaluated  more
details about the coverage analysis are presented in section        also  a complete analysis
on the eect of the corpus size on the quality of the final wordnet is presented in section
    
those words in the corpus that appear in their inflected forms may not be found in the
dictionary  therefore before the beginning of the proposed algorithm  a lemmatizer should
be used so that dierent inflected forms of words are converted to their base form  for
example  plural nouns should be converted to their singular form  to do this  step   tool
 shamsfard  jafari    ilbeygi      b  has been utilized  the step   package is a set of
fundamental tools for persian text processing that provides support for tokenization  spell
checking  morphological analysis  and pos tagging 
another required resource for the proposed method is a bi lingual machine readable
dictionary  an electronic version of the aryanpour dictionary  has been used to extract
the english equivalent for bijankhan words  also  wordnet version     has been used to
extract synsets of their english equivalents 
in the wsd procedure  the context of each word is the sentence containing that word 
a depth first search in wsd has been performed up to a maximum depth of   similar to the
work of navigli and ponzetto      b   as mentioned before in section      if the probability
of the wordnet sense s given for the word w is less than or equal to t  that sense is ignored
in the wsd process of the em algorithm  in our experiments  we have set t         

   the source code is freely available for download at http   ece ut ac ir en node    
   see http   ece ut ac ir dbrg bijankhan 
   see http   www aryanpour com

  

fiautomatic wordnet development for low resource languages

iteration
entropy

table    entropy values with respect to the iteration
 
 
 
 
 
 
                                               

 
       

    evaluation results
in this section  the results of the evaluation of the proposed method in various experiments
are presented 
      convergence of the proposed method
the em algorithm iterates between the expectation and maximization steps  until some
criteria are satisfied  in our experiment  after each iteration  the entropy of synset probabilities per word is calculated and the average of the entropy of all the words is considered 
if the changing of this value in two consecutive iterations becomes near zero  the em algorithm stops  formally  the entropy of a probability distribution is defined as equation
  
h w   



w s log w s   

   

s

entropy is best understood as a measure of uncertainty  as entropy is larger for more
random values  indeed at first  all links for a persian word have equal probability  and so
maximum entropy is granted  after each iteration  some links sink under the threshold
probability and thus the probability of the other links increases  it is expected at the final
step that all incorrect links obtain a very low probability and that correct links obtain a
high probability  therefore  entropy analysis can demonstrate the behaviour of the em
method in changing probabilities  in table    the entropy values per iteration are shown 
at iteration    changing the entropy values reaches the predetermined threshold of      
and the em algorithm stops 
      precision and recall of the wordnet
the primary goal of this work is to construct a high quality wordnet for low resource
languages  after execution of the em algorithm  the probability of assigning each candidate
synset to each word in the target language is finalized  these probabilities are sorted and
those links with a probability under the threshold tremove should be removed from the final
wordnet  the value of tremove determines the size of the wordnet and aects the quality
of the wordnet  so  experiments were conducted that used dierent values for the tremove
including                              and     
to evaluate the resulting wordnet  we re implemented the pmi based method  montazery   faili        and compared our wordnet with it as a baseline  in all experiments 
our wordnet is referred to as the graph based wordnet  in contrast to the pmi based
wordnet  in the evaluation process  two data sets were used     farsnet    manual judges 
farsnet is a semi manually created wordnet in persian  which is available in two versions 
the second release of farsnet contains more than        persian words and phrases that
are organized into about        synsets of nouns  adjectives  adverbs and verbs  farsnet  
  

fitaghizadeh   faili

has also inter lingual relations that connect some of the persian synsets to english ones in
the princeton wordnet     
the second data set consists of a subset of       links in the resulting wordnet  which
were selected randomly and judged manually  each link  w  s  was given to two annotators
to decide if the persian word w is semantically equal to the wordnet s  to ensure the
accuracy of the judges  annotators were selected among people who are native speakers of
persian and at the same time learn english professionally  in the case of disagreement between two judges  a third annotator was asked to decide about the link  the inter annotator
agreement was      which means that in     of judgements  the two annotators agreed 
additionally  we computed cohens kappa coecient  cohen         for two annotators 
which takes into account the amount of agreement that could be expected to occur through
chance  kappa is computed as follows 
 

po  pe
 
   pe

   

where po is the relative observed agreement among annotators  and pe is the hypothetical
probability of chance agreement  for our two annotators  the kappa value was       in
general  if the annotators are in complete agreement  then       if no agreement between
annotators other than what would be expected by chance  as given by pe    then     
after carrying out the manual judgements  the precision and recall of the resulting wordnet
were measured on this set 
the precision of the resulting wordnet is defined as the number of correct links in the
wordnet that also exist in the test data as correct links  divided by the total number of links
in the wordnet that exist in the test data  also  the recall of the wordnet is defined as the
number of correct links in the wordnet that also exist in the test set as correct links  divided
by the total number of correct links in the test set  the accuracy of the wordnet is another
measure  which is defined as the number of correct links in the wordnet that also exist in the
test set plus the number of incorrect links in the test set that do not exist in the wordnet 
divided by the total number of links in the test set  these definitions of precision  recall 
and accuracy of the wordnet were also used in the babelnet project  navigli   ponzetto 
      
figure  a and figure  b represent the precision and recall of the pmi based method
and the proposed method according to farsnet  as shown  the precision and recall of our
wordnet is better than the pmi based method  in these figures  precision is at most     
which seems low for a wordnet to be considered as a reliable resource for that language 
additionally  recall is at most      this is due to the lack of correct links in farsnet  in
the evaluation of the resulting wordnet according to farsnet each link  w  s  can be placed
in one of these categories 
 persian word w does not exist in farsnet  this link is ignored and is not counted 
 persian word w exists in farsnet  however no wordnet synset is given for it  this
link is ignored and not counted 
 persian word w exists in farsnet and at least one wordnet synset is given for it  if
s is one of these wordnet synsets  this link is counted as correct or else it is counted
as incorrect 
  

fiautomatic wordnet development for low resource languages

the wordnet sense distinctions are too fine grained  meaning that several wordnet
synsets may be mapped onto one synset in farsnet  while most of them are not given in
farsnet  therefore  some correct links in our wordnet are counted as incorrect  figure  c
shows the accuracy of the wordnets according to farsnet  which shows that the graph based
wordnet surpasses the pmi based wordnet 
some reasons for low precision according to farsnet are as follows 
 translations of the persian words are inaccurate or incomplete  meaning that the
correct wordnet synset according to farsnet does not exist in the candidate set  for
   j motaalleqat possession  three equivalent
example  for the persian word  ha
english words are written in the aryanpour dictionary  appurtenance  paraphernalia 
  j possession are determined
belongings  in our wordnet  the correct synsets for ha
as follows              noun possession   property    belongings    holding   
 something owned  any tangible or intangible possession that is owned by someone 
that hat is my property  he is a man of property   however according to farsnet 
the correct synset is             noun tops  possession     anything owned or
  j possession to synset noun         
possessed   in this evaluation  the link ha
is considered to be incorrect and is penalized 
 the persian word is not lemmatized correctly  so the english translations and consequently candidate set does not contain the correct synset  for example  the persian
word   p a k   barak barak is a proper noun  while the stemmer recognizes pa k 
 bar load as its stem  which means load 
to resolve the above problems  a set of manually judged links were used in the second
experiment  figure   represents the precision and recall of the resulting wordnet for dierent
values of tremove according to manual judges  parameter tremove demonstrates a threshold 
so those links with a score lower than it should be deleted from the final wordnet  high
values for tremove result in a wordnet with high precision but low recall  on the other hand 
low values for tremove cause a low precision but high recall wordnet  thus there is a trade o
between precision and recall  for tremove        the precision of the pmi based wordnet
is      while precision of the wordnet created by the proposed method is     according
to manual judges  if tremove      which means that all links are contained in the final
wordnet  the precision is      therefore  the initial wordnet seen without executing the
em algorithm has     precision  figures  d and  c show another quality measure for both
wordnets  which is f  measure  definition of the f  measure and a complete analysis about
it is presented in section     
      size and polysemy rate of the wordnet
one of the important aspects of wordnets is their size  large wordnets may have tens
of thousands of sysnsets  miller        patanakul   charnyote        black et al        
piasecki et al          on the other hand  wordnets with more polysemic words are more
useful in nlp and ir tasks  polysemic words are those words that have more than one
sense in the wordnet  finding the correct sense of polysemic words is of great significance
to automatic wordnet construction 
  

fitaghizadeh   faili

graph based
pmi based

    

recall

precision

   

   

graph based
pmi based

   
 

                           

   

   

 

                           

tremove

   

tremove

 a  precision

 b  recall

   

f 

accuracy

    
   
   
graph based
pmi based

   
 

                           

   
graph based
pmi based

    

   

 

                           

tremove

   

tremove

 c  accuracy

 d  f measure

figure    comparison between the wordnets according to the farsnet 
   

recall

    
graph based
pmi based

   
 

                           

graph based
pmi based

   

   

   

 

                           

tremove

tremove

 a  precision

 b  recall
   

graph based
pmi based

   
f 

precision

   

   
   
 

                           

   

tremove

 c  f measure

figure    comparison between the wordnets according to the manual judges 
  

   

fiautomatic wordnet development for low resource languages

table    comparison between size of the wordnets

threshold
   
    
    
    
     
 

pmi based wordnet
unique words word synset polysemy
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
       
    

graph based wordnet
unique words word synset polysemy
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
       
    

in this section  the size of the resulting wordnet and polysemy rate for two wordnets 
pmi based and graph based wordnets  are reported  table   presents the number of unique
words  the number of persian word wordnet synset links and the proportion of the polysemic words based on dierent values for tremove   as tremove decreases from     to       more
unique words will be contained in wordnets  the number of word synset links increases  and
also the proportion of polysemic words to unique words in the wordnet increases  as can be
seen  the wordnet created as a result of the graph based method surpasses the pmi based
wordnet 
if all links were included in the wordnet  then polysemic words are     of the unique
words  however  in our wordnet  by removing those links with a probability of less than
         of words are polysemic  which is     better than the pmi base wordnet  for
tremove        both wordnets have about        unique words  since both methods were
executed on the same corpus  there is no significant dierence in their sizes 
      coverage of the wordnet
to evaluate the coverage of the resulting wordnet  we are interested in observing the coverage
over wordnet synsets and also the coverage over language words  in this section  three
experiments were performed     core concepts coverage     wordnet synset coverage  and
   corpus coverage 
in the first experiment  the coverage of our wordnet over core synsets is evaluated  boydgraber et al         published a list of about       word senses in wordnet      which
contains the       most frequently used word senses  core wordnet         coverage of
a wordnet over this list can be regarded as covering the most common concepts of the
language  so this core wordnet was used to measure the percentage of synsets from this list
covered by pmi based and graph based wordnets  figure  a represents the core coverage
for dierent values of tremove   selecting all links   tremove       causes coverage of     of
the core wordnet  while choosing links that are more probable than      leads to coverage of
    and     of the core wordnet for graph based and pmi based wordnets  respectively 
in the second experiment  the coverage of wordnets over all wordnet synsets is studied 
since the resulting wordnet is a multi lingual wordnet  its coverage of it over wordnet
synsets is a measure of its quality  figure  b represents the coverage of pmi based and
graph based wordnets over wordnet     synsets for dierent values of tremove   this figure
shows that the graph based wordnet covers more wordnet synsets than pmi based wordnet
for all values of tremove   for example  by selecting links with a probability higher than     
  

figraph based
pmi based

   
core coverage

wordnet synsets coverage

taghizadeh   faili

   

   
 

                           

   

    

graph based
pmi based

   
    
   
                           

 

tremove

   

tremove

 a  core coverage

 b  wordnet coverage

figure    coverage of the wordnets over the core synsets and all synsets of the wordnet 
table    comparison between coverage of the wordnets 

farsnet
pmi based wordnet
graph based wordnet

coverage over the bijankhan  unique words 
     
      
      

the graph based wordnet covers     of wordnet synsets  while the pmi based wordnet
covers     of wordnet synsets 
in the third experiment  the coverage of wordnets over the bijankhan corpus is evaluated 
bijankhan is a large corpus and the proposed method was trained over     of it  the rest
of this corpus was used for measuring the word coverage of wordnets  table   demonstrates
the number of unique words of the corpus  covered by pmi based and graph based wordnets 
when tremove        the same evaluation was also performed on farsnet as the baseline
and is also presented in the table    although the training and testing corpus are separate 
there is a significance dierence between farsnet and em based wordnets coverage 
    parameter selection
in the proposed method for wordnet construction after convergence of the em algorithm 
a set of links between words of the target language and synsets of the source language is
obtained  the links that scored lower than the threshold tremove are removed from the final
wordnet  as the previous experiments showed  the value of the tremove aects the quality
of the resulting wordnet  the experiments in section       illustrated that changing tremove
from       to     has a positive eect on the precision but a negative eect on the recall
of the resulting wordnet  indeed  there is a trade o between the precision and the recall 
here a question may arise  what is the best value for the tremove  
in this section  f  measure is used to investigate the quality of the wordnet  considering
both precision and recall  the formula of the f  is as follows 
f      

precision recall
 
precision   recall
  

   

fiautomatic wordnet development for low resource languages

f  is the harmonic mean of the precision and the recall  in order to gain some insight
into the optimum value of tremove   the f  for the resulting wordnet has been calculated for
dierent values of tremove   as for precision and recall  the f  is calculated against both
the manual judgement and the farsnet  figure  c shows that f  decreases from     to
    when tremove increases from       to     for the graph based wordnet according to the
manual judgement  this means that the precision value is more important than that of the
recall and the rate at which precision is decreasing is higher than the rate at which recall is
increasing  therefore  to gain a more precise wordnet  we should increase tremove   however 
we must accept loosing the recall 
on the other hand  figure  d shows that the highest value of the f  for the graph based
wordnet is obtained when tremove       according to farsnet  this fact means that the
recall value has more eect on the f  than the precision value  the reason for this dierence
is due to the low precision values that have been obtained in the evaluation according to
farsnet  as reported in section        farsnet lacks most of the correct mappings between
the persian words and the wordnet synsets  indeed in wordnet construction  the precision
of the final wordnet is more important than its recall 
finally  choosing the threshold tremove has important eect on the quality of the resulting
wordnet  however  this matter depends on the application  in most applications  having
a more precise wordnet is preferential to having a large but not accurate enough one  in
these cases  greater values for tremove is preferential  although  in applications that high
recall is needed  one should choose low values for tremove  
    the eect of corpus size and dictionary
in this section  the eect of the required resources on the final wordnet is looked at  the
proposed method needs a bi lingual dictionary and a mono lingual corpus  in the previous
experiments  the aryanpour dictionary and the bijankhan corpus were used  since the
bijankhan is a large corpus only     of it was used in the previous experiments  to
investigate the eect of corpus size on the quality of the resulting wordnet  the proposed
method has been executed using four other sizes of the bijankhan               and     
additionally  to examine the eect of the dictionary on the quality of the final wordnet  the
google translator  is used in another experiment instead of the aryanpour  the resulting
wordnet is then compared against the wordnet created using aryanpour on the same size
of the bijankhan  the link removal threshold tremove for all experiments in this section is
     the resulting wordnets have been evaluated for precision  recall  accuracy  coverage
over the wordnet core synsets  coverage over all synsets of the wordnet  and the number
of the persian words 
as it is shown in figure    when the size of the corpus increases from    to     of
the bijankhan corpus using the same dictionary  all measures increase except for precision 
which either does not change or changes only slightly  this result is not beyond expectation  indeed  the precision of the resulting wordnet depends on the precision of the wsd
procedure and so does not depend on the size of the corpus  however  new possible senses
of the words are discovered by increasing the size of the corpus and therefore the recall 
accuracy  coverage and the size of the wordnet increase with growth of the corpus size  as
   http   translate google com 

  

fitaghizadeh   faili

aryanpour dictionary
google translator
   

    

   

    

accuracy

recall

precision

    
    
    

    

    

   
    

    
    

  

    

  

  

    

  

  

 a  precision

  
size

size

size

 b  recall

 c  accuracy
   

    

    

   

    
   
    
    

number of words

synset coverage

core coverage

    
    

    

   

 

   

   

    
    

  

  

    

  

size

 d  core coverage

  
size

 e  wordnet synset coverage

    

  

  
size

 f  number of words

figure    evaluation of the resulting wordnet trained on dierent sizes of bijankhan 

figure  f demonstrates  to have a wordnet with at least        words  the corpus size should
be at least     of the bijankhan corpus  figure   also illustrates that the wordnet trained
on the aryanpour dictionary excels the wordnet derived from the google translator  this
experiment demonstrates that the dictionary heavily aects the final wordnet even more
than the corpus size  as a result  having a small corpus but a large dictionary results in a
more precise wordnet than having a large corpus but a small dictionary 
in the last experiment  the proposed method has been executed using the full bijankhan
corpus and the aryanpour dictionary  the precision  recall and accuracy of the resulting
wordnet are          and      respectively  comparing to the wordnet  which was created
using     of the bijankhan and the same dictionary  recall and accuracy increased    and
    accordingly  while the precision does not change  this wordnet has        persian
word and covers     of the core synsets of the wordnet  considering all synsets of the
wordnet  it covers     of them 

   conclusion
in this paper  an em algorithm was employed in order to develop a wordnet for lowresourced languages  we successfully applied unsupervised cross lingual wsd in the expectation step of the algorithm  the proposed method does not use any features specific
  

fiautomatic wordnet development for low resource languages

to the target language  and so it can be used for other languages to generate wordnets 
resources needed for this proposed algorithm include a bi lingual dictionary and a monolingual corpus  the proposed method belongs to the expansion approach and so creates a
multi lingual wordnet in which for each word in the target language  the equivalent synset
in wordnet is known 
the proposed method was applied on the persian language and the quality of the resulting wordnet was examined through several experiments  its precision was     according
to farsnet and     according to the manual judgement  the reason for this dierence is
that the wordnet synsets are too fine grained in comparison to the farsnet synsets  and so
most of the synsets in farsnet should be mapped onto more than one synset in wordnet 
however farsnet provides one or at most two wordnet synsets for most of frasnet synsets 
this problem means that most of the correct links in the resulting wordnet are considered
to be incorrect and thus the reported precision becomes low  also  the resulting wordnet
contains about        words of the persian language from only using     of the bijankhan
corpus  which is more than several wordnets in other languages  additionally      of core
synsets and     of all synsets of wordnet are covered  analysis of the eects of corpus
size and dictionary size of the resulting wordnet showed that the dictionary size can aect
the precision of the wordnet more than the corpus size and therefore it is important to use
large enough dictionaries 

acknowledgements
this research was in part supported from institute for research in fundamental sciences
 no  cs           

references
apidianaki  m     sagot  b          data driven synset induction and disambiguation for
wordnet development  language resources and evaluation                 
atserias  j   climent  s   farreres  x   rigau  g     rodrguez  h          combining
multiple methods for the automatic construction of multilingual wordnets  amsterdam
studies in the theory and history of linguistic science series           
barbu  e     barbu mititelu  v          a case study in automatic building of wordnets 
in proceedings of ontolex       ontologies and rexical resources  pp        jeju
island  korea  asian federation of natural language processing 
basile  p   caputo  a     semeraro  g          an enhanced lesk word sense disambiguation
algorithm through a distributional semantic model  in proceedings of coling      
the   th international conference on computational linguistics  technical papers 
pp            dublin  ireland  international committee on computational linguistics 
black  w   elkateb  s     vossen  p          introducing the arabic wordnet project  in
proceedings of the third international wordnet conference  gwc      pp         
south jeju island  korea  global wordnet association 
  

fitaghizadeh   faili

bond  f     foster  r          linking and extending an open multilingual wordnet  in proceedings of the   st annual meeting of the association for computational linguistics 
pp            sofia  bulgaria  association for computational linguistics 
bond  f   isahara  h   kanzaki  k     uchimoto  k          boot strapping a wordnet using
multiple existing wordnets  in proceedings of the sixth international conference on
language resources and evaluation  lrec     pp            marrakech  morocco 
european language resources association  elra  
boudabous  m  m   chaaben kammoun  n   khedher  n   belguith  l  h     sadat  f 
        arabic wordnet semantic relations enrichment through morpho lexical patterns  in proceeding of  st international conference on communications  signal processing  and their applications  iccspa   pp      american university of sharjah 
united arab emirates  ieee 
boyd graber  j   fellbaum  c   osherson  d     schapire  r          adding dense  weighted
connections to wordnet  in proceedings of the third international wordnet conference  gwc      pp        south jeju island  korea  global wordnet association 
buitelaar  p     cimiano  p          towards the multilingual semantic web  springer
berlin heidelberg 
cohen  j          a coecient of agreement for nominal scales  educational and psychological measurement               
core

wordnet       
core wordnet txt 

http   wordnetcode princeton edu standoff files 

diab  m          the feasibility of bootstrapping an arabic wordnet leveraging parallel
corpora and an english wordnet  in proceedings of the arabic language technologies
and resources  cairo  nemlar 
dini  l   peters  w   liebwald  d   schweighofer  e   mommers  l     voermans  w         
cross lingual legal information retrieval using a wordnet architecture  in proceedings
of the   th international conference on artificial intelligence and law  acail   pp 
        bologna  italy  acm 
elkateb  s   black  w   rodrguez  h   alkhalifa  m   vossen  p   pease  a     fellbaum  c 
        building a wordnet for arabic  in proceedings of the  th international conference on language resources and evaluation  lrec        genoa  italy  european
language resources association  elra  
erjavec  t     fiser  d          building slovene wordnet  in proceedings of the  th international conference on language resources and evaluation  lrec        genoa 
italy  european language resources association  elra  
fellbaum  c     vossen  p          challenges for a multilingual wordnet  language resources and evaluation                 
fiser  d          human language technology  in leveraging parallel corpora and existing
wordnets for automatic construction of the slovene wordnet  pp          springer
berlin heidelberg 
  

fiautomatic wordnet development for low resource languages

gunawan  g     saputra  a          building synsets for indonesian wordnet with monolingual lexical resources  in proceedings of international conference on asian language
processing  ialp   pp          harbin  china  ieee 
hasanuzzaman  m   caen  f   dias  g   ferrari  s     mathet  y          propagation strategies for building temporal ontologies  in proceedings of   rd conference on european
chapter of the association for computational linguistics  pp       guthenburg  sweden  association for computational linguistics 
kaji  h     watanabe  m          automatic construction of japanese wordnet  in proceedings of the  th international conference on language resources and evaluation
 lrec        genoa  italy  european language resources association  elra  
kazakov  d     shahid  a  r          unsupervised construction of a multilingual wordnet
from parallel corpora  in proceedings of the workshop on natural language processing
methods and corpora in translation  lexicography  and language learning  pp      
borovets  bulgaria  association for computational linguistics 
lam  k  n   al tarouti  f     kalita  j          automatically constructing wordnet synsets 
in   nd annual meeting of the association for computational linguistics  acl       
pp          baltimore  usa  association for computational linguistics 
landes  s   leacock  c     tengi  r  i          building semantic concordances  wordnet 
an electronic lexical database                    
mallery  j  c          thinking about foreign policy  finding an appropriate role for artificially intelligent computers  ph d  thesis  mit political science department 
miller  g  a          wordnet  a lexical database for english  communications of the acm 
              
montazery  m     faili  h          automatic persian wordnet construction  in proceedings
of the   rd international conference on computational linguistics  posters  pp     
     beijing  china  association for computational linguistics 
montazery  m     faili  h          unsupervised learning for persian wordnet construction 
in proceedings of recent advances in natural language processing  ranlp   pp 
        hissar  bulgaria  association for computational linguistics 
navigli  r          word sense disambiguation  a survey 
 csur              

acm computing surveys

navigli  r     lapata  m          an experimental study of graph connectivity for unsupervised word sense disambiguation  pattern analysis and machine intelligence  ieee
transactions on                 
navigli  r     ponzetto  s  p          babelnet  building a very large multilingual semantic network  in proceedings of the   th annual meeting of the association for computational linguistics  pp          uppsala  sweden  association for computational
linguistics 
navigli  r     ponzetto  s  p       a   babelnet  the automatic construction  evaluation
and application of a wide coverage multilingual semantic network  artificial intelligence              
  

fitaghizadeh   faili

navigli  r     ponzetto  s  p       b   multilingual wsd with just a few lines of code  the
babelnet api  in proceedings of the   th annual meeting of the association for computational linguistics  acl        pp        jeju  republic of korea  association
for computational linguistics 
oliver  a     climent  s          parallel corpora for wordnet construction  machine translation vs  automatic sense tagging  in proceedings of the   th international conference
on intelligent text processing and computational linguistics  pp          new delhi 
india  springer 
oroumchian  f   tasharofi  s   amiri  h   hojjat  h     raja  f          creating a feasible
corpus for persian pos tagging  tech  rep  tr      university of wollongong  dubai 
otegi  a   arregi  x   ansa  o     agirre  e          using knowledge based relatedness for
information retrieval  knowledge and information systems                 
patanakul  s     charnyote  p          construction of thai wordnet lexical database
from machine readable dictionary  in conference proceedings  the tenth machine
translation summit  pp        phuket  thailand  language technology world 
piasecki  m   kurc  r     broda  b          heterogeneous knowledge sources in graph based
expansion of the polish wordnet  in intelligent information and database systems 
vol        pp          springer 
prabhu  v   desai  s   redkar  h   prabhugaonkar  n   nagvenkar  a     karmali  r         
an ecient database design for indowordnet development using hybrid approach  in
proceedings of the  rd workshop on south and southeast asian natural language
processing  sanlp   pp          mumbai  india  international committee on computational linguistics 
rodrquez  h   farwell  d   ferreres  j   bertran  m   alkhalifa  m     mart  m  a 
        arabic wordnet  semi automatic extensions using bayesian inference  in
proceedings of the sixth international conference on language resources and evaluation  lrec     marrakech  morocco  european language resources association
 elra  
saveski  m     trajkovski  i          automatic construction of wordnets by using machine translation and language modeling  in proceedings of the   th international
multiconference  pp        ljubljana  slovenia  information society 
semantically tagged glosses        http   wordnet princeton edu glosstag shtml 
shamsfard  m          towards semi automatic construction of a lexical ontology for persian  in proceedings of the  th international conference on language resources and
evaluation  lrec        marrakech  morocco  european language resources association  elra  
shamsfard  m   hesabi  a   fadaei  h   mansoory  n   famian  a   bagherbeigi  s   fekri  e  
monshizadeh  m     assi  s  m       a   semi automatic development of farsnet  the
persian wordnet  in proceedings of  th global wordnet conference  mumbai  india 
global wordnet association 
  

fiautomatic wordnet development for low resource languages

shamsfard  m   jafari  h  s     ilbeygi  m       b   step    a set of fundamental tools for
persian text processing  in proceedings of the  th international conference on language resources and evaluation  lrec        valletta  malta  european language
resources association  elra  
tufis  d   cristea  d     stamou  s          balkanet  aims  methods  results and perspectives  a general overview  romanian journal of information science and technology 
             
vossen  p          introduction to eurowordnet  in eurowordnet  a multilingual database
with lexical semantic networks  pp       springer 

  

fi