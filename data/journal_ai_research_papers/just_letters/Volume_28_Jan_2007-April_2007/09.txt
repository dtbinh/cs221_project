journal of artificial intelligence research                 

submitted        published      

closed loop learning of visual control policies
sebastien jodogne
justus h  piater

jodogne montefiore ulg ac be
justus piater ulg ac be

montefiore institute  b   
university of liege  b      liege  belgium

abstract
in this paper we present a general  flexible framework for learning mappings from images to actions by interacting with the environment  the basic idea is to introduce a
feature based image classifier in front of a reinforcement learning algorithm  the classifier
partitions the visual space according to the presence or absence of few highly informative local descriptors that are incrementally selected in a sequence of attempts to remove
perceptual aliasing  we also address the problem of fighting overfitting in such a greedy
algorithm  finally  we show how high level visual features can be generated when the
power of local descriptors is insufficient for completely disambiguating the aliased states 
this is done by building a hierarchy of composite features that consist of recursive spatial
combinations of visual features  we demonstrate the efficacy of our algorithms by solving
three visual navigation tasks and a visual version of the classical car on the hill control
problem 

   introduction
designing robotic controllers quickly becomes a challenging problem  indeed  such controllers face a huge number of possible inputs that can be noisy  must select actions among a
continuous set  and should be able to automatically adapt themselves to evolving or stochastic environmental conditions  although a real world robotic task can often be solved by
directly connecting the perceptual space to the action space through a given computational
mechanism  such mappings are usually hard to derive by hand  especially when the perceptual space contains images  evidently  automatic methods for generating such mappings
are highly desirable  because many robots are nowadays equipped with ccd sensors 
in this paper  we are interested in reactive systems that learn to couple visual perceptions
and actions inside a dynamic world so as to act reasonably  this coupling is known as a
visual  control  policy  this wide category of problems will be called vision for action tasks
 or simply visual tasks   despite about fifty years years of active research in artificial
intelligence  robotic agents are still largely unable to solve many real world visuomotor
tasks that are easily performed by humans and even by animals  such vision for action
tasks notably include grasping  vision guided navigation and manipulation of objects so as
to achieve a goal  this article introduces a general framework that is suitable for building
image to action mappings using a fully automatic and flexible learning protocol 
    vision for action and reinforcement learning
strong neuropsychological evidence suggests that human beings learn to extract useful information from visual data in an interactive fashion  without any external supervisor  gibson
c
    
ai access foundation  all rights reserved 

fijodogne   piater

  spelke         by evaluating the consequence of our actions on the environment  we
learn to pay attention to visual cues that are behaviorally important for solving the task 
this way  as we interact with the outside world  we gain more and more expertise on our
tasks  tarr   cheng         obviously  this process is task driven  since different tasks do
not necessarily need to make the same distinctions  schyns   rodet        
a breakthrough in modern artificial intelligence would be to design an artificial system
that would acquire object or scene recognition skills based only on its experience with
the surrounding environment  to state it in more general terms  an important research
direction would be to design a robotic agent that could autonomously acquire visual skills
from its interactions with an uncommitted environment in order to achieve some set of
goals  learning new visual skills in a dynamic  task driven fashion so as to complete an a
priori unknown visual task is known as the purposive vision paradigm  aloimonos        
one plausible framework to learn image to action mappings according to purposive vision is reinforcement learning  rl   bertsekas   tsitsiklis        kaelbling  littman 
  moore        sutton   barto         reinforcement learning is a biologically inspired
computational framework that can generate nearly optimal control policies in an automatic
way  by interacting with the environment  rl is founded on the analysis of a so called
reinforcement signal   whenever the agent takes a decision  it receives as feedback a real
number that evaluates the relevance of this decision  from a biological perspective  when
this signal becomes positive  the agent experiences pleasure  and we can talk about a reward  
conversely  a negative reinforcement implies a sensation of pain  which corresponds to a
punishment  the reinforcement signal can be arbitrarily delayed from the actions which
are responsible for it  now  rl algorithms are able to map every possible perception to an
action that maximizes the reinforcement signal over time  in this framework  the agent is
never told what the optimal action is when facing a given percept  nor whether one of its
decisions was optimal  rather  the agent has to discover by itself what the most promising
actions are by constituting a representative database of interactions  and by understanding the influence of its decisions on future reinforcements  schematically  rl lies between
supervised learning  where an external teacher gives the correct action to the agent  and
unsupervised learning  in which no clue about the goodness of the action is given  
rl has had successful applications  for example turning a computer into an excellent
backgammon player  tesauro         solving the acrobot control problem  yoshimoto 
ishii    sato         making a quadruped robot learn progressively to walk without any
human intervention  huber   grupen        kimura  yamashita    kobayashi        kohl
  stone         riding a bicycle  randlv   alstrm        lagoudakis   parr        or
controlling a helicopter  bagnell   schneider        ng  coates  diel  ganapathi  schulte 
tse  berger    liang         the major advantages of the rl protocol are that it is fully
automatic  and that it imposes very weak constraints on the environment 
unfortunately  standard rl algorithms are highly sensitive to the number of distinct
percepts as well as to the noise that results from the sensing process  this general problem
is often referred to as the bellman curse of dimensionality  bellman         thus  the high
dimensionality and the noise that is inherent to images forbid the use of basic rl algorithms
for direct closed loop learning of image to action mappings according to purposive vision 
   

ficlosed loop learning of visual control policies

    achieving purposive vision through reinforcement learning
there exists a variety of work in rl on specific robotic problems involving a perceptual
space that contains images  for instance  schaal        uses visual feedback to solve a
pole balancing task  rl has been used to control a vision guided underwater robotic vehicle  wettergreen  gaskett    zelinsky         more recently  kwok and fox        have
demonstrated the applicability of rl to learning sensing strategies using aibo robots 
reinforcement learning can also be used to learn strategies for view selection  paletta  
pinz        and sequential attention models  paletta  fritz    seifert         let us also
mention the use of reinforcement learning in other vision guided tasks such as ball kicking  asada  noda  tawaratsumida    hosoda         ball acquisition  takahashi  takeda 
  asada         visual servoing  gaskett  fletcher    zelinsky         robot docking  weber  wermter    zochios        martnez marn   duckett        and obstacle avoidance  michels  saxena    ng         interestingly  rl is also used as a way of tuning the high level parameters of image processing applications  for example  peng and
bhanu        introduce rl algorithms for image segmentation  whereas yin        proposes algorithms for multilevel image thresholding  and uses entropy as a reinforcement
signal 
all of these applications preprocess the images to extract some high level information
about the observed scene that is directly relevant to the task to be solved and that feeds the
rl algorithm  this requires prior assumptions about the images perceived by the sensors
of the agent  and about the physical structure of the task itself  the preprocessing step
is task specific and is coded by hand  this contrasts with our objectives  which consist in
introducing algorithms able to learn how to directly connect the visual space to the action
space  without using manually written code and without relying on prior knowledge about
the task to be solved  our aim is to develop general algorithms that are applicable to any
visual task that can be formulated in the rl framework 
a noticeable exception is the work by iida et al         who apply rl to seek and reach
targets  and to push boxes  shibata   iida        with real robots  in this work  raw visual
signals directly feed a neural network that is trained by an actor critic architecture  in these
examples  the visual signal is downscaled and averaged into a monochrome  i e  two color 
image of               pixels  the output of four infrared sensors are also added to this
perceptual input  while this approach is effective for the specific tasks  this process can
only be used in a highly controlled environment  real world images are much richer and
could not undergo such a strong reduction in size 
    local appearance paradigm
in this paper  we propose algorithms that rely on the extraction of visual features as a
way to achieve more compact state spaces that can be used as an input to traditional rl
algorithms  indeed  buried in the noise and in the confusion of visual cues  images contain
hints of regularity  such regularities are captured by the important notion of visual features 
loosely speaking  a visual feature is a representation of some aspect of local appearance 
e g  a corner formed by two intensity edges  a spatially localized texture signature  or a
color  therefore  to analyze images  it is often sufficient for a computer program to extract
only useful information from the visual signal  by focusing its attention on robust and highly
   

fijodogne   piater

percepts

image classifier

reinforcements

detected visual class
informative visual features

reinforcement learning

actions

figure    the structure of reinforcement learning of visual classes 

informative patterns in the percepts  the program should thereafter seek the characteristic
appearance of the observed scenes or objects 
this is actually the basic postulate behind local appearance methods that have had
much success in computer vision applications such as image matching  image retrieval and
object recognition  schmid   mohr        lowe         they rely on the detection of
discontinuities in the visual signal thanks to interest point detectors  schmid  mohr   
bauckhage         similarities in images are thereafter identified using a local description
of the neighborhood around the interest points  mikolajczyk   schmid         if two images
share a sufficient number of matching local descriptors  they are considered to belong to
the same visual class 
local appearance techniques are at the same time powerful and flexible  as they are
robust to partial occlusions  and do not require segmentation or  d models of the scenes 
it seems therefore promising to introduce  in front of the rl algorithm  a feature based
image classifier that partitions the visual space into a finite set of distinct visual classes
according to the local appearance paradigm  by focusing the attention of the agent on
highly distinctive local descriptors located at interest points of the visual stimuli  the
symbol corresponding to the detected visual class could then be given as the input of a
classical  embedded rl algorithm  as shown in figure   
this preprocessing step is intended to reduce the size of the input domain  thus enhancing the rate of convergence  the generalization capabilities as well as the robustness
of rl to noise in visual domains  importantly  the same family of visual features can be
applied to a wide variety of visual tasks  thus the preprocessing step is essentially general
and task independent  the central difficulty is the dynamic selection of the discriminative
visual features  this selection process should group images that share similar  task specific
properties together in the same visual class 
    contributions
the key technical contribution of this paper consists in the introduction of reinforcement
learning algorithms that can be used when the perceptual space contains images  the
developed algorithms do not rely on a task specific pre treatment  as a consequence  they
can be used in any vision for action task that can be formalized as a markov decision
problem  we now review the three major contributions that are discussed in this paper 
   

ficlosed loop learning of visual control policies

      adaptive discretization of a visual space
our first contribution is to propose a new algorithm called reinforcement learning of visual
classes  rlvc  that combines the aforementioned ideas  rlvc is an iterative algorithm
that is suitable for learning direct image to action mappings by taking advantage of the
local appearance paradigm  it consists of two simultaneous  interleaved learning processes 
reinforcement learning of a mapping from visual classes to actions  and incremental building
of a feature based image classifier 
initially  the image classifier contains one single visual class  so that all images are
mapped to this class  of course  this introduces a kind of perceptual aliasing  or hidden
state   whitehead   ballard         the optimal decisions cannot always be made  since
percepts requiring different reactions are associated with the same class  the agent then
isolates the aliased classes  since there is no external supervisor  the agent can only rely on
a statistical analysis of the earned reinforcements  for each detected aliased class  the agent
dynamically selects a new visual feature that is distinctive  i e  that best disambiguates the
aliased percepts  the extracted local descriptor is used to refine the classifier  this way 
at each stage of the algorithm  the number of visual classes in the classifier grows  new
visual features are learned until perceptual aliasing vanishes  the resulting image classifier
is finally used to control the system 
our approach is primarily motivated by strong positive results of mccallums u tree
algorithm  mccallum         in essence  rlvc is an adaptation of u tree to visual spaces 
though the internals of the algorithms are different  the originality of rlvc lies in its
exploitation of successful local appearance features  rlvc selects a subset of such highly
relevant features in a fully closed loop  purposive learning process  we show that this
algorithm is of practical interest  as it can be successfully applied to several simulated
visual navigation tasks 
      compacting visual policies
because of its greedy nature  rlvc is prone to overfitting  splitting one visual class can
potentially improve the control policy for all the visual classes  therefore  the splitting
strategy can get stuck in local minima  once a split is made that subsequently proves
useless  it cannot be undone in the original description of rlvc  our second contribution
is to provide rlvc with the possibility of aggregating visual classes that share similar
properties  doing so has at least three potential benefits 
   useless features are discarded  which enhances generalization capabilities 
   rlvc can reset the search for good features  and
   the number of samples that the embedded rl algorithm has at its disposal for each
visual class is increased  which results in better visual control policies 
experiments indeed show an improvement in the generalization abilities  as well as a reduction of the number of visual classes and selected features 
   

fijodogne   piater

      spatial combinations of visual features
finally  the efficacy of rlvc clearly depends on the discriminative power of the visual
features  if their power is insufficient  the algorithm will not be able to completely remove
the aliasing  which will produce sub optimal control policies  practical experiments on
simulated visual navigation tasks exhibit this deficiency  as soon as the number of detected
visual features is reduced or as features are made more similar by using a less sensitive
metric  now  most objects encountered in the world are composed of a number of distinct
constituent parts  e g  a face contains a nose and two eyes  a phone possesses a keypad  
these parts are themselves recursively composed of other sub parts  e g  an eye contains an
iris and eyelashes  a keypad is composed of buttons   such a hierarchical physical structure
certainly imposes strong constraints on the spatial disposition of the visual features 
our third contribution is to show how highly informative spatial combinations of visual
features can be iteratively constructed in the framework of rlvc  this result is promising
for it permits the construction of features at increasingly higher levels of discriminative
power  enabling us to tackle visual tasks that are unsolvable using individual point features
alone  to the best of our knowledge  this extension to rlvc appears to be the very
first attempt to build visual feature hierarchies in a closed loop  interactive and purposive
learning process 

   an overview of reinforcement learning
our framework relies on the theory of rl  which is introduced in this section  in rl  the
environment is traditionally modeled as a markov decision process  mdp   an mdp is
a tuple hs  a  t   ri  where s is a finite set of states  a is a finite set of actions  t is a
probabilistic transition function from s  a to s  and r is a reinforcement function from
s  a to r  an mdp obeys the following discrete time dynamics  if at time t  the agent
takes the action at while the environment lies in a state st   the agent perceives a numerical
reinforcement rt     r st   at    then reaches some state st   with probability t  st   at   st     
thus  from the point of view of the agent  an interaction with the environment is defined
as a quadruple hst   at   rt     st   i  note that the definition of markov decision processes
assumes the full observability of the state space  which means that the agent is able to
distinguish between the states of the environment using only its sensors  this allows us to
talk indifferently about states and percepts  in visual tasks  s is a set of images 
a percept to action mapping is a fixed probabilistic function    s   a from states to
actions  a percept to action mapping tells the agent the probability with which it should
choose an action when faced with some percept  in rl terminology  such a mapping is called
a stationary markovian control policy  for an infinite sequence of interactions starting in a
state st   the discounted return is
rt  


x

 i rt i    

   

i  

where          is the discount factor that gives the current value of the future reinforcements  the markov decision problem for a given mdp is to find an optimal percept to action
mapping that maximizes the expected discounted return  whatever the starting state is  it is
   

ficlosed loop learning of visual control policies

possible to prove that this problem is well defined  in that such an optimal percept to action
mapping always exists  bellman        
markov decision problems can be solved using dynamic programming  dp  algorithms
 howard        derman         let  be a percept to action mapping  let us call the
state action value function q  s  a  of   the function giving for each state s  s and each
action a  a the expected discounted return obtained by starting from state s  taking action
a  and thereafter following the mapping  
q  s  a    e  rt   st   s  at   a   

   

where e denotes the expected value if the agent follows the mapping   let us also define
the h transform from q functions to q functions as
x
 hq  s  a    r s  a    
t  s  a  s    max
q s    a    
   
 
s  s

a a

for all s  s and a  a  note that the h transform is equally referred to as the bellman
backup operator for state action value functions  all the optimal mappings for a given mdp
share the same q function  denoted q and called the optimal state action value function 
that always exists and that satisfies bellmans so called optimality equation  bellman       
hq   q  

   

once the optimal state action value function q is known  an optimal deterministic perceptto action mapping   is easily derived by choosing
   s    argmax q  s  a  

   

aa

for each s  s  another very useful concept from the dp theory is that of optimal value
function v    for each state s  s  v   s  corresponds to the expected discounted return
when the agent always chooses the optimal action in each encountered state  i e 
v   s    max q  s  a  
aa

   

dynamic programming includes the well known value iteration  bellman         policy iteration  howard        and modified policy iteration  puterman   shin        algorithms  value iteration learns the optimal state action value function q   whereas policy
iteration and modified policy iteration directly learn an optimal percept to action mapping 
rl is a set of algorithmic methods for solving markov decision problems when the
underlying mdp is not known  bertsekas   tsitsiklis        kaelbling et al         sutton
  barto         precisely  rl algorithms do not assume the knowledge of t and r  the
input of rl algorithms is basically a sequence of interactions hst   at   rt     st   i of the agent
with its environment  rl techniques are often divided in two categories 
   model based methods that first build an estimate of the underlying mdp  e g  by
computing the relative frequencies that appear in the sequence of interactions   and
then use classical dp algorithms such as value or policy iteration 
   model free methods such as sarsa  rummery   niranjan         t d    barto 
sutton    anderson        sutton         and the popular q learning  watkins 
       that do not compute such an estimate 
   

fijodogne   piater

   reinforcement learning of visual classes
as discussed in the introduction  we propose to insert an image classifier before the rl
algorithm  this classifier maps the visual stimuli to a set of visual classes according to
the local appearance paradigm  by focusing the attention of the agent on highly distinctive
local descriptors detected at the interest points of the images 
    incremental discretization of the visual space
formally  let us call d  the infinite set of local descriptors that can be spanned through the
chosen local description method  the elements of d will be equivalently referred to as visual
features  usually  d corresponds to rn for some n     we assume the existence of a visual
feature detector   that is a boolean function d   s  d   b testing whether a given image
exhibits a given local descriptor at one of its interest points  schmid et al          any
suitable metric can be used to test the similarity of two visual features  e g  mahalanobis
or euclidean distance 
the image classifier is iteratively refined  because of this incremental process  a natural
way to implement the image classifiers is to use binary decision trees  each of their internal
nodes is labeled by the visual feature  the presence of which is to be tested in that node 
the leaves of the trees define a set of visual classes  which is hopefully much smaller than
the original visual space  and upon which it is possible to apply directly any usual rl
algorithm  to classify an image  the system starts at the root node  then progresses down
the tree according to the result of the feature detector d for each visual feature found during
the descent  until reaching a leaf 
to summarize  rlvc builds a sequence c    c    c          of growing decision trees  in a
sequence of attempts to remove perceptual aliasing  the initial classifier c  maps all of its
input images in a single visual class v      at any stage k  the classifier ck partitions the
visual perceptual space s into a finite number mk of visual classes  vk             vk mk   
    learning architecture
the resulting learning architecture has been called reinforcement learning of visual classes
 rlvc   jodogne   piater      a   the basic idea behind our algorithms  namely the
iterative learning of a decision tree  is primarily motivated by adaptive resolution techniques
that have been previously introduced in reinforcement learning  and notably by mccallums
u tree algorithm  mccallum         in this section  this idea is showed to be extremely
fruitful when suitably adapted to visual spaces  the links between rlvc and adaptiveresolution techniques will be more thoroughly discussed in section     
the components of rlvc are depicted in figure    an in depth discussion of each of
those components will be given in the next sections  for the time being  we review each of
them 
rl algorithm  for each classifier in the sequence  an arbitrary  standard rl algorithm
is applied  this provides information such as the optimal state action function  the
optimal value function or the optimal policy that are induced by the current classifier
ck   for the purpose of these computations  either new interactions can be acquired 
   

ficlosed loop learning of visual control policies

figure    the different components of the rlvc algorithm 
or a database of previously collected interactions can be exploited  this component
is covered in sections       and       
aliasing detector  until the agent has learned the visual classes required to complete
its task  from the viewpoint of the embedded rl algorithm  the input space is only
partially observable  the aliasing detector extracts the classes in which perceptual
aliasing occurs  through an analysis of the bellman residuals  indeed  as explained
in section        there exist tight relations between perceptual aliasing and bellman
residuals  if no aliased class is detected  rlvc stops 
feature generator  after having applied the rl algorithm  a database of interactions
hst   at   rt     st   i is available  the feature generator component produces a set f of
candidate visual features for each aliased class vk i   the features that are used to
refine a classifier will be chosen among this set of candidates  this step is further
exposed in sections     and   
feature selector  once the set of candidate features f is built for the aliased visual
class vk i   this component selects the visual feature f   f that best reduces the
perceptual aliasing  if no candidate feature is discriminant  the component returns
the conventional bottom symbol   the feature selector of rlvc is described in
section     
classifier refinement  the leaves that correspond to the aliased classes in the featurebased image classifier are replaced by an internal node testing the presence or absence
of the selected visual features 
post processing  this optional component is invoked after every refinement  and corresponds to techniques for fighting overfitting  details are given in section   
the general outline of rlvc is described in algorithm    note that in all the experiments that are contained in this paper  model based rl algorithms were applied to static
   

fijodogne   piater

algorithm    general structure of rlvc
   k   
   mk   
   ck  binary decision tree with one leaf
   repeat
  
collect n interactions hst   at   rt     st   i
  
apply an arbitrary rl algorithm on the sequence that is mapped through ck
  
ck    ck
  
for all i              mk   do
  
if aliased vk i   then
   
f  generator   st   ck  st     vk i   
   
f   selector  vk i   f  
   
if f      then
   
in ck     refine vk i by testing f 
   
mk    mk      
   
end if
   
end if
   
end for
   
k k  
   
post process ck  
    until ck   ck 
databases of interactions at the fifth step of algorithm    these databases were collected
using a fully randomized exploration policy  this choice was only guided by the ease of
implementation and presentation  any other way of collecting experience could be used as
well  for example by re sampling a new database of interactions at each iteration of rlvc 
the crucial point here is that rlvc generates a representation for visual control policies
only from a set of collected visuomotor experience  which makes rlvc interactive  the following sections describe the remaining algorithms  namely aliased  generator  selector
and post process 
    detection of the aliased visual classes
we now discuss how aliasing can be detected in a classifier ck  
      projection of an mdp through an image classifier
formally  any image classifier ck converts a sequence of n interactions
hst   at   rt     st   i 
to a mapped sequence of n quadruples
hck  st    at   rt     ck  st    i 
upon which the embedded rl algorithm is applied  let us define the mapped mdp mk as
the mdp
hsk   a  tk   rk i 
   

ficlosed loop learning of visual control policies

that is obtained from the mapped sequence  where sk is the set of visual classes that are
known to ck   and where tk and rk have been computed using the relative frequencies in
the mapped sequence  as follows 
consider two visual classes v  v     vk             vk mk   and one action a  a  we define
the following functions 
 t  v  a  equals   if ck  st     v and at   a  and   otherwise 
 t  v  a  v     equals   if ck  st     v   ck  st       v   and at   a  and   otherwise 
  v  a  is the number of ts such that t  v  a      
using this notation  we can write 
 sk    vk             vk mk   
p
 
 tk  v  a  v       n
t   t  v  a  v     v  a  
p
 rk  v  a    n
t   rt t  v  a   v  a  
      optimal q function for a mapped mdp
each mapped mdp mk induces an optimal q function on the domain sk  a that will be
 
denoted q 
k   computing qk can be difficult  in general  there may exist no mdp defined
on the state space sk and on the action space a that can generate a given mapped sequence 
since the latter is not necessarily markovian anymore  thus  if some rl algorithm is run
on the mapped sequence  it might not converge toward q 
k   or not even converge at all 
however  when applied on a mapped sequence  any model based rl method  cf  section   
can be used to compute q 
k if mk is used as the underlying model  under some conditions 
q learning also converges to the optimal q function of the mapped mdp  singh  jaakkola 
  jordan        
in turn  the function q 
k induces another q function on the initial domain s a through
the relation 
qk  s  a    q 
   
k  ck  s   a   
in the absence of aliasing  the agent would perform optimally  and qk would correspond to
q   according to bellman theorem that states the uniqueness of the optimal q function  cf 
section     by equation    the function
bk  s  a     hqk   s  a   qk  s  a 

   

is therefore a measure of the aliasing induced by the image classifier ck   in rl terminology 
bk is bellman residual of the function qk  sutton         the basic idea behind rlvc is
to refine the states that have a non zero bellman residual 
      measuring aliasing
consider a time stamp t in a database of interactions hst   at   rt     st   i  according to
equation    the bellman residual that corresponds to the state action pair  st   at   equals
x
bk  st   at     r st   at     
t  st   at   s    max
qk  s    a     qk  st   at   
   
 
a a

s  s

   

fijodogne   piater

algorithm    aliasing criterion
   aliased vk i    
  
for all a  a do
  
   t   ck  st     vk i  at   a 
  
if          then
  
return true
  
end if
  
end for
  
return false
unfortunately  the rl agent does not have access to the transition probabilities t and to
the reinforcement function r of the mdp modeling the environment  therefore  equation   cannot be directly evaluated  a similar problem arises in the q learning  watkins 
      and the fitted q iteration  ernst  geurts    wehenkel        algorithms  these
algorithms solve this problem by considering the stochastic version of the time difference
that is described by equation    the value
x
t  st   at   s    max
qk  s    a   
    
 
a a

s  s

can indeed be estimated as
max
qk  s    a    
 
a a

    

if the successor s  is chosen with probability t  st   at   s     but following the transitions of
the environment ensures making a transition from st to st   with probability t  st   at   st     
thus
t   rt      max
qk  st     a     qk  st   at  
a  a

 
 
  rt      max
q 
k ck  st       a  qk  ck  st    a 
 
a a

    
    

is an unbiased estimate of the bellman residual for the state action pair  st   at    jaakkola 
jordan    singh          very importantly  if the system is deterministic and in the absence
of perceptual aliasing  these estimates are equal to zero  therefore  a nonzero t potentially
indicates the presence of perceptual aliasing in the visual class vt   ck  st   with respect to
action at   our criterion for detecting the aliased classes consists in computing the q 
k
function  then in sweeping again all the interactions hst   at   rt     st   i to identify nonzero
t   in practice  we assert the presence of aliasing if the variance of the t exceeds a given
threshold    this is summarized in algorithm    where       denotes the variance of a set
of samples 
    generation and selection of distinctive visual features
once aliasing has been detected in some visual class vk i  sk with respect to an action a 
we need to select a local descriptor that best explains the variations in the set of t values
   it is worth noticing that t t corresponds to the updates that would be applied by q learning  where
t is known as the learning rate at time t 

   

ficlosed loop learning of visual control policies

algorithm    canonical feature generator
   generator  s            sn     
  
f    
  
for all i              n  do
  
for all  x  y  such that  x  y  is an interest point of si do
  
f  f   symbol descriptor si   x  y   
  
end for
  
end for
  
return f
corresponding to vk i and a  this local descriptor is to be chosen among a set of candidate
visual features f  
      extraction of candidate features
informally  the canonical way of building f for a visual class vk i consists in 
   identifying all collected visual percepts st such that ck  st     vk i  
   locating all the interest points in all the selected images st   then
   adding to f the local descriptor of all those interest points 
the corresponding feature generator is detailed in algorithm    in the latter algorithm 
descriptor s  x  y  returns the local description of the point at location  x  y  in the image s 
and symbol d  returns the symbol that corresponds to the local descriptor d  f according
to the used metric  however  more complex strategies for generating the visual features can
be used  such a strategy that builds spatial combinations of individual point features will
be presented in section   
      selection of candidate features
the problem of choosing the candidate feature that most reduces the variations in a set of
real valued bellman residuals is a regression problem  for which we suggest an adaptation of
a popular splitting rule used in the cart algorithm for building regression trees  breiman 
friedman    stone         
in cart  variance is used as an impurity indicator  the split that is selected to refine
a particular node is the one that leads to the greatest reduction in the sum of the squared
differences between the response values for the learning samples corresponding to the node
and their mean  more formally  let s    hxi   yi i  be a set of learning samples  where xi  rn
are input vectors of real numbers  and where yi  r are real valued outputs  cart selects
the following candidate feature 


v
v
f    argmin pv     s
  pv      s 
 

    

vf

   note that in our previous work  we used a splitting rule that is borrowed from the building of classification
trees  quinlan        jodogne   piater      a  

   

fijodogne   piater

algorithm    feature selection
   selector vk i   f    
  
f  
 best feature found so far 
  
r   
 variance reduction induced by f   
  
for all a  a do
  
t   t   ck  st     vk i and at   a 
  
for all visual feature f  f do
  
s   t   t  t and st exhibits f  
  
s    t   t  t and st does not exhibit f  
  
s   s    t  
   
s    s     t  
   
r  s      s     s       s   
   
if r   r and the distributions  s   s    are significantly different then
   
f  f
   
r  s
   
end if
   
end for
   
end for
   
return f 

where pv  resp  pv    is the proportion of samples that exhibit  resp  do not exhibit  the
v  resp  s v   is the set of samples that exhibit  resp  do not exhibit 
feature v  and where s
 
the feature v  this idea can be directly transferred in our framework  if the set of xi
corresponds to the set of interactions hst   at   rt     st   i  and if the set of yi corresponds to
the set of t   this is written explicitly in algorithm   
our algorithms exploit the stochastic version of bellman residuals  of course  real environments are in general non deterministic  which generates variations in bellman residuals
that are not a consequence of perceptual aliasing  rlvc can be made somewhat robust to
such a variability by introducing a statistical hypothesis test  for each candidate feature 
a students ttest is used to decide whether the two sub distributions the feature induces
are significantly different  this approach is also used in u tree  mccallum        
    illustration on a simple navigation task
we have evaluated our system on an abstract task that closely parallels a real world scenario
while avoiding any unnecessary complexity  as a consequence  the sensor model we use may
seem unrealistic  a better visual sensor model will be exploited in section     
rlvc has succeeded at solving the continuous  noisy visual navigation task depicted
in figure    the goal of the agent is to reach as fast as possible one of the two exits of
the maze  the set of possible locations is continuous  at each location  the agent has four
possible actions  go up  right  down  or left  every move is altered by a gaussian noise 
the standard deviation of which is    the size of the maze  glass walls are present in the
maze  whenever a move would take the agent into a wall or outside the maze  its location
is not changed 
   

ficlosed loop learning of visual control policies

figure    a continuous  noisy navigation task  the exits of the maze are indicated by boxes
with a cross  walls of glass are identified by solid lines  the agent is depicted at
the center of the figure  each one of the four possible moves is represented by an
arrow  the length of which corresponds to the resulting move  the sensors return
a picture that corresponds to the dashed portion of the image 

the agent earns a reward of     when an exit is reached  any other move  including
the forbidden ones  generates zero reinforcement  when the agent succeeds in escaping the
maze  it arrives in a terminal state in which every move gives rise to a zero reinforcement 
in this task   was set to      note that the agent is faced with the delayed reward problem 
and that it must take the distance to the two exits into consideration for choosing the most
attractive one 
the maze has a ground carpeted with a color image of            pixels that is a
montage of pictures from the coil     database  nene  nayar    murase         the
agent does not have direct access to its  x  y  position in the maze  rather  its sensors
take a picture of a surrounding portion of the ground  this portion is larger than the
blank areas  which makes the input space fully observable  importantly  the glass walls are
transparent  so that the sensors also return the portions of the tapestry that are behind
them  therefore  there is no way for the agent to directly locate the walls  it is obliged to
identify them as the regions of the maze in which an action does not change its location 
   

fijodogne   piater

figure    the deterministic image to action mapping that results from rlvc  sampled at
regularly spaced points  it manages to choose the correct action at each location 

in this experiment  we have used color differential invariants as visual features  gouet
  boujemaa         the entire tapestry includes      different visual features  rlvc
selected     features  corresponding to a ratio of    of the entire set of possible features 
the computation stopped after the generation of    image classifiers  i e  when k reached
     which took    minutes on a    ghz pentium iv using databases of        interactions 
    visual classes were identified  this is a small number  compared to the number of
perceptual classes that would be generated by a discretization of the maze when the agent
knows its  x  y  position  for example  a reasonably sized      grid leads to     perceptual
classes 
figure   shows the optimal  deterministic image to action mapping that results from
the last obtained image classifier ck  
   s    argmax qk  s  a    q 
k  ck  s   a   
aa

   

    

ficlosed loop learning of visual control policies

 a 

 b 

figure     a  the optimal value function  when the agent has direct access to its  x  y 
position in the maze and when the set of possible locations is discretized into a
       grid  the brighter the location  the greater its value   b  the final value
function obtained by rlvc 

figure   compares the optimal value function of the discretized problem with the one obtained through rlvc  the similarity between the two pictures indicates the soundness of
our approach  importantly  rlvc operates with neither pretreatment  nor human intervention  the agent is initially not aware of which visual features are important for its task 
moreover  the interest of selecting descriptors is clear in this application  a direct  tabular
representation of the q function considering all the boolean combinations of features would
have          cells 
the behavior of rlvc on real word images has also been investigated  the navigation
rules were kept identical  but the tapestry was replaced by a panoramic photograph of
          pixels of a subway station  as depicted in figure    rlvc took     iterations to
compute the mapping at the right of figure    the computation time was     minutes on a
   ghz pentium iv using databases of        interactions      distinct visual features were
selected among a set of      possible ones  generating a set of     visual classes  here again 
the resulting classifier is fine enough to obtain a nearly optimal image to action mapping
for the task 

    related work
rlvc can be thought of as performing adaptive discretization of the visual space on the
basis of the presence of visual features  previous reinforcement learning algorithms that
exploit the presence of perceptual features in various contexts are now discussed 
   

fijodogne   piater

 a 

 b 

figure     a  a navigation task with a real world image  using the same conventions than
figure     b  the deterministic image to action mapping computed by rlvc 

   

ficlosed loop learning of visual control policies

      perceptual aliasing
as explained above  the incremental selection of a set of informative visual features necessarily leads to temporary perceptual aliasing  which rlvc tries to remove  more generally 
perceptual aliasing occurs whenever an agent cannot always take the right on the basis of
its percepts 
early work in reinforcement learning has tackled this general problem in two distinct
ways  either the agent identifies and then avoids states where perceptual aliasing occurs
 as in the lion algorithm  see whitehead   ballard         or it tries to build a short term
memory that will allow it to remove the ambiguities on its percepts  as in the predictive
distinctions approach  see chrisman         very sketchily  these two algorithms detect the
presence of perceptual aliasing through an analysis of the sign of q learning updates  the
possibility of managing a short term memory has led to the development of the partially
observable markov decision processes  pomdp  theory  kaelbling  littman    cassandra 
       in which the current state is a random variable of the percepts 
although these approaches are closely related to the perceptual aliasing rlvc temporarily introduces  they do not consider the exploitation of perceptual features  indeed 
they tackle a structural problem in a given control task  and  as such  they assume that
perceptual aliasing cannot be removed  as a consequence  these approaches are orthogonal
to our research interest  since the ambiguities rlvc generates can be removed by further
refining the image classifier  in fact  the techniques above tackle a lack of information inherent to the used sensors  whereas our goal is to handle a surplus of information related
to the high redundancy of visual representations 
      adaptive resolution in finite perceptual spaces
rlvc performs an adaptive discretization of the perceptual space through an autonomous 
task driven  purposive selection of visual features  work in rl that incrementally partitions
a large  either discrete or continuous  perceptual space into a piecewise constant value
function is usually referred to as adaptive resolution techniques  ideally  regions of the
perceptual space with a high granularity should only be present where they are needed 
while a lower resolution should be used elsewhere  rlvc is such an adaptive resolution
algorithm  we now review several adaptive resolution methods that have been previously
proposed for finite perceptual spaces 
the idea of adaptive resolution techniques in reinforcement learning goes back to the
g algorithm  chapman   kaelbling         and has inspired the other approaches that
are discussed below  the g algorithm considers perceptual spaces that are made up of
fixed length binary numbers  it learns a decision tree that tests the presence of informative
bits in the percepts  this algorithm uses a students t test to determine if there is some bit
b in the percepts that is mapped to a given leaf  such that the state action utilities of states
in which b is set are significantly different from the state action utilities of states in which
b is unset  if such a bit is found  the corresponding leaf is split  the process is repeated
for each leaf  this method is able to learn compact representations  even though there is
a large number of irrelevant bits in the percepts  unfortunately  when a region is split 
all the information associated with that region is lost  which makes for very slow learning 
   

fijodogne   piater

concretely  the g algorithm can solve a task whose perceptual space contains      distinct
percepts  which corresponds to the set of binary numbers with a length of     bits 
mccallums u tree algorithm builds upon this idea by combining a selective attention
mechanism inspired by the g algorithm with a short term memory that enables the agent
to deal with partially observable environments  mccallum         therefore  mccallums
algorithms are a keystone in reinforcement learning  as they unify the g algorithm  chapman   kaelbling        with chrismans predictive distinctions  chrisman        
u tree incrementally grows a decision tree through kolmogorov smirnov tests  it has
succeeded at learning behaviors in a driving simulator  in this simulator  a percept consists
of a set of   discrete variables whose variation domains contain between   and   values 
leading to a perceptual space with        possible percepts  thus  the size of the perceptual
space is much smaller than a visual space  however  this task is difficult because the
physical state space is only partially observable through the perceptual space  the driving
task contains         physical states  which means that several physical states requiring
different reactions can be mapped to the same percept through the sensors of the agent 
u tree resolves such ambiguities on the percepts by testing the presence of perceptual
features in the percepts that have been encountered previously in the history of the system 
to this end  u tree manages a short term memory  in this paper  partially observable
environments are not considered  our challenge is rather to deal with huge visual spaces 
without hand tuned pre processing  which is in itself a difficult  novel research direction 
      adaptive resolution in continuous perceptual spaces
it is important to notice that all the methods for adaptive resolution in large scale  finite
perceptual spaces use a fixed set of perceptual features that is hard wired  this has to be
distinguished from rlvc that samples visual features from a possibly infinite visual feature
space  e g  the set of visual features is infinite   and that makes no prior assumptions
about the maximum number of useful features  from this point of view  rlvc is closer to
adaptive resolution techniques for continuous perceptual spaces  indeed  these techniques
dynamically select new relevant features from a whole continuum 
the first adaptive resolution algorithm for continuous perceptual spaces is the darling algorithm  salganicoff         this algorithm  just like all the current algorithms
for continuous adaptive resolution  splits the perceptual space using thresholds  for this
purpose  darling builds a hybrid decision tree that assigns a label to each point in the
perceptual space  darling is a fully on line and incremental algorithm that is equipped
with a forgetting mechanism that deletes outdated interactions  it is however limited to
binary reinforcement signals  and it only takes immediate reinforcements into account  so
that darling is much closer to supervised learning than to reinforcement learning 
the parti game algorithm  moore   atkeson        produces goal directed behaviors in
continuous perceptual spaces  parti game also splits regions where it deems it important 
using a game theoretic approach  moore and atkeson show that parti game can learn
competent behavior in a variety of continuous domains  unfortunately  the approach is
currently limited to deterministic domains where the agent has a greedy controller and
where the goal state is known  moreover  this algorithm searches for any solution to a
given task  and does not try to find the optimal one 
   

ficlosed loop learning of visual control policies

the continuous u tree algorithm is an extension of u tree that is adapted to continuous perceptual spaces  uther   veloso         just like darling  continuous u tree
incrementally builds a decision tree that splits the perceptual space into a finite set of hypercubes  by testing thresholds  kolmogorov smirnov and sum of squared errors are used
to determine when to split a node in the decision tree  pyeatt and howe        analyze
the performance of several splitting criteria for a variation of continuous u tree  they
conclude that students t test leads to the best performance  which motivates the use of
this statistical test in rlvc  cf  section      
munos and moore        have proposed variable resolution grids  their algorithm
assumes that the perceptual space is a compact subset of euclidean space  and begins with
a coarse  grid based discretization of the state space  in contrast with the other abstract
algorithms in this section  the value function and policy vary linearly within each region 
munos and moore use kuhn triangulation as an efficient way to interpolate the value function within regions  the algorithm refines its approximation by refining cells according to
a splitting criterion  munos and moore explore several local heuristic measures of the importance of splitting a cell including the average of corner value differences  the variance of
corner value differences  and policy disagreement  they also explore global heuristic measures involving the influence and variance of the approximated system  variable resolution
grids are probably the most advanced adaptive resolution algorithm available so far 
      discussion
to summarize  several algorithms that are similar in spirit to rlvc have been proposed
over the years  nevertheless  our work appears to be the first that can learn direct image toaction mappings through reinforcement learning  indeed  none of the reinforcement learning
methods above combines all the following desirable properties of rlvc      the set of relevant perceptual features is not chosen a priori by hand  as the selection process is fully
automatic and does not require any human intervention      visual perceptual spaces are explicitly considered through appearance based visual features  and     the highly informative
perceptual features can be drawn out of a possibly infinite set 
these advantages of rlvc are essentially due to the fact that the candidate visual
features are not selected only because they are informative  they are also ranked according
to an information theoretic measure inspired by decision tree induction  breiman et al  
       such a ranking is required  as vision for action tasks induce a large number of visual
features  a typical image contains about a thousand of them   this kind of criterion that
ranks features  though already considered in variable resolution grids  munos   moore 
       seems to be new in discrete perceptual spaces 
rlvc is defined independently of any fixed rl algorithm  which is similar in spirit to
continuous u tree  uther   veloso         with the major exception that rlvc deals with
boolean features  whereas continuous u tree works in a continuous input space  furthermore  the version of rlvc presented in this paper uses a variance reduction criterion for
ranking the visual features  this criterion  though already considered in variable resolution
grids  seems to be new in discrete perceptual spaces 
   

fijodogne   piater

   compacting visual policies
as written in section        this original version of rlvc is subject to overfitting  jodogne  
piater      b   a simple heuristic to avoid the creation of too many visual classes is simply
to bound the number of visual classes that can be refined at each stage of the algorithm 
since splitting one visual class potentially has an impact on the bellman residuals of all the
visual classes  in practice  we first try to split the classes that have the most samples before
considering the others  since there is more evidence of variance reduction for the first  in
our tests  we systematically apply this heuristics  however  it is often insufficient if taken
alone 
    equivalence relations in markov decision processes
since we apply an embedded rl algorithm at each stage k of rlvc  properties like the
optimal value function vk     the optimal state action value function qk      and the optimal
control policy k    are known for each mapped mdp mk   using those properties  it is easy
to define a whole range of equivalence relations between the visual classes  for instance 
given a threshold   r    we list hereunder three possible equivalence relations for a pair
of visual classes  v  v     
optimal value equivalence 
 vk  v    vk  v        
optimal policy equivalence 
 vk  v    qk  v     k  v       
 vk  v      qk  v  k  v         
optimal state action value equivalence 
 a  a   qk  v  a   qk  v     a     
we therefore propose to modify rlvc so that  periodically  visual classes that are
equivalent with respect to one of those criteria are merged together  we have experimentally
observed that the conjunction of the first two criteria tends to lead to the best performance 
this way  rlvc alternatively splits and merges visual classes  the compaction phase should
not be done too often  in order to allow exploration  to the best of our knowledge  this
possibility has not been investigated yet in the framework of adaptive resolution methods
in reinforcement learning 
in the original version of rlvc  the visual classes correspond to the leaves of a decision
tree  when using decision trees  the aggregation of visual classes can only be achieved by
starting from the bottom of the tree and recursively collapsing leaves  until dissimilar leaves
are found  this operation is very close to post pruning in the framework of decision trees
for machine learning  breiman et al          in practice  this means that classes that have
similar properties  but that can only be reached from one another by making a number of
hops upwards then downwards  are extremely unlikely to be matched  this greatly reduces
the interest of exploiting the equivalence relations 
this drawback is due to the rather limited expressiveness of decision trees  in a decision
tree  each visual class corresponds to a conjunction of visual feature literals  which defines a
   

ficlosed loop learning of visual control policies

path from the root of the decision tree to one leaf  to take full advantage of the equivalence
relations  it is necessary to associate  to each visual class  an arbitrary union of conjunctions
of visual features  indeed  when exploiting the equivalence relations  the visual classes are
the result of a sequence of conjunctions  splitting  and disjunctions  aggregation   thus  a
more expressive data structure that would be able to represent general  arbitrary boolean
combinations of visual features is required  such a data structure is introduced in the next
section 
    using binary decision diagrams
the problem of representing general boolean functions has been extensively studied in the
field of computer aided verification  since they can abstract the behavior of logical electronic
devices  in fact  a whole range of methods for representing the state space of richer and
richer domains have been developed over the last few years  such as binary decision diagram
 bdd   bryant         number and queue decision diagrams  boigelot         upward
closed sets  delzanno   raskin        and real vector automata  boigelot  jodogne   
wolper        
in our framework  bdd is a particularly well suited tool  it is a acyclic graph based
symbolic representation for encoding arbitrary boolean functions  and has had much success in the field of computer aided verification  bryant         a bdd is unique when the
ordering of its variables is fixed  but different variable orderings can lead to different sizes
of the bdd  since some variables can be discarded by the reordering process  although the
problem of finding the optimal variable ordering is conp complete  bryant         automatic heuristics can in practice find orderings that are close to optimal  this is interesting
in our case  since reducing the size of the bdd potentially discards irrelevant variables 
which correspond to removing useless visual features 
    modifications to rlvc
to summarize  this extension to rlvc does not use decision trees anymore  but assigns
one bdd to each visual class  two modifications are to be applied to algorithm   
   the operation of refining  with a visual feature f   a visual class v that is labeled by
the bdd b v    consists in replacing v by two new visual classes v  and v  such that
b v      b v    f and b v      b v    f  
   given an equivalence relation  the post process ck   operation consists in merging
the equivalent visual classes  to merge a pair of visual classes  v    v     v  and v  are
deleted  and a new visual class v such that b v     b v     b v    is added  every
time a merging operation takes place  it is advised to carry on variable reordering  to
minimize the memory requirements 
    experiments
we have applied the modified version of rlvc to another simulated navigation task  in
this task  the agent moves between    spots of the campus of the university of liege  cf 
figure     every time the agent is at one of the    locations  its body can aim at four possible
   

fijodogne   piater

n

 c  google map

figure    the montefiore campus at liege  red spots corresponds to the places between
which the agent moves  the agent can only follow the links between the different
spots  its goal is to enter the montefiore institute  that is labeled by a red cross 
where it gets a reward of      

orientations  north  south  west  east  the state space is therefore of size            
the agent has three possible actions  turn left  turn right  go forward  its goal is to enter
a specific building  where it will obtain a reward of       turning left or right induces a
penalty of    and moving forward  a penalty of     the discount factor  is set to     
the optimal control policy is not unique  one of them is depicted on figure   
the agent does not have direct access to its position and its orientation  rather  it
only perceives a picture of the area that is in front of it  cf  figure     thus  the agent
has to connect an input image to the appropriate reaction without explicitly knowing its
geographical localization  for each possible location and each possible viewing direction 
a database of    images of size           with significant viewpoint changes has been
collected  those    databases have been randomly divided into a learning set of    images
and a test set of   images  in our experimental setup  both versions of rlvc learn an
image to action mapping using interactions that only contain images from the learning set 
images from the test set are used to assess the accuracy of the learned visual control policies 
the sift keypoints have been used as visual features  lowe         thresholding on a
mahalanobis distance gave rise to a set of        distinct features  both versions of rlvc
have been applied on a static database of        interactions that has been collected using
a fully randomized exploration policy  the same database is used throughout the entire
algorithm  and this database only contains images that belong to the learning set 
the results of the basic version of rlvc and of the version that is extended by bdds
are reported in figures    and     the original version of rlvc has identified     visual
classes by selecting     sift features  the error rate on the computed visual policy  i e  the
proportion of sub optimal decisions when the agent is presented all the possible stimuli  was
   

ficlosed loop learning of visual control policies

 c  google map

figure    one of the optimal  deterministic control policies for the montefiore navigation
task  for each state  we have indicated the optimal action  the letter f stands
for move forward  r for turn right and l for turn left   this policy
has been obtained by applying a standard rl algorithm to the scenario in which
the agent has direct access to the  p  d  information 

     on the learning set and    when the images of the test set are used  with respect to
the optimal policy when the agent has direct access to its position and viewing direction 
the modified version rlvc was then applied  with one compacting stage every    steps 
the results are clearly superior  there is no error on the learning set anymore  while the
error rate on the test set is       the number of selected features is reduced to     
furthermore  the resulting number of visual classes becomes     instead of      thus  there
is a large improvement in the generalization abilities  as well as a reduction of the number
of visual classes and selected features  interestingly enough  the number of visual classes
     is very close to the number of physical states       which tends to indicate that the
algorithm starts to learn a physical interpretation for its percepts 
to summarize  compacting visual policies is probably a required step to deal with realistic visual tasks  if an iterative splitting process is applied  the price to pay is of course a
   

fijodogne   piater

 c  google map

figure    the percepts of the agent  four possible different percepts are shown  that correspond to the location and viewing direction marked in yellow on the top image 

   

ficlosed loop learning of visual control policies

  

  
rlvc
rlvc   bdd
  

  

  

  

 

  

  

  

  
iterations  k 

   

   

   

error rate    

  

  

  

  

  

  

 
   

 

  

  

  

  
iterations  k 

   

   

   

error rate    

rlvc
rlvc   bdd

 
   

figure     comparison of the error rates between the basic and extended versions of rlvc 
the error of the computed policy as a function of the step counter k on the images
of the learning set  resp  test set  is reported on the left of the figure  resp  on
the right  

higher computational cost  future work will focus on a theoretical justification of the used
equivalence relations  this implies bridging the gap with the theory of mdp minimization  givan  dean    greig        

   learning spatial relationships
as motivated in the introduction  section         we propose to extend rlvc by constructing a hierarchy of spatial arrangements of individual point features  jodogne  scalzo   
piater         the idea of learning models of spatial combinations of features takes its
roots in the seminal paper by fischler and elschlager        about pictorial structures 
which are collections of rigid parts arranged in deformable configurations  this idea has
become increasingly popular in the computer vision community over the   s  and has led to
a large literature about the modeling and the detection of objects  amit   kong        burl
  perona        forsyth  haddon    ioffe         crandall and huttenlocher        provide pointers to recent resources  among such recent techniques  scalzo and piater       
propose to build a probabilistic hierarchy of visual features that is represented through an
acyclic graph  they detect the presence of such a model through nonparametric belief
propagation  sudderth  ihler  freeman    willsky         other graphical models have
been proposed for representing articulated structures  such as pictorial structures  felzenszwalb   huttenlocher        kumar  torr    zisserman         similarly  the constellation
model represents objects by parts  each modeled in terms of both shape and appearance by
gaussian probability density functions  perona  fergus    zisserman        
our work contrasts with these approaches in that the generation of so called composite
features is driven by the task to be solved  this should be distinguished from the techniques
for unsupervised learning of composite features  since the additional information that is
   

fijodogne   piater

   

   
rlvc
rlvc   bdd
   

   

   

   

 

  

  

  

  
iterations  k 

   

   

   

number of classes

   

   

   

   

  

  

 
   

 

  

  

  

  
iterations  k 

   

   

   

number of selected features

rlvc
rlvc   bdd

 
   

figure     comparison of the number of generated classes and selected visual features between the basic and extended versions of rlvc  the number of visual classes
 resp  selected features  as a function of the step counter k is plotted on the left
of the figure  resp  on the right  

embedded inside the reinforcement signal drives the generation of composite features by
focusing the exploration on task relevant spatial arrangements 
in this extension of rlvc  a hierarchy of visual features is built simultaneously with
the image classifier  as soon as no sufficiently informative visual feature can be extracted 
the algorithm tries to combine two visual features in order to construct a higher level of
abstraction  which is hopefully more distinctive and more robust to noise  this extension
to rlvc assumes the co existence of two different kinds of visual features 
primitive features  they correspond to the individual point features  i e  to the localappearance descriptors  cf  section      
composite features  they consist of spatial combinations of lower level visual features 
there is no a priori bound on the maximal height of the hierarchy  therefore  a
composite feature can be potentially combined with a primitive feature  or with a
composite feature 
    detection of visual features
a natural way to represent such a hierarchy is to use a directed acyclic graph g    v  e   in
which each vertex v  v corresponds to a visual feature  and in which each edge  v  v      e
models the fact that v   is a part of the composite feature v  thus  g must be binary 
i e  any vertex should have either no child  or exactly two children  the set vp of the leaves
of g corresponds to the set of primitive features  while the set vc of its internal vertexes
represents the set of composite features 
each leaf vertex vp  vp is annotated with a local descriptor d vp    similarly  each
internal vertex vc  vc is annotated with constraints on the relative position between its
parts  in this work  we consider only constraints on the distances between the constituent
   

ficlosed loop learning of visual control policies

visual features of the composite features  and we assume that they should be distributed
according to a gaussian law g     of mean  and standard deviation   evidently  richer
constraints could be used  such as taking the relative orientation or the scaling factor between the constituent features into consideration  which would require the use of multivariate gaussians 
more precisely  let vc be a composite feature  the parts of which are v  and v    in order
to trigger the detection of vc in an image s  there should be an occurrence of v  and an
occurrence of v  in s such that their relative euclidean distance has a sufficient likelihood 
of being generated by a gaussian of mean  vc   and standard deviation  vc    to ensure
symmetry  the location of the composite feature is then taken as the midpoint between the
locations of v  and v   
the occurrences of a visual feature v in a percept s can be found using the recursive
algorithm    of course  at steps   and   of algorithm    the test does st exhibit v  can
be rewritten as a function of algorithm    by checking if occurrences v  st       
    generation of composite features
the cornerstone of this extension to rlvc is the way of generating composite features 
the general idea behind our algorithm is to accumulate statistical evidence from the relative
positions of the detected visual features in order to find conspicuous coincidences of visual
features  this is done by providing a more evolved implementation of generator s            sn  
than the one of algorithm   
      identifying spatial relations
we first extract the set f of all the  primitive or composite  features that occur within the
set of provided images  s            sn   
f    v  v    i  si exhibits v   

    

we identify the pairs of visual features the occurrences of which are highly correlated within
the set of provided images  s            sn    this simply amounts to counting the number of
co occurrences for each pair of features in f   then only keeping the pairs the corresponding
count of which exceeds a fixed threshold 
let now v  and v  be two features that are highly correlated  a search for a meaningful
spatial relationship between v  and v  is then carried out in the images  s            sn   that
contain occurrences of both v  and v    for each such co occurrence  we accumulate in a set
 the distances between the corresponding occurrences of v  and v    finally  a clustering
algorithm is applied on the distribution  in order to detect typical distances between v 
and v    for the purpose of our experiments  we have used hierarchical clustering  jain 
murty    flynn         for each cluster  a gaussian is fitted by estimating a mean value
 and a standard deviation   finally  a new composite feature vc is introduced in the
feature hierarchy  that has v  and v  as parts and such that  vc      and  vc      
in summary  in algorithm    we replace the call to algorithm   by a call to algorithm   
   

fijodogne   piater

algorithm    detecting composite features
   occurrences v  s   
  
if v is primitive then
  
return   x  y     x  y  is an interest point of s  the local descriptor of which corresponds to d v  
  
else
  
o    
  
o   occurrences subfeature   v   s 
  
o   occurrences subfeature   v   s 
  
for all p
 x    y     o  and  x    y     o  do
  
d   x   x        y   y    
   
if g d   v    v     then
   
o  o     x    x        y    y       
   
end if
   
end for
   
return o
   
end if

algorithm    generation of composite features
   generator  s            sn     
  
f    v  v    i  si exhibits v 
  
f       
  
for all  v    v     f  f do
  
if enough co occurrences of v  and v  in  s            sn   then
  
    
  
for all i              n  do
  
for all occurrences  x    y    of v  in si do
  
for all occurrences
 x    y    of v  in si do
p
   
       x   x        y   y      
   
end for
   
end for
   
end for
   
apply a clustering algorithm on 
   
for each cluster c    d          dm   in  do
   
   mean c 
   
   stddev c 
   
add to f   a composite feature vc composed of v  and v    annotated with a
mean  and a standard deviation 
   
end for
   
end if
   
end for
   
return f  

   

ficlosed loop learning of visual control policies

h p 
   

n

u
   

mg
 

  

  

 

p

   

figure     the car on the hill control problem 

      feature validation
algorithm   can generate several composite features for a given visual class vk i   however 
at the end of algorithm    at most one generated composite feature is to be kept  it is
important to notice that the performance of the clustering method is not critical for our
purpose  indeed  irrelevant spatial combinations are automatically discarded  thanks to the
variance reduction criterion of the feature selection component  in fact  the reinforcement
signal helps to direct the search for a good feature  which is an advantage over unsupervised
methods of building feature hierarchies 
    experiments
we demonstrate the efficacy of our algorithms on a version of the classical car on the hill
control problem  moore   atkeson         where the position and velocity information is
presented to the agent visually 
in this episodic task  a car  modeled by a mass point  is riding without friction on a
hill  the shape of which is defined by the function 

h p   

p  p
 p
if p     
 
p       p if p    

the goal of the agent is to reach as fast as possible the top of the hill  i e  a location such
that p     at the top of the hill  the agent obtains a reward of      the car can thrust left
or right with an acceleration of   newtons  however  because of gravity  this acceleration
is insufficient for the agent to reach the top of the hill by always thrusting toward the right 
rather  the agent has to go left for while  hence acquiring potential energy by going up the
left side of the hill  before thrusting rightward  there are two more constraints  the agent
is not allowed to reach locations such that p      and a velocity greater than   in absolute
value leads to the destruction of the car 
   

fijodogne   piater

      formal definition of the task
formally  the set of possible actions is a           while the state space is s     p  s   
 p       s       the system has the following continuous time dynamics 
p   s
s  

m

a
p

    h    p  



gh    p 
 
    h    p  

where a  a is the thrust acceleration  h    p  is the first derivative of h p   m     is the
mass of the car  and g        is the acceleration due to gravity  these continuous time
dynamics are approximated by the following discrete time state update rule 
st     st   hpt   h  st   
st     pt   hst  
where h       is the integration time step  the reinforcement signal is defined through this
expression 

    if st        st        
r  st   st    a   
 
otherwise 
in our setup  the discount factor  was set to      
this definition is actually a mix of two coexistent formulations of the car on the hill
task  ernst  geurts    wehenkel        moore   atkeson         the major differences
with the initial formulation of the problem  moore   atkeson        is that the set of
possible actions is discrete  and that the goal is at the top of the hill  rather than on a given
area of the hill   just like in the definition from ernst et al          to ensure the existence
of an interesting solution  the velocity is required to remain less than    instead of     and
the integration time step is set to h        instead of       
      inputs of the agent
in previous work  moore   atkeson        ernst et al          the agent was always assumed
to have direct access to a numerical measure of its position and velocity  the only exception
is gordons work in which a visual  low resolution representation of the global scene is given
to the agent  gordon         in our experimental setup  the agent is provided with two
cameras  one looking at the ground underneath  the second at a velocity gauge  this way 
the agent cannot directly know its current position and velocity  but has to suitably interpret
its visual inputs to derive them 
some examples of the pictures the sensors can return are presented in figure     the
ground is carpeted with a color image of           pixels that is a montage of pictures
from the coil     database  nene et al          it is very important to notice that using
individual point features is insufficient for solving this task  since the set of features in the
pictures of the velocity gauge are always the same  to know its velocity  the agent has to
generate composite features sensitive to the distance of the primitive features on the cursor
with respect to the primitive features on the digits 
   

ficlosed loop learning of visual control policies

 a 

 b 
figure      a  visual percepts corresponding to pictures of the velocity gauge when s     
s       and s         b  visual percepts returned by the position sensor  the
region framed with a white rectangle corresponds to the portion of the ground
that is returned by the sensor when p        this portion slides back and forth
as the agent moves 

      results
in this experimental setup  we used color differential invariants  gouet   boujemaa       
as primitive features  among all the possible visual inputs  both for the position and the
velocity sensors   there were    different primitive features  the entire image of the ground
includes     interest points  whereas the images of the velocity gauge include about   
interest points 
the output of rlvc is a decision tree that defines     visual classes  each internal
node of this tree tests the presence of one visual feature  taken from a set of    distinct 
highly discriminant features selected by rlvc  among the    selected visual features  there
were    primitive and    composite features  two examples of composites features that
were selected by rlvc are depicted in figure     the computation stopped after k     
refinement steps in algorithm   
to show the efficacy of our method  we compare its performance with the scenario in
which the agent has a direct perception of its current  p  s  state  in the latter scenario  the
state space was discretized in a grid of        cells  the number    was chosen since it
approximately corresponds to the square root of      the number of visual classes that were
produced by rlvc  this way  rl is provided an equivalent number of perceptual classes in
the two scenarios  figure    compares the optimal value function of the direct perception
   

fijodogne   piater

velocity

 

 

 
 

 

position

 

 a 

velocity

 

 

 
 

 

position

 

 b 

figure      a  the optimal value function  when the agent has a direct access to its current
 p  s  state  and when the input space is discretized in a        grid  the
brighter the location  the greater its value   b  the value function obtained by
rlvc 

   

ficlosed loop learning of visual control policies

figure     two composite features that were generated  in yellow  the primitive features
of which they are composed are marked in yellow  the first feature triggers for
velocities around    whereas the second triggers around   

problem with the one obtained through rlvc  here also  the two pictures are very similar 
which indicates the soundness of our approach 
we have also evaluated the performance of the optimal image to action mapping
    argmax q   p  s   a 

    

aa

obtained through rlvc  for this purpose  the agent was placed randomly on the hill  with
an initial velocity of    then  it used the mapping   to choose an action  until it reached
a final state  a set of        such trials were carried out at each step k of algorithm   
figure    compares the proportion of trials that missed the goal  either because of leaving
the hill on the left  or because of acquiring a too high velocity  in rlvc and in the directperception problem  when k became greater than     the proportion of missed trials was
always smaller in rlvc than in the direct perception problem  this advantage in favor
of rlvc is due to the adaptive nature of its discretization  figure    compares the mean
lengths of the successful trials  the mean length of rlvc trials clearly converges to that
of the direct perception trials  while staying slightly larger 
to conclude  rlvc achieves a performance close to the direct perception scenario  however  the mapping built by rlvc directly links visual percepts to the appropriate actions 
without considering explicitly the physical variables 

   summary
this paper introduces reinforcement learning of visual classes  rlvc   rlvc is designed
to learn mappings that directly connect visual stimuli to output actions that are optimal
for the surrounding environment  the framework of rlvc is general  in the sense that it
can be applied to any problem that can be formulated as a markov decision problem 
the learning process behind our algorithms is closed loop and flexible  the agent
takes lessons from its interactions with the environment  according to the purposive vision
paradigm  rlvc focuses the attention of an embedded reinforcement learning algorithm
on highly informative and robust parts of the inputs by testing the presence or absence of
local descriptors at the interest points of the input images  the relevant visual features
   

fijodogne   piater

  
rlvc
direct perception    x   grid 

missed goal    

  

  

  

 

 

 

 

  

  

  

  

  

  

  

iterations  k 

figure     evolution of the number of times the goal was missed over the iterations of
rlvc 

  

rlvc
direct perception    x   grid 

  

mean length of an interaction

  

  

  

  

  

  

 

 

  

  

  

  

  

  

  

iterations  k 

figure     evolution of the mean lengths of the successful trials over the iterations of rlvc 

   

ficlosed loop learning of visual control policies

are incrementally selected in a sequence of attempts to remove perceptual aliasing  the
discretization process targets zero bellman residuals and is inspired from supervised learning algorithms for building decision trees  our algorithms are defined independently of any
interest point detector  schmid et al         and of any local description technique  mikolajczyk   schmid         the user may choose these two components as he sees fit 
techniques for fighting overfitting in rlvc have also been proposed  the idea is to
aggregate visual classes that share similar properties with respect to the theory of dynamic
programming  interestingly  this process enhances the generalization abilities of the learned
image to action mapping  and reduces the number of visual classes that are built 
finally  an extension of rlvc is introduced that allows the closed loop  interactive
and purposive learning of a hierarchy of geometrical combinations of visual features  this
is to contrast to most of the prior work on the topic  that uses either a supervised or
unsupervised framework  piater        fergus  perona    zisserman        bouchard  
triggs        scalzo   piater         besides the novelty of the approach  we have shown
its practical value in visual control tasks in which the information provided by the individual
point features alone is insufficient for solving the task  indeed  spatial combinations of visual
features are more informative and more robust to noise 

   future work
the area of applications of rlvc is wide  since nowadays robotic agents are often equipped
with ccd sensors  future research includes the demonstration of the applicability of our
algorithms in a reactive robotic application  such as grasping objects by combining visual
and haptic feedback  coelho  piater    grupen         this necessitates the extension of
our techniques to continuous action spaces  for which no fully satisfactory solutions exist to
date  rlvc could also be potentially be applied to human computer interaction  as the
actions need not be physical actions 
the closed loop learning of a hierarchy of visual feature also raises interesting research
directions  for example  the combination of rlvc with techniques for disambiguating
between aliased percepts using a short term memory  mccallum        could solve visual
tasks in which the percepts of the agent alone do not provide enough information for solving
the task  likewise  the unsupervised learning of other kinds of geometrical models  felzenszwalb   huttenlocher        could potentially be embedded in rlvc  on the other hand 
spatial relationships do not currently take into consideration the relative angles between the
parts of a composite feature  doing so would further increase the discriminative power of the
composite features  but requires non trivial techniques for clustering in circular domains 

acknowledgments
the authors thank the associate editor thorsten joachims and the three anonymous reviewers for their many suggestions for improving the quality of the manuscript  sebastien
jodogne gratefully acknowledge the financial support of the belgian national fund for
scientific research  fnrs  
   

fijodogne   piater

references
aloimonos  y          purposive and qualitative active vision  in proc  of the   th international conference on pattern recognition  pp         
amit  y     kong  a          graphical templates for model registration  ieee transactions on pattern analysis and machine intelligence                 
asada  m   noda  s   tawaratsumida  s     hosoda  k          vision based behavior acquisition for a shooting robot by using a reinforcement learning  in proc  of iapr ieee
workshop on visual behaviors  pp         
bagnell  j     schneider  j          autonomous helicopter control using reinforcement
learning policy search methods  in proc  of the international conference on robotics
and automation  ieee 
barto  a   sutton  r     anderson  c          neuronlike adaptive elements that can
solve difficult learning control problems  ieee transactions on systems  man and
cybernetics                 
bellman  r          dynamic programming  princeton university press 
bertsekas  d     tsitsiklis  j          neuro dynamic programming  athena scientific 
boigelot  b          symbolic methods for exploring infinite state spaces  ph d  thesis 
university of liege  liege  belgium  
boigelot  b   jodogne  s     wolper  p          an effective decision procedure for linear
arithmetic with integer and real variables  acm transactions on computational logic
 tocl                 
bouchard  g     triggs  b          hierarchical part based visual object categorization  in
ieee conference on computer vision and pattern recognition  vol     pp         
san diego  ca  usa  
breiman  l   friedman  j     stone  c         
wadsworth international group 

classification and regression trees 

bryant  r          graph based algorithms for boolean function manipulation  ieee transactions in computers                 
bryant  r          symbolic boolean manipulation with ordered binary decision diagrams 
acm computing surveys                 
burl  m     perona  p          recognition of planar object classes  in proc  of the ieee
conference on computer vision and pattern recognition  pp          san francisco
 ca  usa  
chapman  d     kaelbling  l          input generalization in delayed reinforcement learning  an algorithm and performance comparisons  in proc  of the   th international
joint conference on artificial intelligence  ijcai   pp          sydney 
chrisman  l          reinforcement learning with perceptual aliasing  the perceptual
distinctions approach  in national conference on artificial intelligence  pp         
   

ficlosed loop learning of visual control policies

coelho  j   piater  j     grupen  r          developing haptic and visual perceptual categories for reaching and grasping with a humanoid robot  robotics and autonomous
systems  special issue on humanoid robots                  
crandall  d     huttenlocher  d          weakly supervised learning of part based spatial
models for visual object recognition  in proc  of the  th european conference on
computer vision 
delzanno  g     raskin  j  f          symbolic representation of upward closed sets  in
tools and algorithms for the construction and analysis of systems  lecture notes in
computer science  pp          berlin  germany  
derman  c          finite state markovian decision processes  academic press  new york 
ernst  d   geurts  p     wehenkel  l          iteratively extending time horizon reinforcement learning  in proc  of the   th european conference on machine learning  pp 
       dubrovnik  croatia  
ernst  d   geurts  p     wehenkel  l          tree based batch mode reinforcement learning 
journal of machine learning research            
felzenszwalb  p     huttenlocher  d          pictorial structures for object recognition 
international journal of computer vision               
fergus  r   perona  p     zisserman  a          object class recognition by unsupervised
scale invariant learning  in ieee conference on computer vision and pattern recognition  vol     pp          madison  wi  usa  
fischler  m     elschlager  r          the representation and matching of pictorial structures  ieee transactions on computers               
forsyth  d   haddon  j     ioffe  s          finding objects by grouping primitives  in shape 
contour and grouping in computer vision  pp          london  uk   springerverlag 
gaskett  c   fletcher  l     zelinsky  a          reinforcement learning for visual servoing
of a mobile robot  in proc  of the australian conference on robotics and automation 
melbourne  australia  
gibson  e     spelke  e          the development of perception  in flavell  j  h     markman  e  m   eds    handbook of child psychology vol  iii  cognitive development
  th edition    chap     pp       wiley 
givan  r   dean  t     greig  m          equivalence notions and model minimization in
markov decision processes  artificial intelligence                   
gordon  g          stable function approximation in dynamic programming  in proc  of
the international conference on machine learning  pp         
gouet  v     boujemaa  n          object based queries using color points of interest  in
ieee workshop on content based access of image and video libraries  pp       
kauai  hi  usa  
howard  r          dynamic programming and markov processes  technology press and
wiley  cambridge  ma  and new york 
   

fijodogne   piater

huber  m     grupen  r          a control structure for learning locomotion gaits  in  th
int  symposium on robotics and applications  anchorage  ak  usa   tsi press 
iida  m   sugisaka  m     shibata  k          direct vision based reinforcement learning
to a real mobile robot  in proc  of international conference of neural information
processing systems  vol     pp           
jaakkola  t   jordan  m     singh  s          convergence of stochastic iterative dynamic
programming algorithms  in cowan  j  d   tesauro  g     alspector  j   eds    advances in neural information processing systems  vol     pp          morgan kaufmann publishers 
jain  a  k   murty  m  n     flynn  p  j          data clustering  a review  acm computing
surveys                 
jodogne  s     piater  j       a   interactive learning of mappings from visual percepts
to actions  in de raedt  l     wrobel  s   eds    proc  of the   nd international
conference on machine learning  icml   pp          bonn  germany   acm 
jodogne  s     piater  j       b   learning  then compacting visual policies  extended abstract   in proc  of the  th european workshop on reinforcement learning  ewrl  
pp       napoli  italy  
jodogne  s   scalzo  f     piater  j          task driven learning of spatial combinations
of visual features  in proc  of the ieee workshop on learning in computer vision
and pattern recognition  san diego  ca  usa   ieee 
kaelbling  l   littman  m     cassandra  a          planning and acting in partially
observable stochastic domains  artificial intelligence                   
kaelbling  l   littman  m     moore  a          reinforcement learning  a survey  journal
of artificial intelligence research            
kimura  h   yamashita  t     kobayashi  s          reinforcement learning of walking
behavior for a four legged robot  in proc  of the   th ieee conference on decision
and control  orlando  fl  usa  
kohl  n     stone  p          policy gradient reinforcement learning for fast quadrupedal
locomotion  in proc  of the ieee international conference on robotics and automation  pp            new orleans 
kumar  m   torr  p     zisserman  a          extending pictorial structures for object
recognition  in proc  of the british machine vision conference 
kwok  c     fox  d          reinforcement learning for sensing strategies  in proc  of the
ieee international conference on intelligent robots and systems 
lagoudakis  m     parr  r          least squares policy iteration  journal of machine
learning research              
lowe  d          distinctive image features from scale invariant keypoints  international
journal of computer vision                
martnez marn  t     duckett  t          fast reinforcement learning for vision guided
mobile robots  in proc  of the ieee international conference on robotics and automation  pp        barcelona  spain  
   

ficlosed loop learning of visual control policies

mccallum  r          reinforcement learning with selective perception and hidden state 
ph d  thesis  university of rochester  new york 
michels  j   saxena  a     ng  a          high speed obstacle avoidance using monocular
vision and reinforcement learning  in proc  of the   nd international conference in
machine learning  pp          bonn  germany  
mikolajczyk  k     schmid  c          a performance evaluation of local descriptors  in
proc  of the ieee conference on computer vision and pattern recognition  vol    
pp          madison  wi  usa  
moore  a     atkeson  c          the parti game algorithm for variable resolution reinforcement learning in multidimensional state spaces  machine learning     
munos  r     moore  a          variable resolution discretization in optimal control  machine learning             
nene  s   nayar  s     murase  h          columbia object image library  coil       tech 
rep  cucs         columbia university  new york 
ng  a   coates  a   diel  m   ganapathi  v   schulte  j   tse  b   berger  b     liang  e 
        inverted autonomous helicopter flight via reinforcement learning  in proc  of
the international symposium on experimental robotics 
paletta  l   fritz  g     seifert  c          q learning of sequential attention for visual object
recognition from informative local descriptors   in proc  of the   nd international
conference on machine learning  icml   pp          bonn  germany  
paletta  l     pinz  a          active object recognition by view integration and reinforcement learning  robotics and autonomous systems                
peng  j     bhanu  b          closed loop object recognition using reinforcement learning 
ieee transactions on pattern analysis and machine intelligence                 
perona  p   fergus  r     zisserman  a          object class recognition by unsupervised
scale invariant learning  in conference on computer vision and pattern recognition 
vol     p      
piater  j          visual feature learning  ph d  thesis  university of massachusetts 
computer science department  amherst  ma  usa  
puterman  m     shin  m          modified policy iteration algorithms for discounted
markov decision problems  management science               
pyeatt  l     howe  a          decision tree function approximation in reinforcement
learning  in proc  of the third international symposium on adaptive systems  pp 
      havana  cuba 
quinlan  j          c     programs for machine learning  morgan kaufmann publishers
inc   san francisco  ca  usa  
randlv  j     alstrm  p          learning to drive a bicycle using reinforcement learning
and shaping  in proc  of the   th international conference on machine learning  pp 
        madison  wi  usa   morgan kaufmann 
   

fijodogne   piater

rummery  g     niranjan  m          on line q learning using connectionist sytems  tech 
rep  cued f infeng tr      cambridge university 
salganicoff  m          density adaptive learning and forgetting  in proc  of the   th
international conference on machine learning  pp          amherst  ma  usa  
morgan kaufmann publishers 
scalzo  f     piater  j          unsupervised learning of dense hierarchical appearance
representations  in proc  of the   th international conference on pattern recognition 
hong kong 
schaal  s          learning from demonstration  in mozer  m  c   jordan  m     petsche 
t   eds    advances in neural information processing systems  vol     pp           
cambridge  ma  mit press 
schmid  c     mohr  r          local greyvalue invariants for image retrieval  ieee
transactions on pattern analysis and machine intelligence                 
schmid  c   mohr  r     bauckhage  c          evaluation of interest point detectors 
international journal of computer vision                 
schyns  p     rodet  l          categorization creates functional features  journal of
experimental psychology  learning  memory and cognition                 
shibata  k     iida  m          acquisition of box pushing by direct vision based reinforcement learning  in proc  of the society of instrument and control engineers annual
conference  p    
singh  s   jaakkola  t     jordan  m          reinforcement learning with soft state aggregation  in advances in neural information processing systems  vol     pp         
mit press 
sudderth  e   ihler  a   freeman  w     willsky  a          nonparametric belief propagation  in proc  of the ieee conference on computer vision and pattern recognition 
pp         
sutton  r          learning to predict by the methods of temporal differences  machine
learning             
sutton  r     barto  a          reinforcement learning  an introduction  mit press 
takahashi  y   takeda  m     asada  m          continuous valued q learning for visionguided behavior acquisition  in proc  of the international conference on multisensor
fusion and integration for intelligent systems  pp         
tarr  m     cheng  y          learning to see faces and objects  trends in cognitive
sciences              
tesauro  g          temporal difference learning and td gammon  communications of
the acm               
uther  w     veloso  m          tree based discretization for continuous state space reinforcement learning  in proc  of the   th national conference on artificial intelligence
 aaai   pp          madison  wi  usa  
   

ficlosed loop learning of visual control policies

watkins  c          learning from delayed rewards  ph d  thesis  kings college  cambridge  uk  
weber  c   wermter  s     zochios  a          robot docking with neural vision and
reinforcement  knowledge based systems                  
wettergreen  d   gaskett  c     zelinsky  a          autonomous guidance and control for
an underwater robotic vehicle  in proc  of the international conference on field and
service robotics  pittsburgh  usa  
whitehead  s     ballard  d          learning to perceive and act by trial and error 
machine learning          
yin  p  y          maximum entropy based optimal threshold selection using deterministic
reinforcement learning with controlled randomization  signal processing         
     
yoshimoto  j   ishii  s     sato  m          application of reinforcement learning to balancing acrobot  in proc  of the      ieee international conference on systems 
man and cybernetics  pp         

   

fi