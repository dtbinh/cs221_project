journal artificial intelligence research               

submitted       published     

cutset sampling bayesian networks
bozhena bidyuk
rina dechter

bbidyuk ics uci edu
dechter ics uci edu

school information computer science
university california irvine
irvine  ca           

abstract
paper presents new sampling methodology bayesian networks samples
subset variables applies exact inference rest  cutset sampling
network structure exploiting application rao blackwellisation principle sampling
bayesian networks  improves convergence exploiting memory based inference algorithms  viewed anytime approximation exact cutset conditioning
algorithm developed pearl  cutset sampling implemented efficiently
sampled variables constitute loop cutset bayesian network and  generally 
induced width networks graph conditioned observed sampled variables bounded constant w  demonstrate empirically benefit scheme
range benchmarks 

   introduction
sampling common method approximate inference bayesian networks 
exact algorithms impractical due prohibitive time memory demands  often
feasible approach offers performance guarantees  given bayesian network
variables x    x         xn    evidence e  set samples  x t    p  x e  
estimate f x  expected value function f  x  obtained generated
samples via ergodic average 
 x
f  x t     
   
e f  x  e  f x   

number samples  f x  shown converge exact value
increases  central query interest bayesian networks computing posterior
marginals p  xi  e  value xi variable xi   called belief updating 
query  f  x  equals  function  equation reduces counting fraction
occurrences xi   xi samples 
p  xi  e   
 t 


 x
 xi  x t     


   

t  

 xi  x t      iff xi   xi  xi  x t      otherwise  alternatively  mixture estimator used 

 x
 t 
p  xi  xi    
   
p  xi  e    

t  

c
    
ai access foundation  rights reserved 

fibidyuk   dechter

 t 

xi   x t   xi  
significant limitation sampling  however  statistical variance increases
number variables network grows therefore number samples
necessary accurate estimation increases  paper  present sampling scheme
bayesian networks discrete variables reduces sampling variance sampling
subset variables  technique known collapsing rao blackwellisation 
fundamentals rao blackwellised sampling developed casella robert
       liu  wong  kong        gibbs sampling maceachern  clyde 
liu        doucet  gordon  krishnamurthy        importance sampling 
doucet  de freitas  murphy  russell        extended rao blackwellisation particle
filtering dynamic bayesian networks 
basic rao blackwellisation scheme described follows  suppose partition space variables x two subsets c z  subsequently  re write
function f  x  f  c  z   generate samples distribution p  c e  compute e f  c  z  c  e   perform sampling subset c only  generating samples
c      c           c t   approximating quantity interest
e f  c  z  e    ec  ez  f  c  z  c  e   f x   


 x
ez  f  c  z  c t    e   


   

t  

posterior marginals estimates cutset variables obtained using expression similar eq     
 x
 ci  c t     
   
p  ci  e   


using mixture estimator similar eq     
p  ci  e   

 x
 t 
p  ci  ci   e   


   

xi x c  e  e p  xi  e     ec  p  xi  c  e   eq     becomes
p  xi  e   

 x
p  xi  c t    e   


   

since convergence rate gibbs sampler tied maximum correlation
two samples  liu         expect improvement convergence rate
sampling lower dimensional space since    highly correlated variables may
marginalized    dependencies variables inside smaller set likely
weaker variables farther apart sampling distributions
smoothed out  additionally  estimates obtained sampling lower dimensional
space expected lower sampling variance therefore require fewer samples
achieve accuracy estimates  hand  cost generating
sample may increase  indeed  principles rao blackwellised sampling
applied classes probabilistic models specialized structure  kong  liu 
  wong        escobar        maceachern        liu        doucet   andrieu       
andrieu  de freitas    doucet        rosti   gales        
 

ficutset sampling bayesian networks

contribution paper presenting general  structure based scheme
applies rao blackwellisation principle bayesian networks  idea exploit
property conditioning subset variables simplifies networks structure  allowing efficient query processing exact algorithms  general  exact inference variable
elimination  dechter      a        join tree algorithms  lauritzen   spiegelhalter       
jensen  lauritzen    olesen        time space exponential induced width w
network  however  subset variables assigned  i e   conditioned upon 
induced width conditioned network may reduced 
idea cutset sampling choose subset variables c conditioning
c yields sparse enough bayesian network small induced width allow exact
inference  since sample assignment cutset variables  efficiently generate
new sample cutset variables conditioned network computation
p  c e  p  xi  c  e  bounded  particular  sampling set c cuts cycles
network  i e   loop cutset   inference conditioned network becomes
linear  general  c w cutset  namely subset nodes assigned 
induced width conditioned network w  time space complexity computing
next sample o  c  n dw     maximum domain size n    x  

idea exploiting properties conditioning subset variables first proposed exact belief updating context cutset conditioning  pearl        
scheme requires enumerating instantiations cutset variables  since number
instances exponential size cutset  c   sampling cutset space may
right compromise size cutset big  thus  sampling cutset
viewed anytime approximation cutset conditioning approach 
although rao blackwellisation general cutset sampling particular applied context sampling algorithm  introduce principle context gibbs sampling  geman   geman        gilks  richardson    spiegelhalter       
mackay         markov chain monte carlo sampling method bayesian networks 
extension sampling approach graphical models  markov
networks  straight forward  recently demonstrated idea incorporated importance sampling  bidyuk   dechter        
paper defines analyzes cutset sampling scheme investigates empirically
trade offs sampling exact computation variety randomly generated
networks grid structure networks well known real life benchmarks cpcs
networks coding networks  show cutset sampling converges faster pure
sampling terms number samples  dictated theory  almost always
time wise cost effective benchmarks tried  demonstrate applicability
scheme deterministic networks  hailfinder network coding networks 
markov chain non ergodic gibbs sampling converge 
section   provides background information  specifically  section     introduces bayesian
networks  section     reviews exact inference algorithms bayesian networks  section     provides background gibbs sampling  contribution paper presenting
cutset sampling starts section    section   presents empirical evaluation cutset
sampling  present empirical evaluation sampling variance resulting standard error based method batch means  for details  see geyer        
 

fibidyuk   dechter

section    review previous application rao blackwellisation section   provides
summary conclusions 

   background
section  define essential terminology provide background information
bayesian networks 
    preliminaries
use upper case letters without subscripts  x  denote sets variables
lower case letters without subscripts denote instantiation group variables  e g  
x indicates variable set x assigned value   use upper case letter
subscript  xi   denote single variable lower case letter
subscript  xi   denote instantiated variable  e g   xi denotes arbitrary value
domain xi means xi   xi    d xi   denotes domain variable xi  
superscript subscripted lower case letter would used distinguish different specific
values variable  i e   d xi      x i   x i         use x denote instantiation
set variables x    x         xi    xi   xi          xn   xi   x xi denote x element
xi removed  namely  xi    x    x         xi    xi          xn   
definition      graph concepts  directed graph pair d  v  e   v  
 x         xn   set nodes e     xi   xj   xi   xj v   set edges  given
 xi   xj   e  xi called parent xj   xj called child xi   set
xi parents denoted pa xi    pai   set xi children denoted ch xi   
chi   family xi includes xi parents  moral graph directed graph
undirected graph obtained connecting parents nodes
removing arrows  cycle cutset undirected graph subset nodes that 
removed  yields graph without cycles  loop directed graph subgraph
whose underlying graph cycle  directed graph acyclic directed loops 
directed graph singly connected  also called poly tree   underlying undirected
graph cycles  otherwise  called multiply connected 
definition      loop cutset  vertex v sink respect loop l two
edges adjacent v l directed v  vertex sink respect
loop l called allowed vertex respect l  loop cutset directed graph
set vertices contains least one allowed vertex respect loop d 
definition      belief networks  let x    x         xn   set random variables
multi valued domains d x          d xn    belief network  bn   pearl       
pair  g  p   g directed acyclic graph whose nodes variables x
p    p  xi  pai   i           n  set conditional probability tables  cpts  associated
xi   bn represents joint probability distribution product form 
p  x          xn    

n

i  

p  xi  pa xi   

evidence e instantiated subset variables e x 
 

ficutset sampling bayesian networks

structure directed acyclic graph g reflects dependencies
variables using d separation criterion  parents variable xi together children
parents children form markov blanket  denoted markovi   node xi  
use xmarkovi denote x restricted variables markovi   know node
xi independent rest variables conditioned markov blanket  namely 
p  xi  xi     p  xi  xmarkovi   
common query belief networks belief updating task
computing posterior distribution p  xi  e  given evidence e query variable xi
x  reasoning bayesian networks np hard  cooper         finding approximate
posterior marginals fixed accuracy np hard  dagum   luby        abdelbar
  hedetniemi         network poly tree  belief updating inference
tasks accomplished time linear size input  general  exact inference
exponential induced width networks moral graph 
definition      induced width  width node ordered undirected graph
number nodes neighbors precede ordering  width
ordering d  denoted w d   maximum width nodes  induced width
ordered graph  w  d   width ordered graph obtained processing
nodes last first follows  node x processed  preceding neighbors
connected  resulting graph called induced graph triangulated graph 
task finding minimal induced width graph  over possible orderings 
np complete  arnborg        
    reasoning bayesian networks
belief propagation algorithm  introduce section       below  performs belief
updating singly connected bayesian networks time linear size input
 pearl         loopy networks  two main approaches belief updating cutset
conditioning tree clustering  algorithms often referred inference
algorithms  briefly describe idea clustering algorithms section      
conditioning method section       
      variable elimination join tree clustering  jtc 
join tree clustering approach  jtc  refers family algorithms including jointree propagation  lauritzen   spiegelhalter        jensen et al         bucket tree
elimination  dechter            a   idea first obtain tree decomposition
network clusters functions connected tree propagate messages
clusters tree  tree decomposition singly connected undirected
graph whose nodes  called clusters  contain subsets variables input functions
defined variables  tree decomposition must contain function
satisfy running intersection property  maier         unifying perspective treedecomposition schemes see  zhang   poole        dechter      b  kask  dechter  larrosa 
  dechter        
given tree decomposition network  message propagation tree
synchronized  select one cluster root tree propagate messages
 

fibidyuk   dechter

tree  message cluster vi neighbor vj function
separator set vi vj marginalization product functions vi
messages vi received neighbors besides vj   assuming maximum
number variables cluster w     maximum domain size d  time
space required process one cluster o d w       since maximum number clusters
bounded  x    n   complexity variable elimination algorithms cluster tree
propagation schemes o n d w       parameter w  maximum cluster size minus
   called tree width tree decomposition  minimal tree width identical
minimal induced width graph 
      iterative belief propagation  ibp 
belief propagation  bp  iterative message passing algorithm performs exact inference singly connected bayesian networks  pearl         iteration  every node xi
sends j  xi   message child j receives j  xi   message child 
message passing order organized converges two iterations  essence
algorithm join tree clustering approach applied directly poly tree 
applied bayesian networks loops  algorithm usually iterates longer  until may
converge  hence  known iterative belief propagation  ibp  loopy belief propagation  ibp provides guarantees convergence quality approximate posterior
marginals shown perform well practice  rish  kask    dechter        murphy 
weiss    jordan         considered best algorithm inference coding networks
 frey   mackay        kschischang   frey        finding probable variable
values equals decoding process  mceliece  mackay    cheng         algorithm ibp
requires linear space usually converges fast converges  benchmarks  ibp
converged within    iterations less  see section    
      cutset conditioning
tree width w bayesian network large requirements inference
schemes bucket elimination join tree clustering  jtc  exceed available memory 
switch alternative cutset conditioning schemes  pearl        peot   shachter 
      shachter  andersen    solovitz         idea cutset conditioning select
subset variables c x e  cutset  obtain posterior marginals node
xi x c  e using 
x
p  xi  e   
p  xi  c  e p  c e 
   
cd c 

eq     implies enumerate instantiations c  perform exact inference cutset instantiation c obtain p  xi  c  e  p  c e  sum
results  total computation time exponential size cutset
enumerate instantiations cutset variables 
c loop cutset  then  nodes c assigned  bayesian network
transformed equivalent poly tree p  xi  c  e  p  c e  computed via
bp time space linear size network  example  subset  a  d 
loop cutset belief network shown figure    left  evidence e   e  right 
 

ficutset sampling bayesian networks


b

c



f

e

g







b

c



f

e

g



figure    nodes loopy bayesian network  left  instantiated 
network transformed equivalent singly connected network  right  
transformation process  replica observed node created
child node 

figure   shows equivalent singly connected network resulting assigning values
d 
well known minimum induced width w network always less
size smallest loop cutset  bertele   brioschi        dechter         namely 
w      c  c  thus  inference approaches  e g   bucket elimination  never
worse often better cutset conditioning time wise  however  w
large must resort cutset conditioning search order trade space time 
considerations yield hybrid search inference approach  since observed variables
break dependencies network  network observed subset variables
c often transformed equivalent network smaller induced width  wc  
term adjusted induced width  hence  subset variables c x
observed  complexity bounded exponentially adjusted induced width
graph wc  
definition      adjusted induced width  given graph g  x e   adjusted
induced width g relative c  denoted wc   induced width c removed
moral graph 
definition      w cutset  given graph g  x e   subset nodes c x
w cutset g adjusted induced width equals w 
c w cutset  quantities p  xi  c  e  p  c e  computed time
space exponential w  much smaller tree width unconditioned
network  resulting scheme requires memory exponential w time o d c  n d w     
n size network maximum domain size  thus  performance
tuned available system memory resource via bounding parameter w 
given constant w  finding minimal w cutset  to minimize cutset conditioning
time  hard problem  several greedy heuristic approaches proposed
geiger fishelson        bidyuk dechter               elaborate
section     
 

fibidyuk   dechter

    gibbs sampling
since complexity inference algorithms memory exponential networks induced
width  or tree width  since resorting cutset conditioning scheme may take
much time w cutset size large  must often resort approximation
methods  sampling methods bayesian networks commonly used approximation
techniques  section provides background gibbs sampling  markov chain monte
carlo method  one popular sampling schemes focus
paper  although method may applied networks continuous distributions 
limit attention paper discrete random variables finite domains 
      gibbs sampling bayesian networks
ordered gibbs sampler
input  belief network b x  x         xn   evidence e   xi   ei   xi e x  
output  set samples  x t           t  
   
   initialize  assign random value xi variable xi x e d xi    assign
evidence variables observed values 
   generate samples 
    t  generate new sample x t   
 t 
    n  compute new value xi variable xi  
 t 
 t 
 t 
compute distribution p  xi  xmarkovi   sample xi p  xi  xmarkovi   
 t 

set xi   xi  
end
end

figure    gibbs sampling algorithm
given bayesian network variables x    x         xn    evidence e  gibbs
sampling  geman   geman        gilks et al         mackay        generates set
 t 
 t 
samples  x t    sample x t     x         xn   instantiation variables 
 t 
superscript denotes sample index xi value xi sample t  first
 t 
sample initialized random  generating new sample sample xi  
 t 
new value variable xi sampled probability distribution p  xi  xi    recall
 t 

 t   

p  xi  xi     p  xi  x 

 t   

 t   

 t 

 t 

 t 

       xi    xi          xn    denote xi p  xi  xi   
 t 

next sample xi
generated previous sample xi following one two
schemes 
random scan gibbs sampling  given sample x t  iteration t  pick variable
 t 
xi random sample new value xi conditional distribution xi p  xi  xi  
leaving variables unchanged 
systematic scan  ordered  gibbs sampling  given sample x t    sample new
value variable order 
 t 

 t 

x  p  x   x    x         x t 
n  
 

ficutset sampling bayesian networks

 t   

x  p  x   x 

 t 

  x         x t 
n  

   

 t   

xi p  xi  x 

 t   

 t 

       xi    xi          x t 
n  

   

 t   

xn p  xn  x 

 t   

  x 

 t   

       xn   
 t 

bayesian networks  conditional distribution p  xi  xi   dependent
 t 

 t 

assignment markov blanket variable xi   thus  p  xi  xi   p  xi  xmarkovi  
 t 

xmarkovi restriction x t  markovi   given markov blanket xi   sampling
probability distribution given explicitly pearl        

 t 
 t 
p  xj  x t 
   
p  xi  xmarkovi     p  xi  x t 
 
paj  
pai
 j xj chj  

thus  generating complete new sample done o n r  multiplication steps
r maximum family size n number variables 
sequence samples x      x          viewed sequence states markov
 t   
 t     t   t 
 t 
chain  transition probability state  x 
       xi    xi   xi          xn   state
 t   

 t   

 t   

 t 

 t 

 t 

 x 
       xi    xi
  xi          xn   defined sampling distribution p  xi  xi   
construction  markov chain induced gibbs sampling invariant distribution
p  x e   however  since values assigned gibbs sampler variables sample
x t    depend assignment values previous sample x t    follows
sample x n  depends initial state x      convergence markov chain
defined rate distance distribution p  x n   x      e 
stationary distribution p  x e   i e   variational distance  l   distance      converges  
function n  intuitively  reflects quickly inital state x    forgotten 
convergence guaranteed markov chain ergodic  pearl        gelfand
  smith        mackay         markov chain finite number states ergodic
aperiodic irreducible  liu         markov chain aperiodic
regular loops  markov chain irreducible get state si state
sj  including si   non zero probability finite number steps  irreducibility
guarantees able visit  as number samples increases  statistically
important regions state space  bayesian networks  conditions almost always
satisfied long conditional probabilities positive  tierney        
ensure collected samples drawn distribution close p  x e  
burn in time may allocated  namely  assuming takes k samples
markov chain get close stationary distribution  first k samples may included computation posterior marginals  however  determining k hard  jones
  hobert         general  burn in optional sense convergence
estimates correct posterior marginals depend it  completeness
sake  algorithm given figure   
p
convergence conditions satisfied  ergodic average ft  x    t  f  xt  
function f  x  guaranteed converge expected value e f  x   increases 
 

fibidyuk   dechter

words   ft  x  e f  x        finite state markov chain
irreducible aperiodic  following result applies  see liu        theorem         

 ft  x  e f  x    n      f     
    
initial assignment x      variance term  f    defined follows 
 f         f    
    var f  x    h  integrated autocorrelation time 
focus computing posterior marginals p  xi  e  xi x e 
posterior marginals estimated using either histogram estimator 
p  xi   xi  e   


 x
 xi  x t   


p  xi   xi  e   


 x
 t 
p  xi  xi  


mixture estimator 

    

t  

    

t  

histogram estimator corresponds counting samples xi   xi   namely  xi  x t     
 t 
  xi   xi equals   otherwise  gelfand smith        pointed since
mixture estimator based estimating conditional expectation  sampling variance
smaller due rao blackwell theorem  thus  mixture estimator preferred 
 t 
 t 
since p  xi  xi     p  xi  xmarkovi    mixture estimator simply average conditional
probabilities 

 x
 t 
p  xi  e   
p  xi  xmarkovi  
    

t  

mentioned above  markov chain ergodic  p  xi  e  converge exact
posterior marginal p  xi  e  number samples increases  shown roberts
sahu        random scan gibbs sampler expected converge faster
systematic scan gibbs sampler  ultimately  convergence rate gibbs sampler depends
correlation two consecutive samples  liu        schervish   carlin       
liu et al          review subject next section 
    variance reduction schemes
convergence rate gibbs sampler depends strength correlations
samples  which states markov chain   term correlation
used mean samples dependent  mentioned earlier  case
finite state irreducible aperiodic markov chain  convergence rate expressed
maximal correlation states x    x n   see liu        ch       practice 
convergence rate analyzed covariance cov f  x t     f  x t        f
function  called auto covariance 
convergence estimates exact values depends convergence
rate markov chain stationary distribution variance estimator 
  

ficutset sampling bayesian networks

factors contribute value term  f    eq       two main
approaches allow reduce correlation samples reduce sampling variance
estimates blocking  grouping variables together sampling simultaneously 
collapsing  integrating random variables sampling subset  
known rao blackwellisation 
given joint probability distribution three random variables x    z 
depict essence three sampling schemes follows 
   standard gibbs 
x t    p  x y  t    z  t   


 t   

z

 t   

 t   

p  y  x

 t   

p  z x

    

 t 

 z  

 y

    

 t   

 

    

   collapsed  variable z integrated out  
x t    p  x y  t   


 t   

 t   

p  y  x

    
 

    

   blocking grouping x together 
 x t       t      p  x   z  t   
z

 t   

 t   

p  z x

 y

    
 t   

 

    

blocking reduces correlation samples grouping highly correlated
variables blocks  collapsing  highly correlated variables marginalized out 
results smoothing sampling distributions remaining variables
 p  y  x  smoother p  y  x  z    approaches lead reduction sampling
variance estimates  speeding convergence exact values 
generally  blocking gibbs sampling expected converge faster standard gibbs
sampler  liu et al         roberts   sahu         variations scheme
investigated jensen et al         kjaerulff         given number samples 
estimate resulting collapsed gibbs sampler expected lower variance
 converge faster  estimate obtained blocking gibbs sampler  liu et al         
thus  collapsing preferred blocking  analysis collapsed gibbs sampler
found escobar         maceachern         liu              
caveat utilization collapsed gibbs sampler computation
probabilities p  x y  p  y  x  must efficient time wise  case bayesian networks 
task integrating variables equivalent posterior belief updating
evidence variables sampling variables observed  time complexity therefore
exponential adjusted induced width  namely  effective width network
dependencies broken instantiated variables  evidence sampled  
  

fibidyuk   dechter

    importance sampling
since sampling target distribution hard  different sampling methods explore
different trade offs generating samples obtaining estimates  already discussed 
gibbs sampling generates dependent samples guarantees convergence sampling
distribution target distribution  alternative approach  called importance sampling 
generate samples sampling distribution q x  different p  x e 
include weight w t    p  x t   e  q x t    sample x t  computation
estimates follows 

 x
f  xt  w t 
ft  x   


    

t  

convergence ft  x  e f  x   guaranteed long condition p  x e    
  q x       holds  convergence speed depends distance q x 
p  x e  
one simplest forms importance sampling bayesian networks likelihood
weighting  fung   chang        shachter   peot        processes variables topological order  sampling root variables priors remaining variables
conditional distribution p  xi  pai   defined conditional probability table  the evidence variables assigned observed values   sampling distribution close
prior and  result  usually converges slowly evidence concentrated around
leaf nodes  nodes without children  probability evidence small  adaptive  also called dynamic  importance sampling method attempts speed
convergence updating sampling distribution based weight previously generated samples  adaptive importance sampling methods include self importance sampling 
heuristic importance sampling  shachter   peot         and  recently  ais bn  cheng
  druzdzel        epis bn  yuan   druzdzel         empirical section 
compare performance proposed cutset sampling algorithm ais bn
considered state of the art importance sampling algorithm date  although epis bn
shown perform better networks  and  hence  describe ais bn
detail 
ais bn algorithm based observation could sample node
topological order distribution p  xi  pai   e   resulting sample would drawn
target distribution p  x e   since distribution unknown variable
observed descendants  ais bn initializes sampling distributions p    xi  pai   e 
equal either p  xi  pai   uniform distribution updates distribution
p k  xi  pai   e  every l samples next sampling distribution p k    xi  pai   e 
closer p  xi  pai   e  p k  xi  pai   e  follows 
p k    xi  pai   e    p k  xi  pai   e     k   p  xi  pai   e  p k  xi  pai   e  
 k  positive function determines learning rate p  xi  pai   e 
estimate p  xi  pai   e  based last l samples 
  

ficutset sampling bayesian networks

   cutset sampling
section presents cutset sampling scheme  discussed above  sampling
cutset guaranteed statistically efficient  cutset sampling scheme computationally efficient way sampling collapsed variable subset c x  tying
complexity sample generation structure bayesian network 
    cutset sampling algorithm
cutset sampling scheme partitions variable set x two subsets c x c 
objective generate samples space c  c    c         cm   sample c t 
instantiation variables c  following gibbs sampling principles 
 t 
wish generate new sample c t  sampling value ci probability distribution
 t 
 t     t   
 t     t 
 t 
p  ci  ci     p  ci  c 
  c 
       ci    ci          cm    use left arrow denote
 t 

value ci drawn distribution p  ci  ci   

 t 

ci p  ci  ci   e 

    
 t 

compute probability distribution p  ci  ci   e  efficiently sampling
variable ci c  generate samples efficiently  relevant conditional distributions computed exact inference whose complexity tied network structure  denote jt c b  xi   e  generic algorithm class variable elimination
join tree clustering algorithms which  given belief network b evidence e  outputs
posterior probabilities p  xi  e  variable xi x  lauritzen   spiegelhalter       
jensen et al         dechter      a   networks identity clear  use
notation jt c xi   e  
cutset sampling
input  belief network b  cutset c    c         cm    evidence e 
output  set samples ct         t  
   initialize  assign random value c i ci c assign e 
   generate samples 
    t    generate new sample c t    follows 
 t 
    m  compute new value ci variable ci follows 
 t 
a  compute jt c ci   ci   e  
 t 

 t 

b  compute p  ci  ci   e    p  ci   ci   e  
c  sample 
 t   
 t 
ci
p  ci  ci   e 
end
end

    

figure    w cutset sampling algorithm
therefore  sampling variable ci value ci d ci    compute
 t 
 t 
 t 
 t 
p  ci   ci   e  via jt c ci   ci   e  obtain p  ci  ci   e  via normalization  p  ci  ci   e   
 t 

p  ci   ci   e  

  

fibidyuk   dechter

cutset sampling algorithm uses systematic scan gibbs sampler given figure   
clearly  adapted used random scan gibbs sampler well  steps
 a   c  generate sample  t      sample  t   every variable ci c sequence 
 t 
main computation step  a   distribution p  ci   ci   e  ci generated 
requires executing jt c every value ci d ci    separately  step  b  
conditional distribution derived normalization  finally  step  c  samples new value
 t 
obtained distribution  note use p  ci  ci   e  short hand notation
 t   

 t   

 t 

 t 

p  ci  c 
       ci    ci          ck   e   namely  sample new value variable
ci   values variables c  ci  already updated 
next demonstrate process using special case loop cutset  see definition      
example     consider belief network previously shown figure   observed node
e   e loop cutset  a  d   begin sampling process initializing sampling variables
a    d      next  compute new sample values a      d    follows 
p  a d      e 

 

pjt c  a  c      e 
   

    

   


p  d a      e 


 

p  a d   e 
pjt c  d  a      e 

    
    

d   



p  d a      e 

    

process corresponds two iterations inner loop figure    eq            
sample new value variable a  correspond steps  a   c  first iteration  second
iteration  eq            sample new value variable d  since conditioned network
poly tree  figure    right   computing probabilities pjt c  a d t    e  pjt c  d a t      e  via jt c
reduces pearls belief propagation algorithm distributions computed linear time 

    estimating posterior marginals
set samples subset variables c generated  estimate posterior
marginals variable network using mixture estimator  sampling variables 
estimator takes form similar eq      

 x
 t 
p  ci  e   
p  ci  ci   e 


    

t  

variables x c  e  posterior marginal estimator is 

 x
p  xi  e   
p  xi  c t    e 


    

t  

use jt c xi   c t    e  obtain distribution p  xi  c t    e  input bayesian
network conditioned c t  e shown before 
 t 
maintain running sum computed distributions p  ci  ci   e  p  xi  c t    e 
sample generation  sums right hand side eq           readily
available  noted before  estimators p  ci  e  p  xi  e  guaranteed converge corresponding exact posterior marginals increases long markov
  

ficutset sampling bayesian networks

chain cutset c ergodic  cutset variables estimator simple
ergodic average  xi x c  e convergence derived directly first
principles 
theorem     given bayesian network b x  evidence variables e x  cutset
c x e  given set samples c      c           c t   obtained via gibbs sampling
p  c e   assuming markov chain corresponding sampling c ergodic 
xi x c  e assuming p  xi  e  defined eq       p  xi  e  p  xi  e 
 
proof  definition 

 x
p  xi  c t    e 
p  xi  e   


    

t  

instead summing samples  rewrite expression sum
possible tuples c d c  group together samples corresponding tuple
instancep
c  let q c  denote number times tuple c   c occurs set samples
cd c  q c      easy see that 
p  xi  e   

fraction

q c 


x

cd c 

p  xi  c  e 

q c 


    

histogram estimator posterior marginal p  c e   thus  get 
x
p  xi  e   
p  xi  c  e p  c e 
    
cd c 

since markov chain formed samples c ergodic  p  c e  p  c e 
therefore 
x
p  xi  e 
p  xi  c  e p  c e    p  xi  e 
cd c 



    complexity
time space complexity generating samples estimating posterior marginals
via cutset sampling dominated complexity jt c line  a  algorithm
 figure     linear amount additional memory required maintain running
 t 
sums p  ci  ci   e  p  xi  c t    e  used posterior marginal estimators 
      sample generation complexity
clearly  jt c applied network b conditioned cutset variables c
evidence variables e  complexity time space exponential induced width
w conditioned network  o n d w      c w cutset  see definition      
  

fibidyuk   dechter

using notion w cutset  balance sampling exact inference  one end
spectrum plain gibbs sampling sample generation fast  requiring
linear space  may high variance  end  exact algorithm
requiring time space exponential induced width moral graph 
two extremes  control time space complexity using w follows 
theorem      complexity sample generation  given network b x  evidence e  w cutset c  complexity generating new sample time space
o  c  n d w      bounds variables domain size n    x  
proof  c w cutset maximum domain size  complexity
 t 
computing joint probability p  ci   ci   e  conditioned network o n d w      
since operation must repeated ci d ci    complexity processing one
 t 
variable  computing distribution p  ci  ci   e   o n d w        o n d w       finally 
since ordered gibbs sampling requires sampling variable cutset  generating one
sample o  c  n d w      

      complexity estimator computation
posterior marginals cutset variable ci c easily obtained end
sampling process without incurring additional computation overhead  mentioned earlier 
 t 
need maintain running sum probabilities p  ci  ci   e  ci d ci   
estimating p  xi  e   xi x c  e  using eq      requires computing p  xi  c t    e 
sample c t  generated  summary 
theorem      computing marginals  given w cutset c  complexity computing posteriors variables xi x e using samples cutset variables
o t   c    d  n d w      
proof  showed theorem      complexity generating one sample o  c 
n d w       sample c t  generated  computation posterior marginals
remaining variables requires computing p  xi  c t    e  via jt c xi   c t    e 
o n d w       combined computation time one sample o  c  n d w     
n d w        o   c    d  n d w       repeating computation samples  yields
o t   c    d  n d w      
note space complexity w cutset sampling bounded o n d w      

      complexity loop cutset
cutset c loop cutset  algorithm jt c reduces belief propagation  pearl 
 t 
      computes joint distribution p  ci   ci   e  linear time  refer
special case loop cutset sampling general w cutset sampling 
loop cutset w cutset w equals maximum number unobserved
parents  upper bounded maximum indegree node   however  since processing
poly trees linear even large w  induced width capture complexity
  

ficutset sampling bayesian networks

properly  notion loop cutset could better captured via hyperwidth
network  gottlob  leone    scarello        kask et al          hyperwidth polytree   therefore  loop cutset defined   hypercutset  alternatively 
express complexity via networks input size captures total size
conditional probability tables processed follows 
theorem      complexity loop cutset sample generation  c loop cutset 
complexity generating sample o  c    size input
network 
proof  loop cutset network instantiated  belief propagation  bp 
 t 
compute joint probability p  ci   ci   e  linear time o m    pearl        yielding total
time space o  c    sample 

    optimizing cutset sampling performance
analysis complexity generating samples  theorem      overly pessimistic
assuming computation sampling distribution variable cutset
independent  variables may change value moving one sample
next  change occurs one variable time sequence much
computation retained moving one variable next  
show sampling cutset variables done efficiently
reducing factor n  c  theorem      n    c    bounds number
clusters tree decomposition used jt c contains node ci c  assume
control order cutset variables sampled 

x 
x x y 

y 

y 

yn  

yn  

x 

x 

xn  

xn

x x y 

x x y 

xn  xnyn  

figure    bayesian network  top  corresponding cluster tree  bottom  
consider simple network variables x  x        xn      y         yn    cpts
p  xi    xi   yi   p  yi    xi   defined every shown figure    top  join tree
network chain cliques size   given figure    bottom  since loopcutset  sample variables   lets assume use ordering y    y       yn 
generate sample  given current sample  ready generate next sample
applying jt c  or bucket elimination  network whose cutset variables assigned 
  

fibidyuk   dechter

makes network effectively singly connected leaves   actual variables
cluster  algorithm sends message cluster containing xn towards
cluster containing x    cluster  x    x    y    gets relevant message cluster
 x    x    y    sample y    accomplished linear computations clique
 x    x    y    yi d yi   yielding desired distribution p  y       we multiply
functions incoming messages cluster  sum x  x  normalize  
cutset w cutset  computation single clique o d w      
p  y      y  sampled assigned new value  y    cluster  x    x    y   
y    sends message cluster  x    x    y    information necessary
compute p  y      o d w       p  y      available  new value y    y  sampled 
cluster computes sends message cluster  x    x    y     on 
end  obtain full sample via two message passes conditioned network
computation complexity o n d w       example generalized follows 
theorem     given bayesian network n variables  w cutset c  tree decomposition
tr   given sample c         c c    new sample generated o  n    c    d w     
maximum number clusters containing variable ci c 
proof  given w cutset c  definition  exists tree decomposition tr network
 that includes cutset variables  cutset variables c removed 
number variables remaining cluster tr bounded w      lets impose
directionality tr starting arbitrary cluster call r shown figure    let
tci denote connected subtree tr whose clusters include ci   figure    clarity 
collapse subtree ci single node  assume cutset nodes
sampled depth first traversal order dictated cluster tree rooted r 

tc  r
tc 
tck
tc 

tc 

tc 

tc 
figure    cluster tree rooted cluster r subtree cutset node ci
collapsed single node marked tci  

  

ficutset sampling bayesian networks

given sample c t    jt c send messages leaves tr towards root cluster 
assume without loss generality r contains cutset node c  first
sampled c t      jtc pass messages root clusters restricted
 t 
tc   note r tc     based messages p  c    c    c    computed
o d w       repeat computation value c  involving
clusters tc  obtain distribution p  c     o d w      sample new value
c    thus  c  appears clusters  number message passing computations
 after initial o n   pass  o   generate first distribution p  c    
o  d w      
next node depth first traversal order tc  thus  second variable
sampled c    distance variables c  c    denoted dist      shortest
path along tr cluster contains c  cluster contains c    apply jtcs
mesage passing along path take o dist    d w       then 
obtain conditional distribution p  c      recompute messages subtree
tc  value c  d c    o  d w       continue computation similar
manner cutset nodes 
jt c traverses tree depth first order  needs pass messages along
p c 
edge twice  see figure     thus  sum distances traveled i   disti i   
o n    may repeated computation value sampled variable 
this  however  accomplished via message passing restricted individual variables
subtrees bounded   conclude new full sample generated
o  n    c    d w      
worthwhile noting complexity generating sample reduced
factor d  d    which amounts factor        noticing whenever
 t   
 t     t 
 t 
move variable ci ci     joint probability p  c 
       ci
  ci          ck  
already available previous round recomputed  need
 t   
 t   
 t 
 t 
compute p  c 
       ci
  ci          ck   ci      ci     buffering last computed
joint probability  need apply jt c algorithm   times  therefore  total
complexity generating new sample o  n    c     d    d w      
example     figure   demonstrates application enhancements discussed 
depicts moral graph  a   already triangulated  corresponding join tree  b 
bayesian network figure    evidence variable e removed  variables b form
  cutset  join tree network cutset evidence variables removed shown
figure    c   since removing e cluster df e leaves one variable  f  
combine clusters bdf df e one cluster  f g  assuming cutset variables
domains size    initialize b   b    d   
selecting cluster ac root tree  jt c first propagates messages leaves
root shown figure    c  computes p  b    d    e  cluster ac  next 
set b   b    updating functions containing variable b  propagating messages
subtree b consisting clusters ac cf  figure    d    obtain p  b    d    e  
normalizing two joint probabilities  obtain distribution p  b d    e  sample new
value b  assume sampled value b   
  

fibidyuk   dechter

abc
p b a  p c a  
p a 



ac
p b  a  p c a  
p a 

ac
p b  a  p c a  
p a 

ac
p b  a  p c a  
p a 

ac
p b  a  p c a  
p a 

cf
p f b  c  p d  b  

cf
p f b  c  p d  b  

cf
p f b  c  p d  b  

cf
p f b  c  p d  b  

fg
p e d  f  p g d  f 

fg
p e d  f  p g d  f 

fg
p e d  f  p g d  f 

fg
p e d  f  p g d  f 

b
c

bcf
p f b c 

f
g

dfg
p d b   p g d f 



e

dfe
p e d f 

b b   d d   e e
 a 

 b 

 c 

b b 

d d 

d d 

 d 

 e 

 f 

figure    join tree width    b  moral graph  a  transformed join tree
width    c  evidence variable e cutset variables b instantiated  in process  clusters bdf bcf merged cluster cf   
clusters contain variables functions original network  cutset
nodes domains size    d b     b    b     d d     d    d     starting
sample  b    d     messages propagated  c   e  first  sample new value
variable b  d  variable  e   messages propagated
tree compute posterior marginals p   b    d    e  rest variables  f  

next  need compute p  d b    e  sample new value variable d  joint
probability p  d    b    e  readily available since computed sampling new value
b  thus  set   d  compute second probability p  d    b    e  updating functions
clusters cf f g sending updated message cf f g  figure    e   
obtain distribution p  d b    e  normalizing joint probabilities sample new
value d  d  since value changed latest computation  update
functions clusters cf f g propagate updated messages subtree cd
 send message cf f g  
order obtain distributions p   b    d    e  remaining variables a  c  f  
g  need send updated messages join tree  f g cf
cf ac shown figure    f    last step serves initialization
step next sample generation 
example performance cutset sampling significantly better worst
case  sent total   messages generate new sample worst case
suggests least n  c               messages  here  n equals number clusters  
  

ficutset sampling bayesian networks

    finding w cutset
clearly  w cutset sampling effective w cutset small  calls
task finding minimum size w cutset  problem np hard  yet  several heuristic
algorithms proposed  next briefly survey proposals 
larossa dechter        obtain w cutset processing variables elimination
order  next node eliminated  selected using triangulation heuristics  added
cutset current induced width  or degree  greater w  geiger fishelson
       agument idea various heuristics 
bidyuk dechter        select variables included cutset using greedy
heuristics based nodes basic graph properties  such degree node   one
scheme starts empty w cutset heuristically adds nodes cutset
tree decomposition width w obtained  scheme starts set
c   x e containing nodes network cutset removes nodes
set order  algorithm stops removing next node would result tree
decomposition width   w 
alternatively  bidyuk dechter        proposed first obtain tree decomposition
network find minimal w cutset tree decomposition  also nphard problem  via well known greedy algorithm used set cover problem  approach
shown yield smaller cutset previously proposed heuristics used finding
w cutset experiments  section      modification tree decomposition
re computed time node removed tree added w cutset 

   experiments
section  present empirical studies cutset sampling algorithms several classes
problems  use mean square error posterior marginals estimates
measure accuracy  compare traditional gibbs sampling  likelihood weighting
 fung   chang        shachter   peot         state art ais bn adaptive
importance sampling algorithm  cheng   druzdzel         implemented ais bn using
parameters specified cheng druzdzel         using implementation 
made sure sampling algorithms used data access routines
error measures providing uniform framework comparing performance 
reference report performance iterative belief propagation  ibp  algorithm 
    methodology
section detail describe methodology used implementation decisions made
apply collection empirical results 
      sampling methodology
sampling algorithms restarted markov chain every samples  samples
chain  batch  averaged separately 
pm  xi  e   


 x
p  xi  c t    e 

t  

  

fibidyuk   dechter

final estimate obtained sample average chains 

  x
pm  xi  e 
p  xi  e   

m  

restarting markov chain known improve sampling convergence rate  single
chain become stuck generating samples single high probability region without
ever exploring large number high probability tuples  restarting markov chain
different random point  sampling algorithm achieve better coverage
sampling space  experiments  observe significant difference
estimates obtained single chain size chains size therefore 
choose report results multiple markov chains  however  rely
independence random values pm  xi  e  estimate     confidence interval p  xi  e  
implementation gibbs sampling schemes  use zero burn in time  see
section         mentioned earlier  idea burn in time throw away
first k samples ensure remaining samples drawn distribution close
target distribution p  x e   conservative methods estimating k drift
minorization conditions proposed rosenthal        roberts tweedie
              required analysis beyond scope paper  consider
comparison gibbs sampling cutset sampling  objective  fair
sense schemes use k    further  experimental results showed positive
indication burn in time would beneficial  practice  burn in pre processing
time used algorithm find high probability regions distribution p  c e 
case initially spends disproportionally large period time low probability regions 
discarding large number low probability tuples obtained initially  frequency
remaining high probability tuples automatically adjusted better reflect weight 
cpcs   b  n       e      w    

cpcs   b  n       e       lc      w    

lcs
   

    e   

   

  unique samples

    e   

    e   

mse

    e   
    e   
    e   
    e   

lcs

   
   
   
   
   
   

    e   

 

    e   

 

    

    

    

    

     

 

    

    

    

    

     

  samples

  samples

figure    comparing loop cutset sampling mse vs  number samples  left  number unique samples vs  number samples  right  cpcs   b  results
averaged    instances different observations 

benchmarks  observed full gibbs sampling cutset sampling
able find high probability tuples fast relative number samples generated 
example  one benchmarks  cpcs   b  rate generating unique samples 
  

ficutset sampling bayesian networks

namely  ratio cutset instances seen number samples 
decreases time  specifically  loop cutset sampling generates     unique tuples
first      samples  additional     unique tuples generating next      samples 
rate generating unique tuples slows    per      samples range
           samples shown figure    right  means first
hundred samples  algorithm spends time revisiting high probability
tuples  benchmarks  number unique tuple instances generated increases
linearly  as cpcs    and  thus  tuples appear distributed nearly uniformly 
case  need burn in strongly expressed heavy weight
tuples  instead using burn in times  sample initial variable values posterior
marginal estimates generated ibp experiments  sampling time includes
pre processing time ibp 
experiments performed     ghz cpu 
      measures performance
problem instance defined bayesian network b variables x    x         xn  
evidence e x  derived exact posterior marginals p  xi  e  using bucket tree
elimination  dechter            a  computed mean square error  mse 
approximate posterior marginals p  xi  e  algorithm mse defined by 
x x
 
 p  xi  e  p  xi  e   
se   p
 d x
  

xi x e
xi x e d xi  

mean square error primary accuracy measure  results consistent
across well known measures average absolute error  kl distance  squared
hellingers distance show loop cutset sampling  absolute error
averaged values unobserved variables 
x x
 
  p
 p  xi  e  p  xi  e  
xi x e  d xi   
xi x e d xi  

kl distance dk distribution p  xi  e  estimator p  xi  e  defined
follows 
x
p  xi  e 
dk  p  xi  e   p  xi  e    
p  xi  e  lg
p  xi  e 
d x  


benchmark instance  compute kl distance variable xi x e
average results 
x
 
dk  p  xi  e   p  xi  e  
dk  p  p    
 x e 
xi x e

squared hellingers distance dh distribution p  xi  e  estimator
p  xi  e  obtained as 
q
x p
dh  p  xi  e   p  xi  e    
  p  xi  e  p  xi  e   
d xi  

  

fibidyuk   dechter

average squared hellingers distance benchmark instance average
distances posterior distributions one variable 
dh  p  p    

 
 x e 

x

xi x e

dh  p  xi  e   p  xi  e  

average errors different network instances averaged instances
given network  typically     instances  
report confidence interval estimate p  xi  e  using approach similar
well known batch means method  billingsley        geyer        steiger   wilson 
       since chains restarted independently  estimates pm  xi  e  independent 
thus  confidence interval obtained measuring variance estimators
p  xi  e   report results section     
    benchmarks
experimented four classes networks 
cpcs  considered four cpcs networks derived computer based patient
case simulation system  parker   miller        pradhan  provan  middleton    henrion 
       cpcs network representation based internist    miller  pople    myers 
      quick medical reference  qmr   miller  masarie    myers        expert systems  nodes cpcs networks correspond diseases findings conditional
probabilities describe correlations  cpcs   network consists n     nodes
relatively large loop cutset size  lc            nodes   induced width
    cpcs    network consists n      nodes  induced width w    
small loop cutset size  lc    relatively large corresponding adjusted induced
width wlc     cpcs   b larger cpcs network     nodes  adjusted induced
width     loop cutset  lc      exact inference cpcs   b averaged    minutes 
largest network  cpcs   b  consisted     nodes induced width w    
loop cutset size     exact inference time cpcs   b    minutes 
hailfinder network  small network    nodes  exact inference
hailfinder network easy since loop cutset size    yet  network
zero probabilities and  therefore  good benchmark demonstrating convergence
cutset sampling contrast gibbs sampling 
random networks  experimented several classes random networks  random networks    layer networks  grid networks  random networks generated
n      binary nodes  domains size     first     nodes   x         x      
designated root nodes  non root node xi          assigned   parents selected
randomly list predecessors  x         xi     refer class random
networks multi partite random networks distinguish bi partite    layer  random
networks  random   layer networks generated    root nodes  first layer 
    leaf nodes  second layer   yielding total     nodes  sample   layer random
network shown figure    left  non root node  second layer  assigned    
parents selected random among root nodes  nodes assigned domain size
   d xi      x i   x i   
  

ficutset sampling bayesian networks

figure    sample random networks    layer  left   grid  center   coding  right  

  layer multi partite random networks  root nodes assigned uniform priors conditional probabilities chosen randomly  namely  value
p  x i  pai   drawn uniform distribution interval        used compute
complementary probability value p  x i  pai       p  x i  pai   
directed grid networks  as opposed grid shaped undirected markov random
fields  size   x       nodes constructed uniform priors  on single
root node  random conditional probability tables  as described above   sample grid
network shown figure    center  networks average induced width
size     exact inference using bucket elimination required    minutes  
regular structure largest loop cutset containing nearly half
unobserved nodes 
coding networks  experimented coding networks    code bits   
parity check bits  parity check matrix randomized  parity check bit three
parents  sample coding network   code bits    parity checking bits  total  
transmitted bits shown figure    center  total number variables network
experiments         code bits     parity check bits    transmitted bit
code parity check bit   average loop cutset size    induced width
    markov chain produced gibbs sampling whole coding network
ergodic due deterministic parity check function  result  gibbs sampling
converge  however  markov chain corresponding sampling subspace coding
bits ergodic and  thus  cutset sampling schemes converged
show next two sections 
networks  except coding grid networks  evidence nodes selected random
among leaf nodes  nodes without children   since grid network one leaf
node  evidence grid networks selected random among nodes 
benchmark  report chart title number nodes network n   average
number evidence nodes  e   size loop cutset  lc   average induced width
input instance denoted w distinguish induced width w network adjusted
w cutset 
  

fibidyuk   dechter

    results loop cutset sampling
section compare loop cutset sampling pure gibbs sampling  likelihood
weighting  ais bn  ibp  benchmarks  cutset selected evidence
sampling nodes together constitute loop cutset network using algorithm
proposed becker et al          show accuracy gibbs loop cutset sampling
function number samples time 
cpcs networks  results summarized figures       loop cutset curve
chart denoted lcs  for loop cutset sampling   induced width network
wlc loop cutset nodes observed specified caption  identical
largest family size poly tree generated cutset variables removed  plot
time x axis accuracy  mse  y axis  cpcs networks  ibp
always converged converged fast  within seconds   consequently  ibp curve always
straight horizontal line results change convergence achieved 
curves corresponding gibbs sampling  loop cutset sampling  likelihood weighting 
ais bn demonstrate convergence sampling schemes time  three
cpcs networks loop cutset sampling converges much faster gibbs sampling 
exception cpcs   b  figure     right  induced width conditioned singlyconnected network remains high  wlc       due large family sizes thus  loop cutset
sampling generates samples slowly    samples second  compared gibbs sampling
     samples second   since computing sampling distribution exponential w  sampling
single variable o        all variables domains size     result  although loopcutset sampling shows significant reduction mse function number samples
 figure     left   enough compensate two orders magnitude difference
loop cutset rate sample generation  cpcs    figure     cpcs     figure     
cpcs   b  figure     loop cutset sampling achieves greater accuracy ibp within
   seconds less 
comparison importance sampling schemes  observe ais bn algorithm consistently outperforms likelihood weighting ais bn slightly better loopcutset sampling cpcs    probability evidence p  e         relatively high 
cpcs     probability evidence p  e   e    smaller  lcs outperforms ais bn
gibbs sampling curves falls ais bn likelihood weighting  gibbs
sampling loop cutset sampling outperform ais bn cpcs   b cpcs   b
probability evidence small  cpcs   b average p  e   e   cpcs   b probability evidence varies  e     e     note likelihood weighting ais bn
performed considerably worse either gibbs sampling loop cutset sampling
benchmarks function number samples  consequently  left
charts showing convergence gibbs loop cutset sampling function
number samples order zoom two algorithms focus
empirical studies 
coding networks  results coding networks shown figure    
computed error measures coding bits averaged     instances     instances 
different observed values     networks different coding matrices  
noted earlier  markov chains corresponding gibbs sampling coding networks
ergodic and  result  gibbs sampling converge  however  markov
  

ficutset sampling bayesian networks

gibbs

cpcs    n      lc      w       e   

lw

cpcs    n      lc      w       e   

lcs

   e   

ais bn

   e   

gibbs

ibp

   e   

   e   

lcs

   e   

ibp

   e   

mse

mse

   e   
   e   
   e   

   e   
   e   

   e   

   e   

   e   

   e   

   e   
 

    

     

     

     

     

 

     

 

 

 

  samples
lw

cpcs    n      lc      w       e   

  

  

  

hellinger distance

lcs

   e   

ais bn

   e   

gibbs

   e   

lw

cpcs    n      lc      w       e   

ais bn

   e   

kl distance

 

time  sec 

ibp

   e   
   e   
   e   
   e   

gibbs

   e   

lcs

   e   

ibp

   e   
   e   
   e   
   e   
   e   

   e   
 

 

 

 

 

  

  

 

  

 

 

 

  

  

  

time  sec 

time  sec 
lw

cpcs    n      lc      w       e   

ais bn

   e   

absolute error

 

gibbs

   e   

lcs

   e   

ibp

   e   
   e   
   e   
   e   
   e   
 

 

 

 

 

  

  

  

time  sec 

figure    comparing loop cutset sampling  lcs   wlc     gibbs sampling  hereby referred
gibbs   likelihood weighting  lw   ais bn  ibp cpcs   network 
averaged    instances  showing mse function number samples
 top left  time  top right  kl distance  middle left   squared hellingers
distance  middle right   average error  bottom  function time 

  

fibidyuk   dechter

cpcs     n       lc     w      e    

lw

cpcs     n       lc     w      e    

lw

   e   

ais bn

   e   

ais bn

gibbs

gibbs
lcs

ibp

   e   

ibp

   e   

mse

absolute error

lcs

   e   

   e   

   e   

   e   
 

 

 

 

 

  

  

 

  

 

 

 

 

  

  

cpcs     n       lc     w      e    

lw

cpcs     n       lc     w      e    

lw

   e   

ais bn

ais bn

   e   

gibbs

   e   

hellinger distance

gibbs

kl distance

  

time  sec 

time  sec 

lcs
ibp

   e   
   e   
   e   
   e   

lcs

   e   

ibp

   e   

   e   

   e   
 

 

 

 

 

  

  

  

 

 

 

 

time  sec 

 

  

  

  

time  sec 
lw

cpcs     n       lc     w      e    

ais bn

   e   

gibbs
absolute error

lcs
ibp

   e   

   e   

   e   
 

 

 

 

 

  

  

  

time  sec 

figure     comparing loop cutset sampling  lcs   wlc     gibbs sampling  likelihood
weighting  lw   ais bn  ibp cpcs    network  averaged    instances  showing mse function number samples  top left  time
 top right  kl distance  middle left   squared hellingers distance  middle
right   average error  bottom  function time 

  

ficutset sampling bayesian networks

cpcs   b  n       lc      w       e    

cpcs   b  n       lc      w       e    
  e   

gibbs

   e   

lw
ais bn

lcs

   e   

gibbs

ibp

   e   

lcs

mse

mse

  e   

   e   

ibp

  e   

   e   

  e   

   e   
 

    

     

     

     

 

     

 

 

 

  samples

cpcs   b  n       lc      w       e    

  

  

  

cpcs   b  n       lc      w       e    
lw

  e   

ais bn

hellinger distance

gibbs

  e   

lcs
ibp

  e   

lw

  e   

ais bn

kl distance

 

time  sec 

  e   
  e   

gibbs

  e   

lcs

  e   

ibp

  e   
  e   
  e   

 

 

 

 

 

  

  

  

 

 

 

 

time  sec 

 

  

  

  

time  sec 

cpcs   b  n       lc      w       e    
  e   

lw

absolute error

ais bn
gibbs

  e   

lcs
ibp

  e   

  e   
 

 

  

  

  

  

  

time  sec 

figure     comparing loop cutset sampling  lcs   wlc     gibbs sampling  likelihood
weighting  lw   ais bn  ibp cpcs   b network  averaged    instances  showing mse function number samples  top left  time
 top right  kl distance  middle left   squared hellingers distance  middle
right   average error  bottom  function time 

  

fibidyuk   dechter

cpcs   b  n       lc      w       e    

gibbs

cpcs   b  n       lc      w       e    

lcs

   e   

lw
ais bn

   e   

gibbs

ibp

   e   

lcs

   e   

mse

   e   

mse

ibp

   e   

   e   
   e   

   e   

   e   
   e   
   e   

   e   
 

    

    

    

    

    

 

    

  

  

lw

cpcs   b  n       lc      w       e    

  

  

  

lw

cpcs   b  n       lc      w       e    

ais bn

   e   

ais bn

gibbs

   e   

lcs

   e   

ibp

gibbs

   e   

hellinger distance

kl distance

  

time  sec 

  samples

   e   
   e   
   e   
   e   

lcs
   e   

ibp

   e   
   e   
   e   
   e   

 

  

  

  

  

  

  

 

  

  

time  sec 

  

  

  

  

time  sec 
cpcs   b  n       lc      w       e    

lw
ais bn

   e   

gibbs

absolute error

lcs
ibp

   e   

   e   

   e   
 

  

  

  

  

  

  

time  sec 

figure     comparing loop cutset sampling  lcs   wlc      gibbs sampling  likelihood
weighting  lw   ais bn sampling  ibp cpcs   b network  averaged
   instances  showing mse function number samples  top
left  time  top right  kl distance  middle left   squared hellingers
distance  middle right   average error  bottom  function time 

  

ficutset sampling bayesian networks

   e   

   e   

   e   

   e   
   e   

   e   

   e   

   e   
 

 

 

 

 

 

  

 

 

time  sec 

 

 

  

time  sec 
lw
ais bn
gibbs
lcs
ibp

   e   

coding  n      p     lc      w    

lw
ais bn
gibbs
lcs
ibp

   e   

hellinger distance

coding  n      p     lc      w    
   e   

kl distance

lw
ais bn
gibbs
lcs
ibp

   e   

mse

absolute error

coding  n      p     lc      w    

lw
ais bn
gibbs
lcs
ibp

coding  n      p     lc      w    
   e   

   e   
   e   
   e   
   e   
   e   

   e   
   e   
   e   
   e   
   e   

 

 

 

 

 

 

  

time  sec 

 

 

 

 

  

time  sec 

figure     comparing loop cutset sampling  lcs   wlc     gibbs sampling  likelihood
weighting  lw   ais bn  ibp coding networks        averaged
   instances    coding networks      instances total   graphs show average absolute error   top left   mse  top right   kl distance  bottom left  
squared hellingers distance  bottom right  function time 

chain corresponding sampling subspace code bits ergodic therefore 
loop cutset sampling  samples subset coding bits  converges even achieves
higher accuracy ibp time  reality  ibp certainly preferable coding
networks since size loop cutset grows linearly number code bits 
random networks  random multi part networks  figure     top  random
  layer networks  figure     middle   loop cutset sampling always converged faster
gibbs sampling  results averaged    instances network type 
cases  loop cutset sampling achieved accuracy ibp   seconds less    layer
networks  iterative belief propagation performed particularly poorly  gibbs sampling
loop cutset sampling obtained accurate results less second 
hailfinder network  used network  in addition coding networks  compare behavior cutset sampling gibbs sampling deterministic networks  since
hailfinder network contains many deterministic probabilities  markov chain corresponding gibbs sampling variables non ergodic  expected  gibbs sampling fails
loop cutset sampling computes accurate marginals  figure     
  

fibidyuk   dechter

random  n       e       c      w    

  layer  r     p    n       e       lc      w    

gibbs

   e   

lcs

   e   

ibp

   e   

gibbs
lcs

   e   

ibp

mse

mse

   e   
   e   
   e   

   e   
   e   

   e   
   e   

   e   
 

 

  

  

  

  

 

  

 

 

time  sec 

 

 

  

  

time  sec 

figure     comparing loop cutset sampling  lcs   gibbs sampling  ibp random
networks  left    layer random networks  right   wlc    classes
networks  averaged    instances each  mse function time 

hailfinder  n      lc     w      e   

gibbs

   e   

lcs
ibp

mse

   e   

   e   

   e   
 

 

 

 

 

 

 

 

time  sec 

figure     comparing loop cutset sampling  lcs   wlc     gibbs sampling  ibp
hailfinder network     instances  mse function time 

summary  empirical results demonstrate loop cutset sampling cost effective

time wise superior gibbs sampling  measured ratio r   mgc number
samples mg generated gibbs number samples mc generated loop cutset
sampling time period  it relatively constant given network
changes slightly problem instances differ observations   cpcs   
cpcs     cpcs   b  cpcs   b ratios correspondingly                   
 see table   section       obtained r     random networks r    
random   layer networks  ratio values     indicate gibbs sampler generates
  

ficutset sampling bayesian networks

samples faster loop cutset sampling usually case  instances 
variance reduction compensated increased computation time fewer samples
needed converge resulting overall better performance loop cutset sampling
compared gibbs sampling  cases  however  reduction sample size
compensates overhead computation sampling one variable value 
cases  loop cutset sampling generated samples faster gibbs yielding ratio r     
then  improvement accuracy due larger number samples
faster convergence 
    w cutset sampling
section  compare general w cutset scheme different values w
gibbs sampling  main goal study performance w cutset sampling
varies w  completeness sake  include results loop cutset sampling shown
section     
empirical study  used greedy algorithm set cover problem  mentioned
section      finding minimal w cutset  apply algorithm manner
 w      cutset proper subset w cutset and  thus  expected
lower variance converge faster sampling w cutset terms number samples
required  following rao blackwellisation theory   focus empirical study
trade offs cutset size reduction associated increase sample generation
time gradually increase bound w 
used benchmarks included grid networks  sampling
algorithms given fixed time bound  sampling small networks  cpcs  
 w      cpcs     w      exact inference easy  sampling algorithms
allocated    seconds    seconds respectively  larger networks allocated        
seconds depending complexity network fraction exact
computation time 
table   reports size sampling set used algorithm column
reports size corresponding w cutset  example  cpcs   b  average
size gibbs sample  all nodes except evidence       loop cutset size     size
  cutset     on  table   shows rate sample generation different
algorithms per second  observed previously case loop cutset sampling 
special cases cutset sampling generated samples faster gibbs sampler 
example  cpcs   b  loop cutset sampling   cutset sampling generated     samples
per second gibbs sampler able generate     samples  attribute
size cutset sample     nodes less reported table    compared
size gibbs sample  over     nodes  
cpcs networks  present two charts  one chart demonstrates convergence
time several values w  second chart depicts change quality
approximation  mse  function w two time points  half total
sampling time end total sampling time  performance gibbs sampling
cutset sampling cpcs   shown figure     results averaged   
instances      evidence variables  graph left figure    shows mean
square error estimated posterior marginals function time gibbs sampling 
  

fibidyuk   dechter

cpcs  
cpcs   
cpcs   b
cpcs   b
grid  x  
random
 layer
coding

gibbs
  
   
   
   
   
   
   
   

lc
  
 
  
  
   
  
  
  

w  
  
  
  
  
   
  
  
  

sampling set size
w   w   w   w  
  
  
 
 
 
 
 
  
  
  
  
  
  
  
  
   
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 

w  
  
  
  
  
 

w  
  
 

table    markov chain sampling set size function w 

cpcs  
cpcs   
cpcs   b
cpcs   b
grid  x  
random
 layer
coding

gibbs
    
    
   
   
    
    
   
    

lc
      w   
     w   
     w   
   w   
     w   
      w   
     w   
      w   

no  samples
w   w   w   w  
         
   
   
   
   
  
  
   
   
   
   
   
   
  
  
   
   
   
   
    
   
   
   
   
   
   
  
    
   
   
   

w  
   
  
  
  
   
  
   

w  
  
  
  
  
 

w  
  
 

table    average number samples generated per second function w 
loop cutset sampling  w cutset sampling w             second chart shows
accuracy function w  first point corresponds gibbs sampling  points
correspond loop cutset sampling w cutset sampling w ranging     
loop cutset result embedded w cutset values w    explained section     
loop cutset corresponds w cutset w maximum number parents
network  initially  best results obtained      cutset sampling followed
loop cutset sampling  time       cutset sampling become best 
results cpcs    reported figure     charts show loop cutset
sampling w cutset sampling w range     superior gibbs sampling 
chart left shows best cutset sampling schemes  lowest
mse curves       cutset sampling  loop cutset curve falls   
  cutset first outperformed      cutset    seconds  loop cutset
sampling      cutset sampling outperform gibbs sampling nearly two orders
magnitude mse falls  e    gibbs mse remains order  e         cutset sampling results fall between  achieving mse  e    
curves corresponding loop cutset sampling          cutset sampling fall
ibp line means four algorithms outperform ibp first seconds
execution  ibp converges less second     cutset outperforms ibp  
seconds  figure    right  see accuracy results sampling algorithms
  

ficutset sampling bayesian networks

gibbs

   e   

mse

   e   
   e   

ibp

cpcs    n      lc      w       e   

ibp
lcs w  

   e   

 c     w  
 c     w  

   e   

 c     w  
 c    w  

   e   

   e   

mse

cpcs    n      lc      w       e   
   e   

   e   

cutset    sec
cutset     sec

   e   
   e   

   e   

   e   
 

 

 

 

 

  

  

  

gibbs

w  

w  

w  

time  sec 

lcs 
w  

w  

w  

figure     mse function time  left  w  right  cpcs       instances  time
bound    seconds 

mse

   e   

   e   

ibp

cpcs     n       lc     w      e    

cutset     sec

   e   

cutset     sec
   e   

mse

gibbs
ibp
lcs w  
 c     w  
 c    w  
 c    w  
 c    w  

cpcs     n       lc     w      e    
   e   

   e   
   e   
   e   

  

  

time  sec 

lc
s 
w 
 

  

w 
 

 

w 
 

 

w 
 

 

w 
 

 

g

 

ib
bs

   e   

figure     mse function time  left  w  right  cpcs        instances  time
bound    seconds  y scale exponential due large variation performance
gibbs cutset sampling 

   seconds    seconds  agreement convergence curves
left 
cpcs   b  figure      loop cutset sampling      cutset sampling similar
performance  accuracy estimates slowly degrades w increases  loop cutset
sampling w cutset sampling substantially outperform gibbs sampling values w
exceed accuracy ibp within   minute 
example cpcs   b  demonstrate significance adjusted induced
width conditioned network performance cutset sampling  reported
section      loop cutset relatively small  lc     wlc     thus  sampling
one new loop cutset variable value exponential big adjusted induced width 
result  loop cutset sampling computes   samples per second      and   cutset  slightly larger            nodes respectively  see
table     compute samples rates              samples per second  see table    
  

fibidyuk   dechter

cpcs   b  n       lc      w       e    

cpcs   b  n       e       lc      w    

gibbs

  e   

  e   

 c     w  
 c     w  

  e   

 c     w  
 c     w  

  e   

ibp
cutset t   sec
cutset t   sec

mse

mse

  e   

ibp
lcs w  

  e   

  e   

  e   

g

  
w

w 
 

  
w

time  sec 

  

  

w

  

  

  

w

  

  

  

lc
 w

  



  

ib
b

 

w 
 

  e   

  e   

figure     mse function time  left  w  right  cpcs   b     instances  time
bound    seconds  y scale exponential due large variation performance
gibbs cutset sampling 

gibbs
ibp
lcs w  
 c     w  
 c     w  
 c     w  
 c     w  

   e   

mse

   e   
   e   

cpcs   b  n       lc      w       e    
   e   

ibp
cutset      sec
cutset      sec

   e   

mse

cpcs   b  n       lc      w       e    
   e   

   e   

   e   

   e   

   e   

   e   
   e   
 

  

  

  

  

   

   

   

   e   

gibbs

time  sec 

w  

w  

w  

w  

w  

w  

figure     mse function time  left  w  right  cpcs   b     instances  time
bound     seconds  y scale exponential due large variation performance
gibbs cutset sampling 

  cutset closest loop cutset size   c          computes    samples per
second order magnitude loop cutset sampling  results
cpcs   b shown figure     loop cutset sampling results excluded due
poor performance  chart right figure    shows w cutset performed well
range w       far superior gibbs sampling  allowed enough time 
w cutset sampling outperformed ibp well  ibp converged   seconds         
  cutset improved ibp within    seconds    cutset    seconds 
random networks  results    instances random multi partite    instances   layer networks shown figure     see  w cutset sampling
substantially improves gibbs sampling ibp reaching optimal performance
w       classes networks  range  performance similar
loop cutset sampling  case   layer networks  accuracy gibbs sampling
  

ficutset sampling bayesian networks

random  r     n      p     lc      w    

random  r     n      p     lc      w    

   e   
   e   

ibp
cutset t   sec
cutset t   sec

   e   

   e   

   e   

   e   
   e   

g

 layer  r     n      p     lc      w    

   e   

w 
 

ibp

   e   

cutset t   sec
cutset t   sec
   e   

mse

mse

   e   

w 
 

 layer  r     n      p     lc      w    

gibbs
ibp
 lc     w   
 c     w   
 c     w   
 c     w   
 c     w   

   e   

w 
 

time  sec 

w
  

  

  

  

 w

  

lc

  

ib
bs

  

w 
 

   e   
 

w 
 

mse

   e   

   e   

mse

gibbs
ibp
 lc     w   
 c     w   
 c     w   
 c     w   
 c     w   

   e   

   e   

   e   

   e   
 

 

  

  

  

  

time  sec 

   e   
gibbs

w   

lc w   

w   

w   

w   

w   

figure     random multi partite networks  top    layer networks  bottom       nodes 
   instances  mse function number samples  left  w  right  

  

fibidyuk   dechter

ibp order of magnitude less compared cutset sampling  figure     bottom right  
poor convergence accuracy ibp   layer networks observed previously
 murphy et al         
grid    x     e       lc       w      mse
   e   

   e   

ibp
 lc      w   

   e   

   e   

 c      w   

   

   

cutset t    sec

time  sec 

w 
  

  

w 
  

  

w 
  

  

ibb


  

cutset t   sec

   e   
g

 

ibp

   e   

w 
  

   e   

   e   

w 
  

 c     w   

w 
  

 c     w   

   e   

   e   

w 
  
lc
 w
  
 

 c      w   

   e   

mse

mse

grid    x     e       lc       w    

gibbs

figure     random networks      nodes     instances  mse function number
samples  left  w  right  

grid networks  grid networks     nodes    x    class benchmarks full gibbs sampling able produce estimates comparable cutsetsampling  figure      respect accuracy  gibbs sampler  loop cutset sampling 
  cutset sampling best performers achieved similar results  loop cutset
sampling fastest accurate among cutset sampling schemes  still  generated samples   times slowly compared gibbs sampling  table    since
loop cutset relatively large  accuracy loop cutset sampling closely followed
         cutset sampling slowly degrading w increased  grid networks
example benchmarks regular graph structure  that cutset sampling cannot exploit
advantage  small cpts  in two dimensional grid network node
  parents  gibbs sampling strong 
coding   x    n      p     lc      w    

coding    x    n      p     lc      w    

ibp
   e   

cutset t  sec

 c     w   

   e   

cutset t   sec
   e   

 c     w   

   e   

 c     w   

mse

mse

ibp

 lc     w   

   e   

   e   
   e   

   e   
   e   

   e   

   e   
   e   
 

 

 

 

 

  

   e   
w  

time  sec 

w  

lc w  

w  

figure     coding networks     code bits     parity check bits            instances  time
bound   minutes 

  

ficutset sampling bayesian networks

cpcs  
cpcs   
cpcs   b
cpcs   b
grid  x  
random
 layer
coding

time
   sec
   sec
    sec
    sec
    sec
   sec
   sec
   sec

gibbs
    
    
    
    
    
    
   
   

markov chain length
lc w   w   w  
              
   
   
   
   
  
              
   
            
   
   
   
   
   
         
   
   
   
   
   
   
   
   
   
   


w  
   
  
   
   
   
   
  
   

w  
   
   
  
   
  
   

table    individual markov chain length function w  length chain
adjusted sampling scheme benchmark total processing
time across sampling algorithms same 

coding networks  cutset sampling results coding networks shown
figure     here  induced width varied       allowing exact inference 
however  additionally tested observed complexity network grows
exponentially number coding bits  even small increase number
coding bits    yielding total     nodes corresponding adjustments number
parity checking bits transmitted code size  induced width exceeds    
time sample generation scales linearly  collected results    networks
    different parity check matrices     different evidence instantiations  total    
instances   decoding  bit error rate  ber  standard error measure  however 
computed mse unobserved nodes evaluate quality approximate results
precisely  expected  gibbs sampling converge  because markov chain
non ergodic  left charts  charts figure    show loop cutset
optimal choice coding networks whose performance closely followed   cutset
sampling  saw earlier  cutset sampling outperforms ibp 
    computing error bound
second issue convergence sampling scheme always problem predicting
quality estimates deciding stop sampling  section  compare
empirically error intervals gibbs cutset sampling estimates 
gibbs sampling cutset sampling guaranteed converge correct posterior
distribution ergodic networks  however  hard estimate many samples
needed achieve certain degree convergence  possible derive bounds
absolute error based sample variance sampling method samples independent  gibbs mcmc methods  samples dependent cannot apply
confidence interval estimate directly  case gibbs sampling  apply batch
means method special case standardized time series method used
bugs software package  billingsley        geyer        steiger   wilson        
  

fibidyuk   dechter

main idea split markov chain length chains length
  let pm  xi  e  estimate derived single chain            length
 meaning  containing samples  defined equations            estimates pm  x e 
assumed approximately independent large enough   assuming convergence
conditions satisfied central limit theorem holds  pm  x e  distributed
according n  e p  xi  e        posterior marginal p  xi  e  obtained
average results obtained chain  namely 
p  x e   


  x
pm  x e 


    

m  

sampling variance computed usually 
   


x
 
 pm  x e  p  x e   
 
m  

equivalent expression sampling variance is 
pm
p    x e  p    x e 
 
  m  
 

    

  easy compute incrementally storing running sums pm  x e 
   x e   therefore  compute confidence interval         percentile
pm
used random variables normal distribution small sampling set sizes  namely 
 
r  
 
  
    
p p  x e   p  x e      m   

    m    table value distribution  m    degrees freedom 
used batch means approach estimate confidence interval posterior
marginals one modification  since working relatively small sample sets
 a thousand samples  notion large enough well defined 
restarted chain every samples guarantee estimates pm  x e 
truly independent  method batch means provides meaningful error estimates
assuming samples drawn stationary distribution  assume
problems chains mix fast enough samples drawn target
distribution 
applied approach estimate error bound gibbs sampler
cutset sampler  computed     confidence interval estimated posterior
marginal p  xi  e  based sampling variance pm  xi  e     markov chains
described above  computed sampling variance   eq          confidence
interval      xi   eq      averaged nodes 
x x
 
      p
     xi  
n  d xi   


xi d xi  

estimated confidence interval large practical  thus  compared    
empirical average absolute error  
  

ficutset sampling bayesian networks

cpcs  
cpcs   
cpcs   b
cpcs   b
random
 layer
coding
grid  x  


   

   

   

   

   

   

   

   

average error
lc
w  
               
               
               
               
               
               
         
         
               
               
               
               
               
               
               
               

gibbs
       
       
       
       
       
       
       
       
       
       
       
       
       
       

confidence interval
w  
w  
w  
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
                
                
       
               
       
               

w  
       
       
       
       
       
       
       
       
       
       
       
       

table    average absolute error  measured  estimated confidence interval    
function w    markov chains 

 

n

x
 
 d xi   

p



x

xi d xi  

 p  xi  e  p  xi  e  

objective study observe whether computed confidence interval    
 estimated absolute error  accurately reflects true absolute error   namely  verify
        so  investigate empirically whether confidence interval cutsetsampling estimates smaller compared gibbs sampling would expect due
variance reduction 
table   presents average confidence interval average absolute error
benchmarks  benchmark  first row results  row   reports average
absolute error second row results  row       reports     confidence interval 
column table   corresponds sampling scheme  first column reports results
gibbs sampling  second column reports results loop cutset sampling 
remaining columns report results w cutset sampling w range     loop cutset
sampling results cpcs   b included due statistically insignificant number
samples generated loop cutset sampling  gibbs sampling results coding networks
left network ergodic  as mentioned earlier  gibbs sampling
converge 
see networks       validates method measuring
confidence interval  cases estimated confidence interval    
    times size average error relatively small  case cutset sampling 
largest confidence interval max               reported grid networks loop cutset
  

fibidyuk   dechter

sampling  thus  confidence interval estimate could used criteria reflecting
quality posterior marginal estimate sampling algorithm practice  subsequently  comparing results gibbs sampling cutset sampling  observe
significant reduction average absolute error  similar reduction
estimated confidence interval  across benchmarks  estimated confidence interval
gibbs sampler remains        e    time  cutset sampling obtain
       e       classes networks  excluded cpcs     grid    layer
networks  
    discussion
empirical evaluation performance cutset sampling demonstrates that  except
grid networks  sampling cutset usually outperforms gibbs sampling  show
convergence cutset sampling terms number samples dramatically improves
predicted theoretically 
experiments clearly show exists range w values w cutset
sampling outperforms gibbs sampler  performance w cutset sampling deteriorates
increase w yields small reduction cutset size  example cpcs   b
network starting w    increasing w   results reducing sampling
set   node  shown table    
observe loop cutset good choice cutset sampling long
induced width network wlc conditioned loop cutset reasonably small  wlc
large  as cpcs   b   loop cutset sampling computationally less efficient w cutset
sampling w   wlc  
showed section     gibbs sampling loop cutset sampling
outperform state of the art ais bn adaptive importance sampling method
probability evidence small  consequently  w cutset sampling schemes section     outperformed gibbs sampler cpcs   b cpcs   b would outperfrom
ais bn 

   related work
mention related work  idea marginalising variables improve
efficiency gibbs sampling first proposed liu et al          successfully
applied several special classes bayesian models  kong et al         applied collapsing
bivariate gaussian problem missing data  liu        defined collapsed gibbs
sampling algorithm finding repetitive motifs biological sequences applies integrating
two parameters model  similarly  gibbs sampling set collapsed escobar
        maceachern         liu        learning nonparametric bayes problem 
instances above  special relationships problem variables
exploited integrate several variables resulting collapsed gibbs sampling approach 
compared previous research work  contribution defining generic scheme
collapsing gibbs sampling bayesian networks takes advantage networks
graph properties depend specific form relationships
variables 
  

ficutset sampling bayesian networks

jensen et al         combined sampling exact inference blocking gibbs sampling
scheme  groups variables sampled simultaneously using exact inference compute
needed conditional distributions  empirical results demonstrate significant improvement convergence gibbs sampler time  yet  proposed blocking
gibbs sampling  sample contains variables network  contrast  cutset sampling reduces set variables sampled  noted previously  collapsing produces
lower variance estimates blocking and  therefore  cutset sampling require fewer
samples converge 
different combination sampling exact inference join trees described
koller et al         kjaerulff         oller et al  kjaerulff proposed sample
probability distribution cluster computing outgoing messages  kjaerulff
used gibbs sampling large clusters estimate joint probability distribution
p  vi    vi x cluster i  estimated p  vi   recorded instead true joint
distribution conserve memory  motivation high probability tuples
recorded remaining low probability tuples assumed probability   
small clusters  exact joint distribution p  vi   computed recorded  however 
paper analyze introduced errors compare performance scheme
standard gibbs sampler exact algorithm  analysis error given
comparison approaches 
koller et al         used sampling used compute messages sent cluster
cluster j posterior joint distributions cluster tree contains discrete
continuous variables  approach subsumes cluster based sampling proposed
kjaerulff        includes rigorous analysis error estimated posterior
distributions  method difficulties propagation evidence  empirical
evaluation limited two hybrid network instances compares quality
estimates likelihood weighting  instance importance sampling
perform well presence low probability evidence 
effectiveness collapsing sampling set demonstrated previously
context particle filtering method dynamic bayesian networks  doucet  andrieu   
godsill      a  doucet  defreitas    gordon        doucet  de freitas  murphy    russell 
    b   shown sampling subspace combined exact inference  raoblackwellised particle filtering  yields better approximation particle filtering
full set variables  however  objective study limited observation
effect special cases variables integrated easily 
cutset sampling scheme offers generic approach collapsing gibbs sampler
bayesian network 

   conclusion
paper presents w cutset sampling scheme  general scheme collapsing gibbs
sampler bayesian networks  showed theoretically empirically cutset sampling improves convergence rate allows sampling non ergodic network
ergodic subspace  collapsing sampling set  reduce dependence
samples marginalising highly correlated variables smoothing
sampling distributions remaining variables  estimators obtained sampling
  

fibidyuk   dechter

lower dimensional space lower sampling variance  using induced
width w controlling parameter  w cutset sampling provides mechanism balancing
sampling exact inference 
studied power cutset sampling sampling set loop cutset and 
generally  sampling set w cutset network  defined subset
variables that  instantiated  induced width network w   based
rao blackwell theorem  cutset sampling requires fewer samples regular sampling
convergence  experiments showed reduction number samples
time wise cost effective  confirmed range randomly generated real
benchmarks  demonstrated cutset sampling superior state art
ais bn importance sampling algorithm probability evidence small 
since size cutset correlations variables two main
factors contributing speed convergence  w cutset sampling may optimized advancement methods finding minimal w cutset  another promising
direction future research incorporate heuristics avoiding selecting stronglycorrelated variables cutset since correlations driving factors speed
convergence gibbs sampling  alternatively  could combine sample collapsing
blocking 
summary  w cutset sampling scheme simple yet powerful extension sampling
bayesian networks likely dominate regular sampling sampling method 
focused gibbs sampling better convergence characteristics  sampling
schemes implemented cutset sampling principle  particular 
adapted use likelihood weighting  bidyuk   dechter        

references
abdelbar  a  m     hedetniemi  s  m          approximating maps belief networks
np hard theorems  artificial intelligence            
andrieu  c   de freitas  n     doucet  a          rao blackwellised particle filtering via
data augmentation  advances neural information processing systems  mit
press 
arnborg  s  a          efficient algorithms combinatorial problems graphs
bounded decomposability   survey  bit          
becker  a   bar yehuda  r     geiger  d          random algorithms loop cutset
problem  journal artificial intelligence research             
bertele  u     brioschi  f          nonserial dynamic programming  academic press 
bidyuk  b     dechter  r          empirical study w cutset sampling bayesian networks  proceedings   th conference uncertainty artificial intelligence
 uai   pp        morgan kaufmann 
bidyuk  b     dechter  r          finding minimal w cutset problem  proceedings
  th conference uncertainty artificial intelligence  uai   pp       
morgan kaufmann 
  

ficutset sampling bayesian networks

bidyuk  b     dechter  r          cutset sampling likelihood weighting  proceedings   nd conference uncertainty artificial intelligence  uai   pp 
      morgan kaufmann 
billingsley  p          convergence probability measures  john wiley   sons  new york 
casella  g     robert  c  p          rao blackwellisation sampling schemes  biometrika 
             
cheng  j     druzdzel  m  j          ais bn  adaptive importance sampling algorithm
evidenctial reasoning large baysian networks  journal aritificial intelligence
research             
cooper  g          computational complexity probabilistic inferences  artificial
intelligence             
dagum  p     luby  m          approximating probabilistic inference bayesian belief
networks np hard  artificial intelligence                 
dechter  r       a   bucket elimination  unifying framework reasoning  artificial
intelligence            
dechter  r       b   bucket elimination  unifying framework reasoning  artificial
intelligence                 
dechter  r          constraint processing  morgan kaufmann 
doucet  a     andrieu  c          iterative algorithms state estimation jump markov
linear systems  ieee trans  signal processing                   
doucet  a   andrieu  c     godsill  s       a   sequential monte carlo sampling methods bayesian filtering  statistics computing                 
doucet  a   de freitas  n   murphy  k     russell  s       b   rao blackwellised particle
filtering dynamic bayesian networks  proceedings   th conference
uncertainty artificial intelligence  uai   pp         
doucet  a   defreitas  n     gordon  n          sequential monte carlo methods practice 
springer verlag  new york  inc 
doucet  a   gordon  n     krishnamurthy  v          particle filters state estimation jump markov linear systems  tech  rep   cambridge university engineering
department 
escobar  m  d          estimating normal means iwth dirichlet process prior  journal
american statistical aasociation             
frey  b  j     mackay  d  j  c          revolution  belief propagation graphs
cycles  neural information processing systems  vol     
fung  r     chang  k  c          weighing integrating evidence stochastic simulation bayesian networks  proceedings  th conference uncertainty
artificial intelligence  uai   pp          morgan kaufmann 
geiger  d     fishelson  m          optimizing exact genetic linkage computations  proceedings  th annual international conf  computational molecular biology 
pp          morgan kaufmann 
  

fibidyuk   dechter

gelfand  a     smith  a          sampling based approaches calculating marginal densities  journal american statistical association             
geman  s     geman  d          stochastic relaxations  gibbs distributions
bayesian restoration images  ieee transaction pattern analysis machine
intelligence            
geyer  c  j          practical markov chain monte carlo  statistical science            
gilks  w   richardson  s     spiegelhalter  d          markov chain monte carlo practice 
chapman hall 
gottlob  g   leone  n     scarello  f          comparison structural csp decomposition
methods  proceedings   th international joint conference artificial
intelligence  ijcai   pp          morgan kaufmann 
jensen  c   kong  a     kjrulff  u          blocking gibbs sampling large probabilistic expert systems  int  j  human computer studies  special issue realworld applications uncertain reasoning                 
jensen  f  v   lauritzen  s  l     olesen  k  g          bayesian updating causal
probabilistic networks local computation  computational statistics quarterly    
       
jones  g     hobert  j  p          honest exploration intractable probability distributions
via markov chain monte carlo  statist  sci                  
kask  k   dechter  r   larrosa  j     dechter  a          unifying cluster tree decompositions reasoning graphical models  artificial intelligence              
kjrulff  u          hugs  combining exact inference gibbs sampling junction
trees  proceedings   th conference uncertainty artificial intelligence
 uai   pp          morgan kaufmann 
koller  d   lerner  u     angelov  d          general algorithm approximate inference
application hybrid bayes nets  proceedings   th conference
uncertainty artificial intelligence  uai   pp         
kong  a   liu  j  s     wong  w          sequential imputations bayesian missing
data problems  j  american statistical association                   
kschischang  f  r     frey  b  j          iterative decoding compound codes probability propagation graphical models  ieee journal selected areas communications             
larrosa  j     dechter  r          boosting search variable elimination constraint
optimization constraint satisfaction problems  constraints                
lauritzen  s     spiegelhalter  d          local computation probabilities graphical
structures application expert systems  journal royal statistical
society  series b                
liu  j          correlation structure convergence rate gibbs sampler  ph d 
thesis  university chicago 
  

ficutset sampling bayesian networks

liu  j          collapsed gibbs sampler bayesian computations applications
gene regulation problem  journal american statistical association           
       
liu  j   wong  w     kong  a          covariance structure gibbs sampler
applications comparison estimators augmentation schemes  biometrika 
             
liu  j  s          nonparametric hierarchical bayes via sequential imputations  annals
statistics                 
liu  j  s          monte carlo strategies scientific computing  springer verlag  new
york  inc 
maceachern  s   clyde  m     liu  j          sequential importance sampling nonparametric bayes models  next generation  canadian journal statistics     
       
maceachern  s  n          estimating normal means conjugate style dirichlet process
prior  communications statistics simulation computation                 
mackay  d          introduction monte carlo methods  proceedings nato advanced study institute learning graphical models  sept    oct    pp         
maier  d          theory relational databases  computer science press  rockville 
md 
mceliece  r   mackay  d     cheng  j  f          turbo decoding instance pearls
belief propagation algorithm  ieee j  selected areas communication             
miller  r   masarie  f     myers  j          quick medical reference  qmr  diagnostic
assistance  medical computing              
miller  r   pople  h     myers  j          internist    experimental computerbased
diagnostic consultant general internal medicine  new english journal medicine 
                
murphy  k  p   weiss  y     jordan  m  i          loopy belief propagation approximate
inference  empirical study  proceedings   th conference uncertainty
artificial intelligence  uai   pp          morgan kaufmann 
parker  r     miller  r          using causal knowledge create simulated patient cases 
cpcs project extension internist    proceedings   th symp 
comp  appl  medical care  pp         
pearl  j          probabilistic reasoning intelligent systems  morgan kaufmann 
peot  m  a     shachter  r  d          fusion propagation multiple observations
belief networks  artificial intelligence             
pradhan  m   provan  g   middleton  b     henrion  m          knowledge engineering
large belief networks  proceedings   th conference uncertainty artificial
intelligence  seattle  wa  pp         
rish  i   kask  k     dechter  r          empirical evaluation approximation algorithms
probabilistic decoding  proceedings   th conference uncertainty
artificial intelligence  uai   pp          morgan kaufmann 
  

fibidyuk   dechter

roberts  g  o     sahu  s  k          updating schemes  correlation structure  blocking
parameterization gibbs sampler  journal royal statistical society 
series b                 
roberts  g  o     tweedie  r  l          bounds regeneration times convergence
rates markov chains  stochastic processes applications             
roberts  g  o     tweedie  r  l          corregendum bounds regeneration times
convergence rates markov chains  stochastic processes applications     
       
rosenthal  j  s          convergence rates markov chains  siam review             
    
rosti  a  v     gales  m          rao blackwellised gibbs sampling switching linear
dynamical systems  ieee international conference acoustics  speech 
signal processing  icassp        pp         
schervish  m     carlin  b          convergence successive substitution sampling 
journal computational graphical statistics            
shachter  r  d   andersen  s  k     solovitz  p          global conditioning probabilistic
inference belief networks  proceedings   th conference uncertainty
artificial intelligence  uai   pp         
shachter  r  d     peot  m  a          simulation approaches general probabilistic
inference belief networks  proceedings  th conference uncertainty
artificial intelligence  uai   pp         
steiger  n  m     wilson  j  r          convergence properties batch means method
simulation output analysis  informs journal computing                 
tierney  l          markov chains exploring posterior distributions  annals statistics 
                 
yuan  c     druzdzel  m          importance sampling algorithm based evidence
pre propagation  proceedings   th conference uncertainty artificial
intelligence  uai   pp         
zhang  n     poole  d          simple algorithm bayesian network computations 
proceedings   th canadian conference artificial intelligence  pp         

  


