journal artificial intelligence research                 

submitted        published      

closed loop learning visual control policies
sebastien jodogne
justus h  piater

jodogne montefiore ulg ac be
justus piater ulg ac be

montefiore institute  b   
university liege  b      liege  belgium

abstract
paper present general  flexible framework learning mappings images actions interacting environment  basic idea introduce
feature based image classifier front reinforcement learning algorithm  classifier
partitions visual space according presence absence highly informative local descriptors incrementally selected sequence attempts remove
perceptual aliasing  address problem fighting overfitting greedy
algorithm  finally  show high level visual features generated
power local descriptors insufficient completely disambiguating aliased states 
done building hierarchy composite features consist recursive spatial
combinations visual features  demonstrate efficacy algorithms solving
three visual navigation tasks visual version classical car hill control
problem 

   introduction
designing robotic controllers quickly becomes challenging problem  indeed  controllers face huge number possible inputs noisy  must select actions among
continuous set  able automatically adapt evolving stochastic environmental conditions  although real world robotic task often solved
directly connecting perceptual space action space given computational
mechanism  mappings usually hard derive hand  especially perceptual space contains images  evidently  automatic methods generating mappings
highly desirable  many robots nowadays equipped ccd sensors 
paper  interested reactive systems learn couple visual perceptions
actions inside dynamic world act reasonably  coupling known
visual  control  policy  wide category problems called vision for action tasks
 or simply visual tasks   despite fifty years years active research artificial
intelligence  robotic agents still largely unable solve many real world visuomotor
tasks easily performed humans even animals  vision for action
tasks notably include grasping  vision guided navigation manipulation objects
achieve goal  article introduces general framework suitable building
image to action mappings using fully automatic flexible learning protocol 
    vision for action reinforcement learning
strong neuropsychological evidence suggests human beings learn extract useful information visual data interactive fashion  without external supervisor  gibson
c
    
ai access foundation  rights reserved 

fijodogne   piater

  spelke         evaluating consequence actions environment 
learn pay attention visual cues behaviorally important solving task 
way  interact outside world  gain expertise
tasks  tarr   cheng         obviously  process task driven  since different tasks
necessarily need make distinctions  schyns   rodet        
breakthrough modern artificial intelligence would design artificial system
would acquire object scene recognition skills based experience
surrounding environment  state general terms  important research
direction would design robotic agent could autonomously acquire visual skills
interactions uncommitted environment order achieve set
goals  learning new visual skills dynamic  task driven fashion complete
priori unknown visual task known purposive vision paradigm  aloimonos        
one plausible framework learn image to action mappings according purposive vision reinforcement learning  rl   bertsekas   tsitsiklis        kaelbling  littman 
  moore        sutton   barto         reinforcement learning biologically inspired
computational framework generate nearly optimal control policies automatic
way  interacting environment  rl founded analysis so called
reinforcement signal   whenever agent takes decision  receives feedback real
number evaluates relevance decision  biological perspective 
signal becomes positive  agent experiences pleasure  talk reward  
conversely  negative reinforcement implies sensation pain  corresponds
punishment  reinforcement signal arbitrarily delayed actions
responsible it  now  rl algorithms able map every possible perception
action maximizes reinforcement signal time  framework  agent
never told optimal action facing given percept  whether one
decisions optimal  rather  agent discover promising
actions constituting representative database interactions  understanding influence decisions future reinforcements  schematically  rl lies
supervised learning  where external teacher gives correct action agent 
unsupervised learning  in clue goodness action given  
rl successful applications  example turning computer excellent
backgammon player  tesauro         solving acrobot control problem  yoshimoto 
ishii    sato         making quadruped robot learn progressively walk without
human intervention  huber   grupen        kimura  yamashita    kobayashi        kohl
  stone         riding bicycle  randlv   alstrm        lagoudakis   parr       
controlling helicopter  bagnell   schneider        ng  coates  diel  ganapathi  schulte 
tse  berger    liang         major advantages rl protocol fully
automatic  imposes weak constraints environment 
unfortunately  standard rl algorithms highly sensitive number distinct
percepts well noise results sensing process  general problem
often referred bellman curse dimensionality  bellman         thus  high
dimensionality noise inherent images forbid use basic rl algorithms
direct closed loop learning image to action mappings according purposive vision 
   

ficlosed loop learning visual control policies

    achieving purposive vision reinforcement learning
exists variety work rl specific robotic problems involving perceptual
space contains images  instance  schaal        uses visual feedback solve
pole balancing task  rl used control vision guided underwater robotic vehicle  wettergreen  gaskett    zelinsky         recently  kwok fox       
demonstrated applicability rl learning sensing strategies using aibo robots 
reinforcement learning used learn strategies view selection  paletta  
pinz        sequential attention models  paletta  fritz    seifert         let us
mention use reinforcement learning vision guided tasks ball kicking  asada  noda  tawaratsumida    hosoda         ball acquisition  takahashi  takeda 
  asada         visual servoing  gaskett  fletcher    zelinsky         robot docking  weber  wermter    zochios        martnez marn   duckett        obstacle avoidance  michels  saxena    ng         interestingly  rl used way tuning high level parameters image processing applications  example  peng
bhanu        introduce rl algorithms image segmentation  whereas yin        proposes algorithms multilevel image thresholding  uses entropy reinforcement
signal 
applications preprocess images extract high level information
observed scene directly relevant task solved feeds
rl algorithm  requires prior assumptions images perceived sensors
agent  physical structure task itself  preprocessing step
task specific coded hand  contrasts objectives  consist
introducing algorithms able learn directly connect visual space action
space  without using manually written code without relying prior knowledge
task solved  aim develop general algorithms applicable
visual task formulated rl framework 
noticeable exception work iida et al         apply rl seek reach
targets  push boxes  shibata   iida        real robots  work  raw visual
signals directly feed neural network trained actor critic architecture 
examples  visual signal downscaled averaged monochrome  i e  two color 
image              pixels  output four infrared sensors added
perceptual input  approach effective specific tasks  process
used highly controlled environment  real world images much richer
could undergo strong reduction size 
    local appearance paradigm
paper  propose algorithms rely extraction visual features
way achieve compact state spaces used input traditional rl
algorithms  indeed  buried noise confusion visual cues  images contain
hints regularity  regularities captured important notion visual features 
loosely speaking  visual feature representation aspect local appearance 
e g  corner formed two intensity edges  spatially localized texture signature 
color  therefore  analyze images  often sufficient computer program extract
useful information visual signal  focusing attention robust highly
   

fijodogne   piater

percepts

image classifier

reinforcements

detected visual class
informative visual features

reinforcement learning

actions

figure    structure reinforcement learning visual classes 

informative patterns percepts  program thereafter seek characteristic
appearance observed scenes objects 
actually basic postulate behind local appearance methods
much success computer vision applications image matching  image retrieval
object recognition  schmid   mohr        lowe         rely detection
discontinuities visual signal thanks interest point detectors  schmid  mohr   
bauckhage         similarities images thereafter identified using local description
neighborhood around interest points  mikolajczyk   schmid         two images
share sufficient number matching local descriptors  considered belong
visual class 
local appearance techniques time powerful flexible 
robust partial occlusions  require segmentation  d models scenes 
seems therefore promising introduce  front rl algorithm  feature based
image classifier partitions visual space finite set distinct visual classes
according local appearance paradigm  focusing attention agent
highly distinctive local descriptors located interest points visual stimuli 
symbol corresponding detected visual class could given input
classical  embedded rl algorithm  shown figure   
preprocessing step intended reduce size input domain  thus enhancing rate convergence  generalization capabilities well robustness
rl noise visual domains  importantly  family visual features
applied wide variety visual tasks  thus preprocessing step essentially general
task independent  central difficulty dynamic selection discriminative
visual features  selection process group images share similar  task specific
properties together visual class 
    contributions
key technical contribution paper consists introduction reinforcement
learning algorithms used perceptual space contains images 
developed algorithms rely task specific pre treatment  consequence 
used vision for action task formalized markov decision
problem  review three major contributions discussed paper 
   

ficlosed loop learning visual control policies

      adaptive discretization visual space
first contribution propose new algorithm called reinforcement learning visual
classes  rlvc  combines aforementioned ideas  rlvc iterative algorithm
suitable learning direct image to action mappings taking advantage
local appearance paradigm  consists two simultaneous  interleaved learning processes 
reinforcement learning mapping visual classes actions  incremental building
feature based image classifier 
initially  image classifier contains one single visual class  images
mapped class  course  introduces kind perceptual aliasing  or hidden
state   whitehead   ballard         optimal decisions cannot always made  since
percepts requiring different reactions associated class  agent
isolates aliased classes  since external supervisor  agent rely
statistical analysis earned reinforcements  detected aliased class  agent
dynamically selects new visual feature distinctive  i e  best disambiguates
aliased percepts  extracted local descriptor used refine classifier  way 
stage algorithm  number visual classes classifier grows  new
visual features learned perceptual aliasing vanishes  resulting image classifier
finally used control system 
approach primarily motivated strong positive results mccallums u tree
algorithm  mccallum         essence  rlvc adaptation u tree visual spaces 
though internals algorithms different  originality rlvc lies
exploitation successful local appearance features  rlvc selects subset highly
relevant features fully closed loop  purposive learning process  show
algorithm practical interest  successfully applied several simulated
visual navigation tasks 
      compacting visual policies
greedy nature  rlvc prone overfitting  splitting one visual class
potentially improve control policy visual classes  therefore  splitting
strategy get stuck local minima  split made subsequently proves
useless  cannot undone original description rlvc  second contribution
provide rlvc possibility aggregating visual classes share similar
properties  least three potential benefits 
   useless features discarded  enhances generalization capabilities 
   rlvc reset search good features 
   number samples embedded rl algorithm disposal
visual class increased  results better visual control policies 
experiments indeed show improvement generalization abilities  well reduction number visual classes selected features 
   

fijodogne   piater

      spatial combinations visual features
finally  efficacy rlvc clearly depends discriminative power visual
features  power insufficient  algorithm able completely remove
aliasing  produce sub optimal control policies  practical experiments
simulated visual navigation tasks exhibit deficiency  soon number detected
visual features reduced features made similar using less sensitive
metric  now  objects encountered world composed number distinct
constituent parts  e g  face contains nose two eyes  phone possesses keypad  
parts recursively composed sub parts  e g  eye contains
iris eyelashes  keypad composed buttons   hierarchical physical structure
certainly imposes strong constraints spatial disposition visual features 
third contribution show highly informative spatial combinations visual
features iteratively constructed framework rlvc  result promising
permits construction features increasingly higher levels discriminative
power  enabling us tackle visual tasks unsolvable using individual point features
alone  best knowledge  extension rlvc appears
first attempt build visual feature hierarchies closed loop  interactive purposive
learning process 

   overview reinforcement learning
framework relies theory rl  introduced section  rl 
environment traditionally modeled markov decision process  mdp   mdp
tuple hs  a    ri  finite set states  finite set actions 
probabilistic transition function s  r reinforcement function
r  mdp obeys following discrete time dynamics  time t  agent
takes action environment lies state st   agent perceives numerical
reinforcement rt     r st      reaches state st   probability  st     st     
thus  point view agent  interaction environment defined
quadruple hst     rt     st   i  note definition markov decision processes
assumes full observability state space  means agent able
distinguish states environment using sensors  allows us
talk indifferently states percepts  visual tasks  set images 
percept to action mapping fixed probabilistic function     states
actions  percept to action mapping tells agent probability
choose action faced percept  rl terminology  mapping called
stationary markovian control policy  infinite sequence interactions starting
state st   discounted return
rt  


x

rt i    

   

i  

       discount factor gives current value future reinforcements  markov decision problem given mdp find optimal percept to action
mapping maximizes expected discounted return  whatever starting state is 
   

ficlosed loop learning visual control policies

possible prove problem well defined  optimal percept to action
mapping always exists  bellman        
markov decision problems solved using dynamic programming  dp  algorithms
 howard        derman         let percept to action mapping  let us call
state action value function q  s  a    function giving state
action expected discounted return obtained starting state s  taking action
a  thereafter following mapping  
q  s  a    e  rt   st   s    a   

   

e denotes expected value agent follows mapping   let us define
h transform q functions q functions
x
 hq  s  a    r s  a   
 s  a  s    max
q s    a    
   
 
s 



a  note h transform equally referred bellman
backup operator state action value functions  optimal mappings given mdp
share q function  denoted q called optimal state action value function 
always exists satisfies bellmans so called optimality equation  bellman       
hq   q  

   

optimal state action value function q known  optimal deterministic perceptto action mapping easily derived choosing
 s    argmax q  s  a  

   

aa

s  another useful concept dp theory optimal value
function v   state s  v  s  corresponds expected discounted return
agent always chooses optimal action encountered state  i e 
v  s    max q  s  a  
aa

   

dynamic programming includes well known value iteration  bellman         policy iteration  howard        modified policy iteration  puterman   shin        algorithms  value iteration learns optimal state action value function q   whereas policy
iteration modified policy iteration directly learn optimal percept to action mapping 
rl set algorithmic methods solving markov decision problems
underlying mdp known  bertsekas   tsitsiklis        kaelbling et al         sutton
  barto         precisely  rl algorithms assume knowledge r 
input rl algorithms basically sequence interactions hst     rt     st   agent
environment  rl techniques often divided two categories 
   model based methods first build estimate underlying mdp  e g 
computing relative frequencies appear sequence interactions  
use classical dp algorithms value policy iteration 
   model free methods sarsa  rummery   niranjan         d    barto 
sutton    anderson        sutton         popular q learning  watkins 
       compute estimate 
   

fijodogne   piater

   reinforcement learning visual classes
discussed introduction  propose insert image classifier rl
algorithm  classifier maps visual stimuli set visual classes according
local appearance paradigm  focusing attention agent highly distinctive
local descriptors detected interest points images 
    incremental discretization visual space
formally  let us call d  infinite set local descriptors spanned
chosen local description method  elements equivalently referred visual
features  usually  corresponds rn n    assume existence visual
feature detector   boolean function     b testing whether given image
exhibits given local descriptor one interest points  schmid et al         
suitable metric used test similarity two visual features  e g  mahalanobis
euclidean distance 
image classifier iteratively refined  incremental process  natural
way implement image classifiers use binary decision trees  internal
nodes labeled visual feature  presence tested node 
leaves trees define set visual classes  hopefully much smaller
original visual space  upon possible apply directly usual rl
algorithm  classify image  system starts root node  progresses
tree according result feature detector visual feature found
descent  reaching leaf 
summarize  rlvc builds sequence c    c    c          growing decision trees 
sequence attempts remove perceptual aliasing  initial classifier c  maps
input images single visual class v      stage k  classifier ck partitions
visual perceptual space finite number mk visual classes  vk             vk mk   
    learning architecture
resulting learning architecture called reinforcement learning visual classes
 rlvc   jodogne   piater      a   basic idea behind algorithms  namely
iterative learning decision tree  primarily motivated adaptive resolution techniques
previously introduced reinforcement learning  notably mccallums
u tree algorithm  mccallum         section  idea showed extremely
fruitful suitably adapted visual spaces  links rlvc adaptiveresolution techniques thoroughly discussed section     
components rlvc depicted figure    in depth discussion
components given next sections  time being  review
them 
rl algorithm  classifier sequence  arbitrary  standard rl algorithm
applied  provides information optimal state action function 
optimal value function optimal policy induced current classifier
ck   purpose computations  either new interactions acquired 
   

ficlosed loop learning visual control policies

figure    different components rlvc algorithm 
database previously collected interactions exploited  component
covered sections             
aliasing detector  agent learned visual classes required complete
task  viewpoint embedded rl algorithm  input space
partially observable  aliasing detector extracts classes perceptual
aliasing occurs  analysis bellman residuals  indeed  explained
section        exist tight relations perceptual aliasing bellman
residuals  aliased class detected  rlvc stops 
feature generator  applied rl algorithm  database interactions
hst     rt     st   available  feature generator component produces set f
candidate visual features aliased class vk i   features used
refine classifier chosen among set candidates  step
exposed sections       
feature selector  set candidate features f built aliased visual
class vk i   component selects visual feature f f best reduces
perceptual aliasing  candidate feature discriminant  component returns
conventional bottom symbol   feature selector rlvc described
section     
classifier refinement  leaves correspond aliased classes featurebased image classifier replaced internal node testing presence absence
selected visual features 
post processing  optional component invoked every refinement  corresponds techniques fighting overfitting  details given section   
general outline rlvc described algorithm    note experiments contained paper  model based rl algorithms applied static
   

fijodogne   piater

algorithm   general structure rlvc
   k  
   mk  
   ck binary decision tree one leaf
   repeat
  
collect n interactions hst     rt     st  
  
apply arbitrary rl algorithm sequence mapped ck
  
ck   ck
  
            mk  
  
aliased vk i  
   
f generator   st   ck  st     vk i   
   
f selector  vk i   f  
   
f   
   
ck     refine vk i testing f
   
mk   mk      
   
end
   
end
   
end
   
k k  
   
post process ck  
    ck   ck 
databases interactions fifth step algorithm    databases collected
using fully randomized exploration policy  choice guided ease
implementation presentation  way collecting experience could used
well  example re sampling new database interactions iteration rlvc 
crucial point rlvc generates representation visual control policies
set collected visuomotor experience  makes rlvc interactive  following sections describe remaining algorithms  namely aliased  generator  selector
post process 
    detection aliased visual classes
discuss aliasing detected classifier ck  
      projection mdp image classifier
formally  image classifier ck converts sequence n interactions
hst     rt     st   i 
mapped sequence n quadruples
hck  st      rt     ck  st    i 
upon embedded rl algorithm applied  let us define mapped mdp mk
mdp
hsk   a  tk   rk i 
   

ficlosed loop learning visual control policies

obtained mapped sequence  sk set visual classes
known ck   tk rk computed using relative frequencies
mapped sequence  follows 
consider two visual classes v  v    vk             vk mk   one action a  define
following functions 
 v  a  equals   ck  st     v   a    otherwise 
 v  a  v     equals   ck  st     v   ck  st       v     a    otherwise 
 v  a  number ts  v  a      
using notation  write 
sk    vk             vk mk   
p
 
tk  v  a  v       n
t    v  a  v     v  a  
p
rk  v  a    n
t   rt  v  a   v  a  
      optimal q function mapped mdp
mapped mdp mk induces optimal q function domain sk
 
denoted q 
k   computing qk difficult  general  may exist mdp defined
state space sk action space generate given mapped sequence 
since latter necessarily markovian anymore  thus  rl algorithm run
mapped sequence  might converge toward q 
k   even converge all 
however  applied mapped sequence  model based rl method  cf  section   
used compute q 
k mk used underlying model  conditions 
q learning converges optimal q function mapped mdp  singh  jaakkola 
  jordan        
turn  function q 
k induces another q function initial domain
relation 
qk  s  a    q 
   
k  ck  s   a   
absence aliasing  agent would perform optimally  qk would correspond
q   according bellman theorem states uniqueness optimal q function  cf 
section     equation    function
bk  s  a     hqk   s  a  qk  s  a 

   

therefore measure aliasing induced image classifier ck   rl terminology 
bk bellman residual function qk  sutton         basic idea behind rlvc
refine states non zero bellman residual 
      measuring aliasing
consider time stamp database interactions hst     rt     st   i  according
equation    bellman residual corresponds state action pair  st     equals
x
bk  st       r st      
 st     s    max
qk  s    a    qk  st     
   
 


s 

   

fijodogne   piater

algorithm   aliasing criterion
   aliased vk i    
  

  
 t   ck  st     vk i   a 
  
      
  
return true
  
end
  
end
  
return false
unfortunately  rl agent access transition probabilities
reinforcement function r mdp modeling environment  therefore  equation   cannot directly evaluated  similar problem arises q learning  watkins 
      fitted q iteration  ernst  geurts    wehenkel        algorithms 
algorithms solve problem considering stochastic version time difference
described equation    value
x
 st     s    max
qk  s    a   
    
 


s 

indeed estimated
max
qk  s    a    
 


    

successor s  chosen probability  st     s     following transitions
environment ensures making transition st st   probability  st     st     
thus
  rt     max
qk  st     a    qk  st    
a 

 
 
  rt     max
q 
k ck  st       qk  ck  st    a 
 


    
    

unbiased estimate bellman residual state action pair  st      jaakkola 
jordan    singh          importantly  system deterministic absence
perceptual aliasing  estimates equal zero  therefore  nonzero potentially
indicates presence perceptual aliasing visual class vt   ck  st   respect
action   criterion detecting aliased classes consists computing q 
k
function  sweeping interactions hst     rt     st   identify nonzero
  practice  assert presence aliasing variance exceeds given
threshold   summarized algorithm         denotes variance set
samples 
    generation selection distinctive visual features
aliasing detected visual class vk i sk respect action a 
need select local descriptor best explains variations set values
   worth noticing corresponds updates would applied q learning 
known learning rate time t 

   

ficlosed loop learning visual control policies

algorithm   canonical feature generator
   generator  s            sn     
  
f   
  
            n 
  
 x  y   x  y  interest point si
  
f f  symbol descriptor si   x  y   
  
end
  
end
  
return f
corresponding vk i a  local descriptor chosen among set candidate
visual features f  
      extraction candidate features
informally  canonical way building f visual class vk i consists in 
   identifying collected visual percepts st ck  st     vk i  
   locating interest points selected images st  
   adding f local descriptor interest points 
corresponding feature generator detailed algorithm    latter algorithm 
descriptor s  x  y  returns local description point location  x  y  image s 
symbol d  returns symbol corresponds local descriptor f according
used metric  however  complex strategies generating visual features
used  strategy builds spatial combinations individual point features
presented section   
      selection candidate features
problem choosing candidate feature reduces variations set
real valued bellman residuals regression problem  suggest adaptation
popular splitting rule used cart algorithm building regression trees  breiman 
friedman    stone         
cart  variance used impurity indicator  split selected refine
particular node one leads greatest reduction sum squared
differences response values learning samples corresponding node
mean  formally  let    hxi   yi i  set learning samples  xi rn
input vectors real numbers  yi r real valued outputs  cart selects
following candidate feature 


v
v
f   argmin pv  
  pv  
 

    

vf

   note previous work  used splitting rule borrowed building classification
trees  quinlan        jodogne   piater      a  

   

fijodogne   piater

algorithm   feature selection
   selector vk i   f    
  
f
 best feature found far 
  
r  
 variance reduction induced f  
  

  
 t   ck  st     vk i   a 
  
visual feature f f
  
 t   st exhibits f  
  
 t   st exhibit f  
  
 s    t  
   
 s    t  
   
r    s        s  
   
r   r distributions  s     significantly different
   
f f
   
r
   
end
   
end
   
end
   
return f

pv  resp  pv   proportion samples exhibit  resp  exhibit 
v  resp  v   set samples exhibit  resp  exhibit 
feature v 

feature v  idea directly transferred framework  set xi
corresponds set interactions hst     rt     st   i  set yi corresponds
set   written explicitly algorithm   
algorithms exploit stochastic version bellman residuals  course  real environments general non deterministic  generates variations bellman residuals
consequence perceptual aliasing  rlvc made somewhat robust
variability introducing statistical hypothesis test  candidate feature 
students ttest used decide whether two sub distributions feature induces
significantly different  approach used u tree  mccallum        
    illustration simple navigation task
evaluated system abstract task closely parallels real world scenario
avoiding unnecessary complexity  consequence  sensor model use may
seem unrealistic  better visual sensor model exploited section     
rlvc succeeded solving continuous  noisy visual navigation task depicted
figure    goal agent reach fast possible one two exits
maze  set possible locations continuous  location  agent four
possible actions  go up  right  down  left  every move altered gaussian noise 
standard deviation    size maze  glass walls present
maze  whenever move would take agent wall outside maze  location
changed 
   

ficlosed loop learning visual control policies

figure    continuous  noisy navigation task  exits maze indicated boxes
cross  walls glass identified solid lines  agent depicted
center figure  one four possible moves represented
arrow  length corresponds resulting move  sensors return
picture corresponds dashed portion image 

agent earns reward     exit reached  move  including
forbidden ones  generates zero reinforcement  agent succeeds escaping
maze  arrives terminal state every move gives rise zero reinforcement 
task  set      note agent faced delayed reward problem 
must take distance two exits consideration choosing
attractive one 
maze ground carpeted color image           pixels
montage pictures coil     database  nene  nayar    murase        
agent direct access  x  y  position maze  rather  sensors
take picture surrounding portion ground  portion larger
blank areas  makes input space fully observable  importantly  glass walls
transparent  sensors return portions tapestry behind
them  therefore  way agent directly locate walls  obliged
identify regions maze action change location 
   

fijodogne   piater

figure    deterministic image to action mapping results rlvc  sampled
regularly spaced points  manages choose correct action location 

experiment  used color differential invariants visual features  gouet
  boujemaa         entire tapestry includes      different visual features  rlvc
selected     features  corresponding ratio    entire set possible features 
computation stopped generation    image classifiers  i e  k reached
     took    minutes    ghz pentium iv using databases        interactions 
    visual classes identified  small number  compared number
perceptual classes would generated discretization maze agent
knows  x  y  position  example  reasonably sized      grid leads     perceptual
classes 
figure   shows optimal  deterministic image to action mapping results
last obtained image classifier ck  
 s    argmax qk  s  a    q 
k  ck  s   a   
aa

   

    

ficlosed loop learning visual control policies

 a 

 b 

figure     a  optimal value function  agent direct access  x  y 
position maze set possible locations discretized
      grid  brighter location  greater value   b  final value
function obtained rlvc 

figure   compares optimal value function discretized problem one obtained rlvc  similarity two pictures indicates soundness
approach  importantly  rlvc operates neither pretreatment  human intervention  agent initially aware visual features important task 
moreover  interest selecting descriptors clear application  direct  tabular
representation q function considering boolean combinations features would
        cells 
behavior rlvc real word images investigated  navigation
rules kept identical  tapestry replaced panoramic photograph
         pixels subway station  depicted figure    rlvc took     iterations
compute mapping right figure    computation time     minutes
   ghz pentium iv using databases        interactions      distinct visual features
selected among set      possible ones  generating set     visual classes  again 
resulting classifier fine enough obtain nearly optimal image to action mapping
task 

    related work
rlvc thought performing adaptive discretization visual space
basis presence visual features  previous reinforcement learning algorithms
exploit presence perceptual features various contexts discussed 
   

fijodogne   piater

 a 

 b 

figure     a  navigation task real world image  using conventions
figure     b  deterministic image to action mapping computed rlvc 

   

ficlosed loop learning visual control policies

      perceptual aliasing
explained above  incremental selection set informative visual features necessarily leads temporary perceptual aliasing  rlvc tries remove  generally 
perceptual aliasing occurs whenever agent cannot always take right basis
percepts 
early work reinforcement learning tackled general problem two distinct
ways  either agent identifies avoids states perceptual aliasing occurs
 as lion algorithm  see whitehead   ballard         tries build short term
memory allow remove ambiguities percepts  as predictive
distinctions approach  see chrisman         sketchily  two algorithms detect
presence perceptual aliasing analysis sign q learning updates 
possibility managing short term memory led development partially
observable markov decision processes  pomdp  theory  kaelbling  littman    cassandra 
       current state random variable percepts 
although approaches closely related perceptual aliasing rlvc temporarily introduces  consider exploitation perceptual features  indeed 
tackle structural problem given control task  and  such  assume
perceptual aliasing cannot removed  consequence  approaches orthogonal
research interest  since ambiguities rlvc generates removed
refining image classifier  fact  techniques tackle lack information inherent used sensors  whereas goal handle surplus information related
high redundancy visual representations 
      adaptive resolution finite perceptual spaces
rlvc performs adaptive discretization perceptual space autonomous 
task driven  purposive selection visual features  work rl incrementally partitions
large  either discrete continuous  perceptual space piecewise constant value
function usually referred adaptive resolution techniques  ideally  regions
perceptual space high granularity present needed 
lower resolution used elsewhere  rlvc adaptive resolution
algorithm  review several adaptive resolution methods previously
proposed finite perceptual spaces 
idea adaptive resolution techniques reinforcement learning goes back
g algorithm  chapman   kaelbling         inspired approaches
discussed below  g algorithm considers perceptual spaces made
fixed length binary numbers  learns decision tree tests presence informative
bits percepts  algorithm uses students t test determine bit
b percepts mapped given leaf  state action utilities states
b set significantly different state action utilities states
b unset  bit found  corresponding leaf split  process repeated
leaf  method able learn compact representations  even though
large number irrelevant bits percepts  unfortunately  region split 
information associated region lost  makes slow learning 
   

fijodogne   piater

concretely  g algorithm solve task whose perceptual space contains      distinct
percepts  corresponds set binary numbers length     bits 
mccallums u tree algorithm builds upon idea combining selective attention
mechanism inspired g algorithm short term memory enables agent
deal partially observable environments  mccallum         therefore  mccallums
algorithms keystone reinforcement learning  unify g algorithm  chapman   kaelbling        chrismans predictive distinctions  chrisman        
u tree incrementally grows decision tree kolmogorov smirnov tests 
succeeded learning behaviors driving simulator  simulator  percept consists
set   discrete variables whose variation domains contain     values 
leading perceptual space        possible percepts  thus  size perceptual
space much smaller visual space  however  task difficult
physical state space partially observable perceptual space  driving
task contains         physical states  means several physical states requiring
different reactions mapped percept sensors agent 
u tree resolves ambiguities percepts testing presence perceptual
features percepts encountered previously history system 
end  u tree manages short term memory  paper  partially observable
environments considered  challenge rather deal huge visual spaces 
without hand tuned pre processing  difficult  novel research direction 
      adaptive resolution continuous perceptual spaces
important notice methods adaptive resolution large scale  finite
perceptual spaces use fixed set perceptual features hard wired 
distinguished rlvc samples visual features possibly infinite visual feature
space  e g  set visual features infinite   makes prior assumptions
maximum number useful features  point view  rlvc closer
adaptive resolution techniques continuous perceptual spaces  indeed  techniques
dynamically select new relevant features whole continuum 
first adaptive resolution algorithm continuous perceptual spaces darling algorithm  salganicoff         algorithm  current algorithms
continuous adaptive resolution  splits perceptual space using thresholds 
purpose  darling builds hybrid decision tree assigns label point
perceptual space  darling fully on line incremental algorithm equipped
forgetting mechanism deletes outdated interactions  however limited
binary reinforcement signals  takes immediate reinforcements account 
darling much closer supervised learning reinforcement learning 
parti game algorithm  moore   atkeson        produces goal directed behaviors
continuous perceptual spaces  parti game splits regions deems important 
using game theoretic approach  moore atkeson show parti game learn
competent behavior variety continuous domains  unfortunately  approach
currently limited deterministic domains agent greedy controller
goal state known  moreover  algorithm searches solution
given task  try find optimal one 
   

ficlosed loop learning visual control policies

continuous u tree algorithm extension u tree adapted continuous perceptual spaces  uther   veloso         darling  continuous u tree
incrementally builds decision tree splits perceptual space finite set hypercubes  testing thresholds  kolmogorov smirnov sum of squared errors used
determine split node decision tree  pyeatt howe        analyze
performance several splitting criteria variation continuous u tree 
conclude students t test leads best performance  motivates use
statistical test rlvc  cf  section      
munos moore        proposed variable resolution grids  algorithm
assumes perceptual space compact subset euclidean space  begins
coarse  grid based discretization state space  contrast abstract
algorithms section  value function policy vary linearly within region 
munos moore use kuhn triangulation efficient way interpolate value function within regions  algorithm refines approximation refining cells according
splitting criterion  munos moore explore several local heuristic measures importance splitting cell including average corner value differences  variance
corner value differences  policy disagreement  explore global heuristic measures involving influence variance approximated system  variable resolution
grids probably advanced adaptive resolution algorithm available far 
      discussion
summarize  several algorithms similar spirit rlvc proposed
years  nevertheless  work appears first learn direct image toaction mappings reinforcement learning  indeed  none reinforcement learning
methods combines following desirable properties rlvc      set relevant perceptual features chosen priori hand  selection process fully
automatic require human intervention      visual perceptual spaces explicitly considered appearance based visual features      highly informative
perceptual features drawn possibly infinite set 
advantages rlvc essentially due fact candidate visual
features selected informative  ranked according
information theoretic measure inspired decision tree induction  breiman et al  
       ranking required  vision for action tasks induce large number visual
features  a typical image contains thousand them   kind criterion
ranks features  though already considered variable resolution grids  munos   moore 
       seems new discrete perceptual spaces 
rlvc defined independently fixed rl algorithm  similar spirit
continuous u tree  uther   veloso         major exception rlvc deals
boolean features  whereas continuous u tree works continuous input space  furthermore  version rlvc presented paper uses variance reduction criterion
ranking visual features  criterion  though already considered variable resolution
grids  seems new discrete perceptual spaces 
   

fijodogne   piater

   compacting visual policies
written section        original version rlvc subject overfitting  jodogne  
piater      b   simple heuristic avoid creation many visual classes simply
bound number visual classes refined stage algorithm 
since splitting one visual class potentially impact bellman residuals
visual classes  practice  first try split classes samples
considering others  since evidence variance reduction first 
tests  systematically apply heuristics  however  often insufficient taken
alone 
    equivalence relations markov decision processes
since apply embedded rl algorithm stage k rlvc  properties
optimal value function vk     optimal state action value function qk      optimal
control policy k    known mapped mdp mk   using properties  easy
define whole range equivalence relations visual classes  instance 
given threshold r    list hereunder three possible equivalence relations pair
visual classes  v  v     
optimal value equivalence 
 vk  v   vk  v       
optimal policy equivalence 
 vk  v   qk  v     k  v    
 vk  v     qk  v  k  v        
optimal state action value equivalence 
 a a   qk  v  a  qk  v     a    
therefore propose modify rlvc that  periodically  visual classes
equivalent respect one criteria merged together  experimentally
observed conjunction first two criteria tends lead best performance 
way  rlvc alternatively splits merges visual classes  compaction phase
done often  order allow exploration  best knowledge 
possibility investigated yet framework adaptive resolution methods
reinforcement learning 
original version rlvc  visual classes correspond leaves decision
tree  using decision trees  aggregation visual classes achieved
starting bottom tree recursively collapsing leaves  dissimilar leaves
found  operation close post pruning framework decision trees
machine learning  breiman et al          practice  means classes
similar properties  reached one another making number
hops upwards downwards  extremely unlikely matched  greatly reduces
interest exploiting equivalence relations 
drawback due rather limited expressiveness decision trees  decision
tree  visual class corresponds conjunction visual feature literals  defines
   

ficlosed loop learning visual control policies

path root decision tree one leaf  take full advantage equivalence
relations  necessary associate  visual class  arbitrary union conjunctions
visual features  indeed  exploiting equivalence relations  visual classes
result sequence conjunctions  splitting  disjunctions  aggregation   thus 
expressive data structure would able represent general  arbitrary boolean
combinations visual features required  data structure introduced next
section 
    using binary decision diagrams
problem representing general boolean functions extensively studied
field computer aided verification  since abstract behavior logical electronic
devices  fact  whole range methods representing state space richer
richer domains developed last years  binary decision diagram
 bdd   bryant         number queue decision diagrams  boigelot         upward
closed sets  delzanno   raskin        real vector automata  boigelot  jodogne   
wolper        
framework  bdd particularly well suited tool  acyclic graph based
symbolic representation encoding arbitrary boolean functions  much success field computer aided verification  bryant         bdd unique
ordering variables fixed  different variable orderings lead different sizes
bdd  since variables discarded reordering process  although
problem finding optimal variable ordering conp complete  bryant         automatic heuristics practice find orderings close optimal  interesting
case  since reducing size bdd potentially discards irrelevant variables 
correspond removing useless visual features 
    modifications rlvc
summarize  extension rlvc use decision trees anymore  assigns
one bdd visual class  two modifications applied algorithm   
   operation refining  visual feature f   visual class v labeled
bdd b v    consists replacing v two new visual classes v  v 
b v      b v   f b v      b v   f  
   given equivalence relation  post process ck   operation consists merging
equivalent visual classes  merge pair visual classes  v    v     v  v 
deleted  new visual class v b v     b v    b v    added  every
time merging operation takes place  advised carry variable reordering 
minimize memory requirements 
    experiments
applied modified version rlvc another simulated navigation task 
task  agent moves    spots campus university liege  cf 
figure     every time agent one    locations  body aim four possible
   

fijodogne   piater

n

 c  google map

figure    montefiore campus liege  red spots corresponds places
agent moves  agent follow links different
spots  goal enter montefiore institute  labeled red cross 
gets reward      

orientations  north  south  west  east  state space therefore size           
agent three possible actions  turn left  turn right  go forward  goal enter
specific building  obtain reward       turning left right induces
penalty    moving forward  penalty     discount factor set     
optimal control policy unique  one depicted figure   
agent direct access position orientation  rather 
perceives picture area front  cf  figure     thus  agent
connect input image appropriate reaction without explicitly knowing
geographical localization  possible location possible viewing direction 
database    images size          significant viewpoint changes
collected     databases randomly divided learning set    images
test set   images  experimental setup  versions rlvc learn
image to action mapping using interactions contain images learning set 
images test set used assess accuracy learned visual control policies 
sift keypoints used visual features  lowe         thresholding
mahalanobis distance gave rise set        distinct features  versions rlvc
applied static database        interactions collected using
fully randomized exploration policy  database used throughout entire
algorithm  database contains images belong learning set 
results basic version rlvc version extended bdds
reported figures        original version rlvc identified     visual
classes selecting     sift features  error rate computed visual policy  i e 
proportion sub optimal decisions agent presented possible stimuli 
   

ficlosed loop learning visual control policies

 c  google map

figure    one optimal  deterministic control policies montefiore navigation
task  state  indicated optimal action  the letter f stands
move forward  r turn right l turn left   policy
obtained applying standard rl algorithm scenario
agent direct access  p  d  information 

     learning set    images test set used  respect
optimal policy agent direct access position viewing direction 
modified version rlvc applied  one compacting stage every    steps 
results clearly superior  error learning set anymore 
error rate test set       number selected features reduced     
furthermore  resulting number visual classes becomes     instead      thus 
large improvement generalization abilities  well reduction number
visual classes selected features  interestingly enough  number visual classes
     close number physical states       tends indicate
algorithm starts learn physical interpretation percepts 
summarize  compacting visual policies probably required step deal realistic visual tasks  iterative splitting process applied  price pay course
   

fijodogne   piater

 c  google map

figure    percepts agent  four possible different percepts shown  correspond location viewing direction marked yellow top image 

   

ficlosed loop learning visual control policies

  

  
rlvc
rlvc   bdd
  

  

  

  

 

  

  

  

  
iterations  k 

   

   

   

error rate    

  

  

  

  

  

  

 
   

 

  

  

  

  
iterations  k 

   

   

   

error rate    

rlvc
rlvc   bdd

 
   

figure     comparison error rates basic extended versions rlvc 
error computed policy function step counter k images
learning set  resp  test set  reported left figure  resp 
right  

higher computational cost  future work focus theoretical justification used
equivalence relations  implies bridging gap theory mdp minimization  givan  dean    greig        

   learning spatial relationships
motivated introduction  section         propose extend rlvc constructing hierarchy spatial arrangements individual point features  jodogne  scalzo   
piater         idea learning models spatial combinations features takes
roots seminal paper fischler elschlager        pictorial structures 
collections rigid parts arranged deformable configurations  idea
become increasingly popular computer vision community   s  led
large literature modeling detection objects  amit   kong        burl
  perona        forsyth  haddon    ioffe         crandall huttenlocher        provide pointers recent resources  among recent techniques  scalzo piater       
propose build probabilistic hierarchy visual features represented
acyclic graph  detect presence model nonparametric belief
propagation  sudderth  ihler  freeman    willsky         graphical models
proposed representing articulated structures  pictorial structures  felzenszwalb   huttenlocher        kumar  torr    zisserman         similarly  constellation
model represents objects parts  modeled terms shape appearance
gaussian probability density functions  perona  fergus    zisserman        
work contrasts approaches generation so called composite
features driven task solved  distinguished techniques
unsupervised learning composite features  since additional information
   

fijodogne   piater

   

   
rlvc
rlvc   bdd
   

   

   

   

 

  

  

  

  
iterations  k 

   

   

   

number classes

   

   

   

   

  

  

 
   

 

  

  

  

  
iterations  k 

   

   

   

number selected features

rlvc
rlvc   bdd

 
   

figure     comparison number generated classes selected visual features basic extended versions rlvc  number visual classes
 resp  selected features  function step counter k plotted left
figure  resp  right  

embedded inside reinforcement signal drives generation composite features
focusing exploration task relevant spatial arrangements 
extension rlvc  hierarchy visual features built simultaneously
image classifier  soon sufficiently informative visual feature extracted 
algorithm tries combine two visual features order construct higher level
abstraction  hopefully distinctive robust noise  extension
rlvc assumes co existence two different kinds visual features 
primitive features  correspond individual point features  i e  localappearance descriptors  cf  section      
composite features  consist spatial combinations lower level visual features 
priori bound maximal height hierarchy  therefore 
composite feature potentially combined primitive feature 
composite feature 
    detection visual features
natural way represent hierarchy use directed acyclic graph g    v  e  
vertex v v corresponds visual feature  edge  v  v     e
models fact v   part composite feature v  thus  g must binary 
i e  vertex either child  exactly two children  set vp leaves
g corresponds set primitive features  set vc internal vertexes
represents set composite features 
leaf vertex vp vp annotated local descriptor d vp    similarly 
internal vertex vc vc annotated constraints relative position
parts  work  consider constraints distances constituent
   

ficlosed loop learning visual control policies

visual features composite features  assume distributed
according gaussian law g     mean standard deviation   evidently  richer
constraints could used  taking relative orientation scaling factor constituent features consideration  would require use multivariate gaussians 
precisely  let vc composite feature  parts v  v    order
trigger detection vc image s  occurrence v 
occurrence v  relative euclidean distance sufficient likelihood
generated gaussian mean  vc   standard deviation  vc    ensure
symmetry  location composite feature taken midpoint
locations v  v   
occurrences visual feature v percept found using recursive
algorithm    course  steps     algorithm    test st exhibit v 
rewritten function algorithm    checking occurrences v  st       
    generation composite features
cornerstone extension rlvc way generating composite features 
general idea behind algorithm accumulate statistical evidence relative
positions detected visual features order find conspicuous coincidences visual
features  done providing evolved implementation generator s            sn  
one algorithm   
      identifying spatial relations
first extract set f  primitive composite  features occur within
set provided images  s            sn   
f    v v    i  si exhibits v   

    

identify pairs visual features occurrences highly correlated within
set provided images  s            sn    simply amounts counting number
co occurrences pair features f   keeping pairs corresponding
count exceeds fixed threshold 
let v  v  two features highly correlated  search meaningful
spatial relationship v  v  carried images  s            sn  
contain occurrences v  v    co occurrence  accumulate set
distances corresponding occurrences v  v    finally  clustering
algorithm applied distribution order detect typical distances v 
v    purpose experiments  used hierarchical clustering  jain 
murty    flynn         cluster  gaussian fitted estimating mean value
standard deviation   finally  new composite feature vc introduced
feature hierarchy  v  v  parts  vc      vc      
summary  algorithm    replace call algorithm   call algorithm   
   

fijodogne   piater

algorithm   detecting composite features
   occurrences v  s   
  
v primitive
  
return   x  y     x  y  interest point s  local descriptor corresponds d v  
  
else
  
  
  
o  occurrences subfeature   v   s 
  
o  occurrences subfeature   v   s 
  
p
 x    y    o   x    y    o 
  
 x  x        y  y    
   
g d  v    v  
   
   x    x        y    y       
   
end
   
end
   
return
   
end

algorithm   generation composite features
   generator  s            sn     
  
f    v v    i  si exhibits v 
  
f       
  
 v    v    f f
  
enough co occurrences v  v   s            sn  
  
  
  
            n 
  
occurrences  x    y    v  si
  
occurrences
 x    y    v  si
p
   
   x  x        y  y      
   
end
   
end
   
end
   
apply clustering algorithm
   
cluster c    d          dm  
   
  mean c 
   
  stddev c 
   
add f   composite feature vc composed v  v    annotated
mean standard deviation
   
end
   
end
   
end
   
return f  

   

ficlosed loop learning visual control policies

h p 
   

n

u
   

mg
 

  

  

 

p

   

figure     car hill control problem 

      feature validation
algorithm   generate several composite features given visual class vk i   however 
end algorithm    one generated composite feature kept 
important notice performance clustering method critical
purpose  indeed  irrelevant spatial combinations automatically discarded  thanks
variance reduction criterion feature selection component  fact  reinforcement
signal helps direct search good feature  advantage unsupervised
methods building feature hierarchies 
    experiments
demonstrate efficacy algorithms version classical car hill
control problem  moore   atkeson         position velocity information
presented agent visually 
episodic task  car  modeled mass point  riding without friction
hill  shape defined function 

h p   

p  p
 p
p     
 
p       p p   

goal agent reach fast possible top hill  i e  location
p    top hill  agent obtains reward      car thrust left
right acceleration   newtons  however  gravity  acceleration
insufficient agent reach top hill always thrusting toward right 
rather  agent go left while  hence acquiring potential energy going
left side hill  thrusting rightward  two constraints  agent
allowed reach locations p      velocity greater   absolute
value leads destruction car 
   

fijodogne   piater

      formal definition task
formally  set possible actions           state space     p  s   
 p     s      system following continuous time dynamics 
p  
 




p

    h    p  



gh    p 
 
    h    p  

thrust acceleration  h    p  first derivative h p      
mass car  g        acceleration due gravity  continuous time
dynamics approximated following discrete time state update rule 
st     st   hpt   h  st   
st     pt   hst  
h       integration time step  reinforcement signal defined
expression 

    st      st       
r  st   st    a   
 
otherwise 
setup  discount factor set      
definition actually mix two coexistent formulations car hill
task  ernst  geurts    wehenkel        moore   atkeson         major differences
initial formulation problem  moore   atkeson        set
possible actions discrete  goal top hill  rather given
area hill   definition ernst et al          ensure existence
interesting solution  velocity required remain less    instead    
integration time step set h        instead       
      inputs agent
previous work  moore   atkeson        ernst et al          agent always assumed
direct access numerical measure position velocity  exception
gordons work visual  low resolution representation global scene given
agent  gordon         experimental setup  agent provided two
cameras  one looking ground underneath  second velocity gauge  way 
agent cannot directly know current position velocity  suitably interpret
visual inputs derive them 
examples pictures sensors return presented figure    
ground carpeted color image          pixels montage pictures
coil     database  nene et al          important notice using
individual point features insufficient solving task  since set features
pictures velocity gauge always same  know velocity  agent
generate composite features sensitive distance primitive features cursor
respect primitive features digits 
   

ficlosed loop learning visual control policies

 a 

 b 
figure      a  visual percepts corresponding pictures velocity gauge     
              b  visual percepts returned position sensor 
region framed white rectangle corresponds portion ground
returned sensor p        portion slides back forth
agent moves 

      results
experimental setup  used color differential invariants  gouet   boujemaa       
primitive features  among possible visual inputs  both position
velocity sensors      different primitive features  entire image ground
includes     interest points  whereas images velocity gauge include   
interest points 
output rlvc decision tree defines     visual classes  internal
node tree tests presence one visual feature  taken set    distinct 
highly discriminant features selected rlvc  among    selected visual features 
   primitive    composite features  two examples composites features
selected rlvc depicted figure     computation stopped k     
refinement steps algorithm   
show efficacy method  compare performance scenario
agent direct perception current  p  s  state  latter scenario 
state space discretized grid       cells  number    chosen since
approximately corresponds square root      number visual classes
produced rlvc  way  rl provided equivalent number perceptual classes
two scenarios  figure    compares optimal value function direct perception
   

fijodogne   piater

velocity

 

 

 
 

 

position

 

 a 

velocity

 

 

 
 

 

position

 

 b 

figure      a  optimal value function  agent direct access current
 p  s  state  input space discretized       grid 
brighter location  greater value   b  value function obtained
rlvc 

   

ficlosed loop learning visual control policies

figure     two composite features generated  yellow  primitive features
composed marked yellow  first feature triggers
velocities around    whereas second triggers around   

problem one obtained rlvc  also  two pictures similar 
indicates soundness approach 
evaluated performance optimal image to action mapping
  argmax q   p  s   a 

    

aa

obtained rlvc  purpose  agent placed randomly hill 
initial velocity    then  used mapping choose action  reached
final state  set        trials carried step k algorithm   
figure    compares proportion trials missed goal  either leaving
hill left  acquiring high velocity  rlvc directperception problem  k became greater     proportion missed trials
always smaller rlvc direct perception problem  advantage favor
rlvc due adaptive nature discretization  figure    compares mean
lengths successful trials  mean length rlvc trials clearly converges
direct perception trials  staying slightly larger 
conclude  rlvc achieves performance close direct perception scenario  however  mapping built rlvc directly links visual percepts appropriate actions 
without considering explicitly physical variables 

   summary
paper introduces reinforcement learning visual classes  rlvc   rlvc designed
learn mappings directly connect visual stimuli output actions optimal
surrounding environment  framework rlvc general  sense
applied problem formulated markov decision problem 
learning process behind algorithms closed loop flexible  agent
takes lessons interactions environment  according purposive vision
paradigm  rlvc focuses attention embedded reinforcement learning algorithm
highly informative robust parts inputs testing presence absence
local descriptors interest points input images  relevant visual features
   

fijodogne   piater

  
rlvc
direct perception    x   grid 

missed goal    

  

  

  

 

 

 

 

  

  

  

  

  

  

  

iterations  k 

figure     evolution number times goal missed iterations
rlvc 

  

rlvc
direct perception    x   grid 

  

mean length interaction

  

  

  

  

  

  

 

 

  

  

  

  

  

  

  

iterations  k 

figure     evolution mean lengths successful trials iterations rlvc 

   

ficlosed loop learning visual control policies

incrementally selected sequence attempts remove perceptual aliasing 
discretization process targets zero bellman residuals inspired supervised learning algorithms building decision trees  algorithms defined independently
interest point detector  schmid et al         local description technique  mikolajczyk   schmid         user may choose two components sees fit 
techniques fighting overfitting rlvc proposed  idea
aggregate visual classes share similar properties respect theory dynamic
programming  interestingly  process enhances generalization abilities learned
image to action mapping  reduces number visual classes built 
finally  extension rlvc introduced allows closed loop  interactive
purposive learning hierarchy geometrical combinations visual features 
contrast prior work topic  uses either supervised
unsupervised framework  piater        fergus  perona    zisserman        bouchard  
triggs        scalzo   piater         besides novelty approach  shown
practical value visual control tasks information provided individual
point features alone insufficient solving task  indeed  spatial combinations visual
features informative robust noise 

   future work
area applications rlvc wide  since nowadays robotic agents often equipped
ccd sensors  future research includes demonstration applicability
algorithms reactive robotic application  grasping objects combining visual
haptic feedback  coelho  piater    grupen         necessitates extension
techniques continuous action spaces  fully satisfactory solutions exist
date  rlvc could potentially applied human computer interaction 
actions need physical actions 
closed loop learning hierarchy visual feature raises interesting research
directions  example  combination rlvc techniques disambiguating
aliased percepts using short term memory  mccallum        could solve visual
tasks percepts agent alone provide enough information solving
task  likewise  unsupervised learning kinds geometrical models  felzenszwalb   huttenlocher        could potentially embedded rlvc  hand 
spatial relationships currently take consideration relative angles
parts composite feature  would increase discriminative power
composite features  requires non trivial techniques clustering circular domains 

acknowledgments
authors thank associate editor thorsten joachims three anonymous reviewers many suggestions improving quality manuscript  sebastien
jodogne gratefully acknowledge financial support belgian national fund
scientific research  fnrs  
   

fijodogne   piater

references
aloimonos  y          purposive qualitative active vision  proc    th international conference pattern recognition  pp         
amit  y     kong  a          graphical templates model registration  ieee transactions pattern analysis machine intelligence                 
asada  m   noda  s   tawaratsumida  s     hosoda  k          vision based behavior acquisition shooting robot using reinforcement learning  proc  iapr ieee
workshop visual behaviors  pp         
bagnell  j     schneider  j          autonomous helicopter control using reinforcement
learning policy search methods  proc  international conference robotics
automation  ieee 
barto  a   sutton  r     anderson  c          neuronlike adaptive elements
solve difficult learning control problems  ieee transactions systems  man
cybernetics                 
bellman  r          dynamic programming  princeton university press 
bertsekas  d     tsitsiklis  j          neuro dynamic programming  athena scientific 
boigelot  b          symbolic methods exploring infinite state spaces  ph d  thesis 
university liege  liege  belgium  
boigelot  b   jodogne  s     wolper  p          effective decision procedure linear
arithmetic integer real variables  acm transactions computational logic
 tocl                 
bouchard  g     triggs  b          hierarchical part based visual object categorization 
ieee conference computer vision pattern recognition  vol     pp         
san diego  ca  usa  
breiman  l   friedman  j     stone  c         
wadsworth international group 

classification regression trees 

bryant  r          graph based algorithms boolean function manipulation  ieee transactions computers                 
bryant  r          symbolic boolean manipulation ordered binary decision diagrams 
acm computing surveys                 
burl  m     perona  p          recognition planar object classes  proc  ieee
conference computer vision pattern recognition  pp          san francisco
 ca  usa  
chapman  d     kaelbling  l          input generalization delayed reinforcement learning  algorithm performance comparisons  proc    th international
joint conference artificial intelligence  ijcai   pp          sydney 
chrisman  l          reinforcement learning perceptual aliasing  perceptual
distinctions approach  national conference artificial intelligence  pp         
   

ficlosed loop learning visual control policies

coelho  j   piater  j     grupen  r          developing haptic visual perceptual categories reaching grasping humanoid robot  robotics autonomous
systems  special issue humanoid robots                  
crandall  d     huttenlocher  d          weakly supervised learning part based spatial
models visual object recognition  proc   th european conference
computer vision 
delzanno  g     raskin  j  f          symbolic representation upward closed sets 
tools algorithms construction analysis systems  lecture notes
computer science  pp          berlin  germany  
derman  c          finite state markovian decision processes  academic press  new york 
ernst  d   geurts  p     wehenkel  l          iteratively extending time horizon reinforcement learning  proc    th european conference machine learning  pp 
       dubrovnik  croatia  
ernst  d   geurts  p     wehenkel  l          tree based batch mode reinforcement learning 
journal machine learning research            
felzenszwalb  p     huttenlocher  d          pictorial structures object recognition 
international journal computer vision               
fergus  r   perona  p     zisserman  a          object class recognition unsupervised
scale invariant learning  ieee conference computer vision pattern recognition  vol     pp          madison  wi  usa  
fischler  m     elschlager  r          representation matching pictorial structures  ieee transactions computers               
forsyth  d   haddon  j     ioffe  s          finding objects grouping primitives  shape 
contour grouping computer vision  pp          london  uk   springerverlag 
gaskett  c   fletcher  l     zelinsky  a          reinforcement learning visual servoing
mobile robot  proc  australian conference robotics automation 
melbourne  australia  
gibson  e     spelke  e          development perception  flavell  j  h     markman  e  m   eds    handbook child psychology vol  iii  cognitive development
  th edition    chap     pp       wiley 
givan  r   dean  t     greig  m          equivalence notions model minimization
markov decision processes  artificial intelligence                   
gordon  g          stable function approximation dynamic programming  proc 
international conference machine learning  pp         
gouet  v     boujemaa  n          object based queries using color points interest 
ieee workshop content based access image video libraries  pp       
kauai  hi  usa  
howard  r          dynamic programming markov processes  technology press
wiley  cambridge  ma  new york 
   

fijodogne   piater

huber  m     grupen  r          control structure learning locomotion gaits   th
int  symposium robotics applications  anchorage  ak  usa   tsi press 
iida  m   sugisaka  m     shibata  k          direct vision based reinforcement learning
real mobile robot  proc  international conference neural information
processing systems  vol     pp           
jaakkola  t   jordan  m     singh  s          convergence stochastic iterative dynamic
programming algorithms  cowan  j  d   tesauro  g     alspector  j   eds    advances neural information processing systems  vol     pp          morgan kaufmann publishers 
jain  a  k   murty  m  n     flynn  p  j          data clustering  review  acm computing
surveys                 
jodogne  s     piater  j       a   interactive learning mappings visual percepts
actions  de raedt  l     wrobel  s   eds    proc    nd international
conference machine learning  icml   pp          bonn  germany   acm 
jodogne  s     piater  j       b   learning  compacting visual policies  extended abstract   proc   th european workshop reinforcement learning  ewrl  
pp       napoli  italy  
jodogne  s   scalzo  f     piater  j          task driven learning spatial combinations
visual features  proc  ieee workshop learning computer vision
pattern recognition  san diego  ca  usa   ieee 
kaelbling  l   littman  m     cassandra  a          planning acting partially
observable stochastic domains  artificial intelligence                   
kaelbling  l   littman  m     moore  a          reinforcement learning  survey  journal
artificial intelligence research            
kimura  h   yamashita  t     kobayashi  s          reinforcement learning walking
behavior four legged robot  proc    th ieee conference decision
control  orlando  fl  usa  
kohl  n     stone  p          policy gradient reinforcement learning fast quadrupedal
locomotion  proc  ieee international conference robotics automation  pp            new orleans 
kumar  m   torr  p     zisserman  a          extending pictorial structures object
recognition  proc  british machine vision conference 
kwok  c     fox  d          reinforcement learning sensing strategies  proc 
ieee international conference intelligent robots systems 
lagoudakis  m     parr  r          least squares policy iteration  journal machine
learning research              
lowe  d          distinctive image features scale invariant keypoints  international
journal computer vision                
martnez marn  t     duckett  t          fast reinforcement learning vision guided
mobile robots  proc  ieee international conference robotics automation  pp        barcelona  spain  
   

ficlosed loop learning visual control policies

mccallum  r          reinforcement learning selective perception hidden state 
ph d  thesis  university rochester  new york 
michels  j   saxena  a     ng  a          high speed obstacle avoidance using monocular
vision reinforcement learning  proc    nd international conference
machine learning  pp          bonn  germany  
mikolajczyk  k     schmid  c          performance evaluation local descriptors 
proc  ieee conference computer vision pattern recognition  vol    
pp          madison  wi  usa  
moore  a     atkeson  c          parti game algorithm variable resolution reinforcement learning multidimensional state spaces  machine learning     
munos  r     moore  a          variable resolution discretization optimal control  machine learning             
nene  s   nayar  s     murase  h          columbia object image library  coil       tech 
rep  cucs         columbia university  new york 
ng  a   coates  a   diel  m   ganapathi  v   schulte  j   tse  b   berger  b     liang  e 
        inverted autonomous helicopter flight via reinforcement learning  proc 
international symposium experimental robotics 
paletta  l   fritz  g     seifert  c          q learning sequential attention visual object
recognition informative local descriptors   proc    nd international
conference machine learning  icml   pp          bonn  germany  
paletta  l     pinz  a          active object recognition view integration reinforcement learning  robotics autonomous systems                
peng  j     bhanu  b          closed loop object recognition using reinforcement learning 
ieee transactions pattern analysis machine intelligence                 
perona  p   fergus  r     zisserman  a          object class recognition unsupervised
scale invariant learning  conference computer vision pattern recognition 
vol     p      
piater  j          visual feature learning  ph d  thesis  university massachusetts 
computer science department  amherst  ma  usa  
puterman  m     shin  m          modified policy iteration algorithms discounted
markov decision problems  management science               
pyeatt  l     howe  a          decision tree function approximation reinforcement
learning  proc  third international symposium adaptive systems  pp 
      havana  cuba 
quinlan  j          c     programs machine learning  morgan kaufmann publishers
inc   san francisco  ca  usa  
randlv  j     alstrm  p          learning drive bicycle using reinforcement learning
shaping  proc    th international conference machine learning  pp 
        madison  wi  usa   morgan kaufmann 
   

fijodogne   piater

rummery  g     niranjan  m          on line q learning using connectionist sytems  tech 
rep  cued f infeng tr      cambridge university 
salganicoff  m          density adaptive learning forgetting  proc    th
international conference machine learning  pp          amherst  ma  usa  
morgan kaufmann publishers 
scalzo  f     piater  j          unsupervised learning dense hierarchical appearance
representations  proc    th international conference pattern recognition 
hong kong 
schaal  s          learning demonstration  mozer  m  c   jordan  m     petsche 
t   eds    advances neural information processing systems  vol     pp           
cambridge  ma  mit press 
schmid  c     mohr  r          local greyvalue invariants image retrieval  ieee
transactions pattern analysis machine intelligence                 
schmid  c   mohr  r     bauckhage  c          evaluation interest point detectors 
international journal computer vision                 
schyns  p     rodet  l          categorization creates functional features  journal
experimental psychology  learning  memory cognition                 
shibata  k     iida  m          acquisition box pushing direct vision based reinforcement learning  proc  society instrument control engineers annual
conference  p    
singh  s   jaakkola  t     jordan  m          reinforcement learning soft state aggregation  advances neural information processing systems  vol     pp         
mit press 
sudderth  e   ihler  a   freeman  w     willsky  a          nonparametric belief propagation  proc  ieee conference computer vision pattern recognition 
pp         
sutton  r          learning predict methods temporal differences  machine
learning             
sutton  r     barto  a          reinforcement learning  introduction  mit press 
takahashi  y   takeda  m     asada  m          continuous valued q learning visionguided behavior acquisition  proc  international conference multisensor
fusion integration intelligent systems  pp         
tarr  m     cheng  y          learning see faces objects  trends cognitive
sciences              
tesauro  g          temporal difference learning td gammon  communications
acm               
uther  w     veloso  m          tree based discretization continuous state space reinforcement learning  proc    th national conference artificial intelligence
 aaai   pp          madison  wi  usa  
   

ficlosed loop learning visual control policies

watkins  c          learning delayed rewards  ph d  thesis  kings college  cambridge  uk  
weber  c   wermter  s     zochios  a          robot docking neural vision
reinforcement  knowledge based systems                  
wettergreen  d   gaskett  c     zelinsky  a          autonomous guidance control
underwater robotic vehicle  proc  international conference field
service robotics  pittsburgh  usa  
whitehead  s     ballard  d          learning perceive act trial error 
machine learning          
yin  p  y          maximum entropy based optimal threshold selection using deterministic
reinforcement learning controlled randomization  signal processing         
     
yoshimoto  j   ishii  s     sato  m          application reinforcement learning balancing acrobot  proc       ieee international conference systems 
man cybernetics  pp         

   


