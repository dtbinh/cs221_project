journal of artificial intelligence research               

submitted       published     

cutset sampling for bayesian networks
bozhena bidyuk
rina dechter

bbidyuk ics uci edu
dechter ics uci edu

school of information and computer science
university of california irvine
irvine  ca           

abstract
the paper presents a new sampling methodology for bayesian networks that samples
only a subset of variables and applies exact inference to the rest  cutset sampling is a
network structure exploiting application of the rao blackwellisation principle to sampling
in bayesian networks  it improves convergence by exploiting memory based inference algorithms  it can also be viewed as an anytime approximation of the exact cutset conditioning
algorithm developed by pearl  cutset sampling can be implemented efficiently when the
sampled variables constitute a loop cutset of the bayesian network and  more generally 
when the induced width of the networks graph conditioned on the observed sampled variables is bounded by a constant w  we demonstrate empirically the benefit of this scheme
on a range of benchmarks 

   introduction
sampling is a common method for approximate inference in bayesian networks  when
exact algorithms are impractical due to prohibitive time and memory demands  it is often
the only feasible approach that offers performance guarantees  given a bayesian network
over the variables x    x         xn    evidence e  and a set of samples  x t    from p  x e   an
estimate f x  of the expected value of function f  x  can be obtained from the generated
samples via the ergodic average 
 x
f  x t     
   
e f  x  e   f x   
t t
where t is the number of samples  f x  can be shown to converge to the exact value as t
increases  the central query of interest over bayesian networks is computing the posterior
marginals p  xi  e  for each value xi of variable xi   also called belief updating  for this
query  f  x  equals a  function  and the above equation reduces to counting the fraction
of occurrences of xi   xi in the samples 
p  xi  e   
 t 

t
 x
 xi  x t     
t

   

t  

where  xi  x t      iff xi   xi and  xi  x t      otherwise  alternatively  a mixture estimator can be used 
t
 x
 t 
p  xi  xi    
   
p  xi  e    
t
t  

c
    
ai access foundation  all rights reserved 

fibidyuk   dechter

 t 

where xi   x t   xi  
a significant limitation of sampling  however  is that the statistical variance increases
when the number of variables in the network grows and therefore the number of samples
necessary for accurate estimation increases  in this paper  we present a sampling scheme for
bayesian networks with discrete variables that reduces the sampling variance by sampling
from a subset of the variables  a technique also known as collapsing or rao blackwellisation 
the fundamentals of rao blackwellised sampling were developed by casella and robert
       and liu  wong  and kong        for gibbs sampling and maceachern  clyde 
and liu        and doucet  gordon  and krishnamurthy        for importance sampling 
doucet  de freitas  murphy  and russell        extended rao blackwellisation to particle
filtering in dynamic bayesian networks 
the basic rao blackwellisation scheme can be described as follows  suppose we partition the space of variables x into two subsets c and z  subsequently  we can re write any
function f  x  as f  c  z   if we can generate samples from distribution p  c e  and compute e f  c  z  c  e   then we can perform sampling on subset c only  generating samples
c      c           c t   and approximating the quantity of interest by
e f  c  z  e    ec  ez  f  c  z  c  e    f x   

t
 x
ez  f  c  z  c t    e   
t

   

t  

the posterior marginals estimates for the cutset variables can be obtained using an expression similar to eq     
 x
 ci  c t     
   
p  ci  e   
t t

or using a mixture estimator similar to eq     
p  ci  e   

 x
 t 
p  ci  ci   e   
t t

   

for xi  x c  e  e p  xi  e     ec  p  xi  c  e   and eq     becomes
p  xi  e   

 x
p  xi  c t    e   
t t

   

since the convergence rate of gibbs sampler is tied to the maximum correlation between
two samples  liu         we can expect an improvement in the convergence rate when
sampling in a lower dimensional space since    some of the highly correlated variables may be
marginalized out and    the dependencies between the variables inside a smaller set are likely
to be weaker because the variables will be farther apart and their sampling distributions will
be smoothed out  additionally  the estimates obtained from sampling in a lower dimensional
space can be expected to have lower sampling variance and therefore require fewer samples
to achieve the same accuracy of the estimates  on the other hand  the cost of generating
each sample may increase  indeed  the principles of rao blackwellised sampling have been
applied only in a few classes of probabilistic models with specialized structure  kong  liu 
  wong        escobar        maceachern        liu        doucet   andrieu       
andrieu  de freitas    doucet        rosti   gales        
 

ficutset sampling for bayesian networks

the contribution of this paper is in presenting a general  structure based scheme which
applies the rao blackwellisation principle to bayesian networks  the idea is to exploit the
property that conditioning on a subset of variables simplifies the networks structure  allowing efficient query processing by exact algorithms  in general  exact inference by variable
elimination  dechter      a        or join tree algorithms  lauritzen   spiegelhalter       
jensen  lauritzen    olesen        is time and space exponential in the induced width w
of the network  however  when a subset of the variables is assigned  i e   conditioned upon 
the induced width of the conditioned network may be reduced 
the idea of cutset sampling is to choose a subset of variables c such that conditioning
on c yields a sparse enough bayesian network having a small induced width to allow exact
inference  since a sample is an assignment to all cutset variables  we can efficiently generate
a new sample over the cutset variables in the conditioned network where the computation of
p  c e  and p  xi  c  e  can be bounded  in particular  if the sampling set c cuts all the cycles
in the network  i e   it is a loop cutset   inference over the conditioned network becomes
linear  in general  if c is a w cutset  namely a subset of nodes such that when assigned  the
induced width of the conditioned network is w  the time and space complexity of computing
the next sample is o  c   n  dw     where d is the maximum domain size and n    x  

the idea of exploiting properties of conditioning on a subset of variables was first proposed for exact belief updating in the context of cutset conditioning  pearl         this
scheme requires enumerating all instantiations of the cutset variables  since the number of
instances is exponential in the size of the cutset  c   sampling over the cutset space may
be the right compromise when the size of the cutset is too big  thus  sampling on a cutset
can also be viewed as an anytime approximation of the cutset conditioning approach 
although rao blackwellisation in general and cutset sampling in particular can be applied in the context of any sampling algorithm  we will introduce the principle in the context of gibbs sampling  geman   geman        gilks  richardson    spiegelhalter       
mackay         a markov chain monte carlo sampling method for bayesian networks 
extension to any other sampling approach or any other graphical models  such as markov
networks  should be straight forward  we recently demonstrated how the idea can be incorporated into importance sampling  bidyuk   dechter        
the paper defines and analyzes the cutset sampling scheme and investigates empirically
the trade offs between sampling and exact computation over a variety of randomly generated
networks and grid structure networks as well as known real life benchmarks such as cpcs
networks and coding networks  we show that cutset sampling converges faster than pure
sampling in terms of the number of samples  as dictated by theory  and is also almost always
time wise cost effective on all the benchmarks tried  we also demonstrate the applicability of
this scheme to some deterministic networks  such as hailfinder network and coding networks 
where the markov chain is non ergodic and gibbs sampling does not converge 
section   provides background information  specifically  section     introduces bayesian
networks  section     reviews exact inference algorithms for bayesian networks  and section     provides background on gibbs sampling  the contribution of the paper presenting
the cutset sampling starts in section    section   presents the empirical evaluation of cutset
sampling  we also present an empirical evaluation of the sampling variance and the resulting standard error based on the method of batch means  for more details  see geyer        
 

fibidyuk   dechter

in section    we review previous application of rao blackwellisation and section   provides
summary and conclusions 

   background
in this section  we define essential terminology and provide background information on
bayesian networks 
    preliminaries
we use upper case letters without subscripts  such as x  to denote sets of variables and
lower case letters without subscripts to denote an instantiation of a group of variables  e g  
x indicates that each variable in set x is assigned a value   we use an upper case letter
with a subscript  such as xi   to denote a single variable and a lower case letter with a
subscript  such as xi   to denote an instantiated variable  e g   xi denotes an arbitrary value
in the domain of xi and means xi   xi    d xi   denotes the domain of the variable xi   a
superscript in a subscripted lower case letter would be used to distinguish different specific
values for a variable  i e   d xi      x i   x i         we will use x to denote an instantiation of
a set of variables x    x         xi    xi   xi          xn   and xi   x xi to denote x with element
xi removed  namely  xi    x    x         xi    xi          xn   
definition      graph concepts  a directed graph is a pair d  v  e   where v  
 x         xn   is a set of nodes and e     xi   xj   xi   xj  v   is the set of edges  given
 xi   xj    e  xi is called a parent of xj   and xj is called a child of xi   the set of
xi s parents is denoted pa xi    or pai   while the set of xi s children is denoted ch xi    or
chi   the family of xi includes xi and its parents  the moral graph of a directed graph
d is the undirected graph obtained by connecting the parents of all the nodes in d and
removing the arrows  a cycle cutset of an undirected graph a subset of nodes that  when
removed  yields a graph without cycles  a loop in a directed graph d is a subgraph of d
whose underlying graph is a cycle  a directed graph is acyclic if it has no directed loops  a
directed graph is singly connected  also called a poly tree   if its underlying undirected
graph has no cycles  otherwise  it is called multiply connected 
definition      loop cutset  a vertex v is a sink with respect to a loop l if the two
edges adjacent to v in l are directed into v  a vertex that is not a sink with respect to a
loop l is called an allowed vertex with respect to l  a loop cutset of a directed graph d
is a set of vertices that contains at least one allowed vertex with respect to each loop in d 
definition      belief networks  let x    x         xn   be a set of random variables
over multi valued domains d x          d xn    a belief network  bn   pearl        is
a pair  g  p   where g is a directed acyclic graph whose nodes are the variables x and
p    p  xi  pai   i           n  is the set of conditional probability tables  cpts  associated
with each xi   the bn represents a joint probability distribution having the product form 
p  x          xn    

n
y
i  

p  xi  pa xi   

an evidence e is an instantiated subset of variables e  x 
 

ficutset sampling for bayesian networks

the structure of the directed acyclic graph g reflects the dependencies between the
variables using d separation criterion  the parents of a variable xi together with its children
and parents of its children form a markov blanket  denoted markovi   of node xi   we
will use xmarkovi to denote x restricted to variables in markovi   we know that the node
xi is independent of the rest of the variables conditioned on its markov blanket  namely 
p  xi  xi     p  xi  xmarkovi   
the most common query over belief networks is belief updating which is the task of
computing the posterior distribution p  xi  e  given evidence e and a query variable xi 
x  reasoning in bayesian networks is np hard  cooper         finding approximate
posterior marginals with a fixed accuracy is also np hard  dagum   luby        abdelbar
  hedetniemi         when the network is a poly tree  belief updating and other inference
tasks can be accomplished in time linear in size of the input  in general  exact inference is
exponential in the induced width of the networks moral graph 
definition      induced width  the width of a node in an ordered undirected graph
is the number of the nodes neighbors that precede it in the ordering  the width of an
ordering d  denoted w d   is the maximum width over all nodes  the induced width
of an ordered graph  w  d   is the width of the ordered graph obtained by processing the
nodes from last to first as follows  when node x is processed  all its preceding neighbors are
connected  the resulting graph is called induced graph or triangulated graph 
the task of finding the minimal induced width of a graph  over all possible orderings  is
np complete  arnborg        
    reasoning in bayesian networks
belief propagation algorithm  which we introduce in section       below  performs belief
updating in singly connected bayesian networks in time linear in the size of the input
 pearl         in loopy networks  the two main approaches for belief updating are cutset
conditioning and tree clustering  these algorithms are often referred to as inference
algorithms  we will briefly describe the idea of clustering algorithms in section       and
the conditioning method in section       
      variable elimination and join tree clustering  jtc 
the join tree clustering approach  jtc  refers to a family of algorithms including jointree propagation  lauritzen   spiegelhalter        jensen et al         and bucket tree
elimination  dechter            a   the idea is to first obtain a tree decomposition of
the network into clusters of functions connected as a tree and then propagate messages
between the clusters in the tree  the tree decomposition is a singly connected undirected
graph whose nodes  also called clusters  contain subsets of variables and input functions
defined over those variables  the tree decomposition must contain each function once and
satisfy running intersection property  maier         for a unifying perspective of treedecomposition schemes see  zhang   poole        dechter      b  kask  dechter  larrosa 
  dechter        
given a tree decomposition of the network  the message propagation over this tree can
be synchronized  we select any one cluster as the root of the tree and propagate messages
 

fibidyuk   dechter

up and down the tree  a message from cluster vi to neighbor vj is a function over the
separator set vi  vj that is a marginalization of the product of all functions in vi and
all messages that vi received from its neighbors besides vj   assuming that the maximum
number of variables in a cluster is w     and maximum domain size is d  the time and
space required to process one cluster is o d w       since the maximum number of clusters
is bounded by  x    n   the complexity of variable elimination algorithms and cluster tree
propagation schemes is o n  d w       the parameter w  the maximum cluster size minus
   is called the tree width of the tree decomposition  the minimal tree width is identical to
the minimal induced width of a graph 
      iterative belief propagation  ibp 
belief propagation  bp  is an iterative message passing algorithm that performs exact inference for singly connected bayesian networks  pearl         in each iteration  every node xi
sends a j  xi   message to each child j and receives a j  xi   message from each child  the
message passing order can be organized so that it converges in two iterations  in essence the
algorithm is the same as the join tree clustering approach applied directly to the poly tree 
applied to bayesian networks with loops  the algorithm usually iterates longer  until it may
converge  and hence  is known as iterative belief propagation  ibp  or loopy belief propagation  ibp provides no guarantees on convergence or quality of approximate posterior
marginals but was shown to perform well in practice  rish  kask    dechter        murphy 
weiss    jordan         it is considered the best algorithm for inference in coding networks
 frey   mackay        kschischang   frey        where finding the most probable variable
values equals the decoding process  mceliece  mackay    cheng         algorithm ibp
requires linear space and usually converges fast if it converges  in our benchmarks  ibp
converged within    iterations or less  see section    
      cutset conditioning
when the tree width w of the bayesian network is too large and the requirements of inference
schemes such as bucket elimination and join tree clustering  jtc  exceed available memory 
we can switch to the alternative cutset conditioning schemes  pearl        peot   shachter 
      shachter  andersen    solovitz         the idea of cutset conditioning is to select
a subset of variables c  x e  the cutset  and obtain posterior marginals for any node
xi  x c  e using 
x
p  xi  e   
p  xi  c  e p  c e 
   
cd c 

eq     above implies that we can enumerate all instantiations over c  perform exact inference for each cutset instantiation c to obtain p  xi  c  e  and p  c e  and then sum up the
results  the total computation time is exponential in the size of the cutset because we have
to enumerate all instantiations of the cutset variables 
if c is a loop cutset  then  when the nodes in c are assigned  the bayesian network can
be transformed into an equivalent poly tree and p  xi  c  e  and p  c e  can be computed via
bp in time and space linear in the size of the network  for example  the subset  a  d  is a
loop cutset of the belief network shown in figure    left  with evidence e   e  on the right 
 

ficutset sampling for bayesian networks

a
b

c

d

f

e

g

d

a

a

b

c

d

f

e

g

d

figure    when nodes a and d in the loopy bayesian network  left  are instantiated  the
network can be transformed into an equivalent singly connected network  right  
in the transformation process  a replica of an observed node is created for each
child node 

figure   shows an equivalent singly connected network resulting from assigning values to a
and d 
it is well known that the minimum induced width w of the network is always less than
the size of the smallest loop cutset  bertele   brioschi        dechter         namely 
w       c  for any c  thus  inference approaches  e g   bucket elimination  are never
worse and often are better than cutset conditioning time wise  however  when w is too
large we must resort to cutset conditioning search in order to trade space for time  those
considerations yield a hybrid search and inference approach  since observed variables can
break down the dependencies in the network  a network with an observed subset of variables
c often can be transformed into an equivalent network with a smaller induced width  wc  
which we will term the adjusted induced width  hence  when any subset of variables c  x
is observed  complexity is bounded exponentially by the adjusted induced width of the
graph wc  
definition      adjusted induced width  given a graph g  x e   the adjusted
induced width of g relative to c  denoted wc   is its induced width once c is removed
from its moral graph 
definition      w cutset  given a graph g  x e   a subset of nodes c  x is a
w cutset of g if its adjusted induced width equals w 
if c is a w cutset  the quantities p  xi  c  e  and p  c e  can be computed in time and
space exponential in w  which can be much smaller than the tree width of the unconditioned
network  the resulting scheme requires memory exponential in w and time o d c  n d w     
where n is the size of the network and d is the maximum domain size  thus  the performance
can be tuned to the available system memory resource via the bounding parameter w 
given a constant w  finding a minimal w cutset  to minimize the cutset conditioning
time  is also a hard problem  several greedy heuristic approaches have been proposed by
geiger and fishelson        and by bidyuk and dechter               we elaborate more
in section     
 

fibidyuk   dechter

    gibbs sampling
since the complexity of inference algorithms is memory exponential in the networks induced
width  or tree width  and since resorting to the cutset conditioning scheme may take too
much time when the w cutset size is too large  we must often resort to approximation
methods  sampling methods for bayesian networks are commonly used approximation
techniques  this section provides background on gibbs sampling  a markov chain monte
carlo method  which is one of the most popular sampling schemes and is the focus of this
paper  although the method may be applied to the networks with continuous distributions 
we limit our attention in this paper to discrete random variables with finite domains 
      gibbs sampling for bayesian networks
ordered gibbs sampler
input  a belief network b over x  x         xn   and evidence e   xi   ei   xi  e  x  
output  a set of samples  x t     t       t  
   
   initialize  assign random value xi to each variable xi  x e from d xi    assign
evidence variables their observed values 
   generate samples 
for t     to t  generate a new sample x t   
 t 
for i     to n  compute a new value xi for variable xi  
 t 
 t 
 t 
compute distribution p  xi  xmarkovi   and sample xi  p  xi  xmarkovi   
 t 

set xi   xi  
end for i
end for t

figure    a gibbs sampling algorithm
given a bayesian network over the variables x    x         xn    and evidence e  gibbs
sampling  geman   geman        gilks et al         mackay        generates a set of
 t 
 t 
samples  x t    where each sample x t     x         xn   is an instantiation of all the variables 
 t 
the superscript t denotes a sample index and xi is the value of xi in sample t  the first
 t 
sample can be initialized at random  when generating a new sample from sample xi   a
 t 
new value for variable xi is sampled from probability distribution p  xi  xi    recall that
 t 

 t   

p  xi  xi     p  xi  x 

 t   

 t   

 t 

 t 

 t 

       xi    xi          xn    which we will denote as xi  p  xi  xi   
 t 

the next sample xi
is generated from the previous sample xi following one of two
schemes 
random scan gibbs sampling  given a sample x t  at iteration t  pick a variable
 t 
xi at random and sample a new value xi from the conditional distribution xi  p  xi  xi  
leaving other variables unchanged 
systematic scan  ordered  gibbs sampling  given a sample x t    sample a new
value for each variable in some order 
 t 

 t 

x   p  x   x    x         x t 
n  
 

ficutset sampling for bayesian networks

 t   

x   p  x   x 

 t 

  x         x t 
n  

   

 t   

xi  p  xi  x 

 t   

 t 

       xi    xi          x t 
n  

   

 t   

xn  p  xn  x 

 t   

  x 

 t   

       xn   
 t 

in bayesian networks  the conditional distribution p  xi  xi   is dependent only on the
 t 

 t 

assignment to the markov blanket of variable xi   thus  p  xi  xi   p  xi  xmarkovi   where
 t 

xmarkovi is the restriction of x t  to markovi   given a markov blanket of xi   the sampling
probability distribution is given explicitly by pearl        
y
 t 
 t 
p  xj  x t 
   
p  xi  xmarkovi     p  xi  x t 
 
paj  
pai
 j xj chj  

thus  generating a complete new sample can be done in o n  r  multiplication steps
where r is the maximum family size and n is the number of variables 
the sequence of samples x      x          can be viewed as a sequence of states in a markov
 t   
 t     t   t 
 t 
chain  the transition probability from state  x 
       xi    xi   xi          xn   to state
 t   

 t   

 t   

 t 

 t 

 t 

 x 
       xi    xi
  xi          xn   is defined by the sampling distribution p  xi  xi   
by construction  a markov chain induced by gibbs sampling has an invariant distribution
p  x e   however  since the values assigned by the gibbs sampler to variables in a sample
x t    depend on the assignment of values in the previous sample x t    it follows that the
sample x n  depends on the initial state x      the convergence of the markov chain is
defined by the rate at which the distance between the distribution p  x n   x      e  and the
stationary distribution p  x e   i e   variational distance  l   distance  or     converges to  
as a function of n  intuitively  it reflects how quickly the inital state x    can be forgotten 
the convergence is guaranteed as t   if markov chain is ergodic  pearl        gelfand
  smith        mackay         a markov chain with a finite number of states is ergodic if
it is aperiodic and irreducible  liu         a markov chain is aperiodic if it does not have
regular loops  a markov chain is irreducible if we can get from any state si to any state
sj  including si   with non zero probability in a finite number of steps  the irreducibility
guarantees that we will be able to visit  as number of samples increases  all statistically
important regions of the state space  in bayesian networks  the conditions are almost always
satisfied as long as all conditional probabilities are positive  tierney        
to ensure that the collected samples are drawn from distribution close to p  x e   a
burn in time may be allocated  namely  assuming that it takes  k samples for the
markov chain to get close to the stationary distribution  the first k samples may not be included into the computation of posterior marginals  however  determining k is hard  jones
  hobert         in general  the burn in is optional in the sense that the convergence of
the estimates to the correct posterior marginals does not depend on it  for completeness
sake  the algorithm is given in figure   
p
when convergence conditions are satisfied  an ergodic average ft  x    t  t f  xt   for
any function f  x  is guaranteed to converge to the expected value e f  x   as t increases 
 

fibidyuk   dechter

in other words   ft  x   e f  x       as t    for a finite state markov chain that is
irreducible and aperiodic  the following result applies  see liu        theorem         

t  ft  x   e f  x     n      f     
    
for any initial assignment to x      the variance term  f    is defined as follows 
 f         f    
where      var f  x   and   h  is the integrated autocorrelation time 
our focus is on computing the posterior marginals p  xi  e  for each xi x e  the
posterior marginals can be estimated using either a histogram estimator 
p  xi   xi  e   

t
 x
 xi  x t   
t

p  xi   xi  e   

t
 x
 t 
p  xi  xi  
t

or a mixture estimator 

    

t  

    

t  

the histogram estimator corresponds to counting samples where xi   xi   namely  xi  x t     
 t 
  if xi   xi and equals   otherwise  gelfand and smith        have pointed out that since
mixture estimator is based on estimating conditional expectation  its sampling variance
is smaller due to rao blackwell theorem  thus  mixture estimator should be preferred 
 t 
 t 
since p  xi  xi     p  xi  xmarkovi    the mixture estimator is simply an average of conditional
probabilities 
t
 x
 t 
p  xi  e   
p  xi  xmarkovi  
    
t
t  

as mentioned above  when the markov chain is ergodic  p  xi  e  will converge to the exact
posterior marginal p  xi  e  as the number of samples increases  it was shown by roberts and
sahu        that random scan gibbs sampler can be expected to converge faster than the
systematic scan gibbs sampler  ultimately  the convergence rate of gibbs sampler depends
on the correlation between two consecutive samples  liu        schervish   carlin       
liu et al          we review this subject in the next section 
    variance reduction schemes
the convergence rate of the gibbs sampler depends on the strength of the correlations
between the samples  which are also the states of the markov chain   the term correlation
is used here to mean that the samples are dependent  as mentioned earlier  in the case of a
finite state irreducible and aperiodic markov chain  the convergence rate can be expressed
through maximal correlation between states x    and x n   see liu        ch       in practice 
the convergence rate can be analyzed through covariance cov f  x t     f  x t        where f is
some function  also called auto covariance 
the convergence of the estimates to the exact values depends on both the convergence
rate of the markov chain to the stationary distribution and the variance of the estimator 
  

ficutset sampling for bayesian networks

both of these factors contribute to the value of the term  f    in eq       the two main
approaches that allow to reduce correlation between samples and reduce sampling variance
of the estimates are blocking  grouping variables together and sampling simultaneously 
and collapsing  integrating out some of the random variables and sampling a subset   also
known as rao blackwellisation 
given a joint probability distribution over three random variables x  y   and z  we can
depict the essence of those three sampling schemes as follows 
   standard gibbs 
x t     p  x y  t    z  t   
y

 t   

z

 t   

 t   

 p  y  x

 t   

 p  z x

    

 t 

 z  

 y

    

 t   

 

    

   collapsed  variable z is integrated out  
x t     p  x y  t   
y

 t   

 t   

 p  y  x

    
 

    

   blocking by grouping x and y together 
 x t      y  t       p  x  y  z  t   
z

 t   

 t   

 p  z x

 y

    
 t   

 

    

the blocking reduces the correlation between samples by grouping highly correlated
variables into blocks  in collapsing  the highly correlated variables are marginalized out 
which also results in the smoothing of the sampling distributions of the remaining variables
 p  y  x  is smoother than p  y  x  z    both approaches lead to reduction of the sampling
variance of the estimates  speeding up their convergence to the exact values 
generally  blocking gibbs sampling is expected to converge faster than standard gibbs
sampler  liu et al         roberts   sahu         variations on this scheme have been
investigated by jensen et al         and kjaerulff         given the same number of samples 
the estimate resulting from collapsed gibbs sampler is expected to have lower variance
 converge faster  than the estimate obtained from blocking gibbs sampler  liu et al         
thus  collapsing is preferred to blocking  the analysis of the collapsed gibbs sampler can
be found in escobar         maceachern         and liu              
the caveat in the utilization of the collapsed gibbs sampler is that computation of the
probabilities p  x y  and p  y  x  must be efficient time wise  in case of bayesian networks 
the task of integrating out some variables is equivalent to posterior belief updating where
evidence variables and sampling variables are observed  its time complexity is therefore
exponential in the adjusted induced width  namely  in the effective width of the network
after some dependencies are broken by instantiated variables  evidence and sampled  
  

fibidyuk   dechter

    importance sampling
since sampling from the target distribution is hard  different sampling methods explore
different trade offs in generating samples and obtaining estimates  as we already discussed 
gibbs sampling generates dependent samples but guarantees convergence of the sampling
distribution to the target distribution  alternative approach  called importance sampling 
is to generate samples from a sampling distribution q x  that is different from p  x e  and
include the weight w t    p  x t   e  q x t    of each sample x t  in the computation of the
estimates as follows 
t
 x
f  xt  w t 
ft  x   
t

    

t  

the convergence of ft  x  to e f  x   is guaranteed as long as the condition p  x e    
   q x       holds  the convergence speed depends on the distance between q x  and
p  x e  
one of the simplest forms of importance sampling for bayesian networks is likelihood
weighting  fung   chang        shachter   peot        which processes variables in topological order  sampling root variables from their priors and the remaining variables from
conditional distribution p  xi  pai   defined by their conditional probability table  the evidence variables are assigned their observed values   its sampling distribution is close to the
prior and  as a result  it usually converges slowly when the evidence is concentrated around
the leaf nodes  nodes without children  and when the probability of evidence is small  adaptive  also called dynamic  importance sampling is a method that attempts to speed up the
convergence by updating the sampling distribution based on the weight of previously generated samples  adaptive importance sampling methods include self importance sampling 
heuristic importance sampling  shachter   peot         and  more recently  ais bn  cheng
  druzdzel        and epis bn  yuan   druzdzel         in the empirical section  we
compare the performance of the proposed cutset sampling algorithm with ais bn which
is considered a state of the art importance sampling algorithm to date  although epis bn
was shown to perform better in some networks  and  hence  describe ais bn here in more
detail 
ais bn algorithm is based on the observation that if we could sample each node in
topological order from distribution p  xi  pai   e   then the resulting sample would be drawn
from the target distribution p  x e   since this distribution is unknown for any variable
that has observed descendants  ais bn initializes the sampling distributions p    xi  pai   e 
equal to either p  xi  pai   or a uniform distribution and then updates each distribution
p k  xi  pai   e  every l samples so that the next sampling distribution p k    xi  pai   e  will be
closer to p  xi  pai   e  than p k  xi  pai   e  as follows 
p k    xi  pai   e    p k  xi  pai   e     k    p   xi  pai   e   p k  xi  pai   e  
where  k  is a positive function that determines the learning rate and p   xi  pai   e  is an
estimate of p  xi  pai   e  based on the last l samples 
  

ficutset sampling for bayesian networks

   cutset sampling
this section presents the cutset sampling scheme  as we discussed above  sampling on a
cutset is guaranteed to be more statistically efficient  cutset sampling scheme is a computationally efficient way of sampling from a collapsed variable subset c  x  tying the
complexity of sample generation to the structure of the bayesian network 
    cutset sampling algorithm
the cutset sampling scheme partitions the variable set x into two subsets c and x c 
the objective is to generate samples from space c  c    c         cm   where each sample c t 
is an instantiation of all the variables in c  following the gibbs sampling principles  we
 t 
wish to generate a new sample c t  by sampling a value ci from the probability distribution
 t 
 t     t   
 t     t 
 t 
p  ci  ci     p  ci  c 
  c 
       ci    ci          cm    we will use left arrow to denote that
 t 

value ci is drawn from distribution p  ci  ci   

 t 

ci  p  ci  ci   e 

    
 t 

if we can compute the probability distribution p  ci  ci   e  efficiently for each sampling
variable ci  c  then we can generate samples efficiently  the relevant conditional distributions can be computed by exact inference whose complexity is tied to the network structure  we denote by jt c b  xi   e  a generic algorithm in the class of variable elimination
or join tree clustering algorithms which  given a belief network b and evidence e  outputs
the posterior probabilities p  xi  e  for variable xi  x  lauritzen   spiegelhalter       
jensen et al         dechter      a   when the networks identity is clear  we will use the
notation jt c xi   e  
cutset sampling
input  a belief network b  a cutset c    c         cm    evidence e 
output  a set of samples ct   t       t  
   initialize  assign random value c i to each ci  c and assign e 
   generate samples 
for t     to t    generate a new sample c t    as follows 
 t 
for i     to m  compute new value ci for variable ci as follows 
 t 
a  compute jt c ci   ci   e  
 t 

 t 

b  compute p  ci  ci   e    p  ci   ci   e  
c  sample 
 t   
 t 
ci
 p  ci  ci   e 
end for i
end for t

    

figure    w cutset sampling algorithm
therefore  for each sampling variable ci and for each value ci  d ci    we can compute
 t 
 t 
 t 
 t 
p  ci   ci   e  via jt c ci   ci   e  and obtain p  ci  ci   e  via normalization  p  ci  ci   e   
 t 

p  ci   ci   e  

  

fibidyuk   dechter

cutset sampling algorithm that uses systematic scan gibbs sampler is given in figure   
clearly  it can be adapted to be used with the random scan gibbs sampler as well  steps
 a   c  generate sample  t      from sample  t   for every variable ci  c in sequence  the
 t 
main computation is in step  a   where the distribution p  ci   ci   e  over ci is generated 
this requires executing jt c for every value ci  d ci    separately  in step  b   the
conditional distribution is derived by normalization  finally  step  c  samples a new value
 t 
from the obtained distribution  note that we only use p  ci  ci   e  as a short hand notation
 t   

 t   

 t 

 t 

for p  ci  c 
       ci    ci          ck   e   namely  when we sample a new value for variable
ci   the values of variables c  through ci  have already been updated 
we will next demonstrate the process using the special case of loop cutset  see definition      
example     consider the belief network previously shown in figure   with the observed node
e   e and loop cutset  a  d   we begin the sampling process by initializing sampling variables to
a    and d      next  we compute new sample values a      d    as follows 
p  a d      e 

 

pjt c  a  c      e 
   

    

   

a
p  d a      e 


 

p  a d   e 
pjt c  d  a      e 

    
    

d   



p  d a      e 

    

the process above corresponds to two iterations of the inner loop in figure    eq             where
we sample a new value for variable a  correspond to steps  a   c  of the first iteration  in the second
iteration  eq            we sample a new value for variable d  since the conditioned network is a
poly tree  figure    right   computing probabilities pjt c  a d t    e  and pjt c  d a t      e  via jt c
reduces to pearls belief propagation algorithm and the distributions can be computed in linear time 

    estimating posterior marginals
once a set of samples over a subset of variables c is generated  we can estimate the posterior
marginals of any variable in the network using mixture estimator  for sampling variables 
the estimator takes the form similar to eq      
t
 x
 t 
p  ci  e   
p  ci  ci   e 
t

    

t  

for variables in x c  e  the posterior marginal estimator is 
t
 x
p  xi  e   
p  xi  c t    e 
t

    

t  

we can use jt c xi   c t    e  to obtain the distribution p  xi  c t    e  over the input bayesian
network conditioned on c t  and e as shown before 
 t 
if we maintain a running sum of the computed distributions p  ci  ci   e  and p  xi  c t    e 
during sample generation  the sums in the right hand side of eq           will be readily
available  as we noted before  the estimators p  ci  e  and p  xi  e  are guaranteed to converge to their corresponding exact posterior marginals as t increases as long as the markov
  

ficutset sampling for bayesian networks

chain over the cutset c is ergodic  while for the cutset variables the estimator is a simple
ergodic average  for xi  x c  e the convergence can also be derived directly from first
principles 
theorem     given a bayesian network b over x  evidence variables e  x  and cutset
c  x e  and given a set of t samples c      c           c t   obtained via gibbs sampling from
p  c e   and assuming the markov chain corresponding to sampling from c is ergodic  then
for any xi  x c  e assuming p  xi  e  is defined by eq       p  xi  e   p  xi  e  as
t  
proof  by definition 
t
 x
p  xi  c t    e 
p  xi  e   
t

    

t  

instead of summing over samples  we can rewrite the expression above to sum over all
possible tuples c  d c  and group together the samples corresponding to the same tuple
instancep
c  let q c  denote the number of times a tuple c   c occurs in the set of samples
so that cd c  q c    t   it is easy to see that 
p  xi  e   

the fraction

q c 
t

x

cd c 

p  xi  c  e 

q c 
t

    

is a histogram estimator for the posterior marginal p  c e   thus  we get 
x
p  xi  e   
p  xi  c  e p  c e 
    
cd c 

since the markov chain formed by samples from c is ergodic  p  c e   p  c e  as t  
and therefore 
x
p  xi  e  
p  xi  c  e p  c e    p  xi  e 
cd c 



    complexity
the time and space complexity of generating samples and estimating the posterior marginals
via cutset sampling is dominated by the complexity of jt c in line  a  of the algorithm
 figure     only linear amount of additional memory is required to maintain the running
 t 
sums of p  ci  ci   e  and p  xi  c t    e  used in the posterior marginal estimators 
      sample generation complexity
clearly  when jt c is applied to the network b conditioned on all the cutset variables c
and evidence variables e  its complexity is time and space exponential in the induced width
w of the conditioned network  it is o n  d w      when c is a w cutset  see definition      
  

fibidyuk   dechter

using the notion of a w cutset  we can balance sampling and exact inference  at one end
of the spectrum we have plain gibbs sampling where sample generation is fast  requiring
linear space  but may have high variance  at the other end  we have exact algorithm
requiring time and space exponential in the induced width of the moral graph  in between
these two extremes  we can control the time and space complexity using w as follows 
theorem      complexity of sample generation  given a network b over x  evidence e  and a w cutset c  the complexity of generating a new sample is time and space
o  c   n  d w      where d bounds the variables domain size and n    x  
proof  if c is a w cutset and d is the maximum domain size  then the complexity of
 t 
computing joint probability p  ci   ci   e  over the conditioned network is o n  d w      
since this operation must be repeated for each ci  d ci    the complexity of processing one
 t 
variable  computing distribution p  ci  ci   e   is o n  d  d w        o n  d w       finally 
since ordered gibbs sampling requires sampling each variable in the cutset  generating one
sample is o  c   n  d w       

      complexity of estimator computation
the posterior marginals for any cutset variable ci  c are easily obtained at the end of
sampling process without incurring additional computation overhead  as mentioned earlier 
 t 
we only need to maintain a running sum of probabilities p  ci  ci   e  for each ci  d ci   
estimating p  xi  e   xi  x c  e  using eq      requires computing p  xi  c t    e  once a
sample c t  is generated  in summary 
theorem      computing marginals  given a w cutset c  the complexity of computing posteriors for all variables xi  x e using t samples over the cutset variables is
o t    c    d   n  d w      
proof  as we showed in theorem      the complexity of generating one sample is o  c  
n  d w       once a sample c t  is generated  the computation of the posterior marginals
for the remaining variables requires computing p  xi  c t    e  via jt c xi   c t    e  which is
o n  d w       the combined computation time for one sample is o  c   n  d w     
n  d w        o   c    d   n  d w       repeating the computation for t samples  yields
o t    c    d   n  d w       
note that the space complexity of w cutset sampling is bounded by o n  d w      

      complexity of loop cutset
when the cutset c is a loop cutset  algorithm jt c reduces to belief propagation  pearl 
 t 
      that computes the joint distribution p  ci   ci   e  in linear time  we will refer to the
special case as loop cutset sampling and to the general as w cutset sampling 
a loop cutset is also a w cutset where w equals the maximum number of unobserved
parents  upper bounded by the maximum indegree of a node   however  since processing
poly trees is linear even for large w  the induced width does not capture its complexity
  

ficutset sampling for bayesian networks

properly  the notion of loop cutset could be better captured via the hyperwidth of the
network  gottlob  leone    scarello        kask et al          the hyperwidth of a polytree is   and therefore  a loop cutset can be defined as a   hypercutset  alternatively  we
can express the complexity via the networks input size m which captures the total size of
conditional probability tables to be processed as follows 
theorem      complexity of loop cutset sample generation  if c is a loop cutset 
the complexity of generating each sample is o  c   d  m   where m is the size of the input
network 
proof  when a loop cutset of a network is instantiated  belief propagation  bp  can
 t 
compute the joint probability p  ci   ci   e  in linear time o m    pearl        yielding total
time and space of o  c   d  m   for each sample  

    optimizing cutset sampling performance
our analysis of the complexity of generating samples  theorem      is overly pessimistic in
assuming that the computation of the sampling distribution for each variable in the cutset
is independent  while all variables may change a value when moving from one sample to
the next  the change occurs one variable at a time in some sequence so that much of the
computation can be retained when moving from one variable to the next  
we will now show that sampling all the cutset variables can be done more efficiently
reducing the factor of n   c  in theorem     to  n    c     where  bounds the number of
clusters in the tree decomposition used by jt c that contains any node ci  c  we assume
that we can control the order by which cutset variables are sampled 

x 
x x y 

y 

y 

yn  

yn  

x 

x 

xn  

xn

x x y 

x x y 

xn  xnyn  

figure    a bayesian network  top  and corresponding cluster tree  bottom  
consider a simple network with variables x  x        xn    y   y         yn    and cpts
p  xi    xi   yi   and p  yi    xi   defined for every i as shown in figure    top  the join tree
of this network is a chain of cliques of size   given in figure    bottom  since y is a loopcutset  we will sample variables in y   lets assume that we use the ordering y    y       yn  to
generate a sample  given the current sample  we are ready to generate the next sample by
applying jt c  or bucket elimination  to the network whose cutset variables are assigned 
  

fibidyuk   dechter

this makes the network effectively singly connected and leaves only   actual variables in
each cluster  the algorithm sends a message from the cluster containing xn towards the
cluster containing x    when cluster  x    x    y    gets the relevant message from cluster
 x    x    y    we can sample y    this can be accomplished by d linear computations in clique
 x    x    y    for each yi  d yi   yielding the desired distribution p  y       we can multiply
all functions and incoming messages in this cluster  sum out x  and x  and normalize   if
the cutset is a w cutset  each computation in a single clique is o d w      
once we have p  y      y  is sampled and assigned a new value  y    cluster  x    x    y   
y    then sends a message to cluster  x    x    y    which now has all the information necessary
to compute p  y      in o d w       once p  y      is available  a new value y    y  is sampled 
the cluster than computes and sends a message to cluster  x    x    y     and so on  at the
end  we obtain a full sample via two message passes over the conditioned network having
computation complexity of o n  d w       this example can be generalized as follows 
theorem     given a bayesian network having n variables  a w cutset c  a tree decomposition
tr   and given a sample c         c c    a new sample can be generated in o  n    c      d w     
where  is the maximum number of clusters containing any variable ci  c 
proof  given w cutset c  by definition  there exists a tree decomposition tr of the network
 that includes the cutset variables  such that when the cutset variables c are removed  the
number of variables remaining in each cluster of tr is bounded by w      lets impose
directionality on tr starting at an arbitrary cluster that we call r as shown in figure    let
tci denote the connected subtree of tr whose clusters include ci   in figure    for clarity 
we collapse the subtree over ci into a single node  we will assume that cutset nodes are
sampled in depth first traversal order dictated by the cluster tree rooted in r 

tc  r
tc 
tck
tc 

tc 

tc 

tc 
figure    a cluster tree rooted in cluster r where a subtree over each cutset node ci is
collapsed into a single node marked tci  

  

ficutset sampling for bayesian networks

given a sample c t    jt c will send messages from leaves of tr towards the root cluster 
we can assume without loss of generality that r contains cutset node c  which is the first to
be sampled in c t      jtc will now pass messages from root down only to clusters restricted
 t 
to tc   note that r  tc     based on these messages p  c    c    c    can be computed
in o d w       we will repeat this computation for each other value of c  involving only
clusters in tc  and obtain the distribution p  c     in o d w      and sample a new value
for c    thus  if c  appears in  clusters  the number of message passing computations
 after the initial o n   pass  is o   and we can generate the first distribution p  c     in
o   d w      
the next node in the depth first traversal order is tc  and thus  the second variable to
be sampled is c    the distance between variables c  and c    denoted dist      is the shortest
path along tr from a cluster that contains c  to a cluster that contains c    we apply jtcs
mesage passing along that path only which will take at most o dist     d w       then  to
obtain the conditional distribution p  c      we will recompute messages in the subtree of
tc  for each value c   d c    in o   d w       we continue the computation in a similar
manner for other cutset nodes 
if jt c traverses the tree in the depth first order  it only needs to pass messages along
p c 
each edge twice  see figure     thus  the sum of all distances traveled is i   disti i   
o n    what may be repeated is the computation for each value of the sampled variable 
this  however  can be accomplished via message passing restricted to individual variables
subtrees and is bounded by its   we can conclude that a new full sample can be generated
in o  n    c      d w       
it is worthwhile noting that the complexity of generating a sample can be further reduced
by a factor of d  d    which amounts to a factor of   when d      by noticing that whenever
 t   
 t     t 
 t 
we move from variable ci to ci     the joint probability p  c 
       ci
  ci          ck   is
already available from the previous round and should not be recomputed  we only need
 t   
 t   
 t 
 t 
to compute p  c 
       ci
  ci          ck   for ci      ci     buffering the last computed
joint probability  we only need to apply jt c algorithm d    times  therefore  the total
complexity of generating a new sample is o  n    c       d      d w      
example     figure   demonstrates the application of the enhancements discussed  it
depicts the moral graph  a   already triangulated  and the corresponding join tree  b  for the
bayesian network in figure    with evidence variable e removed  variables b and d form
a   cutset  the join tree of the network with cutset and evidence variables removed is shown
in figure    c   since removing d and e from cluster df e leaves only one variable  f  
we combine clusters bdf and df e into one cluster  f g  assuming that cutset variables
have domains of size    we can initialize b   b  and d   d   
selecting cluster ac as the root of the tree  jt c first propagates messages from leaves
to the root as shown in figure    c  and then computes p  b    d    e  in cluster ac  next  we
set b   b    updating all functions containing variable b  and propagating messages through
the subtree of b consisting of clusters ac and cf  figure    d    we obtain p  b    d    e  
normalizing the two joint probabilities  we obtain distribution p  b d    e  and sample a new
value of b  assume we sampled value b   
  

fibidyuk   dechter

abc
p b a  p c a  
p a 

a

ac
p b  a  p c a  
p a 

ac
p b  a  p c a  
p a 

ac
p b  a  p c a  
p a 

ac
p b  a  p c a  
p a 

cf
p f b  c  p d  b  

cf
p f b  c  p d  b  

cf
p f b  c  p d  b  

cf
p f b  c  p d  b  

fg
p e d  f  p g d  f 

fg
p e d  f  p g d  f 

fg
p e d  f  p g d  f 

fg
p e d  f  p g d  f 

b
c

bcf
p f b c 

f
g

dfg
p d b   p g d f 

d

e

dfe
p e d f 

b b   d d   e e
 a 

 b 

 c 

b b 

d d 

d d 

 d 

 e 

 f 

figure    a join tree of width    b  for a moral graph  a  is transformed into a join tree of
width    c  when evidence variable e and cutset variables b and d are instantiated  in the process  clusters bdf and bcf are merged into cluster cf    the
clusters contain variables and functions from the original network  the cutset
nodes have domains of size    d b     b    b     d d     d    d     starting with a
sample  b    d     messages are propagated in  c   e  to first  sample a new value
of variable b  d  and then variable d  e   then messages are propagated up the
tree to compute posterior marginals p   b    d    e  for the rest of the variables  f  

next  we need to compute p  d b    e  to sample a new value for variable d  the joint
probability p  d    b    e  is readily available since it was computed for sampling a new value of
b  thus  we set d   d  and compute the second probability p  d    b    e  updating functions
in clusters cf and f g and sending an updated message from cf to f g  figure    e   
we obtain distribution p  d b    e  by normalizing the joint probabilities and sample a new
value d  for d  since the value has changed from latest computation  we update again
functions in the clusters cf and f g and propagate updated messages in the subtree cd
 send message from cf to f g  
in order to obtain the distributions p   b    d    e  for the remaining variables a  c  f  
and g  we only need to send updated messages up the join tree  from f g to cf and then
from cf to ac as shown in figure    f    the last step also serves as the initialization
step for the next sample generation 
in this example the performance of cutset sampling is significantly better than its worst
case  we have sent a total of   messages to generate a new sample while the worst case
suggests at least n   c   d                messages  here  n equals the number of clusters  
  

ficutset sampling for bayesian networks

    on finding a w cutset
clearly  w cutset sampling will be effective only when the w cutset is small  this calls for
the task of finding a minimum size w cutset  the problem is np hard  yet  several heuristic
algorithms have been proposed  we next briefly survey some of those proposals 
larossa and dechter        obtain w cutset when processing variables in the elimination
order  the next node to be eliminated  selected using some triangulation heuristics  is added
to the cutset if its current induced width  or degree  is greater than w  geiger and fishelson
       agument this idea with various heuristics 
bidyuk and dechter        select the variables to be included in the cutset using greedy
heuristics based on the nodes basic graph properties  such as the degree of a node   one
scheme starts from an empty w cutset and then heuristically adds nodes to the cutset until
a tree decomposition of width  w can be obtained  the other scheme starts from a set
c   x e containing all nodes in the network as a cutset and then removes nodes from the
set in some order  the algorithm stops when removing the next node would result in a tree
decomposition of width   w 
alternatively  bidyuk and dechter        proposed to first obtain a tree decomposition
of the network and then find the minimal w cutset of the tree decomposition  also an nphard problem  via a well known greedy algorithm used for set cover problem  this approach
is shown to yield a smaller cutset than previously proposed heuristics and is used for finding
w cutset in our experiments  section      with a modification that a tree decomposition is
re computed each time a node is removed from the tree and added to the w cutset 

   experiments
in this section  we present empirical studies of cutset sampling algorithms for several classes
of problems  we use the mean square error of the posterior marginals estimates as a
measure of accuracy  we compare with traditional gibbs sampling  likelihood weighting
 fung   chang        shachter   peot         and the state of the art ais bn adaptive
importance sampling algorithm  cheng   druzdzel         we implemented ais bn using
the parameters specified by cheng and druzdzel         by using our own implementation 
we made sure that all sampling algorithms used the same data access routines and the
same error measures providing a uniform framework for comparing their performance  for
reference we also report the performance of iterative belief propagation  ibp  algorithm 
    methodology
in this section we detail describe methodology used and the implementation decisions made
that apply to the collection of the empirical results 
      sampling methodology
in all sampling algorithms we restarted the markov chain every t samples  the samples
from each chain  batch  m are averaged separately 
pm  xi  e   

t
 x
p  xi  c t    e 
t
t  

  

fibidyuk   dechter

the final estimate is obtained as a sample average over m chains 
m
  x
pm  xi  e 
p  xi  e   
m
m  

restarting a markov chain is known to improve the sampling convergence rate  a single
chain can become stuck generating samples from a single high probability region without
ever exploring large number of other high probability tuples  by restarting a markov chain
at a different random point  a sampling algorithm can achieve a better coverage of the
sampling space  in our experiments  we did not observe any significant difference in the
estimates obtained from a single chain of size m  t or m chains of size t and therefore 
we only choose to report the results for multiple markov chains  however  we rely on the
independence of random values pm  xi  e  to estimate     confidence interval for p  xi  e  
in our implementation of gibbs sampling schemes  we use zero burn in time  see
section         as we mentioned earlier  the idea of burn in time is to throw away the
first k samples to ensure that the remaining samples are drawn from distribution close
to target distribution p  x e   while conservative methods for estimating k through drift
and minorization conditions were proposed by rosenthal        and roberts and tweedie
              the required analysis is beyond the scope of this paper  we consider our
comparison between gibbs sampling and cutset sampling  which is the objective  fair in the
sense that both schemes use k    further  our experimental results showed no positive
indication that burn in time would be beneficial  in practice  burn in is the pre processing
time used by the algorithm to find the high probability regions in the distribution p  c e 
in case it initially spends disproportionally large period of time in low probability regions 
discarding a large number of low probability tuples obtained initially  the frequency of the
remaining high probability tuples is automatically adjusted to better reflect their weight 
cpcs   b  n       e      w    

cpcs   b  n       e       lc      w    

lcs
   

    e   

   

  unique samples

    e   

    e   

mse

    e   
    e   
    e   
    e   

lcs

   
   
   
   
   
   

    e   

 

    e   

 

    

    

    

    

     

 

    

    

    

    

     

  samples

  samples

figure    comparing loop cutset sampling mse vs  number of samples  left  and and number of unique samples vs  number of samples  right  in cpcs   b  results are
averaged over    instances with different observations 

in our benchmarks  we observed that both full gibbs sampling and cutset sampling
were able to find high probability tuples fast relative to the number of samples generated 
for example  in one of the benchmarks  cpcs   b  the rate of generating unique samples 
  

ficutset sampling for bayesian networks

namely  the ratio of cutset instances that have not been seen to the number of samples 
decreases over time  specifically  loop cutset sampling generates     unique tuples after the
first      samples  an additional     unique tuples while generating the next      samples 
and then the rate of generating unique tuples slows to    per      samples in the range
from      to       samples as shown in figure    right  that means that after the first
few hundred samples  the algorithm spends most of the time revisiting high probability
tuples  in other benchmarks  the number of unique tuple instances generated increases
linearly  as in cpcs    and  thus  the tuples appear to be distributed nearly uniformly  in
this case  there is no need for burn in because there are no strongly expressed heavy weight
tuples  instead of using burn in times  we sample initial variable values from the posterior
marginal estimates generated by ibp in all of our experiments  our sampling time includes
the pre processing time of ibp 
all experiments were performed on     ghz cpu 
      measures of performance
for each problem instance defined by a bayesian network b having variables x    x         xn  
and evidence e  x  we derived the exact posterior marginals p  xi  e  using bucket tree
elimination  dechter            a  and computed the mean square error  mse  of the
approximate posterior marginals p  xi  e  for each algorithm where mse is defined by 
x x
 
 p  xi  e   p  xi  e   
m se   p
 d x
  
i
xi x e
xi x e d xi  

while the mean square error is our primary accuracy measure  the results are consistent
across other well known measures such as average absolute error  kl distance  and squared
hellingers distance which we show only for loop cutset sampling  the absolute error  is
averaged over all values of all unobserved variables 
x x
 
  p
 p  xi  e   p  xi  e  
xi x e  d xi   
xi x e d xi  

kl distance dk between the distribution p  xi  e  and the estimator p  xi  e  is defined as
follows 
x
p  xi  e 
dk  p  xi  e   p  xi  e    
p  xi  e  lg
p  xi  e 
d x  
i

for each benchmark instance  we compute the kl distance for each variable xi  x e and
then average the results 
x
 
dk  p  xi  e   p  xi  e  
dk  p  p    
 x e 
xi x e

the squared hellingers distance dh between the distribution p  xi  e  and the estimator
p  xi  e  is obtained as 
q
x p
dh  p  xi  e   p  xi  e    
  p  xi  e   p  xi  e   
d xi  

  

fibidyuk   dechter

the average squared hellingers distance for a benchmark instance is the average of the
distances between posterior distributions of one variable 
dh  p  p    

 
 x e 

x

xi x e

dh  p  xi  e   p  xi  e  

the average errors for different network instances are then averaged over all instances of
the given network  typically     instances  
we also report the confidence interval for the estimate p  xi  e  using an approach similar
to the well known batch means method  billingsley        geyer        steiger   wilson 
       since chains are restarted independently  the estimates pm  xi  e  are independent 
thus  the confidence interval can be obtained by measuring the variance in the estimators
p  xi  e   we report results in section     
    benchmarks
we experimented with four classes of networks 
cpcs  we considered four cpcs networks derived from the computer based patient
case simulation system  parker   miller        pradhan  provan  middleton    henrion 
       cpcs network representation is based on internist    miller  pople    myers 
      and quick medical reference  qmr   miller  masarie    myers        expert systems  the nodes in cpcs networks correspond to diseases and findings and conditional
probabilities describe their correlations  the cpcs   network consists of n     nodes and
has a relatively large loop cutset of size  lc            of the nodes   its induced width
is     cpcs    network consists of n      nodes  its induced width is w     it has a
small loop cutset of size  lc    but with a relatively large corresponding adjusted induced
width wlc     the cpcs   b is a larger cpcs network with     nodes  adjusted induced
width of     and loop cutset  lc      exact inference on cpcs   b averaged     minutes 
the largest network  cpcs   b  consisted of     nodes with induced width w     and
loop cutset of size     the exact inference time for cpcs   b is about    minutes 
hailfinder network  it is a small network with only    nodes  the exact inference in
hailfinder network is easy since its loop cutset size is only    yet  this network has some
zero probabilities and  therefore  is a good benchmark for demonstrating the convergence
of cutset sampling in contrast to gibbs sampling 
random networks  we experimented with several classes of random networks  random networks    layer networks  and grid networks  the random networks were generated
with n      binary nodes  domains of size     the first     nodes   x         x       were
designated as root nodes  each non root node xi   i        was assigned   parents selected
randomly from the list of predecessors  x         xi     we will refer to this class of random
networks as multi partite random networks to distinguish from bi partite    layer  random
networks  the random   layer networks were generated with    root nodes  first layer 
and     leaf nodes  second layer   yielding a total of     nodes  a sample   layer random
network is shown in figure    left  each non root node  second layer  was assigned    
parents selected at random among the root nodes  all nodes were assigned a domain of size
   d xi      x i   x i   
  

ficutset sampling for bayesian networks

figure    sample random networks    layer  left   grid  center   coding  right  

for both   layer and multi partite random networks  the root nodes were assigned uniform priors while conditional probabilities were chosen randomly  namely  each value
p  x i  pai   was drawn from uniform distribution over interval        and used to compute
the complementary probability value p  x i  pai        p  x i  pai   
the directed grid networks  as opposed to grid shaped undirected markov random
fields  of size   x   with     nodes were also constructed with uniform priors  on the single
root node  and random conditional probability tables  as described above   a sample grid
network is shown in figure    center  those networks had an average induced width of
size     exact inference using bucket elimination required about    minutes   they had the
most regular structure of all and the largest loop cutset containing nearly a half of all the
unobserved nodes 
coding networks  we experimented with coding networks with    code bits and   
parity check bits  the parity check matrix was randomized  each parity check bit had three
parents  a sample coding network with   code bits    parity checking bits  and total of  
transmitted bits is shown in figure    center  the total number of variables in each network
in our experiments was         code bits     parity check bits  and   transmitted bit for
each code or parity check bit   an average loop cutset size was    and induced width was
    the markov chain produced by gibbs sampling over the whole coding network is not
ergodic due to the deterministic parity check function  as a result  gibbs sampling does
not converge  however  the markov chain corresponding to sampling the subspace of coding
bits only is ergodic and  thus  all of the cutset sampling schemes have converged as we will
show in the next two sections 
in all networks  except coding and grid networks  evidence nodes were selected at random
among the leaf nodes  nodes without children   since a grid network has only one leaf
node  the evidence in the grid networks was selected at random among all nodes  for each
benchmark  we report on the chart title the number of nodes in the network n   average
number of evidence nodes  e   size of loop cutset  lc   and average induced width of the
input instance denoted w to distinguish from the induced width w of the network adjusted
for its w cutset 
  

fibidyuk   dechter

    results for loop cutset sampling
in this section we compare loop cutset sampling with pure gibbs sampling  likelihood
weighting  ais bn  and ibp  in all benchmarks  the cutset was selected so that the evidence
and sampling nodes together constitute a loop cutset of the network using the algorithm
proposed by becker et al          we show the accuracy of gibbs and loop cutset sampling
as a function of the number of samples and time 
cpcs networks  the results are summarized in figures       the loop cutset curve
in each chart is denoted lcs  for loop cutset sampling   the induced width of the network
wlc when loop cutset nodes are observed is specified in the caption  it is identical to the
largest family size in the poly tree generated when cutset variables are removed  we plot
the time on the x axis and the accuracy  mse  on the y axis  in the cpcs networks  ibp
always converged and converged fast  within seconds   consequently  ibp curve is always
a straight horizontal line as the results do not change after the convergence is achieved 
the curves corresponding to gibbs sampling  loop cutset sampling  likelihood weighting 
and ais bn demonstrate the convergence of the sampling schemes with time  in the three
cpcs networks loop cutset sampling converges much faster than gibbs sampling  the only
exception is cpcs   b  figure     right  where the induced width of the conditioned singlyconnected network remains high  wlc       due to large family sizes and thus  loop cutset
sampling generates samples very slowly    samples second  compared to gibbs sampling
     samples second   since computing sampling distribution is exponential in w  sampling
a single variable is o        all variables have domains of size     as a result  although loopcutset sampling shows a significant reduction in mse as a function of the number of samples
 figure     left   it is not enough to compensate for the two orders of magnitude difference
in the loop cutset rate of sample generation  for cpcs    figure     cpcs     figure     
and cpcs   b  figure     loop cutset sampling achieves greater accuracy than ibp within
   seconds or less 
in comparison with importance sampling schemes  we observe that the ais bn algorithm consistently outperforms likelihood weighting and ais bn is slightly better than loopcutset sampling in cpcs    where the probability of evidence p  e         is relatively high 
in cpcs     where probability of evidence p  e   e    is smaller  lcs outperforms ais bn
while gibbs sampling curves falls in between ais bn and likelihood weighting  both gibbs
sampling and loop cutset sampling outperform ais bn in cpcs   b and cpcs   b where
probability of evidence is small  in cpcs   b average p  e   e   and in cpcs   b the probability of evidence varies from  e    to  e     note that likelihood weighting and ais bn
performed considerably worse than either gibbs sampling or loop cutset sampling in all of
those benchmarks as a function of the number of samples  consequently  we left them off
the charts showing the convergence of gibbs and loop cutset sampling as a function of the
number of samples in order to zoom in on the two algorithms which are the focus of the
empirical studies 
coding networks  the results for coding networks are shown in figure     we
computed error measures over all coding bits and averaged over     instances     instances 
with different observed values  of each of the    networks with different coding matrices   as
we noted earlier  the markov chains corresponding to gibbs sampling over coding networks
are not ergodic and  as a result  gibbs sampling does not converge  however  the markov
  

ficutset sampling for bayesian networks

gibbs

cpcs    n      lc      w       e   

lw

cpcs    n      lc      w       e   

lcs

   e   

ais bn

   e   

gibbs

ibp

   e   

   e   

lcs

   e   

ibp

   e   

mse

mse

   e   
   e   
   e   

   e   
   e   

   e   

   e   

   e   

   e   

   e   
 

    

     

     

     

     

 

     

 

 

 

  samples
lw

cpcs    n      lc      w       e   

  

  

  

hellinger distance

lcs

   e   

ais bn

   e   

gibbs

   e   

lw

cpcs    n      lc      w       e   

ais bn

   e   

kl distance

 

time  sec 

ibp

   e   
   e   
   e   
   e   

gibbs

   e   

lcs

   e   

ibp

   e   
   e   
   e   
   e   
   e   

   e   
 

 

 

 

 

  

  

 

  

 

 

 

  

  

  

time  sec 

time  sec 
lw

cpcs    n      lc      w       e   

ais bn

   e   

absolute error

 

gibbs

   e   

lcs

   e   

ibp

   e   
   e   
   e   
   e   
   e   
 

 

 

 

 

  

  

  

time  sec 

figure    comparing loop cutset sampling  lcs   wlc     gibbs sampling  hereby referred
to as gibbs   likelihood weighting  lw   ais bn  and ibp on cpcs   network 
averaged over    instances  showing mse as a function of the number of samples
 top left  and time  top right  and kl distance  middle left   squared hellingers
distance  middle right   and an average error  bottom  as a function of time 

  

fibidyuk   dechter

cpcs     n       lc     w      e    

lw

cpcs     n       lc     w      e    

lw

   e   

ais bn

   e   

ais bn

gibbs

gibbs
lcs

ibp

   e   

ibp

   e   

mse

absolute error

lcs

   e   

   e   

   e   

   e   
 

 

 

 

 

  

  

 

  

 

 

 

 

  

  

cpcs     n       lc     w      e    

lw

cpcs     n       lc     w      e    

lw

   e   

ais bn

ais bn

   e   

gibbs

   e   

hellinger distance

gibbs

kl distance

  

time  sec 

time  sec 

lcs
ibp

   e   
   e   
   e   
   e   

lcs

   e   

ibp

   e   

   e   

   e   
 

 

 

 

 

  

  

  

 

 

 

 

time  sec 

 

  

  

  

time  sec 
lw

cpcs     n       lc     w      e    

ais bn

   e   

gibbs
absolute error

lcs
ibp

   e   

   e   

   e   
 

 

 

 

 

  

  

  

time  sec 

figure     comparing loop cutset sampling  lcs   wlc     gibbs sampling  likelihood
weighting  lw   ais bn  and ibp on cpcs    network  averaged over    instances  showing mse as a function of the number of samples  top left  and time
 top right  and kl distance  middle left   squared hellingers distance  middle
right   and an average error  bottom  as a function of time 

  

ficutset sampling for bayesian networks

cpcs   b  n       lc      w       e    

cpcs   b  n       lc      w       e    
  e   

gibbs

   e   

lw
ais bn

lcs

   e   

gibbs

ibp

   e   

lcs

mse

mse

  e   

   e   

ibp

  e   

   e   

  e   

   e   
 

    

     

     

     

 

     

 

 

 

  samples

cpcs   b  n       lc      w       e    

  

  

  

cpcs   b  n       lc      w       e    
lw

  e   

ais bn

hellinger distance

gibbs

  e   

lcs
ibp

  e   

lw

  e   

ais bn

kl distance

 

time  sec 

  e   
  e   

gibbs

  e   

lcs

  e   

ibp

  e   
  e   
  e   

 

 

 

 

 

  

  

  

 

 

 

 

time  sec 

 

  

  

  

time  sec 

cpcs   b  n       lc      w       e    
  e   

lw

absolute error

ais bn
gibbs

  e   

lcs
ibp

  e   

  e   
 

 

  

  

  

  

  

time  sec 

figure     comparing loop cutset sampling  lcs   wlc     gibbs sampling  likelihood
weighting  lw   ais bn  and ibp on cpcs   b network  averaged over    instances  showing mse as a function of the number of samples  top left  and time
 top right  and kl distance  middle left   squared hellingers distance  middle
right   and an average error  bottom  as a function of time 

  

fibidyuk   dechter

cpcs   b  n       lc      w       e    

gibbs

cpcs   b  n       lc      w       e    

lcs

   e   

lw
ais bn

   e   

gibbs

ibp

   e   

lcs

   e   

mse

   e   

mse

ibp

   e   

   e   
   e   

   e   

   e   
   e   
   e   

   e   
 

    

    

    

    

    

 

    

  

  

lw

cpcs   b  n       lc      w       e    

  

  

  

lw

cpcs   b  n       lc      w       e    

ais bn

   e   

ais bn

gibbs

   e   

lcs

   e   

ibp

gibbs

   e   

hellinger distance

kl distance

  

time  sec 

  samples

   e   
   e   
   e   
   e   

lcs
   e   

ibp

   e   
   e   
   e   
   e   

 

  

  

  

  

  

  

 

  

  

time  sec 

  

  

  

  

time  sec 
cpcs   b  n       lc      w       e    

lw
ais bn

   e   

gibbs

absolute error

lcs
ibp

   e   

   e   

   e   
 

  

  

  

  

  

  

time  sec 

figure     comparing loop cutset sampling  lcs   wlc      gibbs sampling  likelihood
weighting  lw   ais bn sampling  and ibp on cpcs   b network  averaged
over    instances  showing mse as a function of the number of samples  top
left  and time  top right  and kl distance  middle left   squared hellingers
distance  middle right   and an average error  bottom  as a function of time 

  

ficutset sampling for bayesian networks

   e   

   e   

   e   

   e   
   e   

   e   

   e   

   e   
 

 

 

 

 

 

  

 

 

time  sec 

 

 

  

time  sec 
lw
ais bn
gibbs
lcs
ibp

   e   

coding  n      p     lc      w    

lw
ais bn
gibbs
lcs
ibp

   e   

hellinger distance

coding  n      p     lc      w    
   e   

kl distance

lw
ais bn
gibbs
lcs
ibp

   e   

mse

absolute error

coding  n      p     lc      w    

lw
ais bn
gibbs
lcs
ibp

coding  n      p     lc      w    
   e   

   e   
   e   
   e   
   e   
   e   

   e   
   e   
   e   
   e   
   e   

 

 

 

 

 

 

  

time  sec 

 

 

 

 

  

time  sec 

figure     comparing loop cutset sampling  lcs   wlc     gibbs sampling  likelihood
weighting  lw   ais bn  and ibp on coding networks        averaged over
   instances of    coding networks      instances total   the graphs show average absolute error   top left   mse  top right   kl distance  bottom left   and
squared hellingers distance  bottom right  as a function of time 

chain corresponding to sampling the subspace of code bits only is ergodic and therefore 
loop cutset sampling  which samples a subset of coding bits  converges and even achieves
higher accuracy than ibp with time  in reality  ibp is certainly preferable for coding
networks since the size of the loop cutset grows linearly with the number of code bits 
random networks  in random multi part networks  figure     top  and random
  layer networks  figure     middle   loop cutset sampling always converged faster than
gibbs sampling  the results are averaged over    instances of each network type  in
both cases  loop cutset sampling achieved accuracy of ibp in   seconds or less  in   layer
networks  iterative belief propagation performed particularly poorly  both gibbs sampling
and loop cutset sampling obtained more accurate results in less than a second 
hailfinder network  we used this network  in addition to coding networks  to compare the behavior of cutset sampling and gibbs sampling in deterministic networks  since
hailfinder network contains many deterministic probabilities  the markov chain corresponding to gibbs sampling over all variables is non ergodic  as expected  gibbs sampling fails
while loop cutset sampling computes more accurate marginals  figure     
  

fibidyuk   dechter

random  n       e       c      w    

  layer  r     p    n       e       lc      w    

gibbs

   e   

lcs

   e   

ibp

   e   

gibbs
lcs

   e   

ibp

mse

mse

   e   
   e   
   e   

   e   
   e   

   e   
   e   

   e   
 

 

  

  

  

  

 

  

 

 

time  sec 

 

 

  

  

time  sec 

figure     comparing loop cutset sampling  lcs   gibbs sampling  and ibp on random
networks  left  and   layer random networks  right   wlc    in both classes of
networks  averaged over    instances each  mse as a function of time 

hailfinder  n      lc     w      e   

gibbs

   e   

lcs
ibp

mse

   e   

   e   

   e   
 

 

 

 

 

 

 

 

time  sec 

figure     comparing loop cutset sampling  lcs   wlc     gibbs sampling  and ibp on
hailfinder network     instances  mse as a function of time 

in summary  the empirical results demonstrate that loop cutset sampling is cost effective
m
time wise and superior to gibbs sampling  we measured the ratio r   mgc of the number
of samples mg generated by gibbs to the number of samples mc generated by loop cutset
sampling in the same time period  it is relatively constant for any given network and only
changes slightly between problem instances that differ with observations   for cpcs   
cpcs     cpcs   b  and cpcs   b the ratios were correspondingly                 and   
 see table   in section       we also obtained r     for random networks and r     for
random   layer networks  the ratio values     indicate that the gibbs sampler generates
  

ficutset sampling for bayesian networks

samples faster than loop cutset sampling which is usually the case  in those instances 
variance reduction compensated for the increased computation time because fewer samples
are needed to converge resulting in the overall better performance of loop cutset sampling
compared to gibbs sampling  in some cases  however  the reduction in the sample size
also compensates for the overhead computation in the sampling of one variable value  in
such cases  loop cutset sampling generated samples faster than gibbs yielding ratio r     
then  the improvement in the accuracy is due both to larger number of samples and to
faster convergence 
    w cutset sampling
in this section  we compare the general w cutset scheme for different values of w against
gibbs sampling  the main goal is to study how the performance of w cutset sampling
varies with w  for completeness sake  we include results of loop cutset sampling shown in
section     
in this empirical study  we used the greedy algorithm for set cover problem  mentioned
in section      for finding minimal w cutset  we apply the algorithm in such a manner that
each  w      cutset is a proper subset of a w cutset and  thus  can be expected to have a
lower variance and converge faster than sampling on w cutset in terms of number of samples
required  following the rao blackwellisation theory   we focus the empirical study on the
trade offs between cutset size reduction and the associated increase in sample generation
time as we gradually increase the bound w 
we used the same benchmarks as before and included also grid networks  all sampling
algorithms were given a fixed time bound  when sampling small networks  such as cpcs  
 w      and cpcs     w      where exact inference is easy  sampling algorithms were
allocated    seconds and    seconds respectively  for larger networks we allocated        
seconds depending on the complexity of the network which was only a fraction of exact
computation time 
table   reports the size of the sampling set used by each algorithm where each column
reports the size of the corresponding w cutset  for example  for cpcs   b  the average
size of gibbs sample  all nodes except evidence  is      the loop cutset size is     the size
of   cutset is     and so on  table   shows the rate of sample generation by different
algorithms per second  as we observed previously in the case of loop cutset sampling 
in some special cases cutset sampling generated samples faster than gibbs sampler  for
example  for cpcs   b  loop cutset sampling and   cutset sampling generated     samples
per second while the gibbs sampler was able to generate only     samples  we attribute
this to the size of cutset sample     nodes or less as reported in table    compared to the
size of the gibbs sample  over     nodes  
cpcs networks  we present two charts  one chart demonstrates the convergence
over time for several values of w  the second chart depicts the change in the quality
of approximation  mse  as a function of w for two time points  at the half of the total
sampling time and at the end of total sampling time  the performance of gibbs sampling
and cutset sampling for cpcs   is shown in figure     the results are averaged over   
instances with      evidence variables  the graph on the left in figure    shows the mean
square error of the estimated posterior marginals as a function of time for gibbs sampling 
  

fibidyuk   dechter

cpcs  
cpcs   
cpcs   b
cpcs   b
grid  x  
random
 layer
coding

gibbs
  
   
   
   
   
   
   
   

lc
  
 
  
  
   
  
  
  

w  
  
  
  
  
   
  
  
  

sampling set size
w   w   w   w  
  
  
 
 
 
 
 
  
  
  
  
  
  
  
  
   
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 

w  
  
  
  
  
 

w  
  
 

table    markov chain sampling set size as a function of w 

cpcs  
cpcs   
cpcs   b
cpcs   b
grid  x  
random
 layer
coding

gibbs
    
    
   
   
    
    
   
    

lc
      w   
     w   
     w   
   w   
     w   
      w   
     w   
      w   

no  of samples
w   w   w   w  
         
   
   
   
   
  
  
   
   
   
   
   
   
  
  
   
   
   
   
    
   
   
   
   
   
   
  
    
   
   
   

w  
   
  
  
  
   
  
   

w  
  
  
  
  
 

w  
  
 

table    average number of samples generated per second as a function of w 
loop cutset sampling  and w cutset sampling for w          and    the second chart shows
accuracy as a function of w  the first point corresponds to gibbs sampling  other points
correspond to loop cutset sampling and w cutset sampling with w ranging from   to    the
loop cutset result is embedded with the w cutset values at w    as explained in section     
the loop cutset corresponds to the w cutset where w is the maximum number of parents in
the network  initially  the best results were obtained by    and   cutset sampling followed
by the loop cutset sampling  with time     and   cutset sampling become the best 
the results for cpcs    are reported in figure     both charts show that loop cutset
sampling and w cutset sampling for w in range from   to   are superior to gibbs sampling 
the chart on the left shows that the best of the cutset sampling schemes  having the lowest
mse curves  are    and   cutset sampling  the loop cutset curve falls in between    and
  cutset at first and is outperformed by both    and   cutset after    seconds  loop cutset
sampling and    and   cutset sampling outperform gibbs sampling by nearly two orders of
magnitude as their mse falls below  e    while gibbs mse remains on the order of  e    the    and   cutset sampling results fall in between  achieving the mse  e     the
curves corresponding to loop cutset sampling and        and   cutset sampling fall below
the ibp line which means that all four algorithms outperform ibp in the first seconds of
execution  ibp converges in less than a second   the   cutset outperforms ibp after  
seconds  in figure    on the right  we see the accuracy results for all sampling algorithms
  

ficutset sampling for bayesian networks

gibbs

   e   

mse

   e   
   e   

ibp

cpcs    n      lc      w       e   

ibp
lcs w  

   e   

 c     w  
 c     w  

   e   

 c     w  
 c    w  

   e   

   e   

mse

cpcs    n      lc      w       e   
   e   

   e   

cutset    sec
cutset     sec

   e   
   e   

   e   

   e   
 

 

 

 

 

  

  

  

gibbs

w  

w  

w  

time  sec 

lcs 
w  

w  

w  

figure     mse as a function of time  left  and w  right  in cpcs       instances  time
bound    seconds 

mse

   e   

   e   

ibp

cpcs     n       lc     w      e    

cutset     sec

   e   

cutset     sec
   e   

mse

gibbs
ibp
lcs w  
 c     w  
 c    w  
 c    w  
 c    w  

cpcs     n       lc     w      e    
   e   

   e   
   e   
   e   

  

  

time  sec 

lc
s 
w 
 

  

w 
 

 

w 
 

 

w 
 

 

w 
 

 

g

 

ib
bs

   e   

figure     mse as a function of time  left  and w  right  in cpcs        instances  time
bound    seconds  y scale is exponential due to large variation in performance
of gibbs and cutset sampling 

after    seconds and    seconds  they are in agreement with the convergence curves on the
left 
in cpcs   b  figure      loop cutset sampling and    and   cutset sampling have similar
performance  the accuracy of the estimates slowly degrades as w increases  loop cutset
sampling and w cutset sampling substantially outperform gibbs sampling for all values w
and exceed the accuracy of ibp within   minute 
on the example of cpcs   b  we demonstrate the significance of the adjusted induced
width of the conditioned network in the performance of cutset sampling  as we reported
in section      its loop cutset is relatively small  lc     but wlc     and thus  sampling
just one new loop cutset variable value is exponential in the big adjusted induced width 
as a result  loop cutset sampling computes only   samples per second while the      and   cutset  which are only slightly larger having         and    nodes respectively  see
table     compute samples at rates of           and    samples per second  see table    
  

fibidyuk   dechter

cpcs   b  n       lc      w       e    

cpcs   b  n       e       lc      w    

gibbs

  e   

  e   

 c     w  
 c     w  

  e   

 c     w  
 c     w  

  e   

ibp
cutset t   sec
cutset t   sec

mse

mse

  e   

ibp
lcs w  

  e   

  e   

  e   

g

  
w

w 
 

  
w

time  sec 

  

  

w

  

  

  

w

  

  

  

lc
 w

  

s

  

ib
b

 

w 
 

  e   

  e   

figure     mse as a function of time  left  and w  right  in cpcs   b     instances  time
bound    seconds  y scale is exponential due to large variation in performance
of gibbs and cutset sampling 

gibbs
ibp
lcs w  
 c     w  
 c     w  
 c     w  
 c     w  

   e   

mse

   e   
   e   

cpcs   b  n       lc      w       e    
   e   

ibp
cutset      sec
cutset      sec

   e   

mse

cpcs   b  n       lc      w       e    
   e   

   e   

   e   

   e   

   e   

   e   
   e   
 

  

  

  

  

   

   

   

   e   

gibbs

time  sec 

w  

w  

w  

w  

w  

w  

figure     mse as a function of time  left  and w  right  in cpcs   b     instances  time
bound     seconds  y scale is exponential due to large variation in performance
of gibbs and cutset sampling 

the   cutset that is closest to loop cutset in size   c          computes    samples per
second which is an order of magnitude more than loop cutset sampling  the results for
cpcs   b are shown in figure     the loop cutset sampling results are excluded due to its
poor performance  the chart on the right in figure    shows that w cutset performed well
in range of w        and is far superior to gibbs sampling  when allowed enough time 
w cutset sampling outperformed ibp as well  the ibp converged in   seconds  the        
and   cutset improved over ibp within    seconds  and   cutset after    seconds 
random networks  results from    instances of random multi partite and    instances of   layer networks are shown in figure     as we can see  w cutset sampling
substantially improves over gibbs sampling and ibp reaching optimal performance for
w        for both classes of networks  in this range  its performance is similar to that of
loop cutset sampling  in case of   layer networks  the accuracy of both gibbs sampling and
  

ficutset sampling for bayesian networks

random  r     n      p     lc      w    

random  r     n      p     lc      w    

   e   
   e   

ibp
cutset t   sec
cutset t   sec

   e   

   e   

   e   

   e   
   e   

g

 layer  r     n      p     lc      w    

   e   

w 
 

ibp

   e   

cutset t   sec
cutset t   sec
   e   

mse

mse

   e   

w 
 

 layer  r     n      p     lc      w    

gibbs
ibp
 lc     w   
 c     w   
 c     w   
 c     w   
 c     w   

   e   

w 
 

time  sec 

w
  

  

  

  

 w

  

lc

  

ib
bs

  

w 
 

   e   
 

w 
 

mse

   e   

   e   

mse

gibbs
ibp
 lc     w   
 c     w   
 c     w   
 c     w   
 c     w   

   e   

   e   

   e   

   e   
 

 

  

  

  

  

time  sec 

   e   
gibbs

w   

lc w   

w   

w   

w   

w   

figure     random multi partite networks  top  and   layer networks  bottom       nodes 
   instances  mse as a function of the number of samples  left  and w  right  

  

fibidyuk   dechter

ibp is an order of magnitude less compared to cutset sampling  figure     bottom right  
the poor convergence and accuracy of ibp on   layer networks was observed previously
 murphy et al         
grid    x     e       lc       w      mse
   e   

   e   

ibp
 lc      w   

   e   

   e   

 c      w   

   

   

cutset t    sec

time  sec 

w 
  

  

w 
  

  

w 
  

  

ibb
s

  

cutset t   sec

   e   
g

 

ibp

   e   

w 
  

   e   

   e   

w 
  

 c     w   

w 
  

 c     w   

   e   

   e   

w 
  
lc
 w
  
 

 c      w   

   e   

mse

mse

grid    x     e       lc       w    

gibbs

figure     random networks      nodes     instances  mse as a function of the number
of samples  left  and w  right  

grid networks  grid networks having     nodes    x    were the only class of benchmarks where full gibbs sampling was able to produce estimates comparable to cutsetsampling  figure      with respect to accuracy  the gibbs sampler  loop cutset sampling 
and   cutset sampling were the best performers and achieved similar results  loop cutset
sampling was the fastest and most accurate among cutset sampling schemes  still  it generated samples about   times more slowly compared to gibbs sampling  table    since
loop cutset is relatively large  the accuracy of loop cutset sampling was closely followed
by        and   cutset sampling slowly degrading as w increased  grid networks are an
example of benchmarks with regular graph structure  that cutset sampling cannot exploit
to its advantage  and small cpts  in a two dimensional grid network each node has at most
  parents  where gibbs sampling is strong 
coding   x    n      p     lc      w    

coding    x    n      p     lc      w    

ibp
   e   

cutset t  sec

 c     w   

   e   

cutset t   sec
   e   

 c     w   

   e   

 c     w   

mse

mse

ibp

 lc     w   

   e   

   e   
   e   

   e   
   e   

   e   

   e   
   e   
 

 

 

 

 

  

   e   
w  

time  sec 

w  

lc w  

w  

figure     coding networks     code bits     parity check bits            instances  time
bound   minutes 

  

ficutset sampling for bayesian networks

cpcs  
cpcs   
cpcs   b
cpcs   b
grid  x  
random
 layer
coding

time
   sec
   sec
    sec
    sec
    sec
   sec
   sec
   sec

gibbs
    
    
    
    
    
    
   
   

markov chain length
lc w   w   w  
              
   
   
   
   
  
              
   
            
   
   
   
   
   
         
   
   
   
   
   
   
   
   
   
   

t
w  
   
  
   
   
   
   
  
   

w  
   
   
  
   
  
   

table    individual markov chain length as a function of w  the length of each chain m was
adjusted for each sampling scheme for each benchmark so that the total processing
time across all sampling algorithms was the same 

coding networks  the cutset sampling results for coding networks are shown in
figure     here  the induced width varied from    to    allowing for exact inference 
however  we additionally tested and observed that the complexity of the network grows
exponentially with the number of coding bits  even after a small increase in the number of
coding bits to    yielding a total of     nodes after corresponding adjustments to the number
of parity checking bits and transmitted code size  the induced width exceeds     while the
time for each sample generation scales up linearly  we collected results for    networks
    different parity check matrices  with    different evidence instantiations  total of    
instances   in decoding  the bit error rate  ber  is a standard error measure  however 
we computed mse over all unobserved nodes to evaluate the quality of approximate results
more precisely  as expected  gibbs sampling did not converge  because the markov chain
was non ergodic  and was left off the charts  the charts in figure    show that loop cutset
is an optimal choice for coding networks whose performance is closely followed by   cutset
sampling  as we saw earlier  cutset sampling outperforms ibp 
    computing an error bound
second to the issue of convergence of sampling scheme is always the problem of predicting
the quality of the estimates and deciding when to stop sampling  in this section  we compare
empirically the error intervals for gibbs and cutset sampling estimates 
gibbs sampling and cutset sampling are guaranteed to converge to the correct posterior
distribution in ergodic networks  however  it is hard to estimate how many samples are
needed to achieve a certain degree of convergence  it is possible to derive bounds on the
absolute error based on sample variance for any sampling method if the samples are independent  in gibbs and other mcmc methods  samples are dependent and we cannot apply
the confidence interval estimate directly  in case of gibbs sampling  we can apply the batch
means method that is a special case of standardized time series method and is used by the
bugs software package  billingsley        geyer        steiger   wilson        
  

fibidyuk   dechter

the main idea is to split a markov chain of length m  t into m chains of length
t   let pm  xi  e  be an estimate derived from a single chain m           m   of length t
 meaning  containing t samples  as defined in equations            the estimates pm  x e 
are assumed approximately independent for large enough m   assuming that convergence
conditions are satisfied and the central limit theorem holds  the pm  x e  is distributed
according to n  e p  xi  e         so that the posterior marginal p  xi  e  is obtained as an
average of the m results obtained from each chain  namely 
p  x e   

m
  x
pm  x e 
m

    

m  

and the sampling variance is computed as usually 
   

m
x
 
 pm  x e   p  x e   
m  
m  

an equivalent expression for the sampling variance is 
pm
p    x e   m p    x e 
 
   m   m
m  

    

where    is easy to compute incrementally storing only the running sums of pm  x e  and
   x e   therefore  we can compute the confidence interval in the          percentile
pm
used for random variables with normal distribution for small sampling set sizes  namely 
 
r  
 
  
    
p p  x e    p  x e   t     m   
m
where t     m    is a table value from t distribution with  m     degrees of freedom 
we used the batch means approach to estimate the confidence interval in the posterior
marginals with one modification  since we were working with relatively small sample sets
 a few thousand samples  and the notion of large enough m is not well defined  we
restarted the chain after every t samples to guarantee that the estimates pm  x e  were
truly independent  the method of batch means only provides meaningful error estimates
assuming that the samples are drawn from the stationary distribution  we assume that in
our problems the chains mix fast enough so that the samples are drawn from the target
distribution 
we applied this approach to estimate the error bound in the gibbs sampler and the
cutset sampler  we have computed a     confidence interval for the estimated posterior
marginal p  xi  e  based on the sampling variance of pm  xi  e  over    markov chains as
described above  we computed sampling variance    from eq      and the     confidence
interval      xi   from eq      and averaged over all nodes 
x x
 
      p
     xi  
n i  d xi   
i

xi d xi  

the estimated confidence interval can be too large to be practical  thus  we compared    
with the empirical average absolute error  
  

ficutset sampling for bayesian networks

cpcs  
cpcs   
cpcs   b
cpcs   b
random
 layer
coding
grid  x  


   

   

   

   

   

   

   

   

average error
lc
w  
               
               
               
               
               
               
         
         
               
               
               
               
               
               
               
               

gibbs
       
       
       
       
       
       
       
       
       
       
       
       
       
       

and confidence interval
w  
w  
w  
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
       
               
                
                
       
               
       
               

w  
       
       
       
       
       
       
       
       
       
       
       
       

table    average absolute error   measured  and estimated confidence interval     as a
function of w over    markov chains 

 

n

x
 
i  d xi   

p

i

x

xi d xi  

 p  xi  e   p  xi  e  

the objective of this study was to observe whether the computed confidence interval    
 estimated absolute error  accurately reflects the true absolute error   namely  to verify
that          and if so  then investigate empirically whether confidence interval for cutsetsampling estimates will be smaller compared to gibbs sampling as we would expect due to
variance reduction 
table   presents the average confidence interval and average absolute error for our
benchmarks  for each benchmark  the first row of results  row   reports the average
absolute error and the second row of results  row       reports the     confidence interval 
each column in table   corresponds to a sampling scheme  the first column reports results
for gibbs sampling  the second column reports results for loop cutset sampling  the
remaining columns report results for w cutset sampling for w in range     the loop cutset
sampling results for cpcs   b are not included due to statistically insignificant number of
samples generated by loop cutset sampling  the gibbs sampling results for coding networks
are left out because the network is not ergodic  as mentioned earlier  and gibbs sampling
does not converge 
we can see that for all the networks        which validates our method for measuring
confidence interval  in most cases the estimated confidence interval     is no more than
    times the size of average error  and is relatively small  in case of cutset sampling  the
largest confidence interval max               is reported in grid networks for loop cutset
  

fibidyuk   dechter

sampling  thus  the confidence interval estimate could be used as a criteria reflecting the
quality of the posterior marginal estimate by the sampling algorithm in practice  subsequently  comparing the results for gibbs sampling and cutset sampling  we observe not
only a significant reduction in the average absolute error  but also a similar reduction in the
estimated confidence interval  across all benchmarks  the estimated confidence interval of
the gibbs sampler remains        e    at the same time  for cutset sampling we obtain
       e   in   out of   classes of networks  excluded are the cpcs     grid  and   layer
networks  
    discussion
our empirical evaluation of the performance of cutset sampling demonstrates that  except
for grid networks  sampling on a cutset usually outperforms gibbs sampling  we show that
convergence of cutset sampling in terms of number of samples dramatically improves as
predicted theoretically 
the experiments clearly show that there exists a range of w values where w cutset
sampling outperforms gibbs sampler  the performance of w cutset sampling deteriorates
when increase in w yields only a small reduction in the cutset size  an example is cpcs   b
network where starting with w    increasing w by   results in the reducing the sampling
set by only   node  shown in table    
we observe that the loop cutset is a good choice of cutset sampling as long as the
induced width of network wlc conditioned on loop cutset is reasonably small  when wlc
is large  as in cpcs   b   loop cutset sampling is computationally less efficient then w cutset
sampling for w   wlc  
we also showed in section     that both gibbs sampling and loop cutset sampling
outperform the state of the art ais bn adaptive importance sampling method when the
probability of evidence is small  consequently  all the w cutset sampling schemes in section     that outperformed gibbs sampler in cpcs   b and cpcs   b would also outperfrom
ais bn 

   related work
we mention here some related work  the idea of marginalising out some variables to improve
efficiency of gibbs sampling was first proposed by liu et al          it was successfully
applied in several special classes of bayesian models  kong et al         applied collapsing
to the bivariate gaussian problem with missing data  liu        defined a collapsed gibbs
sampling algorithm for finding repetitive motifs in biological sequences applies by integrating
out two parameters from the model  similarly  gibbs sampling set is collapsed in escobar
        maceachern         and liu        for learning the nonparametric bayes problem 
in all of the instances above  special relationships between problem variables have been
exploited to integrate several variables out resulting in a collapsed gibbs sampling approach 
compared to this previous research work  our contribution is in defining a generic scheme
for collapsing gibbs sampling in bayesian networks which takes advantage of the networks
graph properties and does not depend on the specific form of the relationships between
variables 
  

ficutset sampling for bayesian networks

jensen et al         combined sampling and exact inference in a blocking gibbs sampling
scheme  groups of variables were sampled simultaneously using exact inference to compute
the needed conditional distributions  their empirical results demonstrate a significant improvement in the convergence of the gibbs sampler over time  yet  in proposed blocking
gibbs sampling  the sample contains all variables in the network  in contrast  cutset sampling reduces the set of variables that are sampled  as noted previously  collapsing produces
lower variance estimates than blocking and  therefore  cutset sampling should require fewer
samples to converge 
a different combination of sampling and exact inference for join trees was described
by koller et al         and kjaerulff         oller et al  and kjaerulff proposed to sample
the probability distribution in each cluster for computing the outgoing messages  kjaerulff
used gibbs sampling only for large clusters to estimate the joint probability distribution
p  vi    vi  x in cluster i  the estimated p  vi   is recorded instead of the true joint
distribution to conserve memory  the motivation is that only high probability tuples will
be recorded while the remaining low probability tuples are assumed to have probability   
in small clusters  the exact joint distribution p  vi   is computed and recorded  however  the
paper does not analyze the introduced errors or compare the performance of this scheme
with standard gibbs sampler or the exact algorithm  no analysis of error is given nor
comparison with other approaches 
koller et al         used sampling used to compute messages sent from cluster i to
cluster j and the posterior joint distributions in a cluster tree that contains both discrete
and continuous variables  this approach subsumes the cluster based sampling proposed
by kjaerulff        and includes rigorous analysis of the error in the estimated posterior
distributions  the method has difficulties with propagation of evidence  the empirical
evaluation is limited to two hybrid network instances and compares the quality of the
estimates to those of likelihood weighting  an instance of importance sampling that does
not perform well in presence of low probability evidence 
the effectiveness of collapsing of sampling set has been demonstrated previously in the
context of particle filtering method for dynamic bayesian networks  doucet  andrieu   
godsill      a  doucet  defreitas    gordon        doucet  de freitas  murphy    russell 
    b   it was shown that sampling from a subspace combined with exact inference  raoblackwellised particle filtering  yields a better approximation than particle filtering on
the full set of variables  however  the objective of the study has been limited to observation
of the effect in special cases where some of the variables can be integrated out easily  our
cutset sampling scheme offers a generic approach to collapsing a gibbs sampler in any
bayesian network 

   conclusion
the paper presents the w cutset sampling scheme  a general scheme for collapsing gibbs
sampler in bayesian networks  we showed theoretically and empirically that cutset sampling improves the convergence rate and allows sampling from non ergodic network that
has ergodic subspace  by collapsing the sampling set  we reduce the dependence between
samples by marginalising out some of the highly correlated variables and smoothing the
sampling distributions of the remaining variables  the estimators obtained by sampling
  

fibidyuk   dechter

from a lower dimensional space also have a lower sampling variance  using the induced
width w as a controlling parameter  w cutset sampling provides a mechanism for balancing
sampling and exact inference 
we studied the power of cutset sampling when the sampling set is a loop cutset and 
more generally  when the sampling set is a w cutset of the network  defined as a subset of
variables such that  when instantiated  the induced width of the network is  w   based
on rao blackwell theorem  cutset sampling requires fewer samples than regular sampling
for convergence  our experiments showed that this reduction in number of samples was
time wise cost effective  we confirmed this over a range of randomly generated and real
benchmarks  we also demonstrated that cutset sampling is superior to the state of the art
ais bn importance sampling algorithm when the probability of evidence is small 
since the size of the cutset and the correlations between the variables are two main
factors contributing to the speed of convergence  w cutset sampling may be optimized further with the advancement of methods for finding minimal w cutset  another promising
direction for future research is to incorporate the heuristics for avoiding selecting stronglycorrelated variables into a cutset since those correlations are driving factors in the speed
of convergence of gibbs sampling  alternatively  we could combine sample collapsing with
blocking 
in summary  w cutset sampling scheme is a simple yet powerful extension of sampling
in bayesian networks that is likely to dominate regular sampling for any sampling method 
while we focused on gibbs sampling with better convergence characteristics  other sampling
schemes can be implemented with the cutset sampling principle  in particular  it was
adapted for use with likelihood weighting  bidyuk   dechter        

references
abdelbar  a  m     hedetniemi  s  m          approximating maps for belief networks is
np hard and other theorems  artificial intelligence            
andrieu  c   de freitas  n     doucet  a          rao blackwellised particle filtering via
data augmentation  in advances in neural information processing systems  mit
press 
arnborg  s  a          efficient algorithms for combinatorial problems on graphs with
bounded decomposability   a survey  bit          
becker  a   bar yehuda  r     geiger  d          random algorithms for the loop cutset
problem  journal of artificial intelligence research             
bertele  u     brioschi  f          nonserial dynamic programming  academic press 
bidyuk  b     dechter  r          empirical study of w cutset sampling for bayesian networks  in proceedings of the   th conference on uncertainty in artificial intelligence
 uai   pp        morgan kaufmann 
bidyuk  b     dechter  r          on finding minimal w cutset problem  in proceedings
of the   th conference on uncertainty in artificial intelligence  uai   pp       
morgan kaufmann 
  

ficutset sampling for bayesian networks

bidyuk  b     dechter  r          cutset sampling with likelihood weighting  in proceedings of the   nd conference on uncertainty in artificial intelligence  uai   pp 
      morgan kaufmann 
billingsley  p          convergence of probability measures  john wiley   sons  new york 
casella  g     robert  c  p          rao blackwellisation of sampling schemes  biometrika 
             
cheng  j     druzdzel  m  j          ais bn  an adaptive importance sampling algorithm
for evidenctial reasoning in large baysian networks  journal of aritificial intelligence
research             
cooper  g          the computational complexity of probabilistic inferences  artificial
intelligence             
dagum  p     luby  m          approximating probabilistic inference in bayesian belief
networks is np hard  artificial intelligence                 
dechter  r       a   bucket elimination  a unifying framework for reasoning  artificial
intelligence            
dechter  r       b   bucket elimination  a unifying framework for reasoning  artificial
intelligence                 
dechter  r          constraint processing  morgan kaufmann 
doucet  a     andrieu  c          iterative algorithms for state estimation of jump markov
linear systems  ieee trans  on signal processing                   
doucet  a   andrieu  c     godsill  s       a   on sequential monte carlo sampling methods for bayesian filtering  statistics and computing                 
doucet  a   de freitas  n   murphy  k     russell  s       b   rao blackwellised particle
filtering for dynamic bayesian networks  in proceedings of the   th conference on
uncertainty in artificial intelligence  uai   pp         
doucet  a   defreitas  n     gordon  n          sequential monte carlo methods in practice 
springer verlag  new york  inc 
doucet  a   gordon  n     krishnamurthy  v          particle filters for state estimation of jump markov linear systems  tech  rep   cambridge university engineering
department 
escobar  m  d          estimating normal means iwth a dirichlet process prior  journal of
the american statistical aasociation             
frey  b  j     mackay  d  j  c          a revolution  belief propagation in graphs with
cycles  in neural information processing systems  vol     
fung  r     chang  k  c          weighing and integrating evidence for stochastic simulation in bayesian networks  in proceedings of the  th conference on uncertainty in
artificial intelligence  uai   pp          morgan kaufmann 
geiger  d     fishelson  m          optimizing exact genetic linkage computations  in proceedings of the  th annual international conf  on computational molecular biology 
pp          morgan kaufmann 
  

fibidyuk   dechter

gelfand  a     smith  a          sampling based approaches to calculating marginal densities  journal of the american statistical association             
geman  s     geman  d          stochastic relaxations  gibbs distributions and the
bayesian restoration of images  ieee transaction on pattern analysis and machine
intelligence            
geyer  c  j          practical markov chain monte carlo  statistical science            
gilks  w   richardson  s     spiegelhalter  d          markov chain monte carlo in practice 
chapman and hall 
gottlob  g   leone  n     scarello  f          a comparison of structural csp decomposition
methods  in proceedings of the   th international joint conference on artificial
intelligence  ijcai   pp          morgan kaufmann 
jensen  c   kong  a     kjrulff  u          blocking gibbs sampling in very large probabilistic expert systems  int  j  of human computer studies  special issue on realworld applications of uncertain reasoning                 
jensen  f  v   lauritzen  s  l     olesen  k  g          bayesian updating in causal
probabilistic networks by local computation  computational statistics quarterly    
       
jones  g     hobert  j  p          honest exploration of intractable probability distributions
via markov chain monte carlo  statist  sci                  
kask  k   dechter  r   larrosa  j     dechter  a          unifying cluster tree decompositions for reasoning in graphical models  artificial intelligence              
kjrulff  u          hugs  combining exact inference and gibbs sampling in junction
trees  in proceedings of the   th conference on uncertainty in artificial intelligence
 uai   pp          morgan kaufmann 
koller  d   lerner  u     angelov  d          a general algorithm for approximate inference
and its application to hybrid bayes nets  in proceedings of the   th conference on
uncertainty in artificial intelligence  uai   pp         
kong  a   liu  j  s     wong  w          sequential imputations and bayesian missing
data problems  j  of the american statistical association                   
kschischang  f  r     frey  b  j          iterative decoding of compound codes by probability propagation in graphical models  ieee journal on selected areas in communications             
larrosa  j     dechter  r          boosting search with variable elimination in constraint
optimization and constraint satisfaction problems  constraints                
lauritzen  s     spiegelhalter  d          local computation with probabilities on graphical
structures and their application to expert systems  journal of the royal statistical
society  series b                
liu  j          correlation structure and convergence rate of the gibbs sampler  ph d 
thesis  university of chicago 
  

ficutset sampling for bayesian networks

liu  j          the collapsed gibbs sampler in bayesian computations with applications to
a gene regulation problem  journal of the american statistical association           
       
liu  j   wong  w     kong  a          covariance structure of the gibbs sampler with
applications to the comparison of estimators and augmentation schemes  biometrika 
             
liu  j  s          nonparametric hierarchical bayes via sequential imputations  annals of
statistics                 
liu  j  s          monte carlo strategies in scientific computing  springer verlag  new
york  inc 
maceachern  s   clyde  m     liu  j          sequential importance sampling for nonparametric bayes models  the next generation  the canadian journal of statistics     
       
maceachern  s  n          estimating normal means with a conjugate style dirichlet process
prior  communications in statistics simulation and computation                 
mackay  d          introduction to monte carlo methods  in proceedings of nato advanced study institute on learning in graphical models  sept    oct    pp         
maier  d          the theory of relational databases  in computer science press  rockville 
md 
mceliece  r   mackay  d     cheng  j  f          turbo decoding as an instance of pearls
belief propagation algorithm  ieee j  selected areas in communication             
miller  r   masarie  f     myers  j          quick medical reference  qmr  for diagnostic
assistance  medical computing              
miller  r   pople  h     myers  j          internist    an experimental computerbased
diagnostic consultant for general internal medicine  new english journal of medicine 
                
murphy  k  p   weiss  y     jordan  m  i          loopy belief propagation for approximate
inference  an empirical study  in proceedings of the   th conference on uncertainty
in artificial intelligence  uai   pp          morgan kaufmann 
parker  r     miller  r          using causal knowledge to create simulated patient cases 
the cpcs project as an extension of internist    in proceedings of the   th symp 
comp  appl  in medical care  pp         
pearl  j          probabilistic reasoning in intelligent systems  morgan kaufmann 
peot  m  a     shachter  r  d          fusion and propagation with multiple observations
in belief networks  artificial intelligence             
pradhan  m   provan  g   middleton  b     henrion  m          knowledge engineering for
large belief networks  in proceedings of   th conference on uncertainty in artificial
intelligence  seattle  wa  pp         
rish  i   kask  k     dechter  r          empirical evaluation of approximation algorithms
for probabilistic decoding  in proceedings of the   th conference on uncertainty in
artificial intelligence  uai   pp          morgan kaufmann 
  

fibidyuk   dechter

roberts  g  o     sahu  s  k          updating schemes  correlation structure  blocking
and parameterization for the gibbs sampler  journal of the royal statistical society 
series b                 
roberts  g  o     tweedie  r  l          bounds on regeneration times and convergence
rates for markov chains  stochastic processes and their applications             
roberts  g  o     tweedie  r  l          corregendum to bounds on regeneration times and
convergence rates for markov chains  stochastic processes and their applications     
       
rosenthal  j  s          convergence rates for markov chains  siam review             
    
rosti  a  v     gales  m          rao blackwellised gibbs sampling for switching linear
dynamical systems  in ieee international conference on acoustics  speech  and
signal processing  icassp        pp         
schervish  m     carlin  b          on the convergence of successive substitution sampling 
journal of computational and graphical statistics            
shachter  r  d   andersen  s  k     solovitz  p          global conditioning for probabilistic
inference in belief networks  in proceedings of the   th conference on uncertainty in
artificial intelligence  uai   pp         
shachter  r  d     peot  m  a          simulation approaches to general probabilistic
inference on belief networks  in proceedings of the  th conference on uncertainty in
artificial intelligence  uai   pp         
steiger  n  m     wilson  j  r          convergence properties of the batch means method
for simulation output analysis  informs journal on computing                 
tierney  l          markov chains for exploring posterior distributions  annals of statistics 
                 
yuan  c     druzdzel  m          an importance sampling algorithm based on evidence
pre propagation  in proceedings of the   th conference on uncertainty in artificial
intelligence  uai   pp         
zhang  n     poole  d          a simple algorithm for bayesian network computations  in
proceedings of the   th canadian conference on artificial intelligence  pp         

  

fi