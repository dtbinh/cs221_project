journal of artificial intelligence research                 

submitted       published     

proactive algorithms for job shop scheduling with
probabilistic durations
j  christopher beck

jcb mie utoronto ca

department of mechanical   industrial engineering
university of toronto  canada

nic wilson

n wilson  c ucc ie

cork constraint computation centre
university college cork  ireland

abstract
most classical scheduling formulations assume a fixed and known duration for each activity  in this paper  we weaken this assumption  requiring instead that each duration can
be represented by an independent random variable with a known mean and variance  the
best solutions are ones which have a high probability of achieving a good makespan  we
first create a theoretical framework  formally showing how monte carlo simulation can be
combined with deterministic scheduling algorithms to solve this problem  we propose an
associated deterministic scheduling problem whose solution is proved  under certain conditions  to be a lower bound for the probabilistic problem  we then propose and investigate
a number of techniques for solving such problems based on combinations of monte carlo
simulation  solutions to the associated deterministic problem  and either constraint programming or tabu search  our empirical results demonstrate that a combination of the use
of the associated deterministic problem and monte carlo simulation results in algorithms
that scale best both in terms of problem size and uncertainty  further experiments point
to the correlation between the quality of the deterministic solution and the quality of the
probabilistic solution as a major factor responsible for this success 

   introduction
proactive scheduling techniques seek to produce an off line schedule that is robust to execution time events  in this paper  we assume that we do not have perfect knowledge of
the duration of each activity  the durations are determined at execution time when it is
observed that an activity has finished  however  we do have partial knowledge in the form
of a known probability distribution for each duration  at execution time  the activities will
be dispatched according to the sequences defined by the off line schedule and our measure of
robustness is the probability with which a given quality will be achieved  more specifically 
in this paper  we address the problem of job shop scheduling  and related generalizations 
when the durations of the activities are random variables and the objective is to find a
solution which has a high probability of having a good  ideally  minimal  makespan  this
is a challenging problem as even evaluating a solution is a hard problem 
to address this problem  we develop a theoretical framework within which we formally
define the problem and  a  construct an approach  based on monte carlo simulation  for
evaluating both solutions and partial solutions  and  b  show that solving a carefully defined
deterministic job shop scheduling problem results in a lower bound of the probabilistic
c
    
ai access foundation  all rights reserved 

fibeck   wilson

minimum makespan of the probabilistic job shop scheduling problem  we use this framework
to define a number of algorithms embodying three solution approaches 
   branch and bound search with monte carlo simulation  at each search node  the
search is pruned if we can be almost certain  based on the monte carlo simulation 
that the partial solution cannot be extended to a solution better than our current best
solution 
   iterative deterministic search with a descending lower bound  the deterministic job
shop problem whose solution is a lower bound on the probabilistic job shop problem
is defined using a parameter  q  the lower bound proof depends on q being less than
or equal to q   i   a problem instance dependent threshold value for problem instance
i that is difficult to compute  starting with a high q value  we use tree search and
monte carlo simulation to solve a sequence of deterministic problems with decreasing
q values  when q is large  the problems are highly constrained and easy to solve  if
any solutions exist   as q descends  the best probabilistic makespan from previous
iterations is used to restrict the search  if we are able to reach a value of q with
q  q   i  within the cpu time limit  then the search is approximately complete
subject to the sampling error 
   deterministic filtering search  deterministic scheduling algorithms based on constraint
programming and tabu search are used to define a number of filter based algorithms 
all these algorithms operate by generating a series of solution candidates that are
evaluated by monte carlo simulation 
our empirical results indicate that the monte carlo based branch and bound is only
practical for very small problems  the iterative search based on descending q values is as
good as  or better than  the branch and bound algorithm on small problems  and performs
significantly better on larger problems  however  even for medium sized problems  both of
these techniques are inferior to the heuristic approaches based on deterministic filtering 
contributions 

the main contributions of this paper are 

 the introduction of the problem of finding proactive schedules with probabilistic execution guarantees for a class of problems where the underlying deterministic scheduling
problem is np hard 
 the development of a method for generating a lower bound on the probabilistic minimum makespan 
 the development of a particular monte carlo approach for evaluating solutions 
 the design and empirical analysis of a number of approximately complete and heuristic solution techniques based on either constraint based constructive search or tabu
search  and
 the identification of the correlation between deterministic and probabilistic solution
quality as a key factor in the performance of the filter based algorithms 
   

fiproactive algorithms for jsp

plan of paper  in the next section we define the probabilistic job shop scheduling problem  illustrating it with an example  section   discusses related work  in section    we
present our theoretical framework  we formally define the problem  derive our approach
for generating a lower bound based on an associated deterministic job shop problem  and
show how monte carlo simulation can be used to evaluate solutions and partial solutions 
six search algorithms are defined in section   and our empirical investigations and results
appear in section    in section    it is shown how the results of this paper apply to much
more general classes of scheduling problems  directions for future work based on theoretical
and algorithmic extensions are also discussed 

   probabilistic job shop scheduling problems
the job shop scheduling problem with probabilistic durations is a natural extension of the
standard  deterministic  job shop scheduling problem  jsp  
    job shop scheduling problems
a jsp involves a set a of activities  where each ai  a has a positive duration di   for
each instance of a jsp  it is assumed that either all the durations are positive integers  or
they are all positive real numbers   a is partitioned into jobs  and each job is associated
with a total ordering on that set of activities  each activity must execute on a specified
unary capacity resource  no activities that require the same resource can overlap in their
execution  and once an activity is started it must be executed for its entire duration  we
represent this formally by another partition of a into resource sets  two activities are in
the same resource set if and only if they require the same resource 
a solution consists of a total ordering on each resource set  which does not conflict with
the jobs ordering  i e   the union of the resource orderings and job orderings is an acyclic
relation on a  thus  if ai and aj are in the same resource set  a solution either orders ai
before aj  meaning that aj starts no sooner than the end of ai    or aj before ai   the set
of solutions of a job shop problem will be labeled s  a partial solution consists of a partial
ordering on each resource set which can be extended to a solution 
let s be a  partial  solution  a path in s  or an s path  is a sequence of activities such
that if ai immediately precedes aj in the sequence  then either  i  ai and aj are in the
same job  and ai precedes aj in that job  or  ii  ai and aj are in the same resource set
and s orders ai before aj   the length  len    of a path
p   of a solution  is equal to the
sum of the durations of the activities in the path  i e   ai  di   the makespan  make s  
of a solution s is defined to be the length of a longest s path  an s path    is said to be a
critical s path if the length of  is equal to the makespan of the solution s  i e   it is one of
the longest s paths  the minimum makespan of a job shop scheduling problem is defined
to be the minimum value of make s  over all solutions s 
the above definitions focus on solutions rather than on schedules  here  we briefly indicate how our definitions relate to  perhaps more immediately intuitive  definitions focusing
on schedules  a schedule assigns the start time of each activity  and so can be considered as
   our empirical investigations examine the integer case  as shown below  the theoretical results hold also
for the case of positive real number durations 

   

fibeck   wilson

a function from the set of activities a to the set of time points  defining when each activity
starts  the set of time points is assumed to be either the set of non negative integers or
the set of non negative real numbers  let starti be the start time of activity ai  a with
respect to a particular schedule  and let endi   its end time  be starti   di   for ai   aj  a 
write ai  aj for the constraint endi  startj   a schedule is defined to be valid if the
following two conditions hold for any two different activities ai   aj  a   a  if ai precedes
aj in the same job  then ai  aj   and  b  if ai and aj are in the same resource set  then
either ai  aj or aj  ai  since ai and aj are not allowed to overlap  
let z be a valid schedule  define make z   the makespan of z  to be maxai a endi  
the time at which the last activity has been completed  the minimum makespan is defined
to be the minimum value of make z  over all valid schedules 
each solution s defines a valid schedule sched s   where each activity is started as soon
as its immediate predecessors  if any  have finished  and activities without predecessors are
started at time point    so sched s  is a non delay schedule given the precedence constraints
expressed by s   an immediate predecessor of activity aj with respect to a particular
solution is defined to be an activity which is an immediate predecessor of aj either with
respect to the ordering on the job containing aj   or with respect to the ordering  associated
with the solution  on the resource set containing aj   it can be shown that the makespan
of sched s  is equal to make s  as defined earlier  hence justifying our definition 
conversely  given a valid schedule z  we can define a solution  which we call sol z  
by ordering each resource set with the relation  defined above  if z is a schedule  then
the makespan of sched sol z    which is equal to make sol z    is less than or equal to the
makespan of z  this implies that the minimum makespan over all solutions is equal to the
minimum makespan over all valid schedules  therefore  if we are interested in schedules
with the best makespans  we need only consider solutions and their associated schedules 
to summarize  when aiming to find the minimum makespan for a jsp  we can focus on
searching over all solutions  rather than over all schedules  because  i  for any schedule z 
there exists a solution s   sol z  such that z is consistent with s  i e   satisfies the precedence constraints expressed by s   and  ii  for any solution s  we can efficiently construct
a schedule sched s  which is optimal among schedules consistent with s  and furthermore 
the makespan of sched s  is equal to make s   
jsp example  consider a job shop scheduling problem involving two jobs and five activities as shown in figure    the first job consists of the sequence  a    a    a    of activities 
the second job consists of the sequence  a    a     there are three resources involved  a  and
a  require the first resource  hence activities a  and a  cannot overlap  and so either  i 
a  precedes a    or  ii  a  precedes a    activities a  and a  require the second resource 
a  requires the third resource  hence  the resource sets are  a    a      a    and  a    a    
there are four solutions 
 sa involves the orderings a   a  and a   a   
 sb is defined by a   a  and a   a   
 sc by a   a  and a   a    and
 sd by a   a  and a   a   
   

fiproactive algorithms for jsp

a 

a 

a 

a 

a 

a 

a 

a 

a 

a 

a 

a 

a 

a 

solution sa
a 

a 

solution sb
a 

a 

a 

a 

a 

a 

a 

a 

solution sc

a 
solution sd

figure    the example jsp with its four solutions 
the duration of activity ai is di   the sequence  a    a    a    is an sa  path  whose length
is d   d   d    also  if  is the sa  path  a    a    a    a     then len     d   d   d   d    the
only other sa  paths are subsequences of one of these two  hence  make sa    the makespan
of solution sa   is equal to max d    d    d    d    d    d    d      d    d    max d    d    d    
in particular  if d       d       d       d      and d       then make sa        time units 
we then also have make sb         make sc        and make sd         hence  the minimum
makespan is make sa        
let z   sched sa   be the schedule associated with solution sa   this is generated as
follows  a  has no predecessors  so we start a  at the beginning  setting z a         hence
activity a  starts at time point   and ends at time point d    the only predecessor of a 
is a    so we set z a      d    similarly  we set z a      d    and so activity a  ends at
time point d    d    continuing  we set z a      d    d    activity a  has two immediate
predecessors  for this solution  sa    a  and a    and so a  is set to start as soon as both
of these activities have been completed  which is at time point max d    d    d    d    d    
all activities have been completed when a  has been completed  which is at time point
max d   d   d    d   d    d    d   d   max d    d   d     this confirms that the makespan
make sa   of solution sa is equal to the makespan of its associated schedule sched sa   
    independent and general probabilistic job shop scheduling problems
an independent probabilistic job shop scheduling problem is defined in the same way as a
jsp  except that the duration di associated with an activity ai  a is a random variable 
we assume that in each instance of a probabilistic jsp  either all the durations are positive
integer valued random variables  or they are all positive real valued random variables  d i
has  known  distribution pi   expected value i   e di   and variance i    var di    these
   

fibeck   wilson

random variables are fully independent  the length of a path  of a solution s is now
a random variable  which we write as len    the makespan make s  of solution s  the
length of the longest path in s  is therefore also a random variable  which we will sometimes
refer to as the random makespan of s 
we can generalize this to the non independent case  in the probabilistic job shop scheduling problem we have a joint probability measure p over the durations vectors   the intention
is that we can efficiently sample with the joint density function  for example  a bayesian
network might be used to represent p    here  for activity ai   distribution pi is defined to
be the appropriate marginal distribution  with expected value i and variance i   
loosely speaking  in a probabilistic job shop scheduling problem  we want to find as
small a value of d as possible such that there is a solution whose random makespan is  with
high probability  less than d  the deadline for all activities to finish   this time value d
will be called the probabilistic minimum makespan 
evaluating a solution for a deterministic jsp  i e   finding the associated makespan given
a duration for each activity  can be achieved in low degree polynomial time using a longest
path algorithm  without the ordering on each resource set  the disjunctions of resource
constraints that must be satisfied to find a solution turn this very easy problem into the
np complete jsp  garey   johnson         pert networks  on the other hand  generalize
the simple longest path problem by allowing durations to be independent random variables 
leading to a  p complete problem  hagstrom         the probabilistic jsp makes both
these generalizations  consequently  finding the optimal solutions of a probabilistic jsp
appears to be very hard  and we focus on methods for finding good solutions instead 
evaluating  approximately  a solution of a probabilistic jsp can be done relatively
efficiently using monte carlo simulation  for each of a large number of trials we randomly
sample the duration of every activity and generate the makespan associated with that
trial  roughly speaking  we approximately evaluate the solution by evaluating the sampled
distribution of these makespans  this approach is described in detail in section     
almost all of our solution techniques involve associating a deterministic job shop problem
with the given probabilistic job shop problem  by replacing  for some number q  each random
duration by the mean of its distribution plus q times its standard deviation  hence  we set
the duration di of activity ai in the associated deterministic problem to be i  q i for the
case of continuous time  for the case when time points are integers  we set d i   bi  qi c 
for certain values of q  this leads to the minimum makespan of the deterministic problem
being a lower bound for the probabilistic minimum makespan  as shown in section      this
lower bound can be useful for pruning in a branch and bound algorithm  more generally 
we show how solving the associated deterministic problem can be used to help solve the
probabilistic problem 
our assumptions about the joint probability are somewhat restrictive  for example  the
model does not allow an activitys duration to depend on its start time  however  it can be
extended to certain situations of this kind   despite these restrictions  which are common in
related literaturesee section     our model does apply to an interesting class of problems
   we could allow the duration of each activity to be probabilistically dependent only on its start time  given
the additional  very natural  coherence condition that for any time point t    the conditional probability
that endi  t    given starti   t  is monotonically increasing in t  i e   pr endi  t   starti   t    
pr endi  t   starti   t    if t   t    this condition ensures that  for any given solution  there is no

   

fiproactive algorithms for jsp

that has not been previously addressed  extending our model to richer representations by
relaxing our assumptions remains for future work 
probabilistic jsp example  we consider an independent probabilistic job shop scheduling problem with the same structure as the jsp example in figure    the durations of
activities a    a  and a  are now independent real valued random variables  referred to as
d    d  and d    respectively  which are all approximately normally distributed with standard deviation                       and with means              and        the
durations of activities a  and a  are deterministic  being equal to   and    respectively 
let  be the sa  path  a    a    a    a     the length len   of  is an approximately
normally distributed random variablewith mean              and variance              
    and hence standard deviation      
the length of sa  path       a    a    a    is an approximately normal random variable
with mean    and standard deviation      the  random  makespan make sa   of solution
sa is a random variable equaling the maximum of random variables len   and len      
in general  the maximum of two independent normally distributed random variables is not
normally distributed  however   is  with high probability  longer than      so the distribution
of make sa   is approximately equal to the distribution of len   

   previous work
there has been considerable work on scheduling with uncertainty in a variety of fields
including artificial intelligence  ai   operations research  or   fault tolerant computing 
and systems  for surveys of the literature  mostly focusing in ai and or  see the work of
davenport and beck         herroelen and leus         and bidot        
at the highest level  there are two approaches to such problems  proactive scheduling 
where some knowledge of the uncertainty is taken into account when generating an off line
schedule  and reactive scheduling where decisions are made on line to deal with unexpected
changes  while there is significant work in reactive scheduling and  indeed  on techniques
that combine reactive and proactive scheduling such as least commitment approaches  see
the surveys noted above   here our interest is on pure proactive scheduling  three categories
of proactive approaches have been identified  redundancy based techniques  probabilistic
techniques  and contingent policy based techniques  herroelen   leus         we briefly
look at each of these in turn 
    redundancy based techniques
redundancy based techniques generate a schedule that includes the allocation of extra
resources and or time in the schedule  the intuition is that these redundant allocations will
help to cushion the impact of unexpected events during execution  for example  extra time
can be consumed when an activity takes longer than expected to execute  because there
is a clear conflict between insertion of redundancy and common measures of schedule quality
 e g   makespan   the focus of the work tends to be the intelligent insertion of redundancy
in order to achieve a satisfactory trade off between schedule quality and robustness 
advantage in delaying starting an activity when its predecessors have finished  allowing such a delay
would break the assumptions underlying our formulation 

   

fibeck   wilson

it is common in fault tolerant scheduling with real time guarantees to reserve redundant
resources  i e   processors  or time  in the former case  multiple instantiations of a given
process are executed in parallel and error detection can be done by comparing the results
of the different instantiations  in contrast  in time redundancy  some time is reserved for
re execution of a process that fails  given a fault model  either technique can be used to
provide real time guarantees  ghosh  melhem    mosse        ghosh        
a similar approach is used in the work of gao        and davenport  gefflot and beck
       in the context of job shop scheduling  statistical information about the mean time
between failure and the mean repair time of machines is used to either extend the duration
of critical activities in the former work or to require that any solution produced must respect
constraints on the slack of each activity  given a solution  the slack is the room that an
activity has to move without breaking a constraint or increasing the cost  typically  it is
formalized as the difference between an activitys possible time window in a solution  i e   its
latest possible end time less its earliest possible start time  and the duration of the activity 
the advantage of gaos approach is that it is purely a modeling approach  the problem is
changed to incorporate extended durations and any scheduling techniques can be used to
solve the problem  however  davenport et al  show that reasoning about the slack shared
amongst a set of activities can lead to better solutions at the cost of specialized solving
approaches 
leon  wu and storer        present an approach to job shop scheduling where the
objective function is modified to be a linear combination of the expected makespan and
expected delay assuming that machines can break down and that  at execution time  disruptions are dealt with by shifting activities later in time while maintaining the sequence
in the original schedule  while this basic technique is more properly seen as a probabilistic
approach  the authors show that an exact calculation of this measure is intractable unless a
single disruption is assumed  when there are likely to be multiple disruptions  the authors
present a number of surrogate measures  empirically  the best surrogate measure is the
deterministic makespan minus the mean activity slack  unlike  gao and davenport et al  
leon et al  provide a more formal probabilistic foundation  but temporal redundancy plays
a central role in the practical application of their approach 
    probabilistic techniques
probabilistic techniques use representations of uncertainty to reason about likely outcomes
when the schedule is executed   rather than explicitly inserting redundancy in an attempt
to create a robust schedule  probabilistic techniques build a schedule that optimizes some
measure of probabilistic performance  performance measures typically come in two forms 
an expected value such as expected makespan or expected weighted tardiness  and a probabilistic guarantee with respect to a threshold value of a deterministic optimization measure 
an example of the latter measure  as discussed below  is the probability that the flow time
of a schedule will be less than a particular value 
optimal expected value scheduling problems have been widely studied in or  pinedo 
       in many cases  the approach takes the form of dispatch rules or slightly more
complicated polynomial time algorithms that will find the optimal schedule for tractable
   alternative representations of uncertainty such as fuzzy sets can also be used  herroelen   leus        

   

fiproactive algorithms for jsp

problems  e g     and   machine problems  and which serve as heuristics for more difficult
problems  one example of such work in the ai literature is that of wurman and wellman
       which extends decision theoretic planning concepts to scheduling  the problem
studied assumes a single machine  stochastic processing time and stochastic set up time 
and has as its objective the minimization of the expected weighted number of tardy jobs 
the authors propose a state space search and solve the problem of multi objective stochastic
dominance a   critical aspects of this work are the use of a number of sophisticated path
pruning rules and relaxation based heuristics for the evaluation of promising nodes 
a threshold measure is used by burns  punnekkat  littlewood and wright        in a
fault tolerant  single processor  pre emptive scheduling application  the objective is to find
the minimum fault arrival rate such that all tasks can be scheduled to meet their deadlines 
based on a fault model  the probability of observing that fault arrival rate is calculated
and used as a measure of schedule quality  the optimization problem  then  is to find the
schedule that maximizes the probability of all tasks meeting their deadlines under the fault
arrival process 
in a one machine manufacturing context with independent activities  daniels and carrillo        define a  robust schedule as the sequence that maximizes the probability that
the execution will achieve a flow time no greater than a given threshold  while the underlying deterministic scheduling problem is solvable in polynomial time and  indeed  the
minimum expected flow time schedule can be found in polynomial time  it is shown that
finding the  robust schedule is np hard  daniels and carrillo present branch and bound
and heuristic techniques to solve this problem 
    contingent and policy based approaches
unlike the approaches described above  contingent and policy based approaches do not
generate a single off line schedule  rather  what is produced is a branching or contingent
schedule or  in the extreme  a policy  that specifies the actions to be taken when a particular
set of circumstances arises  given the importance of having an off line schedule in terms
of coordination with other entities in the context surrounding the scheduling problem  this
difference can have significant practical implications  see herroelen   leus        for a
discussion  
an elegant example of a contingent scheduling approach is the just in case work of
drummond  bresina and swanson         given an initial  deterministic schedule for a
single telescope observation problem  the approach identifies the activity most likely to fail
based on the available uncertainty information  at this point  a new schedule is produced
assuming the activity does  indeed  fail  repeated application of the identification of the
most likely to fail activity and generation of a new schedule results in a branching schedule
where a number of the most likely contingencies are accounted for in alternative schedules 
at execution time  when an activity fails  the execution switches to the alternative schedule
if one exists  if an alternative does not exist  on line rescheduling is done  empirical results
demonstrate that a significantly larger portion of an existing  branching  schedule can be
executed without having to revert to rescheduling as compared to the original deterministic
schedule 
   

fibeck   wilson

one of the weaknesses of the just in case scheduling surrounds the combinatorics of
multiple resources  with multiple inter dependent telescopes  the problem quickly becomes
intractable  policy based approaches such as markov decision processes  mdps   boutilier 
dean    hanks        have been applied to such problems  here  the objective is to
produce a policy mapping states to actions that will direct the on line execution of the
schedule  when a given state is encountered  the corresponding action is taken  meuleau et
al         apply mdps to a stochastic military resource allocation problem where weapons
must be allocated to targets  given a limited number of weapons and uncertainty about the
effectiveness of a given allocation  an mdp is used to derive an optimal policy where the
states are represented by the number of remaining weapons and targets  and the actions are
the weapon allocation decisions  the goal is to minimize the expected number of surviving
targets  empirical results demonstrated the computational challenges of such an approach
as a   target     weapon problem required approximately   hours of cpu time  albeit on
now outdated hardware  
in the or literature  there has been substantial work  cited in brucker  drexl  mohring 
neumann and pesch        and herroelen and leus        on stochastic resource constraint
project scheduling  a generalization of job shop scheduling  the general form of these
approaches is a multi stage stochastic programming problem  with the objective of finding a
scheduling policy which will minimize the expected makespan  in this context  a scheduling
policy makes decisions on line about what activities to execute  decisions need to be made at
the beginning of the schedule and at the end time of each activity  and the information used
for such decisions must be only that which has become known before the time of decision
making  a number of different classes of policy have been investigated  for example  a
minimal forbidden subset of activities  f   is a set such that the activities in f cannot
be executed simultaneously due to resource constraints  but that any subset of f can be
so executed  a pre selective policy identifies such a set f and a waiting activity  j  f  
such that j cannot be started until at least one activity i  f   j  has been executed 
during execution  j can be started only when at least one other activity in f has finished 
the proactive problem  then  is to identify the waiting activity for each minimal forbidden
subset such that the expected makespan is minimized  the computational challenges of
pre selective policies  in particular  due to the number of minimal forbidden subsets  have
led to work on different classes of policy as well as heuristic approaches 
    discussion
the work in this paper falls within the probabilistic scheduling approaches and is most
closely inspired by the  robustness work of daniels and carrillo         however  unlike
daniels and carrillo  we address a scheduling model where the deterministic problem that
underlies the probabilistic job shop scheduling problem is  itself  np hard  this is the
first work of which we are aware that seeks to provide probabilistic guarantees where the
underlying deterministic problem is computationally difficult 

   theoretical framework
in this section  we develop our theoretical framework for probabilistic job shop problems 
in section      we define how we compare solutions  using what we call  makespans  if the
   

fiproactive algorithms for jsp

 makespan of solution s is less than time value d  then there is at least chance     that
the  random  makespan of s is less than d  as it can be useful to have an idea about how far
a solutions  makespan is from the optimum  makespan  i e   the minimum  makespan
over all solutions   in section      we describe an approach for finding a lower bound for the
optimum  makespan  section     considers the problem of evaluating a given solution  s 
by using monte carlo simulation to estimate the  makespan of s 
in order to separate our theoretical contributions from our empirical analysis  we summarize the notation introduced in this section in section      readers interested primarily
in the algorithms and empirical results can therefore move directly to section   
this section makes use of notation introduced in section    the definitions in section
    of a jsp  a solution  paths in a solution  the makespan of a solution  and the minimum
makespan  and the definitions in section     of a probabilistic jsp and the random makespan
of a solution 
    comparing solutions and probabilistic makespan
in a standard job shop problem  solutions can be compared by considering the associated
makespans  in the probabilistic case  the makespan of a solution is a random variable  so
comparing solutions is less straight forward  we map the random makespan to a scalar
quantity  called the  makespan  which sums up how good it is  solutions are compared by
comparing their associated  makespans  a simple idea is to prefer solutions with smaller
expected makespan  however  there may be a substantial probability that the makespan
of the solution will be much higher than its expected value  instead  we take the following
approach  if we can be confident that the random makespan for solution s is at most d 
but we cannot be confident that the makespan for solution s  is at most d  then we prefer
solution s to solution s   
we fix a value   which is used to bound probabilities  although we imagine that in
most natural applications of this work   would be quite small  e g   less than      we
assume only that  is in the range           if the probability of an event is at least     
then we say that the event is sufficiently certain  the experiments described in section  
use a value of          so that sufficiently certain then means occurs with at least    
chance 
let d be a time value  and let s be a solution  d is said to be  achievable using
s if it is sufficiently certain that all jobs finish by d when we use solution s  that is  if
pr make s   d        where make s  is the random makespan of s 
d is said to be  achievable if there is some solution s such that d is  achievable using
s  i e   if there exists some solution s making it sufficiently certain that all jobs finish by d 
time value d is  achievable if and only if maxss pr make s   d         where the
max is over all solutions s 
define ach  s  to be the set of all d which are  achievable using s  we define d  s  
the  makespan of s  to be the infimum  of ach  s   then d   the  minimum makespan 
is defined to be the infimum of ach   which is the set of all d which are  achievable  so
   that is  the greatest lower bound of ach  s   in fact  as shown by proposition   i   d  s  is the smallest
element of ach  s   hence  ach  s  is equal to the closed interval  d  s      i e   the set of time points
d such that d  d  s  

   

fibeck   wilson

d   inf  d    maxss pr make s   d          we will also sometimes refer to d  s 
as the probabilistic makespan of s  and refer to d as the probabilistic minimum makespan  
we prefer solutions which have better  i e   smaller   makespans  equivalently  solution
s is considered better than s  if there is a time value d which is  achievable using s but
not  achievable using s    optimal solutions are ones whose  makespan is equal to the
 minimum makespan 
we prove some technical properties of  makespans and  achievability relevant for
mathematical results in later sections  in particular  proposition   ii  states that the minimum makespan d is  achievable  i e   there exists some solution which makes it
sufficiently certain that all jobs finish by d   d is the smallest value satisfying this
property 
lemma   with the above notation 
 i  ach  

s

ss

ach  s  

 ii  there exists a solution s such that ach   ach  s  and d   d  s  
 iii  d   minss d  s   the minimum of d  s  over all solutions s 
proof 
 i  d is  achievable
if and only if for some solution s  d  ach  s   which is true if and
s
only if d  ss ach  s  
 ii  consider the following property    on set of time values a  if d  a and d   is a time
value greater than d  i e   d     d   then d    a  that is  a is an interval with no upper
bound  let a and b be two sets with property     then either a  b or b  a   to show
this  suppose otherwise  that neither a  b nor b  a  then there exists some x  a  b
and some y  b  a  x and y must be different  and so we can assume  without loss of
generality  that x   y  then by property     y  a which is the contradiction required  
hence  a  b is either equal to a or equal to b  by using induction  it follows that the
union of a finite number of sets s
with property    is one of the sets  each set ach   s 
satisfies property     therefore  ss ach  s    achs  for some solution s    so  by  i  
ach   achs    this implies also d   d  s    
 iii  let s be any solution and let d be any time value  clearly  if d is  achievable using
s  then d is  achievable  this implies that d  d  s   hence  d  minss d  s   by
 ii   d   d  s  for some solution s  so d   minss d  s   as required 
 

proposition  
 i  let s be any solution  d  s  is  achievable using s  i e   pr make s   d  s   
    
 ii  d is  achievable  i e   there exists some solution s with pr make s   d       
   note that the probabilistic makespan is a number  a time value   as opposed to the random makespan
of a solution  which is a random variable 

   

fiproactive algorithms for jsp

proof 
in the discrete case  when the set of time values is the set of non negative integers  then
the infimum in the definitions of d  s  and d is the same as minimum   i  and  ii  then
follow immediately from the definitions 
we now consider the case when the set of time values is the set of non negative real
numbers 
 
 
 i   for m  n                    let gm   pr     make s d  s   m
   and let gn   pr  n  
 
 
make s   d  s   n    by the countable additivity axiom of probability measures  gm  
p
p
gn   this means that l 
n m
n m gn tends to gm as l tends to infinity  and hence gl  
pl 
p
n m gn tends to    so  we have limm gm      for all m      we
l gn   g m 
 
        by definition of d  s   also pr make s  
have pr make s   d  s    m
 
   so  for all m                pr make s  
d  s     gm   pr make s   d  s    m
d  s         gm   which implies pr make s   d  s         because gm tends to
  as m tends to infinity 

 ii   by part  ii  of lemma    for some solution s  d   d  s   part  i  then implies that
pr make s   d        
 
probabilistic jsp example continued  we continue the example from section    
and section      set  to       corresponding to     confidence  a value of d        is
 achievable using solution sa   since there is more than     chance that both paths  and
   are  simultaneously  shorter than length       and so the probability that the random
makespan make sa   is less than      is more than      
now consider a value ofd         since len    the random length of   has mean   
and standard deviation       the chance that
 len         is approximately the chance
that a normal distribution is no more than   standard deviations above its mean  this
probability is about       therefore  d        is not  achievable using solution s a   since
there is less than      chance that the random makespan make sa   is no more than d 
the  makespan  also referred to as the probabilistic makespan  of solution s a is
therefore between      and       in fact  the  makespan d  sa   is approximately equal
to        since there is approximately     chance that the  random  makespan make s a  
is at most        it is easy to show that d         is not  achievable using any other
solution  so d   the  minimum makespan  is equal to d  sa    and hence about       
    a lower bound for  minimum makespan
in this section we show that a lower bound for the  minimum makespan d can be found
by solving a particular deterministic jsp 
a common approach is to generate a deterministic problem by replacing each random
duration by the mean of the distribution  as we show  under certain conditions  the minimum makespan of this deterministic jsp is a lower bound for the probabilistic minimum
makespan  for instance  in the example  the minimum makespan of such a deterministic
jsp is     and the probabilistic minimum makespan is about        however  an obvious
weakness with this approach is that it does not take into account the spreads of the distributions  this is especially important since we are typically considering a small value of  
   

fibeck   wilson

such as       we can generate a stronger lower bound by taking into account the variances
of the distributions when generating the associated deterministic job shop problem 
generating a deterministic jsp from a probabilistic jsp and a value q  from
a probabilistic job shop problem  we will generate a particular deterministic job shop problem  depending on a parameter q     we will use this transformation for almost all the
algorithms in section    the deterministic jsp is the same as the probabilistic jsp except
with each random duration replaced by a particular time value  solving the corresponding
deterministic problem will give us information about the probabilistic problem  the deterministic jsp consists of the same set a of activities  partitioned into the same resource sets
and the same jobs  with the same total order on each job  the duration of activity a i in the
deterministic problem is defined to be i   qi   where i and i are respectively the mean
and standard deviation of the duration of activity ai in the probabilistic job shop problem 
hence  if q      the associated deterministic problem corresponds to replacing each random
duration by its mean  let makeq  s  be the deterministic makespan of solution s  i e   the
makespan of s for the associated deterministic problem  which is defined to be the length of
the longest s pathsee section       let makeq be the minimum deterministic makespan
over all solutions 
let s be a solution  we say that s is probabilistically optimal if d  s    d   let 
be an s path    is a path in both the probabilistic and deterministic problems    is said
to be a  deterministically  critical path if it is a critical path in the deterministic problem 
the length of  in the deterministic
problem  lenq     is p
equal to the sum
p
p of the durations
of activities in the path 
ai   i   qi    which equals
ai  i   q
ai  i  
we introduce the following rather technical definition whose significance is made clear
by proposition    q is  sufficient if there exists a  deterministically  critical path  in
some probabilistically optimal solution s with pr len     lenq         i e   there is more
than  chance that the random path length is greater than the deterministic length 
the following result shows that an  sufficient value of q leads to the deterministic
minimum makespan makeq being a lower bound for the probabilistic minimum makespan
d   therefore  a lower bound for the deterministic minimum makespan is also a lower
bound for the probabilistic minimum makespan 
proposition   for a probabilistic jsp  suppose q is  sufficient  then  for any solution
s  pr make s   makeq          therefore  makeq is not  achievable  and is a strict
lower bound for the  minimum makespan d   i e   d   makeq  
proof  since q is  sufficient  there exists a  deterministically  critical path  in some  probabilistically  optimal solution so with pr len     lenq         we have lenq     
makeq  so    because  is a critical path  and  by definition of makeq   we have makeq  so   
makeq   so  pr len     makeq       by the definition of makespan  for any sample
of the random durations vector  make so   is at least as a large as len    so  we have
pr make so     makeq       hence  pr make so    makeq        pr make so    
makeq          this implies d  so     makeq since pr make so    d  so         
by proposition   i   since so is a probabilistically optimal solution  d   d  so    and so
d   makeq   also  for any solution s  we have d  s   d   makeq   so d  s    makeq  
which implies that makeq is not  achievable using s  i e   pr make s   makeq           
   

fiproactive algorithms for jsp

      finding  sufficient q values
proposition   shows that we can find a lower bound for the probabilistic minimum makespan
if we can find an  sufficient value of q  and if we can solve  or find a lower bound for  the
associated deterministic problem  this section looks at the problem of finding  sufficient
values of q  by breaking down the condition into simpler conditions 
in the remainder of section      we assume an independent probabilistic jsp 
let  be some path of some solution  define  to be e len    
p the expected value
of the length of   in the probabilistic jsp   which is equal to pai  i   define   to
be var len     the variance of the length of   which is equal to ai  i    since we are
assuming that the durations are independent 
p
defining  adequate b  for b     write b    for    b   which equals ai  i  
qp
 
b
ai  i   we say that b is  adequate if for any  deterministically  critical path  of
any  probabilistically  optimal solution  pr len     b         i e   there is more than 
chance that  is more than b standard deviations longer than its expected length 
if each duration is normally distributed  then len   will be normally distributed  since
it is the sum of independent normal distributions  even if the durations are not normally
distributed  len   will often be close to being normally distributed  cf  the central limit
theorem and its extensions   so  pr len     b     will then be approximately     b  
where  is the unit normal distribution  a b value of slightly less than         will be
 adequate  given approximate normality 
defining b adequate values of q  we say that q is b adequate if there exists a
 deterministically  critical path  in some  probabilistically  optimal solution such that
lenq     b    
the following proposition shows that the task of finding  sufficient values of q can be
broken down  it follows almost immediately from the definitions 
proposition   if q is b adequate for some b which is  adequate  then q is  sufficient 
proof  since q is b adequate  there exists a  deterministically  critical path  in some
 probabilistically  optimal solution s such that lenq     b     since b is  adequate 
pr len     b         and hence pr len     lenq         as required 
 
establishing b adequate values of q  a value q is b adequate if and only if there
exists a  deterministically  critical path  in some  probabilistically  optimal solution
qp such
p
p
p
 
that lenq     b     equivalently 
ai  i  
ai  i   q
ai  i 
ai  i   b
qp

q

mean i    ai  
  where m is
meanp
 i   ai  
ai
the number of activities in path   and mean i   ai      m  ai  i  
if any activity ai is not uncertain  i e   its standard deviation i equals     then it can
be omitted from the summations and means  m then becomes the number of uncertain
activities in path  
that is  q  b

p

ai 

i 

 i

  this can be written as  q 

   

b
m

fibeck   wilson

as is well known  and quite easily shown   the root qmean square of a collection of
mean      ai  
numbers is always at least as large as the mean  hence  mean  i  a   is greater than
i
i
or equal to    therefore  a crude sufficient condition for q to be b adequate is  q  bm  
where m is an upper bound for the number of uncertain activities in any path  for any
probabilistically optimal solution  or we could take m to be an upper bound for the number
of uncertain activities in any path  for any solution   in particular  we could generate badequate q by choosing q   bm  
an  sufficient value of q  putting the two conditions together and using proposition
 
   we have that a q value of a little less than     
will be  sufficient  given that the
m
lengths of the paths are approximately normally distributed  where m is an upper bound
for the number of uncertain activities in any path  for any optimal solution  hence  by
proposition    the minimum makespan makeq of the associated deterministic problem is
then a strict lower bound for the  minimum makespan d   for example  with         
we have                 since there is about      chance that a normal distribution is
more than       standard deviations above its mean   and so we can set q to be a little less
  
than      
m
one can sometimes generate a larger  sufficient value of q  and hence a stronger lower
bound makeq   by focusing only on the significantly uncertain activities  choose value 
between
p   and    for any path   say
p that that activity aj is  uncertain  with respect to
  if
 i   ai    i  j       i   ai     then the sum of the durations of the
activities which are not  uncertain is at most a fraction  of the sum of all the durations in
the path  hence  the activities in  which are not  uncertain have relatively small standard
deviations  if we define m to be an upper bound on the number of  uncertain activities
involved in any path of any  probabilistically  optimal solution  then it can be shown  by

will be b adequate 
a slight modification of the earlier argument  that a q value of    b
m
and hence a q value of a little less than

        

m



will be  sufficient 

the experiments described in section   use  for varying n  problems with n jobs and n
activities per job   solutions which have paths involving very large numbers of activities
are unlikely to be good solutions  in particular  one might assume that  for such problems 
there will be an optimal solution s and a  deterministically  critical s path  involving no
more than  n activities  given this assumption  the following value of q is  sufficient 
 
  e g  
making makeq a lower bound for the probabilistic minimum makespan  q       
 n
q 

     

 n

when          this motivates the choice of q  in table   in section     

probabilistic jsp example continued  the number of uncertain activities in our
running example  see section      figure   and section      is    so one
 can set m     
using          this leads to a choice of q slightly less than                  by
proposition   and the above discussion  such a value of q is  sufficient  the durations of
the associated deterministic problem are given by setting di   i   qi   and so are d      
d        q    d        q    d        q   and d       solution sa is the best solution
with makespan makeq  sa                  q           q           q  hence  the minimum
   

fiproactive algorithms for jsp

deterministic makespan makeq equals approximately        which is a lower bound for the
probabilistic minimum makespan d         illustrating proposition   
however  sc is clearly a poor solution  so we could just consider the other solutions 
 sa   sb   sd    no  deterministically  critical path of these solutions involves more than two
uncertain activities  within
 the range of interest of q values   so we can then set m     
and q                   this leads to the stronger lower bound of                   
which is a very tight lower bound for the  minimum makespan d  
      discussion of lower bound
in our example  we were able to use our approach to construct a very tight lower bound
for the probabilistic minimum makespan  however  this situation is rather exceptional 
two features of the example which enable this to be a tight lower bound are  a  the best
solution has a path which is almost always the longest path  and  b  the standard deviations
of the uncertain durations are all equal  in the above analysis  the root mean square is
approximated  from below  by the mean  this is a good approximation when the standard
deviations are fairly similar  and in an extreme case when the  non zero  standard deviations
of durations are all the same  as in the example   the root mean square is actually equal to
the mean 
more generally  there are a number of ways in which our lower bound will tend to be
conservative  in particular 
 the choice of m will often have to be conservative for us to be confident that it is
a genuine upper bound for the number of uncertain activities in any path for any
optimal solution 
 we are approximating a root mean square of standard deviations by the average of the
standard deviations  this can be a very crude approximation if the standard deviations
of the durations vary considerably between activities 
 we are approximating the random variable make s  by the random length of a particular path 
the strength of our lower bound method  however  is that it is computationally feasible for
reasonably large problems as it uses existing well developed jsp methods 
    evaluating a solution using monte carlo simulation
for a given time value  d  we want to assess if there exists a solution for which there is
a chance of at most  that its random makespan is greater than d  our methods will all
involve generating solutions  or partial solutions   and testing this condition 
as noted earlier  evaluating a solution amounts to solving a pert problem with uncertain durations  a  p complete problem  hagstrom         as in other  p complete
problems such as the computation of dempster shafer belief  wilson         a natural approach to take is monte carlo simulation  burt   garman         we do not try to perform
an exact computation but instead choose an accuracy level  and require that with a high
chance our random estimate is within  of the true value  the evaluation algorithm then
   

fibeck   wilson

has optimal complexity  low degree polynomial  but with a potentially high constant factor
corresponding to the number of trials required for the given accuracy 
to evaluate a solution  or partial solution  s using monte carlo simulation we perform
a  large  number  n   of independent trials assigning values to each random variable  each
trial generates a deterministic problem  and we can check very efficiently if the corresponding
makespan is greater than d  if so  we say that the trial succeeds  the proportion of trials
that succeed is then an estimate of pr make s    d   the chance that the random makespan
of s is more than d  for the case of independent probabilistic jsps  we can generate the
random durations vector by picking  using distribution pi   a value for the random duration
di for each activity ai   for the general case  picking a random durations vector will still
be efficient in many situations  for example  if the distribution is represented by a bayesian
network 
      estimating the chance that the random makespan is greater than d
perform n trials  l              n  
for each  trial  l 
 pick a random durations vector using the joint density function 
 let tl      the trial succeeds  if the corresponding  deterministic  makespan is greater
than d  otherwise  set tl     
p
let t   n  n
l   tl be the proportion of trials that succeed  t is then an estimate of p 
where p   pr make s    d   the chance that a randomly generated durations vector leads
to a makespan  for solution s  greater than d  the expected value of t is equal
q to p  since
  pn
e tl     p and so e t     n l   e tl     p  the standard deviation of t is p  p 
n   which
can be shown as follows  v ar tl     e  tl        e tl       p  p    p    p   the variables
p
p  p 
 
tl are independent so v ar t     n   n
  n
  the random variable n t
i   v ar tl    
n
is binomially distributed  and so  because of the demoivre laplace limit theorem  feller 
       we can use a normal distribution to approximate t  
this means that  for large n   generating a value of t with the above algorithm will  with
high probability  give a value close to pr make s    d   we can choose any accuracy level
     and confidence level r  e g   r          and choose n such that pr  t  p        r 
in particular  if r        and using a normal approximation  choosing a number n of trials
more than    is sufficient  for fixed accuracy level  and confidence level r  the number
of trials n is a constant  it does not depend on the size of the problem  the algorithm
therefore has excellent complexity  the same as the complexity  low order polynomial  of a
single deterministic propagation  and so must be optimal as we clearly cannot hope to beat
the complexity of deterministic propagation  however  the constant factor    can be large
when we require high accuracy 
      when is the solution good enough 
let d be a time value and let s be a solution  suppose  based on the above monte carlo
algorithm using n trials  we want to be confident that d is  achievable using s  i e   that
   

fiproactive algorithms for jsp

pr make s    d      we therefore need the observed t to be at least a little smaller
than   since t is  only  an estimate of pr make s    d  
to formalize this  we shall use a confidence interval style approach  let k     recall
that p   pr make s    d  is an unknown quantity that we want to find information
about  we say that p   is k implausible given the result t  if the following condition
holds  p  p
implies that t is at least k standard deviations below the expected value  i e  
t  p  kn p    p  
if it were the case that p    and p   is k implausible given t   then an unlikely
event would have happened  for example  with k       given the normal approximation  
such an event will only happen about once every    experiments  if k     such an event
will only happen about once every        experiments 
if pr make s    d    is k implausible given the result t   then we can be confident
that pr make s    d      d is  achievable using s  so that d is an upper bound
of d  s  and hence of the  minimum makespan d   the confidence level  based on a
normal approximation of the binomial distribution  is  k   where  is the unit normal
distribution  for example  k     gives a confidence of around       
similarly  for any  between   and      we say that p   is k implausible given the
result t if the following condition holds  p   implies
that t is at least k standard
p
deviations above the expected value  i e   t  p   kn p    p  
the above definitions of k implausibility are slightly informal  the formal definitions
are as follows  suppose             k     t         and n                    we define 
p   is k implausible given p
t if and only if for all p such that   p     the following
condition holds  t  p  kn p    p   similarly  p   is k implausible given t if and
p
only if for all p such that    p    the following condition holds  t  p   kn p    p  
these k implausibility conditions cannot be tested directly using the definition since
p is unknown  fortunately  we have the following result  which gives equivalent conditions
that can be easily checked 
proposition   with the above definitions 
k
n

p
      
p
 ii  p   is k implausible given t if and only if t     kn       
 i  p   is k implausible given t if and only if t   

p
proof   i   if p   is k implausible given t   then setting p to  gives t   kn      
p
as required  conversely  suppose t    kn        the result follows if k     
 

so we can assume that k      write f  x     x  t     k x  x 
  now  since t 
n
p
k      
k
 
  n        we have    t and    t   
so  f        also  f  t      
n
since f  x  is a quadratic polynomial with a positive coefficient of x    this implies that t is
either a solution of the equation f  x       or is between the two solutions  since f      
and    t   it follows that  must either be a solution of f  x       or be greater than the
 
  since p   t  
solution s   this implies  for all p     f  p       and so  p  t      k p  p 
n
q
p  p 
we have for all p   that t  p  k
n   that is  p   is k implausible given t  
proving  i  
   

fibeck   wilson

q

  con ii  if p   is k implausible given t   then setting p to  gives t     k    
q n
q
  then  since        p   implies t  p   k p  p 
since
versely  if t     k    
n
n
the right hand side is a strictly increasing function of p  so p   is k implausible given t  
as required 
 
part  i  of this result shows us how to evaluate a solution s with respect
p to a bound
k

d  if we generate t  using a monte carlo simulation  which is at least n       less
than   then we can have confidence that p     i e   pr make s    d      and so we
can have confidence that d is  achievable using solution s  i e   that d is an upper bound
for the probabilistic makespan d  s   part  ii  is used in the branch and bound algorithm
described in section        for determining if we can backtrack at a node 
      generating an upper approximation of the probabilistic makespan of a
solution
suppose that  given a solution s  we wish to find a time value d which is just large enough
such that we can be confident that the probabilistic makespan of s is at most d  i e   that
d is an upper bound for the  makespan d  s   the monte carlo simulation can be
adapted for this purpose  we simulate the values of the random makespan make s  and
record the distribution of these  we decide on a value of k  corresponding to the desired
degree of confidence  e g   k     corresponds to about       confidence  and choose d
minimal suchpthat the associated t value  generated from the simulation results  satisfies
t    kn        then by proposition   i   pr make s    d    is k implausible
given t   we can therefore be confident that pr make s    d      so we can have
confidence that d is an upper bound for the  makespan d  s  of s  in the balance of this
paper  we will use the notation d s  to represent our  upper  estimate of d   s  found in
this way 

   searching for solutions
the theoretical framework provides two key tools that we use in building search algorithms 
first  we can use monte carlo simulation to evaluate a solution or a partial solution  see
section       second  with the appropriate choice of a q value  we can solve an associated
deterministic problem to find a lower bound on the  minimum makespan for a problem
instance  see section       in this section  we make use of both these tools  and some
variations  to define a number of constructive and local search algorithms  before describing
the algorithms  we recall some of the most important concepts and notation introduced in
these earlier sections 
for all of our algorithms  we explicitly deal only with the case of independent probabilistic jsps where durations are positive integer random variables  given our approach 
however  these algorithms are all valid 
 for the generalized probabilistic case  with the assumptions noted in section    provided we have an efficient way to sample the activity durations 
   

fiproactive algorithms for jsp

 for continuous random variables  provided we have a deterministic solver that can
handle continuous time values 
    summary of notation
the remainder of the paper makes use of notation and concepts from earlier sections  which
we briefly summarize below 
for a jsp or probabilistic jsp  a solution s totally orders activities requiring the same
resource  i e   activities in the same resource set   so that if activity ai and aj require the
same resource  then s either determines that ai must have been completed by the time
aj starts  or vice versa  see section       a partial solution partially orders the set of
activities in each resource set  associated with a solution is a non delay schedule  relative
to the solution   where activities without predecessors are started at time    and other
activities are started as soon as all their predecessors have been completed  the makespan
of a solution is the time when all jobs have been completed in this associated non delay
schedule  for a probabilistic jsp  see section       the makespan make s  of a solution s
is a random variable  since it depends on the random durations 
the quantity we use to evaluate a solution s is d  s   the  makespan of s  also known as
the probabilistic makespan of s   defined in section      the probability that the  random 
makespan of s is more than d  s  is at most   and approximately equal to    more
precisely  d  s  is the smallest time value d such that pr make s    d  is at most   
value  therefore represents a degree of confidence required  the  minimum makespan
d  also known as the probabilistic minimum makespan  is the minimum of d  s  over all
solutions s 
a time value d is  achievable using solution s if and only if there is at most  chance
that the random makespan is more than d  d is  achievable using s if and only if
d  d  s   see section      
solutions of probabilistic jsps are evaluated by monte carlo simulation  see section
      a method is derived for generating an upper approximation of d    we use the
notation d s  to represent this upper approximation  which is constructed so that d s 
is approximately equal to d  s   and there is a high chance that d  s  will be less than
d s see section        d s  thus represents a probable upper bound for the probabilistic
minimum makespan 
with a probabilistic job shop problem we often associate a deterministic jsp  see section
      this mapping is parameterized by a  non negative real  number q  the associated
deterministic jsp has the same structure as the probabilistic jsp  the only difference is
that the duration of an activity ai is equal to i   qi   where i and i are the mean and
standard deviation  respectively  of the duration of ai in the probabilistic problem  we
write makeq  s  for the makespan of a solution s with respect to this associated deterministic
jsp  and makeq for the minimum makespan  the minimum of makeq  s  over all solutions s 
in section      it is shown  using propositions   and   and the further analysis in section
       that for certain values of q  the time value makeq is a lower bound for d  
   

fibeck   wilson

    constructive search algorithms
four constructive search based algorithms are introduced here  each of them uses constraintbased tree search as a core search technique  incorporating simulation and q values in different ways  in this section  we define each constructive algorithm in detail and then provide
a description of the heuristics and constraint propagation building blocks used by each of
them 
      b b n  an approximately complete branch and bound algorithm
given the ability to estimate the probabilistic makespan of a solution  and the ability to
test a condition that implies that a partial solution cannot be extended to a solution with
a better probabilistic makespan  an obviously applicable search technique is branch andbound  b b  where we use monte carlo simulation to derive both upper  and lower bounds
on the solution quality  if we are able to cover the entire search space  such an approach is
approximately complete  only approximately because there is always a small probability
that we miss an optimal solution due to sampling error  
the b b tree is a  rooted  binary tree  associated with each node e in the tree is a
partial solution se   which is a solution if the node is a leaf node  the empty partial solution
is associated with the root node  also associated with each non leaf node e is a pair of
activities  ai   aj   j    i  in the same resource set  whose sequence has not been determined
in partial solution se   the two nodes below e extend se   one sequences ai before aj   the
other adds the opposite sequence  the heuristic used to choose which sequence to try first
is described in section       
the value of global variable d  is always such that we have confidence  corresponding to
the choice of ksee section        that there exists a solution s whose  makespan  d   s  
is at most d    whenever we reach a leaf node  e  we find the upper estimate d     d se  
of the probabilistic makespan d  s   by monte carlo simulation based on the method of
section        we set d     min d    d     variable d  is initialized to some high value 
at non leaf nodes  e  we check to see if it is worth exploring the subtree below e  we
perform a monte carlo simulation for partial solution  se   using the current value of d   
this generates a result t   we use proposition   ii  to determine if pr make s e     d    
is k implausible given t   if it is  then we backtrack  since we can be confident that there
exists no solution extending the partial solution se that improves our current best solution 
if k is chosen sufficiently large  we can be confident that we will not miss a good solution   
we refer to this algorithm as b b n as it performs b ranch and b ound with simulation
at each n ode 
      b b dq l  an approximately complete iterative tree search
for an internal node  e  of the tree  the previous algorithm used monte carlo simulation
 but without strong propagation within each trial  to find a lower bound for the probabilistic
makespans of all solutions extending partial solution se   an alternative idea for generating
   because we are doing a very large number of tests  we need much higher confidence than for a usual
confidence interval  fortunately  the confidence associated with k is  based on the normal approximation
 
 
of a binomial  and the approximation of a tail of a normal distribution  approximately    k     e   k  
and so tends to   extremely fast as k increases 

   

fiproactive algorithms for jsp

b b dq l   
returns the solution with lowest probabilistic makespan
 
 
 
 
 
 

 
 

 s   d    findfirstb bsimleaves     
q  qinit
while q    and not timed out do
 s  d   findoptb bsimleaves d    q 
if s    n il then
s  s  d  d
end
q  q  qdec
end
return s
algorithm    b b dq l  an approximately complete iterative tree search

such a lower bound is to use the approach of section      we find the minimum makespan 
over all solutions extending se   of the associated deterministic problem based on a q value
that is  sufficient  this minimum makespan is then  see proposition    a lower bound for
the probabilistic makespan  standard constraint propagation on the deterministic durations
enables this lower bound to be computed much faster than the simulation of the previous
algorithm  at each leaf node  simulation is used as in b b n to find the estimate of the
probabilistic makespan of the solution 
this basic idea requires the selection of a q value  however  rather than parameterize
this algorithm  as we do with some others below   we choose to perform repeated tree
searches with a descending q value 
the algorithm finds an initial solution  line   in algorithm    and therefore an initial
upper bound  d    on the probabilistic makespan with q      subsequently  starting with
a high q value  one that does not result in a deterministic lower bound   we perform a
tree search  when a leaf  e  is reached  simulation is used to find d se    with such a
high q value  it is likely that the deterministic makespan makeq  se   is much greater than
d se    since we enforce the constraint that makeq  se    d se    finding d se   through
simulation causes the search to return to an interior node  i  very high in the tree such that
makeq  si    d se   where si represents the set of solutions in the subtree below node i  and
makeq  si   is the deterministic lower bound on the makespan of those solutions  with high
q values  we commonly observed in our experiments that there are only a very few nodes
that meet this criterion and  therefore  search is able to very quickly exhaust the search
space  when this happens  we reduce the q value by a small amount  qdec  e g          and
restart the tree search  eventually  and often very quickly  we reach a q value such that
there exists a full solution  se   such that makeq  se    d se    that solution is stored as the
current best and we set d    d se    as in b b n  d  is used as an upper bound on all
subsequent search 
algorithm   presents pseudocode for the basic algorithm  we make use of two functions
not defined using pseudocode 
 findfirstb bsimleaves c  q   creates a jsp with activity durations defined based on
the q value passed in and conducts a branch and bound search where monte carlo
   

fibeck   wilson

simulation is used for each leaf node and standard constraint propagation is used at
interior nodes  the first solution that is found whose probabilistic makespan is less
than c is returned with the value of its probabilistic makespan  when c is set very
high as in line    no backtracking is needed to find a solution and therefore only one
leaf node is visited and only one simulation is performed 
 findoptb bsimleaves c  q   the same as findfirstb bsimleaves c  q  except the
solution with lowest probabilistic makespan is returned rather than the first one found 
if no solution is found  a nil value is returned  unless the q value is low enough
that the deterministic makespan is a lower bound on the probabilistic makespan  this
function does not necessarily return the globally optimal solution 
we find a starting solution with q     to serve as an initial upper bound on the optimal
probabilistic makespan  in practice  b b dq l is run with a limit on the cpu time  if
q     is reached within the time limit  this algorithm is approximately complete 
as noted above  it is possible  especially with a high q value  that for a solution  se  
makeq  se   is much larger than d se    and therefore the search will backtrack to the deepest
interior node such that makeq  si    d se    in fact  the assignment of the d se   value is a
global cut as it is an upper bound on the probabilistic makespan  for technical reasons
beyond the scope of this paper  standard constraint based tree search implementations do
not automatically handle such global cuts  we therefore modified the standard behavior to
repeatedly post the upper bound constraint on makeq  si   causing a series of backtracks up
to the correct interior node 
we refer to this algorithm as b b dq l as it does a series of b ranch and b ound
searches with descending q values and where simulation is used at the leaves of the tree 
b b dq l is an example of a novel constraint based search technique that might be
useful in a wider context  when a problem has a cost function that is expensive to evaluate
but has an inexpensive  parameterizable lower bound calculation  a search based on overconstraining the problem  i e   by choosing a parameter value that will not lead to a lower
bound  and then iteratively relaxing the bounding function  may be worth investigating 
we discuss such an approach in section   
      b b tbs  a heuristic tree search algorithm
previous results with an algorithm similar to b b n  beck   wilson        indicated that
simulation was responsible for a large percentage  e g   over      of the run time  we can
reduce the number of times we require simulation by only simulating solutions that have
a very good deterministic makespan  this deterministic filtering search is the central idea
for the rest of the algorithms investigated in this paper 
a simple method of filtering solutions is to first spend some fixed amount of cpu time
to find a solution  s    with a low deterministic makespan  makeq  s     using a fixed q value
and standard constructive tree search  then  search can be restarted using the same q value
and whenever a solution  si   is found such that makeq  si    makeq  s     a simulation is run
to evaluate d si    our estimate of the probabilistic makespan  d  si    if the probabilistic
makespan found is better than the lowest probabilistic makespan so far  the solution is
stored  search is continued until the entire tree has been explored or the maximum allowed
cpu time has expired  algorithm   contains the pseudocode 
   

fiproactive algorithms for jsp

b b tbs q  
returns the solution with lowest probabilistic makespan found
 
 
 
 
 
 
 

 

 s   dinitial    findoptb b   q  tinitial  
d  
while solutions exist and not timed out do
 s  d   findnextb b dinitial      q  time remaining 
d   simulate s 
if d    d then
s  s  d  d 
end
end
return s
algorithm    b b tbs  a heuristic tree search algorithm
as with algorithm    we make use of a number of functions not defined with pseudocode 
 findoptb b c  q  t   creates a jsp with activity durations defined based on the q
value passed in and conducts a deterministic branch and bound search for up to t cpu
seconds using c as an upper bound on the deterministic makespan  when the search
tree is exhausted or the time limit is reached  the best deterministic solution found
 i e   the one with minimum makespan   together with its deterministic makespan are
returned  no monte carlo simulation is done 
 findnextb b c  q  t   this function produces a sequence of solutions  one solution
each time it is called  whose deterministic makespan is less than c  the problem is
defined using the q value and t is the cpu time limit  the solutions produced are the
leaves of the b b search tree in the order encountered by the algorithm  note that in
algorithm    the c value does not change  given enough cpu time  the algorithm will
evaluate the probabilistic makespan of all solutions whose deterministic makespan is
less than or equal to dinitial  
 simulate s   our standard monte carlo simulation is run on solution s and d s   the
estimate of its probabilistic makespan  d  s   is returned 

the algorithm is not complete  even if the choice of q value results in deterministic
makespans that are lower bounds on the probabilistic makespan  this is because there is
no guarantee that the optimal probabilistic solution will have a deterministic makespan less
than dinitial and therefore  even with infinite cpu time  it may not be evaluated 
the algorithm is called b b tbs for b ranch and b ound t imed b etter s olution  a
fixed cpu time is spent to find a good deterministic solution  and then any deterministic
solution found that is as good as or better than the initial solution is simulated 
      b b i bs  an iterative heuristic tree search algorithm
a more extreme filtering algorithm first finds an optimal deterministic solution and uses
the deterministic makespan as a filter for choosing the solutions to simulate  using a fixed
   

fibeck   wilson

b b i bs q  
returns the solution with smallest probabilistic makespan found
 
 
 
 
 
 
 
 
 

  
  

 s   dinitial    findoptb b   q  t     
d  simulate s  
i 
while not timed out do
while search is not complete do
 s  makeq    findnextb b dinitial       i           q  time remaining 
d  simulate s 
if d   d  then
s  s  d  d
end
end
ii  
end
return s
algorithm    b b i bs  an iterative heuristic tree search algorithm

q value  an optimal solution is found and then simulated  if there is cpu time remaining 
the search does a series of iterations starting by using the optimal deterministic makespan
as the bound  all solutions with a deterministic makespan as good as  or  in general  better
than  the current bound are found and simulated  in subsequent iterations  the bound
on the deterministic makespan is increased  resulting in a larger set of solutions being
simulated  the solution with the lowest estimated probabilistic makespan is returned  on
larger problems  an optimal deterministic makespan may not be found within the cpu
limit  in such a case  the best deterministic solution that is found is simulated and returned
 i e   only one simulation is done  
more formally  after finding an optimal deterministic solution with makespan  make q   
a series of iterations beginning with i     is executed  for each iteration  the bound on
deterministic makespans is set to makeq     i       all solutions  se   whose deterministic
makespans  makeq  se    makeq        i       are simulated and the one with the lowest
probabilistic makespan is returned  algorithm   presents pseudocode which depends on
functions defined above 
the algorithm is complete  when i is large enough so that the cost bound is greater
than the deterministic makespan of all activity permutations  they will all be simulated 
however  i may have to grow unreasonably large and therefore we treat this algorithm as 
practically  incomplete 
we refer to this algorithm as b b i bs for b ranch and b ound i terative b est s olution 
      heuristic and constraint propagation details
the algorithms described above use texture based heuristics to decide on the pair of activities to sequence and which sequence to try first  the heuristic builds resource profiles that
combine probabilistic estimates of the contention that each activity has for each resource
and time point  the maximum point in the resource profiles is selected and an activity
   

fiproactive algorithms for jsp

pair that contends for the resource at the selected time point is heuristically chosen  the
sequence chosen is the one that maximizes the remaining slack  the intuition is that a
pair of activities that is contending for a highly contended for resource and time point is
a critical pair of activities that should be sequenced early in the search  otherwise  via
constraint propagation from other decisions  the time windows of these activities may be
pruned to the point that neither sequence is possible  the texture based heuristics have a
complexity at each search node of o mn    where m is the number of resources and n is the
number of activities on each resource 
for a detailed description and analysis of the texture based heuristic see the work of
beck and fox        and beck        
when constraint propagation is used  i e   all algorithms above except b b n   we
use the strong constraint propagation techniques in constraint based scheduling  temporal
propagation  timetables  le pape  couronne  vergamini    gosselin         edge finder
 nuijten         and the balance constraint  laborie        
    local search algorithms
there is no reason why a deterministic filtering search algorithm needs to be based on
branch and bound  indeed  given our approach of finding and simulating only solutions
with low deterministic makespans  algorithms based on local search may perform better
than constructive search algorithms 
in this section  we present two deterministic filtering algorithms based on tabu search   
we define each algorithm and then discuss the details of the tabu search procedure itself 
      tabu tbs  a tabu search analog of b b tbs
the central idea behind using tabu search for deterministic filtering search is to generate a
sequence of promising deterministic solutions which are then simulated  it seems reasonable
to create an analog of b b tbs using tabu search  for a fixed q and for a fixed amount
tinitial of cpu time  at the beginning of a run  a solution with the lowest possible deterministic makespan  dinitial   is sought  search is then restarted and whenever a solution  s 
is found that has a deterministic makespan makeq  s   dinitial   monte carlo simulation is
used to approximate the probabilistic makespan  the solution with the lowest estimated
probabilistic makespan is returned 
algorithm   presents the pseudocode for this simple approach  we use the following
functions  pseudo code not given  
 findbesttabu c  q  t   this function is analogous to findoptb b c  q  t   tabu search is
run for up to t cpu seconds and the solution with the lowest deterministic makespan
 based on the q value  that is less than c is returned 
 findnexttabu c  q  t   this function is analogous to findnextb b c  q  t   a sequence
of solutions  one solution each time it is called  whose deterministic makespan is less
   early experiments explored an even simpler way of using tabu search to solve the probabilistic jsp by
incorporating simulation into the neighborhood evaluation  given a search state  the move operator  see
section       for details  defines the set of neighboring states  for each neighbor  we can run a monte
carlo simulation and choose the neighbor with the lowest probabilistic makespan  this technique  not
surprisingly  proved impractical as considerable cpu time was spent to determine a single move 

   

fibeck   wilson

tabu tbs q  
returns the solution with lowest probabilistic makespan found
 
 
 
 
 
 
 

 

 s   dinitial    findbesttabu   q  tinitial  
d  
while termination criteria unmet do
 s  d   findnexttabu dinitial      q  time remaining 
d   simulate s 
if d    d then
s  s  d  d 
end
end
return s
algorithm    tabu tbs  a local search filtering algorithm
than c is returned  the problem is defined using the q value and t is the cpu time
limit  the solution produced is the next solution found by the tabu search that meets
the makespan requirement 

we call this algorithm tabu tbs for tabu t imed b etter s olution 
as with b b tbs  the c value is not updated in each iteration  the initial search  line
   is used to find a good deterministic solution and simulation is done on solutions whose
deterministic makespan is better than that of the solution found by the initial search 
      tabu i bs  an iterative tabu search algorithm
the core tabu search implementation for fixed durations does not necessarily use the entire
cpu time  see section        and  in fact  especially on small instances often terminates
very quickly  we can therefore create an iterative tabu based solver for the probabilistic
jsp similar to b b i bs 
in the first phase  using a time limit that is one second less than the overall time limit 
tabu search is used to find a very good deterministic solution  based on a fixed q value 
that solution is then simulated  because the tabu search may terminate before the time
limit has expired  any remaining time is spent generating solutions with a deterministic
makespan within a fixed percentage of the initial solutions deterministic makespan  as
with b b i bs  iterations are run with increasing i value starting with i      in each
iteration  we simulate solutions found by the tabu search whose deterministic makespan is
at most      i     dinitial   where dinitial is the value of the deterministic makespan found
in phase    the solution with the lowest probabilistic makespan is returned  
the algorithm is termed tabu i bs for tabu i terative b est s earch  the pseudocode
for this algorithm is presented in algorithm   
      tabu search details
the tabu search used to find solutions to problems with deterministic durations is the tsab
algorithm due to nowicki and smutnicki         a very restricted move operator  termed
   the tabuf algorithm proposed in beck and wilson        corresponds to the first iteration of tabu i bs 

   

fiproactive algorithms for jsp

tabu i bs q  
returns the solution with smallest probabilistic makespan found
 
 
 
 
 
 
 
 
 

  
  

 s   dinitial    findbesttabu   q  t     
d  simulate s  
i 
while not timed out do
while termination criteria unmet do
 s  makeq    findnexttabu dinitial       i           q  time remaining 
d  simulate s 
if d   d  then
s  s  d  d
end
end
ii  
end
return s
algorithm    tabu i bs  an iterative tabu based filtering algorithm

n   by blazewicz  domschke and pesch        produces a neighborhood by swapping a
subset of the pairs of adjacent activities in the same resource of a given solution  a standard
tabu list of ten moves done in the immediate past is kept so as to escape local minima  we
use the standard aspiration criteria of accepting move on the tabu list only if the resulting
solution is better than any solution found so far 
one of the important additions to the basic tabu search mechanism in tsab is the
maintenance of an elite pool of solutions  these are a small set  i e      of the best solutions that have been encountered so far that is updated whenever a new best solution is
encountered  when the standard tabu search stagnates  i e   it has made a large number of
moves without finding a new best solution   search returns to one of the elite solutions and
continues search from it  that solution is removed from the set of elite solutions  search is
terminated when either the maximum cpu time is reached or when the elite solution pool
is empty 
    summary of algorithms
table   summarizes the algorithms introduced above 

   empirical investigations
our empirical investigations address two main issues  the scaling behavior of both the
approximately complete and heuristic methods as problem size and uncertainty increase
and whether using deterministic methods  which represent the uncertainty through duration
extensions  is a useful approach  with respect to scaling  there are two interesting subquestions  first  how do the approximately complete techniques compare with each other
and  second  is there a cross over point in terms of problem size above which the heuristic
techniques out perform the approximately complete techniques 
   

fibeck   wilson

deterministic
algorithm
b b

complete
yes

b b dq l

b b

yes

b b tbs

b b

no

b b i bs

b b

yes

tabu tbs

tabu

no

tabu i bs

tabu

no

name
b b n

description
b b with simulation at each node to find upper
and lower bounds
b b with deterministic durations used for lower
bounds and simulation is done at each leaf node 
the durations decrease in each iteration 
find a good deterministic solution  s  and
restart search  simulating whenever a
deterministic solution as good as s is found 
find an optimal deterministic solution  s 
restart search simulating whenever a
deterministic solution within i  of s is found
repeat with increasing i 
find a good deterministic solution  s  and
restart search simulating whenever a
deterministic solution as good as s is found 
find as good a deterministic solution  s  as
possible  restart search simulating whenever a
deterministic solution within i  of s is
found  repeat with increasing i 

table    a summary of the algorithms introduced to find the probabilistic makespan for an
instance of the job shop scheduling problem with probabilistic durations 

for the heuristic techniques it is necessary to assign fixed durations to each activity 
a standard approach is to use the mean duration  however  in such cases there is no
representation of the uncertainty surrounding that duration  and this does not take into
account that we want a high probability       of execution  a more general approach is to
heuristically use the formulation for the lower bound on  minimum makespans presented
in section      the duration of activity ai is defined to be i   qi   where q is a fixed
non negative value  and i and i are  respectively  the mean and standard deviation of
the duration of ai   since we are no longer limited to producing a lower bound  we have
flexibility in selecting q  intuitively  we want a q value that leads to a situation where
good deterministic solutions also have low values of the probabilistic makespan d   s   we
experiment with a number of q values based on the analysis in section     as shown in table
   in all cases  we set b          see section      corresponding to          value q   was
generated for each problem instance by monte carlo simulation  simulating        paths
of n activities 
    experimental details
our empirical investigations examine four sets of probabilistic jsps of size                 
             where a        problem has    jobs each consisting of    activities   and for
each set  three uncertainty levels uj                were considered  a deterministic problem
is generated using an existing generator  watson  barbulescu  whitley    howe        with
   

fiproactive algorithms for jsp

q 
 

q 
     

 n

q 
q   q 
 

     

n

q 
meanai  i 
meanai  i

q

table    the q values used in the experiments  the choices of q  and q  are motivated by
the analysis in section       

integer durations drawn uniformly from the interval          three probabilistic instances
at different levels of uncertainty are then produced by setting each mean duration  i to
be the deterministic duration of activity ai   and by randomly drawing  using a uniform
distribution  the standard deviation i of the duration of activity ai from the interval    
uj i    the distribution of each duration is approximately normal  for each problem size  we
generate    deterministic problems which are transformed into    probabilistic instances 
the problem sizes were chosen to elicit a range of behavior  from the small problems 
where the approximately complete algorithms were expected to be able to find and prove
 approximate  optimality  to the larger problems  where even the underlying deterministic
problems could not be solved to optimality within the time limit used  we chose to use
an existing generator rather than  for example  modifying existing benchmark problems 
because it allowed us to have full control over the problem structure  the three levels of
uncertainty are simply chosen to have low  medium  and high uncertainty conditions under
which to compare our algorithms 
given the stochastic nature of the simulation and the tabu search algorithm  each algorithm is run    times on each problem instance with different random seeds  each run has
a time limit of     cpu seconds  each monte carlo simulation uses n        independent
trials 
the hardware used for the experiments is a    ghz pentium   with     mb of main
memory running linux redhat    all algorithms were implemented using ilog scheduler
    
recall that for the b b dq l algorithm  we employ a descending sequence of q values 
for all problems except the        problems  the initial q value  qinit   was set to       and
the decrement  qdec   to       for the        problems  a qinit value of     was used  this
change was made after observing that with qinit         the initial tree search for the     
problems would often fail to find a solution or prove that none existed within a reasonable
amount of time  we believe this is due to problem instances of that size not having any
solution with q        that satisfied the constraint that the simulated makespan must be
less than or equal to the deterministic approximation  i e   that makeq  se    d se  see
section         but yet having a search space that is sufficiently large to require a significant
amount of search to prove it  reducing qinit to     results in an initial solution being found
quickly for all instances 
our primary evaluation criterion is the mean normalized probabilistic makespan  mnpm  
that each algorithm achieved on the relevant subset of problem instances  we display the
data for different subsets to examine algorithm performance for different problem sizes and
uncertainty levels   the mean normalized probabilistic makespan is defined as follows 
   

fibeck   wilson

mnpm  a  l   

  x d a  l 
 l 
dlb  l 

   

ll

where l is a set of problem instances  d a  l  is our mean estimate of the probabilistic
makespan found by algorithm a on l over    runs  dlb  l  is the lower bound on the probabilistic makespan for l  for all problems except         the dlb is found by solving the
deterministic problems using q    a simple  very plausibly  sufficient q value  see section
    and table     each instance was solved using constraint based tree search incorporating
the texture based heuristics and the global constraint propagation used above  a maximum
time of     cpu seconds was given  all  deterministic  problems smaller than        were
easily solved to optimality  however  none of the      problems were solved to optimality 
because of this  the dlb values were chosen to represent the best solutions found  and so
are not true lower bounds 
    results and analysis
table   presents an overview of the results of our experiments for each problem size and
uncertainty level  the results for q   q  are shown for each heuristic algorithm  there was
not a large performance difference among the non zero q values  q     q  and q     we return
to this issue in section        each cell in table   is the mean value over    independent
runs of each of    problems  aside from the      instances  all runs reached the     cpu
second time limit  therefore  we do not report cpu times 

problem
size
  
  
      
      

unc 
level
   
   
 
   
   
 
   
   
 
   
   
 

b b complete
n
dq l
             
             
      
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

algorithms
b b heuristic
tbs
i bs
     
     
     
     
     
     
     
     
     
     
     
     
           
           
           
     
     
     
     
     
     

tabu
tbs
i bs
           
           
           
     
     
     
     
           
     
     
     
     
     
     
           
           
           

table    the mean normalized probabilistic makespans for each algorithm    indicates a
set of runs for which we have  with high confidence  found approximately optimal
makespans   indicates problem sets for which normalization was done with
approximate lower bounds  the lowest mnpm found for each problem set are
shown in bold 

   

fiproactive algorithms for jsp

an impression of the results can be gained by looking at the bold entries that indicate the
lowest mean normalized probabilistic makespan  mnpm  that was found for each problem
set  b b n and b b dq l find approximately optimal solutions only for the smallest
problem set  while the b b dq l and tabu i bs find the lowest probabilistic makespans
for both the      and      problems  performance of the complete b b techniques 
especially b b n  degrade on the        problems where the heuristic b b algorithms
find the lowest probabilistic makespans  finally  on the largest problems  the tabu based
techniques are clearly superior 
one anomaly in the overall results in table   can be seen in the b b n and b b dq l
entries for the      problems  on two of the three uncertainty levels both algorithms terminate before the limit on cpu time resulting in approximately optimal solutions  however 
the mean normalized probabilistic makespans are lower for the b b dq l algorithm  we
conjecture that this is an artifact of the b b dq l algorithm that biases the simulation
toward lower probabilistic makespan values  in b b n  a particular solution  s  is only
simulated once to find d s   in b b dq l  the same solution may be simulated multiple
times leading to the bias  as an illustration  assume b b dq l finds an approximately
optimal solution s while searching the tree corresponding to q   q        on a subsequent
iteration with q   q      q     provided that the deterministic makespan is less than the previously identified probabilistic makespan  i e   makeq  s     d s     solution s will be found
again and simulated again  the actual identity of the current best solution is not used to
determine which solutions to simulate  at each subsequent simulation  if a lower value for
d s   is generated  it will replace the previous lowest probabilistic makespan value  this
leads to a situation where we may re simulate the same solution multiple times  keeping the
lowest probabilistic makespan that is found in any of the simulations  similar re simulation
is possible with the tabu i bs algorithm 
to test the statistical significance of the results in table    we ran a series of randomized
paired t tests  cohen        with p         the results of these statistical tests are
displayed in table   for the different problem sizes  the different uncertainty levels have
been collapsed so that  for example  the      statistics are based on all of the     
instances  the informal impression discussed above is reflected in these tests with b bdq l and tabu i bs dominating for the two smallest problem sizes  the branch and bound
heuristic approaches performing best for the      problems  and the tabu based techniques
delivering the best results for the        problems 
overview  our primary interpretation of the performance of the algorithms in these
experiments is as follows  for the smaller problems     and      the complete techniques
are able to cover the entire search space or at least a significant portion of it  though in the
case of b b dq l  the solutions which are chosen for simulation are heuristically driven
by deterministic makespan values  the lower bound results of section     ensure that very
good solutions will be found provided that iterations with small q values can be run within
the cpu time limit  on the        problems  the complete techniques are not able to
simulate a sufficient variety of solutions as  especially for b b n  the heuristic guidance
is poor  note  however  that b b dq l is competitive with  and  for many problems
sets  better than the tabu based algorithms on the        problems  we believe that
the        results stem from the ability of the b b heuristic algorithms to quickly find
   

fibeck   wilson

problem
size
  
  
      
      

statistical significance
 p        
 b b dq l  tabu i bs     b b tbs  b b i bs  tabu tbs  b b n 
 b b dq l  tabu i bs     b b i bs     b b tbs     tabu tbs     b b n 
 b b tbs  b b i bs     tabu i bs  b b dq l  tabu tbs     b b n 
 tabu tbs  tabu i bs     b b tbs  b b i bs     b b dq l     b b n 

table    the statistically significant relationships among the algorithms for the results
shown in table    algorithms within a set show no significant difference  the
  relation indicates that the algorithms in the left hand set have a significantly
lower mnpm than the algorithms in the right hand set  the set indicated by 
represents a more complicated relationship amongst the algorithms  tabu i bs  
tabu tbs but all other pairs in the set show no significant performance differences 

the optimal deterministic solution and then to systematically simulate all solutions with
deterministic makespans that are close to optimal  in contrast  the tabu based algorithms
do not systematically enumerate these solutions  finally  for the largest problems  we
hypothesize that tabu search techniques result in the best performance as they are able to
find better deterministic solutions to simulate 
problem size  as the size of the problems increase  we see the not unexpected decrease
in the quality of the probabilistic makespans found  a simple and reasonable explanation
for this trend is that less of the search space can be explored within the given cpu time for
the larger problems  there are likely to be other factors that contribute to this trend  e g  
the quality of the lower bound may well systematically decrease as problem size increases  
uncertainty level  the normalized makespan values also increase within a problem size
as the uncertainty level rises  as these results are calculated by normalization against a
lower bound  it is possible that the observed decrease in solution quality is actually due to
a decrease in the quality of the lower bound rather than a reduction in the quality of the
solutions found by the algorithms as uncertainty increases  to test this idea  in table   we
normalized the      results using the optimal probabilistic makespans found by b b n
rather than the deterministic lower bound  the table shows that for the algorithms apart
from b b dq l and tabu i bs  the trend of increasing mean normalized probabilistic
makespan is still evident  for these algorithms  at least  the putative decreasing quality
of the lower bound cannot be the entire explanation for the trend of worse performance
results for higher levels of uncertainty  in section        we revisit this question and provide
evidence that could explain why the algorithms perform worse when uncertainty is increased 
these results also lend credibility to the conjecture that the observed super optimal
performance of b b dq l and tabu i bs on the small problems is due to repeatedly
simulating the same solution  at low levels of uncertainty  repeated simulations of the truly
best solution will not vary greatly  resulting in a mnpm value of about    with higher levels
of uncertainty  the distribution of simulated makespans is wider and  therefore  repeated
simulation of the same solution biases results toward smaller probabilistic makespan values 
this is what we observe in the results of b b dq l and tabu i bs in table   
   

fiproactive algorithms for jsp

unc 
level
   
   
 

b b complete
n
dq l
     
     
     
     
     
     

algorithms
b b heuristic
tbs
i bs
     
     
     
     
     
     

tabu
tbs
i bs
           
           
           

table    the mean normalized probabilistic makespans for each algorithm on the     
problem set normalized by the optimal probabilistic makespans found by b b n 

in the balance of this section  we turn to more detailed analysis of the algorithms 
      analysis  b b complete algorithms
the performance of b b n is poor when it is unable to exhaustively search the branchand bound tree  the high computational cost of running simulation at every node and the
relatively weak lower bound that partial solutions provide  conspire to result in a technique
that does not scale beyond very small problems 
problem
size
  
  
      
      

uncertainty
   
   
 
 
 
   
         
   
   

level
 
 
    
   
   

table    the lowest q value used for each problem size and uncertainty level for the b bdq l  for all problems except         the initial q value is       for the       
problems  the initial q value is    

b b dq l is able to perform somewhat better than b b n on larger problems even
when it is not able to exhaustively search each tree down to q      table   shows the
minimum q values attained for each problem size and uncertainty level  the deterministic
durations defined by the q value serve to guide and prune the search for each iteration and 
therefore  as with the heuristic algorithms  see below   the search is heuristically guided
to the extent that solutions with low deterministic makespans also have low probabilistic
makespans  however  the characteristics of the solutions found by the search are unclear 
recall that b b dq l starts with a high q value that  in combination with the constraint
that the deterministic makespan must be less than or equal to the best simulated probabilis   one idea for improving the lower bound that we did not investigate is to incorporate the resourcebased propagators  e g   edge finding  into the evaluation of a partial solution  in a single trial at an
internal node  the deterministic makespan is found by sampling from the distributions and then finding
the longest path in the temporal network  after the sampling  however  it is possible to apply the
standard propagation techniques which might insert additional edges into the precedence graph and
thereby increase the makespan  improving the lower bound 

   

fibeck   wilson

tic makespan found so far  significantly prunes the search space  ideally  we would like the
search with high q to find solutions with very good probabilistic makespans both because
we wish to find good solutions quickly and because the simulated probabilistic makespan
values are used to prune subsequent search with lower q values  therefore  in an effort
to better understand the b b dq l search  we examine the characteristics of the initial
solutions it finds 
some idea of the quality of the solutions produced by high q values can be seen by
comparing the probabilistic makespan found with high q  the first solution found  with
the best solution found in that run  table   presents this comparison in the form of d f  
the mean normalized makespans for the initial solutions found by b b n and b b dql  these data indicate that the first solution found by b b dq l is much better than
that found by b b n  when b b n searches for its initial solution  the upper bound
on the deterministic makespan does not constrain the problem  a solution is therefore
very easy to find  i e   with no backtracking  but there is little constraint propagation or
heuristic information available to guide the search to a solution with a small makespan  in
contrast  when b b dq l searches for an initial solution  the high q value means that it
is searching in a highly constrained search space because the deterministic makespan must
be less than the probabilistic makespan  therefore  there is a very tight upper bound on
the deterministic makespan  relative to the durations that incorporate the q values   in
many cases  the initial iterations fail to find any feasible solutions  but do so very quickly 
eventually  the q value is low enough to allow a feasible solution  however the search for that
solution is strongly guided by propagation from the problem constraints  in summary  the
initial search for b b n has no guidance from the constraint propagation toward a good
solution while that of b  dq l is guided by constraint propagation in an overly constrained
problem  table   shows that  in these experiments  such guidance tends to result in better
initial solutions  we believe that this observation may be useful more generally in constraint
solving  see section    
to provide a fuller indication of the performance differences  table   also presents the
improvement over the first solution that is achieved  the difference between the first solution
and the last solution  dl   found by each algorithm  dl is the value reported in table    
on the larger problem sets  the improvement made on the first solution by b b dq l
is greater  for the smaller problem sets  the improvement by b b n is greater than for
b b dq l  however  we suspect a ceiling effect reduces the amount that b b dq l can
improve  i e   the initial solutions are already quite close to optimal  
      analysis  heuristic algorithms
we now turn to the performance of the heuristic algorithms  we first examine the hypothesis
that their performance is dependent on two factors  the ability of the algorithms to find
solutions with low deterministic makespans and the correlation between good deterministic
and probabilistic makespans  then we turn to an analysis of the effect of the differing q
values on the heuristic algorithm performance 
finding good deterministic makespans  it was argued above that the performance
of the heuristic techniques  and b b dq l  is dependent upon the ability to find solutions
with good deterministic makespans  to provide evidence for this argument  we looked
   

fiproactive algorithms for jsp

problem
size
  
  
      
      

unc 
level
   
   
 
   
   
 
   
   
 
   
   
 

b b n
df
df  d l
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

b b dq l
df
df  d l
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    the mean normalized makespan for the first solutions found by each algorithm
 df   and the difference between the mean normalized makespans of the first and
last solutions  df  dl   

at the quality of the best deterministic solutions found by b b i bs and tabu i bs  we
hypothesize that the better performing algorithm will also have found better deterministic
solutions than the worse performer 
table   presents results for each algorithm on the two largest problem sets     the mean
normalized deterministic makespan  mndm   is calculated as follows 
mndm  a  l   

makeq  a  l 
  x
 l 
makeq min  l  b b  i  bs 

   

ll

where l is a set of problem instances  makeq  a  l  is the mean deterministic makespan found
by algorithm a on l over    runs  makeq min  l  b b  i  bs  is the lowest deterministic
makespan found by the b b i bs algorithm over all runs on problem l  mndm  therefore 
provides a relative measure of the quality of the average deterministic makespans from the
two algorithms  the higher the value  the worse the average makespan found relative to
b b i bs 
table   is consistent with our hypothesis  on the        problems  where b b i bs
outperforms tabu i bs  the former is able to find solutions with a lower mean deterministic
makespan  for the        problems the results are reversed with tabu i bs finding both
better mean deterministic makespans and better probabilistic makespans 
this result lends support to the original motivation for the deterministic filtering algorithms  the performance of these algorithms in terms of probabilistic solution quality is
positively related to the quality of the deterministic solutions they are able to find  the
next section addresses the question of why this performance relationship is observed 
    we show only the        and        problems sets as they are not influenced by the conjectured
repeated simulation behavior of tabu i bs 

   

fibeck   wilson

problem
size
      
      

uncertainty
level
   
   
 
   
   
 

mndm
b b i bs tabu i bs
     
     
     
     
     
     
     
     
     
     
     
     

table    the mean normalized deterministic makespan  mndm  for b b i bs and tabui bs 

the correlation between deterministic and probabilistic makespan  the ability of the algorithms to find good deterministic makespans would be irrelevant to their
ability to find good probabilistic makespans without some correlation between the two  it
is reasonable to expect that the level of uncertainty in a problem instance has an impact on
this correlation  at low uncertainty the variations in duration are small  meaning that we
can expect the probabilistic makespan to be relatively close to the deterministic makespan 
when the uncertainty level is high  the distribution of probabilistic makespans for a single
solution will be wider  resulting in less of a correlation  we hypothesize that this impact of
uncertainty level contributes to the observed performance degradation  see tables   and   
of the heuristic techniques with higher uncertainty levels as problem size is held constant 
to examine our hypothesis we generated     new        deterministic jsp problem
instances with the same generator and parameters used above  the standard deviations
for the duration of each activity in the     instances were generated independently for
each of five uncertainty levels uj                      resulting in a total of     problem
instances      for each uncertainty level   for each instance and for each of the four q
values  as in table     we then randomly generated     deterministic solutions which were
then simulated  using the r statistical package  r development core team         we
measured the correlation coefficient for each problem set  each cell in table   is the result
of       pairs of data points  the deterministic and probabilistic makespans for     random
deterministic solutions for each of     problem instances 
uncertainty level
   
   
 
 
 

q 
      
      
      
      
      

q 
      
      
      
      
      

q 
      
      
      
      
      

q 
      
      
      
      
      

table    the correlation coefficient  r  comparing pairs of deterministic and probabilistic makespans for a set of        probabilistic jsps  each cell represents the
correlation coefficient for       deterministic  probabilistic pairs 

   

fiproactive algorithms for jsp

table   supports our explanation for the performance of the heuristic techniques  as
the uncertainty level increases  the correlation between the deterministic makespan and
corresponding probabilistic makespan lessens  the strength of the correlation is somewhat
surprising  even for the highest uncertainty level where the standard deviation of the duration of an activity is uniformly drawn from between   and   times its mean duration 
the correlation is above      for q  and q    this is a positive indication for the heuristic
algorithms as it suggests that they may scale well to higher uncertainty levels provided a
reasonable q value is used  we examine the impact of the q values in the original experiments and the implications of the deterministic probabilistic makespan correlation in the
next section 
it should be emphasized that these results are based on correlations between deterministic and probabilistic makespans for randomly generated solutions  we have not addressed
how these correlations might change for high quality solutions  which might be considered
as a more appropriate population from which to sample  one technical difficulty for the
design of an experiment to examine this  is to ensure a sufficiently randomized sample from
the population of good solutions  also  the result could depend strongly on the  rather
arbitrary  particular choice of quality cutoff for solutions 
the effect of the q values  each of the heuristic algorithms requires a fixed q value   
we experimented with four different values  see table     table    displays the significant pairwise differences among the q values for each heuristic as measured by randomized
paired t tests  cohen        with p         as can be observed  there are almost no
significant differences for low levels of uncertainty      or      or for the smallest problem
set  for higher levels of uncertainty and larger problems  using q  is never better than
using one of the higher q values and in many cases  q  results in the worst mean makespan 
among the other q values  for the majority of the problem sets and algorithms there are no
significant differences  for a given algorithm  it is never the case that a lower q value leads
to significantly better results than a higher q value 
the correlation results in table   provide an explanation for these differences  for the
       problems  the performance of the q  algorithms is competitive when there is not
a large difference in the correlations between deterministic and probabilistic solutions  i e  
at uncertainty levels     and       when the uncertainty level is    there is a significant
reduction in the correlation coefficient for q  and a corresponding reduction in the mean
normalized probabilistic makespans found by the algorithms using q   
    summary
the results of our experiments can be summarized as follows 
 the most principled use of simulation  b b n  is only useful on small problems 
the simulation time is a major component of the run time resulting in very little
exploration of the search space 
 algorithm b b dq l  based on the idea of iteratively reducing a parameter that determines the validity of the lower bound  results in equal performance on small prob    we are not addressing the behavior of b b dq l  where the q descends during the run of the algorithm 
we are only examining the algorithms with fixed q values 

   

fibeck   wilson

problem
size
  

  

      

      

unc 
level
   
   
 
all
   
   
 
all
   
   
 
all
   
   
 
all

b b
tbs
i bs
q     q    q      q 
q     q    q   
 q    q    q      q 
 q    q    q      q 
 q    q    q      q 
 q    q    q      q 
 q    q      q 
 q    q      q 
q    q  
q    q    q 
 q    q      q 
q    q  

tabu
tbs
i bs
q     q 
 q    q    q      q 
 q    q    q      q 
q    q  
 q    q      q 
 q    q      q 
q    q  
 q    q    q      q 
q     q 
 q    q    q      q   q    q    q      q 
 q    q    q      q 

 q    q    q      q 

table     the results of pair wise statistical tests for each algorithm and problem set  the
notation a   b indicates that the algorithm using q   a achieved a significantly
better solution  i e   lower probabilistic makespan  than when it used q   b   
indicates no significant differences  all statistical tests are randomized paired t
tests  cohen        with p        

lems and much better performance on larger problems when compared to b b n 
more work is needed to understand the behavior of the algorithm  however preliminary evidence indicates that it is able to find good solutions quickly in the current
application domain 
 a series of heuristic algorithms were proposed based on using deterministic makespan
to filter the solutions which would be simulated  it was demonstrated that the performance of these algorithms depends on their ability to find good deterministic
makespans and the correlation between the quality of deterministic and probabilistic solutions  it was shown that even for problems with a quite high uncertainty
level  deterministic problems can be constructed that lead to a strong deterministic probabilistic makespan correlation 
 central to the success of the heuristic algorithms was the use of a q value that governed
the extent to which duration uncertainty was represented in the durations of activities
in deterministic problems  it was shown that such an incorporation of uncertainty
data leads to a stronger correlation between deterministic and probabilistic makespans
and the corresponding ability to find better probabilistic makespans 
   

fiproactive algorithms for jsp

   extensions and future work
in this section  we look at three kinds of extensions of this work  first  we show that our
theoretical framework in fact applies to far more general probabilistic scheduling problems
than just job shop scheduling  in section      we discuss ways in which the algorithms
for probabilistic jsp presented in this paper might be improved  finally  we discuss the
possibility of developing the central idea in the b b dq l algorithm into a solving approach
for general constraint optimization problems 
    generalization to other scheduling problems
the results in this paper have been derived for the important case of job shop scheduling
problems  in fact  they are valid for a much broader class of scheduling problems  including
resource constrained project scheduling problems of a common form  e g   a probabilistic
version of the deterministic problems studied in the work of laborie         in this section 
we describe how to extend our framework and approaches 
our approach relies on the fact that in the job shop scheduling problem  one can focus
on orderings of activities  rather than directly on assignments of start times for activities 
specifically  the definition of minimum makespan based on orderings is equivalent to the
one based on start time assignments  this equivalence holds much more generally 
first  in        we give some basic definitions and properties which are immediate extensions of those defined in section    then  in        we characterize a class of scheduling
problems which have the properties we require  by use of a logical expression to represent
the constraints of the problem  in       we give the key result relating the schedule based
minimum makespan with the ordering based minimum makespan  section       discusses
the extended class of probabilistic scheduling problems  and section       considers different
optimization functions 
      schedules  orderings and makespans
as in section    we are given a set a of activities  where activity ai  a has an associated positive duration di  for the deterministic case   a schedule  for a  is defined to
be a function from the set of activities to the set of time points  which are non negative
numbers   defining when each activity starts  let z be a schedule  the makespan make z 
of schedule z is defined to be the time at which the last activity has been completed  i e  
maxai a  z ai     di    we say that z orders ai before aj if and only if aj starts no earlier
than ai ends  i e   z ai     di  z aj   
an essential aspect of job shop problems and our approach is that one can focus on
orderings of the activities rather than on schedules  in section   we use the term solution
for an ordering that satisfies the constraints of a given jsp  define an ordering  on a  to be
a strict partial order on a  i e   an irreflexive and transitive relation on the set of activities 
hence  for ordering s  for all ai  a   ai   ai   
  s  and if  ai   aj    s and  aj   ak    s 
then  ai   ak    s  if  ai   aj    s  then we say that s orders ai before aj   we also say that
ai is a predecessor of aj   a path in s  or an s path  is a sequence of activities such that
if ai precedes aj in the sequence  then s orders ai before aj   the length len   of a path
  in an ordering  is defined to be the sum of the durations of the activities in the path 
   

fibeck   wilson

p
i e   ai  di   the makespan  make s   of an ordering s is defined to be the length of a
longest s path  an s path  is said to be a critical s path if the length of  is equal to the
makespan of the ordering s  i e   it is one of the longest s paths 
any schedule has an associated ordering  for schedule z define the ordering sol z  as
follows  sol z  orders activity ai before aj if and only if z orders ai before aj  
conversely  from an ordering one can define a non delay schedule  which is optimal
among schedules compatible with the ordering  by starting each activity as soon as its
predecessors finish  let s be an ordering  we inductively define schedule z   sched s  as
follows  if ai has no predecessor  then we start ai at time    i e   z ai        otherwise 
we set z ai     maxaj pred ai    z aj     dj    where pred ai   is the set of predecessors of ai  
the fact that s is acyclic guarantees that this defines a schedule  as in section      we have
the following two important properties  the first states that the makespan of an ordering
is equal to the makespan of its associated schedule  the second states that the makespan
of a schedule is no better than the makespan of its associated ordering 
proposition  
 i  for any ordering s  make sched s     make s  
 ii  for any schedule z  make sol z    make z  
the proof of these is straight forward  it follows easily by induction that if a schedule z
respects the precedence constraints expressed by an ordering s  then the last activity of any
s path can end no earlier in z than the length of the path  applying this to a critical path
implies  ii  make sol z    make z   and implies half of  i   make sched s    make s   by
working backwards from an activity that finishes last in sched s   and choosing an immediate
predecessor at each stage  one generates  in reverse order  a path in s whose length is equal
to make sched s    hence showing that make sched s    make s   and proving  i  
      positive precedence expressions
we will define a class of scheduling problems  using what we call positive precedence expressions  ppes  to represent the constraints  each of these scheduling problems assumes
no preemption  so activities cannot be interrupted once started  and we will again use
makespan as the cost function 
for activities ai and aj   the expression before i  j  is interpreted as the constraint  on
possible schedules  that activity aj starts no earlier than the end of activity ai   such
expressions are called primitive precedence expressions  a positive precedence expression is
defined to be a logical formula built from primitive precedence expressions  conjunctions and
disjunctions   the term positive is used since they do not involve negations   formally 
the set e of positive precedence expressions  over a  is defined to be the smallest set such
that  a  e contains before i  j  for each ai and aj in a  and  b  if  and  are in e  then
     and      are both in e 
positive precedence expressions over a are interpreted as constraining schedules on a 
let   e be a ppe and let z be a schedule  we define z satisfies  recursively as
follows 
   

fiproactive algorithms for jsp

 z satisfies primitive precedence expression before i  j  if and only if z orders a i before
aj   i e   if z ai     di  z aj   
 z satisfies the conjunction of two constraint expressions if and only if it satisfies both
of them 
 z satisfies the disjunction of two constraint expressions if and only if it satisfies at
least one of them 
similarly  for ordering s and positive precedence expression  we can recursively define
s satisfies  in the obvious way  s satisfies before i  j  if and only if s orders a i before
aj   ordering s satisfies      if and only if it satisfies both  and   ordering s satisfies
     if and only if it satisfies either  or  
positive precedence expressions are powerful enough to represent the constraints of a
job shop scheduling problem  or of a resource constrained project scheduling problem 
jsps as positive precedence expressions  resource constraints in a job shop scheduling problem give rise to disjunctions of primitive precedence expressions  for each pair of
activities ai and aj which require the same resource  the expression before i  j   before j  i 
which expresses that ai and aj do not overlap  one of them precedes the other   the
ordering of activities in a job can be expressed in terms of primitive expressions  before i  j 
when ai precedes aj within some job  hence  the constraints in a job shop problem can be
expressed as a positive precedence expression in conjunctive normal form  i e   a conjunction
of disjunctions of primitive precedence expressions 
rcpsps as ppes  the constraints in a resource constrained project scheduling problem
 rcpsp   pinedo        brucker et al         laborie   ghallab        laborie        can
also be expressed as a positive precedence expression in conjunctive normal form  in an
rcpsp  we have precedence constraints between activities  each of which can be expressed
as a primitive precedence expression  let  be the conjunction of these  in a rcpsp  there
are again a set of resources  each with a positive capacity  associated with each activity a i
and resource r is the rate of usage ai  r  of resource r by activity ai   we have the following
resource constraints on a schedule  for each resource r  at any time point t  the sum of
ai  r  over all activities ai which are in progress at t  i e   which have started by t but not
yet ended  must not exceed the capacity of resource r 
define a forbidden set  or conflict set  to be a set of activities whose total usage of
some resource exceeds the capacity of the resource  let f be the set of forbidden sets   if
we wished  we could delete from f any set which is a superset of any other set in f  we
could also delete any set h which contains elements ai and aj such that ai precedes aj
according to    the resource constraints can be expressed equivalently as  for all h  f 
there exists no time at which every activity in h is in progress  this holds if and only if
for each h  f  there exist two activities in h which do not overlap  since if all pairs of
activities in h overlap then all activities in h are in progress at the latest start time of
activities in h   i e   there exists ai   aj  h with before i  j   hence  a schedule satisfies the
resource constraints if and only if it satisfies the positive precedence expression  defined
   

fibeck   wilson

to be
 

hf

 

before i  j  

ai  aj h
i  j

therefore  expression      represents the rcpsp  i e   a schedule satisfies the constraints
of the rcpsp if and only if it satisfies      
another class of scheduling problems  each of which can be represented by a positive
precedence expression  is the class based on and or precedence constraints  gillies  
liu        mohring  skutella    stork        
      solutions and minimum makespan
for a fixed positive precedence expression  over a  we say that schedule z is valid if it
satisfies   we say that ordering s is a solution if it satisfies   if ordering s satisfies
before i  j   then  by construction  sched s  satisfies before i  j   it also follows immediately
that schedule z satisfies before i  j  if and only if sol z  satisfies before i  j   the following
result can then be proved easily by induction on the number of connectives in  
lemma   for any ppe  over a  if s is a solution  then sched s  is a valid schedule  if
z is a valid schedule  then sol z  is a solution 
the minimum makespan  for   is defined to be the infimum makespan over all valid
schedules  i e   the infimum of make z  over all valid schedules z  the minimum solution
makespan is defined to be the minimum makespan over all solutions  i e   the minimum
of make s  over all solutions s  the following is the key result which links the schedulebased definition of minimum makespan with the solution based definition  it follows from
proposition   and lemma    since for any solution s there is a valid schedule  i e   sched s  
with the same value of makespan  and for any valid schedule z there is a solution  i e  
sol z   with at least as good a value of makespan 
proposition   let  be any positive precedence expression over a  then the minimum
makespan for  is equal to the minimum solution makespan 
      probabilistic scheduling problems based on ppes
the probabilistic versions of the scheduling problems are defined in just the same way as
for jsps  the duration of each activity ai is now a random variable  a positive precedence
expression is used to represent the constraints 
the further definitions in sections   and   can all be immediately extended to this much
more general setting  all the results of the paper still hold  with exactly the same proofs 
in particular  with a probabilistic problem one associates a corresponding deterministic
problem in just the same way  the lower bound results in section     are based on the longest
path characterization of makespan  the monte carlo approach  or at least its usefulness 
relies on the fact that the makespan of a solution is equal to the makespan of the associated
schedule  furthermore  the algorithms in section   extend  given that one has a method of
solving the corresponding deterministic problem 
   

fiproactive algorithms for jsp

the ordering based policies that we use  based on fixing a partial ordering of activities 
irrespective of the sampled values of the durations  are known as earliest start policies
 radermacher         these and other policies have been studied for rcpsps  see e g  
stork        however the aim in that work is to minimize expected makespan  whereas we
are attempting to minimize  makespan  
      different optimization functions
because our approach for evaluating and comparing solutions is based on the use of monte
carlo simulation to generate a sample distribution  our techniques are quite general 
much of the work in the paper also generalizes immediately to other regular cost functions  where regular means that the function is monotonic in the sense that increasing the
end of any activity in a schedule will not decrease the cost  a regular function based on any
efficiently computable measurement of the sample distributions can be accommodated  for
example  we could easily adapt to situations where the probability of extreme solutions is
important by basing the optimization function on the maximum sampled makespan  conversely  we could use measures of the tightness of the makespan distribution for situations
where minimizing variance as a measure of the accuracy of a schedule is important  furthermore  weighted combinations of such functions  e g   the  makespan plus a measure of
distribution tightness  could be easily incorporated 
we can also modify our approach to account for other ways of comparing solutions
based on the sample distributions  for example  we could perform t tests using the sample
distributions to determine if one solution has a significantly lower expected makespan 
    toward better algorithms for probabilistic jsps
there are two directions for future work on the algorithms presented in this paper  first 
b b n could be improved to make more use of deterministic techniques and or to incorporate probabilistic reasoning into existing deterministic techniques  for example  a number
of deterministic lower bound formulations for pert networks exist in the operations research literature  ludwig  mohring    stork        that may be used to evaluate partial
solutions  similarly  perhaps the dominance rules presented by daniels and carrillo       
for the one machine  robustness problem can be generalized to multiple resources  another
approach to improving the b b n performance is to incorporate explicit reasoning about
probability distributions into standard constraint propagation techniques  techniques such
as the longest path calculations and edge finding make inferences based on the propagation
of minimum and maximum values for temporal variables  we believe that many of these
techniques can be adapted to reason about probabilistic intervals  this is related to work
done  for example  on simple temporal networks with uncertainty  morris  muscettola   
vidal        tsamardinos        
a second direction for future work is the improvement of the heuristic algorithms  the
key advantage of these algorithms is that they make use of deterministic techniques for
scheduling  by transforming probabilistic problems into deterministic problems  we bring a
significant set of existing tools to bear on the problem  further developments of this approach include adaptively changing q values during the search in order to find those that lead
to solutions with better values of probabilistic makespan  d  s    a deeper understanding
   

fibeck   wilson

of the relationship between good deterministic solutions and good probabilistic solutions 
building on the work here  is necessary to pursue this work in a principled fashion 
of course  proactive techniques are not sufficient  in practice  schedules are dynamic
and need to be adapted as new jobs arrive or existing jobs are canceled  at execution time  a
reactive component is necessary to deal with unexpected  or sufficiently unlikely  disruptions
that  nonetheless  can occur  a complete solution to scheduling under uncertainty needs to
incorporate all these elements to reason about uncertainty at different levels of granularity
and under different time pressures  see the work of bidot  vidal  laborie and beck       
for recent work in this direction 
    exploiting unsound lower bounds in constraint programming
the b b dq l algorithm may represent a problem solving approach that can be applied
beyond the current application area  if we abstract away the probabilistic jsp application 
the central idea of b b dq l is to exploit an unsound lower bound to  over constrain the
search and then to run subsequent searches with a gradually relaxed unsound lower bound 
such an approach may play to the strengths of constraint programming  searching within
highly constrained spaces 
for example  the assignment problem  ap  is a well known lower bound for the traveling
salesman problem  tsp  and has been used as a cost based constraint in the literature
 focacci  lodi    milano        rousseau  gendreau  pesant    focacci         given a
tsp  p   let ap  p  q  be the corresponding assignment problem with the travel distances
multiplied by q  that is  let dij be the distance between cities i and j in p and let d ij be
the distance between cities i and j in ap  p  q   then d ij   dij  q for q     an approach
similar to that of the b b dq l algorithm can now be applied to solve the tsp 
it would be interesting to investigate how the approach compares with the traditional
optimization approach in constraint programming  it may be particularly useful in applications where the evaluation of partial solutions is very expensive but where there exists a
parameterizable  inexpensive lower bound 

   conclusion
in this paper  we addressed job shop scheduling when the durations of the activities are
independent random variables  a theoretical framework was created to formally define this
problem and to prove the soundness of two algorithm components  monte carlo simulation
to find upper bounds on the probabilistic makespan of a solution and a partial solution 
and a carefully defined deterministic jsp whose optimal makespan is a lower bound on the
probabilistic makespan of the corresponding probabilistic jsp 
we then used these two components together with either constraint programming or
tabu search to define a number of algorithms to solve probabilistic jsps  we introduced
three solution approaches  a branch and bound technique using monte carlo simulation to
evaluate partial solutions  an iterative deterministic search using monte carlo simulation
to evaluate the solutions from a series of increasingly less constrained problems based on
a parameterizable lower bound  and a number of deterministic filtering algorithms which
generate a sequence of solutions to a deterministic jsp  each of which is then simulated
using monte carlo simulation 
   

fiproactive algorithms for jsp

our empirical evaluation demonstrated that the branch and bound technique is only
able to find approximately optimal solutions for very small problem instances  the iterative
deterministic search performs as well as  or better than  the branch and bound approach
for all problem sizes  however  for medium and large instances  the deterministic filtering
techniques perform much more strongly while providing no optimality guarantees  further
experimentation demonstrated that for the techniques using deterministic methods  the
correlation between the deterministic makespan and probabilistic makespan is a key factor
in algorithm performance  taking into account the variance of the duration in a deterministic
problem led to strong correlations and good algorithmic performance 
proactive scheduling techniques seek to incorporate models of uncertainty into an offline  predictive schedule  the goal of such techniques is to increase the robustness of the
schedules produced  this is important because a schedule is not typically generated or
executed in isolation  other decisions such as when to deliver raw materials and how to
schedule up  and down stream factories are all affected by an individual schedule  indeed  a
schedule can be seen as a locus of competing constraints from across a company and supply
chain  fox         differences between a predictive schedule and its execution can be a
significant source of disruption leading to cascading delays across widely separated entities 
the ability  therefore  to develop schedules that are robust to uncertainty is very important 
this paper represents a step in that direction 

acknowledgments
this work has received support from the science foundation ireland under grants    pi   c   
and    in i     the natural sciences and engineering research council of canada  and
ilog  sa  the authors would like to thank daria terekhov and radoslaw szymanek for
comments on previous versions of the paper  preliminary versions of the work reported in
this paper have been published in beck and wilson              

references
beck  j  c          texture measurements as a basis for heuristic commitment techniques
in constraint directed scheduling  ph d  thesis  university of toronto 
beck  j  c     fox  m  s          dynamic problem structure analysis as a basis for
constraint directed scheduling heuristics  artificial intelligence                
beck  j  c     wilson  n          job shop scheduling with probabilistic durations  in
proceedings of the sixteenth european conference on artificial intelligence  ecai    
pp         
beck  j  c     wilson  n          proactive algorithms for scheduling with probabilistic durations  in proceedings of the nineteenth international joint conference on artificial
intelligence  ijcai     pp           
bidot  j          a general framework integrating techniques for scheduling under uncertainty  ph d  thesis  ecole nationale dingeieurs de tarbes 
   

fibeck   wilson

bidot  j   vidal  t   laborie  p     beck  j  c          a general framework for scheduling
in a stochastic environment  in proceedings of the twentieth international joint
conference on artificial intelligence  ijcai     pp       
blazewicz  j   domschke  w     pesch  e          the job shop scheduling problem  conventional and new solution techniques  european journal of operational research 
            
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  journal of artificial intelligence research 
        
brucker  p   drexl  a   mohring  r   neumann  k     pesch  e          resource constrained
project scheduling  notation  classification  models and methods  european journal
of operational research           
burns  a   punnekkat  s   littlewood  b     wright  d          probabilistic guarantees for fault tolerant real time systems  tech  rep  deva tr no      design for validation  esprit long term research project no         available at
http   www fcul research ec org deva 
burt  j  m     garman  m  b          monte carlo techniques for stochastic network analysis  in proceedings of the fourth annual conference on applications of simulation 
pp         
cohen  p  r          empirical methods for artificial intelligence  the mit press  cambridge  mass 
daniels  r     carrillo  j           robust scheduling for single machine systems with
uncertain processing times  iie transactions             
davenport  a  j   gefflot  c     beck  j  c          slack based techniques for robust
schedules  in proceedings of the sixth european conference on planning  ecp       
davenport  a     beck  j  c          a survey of techniques for scheduling with uncertainty 
tech  rep   available at  http   www tidel mie utoronto ca publications php 
drummond  m   bresina  j     swanson  k          just in case scheduling  in proceedings
of the twelfth national conference on artificial intelligence  aaai      pp      
      menlo park  ca  aaai press mit press 
feller  w          an introduction to probability theory and its applications  third edition   john wiley and sons  new york  london 
focacci  f   lodi  a     milano  m          a hybrid exact algorithm for the tsptw 
informs journal on computing                 
fox  m  s          constraint directed search  a case study of job shop scheduling  ph d 
thesis  carnegie mellon university  intelligent systems laboratory  the robotics institute  pittsburgh  pa  cmu ri tr      
gao  h          building robust schedules using temporal protectionan empirical study
of constraint based scheduling under machine failure uncertainty  masters thesis 
department of industrial engineering  university of toronto 
   

fiproactive algorithms for jsp

garey  m  r     johnson  d  s          computers and intractability  a guide to the theory
of np completeness  w h  freeman and company  new york 
ghosh  s          guaranteeing fault tolerance through scheduling in real time systems 
ph d  thesis  university of pittsburgh 
ghosh  s   melhem  r     mosse  d          enhancing real time schedules to tolerate
transient faults  in real time systems symposium 
gillies  d  w     liu  j  w  s          scheduling tasks with and or precedence constraints  siam j  comput              
hagstrom  j  n          computational complexity of pert problems  networks     
       
herroelen  w     leus  r          project scheduling under uncertainty  survey and research
potentials  european journal of operational research                  
laborie  p          algorithms for propagating resource constraints in ai planning and
scheduling  existing approaches and new results  artificial intelligence              
laborie  p          complete mcs based search  application to resource constrained
project scheduling  in proceedings of the nineteenth international joint conference
on artificial intelligence  ijcai     pp         
laborie  p     ghallab  m          planning with sharable resource constraints  in proceedings of the fourteenth international joint conference on artificial intelligence
 ijcai    
le pape  c   couronne  p   vergamini  d     gosselin  v          time versus capacity
compromises in project scheduling  in proceedings of the thirteenth workshop of the
uk planning special interest group 
leon  v  j   wu  s  d     storer  r  h          robustness measures and robust scheduling
for job shop  iie transactions               
ludwig  a   mohring  r     stork  f          a computational study on bounding the
makespan distribution in stochastic project networks  annals of operations research 
          
meuleau  n   hauskrecht  m   kim  k   peshkin  l   kaelbling  l   dean  t     boutilier  c 
        solving very large weakly coupled markov decision processes  in proceedings
of the fifteenth national conference on artificial intelligence  aaai     
mohring  r   skutella  m     stork  f          scheduling with and or precedence constraints  siam j  comput                 
morris  p   muscettola  n     vidal  t          dynamic control of plans with temporal
uncertainty  in proceedings of the seventeenth international joint conference on
artificial intelligence  ijcai     
nowicki  e     smutnicki  c          a fast taboo search algorithm for the job shop problem 
management science                 
nuijten  w  p  m          time and resource constrained scheduling  a constraint satisfaction approach  ph d  thesis  department of mathematics and computing science 
eindhoven university of technology 
   

fibeck   wilson

pinedo  m          scheduling  theory  algorithms  and systems   nd edition   prenticehall 
r development core team         r  a language and environment for statistical computing 
r foundation for statistical computing  vienna  austria  isbn               
radermacher  f  j          scheduling of project networks  annals of operations research 
          
rousseau  l   gendreau  m   pesant  g     focacci  f          solving vrptws with
constraint programming based column generation  annals of operations research 
            
stork  f          branch and bound algorithms for stochastic resource constrained project
scheduling  tech  rep            technische universitat berlin  department of mathematics 
tsamardinos  i          a probabilistic approach to robust execution of temporal plans with
uncertainty  in methods and applications of artificial intelligence  proceedings of the
second hellenic conference on artificial intelligence  vol       of lecture notes in
artificial intelligence  pp        
watson  j  p   barbulescu  l   whitley  l     howe  a          contrasting structured
and random permutation flow shop scheduling problems  search space topology and
algorithm performance  informs journal on computing         
wilson  n          algorithms for dempster shafer theory  in  kohlas  j   moral  s  
 eds   algorithms for uncertainty and defeasible reasoning  volume    handbook of
defeasible reasoning  kluwer academic publishers 
wurman  p     wellman  m          optimal factory scheduling using stochastic dominance
a   in proceedings of the twelfth conference on uncertainty in artificial intelligence
 uai     

   

fi