journal of artificial intelligence research                 

submitted        published     

adaptive load balancing  a study in multi agent
learning
andrea schaerf

aschaerf dis uniroma  it

dipartimento di informatica e sistemistica
universita di roma  la sapienza   via salaria      i       roma  italy

yoav shoham

robotics laboratory  computer science department
stanford university  stanford  ca        usa

moshe tennenholtz

faculty of industrial engineering and management
technion  haifa        israel

shoham flamingo stanford edu
moshet ie technion ac il

abstract
we study the process of multi agent reinforcement learning in the context of load balancing in a distributed system  without use of either central coordination or explicit communication  we first define a precise framework in which to study adaptive load balancing 
important features of which are its stochastic nature and the purely local information
available to individual agents  given this framework  we show illuminating results on the
interplay between basic adaptive behavior parameters and their effect on system eciency 
we then investigate the properties of adaptive load balancing in heterogeneous populations 
and address the issue of exploration vs  exploitation in that context  finally  we show that
naive use of communication may not improve  and might even harm system eciency 

   introduction
this article investigates multi agent reinforcement learning in the context of a concrete
problem of undisputed importance   load balancing  real life provides us with many examples of emergent  uncoordinated load balancing  trac on alternative highways tends to
even out over time  members of the computer science department tend to use the most powerful of the networked workstations  but eventually find the lower load on other machines
more inviting  and so on  we would like to understand the dynamics of such emergent
load balancing systems and apply the lesson to the design of multi agent systems 
we define a formal yet concrete framework in which to study the issues  called a multiagent multi resource stochastic system  which involves a set of agents  a set of resources 
probabilistically changing resource capacities  probabilistic assignment of new jobs to agents 
and probabilistic job sizes  an agent must select a resource for each new job  and the
eciency with which the resource handles the job depends on the capacity of the resource
over the lifetime of the job as well as the number of other jobs handled by the resource over
that period of time  our performance measure for the system aims at globally optimizing
the resource usage in the system while ensuring fairness  that is  a system shouldn t be made
ecient at the expense of any particular agent   two common criteria for load balancing 
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fischaerf  shoham    tennenholtz

how should an agent choose an appropriate resource in order to optimize these measures 
here we make an important assumption  in the spirit of reinforcement learning  sutton 
       the information available to the agent is only its prior experience  in particular 
the agent does not necessarily know the past  present  or future capacities of the resources  
and is unaware of past  current  or future jobs submitted by the various agents  not even
the relevant probability distributions  the goal of each agent is thus to adapt its resourceselection behavior to the behavior of the other agents as well as to the changing capacities
of the resources and to the changing load  without explicitly knowing what they are 
we are interested in several basic questions 

 what are good resource selection rules 
 how does the fact that different agents may use different resource selection rules affect
the system behavior 

 can communication among agents improve the system eciency 
in the following sections we show illuminating answers to these questions  the contribution of this paper is therefore twofold  we apply multi agent reinforcement learning to the
domain of adaptive load balancing and we use this basic domain in order to demonstrate
basic phenomena in multi agent reinforcement learning 
the structure of this paper is as follows  in section   we discuss our general setting 
the objective of this section is to motivate our study and point to its impact  the formal
framework is defined and discussed in section    section   completes the discussion of this
framework by introducing the resource selection rule and its parameters  which function as
the  control knobs  of the adaptive process  in section   we present experimental results
on adaptive behavior within our framework and show how various parameters affect the
eciency of adaptive behavior  the case of heterogeneous populations is investigated in
section    and the case of communicating populations is discussed in section    in section  
we discuss the impact of our results  in section   we put our work in the perspective of
related work  finally  in section    we conclude with a brief summary 

   the general setting

this paper applies reinforcement learning to the domain of adaptive load balancing  however  before presenting the model we use and our detailed study  we need to clarify several
points about our general setting  in particular  we need to explain the interpretation of
reinforcement learning and the interpretation of load balancing we adopt 
much work has been devoted in the recent years to distributed and adaptive load balancing  one can find related work in the field of distributed computer systems  e g   pulidas 
towsley    stankovic        mirchandaney   stankovic        billard   pasquale       
glockner   pasquale        mirchandaney  towsley    stankovic        zhou        eager 
lazowska    zahorjan         in organization theory and management science  e g   malone 
   in many applications the capacities of the resources are known  at least to some extent  this point will
be discussed later  basically  in this paper we wish to investigate how far one can go using only purely
local feedback and without the use of any global information  kaelbling        sutton        

   

fiadaptive load balancing  a study in multi agent learning

       and in distributed ai  e g   bond   gasser         although some motivations of
the above mentioned lines of research are similar  the settings discussed have some essential
differences 
work on distributed computer systems adopts the view of a set of computers each of
which controls certain resources  has an autonomous decision making capability  and jobs
arrive to it in a dynamic fashion  the decision making agents of the different computers
 also called nodes  try to share the system load and coordinate their activities by means of
communication  the actual action to be performed  based on the information received from
other computers  may be controlled in various ways  one of the ways adopted to control
the related decisions is through learning automata  narendra   thathachar        
in the above mentioned work each agent is associated with a set of resources  where both
the agent and the related resources are associated with a node in the distributed system 
much work in management science and in distributed ai adopts a somewhat complementary
view  in difference to classical work in distributed operating systems  an agent is not
associated with a set of resources that it controls  the agents are autonomous entities which
negotiate among themselves  zlotkin   rosenschein        kraus   wilkenfeld        on
the use of shared resources  alternatively  the agents  called managers in this case  may
negotiate the task to be executed with the processors which may execute it  malone        
the model we adopt has the avor of models used in distributed ai and organization
theory  we assume a strict separation between agents and resources  jobs arrive to agents
who make decisions about where to execute them  the resources are passive  i e   do not
make decisions   a typical example of such a setting in a computerized framework is a set
of pcs  each of which is controlled by a different user and submits jobs to be executed on
one of several workstations  the workstations are assumed to be independent of each other
and shared among all the users  the above example is a real life situation which motivated
our study and the terminology we adopt is taken from such a framework  however  there
are other real life situations related to our model in areas different from classical distributed
computer systems 
a canonical problem related to our model is the following one  arthur         an agent 
embedded in a multi agent system  has to select among a set of bars  or a set of restaurants  
each agent makes an autonomous decision but the performance of the bar  and therefore of
the agents that use it  is a function of its capacity and of the number of agents that use it 
the decision of going to a bar is a stochastic process but the decision of which bar to use is
an autonomous decision of the respective agent  a similar situation arises when a product
manager decides which processor to use in order to perform a particular task  the model we
present in section   is a general model where such situations can be investigated  in these
situations a job arrives to an agent  rather than to a node consisting of particular resources 
who decides upon the resource  e g   restaurant  where his job should be executed  there is
a priori no association between agents and resources 
we now discuss the way the agents behave in such a framework  the common theme
among the above mentioned lines of research is that load balancing is achieved by means
of communication among active agents or active resources  through the related decisionmaking agents   in our study we adopt a complementary view  we consider agents who
act in a purely local fashion  based on purely local information as described in the recent
reinforcement learning literature  as we mentioned  learning automata were used in the
   

fischaerf  shoham    tennenholtz

field of distributed computer systems in order to perform adaptive load balancing  nevertheless  the related learning procedures rely heavily on communication among agents  or
among decision making agents of autonomous computers   our work applies recent work
on reinforcement learning in ai where the information the agent gets is purely local  hence 
an agent will know how ecient the service in a restaurant has been only by choosing it as a
place to eat  we don t assume that agents may be informed by other agents about the load
in other restaurants or that the restaurants will announce their current load  this makes
our work strictly different from other work applying reinforcement learning to adaptive load
balancing 
the above features make our model and study both basic and general  moreover  the
above discussion raises the question of whether reinforcement learning  based on purely
local information and feedback  can guarantee useful load balancing  the combination of
the model we use and our perspective on reinforcement learning makes our contribution
novel  nevertheless  as we mentioned above  and as we discuss in section    the model we
use is not original to us and captures many known problems and situations in distributed
load balancing  we apply reinforcement learning  as discussed in the recent ai literature 
to that model and investigate the properties of the related process 

   the multi agent multi resource stochastic system

in this section we define the concrete framework in which we study dynamic load balancing 
the model we present captures adaptive load balancing in the general setting mentioned
in section    we restrict the discussion to discrete  synchronous systems  and thus the
definition below will refer to n   the natural numbers   similar definitions are possible in
the continuous case  we concentrate on the case where a job can be executed using any of
the resources  although somewhat restricting  this is a common practice in much work in
distributed systems  mirchandaney   stankovic        

definition     a multi agent multi resource stochastic system is a   tuple ha  r  p   d  c 
sri  where a   fa          an g is a set of agents  r   fr          rm g is a set of resources 
p   a  n          is a job submission function  d   a  n     is a probabilistic job size
function  c   rn     is a probabilistic capacity function  and sr is a resource selection
rule 

the intuitive interpretation of the system is as follows  each of the resources has a
certain capacity  which is a real number  this capacity changes over time  as determined by
the function c   at each time point each agent is either idle or engaged  if it is idle  it may
submit a new job with probability given by p   each job has a certain size which is also
a real number  the size of any submitted job is determined by the function d   we will
use the unit token where referring to job sizes and resource capacities  but we do not mean
that tokens come only in integer quantities   for each new job the agent selects one of the
resources  this choice is made according to the rule sr  since there is much to say about
this rule  we discuss it separately in the next section 
in our model  any job may run on any resource  furthermore  there is no limit on the
number of jobs served simultaneously by a given resource  and thus no queuing occurs  
however  the quality of the service provided by a resource at a given time deteriorates with
   

fiadaptive load balancing  a study in multi agent learning

the number of agents using it at that time  specifically  at every time point the resource
distributes its current capacity  i e   its tokens  equally among the jobs being served by it 
the size of each job is reduced by this amount and  if it drops to  or below  zero  the job is
completed  the agent is notified of this  and becomes idle again  thus  the execution time
of a job j depends on its size  on the capacity over time of the resource processing it  and
on the number of other agents using that resource during the execution of j  
our measure of the system s performance will be twofold  we aim to minimize timeper token  averaged over all jobs  as well as to minimize the standard deviation of this
random variable  minimizing both quantities will ensure overall system eciency as well as
fairness  the question is which selection rules yield ecient behavior  so we turn next to
the definition of these rules 

   adaptive resource selection rules
the rule by which agents select a resource for a new job  the selection rule  sr   is the
heart of our adaptive scheme and the topic of this section  throughout this section and
the following one we make an assumption of homogeneity  namely  we assume that all
the agents use the same sr  notice that although the system is homogeneous  each agent
will act based only on its local information  in sections   and   we relax the homogeneity
assumption and discuss heterogeneous and communicating populations 
as we have already emphasized  among all possible adaptive srs we are interested in
purely local srs  ones that have access only to the experience of the particular agent  in our
setting this experience consists of results of previous job submissions  for each job submitted
by the agent and already completed  the agent knows the name r of the resource used  the
point in time  tstart   the job started  the point in time  tstop   the job was finished  and the
job size s   therefore  the input to the sr is  in principle  a list of elements in the form
 r  tstart  tstop   s    notice that this type of input captures the general type of systems we
are interested in  basically  we wish to assume as little as possible about the information
available to an agent in order to capture real loosely coupled systems where more global
information is unavailable 
whenever agent i selects a resource for its job execution  i may get its feedback after
non negligible time  where this feedback may depend on decisions made by other agents
before and after agent i s decision  this forces the agent to rely on a non trivial portion of
its history and makes the problem much harder 
there are uncountably many possible adaptive srs and our aim is not to gain exhaustive understanding of them  rather  we have experimented with a family of intuitive and
relatively simple srs and have compared them with some non adaptive ones  the motivation for choosing our particular family of srs is partially due to observations made by
cognitive psychologists on how people tend to behave in multi agent stochastic and recurrent situations  in principle  our set of srs captures the two most robust aspects of these
observations   the law of effect   thronkide        and the  power law of practice   blackburn         in our family of rules  called 
  which partially resembles the learning rules
discussed in the learning automata literature  narendra   thathachar         and partially resembles the interval estimation algorithm  kaelbling         agents do not maintain
complete history of their experience  instead  each agent  a  condenses this history into
   

fischaerf  shoham    tennenholtz

a vector  called the eciency estimator  and denoted by eea   the length of this vector is
the number of resources  and the i th entry in the vector represents the agent s evaluation
of the current eciency of resource i  specifically  eea  r  is a positive real number   this
vector can be seen as the state of a learning automaton  in addition to eea   agent a keeps
a vector jda  which stores the number of completed jobs which were submitted by agent a
to each of the resources  since the beginning of time  thus  within 
  we need only specify
two elements 
   how agent a updates eea when a job is completed
   how agent a selects a resource for a new job  given eea and jda
loosely speaking  eea will be maintained as a weighted sum of the new feedback and the
previous value of eea   and the resource selected will most probably be the one with highest
eea entry except that with low probability some other resource will be chosen  these two
steps are explained more precisely in the following two subsections 

    updating the eciency estimator
we take the function updating eea to be

eea  r     wt        w  eea  r 
where t represents the time per token of the newly completed job and is computed from
the feedback  r  tstart  tstop  s   in the following way  

t    tstop   tstart  s
we take w to be a real value in the interval         whose actual value depends on jda r  
this means that we take a weighted average between the new feedback value and the old
value of the eciency estimator  where w determines the weights given to these pieces of
information  the value of w is obtained from the following function 

w   w        w  jda r 
in the above formula w is a real valued constant  the term      w  jda r  is a correcting
factor  which has a major effect only when jda  r  is low  when jda  r  increases  reaching
a value of several hundreds  this term becomes negligible with respect to w 

    selecting the resource

the second ingredient of adaptive srs in 
 is a function pda selecting the resource for a
new job based on eea and jda   this function is probabilistic  we first define the following
function
 
if jda r     
a  r  n
 
pda r     ee
 
n
e  ee  
if jd  r     
a

a

   using parallel processing terminology  t can be viewed as a stretch factor  which quantifies the stretching
of a program s processing time due to multiprogramming  ferrari  serazzi    zeigner        

   

fiadaptive load balancing  a study in multi agent learning

where n is a positive real valued parameter and e  eea  represents the average of the values
of eea  r  over all resources satisfying jda  r       to turn this into a probability function 
we define the pda as the normalized version of pd a  

pda r     pd a r  
where    rpd a  r  is a normalization factor  
the function pda clearly biases the selection towards resources that have performed
well in the past  the strength of the bias depends on n  the larger the value of n  the
stronger the bias  in extreme cases  where the value of n is very high  e g         the agent
will always choose the resource with the best record  this strategy of  always choosing
the best   although perhaps intuitively appealing  is in general not a good one  it does not
allow the agent to exploit improvements in the capacity or load on other resources  we
discuss this sr in the following subsection  and expand on the issue of exploration versus
exploitation in sections   and   
to summarize  we have defined a general setting in which to investigate emergent load
balancing  in particular  we have defined a family of adaptive resource selection rules 
parameterized by a pair  w  n   these parameters serve as knobs with which we tune the
system so as to optimize its performance  in the next section we turn to experimental
results obtained with this system 

    the best choice sr  bcsr 

the best choice sr  bcsr  is a learning rule that assumes a high value of n  i e  which
always chooses the best resource in a given point  we will assume w is fixed to a given
value while discussing bcsr  in our previous work  shoham   tennenholtz              
we showed that learning rules that strongly resemble bcsr are useful for several natural
multi agent learning settings  this suggests that we need to carefully study it in the case
of adaptive load balancing  as we will demonstrate  bcsr is not always useful in the load
balancing setting 
the difference between bcsr and a learning rule where the value of n is low  is that
in the latter case the agent gives relatively high probability for the selection of a resource
that didn t give the best results in the past  in that case the agent might be able to notice
that the behavior of one of the resources has been improved due to changes in the system 
note that the exploration of  non best  resources is crucial when the dynamics of the
system includes changes in the capacities of the resources  in such cases  the agent could not
take advantage of possible increases in the capacity of resources if it uses the bcsr  one
might wonder  however  whether in cases where the main dynamic changes of the system
stem from load changes  relying on bcsr is sucient  if the latter is true  we will be
able to ignore the parameter n and to concentrate only on the bcsr  in systems where
the capacity of resources is fixed  in order to clarify this point  we consider the following
example 
   if for all r we have jda  r        i e   if the agent is going to submit its very first job   then we assume
the agent chooses a resource randomly  with a uniform probability distribution  

   

fischaerf  shoham    tennenholtz

suppose there are only two resources  r  and r    whose respective  fixed  capacities 
cr  and cr    satisfy the equality cr     cr    assume now that the load of the system varies

between a certain low value and a certain high one 
if the system s load is low and the agents adopt bcsr  then the system will evolve in
a way where almost all of the agents would be preferring r  to r   this is due to the
fact that  in the case of low load  there are only few overlaps of jobs  hence r  is much
more ecient  on the other hand  when the system s load is high  r  could be very busy
and some of the agents would then prefer r   since the performance obtained using the
less crowded resource r  could be better than the one obtained using the overly crowded
resource r   in the extreme case of a very high load  we expect the agents to use r  one
third of the time 
assume now that the load of the system starts from a low level  then increases to a
high value  and then decreases to reach its original value  when the load increases  the
agents  that were mostly using r   will start observing that r  s performance is becoming
worse and  therefore  following the bcsr they will start using r  too  now  when the load
decreases  the agents which were using r  will observe an improvement in the performance
of r   but the value they have stored for r   i e   eea       will still reect the previous
situation  hence  the agents will keep on using r   ignoring the possibility of obtaining
much better results if they moved back to r   in this situation  the randomized selection
makes the agents able to use r   with a certain probability  and therefore some of them
may discover that the performance of r  is better than that of r  and switch back to r  
this will improve the system s eciency in a significant manner 
the above example shows that the bcsr is  in the general case  not a good choice 
this is in general true when the value of n is too high 
in the above discussion we have assumed that the changes in the load are unforeseen  if
we are able to predict the changes in the load  the agents can simply use the bcsr while
the load is fixed and then use a low value of n during the changes  in our case  instead 
without even realizing that the system has changed in some way  the agents would need to
 and  as we will see  would be able to  adapt to dynamic changes as well as to each other 

   experimental results
in this section we compare srs in 
 to each another  as well as to some non adaptive 
benchmark selection rules 
the non adaptive srs we consider in this paper are those in which the agents partition
themselves according to the capacities and the load of the system in a fixed predetermined
manner and each agent uses always the same resource  later in the paper  a sr of this
kind is identified by a configuration vector  which specifies  for each resource  how many
agents use it  when we test our adaptive srs  we compare the performance against the nonadaptive srs that perform best on the particular problem  this creates a highly competitive
set of benchmarks for our adaptive srs 
in addition  we compare our adaptive srs to the load querying sr which is defined as
follows  each agent  when it has a new job  asks all the resources how busy they are and
always chooses the less crowded one 
   

fiadaptive load balancing  a study in multi agent learning

    an experimental setting
we now introduce a particular experimental setting  in which many of the results described
below were obtained  we present it in order to be concrete about the experiments  however 
the qualitative results of our experiments were observed in a variety of other experimental
settings 
one motivation of our particular setting stems from the pcs and workstations problem
mentioned in section    for example  part of our study is related to a set of computers
located at a single site  these computers have relatively high load with some peak hours
during the day and a low load at night  i e   the chances a user of a pc submits a job
is higher during the day time of the week days than at night and on weekend   another
part of our study is related to a set of computers split all around the world  where the
load has quite random structure  i e   due to difference in time zones  users may use pcs in
unpredictable hours  
another motivation of our particular setting stems from the restaurant problem mentioned in section    for discussion on the related  bar problem  see arthur         for
example  we can consider a set of snack bars located at an industrial park  these snack
bars have relatively high loads with some peak hours during the day and low load at night
 i e   the chances an employee will choose to go to a snack bar is higher during the day
because there are more employees present during the day   conversely  we can assume a
set of bars near an airport where the load has quite random structure  i e   the airport
employees may like to use these snack bars in quite unpredicted hours  
although these are particular real situations  we would like to emphasize the general
motivation of our study and the fact that the related phenomena have been observed in
various different settings 
we take n   the number of agents  to be      and m   the number of resources  to be
   in the first set of experiments we take the capacities of the resources to be fixed  in
particular  we take them to be c        c        c        c        c        we assume
that all agents have the same probability of submitting a new job  we also assume that all
agents have the same distribution over the size of jobs they submit  specifically  we assume
it to be a uniform distribution over the integers in the range          
for ease of exposition  we will assume that each point in time corresponds to a second 
and we consequently count the time in minutes  hours  days  and weeks  the hour is our
main point of reference  we assume  for simplicity  that the changes in the system  i e   load
change and capacity change  happen only at the beginning of a new hour  the probability
of submitting a job at each second  which corresponds to the load of the system  can vary
over time  this is the crucial factor to which the agents must adapt  note that agents can
submit jobs at any second  but the probability of such submission may change  in particular
we concentrate on three different values of this quantity  called llo   lhi and lpeak   and we
assume that the system load switches between those values  the actual values of llo   lhi and
lpeak in the following quantitative results are            and     which roughly correspond
to each agent submitting            and    jobs per hour  per agent  respectively 
   

fischaerf  shoham    tennenholtz

load

configuration
time per token
llo
f               g
      
lhi
f                g
      
lpeak f                  g
       
figure    best non adaptive srs for fixed load
in the following  when measuring success  we will refer only to the average time pertoken   however  the adaptive srs that give the best average time per token were also
found to be fair 

    fixed load

we start with the case in which the load is fixed  this case is not the most interesting for
adaptive behavior  however  a satisfactory sr should show reasonably ecient behavior in
that basic case  in order to be useful when the system stabilizes 
we start by showing the behavior of non adaptive benchmark srs in the case of fixed
load   figure   shows those that give the best results  for each of the three loads 
as we can see  there is a big difference between the three loads mentioned above  when
the load is particularly high  the agents should scatter around all the resources at a rate
proportional to their capacities  when the load is low they should all use the best resource 
given the above  it is easy to see that an adaptive sr can be effective only if it enables
moving quickly from one configuration to the other 
in a static setting such as this  we can expect the best non adaptive srs to perform better than adaptive ones  since the information gained by the exploration of the adaptive srs
can be built in in the non adaptive ones  the experimental results confirm this intuition 
as shown in figure   for lhi   the figure shows the performance obtained by the population
when the value of n varies between   to    and for three values of w            and     
note that for the values of  n  w  that are good choices in the dynamic cases  see later in
the paper  values in the intervals        and             respectively   the deterioration in the
performance of the adaptive srs with respect to the non adaptive ones is small  this is an
encouraging result  since adaptive srs are meant to be particularly suitable for dynamic
systems  in the following subsections we see that indeed they are 

    changing load

we now begin to explore more dynamic settings  here we consider the case in which the
load on the system  that is  the probability of agents submitting a job at any time  changes
over time  in this paper we present two dynamic settings  one in which the load changes
according to a fixed pattern with only a few random perturbations and another in which the
load varies in some random fashion  specifically  in the first case we fix the load to be lhi
   in the data shown later we refer  for convenience  to the time for      tokens 
   the non adaptive srs are human designed srs that are used as benchmarks  they assume knowledge of
the load and capacity  which is not available for the adaptive srs we design 

   

fiadaptive load balancing  a study in multi agent learning

a
v
e
r
a
g
e
t
i
m
e
p
e
r
t
o
k
e
n

 
  





 weight  w      
weight  w      
 weight  w      

  





  
  





  
  
  
 



 

 


 


















 
   
   
 
    
exponent of the randomization function  n

 

figure    performance of the adaptive selection rules for fixed load
for ten consecutive hours  for five days a week  with two randomly chosen hours in which
it is lpeak   and to be llo for the rest of the week  in the second case  we fix the number
of hours in a week for each load as in the first case  and we distribute them completely
randomly in a week 
the results obtained for the two cases are similar  figure   shows the results obtained
by the adaptive srs in the case of random load  the best non adaptive deterministic
sr gives the time per token value of        obtained with the configuration  partition of
agents  f                g  the adaptive srs are superior  the load querying sr instead gets
the time per token value of         which is obviously better  but is not so far from the
performances of the adaptive srs 
we also observe the following phenomenon  given a fixed n  resp  a fixed w  the average
time per token is non monotonic in w  resp  in n   this phenomenon is strongly related to
the issue of exploration versus exploitation mentioned before and to phenomena observed
in the study of q learning  watkins        
we also notice how the two parameters n and w interplay  in fact  for each value of
w the minimum of the time per token value is obtained with a different value of n  more
precisely  the higher w is the lower n must be in order to obtain the best results  this means
that  in order to obtain high performance  highly exploratory activity  low n  should be
matched with giving greater weight to the more recent experience  high w   this  parameter
   

fischaerf  shoham    tennenholtz

a
v
e
r
a
g
e
t
i
m
e
p
e
r
t
o
k
e
n

 
  
  
  


 weight  w      




weight  w      
 weight  w      













  



 

    






  
  
  
 













 
   
   
 
    
exponent of the randomization function  n

 

figure    performance of the adaptive selection rules for random load
matching  can be intuitively explained in the following qualitative way  the exploration
activity pays because it allows the agent to detect changes in the system  however  it is
more effective if  when a change is detected  it can significantly affect the eciency estimator
 i e   if w is high   otherwise  the cost of the exploration activity is greater than its gain 

    changing capacities
we now consider the case in which the capacity of the resources can vary over time  in
particular  we will demonstrate our results in the case of the previously mentioned setting 
we will assume the capacities rotate randomly among the resources and  in five consecutive
days  each resource gets the capacity of    for one day     for   days  and    for the other
  days   the load also varies randomly 
the results of this experiment are shown in figure    the best non adaptive sr
in this case gives the time per token value of         obtained with the configuration
f                  g   the adaptive srs give much better results  which are only slightly
   usually the capacities will change in a less dramatic fashion  we use the above mentioned setting in
order to demonstrate the applicability of our approach under severe conditions 
   the load querying sr gives the same results as in the case of fixed capacities  because such sr is
obviously not inuenced by the change 

   

fiadaptive load balancing  a study in multi agent learning

a
v
e
r
a
g
e
t
i
m
e
p
e
r
t
o
k
e
n

 




    




  





    



    

    




  




  

    



 








 weight  w      

weight  w      
 weight  w      




 

 
 
 
     
    
exponent of the randomization function  n

figure    performance of the adaptive selection rules for changing capacities
worse than in the case of fixed capacities  the phenomena we mentioned before are visible
in this case too  see for example how a weight of     mismatches with the low values of n 

   heterogeneous populations
throughout the previous section we have assumed that all the agents use the same sr  i e 
homogeneity assumption  such assumption models the situation in which there is a sort
of centralized off line controller which  in the beginning  tells the agents how to behave and
then leaves the agents to make their own decisions 
the situation described above is very different from having an on line centralized controller which makes every decision  however  we would like now to move even further from
that and investigate the situation in which each agent is able to make its own decision about
which strategy to use and  maybe  adjust it over time 
as a step toward the study of systems of this kind  we drop the homogeneity assumption
and consider the situation in which part of the population uses one sr and the other part
uses a second one 
in the first set of experiments  we consider the setting discussed in subsection     and
we confront one with the other  two populations  called   and    of the same size     agents
each   each population uses a different sr in 
  the sr of population i  for i         will
   

fischaerf  shoham    tennenholtz



a
v
e
r
a
g
e
t
i
m
e
p
e
r
t
o
k
e
n

 



  
  



  























  
  





 


  

  
  


   t 
   t 



   
   
   
 
    
exponent of the randomization function  n   

 

figure    performance of   populations of    agents with n      and w    w       
be determined by the pair of parameters  wi  ni    the measure of success of population i
will be defined as the average time per token of its members  and will be denoted by ti  
figure   shows the result obtained for w    w         and n       and for different
values of n    in the case of randomly varying load 
our results expose the following phenomenon  the two populations obtain different
outcomes from the ones they obtain in the homogeneous case  more specifically  for   
n       the results obtained by the agents which use n  are generally better than the results
obtained by the ones which use n    despite the fact that an homogeneous population which
uses n  gets better results than an homogeneous population which uses n   
the phenomenon described above has the following intuitive explanation  for n  in
the above mentioned range  the population which uses n  is less  exploring   i e   more
 exploiting   than the other one  and when it is left on its own it might not be able to
adapt to the changes in a satisfactory manner  however  when it is joined with the other
population  it gets the advantages of the experimental activity of agents in that population 
without paying for it  in fact  the more exploring agents  in trying to unload the most
crowded resources  make a service to the other agents as well 
it is worth observing in figure   that when n  is low  e g   n      the agents that use
n  take the role of explorers and lose a lot  while the agents that use n  gain from that
situation  conversely  for high values of n   e g   n      the performances of the exploiters 
   

fiadaptive load balancing  a study in multi agent learning

a
v
e
r
a
g
e
t
i
m
e
p
e
r
t
o
k
e
n



 
  
  



  


   t 
   t 

 





 


  
  
  










  
 
















 
   
   
 
    
exponent of the randomization function  n 

 

figure    performance of   populations of       agents with n      and w    w       
which use n    deteriorate  this means that if the exploiters are too static  then they hinder
each other  and the explorers can take advantage of it 
for a better understanding of the phenomena involved  we have experimented with an
asymmetric population  composed of one large group and one small one  instead of two
groups of similar size  figure   shows the results obtained using a setting similar to the
one above  but where population   is composed of    members while population   consists
of only    members  in this case  for every value of n      the exploiters do better than
the explorers  the experiments also show that in this case  the higher n  is the better t 
is  i e  the more the exploiters exploit  the more they gain 
the above results suggest that a single agent gets the best results for itself by being noncooperative and always adopting the resource with the best performance  i e   use bcsr  
given that the rest of the agents use an adaptive  i e   cooperative  sr  however  if all of
the agents are non cooperative then all of them will lose   in conclusion  the selfish interest
of an agent does not match with the interest of the population  this is contrary to results
obtained in other basic contexts of multi agent learning  shoham   tennenholtz        
what we have shown is how  for a fixed value of w  coexisting populations adopting
different values of n interact  similar results are obtained when we fix the value of n and
   this is in fact an illuminating instance of the well known prisoners dilemma  axelrod        

   

fischaerf  shoham    tennenholtz

a
v
e
r
a
g
e
t
i
m
e
p
e
r
t
o
k
e
n

 
  




   t 
   t 

  
  
  




















  

























  
  

 

                                        
weight of the estimator parameter  w  

 

figure    performance of   populations of    agents with n    n      and w       
use two different values for w  in such cases  the agents adopting the lower value of w are
in general the winners  as shown in figure   for n    n      and w         when w is very
low then the corresponding agents get poor results and they are no longer the winners  as
in the case of very high n in figure   
another interesting phenomenon is obtained when confronting adaptive agents with
load querying agents  load querying agents are agents who are able to consult the resources
about where they should submit their jobs  a load querying agent will submit its job to the
most unloaded resource at the given point  when confronting load querying agents with
adaptive ones  the results obtained by the adaptive agents are obviously worse than the
results obtained by the load querying ones  but are better than the results obtained by a
complete population of adaptive agents  this means that load querying agents do not play
the role of  parasites   as the above mentioned  exploiters   the load querying agents help
in maintaining the load balancing among the resources  and therefore help the rest of the
agents  another result we obtain is that agents who adopt deterministic srs may behave
as parasites and worsen the performance of adaptive agents 
these assertions are supported by the experiments described in figure    where a population of    agents  each of which uses an adaptive sr with parameters  n  w   is faced with
a minority of    agents which use different srs  as stated above  in particular  in the four
cases we consider  the minority behaves in the following ways   i  they choose the resource
   

fiadaptive load balancing  a study in multi agent learning

   agents
   agents
t 
      
       
      
      
      
      
       load querying       
      
using res          

t 

      
      
      
      

figure    performance of   populations of       agents with various srs
which gave best results   ii  they are very conservative in updating the history   iii  they
are load querying agents   iiii  they all use deterministically the resource with capacity   
 in our basic experimental setting  

   communication among agents
up to this point  we have assumed that there is no direct communication among the agents 
the motivation for this was that we considered situations in which there were absolutely
no transmission channels and protocols  this assumption is in agreement with the idea of
multi agent reinforcement learning  in systems where massive communication is feasible
we are not so much concerned with multiple agent adaptation  and the problem reduces to
supplying satisfactory communication mechanisms  multi agent reinforcement learning is
most interesting where real life forces agents to act without a priori arranged communication channels and we must rely on action feedback mechanisms  however  it is of interest to
understand the effects of communication on the system eciency  as in shoham   tennenholtz        tan         where the agents are augmented with some sort of communication
capabilities  our study of this extension led to some illuminating results  which we will now
present 
we assume that each agent can communicate only with some of the other agents  which
we call its neighbors  we therefore consider a relation neighbor of and assume it is reexive  symmetric and transitive  as a consequence  the relation neighbor of partitions the
population into equivalence classes  that we call neighborhoods 
the form of communication we consider is based on the idea that the eciency estimators of agents within a neighborhood will be shared among them when a decision is made
 i e   when an agent chooses a resource   the reader should notice that this is a naive
form of communication and that more sophisticated types of communication are possible 
however  the above form of communication is most natural when we concentrate on agents
that update their behavior based only on past information  in particular  this type of
communication is similar to the ones used in the above mentioned work on incorporating
communication into the framework of multi agent reinforcement learning 
we suppose that different srs may be used by different agents in the same population 
but we impose the condition that within a single neighborhood  the same sr is used by all
its members 
we also assume that each agent keeps its own history and updates it by itself in the
usual way  the choice  instead  is based not only on the agent eciency estimator  but on
   

fischaerf  shoham    tennenholtz

a
v
e
r
a
g
e
t
i
m
e
p
e
r
t
o
k
e
n

 
  
  














  





  



  

 

  
 














  









  






     cns of    agents
     cns of   agents
      cns of   agents

 
   
   
 
    
exponent of the randomization function  n

 

figure    performance of the adaptive selection rules for random load profile for communicating agents
the average of the eciency estimators of the agents in the corresponding neighborhood 
such average is called the neighborhood eciency estimator  the neighborhood eciency
estimator has no physical storage  its value is recalculated each time a member needs it 
in order to compare the behavior of communicating agents and non communicating ones 
we assume that in a single population there might be  aside from the neighborhoods defined
above  also some neighborhoods that do not allow the sharing of eciency estimators among
its members  the members of these neighborhoods behave as described in the previous
sections  i e   each agent relies only on its own history  the only thing that is common
among the members of such a neighborhood is that all its members use the same sr 
we call communicating neighborhood  cn   a neighborhood in which the eciency estimators are shared when a decision is taken and non communicating neighborhood  ncn  
a neighborhood in which this is not done 
the first set of experiments we ran  regards a population composed of only cns  all
of the same size  in particular  we considered cns of various sizes  starting from    cns
of size    going to   cns of size     the load profile exploited is the random load change
defined in subsection      the value of w is taken to be      and n is taken to have various
values  the results obtained are shown in figure   
   

fiadaptive load balancing  a study in multi agent learning

the results show that such communicating populations do not get good results  the
reason for this is that members of a cn tend to be very conservative  in the sense that they
mostly use the best resource  in fact  since they rely on an average of several agents  the
picture they have of the system tends to be much more static  in particular  the bigger is
the cn the more conservative its members tend to be  for example  consider the values of
 n  w  that give the best results for non communicating agents  those values give quite bad
performance for cns since they turn to be too conservative 
using more adaptive values of  n  w   the behavior of a communicating population improves and reaches a performance that is just slightly worse than the performance of a
non communicating population  tuning the parameters using a finer grain  it is possible to
obtain a performance that is equal to the one obtained by a non communicating population 
however  it seems clear that no obvious gain is achieved from this form of communication
capability  the intuitive explanation is that there are two opposite effects caused by the
communication  on the one hand  the agents get a fairer picture of the system which prevents them from using bad resources and therefore getting bad performance  on the other
hand  since all of the agents in a cn have a  better  picture of the system  they all tend
to use the best resources and thus they all compete for them  in fact  the agents behave
selfishly and their selfish interest may not agree with the interest of the population as a
whole 
the interesting message that we get is that the fact that some agents may have a
 distorted  picture of the system  which is typical for non communicating populations  
turns out to be an advantage for the population as a whole 
sharing the data among agents leads to poorer performances also because in this case
the agents have common views of loads and target jobs toward the same  lightly loaded 
resources  which quickly become overloaded  in order to profitably use the shared data 
we should allow for some form of reasoning about the fact that the data is shared  this
problem however is out of the scope of this paper  see e g   lesser        
in order to understand the behavior of the system when cns and ncns face each other 
we consider an ncn of    agents together with a set of cns of equal size  for different values
of that size  the results of the corresponding experiments are shown in figure     the
members of the cns  being more inclined to use the best resources  behave as parasites in
the sense explained in section    they exploit the adaptiveness of the rest of the population
to obtain good performance from the best resources  for this reason they get better results
than the rest of the population  as shown by the experimental results 
it it interesting to observe that when the ncn uses a very conservative selection rule 
the cns obtain even better results  the intuitive explanation for this behavior is that
although all groups  i e   both the communicating ones and the one with high value of n 
tend to be conservative  the communicating ones  win  because they are conservative in a
more  clever  way  that is making use of a better picture of the situation 
the conclusion we draw in this section is that the proposed form of communication
between agents may not provide useful means to improve the performance of a population
in our setting  however  we do not claim that communication between agents is completely
useless  nevertheless  we have observed that it does not provide a straightforward significant
improvement  our results support the claim that the sole past history of an agent is a
   

fischaerf  shoham    tennenholtz

   agents
         ncn
         ncn
         ncn
         ncn
          ncn
          ncn
          ncn
          ncn

   agents
         cn
         cns
         cns
          cns
         cn
         cns
         cns
          cns

t 

      
      
      
      
      
      
      
      

t 

      
      
      
      
      
      
      
      

figure     performance of cns and ncns together
reasonable information on which to base its decision  assuming we do not consider available
any kind of real time information  e g   current load of the resources  

   discussion
the previous sections were devoted to a report on our experimental study  we now synthesize our observations in view of our motivation  as discussed in sections   and   
as we mentioned  our model is a general model where active autonomous agents have
to select among several resources in a dynamic fashion and based on local information 
the fact that the agents use only local information makes the possibility of ecient loadbalancing questionable  however  we showed that adaptive load balancing based on purely
local feedback is a feasible task  hence  our results are complementary to the ones obtained
in the distributed computer systems literature  as mirchandaney and stankovic        put
it         what is significant about our work is that we have illustrated that is possible to design
a learning controller that is able to dynamically acquire relevant job scheduling information
by a process of trial and error  and use that information to provide good performance  
the study presented in our paper supplies a complementary contribution where we are able
to show that useful adaptive load balancing can be obtained using purely local information
and in the framework of a general organizational theoretic model 
in our study we identified various parameters of the adaptive process and investigated
how they affect the eciency of adaptive load balancing  this part of our study supplies
useful guidelines for a systems designer who may force all the agents to work based on a
common selection rule  our observations  although somewhat related to previous observations made in other contexts and models  huberman   hogg         enable to demonstrate
aspects of purely local adaptive behavior in a non trivial model 
our results about the disagreement between selfish interest of agents and the common
interest of the population is in sharp contrast to previous work on multi agent learning
 shoham   tennenholtz              and to the dynamic programming perspective of
earlier work on distributed systems  bertsekas   tsitsiklis         moreover  we explore
how the interaction between different agent types affects the system s eciency as well as
   

fiadaptive load balancing  a study in multi agent learning

the individual agent s eciency  the related results can be also interpreted as guidelines
for a designer who may have only partial control of a system 
the synthesis of the above observations teaches us about adaptive load balancing when
one adopts a reinforcement learning perspective where the agents rely only on their local
information and activity  an additional step we performed attempts to bridge some of the
gap between our local view and previous work on adaptive load balancing by communicating
agents  whose decisions may be controlled by learning automata or by other means  we
therefore rule out the possibility of communication about the current status of resources
and of joint decision making  but enable a limited sharing of previous history  we show
that such limited communication may not help  and even deteriorate system eciency  this
leaves us with a major gap between previous work where communication among agents is the
basic tool for adaptive load balancing and our work  much is left to be done in attempting
to bridge this gap  we see this as a major challenge for further research 

   related work
in section   we mentioned some related work in the field of distributed computer systems
 mirchandaney   stankovic        billard   pasquale        glockner   pasquale       
mirchandaney et al         zhou        eager et al          a typical example of such work
is the paper by mirchandaney and stankovic         in this work learning automata are
used in order to decide on the action to be taken  however  the suggested algorithms heavily
rely on communication and information sharing among agents  this is in sharp contrast
to our work  in addition  there are differences between the type of model we use and the
model presented in the above mentioned work and in other work on distributed computer
systems 
applications of learning algorithms to load balancing problems are given by mehra
        mehra and wah         however  in that work as well  the agents  sites  in the
authors  terminology  have the ability to communicate and to exchange workload values 
even though such values are subject to uncertainty due to delays  in addition  differently
from our work  the learning activity is done off line  in particular  in the learning phase the
whole system is dedicated to the acquisition of workload indices  such load indices are then
used in the running phase as threshold values for job migration between different sites 
in spite of the differences  there are some similarities between our work and the abovementioned work  one important similarity is the use of learning procedures  this is in
difference from the more classical work on parallel and distributed computation  bertsekas
  tsitsiklis        which applies numerical and iterative methods to the solution of problems
in network ow and parallel computing  other similarities are related to our study of the
division of the society into groups  this somewhat resembles work on group formation
 billard   pasquale        in distributed computer systems  the information sharing we
allow in section   is similar to the limited communication discussed by tan         in
the classification of load balancing problems given by ferrari         our work falls into
the category of load independent and non preemptive pure load balancing  the problems we
investigate can be also seen as sender initiated problems  although in our case the sender
is the agent and not the  overloaded  resource 
   

fischaerf  shoham    tennenholtz

one may wonder how our work differs from other work on adaptive load balancing
in operations research  or   e g   queuing theory bonomi  doshi  kaufmann  lee   
kumar         indeed  there are some commonalities  in both or and our work  individual
decisions are made locally  based on information obtained dynamically during runtime  and
in both cases the systems constructed are suciently complex that the most interesting
results tend to be obtained experimentally  however  a careful look at the relevant or
literature reveals an essential difference between the perspective of or on the topic and our
reinforcement learning perspective  or permits free communication within the system  and
thus there is no significant element of uncertainty in that framework  in particular  the issue
of exploration versus exploitation  which lies at the heart of our approach  is completely
absent from work in or 
some work on adaptive load balancing and related topics has been carried out also by
the artificial intelligence community  see e g   kosoresow        gmytrasiewicz  durfee   
wehe        wellman         this work too  however  tends to be based on some form of
communication among the agents  whereas in our case the load balancing is obtained purely
from a learning activity 
this article is related to our previous work on co learning  shoham   tennenholtz 
             the framework of co learning is a framework for multi agent learning  which
differs from other frameworks discussed in multi agent reinforcement learning  narendra  
thathachar        tan        yanco   stein        sen  sekaran    hale        due to
the fact that it considers the case of stochastic interactions among subsets of the agents 
where there is purely local feedback revealed to the agents based on these interactions  the
framework of co learning is similar in some respects to a number of dynamic frameworks in
economics  kandori  mailath    rob         physics  kinderman   snell         computational ecologies  huberman   hogg         and biology  altenberg   feldman         our
study of adaptive load balancing can be treated as a study in co learning 
relevant to our work is also the literature in the field of learning automata  see narendra   thathachar         in fact  an agent in our setting can be seen as a learning automaton  therefore  one may hope that theoretical results on interconnected automata and
n player games  see e g   el fattah        abdel fattah        narendra   wheeler jr  
      wheeler jr    narendra        could be imported in our framework  unfortunately 
due to the stochastic nature of job submissions  i e   agent interactions  and the real valued
 instead of binary  feedback  our problem does not fit completely in to the theoretical
framework of learning automata  hence  results concerning optimality  convergence or expediency of learning rules such as linear reward penalty or linear reward inaction  can
not be easily adapted into our setting  the fact that we use a stochastic model for the
interaction among agents  makes our work closely related to the above mentioned work on
co learning  nevertheless  our work is largely inuenced by learning automata theory and
our resource selection rules closely resemble reinforcement schemes for learning automata 
last but not least  our work is related to work applying organization theory and management techniques to the field of distributed ai  fox        malone        durfee  lesser 
  corkill         our model is closely related to models of decision making in management
and organization theory  e g   malone        and applies a reinforcement learning perspective to that context  this makes our work related to psychological models of decision making
 arthur        
   

fiadaptive load balancing  a study in multi agent learning

    summary

this work applies the idea of multi agent reinforcement learning to the problem of load
balancing in a loosely coupled multi agent system  in which agents need to adapt to one another as well as to a changing environment  we have demonstrated that adaptive behavior
is useful for ecient load balancing in this context and identified a pair of parameters that
affect that eciency in a non trivial fashion  each parameter  holding the other parameter
to be fixed  gives rise to a certain tradeoff  and the two parameters interplay in a non trivial
and illuminating way  we have also exposed illuminating results regarding heterogeneous
populations  such as how a group of parasitic less adaptive agents can gain from the exibility of other agents  in addition  we showed that naive use of communication may not
improve  and might even deteriorate  the system eciency 

acknowledgments

we thank the anonymous reviewers and steve minton  whose stimulating comments helped
us in improving on an earlier version of this paper 

references

abdel fattah  y  m          stochastic automata modeling of certain problems of collective
behavior  ieee transactions on systems  man  and cybernetics                  
altenberg  l     feldman  m  w          selection  generalized transmission  and the
evolution of modifier genes  i  the reduction principle  genetics               
arthur  w          inductive reasoning  bounded rationality and the bar problem  tech  rep 
           working paper   santa fe institute  appeared also in american economic
review    
axelrod  r          the evolution of cooperation  new york  basic books 
bertsekas  d     tsitsiklis  j          parallel and distributed computation  numerical
methods  prentice hall 
billard  e     pasquale  j          effects of delayed communication in dynamic group
formation  ieee transactions on systems  man  and cybernetics                    
blackburn  j  m          acquisition to skill  an analysis of learning curves  ihrb report
no     
bond  a  h     gasser  l          readings in distributed artificial intelligence  ablex
publishing corporation 
bonomi  f   doshi  b   kaufmann  j   lee  t     kumar  a          a case study of
adaptive load balancing algorithm  queuing systems           
durfee  e  h   lesser  v  r     corkill  d  d          coherent cooperation among communicating problem solvers  ieee transactions on computers                
   

fischaerf  shoham    tennenholtz

eager  d   lazowska  e     zahorjan  j          adaptive load sharing in homogeneous
distributed systems  ieee transactions on software engineering                  
el fattah  y  m          stochastic automata modeling of certain problems of collective
behavior  ieee transactions on systems  man  and cybernetics                  
ferrari  d          a study of load indices for load balancing schemes  tech  rep  ucb csd
        computer science division  eecs   univ  of california  berkeley 
ferrari  d   serazzi  g     zeigner  a          measurement and tuning of computer
systems  prentice hall 
fox  m  s          an organizational view of distributed systems  ieee transactions on
systems  man  and cybernetics            
glockner  a     pasquale  j          coadaptive behavior in a simple distributed job
scheduling system  ieee transactions on systems  man  and cybernetics         
        
gmytrasiewicz  p   durfee  e     wehe  d          the utility of communication in coordinating intelligent agents  in proc  of the  th nat  conf  on artificial intelligence
 aaai      pp          
huberman  b  a     hogg  t          the behavior of computational ecologies  in huberman  b  a   ed    the ecology of computation  elsevier science 
kaelbling  l          learning in embedded systems  mit press 
kandori  m   mailath  g     rob  r          learning  mutation and long equilibria in
games  mimeo  university of pennsylvania 
kinderman  r     snell  s  l          markov random fields and their applications 
american mathematical society 
kosoresow  a  p          a fast first cut protocol for agent coordination  in proc  of the
  th nat  conf  on artificial intelligence  aaai      pp          
kraus  s     wilkenfeld  j          the function of time in cooperative negotiations  in
proc  of the  th nat  conf  on artificial intelligence  aaai      pp          
lesser  v  r          a retrospective view of fa c distributed problem solving  ieee
transactions on systems  man  and cybernetics                    
malone  t  w          modeling coordination in organizations and markets  management
science                     
mehra  p          automated learning of load balancing strategies for a distributed
computer system  ph d  thesis  department of electrical and computer engineering 
university of illinois at urbana champaign 
   

fiadaptive load balancing  a study in multi agent learning

mehra  p     wah  b  w          population based learning of load balancing policies for a
distributed computer system  in proceedings of computing in aerospace   conference 
aiaa  pp            
mirchandaney  r     stankovic  j          using stochastic learning automata for job
scheduling in distributed processing systems  journal of parallel and distributed
computing             
mirchandaney  r   towsley  d     stankovic  j          analysis of the effects of delays on
load sharing  ieee transactions on computers                     
narendra  k     thathachar  m  a  l          learning automata  an introduction 
prentice hall 
narendra  k     wheeler jr   r  m          an n player sequential stochastic game with
identical payoffs  ieee transactions on systems  man  and cybernetics               
     
pulidas  s   towsley  d     stankovic  j          imbedding gradient estimators in load balancing algorithms  in proceedings of the  th international conference on distributed
computer systems  ieee  pp          
sen  s   sekaran  m     hale  j          learning to coordinate without sharing information 
in proc  of the   th nat  conf  on artificial intelligence  aaai     
shoham  y     tennenholtz  m          emergent conventions in multi agent systems  initial experimental results and observations  in proc  of the  rd int  conf  on principles
of knowledge representation and reasoning  kr      pp          
shoham  y     tennenholtz  m          co learning and the evolution of social activity 
tech  rep  stan cs tr          dept  of computer science  stanford university 
sutton  r          special issue on reinforcement learning  machine learning          
tan  m          multi agent reinforcement learning  independent vs  cooperative agents 
in proceedings of the   th international conference on machine learning 
thronkide  e  l          animal intelligence  an experimental study of the associative
processes in animals  psychological monographs    
watkins  c          learning with delayed rewards  ph d  thesis  cambridge university 
wellman  m  p          a market oriented programming environment and its application to
distributed multicommodity ow problems  journal of artificial intelligence research 
        
wheeler jr   r  m     narendra  k          learning models for decentralized decision
making  automatica                  
   

fischaerf  shoham    tennenholtz

yanco  h     stein  l          an adaptive communication protocol for cooperating mobile robots  in from animals to animats  proceedings of the second international
conference on the simulation of adaptive behavior  pp          
zhou  s          a trace driven simulation study of dynamic load balancing  ieee transactions on software engineering                    
zlotkin  g     rosenschein  j  s          a domain theory for task oriented negotiation  in
proc  of the   th int  joint conf  on artificial intelligence  ijcai      pp          

   

fi