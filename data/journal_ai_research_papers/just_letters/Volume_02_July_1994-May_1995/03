journal artificial intelligence research                 

submitted        published     

rerepresenting restructuring domain theories 
constructive induction approach
steven k  donoho
larry a  rendell

department computer science  univeristy illinois
    n  mathews ave   urbana  il       usa

donoho cs uiuc edu
rendell cs uiuc edu

abstract

theory revision integrates inductive learning background knowledge combining
training examples coarse domain theory produce accurate theory 
two challenges theory revision theory guided systems face  first 
representation language appropriate initial theory may inappropriate
improved theory  original representation may concisely express initial theory 
accurate theory forced use representation may bulky  cumbersome 
dicult reach  second  theory structure suitable coarse domain theory may
insucient fine tuned theory  systems produce small  local changes
theory limited value accomplishing complex structural alterations may
required 
consequently  advanced theory guided learning systems require exible representation
exible structure  analysis various theory revision systems theory guided
learning systems reveals specific strengths weaknesses terms two desired
properties  designed capture underlying qualities system  new system uses
theory guided constructive induction  experiments three domains show improvement
previous theory guided systems  leads study behavior  limitations 
potential theory guided constructive induction 

   introduction
inductive learners normally use training examples  use background knowledge  effectively integrating knowledge induction widely studied research problem  work date area theory revision
knowledge given coarse  perhaps incomplete incorrect  theory problem domain 
training examples used shape initial theory refined  accurate
theory  ourston   mooney        thompson  langley    iba        cohen        pazzani
  kibler        baffes   mooney        mooney         develop exible
robust approach problem learning data theory knowledge
addressing two following desirable qualities 

flexible representation  theory guided system utilize knowledge contained initial domain theory without adhere closely initial
theory s representation language 

flexible structure  theory guided system unnecessarily restricted
structure initial domain theory 

c      ai access foundation morgan kaufmann publishers  rights reserved 

fidonoho   rendell

giving precise definitions terms  motivate work intuitively 

    intuitive motivation
first desirable quality  exibility representation  arises theory representation appropriate describing coarse  initial domain theory may inadequate
final  revised theory  initial domain theory may compact concise
one representation  accurate theory may quite bulky cumbersome representation  furthermore  representation best expressing initial theory may
best carrying refinements  helpful refinement step may clumsy
make initial representation yet carried quite simply another representation 
simple example  coarse domain theory may expressed logical conjunction
n conditions met  accurate theory  though  one
n conditions holds  expressing accurate theory dnf representation
used describe initial theory would cumbersome unwieldy  murphy   pazzani 
       furthermore  arriving final theory using refinement operators suitable
dnf  drop condition  add condition  modify condition  would cumbersome task 
m of n representation adopted  refinement simply involves empirically
finding appropriate m  final theory expressed concisely  baffes   mooney 
      
similarly  second desirable quality  exibility structure  arises theory
structure suitable coarse domain theory may insucient fine tuned
theory  order achieve desired accuracy  restructuring initial theory may
necessary  many theory revision systems act making series local changes 
lead behavior two extremes  first extreme rigidly retain backbone
structure initial domain theory  allowing small  local changes  figure   illustrates
situation  minor revisions made   conditions added  dropped 
modified   refined theory trapped backbone structure initial
theory  local changes needed  techniques proven useful  ourston
  mooney         often required  required  systems often
move extreme  drop entire rules groups rules build entire
new rules groups rules scratch replace them  thus restructure 
forfeit valuable knowledge process  ideal theory revision system would glean
knowledge theory substructures cannot fixed small  local changes
use restructured theory 
intuitive illustration  consider piece software  almost works   sometimes
made useful local operations  fixing couple bugs  adding
needed subroutine  on  cases  though  piece software  almost
works  fact far full working order  may need redesigned restructured 
mistake one extreme try fix program making series patches
original code  mistake extreme discard original program without
learning anything start scratch  best approach would examine
original program see learned design use knowledge
redesign  likewise  attempting improve coarse domain theory series
local changes may yield little improvement theory trapped initial
   

firerepresenting restructuring domain theories

initial theory



b

c

j

e

h

refined theory

n

k l





b

c

p q r

j

w e

f g



n

k l

u



p

v

r

f g

figure    typical theory revision allows limited structural exibility  although conditions added  dropped  modified  revised theory much
constrained structure initial theory 
structure  render original domain theory useless  careful analysis
initial domain theory give valuable guidance design best final theory 
illustrated figure   many substructures taken initial
theory adapted use refined theory  information initial theory
used  structure revised theory restricted structure
initial theory 
initial theory



b

c

n

j

e

h

refined theory

k l

k

p q r

l

f



g



e

h







p

u

v

f g q r

f g

figure    exible structural modification  revised theory taken many substructures initial theory adapted recombined use 
structure revised theory restricted structure
initial theory 

   

fidonoho   rendell

    terminology

paper  training data consist examples classified vectors feature value pairs  assume initial theory set conditions combined using
operators and  or  indicating one classes  unreasonable
believe theories always form  covers much existing theory revision
research 
work intended informal exploration exible representation exible
structure  flexible representation means allowing theory revised using representation language initial theory  example exible representation
introduction new operator combining features   operator used
initial theory  section     example given introducing m of n operator
represent theory originally expressed dnf  flexible structure means limiting
revision theory series small  incremental modifications  example
breaking theory components using building blocks
construction new theory 
constructive induction process whereby training examples redescribed using
new set features  new features combinations original features  bias
knowledge may used construction new features  subtle point
speak exible representation  referring representation
domain theory  training data  although phrase  change representation 
often applied constructive induction  refers change data  paper 
term exible representation reserved change theory representation  thus
system performing constructive induction  changing feature language
data  without exhibiting exible representation  changing representation theory  

    overview

theory revision constructive induction embody complementary aspects machine
learning research community s ultimate goals  theory revision uses data improve
theory  constructive induction use theory improve data facilitate learning 
paper present theory guided constructive induction approach addresses
two desirable qualities discussed section      initial theory analyzed  new
features constructed based components theory  constructed features
need expressed representational language initial theory
refined better match training examples  finally  standard inductive learning
algorithm  c     quinlan         applied redescribed examples 
begin analyzing landmark theory revision learning systems exhibited exibility handling domain theory part played performance  analysis  extract guidelines system design apply
design limited system  effort integrate learning theory data 
borrow heavily theory revision  multistrategy learning  constructive induction communities  guidelines system design fall closest classical constructive
induction methods  central focus paper presentation  another new
system  rather study exible representation structure  manifestation
previous work  guidance future design 
   

firerepresenting restructuring domain theories

section   gives context work analyzing previous research uence work  section   explores promoter recognition domain demonstrates
related theory revision systems behave domain  section    guidelines
theory guided constructive induction presented  guidelines synthesis
positive aspects related research  address two desirable qualities  exibility
representation exibility structure  section   presents specific theory guided
constructive induction algorithm instantiation guidelines set forth earlier
section  results experiments three domains given section   followed
discussion strengths theory guided constructive induction section    section   presents experimental analysis limits applicability simple algorithm
followed discussion limitations future directions work section   

   context related work

although work bears resemblance form objective many papers constructive induction  michalski        fu   buchanan        utgoff        schlimmer       
drastal   raatz        matheus   rendell        pagallo   haussler        ragavan  
rendell        hirsh   noordewier         theory revision  ourston   mooney        feldman  serge    koppel        thompson et al         cohen        pazzani   kibler       
baffes   mooney         multistrategy approaches  flann   dietterich        towell 
shavlik    noordeweir        dzerisko   lavrac        bloedorn  michalski    wnek       
clark   matwin        towell   shavlik         focus upon handful systems  significant  underlying similarities work  section
analyze miro  either  focl  labyrinthk   kbann  neither mofn  grendel
discuss related underlying contributions relationship perspective 

   

miro

 drastal   raatz        seminal work knowledge guided constructive induction  takes knowledge low level features interact uses knowledge
construct high level features training examples  standard learning algorithm
run examples described using new features  domain theory used
shift bias induction problem  utgoff         empirical results showed
describing examples high level  abstract terms improved learning accuracy 
miro approach provides means utilizing knowledge domain theory without
restricted structure theory  substructures domain theory
used construct high level features standard induction algorithm arrange
concept  constructed features used are  others ignored 
others combined low level features  still others may used differently
multiple contexts  end result knowledge domain theory utilized 
structure final theory restricted structure initial theory 
miro provides exible structure 
another benefit miro like techniques applied even partial
domain theory exists  i e   domain theory specifies high level features
link together domain theory specifies high level features
others  one miro s shortcomings provided means making minor changes
miro

   

fidonoho   rendell

domain theory rather constructed features exactly domain theory
specified  representation miro s constructed features primitive   either
example met conditions high level feature not  example miro s
behavior given section     

   

 

  labyrinthk

either focl



either

 ourston   mooney         labyrinthk  thompson et al         
focl  pazzani   kibler        systems represent broad spectrum theory revision
work  make steps toward effective integration background knowledge inductive
learning  although systems many superficial differences regard supervised unsupervised learning  concept description language  etc   share underlying
principle incrementally revising initial domain theory series local changes 
discuss either representative class systems  either s theory
revision operators include  removing unwanted conditions rule  adding needed conditions rule  removing rules  adding totally new rules  either first classifies
training examples according current theory  misclassified  seeks repair
theory applying theory revision operator result correct classification
previously misclassified examples without losing correct examples  thus
series local changes made allow improvement accuracy training
set without losing examples previously classified correctly 
either type methods provide simple yet powerful tools repairing many important
common faults domain theories  fail meet qualities exible representation exible structure  theory revision operators make small  local
modifications existing domain theory  final theory constrained similar
structure initial theory  accurate theory significantly different structure initial theory  systems forced one two extremes discussed
section    first extreme become trapped local maximum similar
initial theory unable reach global maximum local changes made 
extreme drop entire rules groups rules replace new
rules built scratch thus forfeiting knowledge contained domain theory 
also  either carries theory revision steps representation initial
theory  consequently  representation final theory initial
theory  another representation may appropriate revised theory
one initial theory comes  facilities provided accommodate this 
advanced theory revision system would combine locally acting strengths eithertype systems exibility structure exibility representation  example
either s behavior given section     

   

kbann

neither mofn

kbann system  towell et al         towell   shavlik        makes unique contributions theory revision work  kbann takes initial domain theory described symbolically
logic creates neural network whose structure initial weights encode theory 
backpropagation  rumelhart  hinton    mcclelland        applied refinement tool fine tuning network weights  kbann empirically shown give
   

firerepresenting restructuring domain theories

significant improvement many theory revision systems widely used promoter
recognition domain  although work different implementation kbann 
abstract ideologies similar 
one kbann s important contributions takes domain theory one representation  propositional logic  translates less restricting representation  neural
network   logic appropriate representation initial domain theory
promoter problem  neural network representation convenient refining
theory expressing best revised theory  change representation
kbann s real source power  much attention given fact kbann combines symbolic knowledge subsymbolic learner  combination viewed
generally means implementing important change representation  may
change representation gives kbann power  necessarily specific
symbolic subsymbolic implementation  thus kbann system embodies higher level
principle allowing refinement occur appropriate representation 
alternative representation kbann s source power  question must raised
whether actual kbann implementation always best means achieving
goal  neural network representation may expressive required  accordingly  backpropagation often refinement power needed  thus kbann may
carry excess baggage translating neural net representation  performing expensive
backpropagation  extracting symbolic rules refined network  although full
extent kbann s power may needed problems  many important problems may
solvable applying kbann s principles symbolic level using less expensive tools 
neither mofn  baffes   mooney         descendant either  second example
system allows theory revised representation
initial theory  domain theory input neither mofn expressed propositional
logic and or tree  neither mofn interprets theory less rigidly  
rule true time n conditions true  initially set equal n  all
conditions must true rule true   one theory refinement operator
lower particular rule  end result examples close enough
partial match initial theory accepted  neither mofn  since built upon
either framework  includes either like theory revision operators  add condition 
drop condition  etc 
thus neither mofn allows revision take place representation appropriate
revision appropriate concisely expressing best refined theory  neithermofn achieved results comparable kbann promoter recognition domain 
suggests change representation two systems share
give power rather particular implementation  neither mofn
demonstrates small amount representational exibility sometimes enough 
m of n representation employs big change original representation
neural net representation kbann employs yet achieves similar results
arrives much quickly kbann  baffes   mooney        
shortcoming neither mofn since acts making local changes
initial theory  still become trapped structure initial theory  advanced
theory revision system would incorporate neither mofn s kbann s exibility
   

fidonoho   rendell

representation allow knowledge guided theory restructuring  examples
neither mofn s behavior given sections         

   

 s

kbann

grendel

cohen        analyzes class theory revision systems draws insightful conclusions  one  generality  in theory interpretation  comes expense power  
draws principle fact system either focl treats every
domain theory therefore must treat every domain theory general way  argues rather applying general refinement strategy
every problem  small set refinement strategies available narrow
enough gain leverage yet narrow apply single problem  cohen
presents grendel  toolbox translators transforms domain theory
explicit bias  translator interprets domain theory different way 
appropriate interpretation applied given problem 
apply cohen s principle representation domain theories  domain
theories translated representation  general  adaptable representation used order accommodate general case  comes
expense higher computational costs possibly lower accuracy due overfit
stemming unbridled adaptability  neural net representation kbann
translates domain theories allows    measure partial match domain theory    different parts domain theory weighted differently    conditions added
dropped domain theory  options adaptability probably
necessary problems may even detrimental  options kbann
require computationally expensive backpropagation method 
representation used neither mofn adaptable kbann s  
allow individual parts domain theory weighted differently  neithermofn runs quickly kbann small problems probably matches even
surpasses kbann s accuracy many domains   domains fine grained weighting
unfruitful even detrimental  toolbox theory rerepresentation translators analogous
grendel would allow domain theory translated representation
appropriate forms adaptability 

    outlook summary

summary  brie reexamine exible representation exible structure  two
desirable qualities set forth section    consider various systems exemplify
subset desirable qualities 
kbann neither mofn interpreted theory exibly original
representation allowed revised theory adaptable representation 
final  refined theory often many exceptions rule  may tolerate partial
matches missing pieces evidence  may weight evidence heavily
evidence  kbann s neither mofn s new representation may
concise  appropriate representation initial theory  new
representation allows concise expression otherwise cumbersome final theory 
cases principle exible representation 
   

firerepresenting restructuring domain theories

standard induction programs quite successful building concise theories
high predictive accuracy target concept concisely expressed using
original set features  can t  constructive induction means creating
new features target concept concisely expressed  miro uses
constructive induction take advantage strengths domain theory
standard induction  knowledge theory guides construction appropriate
new features  standard induction structures concise description
concept  thus miro like construction coupled standard induction provides
ready powerful means exibly restructuring knowledge contained
initial domain theory  case principle exible structure 

following section introduce dna promoter recognition domain order
illustrate tangibly systems discussed integrate knowledge
induction 

   demonstrations related work
section introduces promoter recognition domain  harley  reynolds    noordewier 
      brie illustrates miro like system  either  kbann  neithermofn behave domain  implemented miro like system promoter domain  versions either neither mofn available ray mooney s group 
kbann s behavior described analyzing  towell   shavlik         chose promoter domain non trivial  real world problem number theory
revision researchers used test work  ourston   mooney        thompson
et al         wogulis        cohen        pazzani   kibler        baffes   mooney       
towell   shavlik         promoter domain one three domains evaluate
work  theory guided constructive induction  section   

    promoter recognition domain

promoter sequence region dna marks beginning gene  example promoter recognition domain region dna classified either promoter
non promoter  illustrated figure    examples consist    features representing sequence    dna nucleotides  feature take values a g c 
representing adenine  guanine  cytosine  thymine corresponding dna position 
features labeled according position p    p    there zero
position   notation  p n   denotes nucleotide n positions upstream
beginning gene  goal predict whether sequence promoter
nucleotides  total     examples available     promoters    non promoters 
promoter recognition problem comes initial domain theory shown figure    quoted almost verbatim towell shavlik s entry uci machine learning
repository   theory states promoter sequences must two regions make
contact protein must acceptable conformation pattern 
four possibilities contact region minus        nucleotides upstream beginning gene   match four possibilities satisfy minus   
contact condition  thus joined disjunction  similarly  four possibilities
   

fidonoho   rendell

dna sequence

p   

p  

c g c
figure    instance promoter domain consists sequence    nucleotides
labeled p    p    nucleotide take values a g c 
representing adenine  guanine  cytosine  thymine 
contact region minus    four acceptable conformation patterns  figure  
gives pictorial presentation portions theory      examples
dataset  none matched domain theory exactly  yielding accuracy     

   

miro

promoter domain

miro like system promoter domain would use rules figure   construct new high level features dna segment  figure   shows example this 
dna segment shown position p    position p     minus    rules
theory shown  four new features  feat minus   feat minus   d 
constructed dna segment  one minus    rule  new features feat minus   feat minus   value   dna fragment
matches first fourth minus    rules  likewise  feat minus   b feat minus   c
value   dna fragment match second third
rules  furthermore  since four minus    rules joined disjunction  new feature 
feat minus   all  created group would value   least one
minus    rules matches 
new features would similarly created minus    rules conformation
rules  standard induction algorithm could applied  implemented mirolike system  figure   gives example theory created it   drastal s original miro used
candidate elimination algorithm  mitchell        underlying induction algorithm 
used c     quinlan          opposed theory revision systems incrementally
modify domain theory  miro broken theory components
fashioned components new theory using standard induction program  thus
miro exhibited exible structure principle domain   restricted
way structure initial theory  rather  miro exploited strengths
standard induction concisely characterize training examples using new features 
   

firerepresenting restructuring domain theories

promoters region protein  rna polymerase  must make contact
helical dna sequence must valid conformation two pieces
contact region spatially align  prolog notation used 
promoter    contact  conformation 
two regions  upstream  beginning gene
rna polymerase makes contact 
contact

   minus     minus    

following rules describe compositions possible contact regions 
minus   
minus   
minus   
minus   

   p    c  p    t  p    t  p    g  p    a  p    c 
 p    t  p    t  p    g 
p    c  p    a 
 p    t  p    t  p    g  p    a  p    c  p    a 
 p    t  p    t  p    g  p    a  p    c 

minus   
minus   
minus   
minus   

   p    t  p    a  p    t  p    a  p    a  p   t 
 p    t  p    a 
p    a 
p   t 
 p    t  p    a  p    t  p    a  p   a  p   t 
 p    t  p    a 
p   t 

following rules describe sequences produce acceptable conformations 
conformation    p    c 
p    t 
p   c 
conformation    p    a 
conformation    p    a 
conformation    p    a 
p    t 

p    a  p    a  p    t  p    t  p    a  p    c  p    g 
p    c  p   g  p   c  p   g  p   c  p   c  p   c 
p    a  p    a 
p    t  p    t  p    a  p    t  p    t  p    g  p   a 
p    a  p    t  p    t  p    t  p    a  p    a  p    t 
p   t 

figure    initial domain theory recognizing promoters  from towell shavlik  
weakness miro displays example allows exibility representation
theory  representation features constructed miro basically
all or none representation initial theory  either dna segment matched rule 
not 

   

either

promoter domain

either like system refines initial promoter theory dropping adding
conditions rules  simulated either turning m of n option neither
ran promoter domain  figure   shows refined theory produced using
randomly selected training set size     initial promoter domain theory
lend revision small  local changes  either limited success 
   

fidonoho   rendell

dna sequence

p   

p  

contact minus   

contact minus   

                           

c

g c

 

                      

     



   









  g   c

 





 



  g c

 





  g c

   


 



 

 

 

 

 

 

figure    contact portion theory  four possibilities
minus    minus    portions theory      matches nucleotide 
conformation portion theory spread display pictorially 
run  program exhibited second behavioral extreme discussed section   
entirely removed groups rules tried build new rules replace
lost  minus    conformation rules essentially removed  new rules
added minus    group  new minus    rules contain condition
p    t previously found minus    group condition p    a previously found
conformation group 
either s behavior example direct result lack exibility representation exibility structure  dicult transform minus    conformation
rules something useful initial representation using either s locally acting operators  either handles dropping sets rules  losing knowledge 
attempting rediscover lost knowledge empirically  end result loss
knowledge lower optimal accuracy shown later section   

   

kbann

promoter domain

figure    modeled figure towell shavlik         shows setup kbann
network promoter theory  slot along bottom represents one nucleotide
dna sequence  node first level bottom embodies single
domain rule  higher levels encode groups rules final concept top 
links shown figure ones initially high weighted  net next filled
fully connected low weight links  backpropagation applied refine
network s weights 
   

firerepresenting restructuring domain theories

dna segment fragment 
   

p    g  p    c  p    t  p    t  p    g  p    a  p    c  p    t  p    t

   

minus    group rules corresponding constructed features 
minus       p    c  p    t  p    t  p    g  p    a  p    c 
minus     p    t  p    t  p    g 
p    c  p    a 
minus     p    t  p    t  p    g  p    a  p    c  p    a 
minus     p    t  p    t  p    g  p    a  p    c 

feat minus      
feat minus   b    
feat minus   c    
feat minus      

feat minus      feat minus     feat minus   b   feat minus   c   feat minus   d     

figure    example feature construction miro like system  constructed
features first fourth rules minus    group true  value     
dna segment matches rules  constructed feature
entire group  feat minus   all  true four minus    rules joined
disjunction 
feat minus   all
 

 
promoter

feat conf b
 

 

feat minus   d
 
non promoter

promoter
 

promoter

figure    example theory created miro like system  dna segment recognized
promoter matches minus    rules  second conformation
rule  fourth minus    rule 
neural net representation appropriate domain propositional
logic representation initial theory  allows measurement partial match
weighting links way subset rule s conditions enough surpass
node s threshold  allows variable weightings different parts theory  therefore  predictive nucleotides weighted heavily  slightly predictive
nucleotides weighted less heavily  kbann limited exibility structure 
refined network result series incremental modifications
initial network  fundamental restructuring theory embodies unlikely  kbann
   

fidonoho   rendell

promoter    contact  conformation 
contact

   minus     minus    

minus   
minus   
minus   
minus   
minus   
minus   

       

p    t 
p    t 
p    t 
p    g 
p    g 
p    t 

p    g 
p    a  p    c 
p    c  p    c 
p    t 
p    a 
p    g 

minus       true 
conformation    true 

figure    revised theory produce either 
promoter

contact

conformation
minus   

p   

minus   

dna sequence

p  

figure    setup kbann network promoter theory 
limited finding best network fundamental structure imposed
initial theory 
one kbann s advantages uses standard learning algorithm foundation  backpropagation widely used consequently improved previous
researchers  theory refinement tools built ground use standard
tool tangentially suffer invent methods handling standard
problems overfit  noisy data  etc  wealth neural net experience resources
available kbann user  neural net technology advances  kbann technology
passively advance it 
   

firerepresenting restructuring domain theories

   

neither mofn

promoter domain

refines initial promoter theory dropping adding conditions rules allowing conjunctive rules true subset
conditions true  ran neither mofn randomly selected training set size
    figure    shows refined promoter theory produced  theory expressed
  m of n rules would require    rules using propositional logic  initial theory s
representation  importantly  unclear system using initial representation would reach    rule theory initial theory  thus m of n representation
adopted allows concise expression final theory facilitates
refinement process 
neither mofn

promoter        contact  conformation   
contact

       minus     minus      

minus           p    t  p    t  p    g 
p    c  p    a   
minus           p    t  p    t  p    g  p    a  p    c
  
minus   
minus   
minus   
minus   

     

 
 
 
 






 
 
p    t 
  p    t  p    a 
 
p    t 

p    t  p    a 
p    a 
p    a 
p    t  p    a  p    a  p   t
p    a 
p    a 

p   t
p   t

  
  
  
p    g   

conformation    true 

figure     revised theory produced neither mofn 
neither mofn displays exibility representation allowing m of n interpretation original propositional logic  allow fine grained refinement
kbann  allow measure partial match  kbann could weight
predictive features heavily  example  minus    rules  perhaps p    t
predictive dna segment promoter p    g therefore
weighted heavily  neither mofn simply counts number true conditions
rule  therefore  every condition weighted equally  kbann s fine grained weighting may
needed domains others  may actually detrimental domains 
advanced theory revision system offer range representations 
kbann  neither mofn limited exibility structure  refined
theory reached series small  incremental modifications initial theory
precluding fundamental restructuring  neither mofn therefore limited finding
best theory fundamental structure initial theory 

   theory guided constructive induction

first half section present guidelines theory guided constructive induction
summarize work discussed sections      remainder section
   

fidonoho   rendell

presents algorithm instantiates guidelines  evaluate algorithm
section   

    guidelines

following guidelines synthesis strengths previously discussed related
work 
miro  new features constructed using components domain
theory  new features combinations existing features  final theory
created applying standard induction algorithm training examples described
using new features  allows knowledge gleaned initial theory
without forcing final theory conform initial theory s backbone structure 
takes full advantage domain theory building high level features
original low level features  takes advantage strength standard induction
  building concise theories high predictive accuracy target concept
concisely expressed using given features 
either  constructed features modifiable various operators
act locally  adding dropping conjuncts constructed feature 
kbann neither mofn  representation constructed features
need exact representation initial theory given  example 
initial theory may given set rules written propositional logic 
new feature constructed rule  need boolean feature
telling whether conditions met  example may count
many conditions rule met  allows final theory formed
expressed representation suitable representation
initial theory 
grendel  complete system offer library interpreters allowing
domain theory translated range representations differing adaptability  one interpreter might emulate miro strictly translating domain theory
boolean constructed features  another interpreter might construct features
count number satisfied conditions corresponding component domain theory thus providing measure partial match  still another interpreter
might construct features weighted sums satisfied conditions 
weights could refined empirically examining set training examples  thus
appropriate amount expressive power applied given problem
without incurring unnecessary expense 

    specific interpreter

section describes algorithm limited instantiation guidelines
described  algorithm intended demonstration distillation synthesis
principles embodied previous landmark systems  contains main module 
tgci described figure     specific interpreter  tgci  described figure    
main module tgci redescribes training testing examples calling tgci 
   

firerepresenting restructuring domain theories

applies c    redescribed examples  just miro applied candidate
elimination algorithm examples redescribing them   tgci  viewed
single interpreter potential grendel like toolbox  takes input single example
domain theory expressed and or tree one shown figure    
returns new vector features example measure partial match
example theory  thus creates new features components domain theory
miro  measures partial match  allows exibility representing
information contained initial theory kbann neither mofn  one
aspect guidelines     appear algorithm either s locally
acting operators adding dropping conditions portion theory 
following two paragraphs explain detail workings tgci  tgci
respectively 
given  example e domain theory root node r  domain
theory and or not tree leaves conditions
tested true false 
return  pair p    f  f   f top feature measuring partial
match e whole domain theory  f vector new features measuring partial match e various parts subparts domain theory 

   r directly testable condition  return p        r true e
p         r false e  
   n   number children r
   child rj r  call tgci  rj  e   store respective results
pj    fj   fj   
   major operator r or  fnew   max  fj   
return p    fnew   concatenate  fnew    f   f        fn   
p
   major operator r and  fnew     nj   fj   n 
return p    fnew   concatenate  fnew    f   f        fn   
   major operator r not  fnew      f   
return p    fnew   f   
figure     tgci  algorithm
tgci  algorithm  given figure     recursive  inputs example e
domain theory root node r  ultimately returns redescription e form
vector new features f   returns value f called top feature used
intermediate calculations described below  base case occurs domain theory
single leaf node  i e   r simple condition   case  line     tgci  returns
top feature   condition true    condition false  new features
returned base case would simply duplicate existing features 
   

fidonoho   rendell

domain theory single leaf node  tgci  recursively calls r s children
 line     child r  rj   processed  returns vector new features fj  which
measures partial match example j th child r various subparts  
returns top feature fj included fj marked special
measures partial match example whole j th child r  n
children  result line   n vectors new features  f  fn   n top features  f 
fn   operator node r  line     fnew   new feature created
node  maximum fj   thus fnew measures closely best r s children come
conditions met example  vector new features returned
case concatenation fnew new features r s children  operator
node r  line     fnew average fj   thus fnew measures closely
r s children group come conditions met example 
vector new features returned case concatenation fnew new
features r s children  operator node r  line     r
one child  fnew f  negated  thus fnew measures extent conditions
r s child met example 
given  set training examples etrain   set testing examples etest  
domain theory root node r 
return  learned concept accuracy testing examples 
   example ei   etrain   call tgci  r ei  returns pi  
 fi   fi   etrain new   ffig 
   example ei   etest  call tgci  r ei   returns pi  
 fi   fi   etest new   ffi g 
   call c    training examples etrain new testing examples
etest new   return decision tree accuracy etest new  
figure     tgci algorithm
tgci  called twice two different examples domain theory 
two vectors new features size  furthermore  corresponding features
measure match corresponding parts domain theory  tgci main module
figure    takes advantage creating redescribed example sets input
example sets  line   redescribes example training set producing new training
set  line   testing set  line   runs standard induction program
c    redescribed example sets  returned decision tree easily interpreted
examining new features used part domain theory
correspond to 

   

tgci 

examples

example tgci  interpreter works  consider toy theory shown
figure     tgci  redescribes input example constructing new feature node
   

firerepresenting restructuring domain theories

input theory  consider situation input example matches conditions a 
b  c e  tgci  evaluates children node    gets values
f       f       f        f       f        since operator node   and  fnew
average values received children                                     
       likewise  condition g matchs f h  fnew node   value
                                two three matching conditions node   give
value        negated node    since node   disjunction 
new feature measures best partial match two children value     
 max              on 
 
 


 
 
 


 

b c e

 

f g h

 

j k

 

l n

figure     example theory form and or tree might used
interpreter generate constructed features 
figure    shows tgci  redescribes particular dna segment using minus   
rules promoter theory  partial dna segment shown along four minus   
rules new feature constructed rule  we given new features names
simplify illustration   first rule  four six nucleotides match  therefore  dna segment feat minus   value                              
second rule  four five nucleotides match  therefore  feat minus   b
value       two minus    rules joined disjunction original domain theory  feat minus   all  new feature constructed
group  takes maximum value four children  therefore  feat minus  
value      feat minus   b value       highest group  intuitively  feat minus   represents best partial match grouping   extent
disjunction partially satisfied  results running tgci  dna
sequence set redescribed training examples  redescribed example value
feat minus   feat minus   d  feat minus   all  nodes promoter domain theory  training set essentially redescribed using new feature vector
derived information contained domain theory  form  off the shelf
induction program applied new example set 
anomalous situations created tgci  gives  good score  seemingly
bad example bad score good example  situations created
logically equivalent theories give different scores single example  occur
   

fidonoho   rendell

dna segment fragment 
   

p    g  p    c  p    t  p    t  p    g  p    c  p    a  p    a  p    t

   

minus    group rules corresponding constructed features 
minus       p    c  p    t  p    t  p    g  p    a  p    c 
minus     p    t  p    t  p    g 
p    c  p    a 
minus     p    t  p    t  p    g  p    a  p    c  p    a 
minus     p    t  p    t  p    g  p    a  p    c 

feat minus         
feat minus   b       
feat minus   c       
feat minus         

feat minus     max feat minus   a  feat minus   b  feat minus   c  feat minus   d        

figure     example tgci  generates constructed features portion
promoter domain theory dna segment  four conditions first
minus    rule match dna segment  therefore  constructed feature
rule value                                         feat minus   all 
new feature entire minus    group takes maximum value
children thus embodying best partial match group 
biased favor situations matched conditions desirable 
matched conditions necessarily better  eliminating anomalies
would remove bias 
tgci 

   experiments analysis
section presents results applying theory guided constructive induction three
domains  promoter domain  harley et al          primate splice junction domain  noordewier  shavlik    towell         gene identification domain  craven   shavlik 
       case tgci  interpreter applied domain s theory examples
order redescribe examples using new features  c     quinlan       
applied redescribed examples 

    promoter domain
figure    shows learning curve theory guided constructive induction promoter
domain accompanied curves either  labyrinthk   kbann  neither mofn 
following methodology described towell shavlik         set     examples
randomly divided training set size    testing set size     learning
curve created training subsets training set size                         
using    examples testing  curves either  labyrinthk   kbann
taken ourston mooney         thompson  langley  iba         towell
   

firerepresenting restructuring domain theories

shavlik        respectively obtained similar methodology    curve
fortgci average    independent random data partitions given along    
confidence ranges  neither mofn program obtained ray mooney s group
used generating neither mofn curve using    data partitions
used tgci  
    
  

either
labyrinth k
neither mofn
kbann
tgci
    confidence neither mofn
    confidence tgci

    
  
    
  
    
  error

  
    
  
    
  
    
  
   
 
   
 
 

 

                                            
size training sample

figure     learning curves theory guided constructive induction systems
promoter domain 
tgci showed improvement either labyrinthk portions curve
performed better kbann neither mofn except smallest training sets  confidence intervals available either  labyrinthk  

  

used testing set size    use conformation portion domain theory 
testing set labyrinthk always consisted    promoters    non promoters 
   baffes mooney        report slightly better learning curve neither mofn obtained 
communication paul baffes  think difference caused random selection
data partitions 
either

   

fidonoho   rendell

kbann  pairwise comparison neither mofn  improvement tgci
significant        level confidence training sets size    larger 

structure initial promoter theory

    
first
conf 
rule

    
first
minus   
rule

    
second
minus   
rule

    
third
minus   
rule

    
fourth
minus   
rule

    
first
minus   
rule

    
second
minus   
rule

    
third
minus   
rule

    
second
conf 
rule

    
third
conf 
rule

    
fourth
conf 
rule

    
fourth
minus   
rule

structure revised promoter theory

    
second
minus   
rule

    
first
minus   
rule

    
second
minus   
rule

    
third
minus   
rule

    
fourth
minus   
rule

figure     revised theory produced theory guided constructive induction borrowed substructures initial theory  whole restricted structure 
figure    compares initial promoter theory theory created tgci  reasons
tgci s improvement inferred figure  tgci extracted components original theory helpful restructured
concise theory  neither kbann neither mofn facilitates radical extraction
restructuring  seen leaf nodes  new theory measures partial match
example components original theory  aspect similar kbann
neither mofn 
part tgci s improvement kbann neither mofn may due knowledge bias con ict latter two systems  situation revision biases con ict
knowledge way undo knowledge s benefits  occur
whenever detailed knowledge opened revision using set examples  revision
guided examples rather examples interpreted set
   

firerepresenting restructuring domain theories

algorithmic biases  biases useful absence knowledge may undo good
knowledge improperly applied  yet biases developed perfected pure induction often unquestioningly applied revision theories  biases governing
dropping conditions neither mofn reweighting conditions kbann may
neutralizing promoter theory s potential  speculate conducted
experiments allowed bias guided dropping adding conditions within tgci 
found techniques actually reduced accuracy domain 
  
    
  
    
c   
backpropagation
kbann
tgci
    confidence tgci
domain theory

  
    
  error

  
    
  
    
  
    
  
    
  
   
 

  

  

  

  

   

   

   

   

   

   

size training sample

figure     learning curves tgci systems primate splice junction domain 

    primate splice junction domain

primate splice junction domain  noordewier et al         involves analyzing dna
sequence identifying boundaries introns exons  exons parts
dna sequence kept splicing  introns spliced out  task involves placing
   

fidonoho   rendell

given boundary one three classes  intron exon boundary  exon intron boundary  neither  imperfect domain theory available       error rate
entire set available examples 
figure    shows learning curves c     backpropagation  kbann  tgci
primate splice junction domain  results kbann backpropagation taken
towell shavlik         curves plain c    tgci algorithm
created training sets size                               testing set size
     curves c    tgci average    independent data partitions 
comparison made neither mofn implementation obtained
could handle two class concepts  training sets larger      kbann  tgci 
backpropagation performed similarly 
accuracy tgci appears slightly worse kbann perhaps significantly  kbann s advantage tgci ability assign fine grained weightings
individual parts domain theory  tgci s advantage kbann ability
easily restructure information contained domain theory  speculate
kbann s capability assign fine grained weights outweighted somewhat rigid structuring domain theory  theory guided constructive induction advantage
speed kbann c     underlying learner  runs much quickly
backpropagation  kbann s underlying learning algorithm 

    gene identification domain
gene identification domain  craven   shavlik        involves classifying given dna
segment coding sequence  one codes protein  non coding sequence 
domain theory available gene identification domain  therefore  created
artificial domain theory using information organisms may favor certain nucleotide
triplets others gene coding  domain theory embodies knowledge dna
segment likely gene coding segment triplets coding favoring triplets
triplets noncoding favoring triplets  decision triplets codingfavoring  noncoding favoring  favored neither  made empirically
analyzing makeup      coding      noncoding sequences  specific artificial domain theory used described online appendix   
figure    shows learning curves c    tgci gene identification domain 
original domain theory yields       error  curves created training
example sets size                    testing separate example set size      
curves average    independent data partitions 
partial curve given neither mofn became prohibitively slow
larger training sets  promoter domain training sets smaller     
tgci neither mofn ran comparable speeds  approximately    seconds sun 
workstation   domain tgci ran approximately   minutes larger training sets 
neither mofn took    times long tgci training sets size         times
long size          times long size       consequently  neither mofn s
curve extends      represents five randomly selected data partitions 
reasons  solid comparison neither mofn tgci cannot made
curves  appears tgci s accuracy slightly better  speculate neither   

firerepresenting restructuring domain theories

  
    
  

tgci
    confidence tgci
c   
neither mofn
domain theory

    

  error

  
    
  
    
  
    
  
 

   

   

   

   

                             

number training examples

figure     learning curves tgci systems gene identification domain 
 s slightly lower accuracy partially due fact revises theory
correctly classify training examples  result theory likely overfits
training examples  tgci need explicitly avoid overfit handled
underlying learner 
mofn

    summary experiments
goal paper present new technique rather understand
behavior landmark systems  distill strengths  synthesize simple
system  tgci  evaluation algorithm shows accuracy roughly matches
exceeds predecessors  promoter domain  tgci showed sizable improvement
many published results  splice junction domain  tgci narrowly falls short
kbann s accuracy  gene identification domain  tgci outperforms neither mofn 
domains tgci greatly improves original theory alone c    alone 
   

fidonoho   rendell

faster closest competitors  tgci runs much     times faster
large datasets  strict quantitative comparison speeds tgci
kbann made    backpropagation known much slower
decision trees  mooney  shavlik  towell    gove            kbann uses multiple hidden
layers makes training time even longer  towell   shavlik            towell
shavlik        point run kbann must made multiple times
different initial random weights  whereas single run tgci sucient 
overall  experiments support two claims paper  first  accuracy tgci
substantiates delineation system strengths terms exible theory representation
exible theory structure  since characterization basis algorithm s
design  second  tgci s combination speed accuracy suggest unnecessary computational complexity avoided synthesizing strengths landmark systems 
following section take closer look strengths theory guided constructive
induction 
tgci

neither mofn

   discussion strengths
number strengths theory guided constructive induction discussed within
context tgci algorithm used experiments 

    flexible representation

discussed section    many domains representation appropriate
initial theory may appropriate refined theory  theory guided constructive induction allows translation initial theory different representation 
accommodate domains  experiments paper representation
needed allowed measurement partial match domain theory  tgci 
accomplished simply counting matching features propagating information theory appropriately  either labyrinthk easily afford
measure partial match therefore appropriate problems best
representation final theory initial theory  kbann allows
finer grained measurement partial match neither mofn work 
price paid computational complexity  theory guided constructive induction framework allows variety potential tools varying degrees granularity
partial match  although one tool used experiments 

    flexible structure

discussed section      strength existing induction programs fashioning concise
highly predictive description concept target concept concisely
described given features  consequently  value domain theory lies
overall structure  feature language sucient  induction program build
good overall theory structure  instead  value domain theory lies information
contains redescribe examples using high level features  high level
features facilitate concise description target concept  systems either
neither mofn reach final theory series modifications initial
   

firerepresenting restructuring domain theories

theory hope gain something keeping theory s overall structure intact  initial
theory suciently close accurate theory  method works  often clinging
structure hinders full exploitation domain theory  theory guided constructive
induction provides means fully exploiting information domain theory
strengths existing induction programs  figure    section     gives comparison
structure initial promoter theory structure revised theory produced
theory guided constructive induction  substructures borrowed  revised
theory whole restructured 

    use standard induction underlying learner

theory guided constructive induction uses standard induction program
underlying learner  need reinvent solutions overfit avoidance  multi class
concepts  noisy data  etc  overfit avoidance widely studied standard induction 
many standard techniques exist  system modifies theory accommodate
set training examples must address issue overfit training examples 
many theory revision systems existing overfit avoidance techniques cannot easily adapted 
problem must addressed scratch  theory guided constructive induction
take advantage full range previous work overfit avoidance standard induction 
multiple theory parts available multi class concepts  interpreter
run multiple theory parts  resulting new feature sets combined 
primate splice junction domain presented section     three classes  intron exon
boundaries  exon intron boundaries  neither  theories given intron exon
exon intron  theories used create new features  new features
concatenated together learning  interpreters tgci  trivially handle
negation domain theory 

    use theory fragments

theory guided constructive induction limited using full domain theories 
part theory available  used  demonstrate this  three experiments
run fragments promoter domain theory used  first
experiment  four minus    rules used  five features constructed   one
feature rule additional feature group  similar experiments
run minus    group conformation group 
figure    gives learning curves three experiments along curves entire theory theory  c    using original features   although conformation
portion theory gives significant improvement c     minus   
minus    portions theory give significant improvements performance  thus even
partial theories theory fragments used theory guided constructive induction
yield sizable performance improvements 
use theory fragments explored means evaluating contribution
different parts theory  figure     conformation portion theory shown
yield improvement  could signal knowledge engineer knowledge
conveyed portion theory useful learner
present form 
   

fidonoho   rendell

  
c   
conformation portion theory
minus    portion theory
minus    portion theory
whole theory

    
  
    
  
    
  

  error

    
  
    
  
    
  
    
  
   
 
   
 
 

 

                                            
size training sample

figure     learning curves theory guided constructive induction fragments
promoter domain theory  minus    portion theory  minus   
portion theory  conformation portion theory used
separately feature construction  curves given full theory
c    alone comparison 

    use multiple theories

theory guided constructive induction use multiple competing even incompatible
domain theories  multiple theories exist  theory guided constructive induction provides
natural means integrating way extract best theories 
tgci  would called input theory producing new features  next  new
features simply pooled together induction program selects among
fashioning final theory  seen small scale promoter domain 
   

firerepresenting restructuring domain theories

  error

figure   minus    rules subsume minus    rules  according entry
uci database   the biological evidence inconclusive respect
correct specificity   handled simply using four possibilities  selection
useful knowledge left induction program 
tgci could used evaluate contributions competing theories
used evaluate theory fragments above  knowledge engineer could use evaluation
guide revision synthesis competing theories 
  
    
  
    
  
    
  
   
 
   
 

tgci using c   
tgci using lfc
    confidence lfc

 

                                            
size training sample

figure     theory guided constructive induction lfc c    underlying
learning system  theory guided constructive induction use inductive
learner underlying learning component  therefore  sophisticated
underlying induction programs improve accuracy 

    easy adoption new techniques

since theory guided constructive induction use standard induction method
underlying learner  improvements made standard induction  theory guided constructive induction passively improves  demonstrate this  tests run lfc
 ragavan   rendell        underlying induction program  lfc decision tree
learner performs example based constructive induction looking ahead combinations features  characteristically  lfc improves accuracy moderate number
examples  figure    shows resulting learning curve along c    tgci curve 
curves average    separate runs data partitions used
program  pairwise comparison improvement lfc c    significant
      level confidence training sets size        sophisticated underlying
induction programs improve accuracy 
   

fidonoho   rendell

   testing limits tgci
purpose section explore performance theory guided constructive
induction theory revision problems ranging easy dicult  easy problems
underlying concept embodied training testing examples matches domain
theory fairly closely  therefore  examples match domain theory fairly
closely  dicult problems underlying concept embodied examples
match domain theory well examples either  although many
factors determine diculty individual problem  aspect important component worth exploring  experiment section intended relate ranges
diculty amount improvement produced tgci 
since number factors affect problem diculty chose theory revision
problems experiment variations single problem 
able hold factors constant vary closeness match domain
theory  wanted avoid totally artificial domains  chose start
promoter domain create  new  domains perverting example set 
 new  domains created perverting examples original promoter
problem either closely match promoter domain theory less closely match
promoter domain theory  positive examples altered  example  one domain
created     fewer matches domain theory original promoter
domain follows  feature value given example examined see matched
part theory  so      probability  randomly reassigned new value
set possible values feature  end result set examples    
fewer matches domain theory original example set   experiment
new domains created                    fewer matches 
features  multiple values may match theory different disjuncts
theory specify different values single feature  example  referring back
figure    feature p    matches two minus    rules value another
two rules value t  single feature might accidentally match one part
theory fact example whole closely matches another part theory 
cases these  true matches separated accidental matches examining
part theory clearly matched example whole expecting
match part theory 
new domains closely matched theory created similar manner 
example  domain created     fewer mismatches domain theory
original promoter domain follows  feature value given example examined
see matched corresponding part theory  not      probability 
reassigned value matched theory  end result set examples
    mismatches domain theory eliminated  experiment
new domains created               fewer mismatches 
ten different example sets created level closeness domain theory 
                   fewer matches                fewer mismatches  total  forty
example sets created matched original theory less closely original
   precisely  would slightly matches     fewer matches features
would randomly reassigned back original matching value 

   

firerepresenting restructuring domain theories

  
  
  
  

  error

  

c   
tgci

  
  
  
  
  
 
 
    

   

   

   

   

 

  

  

  

  

   

closeness theory

figure     seven altered promoter domains created  three closely matched
theory original domains four less closely matched 
    x axis indicates domain positive examples match
domain theory       negative     indicates domain match
positive examples domain theory purely chance  accuracy
c    tgci plotted different levels proximity domain
theory 

example set  thirty example sets created matched original theory
closely original example set  example sets tested using leaveone out methodology using c    tgci algorithm  results summarized
figure     x axis measure theory proximity   closeness example set
domain theory      x axis indicates change original promoter examples 
      x axis means positive example exactly matches domain theory 
       x axis means match feature value positive example
   

fidonoho   rendell

domain theory totally chance    datapoint figure    result averaging
accuracies ten example sets level theory proximity  except
point zero accuracy exact original promoter examples  
one notable portion figure    section      x axis  domains
region greater trivial level mismatch domain theory
moderate mismatch  region tgci s best performance 
domains  tgci achieves high accuracy standard learner  c     using original
feature set gives mediocre performance  second region examine      
x axis level mismatch ranges moderate extreme  region
tgci s performance falls improvement original feature set remains high
shown figure    plots improvement tgci c     final two
regions notice greater    less     x axis  level
mismatch theory examples becomes trivially small  x axis greater     
c    able pick theory s patterns leading high accuracy approaches
tgci s  level mismatch becomes extreme  x axis less      theory gives
little help problem solving resulting similarly poor accuracy methods 
summary  shown figure    variants promoter problem wide range
theory proximity  centered around real promoter problem  theory guided
constructive induction yields sizable improvement standard learners 
  
    

error difference

  error

  
    
  
   
 
   
 
    

   

   

   

   

 

  

  

  

  

   

closeness theory

figure     difference error c    tgci different levels proximity
example set domain theory 

   scale        left half graph may directly comparable scale      
right half graph since equal number matches mismatches
original examples 

   

firerepresenting restructuring domain theories

   conclusion
goal paper present another new system  rather
study two qualities exible representation exible structure  capabilities
intended frame reference analyzing theory guided systems  two principles
provide guidelines purposeful design  distilled essence systems
miro  kbann  neither mofn  theory guided constructive induction
natural synthesis strengths  experiments demonstrated even
simple application two principles effectively integrate theory knowledge
training examples  yet much room improvement  two principles could
quantified made precise  implementations proceed
explored refined 
quantifying representational exibility one step  section   gave three degrees
exibility  one measured exact match theory  one counted number matching
conditions  one allowed weighted sum matching conditions  amount
exibility quantified  finer grained degrees exibility explored 
accuracy assorted domains evaluated function representational
exibility 
finer grained structural exibility would advantageous  presented systems
make small  incremental modifications theory lacking structural exibility  yet
theory guided constructive induction falls extreme  perhaps allowing excessive
structural exibility  fortunately  existing induction tools capable fashioning simple
yet highly predictive theory structures problem features suitably high level 
nevertheless  approaches explored take advantage structure
initial theory without unduly restricted it 
strength discussed section     given attention  although
promoter domain gives small example synthesizing competing theories 
explored domain entire competing  inconsistent theories available
synthesizing knowledge given multiple experts  point made section    
tgci use theory fragments evaluate contribution different parts
theory  explored further 
exploration bias standard induction  utgoff        refers biases ranging
weak strong incorrect correct  strong bias restricts concepts
represented weak bias thus providing guidance learning 
bias becomes stronger  may become incorrect ruling useful concept descriptions  similar situation arises theory revision   theory representation language
inappropriately rigid may impose strong  incorrect bias revision  language
allows adaptability along many dimensions may provide weak bias  grendellike toolbox would allow theory translated range representations
varying dimensions adaptability  utgoff advocates starting strong  possibly incorrect bias shifting appropriately weak correct bias  similarly  theory could
translated successively adaptable representations appropriate bias
found  implemented single tool  many open problems remain along line
research 
   

fidonoho   rendell

converse relationship theory revision constructive induction warrants
examination   theory revision uses data improve theory  constructive induction
use theory improve data facilitate learning  since long term goal machine
learning use data  inference  theory improve them  believe
consideration related methods beneficial  particularly
research area strengths lacks 
analysis landmark theory revision theory guided learning systems led
two principles exible representation exible structure  theory guided
constructive induction based upon high level principles  simple yet achieves
good accuracy  principles provide guidelines future work  yet discussed above 
principles imprecise call exploration 

acknowledgements
would thank geoff towell  kevin thompson  ray mooney  jeff mahoney
assistance getting datapoints kbann  labyrinthk   either  would
thank paul baffes making neither program available advice
setting program s parameters  thank anonymous reviewers constructive
criticism earlier draft paper  gratefully acknowledge support
work dod graduate fellowship nsf grant iri          

references
baffes  p     mooney  r          symbolic revision theories m of n rules 
proceedings      ijcai 
bloedorn  e   michalski  r     wnek  j          multistrategy constructive induction 
aq   mci  proceeding second international workshop multistrategy learning 
clark  p     matwin  s          using qualitative models guide inductive learning 
proceedings      international conference machine learning 
cohen  w          compiling prior knowledge explicit bias  proceedings
     international conference machine learning 
craven  m  w     shavlik  j  w          investigating value good input representation  computational learning theory natural learning systems     forthcoming 
drastal  g     raatz  s          empirical results learning abstraction space 
proceedings      ijcai 
dzerisko  s     lavrac  n          learning relations noisy examples  empirical
comparison linus foil  proceedings      international conference
machine learning 
   

firerepresenting restructuring domain theories

feldman  r   serge  a     koppel  m          incremental refinement approximate
domain theories  proceedings      international conference machine
learning 
flann  n     dietterich  t          study explanation based methods inductive
learning  machine learning             
fu  l  m     buchanan  b  g          learning intermediate concepts constructing
hierarchical knowledge base  proceedings      ijcai 
harley  c   reynolds  r     noordewier  m          creators original promoter dataset 
hirsh  h     noordewier  m          using background knowledge improve inductive
learning dna sequences  tenth ieee conference ai applications san
antonio  tx 
matheus  c  j     rendell  l  a          constructive induction decision trees 
proceedings      ijcai 
michalski  r  s          theory methodology inductive learning  artificial intelligence                  
mitchell  t          version spaces  candidate elimination approach rule learning 
proceedings      ijcai 
mooney  r  j          induction unexplained  using overly general domain theories
aid concept learning  machine learning                 
mooney  r  j   shavlik  j  w   towell  g  g     gove  a          experimental comparison symbolic connectionist learning algorithms  proceedings     
ijcai 
murphy  p     pazzani  m          id  of    constructive induction m of n concepts
discriminators decision trees  proceedings      international conference
machine learning 
noordewier  m   shavlik  j     towell  g          donors original primate splice junction
dataset 
ourston  d     mooney  r          changing rules  comprehensive approach theory
refinement  proceedings      national conference artificial intelligence 
pagallo  g     haussler  d          boolean feature discovery empirical learning  machine
learning               
pazzani  m     kibler  d          utility knowledge inductive learning  machine
learning               
quinlan  j  r          c     programs machine learning  san mateo  ca  morgan
kaufmann 
   

fidonoho   rendell

ragavan  h     rendell  l          lookahead feature construction learning hard concepts  proceedings      international conference machine learning 
rumelhart  d  e   hinton  g  e     mcclelland  j  l          general framework
parallel distributed processing  rumelhart  d  e     mcclelland  j  l   eds   
parallel distributed processing  explorations microarchitecture cognition 
volume i  cambridge  ma  mit press 
schlimmer  j  c          learning representation change  kaufmann  m   ed   
proceedings      national conference artificial intelligence 
thompson  k   langley  p     iba  w          using background knowledge concept
formation  proceedings      international conference machine learning 
towell  g     shavlik  j          knowledge based artificial neural networks  artificial
intelligence              
towell  g   shavlik  j     noordeweir  m          refinement approximately correct
domain theories knowledge based neural networks  proceedings     
national conference artificial intelligence 
utgoff  p  e          shift bias inductive concept learning  michalski  carbonell 
  mitchell  eds    machine learning  vol     chap     pp           san mateo  ca 
morgan kaufmann 
wogulis  j          revising relational domain theories  proceedings      international conference machine learning 

   


