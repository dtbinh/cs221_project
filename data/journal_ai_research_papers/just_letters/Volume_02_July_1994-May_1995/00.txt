journal of artificial intelligence research              

submitted       published     

a system for induction of oblique decision trees
sreerama k  murthy
simon kasif
steven salzberg

department of computer science
johns hopkins university  baltimore  md       usa

murthy cs jhu edu
kasif cs jhu edu
salzberg cs jhu edu

abstract

this article describes a new system for induction of oblique decision trees  this system 
oc   combines deterministic hill climbing with two forms of randomization to find a good
oblique split  in the form of a hyperplane  at each node of a decision tree  oblique decision
tree methods are tuned especially for domains in which the attributes are numeric  although
they can be adapted to symbolic or mixed symbolic numeric attributes  we present extensive empirical studies  using both real and artificial data  that analyze oc  s ability to
construct oblique trees that are smaller and more accurate than their axis parallel counterparts  we also examine the benefits of randomization for the construction of oblique
decision trees 

   introduction
current data collection technology provides a unique challenge and opportunity for automated machine learning techniques  the advent of major scientific projects such as the
human genome project  the hubble space telescope  and the human brain mapping initiative are generating enormous amounts of data on a daily basis  these streams of data
require automated methods to analyze  filter  and classify them before presenting them in
digested form to a domain scientist  decision trees are a particularly useful tool in this context because they perform classification by a sequence of simple  easy to understand tests
whose semantics is intuitively clear to domain experts  decision trees have been used for
classification and other tasks since the     s  moret        safavin   landgrebe         in
the      s  breiman et al  s book on classification and regression trees  cart  and quinlan s work on id   quinlan              provided the foundations for what has become a
large body of research on one of the central techniques of experimental machine learning 
many variants of decision tree  dt  algorithms have been introduced in the last decade 
much of this work has concentrated on decision trees in which each node checks the value
of a single attribute  breiman  friedman  olshen    stone        quinlan            a  
quinlan initially proposed decision trees for classification in domains with symbolic valued
attributes         and later extended them to numeric domains         when the attributes
are numeric  the tests have the form xi   k  where xi is one of the attributes of an example
and k is a constant  this class of decision trees may be called axis parallel  because the tests
at each node are equivalent to axis parallel hyperplanes in the attribute space  an example
of such a decision tree is given in figure    which shows both a tree and the partitioning it
creates in a   d attribute space 

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fifigure    the left side of the figure shows a simple axis parallel tree that uses two attributes 
the right side shows the partitioning that this tree creates in the attribute space 
researchers have also studied decision trees in which the test at a node uses boolean
combinations of attributes  pagallo        pagallo   haussler        sahami        and
linear combinations of attributes  see section     different methods for measuring the
goodness of decision tree nodes  as well as techniques for pruning a tree to reduce overfitting
and increase accuracy have also been explored  and will be discussed in later sections 
in this paper  we examine decision trees that test a linear combination of the attributes
at each internal node  more precisely  let an example take the form x   x    x          xd  cj
where cj is a class label and the xi  s are real valued attributes   the test at each node will
then have the form 
d
x
ai xi   ad      
   
i  

where a           ad   are real valued coecients  because these tests are equivalent to hyperplanes at an oblique orientation to the axes  we call this class of decision trees oblique
decision trees   trees of this form have also been called  multivariate   brodley   utgoff 
       we prefer the term  oblique  because  multivariate  includes non linear combinations of the variables  i e   curved surfaces  our trees contain only linear tests   it is clear
that these are simply a more general form of axis parallel trees  since by setting ai    
for all coecients but one  the test in eq    becomes the familiar univariate test  note
that oblique decision trees produce polygonal  polyhedral  partitionings of the attribute
space  while axis parallel trees produce partitionings in the form of hyper rectangles that
are parallel to the feature axes 
it should be intuitively clear that when the underlying concept is defined by a polygonal space partitioning  it is preferable to use oblique decision trees for classification  for
example  there exist many domains in which one or two oblique hyperplanes will be the
best model to use for classification  in such domains  axis parallel methods will have to ap   the constraint that x            xd are real valued does not necessarily restrict oblique decision trees to
numeric domains  several researchers have studied the problem of converting symbolic  unordered 
domains to numeric  ordered  domains and vice versa  e g    breiman et al         hampson   volper 
      utgoff   brodley        van de merckt               to keep the discussion simple  however  we
will assume that all attributes have numeric values 

 

fifigure    the left side shows a simple   d domain in which two oblique hyperplanes define
the classes  the right side shows an approximation of the sort that an axis parallel
decision tree would have to create to model this domain 
proximate the correct model with a staircase like structure  while an oblique tree building
method could capture it with a tree that was both smaller and more accurate   figure  
gives an illustration 
breiman et al  first suggested a method for inducing oblique decision trees in       however  there has been very little further research on such trees until relatively recently  utgoff
  brodley        heath  kasif    salzberg      b  murthy  kasif  salzberg    beigel       
brodley   utgoff         a comparison of existing approaches is given in more detail in
section    the purpose of this study is to review the strengths and weaknesses of existing
methods  to design a system that combines some of the strengths and overcomes the weaknesses  and to evaluate that system empirically and analytically  the main contributions
and conclusions of our study are as follows 

 we have developed a new  randomized algorithm for inducing oblique decision trees

from examples  this algorithm extends the original      work of breiman et al 
randomization helps significantly in learning many concepts 

 our algorithm is fully implemented as an oblique decision tree induction system and
is available over the internet  the code can be retrieved from online appendix   of
this paper  or by anonymous ftp from ftp   ftp cs jhu edu pub oc  oc  tar z  

 the randomized hill climbing algorithm used in oc  is more ecient than other

existing randomized oblique decision tree methods  described below   in fact  the
current implementation of oc  guarantees a worst case running time that is only
o log n  times greater than the worst case time for inducing axis parallel trees  i e  
o dn  log n  vs  o dn    

 the ability to generate oblique trees often produces very small trees compared to
axis parallel methods  when the underlying problem requires an oblique split  oblique

   note that though a given oblique tree may have fewer leaf nodes than an axis parallel tree which is what
we mean by  smaller  the oblique tree may in some cases be larger in terms of information content 
because of the increased complexity of the tests at each node 

 

fimurthy  kasif   salzberg

trees are also more accurate than axis parallel trees  allowing a tree building system
to use both oblique and axis parallel splits broadens the range of domains for which
the system should be useful 
the remaining sections of the paper follow this outline  the remainder of this section
briey outlines the general paradigm of decision tree induction  and discusses the complexity issues involved in inducing oblique decision trees  section   briey reviews some
existing techniques for oblique dt induction  outlines some limitations of each approach 
and introduces the oc  system  section   describes the oc  system in detail  section  
describes experiments that     compare the performance of oc  to that of several other
axis parallel and oblique decision tree induction methods on a range of real world datasets
and     demonstrate empirically that oc  significantly benefits from its randomization
methods  in section    we conclude with some discussion of open problems and directions
for further research 

    top down induction of decision trees

algorithms for inducing decision trees follow an approach described by quinlan as top down
induction of decision trees         this can also be called a greedy divide and conquer
method  the basic outline is as follows 
   begin with a set of examples called the training set  t   if all examples in t belong
to one class  then halt 
   consider all tests that divide t into two or more subsets  score each test according
to how well it splits up the examples 
   choose   greedily   the test that scores the highest 
   divide the examples into subsets and run this procedure recursively on each subset 
quinlan s original model only considered attributes with symbolic values  in that model 
a test at a node splits an attribute into all of its values  thus a test on an attribute
with three values will have at most three child nodes  one corresponding to each value 
the algorithm considers all possible tests and chooses the one that optimizes a pre defined
goodness measure   one could also split symbolic values into two or more subsets of values 
which gives many more choices for how to split the examples   as we explain next  oblique
decision tree methods cannot consider all tests due to complexity considerations 

    complexity of induction of oblique decision trees

one reason for the relatively few papers on the problem of inducing oblique decision trees is
the increased computational complexity of the problem when compared to the axis parallel
case  there are two important issues that must be addressed  in the context of top down
decision tree algorithms  we must address the complexity of finding optimal separating
hyperplanes  decision surfaces  for a given node of a decision tree  an optimal hyperplane
will minimize the impurity measure used  e g   impurity might be measured by the total
number of examples mis classified  the second issue is the lower bound on the complexity
of finding optimal  e g   smallest size  trees 
 

fifigure    for n points in d dimensions
 n  d   there are n  d distinct axis parallel splits 
  
while there are  d  nd distinct d dimensional oblique splits  this shows all distinct
oblique and axis parallel splits for two specific points in   d 
let us first consider the issue of the complexity of selecting an optimal oblique hyperplane for a single node of a tree  in a domain with
n training instances  each described using
  
d real valued attributes  there are at most  d  nd distinct d dimensional oblique splits  i e  
hyperplanes  that divide the training instances uniquely into two nonoverlapping subsets 
this upper bound derives from the observation that every subset of size d from the n points
can define a d dimensional hyperplane  and each such hyperplane can be rotated slightly
in  d directions to divide the set of d points in all possible ways  figure   illustrates these
upper limits for two points in two dimensions  for axis parallel splits  there are only n  d
distinct possibilities  and axis parallel methods such as c     quinlan      a  and cart
 breiman et al         can exhaustively search for the best split at each node  the problem
of searching for the best oblique split is therefore much more dicult than that of searching
for the best axis parallel split  in fact  the problem is np hard 
more precisely  heath        proved that the following problem is np hard  given a
set of labelled examples  find the hyperplane that minimizes the number of misclassified
examples both above and below the hyperplane  this result implies that any method
for finding the optimal oblique split is likely to have exponential cost
 assuming p    np   
  
intuitively  the problem is that it is impractical to enumerate all  d  nd distinct hyperplanes
and choose the best  as is done in axis parallel decision trees  however  any non exhaustive
deterministic algorithm for searching through all these hyperplanes is prone to getting stuck
in local minima 
   throughout the paper  we use the terms  split  and  hyperplane  interchangeably to refer to the test
at a node of a decision tree  the first usage is standard  moret         and refers to the fact that the
test splits the data into two partitions  the second usage refers to the geometric form of the test 

 

fimurthy  kasif   salzberg

on the other hand  it is possible to define impurity measures for which the problem
of finding optimal hyperplanes can be solved in polynomial time  for example  if one
minimizes the sum of distances of mis classified examples  then the optimal solution can
be found using linear programming methods  if distance is measured along one dimension
only   however  classifiers are usually judged by how many points they classify correctly 
regardless of how close to the decision boundary a point may lie  thus most of the standard
measures for computing impurity base their calculation on the discrete number of examples
of each category on either side of the hyperplane  section     discusses several commonly
used impurity measures 
now let us address the second issue  that of the complexity of building a small tree 
it is easy to show that the problem of inducing the smallest axis parallel decision tree is
np hard  this observation follows directly from the work of hyafil and rivest         note
that one can generate the smallest axis parallel tree that is consistent with the training
set in polynomial time if the number of attributes is a constant  this can be done by
using dynamic programming or branch and bound techniques  see moret        for several
pointers   but when the tree uses oblique splits  it is not clear  even for a fixed number
of attributes  how to generate an optimal  e g   smallest  decision tree in polynomial time 
this suggests that the complexity of constructing good oblique trees is greater than that
for axis parallel trees 
it is also easy to see that the problem of constructing an optimal  e g   smallest  oblique
decision tree is np hard  this conclusion follows from the work of blum and rivest        
their result implies that in d dimensions  i e   with d attributes  the problem of producing
a   node oblique decision tree that is consistent with the training set is np complete  more
specifically  they show that the following decision problem is np complete  given a training
set t with n examples and d boolean attributes  does there exist a   node neural network
consistent with t   from this it is easy to show that the following question is np complete 
given a training set t   does there exist a   leaf node oblique decision tree consistent with
t 
as a result of these complexity considerations  we took the pragmatic approach of trying
to generate small trees  but not looking for the smallest tree  the greedy approach used by
oc  and virtually all other decision tree algorithms implicitly tries to generate small trees 
in addition  it is easy to construct example problems for which the optimal split at a node
will not lead to the best tree  thus our philosophy as embodied in oc  is to find locally
good splits  but not to spend excessive computational effort on improving the quality of
these splits 

   previous work on oblique decision tree induction
before describing the oc  algorithm  we will briey discuss some existing oblique dt
induction methods  including cart with linear combinations  linear machine decision
trees  and simulated annealing of decision trees  there are also methods that induce
tree like classifiers with linear discriminants at each node  most notably methods using
linear programming  mangasarian  setiono    wolberg        bennett   mangasarian 
          a      b   though these methods can find the optimal linear discriminants for
specific goodness measures  the size of the linear program grows very fast with the number
 

fiinduction of oblique decision trees

to induce a split at node t of the decision tree 
normalize values for all d attributes 
l  
while  true 
l   l  

let the current split sl be v  c  where v   pdi   ai xi 
for i             d
for                
search for the  that maximizes the goodness of the split v    ai      c 
let    be the settings that result in highest goodness in these   searches 
ai   ai       c   c        
perturb c to maximize the goodness of sl   keeping a           ad constant 
if jgoodness sl    goodness sl   j   exit while loop 
eliminate irrelevant attributes in fa          ad g using backward elimination 
convert sl to a split on the un normalized attributes 
return the better of sl and the best axis parallel split as the split for t  
figure    the procedure used by cart with linear combinations  cart lc  at each node
of a decision tree 
of instances and the number of attributes  there is also some less closely related work on
algorithms to train artificial neural networks to build decision tree like classifiers  brent 
      cios   liu        herman   yeung        
the first oblique decision tree algorithm to be proposed was cart with linear combinations  breiman et al         chapter     this algorithm  referred to henceforth as cart lc 
is an important basis for oc   figure   summarizes  using breiman et al  s notation  what
the cart lc algorithm does at each node in the decision tree  the core idea of the cartlc algorithm is how it finds the value of  that maximizes the goodness of a split  this
idea is also used in oc   and is explained in detail in section     
after describing cart lc  breiman et al  point out that there is still much room for
further development of the algorithm  oc  represents an extension of cart lc that
includes some significant additions  it addresses the following limitations of cart lc 

 cart lc is fully deterministic  there is no built in mechanism for escaping local

minima  although such minima may be very common for some domains  figure  
shows a simple example for which cart lc gets stuck 

 cart lc produces only a single tree for a given data set 
 cart lc sometimes makes adjustments that increase the impurity of a split  this
feature was probably included to allow it to escape some local minima 

 there is no upper bound on the time spent at any node in the decision tree  it halts
when no perturbation changes the impurity more than   but because impurity may
increase and decrease  the algorithm can spend arbitrarily long time at a node 
 

fimurthy  kasif   salzberg

 

oc 

 

 

initial loc 

 

 

cart lc

 

 

 

figure    the deterministic perturbation algorithm of cart lc fails to find the correct
split for this data  even when it starts from the location of the best axis parallel
split  oc  finds the correct split using one random jump 
another oblique decision tree algorithm  one that uses a very different approach from
cart lc  is the linear machine decision trees  lmdt  system  utgoff   brodley       
brodley   utgoff         which is a successor to the perceptron tree method  utgoff       
utgoff   brodley         each internal node in an lmdt tree is a linear machine  nilsson 
       the training algorithm presents examples repeatedly at each node until the linear
machine converges  because convergence cannot be guaranteed  lmdt uses heuristics to
determine when the node has stabilized  to make the training stable even when the set of
training instances is not linearly separable  a  thermal training  method  frean        is
used  similar to simulated annealing 
a third system that creates oblique trees is simulated annealing of decision trees
 sadt   heath et al       b  which  like oc   uses randomization  sadt uses simulated
annealing  kirkpatrick  gelatt    vecci        to find good values for the coecients of
the hyperplane at each node of a tree  sadt first places a hyperplane in a canonical
location  and then iteratively perturbs all the coecients by small random amounts  initially  when the temperature parameter is high  sadt accepts almost any perturbation of
the hyperplane  regardless of how it changes the goodness score  however  as the system
 cools down   only changes that improve the goodness of the split are likely to be accepted 
though sadt s use of randomization allows it to effectively avoid some local minima  it
compromises on eciency  it runs much slower than either cart lc  lmdt or oc  
sometimes considering tens of thousands of hyperplanes at a single node before it finishes
annealing 
our experiments in section     include some results showing how all of these methods
perform on three artificial domains 
we next describe a way to combine some of the strengths of the methods just mentioned 
while avoiding some of the problems  our algorithm  oc   uses deterministic hill climbing
most of the time  ensuring computational eciency  in addition  it uses two kinds of
randomization to avoid local minima  by limiting the number of random choices  the
algorithm is guaranteed to spend only polynomial time at each node in the tree  in addition 
randomization itself has produced several benefits  for example  it means that the algorithm
 

fiinduction of oblique decision trees

to find a split of a set of examples t  
find the best axis parallel split of t   let i be the impurity of this split 
repeat r times 
choose a random hyperplane h  
 for the first iteration  initialize h to be the best axis parallel split  
step    until the impurity measure does not improve  do 
perturb each of the coecients of h in sequence 
step    repeat at most j times 
choose a random direction and attempt to perturb h in that direction 
if this reduces the impurity of h   go to step   
let i    the impurity of h   if i    i   then set i   i   
output the split corresponding to i  
figure    overview of the oc  algorithm for a single node of a decision tree 
can produce many different trees for the same data set  this offers the possibility of a new
family of classifiers  k decision tree algorithms  in which an example is classified by the
majority vote of k trees  heath et al       a  have shown that k decision tree methods
 which they call k dt  will consistently outperform single tree methods if classification
accuracy is the main criterion  finally  our experiments indicate that oc  eciently finds
small  accurate decision trees for many different types of classification problems 

   oblique classifier    oc  

in this section we discuss details of the oblique decision tree induction system oc   as
part of this description  we include 
 the method for finding coecients of a hyperplane at each tree node 
 methods for computing the impurity or goodness of a hyperplane 
 a tree pruning strategy  and
 methods for coping with missing and irrelevant attributes 
section     focuses on the most complicated of these algorithmic details  i e  the question of
how to find a hyperplane that splits a given set of instances into two reasonably  pure  nonoverlapping subsets  this randomized perturbation algorithm is the main novel contribution
of oc   figure   summarizes the basic oc  algorithm  used at each node of a decision
tree  this figure will be explained further in the following sections 

    perturbation algorithm

oc  imposes no restrictions on the orientation of the hyperplanes  however  in order to be
at least as powerful as standard dt methods  it first finds the best axis parallel  univariate 
split at a node before looking for an oblique split  oc  uses an oblique split only when it
improves over the best axis parallel split  
   as pointed out in  breiman et al         chapter     it does not make sense to use an oblique split when
the number of examples at a node n is less than or almost equal to the number of features d  because the

 

fimurthy  kasif   salzberg

the search strategy for the space of possible hyperplanes is defined by the procedure
that perturbs the current hyperplane h to a new location  because there are an exponential
number of distinct ways to partition the examples with a hyperplane  any procedure that
simply enumerates all of them will be unreasonably costly  the two main alternatives
considered in the past have been simulated annealing  used in the sadt system  heath
et al       b   and deterministic heuristic search  as in cart lc  breiman et al         
oc  combines these two ideas  using heuristic search until it finds a local minimum  and
then using a non deterministic search step to get out of the local minimum   the nondeterministic step in oc  is not simulated annealing  however  
we will start by explaining how we perturb a hyperplane to split the training set t at
a node of the decision tree  let n be the number of examples in t   d be the number of
attributes  or dimensions  for each example  and k be the number of categories  then we
can write tj    xj     xj           xjd  cj   for the j th example from the training set t   where xji is
the value of attribute i and cj is the category label  as defined p
in eq     the equation of the
current hyperplane h at a node of the decision tree is written as di  
 a x   ad        if we
pd i i
substitute a point  an example  tj into the equation for h   we get i    aixji    ad     vj  
where the sign of vj tells us whether the point tj is above or below the hyperplane h  
i e   if vj      then tj is above h   if h splits the training set t perfectly  then all points
belonging to the same category will have the same sign for vj   i e   sign vi    sign vj   iff
category ti    category tj   
oc  adjusts the coecients of h individually  finding a locally optimal value for one
coecient at a time  this key idea was introduced by breiman et al  it works as follows 
treat the coecient am as a variable  and treat all other coecients as constants  then
vj can be viewed as a function of am   in particular  the condition that tj is above h is
equivalent to
vj    
  uj
am   am xxjm   vj def
jm

   

assuming that xjm      which we ensure by normalization  using this definition of uj   the
point tj is above h if am   uj   and below otherwise  by plugging all the points from t
into this equation  we will obtain n constraints on the value of am  
the problem then is to find a value for am that satisfies as many of these constraints
as possible   if all the constraints are satisfied  then we have a perfect split   this problem
is easy to solve optimally  simply sort all the values uj   and consider setting am to the
midpoint between each pair of different values  this is illustrated in figure    in the figure 
the categories are indicated by font size  the larger ui  s belong to one category  and the
smaller to another  for each distinct placement of the coecient am   oc  computes the
impurity of the resulting split  e g   for the location between u  and u  illustrated here  two
examples on the left and one example on the right would be misclassified  see section      
for different ways of computing impurity   as the figure illustrates  the problem is simply
to find the best one dimensional split of the u s  which requires considering just n     values
for am   the value a m obtained by solving this one dimensional problem is then considered
data underfits the concept  by default  oc  uses only axis parallel splits at tree nodes at which n    d 
the user can vary this threshold 

  

fifigure    finding the optimal value for a single coecient am   large u s correspond to
examples in one category and small u s to another 

perturb h m 
for j             n
compute uj  eq    
sort u          un in non decreasing order 
a m   best univariate split of the sorted uj s 
h    result of substituting a m for am in h  
if  impurity h      impurity h  
f am   a m   pmove   pstag g
else if  impurity h    impurity h    
f am   a m with probability pmove
pmove   pmove        pstag g
figure    perturbation algorithm for a single coecient am  

as a replacement for am   let h  be the hyperplane obtained by  perturbing  am to a m   if
h has better  lower  impurity than h   then h  is discarded  if h  has lower impurity  h 
becomes the new location of the hyperplane  if h and h  have identical impurities  then
h  replaces h with probability pstag    figure   contains pseudocode for our perturbation
procedure 
now that we have a method for locally improving a coecient of a hyperplane  we need
to decide which of the d     coecients to pick for perturbation  we experimented with
three different methods for choosing which coecient to adjust  namely  sequential  best
first and random 
seq  repeat until none of the coecient values is modified in the for loop 
for i     to d  perturb h  i 
best  repeat until coecient m remains unmodified 
m   coecient which when perturbed  results in the
maximum improvement of the impurity measure 
perturb h  m 
r     repeat a fixed number of times     in our experiments  
m   random integer between   and d    
perturb h  m 
   the parameter pstag   denoting  stagnation probability   is the probability that a hyperplane is perturbed
to a location that does not change the impurity measure  to prevent the impurity from remaining
stagnant for a long time  pstag decreases by a constant amount each time oc  makes a  stagnant 
perturbation  thus only a constant number of such perturbations will occur at each node  this constant
can be set by the user  pstag is reset to   every time the global impurity measure is improved 

  

fimurthy  kasif   salzberg

our previous experiments  murthy et al         indicated that the order of perturbation
of the coecients does not affect the classification accuracy as much as other parameters 
especially the randomization parameters  see below   since none of these orders was uniformly better than any other  we used sequential  seq  perturbation for all the experiments
reported in section   

    randomization

the perturbation algorithm halts when the split reaches a local minimum of the impurity
measure  for oc  s search space  a local minimum occurs when no perturbation of any
single coecient of the current hyperplane will decrease the impurity measure   of course 
a local minimum may also be a global minimum   we have implemented two ways of
attempting to escape local minima  perturbing the hyperplane with a random vector  and
re starting the perturbation algorithm with a different random initial hyperplane 
the technique of perturbing the hyperplane with a random vector works as follows 
when the system reaches a local minimum  it chooses a random vector to add to the
coecients of the current hyperplane  it then computes the optimal amount by which the
hyperplane should bep perturbed along this random direction  to be more precise  when
a hyperplane h   di   ai xi   ad   cannot be improved by deterministic perturbation 
oc  repeats the following loop j times  where j is a user specified parameter  set to   by
default  

 choose a random vector r    r   r          rd    
 let ff be the amount
by which we want to perturb h in the direction r  in other
pd
words  let h    i    ai   ffri  xi    ad     ffrd     
 find the optimal value for ff 
 if the hyperplane h  thus obtained decreases the overall impurity  replace h with h  
exit this loop and begin the deterministic perturbation algorithm for the individual
coecients 

note that we can treat ff as the only variable in the equation for h    therefore each of the
n examples in t   if plugged into the equation for h   imposes a constraint on the value of
ff  oc  therefore can use its coecient perturbation method  see section      to compute
the best value of ff  if j random jumps fail to improve the impurity  oc  halts and uses
h as the split for the current tree node 
an intuitive way of understanding this random jump is to look atpthe dual space in which
the algorithm is actually searching  note that the equation h   di   ai xi   ad   defines
a space in which the axes are the coecients ai rather than the attributes xi   every point
in this space defines a distinct hyperplane in the original formulation  the deterministic
algorithm used in oc  picks a hyperplane and then adjusts coecients one at a time  thus
in the dual space  oc  chooses a point and perturbs it by moving it parallel to the axes 
the random vector r represents a random direction in this space  by finding the best value
for ff  oc  finds the best distance to adjust the hyperplane in the direction of r 
  

fiinduction of oblique decision trees

note that this additional perturbation in a random direction does not significantly increase the time complexity of the algorithm  see appendix a   we found in our experiments
that even a single random jump  when used at a local minimum  proves to be very helpful 
classification accuracy improved for every one of our data sets when such perturbations
were made  see section     for some examples 
the second technique for avoiding local minima is a variation on the idea of performing
multiple local searches  the technique of multiple local searches is a natural extension
to local search  and has been widely mentioned in the optimization literature  see roth
       for an early example   because most of the steps of our perturbation algorithm
are deterministic  the initial hyperplane largely determines which local minimum will be
encountered first  perturbing a single initial hyperplane is thus unlikely to lead to the best
split of a given data set  in cases where the random perturbation method fails to escape
from local minima  it may be helpful to simply start afresh with a new initial hyperplane 
we use the word restart to denote one run of the perturbation algorithms  at one node of
the decision tree  using one random initial hyperplane   that is  a restart cycles through
and perturbs the coecients one at a time and then tries to perturb the hyperplane in a
random direction when the algorithm reaches a local minimum  if this last perturbation
reduces the impurity  the algorithm goes back to perturbing the coecients one at a time 
the restart ends when neither the deterministic local search nor the random jump can find
a better split  one of the optional parameters to oc  specifies how many restarts to use 
if more than one restart is used  then the best hyperplane found thus far is always saved 
in all our experiments  the classification accuracies increased with more than one restart 
accuracy tended to increase up to a point and then level off  after about       restarts 
depending on the domain   overall  the use of multiple initial hyperplanes substantially
improved the quality of the decision trees found  see section     for some examples  
by carefully combining hill climbing and randomization  oc  ensures a worst case time
of o dn  log n  for inducing a decision tree  see appendix a for a derivation of this upper
bound 

best axis parallel split  it is clear that axis parallel splits are more suitable for some

data distributions than oblique splits  to take into account such distributions  oc  computes the best axis parallel split and an oblique split at each node  and then picks the better
of the two   calculating the best axis parallel split takes an additional o dn log n  time 
and so does not increase the asymptotic time complexity of oc   as a simple variant of
the oc  system  the user can opt to  switch off  the oblique perturbations  thus building
an axis parallel tree on the training data  section     empirically demonstrates that this
axis parallel variant of oc  compares favorably with existing axis parallel algorithms 
   the first run through the algorithm at each node always begins at the location of the best axis parallel
hyperplane  all subsequent restarts begin at random locations 
   sometimes a simple axis parallel split is preferable to an oblique split  even if the oblique split has slightly
lower impurity  the user can specify such a bias as an input parameter to oc  

  

fimurthy  kasif   salzberg

    other details
      impurity measures

oc  attempts to divide the d dimensional attribute space into homogeneous regions  i e  
regions that contain examples from just one category  the goal of adding new nodes to
a tree is to split up the sample space so as to minimize the  impurity  of the training
set  some algorithms measure  goodness  instead of impurity  the difference being that
goodness values should be maximized while impurity should be minimized  many different
measures of impurity have been studied  breiman et al         quinlan        mingers 
    b  buntine   niblett        fayyad   irani        heath et al       b  
the oc  system is designed to work with a large class of impurity measures  stated
simply  if the impurity measure uses only the counts of examples belonging to every category
on both sides of a split  then oc  can use it   see murthy and salzberg        for ways of
mapping other kinds of impurity measures to this class of impurity measures   the user can
plug in any impurity measure that fits this description  the oc  implementation includes
six impurity measures  namely 
  
  
  
  
  
  

information gain
the gini index
the twoing rule
max minority
sum minority
sum of variances

though all six of the measures have been defined elsewhere in the literature  in some
cases we have made slight modifications that are defined precisely in appendix b  our
experiments indicated that  on average  information gain  gini index and the twoing rule
perform better than the other three measures for both axis parallel and oblique trees  the
twoing rule is the current default impurity measure for oc   and it was used in all of
the experiments reported in section    there are  however  artificial data sets for which
sum minority and or max minority perform much better than the rest of the measures 
for instance  sum minority easily induces the exact tree for the pol data set described in
section        while all other methods have diculty finding the best tree 

twoing rule  the twoing rule was first proposed by breiman et al          the value

to be computed is defined as 

k
x

twoingvalue    jtlj n    jtrj n    

i  

jli jtlj   ri jtrjj  

where jtlj  jtrj  is the number of examples on the left  right  of a split at node t   n is
the number of examples at node t   and li  ri   is the number of examples in category i on
the left  right  of the split  the twoingvalue is actually a goodness measure rather than
an impurity measure  therefore oc  attempts to minimize the reciprocal of this value 
the remaining five impurity measures implemented in oc  are defined in appendix b 
  

fiinduction of oblique decision trees

      pruning

virtually all decision tree induction systems prune the trees they create in order to avoid
overfitting the data  many studies have found that judicious pruning results in both smaller
and more accurate classifiers  for decision trees as well as other types of machine learning
systems  quinlan        niblett        cestnik  kononenko    bratko        kodratoff
  manago        cohen        hassibi   stork        wolpert        schaffer        
for the oc  system we implemented an existing pruning method  but note that any tree
pruning method will work fine within oc   based on the experimental evaluations of
mingers      a  and other work cited above  we chose breiman et al  s cost complexity
 cc  pruning        as the default pruning method for oc   this method  which is also
called error complexity or weakest link pruning  requires a separate pruning set  the
pruning set can be a randomly chosen subset of the training set  or it can be approximated
using cross validation  oc  randomly chooses      the default value  of the training data
to use for pruning  in the experiments reported below  we only used this default value 
briey  the idea behind cc pruning is to create a set of trees of decreasing size from the
original  complete tree  all these trees are used to classify the pruning set  and accuracy is
estimated from that  cc pruning then chooses the smallest tree whose accuracy is within k
standard errors squared of the best accuracy obtained  when the   se rule  k      is used 
the tree with highest accuracy on the pruning set is selected  when k      smaller tree size
is preferred over higher accuracy  for details of cost complexity pruning  see breiman et
al         or mingers      a  
      irrelevant attributes

irrelevant attributes pose a significant problem for most machine learning methods  breiman
et al         aha        almuallin   dietterich        kira   rendell        salzberg       
cardie        schlimmer        langley   sage        brodley   utgoff         decision
tree algorithms  even axis parallel ones  can be confused by too many irrelevant attributes 
because oblique decision trees learn the coecients of each attribute at a dt node  one
might hope that the values chosen for each coecient would reect the relative importance
of the corresponding attributes  clearly  though  the process of searching for good coecient
values will be much more ecient when there are fewer attributes  the search space is much
smaller  for this reason  oblique dt induction methods can benefit substantially by using a
feature selection method  an algorithm that selects a subset of the original attribute set  in
conjunction with the coecient learning algorithm  breiman et al         brodley   utgoff 
      
currently  oc  does not have a built in mechanism to select relevant attributes  however  it is easy to include any of several standard methods  e g   stepwise forward selection
or stepwise backward selection  or even an ad hoc method to select features before running
the tree building process  for example  in separate experiments on data from the hubble
space telescope  salzberg  chandar  ford  murthy    white         we used feature selection methods as a preprocessing step to oc   and reduced the number of attributes from   
to    the resulting decision trees were both simpler and more accurate  work is currently
underway to incorporate an ecient feature selection technique into the oc  system 
  

fimurthy  kasif   salzberg

regarding missing values  if an example is missing a value for any attribute  oc  uses
the mean value for that attribute  one can of course use other techniques for handling
missing values  but those were not considered in this study 

   experiments

in this section  we present two sets of experiments to support the following two claims 
   oc  compares favorably over a variety of real world domains with several existing
axis parallel and oblique decision tree induction methods 
   randomization  both in the form of multiple local searches and random jumps  improves the quality of decision trees produced by oc  
the experimental method used for all the experiments is described in section      sections     and     describe experiments corresponding to the above two claims  each experimental section begins with a description of the data sets  and then presents the experimental
results and discussion 

    experimental method

we used five fold cross validation  cv  in all our experiments to estimate classification
accuracy  a k fold cv experiment consists of the following steps 
   randomly divide the data into k equal sized disjoint partitions 
   for each partition  build a decision tree using all data outside the partition  and test
the tree on the data in the partition 
   sum the number of correct classifications of the k trees and divide by the total number
of instances to compute the classification accuracy  report this accuracy and the
average size of the k trees 
each entry in tables   and   is a result of ten   fold cv experiments  i e   the result of tests
that used    decision trees  each of the ten   fold cross validations used a different random
partitioning of the data  each entry in the tables reports the mean and standard deviation
of the classification accuracy  followed by the mean and standard deviation of the decision
tree size  measured as the number of leaf nodes   good results should have high values for
accuracy  low values for tree size  and small standard deviations 
in addition to oc   we also included in the experiments an axis parallel version of oc  
which only considers axis parallel hyperplanes  we call this version  described in section     
oc  ap  in all our experiments  both oc  and oc  ap used the twoing rule  section
       to measure impurity  other parameters to oc  took their default values unless stated
otherwise   defaults include the following  number of restarts at each node      number
of random jumps attempted at each local minimum     order of coecient perturbation 
sequential  pruning method  cost complexity with the   se rule  using     of the training
set exclusively for pruning  
in our comparison  we used the oblique version of the cart algorithm  cart lc 
we implemented our own version of cart lc  following the description in breiman et
al         chapter     however  there may be differences between our version and other
  

fiinduction of oblique decision trees

versions of this system  note that cart lc is not freely available   our implementation
of cart lc measured impurity with the twoing rule and used   se cost complexity
pruning with a separate test set  just as oc  does  we did not include any feature selection
methods in cart lc or in oc   and we did not implement normalization  because the
cart coecient perturbation algorithm may alternate indefinitely between two locations
of a hyperplane  see section     we imposed an arbitrary limit of     such perturbations
before forcing the perturbation algorithm to halt 
we also included axis parallel cart and c    in our comparisons  we used the implementations of these algorithms from the ind     package  buntine         the default
cart  and c     styles  defined in the package were used  without altering any parameter
settings  the cart  style uses the twoing rule and   se cost complexity pruning with
   fold cross validation  the pruning method  impurity measure and other defaults of the
c    style are the same as those described in quinlan      a  

    oc  vs  other decision tree induction methods

table   compares the performance of oc  to three well known decision tree induction
methods plus oc  ap on six different real world data sets  in the next section we will
consider artificial data  for which the concept definition can be precisely characterized 
      description of data sets

star galaxy discrimination  two of our data sets came from a large set of astronom 

ical images collected by odewahn et al   odewahn  stockwell  pennington  humphreys   
zumach         in their study  they used these images to train artificial neural networks
running the perceptron and back propagation algorithms  the goal was to classify each example as either  star  or  galaxy   each image is characterized by    real valued attributes 
where the attributes were measurements defined by astronomers as likely to be relevant for
this task  the objects in the image were divided by odewahn et al  into  bright  and  dim 
data sets based on the image intensity values  where the dim images are inherently more
dicult to classify   note that the  bright  objects are only bright in relation to others
in this data set  in actuality they are extremely faint  visible only to the most powerful
telescopes   the bright set contains      objects and the dim set contains      objects 
in addition to the results reported in table    the following results have appeared on
the star galaxy data  odewahn et al         reported accuracy of       accuracy on the
bright objects  and       on the dim ones  although it should be noted that this study
used a single training and test set partition  heath        reported       accuracy on the
bright objects using sadt  with an average tree size of      leaves  this study also used
a single training and test set  salzberg        reported accuracies of       on the bright
objects  and       on the dim objects  using   nearest neighbor    nn  coupled with a
feature selection method that reduces the number of features 
breast cancer diagnosis  mangasarian and bennett have compiled data on the problem of diagnosing breast cancer to test several new classification methods  mangasarian
et al         bennett   mangasarian            a   this data represents a set of patients
with breast cancer  where each patient was characterized by nine numeric attributes plus
the diagnosis of the tumor as benign or malignant  the data set currently has     entries
  

fimurthy  kasif   salzberg

bright s g
       
      
cart lc
       
      
oc  ap
       
      
cart ap
       
       
c   
       
       
algorithm
oc 

dim s g
       
       
       
       
       
       
       
      
       
       

cancer
       
      
       
      
       
      
       
       
       
      

iris
       
      
       
      
       
      
       
      
       
      

housing
       
      
       
      
       
      
       
      
       
       

diabetes
       
      
       
      
       
       
       
       
       
       

table    comparison of oc  and other decision tree induction methods on six different
data sets  the first line for each method gives accuracies  and the second line gives
average tree sizes  the highest accuracy for each domain appears in boldface 
and is available from the uc irvine machine learning repository  murphy   aha        
heath et al       b  reported       accuracy on a subset of this data set  it then had
only     instances   with an average decision tree size of     nodes  using sadt  salzberg
       reported       accuracy using   nn on the same  smaller  data set  herman and
yeung        reported       accuracy using piece wise linear classification  again using a
somewhat smaller data set 

classifying irises  this is fisher s famous iris data  which has been extensively studied

in the statistics and machine learning literature  the data consists of     examples  where
each example is described by four numeric attributes  there are    examples of each of
three different types of iris ower  weiss and kapouleas        obtained accuracies of      
and       on this data with back propagation and   nn  respectively 

housing costs in boston  this data set  also available as a part of the uci ml repos 

itory  describes housing values in the suburbs of boston as a function of    continuous
attributes and   binary attribute  harrison   rubinfeld         the category variable  median value of owner occupied homes  is actually continuous  but we discretized it so that
category     if value           and   otherwise  for other uses of this data  see  belsley 
      quinlan      b  

diabetes diagnosis  this data catalogs the presence or absence of diabetes among pima

indian females     years or older  as a function of eight numeric valued attributes  the
original source of the data is the national institute of diabetes and digestive and kidney
diseases  and it is now available in the uci repository  smith et al         reported    
accuracy on this data using their adap learning algorithm  using a different experimental
method from that used here 
  

fiinduction of oblique decision trees

      discussion

the table shows that  for the six data sets considered here  oc  consistently finds better
trees than the original oblique cart method  its accuracy was greater in all six domains 
although the difference was significant  more than   standard deviations  only for the dim
star galaxy problem  the average tree sizes were roughly equal for five of the six domains 
and for the dim stars and galaxies  oc  found considerably smaller trees  these differences
will be analyzed and quantified further by using artificial data  in the following section 
out of the five decision tree induction methods  oc  has the highest accuracy on four
of the six domains  bright stars  dim stars  cancer diagnosis  and diabetes diagnosis  on the
remaining two domains  oc  has the second highest accuracy in each case  not surprisingly 
the oblique methods  oc  and cart lc  generally find much smaller trees than the axisparallel methods  this difference can be quite striking for some domains note  for example 
that oc  produced a tree with just    nodes on average for the dim star galaxy problem 
while c    produced a tree with    nodes    times larger  of course  in domains for which
an axis parallel tree is the appropriate representation  axis parallel methods should compare
well with oblique methods in terms of tree size  in fact  for the iris data  all the methods
found similar sized trees 

    randomization helps oc 

in our second set of experiments  we examine more closely the effect of introducing randomized steps into the algorithm for finding oblique splits  our experiments demonstrate that
oc  s ability to produce an accurate tree from a set of training data is clearly enhanced
by the two kinds of randomization it uses  more precisely  we use three artificial data sets
 for which the underlying concept is known to the experimenters  to show that oc  s performance improves substantially when the deterministic hill climbing is augmented in any
of three ways 

 with multiple restarts from random initial locations 
 with perturbations in random directions at local minima  or
 with both of the above randomization steps 
in order to find clear differences between algorithms  one needs to know that the concept
underlying the data is indeed dicult to learn  for simple concepts  say  two linearly
separable classes in   d   many different learning algorithms will produce very accurate
classifiers  and therefore the advantages of randomization may not be detectable  it is
known that many of the commonly used data sets from the uci repository are easy to
learn with very simple representations  holte         therefore those data sets may not be
ideal for our purposes  thus we created a number of artificial data sets that present different
problems for learning  and for which we know the  correct  concept definition  this allows
us to quantify more precisely how the parameters of our algorithm affect its performance 
a second purpose of this experiment is to compare oc  s search strategy with that
of two existing oblique decision tree induction systems   lmdt  brodley   utgoff       
and sadt  heath et al       b   we show that the quality of trees induced by oc  is as
good as  if not better than  that of the trees induced by these existing systems on three
  

fimurthy  kasif   salzberg

artificial domains  we also show that oc  achieves a good balance between amount of
effort expended in search and the quality of the tree induced 
both lmdt and sadt used information gain for this experiment  however  we did
not change oc  s default measure  the twoing rule  because we observed  in experiments
not reported here  that oc  with information gain does not produce significantly different
results  the maximum number of successive  unproductive perturbations allowed at any
node was set at       for sadt  for all other parameters  we used default settings provided
with the systems 
      description of artificial data

ls   the ls   data set has      instances divided into two categories  each instance is

described by ten attributes x          x    whose values are uniformly distributed in the range
       the data is linearly separable with a    d hyperplane  thus the name ls    defined
by the equation x    x    x    x    x    x    x    x    x    x    the instances were all
generated randomly and labelled according to which side of this hyperplane they fell on 
because oblique dt induction methods intuitively should prefer a linear separator if one
exists  it is interesting to compare the various search techniques on this data set where we
know a separator exists  the task is relatively simple for lower dimensions  so we chose
   dimensional data to make it more dicult 

pol this data set is shown in figure    it has      instances in two dimensions  again

divided into two categories  the underlying concept is a set of four parallel oblique lines
 thus the name pol   dividing the instances into five homogeneous regions  this concept is
more dicult to learn than a single linear separator  but the minimal size tree is still quite
small 

rcb rcb stands for  rotated checker board   this data set has been the subject of

other experiments on hard classification problems for decision trees  murthy   salzberg 
       the data set  shown in figure    has      instances in   d  each belonging to one of
eight categories  this concept is dicult to learn for any axis parallel method  for obvious
reasons  it is also quite dicult for oblique methods  for several reasons  the biggest
problem is that the  correct  root node  as shown in the figure  does not separate out any
class by itself  some impurity measures  such as sum minority  will fail miserably on this
problem  although others  e g   the twoing rule  work much better  another problem is
that a deterministic coecient perturbation algorithm can get stuck in local minima in
many places on this data set 
table   summarizes the results of this experiment in three smaller tables  one for each
data set  in each smaller table  we compare four variants of oc  with lmdt and sadt 
the different results for oc  were obtained by varying both the number of restarts and the
number of random jumps  when random jumps were used  up to twenty random jumps
were tried at each local minimum  as soon as one was found that improved the impurity
of the current hyperplane  the algorithm moved the hyperplane and started running the
deterministic perturbation procedure again  if none of the    random jumps improved the
impurity  the search halted and further restarts  if any  were tried  the same training and
test partitions were used for all methods for each cross validation run  recall that the results
  

fiinduction of oblique decision trees

lr 

ot

rr 

  

l  

 

r  

rl 

 
rr 

r  

t  
oo
r

 
 
 
 
  
 
  
   
  
 
  
  
 
     
         
    
 
     
     
    
 
 
 
  
      
    
 
 
 
 
      
    
            
        
 
 
            
 
 
       
 
  
 
   
 
 
 
   
       
       
         
  
   
   
 
        
 
 
 
   
     
 
      
 
 
       
  
  
  
 
         
 
   
     
   
       
  
   
         
 
 
 
 
 
 
 
 
                 
 
 
 
 
 
     
 
 
 
 
           
  
        
      
 
  
 
                    
        
    
 
 
   
      
 
   
   
      
       
 
 
 
 
    
  
   
    
 
 
 
 
 
 
    
         
       
            
 
    
 
 
  
   
            
  
 
 
 
  
 
     
 
 
 
     
 
     
 
        
 
    
 
 
    
 
 
 
 
 
 
 
 
 
 
  
  
           
                          
 
          
 
 
    
ll  
 
 
    
 
      
 
   
                 
 
         
  
 
     
 
   
       
 
 
 
 
      
 
   
  
      
 
 
 
      
 
  
 
 
    
 
 
        
 
   
   
 
        
 
 
  
    
        
  
          
 
  
 
      
 
   
    
   
     
 
      
     
   
 
 
 
 
 
 
 
 
 
 
     
 
    
        
 
    
 
 
  
      
 
 
  
  
  
 
            
   
                    
 
      
 
 
   
  
  
        
 
            
 
   
    
 
   
 
 
 
 
 
  
 
            
         
  
 
 
 
 
 
    
        
                          
 
 
 
 
 
       
 
  
 
 
 
 
   
 
 
  
          
 
 
 
 
 
 
 
 
     
    
 
      
           
   
 
 
      
 
 
 
  
 
 
  
 
  
      
   
     
    
  
 
 
   
   
 
          
 
        
 
 
 
    
 
    
    
 
   
 
    
      
  
 
    
    
   
      
 
  
    
 
   
 
 
 
 
 
  
  
      
  
  
   
      
      
      
 
 
  
 
      
 
  
 
 
 
 
      
              
    
    
    
 
   
 
   

ro

  
rrr

 
 
 
 
  
 
  
   
  
 
  
  
 
     
         
    
 
     
     
    
 
 
 
  
      
    
 
 
 
 
      
    
            
        
 
 
            
 
 
       
 
  
 
   
 
 
 
   
       
       
         
  
   
   
 
        
 
 
 
   
     
 
      
 
 
       
  
  
  
 
         
 
   
     
   
       
  
   
         
 
 
 
 
 
 
 
 
                 
 
 
 
 
 
     
 
 
 
 
           
  
        
      
 
  
 
                    
        
    
 
 
   
      
 
   
   
      
       
 
 
 
 
    
  
   
    
 
 
 
 
 
 
    
         
       
            
 
    
 
 
  
   
            
  
 
 
 
  
 
     
 
 
 
     
 
     
 
        
 
    
 
 
    
 
 
 
 
 
 
 
 
 
 
  
   
           
                          
 
    
 
 
    
   
 
 
    
 
      
 
   
                 
 
         
  
 
     
 
   
       
 
 
 
 
      
 
   
  
      
 
 
 
      
 
  
 
 
    
 
 
        
 
   
   
 
        
 
 
  
    
        
  
          
 
  
 
      
 
   
    
   
     
 
      
     
   
 
 
 
 
 
 
 
 
 
 
     
 
    
        
 
    
 
 
  
      
 
 
  
  
  
 
            
   
                    
 
      
 
 
   
  
  
        
 
            
 
   
    
 
   
 
 
 
 
 
  
 
            
         
  
 
 
 
 
 
    
        
                          
 
 
 
 
 
       
 
  
 
 
 
 
   
 
 
  
          
 
 
 
 
 
 
 
 
     
    
 
      
           
   
 
 
      
 
 
 
  
 
 
  
 
  
      
   
     
 
    
  
 
 
   
   
 
          
 
        
 
 
 
    
 
    
    
 
   
 
    
      
  
 
    
    
   
      
 
  
    
 
   
 
 
 
 
 
  
  
      
  
  
   
      
      
      
 
 
  
 
      
 
  
 
 
 
 
      
              
    
    
    
 
   
 
   

figure    the pol and rcb data sets
linearly separable    d  ls    data
r j accuracy
size
hyperplanes
                   
    
                    
    
                    
     
                     
     
lmdt               
    
sadt                
      
parallel oblique lines  pol  data
r j accuracy
size
hyperplanes
                   
   
                   
   
                    
    
                    
    
lmdt                  
    
sadt               
     
rotated checker board  rcb  data
r j accuracy
size
hyperplanes
                   
   
                    
    
                    
    
                    
     
lmdt                
    
sadt                
      
table    the effect of randomization in oc   the first column  labelled r j  shows the
number of restarts  r  followed by the maximum number of random jumps  j 
attempted by oc  at each local minimum  results with lmdt and sadt are
included for comparison after the four variants of oc   size is average tree size
measured by the number of leaf nodes  the third column shows the average
number of hyperplanes each algorithm considered while building one tree 
  

fimurthy  kasif   salzberg

are an average of ten   fold cvs   the trees were not pruned for any of the algorithms 
because the data were noise free and furthermore the emphasis was on search 
table   also includes the number of hyperplanes considered by each algorithm while
building a complete tree  note that for oc  and sadt  the number of hyperplanes considered is generally much larger than the number of perturbations actually made  because
both these algorithms compare newly generated hyperplanes to existing hyperplanes before
adjusting an existing one  nevertheless  this number is a good estimate of much effort
each algorithm expends  because every new hyperplane must be evaluated according to the
impurity measure  for lmdt  the number of hyperplanes considered is identical to the
actual number of perturbations 
      discussion

the oc  results here are quite clear  the first line of each table  labelled      gives the
accuracies and tree sizes when no randomization is used   this variant is very similar
to the cart lc algorithm  as we increase the use of randomization  accuracy increases
while tree size decreases  which is exactly the result we had hoped for when we decided to
introduce randomization into the method 
looking more closely at the tables  we can ask about the effect of random jumps alone 
this is illustrated in the second line        of each table  which attempted up to    random
jumps at each local minimum and no restarts  accuracy increased by      on each domain 
and tree size decreased dramatically  roughly by a factor of two  in the pol and rcb
domains  note that because there is no noise in these domains  very high accuracies should
be expected  thus increases of more than a few percent in accuracy are not possible 
looking at the third line of each sub table in table    we see the effect of multiple restarts
on oc   with    restarts but no random jumps to escape local minima  the improvement
is even more noticeable for the ls   data than when random jumps alone were used  for
this data set  accuracy jumped significantly  from      to        while tree size dropped
from    to    nodes  for the pol and rcb data  the improvements were comparable to
those obtained with random jumps  for the rcb data  tree size dropped by a factor of  
 from    leaf nodes to    leaf nodes  while accuracy increased from      to       
the fourth line of each table shows the effect of both the randomized steps  among the
oc  entries  this line has both the highest accuracies and the smallest trees for all three
data sets  so it is clear that randomization is a big win for these kinds of problems  in
addition  note that the smallest tree for the rcb data should have eight leaf nodes  and
oc  s average trees  without pruning  had just     leaf nodes  it is clear that for this data
set  which we thought was the most dicult one  oc  came very close to finding the optimal
tree on nearly every run   recall that numbers in the table are the average of      fold
cv experiments  i e   an average of    decision trees   the ls   data show how dicult it
can be to find a very simple concept in higher dimensions the optimal tree there is just a
single hyperplane  two nodes   but oc  was unable to find it with the current parameter
settings   the pol data required a minimum of   leaf nodes  and oc  found this minimalsize tree most of the time  as can be seen from the table  although not shown in the table 
   in a separate experiment  we found that oc  consistently finds the linear separator for the ls   data
when    restarts and     random jumps are used 

  

fiinduction of oblique decision trees

oc  using sum minority performed better for the pol data than the twoing rule or any
other impurity measure  i e   it found the correct tree using less time 
the results of lmdt and sadt on this data lead to some interesting insights  not
surprisingly  lmdt does very well on the linearly separable  ls    data  and does not
require an inordinate amount of search  clearly  if the data is linearly separable  one should
use a method such as lmdt or linear programming  oc  and sadt have diculty finding
the linear separator  although in our experiments oc  did eventually find it  given sucient
time 
on the other hand  for both of the non linearly separable data sets  lmdt produces
much larger trees that are significantly less accurate than those produced by oc  and
sadt  even the deterministic variant of oc   using zero restarts and zero random jumps 
outperforms lmdt on these problems  with much less search 
although sadt sometimes produces very accurate trees  its main weakness was the
enormous amount of search time it required  roughly       times greater than oc  even
using the       setting  one explanation of oc  s advantage is its use of directed search  as
opposed to the strictly random search used by simulated annealing  overall  table   shows
that oc  s use of randomization was quite effective for the non linearly separable data 
it is natural to ask why randomization helps oc  in the task of inducing decision trees 
researchers in combinatorial optimization have observed that randomized search usually
succeeds when the search space holds an abundance of good solutions  gupta  smolka 
  bhaskar         furthermore  randomization can improve upon deterministic search
when many of the local maxima in a search space lead to poor solutions  in oc  s search
space  a local maximum is a hyperplane that cannot be improved by the deterministic
search procedure  and a  solution  is a complete decision tree  if a significant fraction
of local maxima lead to bad trees  then algorithms that stop at the first local maximum
they encounter will perform poorly  because randomization allows oc  to consider many
different local maxima  if a modest percentage of these maxima lead to good trees  then it
has a good chance of finding one of those trees  our experiments with oc  thus far indicate
that the space of oblique hyperplanes usually contains numerous local maxima  and that a
substantial percentage of these locally good hyperplanes lead to good decision trees 

   conclusions and future work
this paper has described oc   a new system for constructing oblique decision trees  we
have shown experimentally that oc  can produce good classifiers for a range of real world
and artificial domains  we have also shown how the use of randomization improves upon
the original algorithm proposed by breiman et al          without significantly increasing
the computational cost of the algorithm 
the use of randomization might also be beneficial for axis parallel tree methods  note
that although they do find the optimal test  with respect to an impurity measure  for each
node of a tree  the complete tree may not be optimal  as is well known  the problem of
finding the smallest tree is np complete  hyafil   rivest         thus even axis parallel
decision tree methods do not produce  ideal  decision trees  quinlan has suggested that his
windowing algorithm might be used as a way of introducing randomization into c     even
though the algorithm was designed for another purpose  quinlan      a    the windowing
  

fimurthy  kasif   salzberg

algorithm selects a random subset of the training data and builds a tree using that   we
believe that randomization is a powerful tool in the context of decision trees  and our
experiments are just one example of how it might be exploited  we are in the process of
conducting further experiments to quantify more accurately the effects of different forms of
randomization 
it should be clear that the ability to produce oblique splits at a node broadens the capabilities of decision tree algorithms  especially as regards domains with numeric attributes 
of course  axis parallel splits are simpler  in the sense that the description of the split only
uses one attribute at each node  oc  uses oblique splits only when their impurity is less
than the impurity of the best axis parallel split  however  one could easily penalize the
additional complexity of an oblique split further  this remains an open area for further
research  a more general point is that if the domain is best captured by a tree that uses
oblique hyperplanes  it is desirable to have a system that can generate that tree  we have
shown that for some problems  including those used in our experiments  oc  builds small
decision trees that capture the domain well 

appendix a  complexity analysis of oc 

in the following  we show that oc  runs eciently even in the worst case  for a data
set with n examples  points  and d attributes per example  oc  uses at most o dn  log n 
time  we assume n   d for our analysis 
for the analysis here  we assume the coecients of a hyperplane are adjusted in sequential order  the seq method described in the paper   the number of restarts at a node will
be r  and the number of random jumps tried will be j   both r and j are constants  fixed in
advance of running the algorithm 
initializing the hyperplane to a random position takes just o d  time  we need to
consider first the maximum amount of work oc  can do before it finds a new location for
the hyperplane  then we need to consider how many times it can move the hyperplane 
   attempting to perturb the first coecient  a    takes o dn   n log n  time  computing
ui  s for all the points  equation    requires o dn  time  and sorting the ui  s takes
o n log n   this gives us o dn   n log n  work 
   if perturbing a  does not improve things  we try to perturb a    computing all the new
ui  s will take just o n  time because only one term is different for each ui   re sorting
will take o n log n   so this step takes o n    o n log n    o n log n  time 
   likewise a          ad will each take o n log n  additional time  assuming we still have not
found a better hyperplane after checking each coecient  thus the total time to cycle
through and attempt to perturb all these additional coecients is  d       o n log n   
o dn log n  
   summing up  the time to cycle through all coecients is o dn log n  o dn n log n   
o dn log n  
   if none of the coecients improved the split  then we attempt to make up to j random
jumps  since j is a constant  we will just consider j     for our analysis  this step
  

fiinduction of oblique decision trees

involves choosing a random vector and running the perturbation algorithm to solve
for ff  as explained in section      as before  we need to compute a set of ui  s and sort
them  which takes o dn   n log n  time  because this amount of time is dominated by
the time to adjust all the coecients  the total time so far is still o dn log n   this is
the most time oc  can spend at a node before either halting or finding an improved
hyperplane 
   assuming oc  is using the sum minority or max minority error measure  it can only
reduce the impurity of the hyperplane n times  this is clear because each improvement
means one more example will be correctly classified by the new hyperplane  thus the
total amount of work at a node is limited to n  o dn log n    o dn  log n    this
analysis extends  with at most linear cost factors  to information gain  gini index and
twoing rule when there are two categories  it will not apply to a measure that  for
example  uses the distances of mis classified objects to the hyperplane   in practice 
we have found that the number of improvements per node is much smaller than n 
assuming that oc  only adjusts a hyperplane when it improves the impurity measure 
it will do o dn  log n  work in the worst case 
however  oc  allows a certain number of adjustments to the hyperplane that do not
improve the impurity  although it will never accept a change that worsens the impurity 
the number allowed is determined by a constant known as  stagnant perturbations   let
this value be s  this works as follows 
each time oc  finds a new hyperplane that improves on the old one  it resets a counter
to zero  it will move the new hyperplane to a different location that has equal impurity at
most s times  after each of these moves it repeats the perturbation algorithm  whenever
impurity is reduced  it re starts the counter and again allows s moves to equally good
locations  thus it is clear that this feature just increases the worst case complexity of oc 
by a constant factor  s 
finally  note that the overall cost of oc  is also o dn  log n   i e   this is an upper
bound on the total running time of oc  independent of the size of the tree it ends up
creating   this upper bound applies to sum minority and max minority  an open question
is whether a similar upper bound can be proven for information gain or the gini index  
thus the worst case asymptotic complexity of our system is comparable to that of systems
that construct axis parallel decision trees  which have o dn    worst case complexity  to
sketch the intuition that leads to this bound  let g be the total impurity summed over all
leaves in a partially constructed tree  i e   the sum of currently misclassified points in the
tree   now observe that each time we run the perturbation algorithm on any node in the
tree  we either halt or improve g by at least one unit  the worst case analysis for one node
is realized when the perturbation algorithm is run once for every one of the n examples 
but when this happens  there would no longer be any mis classified examples and the tree
would be complete 

appendix b  definitions of impurity measures available in oc 

in addition to the twoing rule defined in the text  oc  contains built in definitions of five
additional impurity measures  defined as follows  in each of the following definitions  the
  

fimurthy  kasif   salzberg

set of examples t at the node about to be split contains n       instances that belong to
one of k categories   initially this set is the entire training set   a hyperplane h divides t
into two non overlapping subsets tl and tr  i e   left and right   lj and rj are the number
of instances of category j in tl and tr respectively  all the impurity measures initially
check to see if tl and tr are homogeneous  i e   all examples belong to the same category  
and if so return minimum  zero  impurity 

information gain  this measure of information gained from a particular split was pop 

ularized in the context of decision trees by quinlan         quinlan s definition makes
information gain a goodness measure  i e   something to maximize  because oc  attempts
to minimize whatever impurity measure it uses  we use the reciprocal of the standard value
of information gain in the oc  implementation 

gini index  the gini criterion  or index  was proposed for decision trees by breiman et
al          the gini index as originally defined measures the probability of misclassification
of a set of instances  rather than the impurity of a split  we implement the following
variation 
ginil        
ginir        

k
x
i  
k
x
i  

 li  jtlj  
 ri jtrj  

impurity    jtlj  ginil   jtrj  ginir  n
where ginil is the gini index on the  left  side of the hyperplane and ginir is that on the
right 

max minority  the measures max minority  sum minority and sum of variances were

defined in the context of decision trees by heath  kasif  and salzberg      b    max
minority has the theoretical advantage that a tree built minimizing this measure will have
depth at most log n  our experiments indicated that this is not a great advantage in
practice  seldom do other impurity measures produce trees substantially deeper than those
produced with max minority  the definition is 
minorityl  
minorityr  

k
x
i   i  max li
k
x
i   i  max ri

li
ri

max minority   max minorityl  minorityr 
   sum of variances was called sum of impurities by heath et al 

  

fiinduction of oblique decision trees

sum minority  this measure is very similar to max minority  if minorityl and minorityr are defined as for the max minority measure  then sum minority is just the sum of
these two values  this measure is the simplest way of quantifying impurity  as it simply
counts the number of misclassified instances 
though sum minority performs well on some domains  it has some obvious aws  as
one example  consider a domain in which n        d      and k      i e       examples   
numeric attribute    classes   suppose that when the examples are sorted according to the
single attribute  the first    instances belong to category    followed by    instances of category    followed by    instances of category    then all possible splits for this distribution
have a sum minority of     therefore it is impossible when using sum minority to distinguish which split is preferable  although splitting at the alternations between categories is
clearly better 
sum of variances  the definition of this measure is 
jx
tl j
jx
tl j
variancel    cat tli     cat tlj   jtlj  
variancer  

i  

j   

jx
trj

jx
trj

i  

 cat tri    

j   

cat trj   jtrj  

sum of variances   variancel   variancer
where cat ti  is the category of instance ti   as this measure is computed using the actual
class labels  it is easy to see that the impurity computed varies depending on how numbers
are assigned to the classes  for instance  if t  consists of    points of category   and  
points of category    and if t  consists of    points of category   and   points of category
   then the sum of variances values are different for t  and t   to avoid this problem 
oc  uniformly reassigns category numbers according to the frequency of occurrence of each
category at a node before computing the sum of variances 

acknowledgements
the authors thank richard beigel of yale university for suggesting the idea of jumping in a
random direction  thanks to wray buntine of nasa ames research center for providing the
ind     package  to carla brodley for providing the lmdt code  and to david heath for
providing the sadt code and for assisting us in using it  thanks also to three anonymous
reviewers for many helpful suggestions  this material is based upon work supported by the
national science foundation under grant nos  iri          iri          and iri         

references
aha  d          a study of instance based algorithms for supervised learning  mathematical  empirical and psychological evaluations  ph d  thesis  department of information
and computer science  university of california  irvine 
  

fimurthy  kasif   salzberg

almuallin  h     dietterich  t          learning with many irrelevant features  in proceedings of the ninth national conference on artificial intelligence  pp           san
jose  ca 
belsley  d          regression diagnostics  identifying inuential data and sources of
collinearity  wiley   sons  new york 
bennett  k     mangasarian  o          robust linear programming discrimination of two
linearly inseparable sets  optimization methods and software           
bennett  k     mangasarian  o       a   multicategory discrimination via linear programming  optimization methods and software           
bennett  k     mangasarian  o       b   serial and parallel multicategory discrimination 
siam journal on optimization        
blum  a     rivest  r          training a   node neural network is np complete  in proceedings of the      workshop on computational learning theory  pp        boston 
ma  morgan kaufmann 
breiman  l   friedman  j   olshen  r     stone  c          classification and regression
trees  wadsworth international group 
brent  r  p          fast training algorithms for multilayer neural nets  ieee transactions
on neural networks                 
brodley  c  e     utgoff  p  e          multivariate versus univariate decision trees  tech 
rep  coins cr       dept  of computer science  university of massachusetts at
amherst 
brodley  c  e     utgoff  p  e          multivariate decision trees  machine learning  to
appear 
buntine  w          tree classification software  technology       the third national
technology transfer conference and exposition 
buntine  w     niblett  t          a further comparison of splitting rules for decision tree
induction  machine learning           
cardie  c          using decision trees to improve case based learning  in proceedings of
the tenth international conference on machine learning  pp         university of
massachusetts  amherst 
cestnik  g   kononenko  i     bratko  i          assistant     a knowledge acquisition
tool for sophisticated users  in bratko  i     lavrac  n   eds    progress in machine
learning  sigma press 
cios  k  j     liu  n          a machine learning method for generation of a neural network
architecture  a continuous id  algorithm  ieee transactions on neural networks 
               
  

fiinduction of oblique decision trees

cohen  w          ecient pruning methods for separate and conquer rule learning systems  in proceedings of the   th international joint conference on artificial intelligence  pp           morgan kaufmann 
fayyad  u  m     irani  k  b          the attribute specification problem in decision tree
generation  in proceedings of the tenth national conference on artificial intelligence 
pp           san jose ca  aaai press 
frean  m          small nets and short paths  optimising neural computation  ph d 
thesis  centre for cognitive science  university of edinburgh 
gupta  r   smolka  s     bhaskar  s          on randomization in sequential and distributed
algorithms  acm computing surveys               
hampson  s     volper  d          linear function neurons  structure and training  biological cybernetics              
harrison  d     rubinfeld  d          hedonic prices and the demand for clean air  journal
of environmental economics and management            
hassibi  b     stork  d          second order derivatives for network pruning  optimal
brain surgeon  in advances in neural information processing systems    pp          
morgan kaufmann  san mateo  ca 
heath  d          a geometric framework for machine learning  ph d  thesis  johns
hopkins university  baltimore  maryland 
heath  d   kasif  s     salzberg  s       a   k dt  a multi tree learning method  in
proceedings of the second international workshop on multistrategy learning  pp      
     harpers ferry  wv  george mason university 
heath  d   kasif  s     salzberg  s       b   learning oblique decision trees  in proceedings
of the   th international joint conference on artificial intelligence  pp            
chambery  france  morgan kaufmann 
herman  g  t     yeung  k  d          on piecewise linear classification  ieee transactions on pattern analysis and machine intelligence                  
holte  r          very simple classification rules perform well on most commonly used
datasets  machine learning                
hyafil  l     rivest  r  l          constructing optimal binary decision trees is npcomplete  information processing letters               
kira  k     rendell  l          a practical approach to feature selection  in proceedings
of the ninth international conference on machine learning  pp           aberdeen 
scotland  morgan kaufmann 
kirkpatrick  s   gelatt  c     vecci  m          optimization by simulated annealing 
science                      
  

fimurthy  kasif   salzberg

kodratoff  y     manago  m          generalization and noise  international journal of
man machine studies              
langley  p     sage  s          scaling to domains with many irrelevant features  learning
systems department  siemens corporate research  princeton  nj 
mangasarian  o   setiono  r     wolberg  w          pattern recognition via linear programming  theory and application to medical diagnosis  in siam workshop on
optimization 
mingers  j       a   an empirical comparison of pruning methods for decision tree induction  machine learning                 
mingers  j       b   an empirical comparison of selection measures for decision tree induction  machine learning             
moret  b  m          decision trees and diagrams  computing surveys                  
murphy  p     aha  d          uci repository of machine learning databases   a machinereadable data repository  maintained at the department of information and computer
science  university of california  irvine  anonymous ftp from ics uci edu in the
directory pub machine learning databases 
murthy  s  k   kasif  s   salzberg  s     beigel  r          oc   randomized induction of
oblique decision trees  in proceedings of the eleventh national conference on artificial
intelligence  pp           washington  d c  mit press 
murthy  s  k     salzberg  s          using structure to improve decision trees  tech  rep 
jhu        department of computer science  johns hopkins university 
niblett  t          constructing decision trees in noisy domains  in bratko  i     lavrac 
n   eds    progress in machine learning  sigma press  england 
nilsson  n          learning machines  morgan kaufmann  san mateo  ca 
odewahn  s   stockwell  e   pennington  r   humphreys  r     zumach  w          automated star galaxy descrimination with neural networks  astronomical journal 
                 
pagallo  g          adaptive decision tree algorithms for learning from examples  ph d 
thesis  university of california at santa cruz 
pagallo  g     haussler  d          boolean feature discovery in empirical learning  machine
learning               
quinlan  j  r          learning ecient classification procedures and their application to
chess end games  in michalski  r   carbonell  j     mitchell  t   eds    machine
learning  an artificial intelligence approach  morgan kaufmann  san mateo  ca 
quinlan  j  r          induction of decision trees  machine learning            
  

fiinduction of oblique decision trees

quinlan  j  r          simplifying decision trees  international journal of man machine
studies              
quinlan  j  r       a   c     programs for machine learning  morgan kaufmann publishers  san mateo  ca 
quinlan  j  r       b   combining instance based and model based learning  in proceedings
of the tenth international conference on machine learning  pp          university
of massachusetts  amherst  morgan kaufmann 
roth  r  h          an approach to solving linear discrete optimization problems  journal
of the acm                  
safavin  s  r     landgrebe  d          a survey of decision tree classifier methodology 
ieee transactions on systems  man and cybernetics                  
sahami  m          learning non linearly separable boolean functions with linear threshold unit trees and madaline style networks  in proceedings of the eleventh national
conference on artificial intelligence  pp           aaai press 
salzberg  s          a nearest hyperrectangle learning method  machine learning    
        
salzberg  s          combining learning and search to create good classifiers  tech  rep 
jhu        johns hopkins university  baltimore md 
salzberg  s   chandar  r   ford  h   murthy  s  k     white  r          decision trees for
automated identification of cosmic rays in hubble space telescope images  publications of the astronomical society of the pacific  to appear 
schaffer  c          overfitting avoidance as bias  machine learning              
schlimmer  j          eciently inducing determinations  a complete and systematic
search algorithm that uses optimal pruning  in proceedings of the tenth international
conference on machine learning  pp           morgan kaufmann 
smith  j   everhart  j   dickson  w   knowler  w     johannes  r          using the
adap learning algorithm to forecast the onset of diabetes mellitus  in proceedings
of the symposium on computer applications and medical care  pp           ieee
computer society press 
utgoff  p  e          perceptron trees  a case study in hybrid concept representations 
connection science                 
utgoff  p  e     brodley  c  e          an incremental method for finding multivariate
splits for decision trees  in proceedings of the seventh international conference on
machine learning  pp         los altos  ca  morgan kaufmann 
utgoff  p  e     brodley  c  e          linear machine decision trees  tech  rep     
university of massachusetts at amherst 
  

fimurthy  kasif   salzberg

van de merckt  t          nfdt  a system that learns exible concepts based on decision
trees for numerical attributes  in proceedings of the ninth international workshop on
machine learning  pp          
van de merckt  t          decision trees in numerical attribute spaces  in proceedings of
the   th international joint conference on artificial intelligence  pp            
weiss  s     kapouleas  i          an empirical comparison of pattern recognition  neural
nets  and machine learning classification methods  in proceedings of the   th international joint conference of artificial intelligence  pp           detroit  mi  morgan
kaufmann 
wolpert  d          on overfitting avoidance as bias  tech  rep  sfi tr             the
santa fe institute  santa fe  new mexico 

  

fi