journal of artificial intelligence research                 

submitted        published     

rerepresenting and restructuring domain theories 
a constructive induction approach
steven k  donoho
larry a  rendell

department of computer science  univeristy of illinois
    n  mathews ave   urbana  il       usa

donoho cs uiuc edu
rendell cs uiuc edu

abstract

theory revision integrates inductive learning and background knowledge by combining
training examples with a coarse domain theory to produce a more accurate theory  there
are two challenges that theory revision and other theory guided systems face  first  a
representation language appropriate for the initial theory may be inappropriate for an
improved theory  while the original representation may concisely express the initial theory 
a more accurate theory forced to use that same representation may be bulky  cumbersome 
and dicult to reach  second  a theory structure suitable for a coarse domain theory may
be insucient for a fine tuned theory  systems that produce only small  local changes to
a theory have limited value for accomplishing complex structural alterations that may be
required 
consequently  advanced theory guided learning systems require exible representation
and exible structure  an analysis of various theory revision systems and theory guided
learning systems reveals specific strengths and weaknesses in terms of these two desired
properties  designed to capture the underlying qualities of each system  a new system uses
theory guided constructive induction  experiments in three domains show improvement
over previous theory guided systems  this leads to a study of the behavior  limitations 
and potential of theory guided constructive induction 

   introduction
inductive learners normally use training examples  but they can also use background knowledge  effectively integrating this knowledge into induction has been a widely studied research problem  most work to date has been in the area of theory revision in which the
knowledge given is a coarse  perhaps incomplete or incorrect  theory of the problem domain 
and training examples are used to shape this initial theory into a refined  more accurate
theory  ourston   mooney        thompson  langley    iba        cohen        pazzani
  kibler        baffes   mooney        mooney         we develop a more exible and
more robust approach to the problem of learning from both data and theory knowledge by
addressing the two following desirable qualities 

 flexible representation  a theory guided system should utilize the knowledge contained in the initial domain theory without having to adhere closely to the initial
theory s representation language 

 flexible structure  a theory guided system should not be unnecessarily restricted by
the structure of the initial domain theory 

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fidonoho   rendell

before giving more precise definitions of our terms  we motivate our work intuitively 

    intuitive motivation
the first desirable quality  exibility of representation  arises because the theory representation most appropriate for describing the coarse  initial domain theory may be inadequate
for the final  revised theory  while the initial domain theory may be compact and concise in
one representation  an accurate theory may be quite bulky and cumbersome in that representation  furthermore  the representation that is best for expressing the initial theory may
not be the best for carrying out refinements  a helpful refinement step may be clumsy to
make in the initial representation yet be carried out quite simply in another representation 
as a simple example  a coarse domain theory may be expressed as the logical conjunction
of n conditions that should be met  the most accurate theory  though  is one in which any m
of these n conditions holds  expressing this more accurate theory in the dnf representation
used to describe the initial theory would be cumbersome and unwieldy  murphy   pazzani 
       furthermore  arriving at the final theory using the refinement operators most suitable
for dnf  drop condition  add condition  modify condition  would be a cumbersome task 
but when an m of n representation is adopted  the refinement simply involves empirically
finding the appropriate m  and the final theory can be expressed concisely  baffes   mooney 
      
similarly  the second desirable quality  exibility of structure  arises because the theory
structure that was suitable for a coarse domain theory may be insucient for a fine tuned
theory  in order to achieve the desired accuracy  a restructuring of the initial theory may be
necessary  many theory revision systems act by making a series of local changes  but this
can lead to behavior at two extremes  the first extreme is to rigidly retain the backbone
structure of the initial domain theory  only allowing small  local changes  figure   illustrates
this situation  minor revisions have been made   conditions have been added  dropped 
and modified   but the refined theory is trapped by the backbone structure of the initial
theory  when only local changes are needed  these techniques have proven useful  ourston
  mooney         but often more is required  when more is required  these systems often
move to the other extreme  they drop entire rules and groups of rules and then build entire
new rules and groups of rules from scratch to replace them  thus they restructure  but
they forfeit valuable knowledge in the process  an ideal theory revision system would glean
knowledge from theory substructures that cannot be fixed with small  local changes and
use this in a restructured theory 
as an intuitive illustration  consider a piece of software that  almost works   sometimes
it can be made useful through only a few local operations  fixing a couple of bugs  adding
a needed subroutine  and so on  in other cases  though  a piece of software that  almost
works  is in fact far from full working order  it may need to be redesigned and restructured 
a mistake at one extreme is to try to fix a program like this by making a series of patches in
the original code  a mistake at the other extreme is to discard the original program without
learning anything from it and start from scratch  the best approach would be to examine
the original program to see what can be learned from its design and to use this knowledge
in the redesign  likewise  attempting to improve a coarse domain theory through a series
of local changes may yield little improvement because the theory is trapped by its initial
   

firerepresenting and restructuring domain theories

initial theory

a

b

c

j

d e

h i

refined theory

n

k l m

s

a

b

c

o p q r

j

w e

f g

i

n

k l

u

o

p

v

r

t f g

figure    typical theory revision allows only limited structural exibility  although conditions have been added  dropped  and modified  the revised theory is much
constrained by the structure of the initial theory 
structure  this does not render the original domain theory useless  careful analysis of the
initial domain theory can give valuable guidance for the design of the best final theory 
this is illustrated in figure   where many substructures have been taken from the initial
theory and adapted for use in the refined theory  information from the initial theory has
been used  but the structure of the revised theory is not restricted by the structure of the
initial theory 
initial theory

a

b

c

n

j

d e

h i

refined theory

k l m

k

o p q r

l

f

d

g

m

e

h

i

s

o

p

u

v

f g q r

f g

figure    more exible structural modification  the revised theory has taken many substructures from the initial theory and adapted and recombined them for its use 
but the structure of the revised theory is not restricted by the structure of the
initial theory 

   

fidonoho   rendell

    terminology

in this paper  all training data consist of examples which are classified vectors of feature value pairs  we assume that an initial theory is a set of conditions combined using the
operators and  or  and not and indicating one or more classes  while it is unreasonable
to believe that all theories will always be of this form  it covers much existing theory revision
research 
our work is intended as an informal exploration of exible representation and exible
structure  flexible representation means allowing the theory to be revised using a representation language other than that of the initial theory  an example of exible representation
is the introduction of a new operator for combining features   an operator not used in
the initial theory  in section     the example was given of introducing the m of n operator
to represent a theory originally expressed in dnf  flexible structure means not limiting
revision of the theory to a series of small  incremental modifications  an example of this
is breaking the theory down into its components and using them as building blocks in the
construction of a new theory 
constructive induction is a process whereby the training examples are redescribed using
a new set of features  these new features are combinations of the original features  bias
or knowledge may be used in the construction of the new features  a subtle point is that
when we speak of exible representation  we are referring only to the representation of the
domain theory  not the training data  although the phrase  change of representation  is
often applied to constructive induction  this refers to a change of the data  in our paper 
the term exible representation is reserved for a change of theory representation  thus
a system can be performing constructive induction  changing the feature language of the
data  without exhibiting exible representation  changing the representation of the theory  

    overview

theory revision and constructive induction embody complementary aspects of the machine
learning research community s ultimate goals  theory revision uses data to improve a
theory  constructive induction can use a theory to improve data to facilitate learning  in
this paper we present a theory guided constructive induction approach which addresses the
two desirable qualities discussed in section      the initial theory is analyzed  and new
features are constructed based on the components of the theory  the constructed features
need not be expressed in the same representational language as the initial theory and can
be refined to better match the training examples  finally  a standard inductive learning
algorithm  c     quinlan         is applied to the redescribed examples 
we begin by analyzing how landmark theory revision and learning systems have exhibited exibility in handling a domain theory and what part this has played in their performance  from this analysis  we extract guidelines for system design and apply them to the
design of our own limited system  in an effort to integrate learning from theory and data 
we borrow heavily from the theory revision  multistrategy learning  and constructive induction communities  but our guidelines for system design fall closest to classical constructive
induction methods  the central focus of this paper is not the presentation of  another new
system  but rather a study of exible representation and structure  their manifestation in
previous work  and their guidance for future design 
   

firerepresenting and restructuring domain theories

section   gives the context of our work by analyzing previous research and its inuence on our work  section   explores the promoter recognition domain and demonstrates
how related theory revision systems behave in this domain  in section    guidelines for
theory guided constructive induction are presented  these guidelines are a synthesis of the
positive aspects of related research  and they address the two desirable qualities  exibility
of representation and exibility of structure  section   also presents a specific theory guided
constructive induction algorithm which is an instantiation of the guidelines set forth earlier
in that section  results of experiments in three domains are given in section   followed
by a discussion of the strengths of theory guided constructive induction in section    section   presents an experimental analysis of the limits of applicability of our simple algorithm
followed by a discussion of limitations and future directions of our work in section   

   context and related work

although our work bears some resemblance in form and objective to many papers in constructive induction  michalski        fu   buchanan        utgoff        schlimmer       
drastal   raatz        matheus   rendell        pagallo   haussler        ragavan  
rendell        hirsh   noordewier         theory revision  ourston   mooney        feldman  serge    koppel        thompson et al         cohen        pazzani   kibler       
baffes   mooney         and multistrategy approaches  flann   dietterich        towell 
shavlik    noordeweir        dzerisko   lavrac        bloedorn  michalski    wnek       
clark   matwin        towell   shavlik         we focus only upon a handful of these systems  those that have significant  underlying similarities to our work  in this section we
analyze miro  either  focl  labyrinthk   kbann  neither mofn  and grendel to
discuss their related underlying contributions in relationship to our perspective 

   

miro

 drastal   raatz        is a seminal work in knowledge guided constructive induction  it takes knowledge about how low level features interact and uses this knowledge to
construct high level features for its training examples  a standard learning algorithm is
then run on these examples described using the new features  the domain theory is used
to shift the bias of the induction problem  utgoff         empirical results showed that
describing the examples in these high level  abstract terms improved learning accuracy 
the miro approach provides a means of utilizing knowledge in a domain theory without
being restricted by the structure of that theory  substructures of the domain theory can
be used to construct high level features that a standard induction algorithm will arrange
into a concept  some constructed features will be used as they are  others will be ignored 
others will be combined with low level features  and still others may be used differently in
multiple contexts  the end result is that knowledge from the domain theory is utilized 
but the structure of the final theory is not restricted by the structure of the initial theory 
miro provides exible structure 
another benefit is that miro like techniques can be applied even when only a partial
domain theory exists  i e   a domain theory that only specifies high level features but does
not link them together or a domain theory that specifies some high level features but not
others  one of miro s shortcomings is that it provided no means of making minor changes
miro

   

fidonoho   rendell

in the domain theory but rather constructed the features exactly as the domain theory
specified  also the representation of miro s constructed features was primitive   either
an example met the conditions of a high level feature or did not  an example of miro s
behavior is given in section     

   

 

  and labyrinthk

either focl

the

either

 ourston   mooney         labyrinthk  thompson et al          and
focl  pazzani   kibler        systems represent a broad spectrum of theory revision
work  they make steps toward effective integration of background knowledge and inductive
learning  although these systems have many superficial differences with regard to supervised unsupervised learning  concept description language  etc   they share the underlying
principle of incrementally revising an initial domain theory through a series of local changes 
we will discuss either as a representative of this class of systems  either s theory
revision operators include  removing unwanted conditions from a rule  adding needed conditions to a rule  removing rules  and adding totally new rules  either first classifies its
training examples according to the current theory  if any are misclassified  it seeks to repair
the theory by applying a theory revision operator that will result in the correct classification
of some previously misclassified examples without losing any of the correct examples  thus
a series of local changes are made that allow for an improvement of accuracy on the training
set without losing any of the examples previously classified correctly 
either type methods provide simple yet powerful tools for repairing many important
and common faults in domain theories  but they fail to meet the qualities of exible representation and exible structure  because the theory revision operators make small  local
modifications in the existing domain theory  the final theory is constrained to be similar in
structure to the initial theory  when an accurate theory is significantly different in structure from the initial theory  these systems are forced to one of the two extremes discussed
in section    the first extreme is to become trapped at a local maximum similar to the
initial theory unable to reach the global maximum because only local changes can be made 
the other extreme is to drop entire rules and groups of rules and replace them with new
rules built from scratch thus forfeiting the knowledge contained in the domain theory 
also  either carries out all theory revision steps in the representation of the initial
theory  consequently  the representation of the final theory is the same as that of the initial
theory  another representation may be more appropriate for the revised theory than the
one in which the initial theory comes  but facilities are not provided to accommodate this 
an advanced theory revision system would combine the locally acting strengths of eithertype systems with exibility of structure and exibility of representation  an example of
either s behavior is given in section     

   

kbann

and neither mofn

the kbann system  towell et al         towell   shavlik        makes unique contributions to theory revision work  kbann takes an initial domain theory described symbolically
in logic and creates a neural network whose structure and initial weights encode this theory 
backpropagation  rumelhart  hinton    mcclelland        is then applied as a refinement tool for fine tuning the network weights  kbann has been empirically shown to give
   

firerepresenting and restructuring domain theories

significant improvement over many theory revision systems for the widely used promoter
recognition domain  although our work is different in implementation from kbann  our
abstract ideologies are similar 
one of kbann s important contributions is that it takes a domain theory in one representation  propositional logic  and translates it into a less restricting representation  neural
network   while logic is an appropriate representation for the initial domain theory for the
promoter problem  the neural network representation is more convenient both for refining
this theory and for expressing the best revised theory  this change of representation is
kbann s real source of power  much attention has been given to the fact that kbann combines symbolic knowledge with a subsymbolic learner  but this combination can be viewed
more generally as a means of implementing the important change of representation  it may
be the change of representation that gives kbann its power  not necessarily its specific
symbolic subsymbolic implementation  thus the kbann system embodies the higher level
principle of allowing refinement to occur in an appropriate representation 
if an alternative representation is kbann s source of power  the question must be raised
as to whether the actual kbann implementation is always the best means of achieving this
goal  the neural network representation may be more expressive than is required  accordingly  backpropagation often has more refinement power than is needed  thus kbann may
carry excess baggage in translating into the neural net representation  performing expensive
backpropagation  and extracting symbolic rules from the refined network  although the full
extent of kbann s power may be needed for some problems  many important problems may
be solvable by applying kbann s principles at the symbolic level using less expensive tools 
neither mofn  baffes   mooney         a descendant of either  is a second example
of a system that allows a theory to be revised in a representation other than that of the
initial theory  the domain theory input into neither mofn is expressed in propositional
logic as an and or tree  neither mofn interprets the theory less rigidly   an and
rule is true any time any m of its n conditions are true  initially m is set equal to n  all
conditions must be true for the rule to be true   and one theory refinement operator is to
lower m for a particular rule  the end result is that examples that are a close enough
partial match to the initial theory are accepted  neither mofn  since it is built upon
the either framework  also includes either like theory revision operators  add condition 
drop condition  etc 
thus neither mofn allows revision to take place in a representation appropriate
for revision and appropriate for concisely expressing the best refined theory  neithermofn has achieved results comparable to kbann on the promoter recognition domain 
which suggests that it is the change of representation which these two systems share that
give them their power rather than any particular implementation  neither mofn also
demonstrates that a small amount of representational exibility is sometimes enough  the
m of n representation it employs is not as big a change from the original representation
as the neural net representation which kbann employs yet it achieves similar results and
arrives at them much more quickly than kbann  baffes   mooney        
a shortcoming of neither mofn is that since it acts by making local changes in an
initial theory  it can still become trapped by the structure of the initial theory  an advanced
theory revision system would incorporate neither mofn s and kbann s exibility of
   

fidonoho   rendell

representation and allow knowledge guided theory restructuring  examples of
and neither mofn s behavior are given in sections     and     

   

 s

kbann

grendel

cohen        analyzes a class of theory revision systems and draws some insightful conclusions  one is that  generality  in theory interpretation  comes at the expense of power  
he draws this principle from the fact that a system such as either or focl treats every
domain theory the same and therefore must treat every domain theory in the most general way  he argues that rather than just applying the most general refinement strategy
to every problem  a small set of refinement strategies should be available that are narrow
enough to gain leverage yet not so narrow that they only apply to a single problem  cohen
presents grendel  a toolbox of translators each of which transforms a domain theory into
an explicit bias  each translator interprets the domain theory in a different way  and the
most appropriate interpretation is applied to a given problem 
we apply cohen s principle to the representation of domain theories  if all domain
theories are translated into the same representation  then the most general  adaptable representation has to be used in order to accommodate the most general case  this comes
at the expense of higher computational costs and possibly lower accuracy due to overfit
stemming from unbridled adaptability  the neural net representation into which kbann
translates domain theories allows    a measure of partial match to the domain theory    different parts of the domain theory to be weighted differently    conditions to be added to
and dropped from the domain theory  all these options of adaptability are probably not
necessary for most problems and may even be detrimental  these options in kbann also
require the computationally expensive backpropagation method 
the representation used in neither mofn is not as adaptable as kbann s   it does
not allow individual parts of the domain theory to be weighted differently  neithermofn runs more quickly than kbann on small problems and probably matches or even
surpasses kbann s accuracy for many domains   domains for which fine grained weighting
is unfruitful or even detrimental  a toolbox of theory rerepresentation translators analogous
to grendel would allow a domain theory to be translated into a representation having the
most appropriate forms of adaptability 

    outlook and summary

in summary  we briey reexamine exible representation and exible structure  the two
desirable qualities set forth in section    we consider how the various systems exemplify
some subset of these desirable qualities 
 kbann and neither mofn both interpreted a theory more exibly than its original
representation allowed and revised the theory in this more adaptable representation 
a final  refined theory often has many exceptions to the rule  it may tolerate partial
matches and missing pieces of evidence  it may weight some evidence more heavily
than other evidence  kbann s and neither mofn s new representation may not
be the most concise  appropriate representation for the initial theory  but the new
representation allows concise expression of an otherwise cumbersome final theory 
these are cases of the principle of exible representation 
   

firerepresenting and restructuring domain theories

 standard induction programs have been quite successful at building concise theories
with high predictive accuracy when the target concept can be concisely expressed using
the original set of features  when it can t  constructive induction is a means of creating
new features such that the target concept can be concisely expressed  miro uses
constructive induction to take advantage of the strengths of both a domain theory and
standard induction  knowledge from the theory guides the construction of appropriate
new features  and standard induction structures these into a concise description of
the concept  thus miro like construction coupled with standard induction provides
a ready and powerful means of exibly restructuring the knowledge contained in an
initial domain theory  this is a case of the principle of exible structure 

in the following section we introduce the dna promoter recognition domain in order
to illustrate tangibly how some of the systems discussed above integrate knowledge and
induction 

   demonstrations of related work
this section introduces the promoter recognition domain  harley  reynolds    noordewier 
      and briey illustrates how a miro like system  either  kbann  and neithermofn behave in this domain  we implemented a miro like system for the promoter domain  versions of either and neither mofn were available from ray mooney s group 
kbann s behavior is described by analyzing  towell   shavlik         we chose the promoter domain because it is a non trivial  real world problem which a number of theory
revision researchers have used to test their work  ourston   mooney        thompson
et al         wogulis        cohen        pazzani   kibler        baffes   mooney       
towell   shavlik         the promoter domain is one of three domains in which we evaluate
our work  theory guided constructive induction  in section   

    the promoter recognition domain

a promoter sequence is a region of dna that marks the beginning of a gene  each example in the promoter recognition domain is a region of dna classified either as a promoter
or a non promoter  as illustrated in figure    examples consist of    features representing a sequence of    dna nucleotides  each feature can take on the values a g c  or t
representing adenine  guanine  cytosine  and thymine at the corresponding dna position 
the features are labeled according to their position from p    to p    there is no zero
position   the notation  p n   denotes the nucleotide that is n positions upstream from
the beginning of the gene  the goal is to predict whether a sequence is a promoter from its
nucleotides  a total of     examples are available     promoters and    non promoters 
the promoter recognition problem comes with the initial domain theory shown in figure    quoted almost verbatim from towell and shavlik s entry in the uci machine learning
repository   the theory states that promoter sequences must have two regions that make
contact with a protein and must also have an acceptable conformation pattern  there are
four possibilities for the contact region at minus        nucleotides upstream from the beginning of the gene   a match of any of these four possibilities will satisfy the minus   
contact condition  thus they are joined by disjunction  similarly  there are four possibilities
   

fidonoho   rendell

dna sequence

p   

p  

c g a c t t
figure    an instance in the promoter domain consists of a sequence of    nucleotides
labeled from p    to p    each nucleotide can take on the values a g c  or t
representing adenine  guanine  cytosine  and thymine 
for the contact region at minus    and four acceptable conformation patterns  figure  
gives a more pictorial presentation of portions of the theory  of the     examples in the
dataset  none matched the domain theory exactly  yielding an accuracy of     

   

miro

in the promoter domain

a miro like system in the promoter domain would use the rules in figure   to construct new high level features for each dna segment  figure   shows an example of this  a
dna segment is shown from position p    through position p     the minus    rules from
the theory are also shown  and four new features  feat minus   a through feat minus   d 
have been constructed for that dna segment  one for each minus    rule  the new features feat minus   a and feat minus   d both have the value   because the dna fragment
matches the first and fourth minus    rules  likewise  feat minus   b and feat minus   c
both have the value   because the dna fragment does not match the second and third
rules  furthermore  since the four minus    rules are joined by disjunction  a new feature 
feat minus   all  is created for the group that would have the value   because at least one
of the minus    rules matches 
new features would similarly be created for the minus    rules and the conformation
rules  and a standard induction algorithm could then be applied  we implemented a mirolike system  figure   gives an example theory created by it   drastal s original miro used
the candidate elimination algorithm  mitchell        as its underlying induction algorithm 
we used c     quinlan          as opposed to theory revision systems that incrementally
modify the domain theory  miro has broken the theory down into its components and has
fashioned these components into a new theory using a standard induction program  thus
miro has exhibited the exible structure principle for this domain   it was not restricted
in any way by the structure of the initial theory  rather  miro exploited the strengths of
standard induction to concisely characterize the training examples using the new features 
   

firerepresenting and restructuring domain theories

promoters have a region where a protein  rna polymerase  must make contact and
the helical dna sequence must have a valid conformation so that the two pieces
of the contact region spatially align  prolog notation is used 
promoter    contact  conformation 
there are two regions  upstream  from the beginning of the gene at which the
rna polymerase makes contact 
contact

   minus     minus    

the following rules describe the compositions of possible contact regions 
minus   
minus   
minus   
minus   

   p    c  p    t  p    t  p    g  p    a  p    c 
 p    t  p    t  p    g 
p    c  p    a 
 p    t  p    t  p    g  p    a  p    c  p    a 
 p    t  p    t  p    g  p    a  p    c 

minus   
minus   
minus   
minus   

   p    t  p    a  p    t  p    a  p    a  p   t 
 p    t  p    a 
p    a 
p   t 
 p    t  p    a  p    t  p    a  p   a  p   t 
 p    t  p    a 
p   t 

the following rules describe sequences that produce acceptable conformations 
conformation    p    c 
p    t 
p   c 
conformation    p    a 
conformation    p    a 
conformation    p    a 
p    t 

p    a  p    a  p    t  p    t  p    a  p    c  p    g 
p    c  p   g  p   c  p   g  p   c  p   c  p   c 
p    a  p    a 
p    t  p    t  p    a  p    t  p    t  p    g  p   a 
p    a  p    t  p    t  p    t  p    a  p    a  p    t 
p   t 

figure    the initial domain theory for recognizing promoters  from towell and shavlik  
a weakness miro displays in this example is that it allows no exibility of representation
of the theory  the representation of the features constructed by miro is basically the same
all or none representation of the initial theory  either a dna segment matched a rule  or it
did not 

   

either

in the promoter domain

an either like system refines the initial promoter theory by dropping and adding
conditions and rules  we simulated either by turning off the m of n option in neither
and ran it in the promoter domain  figure   shows the refined theory produced using a
randomly selected training set of size     because the initial promoter domain theory does
not lend itself to revision through small  local changes  either has only limited success 
   

fidonoho   rendell

dna sequence

p   

p  

contact at minus   

contact at minus   

                           

c t

t g a c

 

                      

     

t a a t

   

t

a

or

or

  t t g   c a

 

t a

or

t  

or

  t t g a c a

 

t a

or

  t t g a c

  a  
t a a

t  

or

 

 

 

t a  

 

  t

figure    the contact portion of the theory  there are four possibilities for both the
minus    and minus    portions of the theory  a     matches any nucleotide 
the conformation portion of the theory is too spread out to display pictorially 
in this run  the program exhibited the second behavioral extreme discussed in section   
it entirely removed groups of rules and then tried to build new rules to replace what was
lost  the minus    and conformation rules have essentially been removed  and new rules
have been added to the minus    group  these new minus    rules contain the condition
p    t previously found in the minus    group and the condition p    a previously found
in the conformation group 
either s behavior in this example is a direct result of its lack of exibility of representation and exibility of structure  it is dicult to transform the minus    and conformation
rules into something useful in their initial representation using either s locally acting operators  either handles this by dropping these sets of rules  losing their knowledge  and
attempting to rediscover the lost knowledge empirically  the end result of this loss of
knowledge is lower than optimal accuracy shown later in section   

   

kbann

in the promoter domain

figure    modeled after a figure by towell and shavlik         shows the setup of a kbann
network for the promoter theory  each slot along the bottom represents one nucleotide
in the dna sequence  each node at the first level up from the bottom embodies a single
domain rule  and higher levels encode groups of rules with the final concept at the top  the
links shown in the figure are the ones that are initially high weighted  the net is next filled
out to be fully connected with low weight links  backpropagation is then applied to refine
the network s weights 
   

firerepresenting and restructuring domain theories

a dna segment fragment 
   

p    g  p    c  p    t  p    t  p    g  p    a  p    c  p    t  p    t

   

the minus    group of rules and corresponding constructed features 
minus       p    c  p    t  p    t  p    g  p    a  p    c 
minus     p    t  p    t  p    g 
p    c  p    a 
minus     p    t  p    t  p    g  p    a  p    c  p    a 
minus     p    t  p    t  p    g  p    a  p    c 

feat minus   a    
feat minus   b    
feat minus   c    
feat minus   d    

feat minus   all    feat minus   a   feat minus   b   feat minus   c   feat minus   d     

figure    an example of feature construction in a miro like system  the constructed
features for the first and fourth rules in the minus    group are true  value     
because the dna segment matches these rules  the constructed feature for the
entire group  feat minus   all  is true because the four minus    rules are joined
by disjunction 
feat minus   all
 

 
promoter

feat conf b
 

 

feat minus   d
 
non promoter

promoter
 

promoter

figure    an example theory created by a miro like system  a dna segment is recognized
as a promoter if it matches any of the minus    rules  the second conformation
rule  or the fourth minus    rule 
the neural net representation is more appropriate for this domain than the propositional
logic representation of the initial theory  it allows for a measurement of partial match by
weighting the links in such a way that a subset of a rule s conditions are enough to surpass a
node s threshold  it also allows for variable weightings of different parts of the theory  therefore  more predictive nucleotides can be weighted more heavily  and only slightly predictive
nucleotides can be weighted less heavily  kbann has only limited exibility of structure 
because the refined network is the result of a series of incremental modifications in the
initial network  a fundamental restructuring of the theory it embodies is unlikely  kbann
   

fidonoho   rendell

promoter    contact  conformation 
contact

   minus     minus    

minus   
minus   
minus   
minus   
minus   
minus   

       

p    t 
p    t 
p    t 
p    g 
p    g 
p    t 

p    g 
p    a  p    c 
p    c  p    c 
p    t 
p    a 
p    g 

minus       true 
conformation    true 

figure    a revised theory produce by either 
promoter

contact

conformation
minus   

p   

minus   

dna sequence

p  

figure    the setup of a kbann network for the promoter theory 
is limited to finding the best network with the same fundamental structure imposed on it
by the initial theory 
one of kbann s advantages is that it uses a standard learning algorithm as its foundation  backpropagation has been widely used and consequently improved by previous
researchers  theory refinement tools that are built from the ground up or use a standard
tool only tangentially suffer from having to invent their own methods of handling standard
problems such as overfit  noisy data  etc  a wealth of neural net experience and resources
is available to the kbann user  as neural net technology advances  kbann technology will
passively advance with it 
   

firerepresenting and restructuring domain theories

   

neither mofn

in the promoter domain

refines the initial promoter theory not only by dropping and adding conditions and rules but also by allowing conjunctive rules to be true if only a subset of their
conditions are true  we ran neither mofn with a randomly selected training set of size
    and figure    shows a refined promoter theory produced  the theory expressed here
with   m of n rules would require    rules using propositional logic  the initial theory s
representation  more importantly  it is unclear how any system using the initial representation would reach the    rule theory from the initial theory  thus the m of n representation
adopted not only allows for the concise expression of the final theory but also facilitates the
refinement process 
neither mofn

promoter      of   contact  conformation   
contact

     of   minus     minus      

minus         of   p    t  p    t  p    g 
p    c  p    a   
minus         of   p    t  p    t  p    g  p    a  p    c
  
minus   
minus   
minus   
minus   

     

 
 
 
 

of
of
of
of

 
 
p    t 
  p    t  p    a 
 
p    t 

p    t  p    a 
p    a 
p    a 
p    t  p    a  p    a  p   t
p    a 
p    a 

p   t
p   t

  
  
  
p    g   

conformation    true 

figure     a revised theory produced by neither mofn 
neither mofn displays exibility of representation by allowing an m of n interpretation of the original propositional logic  but it does not allow for as fine grained refinement
as kbann  both allow for a measure of partial match  but kbann could weight more
predictive features more heavily  for example  in the minus    rules  perhaps p    t is
more predictive of a dna segment being a promoter than p    g and therefore should be
weighted more heavily  neither mofn simply counts the number of true conditions in a
rule  therefore  every condition is weighted equally  kbann s fine grained weighting may be
needed in some domains and not in others  it may actually be detrimental in some domains 
an advanced theory revision system should offer a range of representations 
like kbann  neither mofn has only limited exibility of structure  the refined
theory is reached through a series of small  incremental modifications in the initial theory
precluding a fundamental restructuring  neither mofn is therefore limited to finding the
best theory with the same fundamental structure as the initial theory 

   theory guided constructive induction

in the first half of this section we present guidelines for theory guided constructive induction
that summarize the work discussed in sections   and    the remainder of the section
   

fidonoho   rendell

presents an algorithm that instantiates these guidelines  we evaluate the algorithm in
section   

    guidelines

the following guidelines are a synthesis of the strengths of the previously discussed related
work 
 as in miro  new features should be constructed using components of the domain
theory  these new features are combinations of existing features  and a final theory is
created by applying a standard induction algorithm to the training examples described
using the new features  this allows knowledge to be gleaned from the initial theory
without forcing the final theory to conform to the initial theory s backbone structure 
it takes full advantage of the domain theory by building high level features from the
original low level features  it also takes advantage of a strength of standard induction
  building concise theories having high predictive accuracy when the target concept
can be concisely expressed using the given features 
 as in either  the constructed features should be modifiable by various operators
that act locally  such as adding or dropping conjuncts from a constructed feature 
 as in kbann and neither mofn  the representation of the constructed features
need not be the exact representation in which the initial theory is given  for example 
the initial theory may be given as a set of rules written in propositional logic  a
new feature can be constructed for each rule  but it need not be a boolean feature
telling whether all the conditions are met  for example it may be a count of how
many conditions of that rule are met  this allows the final theory to be formed and
expressed in a representation that is more suitable than the representation of the
initial theory 
 like grendel  a complete system should offer a library of interpreters allowing the
domain theory to be translated into a range of representations with differing adaptability  one interpreter might emulate miro strictly translating a domain theory
into boolean constructed features  another interpreter might construct features that
count the number of satisfied conditions of the corresponding component of the domain theory thus providing a measure of partial match  still another interpreter
might construct features that are weighted sums of the satisfied conditions  the
weights could be refined empirically by examining a set of training examples  thus
the most appropriate amount of expressive power can be applied to a given problem
without incurring unnecessary expense 

    a specific interpreter

this section describes an algorithm which is a limited instantiation of the guidelines just
described  the algorithm is intended as a demonstration of the distillation and synthesis
of the principles embodied in previous landmark systems  it contains a main module 
tgci described in figure     and a specific interpreter  tgci  described in figure    
the main module tgci redescribes the training and testing examples by calling tgci 
   

firerepresenting and restructuring domain theories

and then applies c    to the redescribed examples  just as miro applied the candidate
elimination algorithm to examples after redescribing them   tgci  can be viewed as a
single interpreter from a potential grendel like toolbox  it takes as input a single example
and a domain theory expressed as an and or tree such as the one shown in figure    
it returns a new vector of features for that example that measure the partial match of the
example to the theory  thus it creates new features from components of the domain theory
as in miro  but because it measures partial match  it allows exibility in representing
the information contained in the initial theory as in kbann and neither mofn  one
aspect of the guidelines in     that does not appear in this algorithm is either s locally
acting operators such as adding and dropping conditions from a portion of the theory 
the following two paragraphs explain in more detail the workings of tgci  and tgci
respectively 
given  an example e and a domain theory with root node r  the domain
theory is an and or not tree in which the leaves are conditions which can
be tested to be true or false 
return  a pair p    f  f   where f is the top feature measuring the partial
match of e to the whole domain theory  and f is a vector of new features measuring the partial match of e to various parts and subparts of the domain theory 

   if r is a directly testable condition  return p        if r is true for e
and p         if r is false for e  
   n   the number of children of r
   for each child rj of r  call tgci  rj  e   and store the respective results
in pj    fj   fj   
   if the major operator of r is or  fnew   max  fj   
return p    fnew   concatenate  fnew    f   f        fn   
p
   if the major operator of r is and  fnew     nj   fj   n 
return p    fnew   concatenate  fnew    f   f        fn   
   if the major operator of r is not  fnew       f   
return p    fnew   f   
figure     the tgci  algorithm
the tgci  algorithm  given in figure     is recursive  its inputs are an example e and
a domain theory with root node r  it ultimately returns a redescription of e in the form
of a vector of new features f   it also returns a value f called the top feature which is used
in intermediate calculations described below  the base case occurs if the domain theory is
a single leaf node  i e   r is a simple condition   in this case  line     tgci  returns the
top feature   if the condition is true and    if the condition is false  no new features are
returned in the base case because they would simply duplicate the existing features  if the
   

fidonoho   rendell

domain theory is not a single leaf node  tgci  recursively calls itself on each of r s children
 line     when a child of r  rj   is processed  it returns a vector of new features fj  which
measures the partial match of the example to the j th child of r and its various subparts  
it also returns the top feature fj which is included in fj but is marked as special because it
measures the partial match of the example to the whole of the j th child of r  if there are n
children  the result of line   is n vectors of new features  f  to fn   and n top features  f 
to fn   if the operator at node r is or  line     then fnew   the new feature created for that
node  is the maximum of fj   thus fnew measures how closely the best of r s children come
to having its conditions met by the example  the vector of new features returned in this
case is a concatenation of fnew and all the new features from r s children  if the operator
at node r is and  line     then fnew is the average of fj   thus fnew measures how closely
all of r s children as a group come to having their conditions met by the example  the
vector of new features returned in this case is again a concatenation of fnew and all the new
features from r s children  if the operator at node r is not  line     r should only have
one child  and fnew is f  negated  thus fnew measures the extent to which the conditions
of r s child are not met by the example 
given  a set of training examples etrain   a set of testing examples etest   and a
domain theory with root node r 
return  learned concept and accuracy on testing examples 
   for each example ei   etrain   call tgci  r ei  which returns pi  
 fi   fi   etrain new   ffig 
   for each example ei   etest  call tgci  r ei   which returns pi  
 fi   fi   etest new   ffi g 
   call c    with training examples etrain new and testing examples
etest new   return decision tree and accuracy on etest new  
figure     the tgci algorithm
if tgci  is called twice with two different examples but with the same domain theory 
the two vectors of new features will be the same size  furthermore  corresponding features
measure the match of corresponding parts of the domain theory  the tgci main module
in figure    takes advantage of this by creating redescribed example sets from the input
example sets  line   redescribes each example in the training set producing a new training
set  line   does the same for the testing set  line   runs the standard induction program
c    on these redescribed example sets  the returned decision tree can be easily interpreted
by examining which new features were used and what part of the domain theory they
correspond to 

   

tgci 

examples

as an example of how the tgci  interpreter works  consider the toy theory shown in
figure     tgci  redescribes the input example by constructing a new feature for each node
   

firerepresenting and restructuring domain theories

in the input theory  consider the situation where the input example matches conditions a 
b  and d but not c and e  when tgci  evaluates the children of node    it gets the values
f       f       f        f       and f        since the operator at node   is and  fnew
is the average of the values received from the children                                     
       likewise  if condition g matchs but not f and h  fnew for node   will have the value
                                 because two of three matching conditions at node   give
the value        and this is negated by the not at node    since node   is a disjunction 
its new feature measures the best partial match of its two children and has the value     
 max              and so on 
 
 

not
 
 
 

not
 

a b c d e

 

f g h

 

i j k

 

l m n o

figure     an example theory in the form of an and or tree that might be used by the
interpreter to generate constructed features 
figure    shows how tgci  redescribes a particular dna segment using the minus   
rules of the promoter theory  a partial dna segment is shown along with the four minus   
rules and the new feature constructed for each rule  we have given the new features names
here to simplify our illustration   for the first rule  four of the six nucleotides match  therefore  for that dna segment feat minus   a has the value                              
for the second rule  four of the five nucleotides match  therefore  feat minus   b has
the value       because these and the other two minus    rules are joined by disjunction in the original domain theory  feat minus   all  the new feature constructed for this
group  takes the maximum value of its four children  therefore  feat minus   all has the
value      because feat minus   b has the value       the highest in the group  intuitively  feat minus   all represents the best partial match of this grouping   the extent
to which the disjunction is partially satisfied  the results of running tgci  on each dna
sequence is a set of redescribed training examples  each redescribed example has a value for
feat minus   a through feat minus   d  feat minus   all  and all other nodes in the promoter domain theory  the training set is essentially redescribed using a new feature vector
derived from information contained in the domain theory  in this form  any off the shelf
induction program can be applied to the new example set 
anomalous situations can be created in which tgci  gives a  good score  to a seemingly
bad example and a bad score to a good example  situations can also be created where
logically equivalent theories give different scores for a single example  these occur because
   

fidonoho   rendell

a dna segment fragment 
   

p    g  p    c  p    t  p    t  p    g  p    c  p    a  p    a  p    t

   

the minus    group of rules and corresponding constructed features 
minus       p    c  p    t  p    t  p    g  p    a  p    c 
minus     p    t  p    t  p    g 
p    c  p    a 
minus     p    t  p    t  p    g  p    a  p    c  p    a 
minus     p    t  p    t  p    g  p    a  p    c 

feat minus   a       
feat minus   b       
feat minus   c       
feat minus   d       

feat minus   all   max feat minus   a  feat minus   b  feat minus   c  feat minus   d        

figure     an example of how tgci  generates constructed features from a portion of the
promoter domain theory and a dna segment  four of the conditions in the first
minus    rule match the dna segment  therefore  the constructed feature for
that rule has the value                                         feat minus   all 
the new feature for the entire minus    group takes the maximum value of its
children thus embodying the best partial match of the group 
is biased to favor situations where more matched conditions of an and is desirable 
but more matched conditions of an or is not necessarily better  eliminating these anomalies
would remove this bias 
tgci 

   experiments and analysis
this section presents the results of applying theory guided constructive induction to three
domains  the promoter domain  harley et al          the primate splice junction domain  noordewier  shavlik    towell         and the gene identification domain  craven   shavlik 
       in each case the tgci  interpreter was applied to the domain s theory and examples
in order to redescribe the examples using new features  then c     quinlan        was
applied to the redescribed examples 

    the promoter domain
figure    shows a learning curve for theory guided constructive induction in the promoter
domain accompanied by curves for either  labyrinthk   kbann  and neither mofn 
following the methodology described by towell and shavlik         the set of     examples
was randomly divided into a training set of size    and a testing set of size     a learning
curve was created by training on subsets of the training set of size                         
using the    examples for testing  the curves for either  labyrinthk   and kbann were
taken from ourston and mooney         thompson  langley  and iba         and towell
   

firerepresenting and restructuring domain theories

and shavlik        respectively and were obtained by a similar methodology    the curve
fortgci is the average of    independent random data partitions and is given along with    
confidence ranges  the neither mofn program was obtained from ray mooney s group
and was used in generating the neither mofn curve using the same    data partitions
as were used for tgci  
    
  

either
labyrinth k
neither mofn
kbann
tgci
    confidence of neither mofn
    confidence of tgci

    
  
    
  
    
  error

  
    
  
    
  
    
  
   
 
   
 
 

 

                                            
size of training sample

figure     learning curves for theory guided constructive induction and other systems in
the promoter domain 
tgci showed improvement over either and labyrinthk for all portions of the curve
and also performed better than kbann and neither mofn for all except the smallest training sets  confidence intervals were not available for either  labyrinthk   and

  

used a testing set of size    and did not use the conformation portion of the domain theory  the
testing set in labyrinthk always consisted of    promoters and    non promoters 
   baffes and mooney        report a slightly better learning curve for neither mofn than we obtained 
but after communication with paul baffes  we think the difference is caused by the random selection of
data partitions 
either

   

fidonoho   rendell

kbann  but in a pairwise comparison with neither mofn  the improvement of tgci was
significant at the        level of confidence for training sets of size    and larger 

structure of initial promoter theory

     of
first
conf 
rule

     of
first
minus   
rule

     of
second
minus   
rule

     of
third
minus   
rule

     of
fourth
minus   
rule

     of
first
minus   
rule

     of
second
minus   
rule

     of
third
minus   
rule

     of
second
conf 
rule

     of
third
conf 
rule

     of
fourth
conf 
rule

     of
fourth
minus   
rule

structure of revised promoter theory

     of
second
minus   
rule

     of
first
minus   
rule

     of
second
minus   
rule

     of
third
minus   
rule

     of
fourth
minus   
rule

figure     the revised theory produced by theory guided constructive induction has borrowed substructures from the initial theory  but as a whole has not been restricted by its structure 
figure    compares the initial promoter theory with a theory created by tgci  reasons
for tgci s improvement can be inferred from this figure  tgci has extracted the components of the original theory that are most helpful and restructured them into a more
concise theory  neither kbann nor neither mofn facilitates this radical extraction and
restructuring  as seen in the leaf nodes  the new theory also measures the partial match
of an example to components of the original theory  this aspect is similar in kbann and
neither mofn 
part of tgci s improvement over kbann and neither mofn may be due to a knowledge bias conict in the latter two systems  a situation where revision biases conict with
knowledge in such a way as to undo some of the knowledge s benefits  this can occur
whenever detailed knowledge is opened up to revision using a set of examples  the revision
is not guided only by the examples but rather by the examples as interpreted by a set
   

firerepresenting and restructuring domain theories

of algorithmic biases  biases that are useful in the absence of knowledge may undo good
knowledge when improperly applied  yet these biases developed and perfected for pure induction are often unquestioningly applied to the revision of theories  the biases governing
the dropping of conditions in neither mofn and reweighting conditions in kbann may
be neutralizing the promoter theory s potential  we speculate this because we conducted
some experiments that allowed bias guided dropping and adding of conditions within tgci 
we found that these techniques actually reduced accuracy in this domain 
  
    
  
    
c   
backpropagation
kbann
tgci
    confidence of tgci
domain theory

  
    
  error

  
    
  
    
  
    
  
    
  
   
 

  

  

  

  

   

   

   

   

   

   

size of training sample

figure     learning curves for tgci and other systems in the primate splice junction domain 

    the primate splice junction domain

the primate splice junction domain  noordewier et al         involves analyzing a dna
sequence and identifying boundaries between introns and exons  exons are the parts of a
dna sequence kept after splicing  introns are spliced out  the task then involves placing a
   

fidonoho   rendell

given boundary into one of three classes  an intron exon boundary  an exon intron boundary  or neither  an imperfect domain theory is available which has a       error rate on
the entire set of available examples 
figure    shows learning curves for c     backpropagation  kbann  and tgci in the
primate splice junction domain  the results for kbann and backpropagation were taken
from towell and shavlik         the curves for plain c    and the tgci algorithm were
created by training on sets of size                               and testing on a set of size
     the curves for c    and tgci are the average of    independent data partitions 
no comparison was made with neither mofn because the implementation we obtained
could handle only two class concepts  for training sets larger than      kbann  tgci  and
backpropagation all performed similarly 
the accuracy of tgci appears slightly worse than that of kbann but perhaps not significantly  kbann s advantage over tgci is its ability to assign fine grained weightings
to individual parts of a domain theory  tgci s advantage over kbann is its ability to
more easily restructure the information contained in a domain theory  we speculate that
kbann s capability to assign fine grained weights outweighted its somewhat rigid structuring of this domain theory  theory guided constructive induction has an advantage of
speed over kbann because c     its underlying learner  runs much more quickly than
backpropagation  kbann s underlying learning algorithm 

    the gene identification domain
the gene identification domain  craven   shavlik        involves classifying a given dna
segment as a coding sequence  one that codes a protein  or a non coding sequence  no
domain theory was available in the gene identification domain  therefore  we created an
artificial domain theory using the information that organisms may favor certain nucleotide
triplets over others in gene coding  the domain theory embodies the knowledge that a dna
segment is likely to be a gene coding segment if its triplets are coding favoring triplets or if
its triplets are not noncoding favoring triplets  the decision of which triplets were codingfavoring  which were noncoding favoring  and which favored neither  was made empirically
by analyzing the makeup of      coding and      noncoding sequences  the specific artificial domain theory used is described in online appendix   
figure    shows learning curves for c    and tgci in the gene identification domain 
the original domain theory yields       error  the curves were created by training on
example sets of size                    and testing on a separate example set of size      
the curves are the average of    independent data partitions 
only a partial curve is given for neither mofn because it became prohibitively slow
for larger training sets  in the promoter domain where training sets were smaller than     
tgci and neither mofn ran at comparable speeds  approximately    seconds on a sun 
workstation   in this domain tgci ran in approximately   minutes for larger training sets 
neither mofn took    times as long as tgci on training sets of size         times as
long for size      and     times as long for size       consequently  neither mofn s
curve only extends to      and only represents five randomly selected data partitions  for
these reasons  a solid comparison of neither mofn and tgci cannot be made from these
curves  but it appears that tgci s accuracy is slightly better  we speculate that neither   

firerepresenting and restructuring domain theories

  
    
  

tgci
    confidence of tgci
c   
neither mofn
domain theory

    

  error

  
    
  
    
  
    
  
 

   

   

   

   

                             

number training examples

figure     learning curves for tgci and other systems in the gene identification domain 
 s slightly lower accuracy is partially due to the fact that it revises the theory to
correctly classify all the training examples  the result is a theory which likely overfits the
training examples  tgci does not need to explicitly avoid overfit because this is handled
by its underlying learner 
mofn

    summary of experiments
our goal in this paper has not been to present a new technique but rather to understand
the behavior of landmark systems  distill their strengths  and synthesize them into a simple
system  tgci  the evaluation of this algorithm shows that its accuracy roughly matches or
exceeds that of its predecessors  in the promoter domain  tgci showed sizable improvement
over many published results  in the splice junction domain  tgci narrowly falls short of
kbann s accuracy  in the gene identification domain  tgci outperforms neither mofn 
in all these domains tgci greatly improves on the original theory alone and c    alone 
   

fidonoho   rendell

is faster than its closest competitors  tgci runs as much as     times faster than
on large datasets  a strict quantitative comparison of the speeds of tgci
and kbann was not made because    backpropagation is known to be much slower than
decision trees  mooney  shavlik  towell    gove            kbann uses multiple hidden
layers which makes its training time even longer  towell   shavlik         and    towell
and shavlik        point out that each run of kbann must be made multiple times with
different initial random weights  whereas a single run of tgci is sucient 
overall  our experiments support two claims of this paper  first  the accuracy of tgci
substantiates our delineation of system strengths in terms of exible theory representation
and exible theory structure  since this characterization is the basis for this algorithm s
design  second  tgci s combination of speed and accuracy suggest that unnecessary computational complexity can be avoided in synthesizing the strengths of landmark systems 
in the following section we take a closer look at the strengths of theory guided constructive
induction 
tgci

neither mofn

   discussion of strengths
below a number of strengths of theory guided constructive induction are discussed within
the context of the tgci algorithm used in our experiments 

    flexible representation

as discussed in section    for many domains the representation most appropriate for an
initial theory may not be most appropriate for a refined theory  because theory guided constructive induction allows the translation of the initial theory into a different representation 
it can accommodate such domains  in the experiments in this paper a representation was
needed which allowed for a measurement of partial match to the domain theory  tgci 
accomplished this by simply counting the matching features and propagating this information up the theory appropriately  either and labyrinthk do not easily afford this
measure of partial match and therefore are more appropriate for problems in which the best
representation of the final theory is the same as that of the initial theory  kbann allows
a finer grained measurement of partial match than both neither mofn and our work 
but a price is paid in computational complexity  the theory guided constructive induction framework allows for a variety of potential tools with varying degrees of granularity of
partial match  although just one tool is used in our experiments 

    flexible structure

as discussed in section      a strength of existing induction programs is fashioning a concise
and highly predictive description of a concept when the target concept can be concisely
described with the given features  consequently  the value of a domain theory lies not in its
overall structure  if the feature language is sucient  any induction program can build a
good overall theory structure  instead  the value of a domain theory lies in the information
it contains about how to redescribe examples using high level features  these high level
features facilitate a concise description of the target concept  systems such as either and
neither mofn that reach a final theory through a series of modifications in the initial
   

firerepresenting and restructuring domain theories

theory hope to gain something by keeping the theory s overall structure intact  if the initial
theory is suciently close to an accurate theory  this method works  but often clinging to
the structure hinders full exploitation of the domain theory  theory guided constructive
induction provides a means of fully exploiting both the information in the domain theory and
the strengths of existing induction programs  figure    in section     gives a comparison of
the structure of the initial promoter theory to the structure of a revised theory produced by
theory guided constructive induction  substructures have been borrowed  but the revised
theory as a whole has been restructured 

    use of standard induction as an underlying learner

because theory guided constructive induction uses a standard induction program as its
underlying learner  it does not need to reinvent solutions to overfit avoidance  multi class
concepts  noisy data  etc  overfit avoidance has been widely studied for standard induction 
and many standard techniques exist  any system which modifies a theory to accommodate
a set of training examples must also address the issue of overfit to the training examples  in
many theory revision systems existing overfit avoidance techniques cannot be easily adapted 
and the problem must be addressed from scratch  theory guided constructive induction can
take advantage of the full range of previous work in overfit avoidance for standard induction 
when multiple theory parts are available for multi class concepts  the interpreter is
run on the multiple theory parts  and the resulting new feature sets are combined  the
primate splice junction domain presented in section     has three classes  intron exon
boundaries  exon intron boundaries  and neither  theories are given for both intron exon
and exon intron  both theories are used to create new features  and then all new features
are concatenated together for learning  interpreters such as tgci  also trivially handle
negation in a domain theory 

    use of theory fragments

theory guided constructive induction is not limited to using full domain theories  if only
part of a theory is available  this can be used  to demonstrate this  three experiments
were run in which only fragments of the promoter domain theory were used  in the first
experiment  only the four minus    rules were used  five features were constructed   one
feature for each rule and then an additional feature for the group  similar experiments were
run for the minus    group and the conformation group 
figure    gives learning curves for these three experiments along with curves for the entire theory and for no theory  c    using the original features   although the conformation
portion of the theory gives no significant improvement over c     both the minus    and
minus    portions of the theory give significant improvements in performance  thus even
partial theories and theory fragments can be used by theory guided constructive induction
to yield sizable performance improvements 
the use of theory fragments should be explored as a means of evaluating the contribution
of different parts of a theory  in figure     the conformation portion of the theory is shown
to yield no improvement  this could signal a knowledge engineer that the knowledge that
should be conveyed through that portion of the theory is not useful to the learner in its
present form 
   

fidonoho   rendell

  
c   
conformation portion of theory
minus    portion of theory
minus    portion of theory
whole theory

    
  
    
  
    
  

  error

    
  
    
  
    
  
    
  
   
 
   
 
 

 

                                            
size of training sample

figure     learning curves for theory guided constructive induction with only fragments of
the promoter domain theory  the minus    portion of the theory  the minus   
portion of the theory  and the conformation portion of the theory were used
separately in feature construction  curves are also given for the full theory and
for c    alone for comparison 

    use of multiple theories

theory guided constructive induction can use multiple competing and even incompatible
domain theories  if multiple theories exist  theory guided constructive induction provides
a natural means of integrating them in such a way as to extract the best from all theories 
tgci  would be called for each input theory producing new features  next  all the new
features are simply pooled together and the induction program selects from among them
in fashioning the final theory  this is seen on a very small scale in the promoter domain 
   

firerepresenting and restructuring domain theories

  error

in figure   some minus    rules subsume other minus    rules  according to the entry in
the uci database  this is because  the biological evidence is inconclusive with respect to
the correct specificity   this is handled by simply using all four possibilities  and selection
of the most useful knowledge is left to the induction program 
tgci could also be used to evaluate the contributions of competing theories just as it was
used to evaluate theory fragments above  a knowledge engineer could use this evaluation
to guide his own revision and synthesis of competing theories 
  
    
  
    
  
    
  
   
 
   
 

tgci using c   
tgci using lfc
    confidence of lfc

 

                                            
size of training sample

figure     theory guided constructive induction with lfc and c    as the underlying
learning system  theory guided constructive induction can use any inductive
learner as its underlying learning component  therefore  more sophisticated
underlying induction programs can further improve accuracy 

    easy adoption of new techniques

since theory guided constructive induction can use any standard induction method as its
underlying learner  as improvements are made in standard induction  theory guided constructive induction passively improves  to demonstrate this  tests were also run with lfc
 ragavan   rendell        as the underlying induction program  lfc is a decision tree
learner that performs example based constructive induction by looking ahead at combinations of features  characteristically  lfc improves accuracy for a moderate number of
examples  figure    shows the resulting learning curve along with the c    tgci curve 
both curves are the average of    separate runs with the same data partitions used for each
program  in a pairwise comparison the improvement of lfc over c    was significant at the
      level of confidence for training sets of size    and     more sophisticated underlying
induction programs can further improve accuracy 
   

fidonoho   rendell

   testing the limits of tgci
the purpose of this section is to explore the performance of theory guided constructive
induction on theory revision problems ranging from easy to dicult  in easy problems
the underlying concept embodied in the training and testing examples matches the domain
theory fairly closely  therefore  the examples themselves match the domain theory fairly
closely  in dicult problems the underlying concept embodied in the examples does not
match the domain theory very well so the examples do not either  although many other
factors determine the diculty of an individual problem  this aspect is an important component and worth exploring  our experiment in this section is intended to relate ranges of
diculty to the amount of improvement produced by tgci 
since a number of factors affect problem diculty we chose that the theory revision
problems for the experiment should all be variations of a single problem  by doing this we
are able to hold all other factors constant and vary the closeness of match to the domain
theory  because we wanted to avoid totally artificial domains  we chose to start with the
promoter domain and create  new  domains by perverting the example set 
these  new  domains were created by perverting the examples in the original promoter
problem to either more closely match the promoter domain theory or less closely match the
promoter domain theory  only the positive examples were altered  for example  one domain
was created with     fewer matches to the domain theory than the original promoter
domain as follows  each feature value in a given example was examined to see if it matched
part of the theory  if so  with a     probability  it was randomly reassigned a new value
from the set of possible values for that feature  the end result is a set of examples with    
fewer matches to the domain theory than the original example set   for our experiment
new domains such as this were created with                and     fewer matches 
for some features  multiple values may match the theory because different disjuncts
of the theory specify different values for a single feature  for example  referring back to
figure    feature p    matches two of the minus    rules if it has the value a and another
two rules if it has the value t  so a single feature might accidentally match one part of a
theory when in fact the example as a whole more closely matches another part of the theory 
for cases such as these  true matches were separated from accidental matches by examining
which part of the theory most clearly matched the example as a whole and expecting a
match from that part of the theory 
new domains that more closely matched the theory were created in a similar manner  for
example  a domain was created with     fewer mismatches to the domain theory than the
original promoter domain as follows  each feature value in a given example was examined
to see if it matched its corresponding part of the theory  if not  with a     probability 
it was reassigned a value that matched the theory  the end result is a set of examples in
which     of the mismatches with the domain theory are eliminated  for our experiment
new domains such as this were created with           and     fewer mismatches 
ten different example sets were created for each level of closeness to the domain theory 
                   fewer matches  and               fewer mismatches  in total  forty
example sets were created which matched the original theory less closely than the original
   more precisely  there would be slightly more matches than     fewer matches because some features
would be randomly reassigned back to their original matching value 

   

firerepresenting and restructuring domain theories

  
  
  
  

  error

  

c   
tgci

  
  
  
  
  
 
 
    

   

   

   

   

 

  

  

  

  

   

closeness to theory

figure     seven altered promoter domains were created  three that more closely matched
the theory than the original domains and four that less closely matched  a
    on the x axis indicates a domain in which the positive examples match the
domain theory       a negative     indicates a domain in which any match
of the positive examples to the domain theory is purely chance  the accuracy
of c    and tgci are plotted for different levels of proximity to the domain
theory 

example set  and thirty example sets were created which matched the original theory more
closely than the original example set  each of these example sets was tested using a leaveone out methodology using c    and the tgci algorithm  the results are summarized in
figure     the x axis is a measure of theory proximity   closeness of an example set to the
domain theory      on the x axis indicates no change in the original promoter examples 
      on the x axis means that each positive example exactly matches the domain theory 
       on the x axis means that any match of a feature value of a positive example to the
   

fidonoho   rendell

domain theory is totally by chance    each datapoint in figure    is the result of averaging
the accuracies of the ten example sets for each level of theory proximity  except for the
point at zero which is the accuracy of the exact original promoter examples  
one notable portion of figure    is the section between   and    on the x axis  domains
in this region have a greater than trivial level of mismatch with the domain theory but not
more than moderate mismatch  this is the region of tgci s best performance  on these
domains  tgci achieves high accuracy while a standard learner  c     using the original
feature set gives mediocre performance  a second region to examine is between     and  
on the x axis where the level of mismatch ranges from moderate to extreme  in this region
tgci s performance falls off but its improvement over the original feature set remains high
as shown in figure    which plots the improvement of tgci over c     the final two
regions to notice are greater than    and less than     on the x axis  as the level of
mismatch between theory and examples becomes trivially small  x axis greater than     
c    is able to pick out the theory s patterns leading to high accuracy that approaches that
of tgci s  as the level of mismatch becomes extreme  x axis less than      the theory gives
little help in problem solving resulting in similarly poor accuracy for both methods  in
summary  as shown in figure    for variants of the promoter problem there is a wide range
of theory proximity  centered around the real promoter problem  for which theory guided
constructive induction yields sizable improvement over standard learners 
  
    

error difference

  error

  
    
  
   
 
   
 
    

   

   

   

   

 

  

  

  

  

   

closeness to theory

figure     the difference in error between c    and tgci for different levels of proximity
of the example set to the domain theory 

   the scale   to      on the left half of the graph may not be directly comparable with the scale   to    
on the right half of the graph since there were not a equal number of matches and mismatches in the
original examples 

   

firerepresenting and restructuring domain theories

   conclusion
our goal in this paper has not been just to present another new system  but rather to
study the two qualities exible representation and exible structure  these capabilities are
intended as a frame of reference for analyzing theory guided systems  these two principles
provide guidelines for purposeful design  once we had distilled the essence of systems
such as miro  kbann  and neither mofn  theory guided constructive induction was
a natural synthesis of their strengths  our experiments have demonstrated that even a
simple application of the two principles can effectively integrate theory knowledge with
training examples  yet there is much room for improvement  the two principles could be
quantified and made more precise  and the implementations that proceed from them should
be explored and refined 
quantifying representational exibility is one step  section   gave three degrees of
exibility  one measured the exact match to a theory  one counted the number of matching
conditions  and one allowed for a weighted sum of the matching conditions  the amount of
exibility should be quantified  and finer grained degrees of exibility should be explored 
the accuracy in assorted domains should be evaluated as a function of representational
exibility 
finer grained structural exibility would be advantageous  we have presented systems
that make small  incremental modifications in a theory as lacking structural exibility  yet
theory guided constructive induction falls at the other extreme  perhaps allowing excessive
structural exibility  fortunately  existing induction tools are capable of fashioning simple
yet highly predictive theory structures when the problem features are suitably high level 
nevertheless  approaches should be explored that take advantage of the structure of the
initial theory without being unduly restricted by it 
the strength discussed in section     should be given further attention  although the
promoter domain gives a very small example of synthesizing competing theories  this should
be explored in a domain in which entire competing  inconsistent theories are available such as
synthesizing the knowledge given by multiple experts  the point was made in section    
that tgci can use theory fragments to evaluate the contribution of different parts of a
theory  this should also be explored further 
in an exploration of bias in standard induction  utgoff        refers to biases as ranging
from weak to strong and from incorrect to correct  a strong bias restricts the concepts that
can be represented more than a weak bias thus providing more guidance in learning  but as
a bias becomes stronger  it may also become incorrect by ruling out useful concept descriptions  a similar situation arises in theory revision   a theory representation language that
is inappropriately rigid may impose a strong  incorrect bias on revision  a language that
allows adaptability along too many dimensions may provide too weak a bias  a grendellike toolbox would allow a theory to be translated into a range of representations with
varying dimensions of adaptability  utgoff advocates starting with a strong  possibly incorrect bias and shifting to an appropriately weak and correct bias  similarly  a theory could
be translated into successively more adaptable representations until an appropriate bias is
found  we have implemented only a single tool  many open problems remain along this line
of research 
   

fidonoho   rendell

the converse relationship of theory revision and constructive induction warrants further
examination   theory revision uses data to improve a theory  constructive induction can
use theory to improve data to facilitate learning  since the long term goal of machine
learning is to use data  inference  and theory to improve any and all of them  we believe
that a consideration of these related methods can be beneficial  particularly because each
research area has some strengths that the other lacks 
an analysis of landmark theory revision and theory guided learning systems has led
to the two principles exible representation and exible structure  because theory guided
constructive induction was based upon these high level principles  it is simple yet achieves
good accuracy  these principles provide guidelines for future work  yet as discussed above 
the principles themselves are imprecise and call for further exploration 

acknowledgements
we would like to thank geoff towell  kevin thompson  ray mooney  and jeff mahoney for
their assistance in getting the datapoints for kbann  labyrinthk   and either  we would
also like to thank paul baffes for making the neither program available and for advice on
setting the program s parameters  we thank the anonymous reviewers for their constructive
criticism of an earlier draft of this paper  we gratefully acknowledge the support of this
work by a dod graduate fellowship and nsf grant iri          

references
baffes  p     mooney  r          symbolic revision of theories with m of n rules  in
proceedings of the      ijcai 
bloedorn  e   michalski  r     wnek  j          multistrategy constructive induction 
aq   mci  in proceeding of the second international workshop on multistrategy learning 
clark  p     matwin  s          using qualitative models to guide inductive learning  in
proceedings of the      international conference on machine learning 
cohen  w          compiling prior knowledge into an explicit bias  in proceedings of the
     international conference on machine learning 
craven  m  w     shavlik  j  w          investigating the value of a good input representation  computational learning theory and natural learning systems     forthcoming 
drastal  g     raatz  s          empirical results on learning in an abstraction space  in
proceedings of the      ijcai 
dzerisko  s     lavrac  n          learning relations from noisy examples  an empirical
comparison of linus and foil  in proceedings of the      international conference
on machine learning 
   

firerepresenting and restructuring domain theories

feldman  r   serge  a     koppel  m          incremental refinement of approximate
domain theories  in proceedings of the      international conference on machine
learning 
flann  n     dietterich  t          a study of explanation based methods for inductive
learning  machine learning             
fu  l  m     buchanan  b  g          learning intermediate concepts in constructing a
hierarchical knowledge base  in proceedings of the      ijcai 
harley  c   reynolds  r     noordewier  m          creators of original promoter dataset 
hirsh  h     noordewier  m          using background knowledge to improve inductive
learning of dna sequences  in tenth ieee conference on ai for applications san
antonio  tx 
matheus  c  j     rendell  l  a          constructive induction on decision trees  in
proceedings of the      ijcai 
michalski  r  s          a theory and methodology of inductive learning  artificial intelligence                  
mitchell  t          version spaces  a candidate elimination approach to rule learning  in
proceedings of the      ijcai 
mooney  r  j          induction over the unexplained  using overly general domain theories
to aid concept learning  machine learning                 
mooney  r  j   shavlik  j  w   towell  g  g     gove  a          an experimental comparison of symbolic and connectionist learning algorithms  in proceedings of the     
ijcai 
murphy  p     pazzani  m          id  of    constructive induction of m of n concepts for
discriminators in decision trees  in proceedings of the      international conference
on machine learning 
noordewier  m   shavlik  j     towell  g          donors of original primate splice junction
dataset 
ourston  d     mooney  r          changing the rules  a comprehensive approach to theory
refinement  in proceedings of the      national conference on artificial intelligence 
pagallo  g     haussler  d          boolean feature discovery in empirical learning  machine
learning               
pazzani  m     kibler  d          the utility of knowledge in inductive learning  machine
learning               
quinlan  j  r          c     programs for machine learning  san mateo  ca  morgan
kaufmann 
   

fidonoho   rendell

ragavan  h     rendell  l          lookahead feature construction for learning hard concepts  in proceedings of the      international conference on machine learning 
rumelhart  d  e   hinton  g  e     mcclelland  j  l          a general framework for
parallel distributed processing  in rumelhart  d  e     mcclelland  j  l   eds   
parallel distributed processing  explorations in the microarchitecture of cognition 
volume i  cambridge  ma  mit press 
schlimmer  j  c          learning and representation change  in kaufmann  m   ed   
proceedings of the      national conference on artificial intelligence 
thompson  k   langley  p     iba  w          using background knowledge in concept
formation  in proceedings of the      international conference on machine learning 
towell  g     shavlik  j          knowledge based artificial neural networks  artificial
intelligence              
towell  g   shavlik  j     noordeweir  m          refinement of approximately correct
domain theories by knowledge based neural networks  in proceedings of the     
national conference on artificial intelligence 
utgoff  p  e          shift of bias for inductive concept learning  in michalski  carbonell 
  mitchell  eds    machine learning  vol     chap     pp           san mateo  ca 
morgan kaufmann 
wogulis  j          revising relational domain theories  in proceedings of the      international conference on machine learning 

   

fi