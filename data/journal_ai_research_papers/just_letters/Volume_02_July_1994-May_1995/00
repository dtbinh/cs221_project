journal artificial intelligence research              

submitted       published     

system induction oblique decision trees
sreerama k  murthy
simon kasif
steven salzberg

department computer science
johns hopkins university  baltimore  md       usa

murthy cs jhu edu
kasif cs jhu edu
salzberg cs jhu edu

abstract

article describes new system induction oblique decision trees  system 
oc   combines deterministic hill climbing two forms randomization find good
oblique split  in form hyperplane  node decision tree  oblique decision
tree methods tuned especially domains attributes numeric  although
adapted symbolic mixed symbolic numeric attributes  present extensive empirical studies  using real artificial data  analyze oc  s ability
construct oblique trees smaller accurate axis parallel counterparts  examine benefits randomization construction oblique
decision trees 

   introduction
current data collection technology provides unique challenge opportunity automated machine learning techniques  advent major scientific projects
human genome project  hubble space telescope  human brain mapping initiative generating enormous amounts data daily basis  streams data
require automated methods analyze  filter  classify presenting
digested form domain scientist  decision trees particularly useful tool context perform classification sequence simple  easy to understand tests
whose semantics intuitively clear domain experts  decision trees used
classification tasks since     s  moret        safavin   landgrebe        
     s  breiman et al  s book classification regression trees  cart  quinlan s work id   quinlan              provided foundations become
large body research one central techniques experimental machine learning 
many variants decision tree  dt  algorithms introduced last decade 
much work concentrated decision trees node checks value
single attribute  breiman  friedman  olshen    stone        quinlan            a  
quinlan initially proposed decision trees classification domains symbolic valued
attributes         later extended numeric domains         attributes
numeric  tests form xi   k  xi one attributes example
k constant  class decision trees may called axis parallel  tests
node equivalent axis parallel hyperplanes attribute space  example
decision tree given figure    shows tree partitioning
creates   d attribute space 

c      ai access foundation morgan kaufmann publishers  rights reserved 

fifigure    left side figure shows simple axis parallel tree uses two attributes 
right side shows partitioning tree creates attribute space 
researchers studied decision trees test node uses boolean
combinations attributes  pagallo        pagallo   haussler        sahami       
linear combinations attributes  see section     different methods measuring
goodness decision tree nodes  well techniques pruning tree reduce overfitting
increase accuracy explored  discussed later sections 
paper  examine decision trees test linear combination attributes
internal node  precisely  let example take form x   x    x          xd  cj
cj class label xi  s real valued attributes   test node
form 

x
ai xi   ad      
   
i  

a           ad   real valued coecients  tests equivalent hyperplanes oblique orientation axes  call class decision trees oblique
decision trees   trees form called  multivariate   brodley   utgoff 
       prefer term  oblique   multivariate  includes non linear combinations variables  i e   curved surfaces  trees contain linear tests   clear
simply general form axis parallel trees  since setting ai    
coecients one  test eq    becomes familiar univariate test  note
oblique decision trees produce polygonal  polyhedral  partitionings attribute
space  axis parallel trees produce partitionings form hyper rectangles
parallel feature axes 
intuitively clear underlying concept defined polygonal space partitioning  preferable use oblique decision trees classification 
example  exist many domains one two oblique hyperplanes
best model use classification  domains  axis parallel methods ap   constraint x            xd real valued necessarily restrict oblique decision trees
numeric domains  several researchers studied problem converting symbolic  unordered 
domains numeric  ordered  domains vice versa  e g    breiman et al         hampson   volper 
      utgoff   brodley        van de merckt               keep discussion simple  however 
assume attributes numeric values 

 

fifigure    left side shows simple   d domain two oblique hyperplanes define
classes  right side shows approximation sort axis parallel
decision tree would create model domain 
proximate correct model staircase like structure  oblique tree building
method could capture tree smaller accurate   figure  
gives illustration 
breiman et al  first suggested method inducing oblique decision trees       however  little research trees relatively recently  utgoff
  brodley        heath  kasif    salzberg      b  murthy  kasif  salzberg    beigel       
brodley   utgoff         comparison existing approaches given detail
section    purpose study review strengths weaknesses existing
methods  design system combines strengths overcomes weaknesses  evaluate system empirically analytically  main contributions
conclusions study follows 

developed new  randomized algorithm inducing oblique decision trees

examples  algorithm extends original      work breiman et al 
randomization helps significantly learning many concepts 

algorithm fully implemented oblique decision tree induction system
available internet  code retrieved online appendix  
paper  or anonymous ftp ftp   ftp cs jhu edu pub oc  oc  tar z  

randomized hill climbing algorithm used oc  ecient

existing randomized oblique decision tree methods  described below   fact 
current implementation oc  guarantees worst case running time
o log n  times greater worst case time inducing axis parallel trees  i e  
o dn  log n  vs  o dn    

ability generate oblique trees often produces small trees compared
axis parallel methods  underlying problem requires oblique split  oblique

   note though given oblique tree may fewer leaf nodes axis parallel tree which
mean  smaller  the oblique tree may cases larger terms information content 
increased complexity tests node 

 

fimurthy  kasif   salzberg

trees accurate axis parallel trees  allowing tree building system
use oblique axis parallel splits broadens range domains
system useful 
remaining sections paper follow outline  remainder section
brie outlines general paradigm decision tree induction  discusses complexity issues involved inducing oblique decision trees  section   brie reviews
existing techniques oblique dt induction  outlines limitations approach 
introduces oc  system  section   describes oc  system detail  section  
describes experiments     compare performance oc  several
axis parallel oblique decision tree induction methods range real world datasets
    demonstrate empirically oc  significantly benefits randomization
methods  section    conclude discussion open problems directions
research 

    top down induction decision trees

algorithms inducing decision trees follow approach described quinlan top down
induction decision trees         called greedy divide and conquer
method  basic outline follows 
   begin set examples called training set    examples belong
one class  halt 
   consider tests divide two subsets  score test according
well splits examples 
   choose   greedily   test scores highest 
   divide examples subsets run procedure recursively subset 
quinlan s original model considered attributes symbolic values  model 
test node splits attribute values  thus test attribute
three values three child nodes  one corresponding value 
algorithm considers possible tests chooses one optimizes pre defined
goodness measure   one could split symbolic values two subsets values 
gives many choices split examples   explain next  oblique
decision tree methods cannot consider tests due complexity considerations 

    complexity induction oblique decision trees

one reason relatively papers problem inducing oblique decision trees
increased computational complexity problem compared axis parallel
case  two important issues must addressed  context top down
decision tree algorithms  must address complexity finding optimal separating
hyperplanes  decision surfaces  given node decision tree  optimal hyperplane
minimize impurity measure used  e g   impurity might measured total
number examples mis classified  second issue lower bound complexity
finding optimal  e g   smallest size  trees 
 

fifigure    n points dimensions
 n d   n distinct axis parallel splits 
 
 d nd distinct d dimensional oblique splits  shows distinct
oblique axis parallel splits two specific points   d 
let us first consider issue complexity selecting optimal oblique hyperplane single node tree  domain
n training instances  described using
 
real valued attributes   d nd distinct d dimensional oblique splits  i e  
hyperplanes  divide training instances uniquely two nonoverlapping subsets 
upper bound derives observation every subset size n points
define d dimensional hyperplane  hyperplane rotated slightly
 d directions divide set points possible ways  figure   illustrates
upper limits two points two dimensions  axis parallel splits  n
distinct possibilities  axis parallel methods c     quinlan      a  cart
 breiman et al         exhaustively search best split node  problem
searching best oblique split therefore much dicult searching
best axis parallel split  fact  problem np hard 
precisely  heath        proved following problem np hard  given
set labelled examples  find hyperplane minimizes number misclassified
examples hyperplane  result implies method
finding optimal oblique split likely exponential cost
 assuming p    np   
 
intuitively  problem impractical enumerate  d nd distinct hyperplanes
choose best  done axis parallel decision trees  however  non exhaustive
deterministic algorithm searching hyperplanes prone getting stuck
local minima 
   throughout paper  use terms  split   hyperplane  interchangeably refer test
node decision tree  first usage standard  moret         refers fact
test splits data two partitions  second usage refers geometric form test 

 

fimurthy  kasif   salzberg

hand  possible define impurity measures problem
finding optimal hyperplanes solved polynomial time  example  one
minimizes sum distances mis classified examples  optimal solution
found using linear programming methods  if distance measured along one dimension
only   however  classifiers usually judged many points classify correctly 
regardless close decision boundary point may lie  thus standard
measures computing impurity base calculation discrete number examples
category either side hyperplane  section     discusses several commonly
used impurity measures 
let us address second issue  complexity building small tree 
easy show problem inducing smallest axis parallel decision tree
np hard  observation follows directly work hyafil rivest         note
one generate smallest axis parallel tree consistent training
set polynomial time number attributes constant  done
using dynamic programming branch bound techniques  see moret        several
pointers   tree uses oblique splits  clear  even fixed number
attributes  generate optimal  e g   smallest  decision tree polynomial time 
suggests complexity constructing good oblique trees greater
axis parallel trees 
easy see problem constructing optimal  e g   smallest  oblique
decision tree np hard  conclusion follows work blum rivest        
result implies dimensions  i e   attributes  problem producing
  node oblique decision tree consistent training set np complete 
specifically  show following decision problem np complete  given training
set n examples boolean attributes  exist   node neural network
consistent   easy show following question np complete 
given training set   exist   leaf node oblique decision tree consistent
t 
result complexity considerations  took pragmatic approach trying
generate small trees  looking smallest tree  greedy approach used
oc  virtually decision tree algorithms implicitly tries generate small trees 
addition  easy construct example problems optimal split node
lead best tree  thus philosophy embodied oc  find locally
good splits  spend excessive computational effort improving quality
splits 

   previous work oblique decision tree induction
describing oc  algorithm  brie discuss existing oblique dt
induction methods  including cart linear combinations  linear machine decision
trees  simulated annealing decision trees  methods induce
tree like classifiers linear discriminants node  notably methods using
linear programming  mangasarian  setiono    wolberg        bennett   mangasarian 
          a      b   though methods find optimal linear discriminants
specific goodness measures  size linear program grows fast number
 

fiinduction oblique decision trees

induce split node decision tree 
normalize values attributes 
l  
 true 
l   l  

let current split sl v c  v   pdi   ai xi 
           
              
search maximizes goodness split v    ai     c 
let   settings result highest goodness   searches 
ai   ai     c   c    
perturb c maximize goodness sl   keeping a           ad constant 
jgoodness sl    goodness sl   j exit loop 
eliminate irrelevant attributes fa          ad g using backward elimination 
convert sl split un normalized attributes 
return better sl best axis parallel split split  
figure    procedure used cart linear combinations  cart lc  node
decision tree 
instances number attributes  less closely related work
algorithms train artificial neural networks build decision tree like classifiers  brent 
      cios   liu        herman   yeung        
first oblique decision tree algorithm proposed cart linear combinations  breiman et al         chapter     algorithm  referred henceforth cart lc 
important basis oc   figure   summarizes  using breiman et al  s notation 
cart lc algorithm node decision tree  core idea cartlc algorithm finds value maximizes goodness split 
idea used oc   explained detail section     
describing cart lc  breiman et al  point still much room
development algorithm  oc  represents extension cart lc
includes significant additions  addresses following limitations cart lc 

cart lc fully deterministic  built in mechanism escaping local

minima  although minima may common domains  figure  
shows simple example cart lc gets stuck 

cart lc produces single tree given data set 
cart lc sometimes makes adjustments increase impurity split 
feature probably included allow escape local minima 

upper bound time spent node decision tree  halts
perturbation changes impurity   impurity may
increase decrease  algorithm spend arbitrarily long time node 
 

fimurthy  kasif   salzberg

 

oc 

 

 

initial loc 

 

 

cart lc

 

 

 

figure    deterministic perturbation algorithm cart lc fails find correct
split data  even starts location best axis parallel
split  oc  finds correct split using one random jump 
another oblique decision tree algorithm  one uses different approach
cart lc  linear machine decision trees  lmdt  system  utgoff   brodley       
brodley   utgoff         successor perceptron tree method  utgoff       
utgoff   brodley         internal node lmdt tree linear machine  nilsson 
       training algorithm presents examples repeatedly node linear
machine converges  convergence cannot guaranteed  lmdt uses heuristics
determine node stabilized  make training stable even set
training instances linearly separable   thermal training  method  frean       
used  similar simulated annealing 
third system creates oblique trees simulated annealing decision trees
 sadt   heath et al       b  which  oc   uses randomization  sadt uses simulated
annealing  kirkpatrick  gelatt    vecci        find good values coecients
hyperplane node tree  sadt first places hyperplane canonical
location  iteratively perturbs coecients small random amounts  initially  temperature parameter high  sadt accepts almost perturbation
hyperplane  regardless changes goodness score  however  system
 cools down   changes improve goodness split likely accepted 
though sadt s use randomization allows effectively avoid local minima 
compromises eciency  runs much slower either cart lc  lmdt oc  
sometimes considering tens thousands hyperplanes single node finishes
annealing 
experiments section     include results showing methods
perform three artificial domains 
next describe way combine strengths methods mentioned 
avoiding problems  algorithm  oc   uses deterministic hill climbing
time  ensuring computational eciency  addition  uses two kinds
randomization avoid local minima  limiting number random choices 
algorithm guaranteed spend polynomial time node tree  addition 
randomization produced several benefits  example  means algorithm
 

fiinduction oblique decision trees

find split set examples  
find best axis parallel split   let impurity split 
repeat r times 
choose random hyperplane h  
 for first iteration  initialize h best axis parallel split  
step    impurity measure improve  do 
perturb coecients h sequence 
step    repeat j times 
choose random direction attempt perturb h direction 
reduces impurity h   go step   
let i    impurity h   i      set   i   
output split corresponding  
figure    overview oc  algorithm single node decision tree 
produce many different trees data set  offers possibility new
family classifiers  k decision tree algorithms  example classified
majority vote k trees  heath et al       a  shown k decision tree methods
 which call k dt  consistently outperform single tree methods classification
accuracy main criterion  finally  experiments indicate oc  eciently finds
small  accurate decision trees many different types classification problems 

   oblique classifier    oc  

section discuss details oblique decision tree induction system oc  
part description  include 
method finding coecients hyperplane tree node 
methods computing impurity goodness hyperplane 
tree pruning strategy 
methods coping missing irrelevant attributes 
section     focuses complicated algorithmic details  i e  question
find hyperplane splits given set instances two reasonably  pure  nonoverlapping subsets  randomized perturbation algorithm main novel contribution
oc   figure   summarizes basic oc  algorithm  used node decision
tree  figure explained following sections 

    perturbation algorithm

oc  imposes restrictions orientation hyperplanes  however  order
least powerful standard dt methods  first finds best axis parallel  univariate 
split node looking oblique split  oc  uses oblique split
improves best axis parallel split  
   pointed  breiman et al         chapter     make sense use oblique split
number examples node n less almost equal number features d 

 

fimurthy  kasif   salzberg

search strategy space possible hyperplanes defined procedure
perturbs current hyperplane h new location  exponential
number distinct ways partition examples hyperplane  procedure
simply enumerates unreasonably costly  two main alternatives
considered past simulated annealing  used sadt system  heath
et al       b   deterministic heuristic search  cart lc  breiman et al         
oc  combines two ideas  using heuristic search finds local minimum 
using non deterministic search step get local minimum   the nondeterministic step oc  simulated annealing  however  
start explaining perturb hyperplane split training set
node decision tree  let n number examples   number
attributes  or dimensions  example  k number categories 
write tj    xj     xj           xjd  cj   j th example training set   xji
value attribute cj category label  defined p
eq     equation
current hyperplane h node decision tree written di  
 a x   ad       
pd
substitute point  an example  tj equation h   get i    aixji    ad     vj  
sign vj tells us whether point tj hyperplane h  
i e   vj      tj h   h splits training set perfectly  points
belonging category sign vj   i e   sign vi    sign vj   iff
category ti    category tj   
oc  adjusts coecients h individually  finding locally optimal value one
coecient time  key idea introduced breiman et al  works follows 
treat coecient variable  treat coecients constants 
vj viewed function   particular  condition tj h
equivalent
vj    
  uj
  xxjm   vj def
jm

   

assuming xjm      ensure normalization  using definition uj  
point tj h   uj   otherwise  plugging points
equation  obtain n constraints value  
problem find value satisfies many constraints
possible   if constraints satisfied  perfect split   problem
easy solve optimally  simply sort values uj   consider setting
midpoint pair different values  illustrated figure    figure 
categories indicated font size  larger ui  s belong one category 
smaller another  distinct placement coecient   oc  computes
impurity resulting split  e g   location u  u  illustrated here  two
examples left one example right would misclassified  see section      
different ways computing impurity   figure illustrates  problem simply
find best one dimensional split u s  requires considering n     values
  value a m obtained solving one dimensional problem considered
data underfits concept  default  oc  uses axis parallel splits tree nodes n    d 
user vary threshold 

  

fifigure    finding optimal value single coecient   large u s correspond
examples one category small u s another 

perturb h m 
j             n
compute uj  eq    
sort u          un non decreasing order 
a m   best univariate split sorted uj s 
h    result substituting a m h  
 impurity h      impurity h  
f   a m   pmove   pstag g
else  impurity h    impurity h    
f   a m probability pmove
pmove   pmove       pstag g
figure    perturbation algorithm single coecient  

replacement   let h  hyperplane obtained  perturbing  a m  
h better  lower  impurity h   h  discarded  h  lower impurity  h 
becomes new location hyperplane  h h  identical impurities 
h  replaces h probability pstag    figure   contains pseudocode perturbation
procedure 
method locally improving coecient hyperplane  need
decide     coecients pick perturbation  experimented
three different methods choosing coecient adjust  namely  sequential  best
first random 
seq  repeat none coecient values modified loop 
    d  perturb h  i 
best  repeat coecient remains unmodified 
  coecient perturbed  results
maximum improvement impurity measure 
perturb h  m 
r     repeat fixed number times     experiments  
  random integer      
perturb h  m 
   parameter pstag   denoting  stagnation probability   probability hyperplane perturbed
location change impurity measure  prevent impurity remaining
stagnant long time  pstag decreases constant amount time oc  makes  stagnant 
perturbation  thus constant number perturbations occur node  constant
set user  pstag reset   every time global impurity measure improved 

  

fimurthy  kasif   salzberg

previous experiments  murthy et al         indicated order perturbation
coecients affect classification accuracy much parameters 
especially randomization parameters  see below   since none orders uniformly better other  used sequential  seq  perturbation experiments
reported section   

    randomization

perturbation algorithm halts split reaches local minimum impurity
measure  oc  s search space  local minimum occurs perturbation
single coecient current hyperplane decrease impurity measure   of course 
local minimum may global minimum   implemented two ways
attempting escape local minima  perturbing hyperplane random vector 
re starting perturbation algorithm different random initial hyperplane 
technique perturbing hyperplane random vector works follows 
system reaches local minimum  chooses random vector add
coecients current hyperplane  computes optimal amount
hyperplane bep perturbed along random direction  precise 
hyperplane h   di   ai xi   ad   cannot improved deterministic perturbation 
oc  repeats following loop j times  where j user specified parameter  set  
default  

choose random vector r    r   r          rd    
let amount
want perturb h direction r 
pd
words  let h    i    ai   ffri  xi    ad     ffrd     
find optimal value ff 
hyperplane h  thus obtained decreases overall impurity  replace h h  
exit loop begin deterministic perturbation algorithm individual
coecients 

note treat variable equation h    therefore
n examples   plugged equation h   imposes constraint value
ff  oc  therefore use coecient perturbation method  see section      compute
best value ff  j random jumps fail improve impurity  oc  halts uses
h split current tree node 
intuitive way understanding random jump look atpthe dual space
algorithm actually searching  note equation h   di   ai xi   ad   defines
space axes coecients ai rather attributes xi   every point
space defines distinct hyperplane original formulation  deterministic
algorithm used oc  picks hyperplane adjusts coecients one time  thus
dual space  oc  chooses point perturbs moving parallel axes 
random vector r represents random direction space  finding best value
ff  oc  finds best distance adjust hyperplane direction r 
  

fiinduction oblique decision trees

note additional perturbation random direction significantly increase time complexity algorithm  see appendix a   found experiments
even single random jump  used local minimum  proves helpful 
classification accuracy improved every one data sets perturbations
made  see section     examples 
second technique avoiding local minima variation idea performing
multiple local searches  technique multiple local searches natural extension
local search  widely mentioned optimization literature  see roth
       early example   steps perturbation algorithm
deterministic  initial hyperplane largely determines local minimum
encountered first  perturbing single initial hyperplane thus unlikely lead best
split given data set  cases random perturbation method fails escape
local minima  may helpful simply start afresh new initial hyperplane 
use word restart denote one run perturbation algorithms  one node
decision tree  using one random initial hyperplane   is  restart cycles
perturbs coecients one time tries perturb hyperplane
random direction algorithm reaches local minimum  last perturbation
reduces impurity  algorithm goes back perturbing coecients one time 
restart ends neither deterministic local search random jump find
better split  one optional parameters oc  specifies many restarts use 
one restart used  best hyperplane found thus far always saved 
experiments  classification accuracies increased one restart 
accuracy tended increase point level  after       restarts 
depending domain   overall  use multiple initial hyperplanes substantially
improved quality decision trees found  see section     examples  
carefully combining hill climbing randomization  oc  ensures worst case time
o dn  log n  inducing decision tree  see appendix derivation upper
bound 

best axis parallel split  clear axis parallel splits suitable

data distributions oblique splits  take account distributions  oc  computes best axis parallel split oblique split node  picks better
two   calculating best axis parallel split takes additional o dn log n  time 
increase asymptotic time complexity oc   simple variant
oc  system  user opt  switch off  oblique perturbations  thus building
axis parallel tree training data  section     empirically demonstrates
axis parallel variant oc  compares favorably existing axis parallel algorithms 
   first run algorithm node always begins location best axis parallel
hyperplane  subsequent restarts begin random locations 
   sometimes simple axis parallel split preferable oblique split  even oblique split slightly
lower impurity  user specify bias input parameter oc  

  

fimurthy  kasif   salzberg

    details
      impurity measures

oc  attempts divide d dimensional attribute space homogeneous regions  i e  
regions contain examples one category  goal adding new nodes
tree split sample space minimize  impurity  training
set  algorithms measure  goodness  instead impurity  difference
goodness values maximized impurity minimized  many different
measures impurity studied  breiman et al         quinlan        mingers 
    b  buntine   niblett        fayyad   irani        heath et al       b  
oc  system designed work large class impurity measures  stated
simply  impurity measure uses counts examples belonging every category
sides split  oc  use it   see murthy salzberg        ways
mapping kinds impurity measures class impurity measures   user
plug impurity measure fits description  oc  implementation includes
six impurity measures  namely 
  
  
  
  
  
  

information gain
gini index
twoing rule
max minority
sum minority
sum variances

though six measures defined elsewhere literature 
cases made slight modifications defined precisely appendix b 
experiments indicated that  average  information gain  gini index twoing rule
perform better three measures axis parallel oblique trees 
twoing rule current default impurity measure oc   used
experiments reported section    are  however  artificial data sets
sum minority and or max minority perform much better rest measures 
instance  sum minority easily induces exact tree pol data set described
section        methods diculty finding best tree 

twoing rule  twoing rule first proposed breiman et al          value

computed defined as 

k
x

twoingvalue    jtlj n   jtrj n   

i  

jli jtlj   ri jtrjj  

jtlj  jtrj  number examples left  right  split node   n
number examples node   li  ri   number examples category
left  right  split  twoingvalue actually goodness measure rather
impurity measure  therefore oc  attempts minimize reciprocal value 
remaining five impurity measures implemented oc  defined appendix b 
  

fiinduction oblique decision trees

      pruning

virtually decision tree induction systems prune trees create order avoid
overfitting data  many studies found judicious pruning results smaller
accurate classifiers  decision trees well types machine learning
systems  quinlan        niblett        cestnik  kononenko    bratko        kodratoff
  manago        cohen        hassibi   stork        wolpert        schaffer        
oc  system implemented existing pruning method  note tree
pruning method work fine within oc   based experimental evaluations
mingers      a  work cited above  chose breiman et al  s cost complexity
 cc  pruning        default pruning method oc   method 
called error complexity weakest link pruning  requires separate pruning set 
pruning set randomly chosen subset training set  approximated
using cross validation  oc  randomly chooses      the default value  training data
use pruning  experiments reported below  used default value 
brie y  idea behind cc pruning create set trees decreasing size
original  complete tree  trees used classify pruning set  accuracy
estimated that  cc pruning chooses smallest tree whose accuracy within k
standard errors squared best accuracy obtained    se rule  k      used 
tree highest accuracy pruning set selected  k      smaller tree size
preferred higher accuracy  details cost complexity pruning  see breiman et
al         mingers      a  
      irrelevant attributes

irrelevant attributes pose significant problem machine learning methods  breiman
et al         aha        almuallin   dietterich        kira   rendell        salzberg       
cardie        schlimmer        langley   sage        brodley   utgoff         decision
tree algorithms  even axis parallel ones  confused many irrelevant attributes 
oblique decision trees learn coecients attribute dt node  one
might hope values chosen coecient would ect relative importance
corresponding attributes  clearly  though  process searching good coecient
values much ecient fewer attributes  search space much
smaller  reason  oblique dt induction methods benefit substantially using
feature selection method  an algorithm selects subset original attribute set 
conjunction coecient learning algorithm  breiman et al         brodley   utgoff 
      
currently  oc  built in mechanism select relevant attributes  however  easy include several standard methods  e g   stepwise forward selection
stepwise backward selection  even ad hoc method select features running
tree building process  example  separate experiments data hubble
space telescope  salzberg  chandar  ford  murthy    white         used feature selection methods preprocessing step oc   reduced number attributes   
   resulting decision trees simpler accurate  work currently
underway incorporate ecient feature selection technique oc  system 
  

fimurthy  kasif   salzberg

regarding missing values  example missing value attribute  oc  uses
mean value attribute  one course use techniques handling
missing values  considered study 

   experiments

section  present two sets experiments support following two claims 
   oc  compares favorably variety real world domains several existing
axis parallel oblique decision tree induction methods 
   randomization  form multiple local searches random jumps  improves quality decision trees produced oc  
experimental method used experiments described section      sections         describe experiments corresponding two claims  experimental section begins description data sets  presents experimental
results discussion 

    experimental method

used five fold cross validation  cv  experiments estimate classification
accuracy  k fold cv experiment consists following steps 
   randomly divide data k equal sized disjoint partitions 
   partition  build decision tree using data outside partition  test
tree data partition 
   sum number correct classifications k trees divide total number
instances compute classification accuracy  report accuracy
average size k trees 
entry tables     result ten   fold cv experiments  i e   result tests
used    decision trees  ten   fold cross validations used different random
partitioning data  entry tables reports mean standard deviation
classification accuracy  followed mean standard deviation decision
tree size  measured number leaf nodes   good results high values
accuracy  low values tree size  small standard deviations 
addition oc   included experiments axis parallel version oc  
considers axis parallel hyperplanes  call version  described section     
oc  ap  experiments  oc  oc  ap used twoing rule  section
       measure impurity  parameters oc  took default values unless stated
otherwise   defaults include following  number restarts node      number
random jumps attempted local minimum     order coecient perturbation 
sequential  pruning method  cost complexity   se rule  using     training
set exclusively pruning  
comparison  used oblique version cart algorithm  cart lc 
implemented version cart lc  following description breiman et
al         chapter     however  may differences version
  

fiinduction oblique decision trees

versions system  note cart lc freely available   implementation
cart lc measured impurity twoing rule used   se cost complexity
pruning separate test set  oc  does  include feature selection
methods cart lc oc   implement normalization 
cart coecient perturbation algorithm may alternate indefinitely two locations
hyperplane  see section     imposed arbitrary limit     perturbations
forcing perturbation algorithm halt 
included axis parallel cart c    comparisons  used implementations algorithms ind     package  buntine         default
cart  c     styles  defined package used  without altering parameter
settings  cart  style uses twoing rule   se cost complexity pruning
   fold cross validation  pruning method  impurity measure defaults
c    style described quinlan      a  

    oc  vs  decision tree induction methods

table   compares performance oc  three well known decision tree induction
methods plus oc  ap six different real world data sets  next section
consider artificial data  concept definition precisely characterized 
      description data sets

star galaxy discrimination  two data sets came large set astronom 

ical images collected odewahn et al   odewahn  stockwell  pennington  humphreys   
zumach         study  used images train artificial neural networks
running perceptron back propagation algorithms  goal classify example either  star   galaxy   image characterized    real valued attributes 
attributes measurements defined astronomers likely relevant
task  objects image divided odewahn et al   bright   dim 
data sets based image intensity values  dim images inherently
dicult classify   note  bright  objects bright relation others
data set  actuality extremely faint  visible powerful
telescopes   bright set contains      objects dim set contains      objects 
addition results reported table    following results appeared
star galaxy data  odewahn et al         reported accuracy       accuracy
bright objects        dim ones  although noted study
used single training test set partition  heath        reported       accuracy
bright objects using sadt  average tree size      leaves  study used
single training test set  salzberg        reported accuracies       bright
objects        dim objects  using   nearest neighbor    nn  coupled
feature selection method reduces number features 
breast cancer diagnosis  mangasarian bennett compiled data problem diagnosing breast cancer test several new classification methods  mangasarian
et al         bennett   mangasarian            a   data represents set patients
breast cancer  patient characterized nine numeric attributes plus
diagnosis tumor benign malignant  data set currently     entries
  

fimurthy  kasif   salzberg

bright s g
       
      
cart lc
       
      
oc  ap
       
      
cart ap
       
       
c   
       
       
algorithm
oc 

dim s g
       
       
       
       
       
       
       
      
       
       

cancer
       
      
       
      
       
      
       
       
       
      

iris
       
      
       
      
       
      
       
      
       
      

housing
       
      
       
      
       
      
       
      
       
       

diabetes
       
      
       
      
       
       
       
       
       
       

table    comparison oc  decision tree induction methods six different
data sets  first line method gives accuracies  second line gives
average tree sizes  highest accuracy domain appears boldface 
available uc irvine machine learning repository  murphy   aha        
heath et al       b  reported       accuracy subset data set  it
    instances   average decision tree size     nodes  using sadt  salzberg
       reported       accuracy using   nn  smaller  data set  herman
yeung        reported       accuracy using piece wise linear classification  using
somewhat smaller data set 

classifying irises  fisher s famous iris data  extensively studied

statistics machine learning literature  data consists     examples 
example described four numeric attributes     examples
three different types iris ower  weiss kapouleas        obtained accuracies      
      data back propagation   nn  respectively 

housing costs boston  data set  available part uci ml repos 

itory  describes housing values suburbs boston function    continuous
attributes   binary attribute  harrison   rubinfeld         category variable  median value owner occupied homes  actually continuous  discretized
category     value             otherwise  uses data  see  belsley 
      quinlan      b  

diabetes diagnosis  data catalogs presence absence diabetes among pima

indian females     years older  function eight numeric valued attributes 
original source data national institute diabetes digestive kidney
diseases  available uci repository  smith et al         reported    
accuracy data using adap learning algorithm  using different experimental
method used here 
  

fiinduction oblique decision trees

      discussion

table shows that  six data sets considered here  oc  consistently finds better
trees original oblique cart method  accuracy greater six domains 
although difference significant  more   standard deviations  dim
star galaxy problem  average tree sizes roughly equal five six domains 
dim stars galaxies  oc  found considerably smaller trees  differences
analyzed quantified using artificial data  following section 
five decision tree induction methods  oc  highest accuracy four
six domains  bright stars  dim stars  cancer diagnosis  diabetes diagnosis 
remaining two domains  oc  second highest accuracy case  surprisingly 
oblique methods  oc  cart lc  generally find much smaller trees axisparallel methods  difference quite striking domains note  example 
oc  produced tree    nodes average dim star galaxy problem 
c    produced tree    nodes    times larger  course  domains
axis parallel tree appropriate representation  axis parallel methods compare
well oblique methods terms tree size  fact  iris data  methods
found similar sized trees 

    randomization helps oc 

second set experiments  examine closely effect introducing randomized steps algorithm finding oblique splits  experiments demonstrate
oc  s ability produce accurate tree set training data clearly enhanced
two kinds randomization uses  precisely  use three artificial data sets
 for underlying concept known experimenters  show oc  s performance improves substantially deterministic hill climbing augmented
three ways 

multiple restarts random initial locations 
perturbations random directions local minima 
randomization steps 
order find clear differences algorithms  one needs know concept
underlying data indeed dicult learn  simple concepts  say  two linearly
separable classes   d   many different learning algorithms produce accurate
classifiers  therefore advantages randomization may detectable 
known many commonly used data sets uci repository easy
learn simple representations  holte         therefore data sets may
ideal purposes  thus created number artificial data sets present different
problems learning  know  correct  concept definition  allows
us quantify precisely parameters algorithm affect performance 
second purpose experiment compare oc  s search strategy
two existing oblique decision tree induction systems   lmdt  brodley   utgoff       
sadt  heath et al       b   show quality trees induced oc 
good as  better than  trees induced existing systems three
  

fimurthy  kasif   salzberg

artificial domains  show oc  achieves good balance amount
effort expended search quality tree induced 
lmdt sadt used information gain experiment  however 
change oc  s default measure  the twoing rule  observed  experiments
reported here  oc  information gain produce significantly different
results  maximum number successive  unproductive perturbations allowed
node set       sadt  parameters  used default settings provided
systems 
      description artificial data

ls   ls   data set      instances divided two categories  instance

described ten attributes x          x    whose values uniformly distributed range
       data linearly separable    d hyperplane  thus name ls    defined
equation x    x    x    x    x    x    x    x    x    x    instances
generated randomly labelled according side hyperplane fell on 
oblique dt induction methods intuitively prefer linear separator one
exists  interesting compare various search techniques data set
know separator exists  task relatively simple lower dimensions  chose
   dimensional data make dicult 

pol data set shown figure         instances two dimensions 

divided two categories  underlying concept set four parallel oblique lines
 thus name pol   dividing instances five homogeneous regions  concept
dicult learn single linear separator  minimal size tree still quite
small 

rcb rcb stands  rotated checker board   data set subject

experiments hard classification problems decision trees  murthy   salzberg 
       data set  shown figure         instances   d  belonging one
eight categories  concept dicult learn axis parallel method  obvious
reasons  quite dicult oblique methods  several reasons  biggest
problem  correct  root node  shown figure  separate
class itself  impurity measures  such sum minority  fail miserably
problem  although others  e g   twoing rule  work much better  another problem
deterministic coecient perturbation algorithm get stuck local minima
many places data set 
table   summarizes results experiment three smaller tables  one
data set  smaller table  compare four variants oc  lmdt sadt 
different results oc  obtained varying number restarts
number random jumps  random jumps used  twenty random jumps
tried local minimum  soon one found improved impurity
current hyperplane  algorithm moved hyperplane started running
deterministic perturbation procedure again  none    random jumps improved
impurity  search halted restarts  if any  tried  training
test partitions used methods cross validation run  recall results
  

fiinduction oblique decision trees

lr 

ot

rr 

  

l  

 

r  

rl 

 
rr 

r  

t  
oo
r

 
 
 
 
  
 
  
   
  
 
  
  
 
     
         
    
 
     
     
    
 
 
 
  
      
    
 
 
 
 
      
    
            
        
 
 
            
 
 
       
 
  
 
   
 
 
 
   
       
       
         
  
   
   
 
        
 
 
 
   
     
 
      
 
 
       
  
  
  
 
         
 
   
     
   
       
  
   
         
 
 
 
 
 
 
 
 
                 
 
 
 
 
 
     
 
 
 
 
           
  
        
      
 
  
 
                    
        
    
 
 
   
      
 
   
   
      
       
 
 
 
 
    
  
   
    
 
 
 
 
 
 
    
         
       
            
 
    
 
 
  
   
            
  
 
 
 
  
 
     
 
 
 
     
 
     
 
        
 
    
 
 
    
 
 
 
 
 
 
 
 
 
 
  
  
           
                          
 
          
 
 
    
ll  
 
 
    
 
      
 
   
                 
 
         
  
 
     
 
   
       
 
 
 
 
      
 
   
  
      
 
 
 
      
 
  
 
 
    
 
 
        
 
   
   
 
        
 
 
  
    
        
  
          
 
  
 
      
 
   
    
   
     
 
      
     
   
 
 
 
 
 
 
 
 
 
 
     
 
    
        
 
    
 
 
  
      
 
 
  
  
  
 
            
   
                    
 
      
 
 
   
  
  
        
 
            
 
   
    
 
   
 
 
 
 
 
  
 
            
         
  
 
 
 
 
 
    
        
                          
 
 
 
 
 
       
 
  
 
 
 
 
   
 
 
  
          
 
 
 
 
 
 
 
 
     
    
 
      
           
   
 
 
      
 
 
 
  
 
 
  
 
  
      
   
     
    
  
 
 
   
   
 
          
 
        
 
 
 
    
 
    
    
 
   
 
    
      
  
 
    
    
   
      
 
  
    
 
   
 
 
 
 
 
  
  
      
  
  
   
      
      
      
 
 
  
 
      
 
  
 
 
 
 
      
              
    
    
    
 
   
 
   

ro

  
rrr

 
 
 
 
  
 
  
   
  
 
  
  
 
     
         
    
 
     
     
    
 
 
 
  
      
    
 
 
 
 
      
    
            
        
 
 
            
 
 
       
 
  
 
   
 
 
 
   
       
       
         
  
   
   
 
        
 
 
 
   
     
 
      
 
 
       
  
  
  
 
         
 
   
     
   
       
  
   
         
 
 
 
 
 
 
 
 
                 
 
 
 
 
 
     
 
 
 
 
           
  
        
      
 
  
 
                    
        
    
 
 
   
      
 
   
   
      
       
 
 
 
 
    
  
   
    
 
 
 
 
 
 
    
         
       
            
 
    
 
 
  
   
            
  
 
 
 
  
 
     
 
 
 
     
 
     
 
        
 
    
 
 
    
 
 
 
 
 
 
 
 
 
 
  
   
           
                          
 
    
 
 
    
   
 
 
    
 
      
 
   
                 
 
         
  
 
     
 
   
       
 
 
 
 
      
 
   
  
      
 
 
 
      
 
  
 
 
    
 
 
        
 
   
   
 
        
 
 
  
    
        
  
          
 
  
 
      
 
   
    
   
     
 
      
     
   
 
 
 
 
 
 
 
 
 
 
     
 
    
        
 
    
 
 
  
      
 
 
  
  
  
 
            
   
                    
 
      
 
 
   
  
  
        
 
            
 
   
    
 
   
 
 
 
 
 
  
 
            
         
  
 
 
 
 
 
    
        
                          
 
 
 
 
 
       
 
  
 
 
 
 
   
 
 
  
          
 
 
 
 
 
 
 
 
     
    
 
      
           
   
 
 
      
 
 
 
  
 
 
  
 
  
      
   
     
 
    
  
 
 
   
   
 
          
 
        
 
 
 
    
 
    
    
 
   
 
    
      
  
 
    
    
   
      
 
  
    
 
   
 
 
 
 
 
  
  
      
  
  
   
      
      
      
 
 
  
 
      
 
  
 
 
 
 
      
              
    
    
    
 
   
 
   

figure    pol rcb data sets
linearly separable    d  ls    data
r j accuracy
size
hyperplanes
                   
    
                    
    
                    
     
                     
     
lmdt               
    
sadt                
      
parallel oblique lines  pol  data
r j accuracy
size
hyperplanes
                   
   
                   
   
                    
    
                    
    
lmdt                  
    
sadt               
     
rotated checker board  rcb  data
r j accuracy
size
hyperplanes
                   
   
                    
    
                    
    
                    
     
lmdt                
    
sadt                
      
table    effect randomization oc   first column  labelled r j  shows
number restarts  r  followed maximum number random jumps  j 
attempted oc  local minimum  results lmdt sadt
included comparison four variants oc   size average tree size
measured number leaf nodes  third column shows average
number hyperplanes algorithm considered building one tree 
  

fimurthy  kasif   salzberg

average ten   fold cvs   trees pruned algorithms 
data noise free furthermore emphasis search 
table   includes number hyperplanes considered algorithm
building complete tree  note oc  sadt  number hyperplanes considered generally much larger number perturbations actually made 
algorithms compare newly generated hyperplanes existing hyperplanes
adjusting existing one  nevertheless  number good estimate much effort
algorithm expends  every new hyperplane must evaluated according
impurity measure  lmdt  number hyperplanes considered identical
actual number perturbations 
      discussion

oc  results quite clear  first line table  labelled      gives
accuracies tree sizes randomization used   variant similar
cart lc algorithm  increase use randomization  accuracy increases
tree size decreases  exactly result hoped decided
introduce randomization method 
looking closely tables  ask effect random jumps alone 
illustrated second line        table  attempted    random
jumps local minimum restarts  accuracy increased      domain 
tree size decreased dramatically  roughly factor two  pol rcb
domains  note noise domains  high accuracies
expected  thus increases percent accuracy possible 
looking third line sub table table    see effect multiple restarts
oc      restarts random jumps escape local minima  improvement
even noticeable ls   data random jumps alone used 
data set  accuracy jumped significantly              tree size dropped
      nodes  pol rcb data  improvements comparable
obtained random jumps  rcb data  tree size dropped factor  
 from    leaf nodes    leaf nodes  accuracy increased            
fourth line table shows effect randomized steps  among
oc  entries  line highest accuracies smallest trees three
data sets  clear randomization big win kinds problems 
addition  note smallest tree rcb data eight leaf nodes 
oc  s average trees  without pruning      leaf nodes  clear data
set  thought dicult one  oc  came close finding optimal
tree nearly every run   recall numbers table average      fold
cv experiments  i e   average    decision trees   ls   data show dicult
find simple concept higher dimensions the optimal tree
single hyperplane  two nodes   oc  unable find current parameter
settings   pol data required minimum   leaf nodes  oc  found minimalsize tree time  seen table  although shown table 
   separate experiment  found oc  consistently finds linear separator ls   data
   restarts     random jumps used 

  

fiinduction oblique decision trees

oc  using sum minority performed better pol data twoing rule
impurity measure  i e   found correct tree using less time 
results lmdt sadt data lead interesting insights 
surprisingly  lmdt well linearly separable  ls    data 
require inordinate amount search  clearly  data linearly separable  one
use method lmdt linear programming  oc  sadt diculty finding
linear separator  although experiments oc  eventually find it  given sucient
time 
hand  non linearly separable data sets  lmdt produces
much larger trees significantly less accurate produced oc 
sadt  even deterministic variant oc   using zero restarts zero random jumps 
outperforms lmdt problems  much less search 
although sadt sometimes produces accurate trees  main weakness
enormous amount search time required  roughly       times greater oc  even
using       setting  one explanation oc  s advantage use directed search 
opposed strictly random search used simulated annealing  overall  table   shows
oc  s use randomization quite effective non linearly separable data 
natural ask randomization helps oc  task inducing decision trees 
researchers combinatorial optimization observed randomized search usually
succeeds search space holds abundance good solutions  gupta  smolka 
  bhaskar         furthermore  randomization improve upon deterministic search
many local maxima search space lead poor solutions  oc  s search
space  local maximum hyperplane cannot improved deterministic
search procedure   solution  complete decision tree  significant fraction
local maxima lead bad trees  algorithms stop first local maximum
encounter perform poorly  randomization allows oc  consider many
different local maxima  modest percentage maxima lead good trees 
good chance finding one trees  experiments oc  thus far indicate
space oblique hyperplanes usually contains numerous local maxima 
substantial percentage locally good hyperplanes lead good decision trees 

   conclusions future work
paper described oc   new system constructing oblique decision trees 
shown experimentally oc  produce good classifiers range real world
artificial domains  shown use randomization improves upon
original algorithm proposed breiman et al          without significantly increasing
computational cost algorithm 
use randomization might beneficial axis parallel tree methods  note
although find optimal test  with respect impurity measure 
node tree  complete tree may optimal  well known  problem
finding smallest tree np complete  hyafil   rivest         thus even axis parallel
decision tree methods produce  ideal  decision trees  quinlan suggested
windowing algorithm might used way introducing randomization c     even
though algorithm designed another purpose  quinlan      a    the windowing
  

fimurthy  kasif   salzberg

algorithm selects random subset training data builds tree using that  
believe randomization powerful tool context decision trees 
experiments one example might exploited  process
conducting experiments quantify accurately effects different forms
randomization 
clear ability produce oblique splits node broadens capabilities decision tree algorithms  especially regards domains numeric attributes 
course  axis parallel splits simpler  sense description split
uses one attribute node  oc  uses oblique splits impurity less
impurity best axis parallel split  however  one could easily penalize
additional complexity oblique split further  remains open area
research  general point domain best captured tree uses
oblique hyperplanes  desirable system generate tree 
shown problems  including used experiments  oc  builds small
decision trees capture domain well 

appendix a  complexity analysis oc 

following  show oc  runs eciently even worst case  data
set n examples  points  attributes per example  oc  uses o dn  log n 
time  assume n   analysis 
analysis here  assume coecients hyperplane adjusted sequential order  the seq method described paper   number restarts node
r  number random jumps tried j   r j constants  fixed
advance running algorithm 
initializing hyperplane random position takes o d  time  need
consider first maximum amount work oc  finds new location
hyperplane  need consider many times move hyperplane 
   attempting perturb first coecient  a    takes o dn   n log n  time  computing
ui  s points  equation    requires o dn  time  sorting ui  s takes
o n log n   gives us o dn   n log n  work 
   perturbing a  improve things  try perturb a    computing new
ui  s take o n  time one term different ui   re sorting
take o n log n   step takes o n    o n log n    o n log n  time 
   likewise a          ad take o n log n  additional time  assuming still
found better hyperplane checking coecient  thus total time cycle
attempt perturb additional coecients  d      o n log n   
o dn log n  
   summing up  time cycle coecients o dn log n  o dn n log n   
o dn log n  
   none coecients improved split  attempt make j random
jumps  since j constant  consider j     analysis  step
  

fiinduction oblique decision trees

involves choosing random vector running perturbation algorithm solve
ff  explained section      before  need compute set ui  s sort
them  takes o dn   n log n  time  amount time dominated
time adjust coecients  total time far still o dn log n  
time oc  spend node either halting finding improved
hyperplane 
   assuming oc  using sum minority max minority error measure 
reduce impurity hyperplane n times  clear improvement
means one example correctly classified new hyperplane  thus
total amount work node limited n o dn log n    o dn  log n    this
analysis extends  linear cost factors  information gain  gini index
twoing rule two categories  apply measure that 
example  uses distances mis classified objects hyperplane   practice 
found number improvements per node much smaller n 
assuming oc  adjusts hyperplane improves impurity measure 
o dn  log n  work worst case 
however  oc  allows certain number adjustments hyperplane
improve impurity  although never accept change worsens impurity 
number allowed determined constant known  stagnant perturbations   let
value s  works follows 
time oc  finds new hyperplane improves old one  resets counter
zero  move new hyperplane different location equal impurity
times  moves repeats perturbation algorithm  whenever
impurity reduced  re starts counter allows moves equally good
locations  thus clear feature increases worst case complexity oc 
constant factor  s 
finally  note overall cost oc  o dn  log n   i e   upper
bound total running time oc  independent size tree ends
creating   this upper bound applies sum minority max minority  open question
whether similar upper bound proven information gain gini index  
thus worst case asymptotic complexity system comparable systems
construct axis parallel decision trees  o dn    worst case complexity 
sketch intuition leads bound  let g total impurity summed
leaves partially constructed tree  i e   sum currently misclassified points
tree   observe time run perturbation algorithm node
tree  either halt improve g least one unit  worst case analysis one node
realized perturbation algorithm run every one n examples 
happens  would longer mis classified examples tree
would complete 

appendix b  definitions impurity measures available oc 

addition twoing rule defined text  oc  contains built in definitions five
additional impurity measures  defined follows  following definitions 
  

fimurthy  kasif   salzberg

set examples node split contains n       instances belong
one k categories   initially set entire training set   hyperplane h divides
two non overlapping subsets tl tr  i e   left right   lj rj number
instances category j tl tr respectively  impurity measures initially
check see tl tr homogeneous  i e   examples belong category  
return minimum  zero  impurity 

information gain  measure information gained particular split pop 

ularized context decision trees quinlan         quinlan s definition makes
information gain goodness measure  i e   something maximize  oc  attempts
minimize whatever impurity measure uses  use reciprocal standard value
information gain oc  implementation 

gini index  gini criterion  or index  proposed decision trees breiman et
al          gini index originally defined measures probability misclassification
set instances  rather impurity split  implement following
variation 
ginil        
ginir        

k
x
i  
k
x
i  

 li  jtlj  
 ri jtrj  

impurity    jtlj ginil   jtrj ginir  n
ginil gini index  left  side hyperplane ginir
right 

max minority  measures max minority  sum minority sum variances

defined context decision trees heath  kasif  salzberg      b    max
minority theoretical advantage tree built minimizing measure
depth log n  experiments indicated great advantage
practice  seldom impurity measures produce trees substantially deeper
produced max minority  definition is 
minorityl  
minorityr  

k
x
i   i  max li
k
x
i   i  max ri

li
ri

max minority   max minorityl  minorityr 
   sum variances called sum impurities heath et al 

  

fiinduction oblique decision trees

sum minority  measure similar max minority  minorityl minorityr defined max minority measure  sum minority sum
two values  measure simplest way quantifying impurity  simply
counts number misclassified instances 
though sum minority performs well domains  obvious aws 
one example  consider domain n             k      i e       examples   
numeric attribute    classes   suppose examples sorted according
single attribute  first    instances belong category    followed    instances category    followed    instances category    possible splits distribution
sum minority     therefore impossible using sum minority distinguish split preferable  although splitting alternations categories
clearly better 
sum variances  definition measure is 
jx
tl j
jx
tl j
variancel    cat tli     cat tlj   jtlj  
variancer  

i  

j   

jx
trj

jx
trj

i  

 cat tri    

j   

cat trj   jtrj  

sum variances   variancel   variancer
cat ti  category instance ti   measure computed using actual
class labels  easy see impurity computed varies depending numbers
assigned classes  instance  t  consists    points category    
points category    t  consists    points category     points category
   sum variances values different t  t   avoid problem 
oc  uniformly reassigns category numbers according frequency occurrence
category node computing sum variances 

acknowledgements
authors thank richard beigel yale university suggesting idea jumping
random direction  thanks wray buntine nasa ames research center providing
ind     package  carla brodley providing lmdt code  david heath
providing sadt code assisting us using it  thanks three anonymous
reviewers many helpful suggestions  material based upon work supported
national science foundation grant nos  iri          iri          iri         

references
aha  d          study instance based algorithms supervised learning  mathematical  empirical psychological evaluations  ph d  thesis  department information
computer science  university california  irvine 
  

fimurthy  kasif   salzberg

almuallin  h     dietterich  t          learning many irrelevant features  proceedings ninth national conference artificial intelligence  pp           san
jose  ca 
belsley  d          regression diagnostics  identifying uential data sources
collinearity  wiley   sons  new york 
bennett  k     mangasarian  o          robust linear programming discrimination two
linearly inseparable sets  optimization methods software           
bennett  k     mangasarian  o       a   multicategory discrimination via linear programming  optimization methods software           
bennett  k     mangasarian  o       b   serial parallel multicategory discrimination 
siam journal optimization        
blum  a     rivest  r          training   node neural network np complete  proceedings      workshop computational learning theory  pp        boston 
ma  morgan kaufmann 
breiman  l   friedman  j   olshen  r     stone  c          classification regression
trees  wadsworth international group 
brent  r  p          fast training algorithms multilayer neural nets  ieee transactions
neural networks                 
brodley  c  e     utgoff  p  e          multivariate versus univariate decision trees  tech 
rep  coins cr       dept  computer science  university massachusetts
amherst 
brodley  c  e     utgoff  p  e          multivariate decision trees  machine learning 
appear 
buntine  w          tree classification software  technology       third national
technology transfer conference exposition 
buntine  w     niblett  t          comparison splitting rules decision tree
induction  machine learning           
cardie  c          using decision trees improve case based learning  proceedings
tenth international conference machine learning  pp         university
massachusetts  amherst 
cestnik  g   kononenko  i     bratko  i          assistant     knowledge acquisition
tool sophisticated users  bratko  i     lavrac  n   eds    progress machine
learning  sigma press 
cios  k  j     liu  n          machine learning method generation neural network
architecture  continuous id  algorithm  ieee transactions neural networks 
               
  

fiinduction oblique decision trees

cohen  w          ecient pruning methods separate and conquer rule learning systems  proceedings   th international joint conference artificial intelligence  pp           morgan kaufmann 
fayyad  u  m     irani  k  b          attribute specification problem decision tree
generation  proceedings tenth national conference artificial intelligence 
pp           san jose ca  aaai press 
frean  m          small nets short paths  optimising neural computation  ph d 
thesis  centre cognitive science  university edinburgh 
gupta  r   smolka  s     bhaskar  s          randomization sequential distributed
algorithms  acm computing surveys               
hampson  s     volper  d          linear function neurons  structure training  biological cybernetics              
harrison  d     rubinfeld  d          hedonic prices demand clean air  journal
environmental economics management            
hassibi  b     stork  d          second order derivatives network pruning  optimal
brain surgeon  advances neural information processing systems    pp          
morgan kaufmann  san mateo  ca 
heath  d          geometric framework machine learning  ph d  thesis  johns
hopkins university  baltimore  maryland 
heath  d   kasif  s     salzberg  s       a   k dt  multi tree learning method 
proceedings second international workshop multistrategy learning  pp      
     harpers ferry  wv  george mason university 
heath  d   kasif  s     salzberg  s       b   learning oblique decision trees  proceedings
  th international joint conference artificial intelligence  pp            
chambery  france  morgan kaufmann 
herman  g  t     yeung  k  d          piecewise linear classification  ieee transactions pattern analysis machine intelligence                  
holte  r          simple classification rules perform well commonly used
datasets  machine learning                
hyafil  l     rivest  r  l          constructing optimal binary decision trees npcomplete  information processing letters               
kira  k     rendell  l          practical approach feature selection  proceedings
ninth international conference machine learning  pp           aberdeen 
scotland  morgan kaufmann 
kirkpatrick  s   gelatt  c     vecci  m          optimization simulated annealing 
science                      
  

fimurthy  kasif   salzberg

kodratoff  y     manago  m          generalization noise  international journal
man machine studies              
langley  p     sage  s          scaling domains many irrelevant features  learning
systems department  siemens corporate research  princeton  nj 
mangasarian  o   setiono  r     wolberg  w          pattern recognition via linear programming  theory application medical diagnosis  siam workshop
optimization 
mingers  j       a   empirical comparison pruning methods decision tree induction  machine learning                 
mingers  j       b   empirical comparison selection measures decision tree induction  machine learning             
moret  b  m          decision trees diagrams  computing surveys                  
murphy  p     aha  d          uci repository machine learning databases   machinereadable data repository  maintained department information computer
science  university california  irvine  anonymous ftp ics uci edu
directory pub machine learning databases 
murthy  s  k   kasif  s   salzberg  s     beigel  r          oc   randomized induction
oblique decision trees  proceedings eleventh national conference artificial
intelligence  pp           washington  d c  mit press 
murthy  s  k     salzberg  s          using structure improve decision trees  tech  rep 
jhu        department computer science  johns hopkins university 
niblett  t          constructing decision trees noisy domains  bratko  i     lavrac 
n   eds    progress machine learning  sigma press  england 
nilsson  n          learning machines  morgan kaufmann  san mateo  ca 
odewahn  s   stockwell  e   pennington  r   humphreys  r     zumach  w          automated star galaxy descrimination neural networks  astronomical journal 
                 
pagallo  g          adaptive decision tree algorithms learning examples  ph d 
thesis  university california santa cruz 
pagallo  g     haussler  d          boolean feature discovery empirical learning  machine
learning               
quinlan  j  r          learning ecient classification procedures application
chess end games  michalski  r   carbonell  j     mitchell  t   eds    machine
learning  artificial intelligence approach  morgan kaufmann  san mateo  ca 
quinlan  j  r          induction decision trees  machine learning            
  

fiinduction oblique decision trees

quinlan  j  r          simplifying decision trees  international journal man machine
studies              
quinlan  j  r       a   c     programs machine learning  morgan kaufmann publishers  san mateo  ca 
quinlan  j  r       b   combining instance based model based learning  proceedings
tenth international conference machine learning  pp          university
massachusetts  amherst  morgan kaufmann 
roth  r  h          approach solving linear discrete optimization problems  journal
acm                  
safavin  s  r     landgrebe  d          survey decision tree classifier methodology 
ieee transactions systems  man cybernetics                  
sahami  m          learning non linearly separable boolean functions linear threshold unit trees madaline style networks  proceedings eleventh national
conference artificial intelligence  pp           aaai press 
salzberg  s          nearest hyperrectangle learning method  machine learning    
        
salzberg  s          combining learning search create good classifiers  tech  rep 
jhu        johns hopkins university  baltimore md 
salzberg  s   chandar  r   ford  h   murthy  s  k     white  r          decision trees
automated identification cosmic rays hubble space telescope images  publications astronomical society pacific  appear 
schaffer  c          overfitting avoidance bias  machine learning              
schlimmer  j          eciently inducing determinations  complete systematic
search algorithm uses optimal pruning  proceedings tenth international
conference machine learning  pp           morgan kaufmann 
smith  j   everhart  j   dickson  w   knowler  w     johannes  r          using
adap learning algorithm forecast onset diabetes mellitus  proceedings
symposium computer applications medical care  pp           ieee
computer society press 
utgoff  p  e          perceptron trees  case study hybrid concept representations 
connection science                 
utgoff  p  e     brodley  c  e          incremental method finding multivariate
splits decision trees  proceedings seventh international conference
machine learning  pp         los altos  ca  morgan kaufmann 
utgoff  p  e     brodley  c  e          linear machine decision trees  tech  rep     
university massachusetts amherst 
  

fimurthy  kasif   salzberg

van de merckt  t          nfdt  system learns exible concepts based decision
trees numerical attributes  proceedings ninth international workshop
machine learning  pp          
van de merckt  t          decision trees numerical attribute spaces  proceedings
  th international joint conference artificial intelligence  pp            
weiss  s     kapouleas  i          empirical comparison pattern recognition  neural
nets  machine learning classification methods  proceedings   th international joint conference artificial intelligence  pp           detroit  mi  morgan
kaufmann 
wolpert  d          overfitting avoidance bias  tech  rep  sfi tr            
santa fe institute  santa fe  new mexico 

  


