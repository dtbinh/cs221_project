Journal of Artificial Intelligence Research 7 (1997) 231-248

Submitted 7/97; published 11/97

Dynamic Non-Bayesian Decision Making
Dov Monderer
Moshe Tennenholtz

dov@ie.technion.ac.il
moshet@ie.technion.ac.il

Industrial Engineering and Management
Technion { Israel Institute of Technology
Haifa 32000, Israel

Abstract

The model of a non-Bayesian agent who faces a repeated game with incomplete information against Nature is an appropriate tool for modeling general agent-environment
interactions. In such a model the environment state (controlled by Nature) may change arbitrarily, and the feedback/reward function is initially unknown. The agent is not Bayesian,
that is he does not form a prior probability neither on the state selection strategy of Nature,
nor on his reward function. A policy for the agent is a function which assigns an action to
every history of observations and actions. Two basic feedback structures are considered.
In one of them { the perfect monitoring case { the agent is able to observe the previous
environment state as part of his feedback, while in the other { the imperfect monitoring
case { all that is available to the agent is the reward obtained. Both of these settings
refer to partially observable processes, where the current environment state is unknown.
Our main result refers to the competitive ratio criterion in the perfect monitoring case.
We prove the existence of an ecient stochastic policy that ensures that the competitive
ratio is obtained at almost all stages with an arbitrarily high probability, where eciency
is measured in terms of rate of convergence. It is further shown that such an optimal
policy does not exist in the imperfect monitoring case. Moreover, it is proved that in the
perfect monitoring case there does not exist a deterministic policy that satisfies our long
run optimality criterion. In addition, we discuss the maxmin criterion and prove that a
deterministic ecient optimal strategy does exist in the imperfect monitoring case under
this criterion. Finally we show that our approach to long-run optimality can be viewed as
qualitative, which distinguishes it from previous work in this area.

1. Introduction
Decision making is a central task of artificial agents (Russell & Norvig, 1995; Wellman,
1985; Wellman & Doyle, 1992). At each point in time, an agent needs to select among
several actions. This may be a simple decision, which takes place only once, or a more
complicated decision where a series of simple decisions has to be made. The question of
\what should the right actions be" is the basic issue discussed in both of these settings, and
is of fundamental importance to the design of artificial agents.
A static decision-making context (problem) for an artificial agent consists of a set of actions that the agent may perform, a set of possible environment states, and a utility/reward
function which determines the feedback for the agent when it performs a particular action
in a particular state. Such a problem is best represented by a matrix with columns indexed
by the states, rows indexed by the actions and the rewards as entries. When the reward
function is not known to the agent we say that the agent has payoff uncertainty and we
c 1997 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiMonderer and Tennenholtz

refer to the problem as a problem with incomplete information(Fudenberg & Tirole, 1991).
When modeling a problem with incomplete information one must also describe the underlying assumptions on the knowledge of the agent about the reward function. For example, the
agent may know bounds on his rewards, or he may know (or partially know) an underlying
probabilistic structure1 . In a dynamic (multistage) decision-making setup the agent faces
static decision problems over stages. At each stage the agent selects an action to be performed and the environment selects a state. The history of actions and states determines
the immediate reward as well as the next one-shot decision problem. The history of actions
and states also determines the next selected state. Work on reinforcement learning in artificial intelligence (Kaelbling, Littman, & Moore, 1996) has adopted the view of an agent
operating in a probabilistic Bayesian setting, where the agent's last action and the last state
determine the next environment state based on a given probability distribution. Naturally,
the learner may not be a-priori familiar with this probability distribution, but the existence
of the underlying probabilistic model is a key issue in the system's modeling. However, this
assumption is not an ultimate one. In particular, much work in other areas in AI and in
economics have dealt with non-probabilistic settings in which the environment changes in
an unpredictable manner2 . When the agent does not know the inuence of his choices on
the selection of the next state (i.e., he is not certain about the environment strategy), we
say that the agent has strategic uncertainty.
In this paper we use a general model for the representation of agent-environment interactions in which the agent has both payoff and strategic uncertainty. We deal with a
non-Bayesian agent who faces a repeated game with incomplete information against Nature.
In a repeated game against Nature the agent faces the same static decision problem at
each stage while the environment state is taken to be an action chosen by his opponents.
The decision problem is called a game to stress the fact that the agent's action and the
state are independently chosen. The fact that the game is repeated refers to the fact
that the set of actions, the set of possible states, and the one shot utility function do not
vary with time3 . As we said, we consider an agent that has both payoff uncertainty and
strategic uncertainty. That is, he is a-priori ignorant about the utility function (i.e., the
game is of incomplete information) as well as about the state selection strategy of Nature.
The agent is non-Bayesian in the sense that he does not assume any probabilistic model
concerning nature's strategy and in the sense that he does not assume any probabilistic
model concerning the reward function, though he may assume lower and upper bounds4 .
We consider two examples to illustrate the above-mentioned notions and model. Consider
1. For example, the agent may know a probability distribution on a set of reward functions, he may assume
that such a probability exists without any assumption on its structure, or he may have partial information
about this distribution but be ignorant about some of its parameters (e.g., he may believe that the reward
function is drawn according to a normal distribution with an unknown covariance matrix).
2. There are many intermediate cases where it is assumed that the changes are probabilistic with a nonMarkovian structure.
3. In the most general setup, those sets may vary with time. No useful analysis can be done in a model
where those changes are completely arbitrary.
4. Repeated games with complete information, or more generally, multistage games and stochastic games
have been extensively studied in game theory and economics. A very partial list includes: (Shapley,
1953; Blackwell, 1956; Luce & Raiffa, 1957), and more recently (Fudenberg & Tirole, 1991; Mertens,
Sorin, & Zamir, 1995), and the evolving literature on learning (e.g., Fudenberg & Levine 1997). The
incomplete information setup in which the player is ignorant about the game being played was inspired

232

fiDynamic Non-Bayesian Decision Making

an investor, I , who is investing daily in a certain index of the stock market. His daily profits
depends on his action (selling or buying in a certain amount) and on the environment state
{ the percentage change in the price of the index. This investor has complete information
about the reward function because he knows the reward which is realized in a particular
investment and a particular change, but he has strategic uncertainty about the changes
in the index price. So, he is playing a repeated game with complete information against
Nature with strategic uncertainty.
Consider another investor, I 1, who invests in a particular mutual fund. This fund invests
in the stock market with a strategy which is not known to the investor. Assume that each
state represents the vector of percentage changes in the stocks, then the investor does not
know his reward function. For example, he cannot say in advance what would be his profit
if he would buy one unit of this fund and all stock prices increase in 1 percent. Thus, I 1
plays a repeated game with incomplete information. If in addition I 1 does not attempt to
construct a probabilistic model concerning his reward function or market behavior, then he
is non-Bayesian and our analysis may apply to him. For another example, assume that Bob
has to decide on each evening whether to prepare tea or coffee for his wife before she gets
home. His wife wishes to drink either tea or coffee and he wishes to have it ready for her.
The reaction of Bob's wife to tea or coffee may depend on her state that day, which can not
be predicted based on the history of actions and states in previous days. As Bob has just
got married he cannot tell what reward he will get if his wife is happy and he makes her
a cup of tea. Of course he may eventually know it, but his decisions during this learning
period are precisely the subject of this paper.
As an example for the generality of the above-mentioned setting, consider the model of
Markov decision processes with complete or incomplete information. In a Markov decision
process an agent's action in a given state determines (in a probabilistic fashion) the next
state to be obtained. That is, the agent has a structural assumption on the state selection
strategy. A repeated game against Nature without added assumptions captures the fact
that the transition from state to state may depend on the history in an arbitrary way.
When the agent performs an action at in state st , part of his feedback would be u(at; st ),
where u is the reward function. We distinguish between two basic feedback structures. In
one of them { the perfect monitoring case { the agent is able to observe the previous
environment state as part of his feedback, while on the other { the imperfect monitoring
case { all that is available to the agent is the reward obtained5 . Notice that in both of
these feedback structures, the current state is not observed by the agent when he is called
to select an action6 . Both investors I and I 1 face a repeated game with perfect monitoring
because the percentage changes become public knowledge after each iteration.
In the other example, when Bob has to make his decision, if the situation is of imperfect
monitoring, Bob would be only able to observe the reward for his behavior (e.g., whether
by (Harsanyi, 1967). See Aumann and Maschler (1995) for a comprehensive survey. Most of the above
literature deals with (partially) Bayesian agents. Some of the rare exceptions are cited in Section 6.
5. Notice that the former assumption is very popular in the related game theory literature (Aumann &
Maschler, 1995). Many other intermediate monitoring structures may be interesting as well.
6. Such is also the case in the evolving literature on the problem of controlling partially observable Markov
decision processes (Lovejoy, 1991; Cassandra, Kaelbling, & Littman, 1994; Monahan, 1982). In contrast,
Q-learning theory (Watkins, 1989; Watkins & Dayan, 1992; Sutton, 1992) does assume a knowledge of
the current state.

233

fiMonderer and Tennenholtz

she says \thanks", \that's terrible", \this is exactly what I wanted", etc.). In the perfect
monitoring case, Bob will be able to observe his wife's state (e.g., whether she came home
happy, sad, nervous, etc.) in addition to his reward. In both cases Bob (like the investors)
is not able to observe his wife's state before making his decision in a particular day.
Consider the case of a one stage game against Nature, in which the utility function
is known, but the agent cannot observe the current environment state when selecting his
action. How should the agent choose his action? Work on decision making under uncertainty
has suggested several approaches (Savage, 1972; Milnor, 1954; Luce & Raiffa, 1957; Kreps,
1988). One of these approaches is the maxmin (safety-level) approach. According to this
approach the agent would choose an action that maximizes his worst case payoff. Another
approach is the competitive ratio approach (or its additive variant, termed the minmax
regret decision criterion (Milnor, 1954). According to this approach an agent would choose
an action that minimizes the worst case ratio between the payoff he could have obtained
had he known the environment state to the payoff he would actually obtain.7 Returning
back to our example, if Bob would have known the actual state of his wife, he could choose
an action that maximizes his payoff. Since he has no hint about her state, he can go ahead
and choose the action that minimizes his competitive ratio. For example, if this action leads
to a competitive ratio of two, then Bob can guarantee himself that the payoff he would get
is at most half the payoff he could have gotten had he known the actual state of his wife.
Given a repeated game with incomplete information against Nature, the agent would
not be able to choose his one stage optimal action (with respect to the competitive ratio or
maxmin value criteria) at each stage, since the utility function is initially unknown. So, if
Bob does not initially know the reward he would receive for his actions as a function of his
wife's state, then he will not be able to simply choose an action that guarantees the best
competitive ratio. This calls for a precise definition of a long-run optimality criterion. In
this paper we are mainly concerned with policies (strategies) guaranteeing that the optimal
competitive ratio (or the maxmin value) is obtained in most stages. We are interested in
particular in ecient policies, where eciency is measured in terms of rate of convergence.
Hence in Bob's case, we are interested in a policy that if adopted by Bob would guarantee
him on almost any day, with high probability, at least the payoff guaranteed by an action
leading to the competitive ratio. Moreover, Bob will not have to wait much before he will
start getting this type of satisfactory behavior.
In Section 2 we define the above mentioned setting and introduce some preliminaries. In
Sections 3 and 4 we discuss the long-run competitive ratio criterion: In Section 3 we show
that even in the perfect monitoring case, a deterministic optimal policy does not exist.
However, we show that there exists an ecient stochastic policy which ensures that the
long-run competitive ratio criterion holds with a high probability. In Section 4 we show
that such stochastic policies do not exist in the imperfect monitoring case. In Section 5 we
prove that for both the perfect and imperfect monitoring cases there exists a deterministic
ecient optimal policy for the long-run maxmin criterion. In Section 6 we compare our
notions of long-run optimality to other criteria appearing in some of the related literature.
In particular, we show that our approach to long-run optimality can be interpreted as
7. The competitive ratio decision criterion has been found to be most useful in settings such as on-line
algorithms (e.g., Papadimitriou & Yanakakis, 1989).

234

fiDynamic Non-Bayesian Decision Making

qualitative, which distinguishes it from previous work in this area. We also discuss some of
the connections of our work with work in reinforcement learning.

2. Preliminaries
A (one-shot) decision problem (with payoff certainty and strategic uncertainty) is a 3-tuple
D =< A; S; u >, where A and S are finite sets and u is a real-valued function defined on
A  S with u(a; s) > 0 for every (a; s) 2 A  S . Elements of A are called actions and those
of S are called states. u is called the utility function. The interpretation of the numerical
values u(a; s) is context-dependent8 . Let nA denote the number of actions in A, let nS
denote the number of states in S and let n = max(nA ; nS ).
The above-mentioned setting is a classical static setting for decision making, where
there is uncertainty about the actual state of nature (Luce & Raiffa, 1957). In this paper
we deal with a dynamic setup, in which the agent faces the decision problem D, without
knowing the utility function u, over an infinite number of stages, t = 1; 2; : : :. As we
have explained in the introduction, this setting enables us to capture general dynamic nonBayesian decision-making contexts, where the environment may change its behavior in an
arbitrary and unpredictable fashion. As mentioned in the introduction, this is best captured
by means of a repeated game against Nature. The state of the environment at each point
plays the role of an action taken by Nature in the corresponding game. The agent knows
the sets A and S , but he does not know the payoff function u.9 A dynamic decision problem
(with payoff uncertainty and strategic uncertainty) is therefore represented for the agent
by a pair DD =< A; S > of finite sets10. At each stage t, Nature chooses a state st 2 S .
The agent, who does not know the chosen state, chooses at 2 A, and receives u(at; st). We
distinguish between two informational structures. In the perfect monitoring case, the state
st is revealed to the agent alongside the payoff u(at; st). In the imperfect monitoring case,
the states are not revealed to the agent. A generic history available to the agent at stage t +1
is denoted by ht . In the perfect monitoring case, ht 2 Htp = (A  S  R+ )t , where R+ denotes
the set of positive real numbers. In the imperfect monitoring case, ht 2 Htimp = (A  R+ )t.
In the particular case t = 0 we assume that H0p = H0imp = feg is a singleton containing
p
imp = [1 H imp . The symbol H will be
the empty history e. Let H p = [1
t=0 Ht and let H
t=0 t
used for both H p and H imp . A strategy11 for the agent in a dynamic decision problem is
a function F : H ! (A) , where (A) denotes the
P set of probability measures over A.
That is, for every ht 2 H , F (ht ) : A ! [0; 1] and a2A F (ht )(a) = 1. In other words, if
the agent observes the history ht then he chooses at+1 by randomizing amongst his actions,
with the probability F (ht )(a) assigned to the action a. A strategy F is called pure if F (ht )
is a probability measure concentrated on a singleton for every t  0.
In Sections 2{4 the strategy recommended to the agent is chosen according to a \longrun" decision criterion which is induced by the competitive ratio one-stage decision criterion.
8. See the discussion at Section 6.
9. All the results of this paper remain unchanged if the agent does not initially know the set S , but rather
an upper bound on nS .
10. Notice that there is no need to include an explicit transition function in this representation. This is due
to the fact that in the non-Bayesian setup every transition is possible.
11. Strategy is a decision theoretic concept. It coincides with the term policy used in the control theory
literature, and with the term protocol used in the distributed systems literature.

235

fiMonderer and Tennenholtz

The competitive ratio decision criterion, that is described below, may be used by an agent
who faces the decision problem only once, and who knows the payoff function u as well as
the sets A and S . There are other \reasonable" decision criteria that could be used instead.
One of them is the maxmin decision criterion to be discussed in Section 5, while another
is the minmax regret decision criterion (Luce & Raiffa, 1957; Milnor, 1954). The latter is
a simple variant of the competitive ratio (and can be treated similarly), and therefore will
not be treated explicitly in this paper.
For every s 2 S let M (s) be the maximal payoff the agent can get when the state is s.
That is
M (s) = max
u(a; s):
a2A
For every a 2 A and s 2 S define

c(a; s) = uM(a;(ss)) :

Denote c(a) = maxs2S c(a; s), and let





CR = min
c(a) = min
max c(a; s) :
a2A
a2A s2S
CR is called the competitive ratio of D =< A; S; u >. Any action a for which CR = c(a)
is called a competitive ratio action, or in short a CR action. An agent which chooses a
1 fraction from what it could have gotten, had it
CR action guarantees receiving at least CR
1
known the state s. That is, u(a; s)  CR M (s) for every s 2 S . This agent cannot guarantee
a bigger fraction.
In the long-run decision problem, a non-Bayesian agent does not form a prior probability
on the way Nature is choosing the states. Nature may choose a fixed sequence of states or,
more generally, use a probabilistic strategy G, where G : Q ! (S ), and Q = [1
t=0 Qt =
t. Nature can be viewed as a second player that knows the reward function. Its
[1
(
A

S
)
t=0
strategy may of course refer to the whole history of actions and states until a given point
and may depend on the payoff function.
A payoff function u and a pair of probabilistic strategies F; G, where G can depend on u,
generate a probability measure  = F;G;u over the set of infinite histories Q1 = (A  S )1
endowed with the natural measurable structure. For an event B  Q1 we will denote the
probability of B according to  by (B ) or by Prob (B ). More precisely, the probability
measure  is uniquely defined by its values for finite cylinder sets: Let At : Q1 ! A and
St : Q1 ! S be the coordinate random variables which contain the values of the actions
and states selected by the agent and the environment in stage t (respectively). That is,
At(h) = at and St(h) = st for every h = ((a1; s1); (a2; s2); : : :) in Q1 . Then for every T  1
and for every ((a1; s1); : : :; (aT ; sT )) 2 QT ,

Prob ((At; St) = (at; st) for all 1  t  T ) =
where

0

T
Y
t=1

F ('t,1)(at )G( t,1)(st );

and '0 are the empty histories, and for 2  t  T we have
t,1 = ((a1; s1 ); : : :; (at,1; st,1)) ;

236

fiDynamic Non-Bayesian Decision Making

while the definition of 't,1 depends on the monitoring structure. In the perfect monitoring
case,
't,1 = ((a1 ; s1; u(a1; s1)); : : :; (at,1; st,1; u(at,1; st,1))) ;
and in the imperfect monitoring case
't,1 = ((a1; u(a1; s1)); : : :; (at,1; u(at,1; st,1))) :
We now define some auxiliary additional random variables on Q1 . P
Let Xt = 1 if c(At ; St)  CR and Xt = 0 otherwise, and let NT = Tt=1 Xt .12 Let  > 0.
A strategy F is  -optimal if there exists an integer K such that for every payoff function u
and every Nature's strategy G

Prob (NT  (1 ,  )T for every T  K )  1 , 
(1)
where  = F;G;u . A strategy F is optimal if it is  -optimal for all  > 0.
Roughly speaking, NT measures the number of stages in which the competitive ratio (or
a better value) has been obtained in the first T iterations. In an  -optimal strategy there
exists a number K , such that if the system runs for T  K iterations we can get with high
probability that NT is close to 1 (i.e., almost all iterations are good ones). In an optimal

strategy we guarantee that we can get as close as we wish to the situation where all iterations
are good ones, with a probability that is as high as we wish. Notice that we require that the
above-mentioned useful property will hold for every payoff function and for every strategy
of Nature. This strong requirement is a consequence of the non-Bayesian setup; since we do
not have any clue about the reward function or about the strategy selected by Nature (and
this strategy may yield arbitrary sequences of states to be reached), the best policy would
be to insist on good behavior against any behavior adopted by Nature. Notice however that
two other relaxations are introduced here; we require successful behavior in most stages,
and that the whole process would be successful only with some (very high) probability.
The major objective is to find a policy that will enable (1) to hold for every dynamic
decision problem and every Nature strategy. Moreover, we wish (1) to hold for small enough
K . If K is small then our agent can benefit from obtaining the desired behavior already in
an early stage. 13 This will be the subject of the following section. We complete this section
with a useful technical observation. We show that a strategy F is  -optimal if it satisfies
the optimality criterion (1) for every reward function and for every stationary strategy of
nature, where a stationary strategy is defined by a sequence of states z = (st )1
t=1 . In this
strategy Nature chooses st at stage t, independent of the history. Indeed, assume that F is
a strategy for which (1) holds for every reward function and for every stationary strategy
of Nature, then we show that F is  -optimal.
Given any payoff function u and any strategy G,  -optimality with respect to stationary
strategies implies that for  = F;G;u ,
Prob (NT  (1 , )T for every T  K )jS1; S2; : : :)  1 , ;
12. Note that the function c(a; s) depends on the payoff function u and therefore so do the random variables
Xt and Nt .
13. The interested reader may wish to think of our long-run optimality criteria in view of the original work
on PAC learning (Valiant, 1984). In our setting, as in PAC learning, we wish to obtain desired behavior,
in most situations, with high probability, and relatively fast.

237

fiMonderer and Tennenholtz

with probability one. Therefore

Prob(NT  (1 ,  )T for every T  K )  1 , :
Roughly speaking, the above captures the fact that in our non-Bayesian setting we
need to present a strategy that will be good for any sequence of states chosen by Nature,
regardless of the way in which it has been chosen.

3. Perfect Monitoring

In this section we present our main result. This result refers to the case of perfect monitoring, and shows the existence of a  -optimal strategy in this case. It also guarantees that the
desired behavior will be obtained after polynomially many stages. Our result is constructive. We first present the rough idea of the strategy employed in our proof. If the utility
function was known to the agent then he could use the competitive ratio action. Since the
utility function is initially unknown then the agent will use a greedy strategy, where he
selects an action that is optimal as far as the competitive ratio is concerned, according to
the agent's knowledge at the given point. However, the agent will try from time to time to
sample a random action.14 Our strategy chooses a right tradeoff between these exploration
and exploitation phases in order to yield the desired result. Some careful analysis is needed
in order to prove the optimality of the related strategy, and the fact it yields the desired
result after polynomially many stages.
We now introduce our main theorem.

Theorem 3.1: Let DD =< A; S > be a dynamic decision problem with perfect monitoring.

Then for every  > 0 there exists a  -optimal strategy. Moreover, the  -optimal strategy
can be chosen to be ecient in the sense that K (in (1)) can be taken to be polynomial in
max(n; 1 ).

Proof: Recall that nA and nS denote the number of actions and states respectively, and
that n = max(nA ; nS ). In this proof we assume for simplicity that n = nA = nS . Only
slight modifications are required for removing this assumption. Without loss of generality,
 < 1. We define a strategy F as follows: Let M = 8 . That is,

1 
M = 8:
At each stage T  1 we will construct matrices UTF ; CTF and a subset of the actions WT in
the following way: Define U1F (a; s) =  for each a; s. At each stage T > 1, if aT ,1 has been
performed in stage T , 1, and sT ,1 has been observed, then update U by replacing the 
in the (aT ,1; sT ,1) entry with u(aT ,1; sT ,1). At each stage T  1, if UTF (a; s) = , define
F b;s)
CTF (a; s) = 1. If UTF (a; s) =
6 , CTF (a; s) = maxfb:UTF (b;s)6=g UUTTF ((a;s
) . Finally WT is the set
of all a 2 A at which minb2A maxs2S CTF (b; s) is obtained. We refer to elements in WT as
the temporarily good actions at stage T . Let (Zt)t1 be a sequence of i.i.d. f0; 1g random
14. We use a uniform probability distribution to select among actions in the exploration phase. Our result
can be obtained with different exploration techniques as well.

238

fiDynamic Non-Bayesian Decision Making

variables with Prob(Zt = 1) = 1 , M1 . This sequence is generated as part of the strategy,
independent of the observed history. That is at each stage, before choosing an action, the
agent ips a coin, independently of his past observations. At each stage t the agent observes
Zt. If Zt = 1, the agent chooses an action from Wt by randomizing with equal probabilities.
If Zt = 0 the agent randomizes with equal probabilities amongst the actions in A. This
complete the description of the strategy F . Let u be a given payoff function, and let (st)1
t=1
be a given sequence of states. We proceed to show that (1) holds with K being the upper
integer value of ff = max(ff1 + 2; ff2 + 2), where


256
ff1 = 128
2 ln 3

n2 (n 8 + 1) ln
and ff2 =
3
4



 2
2n + 1


:
P

Recall that Xt = 1 if c(At; st )  CR and Xt = 0 otherwise, and that NT = Tt=1 Xt . By
a slight change of notation, we denote by P = Prob the probability measure induced by
F , u and the sequence of states on (A  S  f0; 1g)1 (where f0; 1g corresponds to the Zt
values).
Let " = 8 . Define

BK =

(T
X

t=1

)


1
Zt  1 , M , " T for all T  K :


Roughly speaking, BK captures the cases where temporarily good actions are selected
in most stages.
By (Chernoff, 1952) (see also (Alon, Spencer, & Erdos, 1992)), for every T ,

P

T
X
t=1

!

2
Zt < (1 , 1 , ")T  e, " 2T :

M

Recall that given a set S , S denotes the complement of S .
Hence,
!
1
T
1
2
X
X
X
1
Zt < (1 , , ")T 
e, " 2T :
P (BK )  P
Therefore
Since K > ff1 ,
Define:

T =k

P (BK ) 

t=1
Z1

k ,1

M

e, " 2T dT = "22 e,
2

P (BK ) < 2

T =K

"2 (K ,1)

2

:
(2)

LK = fNT  (1 ,  )T for every T  K g:
Roughly speaking, LK captures the cases where competitive ratio actions (or better

actions in this regard) are selected in most stages.
In order to prove that F is  -optimal (i.e., that (1) is satisfied), we have to prove that

P (LK ) < 
239

(3)

fiMonderer and Tennenholtz

By (2) it suces to prove that

(4)
P (LK jBK )  2
We now define for every t  1, s 2 S and a 2 A six auxiliary random variables, Yt ; Rt; Yts ; Rst; Yts;a ; Rs;a
t .
Let Yt = 1 whenever Zt = 1 and Xt = 0, and Yt = 0 otherwise. Let

RT =

T
X
t=1

Yt :

For every s 2 S let Yts = 1 whenever Yt = 1 and st = s, and Yts = 0 otherwise. Let
T
X
Yts :
t=1
Yts;a = 1

RsT =

For every s 2 S and for every a 2 A, let
whenever Yts = 1 and At = a, and
s;a
Yt = 0 otherwise. Let
T
X
Rs;a
=
Yts;a :
T
t=1

Let g be the integer value of 34 K . We now show that

P (LK jBK )  P(9T  K ; RT  gjBK )

(5)

In order to prove (5) we show that

LK \ BK  f9T  K ; RT  g g \ BK :
Indeed, if w is a path in BK such that for every T  K RT < g , then, at w, for every
T  K,
T
X
X
NT 
Xt  VT , Yt;
1tT;Zt=1

t=1

where VT denotes the number of stages 1  t  T for which Zt = 1. Since w 2 BK ,
N  (1 , 1 , ")T , R > (1 , 1 , ")T , g
T

M

T

M

for every T  K . Since M1 = " = 8 and g  34 K , NT  (1 ,  )T for every T  K . Hence,
w 2 LK .
(5) implies that it suces to prove that

P(9T  K ; RT  gjBK )  2
Therefore it suces to prove that for every s 2 S ,


P 9T  K; RsT  ng jBK  2n :
Hence it suces to prove that for every s 2 S and every a 2 A,
240

(6)

fiDynamic Non-Bayesian Decision Making



 = P 9T  K;

g
Rs;a
T  n2 jBK



 2n 2

(7)

g
In order to prove (7) , note that if the inequality Rs;a
T  n2 is satisfied at gw, then
c(a; s) > CR and a is nevertheless considered to be a good action in at least n2 stages
b;s) > CR. If b is
1  t  T (w.l.o.g. assume that ng2 is an integer). Let b 2 A satisfy uu((a;s
)
ever played in a stage t with st = s, then a 62 Wt for all t  t. Therefore




  P 9T  K; b is not played in the first ng2 stages at which st = sjBK :
Hence
As (1 , x1 )x+1  e,x for x  1,

ut



  1, 1

nM

 g2
n

:

g
  e, n2 (nM +1) < 2n 2 :

Theorem 3.1 shows that ecient dynamic non-Bayesian decisions may be obtained by
an appropriate stochastic policy. Moreover, it shows that  -optimality can be obtained in
time which is a (low degree) polynomial in max(n; 1 ). An interesting question is whether
similar results can be obtained by a pure/deterministic strategy. As the following example
shows, deterministic strategies do not suce for that job.
We give an example in which the agent does not have a  optimal pure strategy.

Example 1: Let A = fa1; a2g and S = fs1; s2g. Assume in negation that the agent has a
 optimal pure strategy f .

Consider the following two decision problems whose rows are indexed by the actions and
whose columns are indexed by the states:

D1 =

1 10
30 2

D2 =

1 30
10 2

with the corresponding ratio matrices:

C1 =

30 1
1 5

C2 =

10 1
1 15
241

!

!

!
!

fiMonderer and Tennenholtz

Assume in addition that in both cases Nature uses the strategy g , defined as follows:
g (ht) = si if f (ht) = ai , i = 1; 2. That is, for every t, (at; zt ) = (a1; s1) or (at; zt) = (a2; s2),
where at and zt denote the action and state selected at stage t, respectively. Let  < 0:25.
Let NTi denote NT for decision problem i. Since f is  -optimal, there exists K such that
for every T  K , NT1  (1 ,  )T and NT2  (1 ,  )T . Note also that the same sequence
((at; zt))t1 is generated in both cases. NK1  (1 ,  )K implies that (a2 ; s2) is used at more
than half of the stages 1; 2; : : :; K . On the other hand, NK2  (1 ,  )K implies that (a1; s1)
is used at more than half of the stages 1; 2; : : :; K . A contradiction.
ut
For analytical completeness, we end this section by proving the existence of an optimal
strategy (and not merely a  -optimal strategy). Such an optimal strategy is obtained by
utilizing m -optimal strategies (whose existence was proved in Theorem 3.1) for intervals of
stages with sizes that converge to infinity, when m ! 0.

Corollary 3.2: In every dynamic decision problem with perfect monitoring there exists
an optimal strategy.

Proof: For m  1, let Fm be a

sequence with limm!1 m = 0. Let
that for every m  1

m -optimal strategy, where (m )1 is a decreasing
m=1
2
(Km )1
be
an
increasing
sequence
of integers such
m=1





m
Prob NT  (1 , 2 )T for every T  Km  1 , 2m ;

and

Pm
K
Km+1  2 j=1 j :
m
Let F be the strategy that for m  1 utilizes Fm at the stages K0 + : : : + Km,1 + 1  t 
K0 + : : : + Km,1 + Km, where K0 = 0. It can be easily verified that F is optimal.

ut

4. Imperfect Monitoring

We proceed to give an example for the imperfect monitoring case, where for suciently
small  > 0, the agent does not have a  -optimal strategy.

Example 2 (Non-existence of -optimal strategies in the imperfect monitoring case)

Let A = fa1; a2g, and S = fs1 ; s2; s3g. Let  < 0 , where 0 is defined at the end
of this proof. Assume in negation that there exists a  -optimal strategy F . Consider the
following two decision problems whose rows are indexed by the actions and whose columns
are indexed by the states:
!
2
a
2
b
2
c
D1 = a b c
242

fiDynamic Non-Bayesian Decision Making

!

2a 2b 2c

D2 =

b c a

where a; b and c are positive numbers satisfying: a > 4b > 16c. For i = 1; 2, let
Ci = (ci (a; s))a2A;s2S be the ratio matrices. That is:

C1 =

1 1 1
2 2 2

!

1 1 2ac
2a 2b 1
b c

C2 =

!

Note that a1 is the unique CR action in D1 and a2 is the unique CR action in D2.
Assume that Nature uses strategy G which randomizes at each stage with equal probabilities
(of 13 ) amongst all 3 states. Given this strategy of Nature, the agent cannot distinguish
between the two decision problems, even if he knows Nature's strategy and he is told that
one of them is chosen. This implies that if 1 and 2 are the probability measures induced
by F and G on (A  S )1 in the decision problems D1 and D2 respectively, then for every
i 2 f1; 2g, the distribution of the stochastic process (NTi )1
T =1 with respect to j , j 2 f1; 2g,
does not depend on j . That is, for every T  1








Prob1 Nti 2 Mt for all t  T = Prob2 Nti 2 Mt for all t  T ; i 2 f1; 2g (8)
for every sequence (Mt )Tt=1 with Mt  f0; 1; : : :; tg for all 1  t  T .
We do not give a complete proof of (8), rather we illustrate it by proving a representing
case. The reader can easily derive the complete proof. We show that

Prob1 (N21 = 0) = Prob2 (N21 = 0)

(9)

Indeed, for j = 1; 2,

Probj (N21 = 0) = 31

3
X

k=1

F (e)(a2)F (a2 ; uj (a2 ; sk ))(a2)

(10)

Let  : f1; 2; 3g ! f1; 2; 3g be defined by  (1) = 3,  (2) = 1, and  (3) = 2. Then

u1(a2 ; sk ) = u2 (a2; s(k) )
for every 1  k  3. Therefore (10) implies (9).
As F is  -optimal, then there exists an integer K such that with a probability of at least
1 ,  with respect to 1 , and hence with respect to 2 , NT1  (1 ,  )T for every T  K .
This implies that with a probability of at least 1 ,  , a1 is played at least at 1 ,  of the
stages up to time T , for all T  K , and in particular for T = K . We choose the integer K
to be suciently large such that according to the Law of Large Numbers, Nature chooses
243

fiMonderer and Tennenholtz

s3 in at least 31 ,  of the stages up to stage K with a probability of at least 1 ,  . Let CR2
and c2t denote CR and ct of decision problem 2, respectively. Then
a > CR = max( 2a ; 2b ):
2
2c
b c
Therefore, if At = a1, then C2(At ; St)  CR2 if and only if St =
6 s3. Hence, with a
2
probability of at least 1 , 2 , in at most  + (1 ,  )( 3 +  ) of these stages c2t  CR2.
Therefore F cannot be  -optimal, if we choose 0 such that 20 < 1 , 0 and

ut

0 + (1 , 0)( 23 + 0 ) < 1 , 0 :

5. Safety Level

For the sake of comparison we discuss in this section the safety level (known also as maxmin)
criterion. Let D =< A; S; u > be a decision problem. Denote

v = max
a min
s u(a; s)
v is called the safety level of the agent (or the maxmin value). Every action a for which
u(a; s)  v for every s is called a safety level action. We consider now the imperfect
monitoring model for the dynamic decision problem < A; S >. Every sequence of states
z = (st)1
t=1 with st 2 S for every t  1 and every pure strategy f of the agent induce
z;f 1
a sequence of actions (at)1
t=1 and a corresponding sequence of payoffs (ut )t=1 , where
z;f
uz;f
t = u(at ; st) for every t  1. Let NT denote the number of stages up to stage T at
which the agent's payoff exceeds the safety level v . That is,
NTz;f = #f1  t  T : uz;f
t  vg

(11)

We say that f is safety level optimal if for every decision problem and for every sequence of
states,
lim 1 N z;f = 1;
T !1 T T

and the convergence holds uniformly w.r.t. all payoff functions u and all sequences of states
in S . That is, for every  > 0 there exists K = K ( ) such that NTz;f  (1 ,  )T for every
T  K for every decision problem < A; S; u > and for every sequence of states z .
Proposition 5.1: Every dynamic decision problem possesses a safety level optimal strategy
in the imperfect monitoring case, and consequently in the perfect monitoring case. Moreover,
such an optimal strategy can be chosen to be strongly ecient in the sense that for every
sequence of states there exists at most nA , 1 stages at which the agent receives a payoff
smaller than his safety level, where nA denotes the number of actions.
Proof: Let n = nA . Define a strategy f as follows: Play each of the actions in
A in the first n stages. For every T  n + 1, and for every history h = hT ,1 =
244

fiDynamic Non-Bayesian Decision Making

((a1; u1); (a2; u2; : : :; (aT ,1; uT ,1)) we define f (h) 2 A as follows: For a 2 A, let vTh (a) =
min ut , where the min ranges over all stages t  T , 1 for which at = a. Define f (h) = aT ,
where aT maximizes vTh (a) over a 2 A. It is obvious that for every sequence of states
1
z = (st )1
t=1 there are at most n , 1 stages at which u(at ; st) < v , where (at )t=1 is the
z;f
sequence of actions generated by f and the sequence of states. Hence NT  T , n, where
NTz;f is defined in (11). Thus for K () = n , T1 NTz;f  1 ,  for every T  K ( ).

ut

6. Discussion

Note that all the notations established in Section 5, and Proposition 5.1 as well, remain
unchanged if we assume that the utility function u takes values in a totally pre-ordered
set without any group structure. That is, our approach to decision making is qualitative
(or ordinal). This distinguishes our work from previous work on non-Bayesian repeated
games, which used the probabilistic safety level criterion as a basic solution concept for the
one shot game15. These works, including (Blackwell, 1956; Hannan, 1957; Banos, 1968;
Megiddo, 1980), and more recently (Auer, Cesa-Bianchi, Freund, & Schapire, 1995; Hart
& Mas-Colell, 1997), used several versions of long-run solution concepts, all based on some
optimization of the average of the utility values over time. That is, in P
all of these papers
the goal is to find strategies that guarantee that with high probability T1 Tt=1 u(At; St) will
be close to vp .
Our work is, to the best of our knowledge, the first to introduce an ecient dynamic
optimal policy for the basic competitive ratio context. Our study and results in sections 2-4
can be easily adapted to the case of qualitative competitive ratio as well. In this approach,
the utility function takes values in some totally pre-ordered set G and in addition we assume
a regret function, that maps G  G to some pre-ordered set H . For g1 ; g2 2 G, (g1; g2)
is the level of regret when the agent receives the utility level g1 rather than g2. Given an
action a and a state s, the regret function will determine the maximal regret, c(a; s) 2 H
of the agent when action a is performed in state s. That is,

c(a; s) = max (u(a; s); u(b; s));
where b ranges over all actions.
The qualitative regret of action a will be the maximal regret of this action over all states.
The optimal qualitative competitive ratio will be obtained by using an action for which the
qualitative regret is minimal. Notice that no arithmetic calculations are needed (or make
any sense) for this qualitative version. Our results can be adapted to the case of qualitative
competitive ratio. For ease of exposition, however, we used the quantitative version of this
model, where a numerical utility function represents the regret function.
15. The probabilistic safety value, vp , of the agent in the decision problem D =< A; S; u > is his maxmin
value when the max ranges over all mixed actions. That is

vp = maxq2(A) mins2S

X

a2A

u(a; s)q(a);

where (A) is the set of all probability distributions q on A.

245

fiMonderer and Tennenholtz

Our work is relevant to research on reinforcement learning in AI. Work in this area,
however, has dealt mostly with Bayesian models. This makes our work complementary to
this work. We wish now to briey discuss some of the connections and differences between
our work and existing work in reinforcement learning.
The usual underlying structure in the reinforcement learning literature is that of an environment which changes as a result of an agent's action based on a particular probabilistic
function. The agent's reward may be probabilistic as well.16 In our notation, a Markov
decision process (MDP) is a repeated game against Nature with complete information and
strategic certainty, in which Nature's strategy depends probabilistically on the last action
and state chosen17 . Standard partially observable MDP (POMDP) can be described similarly by introducing a level of monitoring in between perfect and imperfect monitoring. In
addition, bandit problems can be basically modeled as repeated games against Nature with
a probabilistic structural assumption about Nature's strategy , but with strategic uncertainty about the values of the transition probabilities. For example, Nature's action can
play the role of the state of a slot machine in a basic bandit problem. The main difference between the classical problem and the problem discussed in our setting is that the
state of the slot machine may change in our setting in a totally unpredictable manner (e.g.,
the seed of the machine is manually changed at each iteration). This is not to say that
by solving our learning problem we have solved the problem of reinforcement learning in
MDP, in POMDP, or in bandit problems. In the later settings, our optimal strategy behave
poorly relative to strategies obtained in the theory of reinforcement learning, that take the
particular structure into account.
The non-Bayesian and qualitative setup call for optimality criteria which differ from
the ones used in current work in reinforcement learning. Work in reinforcement learning
discusses learning mechanisms that optimize the expected payoff in the long run. In a
qualitative setting (as described above) long-run expected payoffs may not make much
sense. Our optimality criteria expresses the need to obtain a desired behavior in most
stages. One can easily construct examples where one of these approaches is favorite to the
other one. Our emphasis is on obtaining the desired behavior in a relatively short run.
Though, most analytical results in reinforcement learning have been concerned only with
eventual convergence to desired behavior, some of the policies have been shown to be quite
ecient in practice.
In addition to the previously mentioned differences between our work and work in reinforcement learning, we wish to emphasize that much work on POMDP uses information
structures which are different from those discussed in this paper. Work on POMDP usually
assumes that some observations about the current state may be available (following the presentation by Smallwood & Sondik, 1973), although observations about the previous state
are discussed as well (Boutilier & Poole, 1996). Recall that in the case of perfect monitoring
the previous environment state is revealed, and the immediate reward is revealed in both
prefect and imperfect monitoring. It may be useful to consider also situations where some
16. The results presented in this paper can be extended to the case where there is some randomness in the
reward obtained by the agents as well.
17. Likewise, stochastic games (Shapley, 1953) can be considered as repeated games against Nature with
partial information about Nature's strategy. For that matter one should redefine the concept of state in
such games. A state is a pair (s; a), where s is a state of the system and a is an action of the opponent.

246

fiDynamic Non-Bayesian Decision Making

(partial) observations about the previous state or the current state are revealed from time
to time. How this may be used in our setting is not completely clear, and may serve as a
subject for future research.

References

Alon, N., Spencer, J., & Erdos, P. (1992). The Probabilistic Method. Wiley-Interscience.
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. (1995). Gambling in a rigged
casino: The adversial multi-armed bandit problem. In Proceedings of the 36th Annual
Symposium on Foundations of Computer Science, pp. 322{331.
Aumann, R., & Maschler, M. (1995). Repeated Games with Incomplete Information. The
MIT Press.
Banos, A. (1968). On pseudo games. The Annals of Mathematical Statistics, 39, 1932{1945.
Blackwell, D. (1956). An analog of the minimax theorem for vector payoffs. Pacific Journal
of Mathematic, 6, 1{8.
Boutilier, C., & Poole, D. (1996). Computing optimal policies for partially observable
decision processes using compact representations. In Proceedings of the 13th National
Conference on Artificial Intelligence, pp. 1168{1175.
Cassandra, A., Kaelbling, L., & Littman, M. (1994). Acting optimally in partially observable stochastic domain. In Proceedings of the 12th National Conference on Artificial
Intelligence, pp. 1023{1028.
Chernoff, H. (1952). A measure of the asymptotic eciency for tests of a hypothesis based
on the sum of observations. Annals of Mathematical Statistics, 23, 493{509.
Fudenberg, D., & Levine, D. (1997). Theory of learning in games. miemo.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Hannan, J. (1957). Approximation to bayes risk in repeated play. In Dresher, M., Tucker,
A., & Wolfe, P. (Eds.), Contributions to the Theory of Games, vol. III (Annals of
Mathematics Studies 39), pp. 97{139. Princeton University Press.
Harsanyi, J. (1967). Games with incomplete information played by bayesian players, parts
i, ii, iii. Management Science, 14, 159{182.
Hart, S., & Mas-Colell, A. (1997). A simple adaptive procedure leading to correlated
equilibrium. Discussion paper 126, Center for Rationality and Interactive Decision
Theory, Hebrew University.
Kaelbling, L., Littman, M., & Moore, A. (1996). Reinforcement learning: A survey. Journal
of Artificial Intelligence Research, 4, 237{258.
Kreps, D. (1988). Notes on the Theory of Choice. Westview press.
247

fiMonderer and Tennenholtz

Lovejoy, W. (1991). A survey of algorithmic methods for partially observed markov decision
processes. Annals of Operations Research, 28 (1{4), 47{66.
Luce, R. D., & Raiffa, H. (1957). Games and Decisions- Introduction and Critical Survey.
John Wiley and Sons.
Megiddo, N. (1980). On repeated games with incomplete information played by nonbayesian players. International Journal of Game Theory, 9, 157{167.
Mertens, J.-F., Sorin, S., & Zamir, S. (1995). Repeated games, part a. CORE, DP-9420.
Milnor, J. (1954). Games Against Nature. In Thrall, R. M., Coombs, C., & Davis, R.
(Eds.), Decision Processes. John Wiley & Sons.
Monahan, G. (1982). A survey of partially observable markov decision processes: Theory,
models and algorithms. Management Science, 28, 1{16.
Papadimitriou, C., & Yannakakis, M. (1989). Shortest Paths Without a Map. In Automata,
Languages and Programming. 16th International Colloquium Proceedings, pp. 610{
620.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: A Modern Approach. Prentice Hall.
Savage, L. (1972). The Foundations of Statistics. Dover Publications, New York.
Shapley, L. (1953). Stochastic games. Proceeding of the National Academic of Sciences
(USA), 39, 1095{1100.
Smallwood, R., & Sondik, E. (1973). The optimal control of partially observable markov
processes over a finite horizon. Operations Research, 21, 1071{1088.
Sutton, R. (1992). Special issue on reinforcement learning. Machine Learning, 8 (3{4).
Valiant, L. G. (1984). A theory of the learnable. Comm. ACM, 27 (11), 1134{1142.
Watkins, C., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning, 8 (3{4),
279{292.
Watkins, C. (1989). Learning With Delayed Rewards. Ph.D. thesis, Cambridge University.
Wellman, M., & Doyle, J. (1992). Modular utility representation for decision-theoretic
planning. In Proceedings of the first international conference on AI planning systems.
Morgan Kaufmann.
Wellman, M. (1985). Reasoning about preference models. Tech. rep. MIT/LCS/TR-340,
Laboratory for Computer Science, MIT.

248

fi