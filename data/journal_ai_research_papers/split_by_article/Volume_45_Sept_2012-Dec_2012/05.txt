Journal of Artificial Intelligence Research 45 (2012) 685-729

Submitted 08/12; published 12/12

The Time Complexity of A with Approximate Heuristics on
Multiple-Solution Search Spaces
Hang Dinh

htdinh@iusb.edu

Department of Computer & Information Sciences
Indiana University South Bend
1700 Mishawaka Ave. P.O. Box 7111
South Bend, IN 46634 USA

Hieu Dinh

hieu.dinh@mathworks.com

MathWorks
3 Apple Hill Drive
Natick, MA 01760-2098 USA

Laurent Michel
Alexander Russell

ldm@engr.uconn.edu
acr@cse.uconn.edu

Department of Computer Science & Engineering
University of Connecticut
371 Fairfield Way, Unit 2155
Storrs, CT 06269-2155 USA

Abstract


We study the behavior of the A search algorithm when coupled with a heuristic h satisfying
(1  1 )h  h  (1 + 2 )h , where 1 , 2  [0, 1) are small constants and h denotes the optimal
cost to a solution. We prove a rigorous, general upper bound on the time complexity of A search
on trees that depends on both the accuracy of the heuristic and the distribution of solutions. Our
upper bound is essentially tight in the worst case; in fact, we show nearly matching lower bounds
that are attained even by non-adversarially chosen solution sets induced by a simple stochastic
model. A consequence of our rigorous results is that the effective branching factor of the search
will be reduced as long as 1 + 2 < 1 and the number of near-optimal solutions in the search tree
is not too large. We go on to provide an upper bound for A search on graphs and in this context
establish a bound on running time determined by the spectrum of the graph.
We then experimentally explore to what extent our rigorous upper bounds predict the behavior
of A in some natural, combinatorially-rich search spaces. We begin by applying A to solve the
knapsack problem with near-accurate admissible heuristics constructed from an efficient approximation algorithm for this problem. We additionally apply our analysis of A search for the partial
Latin square problem, where we can provide quite exact analytic bounds on the number of nearoptimal solutions. These results demonstrate a dramatic reduction in effective branching factor of
A when coupled with near-accurate heuristics in search spaces with suitably sparse solution sets.

1. Introduction
The classical A search procedure (Hart, Nilson, & Raphael, 1968) is a method for bringing heuristic
information to bear on a natural class of search problems. One of A s celebrated features is that
when coupled with an admissible heuristic function, that is, one that always returns a lower bound
on the distance to a solution, A is guaranteed to find an optimal solution. While the worst-case
behavior of A (even with an admissible heuristic function) is no better than that of, say, breadthfirst search, both practice and intuition suggest that availability of an accurate heuristic should
decrease the running time. Indeed, methods for computing accurate admissible heuristic functions
for various search problems have been presented in the literature (see, e.g., Felner, Korf, & Hanan,
2004). In this article, we investigate the effect of such accuracy on the running time of A search;
2012 AI Access Foundation. All rights reserved.

fiDinh, Dinh, Michel, & Russell

specifically, we focus on rigorous estimates for the running time of A when coupled with accurate
heuristics.
The initial notion of accuracy we adopt is motivated by the standard framework of approximation algorithms: if f () is a hard combinatorial optimization problem (e.g., the permanent of
a matrix, the value of an Euclidean traveling salesman problem, etc.), an algorithm A is an efficient -approximation to f if A runs in polynomial time and (1  )f (x)  A(x)  (1 + )f (x),
for all inputs x, where f (x) is the optimal solution cost for input x and A(x) is the solution cost
returned by algorithm A on input x. The approximation algorithms community has developed
efficient approximation algorithms for a wide swath of NP-hard combinatorial optimization problems and, in some cases, provided dramatic lower bounds asserting that various problems cannot be
approximated beyond certain thresholds (see Vazirani, 2001; Hochbaum, 1996, for surveys of this
literature). Considering the great multiplicity of problems that have been successfully addressed in
this way (including problems believed to be far outside of NP, like matrix permanent), it is natural to
study the behavior of A when coupled with a heuristic function possessing such properties. Indeed,
in some interesting cases (e.g., Euclidean travelling salesman, matrix permanent, knapsack), hard
combinatorial problems can be approximated in polynomial time to within any fixed constant  > 0;
in these cases, the polynomial depends on the constant . We remark, also, that many celebrated
approximation algorithms with provable performance guarantees proceed by iterative update methods coupled with bounds on the local change of the objective value (e.g., basis reduction in Lenstra,
Lenstra, & Lovasz, 1981, and typical primal-dual methods in Vazirani, 2002).
Encouraged both by the possibility of utilizing such heuristics in practice and the natural question
of understanding the structural properties of heuristics (and search spaces) that indeed guarantee
palatable performance on the part of A , we study the behavior of A when provided with a heuristic
function that is an -approximation to the cost of a cheapest path to a solution. As certain natural
situations arise where approximation quality is asymmetric (i.e., the case of an admissible heuristic),
we slightly refine the notion of accuracy by distinguishing the multiplicative factors in the two sides
of an approximation: we say that a heuristic h is an (1 , 2 )-approximation to the actual cost
function h , or simply (1 , 2 )-approximate, if (1  1 )h  h  (1 + 2 )h . In particular, admissible
heuristics with -approximation are (, 0)-approximate. We will call a heuristic -accurate if it is
(1 , 2 )-approximate and  = 1 + 2 . A detailed description appears in Section 2.1.
1.1 A Sketch of the Results
We initially model our search space as an infinite b-ary tree with a distinguished root. A problem
instance is determined by a set S of nodes of the treethe solutions to the problem. The cost
associated with a solution s  S is simply its depth. The search procedure is equipped with (i.) an
oracle which, given a node n, determines if n  S, and (ii.) an heuristic function h, which assigns
to each node n of the tree an estimate of the actual length h (n) of the shortest (descending) path
to a solution. Let S be a solution set in which the first (and hence optimal) solution appears at
depth d. We establish a family of upper bounds on the number of nodes expanded by A : if h is an
(1 , 2 )-approximation of h , then A finds a solution of cost no worse than (1 + 2 )d and expands
no more than 2b(1 +2 )d + dN1 +2 nodes, where N denotes the number of solutions at depth less
than (1 + )d. See Lemma 3.1 below for stronger results. We emphasize that this bound applies to
any solution space and can be generalized to search models with non-uniform branching factors and
non-uniform edge costs (see Section 5).
We go on to show that this upper bound is essentially tight; in fact, we show that the bound is
nearly achieved even by non-adversarially determined solution spaces selected according to a simple
stochastic rule (see Theorems 3.1 and 4.1.). We remark that these bounds on running time fall off
rapidly as the accuracy of the heuristics increases, as long as the number of near-optimal solutions
is not too large (although it may grow exponentially). For instance, the effective branching factor
of A guided by an admissible -accurate heuristic will be reduced to b if N = O(bd ). However,
686

fiThe Time Complexity of A with Approximate Heuristics

in the worst cases, which occur when the search space has an overwhelming number of near-optimal
solutions, A still has to expand almost as many nodes as brute-force does, regardless of heuristic
accuracy. Likewise, strong guarantees on  < 1 are, in general, necessary to effect appreciable
changes in average branching factor. This is discussed in Theorem 4.2.
After establishing bounds for the tree-based search model, we examine the time complexity of
A on a graph by unrolling the graph into an equivalent tree and then bounding the number of
near-optimal solutions in the tree which are a lift of a solution in the original graph. This appears
in Section 6. Using spectral graph theory, we show that the number N of lifted solutions on the
tree corresponding to a b-regular graph G is O((1+)d ), assuming the optimal solution depth d is
O(logb |G|) and the number solutions in G is constant, where  is the second largest eigenvalue (in
absolute value) of the adjacency matrix of G. In particular,for almost all b-regular graphs in which
b does not grow with the size of graphs, we have   2 b, which yields the effective branching
factor of A search on such graphs is roughly at most 8b(1+)/2 if the heuristic is -accurate. We
also experimentally evaluate these heuristics.
Experimental Results and the Relationship to A in Practice. Of course, these upper
bounds are most interesting if they reflect the behavior of search problems in practice. The bounds
above guarantee, in general, that E, the number of nodes expanded by A with a -accurate heuristic,
satisfies
E 2bd + dN .
Under the plausible condition that N  bd , we have simply E  cbd node expansions for a
constant c that does not depend on  (c may depend on k and/or other properties of the search
space). This suggests the hypothesis that for hard combinatorial problems with suitably sparse
near-optimal solutions,
E  cbd

or, equivalently,

log E  log c + d log b .

(1)

In particular, this suggests a linear dependence of log E on .
To explore this hypothesis, we conducted a battery of experiments on the natural search-tree
presentation of the well-studied Knapsack Problem. Here we obtain an admissible -accurate heuristic by applying the Fully Polynomial Time Approximation Scheme (FPTAS) for the problem due
to the work of Ibarra and Kim (1975) (see also Vazirani, 2001, p. 70), which provides us with a
convenient method for varying  without changing the other parameters of the search. We remark
that the natural search space for the problem is a quite irregular edge-weighted directed graph on
which A can avoid reopening any node. Thus, this search space is equivalent to one of its spanning
subtrees in terms of A s behaviors. In order to focus on computationally nontrivial examples, we
generate Knapsack instances from distributions that are empirically hard for the best known exact
algorithms (Pisinger, 2005). The results of these experiments yield remarkably linear behavior (of
log E as a function of ) for a quite wide window of values: indeed, our tests yield R2 correlation
coefficients (of the least-square linear regression model) in excess of 90% with  in the range (.5, 1)
for most Knapsack instances. See Section 7.1 for details.
While the experimental results discussed above for the Knapsack problem support the linear
scaling of (1), several actual parameters of the search are unknown: for example, we cannot rule
out the possibility that the approximation algorithm, when asked to produce an -approximation,
does not in fact produce a significantly better approximation. While this seems far-fetched, such
behavior could provide spurious evidence for linear scaling. To explore the hypothesis in more
detail, we additionally explore a more artificial search space for the partial Latin square completion
(PLS) problem in which we can provide precise control of  (and, in fact, N ). The PLS problem is
featured in a number of benchmarks for local search and complete search methods. Roughly, this
is the problem of finding an assignment of values to the empty cells of a partially filled n  n table
so that each row and column in the completed table is a permutation of the set {1, . . . , n}. In our
formulation of the problem, the search space is a 2n-regular graph, thus the brute-force branching
687

fiDinh, Dinh, Michel, & Russell

factor is 2n. On this search space, by controlling N , we prove an asymptotic upper bound of

(1 + ) (1 + 1/) n on the effective branching factor of A coupled with any -accurate heuristic.
We also experimentally evaluate the effective branching factor of A with the admissible -accurate
heuristic (1)h , with which A expands more nodes than with any admissible -accurate heuristic
strictly larger than (1  )h .
We remark that while the PLS problem itself is well-studied and natural, we invent specific search
space structure on the problem that allows us to analytically control the number of near-optimal
solutions. Unlike the Knapsack problem, where we can construct an efficient admissible -accurate
heuristic for every fixed  thanks to the given FPTAS, known approximation algorithms for the PLS
problem are much weakerthey provide approximations for specific constants (1/e). To avoid this
hurdle, we construct instances of PLS with known solution, from which we extract the heuristics
(1  )h . Despite these planted solutions and contrived heuristics, the infrastructure provides
an example of a combinatorially rich search space with known solution multiplicity and a heuristic
of known quality, and so provides a means for experimentally measuring the relationship between
heuristic accuracy and running time. Our empirical data results in remarkable agreement with the
theoretical upper bounds. More subtly, by empirically analyzing the linear dependence of log E on
, we see that the effective branching factor of A using the heuristic (1  )h on the given PLS
search space is roughly (2n)0.8 ; see Section 7.2.
As far as we are aware, these are the first experimental results that explore the relationship
between  and E. Understanding heuristic accuracy and solution space structure in general (and
the ensuing bounds on A running time) for problems and heuristics of practical interest remains
an intriguing open problem. We remark that for problems such as the (n2  1)-puzzle, which have
been extensively used as test cases for A , it seems difficult to find heuristics with accuracy sufficient
to significantly reduce average branching factor. The best rigorous algorithms can only give rather
large constant guarantees (Ratner & Warmuth, 1990; Parberry, 1995): in particular, Parberry (1995)
shows that one can quickly compute solutions (and hence approximate heuristics) that are no more
than a factor 19 worse than optimal; the situation is somewhat better for random instances, where
he establishes a 7.5-factor. See Demaines (2001) work for a general discussion.
Observe that any search algorithm not privy to heuristic information requires (bd ) running
time, in general, to find a solution. High probability statements of the same kind can be made if the
solution space is selected from a sufficiently rich family. Such pessimistic lower bounds exist even
in situations where the search space is highly structured (Aaronson, 2004). Our results suggest that
accurate heuristic information can have a dramatic impact on A search, even in face of substantial
solution multiplicity.
This article expands the conference article (Dinh, Russell, & Su, 2007) where the complexity of
A with an -approximate heuristic function was studied over trees. In this article, we generalize this
to asymmetric approximation, develop analogous bounds over general search spaces, establishing a
connection to algebraic graph theory, and report on a battery of supporting experimental results.
1.2 Motivation and Related Work
The A algorithm has been the subject of an enormous body of literature, often investigating its
behavior in relation to a specific heuristic and search problem combination, (e.g., Zahavi, Felner,
Schaeffer, & Sturtevant, 2007; Sen, Bagchi, & Zhang, 2004; Korf & Reid, 1998; Korf, Reid, &
Edelkamp, 2001; Helmert & Roger, 2008). Both space complexity (Korf, 1985) and time complexity
have been addressed at various levels of abstraction. Abstract formulations, involving accuracy
guarantees like those we consider, have been studied, but only in tree models where the search
space possesses a single solution. In this single solution framework, Gaschnig (1979) has given
exponential lower bounds of (bd ) on the time complexity for admissible -accurate heuristics, where
def
b = b/(2)  b (see also Pearl, 1984, p. 180), while Pohl (1977) has studied more restrictive
(additive) approximation guarantees on h which result in linear time complexity. Average-case
688

fiThe Time Complexity of A with Approximate Heuristics

analysis of A based on probabilistic accuracy of heuristics has also been given for single-solution
search spaces (Huyn, Dechter, & Pearl, 1980). These previous analysis suggested that the effect of
heuristic functions would reduce the effective branching factor of the search, which is consistent with
our results when applied to the single-solution model (the special case when N = 1 for all  > 0).
The single solution model, however, appears to be an inappropriate abstraction of most search
problems featuring multiple solutions, as it has been recognized that . . . the presence of multiple
solutions may significantly deteriorate A s ability to benefit from improved precision. (Pearl, 1984,
p. 192) (emphasis added).
The problem of understanding the time complexity in terms of structural properties of h on
multiple-solution spaces has been studied by Korf and Reid (1998), Korf et al. (2001), and Korf
(2000), using an estimate based on the distribution of h() values. In particular, they studied an
abstract search space given by a b-ary tree and concluded that the effect of a heuristic function is
to reduce the effective depth of a search rather than the effective branching factor (Korf & Reid,
1998; Korf et al., 2001). For the case of accurate heuristics with controlled solution multiplicity, this
conclusion directly contradicts our findings, which indicate dramatic reduction in effective branching
factor for such cases. To explain this discrepancy, we observe that their analysis relies on an equilibrium assumption that fails for accurate heuristics (in fact, it fails even for much weaker heuristic
guarantees, such as h(v)  h (v) for small  > 0). The basic structure of their argument, however,
can be naturally adapted to the case of accurate heuristics, in which case it yields a reduction in
effective branching factor. We give a detailed discussion in Section 8.
As a follow-up to Korf and Reid (1998), Korf et al. (2001), and Korfs (2000) work, Edelkamp
(2001) examined A (indeed, IDA ) on undirected graphs, relying on the equilibrium assumption.
Edelkamps new technique is the use of graph spectrum to estimate the number n(`) of nodes at
certain depth ` in the brute-force search tree (same as our cover tree). However, unlike our spectral
analysis, which is of the original search graph G, Edelkamp analyzed the spectrum of a related
equivalence graph, which has quite different structural properties. Specifically, Edelkamp found
that the asymptotic branching factor, defined by the ratio n(`) /n(`1) for large `, equals the largest
eigenvalue of the adjacency matrix of the equivalence graph for certain Puzzle problems. To compare,
our spectral analysis depends on the second largest eigenvalue of the adjacency matrix AG of the
original search graph G, while the largest eigenvalue of AG always equals the branching factor,
assuming G is regular.
Additionally, the analyses of Korf and Reid (1998), Korf et al. (2001), and Korf (2000) (and
therefore, of Edelkamp, 2001) focus on a particular subclass of admissible heuristics, called consistent
heuristics. We remark that the heuristics used in our experiments for the Knapsack problem are
admissible but likely inconsistent. Zhang, Sturtevant, Holte, Schaeffer, and Felner (2009) and Zahavi
et al. (2007) discuss usages of inconsistent heuristics in practice.
Our work below explores both worst-case and average-case time complexity of A search on
both trees and graphs with multiple solutions when coupled with heuristics possessing accuracy
guarantees. We make no assumptions regarding consistency or admissibility of the heuristics, though
several of our results can be naturally specialized to this case. In addition to studying the effect of
heuristic accuracy, our results also shed light on the sensitivity of A to the distribution of solutions
and the combinatorial structure of the underlying search spaces (e.g., graph eigenvalues, which
measure, among other things, the extent of connectedness for graphs). As far as we are aware, these
are the first rigorous results combining search space structure and heuristic accuracy in a single
framework for predicting the behavior of A .

2. Preliminaries
A typical search problem is defined by a search graph with a starting node and a set of goal nodes
called solutions. Any instance of A search on a graph, however, can be simulated by A search on
a cover tree without reducing running time; this is discussed in Section 6.1. Since the number of
689

fiDinh, Dinh, Michel, & Russell

expansions on the cover tree of a graph is larger than or equal to that on the original graph, it is
sufficient to upper bound the running time of A search on the cover tree. With this justification,
we begin with considering the A algorithm for search problems on a rooted tree.
Problem Definition and Notations. Let T be a tree representing an infinite search space, and
let r denote the root of T . For convenience, we also use the symbol T to denote the set of vertices
in the tree T . Solutions are specified by a nonempty subset S  T of nodes in T . Each edge on T
is assigned a positive number called the edge cost. For each vertex v in T , let
 SubTree(v) denote the subtree of T rooted at v,
 Path(v) denote the path in T from root r to v,
 g(v) denote the total (edge) cost of Path(v), and
 h (v) denote the cost of the least costly path from v to a solution in SubTree(v). (We write
h (v) =  if no such solution exists.)
The objective value of this search problem is h (r), the cost of the cheapest path from the root r
to a solution. The cost of a solution s  S is the value of g(s). A solution of cost equal to h (r) is
referred to as optimal.
The A algorithm is a best-first search employing an additive evaluation function f (v) = g(v) +
h(v), where h is a function on T that heuristically estimates the actual cost h . Given a heuristic
function h : T  [0, ], the A algorithm using h for our defined search problem on the tree T is
described as follows:
Algorithm 1 A search on a tree
1. Initialize Open := {r}.
2. Repeat until Open is empty:
(a) Remove from Open a node v at which the function f = g + h is minimum.
(b) If v is a solution, exit with success and return v.
(c) Otherwise, expand node v, adding all its children in T to Open.
3. Exit with failure.
It is known (e.g., Dechter & Pearl, 1985, Lemma 2) that at any time before A terminates, there
is always a vertex v present in Open such that v lies on a solution path and f (v)  M , where M is
the min-max value defined as follows:


def
M = min
max f (u) .
(2)
sS

uPath(s)

This fact leads to the following node expansion conditions:
 Any vertex v expanded by A (with heuristic h) must have f (v)  M . (cf., Dechter & Pearl,
1985, Thm. 3). We say that a vertex v satisfying f (v)  M is potentially expanded by A .
 Any vertex v with
max

f (u) < M

uPath(v)

must be expanded by A (with heuristic h) (cf., Dechter & Pearl, 1985, Thm. 5). In particular,
when the function f monotonically increases along the path from the root r to v, the node v
must be expanded if f (v) < M .
690

fiThe Time Complexity of A with Approximate Heuristics

The value of M will be obtained on the solution path with which A search terminates (Dechter &
Pearl, 1985, Lemma 3), which implies that M is an upper bound for the cost of the solution found
by the A search.
We remark that if h is a reasonable approximation to h along the path to the optimal solution,
this immediately provides some control on M . In particular:
Proposition 2.1. (See also Davis, Bramanti-Gregor, & Wang, 1988) Suppose that for some   1,
h(v)  h (v) for all vertices v lying on an optimal solution path; then M  h (r).
Proof. Let s be an optimal solution. For all v  Path(s),
f (v)  g(v) + h (v) = g(v) + (g(s)  g(v))  g(s) .
Hence M 

max

f (v)  g(s) = h (r).

vPath(s)

In particular, M = h (r) if the heuristic function satisfies h(v)  h (v) for all v  T , in which
case the heuristic function is called admissible. The observation above recovers the fact that A
always finds an optimal solution when coupled with an admissible heuristic function (cf., Pearl,
1984, Thm. 2, 3.1). Admissible heuristics also possess a natural dominance property (Pearl, 1984,
Thm. 7, p. 81): for any admissible heuristic functions h1 and h2 on T , if h1 is more informed than
h2 , i.e., h1 (v) > h2 (v) for all v  T \S, then A using h1 dominates A using h2 , i.e., every node
expanded by A using h1 is also expanded by A using h2 .
2.1 Approximate Heuristics
Recall from the introduction that we shall focus on heuristics providing an (1 , 2 )-approximation to
the actual optimal cost to reach a solution:
Definition. Let 1 , 2  [0, 1]. A heuristic function h is called (1 , 2 )-approximate if
(1  1 )h (v)  h(v)  (1 + 2 )h (v)

for all v  T .

An (1 , 2 )-approximate heuristic is simply called -approximate if both 1   and 2  . If a
heuristic function h is (1 , 2 )-approximate, we shall say that h has a heuristic error 1 + 2 , or h is
(1 + 2 )-accurate.
As we will see below, these two approximation factors control the performance of A search
in rather different ways: while 1 only effects the running time of A , 2 has impact on both the
running time and the quality of the solution found by A . Particularly, the special case 2 = 0
corresponds to admissible heuristics, with which A always finds an optimal solution. In general, by
Proposition 2.1, we have:
Fact 1. If h is (1 , 2 )-approximate, then M  (1 + 2 )h (r).
Hence, the solution found by A using an (1 , 2 )-approximate heuristic must have cost no more
than (1 + 2 )h (r) and thus exceeds the optimal cost by no more than a multiplicative factor equal
2 .
Definition. Let   0. A solution of cost less than (1 + )h (r) is called a -optimal solution.
Assumptions. To simplify the analysis for now, we assume that the search tree T is b-ary and
that every edge is of unit cost unless otherwise specified. In this case, the cost g(v) is simply the
depth of node v in T and h (v) is the shortest distance from v to a solution that is a descendant of v.
Throughout, the parameters b  2 (the branching factor of the search space) and 1  (0, 1], 2  [0, 1]
(the quality of the approximation provided by the heuristic function) are fixed. We rule out the case
1 = 0 for simplicity.
691

fiDinh, Dinh, Michel, & Russell

3. Upper Bounds on Running Time of A on Trees
We are now going to establish upper bounds on the running time of A search on the tree model.
We will first show a generic upper bound that applies to any solution space. We then apply this
generic upper bound to a natural stochastic solution space model.
3.1 A Generic Upper Bound
As mentioned in the introduction, we begin with an upper bound on the time complexity of A
search depending only on the weight distribution of the solution set, in addition to the heuristics
approximation factors. We shall, in fact, upper bound the number of potentially expanded nodes,
which is clearly an upper bound on the number of nodes actually expanded by A :
Lemma 3.1. Let S be a solution set whose optimal solutions lie at depth d. Then, for every   0,
the number of nodes expanded by A search on the tree T with an (1 , 2 )-approximate heuristic is
no more than
2b(1 +2 +1)d + (1  1 )dN1 +2
nodes, where N is the number of -optimal solutions.
The presence of the independent parameter  offers a flexible way to apply the upper bound in
Lemma 3.1. In particular, applying Lemma 3.1 with  = 1 and using the fact that 1  1  1, we
arrive at the upper bound of 2b(1 +2 )d + dN1 +2 mentioned in the introduction. This bound works
best when1 N1 +2 = (b(1 +2 )d ). In general, if N1 +2 = O(b(1 +2 )d ), we should choose the least
  1 for which N1 +2 = O(b(1 +2 )d ). In the opposite case, if N1 +2 = (b(1 +2 +c)d ) for some
positive constant c  1  1 , we can obtain a better bound by choosing  = 1  c/(1  1 ) < 1, since
N1 +2 dominates both terms (b(1 +2 +1)d ) and N1 +2 given such a choice of .
Proof of Lemma 3.1. Let d = h (r) and let  = 1 + 2 . Consider a node v which does not lie on
any path from the root to a -optimal solution, so that h (v)  (1 + )d  g(v). Then
f (v)  g(v) + (1  1 )[(1 + )d  g(v)] = (1  1 )(1 + )d + 1 g(v) .
Recall that a node is potentially expanded by A if its f -value is less than or equal to M . Since
M  (1 + 2 )d, the node v will not be potentially expanded if
(1  1 )(1 + )d + 1 g(v) > (1 + 2 )d .

(3)

Since 1 > 0, the inequality (3) is equivalent to
g(v) > (2 /1  /1 + 1 + )d = (1 + 2 + 1  )d .
In other words, any node at depths in the range

(1 + 2 + 1  )d, (1 + 2 )d
can be potentially expanded only when it lies on the path from the root to some -optimal solution.
On the other hand, on each -optimal
solution path, there are at most (1  1 )d nodes at depths

in (1 + 2 + 1  )d, (1 + 2 )d . Pessimistically assuming that all nodes with depth no more than
(1 + 2 + 1  )d are potentially expanded in addition to those on paths to -optimal solutions
P`
yields the statement of the lemma. (Note that as b  2, i=0 bi  2b` and that every potentially
expanded node v must have depth g(v)  f (v)  M  (1 + 2 )d.)
1. Recall some asymptotic notations: f (n) = (g(n)) means there exist constants c1 , c2 > 0 such that c1 g(n) 
f (n)  c2 g(n) for sufficiently large n; f (n) = (g(n)) means there exists a constant c > 0 such that cg(n)  f (n)
for sufficiently large n.

692

fiThe Time Complexity of A with Approximate Heuristics

3.2 An Upper Bound on a Natural Search Space Model
While actual time complexity will depend, of course, on the precise structure of S and h, we show
below that this bound is essentially tight for a rich family of solution spaces. We consider a sequence
of search problems of increasing difficulty, expressed in terms of the depth d of the optimal solution.
A Stochastic Solution Space Model. For a parameter p  [0, 1], consider the solution set S
which is obtained by independently placing each node of T into S with probability p. In this
setting, S is a random variable and is written Sp . When solutions are distributed according to Sp ,
observe that the expected number of solutions at depth d is precisely pbd and that when p = bd an
optimal solution lies at depth d with constant probability. For this reason, we focus on the specific
values pd = bd and consider the solution set Spd for each d > 0. Recall that under this model, it
is likely for the optimal solutions to lie at depth d and, more generally, we can see that with very
high probability the optimal solutions of any particular subtree will be located near depth d (with
respect to the root of the subtree). We make this precise below.
Lemma 3.2. Suppose the solutions are distributed according to Spk . Then for any node v  T and
t > 0,
td
1  2btd  Pr[h (v) > t]  eb .
Pt
Proof. In the tree SubTree(v), there are n = i=0 bi = (bt+1  1)/(b  1) nodes at depths t or less,
so Pr[h (v) > t] = (1  bd )n . We have

1  nbd  (1  bd )n  exp nbd .
The first inequality is obtained by applying Bernoullis inequality, and the last one is implied from
the fact that 1  x  ex for all x. Observing that
bt 

bt+1  1
 2bt
b1

for b  2 completes the proof.
Observe that in the Spd model, conditioned on the likely event that the optimal solutions appear
at depth d, the expected number of -optimal solutions is (bd ). In this situation, according to
Lemma 3.1, A expands no more than O(b(1 +2 +1)d ) + O(db(1 +2 )d ) vertices in expectation,
for any   0. The leading exponential term in this bound is equal to
max {(1 + 2 + 1  )d, (1 + 2 )d} ,
which is minimal when  = 1. This suggests the best upper bound that can be inferred from the
family of bounds in Lemma 3.1 is poly(d)b(1 +2 )d (for Spd ).
Before discussing the average-case time complexity of A search, we record the following wellknown Chernoff bound, which will be used to control the tail bounds in our analysis later.
Lemma 3.3 (Chernoff bound, Chernoff, 1952). Let Z be the sum of mutually independent indicator
random variables with expected value  = E [Z]. Then for any  > 0,


e
Pr[Z > (1 + )] <
.
(1 + )1+
A detailed proof can be found in the book of Motwani and Raghavan (1995). In several cases
below, while we do not know exactly the expected value of the variable to which we wish to apply
the tail bound in Lemma 3.3, we can compute sufficiently good upper bounds on the expected value.
In order P
to apply the Chernoff
in such a case, we actually require a monotonicity argument:
Pbound
n
n
0
0
If Z =
X
and
Z
=
X
i=1 i
i=1 i are sums of independent and identically distributed (i.i.d.)
indicator random variables so that E [Xi ]  E [Xi0 ], then Pr[Z > ]  Pr[Z 0 > ] for all . With this
argument and by applying Lemma 3.3 for  = e  1, we obtain:
693

fiDinh, Dinh, Michel, & Russell

Corollary 3.1. Let Z be the sum of n i.i.d. indicator random variables so that E [Z]    n, then
Pr[Z > e] < e .
Adopting the search space whose solutions are distributed according to Spd , we are ready to
bound the running time of A on average when guided by an (1 , 2 )-approximate heuristic:
3

Theorem 3.1. Let d be sufficiently large. With probability at least 1  ed  e2d , A search on the
tree T using an (1 , 2 )-approximate heuristic function expands no more than 12d4 b(1 +2 )d vertices
when solutions are distributed according to the random variable Spd .
Proof. Let X be the random variable equal to the total number of nodes expanded by the A with
an (1 , 2 )-approximate heuristic. Of course the exact value of, say, E [X] depends on h; we will prove
upper bounds achieved with high probability for any (1 , 2 )-approximate h. Applying Lemma 3.1
with  = 1, we conclude

X  2b(1 +2 )h (r) + (1  1 )h (r)N1 +2 .
Thus it suffices to control both h (r) and the number N1 +2 of (1 + 2 )-optimal solutions.
We will utilize the fact that in the Spd model, the optimal solutions are unlikely to be located
far from depth d. To this end, let Efar be the event that h (r) > d +  for some  < d to be set

later. Lemma 3.2 immediately gives Pr[Efar ]  eb .
Observe that conditioned on Efar , we have h (r)  d+ and N1 +2  Z, where Z is the random
variable equal to the number of solutions with depth no more than (1 + 1 + 2 )(d + ). We have
d
(1+1 +2 )(d+)
= 2b(1 +2 )d+(1+1 +2 ) < 2b(1 +2 )d+3
E[Z]  b  2b

and, applying the Chernoff bound in Corollary 3.1 to control Z,
h
i


3
Pr Z > 2eb(1 +2 )d+3  exp 2b(1 +2 )d+3  e2b .
Letting Ethick be the event that Z  6b(1 +2 )d+3 , observe
h
i
3
Pr[Ethick ]  Pr Z > 2eb(1 +2 )d+3  e2b .
To summarize: when neither Efar nor Ethick occurs,
X  2b(1 +2 )(d+) + (1  1 )(d + )6b(1 +2 )d+3
 6(d + )b(1 +2 )d+3
 12db(1 +2 )d+3 .
Hence,
h
i

3
Pr X > 12db(1 +2 )d+3  Pr[Efar  Ethick ]  eb + e2b .
To infer the bound stated in our theorem, set b = d so that b(1 +2 )d+3 = d3 b(1 +2 )d , completing
the proof.
Remark By similar methods, other trade-offs between the error probability and the resulting bound
on the number of expanded nodes can be obtained.
694

fiThe Time Complexity of A with Approximate Heuristics

4. Lower Bounds on Running Time of A on Trees

We establish that the upper bounds in Theorem 3.1 are tight to within a O(1/ d) term in the
exponent. We begin by recording the following easy fact about solution distances in this discrete
model.
Fact 2. Let   h (r) be a nonnegative integer. Then for every solution s, there is a node v 
Path(s) such that h (v) = .
Proof. Fix a distance   h (r). We will prove the lemma by induction on the depth of solutions.
The lemma clearly holds for optimal solutions. Consider a solution s which may not be optimal,
and let v  Path(s) be the node which is  level far from s so that h (v)  . If h (v) < , there
must be another solution s0  SubTree(v) that is closer to v. By the induction assumption, there
is a node v 0  Path(s0 ) with h (v 0 ) = . This node v 0 must be an ancestor of v, since the distance
between v and s0 is less than  while the distance between v 0 and s0 is at least , completing the
proof.
We proceed now to the lower bound.
Theorem 4.1.Let d be sufficiently large. For solutions distributed according to Spd , with probability
at least 1  b d , there exists an (1 , 2 )-approximate heuristic function 
h so that the number of
vertices expanded by A search on the tree T using h is at least b(1 +2 )d4 d /8.
Proof. Our plan is to define a pathological heuristic function that forces A to expand as many
nodes as possible. Note that the heuristic function here is allowed to overestimate h . Intuitively,
we wish to construct a heuristic function that overestimates h at nodes close to a solution and
underestimates h at nodes far from solutions, leading A astray whenever possible. Recall that for
every vertex v, it is likely to have a solution lying at depth d of SubTree(v). Thus we can use the
quantity h (v)  d   to formalize the intuitive notion that the node v is close to a solution, where
the quantity  < d will be determined later. Our heuristic function h is formally defined as follows:
(
(1 + 2 )h (v) if h (v)  d  ,
h(v) =
(1  1 )h (v) otherwise.
Observe that the chance for a node to be overestimated is small since, by Lemma 3.2,
Pr[v is overestimated] = Pr[h (v)  d  ]  2b

(4)

for any node v. Also note that if a node v does not have any overestimated ancestor, then the f
values will monotonically increase along the path from root to v.
Naturally, we also wish to ensure that the optimal solution is not too close to the root. Let Eclose
be the event that h (r)  d  . Again by Lemma 3.2,
Pr[Eclose ]  2b .
We then will see that conditioned on the event Eclose , which means h (r) > d  , every
solution will be obscured by an overestimated node that is not too close to a solution. Concretely,
up to issues of integrality, Fact 2 asserts that for every solution s, there must be a node v on the
path from the root to s with h (v) = d  , as long as d   < h (r).
Assume Eclose : then whenever h (v) = d  , we have g(v)  h (r)  (d  ) > 0 and h(v) =
(1 + 2 )(d  ), and thus f (v) > (1 + 2 )(d  ). Since every solution is obscured by some
overestimated node whose f value is larger than (1 + 2 )(d  ), we have M > (1 + 2 )(d  ), where
M is the min-max value defined in (2). It follows that a node v must be expanded if Path(v) does
695

fiDinh, Dinh, Michel, & Russell

not contain any overestimated node and f (v)  (1 + 2 )(d  ). When Path(v) does not contain
an overestimated node, we have f (v) = g(v) + (1  1 )h (v), so
f (v)  (1 + 2 )(d  )  (1  1 )h (v)  (1 + 2 )(d  )  g(v) ,
since 1 < 1. Therefore, we say a node v is required if there is no overestimated node in Path(v) and
(1  1 )h (v)  (1 + 2 )(d  )  g(v). To recap, conditioned on Eclose , the set of required nodes is
a subset of the set of nodes expanded by A search using our defined heuristic function. We will use
the Chernoff bound to control the size of R` which denotes the set of non-required nodes at depth
`.
Let v be a node at depth ` < (1 + 2 )d. Equation (4) implies
Pr[ an overestimated node in Path(v)]  2`b < 1/16 .
The last inequality holds for sufficiently large d, as long as  = poly(d). On the other hand, if
1 < 1, we have




(1 + 2 )(d  )  `
Pr v  R` = Pr h (v) >
1  1


(1+2 )(d)`
d
11
 exp b
(by Lemma 3.2)


(1 +2 )d(1+2 )`
11
.
(5)
= exp b
Now set ` = (1 + 2 )d  (1 + 2 )  logb 4. Then Equation (5) implies
 logd 4 


Pr v  R`  exp b 11  e4  1/16 .
In the case 1 = 1, the event (1  1 )h (v) > (1 + 2)(d  )  ` never
given the value
fi
fi happens
of ` that has been set. Hence, in any case, Pr v  R`  1/8 so that E fiR` fi  b` /8. Applying the
Chernoff bound in Corollary 3.1 again yields
fi fi

Pr fiR` fi > eb` /8  exp(b` /8) .
fi fi
Let Ethin be the event that fiR` fi  b` /2. Since b` /2 > eb` /8,
Pr[Ethin ]  exp(b` /8) .
Putting the pieces together, we have


`
Pr A expands less than b` /2 nodes  Pr[Eclose  Ethin ]  2b + eb /8 .


Setting  = 2 d we have ` = (1 + 2 )d  2(1 + 2 ) d  logb 4, and thus
h
i


Pr A expands less than b(1 +2 )d4 d /8 nodes  b d
for sufficiently large d.
For contrast, we now explore the behavior of A with an adversarially selected solution set; this
achieves a lower bound which is nearly tight (in comparison with the general upper bound on the
worst-case running time of A obtained by setting  = 0 in the bound of Lemma 3.1 above).
696

fiThe Time Complexity of A with Approximate Heuristics

Theorem 4.2. For any d > 1, there exists a solution set S whose optimal solutions lie at depth d
and an (1 , 2 )-approximate heuristic function h such that the A on the tree T using h expands at
least b(1+2 )d12 /1 nodes.
Proof. Consider a solution set S in which all 2 -optimal solutions share an ancestor u lying at depth
1. Furthermore, S contains every node at depth (1 + 2 )d that is not a descendant of u, where
d = h (r).
Now define an (1 , 2 )-approximate heuristic h as follows: h(u) = (1 + 2 )h (u) and h(v) =
(1  1 )h (v) for all v 6= u. With this heuristic, every 2 -optimal solution is hidden from the search
procedure by its ancestor u. Precisely, since f (u) = 1 + (1 + 2 )(d  1) = (1 + 2 )d  2 , every
2 -optimal solution s (which is a descendant of u) will have
max
vPath(s)

f (v)  f (u) = (1 + 2 )d  2 .

Thus M  (1 + 2 )d  2 , where M is the min-max value defined in Equation (2).
Let v be any node at depth `  (1 + 2 )d that does not lie inside of SubTree(u). Note that the
f values monotonically increase along the path from root r to v, which implies that the node v must
be expanded if f (v) < M . On the other hand, since every non-descendant of u at depth (1 + 2 )d is
a solution, we have ` + h (v)  (1 + 2 )d, and thus
f (v)  ` + (1  1 )[(1 + 2 )d  `] = (1  1 )(1 + 2 )d + 1 ` .
Hence, the node v must be expanded if (1  1 )(1 + 2 )d + 1 ` < (1 + 2 )d  2 , which is equivalent
to ` < (1 + 2 )d  2 /1 . It follows that the number of nodes expanded by A is at least
(1+2 )d12 /1

X
`=0

(1+2 )d22 /1

b` 

X

b` = b(1+2 )d12 /1 .

`=0

According to Theorem 4.2, if we set 2 = 0 and let 1 be arbitrarily small provided 1 > 0, then
we can obtain a near-accurate heuristic which forces A to expand at least as many as bd1 nodes.
This lower bound partially explains why A can perform so poorly, even with an almost perfect
heuristic, in certain applications (Helmert & Roger, 2008): The adversarially-chosen solution set
given in the proof of Theorem 4.2 has an overwhelming number of near-optimal solutions. Indeed,
N+2  b(1+2 )d  b(1+2 )d1  b(1+2 )d1
for any  > 0.

5. Generalizations: Non-uniform Edge Costs and Branching Factors
In this section, we discuss how the generic upper bounds of Lemma 3.1 can be generalized to apply
to more natural search models such as those with non-uniform branching factors and non-uniform
edge costs; in Section 6, we show how these can be extended to general graph search models.
Now we consider a general search tree without the assumptions of uniform branching factor and
uniform edge costs. From the same argument given in the proof of Lemma 3.1, we derive the assertion
that when the heuristic is (1 , 2 )-approximate, any node of cost more than (1 + 2 + 1  )c will
not be potentially expanded if it does not lie on a (1 + 2 )-optimal solution path, where  is an
arbitrary nonnegative number and c = h (r) is the optimal solution cost.
Hence, the number of nodes potentially expanded by A with an (1 , 2 )-approximate heuristic
is bounded by


F (1 + 2 + 1  )c + R (1 + 2 + 1  )c , 1 + 2 .
(6)
697

fiDinh, Dinh, Michel, & Russell

Here F () is the number of nodes with cost no more than , which we call free nodes; R(, ) is the
number of nodes with cost in the range (, (1 + 2 )c ] that lie on a -optimal solution path, which
we call restricted nodes.
To bound the number of free and restricted nodes, respectively, we assume that the branching
factors are upper bounded and edge costs are lower bounded. Let B  2 be the maximal branching
factor and let m be the minimal edge cost. Since any node with cost no more than  must lie at
depth no larger than /m, we have
F ()  2B /m .
On each -optimal solution path, there are at most ((1 + 2 )c  )/m nodes of cost in the range
(, (1 + 2 )c ]. Thus,
(1 + 2 )c  
 N .
R(, ) 
m
Letting  = (1 + 2 + 1  )c ,  = 1 + 2 , and applying the bounds for F () and R(, ) to the
bound in (6), we obtain another upper bound on the number of expanded nodes when the heuristic
is (1 , 2 )-approximate:


2B (1 +2 +1)c

/m

+ N1 +2 (1  1 )c /m

(7)

for any   0. This equation (7) is a generalized version of the bound in Lemma 3.1. Substituting
 = 1 in (7), we arrive at the following simpler upper bound on the number of expanded nodes:
2B (1 +2 )c



/m

+ N1 +2 (1  1 )c /m .

(8)

6. Bounding Running Time of A on Graphs
In previous parts, we have established bounds on the running time of A on the tree model. Now
we will apply those bounds to A on the graph model. In order to do that, we will first unroll the
graph into a cover tree, and then bound the number of solutions lifted to the cover tree.
6.1 Unrolling Graphs into Trees
The preceding generic upper bounds are developed for tree-based models; in this section we discuss
a natural extension to general graph search models. The principal connection is obtained by unrolling a graph into a tree on which A expands at least as many nodes as it does on the original
graph (including repetitions). More specifically, given a directed graph G and starting node x0 in G,
we define a cover tree T (G) whose nodes are in one-to-one correspondence with finite-length paths
in G from x0 . We shall write a path (x0 , . . . , x` ) in G as a node in T (G). The root of T (G) is
(x0 ). The parent of a node (x0 , x1 , . . . , x` ) in T (G) is the node (x0 , x1 , . . . , x`1 ), and the edge cost
between the two nodes (x0 , x1 , . . . , x`1 ) and (x0 , x1 , . . . , x` ) in T (G) equals the cost of the edge
(x`1 , x` ) in G. Hence, for each node P in T (G), the cost value g(P ) is equal to the total edge
cost on the path P in G. A node (x0 , . . . , x` ) in T (G) is designated as a solution whenever x` is a
solution in G.
A node in T (G) that corresponds to a path ending at node x  G will be called a copy of x.
Observe that a solution in G may lift multiple times to solutions in T (G), as each node in G may
have multiple copies in T (G). Figure 1 illustrates an example of unrolling a graph into a cover tree.
In this example, node s is a solution in the graph and its first two copies in the cover tree correspond
to the paths (0, 3, s) and (0, 5, 3, s), where 0 is the starting node in the given graph.
The A search on graph G is described in Algorithm 2 below, in which h(x) is the heuristic
at node x, g(x) is the cost of the current path from x0 to x, and c(x, x0 ) denotes the cost of the
edge (x, x0 ) in G. We assume the value of h(x) depends only on x, i.e., h(x) does not depend on
a particular path from x0 to x. Unlike A search on a tree, for each node x in Open or Closed,
698

fiThe Time Complexity of A with Approximate Heuristics

0
1
6

1

3

5

2

2

s

3

6

1

1

5

2

s

2
0

5

3
s

Figure 1: Unrolling a graph into a cover tree.
Algorithm 2 also keeps track of the current path P from x0 to x through the pointers, and the
current f -value of x is equal to g(P ) + h(x). This current path is the cheapest path from x0 to x
that passes only nodes that have been expanded.
Algorithm 2 A search on a graph (Pearl, 1984, p. 64)
1. Initialize Open := {x0 } and g(x0 ) := 0.
2. Repeat until Open is empty.
(a) Remove from Open and place on Closed a node x for which the function f = g + h is
minimum.
(b) If x is a solution, exit with success and return x.
(c) Otherwise, expand x, generating all its successors. For each successor x0 of x,
i. If x0 is not on Open or Closed, estimate h(x0 ) and calculate f (x0 ) = g(x0 ) + h(x0 )
where g(x0 ) = g(x) + c(x, x0 ), and put x0 to Open with pointer back to x.
ii. If x0 is on Open or Closed, compare g(x0 ) and g(x) + c(x, x0 ). If g(x) + c(x, x0 ) <
g(x0 ), direct the pointer of x0 back to x and reopen x0 if it is in Closed.
3. Exit with failure.
Now consider A search on the cover tree T (G) of graph G using the same heuristic function h:
for each node P in T (G), set the heuristic value h(P ) to be equal to h(x) if P is a copy of node
x  G, i.e., P is a path in G from x0 to x. Observe that the cover tree T (G) and the graph G share
the same threshold M (defined in Equation (2)). Hence, whenever a node x  G is expanded with
current path P , we must have g(P ) + h(x)  M , which implies that P is potentially expanded by
A search on the cover tree T (G). This shows the following fact:
Fact 3. The number of node expansions by A on G is no more than the number of nodes potentially
expanded by A on T (G) using the same heuristic.
Here, by node expansion, we mean an execution of the expand step of A , i.e. Step (2c). Note
that, in general, a node in G can be expanded many times along different paths.
Remark The running time of A on the cover tree can also be used to upper bound the running time
of iterative-deepening A (IDA ) on the graph. Recall that the running time of IDA is dominated
by its last iteration. On the other hand, the last iteration of IDA on G is merely depth-first search
699

fiDinh, Dinh, Michel, & Russell

on the cover tree T (G) up to the cost threshold M . Hence, the number of expansions in the last
iteration of IDA is no more than the number of nodes potentially expanded by A on the cover
tree.
So, to upper-bound time complexity of A or IDA on a graph, it suffices to unroll the graph
into the cover tree and apply upper bounds on the number of nodes potentially expanded by A on
the cover tree. In particular, the bound in Equation (7) can be applied directly to the A search on
G.
Note that while these bounds can be applied directly, the problem of determining exactly how
solutions in G lift to solutions in the cover tree depends on delicate structural properties of G
specifically, it depends on the growth of the number of distinct paths from x0 to a solution as
a function of the length of these paths. In particular, in order to obtain general results on the
complexity of A in this model, we must invoke some measure of the connectedness of the graph G.
Below we show how to bound the complexity of A in terms of spectral properties of G. We choose
this approach because it offers a single parameter notion of connectedness (the second eigenvalue)
that is both analytically tractable and can actually be analyzed or bounded for many graphs of
interest, including various families of Cayley graphs and combinatorial graphs by methods such as
conductance.
6.2 An Upper Bound via Graph Spectra
We shall consider an undirected2 graph G on n vertices as the search space. Let x0 be the starting
node and let S be the set of solutions in G. For simplicity, assume G is b-regular (2 < b  n) and
the edge costs are uniformly equal to one, so the cover tree T (G) is b-ary and has uniform edge cost.
We assume, additionally, that |S| is treated as a constant when n  .
By Fact 3 and Lemma 3.1, the number of node expansions by A on G with an (1 , 2 )approximate heuristic is at most 2b(1 +2 )d + dN1 +2 , where d is the optimal solution cost, which
equals the optimal solution depth in T (G), and N is the number of -optimal solutions in T (G).
Our goal now is to upper bound N (of the cover tree T (G)) in terms of spectral properties of G.
We introduce the principal definitions of spectral graph theory below, primarily to set down
notation. A more complete treatment of spectral graph theory can be found in the work of Chung
(1997).
Graph Spectra. For a graph G, let A be the adjacency matrix of G: A(x, y) = 1 if x is adjacent
to y, and 0 otherwise. This is a real, symmetric matrix (AT = A) and thus has real eigenvalues
b = 1/b  A
b = 1  2  . . .  n  b, by the spectral theorem (Horn & Johnson, 1999). Let A
b
denote the normalized adjacency matrix of G; then A has eigenvalues 1 = 1  2  . . .  n  1,
which are referred to as the spectrum of G, where i = i /b. These eigenvalues, along with their
associated eigenvectors, determine many combinatorial aspects of the graph G. In most applications
def
of graph eigenvalues, however, only the critical value  = (G) = max {|2 |, |n |} is invoked
and, moreover, the real parameter of interest is the gap between  = /b and the largest eigenvalue
1 = 1 of the normalized adjacency matrix. Intuitively,  measures the connectedness of G.
Sparsely connected graphs have   1; for the n-cycle, for example,  = 1  O(1/n). The hypercube
on N = 2n vertices has  = 1  (1/ log N ). Similar bounds on  and , are known for many
families of Cayley graphs. Random graphs, even of constant degree b  3, achieve  = o(1) with
high probability. In fact, a recent result of Friedman (2003) strengthens this:
Theorem 6.1. (Friedman, 2003) Fix a real c > 0 and an integer b  2. Then with probability
1  o(1) (as n  ),

(Gn,b )  2 b  1 + c
2. While one can produce an analogous cover tree in the directed case, the spectral machinery we apply in the next
section is somewhat complicated by the presence of directed edges. See the work of Chung (2006) and Horn and
Johnson (1999, Perron-Frobenius theorem) for details.

700

fiThe Time Complexity of A with Approximate Heuristics

where Gn,b is a random b-regular graph on n vertices.
We remark that for any non-bipartite connected graph with diameter D, we always have  
b  1/(Dn). Under stronger conditions, when the graph is vertex-transitive (which is to say that
for any pair v0 , v1 of vertices of G there is an automorphism of G sending v0 to v1 ), one has
  b  (1/D2 ) (Babai, 1991). While vertex transitivity is a strong condition, it is satisfied by
many natural algebraic search problems (e.g., 15-puzzle-like search spaces and the Rubiks cube).
The principal spectral tool we apply in this section is described in Lemma 6.1 below. We begin
with some notation.
Notations. Any function  on G can be viewed as a column vector indexed by the vertices in G
and vice versa. For each vertex x  G, let 1x denote the function on G that has value 1 at x and
0 at everyPvertex other than x. For any real-valued functions ,  on G, define the
pinner product
h, i = xG (x)(x). We shall use k  k to denote the L2 -norm, i.e., kk = h, i for any
function  on G.
b is symmetric and real, by spectral theorem (Horn & Johnson, 1999), there
Recall that since A
exist associated eigenfunctions 1 , . . . , n that form an orthonormal basis for the space of real-valued
b In particular,
functions on G, where i is the eigenfunction associated with the eigenvalue i of A.
b
we have
Pn Ai = i i and ki k = 1 for all i, and hi , j i = 0 for all i 6= j. In this basis, we can write
 = i=1 h, i i i for any real-valued function  on G.
Lemma 6.1. Let G be an undirected b-regular graph with n vertices, and  = (G)/b. For any
probability distributions p and q on vertices of G, and any integers s, t  0,
fi
fiD


E
fi
fi s
b p, A
bt q  1 fi  s+t kpk  kqk  1 .
fi A
fi
nfi
n
Pn

Pn
ai i and q = j=1 bj j where ai = hp, i i , bj = hq, j i. Then
+
* n
n
n
n
D
E
X
X
X
X
t
s
t
s
b p, A
bq =
s+t
ai bi .
ai bj si tj hi , j i =
bj j j =
A
ai i i ,
i

Proof. Write p =

i=1

i=1

i,j=1

j=1

i=1

By the Cauchy-Schwartz inequality,
n
X

v
!
! n
u n
X
u X
2
2
t
|ai bi | 
bi = kpk  kqk .
ai

i=1

i=1

i=1
1/n

Without loss of generality, assume 1 (x) =
for all vertices x  G. Since p is a probability
distribution,
X
1
1 X
p(x) =  .
a1 = hp, 1 i =
p(x)1 (x) = 
n
n
xG

Similarly, b1 =

1 .
n

Thus, a1 b1 =

xG

1
n.

So we have
fi n
fi
fiD
fi
E 1 fifi fiX
fi s
fi
fi
s+t
b p, A
bt q  fi = fi
fi A

a
b
fi
i
i
i
fi
fi
nfi fi
i=2

 s+t

n
X

|ai bi |

(as  = max |i |)
2in

i=2



 s+t kpk  kqk 
completing the proof of the lemma.
701

1
n


,

fiDinh, Dinh, Michel, & Russell

With Lemma 6.1 in hand, we establish the following bound on the number of paths of a prescribed
length ` connecting a pair of vertices. We then apply this to control the number of -optimal solutions
in the cover tree of G. Let P` (u, v) denote the number of paths in G of length ` from u to v.
Lemma 6.2. Let G be an undirected b-regular graph with n vertices, and  = (G). For any vertices
u, v in G and `  0,
fi
fi


`fi
fi
fiP` (u, v)  b fi  ` 1  1 < ` .
fi
nfi
n
Proof. Since P` (u, v) is the number of `-length paths from u to v, we have P` (u, v) = b` p(`) (v), where
p(`) (v) is the probability that a natural random walk on G of length
` starting
from u ends up at
D
E


ff
P` (u,v)
`
(`)
`
(`)
(`)
b
b
= 1v , A 1u . Applying Lemma 6.1
v. Since p = A 1u and p (v) = 1v , p , we have
`
b

yields
fi
fi fi




E 1 fifi
fi P` (u, v)
1 fifi fifiD
`
b
fi
fi  ` k1v k  k1u k  1 = ` 1  1 .
1
,
A
1


=
v
u
fi b`
nfi fi
nfi
n
n
As  = /b, multiplying both sides of the last inequality by b` completes the proof for the lemma.
The major consequence of Lemma 6.2 in our application is the following bound on the number
of -optimal solutions in T (G).
Theorem 6.2. Let G be an undirected b-regular graph with n vertices, and  = (G). For sufficiently
large n and any   0, the number of -optimal solutions in T (G) is
 (1+)d

b
N < 2|S|
+ (1+)d ,
n
where d is the depth of optimal solutions in T (G), and S is the set of solution nodes in G.
Proof. For each solution s  S, the number of copies of s at level ` in T (G) equals P` (x0 , s), which
is less than b`/n + ` by Lemma 6.2. Hence, the number of solutions at level ` in T (G) is
 `

X
b
P` (x0 , s) < |S|
+ ` .
(9)
n
sS

Summing up both sides of (9) for ` ranging from d to (1 + )d, we have


(1+)d
(1+)d
(1+)d
X X
X
X
1
N =
P` (x0 , s) < |S| 
b` +
`  .
n
`=d sS

`=d

`=d

When n is sufficiently large, we have   2. Thus,


1 (1+)d
N < |S|
2b
+ 2(1+)d .
n

Note that b(1+)d
/n = O(1) if d =O(logb n). As mentioned earlier (Theorem 6.1), most b-regular
graphs have   2 b  1 + o(1)  2 b. Assuming G has this spectral property and d = O(logb n),
Theorem 6.2 gives


N = O((1+)d ) = O 2(1+)d b(1+)d/2 .
In such cases, the number of node expansions by A on G using an (1 ,   1 )-approximate heuristic
is O(d2(1+)d b(1+)d/2 ), which implies the effective branching factor of A is roughly bounded by
21+ b(1+)/2 < 8b(1+)/2 .
702

fiThe Time Complexity of A with Approximate Heuristics

7. Experimental Results
As discussed in the introduction, the bounds established thus far guarantee that E, the number of
nodes expanded by A using a -accurate heuristic, satisfies
E  2bd + dN  cbd
under the assumption that N  bd . (Here, as before, b is the branching factor, d is the optimal
solution depth, and c is some constant.) This suggests the hypothesis that for hard combinatorial
problems with suitably sparse near-optimal solutions,
log E  d log b +  .

(10)

where  is a constant determined by the search space and heuristic but independent from . In
particular, this suggests a linear dependence of log E on . We experimentally investigated this
hypothesized relationship with a family of results involving the Knapsack problem and the partial
Latin square problem. As far as we are aware, these are the first experimental results specifically
investigating this dependence.
We remark that in order for such an experimental framework to really cast light on the bounds we
have presented for A , one must be able to furnish a heuristic with known approximation guarantees.
7.1 A Search for Knapsack
We begin with describing a family of experimental results for A search coupled with approximate
heuristics for solving the Knapsack problem. This problem has been extremely well-studied by a
wide variety of fields including finance, operations research, and cryptography (Kellerer, Pferschy,
& Pisinger, 2004). As the Knapsack problem is NP-hard (Karp, 1972), no efficient algorithm can
solve it exactly unless NP = P. Despite that, this problem admits an FPTAS (Vazirani, 2001, p.
70), an algorithm that will return an -approximation to the optimal solution in time polynomial in
both 1/ and the input size. We use this FPTAS to construct approximate admissible heuristics for
the A search, which yields an exact algorithm for Knapsack that may expand far fewer nodes than
straightforward exhaustive search. (Indeed, the resulting algorithm is, in general, more efficient than
exhaustive search.)
7.1.1 A Search Model for Knapsack
Consider a Knapsack instance given by n items, and let [n] = {1, . . . , n}. Each item i  [n] has
weight wi > 0 and profit pi > 0. The knapsack has capacity c > 0. The task is to find a set of items
with maximal total profit such that its total weight is at most c. This Knapsack instance will be
denoted as a tuple h[n], p, w, ci. The Knapsack instance restricted to a subset X  [n] is denoted
hX, p, w, ci. For each subset X  [n], we will let w(X) P
and p(X) denote the P
total weight and the
total profit, respectively, of all items in X, i.e., w(X) = iX wi and p(X) = iX pi .
Search Space. We represent the Knapsack instance h[n], p, w, ci as a search space as follows. Each
state (or node) in the search space is a nonempty subset X  [n]. A move (or edge) from one state
X to another state is taken by removing an item from X. The cost of such a move is the profit of
the removed item. A state X  [n] is designated as a solution if w(X)  c. The initial state is the
set [n]. See Figure 2 for an example of the search space with n = 4.
This search space is an irregular directed graph whose out-degrees span in a wide range, from 2
to n  1. Moreover, for any two states X1 , X2 with X2  X1  [n], there are |X1 \ X2 |! paths on
this search graph from X1 to X2 . Moreover, every path from X1 to X2 has the same cost equal to
p(X1 )  p(X2 ). This feature of the search graph makes A behave like it does on a spanning subtree
of the graph: no state in this search graph will be reopened. Hence, for any state X  [n], the cost
703

fiDinh, Dinh, Michel, & Russell

{1, 2, 3, 4}

{1, 2, 3}

{1, 2}

{1, 2, 4}

{1, 3}

{1, 3, 4}

{1, 4}

{1}

{2, 3}

{2}

{2, 3, 4}

{2, 4}

{3}

{3, 4}

{4}

Figure 2: The search space for a Knapsack instance given by the set of 4 items {1, 2, 3, 4}. Solution
states and edge costs are not indicated in this figure.
of any path from the starting state to X is
g(X) = p([n] \ X) = p([n])  p(X) ,
and the cheapest cost to reach a solution from a state X  [n] is
h (X) = p(X)  Opt(X) ,
where Opt(X) is the total profit of an optimal solution to the Knapsack instance hX, p, w, ci, i.e.,
Opt(X) = max {p(X 0 ) | X 0  X and w(X 0 )  c} .
def

Observe that a solution state X   [n] on the search space h[n], p, w, ci is optimal if and only if
g(X  ) is minimal, or equivalently, p(X  ) is maximal, which means that X  is an optimal solution
to the Knapsack instance h[n], p, w, ci.
Heuristic Construction. Fix a constant   (0, 1). In order to prove the linear dependence of
log E on , we wish to have an efficient -accurate heuristic H on the aforementioned Knapsack
search space h[n], p, w, ci. Moreover, in order to guarantee that the solution returned by the A
search is optimal, we insist that H be admissible, so H must satisfy:
(1  )h (X)  H (X)  h (X) X  [n] .
The main ingredient for constructing such a heuristic is an FPTAS described in the book of Vazirani
(2001, p. 70). This FPTAS is an algorithm, denoted A, that returns a solution with total
 profit
at least (1  )Opt(X) to each Knapsack instance hX, p, w, ci and runs in time O |X|3 / , for any
  (0, 1). For each nonempty subset X  [n], let A (X) denote the total profit of the solution
returned by algorithm A with error parameter  to the Knapsack instance hX, p, w, ci. Then we
have for any   (0, 1),
(1  )Opt(X)  A (X)  Opt(X) ,
which implies
p(X) 

A (X)
 h (X)  p(X)  A (X) .
1

(11)

 (X)
Thus we may work with the heuristic H (X) = p(X)  A1
, which guarantees admissibility.
However, this definition of H does not guarantee -approximation for H : with this definition, the
condition (1  )h (X)  H (X) is equivalent to

(1  )h (X)  p(X) 
704

A (X)
,
1

(12)

fiThe Time Complexity of A with Approximate Heuristics

which does not always hold. Since h (X)  p(X)  A (X), the condition of (12) will be satisfied if
(1  )(p(X)  A (X))  p(X) 

A (X)
.
1

(13)

 (X)
if Equation (13) holds. Otherwise, we will define
Hence, we will define H (X) = p(X)  A1
H (X) differently, still ensuring that it is -approximate and admissible. Note that if X is not a
solution, at least one item in X must be removed in order to reach a solution contained in X, thus
h (X) = p(X)  Opt(X)  m, where m is the smallest profit of all items. This gives another option
to define H (X) that will guarantee the admissibility. In summary, we define the heuristic function
H as follows: for all non-solution state X,
(
 (X)
if (13) holds
p(X)  A1
def
(14)
H (X) =
m
otherwise,

where  will be determined later so that H is -approximate. If X is a solution, we simply set
H (X) = 0, because h (X) = 0 in this case. Then H is admissible, regardless of .
To make sure that H is -approximate, it remains to consider the case when (13) does not hold,
 (X)
i.e., p(X)  A1
< (1  )(p(X)  A (X)), for any non-solution state X. In such a case, we have
p(X)  A (X) 



A (X) 
(p([n])  m) .
(1  )
(1  )

(15)

The last inequality is due to the assumption that X is not a solution. Now we want to choose  such
that

m
(p([n])  m) 
(16)
(1  )
1
which, combining with (11) and (15), will imply (1  )h (X)  m = H (X). Therefore, we will
choose  such that

1 = 1 +  1  1 (p([n])/m  1) .

3 1
Since the running time to
compute
A
(X)
is
O
|X|

, the running time to compute H (X)


3 1
will be O |X|  p([n])/m , which is polynomial in both n and  1 if all the profits are bounded
some range [m, poly(n)m]. The A search using the heuristic H for the given Knapsack space
h[n], p, w, ci is described in Algorithm 3 below.
7.1.2 Experiments
In order to avoid easy instances, we focus on two families of Knapsack instances identified and studied
by Pisinger (2005) that are difficult for existing exact algorithms, including dynamic programming
algorithms and branch-and-bound algorithms:
Strongly Correlated: For each item i  [n], choose its weight wi as a random integer in the
range [1, R] and set its profit pi = wi + R/10. This correlation between weights and profits
reflects a real-life situation where the profit of an item is proportional to its weight plus some
fixed charge.
Subset Sum: For each item i  [n], choose its weight wi as a random integer in the range [1, R]
and set its profit pi = wi . Knapsack instances of this type are instances of the subset sum
problem.
For our tests
P we set the data range parameter R := 1000 and choose the knapsack capacity as
c = (t/101) i[n] wi , where t is a random3 integer in the range [30, 70].
3. In the paper of Pisinger (2005), t is a fixed integer between 1 and 100, and the average runtime of all tests
corresponding to all values of t was reported.

705

fiDinh, Dinh, Michel, & Russell

Algorithm 3 A Search for Knapsack
Input: hn, p, w, c, i; where n is the number of items, pi and wi are the profit and weight of item
i  [n], c is the capacity of the knapsack, and   (0, 1) is an error parameter for the heuristic.
Oracle: The FPTAS algorithm A for the Knapsack problem
(2001, p. 70).
P described by Vazirani
P
Notation: For each subset X  [n] of items, let p(X) = iX pi , w(X) = iX wi .
Output: a subset X   [n] of items such that w(X  )  c and p(X  ) is maximal.
1. Put the start node [n] on Open. Let m = min1in pi . Set  such that

1 = 1 +  1  1 (p([n])/m  1) .
2. Repeat until Open is empty:
(a) Remove from Open and place on Closed a node X for which g(X) + h(X) is minimum.
(b) If w(X)  c, exit with success and return X, an optimal solution.
(c) Otherwise, expand X: For each item i  X, let X 0 = X \ {i},
i. If X 0 is not on Open or Closed, set g(X 0 ) := g(X) + p(i) = p([n])  p(X 0 ), and
compute the heuristic h(X 0 ) as follows:
A. If X 0 is a solution, set h(X 0 ) := 0.
B. Otherwise, run algorithm A on the Knapsack input hX 0 , p, w, ci with error parameter , and let A(X 0 ) denote the total profit of the solution returned by algorithm
A. Then set
(
0
0
)
)
0
0
if p(X 0 )  A(X
p(X 0 )  A(X
0
1
1  (1  )(p(X )  A(X ))
h(X ) :=
m
otherwise.
Then put X 0 to Open with pointer back to X.
ii. Otherwise (X 0 is on Open or Closed, so g(X 0 ) has been calculated), if g(X)+p(i) <
g(X 0 ), direct the pointer of X 0 back to X and reopen X 0 if it is in Closed.
[Remark: Since all paths from the starting node to X 0 have the same cost, the
condition g(X) + p(i) < g(X 0 ) never holds. In fact, this step can be discarded.]
3. Exit with failure.

706

fiThe Time Complexity of A with Approximate Heuristics

After generating a Knapsack instance h[n], p, w, ci of either type described above, we run a series
of the A search using the given heuristic H , with various values of , as well as breath first search
(BFS), to solve the Knapsack instance. When each search finishes, the values of E and d are reported,
where E is the number of nodes (states) expanded by the search, and d is the depth of the optimal
solution found by the search. In this Knapsack search space, k equals the number of items removed
from the original set [n] to obtain the optimal solution found by the search. The overall runtime for
each search, including the time for computing the heuristic, is also reported. In addition, we report
the optimal value h ([n]) and the minimal edge cost m (i.e., minimal profit) of the search space for
each Knapsack instance tested.
To specify appropriate size n for each Knapsack instance type, we ran a few exploratory experiments and identified the largest possible value of n for which most search instances would finish
within a few hours. Then we chose those values of n (n = 23 for the Strongly Correlated type, and
n = 20 for the Subset Sum type) for our final experiments. Observing that the optimal solution
depths resulted from Knapsack instances of these sizes are fairly small, ranging from 5 to 15, we
selected sample points for  in the high interval [0.5, 1) with a distance between two consecutive
points large enough so that the sensitiveness of E to  can be seen. In particular, we selected eight
sample points for  from 8/16 = 0.5 to 15/16 = 0.9375 with the distance of 1/16 = 0.0625 between
two consecutive points. In our final experiments, we generated 20 Knapsack instances of each type
with the selected parameters for n and .
Experimental Results. Results for our final experiments are shown in Tables 1, 2, 3, 4, 5, and 6,
in which the rows corresponding to breath first search are indicated with BFS under the column
of . These data show, as expected, that A search outperforms breath first search in terms of
the number of nodes expanded and, naturally, that the smaller , the fewer nodes A expands. As
a result, the effective branching factor of A will decrease as  decreases (as long as all optimal
solutions in the given search space are located at the same depth). Recall that if A expands E
nodes and finds a solution at depth d, then its effective branching factor is the branching factor of a
uniform tree of depth d and E nodes (Russell & Norvig, 1995, p. 102), i.e., the number b satisfying
E = 1 + b + (b )2 +    + (b )d . Clearly, (b )d  E and, if b  2, we have E  2(b )d . As we shall
focus solely on values of b  2, we simply use E 1/d as a proxy for effective branching factor, content
that this differs from the actually quantity by a factor no more than 21/d . (Of course, as b grows
this error decays even further). The effective branching factors, calculated as E 1/d , of A search
and breath first search for Knapsack instances of type Strongly Correlated are shown in Tables 1,
2, and 3. Note that for Knapsack instances of the Subset Sum type, one cannot directly compare
effective branching factors, as the optimal solutions found by different search instances can appear
at different depths.
Our primary goal in these experiments is to investigate the proposed linear dependence which,
in this case of non-uniform branching factors and non-uniform edge costs, we may express
log E  d log bBFS +  ,

(17)

where d is the average optimal solution depth, bBFS is the effective branching factor of breath first
search, and  is a constant not depending on . To examine to what extend our data supports
this hypothesis, we calculate the least-squares linear fit (or linear fit for short) of log E (for
each Knapsack instance, varying ) using the least-squares linear regression model, and measure
the coefficient of determination R2 . In our experiments, 17 out of 20 Knapsack instances of type
Strongly Correlated and all 20 Knapsack instances of type Subset Sum have the R2 value at least
0.9. For these instances, over 90% of the variation in log E depends linearly on , a remarkable fit.
See Figure 5 for detailed histograms of R2 values for our Knapsack instances. The median R2 is
0.9534 for Knapsack instances of type Strongly Correlated, and is 0.9797 for those of type Subset
Sum. Graphs of log E and its linear fit for Knapsack instances with the median R2 among those of
the same type are shown in Figures 3 and 4. Note that as there are an even number of instances of
707

fiDinh, Dinh, Michel, & Russell

each type, there is no single instance with the median value. The instances shown in these graphs
actually have the R2 value below the median.
Knapsack instance of type Strongly Correlated with median R2
Instance 17
Linear fit
BFS

log10 E

6
5
4
3
2
0.5

0.6

0.7
0.8
0.9
Heuristic error 

1

Figure 3: Graph of log10 E and its least-squares linear fit for the Knapsack instance of type Strongly
Correlated with the median R2 (see data in Table 3).

Knapsack instance of type Subset Sum with median R2
5.3

Instance 14
Linear fit
BFS

log10 E

5.25

5.2

5.15
0.5

0.6

0.7
0.8
0.9
Heuristic error 

1

Figure 4: Graph of log10 E and its least-squares linear fit for the Knapsack instance of type Subset
Sum with the median R2 (see data in Table 5).

Remark Of course, there may be instances that poorly fit our prediction of linear dependence, such
as instance #20 of Strongly Correlated type whose R2 value is only 0.486, though those instances
708

fiThe Time Complexity of A with Approximate Heuristics

rarely show up in our experiments. In such an instance, the A search using heuristic function H
may explore even fewer nodes than the A search using H does, for some small  > 0. This
phenomenon can be explained by the degree to which we can control the accuracy of our heuristic
function H . In particular, we can only guarantee that H is admissible and -approximate, while
in reality it may provide an approximation better than  to all nodes that are opened. Note that H
is not proportional to (1  ). Hence, H may be occasionally more accurate than H for some
small  > 0, resulting in fewer nodes expanded.

Frequency

Histograms of R2 values for Knapsack instance families
Strongly correlated
Subset Sum

10

5

[0.97, )

[0.93, )

[0.90, )

[0.87, )

[0.83, )

[0.80, )

[0.77, )

[0.73, )

[0.70, )

[0.67, )

[0.63, )

[0.60, )

[0.57, )

[0.53, )

[0.50, )

[0.47, )

[0.43, )

[0.40, )

0

R2 value bin limits
Figure 5: Histograms of the R2 values for Knapsack instances.
To analyze more deeply how our data fit the model of Equation (17), we calculate the slope of
the least-squares linear fit of log10 E for each Knapsack instance of type Strongly Correlated. Note
that for such an instance, every search has the same optimal solution depth, denoted d, and thus,
d = d. Our data, given in Figure 6, show that for all but one instance with the worst R2 value, the
slope a of the linear fit of log10 E is fairly close to d log10 bBFS , which is the slope of the hypothesized
line given in Equation (17). Specifically, for any Knapsack instance of type Strongly Correlated,
except instance #20,
0.73d log10 bBFS  a  1.63d log10 bBFS .

7.2 A Search for Partial Latin Square Completion
The experimental results discussed above for the Knapsack problem support the hypothesis of linear
scaling (cf., Equation (1) or (10)). However, several structural features of the search space and
heuristic are unknown: for example, we cannot rule out the possibility that the approximation
algorithm, when asked to produce an -approximation, does not in fact produce a significantly
better approximation; likewise, we have no explicit control on the number of near-optimal solutions.
In order to explore the hypothesis in more detail, we experimentally and analytically investigate a
search space for the partial Latin square completion problem in which we can provide precise analytic
control of heuristic error  as well as the number of -optimal solutions N .
7.2.1 The Partial Latin Square completion (PLS) Problem
A Latin square of order n is an n  n table in which each row and column is a permutation of the
set [n] = {1, . . . , n}. If only a few cells in an n  n table are filled with values from [n] in such a
709

fiDinh, Dinh, Michel, & Russell

Knapsack instance type: Strongly Correlated
Instance
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

Optimal
solution
depth d
11
9
6
7
7
6
7
8
6
9
7
5
7
5
6
9
7
8
7
7

Effective branching
factor of breath first
search bBFS
4.2092
5.3928
10.8551
8.2380
8.0194
10.6780
8.7068
6.7742
11.4102
5.5412
8.3260
18.0486
8.0308
15.0964
10.0070
5.7863
8.3155
6.9106
8.3602
7.0964

Slope
of
linear fit a

a/(d log10 bBFS )

Coefficient of
determination R2

7.6583
4.8966
10.1038
6.4279
5.0882
6.4511
7.9087
6.5616
8.6847
6.3690
9.7685
7.7848
6.0376
7.3004
4.4219
7.1815
9.1738
9.2837
7.1807
1.0055

1.1154
0.7435
1.6260
1.0027
0.8040
1.0454
1.2021
0.9872
1.3690
0.9517
1.5161
1.2392
0.9533
1.2385
0.7368
1.0466
1.4247
1.3823
1.1123
0.1688

0.9395
0.9183
0.9647
0.9710
0.9161
0.9696
0.9436
0.9782
0.9571
0.9461
0.9689
0.9314
0.9646
0.9676
0.8788
0.8698
0.9498
0.9729
0.9770
0.4860

Figure 6: Slopes of the least-squares linear fits of log10 E (varying ) for the Knapsack instances of
type Strongly Correlated. Details of these least-squares linear fits are given in Tables 1, 2, and 3.
The R2 values for these Knapsack instances are also included in this figure.
way that no value appears twice in a single row or column, then the table is called a partial Latin
square. A completion of a partial Latin square L is a Latin square that can be obtained by filling
the empty cells in L, see Figure 7 for an example. Note that not every partial Latin square has a
completion. Since the problem of determining whether a partial Latin square has a completion is
NP-complete (Colbourn, 1984), its search version (denoted PLS), i.e., given a partial Latin square
L find a completion of L if one exists, is NP-hard.
1

2
5

3
1
4

1
2

1
2
3
4
5

4
3

2
3
5
1
4

3
5
4
2
1

4
1
2
5
3

5
4
1
3
2

Figure 7: A 5  5 partial Latin square (right) and its unique completion (left).
The PLS problem (also known as partial quasi-group completion) has been used in the recent
past as a source of benchmarks for the evaluation of search techniques in constraint satisfaction
and Boolean satisfiability (Gomes & Shmoys, 2002). Indeed, partially filled Latin squares carry
embedded structures that are the trademark of real-life applications in scheduling and time-tabling.
Furthermore, hard instances of the partially filled Latin square trigger heavy-tail behaviors in
backtrack search algorithms which are common-place in real-life applications and require randomization and or restarting (Gomes, Selman, & Kautz, 1998). Additionally, the PLS problem exhibits
a strong phase transition phenomena at the satisfiable/unsatisfiable boundary (when 42% of the
cells are filled) which can be exploited to produce hard instances. We remark that the underlying
710

fiThe Time Complexity of A with Approximate Heuristics

structure of Latin squares can be found in other real-word applications including scheduling, timetabling (Tay, 1996), error-correcting code design, psychological experiments design and wavelength
routing in fiber optics networks (Laywine & Mullen, 1998; Kumar, Russell, & Sundaram, 1996).
7.2.2 A Search Model for PLS
Fix a partial Latin square L of order n with c > 0 completions. We divide the cells of the n  n table
into two types: the black cells, those that have been filled in L, and the white cells, those that are
left blank in L. Let k be the number of white cells. The white cells are indexed from 0 to k  1 in a
fixed order, e.g., left to right and top to bottom of the table. The task of A search now is to find a
completion of L. Hard instances are obtained when the white cells are uniformly distributed within
every row and every column and when the density of black cells is (n2  k)/n2  42% to tap into
the phase transition. We further insure that the number of completions is c = O(1) (c is exactly 1
for the experiments).
To structure the search space for this problem, we place the white cells on a virtual circle so that
the white cells of index i and (i + 1) mod k are adjacent. We can move along the circle, each step is
either forward (from a white cell of index i to the cell of index (i + 1) mod k) or backward (from a
white cell of index i to the cell of index (i  1) mod k) and may set the content of the current cell.
Formally, we define the search graph, denoted GL , for the PLS instance given by L as follows: Each
state (or node) of GL is a pair (, p), in which p  {0, . . . k  1} indicates the index of the current
white cell, and  : {0, . . . , k  1}  {0, . . . , n} is a function representing the current assignment of
values to the white cells (we adopt the convention that (j) = 0 means the white cell of index j
has not been filled). There is a directed link (or edge) from state (, p) to state (, q) in the search
graph GL if and only if q = (p  1) mod k, (q)6= 0, and (j) = (j) for all j 6= q. In other words,
the link from state (, p) to state (, q) represents the step consisting of moving from the white
cell of index p to the white cell of index q, and setting the value (q) to the white cell of index q.
Figure 8 illustrates the links from one state to another in GL . The cost of every link in GL is a unit.
Obviously, this search graph is regular and has (out-)degree of 2n.
The starting state is (0 , 0) where 0 (j) = 0 for all j. A goal state (or solution) is of the form
( , p), where  is the assignment corresponding to a completion of L, and p  {0, . . . , k  1}. So,
a solution on the cover tree of GL is a path in the search graph GL from the starting state to a
goal state, and the length of an optimal solution is equal to k. We will show that the number of
-optimal solutions in the cover tree of GL is not too large.
Lemma 7.1. Let L be an n  n partial Latin square with k white cells. Let  be the assignment
corresponding to a completion of L. For any 0  t < k, the number of paths
 of length k + t in GL

from the starting state to a goal state of the form ( , ) is no more than 2 t + 2 + t k+t
nt .
t
Proof. We represent a path in GL of length k + t from the starting state as a pair hP, ~v i, in which
P is a (k + t)-length path in the circle of white cells starting from the white cell of index 0, and
~v = (v1 , . . . , vk+t ) is a sequence of values in [n] with vi being the value assigned to the white cell
visited at the ith step of the path P . Consider a pair hP, ~v i that represents a path in GL ending up at
a goal state ( , ). Since  (j) 6= 0 for all j, every white cell must be visited at some non-zero step
of P . Let sj > 0 be the last step at which the white cell of index j is visited. Then we must have
vsj =  (j) for all j  {0, . . . , k  1}. Given such a path P , there are nt ways of assigning values to
the white cells in order to eventually obtain the assignment  . Thus, the number of (k + t)-length
paths in GL from the starting state to a goal state ( , ) is equal to |Pt |nt , where Pt is the set of
(k + t)-length paths on the circle of white cells that start at white cell of index 0 and visit every
white cell.
It remains to upper bound |Pt |. Consider a path P  Pt ; our strategy is to bound the number of
backward (or forward) steps in P . As t < k, there are at least k  t  1 white cells visited exactly
711

fiDinh, Dinh, Michel, & Russell

0

k1

5

k2

2

1

4

2

3

3

1
0

k1

5

k2

2

1

4

3

1, v
h

2

3

4

i

3

2

3

2

2

3

5

1

5

5

2

1

0

k1
k2

5

5
3

p

4
2

p

4

1

1

p1

v

4

3

h+

1, v
i

1

4

2

3

4

1

1

3

p

v

2

p+1

2

5
3

5

1

Figure 8: The links connecting states in a PLS search graph. The label h+1, vi (resp., h1, vi) on
the links means moving forward (resp., backward) and setting value v  [n] to the next white cell.

once in P . Let w be the index of a white cell that is visited exactly once in P and let s be the step
at which the white cell w is visited.
Assume the step s is a forward step, i.e., the white cell visited at step s  1 is (w  1) mod k.
Let w0 be the farthest white cell from w in the backward direction that is visited before step s.
Precisely, w0 = (w  `) mod k, where ` is the maximal number in {0, . . . , k  1} for which the white
cell (w  `) mod k is visited before step s. Let wj = (w0 + j) mod k, for j = 0, . . . , k  1. Note that
w` = w. Then the set of white cells visited at the first s steps is {w0 , w1 , . . . , w` }, and by deleting
some steps among the first s steps in P we will obtain the path (w0 , w1 , . . . , w` ) from w0 to w` in
the forward direction. Each of the white cells w`+1 , . . . , wk1 must be visited at a step after step
s and also in the forward direction because the white cell w` is visited only once and at a forward
step. Thus, by deleting t steps from P we obtain a path visiting the white cells w0 , w1 , . . . , wk1 in
the forward direction. Let s0 , . . . , sk1 be the steps in P that are not deleted, where wj is visited
at step sj in P , and 1  s0 < s1 < . . . < sk1  k + t. Then steps s1 , . . . , sk1 are all forward steps
(step s0 can be forward or backward). Moreover, the number of backward steps and the number of
forward steps between sj1 and sj must be equal for all j = 1, . . . , k  1. Let  be the number of
deleted steps before s0 and after sk1 so that there are exactly (t  )/2 backward steps between
s0 and sk . This shows there are at most
 + 1 + (t  )/2 = 1 + (t + )/2  t + 1 backward steps

in P . Note that there at most k+t
paths in Pt that have exactly j backward steps. Path P has
j
t + 1 backward steps only when  = t (and thus sj = sj1 + 1 for all j = 1, . . . , k  1) and every
step from 1 to s0 and after sk1 is backward. There are t + 1 such paths in Pt , each corresponding
to a choice of s0  {1, . . . , t + 1}.
Similarly, if the step s is a backward step, then there are at most t + 1 forwardsteps in P . Also,
there are t + 1 paths in Pt that have exactly t + 1 forward steps, and at most k+t
paths in Pt that
j
712

fiThe Time Complexity of A with Approximate Heuristics

have exactly j forward steps. Hence,

|Pt |  2 t + 1 +


t 
X
k+t
j=0

The last inequality holds since the coefficient

j






2 t+2+t k+t
.
t

k+t
j



increases as j increases for j < (k + t)/2.

The upper bound in Lemma 7.1 is achieved when t = 0. In fact, there are four ways to visit
every white cell in k steps starting from the white cell 0: taking either k forward steps or k backward
steps or one backward step followed by k  1 forward steps or one forward steps followed by k  1
backward steps. So the number of optimal solutions in the cover tree of GL is equal to 4c, since
there are c completions of the initial partial Latin square.
Theorem 7.1. Let L be an n  n partial Latin square with k white cells and c completions. For
any 0 <  < 1, the number of nodes expanded by A search on GL with a -accurate heuristic is no
more than B(), where
B() =

(
2(2n)k + 4ck 
2(2n)

k

if k < 1 ,

+ 4ck bkc + 2 + bkc

k+bkc
bkc



n

bkc

if k  1 .

Proof. By Lemma 3.1, the number of nodes expanded by A search on GL with a -accurate heuristic
is upper-bounded by 2(2n)k + kN , where N is the number of -optimal solutions in the cover tree
of GL . So, we only need to bound N .
If k < 1, then N equals the number of optimal solutions, which implies the upper bound of
2(2n)k + 4ck on the number of expanded nodes by A .
In the general case, let ` = bkc. Since k < k, by Lemma 7.1, we have



`
X
k+t
2 t+2+t
nt
t
t=0
!

 X
`
`
X
k+`
t
t
 2c
(t + 2)n + 2c
tn
`
t=0
t=0


k+`
`
 4c(` + 2)n + 4c
`n` .
`

N  c



The second inequality holds because k+t
 k+`
for all t  `. The last inequality is obtained by
t
`
P`
P`
t
`
applying the fact that t=0 tn  2`n and t=0 nt  2n` for all integers n  2 and `  0, which
can be proved easily by induction on `. Hence, the number of nodes expanded by A is no more
than



k+`
2(2n)k + 4ck ` + 2 + `
n` .
`

Corollary 7.1. Suppose 0 <  < 1. Then the number of nodes expanded by A search on GL with
a -accurate heuristic is


k
k
O k 3/2 (1 + ) (1 + 1/) nk .

Proof. By Theorem 7.1, all we need is an upper bound on the binomial coefficient k+`
for large k,
`
where ` = bkc. Since both k and ` are large, we will bound this binomial coefficient using Stirlings
713

fiDinh, Dinh, Michel, & Russell



n
formula, which asserts that n!  2n ne . More precisely, write n! = 2n
as n  . We have


k+`
(k + `)!
=
`
k!`!
p
k+`
2(k + `) k+`
k+`
e
=


`
k
2k ke k  2` e` `

k + ` (k + `)k+`
k+`

.
 
=
k `
k k ``
2k`


n n
e

n , then n  1

By Stirlings formula, the term k+` /k ` is O(1). Since
k+`
k
k
=1+
1+
 1 + 2/
`
bkc
k  1



for k > 2/, the term k + `/ 2k` is O(1/ k). The remaining term is
(k + `)k+`
=
k k ``


k 
`

k
`
k
1
k
1+
1+
 (1 + ) 1 +
k
`

x

since `  k and the function g(x) = (1 + k/x) monotonically increases for x > 0. Hence,



k !
k+`
1
1
= O  (1 + )k 1 +
.
`

k
From Theorem 7.1, the number of nodes expanded by A is no more than
 
 
k + ` k
B() = 2(2n)k + O k 2
n
`
!

k
1
3/2
k
k
= O k (1 + ) 1 +
n
.


It follows from the above corollary that the effective branching factor of A using a -accurate

heuristic on GL is asymptotically at most (1 + ) (1 + 1/) n , which is significantly smaller than

the brute-force branching factor of 2n, since both (1 + )n and (1 + 1/) converge to 1 as   0.
7.2.3 Experiments
Given the search model for the PLS problem described above, we provide experimental results of
A search on a few PLS instances, each of which is determined by a large partial Latin square with
a single completion. For each PLS instance in our experiments, we run A search with different
heuristics of the form (1  )h given by various values of   [0, 1). We emphasize that by the dominance property of admissible heuristics, the number of nodes expanded by A using any admissible
-accurate heuristic strictly larger than (1  )h is less than or equal to that by the A using the
heuristic (1  )h . In other words, the heuristic (1  )h is worse than most admissible -accurate
heuristics.
To build the oracle for the heuristic (1  )h on a search graph GL , we use the information
about the completion of the partial Latin square L to compute h . Consider a partial Latin square
L with k white cells, and an arbitrary state (, p) in GL . We will show how to compute the optimal
714

fiThe Time Complexity of A with Approximate Heuristics

cost h (, p) to reach a goal state in GL from state (, p). Let X() be the set of white cells at
which  disagrees with the completion of L, then h (, p) is equal to the length of the shortest
paths on the cycle starting from p and then visiting every point in X(). The case in which
|X() \ {p} |  1 is easy to handle, so we shall assume |X() \ {p} |  2 from now on. In particular,
suppose X() \ {p} = {p1 , . . . , p` } with ` > 1, where pj is the j th point in X() \ {p} that is visited
when moving forward (clockwise) from p; see Figure 9. There are two types of paths on the cycle
starting from p and visiting every point in X() \ {p}: type I includes those that do not visit p, type
II includes those visiting p. Let `1 and `2 be the length of the shortest paths of type I and type II,
respectively. Then
(
min {`1 , `2 }
if p 6 X() ,

h (, p) =
min {`1 + 2, `2 } if p  X() .
So now we only need to compute `1 and `2 . Computing `1 is straightforward: it is realized by either
moving forward from p to p` or moving backward from p to p1 . That is
`1 = min {p`  p, p  p1 }
def

where z = z mod k for any integer z. To compute `2 , we consider two options for each j: option
(a) moving forward from p to pj and then moving backward from pj to pj+1 , option (b) moving
backward from p to pj+1 and then moving forward from pj+1 to pj . Thus,


`2 = min min {pj  p, p  pj+1 } + pj  pj+1 .
1j<`

moving forward
p1

p

p+1

p1

pm

p2

pm1
pj+1

pj

Figure 9: Layout of the points in X().
Now we describe our experiments in detail. We generate six partial Latin squares with orders
from 10 to 20 in the following way. Initially, we generate several partial Latin squares obtained at or
near the phase transition with white cells uniformly distributed within every row and column. Each
instance is generated from a complete Latin square with a suitably chosen random subsets of its
cells cleared. Each candidate partial Latin square is solved again with an exhaustive backtracking
search method to find all completions. The subset of candidates with exactly one completion is
retained for the experiments. For each partial Latin square L and each chosen value of , we record
the total number E of nodes expanded by A on the search graph GL with the (1  )h heuristic.
Then, as in the Knapsack experiments, the effective branching factor of A is calculated as E 1/k ,
since the optimal solution depth in GL equals the number of white cells in L. Our first purpose is to
compare these effective branching factors obtained from experiments to our upper bound obtained
715

fiDinh, Dinh, Michel, & Russell

from theoretical analysis. Recall from Theorem 7.1 that E  B(), where in this case
B() =

(
2(2n)k + 4k 
2(2n)

k

if k < 1 ,

+ 4k bkc + 2 + bkc

k+bkc
bkc



n

bkc

if k  1 .

1/k

Therefore, we calculate the theoretical upper bound B()
on the effective branching factor E 1/k .
1/k
For deeper comparison, we calculate the multiplicative gap B() /E 1/k between our theoretical
bound and the actual values. In our empirical results given in Tables 7 and 8, these multiplicative
gaps are close to 1 when  is small and k is large. Notice that for each given k, the upper bounds
of B() are almost the same for the s with the same value of bkc. This is why the multiplicative
gaps for those s sometimes increase when  decrease. However, the multiplicative gaps decrease
as bkc decreases, for each fixed k. Our upper bounds in the cases with k < 1 are much tighter
than in the others (with the same k) because in the cases of k < 1 we can compute the number of
-optimal solutions exactly. Also observe that, for each fixed , the multiplicative gaps decrease as k
increases. Finally, the experiments show a dramatic gap between the effective branching factors and
the corresponding brute-force branching factor, which equals 2n. In fact, for each instance, both the
1/k
effective branching factor E 1/k and our theoretical upper bound B()
approach 1 as  approaches
0.
As in the experiments for the Knapsack problem, our data for the partial Latin square problem
also support the linear dependance of log E on . In particular, all but one partial Latin square
instances have the R2 larger than 0.9 (the worst one has R2 value equal to 0.8698). The median R2
value for our partial Latin square instances is 0.9304. The graph for the instance with the median
R2 is shown in Figure 10.
Partial Latin Square instance with median R2
Instance 4
Linear fit

6

log10 E

5
4
3
2
0

0.5

1
1.5
2
Heuristic error 

2.5

3
102

Figure 10: Graph of log10 E and its least-squares least-squares linear fit (or Linear fit) for the
partial Latin square instance with the median R2 (see data in Table 8).
We also investigate how the slope of the least-squares linear fit of log E approximates the slope
of d log b in the hypothesized linear dependence of Equation (10). Recall that in this case, the
branching factor is b = 2n and the optimal solution depth is d = k. Figure 11 shows that, for every
PLS instance in our experiment, the slope  of the least-squares linear fit of log10 E approximates
716

fiThe Time Complexity of A with Approximate Heuristics

to k log10 (2n) by a factor of 0.8, i.e.,   0.8k log10 (2n). In other words, our experimental results
for the PLS indicate the following relationship:
log10 E    0.8k log10 (2n) +  ,

or equivalently, E  (2n)0.8k .

Thus, empirically, the effective branching factor of A search using the heuristic (1)h on the given
PLS search space approximates to (2n)0.8 . By the dominance property of admissible heuristics,
this is also an empirical upper bound on the effective branching factor of A using any admissible
-accurate on the same search space.
Instance #

n

k

1
2
3
4
5
6

10
12
14
16
18
20

44
63
86
113
143
176

Slope of linear
fit line 
43.3901
73.7527
98.3613
142.7056
179.1665
225.4152

/(k log10 (2n))
0.7580
0.8482
0.7903
0.8390
0.8050
0.7995

Figure 11: Slopes of the least-squares linear fits of log10 E for the partial Latin square instances.

8. Reduction in Depth vs. Branching Factor; Comparison with Previous
Work
In this section we compare our results with those obtained by Korf et al. (Korf & Reid, 1998; Korf
et al., 2001). As mentioned in the introduction, they concluded that the effect of a heuristic function
is to reduce the effective depth of a search rather than the effective branching factor. Considering
the striking qualitative difference between their findings and ours, it seems interesting to discuss
why their conclusions do not apply to accurate heuristics.
They study the b-ary tree search model, as above, and permit multiple solutions. However, their
analysis depends critically on the following equilibrium assumption:
Equilibrium Assumption: The number of nodes at depth i with heuristic value not exceeding `
is bi P (`), where P (`) is the probability that h(v)  ` when v is chosen uniformly at random among
all nodes of given depth, in the limit of large depth.
We remark that while the equilibrium assumption is a strong structural requirement, it holds in
expectation for a rich class of symmetric search spaces. To be specific, for any state-transitive
search space,4 like the Rubiks cube, the quantity bi P (`) is precisely the expected number of vertices
at depth i with h(v)  ` if the goal state (or initial state) is chosen uniformly at random. Korf
et al. (2001) observe that under the equilibrium assumption, one can directly control the number
of expanded
P nodes of total weight no more than `, a quantity we denote E(`): indeed, in this case
E(`) = i` bi P (`  i). With this in hand, they consider the ratio
P`
P`
i
bi1 P (`  i)
E(`)
i=0 b P (`  i)
= P`1
= b  Pi=0
 b,
(18)
`
i
i1 P (`  i)
E(`  1)
i=0 b P (`  1  i)
i=1 b
and conclude that E(d)  bd1 E(1); thus the effective branching factor is
q
p
d
bd1 E(1)  b d E(1)
4. We say that a search space is state-transitive if the structure of the search graph is independent of the starting
node. Note that any Cayley graph has this property, so natural search spaces formed from algebraic problems
like the Rubiks cube or 15-puzzle, with the right choice of generators, have this property.

717

fiDinh, Dinh, Michel, & Russell

if the optimal solution lies at depth d.
A difficulty with this approach is that even in the presence of a mildly accurate heuristic satisfying, for example,
h(v)  h (v) for small, constant,  > 0 ,
the actual values of these quantities satisfy
E(1) = E(2) =    = E(t) = 0
for all t  d. (Even the root of the tree has h(root)    d.) Observe,
then, that 
if E(d) = 1 the
p
d
argument above actually results in an effective branching factor of d bdd E(d) = b(1)d = b1 ,
yielding reduction in the branching factor. Indeed, applying this technique to infer estimates on
the complexity of A , even assuming the equilibrium
P i assumption, appears to require control of
the threshold quantity `0 at which the quantities
b P (`0  i) become non-negligible. Of course,
the equilibrium assumption may well apply to heuristics with weaker or, for example, nonuniform
accuracy.
One perspective on this issue can be obtained by considering the case of search on a b-regular
(non-bipartite, connected) graph G = (V, E) and observing that the selection of a node uniformly
at random from all nodes of a given depth, in the limit of large depth is, in this case, equivalent
to selection of a random node in the graph. If we again consider a mildly accurate heuristic h for
which, say, h(v)  h (v) for a small constant , we have bi P (`)  bi Prv [  dist(v, S)  `], where v
is chosen uniformly at random in the graph, S is the set of solution nodes, and dist(v, S) denotes
the length of the shortest path from v to a node of S. As
|S|  b`
|{v | dist(v, S)  `/}|

Pr[dist(v, S)  `/] =
v
|V |
|V |

1

in any b-regular graph, we can only expect the relation of equation (18) to hold past the threshold
value `0   logb (|S|/|V |).

Acknowledgments
We wish to thank anonymous reviewers for their constructive comments. Author Hang Dinh was
supported by IU South Bend Faculty Research Grant. Author Laurent Michel was supported by
the NSF under grant #0642906. Author Alexander Russell was supported by the NSF under grant
#1117426.

718

fiThe Time Complexity of A with Approximate Heuristics

Appendix A: Tables of Experimental Results
#

n

Heuristic
error 

1

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

Total
node
expansions
E
5627
5882
167660
211946
772257
1470135
6118255
7154310
7347748

2

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

3

23
23
23
23
23
23
23
23
23

4

Optimal
solution
depth d

Search
time
(seconds)

11
11
11
11
11
11
11
11
11

125
101
858
744
1341
1318
2025
1653
1101

10887/200
10887/200
10887/200
10887/200
10887/200
10887/200
10887/200
10887/200

44481
45537
372163
474221
1358735
2508134
3508255
3569052
3857597

9
9
9
9
9
9
9
9
9

622
507
1497
1293
1751
1734
1469
1047
566

7820/157
7820/157
7820/157
7820/157
7820/157
7820/157
7820/157
7820/157

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

94
125
5528
9002
31800
109080
879884
1477032
1636093

6
6
6
6
6
6
6
6
6

6
7
98
105
206
301
707
560
224

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

3696
21847
44166
53464
253321
760792
1975195
2317663
2574876

7
7
7
7
7
7
7
7
7

5

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

23645
30501
72597
308417
968504
1681026
1833872
1833644
2132977

6

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

7

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

Effective
branching

d
factor
E
2.192473
2.201325
2.985026
3.049315
3.42966
3.636376
4.139674
4.198968
4.209164

log10 E

Linear
fit to
log10 E

3.7503
3.7695
5.2244
5.3262
5.8878
6.1674
6.7866
6.8546
6.8662

3.7956
4.2742
4.7529
5.2315
5.7102
6.1888
6.6674
7.1461

3.284459
3.293033
4.158822
4.272327
4.802412
5.140898
5.336203
5.3464
5.392783

4.6482
4.6584
5.5707
5.6760
6.1331
6.3994
6.5451
6.5526
6.5863

4.7018
5.0078
5.3139
5.6199
5.9259
6.2320
6.5380
6.8441

5991/121
5991/121
5991/121
5991/121
5991/121
5991/121
5991/121
5991/121

2.132331
2.236068
4.204955
4.560962
5.628654
6.912326
9.788983
10.671652
10.855121

1.9731
2.0969
3.7426
3.9543
4.5024
5.0377
5.9444
6.1694
6.2138

1.9674
2.5989
3.2304
3.8619
4.4934
5.1248
5.7563
6.3878

86
256
303
258
553
788
957
694
383

6343/154
6343/154
6343/154
6343/154
6343/154
6343/154
6343/154
6343/154

3.233523
4.16786
4.608759
4.73628
5.914977
6.921191
7.93182
8.115082
8.23801

3.5677
4.3394
4.6451
4.7281
5.4037
5.8813
6.2956
6.3651
6.4108

3.7471
4.1489
4.5506
4.9524
5.3541
5.7558
6.1576
6.5593

7
7
7
7
7
7
7
7
7

305
285
429
754
1074
1047
823
585
306

6785/205
6785/205
6785/205
6785/205
6785/205
6785/205
6785/205
6785/205

4.215217
4.371357
4.947855
6.083628
7.164029
7.751179
7.848145
7.848005
8.019382

4.3737
4.4843
4.8609
5.4891
5.9861
6.2256
6.2634
6.2633
6.3290

4.3803
4.6983
5.0163
5.3343
5.6523
5.9703
6.2883
6.6064

1981
12316
21699
26575
131561
395118
1080314
1282206
1482293

6
6
6
6
6
6
6
6
6

46
139
151
131
290
431
547
409
219

5012/148
5012/148
5012/148
5012/148
5012/148
5012/148
5012/148
5012/148

3.543894
4.80557
5.281289
5.462761
7.131615
8.566192
10.129585
10.423006
10.677978

3.2969
4.0905
4.3364
4.4245
5.1191
5.5967
6.0336
6.1080
6.1709

3.4645
3.8677
4.2709
4.6741
5.0773
5.4805
5.8837
6.2869

1834
1956
2039
23275
30974
173886
675468
3440759
3793204

7
7
7
7
7
7
7
7
7

51
43
36
159
138
332
526
984
568

6187/122
6187/122
6187/122
6187/122
6187/122
6187/122
6187/122
6187/122

2.925499
2.952538
2.970119
4.20573
4.380978
5.605434
6.80457
8.586333
8.706789

3.2634
3.2914
3.3094
4.3669
4.4910
5.2403
5.8296
6.5367
6.5790

2.8110
3.3053
3.7996
4.2939
4.7882
5.2825
5.7768
6.2711

R2

0.9395

0.9183

0.9647

0.9710

0.9161

0.9696

0.9436

Table 1: Results for the Knapsack instances of type Strongly Correlated.

719

fiDinh, Dinh, Michel, & Russell

#

n

Heuristic
error 

8

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

Total
node
expansions
E
8299
8741
58455
93500
216413
536713
2569955
4096150
4434697

9

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

10

23
23
23
23
23
23
23
23
23

11

Optimal
solution
depth d

Search
time
(seconds)

8
8
8
8
8
8
8
8
8

129
105
335
332
479
558
1066
1027
655

6400/153
6400/153
6400/153
6400/153
6400/153
6400/153
6400/153
6400/153

430
460
5313
9507
11268
88158
790402
2008558
2206805

6
6
6
6
6
6
6
6
6

19
16
84
91
75
229
646
673
334

5835/121
5835/121
5835/121
5835/121
5835/121
5835/121
5835/121
5835/121

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

14162
15321
178178
214332
872080
2128661
3942938
4543001
4924992

9
9
9
9
9
9
9
9
9

192
162
669
574
1052
1306
1379
1118
721

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

315
330
974
22374
26883
199464
783863
2579423
2773773

7
7
7
7
7
7
7
7
7

12

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

1029
1163
1310
3968
14820
75333
363263
1710935
1915195

13

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

14

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

Effective
branching

d
factor
E
3.089429
3.109533
3.943235
4.181686
4.644195
5.202568
6.327624
6.707288
6.7742

log10 E

Linear
fit to
log10 E

3.9190
3.9416
4.7668
4.9708
5.3353
5.7297
6.4099
6.6124
6.6469

3.7753
4.1854
4.5955
5.0056
5.4157
5.8258
6.2359
6.6460

2.747334
2.778388
4.177245
4.602643
4.734867
6.671297
9.615562
11.232611
11.410219

2.6335
2.6628
3.7253
3.9780
4.0518
4.9453
5.8978
6.3029
6.3438

2.3749
2.9177
3.4605
4.0033
4.5461
5.0889
5.6317
6.1745

6762/171
6762/171
6762/171
6762/171
6762/171
6762/171
6762/171
6762/171

2.892252
2.917641
3.832024
3.911497
4.571533
5.048042
5.405911
5.491674
5.541159

4.1511
4.1853
5.2509
5.3311
5.9406
6.3281
6.5958
6.6573
6.6924

4.1618
4.5599
4.9579
5.3560
5.7541
6.1521
6.5502
6.9482

19
15
29
232
195
514
751
880
406

6465/106
6465/106
6465/106
6465/106
6465/106
6465/106
6465/106
6465/106

2.274582
2.289748
2.672619
4.182076
4.293214
5.716412
6.950792
8.240087
8.326044

2.4983
2.5185
2.9886
4.3497
4.4295
5.2999
5.8942
6.4115
6.4431

2.1619
2.7724
3.3830
3.9935
4.6040
5.2146
5.8251
6.4356

5
5
5
5
5
5
5
5
5

35
29
25
50
92
212
380
589
283

5073/106
5073/106
5073/106
5073/106
5073/106
5073/106
5073/106
5073/106

4.003899
4.103136
4.201983
5.244624
6.826053
9.449244
12.943277
17.646017
18.048562

3.0124
3.0656
3.1173
3.5986
4.1708
4.8770
5.5602
6.2332
6.2822

2.5015
2.9880
3.4746
3.9611
4.4477
4.9342
5.4208
5.9073

6701
7084
43514
71911
85427
376321
1441947
1963475
2154280

7
7
7
7
7
7
7
7
7

154
127
379
383
313
573
862
655
324

6072/122
6072/122
6072/122
6072/122
6072/122
6072/122
6072/122
6072/122

3.520395
3.548459
4.598978
4.941148
5.064232
6.259049
7.583154
7.925079
8.030775

3.8261
3.8503
4.6386
4.8568
4.9316
5.5756
6.1589
6.2930
6.3333

3.6957
4.0730
4.4504
4.8277
5.2050
5.5824
5.9597
6.3371

418
3629
7016
8503
51480
178163
550403
668276
784088

5
5
5
5
5
5
5
5
5

15
63
74
62
162
258
352
246
110

4636/140
4636/140
4636/140
4636/140
4636/140
4636/140
4636/140
4636/140

3.343761
5.151781
5.877842
6.108217
8.756443
11.22441
14.064884
14.621475
15.096385

2.6212
3.5598
3.8461
3.9296
4.7116
5.2508
5.7407
5.8250
5.8944

2.8386
3.2949
3.7512
4.2075
4.6637
5.1200
5.5763
6.0326

R2

0.9782

0.9571

0.9461

0.9689

0.9314

0.9646

0.9676

Table 2: Results for the Knapsack instances of type Strongly Correlated.

720

fiThe Time Complexity of A with Approximate Heuristics

#

n

Heuristic
error 

15

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

Total
node
expansions
E
15713
17658
126261
172936
511397
809884
814774
814389
1004228

16

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

17

23
23
23
23
23
23
23
23
23

18

Optimal
solution
depth d

Search
time
(seconds)

6
6
6
6
6
6
6
6
6

218
184
536
466
647
600
435
291
140

5825/211
5825/211
5825/211
5825/211
5825/211
5825/211
5825/211
5825/211

1851
1870
2504
2551
22976
43228
267829
2798746
7270715

9
9
9
9
9
9
9
9
9

44
36
35
29
113
122
283
842
1104

7275/117
7275/117
7275/117
7275/117
7275/117
7275/117
7275/117
7275/117

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

656
665
711
17143
28608
190546
844063
2558990
2749381

7
7
7
7
7
7
7
7
7

33
26
21
192
194
514
813
895
405

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

683
772
18190
24869
136138
308550
2311528
4805568
5201719

8
8
8
8
8
8
8
8
8

19

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

2854
3140
11500
38170
51667
270043
1107776
2600747
2854529

20

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

158012
505837
589456
700571
682245
682583
682855
682455
906305

h ([n])/m

Effective
branching

d
factor
E
5.004682
5.102977
7.082907
7.464159
8.942515
9.654663
9.664355
9.663593
10.007034

log10 E

Linear
fit to
log10 E

4.1963
4.2469
5.1013
5.2379
5.7088
5.9084
5.9110
5.9108
6.0018

4.3104
4.5868
4.8631
5.1395
5.4159
5.6922
5.9686
6.2450

2.306987
2.309606
2.385756
2.390691
3.052011
3.274048
4.009547
5.203904
5.786254

3.2674
3.2718
3.3986
3.4067
4.3613
4.6358
5.4279
6.4470
6.8616

2.7061
3.1549
3.6038
4.0526
4.5015
4.9503
5.3992
5.8480

6501/102
6501/102
6501/102
6501/102
6501/102
6501/102
6501/102
6501/102

2.525892
2.530814
2.555112
4.025961
4.331527
5.679181
7.024655
8.23073
8.315545

2.8169
2.8228
2.8519
4.2341
4.4565
5.2800
5.9264
6.4081
6.4392

2.3428
2.9162
3.4895
4.0629
4.6363
5.2096
5.7830
6.3564

14
13
114
107
280
323
889
1083
790

6012/164
6012/164
6012/164
6012/164
6012/164
6012/164
6012/164
6012/164

2.261011
2.295896
3.407839
3.543703
4.382757
4.854737
6.244352
6.842552
6.910641

2.8344
2.8876
4.2598
4.3957
5.1340
5.4893
6.3639
6.6817
6.7161

2.7250
3.3052
3.8855
4.4657
5.0459
5.6262
6.2064
6.7866

7
7
7
7
7
7
7
7
7

65
56
121
210
185
412
682
772
415

5503/119
5503/119
5503/119
5503/119
5503/119
5503/119
5503/119
5503/119

3.116279
3.159085
3.802767
4.51369
4.713203
5.969239
7.302863
8.249784
8.360249

3.4555
3.4969
4.0607
4.5817
4.7132
5.4314
6.0445
6.4151
6.4555

3.2041
3.6529
4.1017
4.5505
4.9993
5.4481
5.8969
6.3457

7
7
7
7
7
7
7
7
7

866
1173
965
797
631
484
357
235
123

6592/295
6592/295
6592/295
6592/295
6592/295
6592/295
6592/295
6592/295

5.529298
6.52918
6.673447
6.840134
6.814281
6.814763
6.815151
6.814581
7.096418

5.1987
5.7040
5.7705
5.8455
5.8339
5.8342
5.8343
5.8341
5.9573

5.5119
5.5748
5.6376
5.7005
5.7633
5.8262
5.8890
5.9518

R2

0.8788

0.8698

0.9498

0.9729

0.9770

0.4860

Table 3: Results for the Knapsack instances of type Strongly Correlated.

721

fiDinh, Dinh, Michel, & Russell

Linear fit
to log10 E
5.8687
5.8803
5.8919
5.9036
5.9152
5.9268
5.9384
5.9500

#

n

Total
node
expansions E
731425
761013
782339
805295
828252
845545
865626
885943
900630

Optimal
soln. depth d
11
15
12
12
12
10
11
14
13

Search time,
seconds
1090
878
716
579
463
360
267
179
80

log10 E

20
20
20
20
20
20
20
20
20

Heuristic
error 
0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

1

5509/28
5509/28
5509/28
5509/28
5509/28
5509/28
5509/28
5509/28

5.8642
5.8814
5.8934
5.9060
5.9182
5.9271
5.9373
5.9474
5.9545

2

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

67164
71824
76627
80614
84553
90166
96506
99536
104144

6
9
7
8
8
9
7
7
8

259
208
168
136
107
82
58
35
10

2984/28
2984/28
2984/28
2984/28
2984/28
2984/28
2984/28
2984/28

4.8271
4.8563
4.8844
4.9064
4.9271
4.9550
4.9846
4.9980
5.0176

4.8311
4.8558
4.8804
4.9050
4.9297
4.9543
4.9790
5.0036

3

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

222293
232989
244871
256250
266235
274056
279890
283160
291239

11
12
8
9
9
8
11
9
9

533
432
353
285
226
173
126
81
28

3687/26
3687/26
3687/26
3687/26
3687/26
3687/26
3687/26
3687/26

5.3469
5.3673
5.3889
5.4087
5.4253
5.4378
5.4470
5.4520
5.4642

5.3552
5.3706
5.3861
5.4015
5.4170
5.4324
5.4479
5.4633

4

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

290608
304974
313598
323477
331235
336665
340874
342644
360837

10
10
9
9
9
10
9
9
8

329
272
225
185
151
121
92
64
33

3883/56
3883/56
3883/56
3883/56
3883/56
3883/56
3883/56
3883/56

5.4633
5.4843
5.4964
5.5098
5.5201
5.5272
5.5326
5.5348
5.5573

5.4734
5.4834
5.4935
5.5035
5.5136
5.5237
5.5337
5.5438

5

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

851515
873968
893378
912734
927408
940724
950209
958343
967863

11
14
12
14
13
12
12
13
12

740
609
498
410
335
267
206
142
88

7731/77
7731/77
7731/77
7731/77
7731/77
7731/77
7731/77
7731/77

5.9302
5.9415
5.9510
5.9603
5.9673
5.9735
5.9778
5.9815
5.9858

5.9348
5.9421
5.9494
5.9567
5.9641
5.9714
5.9787
5.9860

6

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

75858
81410
88494
94585
100329
106409
110656
114601
117496

10
8
6
9
7
5
9
9
4

488
363
287
225
177
134
94
55
11

2327/11
2327/11
2327/11
2327/11
2327/11
2327/11
2327/11
2327/11

4.8800
4.9107
4.9469
4.9758
5.0014
5.0270
5.0440
5.0592
5.0700

4.8895
4.9155
4.9416
4.9676
4.9936
5.0197
5.0457
5.0717

7

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

712138
748095
778565
799378
823236
844925
870175
897407
909075

11
12
15
11
13
13
13
12
14

1178
947
765
618
490
378
280
185
80

6456/33
6456/33
6456/33
6456/33
6456/33
6456/33
6456/33
6456/33

5.8526
5.8740
5.8913
5.9028
5.9155
5.9268
5.9396
5.9530
5.9586

5.8590
5.8727
5.8864
5.9001
5.9138
5.9275
5.9412
5.9549

R2

0.9918

0.9959

0.9649

0.9369

0.9687

0.9833

0.9895

Table 4: Results for the Knapsack instances of type Subset Sum.

722

fiThe Time Complexity of A with Approximate Heuristics

Linear fit
to log10 E
5.4113
5.4416
5.4719
5.5023
5.5326
5.5629
5.5932
5.6236

#

n

Total
node
expansions E
252054
279643
299328
324182
340530
361756
385942
423848
454094

Optimal
soln. depth d
10
12
9
11
9
10
10
9
9

Search time,
seconds
2274
1607
1159
878
666
494
344
201
42

log10 E

20
20
20
20
20
20
20
20
20

Heuristic
error 
0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

8

3514/7
3514/7
3514/7
3514/7
3514/7
3514/7
3514/7
3514/7

5.4015
5.4466
5.4761
5.5108
5.5322
5.5584
5.5865
5.6272
5.6571

9

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

284146
301301
318308
330924
338590
345335
351027
356374
369094

9
8
7
9
9
9
10
10
8

628
507
412
334
263
203
146
92
34

4494/34
4494/34
4494/34
4494/34
4494/34
4494/34
4494/34
4494/34

5.4535
5.4790
5.5028
5.5197
5.5297
5.5382
5.5453
5.5519
5.5671

5.4677
5.4812
5.4947
5.5083
5.5218
5.5353
5.5489
5.5624

10

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

812828
852539
881657
903389
923450
941277
954861
970871
985526

11
13
15
12
15
11
14
14
12

1078
874
711
579
466
356
266
180
88

6963/39
6963/39
6963/39
6963/39
6963/39
6963/39
6963/39
6963/39

5.9100
5.9307
5.9453
5.9559
5.9654
5.9737
5.9799
5.9872
5.9937

5.9193
5.9298
5.9403
5.9508
5.9613
5.9717
5.9822
5.9927

11

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

872387
892404
907719
920529
930373
939495
945766
948094
961185

12
13
12
12
12
13
12
11
11

527
441
366
306
260
214
169
125
85

7270/102
7270/102
7270/102
7270/102
7270/102
7270/102
7270/102
7270/102

5.9407
5.9506
5.9580
5.9640
5.9687
5.9729
5.9758
5.9769
5.9828

5.9456
5.9507
5.9558
5.9609
5.9660
5.9711
5.9762
5.9813

12

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

544749
572592
596732
622826
644836
662145
682257
705866
720827

13
8
12
9
11
12
11
11
13

997
804
656
528
420
329
242
158
64

5752/35
5752/35
5752/35
5752/35
5752/35
5752/35
5752/35
5752/35

5.7362
5.7578
5.7758
5.7944
5.8094
5.8210
5.8339
5.8487
5.8578

5.7422
5.7579
5.7736
5.7893
5.8050
5.8207
5.8364
5.8521

13

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

592766
628513
662306
684651
713728
745263
781953
824260
861415

10
10
11
13
12
10
11
11
11

1824
1319
1040
828
645
487
344
216
74

7445/30
7445/30
7445/30
7445/30
7445/30
7445/30
7445/30
7445/30

5.7729
5.7983
5.8211
5.8355
5.8535
5.8723
5.8932
5.9161
5.9352

5.7767
5.7963
5.8159
5.8355
5.8552
5.8748
5.8944
5.9140

14

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

137368
148933
157793
165368
172383
179983
186068
191426
197634

8
7
10
9
9
7
9
8
10

561
450
363
289
226
173
123
73
18

3509/22
3509/22
3509/22
3509/22
3509/22
3509/22
3509/22
3509/22

5.1379
5.1730
5.1981
5.2185
5.2365
5.2552
5.2697
5.2820
5.2959

5.1513
5.1713
5.1913
5.2113
5.2314
5.2514
5.2714
5.2914

R2

0.9925

0.9282

0.9593

0.9409

0.9901

0.9963

0.9761

Table 5: Results for the Knapsack instances of type Subset Sum.

723

fiDinh, Dinh, Michel, & Russell

Linear fit
to log10 E
4.5311
4.5760
4.6209
4.6658
4.7107
4.7556
4.8006
4.8455

#

n

Total
node
expansions E
34937
38617
41757
45036
49231
54428
62409
75602
84284

Optimal
soln. depth d
9
6
10
9
10
7
9
7
8

Search time,
seconds
1022
772
529
353
272
186
128
72
8

log10 E

20
20
20
20
20
20
20
20
20

Heuristic
error 
0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

15

3124/9
3124/9
3124/9
3124/9
3124/9
3124/9
3124/9
3124/9

4.5433
4.5868
4.6207
4.6536
4.6922
4.7358
4.7952
4.8785
4.9257

16

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

476547
498939
523867
558927
592373
626403
668497
725325
768536

10
11
10
10
9
10
12
12
13

3224
2097
1536
1181
911
675
468
281
71

5442/11
5442/11
5442/11
5442/11
5442/11
5442/11
5442/11
5442/11

5.6781
5.6980
5.7192
5.7474
5.7726
5.7969
5.8251
5.8605
5.8857

5.6718
5.6976
5.7235
5.7493
5.7751
5.8010
5.8268
5.8527

17

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

641544
666837
702032
737893
772405
810089
852271
902227
964897

15
11
12
14
14
14
14
12
14

3751
2791
1991
1495
1124
827
570
337
86

7157/11
7157/11
7157/11
7157/11
7157/11
7157/11
7157/11
7157/11

5.8072
5.8240
5.8464
5.8680
5.8878
5.9085
5.9306
5.9553
5.9845

5.8045
5.8256
5.8468
5.8679
5.8891
5.9102
5.9313
5.9525

18

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

321490
338267
358571
379827
399061
419052
443204
486366
508524

9
10
9
10
9
10
9
10
10

1215
952
760
600
466
356
252
157
47

4631/20
4631/20
4631/20
4631/20
4631/20
4631/20
4631/20
4631/20

5.5072
5.5293
5.5546
5.5796
5.6010
5.6223
5.6466
5.6870
5.7063

5.5047
5.5293
5.5540
5.5786
5.6033
5.6279
5.6525
5.6772

19

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

104698
110845
116893
122710
128398
131887
133658
134205
142348

7
8
10
8
6
9
10
5
8

251
206
169
137
110
84
60
37
13

3373/44
3373/44
3373/44
3373/44
3373/44
3373/44
3373/44
3373/44

5.0199
5.0447
5.0678
5.0889
5.1086
5.1202
5.1260
5.1278
5.1534

5.0322
5.0482
5.0641
5.0800
5.0959
5.1119
5.1278
5.1437

20

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

275501
286961
296924
305914
315286
322234
324077
324471
348398

10
9
9
7
9
8
9
10
9

352
292
240
196
159
126
94
65
32

5262/94
5262/94
5262/94
5262/94
5262/94
5262/94
5262/94
5262/94

5.4401
5.4578
5.4726
5.4856
5.4987
5.5082
5.5106
5.5112
5.5421

5.4489
5.4594
5.4699
5.4804
5.4909
5.5013
5.5118
5.5223

R2

0.9739

0.9947

0.9988

0.9932

0.9349

0.9299

Table 6: Results for the Knapsack instances of type Subset Sum.

724

fiThe Time Complexity of A with Approximate Heuristics

Effective
branching
factor
E 1/k
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.11793532
1.12483883
1.13029527
1.13481129
1.13744262
1.13983570
1.14203051
1.14306342
1.14405770
1.15327789
1.19493326
1.20344942
1.21724042
1.22842928
1.23335496
1.24222170
1.24615895
1.24988516
1.25689894
1.26697571
1.28154406
1.28838000
1.29446689
1.29905304
1.30420852
1.30895150
1.31267920
1.31682768
1.32748452
1.34592652

Upper
bound
B(d)1/k

B(d)1/k

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03
0.0325
0.035
0.0375
0.04
0.0425
0.045
0.0475
0.05
0.0525
0.055
0.0575
0.06
0.0625
0.065
0.0675
0.07
0.0725
0.075
0.0775
0.08
0.0825
0.085
0.0875
0.09
0.0925
0.095
0.0975

Total
node
expansions
E
87
87
87
87
87
87
87
87
87
87
135
177
219
261
289
317
345
359
373
531
2530
3458
5709
8539
10183
13956
16041
18293
23400
33251
54989
69492
85507
99904
118924
139520
158117
181666
258998
475269

1.12498287
1.12509476
1.12524953
1.12546320
1.12575740
1.12616102
1.12671203
1.12745936
1.12846421
1.12980027
1.29413023
1.29413756
1.29414775
1.29416190
1.29418158
1.29420890
1.29424685
1.29429954
1.29437264
1.48549510
1.48549548
1.48549601
1.48549674
1.48549775
1.48549917
1.48550113
1.48550386
1.48550766
1.68167021
1.68167024
1.68167029
1.68167036
1.68167046
1.68167059
1.68167077
1.68167103
1.68167138
1.88726770
1.88726771
1.88726771

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03
0.0325
0.035
0.0375
0.04
0.0425
0.045
0.0475
0.05
0.0525
0.055
0.0575
0.06
0.0625
0.065
0.0675
0.07

125
125
125
125
125
125
125
295
599
789
979
1093
1207
1759
8006
18159
31829
39898
53605
63934
151470
240217
418262
569663
823942
1.03E+06
1.39E+06
3.35E+06
6.43E+06

1.07965322
1.07965322
1.07965322
1.07965322
1.07965322
1.07965322
1.07965322
1.09446915
1.10684330
1.11169422
1.11550813
1.11746021
1.11922136
1.12593198
1.15334431
1.16843520
1.17889026
1.18312592
1.18868491
1.19201428
1.20844644
1.21732463
1.22808758
1.23412462
1.24137536
1.24580697
1.25172483
1.26929396
1.28251719

1.09187259
1.09196102
1.09210593
1.09234240
1.09272569
1.09334029
1.09430956
1.21404534
1.21404945
1.21405622
1.21406738
1.21408579
1.21411611
1.34843434
1.34843446
1.34843466
1.34843500
1.34843555
1.34843647
1.34843797
1.48271141
1.48271142
1.48271144
1.48271147
1.48271152
1.48271161
1.62031036
1.62031036
1.62031037

#

n

k

Heuristic
error 

1

10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10

44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44

2

12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12

63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63

log10 E

Linear
fit to
log10 E

1.01640297
1.01650406
1.01664390
1.01683694
1.01710275
1.01746741
1.01796524
1.01864044
1.01954830
1.02075541
1.15760743
1.15050932
1.14496431
1.14042036
1.13779944
1.13543461
1.13328571
1.13230772
1.13138755
1.28806345
1.24316189
1.23436514
1.22038072
1.20926599
1.20443766
1.19584220
1.19206612
1.18851532
1.33795181
1.32731056
1.31222199
1.30525960
1.29912203
1.29453574
1.28941863
1.28474663
1.28109852
1.43319261
1.42168717
1.40220709

1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
2.1303
2.2480
2.3404
2.4166
2.4609
2.5011
2.5378
2.5551
2.5717
2.7251
3.4031
3.5388
3.7566
3.9314
4.0079
4.1448
4.2052
4.2623
4.3692
4.5218
4.7403
4.8419
4.9320
4.9996
5.0753
5.1446
5.1990
5.2593
5.4133
5.6769

1.2674
1.3758
1.4843
1.5928
1.7013
1.8097
1.9182
2.0267
2.1352
2.2436
2.3521
2.4606
2.5691
2.6775
2.7860
2.8945
3.0030
3.1114
3.2199
3.3284
3.4369
3.5454
3.6538
3.7623
3.8708
3.9793
4.0877
4.1962
4.3047
4.4132
4.5216
4.6301
4.7386
4.8471
4.9555
5.0640
5.1725
5.2810
5.3894
5.4979

1.01131786
1.01139977
1.01153399
1.01175301
1.01210802
1.01267728
1.01357504
1.10925497
1.09685756
1.09207747
1.08835368
1.08646892
1.08478640
1.19761616
1.16915170
1.15405173
1.14381723
1.13972277
1.13439352
1.13122636
1.22695666
1.21800824
1.20733363
1.20142768
1.19441030
1.19016159
1.29446211
1.27654461
1.26338296

2.0969
2.0969
2.0969
2.0969
2.0969
2.0969
2.0969
2.4698
2.7774
2.8971
2.9908
3.0386
3.0817
3.2453
3.9034
4.2591
4.5028
4.6010
4.7292
4.8057
5.1803
5.3806
5.6214
5.7556
5.9159
6.0134
6.1431
6.5244
6.8080

1.3953
1.5797
1.7641
1.9485
2.1328
2.3172
2.5016
2.6860
2.8704
3.0547
3.2391
3.4235
3.6079
3.7923
3.9767
4.1610
4.3454
4.5298
4.7142
4.8986
5.0829
5.2673
5.4517
5.6361
5.8205
6.0049
6.1892
6.3736
6.5580

E 1/k

R2

0.9482

0.9693

Table 7: Results for partial Latin square instances.

725

fiDinh, Dinh, Michel, & Russell

Effective
branching
factor
E 1/k
1.06161017
1.06161017
1.06161017
1.06161017
1.06161017
1.06615920
1.07302532
1.07624311
1.07854600
1.07979831
1.10835983
1.12621485
1.13582148
1.14198131
1.14598774
1.15596951
1.16381138
1.16872905
1.17320907
1.17776024

Upper
bound
B(d)1/k

B(d)1/k

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03
0.0325
0.035
0.0375
0.04
0.0425
0.045
0.0475

Total
node
expansions
E
171
171
171
171
171
247
429
555
667
737
6959
27506
57104
90923
122879
259053
463344
665871
925306
1.29E+06

1.07034588
1.07042098
1.07057335
1.07087962
1.07148429
1.16291112
1.16291347
1.16291827
1.16292811
1.16294821
1.26274158
1.26274166
1.26274182
1.26274214
1.36083647
1.36083648
1.36083648
1.36083649
1.36083651
1.45985179

113
113
113
113
113
113
113
113
113
113
113
113
113

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03

225
225
225
225
799
1719
2317
2731
50236
144797
258735
516942
1.97E+06

1.04909731
1.04909731
1.04909731
1.04909731
1.06092884
1.06814635
1.07097198
1.07253118
1.10053004
1.11088842
1.11660964
1.12346988
1.13686805

18
18
18
18
18
18
18
18
18

143
143
143
143
143
143
143
143
143

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02

285
285
285
743
2579
3659
39137
246338
535932

20
20
20
20
20
20
20

176
176
176
176
176
176
176

0
0.0025
0.005
0.0075
0.01
0.0125
0.015

351
351
351
2425
4125
107153
619190

#

n

k

Heuristic
error 

3

14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14

86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86

4

16
16
16
16
16
16
16
16
16
16
16
16
16

5

6

log10 E

Linear
fit to
log10 E

1.00822873
1.00829948
1.00844300
1.00873150
1.00930108
1.09074810
1.08377077
1.08053493
1.07823691
1.07700503
1.13928848
1.12122626
1.11174321
1.10574676
1.18747908
1.17722523
1.16929298
1.16437295
1.15992669
1.23951526

2.2330
2.2330
2.2330
2.2330
2.2330
2.3927
2.6325
2.7443
2.8241
2.8675
3.8425
4.4394
4.7567
4.9587
5.0895
5.4134
5.6659
5.8234
5.9663
6.1109

1.4986
1.7445
1.9904
2.2363
2.4822
2.7281
2.9740
3.2199
3.4658
3.7117
3.9576
4.2035
4.4494
4.6953
4.9412
5.1871
5.4330
5.6789
5.9248
6.1707

1.05563497
1.05570312
1.05588217
1.05634285
1.12838087
1.12838284
1.12838808
1.12840202
1.20572650
1.20572656
1.20572671
1.28088203
1.28088203

1.00623170
1.00629666
1.00646733
1.00690645
1.06357828
1.05639348
1.05361121
1.05209251
1.09558708
1.08537144
1.07981041
1.14011248
1.12667607

2.3522
2.3522
2.3522
2.3522
2.9025
3.2353
3.3649
3.4363
4.7010
5.1608
5.4129
5.7134
6.2952

1.6772
2.0340
2.3907
2.7475
3.1042
3.4610
3.8178
4.1745
4.5313
4.8881
5.2448
5.6016
5.9584

1.04031952
1.04031952
1.04031952
1.04731385
1.05646789
1.05905525
1.07675277
1.09069423
1.09663904

1.04542550
1.04549145
1.04572413
1.10463010
1.10463134
1.10463580
1.16693654
1.16693656
1.16693665

1.00490809
1.00497148
1.00519515
1.05472691
1.04558912
1.04303887
1.08375532
1.06990257
1.06410278

2.4548
2.4548
2.4548
2.8710
3.4115
3.5634
4.5926
5.3915
5.7291

1.8665
2.3144
2.7623
3.2103
3.6582
4.1061
4.5540
5.0019
5.4498

1.03386057
1.03386057
1.03386057
1.04527681
1.04843662
1.06802045
1.07871841

1.03797380
1.03804139
1.03837263
1.08739144
1.08739402
1.13899860
1.13899862

1.00397851
1.00404389
1.00436428
1.04029040
1.03715761
1.06645766
1.05588132

2.5453
2.5453
2.5453
3.3847
3.6154
5.0300
5.7918

1.9462
2.5098
3.0733
3.6368
4.2004
4.7639
5.3275

E 1/k

R2

0.9335

0.9274

0.9120

0.8698

Table 8: Results for partial Latin square instances.

726

fiThe Time Complexity of A with Approximate Heuristics

References
Aaronson, S. (2004). Lower bounds for local search by quantum arguments. In Proceedings of the
36th Annual ACM Symposium on Theory of Computing (STOC). ACM Press.
Babai, L. (1991). Local expansion of vertex-transitive graphs and random generation in finite groups.
In Proceedings of the 23rd annual ACM symposium on Symposium of Theory of Computing,
pp. 164174.
Chernoff, H. (1952). A measure of asymptotic efficiency for tests of hypothesis based on the sum of
observations. Annals of Mathematical Statistics, 23, 493507.
Chung, F. (2006). The diameter and Laplacian eigenvalues of directed graphs. Electronic Journal
of Combinatorics, 13 (4).
Chung, F. R. K. (1997). Spectral Graph Theory. American Mathematical Society.
Colbourn, C. J. (1984). The complexity of completing partial Latin squares. Discrete Applied
Mathematics, 8 (1), 2530.
Davis, H., Bramanti-Gregor, A., & Wang, J. (1988). The advantages of using depth and breadth
components in heuristic search. In Ras, Z., & Saitta, L. (Eds.), Proceedings of the Third
International Symposium on Methodologies for Intelligent Systems, pp. 1928, North-Holland,
Amsterdam. Elsevier.
Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies and the optimality of A*. J.
ACM, 32 (3), 505536.
Demaine, E. D. (2001). Playing games with algorithms: Algorithmic combinatorial game theory.
In Proc. 26th Symp. on Math Found. in Comp. Sci., Lect. Notes in Comp. Sci., pp. 1832.
Springer-Verlag.
Dinh, H., Russell, A., & Su, Y. (2007). On the value of good advice: The complexity of A* with
accurate heuristics. In Proceedings of the Twenty-Second Conference on Artificial Intelligence
(AAAI-07), pp. 11401145.
Edelkamp, S. (2001). Prediction of regular search tree growth by spectral analysis. In Proceedings
of the Joint German/Austrian Conference on AI: Advances in Artificial Intelligence, KI 01,
pp. 154168, London, UK, UK. Springer-Verlag.
Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. Journal of
Artificial Intelligence Research, 22, 279318.
Friedman, J. (2003). A proof of Alons second eigenvalue conjecture. In STOC 03: Proceedings of
the thirty-fifth annual ACM symposium on Theory of computing, pp. 720724, New York, NY,
USA. ACM.
Gaschnig, J. (1979). Perfomance measurement and analysis of certain search algorithms. Ph.D.
thesis, Carnegie-Mellon University, Pittsburgh, PA.
Gomes, C., & Shmoys, D. (2002). Completing quasigroups or Latin squares: A structured graph
coloring problem. In Johnson, D. S., Mehrotra, A., & Trick, M. (Eds.), Proceedings of the
Computational Symposium on Graph Coloring and its Generalizations, pp. 2239, Ithaca, New
York, USA.
Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through randomization. In AAAI 98/IAAI 98: Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, pp. 431437, Menlo Park,
CA, USA. American Association for Artificial Intelligence.
Hart, P., Nilson, N., & Raphael, B. (1968). A formal basis for the heuristic determination of minimum
cost paths. IEEE Transactions on Systems Science and Cybernetics, SCC-4 (2), 100107.
727

fiDinh, Dinh, Michel, & Russell

Helmert, M., & Roger, G. (2008). How good is almost perfect?. In Proceedings of AAAI-08.
Hochbaum, D. (1996). Approximation Algorithms for NP-hard Problems. Brooks Cole.
Horn, R., & Johnson, C. (1999). Matrix Analysis. Cambridge University Press, Cambridge, UK.
Huyn, N., Dechter, R., & Pearl, J. (1980). Probabilistic analysis of the complexity of A*. Artificial
Intelligence, 15, 241254.
Ibarra, O. H., & Kim, C. E. (1975). Fast approximation algorithms for the knapsack and sum of
subset problems. Journal of the ACM, 22 (4), 463468.
Karp, R. M. (1972). Reducibility among combinatorial problems. In Miller, R. E., & Thatcher,
J. W. (Eds.), Complexity of Computer Computations, p. 85103. New York: Plenum.
Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack problems. Springer.
Korf, R. (1985). Depth-first iterative deepening: An optimal admissible tree search. Artificial
Intelligence, 27, 97109.
Korf, R., & Reid, M. (1998). Complexity analysis of admissible heuristic search. In Proceedings of
the National Conference on Artificial Intelligence (AAAI-98), pp. 305310.
Korf, R., Reid, M., & Edelkamp, S. (2001). Time complexity of iterative-deepening-A*. Artificial
Intelligence, 129 (1-2), 199218.
Korf, R. E. (2000). Recent progress in the design and analysis of admissible heuristic functions. In
AAAI/IAAI 2000, pp. 11651170. Also in SARA 02: Proceedings of the 4th International
Symposium on Abstraction, Reformulation, and Approximation.
Kumar, R., Russell, A., & Sundaram, R. (1996). Approximating Latin square extensions. In COCOON 96: Proceedings of the Second Annual International Conference on Computing and
Combinatorics, pp. 280289, London, UK. Springer-Verlag.
Laywine, C., & Mullen, G. (1998). Discrete Mathematics using Latin Squares. Interscience Series in
Discrete mathematics and Optimization. Wiley.
Lenstra, A. K., Lenstra, H. W., & Lovasz, L. (1981). Factoring polynomials with rational coefficients.
Tech. rep. 82-05, Universiteit Amsterdam.
Motwani, R., & Raghavan, P. (1995). Randomized Algorithms. Cambridge University Press.
Parberry, I. (1995). A real-time algorithm for the (n2  1)-puzzle. Inf. Process. Lett, 56, 2328.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving. AddisonWesley, MA.
Pisinger, D. (2005). Where are the hard knapsack problems?. Computers and Operations Research,
32, 22712284.
Pohl, I. (1977). Practical and theoretical considerations in heuristic search algorithms. In Elcock,
W., & Michie, D. (Eds.), Machine Intelligence, Vol. 8, pp. 5572. Ellis Horwood, Chichester.
Ratner, D., & Warmuth, M. (1990). The (n2  1)-puzzle and related relocation problems. Journal
for Symbolic Computation, 10 (2), 111137.
Russell, S., & Norvig, P. (1995). Artificial Intelligence - A Modern Approach. Prentice Hall, New
Jersey.
Sen, A. K., Bagchi, A., & Zhang, W. (2004). Average-case analysis of best-first search in two
representative directed acyclic graphs. Artif. Intell., 155 (1-2), 183206.
Tay, T.-S. (1996). Some results on generalized Latin squares. Graphs and Combinatorics, 12, 199
207.
Vazirani, V. (2001). Approximation Algorithms. Springer-Verlag.
728

fiThe Time Complexity of A with Approximate Heuristics

Vazirani, V. (2002). Primal-dual schema based approximation algorithms. In Theoretical Aspects of
Computer Science: Advanced Lectures, pp. 198207. Springer-Verlag, New York.
Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. (2007). Inconsistent heuristics. In Proceedings
of AAAI-07, pp. 12111216.
Zhang, Z., Sturtevant, N. R., Holte, R., Schaeffer, J., & Felner, A. (2009). A* search with inconsistent
heuristics. In Proceedings of the 21st international jont conference on Artifical intelligence,
IJCAI09, pp. 634639, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.

729

fi