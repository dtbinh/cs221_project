Journal of Artificial Intelligence Research 52 (2015) 1-95

Submitted 07/14; published 01/15

Coherent Predictive Inference under Exchangeability
with Imprecise Probabilities
Gert de Cooman
Jasper De Bock

gert.decooman@UGent.be
jasper.debock@UGent.be

Ghent University, SYSTeMS Research Group
TechnologieparkZwijnaarde 914
9052 Zwijnaarde, Belgium

Mrcio Alves Diniz

marcio.alves.diniz@gmail.com

Federal University of So Carlos, Department of Statistics
Rod. Washington Luis, km 235
So Carlos, Brazil

Abstract
Coherent reasoning under uncertainty can be represented in a very general manner by
coherent sets of desirable gambles. In a context that does not allow for indecision, this leads
to an approach that is mathematically equivalent to working with coherent conditional
probabilities. If we do allow for indecision, this leads to a more general foundation for coherent
(imprecise-)probabilistic inference. In this framework, and for a given finite category set,
coherent predictive inference under exchangeability can be represented using Bernstein
coherent cones of multivariate polynomials on the simplex generated by this category set.
This is a powerful generalisation of de Finettis Representation Theorem allowing for both
imprecision and indecision.
We define an inference system as a map that associates a Bernstein coherent cone of
polynomials with every finite category set. Many inference principles encountered in the
literature can then be interpreted, and represented mathematically, as restrictions on such
maps. We discuss, as particular examples, two important inference principles: representation
insensitivitya strengthened version of Walleys representation invarianceand specificity.
We show that there is an infinity of inference systems that satisfy these two principles,
amongst which we discuss in particular the skeptically cautious inference system, the inference
systems corresponding to (a modified version of) Walley and Bernards Imprecise Dirichlet
Multinomial Models (IDMM), the skeptical IDMM inference systems, and the Haldane
inference system. We also prove that the latter produces the same posterior inferences as
would be obtained using Haldanes improper prior, implying that there is an infinity of
proper priors that produce the same coherent posterior inferences as Haldanes improper one.
Finally, we impose an additional inference principle that allows us to characterise uniquely
the immediate predictions for the IDMM inference systems.

1. Introduction
This paper deals with predictive inference for categorical variables. We are therefore concerned
with a (possibly infinite) sequence of variables Xn that assume values in some finite set of
categories A. After having observed a number n of them, and having found that, say X1 = x1 ,
X2 = x2 , . . . , Xn = xn , we consider some subjects belief model for the next n variables
Xn+1 , . . . Xn+n . In the probabilistic traditionand we want to build on this tradition in the
2015 AI Access Foundation. All rights reserved.

fiDe Cooman, De Bock, & Diniz

context of this paperthis belief can be modelled by a conditional predictive probability
mass function pn (|x1 , . . . , xn ) on the set An of their possible values. These probability mass
functions can be used for prediction or estimation, for statistical inferences, and in decision
making involving the uncertain values of these variables. In this sense, predictive inference lies
at the heart of statistics, and more generally, of learning under uncertainty. For this reason,
it is also of crucial importance for dealing with uncertainty in Artificial Intelligence, where
for instance, intelligent systems have to learn about multinomial probabilities, or Markov
transition probabilities, rates of occurrence for phenomena, local probabilities in Bayesian or
credal networks and so on. We refer to the synthesis by Geisser (1993) and the collection of
essays by Zabell (2005) for good introductions to predictive inference and the underlying
issues that the present paper will also be concerned with.
What connects these predictive probability mass functions for various values of n, n and
(x1 , . . . , xn ) are the requirements of time consistency and coherence. The former requires
that when n1  n2 , then pn1 (|x1 , . . . , xn ) can be obtained from pn2 (|x1 , . . . , xn ) through the
usual marginalisation procedure; while the latter essentially demands that these conditional
probability mass functions should be connected with time-consistent unconditional probability
mass functions through Bayess Rule.
A common assumption about the variables Xn is that they are exchangeable, meaning
roughly that the subject believes that the order in which they are observed, or present
themselves, has no influence on the decisions and inferences he will make regarding these
variables. This assumption, and the analysis of its consequences, goes back to de Finetti
(1937) (see also Cifarelli & Regazzini, 1996). His famous Representation Theorem states,
in essence, that the time-consistent and coherent conditional and unconditional predictive
probability mass functions associated with a countably infinite exchangeable sequence of
variables in A are completely characterised by1 and completely characterisea unique
probability measure on the Borel sets of the simplex of all probability mass functions on A,
called their representation.2
This leads us to the central problem of predictive inference: since there is an infinity of
such probability measures on the simplex, which one does a subject choose in a particular
context, and how can a given choice be motivated and justified? The subjectivists of de
Finettis persuasion might answer that this question needs no answer: a subjects personal
predictive probabilities are entirely his, and time consistency and coherence are the only
requirements he should heed. Earlier scholars, like Laplace and Bayes, whom we would now
also call subjectivists, invoked the Principle of Indifference to justify using a specific class of
predictive mass functions. Proponents of the logicist approach to predictive inference would
try enunciating general inference principles in order to narrow down, and hopefully eliminate
entirely, the possible choices for the representing probability measures on the simplex. The
logicians W. E. Johnson (1924) and, in a much more systematic fashion, Rudolf Carnap (1952)
1. . . . unless the observed sequence has probability zero.
2. Actually, in order to clarify the connection with what we shall do later on, the essence of de Finettis
argument is that the representation is a coherent prevision on the set of all multinomial polynomialsor
equivalently, of all continuous real functionson this simplex (De Cooman, Quaeghebeur, & Miranda,
2009b). As a (finitely additive) coherent prevision, it can be extended uniquely only so far as to the set
of all lower semicontinuous functions, but it does determine a unique (countably additive) probability
measure on the Borel sets of that simplex, through the F. Riesz Representation Theorem (De Cooman &
Miranda, 2008a; Troffaes & De Cooman, 2014).

2

fiCoherent Predictive Inference under Exchangeability

tried to develop an axiom system for predictive inference based on such reasonable inference
principles. Carnaps first group of axioms is related to what we have called coherence, but
as we suggested, these by themselves are too weak to single out a particular predictive
model. His second group consisted of invariance axioms, including exchangeability. He also
included an axiom of instantial relevance, translating the intuitive principle that predictive
inferences should actually learn from experience. His last axiom, predictive irrelevance, was
also proposed earlier by Johnson and called the sufficientness postulate by Good (1965).
Armed with these axioms, Carnap was able to derive a continuum of probabilistic inference
rules, closely related to the Dirichlet multinomial model and to the Imprecise Dirichlet
Multinomial Model (IDMM) proposed by Walley (1996) and Walley and Bernard (1999),
which we discuss in Appendices C and D, respectively.
Our point of view holds the middle ground between the subjectivist and logicist positions:
it should be possible for a subject to make assessments for certain predictive probabilities,
and to combine these with certain inference principles he finds reasonable, or which suit his
purpose for the problem at hand. Indeed, the inference systems we introduce and discuss
in Section 6, and the notion of conservative coherent inferenceor natural extensionwe
associate with them, provide an elegant framework and tools for making conservative coherent
predictive inferences that combine (local) subjective probability assessments with (general)
inference principles. And our work in Section 15 on characterising the immediate predictions
for the IDMM constitutes an exercise inor an example forprecisely that.
This idea of conservative probabilistic inference brings us to what we believe is the
main contribution of this paper. It is a central idea in de Finettis (1975) approach to
probabilitybut also of course implicit in the Markov and Chebyshev inequalitiesthat
when a subject makes probability assessments, we can consider them as bounds on so-called
precise probability models. Calculating such most conservative but tightest bounds is indeed
what de Finettis (1975) Fundamental Theorem of Prevision (see also Lad, 1996) is about.
The theory of imprecise probabilities, brought to a synthesis by Williams (1976) and Walley
(1991, 2000), but going back to Boole (1952) and Keynes (1921), with crucial contributions
by quite a number of statisticians and philosophers (Smith, 1961; Levi, 1980; Seidenfeld,
Schervish, & Kadane, 1999), looks at conservative probabilistic inference precisely in this
way: how can we calculate as efficiently as possible the consequencesin the sense of most
conservative tightest boundsof making certain probability assessments. These may be local
assessments, such as inequalities imposed on the probabilities or previsions of certain events
or variables, or structural assessments, such as independence, or exchangeability.
One advantage of imprecise probability models is that they allow for imprecision, or in
other words, the use of partial probability assessments using bounding inequalities rather
than equalities. Another, related, advantage is that they allow for indecision to be modelled
explicitly: loosely stated, if the imposed bounds on probabilities allow for more than one
probability model as a solution, it may very well be that of two actions, the first has the
higher expected utility for one compatible probability model, and the smaller for another
compatible probability model, meaning that neither action is robustly preferred over the other.
So with this current stated model for his beliefs, a subject is then undecided between these
actions. In Section 2, we give a concise overview of the relevant ideas, models and techniques
in the field of imprecise probabilities. A much more extensive and detailed recent overview of
this area of research was published by Augustin, Coolen, De Cooman, and Troffaes (2014).
3

fiDe Cooman, De Bock, & Diniz

The present paper, then, can be described as an application of ideas in imprecise probabilities to predictive inference. Its aim is to studyand develop a general framework for
dealing withconservative coherent predictive inference using imprecise probability models.
Using such models will also allow us to represent a subjects indecision, which we believe is a
natural state to be in when knowing, or having learned little, about the problem at hand.
It seems important that theories of learning under uncertainty in general, and predictive
inference in particular, at least allow us (i) to start out with conservative, very imprecise and
indecisive models when little has been learned, and (ii) to become more precise and decisive
as more observations come in. We shall see that the abstract notion of an inference system
that we introduce further on, allows forbut does not necessarily forcesuch behaviour,
and we shall give a number of examples of concrete inference systems that display it.
Our work here builds on, but manages to reach much further than, an earlier paper by
one of the authors (De Cooman, Miranda, & Quaeghebeur, 2009a). One reason why it does
so, is that this earlier work deals only with immediate prediction models, and as we shall
see further on, predictive inference using imprecise probabilities is not completely determined
by immediate prediction, contrary to what we can expect when using precise probabilities.
But the main reason is that we are now in a position to use a very powerful mathematical
language to represent imprecise-probabilistic inferences: Walleys (2000) coherent sets of
desirable gambles. Earlier imprecise probability models (Boole, 1952, 1961; Koopman, 1940)
centred on lower and upper probability bounds for eventsor propositions. Later on (Walley,
1991, Section 2.7), it became apparent that this language of events and lower and upper
probabilities is lacking in power of expression: a much more expressive theory uses random
variables and their lower previsions or expectations. This successful theory of coherent lower
previsions is by now quite well developed (Walley, 1991; Augustin et al., 2014; Troffaes &
De Cooman, 2014). But it faces a number of problems, such as its mathematical as well as
conceptual complexity, especially when dealing with conditioning and independence, and
the fact that, as is the case with many other approaches to probability, and as we shall see
further on in Section 2.5, it has issues with conditioning on sets of (lower) probability zero.
A very attractive solution to these problems was offered by Walley (2000), in the form of
coherent sets of desirable gambles, inspired by earlier ideas (Smith, 1961; Williams, 1975b;
Seidenfeld, Schervish, & Kadane, 1995). Here, the primitive notions are not probabilities of
events, nor expectations of random variables. The focus is rather on whether a gamble, or
a risky transaction, is desirable to a subjectstrictly preferred to the zero transaction, or
status quo. And a basic belief model is now not a probability measure or lower prevision,
but a set of desirable gambles. Of course, stating that a gamble is desirable also leads to a
particular lower prevision assessment: it provides a lower bound of zero on the prevision of
the gamble. We explain why we prefer to use sets of desirable gambles as basic uncertainty
models in Section 2.
In summary, then, our aim in this paper is to use sets of desirable gambles to extend the
existing probabilistic theory of predictive inference. Let us explain in some detail how we
intend to go about doing this. The basic building blocks are introduced in Sections 27. As
already indicated above, we give an overview of relevant notions and results concerning our
imprecise probability model of choicecoherent sets of desirable gamblesin Section 2. In
particular, we explain how to use them for conservative inference as well as conditioning;
4

fiCoherent Predictive Inference under Exchangeability

how to derive more commonly used models, such as lower previsions and lower probabilities,
from them; and how they relate to precise probability models.
In Section 3, we explain how we can describe a subjects beliefs about a sequence of
variables in terms of predictive sets of desirable gambles, and the derived notion of predictive
lower previsions. These imprecise probability models generalise the above-mentioned predictive
probability mass functions pn (|x1 , . . . , xn ), and they constitute the basic tools we shall be
working with. We also explain what are the proper formulations for the above-mentioned
time consistency and coherence requirements in this more general context.
In Section 4, we discuss a number of inference principles that we believe could be reasonably
imposed on predictive inferences, and we show how to represent them mathematically in
terms of predictive sets of desirable gambles and lower previsions. Pooling invarianceor
what Walley (1996) has called the Representation Invariance Principle (RIP)and renaming
invariance seem reasonable requirements for any type of predictive inference, and category
permutation invariance seems a natural thing to require when starting from a state of
complete ignorance. Taken together, they constitute what we call representation insensitivity.
It means that predictive inferences remain essentially unchanged when we transform the
set of categories, or in other words that they are essentially insensitive to the choice of
representationthe category set. Another inference principle we look at imposes the so-called
specificity property: when predictive inference is specific, then for a certain type of question
involving a restricted number of categories, a more general model can be replaced by a more
specific model that deals only with the categories of interest, and will produce the same
relevant inferences (Bernard, 1997).
The next important step is taken in Section 5, where we recall from the literature (De
Cooman et al., 2009b; De Cooman & Quaeghebeur, 2012) how to deal with exchangeability
when our predictive inference models are imprecise. We recall that de Finettis Representation
Theorem can be significantly generalised. In this case, the time-consistent and coherent
predictive sets of desirable gambles are completely characterised by a set of (multivariate)
polynomials on the simplex of all probability mass functions on the category set.3 This
set of polynomials must satisfy a number of properties, which taken together define the
notion of Bernstein coherence. Without becoming too technical at this point, the conclusion
of this section is that, in our more general context, the precise-probabilistic notion of a
representing probability measure on the simplex of all probability mass functions is replaced
by a Bernstein coherent set of polynomials on this simplex. This set of polynomials serves
completely the same purpose as the representing probability measure: it completely determines,
and conveniently and densely summarises, all predictive inferences. This is the reason why
the rest of the developments in the paper are expressed in terms of such Bernstein coherent
sets of polynomials.
We introduce coherent inference systems in Section 6 as maps that associate with any
finite set of categories a Bernstein coherent set of polynomials on the simplex of probability
mass functions on that set. So a coherent inference system is a way of fixing completely all
coherent predictive inferences for all possible category sets. Our reasons for introducing such
coherent inference systems are twofold. First of all, the inference principles in Section 4 impose
connections between predictive inferences for different category sets, so we can represent such
3. In contradistinction with de Finettis version, our version has no problems with conditioning on observed
sequences of (lower) probability zero.

5

fiDe Cooman, De Bock, & Diniz

inference principles mathematically as restrictions on coherent inference systems, which is the
main topic of Section 7. Secondly, it allows us to extend the method of natural extensionor
conservative inferenceintroduced in Section 2.2, to also take into account principles for
predictive inference, or more generally, predictive inference for multiple category sets at once.
This leads to a method of combining (local) predictive probability assessments with (global)
inference principles to produce the most conservative predictive inferences compatible with
them.
As a first illustration of the power of our methodology, we look at immediate prediction
in Section 8: what implications do representation insensitivity and specificity have for
predictive inference about the single next observation? We show that our approach allows us
to streamline, simplify and significantly extend previous attempts in this direction by De
Cooman et al. (2009a).
The material in Sections 914 shows, by producing explicit examples, that there are
quite a few different typeseven uncountable infinitiesof coherent inference systems that
are representation insensitive and/or specific. We discuss the vacuous and nearly vacuous
inference systems in Sections 9 and 10, the skeptically cautious inference system in Section 11,
the family of IDMM inference systems in Section 12, the family of skeptical IDMM inference
systems in Section 13, and the Haldane inference system in Section 14. Most of these inference
systems, apart from the IDMM, appear here for the first time. Also, we believe that we are
the first to publish a detailed and explicitas well as still elegantproof that the IDMM
inference systems are indeed representation insensitive and specific. It should already be
mentioned here, however, that our IDMM inference systems are based on a modified, and
arguably better behaved, version of the models originally introduced by Walley and Bernard
(see Walley, 1996; Walley & Bernard, 1999; Bernard, 2005); we refer to Appendix D for more
explanation, with a proof that the original IDMM is not specific and that, contrary to what
is often claimed, it does not satisfy the so-called nestedness property.
Our results disprove the conjecture (Bernard, 2007; De Cooman et al., 2009a) that the
IDMM inference systemsour version or the original oneare the only ones, or even the
most conservative ones, that satisfy both representation insensitivity and specificity. But we
do show in Section 15 that the IDMM family of immediate predictionswhich are the same
for our version and for the original oneare in a definite sense the most conservative ones
that are representation insensitive and specific, and satisfy another requirement, which we
have called having concave surprise.
In the conclusion (Section 16) we point to a number of surprising consequences of our
results, and discuss avenues for further research.
In order to make this paper as self-contained as possible, we have included a number of
appendices with additional discussion. To help the reader find his way through the many
notions and notations we need in this paper, Appendix A provides of list of the most common
ones, with a short hint at their meaning, and where they are introduced. Appendix B provides
useful and necessary background on the theory of multivariate polynomials on simplices, and
the important part that Bernstein basis polynomials have there. Our discussion of IDMM
inference systems relies quite heavily on Dirichlet densities on simplices, and the expectation
operators associated with them. We discuss their most important and relevant properties in
Appendix C. Appendix D contains a discussion of the original IDM and IDMM models, as
proposed by Walley and Bernard (see Walley, 1991, 1996; Walley & Bernard, 1999; Bernard,
6

fiCoherent Predictive Inference under Exchangeability

2005), where we show that some of the claims they make about this model need to be
more carefully formulated. As we stated above, this is our main reason for introducing, in
Section 12, our own modified version of the IDMM models, which does not suffer from such
shortcomings, and produces the same immediate prediction models as the original version.
Finally, in an effort to make this lengthy paper as readable as possible, we have moved all
proofs, and some additional technical discussion, to Appendix E.

2. Imprecise Probability Models
In this section, we give a concise overview of imprecise probability models for representing,
and making inferences and decisions under, uncertainty. As suggested in the Introduction,
we shall focus on sets of desirable gambles as our uncertainty models of choice.
Let us briefly summarise in the next section why, in the present paper, we work with such
sets as our basic uncertainty models for doing conservative probabilistic inference. The reader
who wants to dispense with motivation can proceed to Section 2.2, where we introduce the
mathematics behind these models. In later sections, we shall of course also briefly mention
derived results in terms of the more familiar language of (lower) previsions and probabilities.
2.1 Why Sets of Desirable Gambles?
First of all, a number of examples in the literature (Moral, 2005; Couso & Moral, 2011; De
Cooman & Quaeghebeur, 2012; De Cooman & Miranda, 2012) have shown that working
with and making inferences using such models is more general and more expressive. It is
also simpler and more elegant from a mathematical point of view, and it has a very intuitive
geometrical interpretation (Quaeghebeur, 2014). We shall see in Sections 2.4 and 3 that
marginalisation and conditioning are especially straightforward, and that there are no issues
with conditioning on sets of (lower) probability zero.
Also, it should become apparent from the discussion in Section 2.2, and has been explained
in some detail by Moral and Wilson (1995) and De Cooman and Miranda (2012), that the
similarity between accepting a gamble on the one hand and accepting a proposition to be true
on the other, gives a very logical flavour to conservative probabilistic inference. Indeed, there
is a strong analogy between the two, which connects conservative probabilistic inferencealso
called natural extension in the fieldwith logical deduction: where in classical propositional
logic we are looking for the smallest deductively closed set that contains a number of given
propositions, in an imprecise probabilities context we are looking for the smallest coherent set
of desirable gambles that contains a number of given gambles. In the context of this analogy,
precise probability models are closely related to complete, or maximal, deductively closed
setsperfect information states. This is a clear indication that precise probability models by
themselves are not well suited for dealing with conservative inference, and that we need the
broader context of imprecise probability models as a natural language and setting in which
to do this. So in summary, working with sets of desirable gambles encompasses and subsumes
as special cases both classical (or precise) probabilistic inference and inference in classical
propositional logic; see the detailed discussion by De Cooman and Miranda (2012).
Finally, as we briefly explain in Section 5, De Cooman and Quaeghebeur (2012) have
shown that working with sets of coherent desirable gambles is especially illuminating in the
context of modelling exchangeability assessments: it exposes the simple geometrical meaning
7

fiDe Cooman, De Bock, & Diniz

of the notion of exchangeability, and leads to a simple and particularly elegant proof of a
significant generalisation of de Finettis (1937) Representation Theorem for exchangeable
random variables.
In summary, we work with sets of desirable gambles because they are the most powerful,
expressive and general models at hand, because they are very intuitive to work withthough
unfortunately less familiar to most people not closely involved in the field, and, very
importantly, because they avoid problems with conditioning on sets of (lower) probability
zero. For more details, we refer to the work of Walley (2000), Moral (2005), Couso and Moral
(2011), De Cooman and Quaeghebeur (2012), and Quaeghebeur (2014).
2.2 Coherent Sets of Desirable Gambles and Natural Extension
We consider a variable X that assumes values in some finite4 possibility space A. We model
a subjects beliefs about the value of X by looking at which gambles on this variable the
subject finds desirable, meaning that he strictly prefers5 them to the zero gamblethe status
quo. This is a very general approach, that extends the usual rationalist and subjectivist
approach to probabilistic modelling to allow for indecision and imprecision.
A gamble is a real-valued function f on A. It is interpreted as an uncertain reward f (X)
that depends on the value of X, and is expressed in units of some predetermined linear utility.
It represents the reward the subject gets in a transaction where first the actual value x of
X is determined, and then the subject receives the amount of utility f (x)which may be
negative, meaning he has to pay it. Throughout the paper, we use the device of writing f (X)
when we want to make clear what variable X the gamble f depends on.
Events are subsets of the possibility space A. With any event B  A we can associate a
special gamble IB , called its indicator, which assumes the value 1 on B and 0 elsewhere.
We denote the set of all gambles on A by L(A). It is a linear space under point-wise
addition of gambles, and point-wise multiplication of gambles with real numbers. For any
subset A of L(A), posi(A) is the set of all positive linear combinations of gambles in A:
posi(A) :=

{
n

}
k fk : fk  A, k  R>0 , n  N .

(1)

k=1

Here, N is the set of natural numbers (without zero), and R>0 is the set of all positive real
numbers. A convex cone of gambles is a subset A of L(A) that is closed under positive linear
combinations, meaning that posi(A) = A.
For any two gambles f and g on A, we write f  g if (x  A)f (x)  g(x), and f > g
if f  g and f 6= g. A gamble f > 0 is called positive. A gamble g  0 is called non-positive.
4. For the sake of simplicity, we restrict this discussion to finite possibility spaces, because this is all we
really need for the purposes of this paper. In a very limited number of remarks further on, we shall have
occasion to mention related notions for infinite possibility spaces, but we will give ample references there
to guide the interested reader to the relevant literature.
5. We want to point out that the notion of strict preferenceor preference without indifferencecommonly
used in preference modelling, should not be confused with Walleys (1991, Section 3.7.7) notion of strict
desirability, which is only one of the many ways to construct from a lower prevision a set of gambles that
are strictly preferred to the zero gamble; see also the discussion near the end of Section 2.5. For more
details, we refer to a recent paper by Quaeghebeur, De Cooman, and Hermans (2014).

8

fiCoherent Predictive Inference under Exchangeability

L>0 (A) denotes the convex cone of all positive gambles, and L0 (A) the convex cone of all
non-positive gambles.
We collect the gambles that a subject finds desirablestrictly prefers6 to the zero gamble
into his set of desirable gambles, and we shall take such sets as our basic uncertainty models.
Of course, they have to satisfy certain rationality criteria:
Definition 1 (Coherence). A set of desirable gambles D  L(A) is called coherent if it
satisfies the following requirements:
D1. 0 
/ D;
D2. L>0 (A)  D;
D3. D = posi(D).
D(A) denotes the set of all coherent sets of desirable gambles on A.
Requirement D3 turns D into a convex cone. Due to D2, it includes L>0 (A); by D1D3, it
avoids non-positivity:
D4. if f  0 then f 
/ posi(D), or equivalently L0 (A)  posi(D) = .
L>0 (A) is the smallest coherent subset of L(A). This so-called vacuous model therefore
reflects minimal commitments on the part of the subject: if he knows absolutely nothing
about the likelihood of the different outcomes, he will only strictly prefer to zero those
gambles that never decrease his wealth and have some possibility of increasing it.
When D1  D2 , a subject with a set of desirable gambles D1 is more conservative, or less
committal, than a subject with a set of desirable gambles D2 , simply because the latter strictly
prefers to zero all the gambles that the former does, and possibly more. The inclusion relation
imposes a natural partial ordering on sets of desirable gambles, with a simple interpretation
of is at least as conservative as.
 For any non-empty family of coherent sets of desirable gambles Di , i  I, its intersection
iI Di is still coherent. This simple result underlies the notion of (conservative) coherent
inference. If a subject gives us an assessmenta set A  L(A) of gambles on A that he
finds desirablethen it tells us exactly when this assessment can be extended to a coherent
set of desirable gambles, and how to construct the smallestand therefore least committal
or most conservativesuch set:
Theorem 2 (Natural Extension, De Cooman & Quaeghebeur, 2012). Let A  L(A), and
define its natural extension by:7

EA :=
{D  D(A) : A  D} .
Then the following statements are equivalent:
(i) A avoids non-positivity: L0 (A)  posi(A) = ;
(ii) A is included in some coherent set of desirable gambles;
6. See footnote 5.

7. As usual, in this expression, we let  = L(A).

9

fiDe Cooman, De Bock, & Diniz

(iii) EA 6= L(A);
(iv) the set of desirable gambles EA is coherent;
(v) EA is the smallest coherent set of desirable gambles that includes A.
When any (and hence all) of these equivalent statements holds, EA = posi(L>0 (A)  A).
Moreover, A is coherent if and only if A 6= L(A) and EA = A.
2.3 Maximal Coherent Sets of Desirable Gambles
An element D of D(A) is called maximal if it is not strictly included in any other element of
D(A), or in other words, if adding any gamble f to D makes sure we can no longer extend
the set D  {f } to a set that is still coherent:
(D0  D(A))(D  D0  D = D0 ).
M(A) denotes the set of all maximal elements of D(A). A coherent set of desirable gambles
D is maximal if and only if for all non-zero gambles f on A, f 
/ D  f  D (see Couso &
Moral, 2011 for the case of finite A, and De Cooman & Quaeghebeur, 2012 for the infinite
case). Coherence and natural extension can be described completely in terms of maximal
elements:
Theorem 3 (Couso & Moral, 2011; De Cooman & Quaeghebeur, 2012). A set A avoids
non-positivity
if and only if there is some maximal D  M(A) such that A  D. Moreover,

EA = {D  M(A) : A  D}.
2.4 Conditioning with Sets of Desirable Gambles
Let us suppose that our subject has a coherent set D of desirable gambles on A, expressing his
beliefs about the value that a variable X assumes in A. We can then ask what his so-called
updated set DcB of desirable gambles on B would be, were he to receive the additional
informationand nothing morethat X actually belongs to some subset B of A. The
updating, or conditioning, rule for sets of desirable gambles states that:
g  DcB  gIB  D for all gambles g on B.

(2)

It states that the gamble g is desirable to a subject were he to observe that X  B if
and only if the called-off gamble gIB is desirable to him. This called-off gamble gIB is the
gamble on the variable X that gives a zero rewardis called offunless X  B, and in
that case reduces to the gamble g on the new possibility space B. The updated set DcB
is a set of desirable gambles on B that is still coherent, provided that D is (De Cooman
& Quaeghebeur, 2012). See the discussions by Moral (2005), Couso and Moral (2011), De
Cooman and Quaeghebeur (2012), De Cooman and Miranda (2012) and Quaeghebeur (2014)
for more detailed information on updating sets of desirable gambles.
2.5 Coherent Lower Previsions
We now use coherent sets of desirable gambles to introduce derived concepts, such as coherent
lower previsions, and probabilities.
10

fiCoherent Predictive Inference under Exchangeability

Given a coherent set of desirable gambles D, the functional P defined on L(A) by
P (f ) := sup {  R : f    D} for all f  L(A),

(3)

is a coherent lower prevision (Walley, 1991, Thm. 3.8.1). This means that it is a lower
envelope of the expectations associated with some set of probability mass functions,8 or,
equivalently, that it satisfies the following coherence properties (Walley, 1991, 2000; De
Cooman & Quaeghebeur, 2012; Miranda & De Cooman, 2014; Troffaes & De Cooman, 2014):
P1. P (f )  min f for all gambles f on A;
P2. P (f + g)  P (f ) + P (g) for all gambles f, g on A;
P3. P (f ) = P (f ) for all gambles f on A and all real   0.
Here we used the notation min f := min {f (x) : x  A}; max f is defined similarly. The
conjugate upper prevision P is defined by P (f ) := inf {  R :   f  D} = P (f ). The
following properties are implied by P1P3:
P4. max f  P (f )  P (f )  min f for all gambles f on A;
P5. P (f + ) = P (f ) +  and P (f + ) = P (f ) +  for all gambles f on A and all   R.
For any gamble f , P (f ) is called the lower prevision of f , and it follows from Equation (3)
that it can be interpreted as the subjects supremum desirable price for buying the gamble f .
For any event B, P (IB ) is also denoted by P (B), and called the lower probability of B; it
can be interpreted as the subjects supremum desirable rate for betting on B. Similarly for
upper previsions and upper probabilities.
The lower prevision associated with the vacuous set of desirable gambles L>0 (A) is given
by P (f ) = min f . It is called the vacuous lower prevision, and it is the point-wise smallest,
or most conservative, of all coherent lower previsions.
The coherent conditional model DcB, with B a non-empty subset of A, induces a conditional lower prevision P (|B) on L(B), by invoking Equation (3):
P (g|B) := sup {  R : g    DcB} = sup {  R : [g  ]IB  D}
for all gambles g on B. (4)
It is not difficult to show (Walley, 1991) that P and P (|B) are related through the following
coherence condition:
P ([g  P (g|B)]IB ) = 0 for all g  L(B),
(GBR)
called the Generalised Bayes Rule. This rule allows us to infer P (|B) uniquely from P ,
provided that P (B) > 0. Otherwise, there is usually an infinity of coherent lower previsions
P (|B) that are coherent with P in the sense that they satisfy (GBR), or equivalently, that
there is some coherent set of desirable gambles D that leads to both P and P (|B). Two
8. This statement is valid because we are working with finite A. For infinite A, similar results can be shown
to hold (Walley, 1991; De Cooman & Quaeghebeur, 2012; Miranda & De Cooman, 2014; Troffaes &
De Cooman, 2014), and then the expectations involved are coherent previsionsexpectation operators
associated with finitely additive probability measures. See also the discussion in Section 2.6.

11

fiDe Cooman, De Bock, & Diniz

particular conditioning rules, namely natural and regular extension (Walley, 1991; Miranda
& De Cooman, 2014), always produce conditional lower previsions that satisfy GBR, and
are therefore coherent with P . When P (B) > 0but not necessarily when P (B) = 0!they
always produce the point-wise smallest and largest coherent conditional lower previsions,
respectively (Miranda, 2009; Miranda & De Cooman, 2014).9
Many different coherent sets of desirable gambles lead to the same coherent lower prevision
P , and they typically differ only in their boundaries. In this sense, coherent sets of desirable
gambles are more informative than coherent lower previsions: a gamble with positive lower
prevision is always desirable and one with a negative lower prevision never, but a gamble
with zero lower prevision lies on the border of the set of desirable gambles, and the lower
prevision does not generally provide information about the desirability of such gambles. If
such border behaviour is importantand it is when dealing with conditioning on events with
zero (lower) probability (Walley, 2000; Moral, 2005; Couso & Moral, 2011; Quaeghebeur,
2014)it is useful to work with sets of desirable gambles rather than lower previsions, because
as Equations (2) and (4) tell us, they allow us to derive unique conditional models from
unconditional ones: with a coherent set of desirable gambles D there corresponds a unique
conditional set of desirable gambles DcB and a unique conditional lower prevision P (|B), for
any non-empty event B. The smallest set of desirable gambles that induces a given coherent
lower prevision, is called the associated set of strictly desirable gambles (Walley, 1991) and is
given by {f  L(A) : f > 0 or P (f ) > 0}. See the papers by Walley (2000) and Quaeghebeur
(2014) for additional discussion about why sets of desirable gambles are more informative
than coherent lower previsions.
2.6 Linear Previsions and Credal Sets
When the coherent lower and the upper prevision coincide on all gambles, then the real
functional P defined on L(A) by P (f ) := P (f ) = P (f ) for all f  L(A) is a coherent prevision.
Since we assumed that A is finite,10 this means that it corresponds
to the expectation

operator associated with a probability mass function p: P (f ) = xA f (x)p(x) =: Ep (f ) for
all f  L(A), where p(x) := P (I{x} ) for all x  A. This happens in particular if the lower
and upper previsions are induced by a maximal coherent set of desirable gambles. Indeed, up
to boundary behaviour, the so-called precise probability models P correspond to maximal
coherent sets of desirable gambles; see the discussions by Williams (1975a), Miranda and
Zaffalon (2011, Proposition 6) and Couso and Moral (2011, Section 5) for more information.
For coherent previsions P , the Generalised Bayes Rule (GBR) reduces to Bayess Rule:
P (gIB ) = P (B)P (g|B) for all g  L(B),

(BR)

indicating that this central probabilistic updating rule is a special case of Equation (2).
9. The conditional lower previsions in Section 12 on the IDMM are produced by regular extension. The
models in Sections 11, 13 and 14 have the same lower previsions amongst them, but in nearly all cases have
very different conditional lower previsions, even though in these cases the natural and regular extensions
coincidethey are vacuous there.
10. As already hinted at in footnote 8, similar things can still be said for infinite A, but this would unduly
complicate the discussion. For more details, see the work by Walley (1991), Troffaes and De Cooman
(2014) and Miranda and De Cooman (2014).

12

fiCoherent Predictive Inference under Exchangeability

Because we assumed that A is finite, we can define the so-called credal set M(P ) associated
with a coherent lower prevision P as:
M(P ) := {p  A : (f  L(A))Ep (f )  P (f )} ,
which is a closed and convex subset of the so-called simplex A of all probability mass
functions on A.11 Then P is the lower envelope of M(P ): P (f ) = min {Ep (f ) : p  M(P )}
for all f  L(A) (Walley, 1991; Miranda & De Cooman, 2014; Troffaes & De Cooman,
2014). In this sense, such convex closed sets of precise probability models can also be seen
as imprecise probability models, and they are mathematically equivalent to coherent lower
previsions. They are therefore also less general and powerful than coherent sets of desirable
gambles, and also suffer from problems with conditioning on events with (lower) probability
zero.12

3. Predictive Inference
Predictive inference, in the specific sense we are focussing on here, considers a number of
variables X1 , . . . , Xn assuming values in the same category set Awe define a category set
as any non-empty finite set.13 In what follows, we shall have occasion to use many different
category sets, and we shall use italic capitals such as A, B, C, D, . . . to refer to them.
We start our discussion of predictive inference models in the most general and representationally powerful language: coherent sets of desirable gambles, as introduced in the previous
section. Further on, we shall also pay some attention to more specific derived models, such
as predictive lower previsions, and predictive lower probabilities.
Predictive inference assumes generally that a number n of observations have been made,
 = (x1 , . . . , xn ) of the first n variables X1 , . . . , Xn . Based on this
so we know the values 
n c
 for the values
 a subject then has a posterior predictive model DA
observation sample ,
n
n
 is a coherent set of
that the next n variables Xn+1 , . . . , Xn+n assume in A . This DA c
desirable gambles f (Xn+1 , . . . , Xn+n ) on An . Here we assume that n  N. On the other
hand, we want to allow that n  N0 := N  {0}, which is the set of all natural numbers with
zero: we also want to be able to deal with the case where no previous observations have been
n a prior predictive model.14 Of course,
made. In that case, we call the corresponding model DA
technically speaking, n + n  n.
As we said, the subject may also have a prior, unconditional model, for when no obn of
servations have yet been made. In its most general form, this will be a coherent set DA
11. See Section 5.2 for an explicit definition of A .
12. Using sets of full conditional measures (Dubins, 1975; Cozman, 2013), rather than sets of probability
mass functions, leads to an imprecise probability model that is related to sets of desirable gambles (Couso
& Moral, 2011), and has no problems with conditioning on sets of lower probability zero either, but we
feel it is less elegant and mathematically more complicated.
13. For formal reasons, we include the trivial case of category sets with a single element, in which case we are
certain about the value that the variables assume.
14. So the terms posterior and prior in association with predictive models indicate whether or not previous
observations have been made. But, in order to avoid the well-known issues with temporal coherence
(Zaffalon & Miranda, 2013), we are assuming here that the prior and posterior models are based on a
subjects beliefs before any observations have been made, so the posterior models refer to hypothetical
future situations.

13

fiDe Cooman, De Bock, & Diniz

n
desirable gambles f (X1 , . . . , Xn ) on An , for some n  N. He may also have coherent sets DA
n
of desirable gambles f (X1 , . . . , Xn ) on A , where n can be any natural number such that
n and D n must then be related to each other through the following
n  n; and the sets DA
A
marginalisation, or time consistency, requirement:15
n
n
f (X1 , . . . , Xn )  DA
 f (X1 , . . . , Xn )  DA
for all gambles f on An .

(5)

In this expression, and throughout this paper, we identify a gamble f on An with its cylindrical
extension f 0 on An , defined by f 0 (x1 , . . . , xn , . . . , xn ) := f (x1 , . . . , xn ) for all (x1 , . . . , xn )  An .
If we introduce the marginalisation operator margn () :=   L(An ), then the time consistency
n = marg (D n ) = D n  L(An ).
condition can also be rewritten simply as DA
n
A
A
n and posterior (conditional) ones D n c
Prior (unconditional) predictive models DA
A  must
also be related through the following updating requirement:
n
n
  f (Xn+1 , . . . , Xn+n )I{}
f (Xn+1 , . . . , Xn+n )  DA
c
 (X1 , . . . , Xn )  DA

for all gambles f on An , (6)
which is a special case of Equation (2): the gamble f (Xn+1 , . . . , Xn+n ) is desirable after observ if and only if the gamble f (Xn+1 , . . . , Xn+n )I{}
ing the sample 
 (X1 , . . . , Xn ) is desirable
before any observations are made. This called-off gamble f (Xn+1 , . . . , Xn+n )I{}
 (X1 , . . . , Xn )

is the gamble that gives zero rewardis called offunless the first n observations are ,
and in that case reduces to the gamble f (Xn+1 , . . . , Xn+n ) on the remaining variables
Xn+1 , . . . , Xn+n . The updating requirement is a generalisation of Bayess Rule for updating,
and in fact reduces to it when the sets of desirable gambles lead to (precise) probability
mass functions, as described in Section 2.6 and proved in detail by Walley (2000) and also
by De Cooman and Miranda (2012). But contrary to Bayess Rule for probability mass
functions, the updating rule (6) for coherent sets of desirable gambles clearly does not suffer
from problems when the conditioning event has (lower) probability zero: it allows us to infer
a unique conditional model from an unconditional one, regardless of the (lower or upper)
probability of the conditioning event. We refer to the work of De Cooman and Miranda
(2012) for detailed discussions of marginalisation and updating of sets of desirable gambles in
a many-variable context.
As explained in Section 2.5, we can use the relationship (3) to derive prior (unconditional)
n through:
predictive lower previsions P nA () on L(An ) from the prior set DA
n
P nA (f ) := sup {  R : f    DA
} for all gambles f on An and all 1  n  n,

 on L(An ) from the posterior
and posterior (conditional) predictive lower previsions P nA (|)
n
 through:
sets DA c
{
}
n
 := sup   R : f    DA
 for all gambles f on An .
P nA (f |)
c
15. See also the related discussion of this notion by De Cooman and Miranda (2008b) and De Cooman and
Quaeghebeur (2012); it should not be confused with the temporal consistency discussed by Goldstein
(1983, 1985) and Zaffalon and Miranda (2013).

14

fiCoherent Predictive Inference under Exchangeability

Further on, we shall also want to condition predictive lower previsions on the additional
information that (Xn+1 , . . . , Xn+n )  B n , for some proper subset B of A. Using the ideas in
Sections 2.4 and 2.5, this leads for instance to the following lower prevision:
{
}
n
 B n ) := sup   R : [g  ]IB n  DA
 for all gambles g on B n ,
P nA (g|,
c
(7)
 conditioned on the event B n .
which is the lower prevision P nA (|)

4. Principles for Predictive Inference
So far, we have introduced coherence, marginalisation and updating as basic rationality
requirements that prior and posterior predictive inference models must satisfy. But it could
be envisaged that other requirementsother inference principlescan be imposed on our
inference models. Because we want to show further on how to deal with such additional
requirements in a theory for conservative predictive inference, we now discuss, by way of
examples, a number of additional conditions, which have been suggested by a number of
authors as reasonable properties ofor requirements forpredictive inference models. We
want to stress here that by considering these requirements as examples, we do not want to
defend using them in all circumstances, or mean to suggest that they are always reasonable or
useful. They are what they are: inference principles that we might want to impose, and whose
implications for conservative predictive inference we might therefore want to investigate.
4.1 Pooling Invariance
We first consider Walleys (1996) notion of representation invariance, which we prefer to call
pooling invariance. Consider any set of categories A, and a partition B of A with non-empty
partition classes. We can of course consider the partition B as a set of categories as well.
Therefore, in order to streamline the discussion and notation, we shall henceforth denote it
by Bas stated before, we want to use italic capitals for category sets. Each of its elements
some subset C of Acorresponds to a single new category, which consists of the original
categories x  C being pooledconsidered as one. Denote by (x) the unique element of the
partition B that an original category x  A belongs to. This leads us to consider a surjective
(onto) map  from A to B.
We say that a gamble g on An does not differentiate between pooled categories when:
g() = g() for all ,   An such that (k  {1, . . . , n})(xk ) = (yk ),
which means that there is some gamble f on B n such that:
(  An )g() = f ((x1 ), . . . , (xn )).
The idea underlying this formulaor requirementis that with a sample  = (x1 , . . . , xn ) 
An , there corresponds a sample  := ((x1 ), . . . , (xn ))  B n of pooled categories. Pooling
invariance requires that for gambles g = f   that do not differentiate between pooled
categories, it should make no difference whether we make predictive inferences using the set
of original categories A, or using the set of pooled categories B. More formally, in terms of
predictive lower previsions:

15

fiDe Cooman, De Bock, & Diniz

 = P nB (f |)

P nA (f  ) = P nB (f ) and P nA (f  |)
  An ,
for all n, n  N considered, all gambles f on B n and all 
or alternatively, and more generally, in terms of predictive sets of desirable gambles:
n
n
n
n
  f  DB

f    DA
 f  DB
and f    DA
c
c

  An .
for all n, n  N considered, all gambles f on B n and all 
Pooling invariance seems a reasonable principle to uphold in cases where the category
set is not known in full detail. In that case it is useful to start from a limited set of broadly
defined categories, and allow the creation of new ones, by pooling or splitting old categories
as the observations proceed. In this context, recall Walleys (1996) example: if we have a
closed bag containing coloured marbles, what is the probability of drawing a red marble
from it? With no further information, our subject has no idea about the colours of the
marbles in the bag, making it difficult to construct a suitable detailed category set for
such an experiment. After a few draws from the bag, if the predictive inference model used
respects pooling invariance, the inferences that are made about red marbles when he uses
the category set {red, yellow, blue, other} should be the same as those using the category
set {red, non-red}, where all the colours different from red are pooled together into a single
category. It appears that pooling invariance is a typically useful principle, for instance, in
sampling species problems, when one wants to assess the prevalence of a given species in
certain area.
There is a special case of pooling invariance, called embedding invariance,16 which concentrates on the case without prior observations. In terms of lower previsions:
P nA (f  ) = P nB (f ) for all n  N considered, and all gambles f on B n ,
or alternatively, and more generally, in terms of sets of desirable gambles:
n
n
f    DA
 f  DB
for all n  N considered, and all gambles f on B n .

4.2 Renaming Invariance
Besides pooling invariance, we may also require renaming invariance: as long as no confusion
can arise, it should not matter for a subjects predictive inferences what names, or labels, he
gives to the different categories.
This may seem too trivial to even mention, and as far as we know, it is always implicitly
taken for granted in predictive inference. But it will be well to devote some attention to it
here, in order to distinguish it from the category permutation invariance to be discussed
shortly, with which it is easily confused if we do not pay proper attention. If we have a
renaming bijection (a one-to-one and onto map)  between a set of original categories A
and a set of renamed categories C, where we clearly distinguish between the elements of
A and those of C, then with a sample  = (x1 , . . . , xn )  An of original categories, there
corresponds a sample of renamed categories  := ((x1 ), . . . , (xn )). And with a gamble
16. Walley calls the underlying requirement that the (lower) probability of an event A should not depend on
the possibility space into which A is embedded, the Embedding Principle (Walley, 1991, Section 5.5.1).

16

fiCoherent Predictive Inference under Exchangeability

f on the set C n of renamed samples, there corresponds a gamble f   on the set An of
original samples. Clearly, we can then require that it should make no difference whether we
make predictive inferences using the set of original categories A, or using the set of renamed
categories C. More formally, in terms of predictive lower previsions:
 = P nC (f |)

P nA (f  ) = P nC (f ) and P nA (f  |)
  An ,
for all n, n  N considered, all gambles f on C n and all 
or alternatively, and more generally, in terms of predictive sets of desirable gambles:
n
n
n
n
  f  DC

f    DA
 f  DC
and f    DA
c
c

  An .
for all n, n  N considered, all gambles f on C n and all 
4.3 Category Permutation Invariance
We shall be especially interested in predictive inference where a subject starts from a state of
prior ignorance. In such a state, he has no reason to distinguish between the different elements
of any set of categories A he has chosen. To formalise this idea, consider a permutation
$ of the elements of A.17 With a sample  in An , there corresponds a permuted sample
$ := ($(x1 ), . . . , $(xn )). And with any gamble f on An , there corresponds a permuted
gamble f  $ on An . If a subject has no reason to distinguish between categories z and their
images $(z), it make sense to require the following category permutation invariance:18
 = P nA (f |$)

P nA (f  $) = P nA (f ) and P nA (f  $|)
  An ,
for all n, n  N considered, all gambles f on An and all 
or alternatively, and more generally, in terms of predictive sets of desirable gambles:
n
n
n
n
  f  DA

f  $  DA
 f  DA
and f  $  DA
c
c$

  An .
for all n, n  N considered, all gambles f on An and all 
Formally, this requirement closely resembles renaming invariance, but whereas the latter is
a trivial requirement, category permutation invariance is a symmetry requirement between
categories that can only be justified when our subject has no reason to distinguish between
them, which may for instance be justified when he starts out from a state of prior ignorance.
To draw attention to the difference between the two in a somewhat loose manner: category
permutation invariance allows for confusion between new and old categories, something which
renaming invariance carefully avoids.
To see why such a principle could be reasonable, recall Walleys (1996) bag of marbles
example, introduced above when discussing pooling invariance. Since, before having drawn any
17. This permutation $ of the elements of A, or in other words of the categories, should be contrasted with
permutations  of the order of the observations, i.e. of the time set {1, . . . , n}, considered in our discussion
of exchangeability, further on in Section 5.
18. This requirement is related to the notion of (weak) permutation invariance that De Cooman and Miranda
(2007) have studied in much detail in a paper dealing with symmetry in uncertainty modelling. It goes
back to Walleys (1991, Section 5.5.1) Symmetry Principle.

17

fiDe Cooman, De Bock, & Diniz

marbles from the bag, our subject has no idea how the marbles are coloured, he is in a state
of complete prior ignorance. Therefore, if he starts out with the sample space {red, non-red},
and observes the outcomes of a few draws, say twice non-red, he can consider the probability
of obtaining a red marble on the next draw. But due to the symmetry originating in complete
ignorance, if he were to permute the categories, calling the red marbles non-red and the
non-red ones red, the situation he is now looking at is completely the same as before, and
therefore his probability of obtaining a non-red marble on the next draw after observing
twice red, must be the same as that for observing a red one, after observing non-red twice.
This principle is reminiscent of the Axiom A8 proposed by Carnap (1952) for his system of
inductive logic. Of course, this is not a reasonable principle when our subject has some prior
knowledge about the problem that would, for instance, allow him to impose an ordering on
the categories.
4.4 Representation Insensitivity
We shall call representation insensitivity the combination of pooling, renaming and category
permutation invariance. It means that predictive inferences remain essentially unchanged
when we transform the set of categories, or in other words that they are insensitive to
the choice of representationthe category set. It is not difficult to see that representation
insensitivity can be formally characterised as follows. Consider two category sets A and
D such that there is a so-called relabelling map  : A  D that is onto, i.e. such that
D = (A) := {(x) : x  A}. Then with a sample  in An , there corresponds a transformed
sample  := ((x1 ), . . . , (xn )) in Dn . And with any gamble f on Dn there corresponds a
gamble f   on An .
4.4.1 Representation Insensitivity
For all category sets A and D such that there is an onto map  : A  D, all n, n  N
  An and all gambles f on Dn :
considered, all 
 = P nD (f |),

P nA (f  ) = P nD (f ) and P nA (f  |)

(RI1)

or alternatively, and more generally, in terms of predictive sets of desirable gambles:
n
n
n
n
  f  DD

f    DA
 f  DD
and f    DA
c
c.

(RI2)

There is also the weaker combination of pooling, renaming and category permutation
invariance for models with no prior observations.
4.4.2 Prior Representation Insensitivity
For all category sets A and D such that there is an onto map  : A  D, all n  N considered
and all gambles f on Dn :
P nA (f  ) = P nD (f ),
(EI1)
or alternatively, and more generally, in terms of sets of desirable gambles:
n
n
f    DA
 f  DD
.

18

(EI2)

fiCoherent Predictive Inference under Exchangeability

4.5 Specificity
We now turn to another, rather peculiar but in our view intuitively appealing, potential property of predictive inferences. Assume that in addition to observing a sample of observations
 of n observations in a category set A, our subject comes to know or determine in some

way that the n following observations will belong to a proper subset B of A, and nothing
elsewe might suppose for instance that an observation of (Xn+1 , . . . , Xn+n ) has been made,
but that it is imperfect, and only allows him to conclude that (Xn+1 , . . . , Xn+n )  B n .
We can then impose the following requirement, which uses models conditioned on the
event B n . Such conditional models have been introduced through Equations (2) and (4); see
also the discussion leading to Equation (7), near the end of Section 3.
4.5.1 Specificity
  An and all
For all category sets A and B such that B  A, all n, n  N considered, all 
gambles f on B n :
 B n ) = P nB (f |
 B ),
P nA (f |B n ) = P nB (f ) and P nA (f |,

(SP1)

or alternatively, and more generally, in terms of predictive sets of desirable gambles:
n
n
n
n
  f  DB
 B,
f IB n  DA
 f  DB
and f IB n  DA
c
c

(SP2)

 B is the tuple of observations obtained by eliminating from the tuple 
 all observawhere 
 B is the empty tuple, so when no observations in
tions not in B. In these expressions, when 
 are in B, the posterior predictive model is simply taken to reduce to the prior predictive

model.
Specificity means that the predictive inferences that a subject makes are the same as the
ones he would get by focussing on the category set B, and at the same time discarding all the
previous observations producing values outside B, in effect only retaining the observations
that were inside B! It is as if knowing that the future observations belong to B allows our
subject to ignore all the previous observations that happened to lie outside B. The term
specificity in this context seems to have been proposed by Bernard (1997, 2005), based on
work by Rouanet and Lecoutre (1983). In a so-called specific inference approach, for questions,
inferences and decisions involving only a restricted number of categories, a more general
model can be replaced by a more specific model that deals only with the categories of interest,
and if specificity is respected, the general and the specific models will produce the same
inferences. Specificity seems to be a relevant principle when analysing categorical data that
can be described by tree structures, as in the case of, for instance, patients that are classified
according to symptoms (Bernard, 1997).
To give a very simple example involving, once again, Walleys bag of marbles, our subject
may have observed, after some drawings, green, red, blue and white marbles. He is asked for
his probability of drawing a red marble next, but some other observer has already seen what
it is, and informs us that it is either green or redperhaps due to bad lighting conditions or
because shes colour blind. If the subject uses a specific inference model, he can disregard
the previous observations involving other colours than green and red.
19

fiDe Cooman, De Bock, & Diniz

4.6 Prior Near-Ignorance
We use the notion of near-ignorance as defined by Walley (1991, p. 521) to give the following
definition of prior near-ignorance in our context of predictive inference; see also the related
discussions by Walley (1991, Section 5.3.2), Walley (1997, Section 3) and Walley and Bernard
(1999, Section 2.3). We also refer to the paper by Piatti, Zaffalon, Trojani, and Hutter (2009)
for an interesting discussion of why prior near-ignorance may produce undesirable results in
certain contexts.
4.6.1 Prior Near-Ignorance
The prior model for any single variable Xk assuming values in some arbitrary category set A
is vacuous, so for any category set A, any n  N considered, any 1  k  n and all gambles
f on A:
P nA (extnk (f )) = min f,
or alternatively, and more generally, in terms of sets of desirable gambles:
n
extnk (f )  DA
 f > 0,

where extnk (f ) denotes the cylindrical extension of f to a gamble on An . It is defined by
extnk (f )(x1 , . . . , xn ) := f (xk ) for all (x1 , . . . , xn )  An . A perhaps more intuitive, if less
formally correct, notation for this gamble is f (Xk ).
Theorem 4. Prior representation insensitivity implies prior near-ignorance.
This simple result implies that no model all of whose predictive previsions are precise can
be prior representation insensitive, let alone representation insensitive, as its prior model
for immediate predictions should then be vacuous. We shall see in Section 14 that it is
nevertheless possible for representation insensitive coherent inferences to deploy precise
posterior predictive previsions.

5. Adding Exchangeability to the Picture
We are now, for the remainder of this paper, going to add two additional assumptions.
The first assumption is that there is, in principle, no upper bound on the number of
variables that we can take into account. In other words, when we are considering n variables
X1 , . . . , Xn , we can always envisage looking at one more variable Xn+1 . This effectively
means that we are dealing with a countably infinite sequence of variables X1 , . . . , Xn , . . .
that assume values in the same category set A.
n of coherent
For our predictive inference models, this means that there is a sequence DA
sets of desirable gambles on An , n  N. This sequence should of course be time-consistent in
the sense of Requirement (5), meaning that
n1
n2
n2
(n1 , n2  N)(n1  n2  DA
= margn1 (DA
) = DA
 L(An1 )).

The second assumption is that this sequence of variables is exchangeable, which means,
roughly speaking, that the subject believes that the order in which these variables are observed,
20

fiCoherent Predictive Inference under Exchangeability

or present themselves, has no influence on the decisions and inferences he will make regarding
them.19
In this section, we explain succinctly how to deal with these assumptions technically, and
what their consequences are for the predictive models we are interested in. For a detailed
discussion and derivation of the results presented here, we refer to the papers by De Cooman
et al. (2009b) and De Cooman and Quaeghebeur (2012).
We begin with some useful notation, which will be employed numerous times in what
follows. Consider any element   RA . We consider  as an A-tuple, with as many (real)
components
 x  R as there are categories x in A. For any subset B  A, we then denote by
B := xB x the sum of its components over B.
5.1 Permutations, Count Vectors and the Hypergeometric Distribution
Consider an arbitrary n  N. We denote by  = (x1 , . . . , xn ) a generic, arbitrary element of An .
P n is the set of all permutations  of the index set {1, . . . , n}. With any such permutation ,
we can associate a permutation of An , also denoted by , and defined by ()k := x(k) , or
in other words, (x1 , . . . , xn ) := (x(1) , . . . , x(n) ). Similarly, we lift  to a permutation  t of
L(An ) by letting  t f := f  , so ( t f )() := f ().
The permutation invariant atoms [] := { :   P n },   An are the smallest permutation invariant subsets of An . We introduce the counting map  : An  NAn :  7  (),
where the count vector  () is the A-tuple with components
Tz () := |{k  {1, . . . , n} : xk = z}| for all z  A,
and the set of possible count vectors for n observations in A is given by
{
}
NAn :=   NA
0 : mA = n .

(8)

(9)

So Tz () is the number of times the category z appears in the sample . If  =  (), then
[] = {  An :  () = }, so the atom [] is completely determined by the single count
vector  of all its elements, and is therefore also denoted by [].
We also consider the linear expectation operator HynA (|) associated with the uniform
distribution on the invariant atom []:
HynA (f |) :=


1
f () for all gambles f on An ,
|[]|

(10)

[]

where the number of elements () := |[]| in the invariant atom [] is given by the
multinomial coefficient:
(
) ( )
mA
n
n!
:= 
() =
=
.
(11)


zA mz !
This expectation operator in Equation (10) characterisesor is the one associated with
a (multivariate) hyper-geometric distribution (Johnson, Kotz, & Balakrishnan, 1997, Section 39.2), associated with random sampling without replacement from an urn with n balls
19. Exchangeability was also assumed by Carnaphis Axiom A7and Johnson (1924), who named it the
permutation postulate.

21

fiDe Cooman, De Bock, & Diniz

of types z  A, whose composition is characterised by the count vector . This is borne out
0
by the fact that, for any   An , with 0  n0  n and 0 =  (),
{
(  0 )/() if 0  
HynA (I{} |) =
0
otherwise
is the probability of randomly selecting, without replacement, a sequence of n0 balls of types
 from an urn with n balls whose composition is determined by the count vector . See
also the running example below for a more concrete illustration.
This hyper-geometric expectation operator can also be seen as a linear transformation
HynA between the linear space L(An ) and the generally much lower-dimensional linear space
L(NAn ), turning a gamble f on An into a so-called count gamble HynA (f ) := HynA (f |) on
count vectors.
Running Example. In order to make our argumentation, and the notions we introduce and
discuss, more tangible and concrete, we shall use a very simple running example, to which
we shall come back repeatedly in a number of sections. The notations and assumptions made
here will be maintained throughout the series.
Consider a (potentially infinite) sequence of coin flips, whose successive outcomes we
denote by the variables X1 , X2 , . . . Xn , . . . assuming values in the category set {H , T }. To
make this somewhat more interesting than the usual run-of-the-mill example, assume that at
each stepfor each coin flipNathalie selects a coin from a bag of three coins, and hands it
to Arthur, who then proceeds to flip it. The coin is then put back into the bag for the next
step. The subject whose beliefs we are modelling, may or may not know something about
the nature of the coins, or about how Nathalie is choosing the coins for the subsequent flips:
she might choose them completely at random, or she might have a specific deterministic
mechanism for selecting them, or . . .
 = (H , T , H , H ) of the first n = 4 observed coin flips. The count
Consider the sequence 
 that corresponds to this sequence is given by its components
vector  ()
TH ((H , T , H , H )) = 3 and TT ((H , T , H , H )) = 1,
 = (3, 1), letting the first component always refer to H , from now
and we will denote it by 
on. The corresponding permutation invariant atom is
[(H , T , H , H )] = [(3, 1)] = {(T , H , H , H ), (H , T , H , H ), (H , H , T , H ), (H , H , H , T )}
4!
3!1!

4
= 4 elements. The set of possible count vectors is given by N{H
,T } =
 := {(H , T ), (T , H )}  {H , T }2 of
{(0, 4), (1, 3), (2, 2), (3, 1), (4, 0)}. Consider the event HT
two different outcomes for the first two observations, then

and it has ((3, 1)) =

1
1
Hy4{H ,T } (IHT
 |(3, 1)) = (1 + 1 + 0 + 0) =
4
2
is the probability of observing two different outcomes in two random draws without replacement from an urn containing three balls marked H and one ball marked T , and whose
composition is therefore determined by the count vector (3, 1).

22

fiCoherent Predictive Inference under Exchangeability

5.2 The Multinomial Distribution
Next, we consider the simplex A of all probability mass functions  on A:

{
}
A :=   RA :   0 and A = 1 , where, as before: A :=
x .

(12)

xA

With a probability mass function   A on A, there corresponds the following multinomial
expectation operator MnnA (|):20
MnnA (f |) :=


An

f ()



zTz () for all gambles f on An ,

(13)

zA

which characterises the multinomial distribution, associated with n independent trials of an
experiment with possible outcomes in A and probability mass function . Observe that
)


 ( 1
f ()) ()
zmz
MnnA (f |) =
()
n
zA
NA
[]


n
mz
=
HyA (f |)()
z = CoMnnA (HynA (f )|),
n
NA

zA

where we used the so-called count multinomial expectation operator:21


CoMnnA (g|) :=
g()()
zmz for all gambles g on NAn .
n
NA

(14)

zA

Running Example. Consider n = 4 independent trials of an experiment with possible outcomes
in the category set {H , T } and probability mass function  = (H , T ). Then
3
2 2
3
2
Mn4{H ,T } (IHT
 |(H , T )) = 2H T + 4H T + 2H T = 2H T (H + T ) = 2H T ,

 . Observe, by the way, that Mnn
gives the probability of the event HT
 |(H , T )) =
{H ,T } (IHT
2H T for all n  2.
With the gamble fHT
 := IHT
 on observation sequences (X1 , . . . , X4 ), there corresponds
4
a count gamble gHT
 := Hy{H ,T } (fHT
 |) given by:
gHT
 (0, 4) = 0 and gHT
 (1, 3) =

1
2
1
and gHT
and gHT
and gHT
 (2, 2) =
 (3, 1) =
 (4, 0) = 0,
2
3
2

and

1
2 2 2
1 3
3
CoMn4{H ,T } (g|(H , T )) = 4H T
+ 6H
T + 4H
T = 2H T
2
3
2
leads to the same polynomial as before, as it should.



20. To avoid confusion, we make a (perhaps non-standard) distinction between the multinomial expectation,
which is associated with sequences of observations, and the count multinomial expectation, associated
with their count vectors.
21. See footnote 20.

23

fiDe Cooman, De Bock, & Diniz

5.3 Multivariate Polynomials

Let us introduce the notation NA := mN NAm for the set of all possible count vectors
corresponding to samples of at least one observation. In Equation (9), we can also let n = 0,
which turns NA0 into the singleton
containing only the null count vector 0, all of whose

components are zero. Then mN0 NAm = NA  {0} is the set of all possible count vectors.
For any such count vector   NA  {0}, we consider the (multivariate) Bernstein basis
polynomial BA, of degree mA on A , defined by:
(
)

mA  mz
mz
BA, () := ()
z =
z for all   A .
(15)

zA

zA

In particular, of course, BA,0 = 1.
Any linear combination p of Bernstein basis polynomials of degree n  0 is a (multivariate)
polynomial on A , whose degree deg(p) is at most n.22 We denote the linear space of all these
polynomials of degree up to n by V n (A). Of course, polynomials of degree zero are simply real
constants. We have gathered relevant and useful information about multivariate polynomials
in Appendix B. It follows from the discussion there that, for any n  0, we can introduce a
linear isomorphism CoMnnA between the linear spaces L(NAn ) and V n (A):
 with any gamble g
on NAn , there corresponds a polynomial CoMnnA (g) := CoMnnA (g|) = N n g()BA, in
A
V n (A), and conversely, for any polynomial p  V n (A) there is a unique gamble bnp on NAn
such that p = CoMnnA (bnp ).23 Observe that in particular, for any n  0 and   NAn :
CoMnnA ({}|) = BA, () for all   A .
(16)

We denote by V (A) := nN0 V n (A) the linear space of all (multivariate) polynomials on A ,
of arbitrary degree.
A set HA  V (A) of polynomials on A is called Bernstein coherent if it satisfies the
following properties:
B1. 0 
/ HA ;
B2. V + (A)  HA ;
B3. posi(HA ) = HA .
Here, V + (A) is the set of Bernstein positive polynomials on A : those polynomials p for which
there is some n  deg(p) such that bnp > 0. It follows from Proposition 28 in Appendix B
that V + (A) is a subset of the set V ++ (A) of all polynomials p such that p() > 0 for all 
in the interior int(A ) := {  A : (x  A)x > 0} of A . As a consequence of B1B3, we
find for the set V0 (A) := V + (A) of Bernstein negative polynomials that:
B4. V0 (A)  HA = .
22. The degree may be smaller than n because the sum of all Bernstein basis polynomials of fixed degree is
one. Strictly speaking, these polynomials p are restrictions to A of multivariate polynomials q on RA ,
called representations of p. For any p, there are multiple representations, with possibly different degrees.
The smallest such degree is then called the degree deg(p) of p.
23. Strictly speaking, Equation (14) only defines the count multinomial expectation operator CoMnn
A for
n > 0, but it is clear that the definition extends trivially to the case n = 0.

24

fiCoherent Predictive Inference under Exchangeability

Finally, every Bernstein coherent set HA of polynomials on A induces a lower prevision
H A on V (A) defined by:
H A (p) := sup {  R : p    HA } for all p  V (A).

(17)

This lower prevision is coherent, in the mathematical sense that it satisfies the coherence
requirements P1P3.24
5.4 Exchangeability and the Representation Theorem
We are now ready to deal with exchangeability. We shall give a definition for coherent sets of
desirable gambles that generalises de Finettis (1937, 1975) definition, and which allows for a
significant generalisation of his Representation Theorem.
First of all, fix n  N. Then the subject considers the variables X1 , . . . , Xn to be
exchangeable when he does not distinguish between any gamble f on An and its permuted
version  t f , or in other words, if the gamble f   t f is equivalent to the zero gamble foror
indifferent tohim. This means that he has a so-called set of indifferent gambles:
{
}
n
:= f   t f : f  L(An ) and   P n .
IA
n , then this set must be compatible
If the subject also has a coherent set of desirable gambles DA
n , in the sense that it must satisfy the rationality
with the set of indifferent gambles IA
n
n
n
requirement DA + IA = DA ; see the detailed explanations and justifications by De Cooman
and Quaeghebeur (2012) and Quaeghebeur et al. (2014) of this so-called desiring sweetened
n , are
deals requirement. We then say that the sequence X1 , . . . , Xn , and the model DA
exchangeable.
Next, the countably infinite sequence of variables X1 , . . . , Xn . . . is called exchangeable if
n, n  N
all the finite subsequences X1 , . . . , Xn are, for n  N. This means that all models DA
are exchangeable. They should of course also be time-consistent.
We can now formulate a powerful generalisation of de Finettis (1937, 1975) Representation
Theorem, which is a straightforward compilation of various results proved by De Cooman
and Quaeghebeur (2012):

Theorem 5 (Representation Theorem, De Cooman & Quaeghebeur, 2012). The sequence
n of desirable gambles on An , n  N is coherent, time-consistent and exchangeable
of sets DA
if and only if there is a Bernstein coherent set HA of polynomials on A such that for all
  NA and all 
  []:

n  N, all gambles f on An , all 
n
n
  MnnA (f )BA,
f  DA
 MnnA (f )  HA and f  DA
c
  HA .

In that case this representation HA is unique and given by HA :=



(18)

n
n
nN MnA (DA ).

It follows from Condition (18) that HA completely determines all predictive inferences about
n and all
the sequence of variables X1 , . . . , Xn , . . . , as it fixes all prior predictive models DA
24. Actually, a suitably adapted version, where the underlying possibility space need no longer be finite
(Walley, 1991; Troffaes & De Cooman, 2014), and where the domain is restricted to the polynomials on
A (De Cooman & Quaeghebeur, 2012).

25

fiDe Cooman, De Bock, & Diniz

n c.
 25 This tells us that the representation HA is a set of
posterior predictive models DA
polynomials that plays the same role as a probability measure, or density, or distribution
function, on A in the precise-probabilistic case.
Indeed, the corresponding coherent lower prevision H A on V (A) is given by Equation (17),
and it can be shown to determine a convex closed (compact) set

M(H A ) := {HA : (p  V (A))HA (p)  H A (p)}
of coherent previsions HA on V (A) (Walley, 1991; De Cooman et al., 2009b; De Cooman &
Quaeghebeur, 2012; Troffaes & De Cooman, 2014). As we pointed out in footnote 2and
will come back to further on in footnote 36each such coherent prevision HA uniquely
determines a -additive probability measure on the Borel sets of A , and therefore the set of
polynomials HA , via M(H A ), uniquely determines a set of such probability measures. But, as
we have argued before, HA is more informative than H A and M(H A ), and has no problems
with conditioning on sets of lower probability zero: a Bernstein coherent set of polynomials
HA determines a unique lower prevision H A , and therefore through M(H A ) a unique set of
probability measuresand densities if they are absolutely continuouson the simplex A ,
but the converse is not necessarilyand usually notthe case. A set of probability densities
can be used to define a coherent set of polynomialswe provide an example of how to do
this in Section 12but there will generally be more than one coherent set of polynomials
that leads to this same set of densities, and the updating behaviour for these different sets of
polynomials can be different on conditioning events of lower probability zero.
n c
 only depend on
Condition (18) also tells us that the posterior predictive models DA
 through the count vector 
 =  ():
 count vectors are sufficient
the observed sequence 
statistics under exchangeability. For this reason, we shall from now on denote these posterior
n c
n c.
 as well as by DA
 Also, every now and then, we shall use
predictive models by DA
n
n
DA c0 as an alternative notation for DA .
An immediate but interesting consequence of Theorem 5 is that updating on observations
preserves exchangeability: after observing the values of the first n variables, with count
 the remaining sequence of variables Xn+1 , Xn+2 , . . . is still exchangeable, and
vector ,
Condition (18) tells us that its representation is given by the Bernstein coherent set of
 defined by:
polynomials HA c
 := {p  V (A) : BA,
HA c
 p  HA } .

(19)

If we compare this with Expressions (2) and (6), this tells us that, essentially, Bernstein
basis polynomials serve as likelihood functions for updating sets of polynomials. We use
 to refer to the coherent lower prevision on V (A) derived from HA c
 by means of
H A (|)
 = 0, we find that HA c0 = HA and H A (|0) = H A .
Equation (17). For the special case 
 are related through the following version of the Generalised
Observe that H A and H A (|)
Bayes Rule:

H A ([p  H A (p|)]B
(20)
 ) = 0 for all p  V (A).
A,
 is completely determined by HA . One can consider HA as a prior model on
Clearly, HA c
 plays the role of the posterior that is derived from it.
the parameter space A , and HA c
25. This should be contrasted with the usual precise-probabilistic version, where the posterior predictive
models are only uniquely determined if the observed sequences has non-zero probability; see also footnote 3.

26

fiCoherent Predictive Inference under Exchangeability

We see from Condition (18) and Equation (19) thatsimilarly to what happens in a preciseprobabilistic settingthe multinomial distribution serves as a direct link between on the one
n and, on the other hand,
hand, the prior HA and its prior predictive inference models DA
n c.
 and its posterior predictive inference models DA
 Recalling our
the posterior HA c

  NA  {0}:
convention for  = 0, we can summarise this as follows: for all n  N and all 
{
}
n
 = f  L(An ) : MnnA (f )  HA c

DA
c
(21)
and, as an immediate consequence:
{
}
 = sup   R : MnnA (f )    HA c
 for all f  L(An )
P nA (f |)

(22)

or, equivalently:
 = H A (MnnA (f )|)
 for all f  L(An ).
P nA (f |)

(23)

From a practical point of view, Equation (23) will often be easier to work with than Equa will often admit a simpler expression
tion (22), because as we shall see further on, H A (|)
 compare Equations (45), (54), (61) and (69) with Equations (49), (55), (65)
than HA c;
 is not always uniquely determined by H A : the relaand (73), respectively. But, H A (|)
 uniquely from H A if the prior lower probability
tion (20) only allows us to determine H A (|)

H A (BA,
)
of
observing
is
non-zero.
Therefore,
the sets of polynomials HA are the more


 uniquely. As a quite dramatic
fundamental models, as they allow us to determine the HA c
illustration of this, we shall further on in Sections 11, 13 and 14 come across a number of
quite different inference systemswith different HA that give rise to the same prior H A

but different posterior H A (|)!
Running Example. We now assume that our subject assesses the sequence of coin flips to be
exchangeable, and that he finds desirable any gamble of the type   I{H } (Xn ), for some fixed
  (0, 1]; so his upper probability for observing heads on any coin flip is at most . Since
we infer from Equation (13) that for any N  n, MnN
{H ,T } (I{H } (Xn )|) = H , we infer from
Theorem 5 that this assessment corresponds to the following coherent set of polynomials:
{
}
H := 1 p+ + 2 (  H ) : p+  V + ({H , T }), 1 , 2  R0 and max{1 , 2 } > 0 ,
which is the smallest Bernstein coherent set of polynomials that contains the polynomial
  H ; for more explanation, see also the discussions by De Cooman et al. (2009b) and
De Cooman and Quaeghebeur (2012). It then followsafter some manipulationsfrom
Equation (17) and Proposition 28 that the corresponding lower prevision on V ({H , T }) is
completely determined by the following optimisation:
H  (p) = sup

min [p() + (H  )]

0 {H ,T }

 is given by
Hence, the lower probability of the event HT
H  (2H T ) = sup min [2x(1  x) + (x  )] = 0,
0 x[0,1]

and its upper probability by
H  (2H T ) = H  (2H T ) = inf max [2x(1  x)  (x  )]
0 x[0,1]

27

fiDe Cooman, De Bock, & Diniz

=

{
2(1  )
1
2

if   12
otherwise.


This tells us that exchangeability alone already guarantees that the upper probability of HT
1
is at most 2 . If all three coins in the bag are assumed to be biased towards heads, so  < 12 ,
this upper probability drops below 12 .

To finish this section on representation, we want to stress that the polynomials on A
should not be given a behavioural interpretation as gambles that may or may not be desirable:
they are merely mathematical and representational tools that help us characterise which
gambles on observation sequences are desirable.26 Similarly, the set of polynomials HA and
the lower prevision H A are merely mathematical tools that allow for a more convenient
representation of predictive models on observation sequences.
Running Example. To illustrate why the polynomial representation is so much more convenient
and efficient, recall that if we want to make inferences about a sequence of coin flips of
length n, we need to work with sets of desirable gambles on {H , T }n , or in other words,
with cones in a 2n -dimensional space. If we work with their polynomial representations,
we are led to consider cones of polynomials of degree up to n, which constitute a linear
space that is spanned by the n + 1 Bernstein basis polynomials of degree n, and is therefore
only n + 1-dimensional. Working with these polynomial representations therefore leads to a
dramaticexponentialreduction in complexity.


6. Reasoning about Inference Systems
We have seen in the previous section that, once we fix a category set A, predictive inferences
about exchangeable sequences assuming values in A are completely determined by a Bernstein
coherent set HA of polynomials on A . So if we had some way of associating a Bernstein
coherent set HA with every possible set of categories A, this would completely fix all predictive
inferences. This leads us to the following definition.
Definition 6 (Inference Systems). We denote by F the collection of all category sets, i.e. finite
non-empty sets. An inference system is a map  that maps any category set A  F to some
set of polynomials (A) = HA on A . An inference system  is called coherent if for all
category sets A  F, (A) is a Bernstein coherent set of polynomials on A .
So, a coherent inference system is a way to systematically associate coherent predictive
inferences with any category set. Since the inference principles in Section 4 impose connections
between predictive inferences for different category sets, we now see that we can interpret
these inference principlesor rather, represent them mathematicallyas properties of, or
restrictions on, coherent inference systems. This is what we shall do in Section 7, and
it provides one important motivation for our introducing such systems. Another, equally
26. It makes no operational, behavioural sense to consider the notion of accepting a polynomial, or finding it
desirable. This is very much like the classical case, where for de Finetti (1975) the probability distributions
on the simplex A are only to be used as as mathematical representations, but have no direct behavioural
meaningalthough some Bayesians less careful about foundations than de Finetti might not care to make
this distinction.

28

fiCoherent Predictive Inference under Exchangeability

important reason for doing so, is that it allows us to extend the method of natural extension
or conservative inferenceintroduced in Section 2.2, to also take into account inference
principles for predictive inference, or more generally, predictive inference for multiple category
sets at once.
To see how this comes about, let us show how we can do conservative reasoning with
inference systems. For any two inference systems 1 and 2 , we say that 1 is less committal 
or more conservativethan 2 , and we write 1 v 2 if
(A  F)1 (A)  2 (A).
This simply means that the predictive inferences for each category set A are less committal
for the first than for the second inference system. If we denote by S the set of all inference
systems, then clearly this set is partially ordered by v. Actually, it is a complete lattice,
where the infimum and supremum of any non-empty family i , i  I are given by:
(

)
(
)


inf i (A) =
i (A) and sup i (A) =
i (A) for all category sets A.
iI

iI

iI

iI

We denote by C the set of all coherent inference systems:
C := {  S : (A  F)(A) is Bernstein coherent} .

(24)

Then it is clear that C is a complete meet-semilattice, meaning that it is closed under arbitrary
non-empty infima:27
(i  I)i  C  inf i  C.
(25)
iI

The bottom of this structurethe most conservative coherent inference systemis called
the vacuous inference system V , and it is the coherent inference system given by:
V (A) = V + (A) for all category sets A.
We shall come back in some detail to this vacuous inference system in Section 9.
The property (25) allows us to do conservative reasoning with coherent inference systems.
Suppose, for instance, that for some collection of category sets F  F, we have assessments
A in the form of a set of polynomials AA  V (A), A  F. Then, if it exists, the most
conservative coherent inference system A that is compatible with these assessments is given
by:
A = inf {  C : (A  F)AA  (A)} .
And, of course, it will exist if and only the set of polynomials AA is included in some
Bernstein coherent set of polynomials HA on A, for all A  F. In that case, it is not difficult
to see, given the discussion in Section 5.3, that A (A) = posi(V + (A)  AA ) for A  F and
A (A) = V + (A) for A  F \ F.
27. It is not necessarily closed under suprema, however, as the union of Bernstein coherent sets of polynomials
need not be Bernstein coherent.

29

fiDe Cooman, De Bock, & Diniz

7. Representation Insensitivity and Specificity under Exchangeability
Let us now investigate what form the inference principles of representation insensitivity (RI2)
and specificity (SP2) take for predictive inference under exchangeability, when such inference
can be completely characterised by Bernstein coherent sets of polynomials. This will allow us
to reformulate these principles as constraints onor properties ofinference systems.
7.1 Representation Insensitivity
We recall the notations and assumptions in Section 4.4. With the surjective (onto) map
 : A  D we associate the surjective map R : RA  RD by letting:
R ()z :=



x

for all   RA and all z  D.

(26)

xA : (x)=z

This map allows us to give the following elegant characterisation of representation insensitivity.
Theorem 7. A coherent inference system  is representation insensitive if and only if for
all category sets A and D such that there is an onto map  : A  D, for all p  V (D) and
all   NA  {0}:
(p  R )BA,  (A)  pBD,R ()  (D).
(RI3)
Running Example. Assume now that the coins in the bag are actually rather thick, implying
that there is a non-negligible chance that they do not fall on one of their flat sides, but
remain upright. If we denote this new up state by U , then we have a new category set
A := {H , T , U }. If we also consider a new flat state F , meaning either heads or tails, then
we can also consider, instead of A, the category set D := {F , U } that does not distinguish
between heads and tails. The relabelling map  with (H ) := (T ) := F and (U ) := U
identifies the proper relations between the categories in A and D.
Suppose now that we want to say something about the lower probability of the event
 of observing U on one flip and H or T on the other, immediately after observing the
UF
sequence (H , U , H , T ) with count vector  = (2, 1, 1)the last count in three from now on
refers to the number of U s in the observation sequence. In the A-domain, the gamble IUF

n
28
can be expressed by the polynomial q = Mn{H ,T ,U } (IUF
 ), n  2 given by:
q() = 2(H + T )U for all   {H ,T ,U } .
2   belong
So we want to find out whether polynomials of the type [2(H + T )U  ]12H
T U
to ({H , T , U }); see Equation (18).
On the other hand, as we have seen previously, in the D-domain, the gamble IUF
 can
be expressed by the polynomial p given by p() = 2F U for all   {F ,U } . Observe that
q = p  R . The count vector  = (2, 1, 1) in the A-domain corresponds to a count vector
R () = (3, 1) in the D-domain, where the first component refers to the number of F s and
the second to the number of U s. So here, we need to check whether polynomials of the type
[2F U  ]43F U belong to ({F , U }).

28. As before in similar contexts, it is easy to check that this polynomial remains the same for all n  2.

30

fiCoherent Predictive Inference under Exchangeability

The nice thing about representation insensitivity is that it makes checking whether
2  
polynomials of the type [2(H + T )U  ]12H
T U belong to ({H , T , U }) in the Adomain equivalent to checking whether polynomials of the type [2F U  ]43F U belong
to ({F , U }) in the D-domain.

Very interestingly, representation insensitivity is preserved under taking arbitrary nonempty infima of coherent inference systems, which allows us to look for the most conservative
representation insensitive coherent inference system that is compatible with an assessment A
on F, in a way that is a straightforward extension of the discussion near the end of Section 6.
Theorem 8. Consider any non-empty family i , i  I of representation insensitive coherent
inference systems. Then their infimum inf iI i is a representation insensitive coherent
inference system as well.
7.2 Specificity
Next, we turn to specificity, and recall the notations and assumptions in Section 4.5. Let us
define the surjective restriction map rB : RA  RB by:
rB ()z := z for all   RA and all z  B,

(27)

so in particular, rB () is the count vector on B obtained by restricting to B the (indices of
the) components of the count vector  on A. We also define the one-to-one injection map
iA : RB  RA by:
{
x if x  B
iA ()x :=
for all   RB and all x  A.
(28)
0
otherwise
This map can be used to define the following one-to-one maps IrB,A : V (B)  V (A), for any
r  N0 , as follows:

IrB,A (p) :=
bdeg(p)+r
()BA,iA () for all polynomials p in V (B).
(29)
p
deg(p)+r

NB

They derive their meaning from the following observation. A polynomial p on B can be
equivalently represented in any Bernstein basis on B of degree deg(p) + r. But when we
interpret these different representations as polynomials on A , they are no longer equivalent,
and lead to different polynomials IrB,A (p), r  N0 . The following propositions clarify what
exactly the effect of the operator IrB,A is.
Proposition 9. For any polynomial p on B and any r  N0 : IrB,A (p)  iA = p.
We introduce the following notation, for any   A such that B > 0: |+
B := rB ()/B .
Observe that |+


whenever

>
0.
B
B
B
Proposition 10. Consider any polynomial p on B , any r  N0 and any   A . When
deg(p) + r = 0 then p = c  R, and IrB,A (p|) = c. Otherwise, when deg(p) + r > 0:
{
deg(p)+r
B
p(|+
if B > 0
B)
IrB,A (p|) =
0
otherwise.
31

fiDe Cooman, De Bock, & Diniz

The maps IrB,A allow us to give the following elegant characterisation of specificity:
Theorem 11. A coherent inference system  is specific if and only if for all category sets A
and B such that B  A, for all p  V (B), all   NA  {0} and all r  N0 :
IrB,A (p)BA,  (A)  pBB,rB ()  (B).

(SP3)

Running Example. Suppose, as before, that we have made the observation (H , U , H , T ),
with count vector  = (2, 1, 1). We are interested in the posterior lower probability of the
 , where somebody has told us that neither of the two subsequent coin flipsafter
event HT
the first fourresulted in U . In a specific inference system, we are allowed to consider this
predictive inference problem in the reduced category space B = {H , T }, rather than in the
category space A = {H , T , U }. But then, in the B-space, we have to use the reduced count
vector rB () = (2, 1), obtained by leaving out the number of observed U s. The polynomials
we are lead to consider here, are therefore of the type [2H T  ]32H T , for which we want
to know whether they belong to ({F , U }).
In the A-space, the polynomial p() = 2H T  , whose degree is deg(p) = 2, is
transformed into the polynomials
]
[
T
H
r
IB,A (p|) = 2
  (H + T )2+r = [2H T  (H + T )2 ](H + T )r
H + T H +  T
for r  N0 . It follows from the argumentation in the proof of Theorem 11 that the original
problem requires us to check whether polynomials of the type
2
[2H T  (H + T )2 ](H + T )r 12H
T U

are in ({H , T , U }). Specificity allows us to look at the problem in the B-space, which is
easier.

Observe the close formal similarity between the conditions (RI3) and (SP3). It should
therefore not surprise us that specificity, too, is preserved under taking arbitrary non-empty
infima of inference systems.
Theorem 12. Consider any non-empty family i , i  I of specific coherent inference systems.
Then their infimum inf iI i is a specific coherent inference system as well.
Let us denote by Crs the set of all coherent inference systems that are both representation
insensitive and specific. It follows from Theorems 8 and 12 that Crs , like C, is closed under
arbitrary non-empty infima, so we can perform conservative reasoning, in very much the
same way as we discussed near the end of Section 6.

8. Immediate Prediction
If we have an inference system , we can look at the special case of immediate prediction,
where for a given category set A, after observing a sample of n  0 variables with count vector
  NAn , we want to express beliefs about the value that the next observation Xn+1 will

assume in A. So this is the specific case of predictive inference with n = 1, and Condition (18)
  NAn :
can now be simplified somewhat, as for all gambles f on A and all 
1
1
  BA,
f  DA
 SA (f )  (A) and f  DA
c
 SA (f )  (A),

32

fiCoherent Predictive Inference under Exchangeability

where we let the
so-called sampling expectation SA (f ) be the linear polynomial on A given
by SA (f |) := xA f (x)x for all   A .
The reason for this is that NA1 = {x : x  A} where x is the count vector corresponding
to a single observation of category x, or in other words, exz = xz for all z  A [Kronecker
delta]. Hence, for all x  A and any   A :
( ) 
( ) 
1
1
x
x
f
(z)
=
f
(x)
and
B
()
=
zez = x ,
Hy1A (f |x ) =
A,
x
x

x
zA

z[ ]

leading to:
Mn1A (f |) =



Hy1A (f |x )BA,x () =

1
x NA



f (x)x = SA (f |).

(30)

xA

It is a matter of straightforward verification that, due to the Bernstein coherence of HA , the
1 c
 is a coherent set of desirable gambles on A, for
so-called immediate prediction model DA
  NA  {0}. It induces the following predictive lower previsions:
every count vector 
{
}
1
 = sup   R : f    DA

P 1A (f |)
c
= sup {  R : [SA (f )  ]BA,
  (A)} .

(31)

Immediate prediction in the context of exchangeable imprecise probability models has
been studied in some detail by De Cooman et al. (2009a). Lower previsions, rather than
sets of desirable gambles, were the model of choice in that paper, and because of that, the
authors encountered problems with conditioning on sets of (lower) probability zero. In fact,
it is these problems that provided the motivation for dealing with the much more general
problem of (not necessarily immediate) predictive inference using sets of desirable gambles
in the present paper. In this section, we want to illustrate how many of the results proved
there can be made stronger (and with easier proofs, as is borne out in Appendix E.3) in the
present context.
The requirement (RI2) for representation insensitivity reduces to the following simpler
requirement on immediate prediction models: for all category sets A and D such that there
is an onto map  : A  D, for all gambles f on D and all   NA  {0}:
1
1
f    DA
c  f  DD
cR ().

(RI4)

Similarly, the requirement (SP2) for specificity reduces to the following simpler requirement
on immediate prediction models: for all category sets A and B such that B  A, for all
gambles f on B and all   NA  {0}:
1
1
f IB  DA
c  f  DB
crB ().

(SP4)

Let us now show that there is a simple characterisation of the immediate prediction
models that satisfy representation insensitivity. To get there, observe that we can consider
any gamble g on a category set A as a (surjective) pooling map from A to the finite subset
g(A) of Ralso a category set. The corresponding Rg : RA  Rg(A) is given by:

Rg ()r =
x for all r  g(A).
xA : g(x)=r

33

fiDe Cooman, De Bock, & Diniz

This simple idea allows for an intriguing reformulation of the representation insensitivity
requirement on immediate prediction models:
Proposition 13. The immediate prediction models associated with a coherent inference
system are representation insensitive if and only if for all category sets A, all gambles g on
A and all count vectors   NA  {0}:
1
1
g  DA
c  idg(A)  Dg(A)
cRg ().

(RI5)

Here, for any non-empty set B, we denote by idB the identity map on B, defined by idB (z) := z
for all z  B.
Proposition 13 tells us that whether a gamble is desirable depends only on the values it
assumesand not on where they are assumedand on the number of times each of these
values has been observed in the pastor rather would have been if we had been observing
the g(Xk ) rather than the Xk .
Let us now focus on what happens for events. Consider any event B  A that is nontrivial meaning that B is neither empty nor equal to A. Then for any real  the gamble
IB   assumes two values, 1   and , so we see after applying Proposition 13 that for
all   NA  {0}:
1
1
IB    DA
c  id{1,}  D{1,}
c(mB , mA\B ),

and therefore
{
}
1
P 1A (B|) = sup   R : IB    DA
c
{
}
1
= sup   R : id{1,}  D{1,}
c(mB , mA\B ) =: (mA , mB ),

(32)
(33)

meaning that, by representation insensitivity, the predictive lower probability for a non-trivial
event B depends only on the number of times mB that it has been observed in the past
experiments, and the total number of observations mA . The same thing holds for its predictive
upper probability 1  (mA , mA  mB ). For precise predictive probabilities, a similar property
is known as Johnsons sufficientness postulate (Johnson, 1924; Zabell, 1982).
So for any representation insensitive{coherent inference system,
we see that we can define a
}
2
so-called lower probability function  : (n, k)  N0 : k  n  [0, 1] through Equation (33),
which completely characterises the one-step-ahead predictive lower and upper probabilities29
for all non-trivial events and all count vectors. We shall now use the representation insensitivity
and specificity requirements to try and say more about this lower probability function. The
following theorem strengthens, simplifies, and extends similar results by De Cooman et al.
(2009a).
Theorem 14. Consider a representation insensitive coherent inference system . Then the
associated lower probability function  has the following properties:
L1.  is bounded: 0  (n, k)  1 for all n, k  N0 such that k  n.
L2.  is super-additive in its second argument: (n, k + `)  (n, k) + (n, `) for all
n, k, `  N0 such that k + `  n.
29. . . . but not necessarily the predictive lower and upper previsions . . .

34

fiCoherent Predictive Inference under Exchangeability

L3. (n, 0) = 0 for all n  N0 .
L4. (n, k)  k(n, 1) and n(n, 1)  1 for all n, k  N0 such that 1  k  n.
L5.  is non-decreasing in its second argument: (n, k)  (n, `) for all n, k, `  N0 such
that k  `  n.
L6. (n, k)  (n + 1, k) + (n, k)[(n + 1, k + 1)  (n + 1, k)] for all n, k  N0 such that
k  n.
L7.  is non-increasing in its first argument: (n + 1, k)  (n, k) for all n, k  N0 such
that k  n.
L8. Suppose that (n, 1) > 0 for all n  N, and let sn :=
1
. Then sn  0 and sn+1  sn .
(n, 1) = n+s
n

1
(n,1)

 n, or equivalently,

If  is moreover specific, then  has the following properties:
n
L9. Consider any real   (0, 1) and suppose that (1, 1)  , then (n, n)  1+n
for
1
all n  N0 . As a consequence, consider any s > 0 and suppose that (1, 1)  1+s , then
n
(n, n)  n+s
for all n  N0 .

We know from Theorem 4 that representation insensitive coherent inference systems are
near-ignorant, meaning that they are vacuous and therefore completely indecisive about any
single observation when no prior observations have been made. This is also borne out by
Theorem 14.L3. Let us define the imprecision function by
(n, k) := 1  (n, n  k)  (n, k) for all n, k  N0 such that k  n.

(34)

1

It is clear that P A (B|)  P 1A (B|) = (mA , mB ) is the width of the probability interval
for an event B that has been observed before mB out of mA times. For a representation
insensitive coherent inference system whose imprecision function (n, k) satisfies the following
property:
}
(n + 1, k)  (n, k)
for all 0  k  n,
(35)
(n + 1, k + 1)  (n, k)
the imprecision does not increase as the total number of observations increases. This suggests
that such representation insensitive coherent inference systems will display some of the
desirable behaviour mentioned in the Introduction: they are conservative when little has been
learned, and they never become less precise as more observations come in. In the following
sections, we intendamongst other thingsto take a closer look at whether this behaviour
is present in a number of such systems.
Immediate prediction is very important for predictive inference with precise probabilities,
as the Law of Total Probability guarantees that it is completely determined by its immediate
predictions. Perhaps surprisingly, this is not the case for predictive inference with imprecise
probabilities: Appendix D provides a counterexample. This also points to some of the
limitations in scope of the earlier work by De Cooman et al. (2009a). For this reason, we now
leave immediate prediction models for what they are, and in the rest of this paper concentrate
on the more general notion of an inference system.
35

fiDe Cooman, De Bock, & Diniz

9. The Vacuous Inference System
In this and the following sections, we provide explicit and interesting examples of representation insensitive, and of specific coherent inference systems. We begin with the simplest
one: the vacuous inference system V , which we introduced in Section 6 as the smallest,
or most conservative, coherent inference system. It associates with any category set A the
smallest Bernstein coherent set V (A) = HV,A := V + (A) containing all the Bernstein positive
polynomialsthe ones that are guaranteed to be there anyway, by Bernstein coherence alone.
We deduce from Proposition 30 in Appendix B that:
 = HV,A = V + (A) for all 
  NA  {0},
HV,A c
and from Proposition 28 in Appendix B that:
{
}
 = H V,A (p) = sup   R : p    V + (A)
H V,A (p|)
= min p = min p() for all p  V (A).
A

The predictive models for this inference system are now straightforward to find, as they
  NA  {0}, we
follow directly from Equations (21) and (23). For any n  N and any 
deduce that:
{
}
n
n
 = f  L(An ) : MnnA (f )  V + (A) ,
DV,A
= DV,A
c
(36)
and
 = min MnnA (f |) for all f  L(An ).
P nV,A (f ) = P nV,A (f |)
A

(37)

In particular:
1
1
 = L>0 (A),
DV,A
= DV,A
c

P 1V,A (f )

=


P 1V,A (f |)

(38)

= min f for all f  L(A),

(39)

and
V (n, k) = 0 for all n, k  N0 such that k  n.

(40)

These are the most conservative exchangeable predictive models there are, and they arise from
making no other assessments than exchangeability alone. As we gather from Equations (36)
(40), they are not very interesting, because they involve no non-trivial commitments, and
they do not allow for learning from observations. This is also borne out by the corresponding
imprecision function, which is given by:
V (n, k) = 1 for all n, k  N0 such that k  n.
Running Example. We have seen before that Mnn{H ,T } (IHT
 |) = 2H T for all n  2, and
therefore
n
P nV,{H ,T } (IHT
 ) = P V,{H ,T } (IHT
 |(3, 1)) =

min

Mnn{H ,T } (IHT
 |) =

max

Mnn{H ,T } (IHT
 |) =

{H ,T }

min

{H ,T }

2H T = 0

and
n

n

P V,{H ,T } (IHT
 ) = P V,{H ,T } (IHT
 |(3, 1)) =

{H ,T }

36

1
2H T = .
{H ,T }
2
max

fiCoherent Predictive Inference under Exchangeability

This shows that the vacuous inference model does not produce completely vacuous inferences:
it allows us to find out the consequences of making no other assessments than exchangeability.
But it does not allow us to change our lower and upper probabilities and previsions when
new observations come in.

Even though it makes no non-trivial inferences, the vacuous inference system satisfies
representation insensitivity, but it is not specific.
Theorem 15. The vacuous inference system V is coherent and representation insensitive.
Let us show by means of a counterexample that V is not specific,
Running Example. Let us go back to inferences about the category space A = {H , T , U } and
the reduced category space B = {H , T }. Consider the polynomial p() = 2H  H T + 2T
on {H ,T } . This polynomial is Bernstein positiveso p  V + ({H , T })because
p() = (2H  H T + 2T )(H + T ) = 3H + 3T
has an expansion in the Bernstein basis of degree 3 that is positive. But let us now consider
the corresponding polynomial on {H ,T ,U } :
2
2
q() := I0B,A (p|) = H
 H T + T
.

(41)

This polynomial is not Bernstein positive: it is easy to see that for every n  N0 ,
2
2
q() = (H
 H T + T
)(H + T + U )n
n . So q = I0 (p) 
will always have a term H T U
/ V + ({H , T , U }), and we infer from
B,A
Theorem 11 that V cannot be specific.


In the following sections, we shall prove that there are an infinity of more committal,
specific and representation insensitive coherent inference systems. We begin by introducing
a slightly modified version of the vacuous inference system that is coherent, representation
insensitive and specific.

10. The Nearly Vacuous Inference System
Let us introduce the nearly vacuous inference system NV the reason for its name will
become clear presentlyby:
NV (A) := HNV,A := V ++ (A) := {p  V (A) : (  int(A ))p() > 0}
for all category sets A.
Since V ++ (A) consists of all the polynomials that are positive on int(A ), we deduce from
  NA  {0}: HNV,A c
 = HNV,A = V ++ (A)
Proposition 28 in Appendix B that, for any 
and that:
 = H NV,A (p) =
H NV,A (p|)

p() = min p() for all p  V (A).

inf
int(A )

37

A

fiDe Cooman, De Bock, & Diniz

Since we know from Proposition 28 in Appendix B, and the counterexample following it, that
generally speaking V + (A)  V ++ (A), we see that this inference system is less conservative
than the vacuous one. As was the case for the vacuous inference system, the predictive models
for this nearly vacuous inference system are straightforward to find, as they follow directly
  NA  {0}, we deduce that:
from Equations (21) and (23). For any n  N and any 
{
}
n
n
 = f  L(An ) : MnnA (f )  V ++ (A) ,
DNV,A
= DNV,A
c
and
 = min MnnA (f |) for all f  L(An ).
P nNV,A (f ) = P nNV,A (f |)
A

In particular:
1
1
 = L>0 (A),
DNV,A
= DNV,A
c

 = min f for all f  L(A).
P 1NV,A (f ) = P 1NV,A (f |)
We see that the immediate prediction models, and the predictive lower previsions, for this
inference system are exactly the same as the ones for the vacuous inference systems.30 They
too do not allow for learning from observations.
Interestingly, and in contrast with the vacuous inference system, the nearly vacuous
inference system is specific, which already tells us that Crs 6= .
Theorem 16. The nearly vacuous inference system NV is coherent, representation insensitive and specific: NV  Crs .

11. The Skeptically Cautious Inference System
We now construct a rather simple inference system that is quite intuitive and slightly more
informative than the vacuous and nearly vacuous ones. Suppose that our subject uses the
following system for making inferences based on a sequence of n > 0 observations with count
 in a category set A. He is skeptical in that he believes that in the future, he will
vector ,
only observe categories that he has seen previously, so only categories in the set:
 := {x  A : mx > 0} .
A[]

(42)

But he is also cautious, because his beliefs about which of these already observed categories
will be observed in the future, are nearly vacuous. To explain this, assume first that in
particular, for n future observations, he has vacuous beliefs about which count vector he will
observe in the set
{
}
n
n


  NA : (y  A \ A[])my = 0 = NA[

]
 that he holds possible after observing the count vector ,

of those future count vectors 
 31 By Lemma 47 in Appendix B,
namely those count vectors with no observation outside A[].
30. This is a first example that shows that the immediate prediction models do not completely determine the
inference system. We shall come across another example in Appendix D.
31. The last equality in the equation above is actually a device that allows us to identify the count vectors on
 are zero, with count vectors on A[].
 We shall be using it repeatedly,
A whose components outside A[]
without explicit further mention, in the rest of this paper.

38

fiCoherent Predictive Inference under Exchangeability

this would lead us to associate the following set of polynomials with any count vector   NA :
{
}
+
n
V[]
(A) := p  V (A) : (n  deg(p)) bnp |NA[]
>0
{
}
+
= p  V (A) : p|A[]  V (A[]) .
But, because we already know that the vacuous models V + (A) do not lead to specific systems,
whereas the nearly vacuous models V ++ (A) do, we will modify this slightly, and rather
associate the following set of polynomials with any count vector   NA :
{
}
++
V[]
(A) := p  V (A) : p|A[]  V ++ (A[]) .
++
The polynomials in V[]
(A) are desirable in representation32 after observing a sample with
count vector , so we infer from Equation (19) that the subject considers as desirable in
representation all polynomials in:
{
}
++
++
V[]
(A)BA, = pBA, : p  V[]
(A) .

We are thus led to consider the following assessment:

++
ASC,A :=
V[]
(A)BA, ,
NA

and the set of all its positive linear combinations:
HSC,A

:= posi (ASC,A ) =

{
`

pk BA,k : `  N, nk  N, k 

NAnk , pk

}



++
V[
(A)
k]

. (43)

k=1

The following proposition guarantees that the sets HSC,A are the appropriate most conservative
models that summarise the exchangeable inferences for our skeptically cautious subject.
Proposition 17. HSC,A is the smallest Bernstein coherent set of polynomials on A that
includes ASC,A .
This also shows that the inference system SC , defined by SC (A) := HSC,A for all category
sets A, is coherent. We shall call it the skeptically cautious inference system.
We now want to find out how updating works in this system. To this end, we introduce a
slight generalisation of the set defined in Equation (43). Consider any   NA  {0}, and let
HSC,A, :=

{
`

pk BA,k : `  N, nk  N0 , mA + nk > 0, k 

NAnk , pk

}



++
V[+
(A)
k]

,

k=1

(44)
so we see that, in particular, HSC,A = HSC,A, for  = 0.
The sets HSC,A, have the following interesting characterisation:
32. As stated before, polynomials have no direct behavioural but only an indirect representational meaning,
as conveniently condensed representations for desirable gambles on observation sequences. Hence our
caution here in using the term desirable in representation.

39

fiDe Cooman, De Bock, & Diniz

Proposition 18. For all   NA  {0}:
{
}
HSC,A, = p  V (A) \ {0} : (K  min SA, (p))p|K  V ++ (K) ,

(45)

where
{
}
SA, (p) :=  =
6 K  A : A[]  K and p|K 6= 0 .

(46)

By min SA, (p) we mean the set of all minimal, or non-dominating, elements of SA, (p),
so min SA, (p) := {C  SA, (p) : (K  SA, (p))(K  C  K = {C)}. We formally extend
}
Equation (42) to include the case  = 0, so A[0] =  and SA,0 (p) =  =
6 K  A : p|K 6= 0 .
Proposition 19. For all   NA  {0}: HSC,A c = HSC,A, .
By combining this result with Equation (21), we can deriveadmittedly rather involved
expressions for the predictive sets of desirable gambles for the skeptically cautious inference
  NA  {0}:
system. For all n  N and 
{
}
n
 = f  L(An ) : MnnA (f )  HSC,A,
c
(47)
DSC,A
 .
  NA :
For immediate prediction, these expressions simplify significantly. For any 
{
}
1
1
 = f  L(A) : f |A[]
DSC,A
= L>0 (A) and DSC,A
c
 > 0  L>0 (A).

(48)

  NA :
The lower previsions that are derived from HSC,A,
 are more tractable. For any 
 =
H SC,A (p) = min p(x ) and H SC,A (p|)
xA

min p() for all p  V (A),

A[]


(49)

where, for any x  A, x is the degenerate probability mass function on A that assigns all
probability mass to x.
The predictive lower previsions for the skeptically cautious inference system are now
  NA :
easily obtained by combining Equations (49) and (23). For any n  N and any 
 =
P nSC,A (f |)

min MnnA (f |) for all f  L(An )

A[]


(50)

and
P nSC,A (f ) = min f (x, x, . . . , x) for all f  L(An ).

(51)

 = min f (x) for all f  L(A).
P 1SC,A (f ) = min f and P 1SC,A (f |)

(52)

xA

In particular:

xA[]

The lower probability function is given by:
{
1 if k = n > 0
SC (n, k) =
0 otherwise

for all n, k  N0 such that k  n,

and the corresponding imprecision function by:
{
1 if n = 0 or 0 < k < n
SC (n, k) =
0 otherwise
40

for all n, k  N0 such that k  n.

fiCoherent Predictive Inference under Exchangeability

Running Example. As before, Mnn{H ,T } (IHT
 |) = 2H T for all n  2. If we also take into
account that {H , T }[(3, 1)] = {H , T }, we get:
n
P nSC,{H ,T } (IHT
 ) = P SC,{H ,T } (IHT
 |(3, 1)) =

min

Mnn{H ,T } (IHT
 |) =

{H ,T }

max

Mnn{H ,T } (IHT
 |) =

{H ,T }

{H ,T }

min

2H T = 0

max

1
2H T = .
2

and
n

n

P SC,{H ,T } (IHT
 ) = P SC,{H ,T } (IHT
 |(3, 1)) =

{H ,T }

Because all categories are observed for the count vector (3, 1)meaning that {H , T }[(3, 1)] =
{H , T }we find the same inferences as for the vacuous inference system.

Interestingly, the coherent inference system SC also satisfies both representation insensitivity and specificity.
Theorem 20. The skeptically cautious inference system SC is coherent, representation
insensitive and specific: SC  Crs .

12. The IDMM Inference Systems
Imprecise Dirichlet Modelsor IDMs, for shortare a family of parametric inference models
introduced by Walley (1996) as conveniently chosen sets of Dirichlet densities diA (|) with
constant prior weight s:
{
}
{diA (|) :   KsA } , with KsA :=   RA
(53)
>0 : A = s = {s :   int(A )} ,
for any value of the (so-called) hyperparameter s  R>0 and any category set A. The Dirichlet
densities diA (|) are defined on int(A ); see Appendix C for an explicit definition and
extensive discussion.
These IDMs generalise the Imprecise Beta models introduced earlier by Walley (1991). In
a later paper, Walley and Bernard (1999) focussed on a closely related family of predictive
inference models, which they called the Imprecise Dirichlet Multinomial Modelsor IDMMs,
for short.33 We refer to these papers, and to a more recent overview paper by Bernard
(2005) for extensive motivating discussion of the IDM(M)s, their inferences and properties.
For precise Dirichlet models and their expectations, and the related Dirichlet multinomial
models, we have gathered in Appendix C the most important facts, properties and results,
necessary for a proper understanding of our present discussion of the IDM(M)s in the context
of inference systems.
One of the reasons Walley (1996) had for suggesting the IDM as a reasonable model is
precisely that it satisfies the pooling34 invariance properties we discussed in Section 4.1. This
is also discussed with more emphasis by Walley and Bernard (1999) and Bernard (2005), but
we know of no detailed and explicit formulations of these properties in the literature, and
the proofs we have seen are fairly sketchy. Bernard (1997, 2005) also suggests that the IDM
33. In the later paper, Walley and Bernard (1999) clearly distinguish in name between the parametric IDMs
and the predictive IDMMs, while in the earlier paper by Walley (1996), both types of models are referred
to as IDMs.
34. Walley uses the term representation invariance rather than pooling invariance.

41

fiDe Cooman, De Bock, & Diniz

and the underlying precise Dirichlet models satisfy a so-called specificity property, which
we have tried to translate to the present context of predictive inference in Section 4.5.
In the present section, we use the ideas behind Walley and Bernards IDM(M)s to construct
an interesting family of coherent inference systems, and we give a detailed and formal proof
in Appendix E of the fact that these inference systems are indeed representation insensitive
and specific. Interestingly, we shall need a slightly modified version of Walleys IDM(M)
to make things work. The reason for this is that Walleys original version, as described by
Expression (53), has a number of less desirable properties, that seem to have been either
unknown to, or ignored by, Walley and Bernard. We describe these shortcomings in some
detail in Appendix D. For our present purposes, it suffices to mention that, contrary to what
is often claimed, and in contradistinction with our new version, inferences using the original
version of the IDM(M) do not necessarily become more conservative (or less committal) as
the hyperparameter s increases.
In our version, rather than using the hyperparameter sets KsA , we consider the sets
{
}
sA :=   RA
>0 : A < s for all s  R>0 .
Observe that
{
}
sA = s0  : s0  R>0 , s0 < s and   int(A ) =



0

KsA .

0<s0 <s

For any s  R>0 , and any category set A, we now consider the following set of polynomials
p, with positive Dirichlet expectation DiA (p|) for all hyperparameters   sA :
s
:= {p  V (A) : (  sA ) DiA (p|) > 0} .
HIDM,A

We shall see further on in Theorem 21 that this set is Bernstein coherent. We call the inference
system sIDM , defined by:
s
sIDM (A) := HIDM,A
for all category sets A,

the IDMM inference system with hyperparameter s > 0. The corresponding updated models
  NA  {0}, given by:
are, for any 
s
 = {p  V (A) : (  sA ) DiA (p|
 + ) > 0}
HIDM,A
c

(54)

 = inf s DiA (p|
 + ) for all p  V (A).
H sIDM,A (p|)

(55)

and
A

Using these expressions, the predictive models for the IDMM inference system are straightforward to find; it suffices to apply Equations (21) and (23). For any n  N and any
  NA  {0}:

{
}
s,n
 = f  L(An ) : (  sA ) DiA (MnnA (f )|
 + ) > 0 ,
DIDM,A
c
(56)
and
 = inf s DiA (MnnA (f )|
 + ) for all f  L(An ),
P s,n
IDM,A (f |)
A

42

(57)

fiCoherent Predictive Inference under Exchangeability

where, using the notations introduced in Appendix C:
 + ) = DiMnnA (HynA (f )|
 + )
DiA (MnnA (f )|
( ) 

n
1
n

=
HyA (f |)
(mx + x )(mx ) .
(n) 

(mA + A )
n
xA

N

(58)

A

In general, these expressions seem forbidding, but the immediate prediction models are
  NA  {0}:
manageable enough. For any 
{
}
1 
s,1
 = f  L(A) : f > 
(59)
DIDM,A c
f (x)mx ,
s
xA

1
s

P s,1
(f
|
)
=
f (x)mx +
min f for all f  L(A),
(60)
IDM,A
mA + s
mA + s
xA

and

k
for all n, k  N0 such that k  n.
n+s
The corresponding imprecision function is given by:
sIDM (n, k) =

sIDM (n, k) =

s
for all n, k  N0 such that k  n,
n+s

and it is decreasing in its first and constant in its second argument, which implies that it
satisfies Condition (35). This suggests that IDMM inference systems are conservative when
little has been learned, and become more precise as more observations come in.
Running Example. As before, with Mnn{H ,T } (IHT
 |) = 2H T for all n  2, we find that,
using the results in Appendix C:
 )
(

Di{H ,T } Mnn{H ,T } (IHT
 |)  =

2H T
.
(H + T )(H + T + 1)

It is then not very difficult to verify using Equation (57) that for any 0 < s:
s,n

P s,n
 ) = 0 and P IDM,{H ,T } (IHT
 ) =
IDM,{H ,T } (IHT

1 s
.
21+s

After observing the count vector (3, 1), we find after some manipulations that:
2(3 + )
2(3 + s)
=
,
0<<s (4 + )(5 + )
(4 + s)(5 + s)

P s,n
 |(3, 1)) = inf
IDM,{H ,T } (IHT
and similarly:




6

1+s
(4 + s)(5 + s)
s,n
P IDM,{H ,T } (IHT
 |(3, 1)) =

1
4+s


25+s

if s  2
if s  2.

Observe that for infinitely large s, we recover the inferences for the vacuous system.
43



fiDe Cooman, De Bock, & Diniz

Interestingly, the immediate prediction models for our version of the IDMM inference
system coincide with those of Walleys original version. Hence, in the many practical applications that are concerned with immediate prediction only, both approaches yield identical
results.
The IDMM inference systems constitute an uncountably infinite family of coherent
inference systems, each of which satisfies the representation insensitivity and specificity
requirements.
Theorem 21. For any s  R>0 , the IDMM inference system sIDM is coherent, representation
insensitive and specific: sIDM  Crs .
s
Since Crs is closed under non-empty infima, the infimum 
IDM of all IDM , s > 0 is
still coherent, representation insensitive and specific, and more conservative than any of the
IDMM inference systems. It is given by:
{
}
+++

(A) := p  V (A) : (  RA
IDM (A) = V
>0 ) DiA (p|) > 0 ,

and although this set generally strictly includes the sets V + (A) and V ++ (A), the associated
immediate prediction models and predictive lower previsions can be shown to coincide with
the ones for the vacuous and nearly vacuous inference systems.

13. The Skeptical IDMM Inference Systems
We now combine the ideas in the previous two sections: we suppose that our subject uses
the following system for making inferences based on a sequence of n > 0 observations with
 in a category set A. As before in Section 11, he is skeptical in that he
count vector ,
believes that in the future, he will only observe categories that he has seen previously, so only
 But rather than being cautious in having completely vacuous
categories in the set A[].
beliefs about which of these already observed categories will be observed in the future, he
uses an IDMM-like inference for them, as described in Section 12.
It turns out this can be done quite simply by replacing, in the characterisation (45) of
the sets HSC,A, of the skeptically cautious inference system, the nearly vacuous models
s
V ++ (K) by the appropriate IDMM models HIDM,K
crK (). So we define, for any category
set A, any   NA  {0} and any s  R>0 , the following set of polynomials:
{
}
s
s
:= p  V (A) \ {0} : (K  min SA, (p))p|K  HIDM,K
HSI,A,
crK () ,
(61)
where we recall that if K  min SA, (p), then A[]  K and therefore K[rK ()] =
A[]  K = A[], so  and rK () are essentially the same count vectors. We also let
s
s
:= HSI,A,
HSI,A
for  = 0, or in other words:
}
{
s
s
:= p  V (A) \ {0} : (K  min SA,0 (p))p|K  HIDM,K
HSI,A
,
{
}
where, again, SA,0 (p) =  =
6 K  A : p|K 6= 0 . In the remainder of this section, we show
s
that the sets of polynomials HSI,A
indeed lead to the definition of a reasonable and potentially
useful type of inference system. We begin with coherence.
s
Proposition 22. HSI,A
is a Bernstein coherent set of polynomials on A .

44

fiCoherent Predictive Inference under Exchangeability

s
This shows that the inference system sSI , given by sSI (A) := HSI,A
for all category sets A,
s
is coherent. We call SI the skeptical IDMM inference system with hyperparameter s.
We now want to find out how updating works in this inference system. The following
proposition should not really come as a surprise.
s
s
Proposition 23. For any   NA  {0}: HSI,A
c = HSI,A,
.

By combining this with Equation (21), we obtain the followingagain, rather involved
predictive sets of desirable gambles for the skeptical IDMM inference systems. For any n  N
  NA  {0}:
and any 
{
}
s,n
s
 = f  L(An ) : MnnA (f )  HSI,A,
DSI,A
c
(62)
 .

s
 are rather abstract, this is not the case for the
Although the expressions for HSI,A
c
  NA :
corresponding lower previsions. For any 

H sSI,A (p) = min p(x ) for all p  V (A)
xA

(63)

and
 =
H sSI,A (p|)

inf

sA[]


 + )
DiA[]
|rA[]
 (p|A[]
 ()


 for all p  V (A).
= H sIDM,A[]
|rA[]
 ())
 (p|A[]


(64)
(65)

Combining this with Equation (23), we immediately obtain the following predictive lower
  NA :
previsions for the skeptical IDMM inference systems. For any n  N and any 
n
P s,n
SI,A (f ) = min f (x, x, . . . , x) for all f  L(A )
xA

and
 =
P s,n
SI,A (f |)

inf

sA[]


n
 + )
DiA[]
 (MnA[]
 ()
 (f |A[]
 n )|rA[]

 for all f  L(An ).
= P s,n
 ())
 n |rA[]
 (f |A[]
IDM,A[]

(66)

The immediate prediction models of the skeptical IDMM inference systems are surprisingly
more manageable:
s,1
DSI,A
= L>0 (A) and P s,1
SI,A (f ) = min f for all f  L(A)

  NA :
and, for any 
{
}
1 
s,1
 = f  L(A) : f |A[]
DSI,A c
f (x)mx  L>0 (A)
 >
s

(67)


xA[]


1
s
s,1
 =
(f |)
f (x)mx +
min f (x) for all f  L(A).
P SI,A
mA + s
mA + s xA[]


xA[]

45

(68)

fiDe Cooman, De Bock, & Diniz

The lower probability function is given by:
{
k
if k < n or n = 0
s
SI (n, k) = n+s
1
if k = n > 0
and the corresponding imprecision function by:
{
s
if n = 0 or 0 < k < n
s
SI (n, k) = n+s
0
otherwise

for all n, k  N0 such that k  n,

for all n, k  N0 such that k  n.

When we consider the case n > 0, we see that sSI (n, n) = 0 but sSI (n + 1, n) =
this imprecision function does not satisfy Condition (35).

s
n+1+s

> 0, so

Running Example. Because {H , T }[(3, 1)] = {H , T }, we infer from Equation (66) that the
 are the same as for the IDMM inference systems.
inferences about the event HT

All the coherent inference systems sSI also satisfy both representation insensitivity and
specificity.
Theorem 24. For each s  R>0 , the corresponding skeptical IDMM inference system is
coherent, representation insensitive, and specific: sSI  Crs .
s
Since Crs is closed under non-empty infima, the infimum 
SI of all SI , s > 0 is still
coherent, representation insensitive and specific, and more conservative than any of the
skeptical IDMM inference systems. It can be shown that the associated immediate prediction
models and predictive lower previsions coincide with the ones for the skeptically cautious
inference system.

14. The Haldane Inference System
We already know from our discussion of near-ignorance following Theorem 4 that no representation insensitive coherent inference system can be fully precise, as its immediate prediction
models before observations have been made, must be completely vacuous. But we can ask
ourselves whether there are representation insensitive (and specific) inference systems whose
posterior predictive lower previsions become precise (linear) previsions. This is the problem we
address in this section. We shall first construct such an inference system, and then show that
this system is, in some definite sense, unique in having linear posterior predictive previsions.
We use the family of all IDMM inference systems sIDM , s  R>0 , to define an inference
system H that is more committal than any of them:


s
HIDM,A
=
sIDM (A) for all category sets A.
H (A) = HH,A :=
sR>0

sR>0

We call this H the Haldane inference system, for reasons that will become clear further on
in this section.
Theorem 25. The Haldane inference system H is coherent, representation insensitive and
specific: H  Crs .
46

fiCoherent Predictive Inference under Exchangeability

Due to its representation insensitivity, the Haldane system satisfies prior near-ignorance.
This implies that before making any observation, its immediate prediction model is vacuous,
and as far away from a precise probability model as possible. But we are about to show
that, after making even a single observation, its inferences become precise-probabilistic: they
coincide with the inferences generated by the Haldane (improper) prior.
To get there, we first take a look at the models involving sets of desirable gambles. For
  NA  {0}:
any 

s
 = {p  V (A) : (s  R>0 )(  sA ) DiA (p|
 + ) > 0} =

HH,A c
HIDM,A
c.
sR>0
(69)
The corresponding predictive models are easily derived by applying Equation (21). For any
  NA  {0}:
n  N and any 
{
}
n
 = f  L(An ) : (s  R>0 )(  sA ) DiA (MnnA (f )|
 + ) > 0
DH,A
c

s,n

=
DIDM,A
c.
(70)
sR>0

The immediate prediction models are obtained by combining Equations (70) and (59). For
  NA :
any 
{
}

1
1
 = f  L(A) :
DH,A = L>0 (A) and DH,A c
f (x)mx > 0  L>0 (A).
xA

It turns out that the expressions for the corresponding lower previsions are much more
  NA  {0}:
manageable. First of all, we find for any 
 + ) = lim H sIDM,A (p|)
 for all p  V (A).
inf DiA (p|

 = lim
H H,A (p|)

s+0 sA

s+0

(71)

 = 0, this simplifies to:
In particular, for 
H H,A (p) = min p(x ) for all p  V (A),
xA

(72)

  NA , we find linear previsions:35
whereas for any 
 = H H,A (p|)
 = HH,A (p|)
 = DiA (p|)
 for all p  V (A).
H H,A (p|)

(73)

The corresponding predictive models are easily derived by applying Equation (23). For any
  NA  {0}:
n  N and any 
 = lim
P nH,A (f |)

 + ) = lim P s,n
 for all f  L(An ).
inf DiA (MnnA (f )|
IDM,A (f |)

s+0 sA

s+0

(74)
 = 0:
In particular, for 
P nH,A (f ) = min f (x, x, . . . , x) for all f  L(An ),
xA

35. The Dirichlet expectations DiA (|) are strictly speaking defined for   RA
>0 , but as we argue in
Appendix C, they can be continuously extended to  with some components zero, and the others strictly
positive.

47

fiDe Cooman, De Bock, & Diniz

  NA :
and for any 

P nH,A (f |)

=

n

P H,A (f |)

=

n

PH,A
(f |)

=


n
NA

( ) 
(nx )
n
xA mx
.
(n)

m

HynA (f |)

(75)

A

  NA :
For the immediate prediction models, we find that for any 

mx
1
 =
P 1H,A (f ) = min f and PH,A
(f |)
f (x)
for all f  L(A),
mA
xA

and the lower probability function is given by:
{
k
if n > 0
H (n, k) = n
for all n, k  N0 such that k  n.
0
if n = 0
The corresponding imprecision function is given by:
{
1 if n = 0
H (n, k) =
for all n, k  N0 such that k  n,
0 if n > 0
and it satisfies Condition (35), which suggests that also the Haldane inference system displays
albeit in an extreme and not very interesting mannerthe desirable behaviour mentioned in
the Introduction: it is conservative when little has been learned, and it never become less
precise as more observations come in.
Running Example. We can use Equation (74) and the results previously obtained for the
IDMM inference systems to find that
n

n
P nH,{H ,T } (IHT
 ) = 0 and PH,{H ,T } (IHT
 |(3, 1)) =
 ) = P H,{H ,T } (IHT

3
.
10

We want to point out that the first equalities do not contradict the prior near-ignorance of
the Haldane inference system, as that only pertains to immediate predictions: predictions
about single future observations.

The precise posterior predictive previsions in Equation (75) are exactly the ones that
would be found were we to formally apply Bayess rule with a multinomial likelihood and
Haldanes improper prior (Haldane,
1945; Jeffreys, 1998; Jaynes, 2003), whose density is a
function on int(A ) proportional to xA x1 . This, of course, is why we use Haldanes name
for the inference system that produces them. Our argumentation shows that there is nothing
wrong with these posterior predictive previsions, as they are based on coherent inferences. In
fact, our analysis shows that there is an infinity of precise and proper priors on the simplex
A that, together with the multinomial likelihood, are coherent with these posterior predictive
previsions: every coherent prevision on V (A) that dominates the coherent lower prevision
H H,A on V (A).36 For binomial parametric inferences under the Haldane prior, Walley (1991,
Section 7.4.8) comes to a related conclusion in a completely different manner.
36. It is an immediate consequence of the F. Riesz Representation Theorem that each such coherent prevision
is the restriction to polynomials of the expectation operator of some unique -additive probability measure
on the Borel sets of A ; see for instance the discussion by De Cooman and Miranda (2008a) and also
footnote 2.

48

fiCoherent Predictive Inference under Exchangeability

There is a simple argument to show that these Haldane posterior predictive previsions
are the only precise ones that are compatible with representation insensitivity. Indeed, it
can be shown that for any representation insensitive coherent inference system with precise
posterior predictive previsions, the lower probability function must satisfy (n, k) = k/n for
n > 0 and 0  k  n,37 and then it is straightforward to prove, using Bayess Theorem to go
from immediate prediction to more general predictive inference, that the posterior predictive
previsions must be Haldanes.

15. Characterisation of the IDMM Immediate Predictions
The lower probability function (n, k) for a representation insensitive coherent inference
system gives the lower probability of observing a non-trivial event that has been observed k
times before in n trials.
Now suppose that a subject specifies a single lower probability, namely the value of
(1, 1)  [0, 1]: the probability of observing something (again) that has been observed (once)
in a single trial. Then we can ask ourselves what the most conservative consequences of such
an assessment are, if we take representation insensitivity and specificity for granted. In other
words, what is the most conservative representation insensitive and specific coherent inference
system that has (at least) this given value (1, 1) for its lower probability function? This
question makes sense because the representation insensitive and specific coherent inference
systems constitute a complete meet-semilattice by Statement (25) and Theorems 8 and 12.38
Clearly, if (1, 1) = 0, this is the smallest representation insensitive and specific coherent
inference system, which as we know from the discussion in Sections 9 and 10, must have the
same immediate prediction models and predictive lower previsions as the (nearly) vacuous
inference system. We consider the case that 0 < (1, 1) < 1,39 or in other words, to use a
parametrisation that will turn out to be more convenient for our purposes, that:
(1, 1) =

1
1
for some positive real number s :=
 1.
1+s
(1, 1)

(76)

Let us denote this most conservative inference system by s , and its lower probability
1
function by s , then by assumption s (1, 1)  1+s
. It now follows from Theorem 14.L9 that
n
s
 (n, n)  n+s for all n  N0 . But since for the IDMM inference system sIDM , Equation (60)
n
tells us that sIDM (n, n) = n+s
, and since by assumption sIDM (n, n)  s (n, n), we conclude
that:
n
s (n, n) = sIDM (n, n) =
for all n  N0 .
(77)
n+s
It has been surmised (Bernard, 2007; De Cooman et al., 2009a) that the IDMM inference
system with hyperparameter s could be the smallest, most conservative, representation
1
insensitive and specific coherent inference system with a given value (1, 1) = 1+s
. In
fact, trying to prove this was what made us start research on the present paper. But
this conjecture turns out to be false: apart from the lower bound (77) on the (n, n),
37. It suffices to exploit the additivity of precise probabilities and the symmetry implied by representation
insensitivity; for an explicit proof, see the paper by De Cooman et al. (2009a, Thm. 7).
38. See the discussion near the end of Section 7.
39. We surmise, but do not prove here, that the most conservative representation insensitive and specific
coherent inference system corresponding to (1, 1) = 1 might be the skeptically cautious one.

49

fiDe Cooman, De Bock, & Diniz

representation insensitivity and specificity impose no lower bounds on the (n, k) for k < n.
To see this, consider the inference system sMC := inf{SC , sIDM }, which by Statement (25)
and Theorems 8, 12, 20 and 21 is coherent, representation insensitive and specific: sMC  Crs .
Its lower probability function sMC satisfies:
{
n
n
min{1, n+s
} = n+s
if k = n > 0
sMC (n, k) = min{SC (n, k), sIDM (n, k)} =
k
min{0, n+s } = 0
otherwise,
substantiating the claim we made above. See also Figure 1, where we have depicted lower (and
upper) probability functions for the Haldane system H , the IDMM system sIDM , sMC and
n
s
the inference system inf{4s
SI , IDM }. The latter three all share the same value n+s for (n, n),
n  0. We conjecture that sMC could be the smallest, most conservative, representation
1
insensitive and specific coherent inference system with a given value (1, 1) = 1+s
, but offer
no proof for this.
(n, k)
1
n
n+s

s
n+s

0
0

1

2

...

n1

n

k

Figure 1: Lower and upper probability functions: H for the Haldane system (dark grey,
s
4), sIDM for the IDMM system with hyperparameter s (blue, ?), min{4s
SI , IDM }
(orange, ) and sMC = min{SC , sIDM } (red, ). This specific plot was made for
n = 10 and s = 2.
This means that if we want to characterise the IDMM inference systems in any way as
the most conservative ones, we need to add, besides coherence, representation insensitivity
and specificity, another requirement that is preserved under taking infima. One possible
candidate for this, which we shall prove does the job and is inspired by Figure 1, is the
following requirement.
Let us define the subjects surprise of an event as his supremum rate for betting on the
opposite event, or in other words, his lower probability for the opposite event. This surprise
is highclose to onewhen the subject believes strongly that the event will not occur, and
lowclose to zerowhen the subject has no strong beliefs that it will not occur.
50

fiCoherent Predictive Inference under Exchangeability

This allows us to associate a so-called surprise function (n, k) := (n, n  k) with a
lower probability function, where (n, k) is the subjects surprise when observing a non-trivial
event that has been observed k out of n times before.
It follows from Theorem 14.L5 that for any representation insensitive system, the surprise
function is non-increasing in its second argument:
(n, k) := (n, k + 1)  (n, k) = (n, n  k  1)  (n, n  k)  0 for 0  k  n  1.
This is a fairly intuitive property: the more often an event has been observed before, the
smaller the surprise is at seeing it again.
We shall say that a representation insensitive system has concave surprise if
2 (n, k) := (n, k + 1)  (n, k)  0 for 0  k  n  2,
where, of course, 2 (n, k) = (n, n  k  2)  2(n, n  k  1) + (n, n  k). It is not
difficult to see that having concave surprise is preserved under taking non-empty infima
of inference systems, so it makes sense to go looking for the smallest (most conservative)
coherent representation insensitive and specific coherent inference system that has concave
surprise, and satisfies some additional local assessments, such as (76).
Looking at Figure 1 makes us suspect that the IDMM inference system sIDM might be
this system, but again, we offer no proof for this conjecture. We can however provide a proof
for the following, related but (probably) weaker, statement, which focusses on immediate
prediction only:
Theorem 26. The immediate prediction models P 1A (|),   NA  {0} for the smallest
(most conservative) coherent representation insensitive and specific coherent inference system
 that has concave surprise and satisfies (76), coincide with the ones for the IDMM inference
system sIDM with hyperparameter s.

16. Conclusion
We believe this is the first paper that tries to deal in a systematic fashion with principles for
predictive inference under exchangeability using imprecise probability models. Two salient
features of our approach are (i) that we consistently use coherent sets of desirable gambles as
our uncertainty models of choice; and (ii) that our notion of an inference system allows us to
derive a conservative predictive inference method combining both local predictive probability
assessments and general inference principles.
The first feature is what allows us, in contradistinction with most other approaches in
probability theory, to avoid problems with determining unique conditional models from
unconditional ones when conditioning on events with (lower) probability zero. A set of
n c
 and
polynomials HA completely determines all prior and posterior predictive models DA
n
n
 even when the (lower) prior probability P A ([])
 = H A (BA,
P A (|),
 ) of observing the
 is zero. An approach using only lower previsions and probabilities would make
count vector 
this much more complicated and involved, if not impossible. Interestingly, we can provide
a perfect illustration of this fact using the results in Sections 11, 13 and 14.40 The three
40. Something similarly dramatic happens in Sections 9 and 10: the inference systems there have the same
immediate prediction models and the same (predictive) lower previsions, but one is specific and the other
is not.

51

fiDe Cooman, De Bock, & Diniz

inference systems that are described therethe skeptically cautious, the skeptical IDMM and
the Haldane systemshave, for any given category set A, three different sets of polynomials
HA . Nevertheless, as we can gather from Equations (49), (63) and (72), they have the same
lower prevision H A and therefore the same prior predictive models P nA . And any count vector
  NA has the same prior lower probability:


 = H A (BA,
P nA ([])
 ) = min BA,
 (x ) = 0.
xA

 and the
This zero lower probability makes sure that the posterior lower previsions H A (|)
 are not uniquely determined by the prior lower prevision
posterior predictive models P nA (|)
H A : we infer from Equations (49), (65) and (73) that they are indeed very different for these
three types of inference systems. We fail to see how we could have come up withlet alone
proved the necessary results forthese three systems relying only on lower prevision or credal
set theory.
We canand musttake this line of argumentation even further. By Theorem 4, any
inference system that satisfies (prior) representation insensitivity has near-vacuous prior
predictive models, and therefore, by time consistency and coherence [monotonicity], we see
n
 = 0 for any
that its prior predictive lower previsions must satisfy H A (BA,
 ) = P A ([])
  NA as well. This simply means that it is impossible in a (prior) representation insensitive

coherent inference system for the lower prevision H A to uniquely determine the conditional
 And therefore any systematic way of dealing with such inference
lower previsions H A (|).
systems must be able to resolveor deal withthis non-unicity in some way. We believe
our approach involving coherent sets of desirable gambles is one of the mathematically more
elegant ways of doing this.
The second feature has allowed us, as an example, to characterise the IDMM immediate
predictions as the most conservative ones satisfying a number of inference principles. The
approach we follow canat least in principlealso be used for other types of inference
systems and other inference principles. The key requirement for an inference principle to
make it amenable to our approach is that, when formulated as a property of an inference
system, it should be preserved under taking arbitrary non-empty infima. The three inference
principles that we have been considering aboverepresentation insensitivity, specificity and
having concave surprisehave this property, but there is nothing that prevents our analysis
and approach from being extended to any other inference principle that has it too. The only
complications we see, at this point, are of a technical mathematical nature. The reader will no
doubt have noticed that our proofs for the results in the later sections are quite involved and
technical, and rely quite heavily on properties of polynomials on a simplex. We feel that in
the present paper we have made some headway into this mathematical territory, for instance
with our new discussion about the Bernstein positivity of polynomials near Proposition 28
in Appendix B. In the Conclusions of a paper by De Cooman and Quaeghebeur (2012), a
characterisation of Bernstein positivity was mentioned as an open problem with interesting
practical applications in doing inferencenatural extensionunder exchangeability. But
much remains open for further exploration, and a more determined study of the mathematical
structure and properties of such polynomials would certainly help in alleviating the technical
difficulties of working with inference principles in inference systems.
While this paper has only just opened up what we feel to be an interesting line of
research into the foundations of predictive inference, it nevertheless has provided answers to
52

fiCoherent Predictive Inference under Exchangeability

a number ofif not allopen problems formulated in the Conclusions of an earlier paper by
De Cooman et al. (2009a), who tried to deal with representation insensitivity in immediate
prediction. As a first example: it was asked there whether there are representation insensitive
coherent inference systems whose lower probability functions are not additive in the second
argument? It suffices to look at Figure 1 to see that the answer is, clearly, yes. Another
question was: are there representation insensitive coherent inference systems that are not
mixing predictive systems?41 It follows from Equation (68) that the answer is yes: each of
the skeptical IDMM inference systems provides an example. Finally, we can use the infimum
sMC of the skeptically cautious inference system SC and an IDMM inference system sIDM ,
mentioned briefly in Section 15, to answer two more questions. Are there representation
insensitive coherent inference systems for which the inequality in Theorem 14.L6 is strict?
And are there representation insensitive coherent inference systems whose behaviour on
gambles is not completely determined by their lower probability function? The inference
system sMC provides a positive answer to both questions.
Most of the inference systems mentioned above, apart from the IDMM and the Haldane
systems, appear here for the first time. Some of them may appear contrived and perhaps
even artificial, but we have found them to be most useful in constructing (counter)examples,
shaping intuition, and building new models, as Figure 1 and the argumentation above clearly
indicate. We might also wonder whether there are other representation insensitive and/or
specific coherent inference systems, which cannot be produced as appropriately chosen infima
of the examples we have introduced here. We suggest, as candidates for further consideration,
the inference systems that can be derived using Walleys (1997) bounded derivative model,
and inference systems that can be constructed using sets of infinitely divisible distributions,
as recently proposed by Mangili and Benavoli (2013). The framework provided here, as well as
the simple characterisation results of Theorems 7 and 11, should be quite useful in addressing
this and similar problems.
To end, we want to draw attention once again to a simple and direct, but quite appealing,
consequence of our argumentation in Section 14: there is an infinity of precise and proper
priors that, together with the multinomial likelihood, are coherent with the Haldane posterior
predictive previsions. So, there is no need for improper priors to justify these posteriors, as
there are proper priors that will do the job perfectly well. This (precise-)probabilistic conclusion
follows easily when looking at the problem using the more general and powerful language
of imprecise probabilities. Moreover, we have seen that properties such as representation
insensitivity cannot be satisfied by precise probabilistic models. Finally, the entire framework
of conservative predictive inference using inference principles would be impossible to develop
within the more limitative context of precise probabilities. This shows that there are distinct
advantages to using imprecise probability models for dealing with predictive inference.

Acknowledgements
Gert de Coomans research was partially funded through project number 3G012512 of the
Research Foundation Flanders (FWO). Jasper De Bock is a PhD Fellow of the Research
41. Loosely speaking: that cannot be written as a (specific kind of) convex mixture of the Haldane inference
system and an IDMM inference system; see the paper by De Cooman et al. (2009a, Section 5) for more
information.

53

fiDe Cooman, De Bock, & Diniz

Foundation Flanders and wishes to acknowledge its financial support. Marcio Diniz was
supported by FAPESP (So Paulo Research Foundation), under the project 2012/14764-0
and wishes to thank the SYSTeMS Research Group at Ghent University for its hospitality and
support during his sabbatical visit there. The authors would like to thank three anonymous
reviewers for their many insightful comments and suggestions aimed at making this paper
easier to read and cleaning up misunderstandings. A special thank you also to the great
Arthur Van Camp for his enthusiasm in everything and, in particular, in helping us check
little examples.

Appendix A. Notation
In this appendix, we provide a list of the most commonly used and most important notation,
and where it is defined or first introduced.
notation

meaning

introduced where

A, B, C, D
IB
X, Xn
n
n
posi(A)
L(A)
L>0 (A)
L0 (A)




n
DA

category sets, events
indicator of an event B
variable, variable at time n
number of already observed variables
number of to be observed variables
cone generated by A
set of all gambles on A
set of all positive gambles on A
set of all non-positive gambles on A
observed sample
observed count vector
prior predictive set of desirable gambles
for category set A and n future observations
posterior predictive set of desirable gambles
prior predictive lower prevision
posterior predictive lower prevision
pooling map or relabelling map
renaming bijection
category permutation
sample with observations outside B eliminated
counting map
set of count vectors for n observations
set of all count vectors, with zero
hypergeometric expectation operator
multinomial coefficient with count vector 
multinomial expectation operator
simplex of all probability mass functions on A
sum of components x of  over x  B
Bernstein basis polynomial
set of polynomials of degree up to n on A

Section 1
Section 2.2
Section 1
Section 1
Section 1
Equation (1)
Section 2.2
Section 2.2
Section 2.2
Section 3
Section 5.4
Section 3

n c,
n c
 DA

DA
n
P A ()
 P nA (|)

P nA (|),


$
 B


NAn
NA , NA  {0}
HynA (|)
()
MnnA (|)
A
B
BA,
V n (A)

54

Section 3
Section 3
Section 3
Sections 4.1&4.4
Section 4.2
Sections 4.3
Section 4.5
Equation (8)
Equation (9)
Section 5.3
Equation (10)
Equation (11)
Equation (13)
Equation (12)
Equation (12)
Equation (15)
Section 5.3

fiCoherent Predictive Inference under Exchangeability

V (A)
V + (A)
V ++ (A)
HA

HA c
HA

H A (|)
F

C
Crs
R
rB
iA
IrB,A
SA



subscript
subscript
subscript
subscript
subscript
subscript
subscript

A[]
++
V[]
(A)

V
NV
SC
IDM
SI
H
OI

diA (|)
DiA (|)
DiMnnA (|)
bnp

set of all polynomials on A
set of Bernstein positive polynomials on A
set of polynomials on A
that are positive on int(A )
representing set of polynomials
updated representing set of polynomials
lower prevision induced by HA

lower prevision induced by HA c
set of all category sets
inference system
set of all coherent inference systems
set of coherent inference systems that are
representation insensitive and specific
extended relabelling map
restriction map
injection map
extended injection map
sampling expectation
lower probability function
imprecision function
surprise function
related to vacuous inference system
related to nearly vacuous inference system
related to skeptically cautious inference system
related to IDMM inference systems
related to skeptical IDMM inference systems
related to Haldane inference system
related to original IDMM inference systems
categories in A already observed
set of polynomials on A
that are positive on int(A[] )
Dirichlet density
Dirichlet expectation operator
Dirichlet multinomial expectation operator
expansion of polynomial p
in Bernstein basis of degree n

Section 5.3
Section 5.3
Section 10
Theorem 5
Equation (19)
Equation (17)
Equation (20)
Definition 6
Definition 6
Equation (24)
Theorem 12
Equation (26)
Equation (27)
Equation (28)
Equation (29)
Section 8
Equation (33)
Equation (34)
Section 15
Section 9
Section 10
Section 11
Section 12
Section 13
Section 14
Appendix D
Equation (42)
Section 11
Appendix
Appendix
Appendix
Appendix

C
C
C
B

Appendix B. Multivariate Bernstein Basis Polynomials
With any n  0 and   NAn there corresponds
Bernstein basis polynomial
 a (multivariate)
m
x
:=
of degree n on A , given by BA, ()
() xA x ,   A . These polynomials have a
number of very interesting properties (see for instance Prautzsch, Boehm, & Paluszny, 2002,
Chapters 10 and 11), which we list here:
BB1. The set {BA, : 
  NAn } of all Bernstein basis polynomials of fixed degree n is linearly
independent: if N n  BA, = 0, then  = 0 for all  in NAn .
A

55

fiDe Cooman, De Bock, & Diniz

 NAn } of all Bernstein basis polynomials of fixed degree n forms a
BB2. The set {BA, : 
partition of unity: N n BA, = 1.
A

BB3. All Bernstein basis polynomials are non-negative, and strictly positive on the interior
int(A ) of A .
BB4. The set {BA, :   NAn } of all Bernstein basis polynomials of fixed degree n forms a
basis for the linear space of all polynomials whose degree is at most n.
Property BB4 follows from BB1 and BB2.42 It follows from BB4 that:
BB5. Any polynomial p has a unique expansion in terms of the Bernstein basis polynomials
also called Bernstein expansionof fixed degree n  deg(p),
or in other words, there is a unique count gamble bnp on NAn such that:

p() =
bnp ()BA, () for all   A .

(78)

n
NA

This tells us [also use BB2 and BB3] that each p() is a convex combination of the Bernstein
coefficients bnp (),   NAn whence:
min bnp  min p  p()  max p  max bnp for all   A .

(79)

The following proposition adds more detail to this picture.
Proposition 27. For any polynomial p on A :
lim [min bnp , max bnp ] = [min p, max p] = p(A ).

n+
ndeg(p)

Proof of Proposition 27. Since bnp converges uniformly to the polynomial p as n  +
(Trump & Prautzsch, 1996), in the sense that
 (  )



lim maxn p
 bnp () = 0,
n+ NA
n
ndeg(p)

we find that
lim

n+
ndeg(p)

min bnp  min p =

lim

[
]
minn bnp ()  min p

n+ NA
ndeg(p)

[
(  )]
minn bnp ()  p
n+ NA
n
ndeg(p)
 (  )



  lim maxn p
 bnp () = 0,
n+ NA
n



lim

ndeg(p)

and therefore limn+,ndeg(p) min bnp  min p. Furthermore, by Statement (79), we see that
limn+,ndeg(p) min bnp  min p. Hence indeed limn+,ndeg(p) min bnp = min p. The proof
for the other equality is completely analogous.
42. To see how: clearly all polynomials are by definition linear combinations of Bernstein basis polynomials,
of possibly different degrees. For each of the terms, use BB2 to raise the degree to a common higher
degree nmultiply it by an appropriate version of 1. This shows that the Bernstein basis polynomials of
fixed degree n are generating for all polynomials of lower degrees. They are also independent by BB1.

56

fiCoherent Predictive Inference under Exchangeability

Using the above results, we can prove a number of useful relations between the Bernstein
positivity of a polynomial and its positivity on (the interior of) the simplex. They are related
to a property first proved by Hausdorff in the univariate case (Hausdorff, 1923, p. 124).
Proposition 28. Let p be any polynomial on A . Consider the following statements:
(i) (  A )p() > 0;
(ii) p  V + (A), meaning that there is some n  deg(p) such that bnp > 0;
(iii) p  V ++ (A), meaning that (  int(A ))p() > 0;
(iv) (  A )p()  0.
Then (i)(ii)(iii)(iv).
Proof of Proposition 28. The first implication is a direct consequence of Proposition 27: we
infer from (i) and the continuity of p that min p > 0 and therefore, by Proposition 27, that
limn+,ndeg(p) min bnp = min p > 0, which implies (ii).
To prove that (ii)(iii), assume that there is some n  deg(p) such that bnp > 0, and
consider any   int(A ). Then since BA, () > 0 for all   NAn [BB3], and since by
assumption bnp  0 and bnp () > 0 for some   NAn , we see that
p() =



bnp ()BA, ()  bnp ()BA, () > 0.

n
NA

The third implication is an immediate consequence of the continuity of p.
The following counterexample shows that not necessarily V + (A) = V ++ (A).
Running Example. We go back to the polynomial q on {H ,T ,U } defined in Equation (41):
2
2
q() = H
 H T + T
= (H  T )2 + H T for all   {H ,T ,U } .

We have already argued that this polynomial is not Bernstein positive. Nevertheless, it is
obviously positive on the interior of {H ,T ,U } .

It is also quite easy to trace the effect on the Bernstein expansion of multiplying with a
Bernstein basis polynomial:
Proposition 29. For all polynomials p on A , all natural n  deg(p), all   NA  {0}
and all   NAn+mA :

()
bn (  )
if   
p
n+mA
(

)()
bpBA, () =

0
otherwise.
Proof of Proposition 29. Observe that:
( 
)

n
pBA, =
bp ()BA, BA, =
bnp ()BA, BA,
n
NA

n
NA

57

fiDe Cooman, De Bock, & Diniz

=



bnp ()

n
NA

( + )
BA,+ ,
()()

and use the uniqueness of the (Bernstein) basis expansion.
This allows us to prove the following simple but interesting result about Bernstein positivity:
Proposition 30. Consider any   NA  {0} and any polynomial p on A . Then:
pBA,  V + (A)  p  V + (A).
Proof of Proposition 30. First, assume that pBA,  V + (A), so there is some natural n 
n
A
deg(p) such that bn+m
pBA, > 0. Then it follows from Proposition 29 that also bp > 0, and
therefore p  V + (A).
Assume, conversely, that p  V + (A), so there is some n  deg(p) such that bnp > 0. Then
+
A
it follows from Proposition 29 that also bn+m
pBA, > 0, and therefore pBA,  V (A).

Appendix C. The Dirichlet Distribution
The density diA (|) of the Dirichlet distribution with hyperparameter   RA
>0 is given by:
diA (|) := 


(A )
xx 1 for all   int(A ),
(
)
x
xA
xA

and for any polynomial p on A we define the corresponding expectation as:43


(A )
p() 
DiA (p|) :=
xx 1 d.
(
)
x
A
xA
xA

In particular,


(

) 


(A )
xx 1 d
(x )
A
xA
xA
xA
( )
( ) 

n
(A )
(mx + x )
1
n
=
x (mx ) ,
=
 (n + A )
(x )
A (n) 

DiA (BA, |) =

n


xmx 

(80)

xA

xA

using the ascending factorial (r) := (+r)
() = ( + 1) . . . ( + r  1), with   R and
r  N0 .
The Dirichlet distribution can be used as a prior in combination with a multinomial
likelihood, leading to the so-called Dirichlet multinomial distribution, which can be described
as follows. The probability of observing (a sample with n  0 observations with) count vector
  NA  {0} in a multinomial process with Dirichlet prior density diA (|) is given by:

n
DiMnA ({}|) :=
CoMnnA ({}|) diA (|) d
A

43. The integrals in this section can be interpreted as multiple Riemann integrals.

58

fiCoherent Predictive Inference under Exchangeability


BA, () diA (|) d = DiA (BA, |),

=
A

where the second equality follows from Equation (16). Therefore, more generally, if we take
the expansion of the polynomial p in Bernstein basis polynomials of degree n  deg(p):
DiA (p|) =



bnp () DiA (BA, |) =

n
NA

= DiMnnA



bnp () DiMnnA (I{} |)

n
NA

( 
n
NA

 )

n
bp ()I{}  = DiMnnA (bnp |),

which is the Dirichlet multinomial expectation of the count gamble bnp . This is the general
and useful relationship between the Dirichlet expectation of a polynomial p, and the Dirichlet
multinomial expectation of its Bernstein expansion bnp . Although these expectations are
strictly speaking only defined for   RA
>0 , we can extend their definition continuously to
elements  of RA
\
{0}
by
taking
appropriate
limits, as Equation (80) indicates.

C.1 Special Properties of the Dirichlet Distribution
We now recall a few interesting properties of the Dirichlet distribution. We begin with the
updating property:
Proposition 31 (Updating). For any category set A, any polynomial p  V (A), any count
vector   NA  {0} and any   RA
>0 :
DiA (pBA, |) = DiA (BA, |) DiA (p| + ).
Proof of Proposition 31.

DiA (pBA, |) =
p()BA, () diA (|) d
A
(
)


mA  mx (A )
xx 1 d
=
p()
x 
(
)

x
A
xA
xA
xA
(
)


(mx + x )
mA
(A )
=
p() diA (| + ) d
 (mA + A )
(x )
A
xA

= DiA (BA, |) DiA (p| + ),
where the last equality follows from Equation (80).
Next, we turn to the so-called renaming property:
Proposition 32 (Renaming). For any category sets A and C such that there is some bijective
(one-to-one and onto) map  : A  C, any polynomial p  V (C) and any   RA
>0 :
DiA (p  R |) = DiC (p|R ()).
59

fiDe Cooman, De Bock, & Diniz

Proof of Proposition 32. Due to the linear nature of the Dirichlet expectation, it clearly
suffices to prove the property for the Bernstein basis polynomials p = BC, , where  
NC  {0}. Observe that R is a bijection too. Then, using Equation (80), if we let  := R ()
and  := R1 (), so z = 1 (z) and mz = n1 (z) for all z  C, and A = C and
nA = mC , we get:
(
)
 (mz + z )
mC
(C )
DiC (BC, |R ()) = DiC (BC, |) =
 (mC + C )
(z )
zC
( )
 (n1 (z) + 1 (z) )
(A )
nA
=
 (nA + A )
(1 (z) )
zC
( )
 (nx + x )
(A )
nA
=
= DiA (BA, |),
 (nA + A )
(x )
xA

and if we take into account that for all   int(A ):
(BC,  R )() = BC, (R ())
(
)
(
)
(
)
mC  mz
mC 
mC  n1 (z)
mz
=
1 (z)
1 (z) =
R ()z =



zC
zC
zC
( ) 
nA
xnx = BA, (),
=

xA

we see that indeed DiC (BC, |R ()) = DiA (BC,  R |).
The so-called pooling property generalises the renaming property:
Proposition 33 (Pooling). For any category sets A and D such that there is some onto
map  : A  D, any polynomial p  V (D) and any   RA
>0 :
DiA (p  R |) = DiD (p|R ()).
Proof of Proposition 33. Due to the linear nature of the Dirichlet expectation, it again suffices
to prove the property for the Bernstein basis polynomials p = BD, , where   ND  {0}.
Also, if we take into account the renaming property of Proposition 32, it is enough to consider
the following special case, where we have some non-empty set Do and different categories b,
c and d not belonging to it, let A := Do  {b, c} and D := Do  {d}, and define  by letting
(x) := x if x  Do and (b) = (c) := d.
Then on the one hand, taking into account Equation (80), letting  := R () :
(
)
 (mz + z )
mD
(D )
DiD (BD, |R ()) = DiD (BD, |) =
 (mD + D )
(z )
zD
(
)
mD
(D )
(md + d )  (mz + z )
=
.
(81)
 (mD + D ) (d )
(z )
zDo

On the other hand,

60

fiCoherent Predictive Inference under Exchangeability

DiA (BD,  R |)
( 
)
)
 (
mD
md
mz
(b + c )
z
diA (|) d
=

A
zDo
)
(


(A )
mD

=
(b + c )md bb 1 cc 1
zmz +z 1 d

xA (x ) A
zDo
(
)
)
(

m
d
 md

(A )
mD

bk+b 1 cmd k+c 1
zmz +z 1 d
=
k
(
)

x
A
xA
zDo
k=0

)
(
)
(
m
d

(A )
md (k + b )(md  k + c ) zDo (mz + z )
mD

=
.

k
(mD + A )
xA (x )
k=0

So, if we compare both results and recall that D = A , z = z for z  Do and d = b + c ,
we see that we must prove that:
)
md (

1
md
(md + b + c )
=
(k + b )(md  k + c )
(b + c )
(b )(c )
k
k=0

or equivalently, using ascending factorials:
(b + c )(md ) =

)
md (

md
b (k) c (md k) .
k

(82)

k=0

So we see that proving the pooling property is essentially equivalent to proving Equation (82),
which is the binomial theorem for ascending factorials. This is a well-known result, and it
follows from the fact that ascending factorials are Sheffer sequences of binomial type (Sheffer,
1939). For completeness, we give a proof for it here, which is now very easy, because we have
just shown that it will hold if we can prove the pooling property in the particular case that
Do = {a}, where a is a category different from b, c and d. So A = {a, b, c} and D = {a, d},
and in this case we can rewrite Equation (81) as:
DiD (BD, |R ())
(
)
ma + md
(a + b + c )
(md + b + c ) (ma + a )
=
ma
(ma + md + a + b + c ) (b + c )
(a )
whereas
DiA (BD,  R |) =
where we let
 1 ( 
I :=
0
 1

(1 

0

(1 

=

1a

a )md bb 1 (1

a )md ama +a 1

( 

(
)
ma + md (a + b + c )
I
ma
(a )(b )(c )

 a 

1a

b )c 1 ama +a 1 db

bb 1 (1

 a  b )

c 1

)
da

)

db da
(  1
)
 1
md +b +c 1 ma +a 1
b 1
c 1
=
(1  a )
a
t
(1  t)
dt da
0

0

0

0

61

fiDe Cooman, De Bock, & Diniz

= B(ma + a , md + b + c )B(b , c ) =

(ma + a )(md + b + c ) (b )(c )
,
(ma + md + a + b + c ) (b + c )

using the well-known evaluation of the Beta function in terms of Gamma functions.
Finally, we look at properties related to restriction.
Proposition 34 (Restriction). For any category sets A and B such that B  A, any
polynomial p  V (B), any   RA
>0 and any r  N0 :
DiA (IrB,A (p)|) =

(deg(p) + r + B )
(A )
DiB (p|rB ()).
(deg(p) + r + A )
(B )

Proof of Proposition 34. Let n := deg(p) + r, then due to the linearity of the Dirichlet
expectation operator, and Equations (29) and (80):

DiA (IrB,A (p)|) =
bnp () DiA (BA,iA () |)
n
NB



( )
n
(A )
xB (nx + x )
xA\B (x )


=
 (n + A )
n
xA\B (x )
xB (x )
NB
( )

n
(A )  (nx + x )
=
bnp ()
 (n + A )
(x )
n


bnp ()

NB

=


n
NB

=

xB

bnp ()

(A ) (n + B )
DiB (BB, |rB ())
(n + A ) (B )

(A ) (n + B )
DiB (p|rB ()),
(n + A ) (B )

concluding the proof.

Appendix D. The Original IDMM Inference System by Walley and
Bernard
The IDMM inference system sIDM , as we introduced it in Section 12, differs from the one
originally proposed by Walley and Bernard (1999).44 In this appendix, we discuss the original
IDMM inference system, which we denote by sOI , explain how it is related to ours, and
illustrate some of the advantages our version has over the one by Walley and Bernard.
D.1 Defining the Original IDMM Inference System
For any s  R>0 , and any category set A, consider the following set of polynomials:
s
:= {p  V (A) : (  KsA ) DiA (p|) > 0}
HOI,A

= {p  V (A) : (  int(A )) DiA (p|s) > 0} .
44. Strictly speaking, Walley and Bernard did not propose an inference system in our sense, but rather a
collection of prior and posterior predictive lower previsions for each category set A. The inference system
we call the original IDMM inference system is one that produces these predictive lower previsions.

62

fiCoherent Predictive Inference under Exchangeability

For reasons that should become clear shortly, we call the inference system sOI defined by
s
sOI (A) := HOI,A
for all category sets A,

the original IDMM inference system with hyperparameter s > 0. Updating is done in much
  NA  {0}:
the same way as for the inference system sIDM in Section 12. For any 
s
 = {p  V (A) : (  int(A )) DiA (p|
 + s) > 0} ,
HOI,A
c

and this should be compared with Equation (54). We leave it as an exercise to the reader to
check that sOI is coherent and representation insensitive.45 However, as illustrated by the
counterexample in Section D.3, sOI is not specific.
The predictive models of sOI are easily derived by mimicking the approach used in
Section 12 to derive the predictive models of sIDM ; see Equations (56) and (57). For any
  NAn :
n  N0 , n  N and any 
{
}
s,n
 = f  L(An ) : (  int(A )) DiA (MnnA (f )|
 + s) > 0 ,
DOI,A
c

(83)

 =
P s,n
OI,A (f |)

(84)

and
inf
int(A )

 + s) for all gambles f on An .
DiA (MnnA (f )|

The latter expression motivates why we refer to sOI as the original IDMM inference system:
its predictive lower previsions coincide with those proposed by Walley and Bernard (1999).
Using Equation (83) for n = 1, and mimicking the argument in the proof of Equation (59) in
Appendix E.7, we see that
s,1
 =
DOI,A
c

{
f  L(A) : f > 

}
1 
s,1
 for all 
  NA  {0}.
f (x)mx = DIDM,A
c
s
xA

This tells us that the IDMM and the original IDMM have the same immediate prediction
models. The corresponding immediate predictive lower previsions for the original IDMM are
well-known and are of course identical to the ones produced by our version of the IDMM
inference system, as given by Equation (60). However, as the examples in the next section
illustrate, this equality does not extend beyond immediate prediction: the IDMM and the
original IDMM are different coherent inference systems, which leads us to the general and
important conclusion that coherent inference systems are not completely determined by their
immediate prediction models.
Nevertheless, both approaches are closely related; by comparing Equations (84) and (57),
  NAn
we see that for any n  N0 , n  N and any 
0

,n
 = inf0 P sOI,A
 for all gambles f on An .
P s,n
(f |)
IDM,A (f |)
0<s <s

45. The proof is very similar to the one for sIDM [see Theorem 21].

63

(85)

fiDe Cooman, De Bock, & Diniz

D.2 The Original IDMM Inference System Is Not Monotone in s
The hyperparameter s of the original IDMM inference system is usually interpreted as
a degree of caution. Higher values of s are often claimed to produce inferences that are
more cautious and less informative. The following quote from Walley and Bernard (1999,
Section 2.4) makes this explicit:
If B is any event concerning future observations, the IDMM(s) produces intervals
of posterior probabilities [P (B|), P (B|)] which are nested and become wider as
s increases. This means that the inferences produced by two IDMMs with different
values of s are always consistent with each other, and the effect of increasing s is
simply to make inferences more cautious and less informative.
Similar statements can be found in related papers by Walley (1996, Section 2.5) and Bernard
(2005, Section 4.6). Although this is indeed true for many inferences, including many important
onesfor example, the immediate predictions, it does not hold for any event concerning
future observations, as illustrated by the following example, where the lower probability of
an event concerning two future observations is shown to initially increase with s.
Example 1. Consider a situation where the possibility space A consists of two elements
only, say heads (H) and tails (T ), each of which has been observed once, so n = 2 and
 = (mH , mT ) = (1, 1). We are interested in the predictive lower probability that during the

next two trials, heads and tails will each be observed once: so n = 2 and we are looking for the
 = {(H, T ), (T, H)}, with 
 = (mH , mT ) = (1, 1).
predictive lower probability of the event []
For the original IDMM inference system, the following formula provides a closed-form
expression:
 =
P s,n
 |)
OI,A (I[]

 + s) = inf DiA (BA,
 + s)
DiA (MnnA (I[]
 |
 )|
int(A )
( ) 
1
n
(mx + stx )(mx )
= inf
(n)


int(A ) (n + s)
xA
inf

int(A )

= inf

0<t<1

2(1 + st)(1 + s(1  t))
2(1 + s)
=
.
(2 + s)(3 + s)
(2 + s)(3 + s)

 initially increases with s; see also Figure 2.
We conclude that P s,n
 |)
OI,A (I[]

(86)


For our version of the IDMM inference system, the statement made in the aforementioned
quote does hold for any event concerning future observations. This follows trivially from
Equation (85). We illustrate this in our next example.
Example 2. Consider again the problem in Example 1. This time, we solve it using our version
of the IDMM. The result is also depicted in Figure 2, as a function of the hyperparameter s.
s,n
 P IDM,A
 is a non-increasing function of s. Indeed,
In contrast with P s,n
(I[]
 |),
 |)
OI,A (I[]

0 ,n
1

 =
(I[]
if 0 < s < 1
P sOI,A

 |)
slim
0 0
3
s,n
 =
P IDM,A (I[]
 |)

2(1 + s)

P s,n
 =
if s  1
 |)
OI,A (I[]
(2 + s)(3 + s)
is the closed-form expression we find by combining Equations (85) and (86).
64



fiCoherent Predictive Inference under Exchangeability

0.36


P s,n
 |)
OI,A (I[]

0.34
0.32


P s,n
 |)
IDM,A (I[]

0.3
0.28
s
0

0.5

1

1.5

2

Figure 2: Lower probability of observing two different outcomes during the next two experiments, given that the possibility space consists of two categories, each of which
has already been observed once: solutions according to sOI (solid line) and sIDM
(dashed and solid line); see Examples 1 and 2 for more information.

Clearly, the inferences for sOI and sIDM can differ: it suffices to compare the results in
Examples 1 and 2; see Figure 2 as well. Therefore, it seems clear that Walleys (1996, p. 51)
statement that [. . . ] s can be allowed to vary between 0 and s, and this produces exactly
the same inferences as the IDM with s = s. or equivalently, that sOI and sIDM produce the
same inferences, should be taken to apply to immediate prediction only.
D.3 The Original IDMM Inference System Is Not Specific
As announced in Theorem 21, our version of the IDMM inference system is specific. We now
show that, at least for some values of the hyperparameter s, this is not true for the original
version.
  NBn . Then for all A  B and all f  L(B n ):
Consider any n  N0 , n  N and any 
s,n
  (  int(A )) DiA (MnnA (f IB n )|iA ()
 + s) > 0
f IB n  DOI,A
ciA ()

 + srB ()) > 0,
 (  int(A )) DiB (MnnB (f )|
where the last equivalence is a consequence of Propositions 41 and 34 and the fact that
 = .
 If in particular B  A, it is not hard to see that sB = {srB () :   int(A )},
rB (iA ())
which implies that:
s,n
s,n
  (  sB ) DiB (MnnB (f )|
 + ) > 0  f  DIDM,B

f IB n  DOI,A
ciA ()
c,

and therefore also:

 B n ) = inf s DiB (MnnB (f )|
 + ) = P s,n
P s,n
OI,A (f |iA (),
IDM,B (f |).
B

65

fiDe Cooman, De Bock, & Diniz

On the other hand, due to Equation (SP1), if sOI were specific, we would have that:
 B n ) = P s,n


= P s,n
P s,n
OI,A (f |iA (),
OI,B (f |rB (iA ()))
OI,B (f |).
 and P s,n
 to
Hence, in order for sOI to be specific, it is necessary for P s,n
OI,B (|)
IDM,B (|)
coincide. As illustrated by the examples in the previous section, this is not necessarily the
case. Therefore, sOI is not always specific. In the counterexample we have provided, the
difference occurs for s < 1 only, whereas in practice, s is usually chosen to be either 1 or
2 (Walley & Bernard, 1999, Section 2.4). It would be interesting to see whether similar
counterexamples can be constructed for s  1.
That the original IDMM inference systems are not specific, apparently contradicts Theorem 11 by De Cooman et al. (2009a), which seems to state that they are. But in fact, what
that theorem states is that the original IDMM immediate prediction models satisfy a weaker
specificity condition, tailored to immediate prediction only. Since the immediate prediction
models for the original IDMM and the IDMM coincide, there is no contradiction.

Appendix E. Proofs and Additional Results That Are More Technical
E.1 Proofs of Results in Section 4
Proof of Theorem 4. For the sake of notational simplicity, we use the intuitive notation f (Xk )
for extnk (f ). We give the proof for the most general definition, in terms of sets of desirable
gambles. The proof for lower previsions then follows immediately.
Consider any category set A, any n  N, any 1  k  n and any gamble f on A such
n we may assume without loss of generality that A is not a singleton. This
that f (Xk )  DA
already implies that f 6 0, by coherence [D4]. Hence in particular f 6= 0 and max f > 0.
Assume ex absurdo that f 6> 0, then there must be some a  A for which f (a) < 0. Define
the gamble g on A by letting g(a) := f (a) and g(x) := max f > 0 for all x  A \ {a}. Then
g  f and therefore g(Xk )  f (Xk ), which implies, by coherence [use D2 and D3], that also
n . If we now let  := max f  f (a) > 0 and  := f (a)/ > 0, and define the
g(Xk )  DA
n , because  > 0.
gamble h := g/ =  + IA\{a} , then also, by coherence [D3], h(Xk )  DA
Now consider any natural number N  2, then it follows from repeatedly applying pooling
n
and renaming invariance in an appropriate manner that  + I{a1 } (Zk )  D{a
, where
1 ,...,aN }
Zk is any variable that assumes the value a1 when Xk 6= a and that assumes some value in
{a2 , . . . , aN } when Xk = a. By repeatedly applying category permutation invariance, we find
n
that  + I{a` } (Zk )  D{a
for all `  {1, . . . , N }. Coherence [D3] then tells us that
1 ,...,aN }
N
n
N  + 1 = `=1 [ + I{a` } (Zk )]  D{a
. This leads to a contradiction with coherence
1 ,...,aN }
[D4] if we choose N large enough.
E.2 Proofs of Results in Section 7
Proposition 35. For all n  N and   An :  () = R ( ()).
Proof of Proposition 35. Consider any z  D, then
Tz () = |{k  {1, . . . , n} : (xk ) = z}| =


yA : (y)=z

66

|{k  {1, . . . , n} : xk = y}|

fiCoherent Predictive Inference under Exchangeability



=

Ty () = R ( ())z ,

yA : (y)=z

concluding the proof.
Lemma 36. For all n  N, all   NAn and all   Dn :

1
1
I{} () =
I
().
()
(R ()) [R ()]
[]


Proof of Lemma 36. Consider the map M : Dn  R defined by M := [] I{} . Then
for any permutation  of the index set {1, . . . , n} and any   Dn , we see that


M () =
I{} () =
I{(1 )} ()
[]

[]



=



I{} () =

[]

I{} () = M (),

[]

which tells us that M is permutation invariant and thereforeconstant on the atoms
[],   NDn . This means that, with obvious notations, M = N n M ()I[] . Now
D
M () > 0 implies that there is some   [] such that  = , and therefore, by
Proposition 35,  () =  () = R ( ()) = R () and therefore   [R ()]. This tells
us that M () = 0 unless  = R () and therefore M = M (R ())I[R ()] . Now if
we plug f := 1 into Equation (87), we see that


() =
M () =
M (R ())I[R ()] () = M (R ())(R ()).
Dn

Dn

Lemma 37. For all n  N and all   NDn :


BD,  R =

BA,

n : R ()=
NA


Proof of Lemma 37. For any  in A , we have that
( )  ( 
)nz
n
(BD,  R )() =
x

zD x1 ({z})
( ) 
( )

n
nz
=
z


nz
z
zD  N

( )
n
=

=

1 ({z})

( 


n : R ()=
NA



n:
NA

R ()=

(

n


67

xA

z

xmx

x1 ({z})

xmx

)  (

xA

) 

concluding the proof.



xmx =

zD

)

nz

|1 ({z})


n:
NA

R ()=

BA, (),

fiDe Cooman, De Bock, & Diniz

This lemma allows us to prove two related propositions.
Proposition 38. For all n  N and all gambles f on Dn : MnnA (f  ) = MnnD (f )  R .
Proof of Proposition 38. First of all, we have for any count vector  in NAn that
HynA (f  |) =


 
1
1
f () =
I{} ()f ()
()
()
n
[] D

[]



=

f ()

Dn

I{} ()

(87)

[]

1
I
()
f ()
(R ()) [R ()]
n



=

D

=

1
()



1
(R ())



f () = HynD (f |R ()),

[R ()]

where the fourth equality follows from Lemma 36. Therefore indeed:


MnnA (f  ) =
HynA (f  |)BA, =
HynD (f |R ())BA,
n
NA

=



n
NA

HynD (f |)

n
ND

=





BA,

n : R ()=
NA


HynD (f |)(BD,  R ) = MnnD (f )  R ,

n
ND

where the fourth equality now follows from Lemma 37.
Proposition 39. For all polynomials p on D and all n  N0 such that n  deg(p):
bnpR = bnp  R .
Proof of Proposition 39. We find after expanding p in the appropriate Bernstein basis:
( 
)

n
p  R =
bp ()BD,  R =
bnp ()(BD,  R )
n
ND

=



bnp ()

n
ND

=



n
ND



BA, =





bnp (R ())BA,

n
n : R ()=
ND
NA


n : R ()=
NA


(bnp  R )()BA, ,

n
NA

where the third equality follows from Lemma 37. The desired result now follows from the
uniqueness of an expansion in a (Bernstein) basis.
Proof of Theorem 7. Fix any category sets A and D such that there is an onto map  : A  D,
  An and any gamble f on Dn . We use the notation HA := (A) and
any n, n  N, any 
68

fiCoherent Predictive Inference under Exchangeability

HD := (D), and we transform Condition (RI2) using the equivalence in Condition (18). On
 :=  ():

the one hand, letting 
n
f    DA
 MnnA (f  )  HA  MnnD (f )  R  HA
n
n
n
  BA,
f    DA
c
 MnA (f  )  HA  BA,
 (MnD (f )  R )  HA ,

where the second equivalences follow from Proposition 38. On the other hand, recalling that
 = R ( ())
 = R ()
 by Proposition 35:
 ()
n
f  DD
 MnnD (f )  HD
n
n
  BD,R ()
f  DD
c
 MnD (f )  HD .

This tells us that the equivalences in Condition (RI2) can be rewritten as:
MnnD (f )  R  HA  MnnD (f )  HD
n
n
BA,
 (MnD (f )  R )  HA  BD,R ()
 MnD (f )  HD .

The proof is complete if we observe (and recall from the discussion in Section 5.3 and
Appendix B) that by varying n  N and f  L(Dn ), we can let p := MnnD (f ) range over all
  An , we can let 
 :=  ()
 range
polynomials on D , and that by varying n  N and 
over all count vectors in NA .
Proof of Theorem 8. Let, for ease of notation  := inf iI i , then  is coherent using Equation (25). Consider any category sets A and D such that there is an onto map  : A  D,
any p  V (D) and any   NA  {0}. Then, using the representation insensitivity of the
coherent i and Theorem 7:
(p  R )BA,  (A)  (i  I)(p  R )BA,  i (A)
 (i  I)pBD,R ()  i (D)  pBD,R ()  (D),
and this concludes the proof.
Proposition 40. For all   An ,  (B ) = rB ( ()).
Proof of Proposition 40. Immediate, since B is a sample whose components all belong to
B, and for each category in B, the number of times it occurs in B is exactly the same as
the number of times it occurs in .
Proof of Proposition 9. Consider any   B , and let, for simplicity of notation  = iA ().
Then since for any   NBn , with n := deg(p) + r
(
) 
( ) 
n
n
iA ()x
BA,iA () () =
x
=
nx x = BB, (),
iA ()

xA

xB

we see that indeed:
IrB,A (p|) =



bnp ()BA,iA () () =

n
NB


n
NB

69

bnp ()BB, () = p().

fiDe Cooman, De Bock, & Diniz

Proof of Proposition 10. When deg(p) + r = 0, then r = 0 and p = c  R, and trivially
IrB,A (p|) = I0B,A (c)() = c. So let us assume that deg(p) + r > 0. First of all, observe that
deg(p)+r

for all   NB

and all   A :
(
)
(
)
deg(p) + r  iA ()x
deg(p) + r  nx
BA,iA () () =
x
=
x
iA ()

xA
xB
{
deg(p)+r
B
BB, (|+
if B > 0
B)
=
0
otherwise.

(88)

It therefore already follows from Condition (29) that IrB,A (p|) = 0 if B = 0. Let us therefore
assume that B > 0. Then Condition (29) and Equation (88) tell us that:

IrB,A (p|) =
bdeg(p)+r
()BA,iA () ()
p
deg(p)+r

NB

deg(p)+r



=

bdeg(p)+r
()B
p

BB, (|+
B)

deg(p)+r

NB

deg(p)+r

deg(p)+r



= B

bdeg(p)+r
()BB, (|+
p
B ) = B

p(|+
B ),

deg(p)+r

NB

which concludes the proof.
Proposition 41. For all n  N and all gambles f on B n :
MnnA (f IB n ) = IrB,A (MnnB (f )), where r := n  deg(MnnB (f )).
Proof of Proposition 41. First of all, we have for any count vector  in NAn thatwith some
slight abuse of notation:
HynA (f IB n |) =


1
1
(f IB n )() =
()
()
[]



f ()

[]B n

is zero unless  = iA () for some   NBn . In that case, since then obviously () = (),
and   [iA ()]  B n    []again with some slight abuse of notation:
HynA (f IB n |iA ()) =

1 
f () = HynB (f |).
()
[]

Therefore, if we recall Condition (29):


HynB (f |)BA,iA ()
MnnA (f IB n ) =
HynA (f IB n |iA ())BA,iA () =
n
NB

n
NB

= IrB,A (MnnB (f )),
where r := n  deg(MnnB (f )).
70

fiCoherent Predictive Inference under Exchangeability

Proof of Theorem 11. Fix any category sets A and B such that B  A, any n, n  N, any
  An and any gamble f on B n . We use the notation HA := (A) and HB := (B), and

we transform Condition (SP2) using the equivalence in Condition (18). On the one hand,
 :=  ()
 and r := n  deg(MnnB (f )):
letting 
n
f IB n  DA
 MnnA (f IB n )  HA  IrB,A (MnnB (f ))  HA
n
n
r
n
  BA,
f IB n  DA
c
 MnA (f IB n )  HA  BA,
 IB,A (MnB (f ))  HA ,

where the second equivalences follow from Proposition 41. On the other hand, recalling that
 B ) = rB ( ())
 = rB ()
 by Proposition 40:
 (
n
f  DB
 MnnB (f )  HB
n
n
 B  BB,rB ()
f  DB
c
 MnB (f )  HB .

This tells us that the equivalences in Condition (SP2) can be rewritten as:
IrB,A (MnnB (f ))  HA  MnnB (f )  HB
r
n
n
BA,
 IB,A (MnB (f ))  HA  BB,rB ()
 MnB (f )  HB .

The proof is complete if we recall from the discussion in Section 5.3 and Appendix B that
by varying n  N and f  L(B n ), we can let p := MnnB (f ) = CoMnnB (HynB (f )) range over
all polynomials on B and r = n  deg(MnnB (f )) range over all elements of N0 , and that by
  An , we can let 
 :=  ()
 range over all count vectors in NA .
varying n  N and 
Proof of Theorem 12. Let, for ease of notation  := inf iI i , then  is coherent using
Equation (25). Consider any category sets A and B such that B  A, any p  V (B), any
  NA  {0} and any r  N0 . Then, using the specificity of the i :
IrB,A (p)BA,  (A)  (i  I)IrB,A (p)BA,  i (A)
 (i  I)pBB,rB ()  i (B)  pBB,rB ()  (B),
which concludes the proof.
E.3 Proofs of Results in Section 8
Proof of Proposition 13. For sufficiency, fix a category set A, a gamble g on A, and a count
vector   NA  {0}. Condition (RI4) with D := g(A),  := g, f := idD yields Condition (RI5).
For necessity, fix category sets A and D such that there is an onto map  : A  D, a
gamble f on D, and a count vector   NA  {0}. Observe that (f  )(A) = f (D) and that
for all r  f (D)



Rf  ()r =
mx =
mx
xA : (f )(x)=r

zD : f (z)=r xA : (x)=z



=

zD : f (z)=r

71

R ()z = Rf (R ())r ,

fiDe Cooman, De Bock, & Diniz

so Rf  = Rf  R . We now infer by invoking Condition (RI5) twice that:
1
1
f    DA
c  id(f )(A)  D(f
)(A) cRf  ()
1
 idf (D)  Df1 (D) cRf (R ())  f  DD
cR (),

concluding the proof.
Proof of Theorem 14. The arguments in this proof rely heavily on the following expression
for the lower probability function:
{
}
1
(n, k) = sup   R : I{a}    D{a,b}
c(k, n  k)
}
{
= sup   R : ak bnk [a  ]  ({a, b})
(89)
and other related expressions that are equivalent to it by representation insensitivity and
Bernstein coherence [B3]. Both expressions follow from Equations (32) and (33), Bernstein
coherence [B3] and representation insensitivity in its form (RI4).
L1. Immediate from Bernstein coherence and the fact that (n, k) is a lower probability:
use Equation (89), B2 and B4.
L2. Fix any non-negative integers n, k and ` such that k + `  n. Consider any real
 < (n, k) and  < (n, `), then it follows from applying Equation (89) and Condition (RI4)
that both xk y` znk` [x  ]  ({x, y, z}) and xk y` znk` [y  ]  ({x, y, z}), whence,
by Bernstein coherence [B3], xk y` znk` [(x + y )  ( + )]  ({x, y, z}). Applying
Equation (89) and Condition (RI4) again tells us that uk+` znk` [u  ( + )]  ({u, z}),
whence  +   (n, k + `).
L3, L4 and L5 are immediate consequences of L1 and L2.
L6. Consider the category set A := {a, b} and the count vector  with ma := k and
mb := n  k. Define the gamble g on A by g(a) := (n + 1, k + 1) and g(b) := (n + 1, k).
Then g(a)  g(b) by L5, and therefore the coherence [P5 and P3] of the predictive lower
prevision P 1A (|) tells us that P 1A (g|) = g(b) + [g(a)  g(b)]P 1A ({a}|) = (n + 1, k) +
(n, k)[(n + 1, k + 1)  (n + 1, k)] [see also Equation (33)]. So it clearly suffices to prove
that P 1A (g|)  (n, k) = P 1A ({a}|). Consider any  < P 1A (g|), then it follows using
Equation (31) that:
ak bnk [g(a)a + g(b)b  ]  (A).
(90)
Also, for any  > 0, both ak+1 bnk [a  g(a) + ]  (A) and ak bn+1k [a  g(b) + ]  (A),
and therefore, by coherence [B3], and recalling that a + b = 1,
(A) 3 ak+1 bnk [a  g(a) + ] + ak bn+1k [a  g(b) + ]
= ak bnk [a  g(a)a  g(b)b + ]. (91)
Combining Statements (90) and (91) using coherence [B3], this leads to ak bnk [a   + ] 
(A), whence (n, k)    , and this completes the proof.
L7. Use L1 and L5 to find that (n, k)[(n + 1, k + 1)  (n + 1, k)]  0, and then use L6.
L8. That sn  0 follows from L4, so we only need to prove that sn+1  sn , or equivalently,
that (n, 1)  (n + 1, 1)[1 + (n, 1)]. Indeed:
(n, 1)  (n + 1, 1) + (n, 1)[(n + 1, 2)  (n + 1, 1)]
72

fiCoherent Predictive Inference under Exchangeability

 (n + 1, 1) + (n, 1)[2(n + 1, 1)  (n + 1, 1)]
= (n + 1, 1) + (n, 1)(n + 1, 1),
where the first inequality follows from L6 with k = 1, and the second from L4 and L1.
L9. The inequalities hold trivially for n = 0, due to L1. So consider any n  N, and
category sets A := {x, y} and B := {x1 , x2 , . . . , xn , y}. Let 0 <  <  and  :=    > 0.
Since (1, 1) > , we see that x [x  ]  (A), or equivalently, x [x (1  )  y ]  (A),
since x + y = 1. Representation insensitivity [use Equation (89) and Condition (RI4)] then
tells us that xk [xk (1  
)  y ]  ({xk , y}), and specificity [use Theorem 11] allows us
to infer from this that ( nk=1 xk
)[xk (1  )
n y ]  (B), for all k  {1, . . . , n}. Now
n
infer from coherence [B3] that ( k=1 xk )[ k=1 xk (1  )  ny ]  (B), and apply
representation insensitivity to get to xn [x (1  )  ny ]  (A). Since y = 1  x , this
n
is equivalent to xn [x (1   + n)  n]  (A). This shows that (n, n)  1+n
, using
Equation (89). The rest of the proof is now immediate.
E.4 Proofs of Results in Section 9
Proof of Theorem 15. That V is coherent is obvious, because for each category set A  F,
V (A) = V + (A) is a Bernstein coherent set of polynomials on A .
To prove representation insensitivity, we use Theorem 7. Consider any category sets A
and D such that there is an onto map  : A  D, any p  V (D) and any   NA  {0}.
Then indeed
(p  R )BA,  V + (A)  p  R  V + (A)  p  V + (D)  pBD,R ()  V + (D),
where the first and last equivalences follow from Proposition 30, and the second one from
Lemma 48 with K = A.
E.5 Proofs of Results in Section 10
Proof of Theorem 16. That V is coherent is obvious, because for each category set A  F,
V (A) = V ++ (A) is obviously a convex cone that includes V + (A) [Proposition 28] and does
not contain the zero polynomial: V ++ (A) is therefore a Bernstein coherent set of polynomials
on A .
To prove representation insensitivity, we use Theorem 7. Consider any category sets A
and D such that there is an onto map  : A  D, any p  V (D) and any   NA  {0}.
Then indeed
(p  R )BA,  V ++ (A)  (  int(A ))p(R ())BA, () > 0
 (  int(A ))p(R ()) > 0
 (  int(D ))p() > 0
 (  int(D ))p()BD,R () () > 0  pBD,R ()  V ++ (D),
where the second and fourth equivalences follow from the Bernstein positivity of the Bernstein
basis polynomials and Proposition 28, and the third one from Lemma 48 with K = A.
73

fiDe Cooman, De Bock, & Diniz

To prove specificity, we use Theorem 11. Consider any category sets A and B such that
B  A, any p  V (B), any   NA  {0} and any r  N0 . Then indeed:
IrB,A (p)BA,  V ++ (A)  (  int(A ))IrB,A (p|)BA, () > 0
 (  int(A ))IrB,A (p|) > 0
 (  int(B ))p() > 0
 (  int(B ))p()BB,rB () () > 0  pBB,rB ()  V ++ (B),
where the second and fourth equivalences follow from the Bernstein positivity of the Bernstein
basis polynomials and Proposition 28, and the third one from Lemma 52 with K = A.
E.6 Proofs of Results in Section 11
Below, we use the convenient device of identifying, for any proper subset B of A, an element
 of B with the unique corresponding element  = iA () of A whose components outside
B are zero:
(x  B)x = x and (x  A \ B)x = 0.
Also observe that, using this convention, we can identify int(A[] ) with a subset of A , and
then characterise it as follows:
for any   A :   int(A[] )  (x  A)(x > 0  mx > 0).
Proof of Proposition 17. It clearly suffices to prove that V + (A)  HSC,A and 0 
/ HSC,A .
The first statement is easy to prove because ASC,A trivially includes all non-constant
Bernstein basis polynomials, by Proposition 28. Since V + (A) consists of finite, strictly positive
linear combinations of these non-constant Bernstein basis polynomials, we immediately have
that V + (A)  HSC,A .
To prove the second statement, suppose ex absurdo that 0  HSC,A . This implies that
++
there are finitely many nk > 0, count vectors k in NAnk and pk  V[
(A) such that
k]

0 = k pk BA,k . It is always possible to find (at least) one such count vector, 1 say,
for which A[k ] 6 A[1 ] for all k. In other words, we have either A[k ] = A[1 ] or
A[k ] \ A[1 ] 6= . Now consider any   int(A[1 ] ). If A[k ] \ A[1 ] 6= , then
++
BA,k () = 0. If A[k ] = A[1 ], then BA,k () > 0, and moreover, since pk  V[
(A),
k]

pk () > 0. Hence 0 = k pk ()BA,k () > 0, a contradiction.
Lemma 42. Consider any   NA  {0} and p  HSC,A, , so there are `  N, nk  N0

++
such that mA + nk > 0, k  NAnk and pk  V[+
(A) such that p = `k=1 pk BA,k .
k]
Then
SA, (p) = {K  A : A[ + k ]  K for some k  {1, . . . , `}}
and therefore
min SA, (p) = min {A[ + k ] : k  {1, . . . , `}} .
Proof of Lemma 42. The second statement is trivial, given the first. So we restrict our
attention to proving the first statement.
Assume first that A[ + r ]  K  A for some r  {1, . . . , `}. Then clearly K 6= ,
since mA + nr > 0. We may assume without loss of generality that A[ + r ] is a minimal
74

fiCoherent Predictive Inference under Exchangeability

element of the set {A[ + k ] : k  {1, . . . , `}}. Consider any   int(A[+r ] ), whence
also   K . Now for all k  {1, . . . , `} such that A[ + k ] = A[ + r ]and there
++
clearly is at least one such kwe see that both pk () > 0 since pk  V[+
(A), and
k]
BA,k () > 0, whence (pk BA,k )() > 0. For all other k we must have that A[ + k ] \
A[ + r ] 6= , and therefore (pk BA,k )() = 0 since BA,k () = 0. This guarantees that

p() = `k=1 (pk BA,k )() > 0, whence indeed K  SA, (p), since we already know that
  K , A[]  A[ + r ]  K and K 6= .
Assume, conversely, that K  SA, (p), which implies that  6= K  A and A[]  K,
and that there is some   K such that p() 6= 0. Observe that A[ + k ] = A[]A[k ],
and assume ex absurdo that A[ + k ] * K and therefore A[k ] * K for all k  {1, . . . , `}.
Fix any k  {1, . . . , `}, then there is some x  A[
/ K, and therefore x = 0,
 k ] such that x 
whence BA,k () = 0. This shows that p() = `k=1 (pk BA,k )() = 0, a contradiction.
Lemma 43. Consider any   NA {0}, any p  V (A) and any n  N such that n  deg(p).
Then for all   NAn :
bnp () 6= 0  (K  min SA, (p))K \ A[]  A[].
Proof of Lemma 43. Fix any  in NAn . We prove the contraposition, so suppose that for
all K in min SA, (p), we have that K \ A[] * A[] and therefore K * A[ + ], since
A[ + ] = A[]  A[]. Hence, A[ + ] 
/ SA, (p). Since moreover  6= A[ + ] and
A[]  A[ + ], we infer from Equation (46) that p|B = 0, where we let, for ease of
notation, B := A[ + ]. We can rewrite this as [see also Lemma 47]:
0 = p|B =


n
NA

bnp ()BA, |B =


n
NB

bnp ()BA, |B =



bnp ()BB, .

n
NB

Due to the uniqueness of the Bernstein expansion, this is only possible if bnp () = 0 for all
n
n
  NA[+]
. This concludes the proof since, clearly,   NA[+]
.

Proof of Proposition 18. First, assume that p  HSC,A, , implying that p = `k=1 pk BA,k
++
for some `  N, nk  N0 such that mA + nk > 0, k  NAnk and pk  V[+
(A). It already
k]
follows from Lemma 42 that p =
6 0 and that min SA, (p) = min {A[ + k ] : k  {1, . . . , `}}.
Consider now any K  min {A[ + k ] : k  {1, . . . , `}} and any   int(K ). Then for all
k, we have that either A[ + k ] = K or A[ + k ] \ K 6= . If A[ + k ] = Kwhich
happens for at least one k, due to our choice of Kthen pk () > 0 and BA,k () > 0. If
A[ + k ] \ K 6= , then since A[]  K, A[k ] \ K =
6 , implying that BA,k () = 0.
Hence, p() > 0. Since this holds for all   int(K ), we find that p|K  V ++ (K).
Assume, conversely, that p  V (A)\{0} and 
that p|K  V ++ (K) for
 all K n min SA, (p).
n
Fix any n  N such that n  deg(p), then p = N n bp ()BA, = M bp ()BA, , with
A
{
}
M :=   NAn : bnp () 6= 0 . Since p 6= 0, we infer from Equation (46) that min SA, (p) 6= 
[observe that A  SA, (p)]. We know from Lemma 43 that for any   M , there is at least
one K  min SA, (p) such that K \ A[]  A[]. Let us just pick any of these K, and call
it K . Now let, for any K  min SA, (p), MK := {  M : K = K}, then we have found
a way to divide M into disjoint subsets MK , one for every K  min SA, (p) and some of
75

fiDe Cooman, De Bock, & Diniz


which may be empty, such that K \ A[]  A[] for all   MK , M = Kmin SA, (p) MK


and therefore p = Kmin SA, (p) MK bnp ()BA, .
Now fix any K  min SA, (p), then we construct a count vector K by letting (mK )x := 1
if x  K \ A[] and (mK )x := 0 otherwise. Notice that K  NAnK , with nK the number of
elements |K \ A[]| in the set K \ A[], and therefore nK  n. Consider any   MK , then
since (mK )x = 1 implies that x  A[] and therefore x  1, we see that for all   A :
BA, () = ()



xx = ()

xA[]



xx (mK )x

xA[]



x(mK )x

xA[]

= (K, )BA,K ()BA,K (),

n
1 ( )1 . Hence, we can rewrite
where (K, ) := ()( 

)
K
K
MK bp ()BA,

:= MK (K, )bnp ()BA,K . In this way, we find that p =
as
 pK BA,K , where pK
Kmin SA, (p) BA,K pK .
Hence, if we fix any K  min SA, (p) 6= , then we are left to prove that mA + nK > 0
++
and pK  V[+
(A). Assume first, ex absurdo, that mA + nK = 0. Then in particular
K]
++
K = , which contradicts K  SA, (p). So it remains to prove that pK  V[+
(A).
K]
Consider any   int(A[+K ] ). Then we can derive from K  min SA, (p)  SA, (p)
that A[]  K. Since A[K ] = K \A[], this implies that A[ + K ] = A[]A[K ] =
A[]  (K \ A[]) = K, and therefore also   int(K ). For all K 0  min SA, (p) \ {K},
K0 \ K =
6  and therefore BA,K 0 () = 0. Hence, p() = BA,K ()pK (). We know that
p() > 0 because p|K  V ++ (K) and that BA,K () > 0 because A[K ] = K \ A[]  K.
We conclude that indeed pK () > 0.
Lemma 44. For all   NA  {0} and p  V (A):
SA, (p) = SA,0 (pBA, ) and therefore min SA, (p) = min SA,0 (pBA, ).
Proof of Lemma 44. First, assume that K  SA, (p). Then  6= K  A, A[]  K and
p|K 6= 0. From this last inequality and the continuity of polynomials, we infer that there is
some   int(K ) such that p() 6= 0. Since A[]  K, we find that p()BA, () 6= 0 and
therefore (pBA, )|K 6= 0.
Assume, conversely, that K  SA,0 (pBA, ). Then  =
6 K  A and (pBA, )|K 6= 0. This
last inequality implies that there is some   K such that (pBA, )() 6= 0 and therefore
both BA, () 6= 0 and p() 6= 0. From BA, () 6= 0, we derive that A[]  K and from
p() 6= 0, we derive that p|K 6= 0.
Proof of Proposition 19. By the way HSC,A and HSC,A, are constructed [see the defining
expressions (43) and (44)], it clearly suffices to prove that HSC,A c  HSC,A, . Consider
therefore any p  V (A) such that pBA,  HSC,A , which by Proposition 18, implies that
pBA, 6= 0 and that (pBA, )|K  V ++ (K) for all K  min SA,0 (pBA, ). We now set out
to prove that p  HSC,A, . Applying Proposition 18 again, and since, clearly, p 6= 0, we
see that it suffices to show that p|K  V ++ (K) for all K  min SA, (p). So consider any
K  min SA, (p). Then, by Lemma 44, K  min SA,0 (pBA, ), so we have already argued
above that (pBA, )|K  V ++ (K). Hence indeed also p|K  V ++ (K).
76

fiCoherent Predictive Inference under Exchangeability

 
Proof of Equation (48). Combining Equations (47) and (30), we see that, for any 
NA  {0}:
1
 = {f  L(A) : SA (f )  HSC,A,
DSC,A
c
(92)
 }.
Also, for any f  L(A) and any  =
6 K  A:
SA (f ) = 0  f = 0, SA (f )|K  V ++ (K)  f |K > 0 and SA (f )|K = 0  f |K = 0. (93)
 = 0. For any f  L(A):
We start with the case 
min SA,0 (SA (f )) = {{x} : x  A and f (x) 6= 0} ,
1
because of Statement (93). Hence, by Proposition 18 and Equations (92) and (93): DSC,A
=
L>0 (A).
  NA . For all f  L(A):
Next, we consider any 

{

{A[]}
if f |A[]
6 0
 =
min SA,
 (SA (f )) =
  {x} : x  A \ A[]
 and f (x) 6= 0} if f |A[]
{A[]
 =0

(94)

because of Equation (93). Now recall Proposition 18 and Equations (92) and (93) and
1
 if and
consider two cases: f |A[]
 6= 0 and f |A[]
 = 0. If f |A[]
 6= 0, then f  DSC,A c
only {if f 6= 0 [which is redundant]
and f |A[]
 > 0 or, equivalently [since f |A[]
 6= 0], if
}
1
 if and only if
f  h  L(A) : h|A[]
 > 0  L>0 (A). If f |A[]
 = 0, then f  DSC,A c
 or, equivalently [since f |A[]
f 6={0 and f (x)  0 for }
all x  A \ A[]
 = 0], again if
f  h  L(A) : h|A[]
 > 0  L>0 (A).
Proof of Equation (49). We start with the first part of Equation (49). Due to Equation (17)
and Proposition 19, it suffices to prove that, for any p  V (A), minxA p(x ) > 0  p 
HSC,A,0 and minxA p(x ) < 0  p 
/ HSC,A,0 .

First, assume that minxA p(x ) < 0. Then there is some y  A for which p(y ) < 0.
Hence, since p|{y} = p(y ) < 0, we find that {y}  min SA,0 (p) and therefore also that
p
/ HSC,A,0 , by Proposition 18.
Next, assume that minxA p(x ) > 0. Then p|{x} = p(x ) > 0 for all x  A, implying
that min SA,0 (p) = {{x} : x  A} and therefore also, since p 6= 0, that p  HSC,A,0 , by
Proposition 18.
We now turn to the second part of Equation (49). Due to Equation (17) and Proposition 19,
  NA and any p  V (A), minA[]
it suffices to prove that, for any 
p() > 0  p 

HSC,A,
p() < 0  p 
/ HSC,A,
 and minA[]
.

First, assume that minA[]
p()
<
0.
Then
there is some   int(A[]
 ) for which

++

 
p() < 0, implying that p|A[]
6= 0 and p|A[]

/ V (A[]).
Hence, we find that A[]


min SA,
/ HSC,A,
 (p) and therefore also that p 
 , by Proposition 18.

Next, assume that minA[]
p()
>
0.
Then
p|A[]
=
6 0 and p|A[]
 V ++ (A[]).



 and therefore also, since p 6= 0, that p  HSC,A,
Hence, we find that min SA,
 (p) = {A[]}
,
by Proposition 18.
77

fiDe Cooman, De Bock, & Diniz

Proof of Equation (52). The first part of Equation (52) is a trivial consequence of Equa  NA and any f  L(A). Then, combining
tion (51). For the second part, consider any 
Equations (50) and (30):


 = min
f (x)x = min f (x).
P 1SC,A (f |)
f (x)x = min
A[]


xA

A[]



xA[]


xA[]

Lemma 45. Consider any category sets A and D such that there is an onto map  : A  D,
any p  V (D) and any  =
6 K  A. Then (p  R )|K 6= 0  p|(K) 6= 0.
Proof of Lemma 45. First, assume that p|(K) 6= 0, so there is some   (K) such that
p() 6= 0. Now choose any   K such that R () = . Then, clearly, (p  R )() =
p(R ()) = p() 6= 0 and therefore (p  R )|K 6= 0.
Assume, conversely, that (pR )|K 6= 0, so there is some   K such that (pR )() 6= 0.
If we let  := R (), then   (K) and p() = p(R ()) = (p  R )() 6= 0. Hence,
p|(K) 6= 0.
Lemma 46. Consider any category sets A and D such that there is an onto map  : A  D,
any p  V (D), and any   NA  {0}. Then
{
}
SA, (p  R ) = K  A : A[]  K and (K)  SD,R () (p) ,
and therefore
(SA, (p  R )) = SD,R () (p) and (min SA, (p  R )) = min SD,R () (p).
Proof of Lemma 46. We start by proving the first statement. First, assume that K  SA, (p
R ), implying that  =
6 K  A, A[]  K and (p  R )|K 6= 0. Then  6= (K)  D,
D[R ()] = (A[])  (K) and, by Lemma 45, p|(K) 6= 0. Hence, (K)  SD,R () (p).
Conversely, assume that K  A, A[]  K and (K)  SD,R () (p). Then  =
6 (K), which
implies that  =
6 K, and also p|(K) 6= 0, which, by Lemma 45, implies that (p  R )|K 6= 0.
Hence, K  SA, (p  R ).
The first statement implies that (SA, (p  R ))  SD,R () (p) and therefore, in order
to prove the second statement, it suffices to show that SD,R () (p)  (SA, (p  R )) or,
equivalently, that for every L  SD,R () (p), there is some K  SA, (p  R ) such that
(K) = L. So choose any L  SD,R () (p) and let K := {x  A : (x)  L} = 1 (L). Then
(K) = L because  is onto, and since (A[]) = D[R ()]  L, it follows that A[]  K.
Hence, by the first statement, K  SA, (p  R ).
To prove the third statement, first assume that K  min SA, (p  R ), implying that
K  SA, (p  R ) and that, for all K 0  SA, (p  R ), K 0 6 K. By the second statement,
(K)  SD,R () (p). To prove that (K)  min SD,R () (p), assume ex absurdo that there
is some L  SD,R () (p) such that L  (K). Let K 0 := {x  K : (x)  L} = K  1 (L).
Then K 0  K and (K 0 ) = L, and therefore, by Lemma 45, (p  R )|K 0 6= 0, because
K 0 6=  and p|L =
6 0. Since L  SD,R () (p), we see that (A[]) = D[R ()]  L and
therefore A[]  1 (L). Since K  SA, (p  R ), we also know that A[]  K, and
therefore A[]  K  1 (L) = K 0 . This tells us that K 0  SA, (p  R ), a contradiction.
Assume, conversely, that L  min SD,R () (p), implying that L  SD,R () (p). Then, by
78

fiCoherent Predictive Inference under Exchangeability

the second statement, there is some K 0  SA, (p  R ) such that (K 0 ) = L. Hence, there
is some K  min SA, (p  R ) such that K  K 0 and therefore (K)  (K 0 ) = L. Since
L  min SD,R () (p) and since, due to the second statement, (K)  SD,R () (p), we also
have that (K) 6 L and therefore (K) = L.
Lemma 47. Let  =
6 K  A and let p be any polynomial on A . Then for any n  deg(p):
bnp| = bnp |NKn .
K

Proof of Lemma 47. It follows from

p() =
bnp ()BA, () for all   A
n
NA

that for all   K :
p|K () =


n
NA

=





bnp ()BA, (iA ()) =

bnp ()BA, (iA ())

n : A[]K
NA

bnp |NKn ()BK, (),

n
NK

and this completes the proof.
Lemma 48. Consider any category sets A and D such that there is an onto map  : A  D,
any p  V (D) and any  =
6 K  A. Then:
(i) (p  R )|K  V + (K)  p|(K)  V + ((K));
(ii) (p  R )|K  V ++ (K)  p|(K)  V ++ ((K)).
Proof of Lemma 48. The first statement follows from the fact that, for all n  deg(p):
bn(pR )|

K

n
> 0  bn(pR ) |NKn > 0  (bnp  R )|NKn > 0  bnp |N(K)
> 0  bnp|

> 0,
(K)

where the first and last equivalence are due to Lemma 47, the second equivalence follows
n) = Nn
from Proposition 39, and the third equivalence holds because R (NK
(K) .
We now turn to the second statement, where we have to prove that the following statements
are equivalent:
(a) (  int(K ))p(R ()) > 0;
(b) (  int((K) ))p() > 0.
First assume that (a) holds, and consider any   int((K) ). We have to prove that p() > 0.
1
We construct a   K as follows.
 Consider any z  (K). For all x   ({z})  K, choose
the x > 0 in such a way that xK : (x)=z x = z . In this way, we have found a   K
satisfying R () = , and such that moreover x > 0 for all x  K, whence   int(K ). We
now infer from (a) that indeed p() = p(R ()) > 0.
Assume, conversely, that (b) holds, and consider any   int(K ). Then, for any z  D,
R ()z > 0 if z  (K) and R ()z = 0 otherwise. This means that R ()  int((K) ) and
we infer from (b) that indeed p(R ()) > 0.
79

fiDe Cooman, De Bock, & Diniz

Proposition 49. SC is representation insensitive.
Proof of Proposition 49. We use the characterisation of representation insensitivity in Theorem 7. Consider any category sets A and D such that there is an onto map  : A  D,
any p  V (D) and any   NA  {0}. Then, by Proposition 19, we need to prove that
p  R  HSC,A,  p  HSC,D,R () .
First, assume that p  HSC,D,R () , which, by Proposition 18, implies that p 6= 0 and
that p|L  V ++ (L) for all L  min SD,R () (p). Applying Lemma 45 with K = A, we infer
from p =
6 0 that p  R 6= 0. Consider now any K  min SA, (p  R ). Then, by Lemma 46,
(K)  min SD,R () (p), implying that, due to the assumption, p|(K)  V ++ ((K)). Since
K 6= , we can apply Lemma 48 to find that (p  R )|K  V ++ (K). Hence, by Proposition 18,
p  R  HSC,A, .
Assume, conversely, that pR  HSC,A, , which, by Proposition 18, implies that pR 6= 0
and that (p  R )|K  V ++ (K) for all K  min SA, (p  R ). Applying Lemma 45, with
K = A, we infer from p  R 6= 0 that p 6= 0. Now, consider any L  min SD,R () (p), then
by Lemma 46, there is some K  min SA, (p  R ) such that (K) = L. Since K =
6  and,
by assumption, (p  R )|K  V ++ (K), we infer from Lemma 48 that p|L  V ++ (L). Hence,
by Proposition 18, p  HSC,D,R () .
Lemma 50. Consider any category sets A and B such that B  A, any p  V (B), any
K  A such that K  B 6=  and any r  N0 . Then IrB,A (p)|K 6= 0  p|KB 6= 0.
Proof of Lemma 50. We may assume without loss of generality that r + deg(p) > 0, as the
proof is trivial otherwise.
First, assume that p|KB =
6 0, which means that there is some   KB such that
p() 6= 0. Then  := iA ()  K , and we infer from Proposition 9 that IrB,A (p|) = p() 6= 0
and therefore IrB,A (p)|K 6= 0.
Assume, conversely, that IrB,A (p)|K 6= 0, which means, due to the continuity of polynomials, that there is some   int(K ) such that IrB,A (p|) 6= 0. We now infer from K  B 6= 
+
that B > 0, so Proposition 10 guarantees that p(|+
B ) 6= 0. Since |B  KB , we find that
p|KB 6= 0.
Lemma 51. Consider any category sets A and B such that B  A, any p  V (B) any
r  N0 such that r + deg(p) > 0, and any   NA  {0}. Then
{
}
SA, (IrB,A (p)) = K  A : A[]  K and K  B  SB,rB () (p) ,
and therefore
{
}
SB,rB () (p) = K  B : K  SA, (IrB,A (p))
and
{
}
min SB,rB () (p) = K  B : K  min SA, (IrB,A (p)) .
Proof of Lemma 51. We begin with the first statement. First, assume that K  SA, (IrB,A (p))
and therefore that  6= K  A, A[]  K and IrB,A (p)|K =
6 0. Then K  A implies that
KB  B, A[]  K implies that B[rB ()] = A[]B  KB. Moreover, IrB,A (p)|K =
6 0
together with Proposition 10 and r + deg(p) > 0 implies that K  B 6= , which in turn, by
Lemma 50, implies that p|KB 6= 0. Hence, K  B  SB,rB () (p). Conversely, assume that
80

fiCoherent Predictive Inference under Exchangeability

K  A, A[]  K and K  B  SB,rB () (p). Then K  B 6= , implying that K 6= , and
p|KB 6= 0, which, by Lemma 50, implies that IrB,A (p)|K 6= 0. Hence, K  SA, (IrB,A (p)).
In order to prove the second statement, it clearly suffices to show that SB,rB () (p) 
{K  B : K  SA, (IrB,A (p))}, since the converse inclusion follows directly from the first
statement. So consider any L  SB,rB () (p) and let K := L  A[]. Then K  A, A[]  K
and K  B = L  (A[]  B) = L  B[rB ()] = L. Hence, by the first statement, indeed
K  SA, (IrB,A (p)).
To prove the third statement, first assume that K  min SA, (IrB,A (p)), implying that
in particular K  SA, (IrB,A (p)). Then, by the second statement, K  B  SB,rB () (p). To
prove that K  B  min SB,rB () (p), consider any L  SB,rB () (p) such that L  K  B, and
let K 0 := L  A[]. Then, by an argument identical to the one used in the proof of the second
statement, K 0  B = L and K 0  SA, (IrB,A (p)). However, since K 0  B = L  K  B and
K 0 \ B = A[] \ B  K \ B, we find that K 0 = (K 0  B)  (K 0 \ B)  (K  B)  (K \ B) = K,
and therefore K 0 = K, by assumption. Hence indeed L = K 0  B = K  B. Assume,
conversely, that L  min SB,rB () (p), implying that L  SB,rB () (p). Then, by the second
statement, there is some K 0  SA, (IrB,A (p)) such that K 0  B = L, so there is some
K  min SA, (IrB,A (p)) such that K  K 0 and therefore K  B  K 0  B = L. Since
L  min SB,rB () (p) and, by the second statement, K  B  SB,rB () (p), we also have that
K  B = L.
Lemma 52. Consider any category sets A and B such that B  A, any p  V (B), any K  A
such that K  B 6=  and any r  N0 . Then IrB,A (p)|K  V ++ (K)  p|KB  V ++ (K  B).
Proof of Lemma 52. We may assume without loss of generality that r + deg(p) > 0, as the
proof is trivial otherwise. Using Proposition 10, and considering that, since K  B =
6 , B > 0
for any   int(K ), it then suffices to prove that the following statements are equivalent:
(a) (  int(K ))p(|+
B ) > 0;
(b) (  int(KB ))p() > 0.
First assume that (a) holds, and consider any   int(KB ). We have to prove that p() > 0.
We construct
a   K as follows. For any x  K \ B, choose x > 0 in such a way that

 := xK\B x < 1, which is always possible. And for any x  K B, let x := (1)x > 0.
Then it follows from this construction that B = 1   > 0, |+
B =  and   int(K ), so we
infer from (a) that indeed p() = p(|+
)
>
0.
B
Assume, conversely, that (b) holds, and consider any   int(K ). Then B > 0 because
+
K B =
6 0 and therefore, for all z  B, (|+
B )z > 0  z  K  B. Hence |B  int(KB ),
+
so we infer from (b) that p(|B ) > 0.
Proposition 53. SC is specific.
Proof of Proposition 53. We use the characterisation of specificity in Theorem 11. Consider
any category sets A and B such that B  A, any p  V (B), any   NA {0}, and any r  N0 .
Then, by Proposition 19, we need to prove that IrB,A (p)  HSC,A,  p  HSC,B,rB () .
First, assume that p  HSC,B,rB () , which, by Proposition 18, implies that p 6= 0 and
that p|L  V ++ (L) for all L  min SB,rB () (p). Applying Lemma 50 with K = A, we infer
from p 6= 0 that IrB,A (p) 6= 0. Consider any K  min SA, (IrB,A (p)), then by Lemma 51,
81

fiDe Cooman, De Bock, & Diniz

K  B  min SB,rB () (p), implying that, due to the assumption, p|KB  V ++ (K  B).
Since K  B 6= 0, we can apply Lemma 52 to find that IrB,A (p)|K  V ++ (K). Hence, again
by Proposition 18, IrB,A (p)  HSC,A, .
Assume, conversely, that IrB,A (p)  HSC,A, , which, by Proposition 18, implies that
r
IB,A (p) 6= 0 and that IrB,A (p)|K  V ++ (K) for all K  min SA, (IrB,A (p)). From Lemma 50
with K = A, and from IrB,A (p) 6= 0, we infer that p 6= 0. Consider any L  min SB,rB () (p),
then, by Lemma 51, there is some K  min SA, (IrB,A (p)) such that KB = L. Since therefore
K  B 6= 0 and since, by assumption, IrB,A (p)|K  V ++ (K), we infer from Lemma 52 that
p|L  V ++ (L). Hence, by Proposition 18, p  HSC,B,rB () .
Proof of Theorem 20. This is an immediate consequence of Propositions 17 [coherence], 49
[representation insensitivity] and 53 [specificity].
E.7 Proofs of Results in Section 12
  NA  {0} and any p  V (A). Then
Proof of Equation (54). Consider any 
s
s
  BA, p  HIDM,A
p  HIDM,A
c
 (  sA ) DiA (BA, p|) > 0

 + ) > 0
 (  sA ) DiA (BA, |) DiA (p|
 + ) > 0,
 (  sA ) DiA (p|
where the third equivalence follows from the Updating Property of the Dirichlet expectation
[Proposition 31].
  NA  {0}. Then, combining Equations (56)
Proof of Equation (59). Consider any 
and (58) for n = 1:
{
}

mx + x
s,1
 = f  L(A) : (  sA )
DIDM,A
c
f (x)
>0 .
mA + A
xA

Now consider any f  L(A). Then for all   sA :

xA

f (x)



mx + x
x
1 
>0
f (x)(mx + x ) > 0 
>
f (x)
f (x)mx .
mA + A
s
s
xA

xA

Combining the equations above, and letting c :=  1s
find that:

xA



xA f (x)mx

s,1
  (s0  (0, s))(  int(A ))
f  DIDM,A
c

for ease of notation, we

s0 
f (x)tx > c.
s

(95)

xA

If f  c, then there is some y  A for which f (y) < c and therefore, by Statement (95),
s,1
 [choose s0 and ty close enough to s and 1, respectively]. If f = c, then due to
f
/ DIDM,A
c
s,1
 Finally, let us
the definition of c, f = c = 0. Hence, again by Statement (95), f 
/ DIDM,A
c.
see what happens 
if f > c. Then clearly c  0. Consider any s0  (0, 
s) and any   int(A ).
0
0
Then since f > c, xA f (x)tx > c and therefore also, since c  0, ss xA f (x)tx > ss c  c.
s,1
 by Statement (95).
Hence f  DIDM,A
c
82

fiCoherent Predictive Inference under Exchangeability

  NA  {0} and any f  L(A). Then by combining
Proof of Equation (60). Consider any 
Equations (57) and (58):

mx + x
mx + s0 tx
= inf
inf
f (x)
A
mA + A s0 (0,s) int(A )
mA + s0
xA
xA
(
)


1
s0
= inf
f (x)mx +
inf
f (x)tx
mA + s0 int(A )
s0 (0,s) mA + s0
xA
xA
(
)

1
s0
= inf
f (x)mx +
min f
mA + s0
s0 (0,s) mA + s0
xA

s
1
f (x)mx +
min f,
=
mA + s
mA + s

 = inf s
P s,1
IDM,A (f |)



f (x)

xA

where the last equality follows from min f 

mx
xA f (x) mA , a



property of convex combinations.

Proof of Theorem 21. For coherence, if we fix any category set A, then we must prove that
s
HIDM,A
satisfies the requirements B1B3 of Bernstein coherence. This is trivial from the
s
definition of HIDM,A
, the linearity of the Dirichlet expectation operator, and the fact that
the Dirichlet expectation of any Bernstein basis polynomial is positive.
Next, we turn to representation insensitivity, and use its characterisation in Theorem 7.
Consider any category sets A and D such that there is an onto map  : A  D, any p  V (D)
and any   NA  {0}. Then, using the Pooling Property [Proposition 33] of the Dirichlet
expectation and Equation (54), we find that indeed:
s
(p  R )BA,  HIDM,A
 (  sA ) DiA (p  R | + ) > 0

 (  sA ) DiD (p|R ( + )) > 0
s
 (  sD ) DiD (p|R () + ) > 0  pBD,R ()  HIDM,D
,

where the third equivalence follows from the equality sD = R (sA ).
Finally, we turn to specificity, and use its characterisation in Theorem 11. Consider
any category sets A and B such that B  A, any p  V (B), any   NA  {0} and any
r  N0 . Then, using the Restriction Property [Proposition 34] of the Dirichlet expectation
and Equation (54), we find that indeed:
s
IrB,A (p)BA,  HIDM,A
 (  sA ) DiA (IrB,A (p)| + ) > 0

 (  sA ) DiB (p|rB ( + )) > 0
s
 (  sB ) DiB (p|rB () + ) > 0  pBB,rB ()  HIDM,B
,

where the third equivalence follows from sB = rB (sA ).
E.8 Proofs of Results in Section 13
s
Lemma 54. For any p1 , p2  HSI,A
: SA,0 (p1 + p2 ) = SA,0 (p1 )  SA,0 (p2 ).

83

fiDe Cooman, De Bock, & Diniz

Proof of Lemma 54. First, consider any K  SA,0 (p1 + p2 ), meaning that  6= K  A and
(p1 + p2 )|K 6= 0. Assume, ex absurdo, that K 
/ SA,0 (p1 ) and K 
/ SA,0 (p2 ). Then p1 |K = 0
and p2 |K = 0 and therefore (p1 + p2 )|K = 0, which is a contradiction. Hence indeed
K  SA,0 (p1 )  SA,0 (p2 ).
Next, consider any K  SA,0 (p1 )  SA,0 (p2 ), implying that  =
6 K  A. Then there is
at least one K 0  min(SA,0 (p1 )  SA,0 (p2 )) such that K 0  K, and we can assume without
loss of generality that K 0  SA,0 (p1 ). Since K 0  min(SA,0 (p1 )  SA,0 (p2 )), we have that
L 6 K 0 for all L  SA,0 (p1 )  SA,0 (p2 ), and therefore K 0  min SA,0 (p1 ). This already tells us
s
0
that p1 |K 0  HIDM,K
0 . There are now two possibilities. The first one is that K  SA,0 (p2 ),
s
and then, in very much the same way as as above, we find that p2 |K 0  HIDM,K 0 . Hence,
s
s
due to the Bernstein coherence [B3] of HIDM,K
0 , (p1 + p2 )| 0 = p1 | 0 + p2 | 0  HIDM,K 0 .
K
K
K
0
0
The second possibility is that K 
/ SA,0 (p2 ), and then p2 |K 0 = 0 since K 6= , so we find,
s
here too, that (p1 + p2 )|K 0 = p1 |K 0 + p2 |K 0 = p1 |K 0  HIDM,K
0 . In both cases, therefore,
s
s
(p1 + p2 )|K 0  HIDM,K 0 , and the Bernstein coherence [B1] of HIDM,K
0 allows us to conclude
6 0. Since K 0  K, we find that also (p1 + p2 )|K 6= 0 and therefore that
that (p1 + p2 )|K 0 =
K  SA,0 (p1 + p2 ).
s
s
Proof of Proposition 22. Since 0 
/ HSI,A
, we are left to prove that V + (A)  HSI,A
and that,
s
s
s
for all  > 0 and p, p1 , p2  HSI,A , p  HSI,A and p1 + p2  HSI,A .
s
First, consider any  > 0 and p  HSI,A
. Then, clearly, SA,0 (p) = SA,0 (p) and therefore
min SA,0 (p) = min SA,0 (p). Then for any K  min SA,0 (p), we have that K  min SA,0 (p),
s
s
which, since p  HSI,A
, implies that p|K  HIDM,K
and therefore, due to the Bernstein
s
s
coherence of HIDM,K , that (p)|K = (p|K )  HIDM,K
. Furthermore, since p 6= 0 also
s
p 6= 0, and therefore p  HSI,A .
s
Next, consider any p1 , p2  HSI,A
. Then p1 6= 0 and p2 6= 0, implying that SA,0 (p1 ) 6= 
and SA,0 (p2 ) 6= , and therefore SA,0 (p1 )  SA,0 (p2 ) 6= . Applying Lemma 54, we find
that SA,0 (p1 + p2 ) 6= , so there is some K such that  6= K  A, (p1 + p2 )|K =
6 0 and
therefore p1 + p2 6= 0. Then for any K 0  min SA,0 (p1 + p2 ), or equivalently, due to Lemma 54,
K 0  min(SA,0 (p1 )  SA,0 (p2 )). Then, by applying the same reasoning as in the second part
s
s
of the proof of Lemma 54, we find that (p1 + p2 )|K 0  HIDM,K
0 . Hence, p1 + p2  HSI,A .
s
Since we have already shown that HSI,A is closed under taking positive linear combinations,
and since V + (A) consists of positive linear combinations of Bernstein basis polynomials, we
s
only need to show that HSI,A
contains all Bernstein basis polynomials in order to prove that
+
s
V (A)  HSI,A . So consider any   NA  {0}. Then, for any K such that  6= K  A, we
have that BA, |K = BK,rK () if A[]  K, and BA, |K = 0 otherwise. This implies that
s
SA,0 (BA, ) = { =
6 K  A : A[]  K} and that, due to the Bernstein coherence of HIDM,K
,
s
s
BA, |K = BK,rK ()  HIDM,K for all K  SA,0 (BA, ). Hence, BA, |K  HIDM,K for all
s
K  min SA,0 (BA, ). Since also BA, 6= 0, we find that indeed BA,  HSI,A
.
s
s
Proof of Proposition 23. We first prove that HSI,A
c  HSI,A,
. Consider any p  V (A)
s
s
such that pBA,  HSI,A , meaning that pBA, 6= 0 and that (pBA, )|K  HIDM,K
for all
s
K  min SA,0 (pBA, ). We set out to prove that p  HSI,A, . Since, clearly, p 6= 0, it suffices to
s
show that p|K  HIDM,K
crK () for all K  min SA, (p). So consider any K  min SA, (p),
implying that A[]  K and therefore also that K[rK ()] = A[]. We also infer from
s
Lemma 44 that K  min SA,0 (pBA, ), which tells us that (pBA, )|K  HIDM,K
. Since
s
(pBA, )|K = p|K BA, |K = p|K BK,rK () , we find that p|K  HIDM,K crK ().

84

fiCoherent Predictive Inference under Exchangeability

s
s
s
Next, we prove that HSI,A,
 HSI,A
c. Consider any p  HSI,A,
, meaning that
s
p 6= 0 and p|K  HIDM,K crK () for all K  min SA, (p). We set out to prove that
s
s
pBA,  HSI,A
or, equivalently, that pBA, =
for all
6 0 and that (pBA, )|K  HIDM,K
K  min SA,0 (pBA, ). Since p 6= 0, the continuity of polynomials guarantees that there is
some   int(A ) such that p() 6= 0 and therefore also (pBA, )() 6= 0. So we know already
that pBA, 6= 0. Consider any K  min SA,0 (pBA, ). Then, by Lemma 44, K  min SA, (p),
s
s
implying that A[]  K and p|K  HIDM,K
crK () and therefore p|K BK,rK ()  HIDM,K
.
s
Since moreover p|K BK,rK () = (pBA, )|K , we find that indeed (pBA, )|K  HIDM,K .

Proof of Equation (63). Due to Equation (17), it suffices to prove that, for any p  V (A),
s
s
minxA p(x ) > 0  p  HSI,A
and minxA p(x ) < 0  p 
/ HSI,A
.

First, assume that minxA p(x ) < 0. Then there is some y  A for which p(y ) < 0.
Hence, since p|{y} = p(y ) < 0, we find that {y}  min SA,0 (p) and therefore also, due to
s
s
the Bernstein coherence of HIDM,{y}
[see Theorem 21], that p|{y} 
/ HIDM,{y}
, from which
s
we infer that p 
/ HSI,A .
Next, assume that minxA p(x ) > 0. Then p|{x} = p(x ) > 0 for all x  A, implying
s
that min SA,0 (p) = {{x} : x  A} and that, for all x  A, p|{x}  HIDM,{x}
, again because
s
s
of the Bernstein coherence of HIDM,{x} . Hence, since p 6= 0, we find that p  HSI,A
.
Proof of Equations (64) and (65). Equation (65) follows directly from Equation (55). We
prove Equation (64). Due to Equation (17) and Proposition 23, it suffices to prove that, for
  NA and any p  V (A):
any 
s
s
 > 0  p  HSI,A,
 <0p
c(p, )
/ HSI,A,
 and c(p, )
,



where, for ease of notation, we let
 :=
c(p, )

inf

sA[]


 + ).
DiA[]
|rA[]
 (p|A[]
 ()


 < 0, implying that DiA[]
 + ) < 0 for
First, assume that c(p, )
|rA[]
 (p|A[]
 ()

s
some   A[]
6= 0 and, by Equation (54), that p|A[]

/
 and therefore also that p|A[]


s


HIDM,A[]
 (p) and therefore also that
 (). Hence, we find that A[]  min SA,
 crA[]
s
p
/ HSI,A,
.


 > 0, implying that p|A[]
Next, assume that c(p, )
6= 0 and, by Equation (54), that

s

 and therefore
p|A[]

H
cr
(
).
Hence,
we
find
that
min
S
 (p) = {A[]}
A,

A[]

IDM,A[]

s
also that p  HSI,A,
.
  NA :
Proof of Equation (67). By combining Equations (62) and (30), we see that, for any 
{
}
s,1
s
 = f  L(A) : SA (f )  HSI,A,
DSI,A
c
(96)
 .

Consider now any f  L(A) and distinguish between two cases: f |A[]
 6= 0 and f |A[]
 = 0.
If f |A[]
 6= 0 [and therefore also f 6= 0], then
s,1
s
  SA (f )|A[]
f  DSI,A
c
 HIDM,A[
 ()
 crA[]
]

s
 SA[]
 (f |A[]
 )  HIDM,A[]
 ()
 crA[]

85

fiDe Cooman, De Bock, & Diniz

s,1
 f |A[]
  DIDM,A[]
 ()
 crA[]

1
1 
 f |A[]
>

f
(x)
m

f
>

f (x)mx or f > 0,
x


|A[]
s
s

xA[]


xA[]

where the first equivalence is due to Statement (93) and Equations (94), (61) and (96). The
second equivalence follows from the definition of SA and SA[]
 and the third one is due to
Equations (21) and (30). The fourth equivalence is a consequence of Equation (67) and the
final equivalence holds because f > 0 is redundant, given that f |A[]
 6= 0.
If f |A[]
 = 0, then [again, using Statement (93) and Equations (94), (61) and (96)]
s,1
 if and only if f 6= 0 and if for all x  A \ A[]:

f  DSI,A
c
s
f (x) = 0 or SA (f )|A[]{x}
 HIDM,A[
crA[]{x}
().


]{x}

s
Since f |A[]
crA[]{x}
() is Bernstein coherent [Theorem 21], the
 = 0 and HIDM,A[]{x}


latter statement is equivalent to f (x) > 0. Hence, we find that:
s,1
  f 6= 0 and (x  A \ A[])f

f  DSI,A
c
(x)  0

f >0
 f |A[]
 >

1 
f (x)mx or f > 0,
s

xA[]

where the second and third equivalences are consequences of f |A[]
 = 0.
Lemma 55. Consider any category sets A and D such that there is an onto map  : A  D,
any p  V (D), any   NA  {0}, and any  6= K  A such that A[]  K. Then
s
s
(p  R )|K  HIDM,K
crK ()  p|(K)  HIDM,(K)
cr(K) (R ()).
Proof of Lemma 55. Let A := K, D := (K),  := |K and p := p|(K) . Then  is
an onto map from A to D , p  V (D ) and p  R = p|(K)  R|K = (p  R )|K .
mA
Since A[]  K, we can identify  with an element  := rK () of NK
and therefore
the result follows from the representation insensitivity of the IDMM inference system with
hyperparameter s, because then also R ( ) = R|K (rK ()) = r(K) (R ()):
s


s

p  R  HIDM,A
 c  p  HIDM,D  cR ( ).

Proposition 56. sSI is representation insensitive.
Proof of Proposition 56. We use the characterisation of representation insensitivity in Theorem 7. Consider any category sets A and D such that there is an onto map  : A  D,
any p  V (D) and any   NA  {0}. Then, by Proposition 23, we need to prove that
s
s
p  R  HSI,A,
 p  HSI,D,R
.
 ()
s
s
First, assume that p  HSI,D,R () , meaning that p 6= 0 and p|L  HIDM,L
crL (R ())
for all L  min SD,R () (p). Applying Lemma 45 with K = A, we infer from p 6= 0 that
p  R 6= 0. Consider any K  min SA, (p  R ), so  6= K  A and A[]  K. Then,
by Lemma 46, (K)  min SD,R () (p), implying that, due to the assumption, p|(K) 
86

fiCoherent Predictive Inference under Exchangeability

s
s
HIDM,(K)
cr(K) (R ()). Applying Lemma 55, we find that (p  R )|K  HIDM,K
crK ().
s
Hence, p  R  HSI,A, .
s
Assume, conversely, that p  R  HSI,A,
, meaning that p  R 6= 0 and that (p  R )|K 
s
HIDM,K crK () for all K  min SA, (p  R ). Applying Lemma 45 with K = A, we infer
from p  R 6= 0 that p 6= 0. Consider any L  min SD,R () (p). By Lemma 46, there
is some K  min SA, (p  R ) such that (K) = L. Since  6= K  A, A[]  K
s
and, by the assumption, (p  R )|K  HIDM,K
crK (), we infer from Lemma 55 that
s
s
p|L  HIDM,L crL (R ()). Hence, p  HSI,D,R () .

Lemma 57. Consider any category sets A and B such that B  A, any p  V (B), any
  NA  {0}, any r  N0 and any K  A such that K  B 6=  and A[]  K. Then
s
s
IrB,A (p)|K  HIDM,K
crK ()  p|KB  HIDM,KB
crKB ().
Proof of Lemma 57. Let A := K, B  := K  B, p := p|KB and r = deg(p)  deg(p ) + r.
Then B   A , p  V (B  ), r  r  0, r + deg(p ) = r + deg(p), and


bdeg(p)+r
()BA,iA ()|K =
bdeg(p)+r
()BA,iA ()|K
IrB,A (p)|K =
p
p
deg(p)+r

deg(p)+r

NB

=



NB
B[]K


bdeg(p)+r
()BK,iK () = IrB  ,A (p ),
p|
KB

deg(p)+r

NKB

where the third equality follows from the unicity of the Bernstein expansion of a polynomial.
Since A[]  K, we can identify  with an element  := rK () of NK {0} and therefore
the result follows from the specificity of the IDMM inference system with hyperparameter s,
because then also rB  ( ) = rKB ():


s


s

IrB  ,A (p )  HIDM,A
 c  p  HIDM,B  crB  ( ).

Proposition 58. sSI is specific.
Proof of Proposition 58. We use the characterisation of specificity in Theorem 11. Consider
any category sets A and B such that B  A, any p  V (B), any   NA  {0} and any
s
s
r  N0 . Then, by Proposition 23, we need to prove that IrB,A (p)  HSI,A,
 p  HSI,B,r
.
B ()
It is clear from Propositions 10 and 22 that we can assume without loss of generality that
r + deg(p) > 0.
s
s
First, assume that p  HSI,B,r
, implying that p =
6 0 and that p|L  HIDM,L
crBL ()
B ()
for all L  min SB,rB () (p). Applying Lemma 50 with K = A, we infer from p 6= 0 that
IrB,A (p) 6= 0. Consider any K  min SA, (IrB,A (p)). Then we infer from Lemma 51 that K 
s
crKB ().
B  min SB,rB () (p), implying that, due to our assumption, p|KB  HIDM,KB
r
s
Since K  B 6= 0 and A[]  K, IB,A (p)|K  HIDM,K crK () because of Lemma 57. Hence,
s
IrB,A (p)  HSI,A,
.
s
Assume, conversely, that IrB,A (p)  HSI,A,
, which implies that IrB,A (p) 6= 0 and that
r
s
IB,A (p)|K  HIDM,K crK () for all K  min SA, (IrB,A (p)). Applying Lemma 50 with
K = A, we infer from IrB,A (p) 6= 0 that p 6= 0. Consider any L  min SB,rB () (p). By
Lemma 51, there is some K  min SA, (IrB,A (p)) such that K  B = L. Since K  B 6= 0,
87

fiDe Cooman, De Bock, & Diniz

s
A[]  K and, by assumption, IrB,A (p)|K  HIDM,K
crK (), we infer from Lemma 57
s
s
that p|KB  HIDM,KB crKB (), or in other words, p|L  HIDM,L
crBL (). Hence,
s
p  HSI,B,rB () .

Proof of Theorem 24. This is an immediate consequence of Propositions 22 [coherence], 56
[representation insensitivity] and 58 [specificity].
E.9 Proofs of Results in Section 14
Proof of Theorem 25. We begin with coherence. Consider any category set A, then we have
s
to prove that HH,A is Bernstein coherent. For B1, recall that 0 
/ HIDM,A
for all s > 0, and
+
s
therefore also 0 
/ HH,A . Similarly, for B2, recall that V (A)  HIDM,A for all s > 0, and
+
therefore also V (A)  HH,A . For B3, consider n  N and k  R>0 and pk  HH,A for
s
all k  {1, . . . ,
n}. Then there is some s > 0 such that pk  HIDM,A
for all k  {1, . . . , n},
n
s
and
n therefore k=1 k pk  HIDM,A , by Bernstein coherence [Theorem 21]. Hence indeed
k=1 k pk  HH,A .
Next, we turn to representation insensitivity, and use its characterisation in Theorem 7.
Consider any category sets A and D such that there is an onto map  : A  D, any p  V (D)
and any   NA  {0}. Then we find that indeed:
s
(p  R )BA,  HH,A  (s  R>0 )(p  R )BA,  HIDM,A
s
 (s  R>0 )pBD,R ()  HIDM,D
 pBD,R ()  HH,D ,

where the second equivalence follows from the representation insensitivity of the IDMM
inference systems [Theorem 21].
Finally, we turn to specificity, and use its characterisation in Theorem 11. Consider any
category sets A and B such that B  A, any p  V (B), any   NA  {0} and any r  N0 .
Then we find that indeed:
s
IrB,A (p)BA,  HH,A  (s  R>0 )IrB,A (p)BA,  HIDM,A
s
 (s  R>0 )pBB,rB ()  HIDM,B
 pBB,rB ()  HH,B ,

where the second equivalence follows from the specificity of IDMM inference systems [Theorem 21].
  NA  {0} and any p  V (A):
Proof of Equation (69). For any 
s
  pBA,
p  HH,A c
  HH,A  (s  R>0 )pBA,
  HIDM,A
s

 (s  R>0 )p  HIDM,A
c.

Combined with Equation (54), this yields the desired result.
  NA  {0} and any p  V (A):
Proof of Equation (71). For any 
 = sup {  R : p    HH,A c}

H H,A (p|)
{
}
s

= sup sup   R : p    HIDM,A
c
sR>0

88

fiCoherent Predictive Inference under Exchangeability

 + ) = lim
inf s DiA (p|

= sup

 + ),
inf DiA (p|

s+0 sA

sR>0 A

where the second equality is due to Equation (69), and the third one due to Equation (54).
Proof of Equation (72). Consider any p  V (A) and apply Equation (71):
H H,A (p) = lim

inf DiA (p|) = lim

s+0 sA

inf DiA (p|s0 )

inf

s+0 int(A ) s0 (0,s)

(97)

Now fix any n  max{deg(p), 1} and any   int(A ). Using Equation (80), we find that for
all   NAn :
(

1

0

DiA (BA, |s ) =

s0 (n)

)
( ) 
n  0 (mx )
n
1
(m )
(s tx )
= (n)
(s0 tx ) x ,
0


s
xA
xA[]

where for all x  A[]:
(mx )

(s0 tx )

= (s0 tx )(s0 tx + 1) . . . (s0 tx + mx  1) = s0 tx (mx  1)![1 + O(s0 )]

and similarly:
1
s0 (n)

=

s0 (n

1
[1 + O(s0 )].
 1)!

Hence, we find that
( 
0

DiA (BA, |s ) =

xA[] tx (mx

 1)!

)

(n  1)!

s0|A[]|1 [1 + O(s0 )].

We now consider two cases: |A[]| > 1 and |A[]| = 1 [since n  1, these cases are
exhaustive]. If |A[]| > 1, then DiA (BA, |s0 ) = O(s0 ). If |A[]| = 1 or, equivalently, if
there is some x  A such that  = nx , then DiA (BA,nx |s0 ) = tx [1 + O(s0 )]. If we combine
this with Equation (78), we find that
DiA (p|s0 ) =



bnp () DiA (BA, |s0 ) =

n
NA



bnp (nx )tx + O(s0 ).

xA

Furthermore, again due to Equation (78):
bnp (nx ) =



bnp ()BA, (x ) = p(x ) for all x  A.

n
NA

Hence, we conclude that
DiA (p|s0 ) =



p(x )tx + O(s0 ),

xA

which, combined with Equation (97), leads to the desired result.
89

fiDe Cooman, De Bock, & Diniz

  NA and any p  V (A) and use Equation (71):
Proof of Equation (73). Consider any 
 = lim
H H,A (p|)

 = lim sup DiA (p|
 + ). (98)
 + ) and H H,A (p|)
inf DiA (p|

s+0 sA

s+0 s

A

 is Bernstein coherent [Theorem 25], it follows that H H,A (|)
 is a coherent
Since HH,A c
 is super-additive, and that its conjugate upper
lower prevision. This implies that H H,A (|)
 is sub-additive. Hence, it suffices to prove the equalities in Equation (73)
prevision H H,A (|)
for any Bernstein basis polynomial p = BA, , where   NA  {0}. Now for any   sA we
gather from Equation (80) in Appendix B that:
( ) 
n
 + ) =
(mx + x )(nx ) .
DiA (BA, |
(n) 
(mA + A )
xA
1

Observe that:
x)
(mx + x )(nx ) = (mx + x )(mx + x + 1) . . . (mx + x + nx  1) = m(n
[1 + O(x )],
x

and similarly, since mA > 0:
1
(mA + A )

(n)

1

=

(n)

mA

[1 + O(A )]

Therefore:
( ) 
(nx )

n
xA mx
 + ) =
DiA (BA, |
[1
+
O(
)]
[1 + O(x )],
A
(n)

m
xA

A

which, using Equation (98), leads to:46
( ) 
(nx )
n
xA mx
 = H H,A (BA, |)
 =

H H,A (BA, |)
= DiA (BA, |).
(n)

m
A

E.10 Proofs of Results in Section 15
Proof of Theorem 26. We have already argued above that there is a smallest such inference
system , and we shall denote its lower probability function by . First, assume that n  2.
If we denote (n, k) := (n, k + 1)  (n, k), then it follows from the assumptions that
(n, k + 1)  (n, k) for 0  k  n  2.

(99)

We are first going to prove by induction that this implies that
(n, k) 

k
(n, n) for 0  k  n.
n

46. See footnote 35.

90

(100)

fiCoherent Predictive Inference under Exchangeability

Observe that this inequality holds trivially for k = 0 [Theorem 14.L1]. So assume that the
inequality holds for k = `, where `  {0, . . . , n  1}. Then we must show that it also holds
for k = ` + 1. Assume, ex absurdo, that it does not, and therefore
(n, ` + 1) <

`+1
1
(n, n)  (n, `) + (n, n),
n
n

(101)

where the second inequality follows from the induction hypothesis. Now we also have that
(n, n) = (n, ` + 1) +

n1


(n, m)  (n, ` + 1) + (n  `  1)(n, `)

m=`+1

<

`+1
n`1
(n, n) +
(n, n) = (n, n),
n
n

where the first inequality follows from Equation (99), and the second from the first and
second inequalities in Equation (101). This is a contradiction, which completes our proof by
induction of (100).
We infer from (100), Theorem 14.L9 and assumption (76) that
(n, k) 

k n
k
=
for 0  k  n.
nn+s
n+s

(102)

Also observe that this inequality holds trivially for n  {0, 1}. We then get for the predictive
lower prevision P 1A (h|) of any gamble h on A:

[h(x)  min h]P 1A (I{x} |)
P 1A (h|) = min h + P 1A (h  min h|)  min h +
xA

= min h +



[h(x)  min h](n, mx )

xA

 min h +


xA

[h(x)  min h]

mx
= P s,1
IDM,A (h|),
n+s

where the first equality and the first inequality follow from the coherence [P5, P2 and P3]
of P 1A (|), the second equality from representation insensitivity [Equation (33)], and the
second inequality from Equation (102). For the converse inequality, observe that the IDMM
inference system sIDM is coherent, representation insensitive, and specific by Theorem 21,
clearly has concave surprise, satisfies assumption (76), and therefore dominates the smallest
such inference system.

References
Augustin, T., Coolen, F. P. A., De Cooman, G., & Troffaes, M. C. M. (Eds.). (2014).
Introduction to Imprecise Probabilities. John Wiley & Sons.
Bernard, J.-M. (1997). Bayesian analysis of tree-structured categorized data. Revue Internationale de Systmique, 11, 1129.
Bernard, J.-M. (2005). An introduction to the imprecise Dirichlet model for multinomial
data. International Journal of Approximate Reasoning, 39, 123150.
91

fiDe Cooman, De Bock, & Diniz

Bernard, J.-M. (2007). In personal conversation..
Boole, G. (1847, reprinted in 1961). The Laws of Thought. Dover Publications, New York.
Boole, G. (2004, reprint of the work originally published by Watts & Co., London, in 1952).
Studies in Logic and Probability. Dover Publications, Mineola, NY.
Carnap, R. (1952). The continuum of inductive methods. The University of Chicago Press.
Cifarelli, D. M., & Regazzini, E. (1996). De Finettis contributions to probability and statistics.
Statistical Science, 11, 253282.
Couso, I., & Moral, S. (2011). Sets of desirable gambles: conditioning, representation, and
precise probabilities. International Journal of Approximate Reasoning, 52 (7), 1034
1055.
Cozman, F. G. (2013). Independence for full conditional probabilities: Structure, factorization, non-uniqueness, and bayesian networks. International Journal Of Approximate
Reasoning, 54 (9), 12611278.
De Cooman, G., & Miranda, E. (2007). Symmetry of models versus models of symmetry. In
Harper, W. L., & Wheeler, G. R. (Eds.), Probability and Inference: Essays in Honor of
Henry E. Kyburg, Jr., pp. 67149. Kings College Publications.
De Cooman, G., & Miranda, E. (2008a). The F. Riesz Representation Theorem and finite
additivity. In Dubois, D., Lubiano, M. A., Prade, H., Gil, M. A., Grzegorzewski,
P., & Hryniewicz, O. (Eds.), Soft Methods for Handling Variability and Imprecision
(Proceedings of SMPS 2008), pp. 243252. Springer.
De Cooman, G., & Miranda, E. (2008b). Weak and strong laws of large numbers for coherent
lower previsions. Journal of Statistical Planning and Inference, 138 (8), 24092432.
De Cooman, G., & Miranda, E. (2012). Irrelevant and independent natural extension for
sets of desirable gambles.. Journal of Artificial Intelligence Research, 45, 601640.
De Cooman, G., Miranda, E., & Quaeghebeur, E. (2009a). Representation insensitivity in
immediate prediction under exchangeability. International Journal of Approximate
Reasoning, 50 (2), 204216.
De Cooman, G., & Quaeghebeur, E. (2012). Exchangeability and sets of desirable gambles.
International Journal of Approximate Reasoning, 53 (3), 363395. Special issue in
honour of Henry E. Kyburg, Jr.
De Cooman, G., Quaeghebeur, E., & Miranda, E. (2009b). Exchangeable lower previsions.
Bernoulli, 15 (3), 721735.
de Finetti, B. (1937). La prvision: ses lois logiques, ses sources subjectives. Annales de
lInstitut Henri Poincar, 7, 168. English translation by Kyburg Jr. and Smokler
(1964).
de Finetti, B. (1970). Teoria delle Probabilit. Einaudi, Turin.
de Finetti, B. (19741975). Theory of Probability: A Critical Introductory Treatment. John
Wiley & Sons, Chichester. English translation de Finettis (1970) book, two volumes.
Dubins, L. E. (1975). Finitely additive conditional probabilities, conglomerability and
disintegrations. The Annals of Probability, 3, 8899.
92

fiCoherent Predictive Inference under Exchangeability

Geisser, S. (1993). Predictive Inference: An Introduction. Chapman & Hall.
Goldstein, M. (1983). The prevision of a prevision. Journal of the American Statistical
Society, 87, 817819.
Goldstein, M. (1985). Temporal coherence. In Bernardo, J. M., DeGroot, M. H., Lindley,
D. V., & Smith, A. F. M. (Eds.), Bayesian Statistics, Vol. 2, pp. 231248. North-Holland,
Amsterdam. With discussion.
Good, I. J. (1965). The Estimation of Probabilities: An Essay on Modern Bayesian Methods.
The MIT Press.
Haldane, J. B. S. (1945). On a method of estimating frequencies. Biometrika, 33, 222225.
Hausdorff, F. (1923). Momentprobleme fr ein endliches Intervall. Mathematische Zeitschrift,
13, 220248.
Jaynes, E. T. (2003). Probability Theory: The Logic of Science. Cambridge University Press.
Jeffreys, H. (1998). Theory of Probability. Oxford Classics series. Oxford University Press.
Reprint of the third edition (1961), with corrections.
Johnson, N. L., Kotz, S., & Balakrishnan, N. (1997). Discrete Multivariate Distributions.
Wiley Series in Probability and Statistics. John Wiley and Sons, New York.
Johnson, W. E. (1924). Logic, Part III. The Logical Foundations of Science. Cambridge
University Press. Reprinted by Dover Publications in 1964.
Keynes, J. M. (1921). A Treatise on Probability. Macmillan, London.
Koopman, B. O. (1940). The Axioms and Algebra of Intuitive Probability. The Annals of
Mathematics, Second Series, 41 (2), 269292.
Kyburg Jr., H. E., & Smokler, H. E. (Eds.). (1964). Studies in Subjective Probability. Wiley,
New York. Second edition (with new material) 1980.
Lad, F. (1996). Operational Subjective Statistical Methods: A Mathematical, Philosophical
and Historical Introduction. John Wiley & Sons.
Levi, I. (1980). The Enterprise of Knowledge. MIT Press, London.
Mangili, F., & Benavoli, A. (2013). New prior near-ignorance models on the simplex.
In Cozman, F., Denux, T., Destercke, S., & Seidenfeld, T. (Eds.), ISIPTA 13 
Proceedings of the Eighth International Symposium on Imprecise Probability: Theories
and Applications, pp. 213222. SIPTA.
Miranda, E. (2009). Updating coherent lower previsions on finite spaces. Fuzzy Sets and
Systems, 160 (9), 12861307.
Miranda, E., & De Cooman, G. (2014). Introduction to Imprecise Probabilities, chap. Lower
previsions. John Wiley & Sons.
Miranda, E., & Zaffalon, M. (2011). Notes on desirability and conditional lower previsions.
Annals Of Mathematics And Artificial Intelligence, 60 (3-4), 251309.
Moral, S. (2005). Epistemic irrelevance on sets of desirable gambles. Annals of Mathematics
and Artificial Intelligence, 45, 197214.
93

fiDe Cooman, De Bock, & Diniz

Moral, S., & Wilson, N. (1995). Revision rules for convex sets of probabilities. In Coletti,
G., Dubois, D., & Scozzafava, R. (Eds.), Mathematical Models for Handling Partial
Knowledge in Artificial Intelligence, pp. 113128. Plenum Press, New York.
Piatti, A., Zaffalon, M., Trojani, F., & Hutter, M. (2009). Limits of learning about a categorical
latent variable under prior near-ignorance. International Journal Of Approximate
Reasoning, 50 (4), 597611.
Prautzsch, H., Boehm, W., & Paluszny, M. (2002). Bzier and B-Spline Techniques. Springer,
Berlin.
Quaeghebeur, E. (2014). Introduction to Imprecise Probabilities, chap. Desirability. John
Wiley & Sons.
Quaeghebeur, E., De Cooman, G., & Hermans, F. (2014). Accept & reject statement-based
uncertainty models. International Journal of Approximate Reasoning. Accepted for
publication.
Rouanet, H., & Lecoutre, B. (1983). Specific inference in ANOVA: From significance tests
to Bayesian procedures. British Journal of Mathematical and Statistical Psychology,
36 (2), 252268.
Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1995). A representation of partially ordered
preferences. The Annals of Statistics, 23, 21682217. Reprinted in the collection by
Seidenfeld et al. (1999, pp. 69129).
Seidenfeld, T., Schervish, M. J., & Kadane, J. B. (1999). Rethinking the Foundations of
Statistics. Cambridge University Press, Cambridge.
Sheffer, I. M. (1939). Some properties of polynomial sets of type zero. Duke Mathematical
Journal, 5, 590622.
Smith, C. A. B. (1961). Consistency in statistical inference and decision. Journal of the
Royal Statistical Society, Series A, 23, 137.
Troffaes, M. C. M., & De Cooman, G. (2014). Lower Previsions. Wiley.
Trump, W., & Prautzsch, H. (1996). Arbitrary degree elevation of Bzier representations.
Computer Aided Geometric Design, 13, 387398.
Walley, P. (1991). Statistical Reasoning with Imprecise Probabilities. Chapman and Hall,
London.
Walley, P. (1996). Inferences from multinomial data: learning about a bag of marbles. Journal
of the Royal Statistical Society, Series B, 58, 357. With discussion.
Walley, P. (1997). A bounded derivative model for prior ignorance about a real-valued
parameter. Scandinavian Journal of Statistics, 24 (4), 463483.
Walley, P. (2000). Towards a unified theory of imprecise probability. International Journal
of Approximate Reasoning, 24, 125148.
Walley, P., & Bernard, J.-M. (1999). Imprecise probabilistic prediction for categorical data.
Tech. rep. CAF-9901, Laboratoire Cognition et Activites Finalises, Universit de
Paris 8.
94

fiCoherent Predictive Inference under Exchangeability

Williams, P. M. (1975a). Coherence, strict coherence and zero probabilities. In Proceedings
of the Fifth International Congress on Logic, Methodology and Philosophy of Science,
Vol. VI, pp. 2933. Dordrecht. Proceedings of a 1974 conference held in Warsaw.
Williams, P. M. (1975b). Notes on conditional previsions. Tech. rep., School of Mathematical
and Physical Science, University of Sussex, UK. See also the revised journal version by
Williams (2007).
Williams, P. M. (1976). Indeterminate probabilities. In Przelecki, M., Szaniawski, K., &
Wojcicki, R. (Eds.), Formal Methods in the Methodology of Empirical Sciences, pp.
229246. Reidel, Dordrecht. Proceedings of a 1974 conference held in Warsaw.
Williams, P. M. (2007). Notes on conditional previsions. International Journal of Approximate
Reasoning, 44, 366383.
Zabell, S. L. (1982). W. E. Johnsons sufficientness postulate. The Annals of Statistics, 10,
10901099. Reprinted in the collection by Zabell (2005).
Zabell, S. L. (2005). Symmetry and Its Discontents: Essays on the History of Inductive Probability. Cambridge Studies in Probability, Induction, and Decision Theory. Cambridge
University Press, Cambridge, UK.
Zaffalon, M., & Miranda, E. (2013). Probability and time. Artificial Intelligence, 198, 151.

95

fi