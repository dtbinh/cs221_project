Journal of Artificial Intelligence Research 44 (2012) 491-532

Submitted 11/11; published 07/12

Riffled Independence for Efficient Inference
with Partial Rankings
Jonathan Huang

jhuang11@stanford.edu

James H. Clark Center
Stanford University, Stanford CA 94305, USA

Ashish Kapoor

akapoor@microsoft.com

Microsoft Research
One Microsoft Way
Redmond WA 98052-6399, USA

Carlos Guestrin

guestrin@cs.cmu.edu

Gates Hillman Complex, Carnegie Mellon University,
5000 Forbes Avenue, Pittsburgh, PA 15213, USA

Abstract
Distributions over rankings are used to model data in a multitude of real world settings
such as preference analysis and political elections. Modeling such distributions presents
several computational challenges, however, due to the factorial size of the set of rankings
over an item set. Some of these challenges are quite familiar to the artificial intelligence
community, such as how to compactly represent a distribution over a combinatorially large
space, and how to efficiently perform probabilistic inference with these representations.
With respect to ranking, however, there is the additional challenge of what we refer to as
human task complexity  users are rarely willing to provide a full ranking over a long list
of candidates, instead often preferring to provide partial ranking information.
Simultaneously addressing all of these challenges  i.e., designing a compactly representable model which is amenable to efficient inference and can be learned using partial
ranking data  is a difficult task, but is necessary if we would like to scale to problems
with nontrivial size. In this paper, we show that the recently proposed riffled independence
assumptions cleanly and efficiently address each of the above challenges. In particular, we
establish a tight mathematical connection between the concepts of riffled independence and
of partial rankings. This correspondence not only allows us to then develop efficient and
exact algorithms for performing inference tasks using riffled independence based representations with partial rankings, but somewhat surprisingly, also shows that efficient inference
is not possible for riffle independent models (in a certain sense) with observations which do
not take the form of partial rankings. Finally, using our inference algorithm, we introduce
the first method for learning riffled independence based models from partially ranked data.

1. Probabilistic Modeling of Ranking Data: Three Challenges
Rankings arise in a number of machine learning application settings such as preference analysis for movies and books (Lebanon & Mao, 2008) and political election analysis (Gormley
& Murphy, 2007; Huang & Guestrin, 2010). In many of these problems, it is of great interest
to build statistical models over ranking data in order to make predictions, form recommendations, discover latent trends and structure and to construct human-comprehensible data
summaries.
c
2012
AI Access Foundation. All rights reserved.

fiHuang, Kapoor & Guestrin

Modeling distributions over rankings is a difficult problem, however, due to the fact that
as the number of items being ranked increases, the number of possible rankings increases
factorially. This combinatorial explosion forces us to confront three central challenges when
dealing with rankings. First, we need to deal with storage complexity  how can we compactly represent a distribution over the space of rankings?1 Then there is algorithmic complexity  how can we efficiently answer probabilistic inference queries given a distribution?
Finally, we must contend with what we refer to as human task complexity, which is a
challenge stemming from the fact that it can be difficult to accurately elicit a full ranking over
a large list of candidates from a human user; choosing from a list of n! options is no easy task
and users typically prefer to provide partial information. Take the American Psychological
Association (APA) elections, for example, which allow their voters to rank order candidates
from favorite to least favorite. In the 1980 election, there were five candidates, and therefore
5! = 120 ways to rank those five candidates. Despite the small candidate list, most voters
in the election preferred to only specify their top-k favorite candidates rather than writing
down full rankings on their ballots (see Figure 1). For example, roughly a third of voters
simply wrote down their single favorite candidate in this 1980 election.
These three intertwined challenges of storage, algorithmic, and human task complexity
are the central issues of probabilistic modeling for rankings, and models that do not efficiently
handle all three sources of complexity have limited applicability. In this paper, we examine
a flexible and intuitive class of models for rankings based on a generalization of probabilistic
independence called riffled independence, proposed in our recent work (Huang & Guestrin,
2009, 2010). While our previous papers have focused primarily on representational (storage
complexity) issues, we now concentrate on inference and incomplete observations (i.e., partial
rankings), showing that in addition to storage complexity, riffle independence based models
can efficiently address issues of algorithmic and human task complexity.
In fact the two issues of algorithmic and human task complexity are intricately linked
for riffle independent models. By considering partial rankings, we give users more flexibility
to provide as much or as little information as they care to give. In the context of partial
ranking data, the most relevant inference queries also take the form of partial rankings. For
example, we might want to predict a voters second choice candidate given information about
his first choice. One of our main contributions in this paper is to show that inference for
such partial ranking queries can be performed particularly efficiently for riffle independent
models.
The main contributions of our work are as follows:2
 We reveal a natural and fundamental connection between riffle independent models
and partial rankings. In particular, we show that the collection of partial rankings
over an item set form a complete characterization of the space of observations upon
1. Note that it is common to wonder why one would care to represent a distribution over all rankings if the
number of sample rankings is never nearly as large. This problem that the number of samples is always
much smaller than n! however, means that most rankings are never observed, limiting our ability to
estimate the probability of an arbitrary ranking. The only way to overcome the paucity of samples is to
exploit representational structure, which is very much in alignment with solving the storage complexity
issue.
2. This paper is an extended presentation of our paper (Huang, Kapoor, & Guestrin, 2011) which appeared
in the 2011 Conference on Uncertainty in Artificial Intelligence (UAI) as well as results from the first
authors dissertation (Huang, 2011).

492

fiEfficient Inference With Partial Rankings

First
Choice

Second
Choice

Third
Choice

Fourth
Choice

Fifth
Choice

# of
votes

5

3

4

2

1

37

3

4

5

1

2

30

1

2

3

---

---

27

3

---

---

---

---

1198

4

1

3

---

---

15

1

3

---

---

---

302

3

1

2

5

4

186

Figure 1: Example partial ranking data (taken from the American Psychological Association
election dataset, 1980)

which one can efficiently condition a riffle independent model. As a result, we show
that when ranked items satisfy the riffled independence relationship, conditioning on
partial rankings can be done efficiently, with running time O(n|H|), where |H| denotes
the number of model parameters.
 We prove that, in a sense (which we formalize), it is impossible to efficiently condition
riffle independent models on observations that do not take the form of partial rankings.
 We propose the first algorithm that is capable of efficiently estimating the structure
and parameters of riffle independent models from heterogeneous collections of partially
ranked data.
 We show results on real voting and preference data evidencing the effectiveness of our
methods.

2. Riffled Independence For Rankings
A ranking, , of items in an item set  is a one-to-one mapping between  and a rank
set R = {1, . . . , n} and is denoted using vertical bar notation as  1 (1)| 1 (2)| . . . | 1 (n).
We say that  ranks item i1 before (or over) item i2 if the rank of i1 is less than the
rank of i2 . For example,  might be {Corn, P eas, Apples, Oranges} and the ranking
Corn|P eas|Apples|Oranges encodes a preference of Corn over Peas which is in turn preferred over Apples and so on. The collection of all possible rankings of item set  is denoted
by S (or just Sn when  is implicit).
Since there are n! rankings of n items, it is intractable to estimate or even explicitly
represent arbitrary distributions on Sn without making structural assumptions about the
underlying distribution. While there are many possible simplifying assumptions that one can
make, we focus on an approach that we have proposed in recent papers (Huang & Guestrin,
2009, 2010) in which the ranks of items are assumed to satisfy an intuitive generalized notion
of probabilistic independence known as riffled independence. In this paper, we argue that
riffled independence assumptions are particularly effective in settings where one would like
to make queries taking the form of partial rankings. In the remainder of this section, we
review riffled independence.
493

fiHuang, Kapoor & Guestrin

The riffled independence assumption posits that rankings over the item set  are generated by independently generating rankings of smaller disjoint item subsets (say, A and
B) which partition , and piecing together a full ranking by interleaving (or riffle shuffling)
these smaller rankings together. For example, to rank our item set of foods, one might first
rank the vegetables and fruits separately, then interleave the two subset rankings to form a
full ranking. To formally define riffled independence, we use the notions of relative rankings
and interleavings.
Definition 1 (Relative ranking map). Given a ranking   S and any subset A  , the
relative ranking of items in A, A (), is a ranking,   SA , such that (i) < (j) if and
only if (i) < (j).
Definition 2 (Interleaving map). Given a ranking   S and a partition of  into disjoint
sets A and B, the interleaving of A and B in  (denoted, AB ()) is a (binary) mapping
from the rank set R = {1, . . . , n} to {A, B} indicating whether a rank in  is occupied by A
or B. As with rankings, we denote the interleaving of a ranking by its vertical bar notation:
[AB ()](1)|[AB ()](2)| . . . |[AB ()](n).
Example 3. Consider a partitioning of an item set  into vegetables A = {Corn, P eas}
and fruits B = {Apples, Oranges}, as well as a full ranking over these four items:  =
Corn|Oranges|P eas|Apples. In this case, the relative ranking of vegetables in  is A () =
Corn|P eas and the relative ranking of fruits in  is B () = Oranges|Apples. The interleaving of vegetables and fruits in  is AB () = A|B|A|B.
Definition 4 (Riffled Independence). Let h be a distribution over S and consider a subset
of items A   and its complement B. The sets A and B are said to be riffle independent
if h decomposes (or factors) as:
h() = mAB (AB ())  fA (A ())  gB (B ()),
for distributions mAB , fA and gB , defined over interleavings and relative rankings of A and
B respectively. In other words, A and B are riffle independent if the relative rankings of
A and B, as well as their interleaving are mutually independent. We refer to mAB as the
interleaving distribution and fA and gB as the relative ranking distributions.
Riffled independence has been found to approximately hold in a number of real datasets
(Huang & Guestrin, 2012). When such relationships can be identified in data, then instead
of exhaustively representing all n! ranking probabilities, one can represent just the factors
mAB , fA and gB , which are distributions over smaller sets.
2.1 Hierarchical Riffle Independent Models
The relative ranking factors fA and gB are themselves distributions over rankings. To further
reduce the parameter space, it is natural to consider hierarchical decompositions of item sets
into nested collections of partitions (like hierarchical clustering). For example, Figure 2.1
shows a hierarchical decomposition where vegetables are riffle independent of fruits among
the healthy foods, and these healthy foods are, in turn, riffle independent of the subset of
desserts: {Doughnuts, M &M s}.
494

fiEfficient Inference With Partial Rankings

{C,P,A,O,D,M}

{D,M}

{C,P,A,O}

Doughnuts, M&Ms

{C,P}

{A,O}

Corn, Peas

Apples, Oranges

Figure 2: An example of a hierarchy over six food items.
For simplicity, we restrict consideration to binary hierarchies, defined as tuples of the
form H = (HA , HB ), where HA and HB are either (1) null, in which case H is called a leaf,
or (2) hierarchies over item sets A and B respectively. In this second case, A and B are
assumed to form a nontrivial partitioning of the item set.
Definition 5. We say that a distribution h factors riffle independently with respect to a
hierarchy H = (HA , HB ) if item sets A and B are riffle independent with respect to h,
and both fA and gB factor riffle independently with respect to subhierarchies HA and HB ,
respectively.
Like Bayesian networks, these hierarchies represent families of distributions obeying a
certain set of (riffled) independence constraints and can be parameterized locally. To draw
from such a model, one generates full rankings recursively starting by drawing rankings of
the leaf sets, then working up the tree, sequentially interleaving rankings until reaching the
root. The parameters of these hierarchical models are simply the interleaving and relative
ranking distributions at the internal nodes and leaves of the hierarchy, respectively.
In general, the number of total parameters required to represent a hierarchical riffle
independent model can (as with Bayesian networks) still scale exponentially in the number

of items. For example, the number of interleavings of p items with n  p items is np .
It is often the case however, that much fewer parameters are necessary. For example, thin
models (Huang & Guestrin, 2012), in which the number of items factored out of the model at
each stage of the hierarchy is never more than a small constant k, can always be represented
with a (degree k) polynomial number of parameters. We will use |H| to refer to the number
of parameters necessary for representing a distribution which factors according to hierarchy
H.
By decomposing distributions over rankings into small pieces (like Bayesian networks
have done for other distributions), these hierarchical models allow for better interpretability,
efficient probabilistic representation, low sample complexity, efficient MAP optimization,
and, as we show in this paper, efficient inference.
Example 6. In Figure 3(a), we reproduce the hierarchical structure that was learned using
a fully ranked subset of the APA data consisting of 5000 training examples in Huang and
Guestrin (2012). There were five candidates in the election: (1) William Bevan, (2) Ira
Iscoe, (3) Charles Kiesler, (4) Max Siegle, and (5) Logan Wright (Marden, 1995). Strikingly,
the structure that is learned using an algorithm (maximum likelihood) which knows nothing
about the underlying politics of the APA, has leaf nodes which correspond exactly to the
political coalitions that dominated the APA in the 1980 election  the research psychologists
495

fiHuang, Kapoor & Guestrin

A={12345}

B={1345}

C={2}
Community
psychologists

mB,C()
.14

B|C|B|B|B

.19

B|B|C|B|B

.25

B|B|B|C|B

.25

B|B|B|B|C

.18



mD,E()



fC()

D|D|E|E

.28

2

1.00

D|E|D|E

.12

D|E|E|D

.12

D={13}

E={45}

E|D|D|E

.14

Research
psychologists

Clinical
psychologists

E|D|E|D

.12

E|E|D|D

.22

(a) Hierarchical structure learned
via MLE using 5000 full rankings
from the APA dataset.


C|B|B|B|B



fD()



fE()

1|3

.50

4|5

.48

3|1

.50

5|4

.52

(b) Riffle independent model parameters learned via MLE using
5000 full rankings from the APA dataset.

Figure 3: Example hierarchical model for the APA election. Candidates are enumerated
as: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, and (5)
Logan Wright (Marden, 1995).

(candidates 1 and 3), the clinical psychologists (candidates 4 and 5), and the community
psychologists (candidate 2).
In Figure 3(b), we plot the corresponding parameter distributions that are learned via
maximum likelihood. There are three relative ranking distributions, each corresponding to a
political party, as well as two interleaving distributions (one for the interleaving of research
and clinical psychologists, and one for the interleaving of the community psychologist and all
remaining candidates). Since each parameter distribution is constrained to sum to 1, there
are a total of 11 free parameters.
2.2 Model Estimation
In this paper we estimate riffle independent models based on the methods introduced in our
earlier work. Given the hierarchial structure of a model, the maximum likelihood parameter
estimates of a hierarchical riffle independent model are straightforward to compute via frequency estimates. But how to estimate the correct structure of a model is a more challenging
problem. The key insight lies in noticing that if two subsets A and B are riffle independent,
then for any i  A and j, k  B, the independence relation (i)  ((j) < (k)) must hold.
Our structure learning algorithms operate by hunting for these tripletwise independence
relations within the data. We defer interested readers to the details in (Huang & Guestrin,
2012).
496

fiEfficient Inference With Partial Rankings

Note that in our earlier work, we assumed that our algorithms have access to a dataset
consisting of i.i.d. full rankings provided by users. In the current work, we will relax our
assumptions by allowing for users to provide partially ranked data. One assumption throughout, however, is that each user has a full ranking in mind over the items. In particular, our
current work does not address the incomplete ranking problem, in which users might not
have seen all of the items (we discuss possible extensions to the incomplete ranking setting
in Section 9.

3. Decomposable Observations
Given a prior distribution, h, over rankings and an observation O, Bayes rule tells us that
the posterior distribution, h(|O), is proportional to L(O|)  h(), where L(O|) is the
likelihood function. This operation of conditioning h on an observation O is typically computationally intractable since it requires multiplying two n! dimensional functions, unless
one can exploit structural decompositions of the problem. In this section, we describe a decomposition for a certain class of likelihood functions over the space of rankings in which the
observations are factored into simpler parts. When an observation O is decomposable in
this way, we show that one can efficiently condition a riffle independent prior distribution on
O. For simplicity in this paper, we focus primarily on subset observations whose likelihood
functions encode membership with some subset of rankings in Sn .
Definition 7 (Subset observations). A subset observation O is a binary observation whose
likelihood is proportional to the indicator function of some subset of Sn  i.e.,

1 if   O
.
L(O|) =
0 otherwise
As a running example, we will consider the class of first place observations throughout
the chapter (we will consider far more general observation models in later sections). The first
place observation O =Corn is ranked first, for example, is associated with the collection of
rankings placing the item Corn in first place (O = { : (Corn) = 1}). We are interested in
computing the posterior h(|  O). Thus in the first place scenario, we are given a voters
top choice and we would like to infer his preferences over the remaining candidates.
Given a partitioning of the item set  into two subsets A and B, it is sometimes possible
to decompose (or factor ) a subset observation involving items in  into smaller subset observations involving A, B and the interleavings of A and B independently. Such decompositions
can often be exploited for efficient inference.
Example 8.
 Consider the first place observation
O = Corn is ranked first,
which can be decomposed into two independent observations  an observation on the
relative ranking of Vegetables, and an observation on the interleaving of Vegetables and
Fruits:
497

fiHuang, Kapoor & Guestrin

 OA = Corn is ranked first among Vegetables,
 OA,B = First place is occupied by a Vegetable.
To condition on O in this case, one updates the relative ranking distribution over
Vegetables (A) by zeroing out rankings of vegetables which do not place Corn in first
place, and updates the interleaving distribution by zeroing out interleavings which do
not place a Vegetable in first place, then normalizes the resulting distributions.
 An example of a nondecomposable observation is the observation
O = Corn is in third place.
To see that O does not decompose (with respect to Vegetables and Fruits), it is enough
to notice that the interleaving of Vegetables and Fruits is not independent of the relative
ranking of Vegetables. If, for example, an element   O interleaves A (Vegetables)
and B (Fruits) as AB () = A|B|A|B, then since (Corn) = 3, the relative ranking
of Vegetables is constrained to be A () = P eas|Corn. Since the interleavings and
relative rankings are not independent, we see that O cannot be decomposable.
Formally, we use riffle independent factorizations to define decomposability with respect
to a hierarchy H of the item set.
Definition 9 (Decomposability). Given a hierarchy H over the item set, a subset observation O decomposes with respect to H if its likelihood function L(O|) factors riffle
independently with respect to H.
When subset observations and the prior decompose according to the same hierarchy, we
can show (as in Example 8) that the posterior also decomposes.
Proposition 10. Let H be a hierarchy over the item set. Given a prior distribution h and
a subset observation O which both decompose with respect to H, the posterior distribution
h(|O) also factors riffle independently with respect to H.
Proof. Denote the likelihood function corresponding to O by L (in this proof, it does not
matter that O is assumed to be a subset observation  the result holds for arbitrary
likelihoods).
We use induction on the size of the item set n = ||. The base case n = 1 is trivially
true. Next consider the general case where n > 1. The posterior distribution, by Bayes rule,
can be written h(|O)  L()  h(). There are now two cases. If H is a leaf node, then
the posterior h0 trivially factors according to H, and we are done. Otherwise, L and h both
factor, by assumption, according to H = (HA , HB ) in the following way:
L() = mL (AB ())fL (A ())gL (B ()), and h() = mh (AB ())fh (A ())gh (B ()).
Multiplying and grouping terms, we see that the posterior factors as:
h(|O) = [mL  mh ](AB ())  [fL  fh ](A ())  [gL  gh ](B ()).
To show that h(|O) factors with respect to H, we need to demonstrate (by Definition 5)
that the distributions [fL  fh ] and [gL  gh ] (after normalizing) factor with respect to HA and
498

fiEfficient Inference With Partial Rankings

HB , respectively. Since fL and fh both factor according to the hierarchy HA by assumption
and |A| < n since H is not a leaf, we can invoke the inductive hypothesis to show that the
posterior distribution, which is proportional to fL  fh must also factor according to HA .
Similarly, the distribution proportional to gL  gh must factor according to HB .

4. Complete Decomposability
The condition of Proposition 10, that the prior and observation must decompose with respect to exactly the same hierarchy, is a sufficient one for efficient inference, but it might at
first glance seem so restrictive as to render the proposition useless in practice. To overcome
this limitation of hierarchy specific decomposability, we explore a special family of observations (which we call completely decomposable) for which the property of decomposability
does not depend specifically on a particular hierarchy, implying in particular that for these
observations, efficient inference is always possible (provided that efficient representation of
the prior distribution is also possible).
To illustrate how an observation can decompose with respect to multiple hierarchies over
the item set, consider again the first place observation O =Corn is ranked first. We argued
in Example 8 that O is a decomposable observation. Notice however that decomposability
for this particular observation does not depend on how the items are partitioned by the
hierarchy. Specifically, if instead of Vegetables and Fruits, the sets A = {Corn, Apples} and
B = {P eas, Oranges} are riffle independent, a similar decomposition of O would continue
to hold, with O decomposing as an observation on the relative ranking of items in A (Corn is
first among items in A), and an observation on the interleaving of A and B (First place is
occupied by some element of A).
To formally capture this notion that an observation can decompose with respect to
arbitrary underlying hierarchies, we define complete decomposability:
Definition 11 (Complete decomposability). We say that a subset observation O is completely decomposable if it decomposes with respect to every possible hierarchy over the item
set . We denote the collection of all possible completely decomposable (subset) observations as C. See Figure 4 for an illustration of the set C.
Conceptually, completely decomposable observations correspond to indicator functions
that are as riffle independent as possible. Complete decomposability is a guarantee for an
observation O that one can always exploit any available factorized structure of the prior
distribution in order to efficiently condition on O.
Proposition 12. Let H be any binary hierarchy over the item set. Given a prior h which
factorizes with respect to H, and a completely decomposable observation O, the posterior
h(|O) also decomposes with respect to H.
Proof. Proposition 12 follows as a simple corollary to Proposition 10.
Example 13. The simplest example of a completely decomposable observation is the uniform observation Ounif = S , which includes all possible rankings and corresponds to a
uniform indicator function unif over rankings. Given any hierarchy H, unif can be shown
to decompose riffle independently with respect to H, where each factor is also uniform, and
hence Ounif is completely decomposable.
499

fiHuang, Kapoor & Guestrin

H1

H6

H2

Completely
Decomposable
Observations

H5

H3

H4

Figure 4: A diagram illustrating the collection of completely decomposable observations, C.
Each shaded region (labeled Hi ) above represents the family of subset observations
over Sn which decompose with respect to the hierarchy Hi . The collection C can be
seen as the intersection over all such shaded regions, and subset observations which
lie inside of this intersection are ones for which conditioning can be performed in
linear time (in the number of model parameters).

The uniform observation is of course not particularly interesting in the context of Bayesian
inference, but on the other hand, given the stringent conditions in Definition 11, it is not
obvious that nontrivial completely decomposable observations can even exist. Nonetheless,
there do exist nontrivial examples (such as the first place observations), and in the next
section, we exhibit a rich and general class of completely decomposable observations.

5. Complete Decomposability of Partial Ranking Observations
In this section we discuss the mathematical problem of fully characterizing the class of
completely decomposable observations. Our main contribution in this section is to show
that completely decomposable observations correspond precisely to partial rankings of the
item set.
Partial rankings. We begin our discussion by introducing partial rankings, which allow
for items to be tied with respect to a ranking  by dropping verticals from the vertical bar
representation of .
Definition 14 (Partial ranking observation). Let 1 , 2 ,. . . , r be an ordered collection
of subsets which partition  (i.e., i i =  and i  j =  if i 6= j). The partial ranking
observation 3 corresponding to this partition is the collection of rankings which rank items
3. As remarked by Ailon (2007), we note that The term partial ranking used here should not be confused
with two other standard objects: (1) Partial order, namely, a reflexive, transitive anti-symmetric binary

500

fiEfficient Inference With Partial Rankings

in i before items in j if i < j. We denote this partial ranking as 1 |2 | . . . |r and say
that it has type  = (|1 |, |2 |, . . . , |r |). We denote the collection of all partial rankings
(over n items) as P.
Each partial ranking as defined above can be viewed as a coset of the subgroup S =
S1  S2      Sr . Given the type  and any full ranking   S , there is only one
partial ranking of type  containing , thus we will therefore equivalently denote the partial
ranking 1 |2 | . . . |r as S , where  is any element of 1 |2 | . . . |r . Note that this coset
notation allows for multiple rankings  to refer to the same partial ranking S .
The space of partial rankings as defined above captures a rich and natural class of
observations. In particular, partial rankings encompass a number of commonly occurring
special cases, which have traditionally been modeled in isolation, but in our work (as well
as recent works such as Lebanon & Lafferty, 2003; Lebanon & Mao, 2008) can be used in a
unified setting.
Example 15. Partial ranking observations include:
 (First place, or Top-1 observations): First place observations correspond to partial
rankings of type  = (1, n  1). The observation that Corn is ranked first can be
written as Corn|Peas,Apples,Oranges.
 (Top-k observations): Top-k observations are partial rankings with type  = (1, . . . , 1, n
k). These generalize the first place observations by specifying the items mapping to the
first k ranks, leaving all n  k remaining items implicitly ranked behind. For example,
the observation that Corn is ranked first and Peas is ranked second can be written as
Corn|Peas|Apples,Oranges.
 (Desired/less desired dichotomy): Partial rankings of type  = (k, n  k) correspond to
a subset of k items being preferred or desired over the remaining subset of n  k items.
For example, partial rankings of type (k, n  k) might arise in approval voting in which
voters mark the subset of approved candidates, implicitly indicating disapproval of the
remaining n  k candidates.
 (Ratings): Finally, partial rankings can come in the form of rating data where, for
example, restaurants are rated as, ?, ??, or ? ? ?. A corresponding partial ranking
would thus tie restaurants that are rated with the same number of stars, while ranking
restaurants with more stars above restaurants with fewer stars.
 (Trivial observations): Partial rankings of type  = (n) refer to trivial observations
whose likelihood functions are uniform on the entire space of rankings, S . The trivial
observation for rankings of the item set  = {Corn, P eas, Apples}, for example, can
simply be written simply as Corn, P eas, Apples.
To show how partial ranking observations decompose, we will exhibit an explicit factorization with respect to a hierarchy H over items. For simplicity, we begin by considering the
single layer case, in which the items are partitioned into two leaf sets A and B. Our factorization depends on the following notions of consistency of relative rankings and interleavings
with a partial ranking.
relation; and (2) A ranking of a subset of  [which we discuss in Section 9 as incomplete rankings]. In
search engines, for example, although only the top-k elements of  are returned, the remaining n  k
are implicitly assumed to be ranked behind [and therefore, search engines return partial rankings].

501

fiHuang, Kapoor & Guestrin

Definition 16 (Restriction consistency). Given a partial ranking S  = 1 |2 | . . . |r and
any subset A  , we define the restriction of S  to A as the partial ranking on items in
A obtained by intersecting each i with A. Hence the restriction of S  to A is:
[S ]A = 1  A|2  A| . . . |r  A.
Given a ranking, A of items in A, we say that A is consistent with the partial ranking
S  if A is a member of the restriction of S  to A, [S ]A .
Definition 17 (Interleaving consistency). Given an interleaving AB of two sets A, B which
partition , we say that AB is consistent with a partial ranking S  = 1 | . . . |r (with
type ) if the first 1 entries of AB contain the same number of As and Bs as 1 , and the
second 2 entries of AB contain the same number of As and Bs as 2 , and so on. Given a
partial ranking S , we denote the collection of consistent interleavings as [S ]AB .
For example, consider the partial ranking
S  = Corn, Apples|P eas, Oranges,
which places a single vegetable and a single fruit in the first two ranks, and a single vegetable
and a single fruit in the last two ranks. Alternatively, S  partially specifies an interleaving
AB|AB. The full interleavings A|B|B|A and B|A|B|A are consistent with S  (by dropping
vertical lines) while A|A|B|B is not consistent (since it places two vegetables in the first two
ranks).
Using the notions of consistency with a partial ranking, we show that partial ranking
observations are decomposable with respect to any binary partitioning (i.e., single layer
hierarchy) of the item set.
Proposition 18 (Single layer hierarchy). For any partial ranking observation S  and any
binary partitioning of the item set (A, B), the indicator function of S , S  , factors riffle
independently as:
S  () = mAB (AB ())  fA (A ())  gB (B ()),

(5.1)

where the factors mAB , fA and gB are the indicator functions for consistent interleavings
and relative rankings, [S ]AB , [S ]A and [S ]B , respectively.
The single layer decomposition of Proposition 18 can be turned into a recursive decomposition for partial ranking observations over arbitrary binary hierarchies, which establishes
our main result. In particular, given a partial ranking S  and a prior distribution which
factorizes according to a hierarchy H, we first condition the topmost interleaving distribution by zeroing out all parameters corresponding to interleavings which are not consistent
with S , and normalizing the distribution. We then need to condition the subhierarchies
HA and HB on relative rankings of A and B which are consistent with S , respectively.
Since these consistent sets, [S ]A and [S ]B , are partial rankings themselves, the same
algorithm for conditioning on a partial ranking can be applied recursively to each of the
subhierarchies HA and HB . To be precise, we show that:
Theorem 19. Every partial ranking is completely decomposable (P  C).
502

fiEfficient Inference With Partial Rankings

prcondition (Prior hprior , Hierarchy H, Observation S  = 1 |2 | . . . |r )
if isLeaf(H) then
forall  do

hprior () if   S 
hpost () 
;
0
otherwise
Normalize (hpost ) ;
return (hpost );
else
forall  do

mprior ( ) if   [S ]AB
mpost ( ) 
;
0
otherwise
Normalize (mpost ) ;
f (A ) prcondition (fprior , HA , [S ]A ) ;
g(B ) prcondition (gprior , HB , [S ]B ) ;
return (mpost , fpost , gpost );

Algorithm 1: Pseudocode for prcondition, an algorithm for recursively conditioning a hierarchical riffle independent prior distribution on partial ranking observations. See Definitions 16
and 17 for [S ]A , [S ]B , and [S ]AB . The runtime of prcondition is O(n  |H|), where |H|
is the number of model parameters. Input: All parameter distributions of the prior hprior represented in explicit tabular form, and an observation S  in the form of a partial ranking. Output:
All parameter distributions of the posterior hpost represented in explicit tabular form.

Since the proof of Theorem 19 is fairly straight forward given the form of the factorization (Equation 5.1), it is deferred to the Appendix. As a consequence of Theorem 19 and
Proposition 12, conditioning on partial ranking observations can be performed efficiently.
See Algorithm 1 for details on our recursive conditioning algorithm.
What is the running time complexity of conditioning on a partial ranking? The recursion
of Algorithm 1 operates on each parameter distribution once, setting the probabilities of
the interleavings or relative rankings in each such distribution to either zero or not, then
normalizing. To decide whether to zero out a probability or not, one must check a partial
ranking for consistency against either an interleaving or relative ranking, which requires at
most O(n) time. Therefore, in total, Algorithm 1 requires O(n  |H|) time, where |H| is
the total number of model parameters. Notice that the complexity of conditioning depends
linearly on the complexity of the prior  whenever the prior distribution can be compactly
represented, efficient inference for partial ranking observations is also possible. As we have
stated in Section 2, |H| can in general scale exponentially in n, but for thin chain models,
in which the number of items factored out of the model at each stage is never more than
a small constant k, verifying interleaving or relative ranking consistency can be performed
in constant time, implying that the conditioning operation is linear in the number of model
parameters, and guaranteed to be polynomial in n.
Example 20. In this example, we consider conditioning the APA distribution from Example 6 on the observation O that Candidate 3 is ranked in first place, which can also
be represented as the partial ranking O = 3|1, 2, 4, 5. Recall that candidate 3 was Charles
Kiesler, who was a research psychologist.
In Figure 5(a) we show again the structure and parameters of the prior distribution for
the APA election data, highlighting in particular the interleavings and relative rankings which
503

fiHuang, Kapoor & Guestrin



mB,C()



C|B|B|B|B

.14

C|B|B|B|B

mB,C()
0

B|C|B|B|B

.19

B|C|B|B|B

.22

B|B|C|B|B

.25

B|B|C|B|B

.29

B|B|B|C|B

.25

B|B|B|C|B

.29

B|B|B|B|C

.18

B|B|B|B|C

.21



mD,E()



fC()



mD,E()



fC()

D|D|E|E

.28

2

1.00

D|D|E|E

.54

2

1.00

D|E|D|E

.12

D|E|D|E

.23

D|E|E|D

.12

D|E|E|D

.23

E|D|D|E

.14

E|D|D|E

0

E|D|E|D

.12

E|D|E|D

0

E|E|D|D

.22

E|E|D|D

0



fD()



fE()



fD()



fE()

1|3

.50

4|5

.48

1|3

0

4|5

.48

3|1

.50

5|4

.52

3|1

1.00

5|4

.52

(a) Structure and parameters of the prior distribution (with consistent relative rankings and interleavings highlighted).

(b) Structure and parameters of the posterior distribution after conditioning.

Figure 5: Example of conditioning the APA hierarchy (from Example 6) on the first place
observation that Candidate 3 is ranked in first place.

are consistent with O. For example, of the possible interleavings of research psychologists
(D) with clinical psychologists (E), the interleavings that are consistent with O are those
which rank a research psychologist first among the research and clinical psychologists. There
are therefore only three consistent interleavings: D|D|E|E, D|E|D|E, and D|E|E|D.
Conditioning on O sets all relative rankings and interleavings which are not consistent
with O to zero and normalizes each resulting parameter distribution. The resulting riffle
independent representation of the posterior distribution is shown in Figure 5(b).

5.1 An Impossibility Result
It is interesting to consider what completely decomposable observations exist beyond partial
rankings. One of our main contributions is to show that there are no such observations.
Theorem 21 (Converse of Theorem 19). Every completely decomposable observation takes
the form of a partial ranking (C  P).
Together, Theorems 19 and 21 form a significant insight into the nature of rankings,
showing that the notions of partial rankings and riffled independence are deeply connected.
In fact, our result shows that it is even possible to define partial rankings via complete
decomposability!
As a practical matter, Theorem 21 shows that there is no algorithm based on simple
multiplicative updates to the parameters which can exactly condition on observations which
do not take the form of partial rankings. The computational complexity of conditioning on
observations which are not partial rankings remains open. We conjecture that approximate
inference approaches may be necessary for efficiently handling more complex observations.
504

fiEfficient Inference With Partial Rankings

5.2 Proof of the Impossiblity Result (Theorem 21)
We now turn to proving Theorem 21. Since this proof is significantly longer and less obvious
than the proof for its converse (Theorem 19), we sketch the main ideas that drive the proof
here and refer interested readers to details in the Appendix.
Recall that the definition of the linear span of a set of vectors in a vector space is the
intersection of all linear subspaces containing that set of vectors. To prove Theorem 21, we
introduce analogous concepts of the span of a set of rankings.
Definition 22 (rspan and pspan). Let X  Sn be any collection of rankings. We define
pspan(X) to be the intersection of all partial rankings containing X. Similarly, we define
rspan(X) to be the intersection of all completely decomposable observations containing X.
More formally,
\
\
pspan(X) =
S , and rspan(X) =
O.
O:XO, OC

S :XS 

For example, if X = {Corn|P eas|Apples, Apples|P eas|Corn}, it can be checked that
the only partial ranking of all three items containing both items of X is the entire set itself.
Thus pspan(X) = Corn, P eas, Apples.
Our proof strategy is to establish two claims: (1) that the pspan of any set is always
a partial ranking, and (2) that in fact, the rspan and pspan of a set X are exactly the
same sets. Since claim (1) is a fact about partial rankings and does not involve riffled
independence, we defer all related proofs to the Appendix. Thus we have:
Lemma 23. For any X  Sn , pspan(X) is a partial ranking.
Proof. See Appendix.
The following discussion will instead sketch a proof of claim (2). We first show, however,
that Theorem 21 must hold if it is indeed true that claims (1) and (2) hold.
Proof. (of Theorem 21): Given some O  C, we want to show that O  P. By claim
(2), rspan(O) = pspan(O). Since O is an element of C, however, we also have that
O = rspan(O), and thus that O = pspan(O). Finally Lemma 23 (claim (2)) guarantees
that pspan(O) is a partial ranking, and so we conclude that O  P.
We now proceed to establish the claim that rspan(X) = pspan(X). The following
proposition lists several basic properties of the rspan that we will use in several of the
proofs. They all follow directly from definition so we do not write out the proofs.
Proposition 24.
I. (Monotonicity) For any X, X  rspan(X).
II. (Subset preservation) For any X, X 0 such that X  X 0 , rspan(X)  rspan(X 0 ).
III. (Idempotence) For any X, rspan(rspan(X)) = rspan(X).
One inclusion of our proof that rspan(X) = pspan(X) follows directly from the fact
that P  C (Theorem 19):
505

fiHuang, Kapoor & Guestrin

formPspan(X)
X0  X; t  0;
while S , S 0  0  Xt which disagree on the relative ordering of items a1 , a2 do
Xt   ;
foreach S   Xt do
Add any partial ranking obtained by deleting a vertical bar from S  between items
a1 and a2 to Xt ;
t  t + 1;
return (any element of Xt ) ;

Algorithm 2: Pseudocode for computing pspan(X). formPspan(X) takes a set of partial
rankings (or full rankings) X as input and outputs a partial ranking. This algorithm iteratively
deletes vertical bars from elements of X until they are in agreement. Note that it is not necessary
to keep track of t, but we do so here to ease notation in the proofs. Nor is this algorithm the most
direct way of computing pspan(X), but again, it simplifies the proof of our main theorem.

Lemma 25. For any subset of orderings, X, rspan(X)  pspan(X).
Proof. Fix a subset X  Sn and let  be any element of rspan(X). We would like to show
 to be an element of pspan(X). Consider any partial ranking S   P which covers X
(i.e.,  0  S  for all  0  X). We want to see that   S . By Theorem 19, P  C,
and therefore, S   C. Since   rspan(X), and  0  S  for all  0  X, we conclude,
by definition of rspan, that   S . Since this holds for any partial ranking covering X,
  pspan(X).
What remains is the task of establishing the reverse inclusion:
Proposition 26. For any subset of orderings, X, rspan(X)  pspan(X).
To prove Proposition 26, we consider the problem of computing the partial ranking span
(pspan) of a given set of rankings X. In Algorithm 2, we show a simple procedure based
on iteratively finding rankings in X which disagree on the pairwise ranking of two items,
and replacing those rankings by a partial ranking in which a vertical bar between those two
elements have been removed. We show that this algorithm provably outputs the correct
result.
Proposition 27. Given a set of rankings X as input, Algorithm 2 outputs pspan(X).
Proof. See Appendix.
As a final step before being able to prove Proposition 26, we prove the following two
technical lemmas which relate the computation of the pspan in Algorithm 2 to riffled independence, and really form the heart of our argument. In particular, for a completely
decomposable observation O  C, Lemma 28 below shows how a ranking contained in O
can force other rankings to also be contained in O.
Lemma 28. Let O  C and suppose there exist 1 , 2  O which disagree on the relative
ranking of items i, j  . Then the ranking obtained by swapping the relative ranking of
items i, j within any 3  O must also be contained in O.
506

fiEfficient Inference With Partial Rankings

Proof. Let h be the indicator distribution corresponding to the observation O. We will
show that swapping the relative ranking of items i, j in 3 will result in a ranking which is
assigned nonzero probability by h, thus showing that this new ranking is contained in O.
Let A = {i, j} and B = \A. Since O  C, h must factor riffle independently according
to the partition (A, B). Thus,
h(1 ) = m(AB (1 ))  f (A (1 ))  g(B (1 )) > 0, and
h(2 ) = m(AB (2 ))  f (A (2 ))  g(B (2 )) > 0.
Since 1 and 2 disagree on the relative ranking of items in A, this factorization implies in
particular that both f (A = i|j) > 0 and f (A = j|i) > 0. Since h(3 ) > 0, it must also
be that each of m(AB (3 )), f (A (3 )), and g(B (3 )) have positive probability. We can
therefore swap the relative ranking of A, A , to obtain a new ranking which has positive
probability since all of the terms in the decomposition of this new ranking have positive
probability.
Lemma 29 below provides conditions under which removing a vertical bar from one of
the rankings in X will not change the support of a completely riffle independent distribution. To illustrate with an example, consider a completely decomposable observation O
which contains the partial ranking S  = Corn, P eas|Apples, Oranges as a subset. What
Lemma 29 guarantees is that, if, in addition, there exists any element  in O which disagrees
with S  on the relative ordering of, say, P eas and Oranges, then in fact the partial ranking
S 0  0 = Corn, P eas, Apples, Oranges (with the bar removed from S ) must also be a
subset of O. Formally,
Lemma 29. Let S  = 1 | . . . |i |i+1 | . . . |k be a partial ranking on item set , and
S 0  0 = 1 | . . . |i  i+1 | . . . |k , the partial ranking in which the sets i and i+1 are
merged. Let a1  ij=1 j and a2  kj=i+1 j . If O is any element of C such that S   O
and there additionally exists a ranking   O which disagrees with S  on the relative
ordering of a1 , a2 , then S 0  0  O.
Proof. The key strategy in our proof of Lemma 29 is to argue that large subsets of rankings
must be contained in a completely decomposable observation O by decomposing rankings
into transpositions and invoking the technical lemma from above (Lemma 28) repeatedly.
See the Appendix for details.
We now can use Lemma 29 to show that the reverse inclusion of Proposition 26 also
holds, establishing that the two sets rspan(X) and pspan(X) are in fact equal and thereby
proving the desired result, that C  P.
Proof. (of Proposition 26) At each iteration t, Algorithm 2 producesS
a set of partial rankings,
Xt . We denote the union of all partial rankings at time t as Xt  S Xt S . Note that
X0 = X and XT = pspan(X). The idea of our proof will be to show that at each iteration
t, the following set inclusion holds: rspan(Xt )  rspan(Xt1 ). If indeed this holds, then
507

fiHuang, Kapoor & Guestrin

after the final iteration T , we will have shown that:
pspan(X) = XT ,

(Proposition 27)

 rspan(XT ),

(Monotonicity, Proposition 24)

 rspan(X0 ),

(since rspan(Xt )  rspan(Xt1 ), shown below),

 rspan(X)

(X0 = X, see Algorithm 2)

which would prove the Proposition.
It remains now to show that rspan(Xt )  rspan(Xt1 ). We claim that Xt  rspan(Xt1 ).
Let   Xt . If   Xt1 , then since Xt1  rspan(Xt1 ), we have   rspan(Xt1 ) and
the proof is done. Otherwise,   Xt \Xt1 . In this second case, we use the fact that
at iteration t, the vertical bar between i and i+1 was deleted from the partial ranking S  = 1 | . . . |i |i+1 | . . . |k (which is a subset of Xt1 ) to form the partial ranking
S 0  0 = 1 | . . . |i  i+1 | . . . |k . (which is a subset of Xt ). Furthermore, in order for the
vertical bar to have been deleted by the algorithm, there must have existed some partial
ranking (and therefore some full ranking  0 ) that disagreed with S  on the relative ordering of items a1 , a2 on opposite sides of the bar. Since   Xt \Xt1 we can assume that
  S 0  0 .
We now would like to apply Lemma 29. Note that for any O  C such that Xt1  O,
we also have S   O, since S   Xt1 . An application of Lemma 29 then shows that
S 0  0  O and therefore that   O.
We have shown in fact that   O holds for any observation O  C such that Xt1 
O, and therefore taking the intersection of supports over all O  C, we see that Xt 
rspan(Xt1 ). Taking the rspan of both sides yields:
rspan(Xt )  rspan(rspan(Xt1 )),
 rspan(Xt1 ).

(Subset preservation, Proposition 24)

(Idempotence, Proposition 24)

5.3 Going Beyond Subset Observations
Though we have stated all of our results so far for subset observations, we now comment
on what our theory would look like if we had considered general likelihood functions. In
order to avoid confusion, we here refer to a more general class of functions that we call completely decomposable functions, instead of the completely decomposable subset observations
of Definition 11.
Definition 30. A function h : Sn  R is called a completely decomposable function if it
factors riffle independently with respect to every hierarchy over the item set . We denote
e
the collection of all possible completely decomposable functions as C.
e are very nearly the same. It is quite simple to restate Theorem 19
As we discuss, C and C
with respect to the general case of completely decomposable functions:
Theorem. Every partial ranking indicator function is a completely decomposable function.
508

fiEfficient Inference With Partial Rankings

Unfortunately, the proof of its converse (Theorem 21) does not easily generalize, and
instead can only be used to show that the support ({  Sn : h() > 0}) of every completely
decomposable function is a partial ranking. It is natural, however, to suspect that a full
converse does indeed exist  that every completely decomposable function is proportional to
the indicator function of some partial ranking. In fact, this suspected converse only almost
holds. We have:
Theorem. If h is any completely decomposable function supported on a partial ranking
S  = 1 | . . . |r where |i | =
6 2 for all i = 1, . . . , r, then h is proportional to the indicator
function on S .
Proof. See Appendix.
Example 31. For completely decomposable functions, it is not possible to do away with the
assumption that |i | =
6 2 for all i. As an example, the function defined below as:

 2/3 if  = Corn|P eas|Apples
1/3 if  = P eas|Corn|Apples ,
h() =

0
otherwise
is supported on the partial ranking S  = Corn, P eas|Apples (where |1 | = 2), and is not
proportional to any indicator function (i.e., it is not uniform on rankings which are not
assigned positive probability).
However, it is still possible to show that h is a completely decomposable function. To
prove so, it is necessary to establish only three things: that {Corn, P eas} and {Apples}
are riffle independent, that {Corn, Apples} and {P eas} are riffle independent, and that
{P eas, Apples} and {Corn} are riffle independent. For example, with respect to the partitioning into sets A = {Corn, Apples} and B = {P eas}, we see that
h() = m(AB ())  f (A ())  g(B ()),
where:

 2/3
1/3
m(AB ) =

0

if AB = A|B|A
if AB = B|A|A ,
if AB = A|A|B


f (A ) =

1
0

if {AC} = A|C
,
otherwise

g(B ) = 1.

Therefore, when |i | = 2, it is possible to have completely decomposable functions which
are not uniform on their supports.
5.4 Conditioning on Noisy Observations
We conclude this section with a remark on handling noise in observations. While we have
assumed in this paper that observed partial rankings are always consistent with a users
underlying full ranking, there are situations in which one may wish to model a noisier
setting, where the partial rankings may be misreported with some small probability. A
natural model that accounts for noise, for example, might be:

1   if   O
L(O|) =
.
(5.2)

|O|1 otherwise
509

fiHuang, Kapoor & Guestrin

If a prior distribution factorizes with respect to a hierarchy H, then conditioning on the
noisy likelihood of Equation 5.2 results in a posterior distribution which can be written
as a weighted mixture of the prior distribution and the posterior that would have resulted
from conditioning on a noise-free observation. While each component of this posterior
distribution factorizes with respect to H, the mixture itself does not factor in general (and
should not factor according to our theory). As a result, iteratively conditioning on multiple
partial rankings according to the noisy likelihood function above would quickly lead to
an unmanageable number of mixture components. We therefore believe that approximate
inference methods for conditioning on multiple noisy partial ranking observations is a fruitful
area for further research.

6. Model Estimation from Partially Ranked Data
In many ranking based applications, datasets are predominantly composed of partial rankings rather than full rankings due to the fact that for humans, partial rankings are typically
easier and faster to specify. In addition, many datasets are heterogeneous, containing partial
ranking of different types. For example, in the American Psychological Assoication as well
as the Irish House of Parliament elections, voters are allowed to specify their top-k candidate
choices for any value of k (see Figures 7(a) and 7(b)). In this section we use the efficient
inference algorithm proposed in Section 5 for estimating a riffle independent model from
partially ranked data. Because estimating a model using partially ranked data is typically
considered to be more difficult than estimating one using only full rankings, a common practice (e.g., see Huang & Guestrin, 2010) has been to simply ignore the partial rankings in a
dataset. The ability of a method to incorporate all of the available data however, can lead
to significantly improved model accuracy as well as wider applicability of that method. In
this section, we propose the first efficient method for estimating the structure and parameters
of a hierarchical riffle independent model from heterogeneous datasets consisting of arbitrary
partial ranking types. Central to our approach is the idea that given someones partial preferences, we can use the efficient algorithms developed in the previous section to infer his full
preferences and consequently apply previously proposed algorithms which are designed to
work with full rankings.
6.1 Censoring Interpretations of Partial Rankings
The model estimation problem for full rankings is stated as follows. Given i.i.d. training
examples  (1) , . . . ,  (m) (consisting of full rankings) drawn from a hierarchical riffle independent distribution h, recover the structure and parameters of h.
In the partial ranking setting, we again assume i.i.d. draws, but that each training
example  (i) undergoes a censoring process producing a partial ranking consistent with  (i) .
For example, censoring might only allow for the ranking of the top-k items of  (i) to be
observed. While we allow for arbitrary types of partial rankings to arise via censoring, we
make a common assumption that the partial ranking type resulting from censoring  (i) does
not depend on  (i) itself.
510

fiEfficient Inference With Partial Rankings

6.2 Algorithm
We treat the model estimation from partial rankings problem as a missing data problem. As
with many such problems, if we could determine the full ranking corresponding to each observation in the data, then we could apply algorithms which work in the completely observed
data setting. Since full rankings are not given, we utilize an Expectation-Maximization (EM)
approach in which we use inference to compute a posterior distribution over full rankings
given the observed partial ranking. In our case, we then apply the algorithms from Huang
and Guestrin (2010, 2012) which were designed to estimate the hierarchical structure of a
model and its parameters from a dataset of full rankings.
Given an initial model h and a collection of training examples {O(1) , O(2) , . . . , O(m) }
consisting of partial rankings, our EM-based approach alternates between the following two
steps until convergence is achieved.
 (E-step): For each observation, O(i) = S (i)  (i) , in the training examples, we use
inference to compute a posterior distribution over the full ranking  that could have
generated O(i) via censoring, h(|O(i) = S (i)  (i) ). Since the observations take the
form of partial rankings and are hence completely decomposable, we use the efficient
algorithms in Section 5 to perform the E-step.
 (M-step): In the M-step, one maximizes the expected log-likelihood of the training
data with respect to the model. When the hierarchical structure of the model has been
provided, or is known beforehand, our M-step can be performed using standard methods for optimizing parameters. When the structure is unknown, we use a structural
EM approach, which is analogous to methods from the graphical models literature for
structure learning from incomplete data (Friedman, 1997, 1998).
Unfortunately, the (riffled independence) structure learning algorithm of Huang and
Guestrin (2010) is unable to directly use the posterior distributions computed from
the E-step. Instead, observing that sampling from riffle independent models can be
done efficiently and exactly (as opposed to, for example, MCMC methods), we simply
sample full rankings from the posterior distributions computed in the E-step and
pass these full rankings into the structure learning algorithm of Huang and Guestrin
(2010). The number of samples that are necessary, instead of scaling factorially, scales
according to the number of samples required to detect riffled independence (which
under mild assumptions is polynomial in n, Huang & Guestrin, 2010).

7. Related Work
Rankings and permutations have recently become an active area of research in machine
learning due in part to the hinge role that they play in information retrieval and preference
elicitation. Algorithms such as the RankSVM (Joachims, 2002) and RankBoost (Freund,
Iyer, Schapire, & Singer, 2003), for example, have been successful in the large scale ranking
problems that appear in web search. The main aims of our work differ from these web
scale settings however  instead of seeking a single optimal ranking with respect to some
objective function, we seek an understanding of a large collection of rankings via density
estimation. In the following, we outline two major lines of research which have influenced
our work.
511

fiHuang, Kapoor & Guestrin

7.1 Additive and Multiplicative Decompositions
Our paper builds in particular upon a thread of recent work on tractable models for permutation data based on function decompositions. Kondor, Howard, and Jebara (2007) and
Huang, Guestrin, and Guibas (2008, 2009) considered additive decompositions of a distribution into a weighted sum of Fourier basis functions. These papers show that low-frequency
Fourier assumptions can often be effective for coping with the representational complexity
of working with distributions over permutations. They show in particular that conditioning
prior distributions on the low frequency likelihood functions that often arise in multiobject
tracking problems can be performed especially efficiently.
Unfortunately, low frequency assumptions are not as applicable for distributions defined
over rankings, and to address ranking problems specifically, Huang and Guestrin (2009,
2010) introduced the concept of riffled independence as a useful generalization of probabilistic independence for rankings. Using multiplicative decompositions based on riffled
independence, we showed that it is possible to learn the hierarchical structure of a model
given a fully ranked dataset. While our previous papers on the topic of riffled independence
focused more on problems related to efficiently representing distributions, the main focus of
our current paper lies in efficient reasoning/inference and tackling human task complexity
by considering partial rankings.
It is interesting to note that while it is natural and efficient to condition a Fourier based
representation on low-frequency observations (involving a very small number of items) such
as O =Alice is in third place, a multiplicative decomposition based on riffled independence
would not be able to efficiently condition on the same observation. On the other hand,
multiplicative decompositions allow us to condition on top-k observations efficiently (independently of the size of k), whereas top-k observations would be difficult to handle in a
Fourier theoretic setting (except for very small k).
7.2 Mallows Models
Our work also fits into a larger body of research about the well known Mallows distribution
over rankings, parameterized by:
h(; , 0 )  d (,0 ) ,

(7.1)

where the function d refers to the Kendalls tau distance metric on rankings. A Mallows
distribution (Equation 7.1) can always be shown to be a special case of a hierarchical riffle independent model in which items are sequentially factored out of the model one by
one (Huang, 2011) (see Figure 6).
Mallows models (as well as other similar distance based models) have the advantage
that they can compactly represent distributions for very large n, and admit conjugate prior
distributions (Meila, Phadnis, Patterson, & Bilmes, 2007). Estimating parameters has been
a popular problem for statisticians  recovering the optimal 0 from data is known as the
consensus ranking or rank aggregation problem and is known to be N P -hard (Bartholdi,
Tovey, & Trick, 1989). Many authors have focused on approximation algorithms instead.
Like Gaussian distributions, Mallows models tend to lack flexibility, and so Lebanon and
Mao (2008) propose a nonparametric model of ranked (and partially ranked) data based
on placing weighted Mallows kernels on top of training examples, which, as they show, can
512

fiEfficient Inference With Partial Rankings

{Corn,Peas,Apples,Oranges,Doughnuts}

{Corn}

{Peas,Apples,Oranges,Doughnuts}

{Peas}

{Apples,Oranges,Doughnuts}
{Apples}

{Oranges,Doughnuts}
{Oranges}

{Doughnuts}

Figure 6: A Mallows model always factors according to what we refer to as a chain
structure in which items are factored out one by one. The Mallows distribution over five items from our food item set with mode (or central ranking) at
0 = Corn|P eas|Apples|Oranges|Doughnuts, for example, must factor according to the above hierarchical structure.

realize a far richer class of distributions, and can be learned efficiently. However, they do
not address the inference problem, and it is not immediately clear in many Mallows models
papers whether one can efficiently perform inference operations like marginalization and
conditioning in such models. Riffle independent models, on the other hand, encompass a
class of distributions which is both rich as well as interpretable, and additionally, we have
identified precise conditions under which efficient conditioning is possible (the conditions
being that the observations take the form of partial rankings).
There are several recent works to model partial rankings using Mallows based models.
Busse, Orbanz, and Buhmann (2007) learned finite mixtures of Mallows models from topk data (also using an EM approach). Lebanon and Mao (2008), as we have mentioned,
developed a nonparametric model based on Mallows models which can handle arbitrary
types of partial rankings. In both settings, a central problem is to marginalize a Mallows
model over all full rankings which are consistent with a particular partial ranking. To do so
efficiently, both papers rely on the fact (first shown in Fligner & Verducci, 1986) that this
marginalization step can be performed in closed form. This closed form equation of Fligner
and Verducci (1986), however, can be seen as a very special case of our setting since Mallows
models can always be shown to factor riffle independently according to a chain structure.
Specifically, to compute the sum over rankings which are consistent with a partial ranking
S , it is necessary to condition on S , and to compute the normalization constant of the
resulting function. The conditioning step can be performed using the methods that we have
described in this paper, and the normalization constant can be computed by multiplying
the normalization constant of each factor of the hierarchical decomposition. Thus, instead
of resorting to the more complicated mathematics of inversion combinatorics, our theory of
complete decomposability offers a simple conceptual way to understand why Mallows models
can be conditioned efficiently on partial ranking observations.
513

fiHuang, Kapoor & Guestrin

Finally in recent related work, Lu and Boutilier (2011) considered an even more general
class of observations based on DAG (directed acyclic graph) based observations in which
probabilities of rankings which are not consistent with a DAG of relative ranking relations
are set to zero. Lu and Boutilier show in particular that the conditioning problem for
their DAG-based class of observations is #P -hard. They additionally propose an efficient
rejection sampling method for performing probabilistic inference within the general class
of DAG observations and prove that the sampling method is exact for the class of partial
rankings that we have discussed in this paper.

8. Experiments
In this section, we demonstrate our method for learning hierarchical riffle independent models from partial rankings on simulated data as well as real datasets taken from different
domains. In all experiments, we initialize distributions to be uniform, and do not use random restarts.
8.1 Datasets
In addition to roughly 5000 full rankings, the APA dataset has over 10,000 top-k rankings of
5 candidates. In previous work, we had used only the full rankings of the APA data (Huang
& Guestrin, 2010, 2012), but now we are able to use the entire dataset. Figure 7(a) plots,
for each k  {1, . . . , 5}, the number of ballots in the APA data of length k.
Likewise, the Meath dataset (Gormley & Murphy, 2007) which was taken from the 2002
Irish Parliament election has over 60,000 top-k rankings of 14 candidates. As with the APA
data, we had used only the full rankings of the Meath data in previous work, but here we use
the entire dataset. Figure 7(b) plots, for each k  {1, . . . , 14}, the number of ballots in the
Meath data of length k. In particular, note that the vast majority of ballots in the dataset
consist of partial rather than full rankings, with over half of the electorate preferring to list
only their favorite three or four candidates. We can run inference (Algorithm 1) on over
5000 top-k examples for the Meath data in 10 seconds on a dual 3.0 GHz Pentium machine
with an unoptimized Python implementation. Using brute force inference, we estimate
that the same job would require roughly one hundred years.
We extracted a third dataset from a database of searchtrails collected by White and
Drucker (2007), in which browsing sessions of roughly 2000 users were logged during 20082009. In many cases, users are unlikely to read articles about the same news story twice,
and so it is often possible to think of the order in which a user reads through a collection
of articles as a top-k ranking over articles concerning a particular story/topic. The ability
to model visit orderings would allow us to make long term predictions about user browsing
behavior, or even recommend curriculums over articles for users. We ran our algorithms on
roughly 300 visit orderings for the eight most popular posts from www.huffingtonpost.com
concerning Sarah Palin, a popular subject during the 2008 U.S. presidential election. Since
no user visited every article, there are no full rankings in the data and thus there does not
even exist the option of learning using only the subset of full rankings.
514

fiEfficient Inference With Partial Rankings

number of votes

number of votes

6000
5000

4000
3000
2000
1000

20,000

10,000

0

0

1

2

2
3
4
5
number of candidates

4

6

8 10 12 14
k

(a) APA election data

(b) Irish election data

Figure 7: Histograms of top-k ballot lengths in the APA and Irish election datasets. Whereas
the majority of the electorate provided full rankings in the APA election data
(probably due to the fact that there were only five candidates), the vast majority
of voters in the Irish election data provided only their top-3 or top-4 choices.
{12345}

{12345}
{12345}

{2}

{2345}

{2}

{2345}
{1345}

{345}

{1}
{3}

{45}

(a) Structure learned using
only the subset of full rankings
(out of the 300 given training
examples)

{345}

{1}
{5}

{34}

(b) Structure learned using all
training examples after 1 iteration of EM

{13}
Research

{2}
Community
psychologists

{45}
Clinical

(c) Structure learned using all
training examples after structural convergence (3 iterations)

Figure 8: Structure learning with a subset of the APA dataset (300 rankings, randomly
sampled, including both full and partial rankings).

8.2 APA Structure Learning Results
Due to the unordinarily large number of full rankings in the APA data, the gains made by
additionally using partially ranked data are insignificant. To better illustrate the benefits of
partial rankings, we subsampled a dataset of 300 rankings (including both full and partial
rankings) and present results with this smaller dataset. Performing structure learning using
only the full rankings of these 300 training examples (consisting of roughly 100 examples),
one obtains the structure in Figure 8(a), which can be seen to not match the correct
structure of Figure 3(a) which was learned using 5000 full rankings. Figures 8(b) and 8(c)
515

fiHuang, Kapoor & Guestrin

training time (seconds)

test log-likelihood

x 10 4
-2
-3
-4
-5

-6
EM

Flat-EM

4

training time (seconds)

test log-likelihood

-5
-5.2
-5.4
-5.6
-5.8
-6
k>2

10
5

0
EM

Flat-EM Uniform
Fill-In

(b) Training time comparison of our
EM approach against the FlatEM and
Uniform Fill-In methods.

x 10

k>1

15

Uniform
Fill-In

(a) Test set log-likelihood comparison
of our EM approach against the FlatEM
and Uniform Fill-In methods.

k>0

20

2.5
2
1.5

1

k>3

(c) Test set log-likelihoods, training
only with top-t rankings with t larger
than a fixed k.

k>0

k>1

k>2

k>3

(d) Training times, training only with
top-t rankings with t larger than a fixed
k.

Figure 9: APA experimental results  each experiment repeated with 200 bootstrapped
resamplings of the data

plot the results of our EM algorithm with the former displaying the resulting structure after
just a single EM iteration and the latter the result after structural convergence, which occurs
by the third iteration, showing that our method can learn the correct structure given just
300 training examples.
We compared our EM algorithm against two alternative baseline approaches that we
refer to in our plots as FlatEM and Uniform Fill-in. The FlatEM algorithm is the same as
the EM algorithm above except for two details: (1) it performs conditioning exhaustively
instead of exploiting the factorized model structure, and (2) it performs the M-step without
sampling. The Uniform Fill-in approach treats every top-k ranking in the training set as
a uniform collection of votes for all of the full rankings consistent with that top-k ranking,
and is accomplished by using just one iteration of our EM algorithm.
In Figure 9(a) we plot test set loglikelihoods corresponding to each approach, with EM
and FlatEM having almost identical results and both performing much better than the
Uniform Fill-in approach. On the other hand, Figure 9(b), which compares running times
of the three approaches, shows that FlatEM can be far more costly (for most datasets, it
cannot even be run in a reasonable amount of time).
516

fiEfficient Inference With Partial Rankings

1st iteration

2nd iteration

{0,1,2,3,4,5,6,7}

3rd iteration

{0,1,2,3,4,5,6,7}
{0,1,2,3,4,5,6,7}

{5}

{0,1,2,3,4,6,7}

{5}

{0,1,2,3,4,6,7}
{5}

{0,1,2,3,4,7}

{6}

{0,1,2,3,4,7}

{0,1,2,3,4,6,7}

{7}
{0,1,2,3,4,7}

{0,1,2,3,4}

{7}

{0,1,2,3,4}

{6}
{0,2,3}

{0,2,3}

{1,4}

Log likelihood: -818.6579
(a)

{0,2,3}

{7}

{1,4,6}

{1,4}

Log likelihood: -769.2369
(b)

Log likelihood: -767.2760
(c)

Figure 10: Iterations of Structure EM for the Sarah Palin data with structural changes at
each iteration highlighted in red. Structural convergence occurs after just three
iterations. Note that this structure was discovered using only visit orders, and
that no text information from pages was incorporated in the learning process.
This figure is best viewed in color.

To verify that partial rankings do indeed make a difference in the APA data, we plot
the results of estimating a model from the subsets of APA training data consisting of top-k
rankings with length larger than some fixed k. Figures 9(c) and 9(d) show the log-likelihood
and running times for k = 0, 1, 2, 3 with k = 0 being the entire training set and k = 3
being the subset of training data consisting only of full rankings. As our results show,
including partial rankings does indeed help on average for improving test log-likelihood
(with diminishing returns).
8.3 Structure Discovery with EM with Larger n.
Our experiments have led to several observations about using EM for learning with partial
rankings. First, we observe that typical runs converge to a fixed structure quickly, with no
more than three EM iterations. Figure 10 shows the progress of EM on the Sarah Palin
data, whose structure converges by the third iteration. As expected, the log-likelihood
increases at each iteration, and we remark that the structure becomes more interpretable
 for example, the leaf set {0, 2, 3} corresponds to the three posts about Palins wardrobe
before the election, while the posts from the leaf set {1, 4, 6} were related to verbal gaffes
made by Palin during the campaign. Notice that this structure is discovered purely using
data about visit orders and that no text information was used in our experiments.
517

fiHuang, Kapoor & Guestrin

-2.72

x 104

test log-likelihood

# of EM iterations before
convergence

30

25
20
15

10

EM with
decomposable
conditioning

-2.8

[Lebanon & Mao, 08]

-2.84
-2.88

5
0

-2.76

0

0 1 2 3 4 5 6 7 8 9 10 11 12
k

250 1000 4000 16000 64000
# of partial rankings in training set
(in addition to full rankings)

(a)

(b)

Figure 11: (a): Number of EM iterations required for convergence if the training set only
contains rankings of length longer than k. (b): Density estimation from synthetic
data. We plot test loglikelihood when learning from 343 full rankings and between
0 and 64,000 additional partial rankings.

5000 training
examples

4

Test log-likelihood

x 10
-4.5

Ours

-4.6

25000 training
examples

5

x 10
-1.26

[LM08]

[LM08]

-1.28

Ours

-1.3

-4.7

Ours

-1.32

[LM08]
Ours

-4.8

[LM08]

-1.34

-4.9

Full only

Mixed (Full+Partial)

Full only

Mixed (Full+Partial)

Figure 12: Density estimation from small (5000 examples) and large subsets (25000 examples) of the Meath data. We compare our method against the work by Lebanon
and Mao (2008) in two settings: (1) training on all available data and (2) training
on the subset of full rankings.

Secondly, the number of EM iterations required to reach convergence in log-likelihood
depends on the types of partial rankings observed. We ran our algorithm on subsets of
the Meath dataset, each time training on m = 2000 rankings all with length larger than
518

fiEfficient Inference With Partial Rankings

some fixed k. Figure 11(a) shows the number of iterations required for convergence as a
function of k (with 20 bootstrap trials for each k). We observe fastest convergence for
datasets consisting of almost-full rankings and slowest convergence for those consisting of
almost-empty rankings, with almost 25 iterations necessary if one trains using rankings of
all types. Finally we remark that the model obtained after the first iteration of EM is
interesting and can be thought of as the result of pretending that each voter is completely
ambivalent regarding the n  k unspecified candidates.
8.4 The Value of Partial Rankings
We now verify again with larger n that using partial rankings in addition to full rankings
allows us to achieve better density estimates. We first learned models from synthetic data
drawn from a hierarchy, training using 343 full rankings plus varying numbers of partial
ranking examples (ranging between 0-64,000). We repeat each setting with 20 bootstrap
trials, and for evaluation, we compute the log-likelihood of a testset with 5000 examples.
For speed, we learn a structure H only once and fix H to learn parameters for each trial.
Figure 11(b), which plots the test log-likelihood as a function of the number of partial
rankings made available to the training set, shows that we are indeed able to learn more
accurate distributions as more and more data in the form of partial rankings are made
available.
8.5 Comparing to a Nonparametric Model
Comparing the performance of riffle independent models to other approaches was not possible in previous work since we had not been able to handle partial rankings. Using the
methods developed in our current paper, however, we compare riffle independent models
with the state-of-the-art nonparametric estimator of Lebanon and Mao (2008) (to which we
hereby refer as the LM08 estimator) on the same data (setting their regularization parameter to be C =1,2,5, or 10 via a validation set). Figure 11(b) shows (naturally) that when the
data are drawn synthetically from a riffle independent model, then our EM method significantly outperforms the LM08 estimator. We remark that in theory, the LM08 is guaranteed
to catch up in performance (under appropriate conditions) given enough training examples.
For the Meath data, which is only approximately riffle independent, we trained on subsets
of size 5,000 and 25,000 (testing on remaining data). For each subset, we evaluated our EM
algorithm for learning a riffle independent model against the LM08 estimator when (1)
using only full ranking data, and (2) using all data. As before, both methods do better
when partial rankings are made available.
For the smaller training set, the riffle independent model performs as well or better than
the LM08 estimator. For the larger training set of 25,000, we see that the nonparametric
method starts to perform slightly better on average, the advantage of a nonparametric
model being that it is guaranteed to be consistent, converging to the correct model given
enough data. The advantage of riffle independent models, however, is that they are simple,
interpretable, and can highlight global structures hidden within the data.
519

fiHuang, Kapoor & Guestrin

9. Future Directions
There remain several possible extensions to the current work. We list a few such open
questions and extensions in the following.
9.1 Inference with Incomplete Rankings
We have shown in this paper that one can exploit riffled independence structure to condition
on an observation if and only if it takes the form of a partial ranking. While the space of
partial rankings is both rich and useful in many settings, it does not cover an important class
of observations: that of incomplete rankings, which are defined to be a ranking (or partial
ranking) of a subset of the itemset . For example, Theorem 21 shows that the conditioning problem for pairwise observations of the form Apples are preferred over Bananas is
nondecomposable. Note that top-k rankings are considered to be complete rankings since
they implicitly rank all other items in the last n  k positions.
How then, can we tractably condition on incomplete rankings? One possible approach
is to convert to a Fourier representation using the methods from (Huang & Guestrin, 2012),
then conditioning on a pairwise ranking observation using the Fourier domain conditioning
algorithm proposed in (Huang et al., 2008). This Fourier domain approach would be useful if one were particularly interested in low-order marginal probabilities of the posterior
distributions.
When the Fourier approach is not viable, another option may be to assume that the
posterior distribution takes on a particular riffle independent structure (in the same way
that mean field methods from the graphical models literature would assume a factorized
posterior). The research question of interest is: which hierarchical structure should be used
for the purposes of approximating the posterior?
9.2 Reexamining Data Independence Assumptions
In this paper, we have assumed throughout that training examples are independent and
identically distributed. However in practice these are not always safe assumptions as a
number of factors can impact the validity of both. For example, in an internet survey in
which a user must perform a series of preference ranking tasks in sequence, a concern is that
the users prior ranking tasks may bias the results of his future rankings.
Another source of bias lies in the reference ranking that may be displayed, in which the
user is asked to rearrange items by dragging and dropping. On the one hand, showing
everyone the same reference ranking may bias the resulting data. But on the other hand,
showing every user a different reference ranking may mean that the training examples are
not exactly identically distributed.
Yet another form of bias lies in the partial ranking types that are reported in data. To
formulate our EM algorithm, we have assumed that a users preferences does not influence
whether he chooses to, say, report a full ranking instead of a top-3 ranking. In practice,
however, partial ranking types and user preferences are often correlated. In the Irish elections, for example, where there is typically only one Sinn Fein candidate, those who rank
Sinn Fein first are typically more likely to have only reported their top-1 choice.
520

fiEfficient Inference With Partial Rankings

Understanding, identifying, and finally, learning in spite of the different types of biases
that may occur in eliciting preference data remains a fundamental problem in ranking.
9.3 Probabilistic Modeling of Strategic Voting
It is interesting to consider the differences between the actual vote distributions considered in
this paper against the approximate riffle independent distributions. Take the APA dataset,
for example, in which the optimal approximation by a riffle independent hierarchy reflects
the underlying political coalitions within the organization. Upon comparison between the
approximation and the empirical distribution, however, some marked differences arise. For
example, the riffle independent approximation underestimates the number of votes obtained
by candidate 3 (a research psychologist) who ultimately won the election.
One possible explanation for the discrepancy may lie in the idea that voters tend to vote
strategically in APA elections, placing stronger candidates of opposing political coalitions
lower in the ranking, rather than revealing their true preferences. An interesting line of
future work lies in detecting and studying the presence of such strategic voting in election
datasets. Open questions include (1) verifying mathematically whether strategic voting does
indeed exist in, say, the APA election data, and (2) if so, why the strategic voting effect is
not strong enough to overwhelm our riffled independence structure learning algorithms, and
(3) how strategic voting can manifest itself in partial ranking votes.

10. Conclusion
In probabilistic reasoning problems, it is often the case that certain data types suggest
certain distribution representations. For example, sparse dependency structure in the data
often suggests a Markov random field (or other graphical model) representation (Friedman,
1997, 1998). For low-order permutation observations (depending on only a few items at a
time), recent work (Huang et al., 2009; Kondor, 2008) has shown that a Fourier domain
representation is appropriate. For preference ranking scenarios, one must contend with
human task complexity  the difficulty involved for a human to rank a long list of items
and often leads to partially, instead of fully ranked data. In this paper, we have shown that
when data takes the form of partial rankings, then hierarchical riffle independent models are
a natural representation.
As with conjugate priors, we showed that a riffle independent model is guaranteed to
retain its factorization structure after conditioning on a partial ranking (which can be performed in efficiently). Most surprisingly, our work shows that observations which do not
take the form of partial rankings are not amenable to simple multiplicative update based
conditioning algorithms. Finally, we showed that it is possible to learn hierarchical riffle
independent models from partially ranked data, significantly extending the applicability of
previous work.

Acknowledgments
This project was formulated and largely conducted during an internship by Jonathan Huang
at Microsoft Research. Additional work was supported in part by ONR under MURI
N000140710747, and ARO under MURI W911NF0810242. Carlos Guestrin was funded
521

fiHuang, Kapoor & Guestrin

in part by NSF Career IIS-064422. We thank Eric Horvitz, Ryen White, Dan Liebling, and
Yi Mao for discussions.

Appendix A. Proofs
In this appendix, we provide supplementary proofs of some of the theoretical results in this
paper.
A.1 Proof of Theorem 19
To prove Theorem 19 (as well as later results), we will refer to rank sets.
Definition 32. Given a partial ranking of type , we denote the rank set occupied by
i by Ri . Note that Ri depends only
on  and can be written as R1 = {1, . . . , 1 },
P
R2 = {1 + 1, . . . , 1 + 2 }, . . . , Rr = { r1
i=1 i + 1, . . . , n}.
And we will refer to the following basic fact regarding rank sets:
Proposition 33.   S  = 1 | . . . |r if and only if for each i, (i ) = Ri .
Proof. (of Theorem 19) We use induction on the size of the itemset. The cases n = 1, 2 are
trivial since every distribution on S1 or S2 factors riffle independently. We now consider the
more general case of n > 2.
Fix a partial ranking S  = 1 |2 | . . . |r of type  and a binary partition of the item
set into subsets A and B. We will show that the indicator function S  factors as:
S  () = m(AB ())  f (A ())  g(B ()),

(A.1)

where factors m, f and g are the indicator functions for the set of consistent interleavings,
[S ]AB , and the sets of consistent relative rankings, [S ]A and [S ]B , respectively. If
Equation A.1 is true, then we will have shown that S  must decompose with respect
to the top layer of H. To show that S  decomposes hierarchically, we must also show
that the relative ranking factors fA and gB decompose with respect to HA and HB , the
subhierarchies over the item sets A and B. To establish this second step (assuming that
Equation A.1 holds), note that fA and gB are indicator functions for the restricted partial
rankings, [S ]A and [S ]B , which themselves are partial rankings over smaller item sets
A and B. The inductive hypothesis (and the fact that A and B are assumed to be strictly
smaller sets than ) then shows that the functions fA and gB both factor according to their
respective subhierarchies.
We now turn to establishing Equation A.1. It suffices to prove that the following two
statements are equivalent:
I. The ranking  is consistent with the partial ranking S  (i.e.,   S ).
II. The following three conditions hold:
(a) The interleaving AB () is consistent with S  (i.e., AB ()  [S ]AB ), and
(b) The relative ranking A () is consistent with S  (i.e., A ()  [S ]A ), and
(c) The relative ranking B () is consistent with S  (i.e., B ()  [S ]B ).
522

fiEfficient Inference With Partial Rankings

 (I  II): We first show that   S  implies conditions (a), (b) and (c).
(a) If   S , then for each i,
|j  Ri : AB (j) = A| = |j  Ri :  1 (j)  A|,
= |k  i : k  A|,

(by Definition 2)

(by Proposition 33)

= |i  A|.
The same argument (replacing A with B) shows that for each i, we have |j 
Ri : AB (j) = B| = |i  B|. These two conditions (by Definition 17) show that
AB is consistent with S .
(b) If   S , then (by Definition 14)  ranks items in i before items in j for any
i < j. Intersecting each i with A, we also see that  ranks any item in i  A
before any item in j  A for all i, j. By Definition 2, A () also ranks any item
in i  A before any item in j  A for all i, j. And finally by Definition 16 again,
we see that A () is consistent with the partial ranking S .
(c) (Same argument as (b)).
 (II  I): We now assume conditions (a), (b), and (c) to hold, and show that   S .
By Proposition 33 it is sufficient to show that if an item k  i , then (k)  Ri . To
prove this claim, we show by induction on i that if an item k  i  A, then (k)  Ri
(and similarly if k  i  B, then (k)  Ri ).
Base case. In the base case (i = 1), we assume that k  1 A, and the goal is to show
that (k)  R1 . By condition (a), we have that AB ()  [S ]AB . By Definition 17,
this means that: |1  A| = {j  R1 : [AB ()](j) = A} = {j  R1 :  1 (j)  A}. In
words, there are m = |1  A| items from A which lie in rank set R1 = {1, . . . , 1 }. To
show that an item k  A maps to a rank in R1 , we now must show that in the relative
ranking of elements in A, k is among the first m. By condition (b), A ()  [S ]A ,
implying that the item subset 1  A occupies the first m positions in the relative
ranking of A. Since k  1  A, item k is among the first m items ranked by A ()
and therefore (k)  R1 . A similar argument shows that k  1  B implise that
(k)  R1 .
Inductive case. We now show that if k  i  A, then (k)  Ri . By condition (b),
A ()  [S ]A , implying that the item subset i A (and hence, item k) occupies the
first m = |i  A| positions in the relative ranking of A beyond the items i1
j=1 (j 
A). By the inductive hypothesis and mutual exclusivity, these items, together with
i1
i1
j=1 (j  B) occupy ranks j=1 Rj , and therefore (k)  R` for some `  i. On the
other hand, condition (a) assures us that |i  A| = {j  Ri :  1 (j)  A}  or in
other words, that the ranks in Ri are occupied by exactly m items of A. Therefore,
(k)  Ri . Again, a similar argument shows that k  i  B implies that (k)  Ri .

A.2 The pspan of a Set is Always a Partial Ranking
To reason about the pspan of a set of rankings, we first introduce some basic concepts
regarding the combinatorics of partial rankings. The collection of partial rankings over 
523

fiHuang, Kapoor & Guestrin

forms a partially ordered set (poset) where S 0  0  S  if S  can be obtained from S 0  0 by
dropping vertical lines. For example, on S3 , we have that 1|2|3  12|3. The Hasse diagram
is the graph in which each node corresponds to a partial ranking and a node x is connected
to node y via an edge if x  y and there exists no partial ranking z such that x  z  y
(see Lebanon & Mao, 2008). At the top of the Hasse diagram is the partial ranking 1, 2, . . . , n
(i.e., all of S ) and at the bottom of the Hasse diagram lie the full rankings. See Figure 13
for an example of the partial ranking lattice on S3 .
Lemma 34. [Lebanon & Mao, 2008] Given any two partial rankings S , S 0  0 , there
exists a unique supremum of S  and S 0  0 (a node Ssup sup such that S   Ssup sup
and S 0  0  Ssup sup , and any other such node is greater than Ssup sup ). Similarly, there
exists a unique infimum of S  and S 0  0 .
Lemma 35. Given two partial rankings S , S 0  0 , the relation S 0  0  S  holds if and
only S  lies above S 0  0 in the Hasse diagram.
Proof. If S  lies above S 0  0 in the Hasse diagram, then S 0  0  S  is trivial since S 
can be obtained by dropping vertical bars of S 0  0 . Now given that S  does not lie above
S 0  0 , we would like to show that S 0  0 6 S . Let Sinf inf be the unique infimum of
S  and S 0  0 as guaranteed by Lemma 34. By the definition of the Hasse diagram, both
S  and S  can be obtained by dropping verticals from the vertical bar representation
of Sinf inf . Since S  does not lie above S 0  0 , there must be a vertical bar that was
dropped by S 0  0 which was not dropped by S  (if there does not exist such a bar, then
S 0  0  S ), and hence there must exist a pair of items i, j separated by a single vertical
bar in S  but unseparated in S 0  0 . Therefore there exists   S 0  0 such that (j) < (i)
even though there exists no such   S . We conclude that S 0  0 6 S .
Lemma 36 (Lemma 23 in main body). For any X  Sn , pspan(X) is a partial ranking.
Proof. Consider any subset X  Sn . A partial ranking containing every element in X
must be an upper bound of every element of X in the Hasse diagram by Lemma 35. By
Lemma 34, there must exist a unique least upper bound (supremum) of X, Ssup sup , such
that for any common upper bound S  of X, S  must also be an ancestor of Ssup sup and
hence Ssup sup  S . We therefore see that any partial ranking containing X must be a
superset of Ssup sup . On the other hand, Ssup sup is itself a partial ranking containing X.
Since pspan(X) is the intersection of partial rankings containing X, we have pspan(X) =
Ssup sup and therefore that pspan(X) must be a partial ranking.
A.3 Proofs for the Claim that rspan(X) = pspan(X)
To simplify the notation in some of the remaining proofs, we introduce the following definition.
Definition 37 (Ties). Given a partial ranking S  = 1 | . . . |r , we say that items a1 and
a2 are tied (written a1  a2 ) with respect to S  if a1 , a2  i for some i.
The following basic properties of the tie relation are straightforward.
Proposition 38.
524

fiEfficient Inference With Partial Rankings

123
1|23

12|3

13|2

2|13

3|12

23|1

1|2|3

1|3|2

2|1|3

3|1|2

2|3|1

3|2|1

Figure 13: The Hasse diagram for the lattice of partial rankings on S3 .
I. With respect to a fixed partial ranking S , the tie relation, , is an equivalence relation
on the item set (i.e., is reflexive, symmetric and transitive).
II. If there exist ,  0  S  which disagree on the relative ranking of items a1 and a2 ,
then a1  a2 with respect to S .
III. If S   S 0  0 , and a1  a2 with respect to S , then a1  a2 with respect to S 0  0 .
IV. If a1  a2 with respect to S , and (a1 ) < (a3 ) < (a2 ) for some item a3   and
some   S , then a1  a2  a3 .
Proposition 39. Given a set of rankings X as input, Algorithm 2 outputs pspan(X).
Proof. We prove three things, which together prove the proposition: (1) that the algorithm
terminates, (2) that at each stage the elements of X are contained in pspan(X), and (3)
that upon termination, pspan(X) is contained in each element of X.
1. First we note that the algorithm must terminate in finitely many iterations of the while
loop since at each stage at least one vertical bar is removed from a partial ranking,
and when all of the vertical bars have been removed from the elements of X, there are
no disagreements on relative ordering.
2. We now show that at any stage in the algorithm, every element of Xt is a subset of
the pspan(X). At initialization, of course, if S   X0 , then it is simply a singleton
set consisting of an element of X, and therefore S   pspan(X).
Suppose now that S   pspan(X) for every S   Xt . If S  is replaced by S 
in Xt+1 , then we want to show that S   pspan(X) as well. From Algorithm 2,
for some j, if S  = 1 | . . . |j |j+1 | . . . |r , S  can be written as 1 | . . . |j 
j+1 | . . . |r , where the vertical bar between j and j+1 is deleted due to the existence
of some partial ranking in Xt , S 0  0  Xt which disagrees with S  on the relative
ordering of items a1 , a2 on opposite sides of the bar. Since S  and S 0  0 are both
subsets of pspan(X) by assumption, we know that a1  a2 with respect to pspan(X)
(Proposition 38, II). Suppose now that a1  i and a2  i0 . Then for any x  i
and y  i0 , we have x  a1 and y  a2 with respect to pspan(X) by (III) of
Proposition 38. Moreover, by (I, transitivity), we see that x  y with respect to
pspan(X). for any two elements of i and i0 . By (IV) of Proposition 38, all the
items lying in i , i+1 , . . . , i0 are thus tied with respect to pspan(X) and therefore
removing any bar between items a1 and a2 (producing, for example, S ) results in a
partial ranking which is a subset of pspan(X).
525

fiHuang, Kapoor & Guestrin

3. Finally, upon termination, if some ranking   X is not contained in some element
S   Xt , then there would exist two items a1 , a2 whose relative ranking  and S 
disagree upon, which is a contradiction. Therefore, every element S   Xt contains
every element of X and thus pspan(X)  S  for every S   Xt .

Lemma 40. Let S  = 1 | . . . |i |i+1 | . . . |k be a partial ranking on item set , and
S 0  0 = 1 | . . . |i  i+1 | . . . |k , the partial ranking in which the sets i and i+1 are
merged. Let a1  ij=1 j and a2  kj=i+1 j . If O is any element of C such that S   O
and there additionally exists a ranking   O which disagrees with S  on the relative
ordering of a1 , a2 , then S 0  0  O.
Proof. We will fix a completely decomposable O and again work with h, the indicator
distribution corresponding to O. Let   S 0  0 . To prove the lemma, we need to establish
that h() > 0. Let  0 be any element of S  such that  0 (k) = (k) for all k  \(i i+1 ).
Since S   supp(h) by assumption, we have that h( 0 ) > 0.
Since  0 and  match on all items except for those in i  i+1 , there exists a sequence
of rankings  0 ,  1 ,  2 , . . . ,  m =  such that adjacent rankings in this sequence differ only
by a pairwise exchange of items b1 , b2  i  i+1 . We will now show that at each step
along this sequence, h( t ) > 0 implies that h( t+1 ) > 0, which will prove that h() > 0.
Suppose now that h( t ) > 0 and that  t and  t+1 differ only by the relative ranking of
items b1 , b2  i  i+1 (without loss of generality, we will assume that  t (b2 ) <  t (b1 ) and
 t+1 (b1 ) <  t+1 (b2 )).
The idea of the following paragraph is to use the previous lemma (Lemma 28) to prove
that  t+1 has positive probability and to do so, it will be necessary to argue that there
exists some ranking  0 such that h( 0 ) > 0 and  0 (b1 ) <  0 (b2 ) (i.e.,  0 disagrees with  t
on the relative ranking of b1 , b2 ). Let  be any element of S . If a1  i , rearrange 
such that a1 is ranked first among elements of i . If a2  i+1 , further rearrange  such
that a2 is ranked last among elements of i+1 . Note that  is still an element of S  after
the possible rearrangements and therefore h() > 0. We can assume that (b2 ) < (b1 )
since otherwise we will have shown what we wanted to show. Thus the relative ordering of
a1 , a2 , b1 , b2 within  is a1 |b2 |b1 |a2 . Note that we treat the case where the items a1 , a2 , b1 , b2
are distinct, but the same argument follows in the cases when a1 = b2 or a2 = b1 .
Now since  disagrees with S  on the relative ordering of a1 , a2 by assumption (and
hence disagrees with ), we apply Lemma 28 to conclude that swapping the relative ordering
of a1 , a2 within  (obtaining a2 |b2 |b1 |a1 ) results in a ranking,  0 , such that h( 0 ) > 0.
Finally, observe that  and  0 must now disagree on the relative ranking of a2 , b2 , and
invoking Lemma 28 again shows that we can swap the relative ordering of a2 , b2 within 
(obtaining a1 |a2 |b1 |b2 ) to result in a ranking  0 such that h( 0 ) > 0. This element  0 ranks
b1 before b2 , which is what we wanted to show.
We have shown that there exist rankings which disagree on the relative ordering of b1
and b2 with positive probability under h. Again applying Lemma 28 shows that we can swap
the relative ordering of items b1 , b2 within  t to obtain  t+1 such that h( t+1 ) > 0, which
concludes the proof.
526

fiEfficient Inference With Partial Rankings

A.4 Uniformity of C Functions Over a Partial Ranking
We have thus far shown that any element of C must be supported on some partial ranking.
In the following, we show that (up to a certain class of exceptions), such an element must
assign uniform probability to all members of this partial ranking.
Theorem 41. If h is any completely decomposable function supported on a partial ranking
S  = 1 | . . . |r where |i | =
6 2 for all i = 1, . . . , r, then h is uniform on S  (i.e.,
1
Q
h() =
|i | for all   S ).
i

To establish Theorem 41, we must establish two supporting results: (1) Lemma 42 which
factors h into r smaller completely decomposable functions, each of which is nonzero everywhere on its domain, and (2) Theorem 43 which establishes uniformity for any completely
decomposable function which is nonzero everywhere on its domain.
Lemma 42. Any completely decomposable Q
function, h, supported on the partial ranking
S  = 1 | . . . |r , must factor as: h() = ri=1 h((i )), where each factor distribution
h((i )) is itself a completely decomposable function on Si .
Proof. Since h is completely decomposable, we have that (i ) is riffle independent of
(\i ) for each i. Since h is supported on the partial ranking S  = )1 | . . . |r , however,
the interleaving of i with its complement is deterministic and therefore we conclude in fact
that (i ) is fully independent
of (\i ). Since (i )  (\i ) for each i, we have the
Qr
factorization: h() = i=1 h((i )).
We now turn to establishing that each factor h((i )) is itself a completely decomposable
observation. Fix i = 1 (without loss of generality) and consider any partition of the set 1
into subsets A  B. We would like to see that the sets A and B are riffle independent of each
other with respect to h((1 )). Since h is assumed to be completely decomposable, we know
that A is riffle independent of its complement, B(\1 ). In other words, if B = B(\1 ),
then the variables A (), AB , B (the relative ranking of A, the interleaving of A with all
remaining items, and the relative ranking of all remaining items, respectively) are mutually
independent. We then observe that (1) the interleaving of A and B, AB , is a deterministic
function of the interleaving of AB and (2) the relative ranking of B, B , is a deterministic
function of B , thus proving that A , AB and B are mutually independent and hence that
A and B are riffle independent.
Theorem 43. Let h a completely decomposable function such that h() > 0 for all   Sn
for n > 2. Then for any two rankings 1 , 2 which differ by a single transposition, we have
h(1 ) = h(2 ).
Our proof strategy for Theorem 43 will involve examining the ratio between the two
probabilities h(1 ) and h(2 ). We then define an operation transforming 1 and 2 into new
rankings 10 and 20 such that the ratio between the rankings is preserved (i.e., h(1 )/h(2 ) =
h(10 )/h(20 )). By performing a sequence of such ratio-preserving operations, we show that:
h(1 )
h(2 )
=
,
h(2 )
h(1 )
from which Theorem 43 easily follows.
527

fiHuang, Kapoor & Guestrin

We will use two types of operations which transform a ranking into a new ranking: (1)
changing the interleaving of two sets A and B within a ranking , and (2), changing the
relative ranking of a set A within a ranking . More precisely, given a ranking  and a
partitioning of the item set into subsets A and B, we can uniquely index  as a triplet
(, A , B ), where  = A,B (), A = A (), and B = B (). The two operations are
defined as follows:
1. Changing the interleaving of A, B within  to  0 : yields the new ranking  0 which is
indexed by ( 0 , A , B ).
0 (or  0 ): yields the new
2. Changing the relative ranking of A (or B) within  to A
B
0
0
0
ranking  which is indexed by (, A , B ) [or (, A , B )].

If we use the above operations to obtain from 10 and 20 , we are interested in conditions
under which this transformation is ratio-preserving (i.e., h(1 )/h(2 ) = h(10 )/h(20 )). The
following lemma provides sufficient conditions for ratio-preservation.
Lemma 44. Let h be any completely decomposable function and consider 1 , 2  Sn such
that h(2 ) > 0. Then for any partitioning of the item set into subsets A and B, we have:
1. If 1 and 2 match on the interleaving of A and B (i.e., A,B (1 ) = AB (2 )), then
h(10 )
h(1 )
0
0
h(2 ) = h(20 ) , where 1 and 2 are formed by changing the interleaving of the sets A
and B within 1 and 2 to be any new interleaving  0 .
2. If 1 and 2 match on the relative ranking of A (or B) (i.e., A (1 ) = A (2 ) (or
h( 0 )
h(1 )
= h(10 ) , where 10 and 20 are formed by changing the
B (1 ) = B (2 ))), then h(
2)
2
0
relative ranking of set A (or B) within 1 and 2 to be any new relative ranking A
0
(or B ).
Proof. Since the proofs of parts 1 and 2 are nearly identical, we just prove part 1 here.
Since h  C, the sets A and B are riffle independent by assumption, and hence we have the
factorizations:
h(1 )
m(1 )  f (1A )  g(2B )
=
.
h(2 )
m(2 )  f (2A )  g(2B )

If 1 and 2 match on the interleaving of the sets A and B, then we have that  = 1 = 2 ,
and thus the interleaving terms, m(1 ) and m(2 ) are the same in both the numerator and
denominator.
On the other hand, if we examine the ratio between h(10 ) and h(20 ), we also see that
the interleaving terms must cancel:
h(1 )
m(10 )  f (1A )  g(2B )
=
.
h(2 )
m(20 )  f (2A )  g(2B )

We therefore have that:
f (1A )  g(piB
h(10 )
h(1 )
1 )
=
=
.
A
B
h(2 )
h(20 )
f (2 )  g(2 )

528

fiEfficient Inference With Partial Rankings

Having now established Lemma 44, we turn to establishing three short claims (using
the lemma) that will allow us to prove finally prove Theorem 43. It is interesting to note
that we require n > 2 (strictly) in claim III below in which we swap the order of i and j
in numerator and denominator. The third item k in our proof below can be thought of as
playing the role of a dummy variable analogous to the temporary storage variables that one
might use in implementing a swap function. The necessity of this third item is precisely why
our result does not hold in the special case that n = 2.
Proposition 45. Let h : Sn  R be a completely decomposable function with n > 2 with
h() > 0 for all   Sn . We have the following equivalences (where in each of the below
ratios, entries which have not been explicitly written out are assumed to match identically
in both the numerator and denominator).
I.

II.

h(i|j| . . . |k| . . . )
h(i|j|k| . . . )
=
.
h(j|i| . . . |k| . . . )
h(j|i|k| . . . )
h(. . . |i| . . . |j| . . . )
h(i|j| . . . )
=
.
h(. . . |j| . . . |i| . . . )
h(j|i| . . . )

III.

h(j|i|k| . . . )
h(i|j|k| . . . )
=
.
h(j|i|k| . . . )
h(i|j|k| . . . )

Proof.
I. Equality holds in I since 1 and 2 match on the interleaving of the sets A = {k} with
B = \{k}. Thus we can change the interleaving of A and B in both 1 and 2 so
that item k is inserted in rank 3 while preserving the ratio.
II. Equality holds in II since 1 and 2 match on the interleaving of the sets A = {i, j}
with B = \{i, j}. Thus we can change the interleaving of A and B in both 1 and
2 so that items i and j occupy the first two ranks while preserving the ratio between
h(1 ) and h(2 ).
III. In the following we use 1 and 2 to refer to the arguments in the numerator and
denominator, respectively, of the preceding line.
h(i|j|k| . . . )
h(i|k|j| . . . )
=
,
h(j|i|k| . . . )
h(k|i|j| . . . )
h(j|i|k| . . . )
=
,
h(j|k|i| . . . )
h(i|j|k| . . . )
=
,
h(i|k|j| . . . )
h(k|j|i| . . . )
=
,
h(k|i|j| . . . )
h(j|i|k| . . . )
=
,
h(i|j|k| . . . )

(since 1 , 2 match on the relative ranking of {j, k})
(since 1 , 2 match on the interleaving of {j} with \{j})
(since 1 , 2 match on the relative ranking of {i, j})
(since 1 , 2 match on the relative ranking of {i, k})
(since 1 , 2 match on the interleaving of {k} with \{k}).

529

fiHuang, Kapoor & Guestrin

Proof. (of Theorem 43) We want to show that if two rankings differ by a single transposition,
then they are assigned equal probability under h. Suppose then that 2 is obtained from
1 by swapping the ranks of items i and j. Additionally, let k be any item besides i and j
(such an item must exist since n > 2). In the following, we use Proposition 45 to show that
h(1 )/h(2 ) = h(2 )/h(1 ). As before, entries which have not been explicitly written out
are assumed to match identically in both the numerator and denominator.
h(. . . |i| . . . |j| . . . )
h(i|j| . . . )
h(1 )
=
=
, (by Prop. 45, Part II)
h(2 )
h(. . . |j| . . . |i| . . . )
h(j|i| . . . )
h(i|j| . . . |k| . . . )
h(i|j|k| . . . )
=
=
, (by Prop. 45, Part I)
h(j|i| . . . |k| . . . )
h(j|i|k| . . . )
h(j|i|k| . . . )
, (by Prop. 45, Part III)
=
h(i|j|k| . . . )
h(j|i| . . . |k| . . . )
=
, (by Prop. 45, Part I)
h(i|j| . . . |k| . . . )
h(j|i| . . . )
h(. . . |j| . . . |i| . . . )
=
=
, (by Prop. 45, Part II)
h(i|j| . . . )
h(. . . |i| . . . |j| . . . )
h(2 )
=
.
h(1 )

Since we have assumed h(1 ) and h(2 ) > 0, we must conclude that h(1 ) = h(2 ).
Finally, we assemble all of our supporting results to prove Theorem 41.
Proof. (of Theorem 41) By Lemma 42, a completely decomposable function h must factor
as:
r
Y
h() =
h((i )),
(A.2)
i=1

where each factor distribution h((i )) is itself a completely decomposable function on Si .
By assumption, |i | 6= 2. If |i | = 1, then its corresponding factor h((i )) must trivially
be uniform. Otherwise, we have that |i | > 2. In this latter case, we apply Theorem 43 to
h((i )) to show that it must assign equal probability to any two rankings that differ by a
single transposition. However, given any rankings 1 , 2  Si , we can obtain a sequence of
transpositions that transforms 1 into 2 , and therefore, Theorem 43 in fact implies that the
factor h((i )) is constant on all inputs. Having proved that each factor in Equation A.2 is
constant, we conclude that h must be constant on its support.

References
Ailon, N. (2007). Aggregation of partial rankings, p-ratings and top-m lists. In Proceedings
of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, SODA 07,
New Orleans, Louisiana.
Bartholdi, J. J., Tovey, C. A., & Trick, M. (1989). Voting schemes for which it can be
difficult to tell who won. Social Choice and Welfare, 6(2).
530

fiEfficient Inference With Partial Rankings

Busse, L. M., Orbanz, P., & Buhmann, J. (2007). Cluster analysis of heterogeneous rank
data. In The 24th Annual International Conference on Machine Learning, Corvallis,
Oregon.
Fligner, M. A., & Verducci, J. S. (1986). Distance based ranking models. Journal of the
Royal Statistical Society, 48.
Freund, Y., Iyer, R., Schapire, R. E., & Singer, Y. (2003). An efficient boosting algorithm for
combining preferences. Journal of Machine Learning Research (JMLR), 4, 933969.
Friedman, N. (1997). Learning belief networks in the presence of missing values and hidden variables. In Proceedings of the Fourteenth International Conference on Machine
Learning, ICML 97, pp. 125133, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Friedman, N. (1998). The bayesian structural em algorithm. In The 14th Conference on
Uncertainty in Artificial Intelligence, UAI 98, Madison, Wisconsin.
Gormley, C., & Murphy, B. (2007). A latent space model for rank data. In Proceedings
of the 2006 conference on Statistical network analysis, ICML06, pp. 90102, Berlin,
Heidelberg. Springer-Verlag.
Huang, J., Kapoor, A., & Guestrin, C. (2011). Efficient probabilistic inference with partial
ranking queries. In The 27th Conference on Uncertainty in Artificial Intelligence, UAI
11, Barcelona, Spain.
Huang, J. (2011). Probabilistic Reasoning and Learning on Permutations: Exploiting Structural Decompositions of the Symmetric Group. Ph.D. thesis, Carnegie Mellon University.
Huang, J., & Guestrin, C. (2009). Riffled independence for ranked data. In Bengio, Y.,
Schuurmans, D., Lafferty, J., Williams, C. K. I., & Culotta, A. (Eds.), Advances in
Neural Information Processing Systems 22, NIPS 08, pp. 799807. MIT Press.
Huang, J., & Guestrin, C. (2010). Learning hierarchical riffle independent groupings from
rankings. In Proceedings of the 27th Annual International Conference on Machine
Learning, ICML 10, pp. 455462, Haifa, Israel.
Huang, J., & Guestrin, C. (2012). Uncovering the riffled independence structure of ranked
data. Electronic Journal of Statistics, 6, 199230.
Huang, J., Guestrin, C., & Guibas, L. (2008). Efficient inference for distributions on permutations. In Platt, J., Koller, D., Singer, Y., & Roweis, S. (Eds.), Advances in Neural
Information Processing Systems 20, NIPS 07, pp. 697704. MIT Press, Cambridge,
MA.
Huang, J., Guestrin, C., & Guibas, L. J. (2009). Fourier theoretic probabilistic inference
over permutations. Journal of Machine Learning Research (JMLR), 10, 9971070.
Joachims, T. (2002). Optimizing search engines using clickthrough data. In Proceedings of
the eighth ACM SIGKDD international conference on Knowledge discovery and data
mining, KDD 02, pp. 133142, New York, NY, USA. ACM.
Kondor, R., Howard, A., & Jebara, T. (2007). Multi-object tracking with representations
of the symmetric group. In Meila, M., & Shen, X. (Eds.), Proceedings of the Eleventh
531

fiHuang, Kapoor & Guestrin

International Conference on Artificial Intelligence and Statistics March 21-24, 2007,
San Juan, Puerto Rico, Vol. Volume 2 of JMLR: W&CP.
Kondor, R. (2008). Group theoretical methods in machine learning. Ph.D. thesis, Columbia
University.
Lebanon, G., & Lafferty, J. (2003). Conditional models on the ranking poset. In S. Becker,
S. T., & Obermayer, K. (Eds.), Advances in Neural Information Processing Systems
15, NIPS 02, pp. 415422, Cambridge, MA. MIT Press.
Lebanon, G., & Mao, Y. (2008). Non-parametric modeling of partially ranked data. In Platt,
J. C., Koller, D., Singer, Y., & Roweis, S. (Eds.), Advances in Neural Information
Processing Systems 20, NIPS 07, pp. 857864, Cambridge, MA. MIT Press.
Lu, T., & Boutilier, C. (2011). Learning mallows models with pairwise preferences. In
The 28th Annual International Conference on Machine Learning, ICML 11, Bellevue,
Washington.
Marden, J. I. (1995). Analyzing and Modeling Rank Data. Chapman & Hall.
Meila, M., Phadnis, K., Patterson, A., & Bilmes, J. (2007). Consensus ranking under the
exponential model. Tech. rep. 515, University of Washington, Statistics Department.
White, R., & Drucker, S. (2007). Investigating behavioral variability in web search. In
Proceedings of the 16th international conference on World Wide Web, WWW 07,
Banff, Alberta, Canada. ACM.

532

fi