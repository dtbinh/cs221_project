Journal of Artificial Intelligence Research 9 (1998) 37-97

Submitted 2/98; published 9/98

The Divide-and-Conquer Subgoal-Ordering Algorithm
for Speeding up Logic Inference
Oleg Ledeniov
Shaul Markovitch

olleg@cs.technion.ac.il
shaulm@cs.technion.ac.il

Computer Science Department
Technion { Israel Institute of Technology
Haifa 32000, Israel

Abstract

It is common to view programs as a combination of logic and control: the logic part
defines what the program must do, the control part { how to do it. The Logic Programming paradigm was developed with the intention of separating the logic from the control.
Recently, extensive research has been conducted on automatic generation of control for
logic programs. Only a few of these works considered the issue of automatic generation of
control for improving the eciency of logic programs. In this paper we present a novel algorithm for automatic finding of lowest-cost subgoal orderings. The algorithm works using
the divide-and-conquer strategy. The given set of subgoals is partitioned into smaller sets,
based on co-occurrence of free variables. The subsets are ordered recursively and merged,
yielding a provably optimal order. We experimentally demonstrate the utility of the algorithm by testing it in several domains, and discuss the possibilities of its cooperation with
other existing methods.

1. Introduction
It is common to view programs as a combination of logic and control (Kowalski, 1979). The
logic part defines what the program must do, the control part { how to do it. Traditional
programming languages require that the programmers supply both components. The Logic
Programming paradigm was developed with the intention of separating the logic from the
control (Lloyd, 1987). The goal of the paradigm is that the programmer specifies the logic
without bothering about the control, which should be supplied by the interpreter.
Initially, most practical logic programming languages, such as Prolog (Clocksin & Mellish, 1987; Sterling & Shapiro, 1994), did not include the means for automatic generation of
control. As a result, a Prolog programmer had to implicitly define the control by the order of
clauses and of subgoals within the clauses. Recently, extensive research has been conducted
on automatic generation of control for logic programs. A major part of this research is concerned with control that affects correctness and termination of logic programs (De Schreye
& Decorte, 1994; Somogyi, Henderson, & Conway, 1996b; Cortesi, Le Charlier, & Rossi,
1997). Only a few of these works consider the issue of automatic generation of control for
improving the eciency of logic programs. Finding a good ordering that leads to ecient
execution requires a deep understanding of the logic inference mechanism. Hence, in many
cases, only expert programmers are able to generate ecient programs. The problem intensifies with the recent development of the field of inductive logic programming (Muggleton
c 1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiLedeniov & Markovitch

& De Raedt, 1994). There, logic programs are automatically induced by learning. Such
learning algorithms are commonly built with the aim of speeding up the induction process
without considering the eciency of resulting programs.
The goal of the research described in this paper is to design algorithms that automatically find ecient orderings of subgoal sequences. Several researchers have explored the
problem of automatic reordering of subgoals in logic programs (Warren, 1981; Naish, 1985b;
Smith & Genesereth, 1985; Natarajan, 1987; Markovitch & Scott, 1989). The general subgoal ordering problem is known to be NP-hard (Ullman, 1982; Ullman & Vardi, 1988).
Smith and Genesereth (1985) and Markovitch and Scott (1989) present search algorithms
for finding optimal orderings. These algorithms are general and carry exponential costs for
non-trivial sets of subgoals. Natarajan (1987) describes an ecient algorithm for the special
case where subgoals in the set do not share free variables.
In this paper we present a novel algorithm for subgoal ordering. We call two subgoals
that share a free variable dependent. Unlike Natarajan's approach, which can only handle
subgoal sets that are completely independent, our algorithm can deal with any subgoal
set, while making maximal use of the existing dependencies for acceleration of the ordering
process. In the worst case the algorithm { like that of Smith and Genesereth { is exponential.
Still, in most practical cases, our algorithm exploits subgoal dependencies and finds optimal
orderings in polynomial time.
We start with an analysis of the ordering problem and demonstrate its importance
through examples. We then show how to compute the cost of a given ordering based on
the cost and the number of solutions of the individual subgoals. We describe the algorithm
of Natarajan and the algorithm of Smith and Genesereth and show how the two can be
combined into an algorithm that is more ecient and general than each of the two. We
show drawbacks of the combined algorithm and introduce the new algorithm, which avoids
these drawbacks. We call it the Divide-and-Conquer algorithm (dac algorithm). We prove
the correctness of the algorithm, discuss its complexity and compare it to the combined
algorithm. The dac algorithm assumes knowledge of the cost and the number of solutions
of the subgoals. This knowledge can be obtained by machine learning techniques such as
those employed by Markovitch and Scott (1989). Finally, we test the utility of our algorithm
by running a set of experiments on artificial and real domains.
The dac algorithm for subgoal ordering can be combined with many existing methods
in logic programming, such as program transformation, compilation, termination control,
correctness verification, and others. We discuss the possibilities of such combinations in the
concluding section.
Section 2 states the ordering problem. Section 3 describes existing ordering algorithms
and their combination. Section 4 presents the new algorithm. Section 5 discusses the
acquisition of the control knowledge. Section 6 contains experimental results. Section 7
contains a discussion of practical issues, comparison with other works and conclusions.

2. Background: Automatic Ordering of Subgoals
We start by describing the conventions and assumptions accepted in this paper. Then we
demonstrate the importance of subgoal ordering and discuss its validity. Finally, we present
a classification of ordering methods and discuss related work.
38

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

2.1 Conventions and Assumptions

All constant, function and predicate symbols in programs begin with lower case letters,
while capital letters are reserved for variables. Braces are used to denote unordered sets
(e.g., fa; b; cg), and angle brackets are used for ordered sequences (e.g., ha; b; ci). Parallel
lines (k) denote concatenations of ordered sequences of subgoals. When speaking about
abstract subgoals (and not named predicates of concrete programs), we denote separate
subgoals by capital letters (A; B : : :), ordered sequences of subgoals by capitalized vectors
~ O~ S : : :), and sets of subgoals by calligraphic capitals (B; S : : :).  (S ) denotes the set of
(B;
all permutations of S .
We assume that the programs we work with are written in pure Prolog, i.e., without cut
operators, meta-logical or extra-logical predicates. Alternatively, we can assume that only
pure Prolog sub-sequences of subgoals are subject to ordering. For example, given a rule of
the form
A B1 ; B2; B3; !; B4; B5; B6:
only its final part fB4 ; B5; B6g can be ordered (without affecting the solution set).
In this work we focus upon the task of finding all the solutions to a set of subgoals.

2.2 Ordering of Subgoals in Logic Programs
A logic program is a set of clauses:

A

B1 ; B2 ; : : :; Bn :

(n  0)

where A; B1 ; : : :; Bn are literals (predicates with arguments). To use such a clause for
proving a goal that matches A, we must prove that all B -s hold simultaneously, under
consistent bindings of the free variables. A solution is such a set of variable bindings. The
solution set of a goal is the bag of all its solutions created by its program.
A computation rule defines which subgoal will be proved next. In Prolog, the computation rule always selects the leftmost subgoal in a goal. If a subgoal fails, backtracking is
performed { the proof of the previous subgoal is re-entered to generate another solution.
For a detailed definition of the logic inference process, see Lloyd (1987).

Theorem 1 The solution set of a set of subgoals does not depend on the order of their

execution.

Proof: When we are looking for all solutions, the solution set does not depend on the

computation rule chosen (Theorems 9.2 and 10.3 in Lloyd, 1987). Since a transposition of
subgoals in an ordered sequence can be regarded as a change of the computation rule (the
subgoals are selected in different order), such transposition does not change the solution
set.
2
This theorem implies that we may reorder subgoals during the proof derivation. Yet the
eciency of the derivation strongly depends on the chosen order of subgoals. The following
example illustrates how two different orders can lead to a large difference in execution
eciency.
39

fiLedeniov & Markovitch

parent(abraham,isaac).
parent(sarah,isaac).
parent(abraham,ishmael).
parent(isaac,esav).
parent(isaac,jakov).

... More parent clauses ...

male(abraham).
male(isaac).
male(ishmael).
male(jakov).
male(esav).

... More male clauses ...

brother(X,Y)
male(X), parent(W,X), parent(W,Y), X=/=Y.
father(X,Y)
male(X), parent(X,Y).
uncle(X,Y)
parent(Z,Y), brother(X,Z).

... More rules of relations ...

Figure 1: A small fragment of a Biblical database describing family relationships.

Example 1
Consider a Biblical family database such as the one listed in Figure 1 (a similar database
appears in the book by Sterling & Shapiro, 1994). The body of the rule defining the
uncle-nephew (or uncle-niece) relation can be ordered in two ways:
1. uncle(X,Y) brother(X,Z), parent(Z,Y).
2. uncle(X,Y) parent(Z,Y), brother(X,Z).
To prove the goal uncle(ishmael,Y) using the first version of the rule, the interpreter will
first look for Ishmael's siblings (and find Isaac) and then for the siblings' children (Esav
and Jacov). The left part of Figure 2 shows the associated proof tree with a total of 10
nodes. If we use the second version of the rule, the interpreter will create all the parentchild pairs available in the database, and will test for each parent whether he (or she) is
Ishmael's sibling. The right part of Figure 2 shows the associated proof tree with a total
of 4(N , 2) + 6  2 + 2 = 4N + 6 nodes, where N is the number of parent-child pairs in the
database. The tree contains two success branches and N , 2 failure branches; in the figure
we show one example of each. While the two versions of the rule yield identical solution
sets, the first version leads to a much smaller tree and to a faster execution.
Note that this result is true only for the given mode (bound,free) of the head literal;
for the mode (free,bound), as in uncle(X,jacov), the outcome is the contrary: the second
version of the rule yields a smaller tree.

2.3 Categories of Subgoal Ordering Methods

Assume that the current conjunctive goal (the current resolvent) is fA1; A2g. Assume that
we use the rule \A1 A11; A12:" to reduce A1 . According to Theorem 1, the produced
resolvent, fA11 ; A12; A2g, can be executed in any order. We call ordering methods that
allow any permutation of the resolvent interleaving ordering methods, since they permit
40

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

uncle(X,Y)

brother(X,Z), parent(Z,Y). uncle(X,Y)

uncle(ishmael,Y)

uncle(ishmael,Y)

parent(Z,Y), brother(ishmael,Z)

brother(ishmael,Z), parent(Z,Y)

Z=adam,
Y=cain

male(ishmael), parent(W,ishmael), parent(W,Z),
ishmael =/= Z, parent(Z,Y)

brother(ishmael,adam)
parent(W,ishmael), parent(W,Z),
ishmael =/= Z, parent(Z,Y)
W=abraham
parent(abraham,Z), ishmael=/=Z, parent(Z,Y)
Z=ishmael
Z=isaac
ishmael =/= ishmael,
parent(ishmael,Y)

isaac =/= ishmael,
parent(isaac,Y)

parent(Z,Y), brother(X,Z).

Z=isaac,
Y=jacov

other
parent-child
pairs

male(ishmael), parent(W,ishmael),
parent(W,adam), ishmael =/= adam
parent(W,ishmael), parent(W,adam),
ishmael =/= adam
W=abraham
parent(abraham,adam), ishmael =/=adam

parent(isaac,Y)
Y=esav
Y=jacov

brother(ishmael,isaac)

male(ishmael), parent(W,ishmael),
parent(W,isaac), ishmael=/=isaac
parent(W,ishmael), parent(W,isaac),
ishmael =/= isaac
W=abraham
parent(abraham,isaac), ishmael=/=isaac
ishmael =/= isaac

Figure 2: Two proof trees obtained with different orderings of a single rule in Example 1.
interleaving of subgoals from different rule bodies. When ordering is performed only on
rule bodies before using them for reduction, the method is non-interleaving. In the above
example, interleaving methods will consider all 6 permutations of the resolvent, while noninterleaving methods will consider only two orderings: hA11 ; A12; A2i and hA12; A11; A2i.
Interleaving ordering methods deal with significantly more possible orderings than noninterleaving methods. That means that they can find more ecient orderings. On the
other hand, the space of possible orderings may become prohibitively large, requiring too
many computational resources.
Subgoal ordering can take place at various stages of the proof process. We divide all
subgoal ordering methods into static, semi-dynamic and dynamic.

 Static ordering: The rule bodies are ordered before the execution starts. No ordering takes place during the execution.

 Semi-dynamic ordering: Whenever a rule is selected for reduction, its body is
ordered. The order of its subgoals does not change after the reduction takes place.

 Dynamic ordering: The ordering decision is made at each inference step.
Static methods add no overhead to the execution time. However, the optimal ordering
of a rule often depends on a particular binding of a variable, which can be known only at
run-time. For instance, in Example 1 we saw that the first ordering of the rule is better
for proving the goal uncle(ishmael,Y). And yet, for the goal uncle(X,jacov), it is the
second ordering that yields more ecient execution. To handle such cases statically, we
must compute the optimal ordering for each possible binding.
41

fiLedeniov & Markovitch

Obviously, static ordering can only be non-interleaving. The dynamic method is more
exible, since it can use more updated knowledge about variable bindings, but it also carries
the largest runtime overhead, since it is invoked several times for each use of a rule body.
The semi-dynamic method is a compromise between the two: it is more powerful than the
static method, because it can dynamically propose different orderings for different instances
of the same rule; it also carries less overhead than the dynamic method, because it is invoked
only once for each use of a rule body.
The total time of proving a goal is the sum of the ordering time and the inference time.
Interleaving and dynamic methods have the best potential for reducing the inference time,
but may significantly augment the ordering time. Static methods do not devote time to
ordering (it is done off-line), but have a limited potential for reducing the inference time.
The algorithms described in this paper can be used for all categories of ordering methods,
although in the experiments described in Section 6 we have only implemented semi-dynamic,
non-interleaving ordering methods: on each reduction, the rule body is ordered and added
to the left end of the resolvent, and then the leftmost literal of the resolvent is selected for
the next reduction step.

2.4 Related Work

The problem of computational ineciency of logic inference was the subject of extensive
research. The most obvious aspect of this ineciency is the possible non-termination of
a proof. Several researchers developed compile-time and run-time techniques to detect
and avoid infinite computations (De Schreye & Decorte, 1994). A certain success was
achieved in providing more advanced control through employment of co-routining for interpredicate synchronization purposes (Clark & McCabe, 1979; Porto, 1984; Naish, 1984).
Also, infinite computations can be avoided by pruning infinite branches that do not contain
solutions (Vasak & Potter, 1985; Smith, Genesereth, & Ginsberg, 1986; Bol, Apt, & Klop,
1991). In the NAIL! system (Morris, 1988) subgoals are automatically reordered to avoid
nontermination.
Still, even when the proof is finite, it is desirable to make it more ecient. Several
researchers studied the problem of clause ordering (Smith, 1989; Cohen, 1990; Etzioni,
1991; Laird, 1992; Mooney & Zelle, 1993; Greiner & Orponen, 1996). If we are looking for
all the solutions of a goal, then the eciency does not depend on the clause order (assuming
no cuts). Indeed, if some predicate has m clauses, and for some argument bindings these
clauses produce all their solutions in times t1 ; t2 : : :tm , then all solutions of the predicate
under these bindings are obtained in time t1 + t2 + : : : + tm , regardless of the order in
which the clauses are applied. Different clause orderings correspond to different orders in
which branches are selected in a proof tree; if we traverse the entire tree, then the number
of traversal steps does not depend on the order of branch selection, though the order of
solutions found does depend on it.
Subgoal ordering, as was demonstrated in Example 1, can significantly affect the eciency of proving a goal. There are two major approaches to subgoal ordering. The first
approach uses various heuristics to order subgoals, for example:

 Choose a subgoal whose predicate has the smallest number of matching clauses (Minker,
1978).

42

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

 Prefer a subgoal with more constants (Minker, 1978).
 Choose a subgoal with the largest size, where the size is defined as the number of
occurrences of predicate symbols, function symbols, and variables (Nie & Plaisted,
1990).

 Choose a subgoal with the largest mass, where the mass of a subgoal depends on
the frequency of its arguments and sub-arguments in the entire goal (Nie & Plaisted,
1990).

 Choose a subgoal with the least number of solutions (Warren, 1981; Nie & Plaisted,
1990).

 Apply \tests" before \generators" (Naish, 1985a).
 Prefer calls that fail quickly (Naish, 1985b).
The heuristic methods usually execute quickly, but may yield suboptimal orderings.
The second approach, which is adopted in this paper, aims at finding optimal orderings (Smith & Genesereth, 1985; Natarajan, 1987; Markovitch & Scott, 1989). Natarajan
proposed an ecient way to order a special sort of subgoal set (where all subgoals are independent), while Smith and Genesereth proposed a general, but inecient algorithm. In
the following section we build a unifying framework for dealing with subgoal ordering and
describe variations on Natarajan's and Smith and Genesereth's algorithms. We also show
how the two can be combined for increased eciency.

3. Algorithms for Subgoal Ordering in Logic Programs
The goal of the work presented here is to order subgoals for speeding up logic programs. This
section starts with an analysis of the cost of executing a sequence of subgoals. The resulting
formula is the basis for the subsequent ordering algorithms. Then we discuss dependence
of subgoals and present existing ordering algorithms for independent and dependent sets of
subgoals. Finally, we combine these algorithms into a more general and ecient one.

3.1 The Cost of Executing a Sequence of Subgoals

In this subsection we analyze the cost of executing a sequence of subgoals. The analysis
builds mainly on the work of Smith and Genesereth (1985).
Let S = fA1; A2; : : :Ak g be a set of subgoals and b be a binding. We denote Sols(S ) to
be the solution set of S , and define Sols(;) = f;g. We denote Ai jb to be Ai whose variables
are bound according to b (Ai j; = Ai ). Finally, we denote Cost(Ai jb ) to be the amount of
resources needed for proving Ai jb . Cost(Ai jb ) should reect the time complexity of proving
Ai under binding b. For example, the number of unification steps is a natural measure of
complexity for logic programs (Itai & Makowsky, 1987).
To obtain the cost of finding all the solutions of an ordered sequence of subgoals

S~ = hA1; A2; A3; : : :Ani;
43

(1)

fiLedeniov & Markovitch

we note that the proof-tree of A1 is traversed only once, the tree of A2 is traversed once
for each solution generated by A1 , the tree of A3 { once for each solution of fA1; A2g, etc.
Consequently, the total cost of proving Equation 1 is

Cost(hA1; : : :An i) = Cost(A1) +
=

X

Cost(A2jb) + : : : +

b2Sols(fA1 g)

n
X

X

Cost(An jb ) =

b2Sols(fA1 ;:::An,1 g)

X

Cost(Aijb):
i=1 b2Sols(fA1 ;:::Ai,1 g)

(2)

To compute Equation 2 one must know the cost and the solution set for each subgoal
under each binding. To reduce the amount of information needed, we derive an equivalent
formula, which uses average cost and average number of solutions.

Definition: Let B be a set of subgoals, A a subgoal. Define cost(A)jB to be the average
cost of A over all solutions of B and nsols(A)jB to be its average number of solutions over
all solutions of B:
8
>
P (A); Cost(Ajb) B = ;
< Cost
B
cost(A)jB = > b2Sols
; B=
6 ;; Sols(B) =6 ;
j
Sols(B)j
: undefined;
B 6= ;; Sols(B) = ;
( )

8
>
j;
B=;
< jPSols(fAgj)Sols
(
f
A
j
g
)
j
b
B
nsols(A)jB = > b2SolsjSols
; B=
6 ;; Sols(B) 6= ;
: undefined; (B)j
B 6= ;; Sols(B) = ;
( )

From the first definition, it follows that:

X

Cost(Aijb ) = jSols(fA1; : : :Ai,1 g)j  cost(Ai)jfA ;:::Ai, g :
1

b2Sols(fA1 ;:::Ai,1 g)

1

(3)

If we apply the second definition recursively, we obtain

jSols(fA1; : : :Aig)j =
=
=

X

jSols(fAijbg)j
b2Sols(fA1 ;:::Ai,1 g)
jSols(fA1; : : :Ai,1 g)j  nsols(Ai)jfA1;:::Ai,1g
Yi
: : : = nsols(Aj )jfA1:::Aj,1 g:
j =1

(4)

Note that we defined Sols(;) = f;g; thus, these equations hold also for i = 1. Incorporation
of Equations 3 and 4 into Equation 2 yields

Cost(hA1; A2; : : :An i) =

20i,1
n
X
4@ Y
i=1

j =1

1
3
nsols(Aj )jfA :::Aj, gA  cost(Ai )jfA :::Ai, g5 :
1

44

1

1

1

(5)

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

For each subgoal Ai , its average cost is multiplied by the total number of solutions of
all the preceding subgoals. We can define average cost and number of solutions for every
continuous sub-sequence of Equation 1: 8k1 ; k2; 1  k1  k2  n,
cost(hA1; : : :Ak i)j; , cost(hA1; : : :Ak ,1 i)j;
(6)
cost(hAk ; : : :Ak i)jfA ;:::Ak , g =
20 nsols(hA1; : : :Ak ,1 i)1j;
3
1

2

2

1

1

1

1

=

k
i,1
X
4@ Y
2

i=k1

j =k1

1

nsols(Aj )jfA ;:::Aj, g A  cost(Ai )jfA ;:::Ai, g 5
1

1

1

1

k
Y
nsols(hA1; : : :Ak i)j;
nsols(hAk ; : : :Ak i)jfA ;:::Ak , g =
=
nsols(Ai)jfA ;:::Ai, g
nsols(hA1; : : :Ak ,1 i)j; i=k
1

2

2

1

1

1

1

2

1

1

(7)

1

The values of cost(Ai ) and nsols(Ai ) depend on the position of Ai in the ordered sequence. For example, assume that we want to find Abraham's sons, using the domain of
Example 1. The unordered conjunctive goal is fmale(Y),parent(abraham,Y)g. Let there
be N males in the database (two of them, Isaac and Ishmael, are Abraham's sons):
nsols(male(Y))j; = N
nsols(parent(abraham,Y))j; = 2
nsols(male(Y))jfparent(abraham,Y)g = 1 nsols(parent(abraham,Y))jfmale(Y)g = 2=N
Note that nsols(hmale(Y),parent(abraham,Y)i) = 2 = nsols(hparent(abraham,Y),male(Y)i),
exactly as Theorem 1 predicts.
Having defined the cost of a sequence of subgoals, we can now define the objective of
our ordering algorithms:

Definition: Let S be a set of subgoals. Define (S ) to be set of all permutations of
S . O~ S 2 (S ) is a minimal ordering of S (denoted Min(O~ S ; S )), if its cost according to
Equation 5 is minimal over all possible permutations of S :
Min(O~ S ; S ) () 8OS0 2  (S ) : Cost(O~ S )  Cost(OS0 ):
The total execution time is the sum of the time which is spent on ordering, and the
inference time spent by the interpreter on the ordered sequence. In this paper we focus
upon developing algorithms for minimizing the inference time. Elsewhere (Ledeniov &
Markovitch, 1998a, 1998b) we present algorithms that attempt to reduce the total execution
time.
The values of cost and number of solutions can be obtained in various ways: by exact
computation, by estimation and bounds, and by learning. Let us assume at the moment
that there exists a mechanism that returns the average cost and number of solutions of a
subgoal in time  . In Section 5 we show how this control knowledge can be obtained by
inductive learning.

3.2 Ordering of Independent Sets of Subgoals

The general subgoal ordering problem is NP-hard (Ullman & Vardi, 1988). However, there
is a special case where ordering can be performed eciently: if all the subgoals in the
45

fiLedeniov & Markovitch

given set are independent, i.e. do not share free variables. This section begins with the
definition of subgoal dependence and related concepts. We then show an ordering algorithm
for independent sets and prove its correctness.
3.2.1 Dependence of Subgoals

Definition: Let S and B be sets of subgoals (B is called the binding set of S ). A pair of
subgoals in S is directly dependent under B, if they share a free variable not bound by a
subgoal of B.
A pair of subgoals is indirectly dependent with respect to S and B if there exists a third
subgoal in S which is directly dependent on one of them under B, and dependent (directly
or indirectly) on the other one under B. A pair of subgoals of S is independent under B if
it is not dependent under B (either directly or indirectly). A subgoal is independent of S
under B if it is independent of all members of S under B.
Two subsets S1  S and S2  S are mutually independent under the binding set B if
every pair of subgoals (A1; A2), such that A1 2 S1 and A2 2 S2, is independent under B.
The entire set S is called independent under the binding set B if all its subgoal pairs
are independent under B, and is called dependent otherwise. A dependent set of subgoals
is called indivisible if all its subgoal pairs are dependent under B, and divisible otherwise.
A divisibility partition of S under B, DPart(S ; B), is a partition of S into subsets that
are mutually independent and indivisible under B, except at most one subset which contains
all the subgoals independent of S under B. It is easy to show that DPart(S ; B) is unique.
For example, let S0 = fa; b(X ); c(Y ); d(X; Y ); e(Z ); f (Z; V ); h(W )g. With respect to
S0 and an empty binding set, the pair fb(X ); d(X; Y )g is directly dependent, fb(X ); c(Y )g
is indirectly dependent and fb(X ); e(Z )g is independent. If we represent a set of subgoals

as a graph, where subgoals are vertices and directly dependent subgoals are connected by
edges, then dependence is equivalent to connectivity and indivisible subsets are equivalent
to connected components of size greater than 1. The divisibility partition is the partition
of a graph into connected components, with all the \lonely" vertices collected together, in
a special component. Figure 3 shows an example of such a graph for the set S0 and for
an empty binding set. The whole set is divisible into four mutually independent subsets.
The subsets fe(Z ); f (Z; V )g and fb(X ); c(Y ); d(X; Y )g are indivisible. Elements of the
divisibility partition DPart(S0 ; ;) are shown by dotted lines.
If a subgoal is independent of the set, then its average cost and number of solutions do
not depend on its position within the ordered sequence:
P
Cost(Ajb) jSols(B)j  Cost(A)
=
= Cost(A);
cost(A)jB = b2Sols(B)
jSols(B)j
jSols(B)j

P

b2Sols(B) jSols(fAjbg)j

= jSols(B)j  jSols(fAg)j = jSols(fAg)j:
jSols(B)j
In this case we can omit the binding information and write cost(Ai ) instead of cost(Ai)jfA :::Ai, g ,
and nsols(Ai ) instead of nsols(Ai )jfA :::Ai, g.
In practice, program rule bodies rarely feature independent sets of literals. An example
is the following clause, which states that children like candy:
nsols(A)jB =

jSols(B)j

1

1

1

46

1

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

fa,

b(X), c(Y), d(X,Y), e(Z), f(Z,V), h(W)

a
h(W)

b(X)

e(Z)
f(Z,V)

g)

c(Y)

d(X,Y)

Figure 3: An example of a graph representing a set of subgoals. Directly dependent subgoals

are connected by edges. Independent subgoals and indivisible subsets are equivalent to
connected components (surrounded by dashed lines). The divisibility partition (under
the empty binding set) is shown by dotted lines.
likes(X,Y)

child(X), candy(Y).

More often, independent rule bodies appear not because they are written as such in the
program text, but because some variables are bound in (initially dependent) rule bodies, as
a result of clause head unification. For example, if the rule
father(X,Y)

male(X), parent(X,Y).

is used to reduce father(abraham,W), then X is bound to abraham, and the rule body
becomes independent. Rule bodies often become independent after substitutions are performed in the course of the inference process.
3.2.2 Algorithm for Ordering Independent Sets by Sorting

Let S~ be an ordered sub-sequence of subgoals, B a set of subgoals. We denote
~
cn(S~ )jB = nsols(S)~jB , 1 :
cost(S )jB
The name \cn" reects the participation of cost and nsols in the definition. When the subsequence S~ is independent of other subgoals, the binding information (jB ) can be omitted.
Together, the average cost, average number of solutions, and cn value of a subgoal will be
called the control values of this subgoal.
For independent sets, there exists an ecient ordering algorithm, listed in Figure 4. The
complexity of this algorithm is O(n( + log n)): O(n   ) to obtain the control values of n
subgoals, and O(n log n) to perform the sorting (Knuth, 1973). To enable the division, we
must define the cost so that cost(Ai ) is always positive. If we define the cost as the number
of unifications performed, then always cost(Ai )  1, under a reasonable assumption that
predicates of all rule body subgoals are defined in the program. (In this case, at least one
unification is performed for each subgoal). Similar algorithms were proposed by Simon and
Kadane (1975) and Natarajan (1987).
Example 2 Let the set of independent subgoals be fp; q; rg, with the following control values:
47

fiLedeniov & Markovitch

Algorithm 1
Let S = fA1; A2; : : :An g be a set of subgoals.
(Ai ),1
Sort S using cn(Ai ) = nsols
cost(Ai ) as the key for Ai , and return the result.
Figure 4: The algorithm for ordering subgoals by sorting.
p q
r
cost 10 20
5
nsols 1 5
0:1
cn
0 0:2 ,0:18
We compute the costs of all possible orderings, using Equation 5:

Cost(hp; q; ri) = 10 + 1  20 + 1  5  5 = 55
Cost(hp; r; q i) = 10 + 1  5 + 1  0:1  20 = 17
Cost(hq; p; ri) = 20 + 5  10 + 5  1  5 = 95
Cost(hq; r; pi) = 20 + 5  5 + 5  0:1  10 = 50
Cost(hr; p; q i) = 5 + 0:1  10 + 0:1  1  20 = 8
Cost(hr; q; pi) = 5 + 0:1  20 + 0:1  5  10 = 12
The minimal ordering is hr; p; q i, and this is exactly the ordering which is found much
more quickly by Algorithm 1 for the set fp; q; rg: r has the smallest cn value, ,0:18, then
goes p with cn(p) = 0, and finally q with cn(q ) = 0:2.
Note that the sorting algorithm reects a well-known principle: The best implementations of generate-and-test programs are obtained with the tests placed as early as possible
in the rule body and the generations as late as possible (Naish, 1985a). Of course, the
cheap tests should come first, while the expensive ones should come last. If one looks at
the cn measure, one quickly realizes that tests should be put in front (because nsols < 1,
so cn < 0), while generator subgoals should move towards the end (nsols > 1, so cn > 0).
The weakness of the \test-first" principle is in the fact that not every subgoal can be easily
tagged as a test or a generator. If one subgoal has nsols < 1 and another one has nsols > 1,
then their order is obvious even without looking at the costs (because their cn values have
different signs). But if both subgoals have nsols < 1, or both have nsols > 1, then the
decision is not so simple. Sorting by cn can correctly handle all the possible cases.
3.2.3 Correctness Proof of the Sorting Algorithm for Independent Sets

We saw that Algorithm 1 found a minimal ordering in Example 2. We are now going to
prove that Algorithm 1 always finds a minimal ordering for independent sets. First we
show an important lemma which will also be used in further discussion. This lemma states
48

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

that substitution of a sub-sequence by its cheaper permutation makes the entire sequence
cheaper.

Lemma 1
Let S~ = A~ kB~ kC~ , S~ 0 = A~ kB~ 0 kC~ , where B~ and B~ 0 are permutations of one another, and A~
either is empty or has nsols(A~ ) > 0. Then

Cost(S~ ) < Cost(S~ 0 ) () cost(B~ )jA~ < cost(B~ 0 )jA~ ;
Cost(S~ ) = Cost(S~ 0 ) () cost(B~ )jA~ = cost(B~ 0 )jA~ :

Proof: If A~ and C~ are not empty,
Cost(S~ ) , Cost(S~ 0 ) = Cost(A~ kB~ kC~ ) , Cost(A~ kB~ 0 kC~ ) =


(5)
= cost(A~ )j; + nsols(A~ )j;  cost(B~ )jA~ + nsols(A~ kB~ )j;  cost(C~ )jA~ kB~ ,
 ~

cost(A)j; + nsols(A~ )j;  cost(B~ 0 )jA~ + nsols(A~ kB~ 0 )j;  cost(C~ )jA~ kB~ 0 :

By Theorem 1, B~ and B~ 0 produce the same solution sets. Hence, the third terms in the
parentheses above are equal, and





Cost(S~ ) , Cost(S~ 0) = nsols(A~ )j;  cost(B~ )jA~ , cost(B~ 0 )jA~ :
Since nsols(A~ ) > 0, the sign of Cost(S~ ) , Cost(S~ 0 ) coincides with the sign of cost(B~ )jA~ ,
cost(B~ 0 )jA~ .
If A~ or C~ is empty, the proof is similar.
2
Definition: Let S~ = A~ kB~ 1kC~ kB~ 2kD~ be an ordered sequence of subgoals (A~, C~ and D~ may
be empty sequences). With respect to S~ , the pair hB~ 1 ; B~ 2i is

 cn-ordered, if cn(B~ 1)jA~  cn(B~ 2)jA~[B~ [C~
1

 cn-inverted, if cn(B~ 1)jA~ > cn(B~ 2)jA~[B~ [C~
1

We now show that two adjacent mutually independent sequences of subgoals in a minimal
ordering must be cn-ordered.

Lemma 2
Let S~ = A~ kB~ 1 kB~ 2 kC~ , S~ 0 = A~ kB~ 2 kB~ 1 kC~ , where B~ 1 , B~ 2 are mutually independent under A~ .
Let A~ either be empty or have nsols(A~ ) > 0. Then

Cost(S~ ) < Cost(S~ 0) () cn(B~ 1)jA~ < cn(B~ 2)jA~ ;
Cost(S~ ) = Cost(S~ 0) () cn(B~ 1)jA~ = cn(B~ 2)jA~ :
49

fiLedeniov & Markovitch

Proof:
Cost(S~ ) < Cost(S~ 0) Lemma
() 1 cost(B~ 1kB~ 2)jA~ < cost(B~ 2kB~ 1)jA~
() cost(B~ 1)jA~ + nsols(B~ 1)jA~ cost(B~ 2)jA~[B~ <
cost(B~ 2 )jA~ + nsols(B~ 2 )jA~ cost(B~ 1 )jA~[B~
indep.fB~ 1 ; B~ 2g
()
cost(B~ 1 )jA~ + nsols(B~ 1 )jA~ cost(B~ 2 )jA~ <
cost(B~ 2 )jA~ + nsols(B~ 2 )jA~ cost(B~ 1 )jA~
() nsols(B~ 1)jA~  cost(B~ 2)jA~ , cost(B~ 2)jA~ <
nsols(B~ 2 )jA~  cost(B~ 1 )jA~ , cost(B~ 1 )jA~
cost(B~ i )jA~ >0 nsols(B~ 1 )jA~ , 1 nsols(B~ 2 )jA~ , 1
()
<
cost(B~ 1 )jA~
cost(B~ 2 )jA~
() cn(B~ 1)jA~ < cn(B~ 2)jA~
1
2

Cost(S~ ) = Cost(S~ 0)

()

cn(B~ 1 )jA~ = cn(B~ 2)jA~ | similar.

2

In an independent set, all subgoal pairs are independent, in particular all adjacent pairs.
So, in a minimal ordering of an independent set, all adjacent subgoal pairs must be cnordered; otherwise, the cost of the sequence can be reduced by a transposition of such pair.
This conclusion is expressed in the following theorem.

Theorem 2
Let S be an independent set. Let S~ be an ordering of S . S~ is minimal iff all the subgoals in
S~ are sorted in non-decreasing order by their cn values.

Proof:

1. Let S~ be a minimal ordering of S . If S~ contains a cn-inverted adjacent pair of subgoals,
then transposition of this pair reduces the cost of S~ (Lemma 2), contradicting the
minimality of S~ .

2. Let S~ be some ordering of S , whose subgoals are sorted in non-decreasing order by
cn. Let S~ 0 be a minimal ordering of S . According to item 1, S~ 0 is also sorted by
cn. The only possible difference between the two sequences is the internal ordering
of sub-sequences with equal cn values. The ordering of each such sub-sequence in
S~ can be transformed to the ordering of its counterpart sub-sequence in S~ 0 by a
finite number of transpositions of adjacent subgoals. By Lemma 2, transpositions of
adjacent independent subgoals with equal cn values cannot change the cost of the
sequence. Therefore, Cost(S~ ) = Cost(S~ 0), and S~ is a minimal ordering of S (since S~ 0
is minimal).
2

Corollary 1 Algorithm 1 finds a minimal ordering of an independent set of subgoals.
50

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

3.3 Ordering of Dependent Sets of Subgoals

Algorithm 1 does not guarantee finding a minimal ordering when the given set of subgoals
is dependent, as the following proposition shows.

Proposition 1 When the given set of subgoals is dependent, then:
1. The result of Algorithm 1 on it is not always defined.
2. Even when the result is defined, it is not always a minimal ordering of the set.

Proof: Both claims are proved by counter-examples.
1. We show a set of subgoals that cannot be ordered by sorting.
The program:
Control values:
a(X )j; a(X )jfb(X )g b(X )j; b(X )jfa(X )g
a(c1).
b(c1).
cost
2
2
2
2
a(c2).
b(c2).
nsols
2
1
2
1
1
1
cn
0
0
2
2

The set fa(X), b(X)g has two possible orderings, ha(X ); b(X )i and hb(X ); a(X )i.
Both orderings have minimal cost, though neither one is sorted by cn: each ordering
has cn = 12 for its first subgoal, and cn = 0 for the second one. Sorting by cn is
impossible here: when we transpose subgoals, their cn values are changed, and the
pair becomes cn-inverted again.
2. We show a set of subgoals that can be ordered by sorting, but its sorted ordering is
not minimal.
The program:
Control values:
a(X )j; a(X )jfb(X )g b(X )j; b(X )jfa(X )g
a(c1).
cost
2
2
8
2
a(c1).
2
2
1
1
nsols
b(c1).
1
1
cn
0
0
b(c2)
a(c1), a(c2).
2
2
Let the unordered set of subgoals be fa(X), b(X)g. Its ordering hb(X ); a(X )i is sorted
by cn, while ha(X ); b(X )i is not. But ha(X ); b(X )i is cheaper than hb(X ); a(X )i:
cost(ha(X ); b(X )i) = 2 + 2  2 = 6

cost(hb(X ); a(X )i) = 8 + 1  2 = 10

2
Since sorting cannot guarantee minimal ordering for dependent subgoals, we now consider alternative ordering algorithms. The simplest algorithm checks every possible permutation of the set and returns the one with the minimal cost. The listing for this algorithm
is shown in Figure 5.
This algorithm runs in O(  n!) time, where  is the time it takes to compute the control
values for one subgoal, and n is the number of subgoals.
The following observation can help to reduce the ordering time at the expense of additional space. Ordered sequences can be constructed incrementally, by adding subgoals to
51

fiLedeniov & Markovitch

Algorithm 2
For each permutation of subgoals, find its cost according to Equation 5.
Store the currently cheapest permutation and update it when a cheaper
one is found.
Finally, return the cheapest permutation.

Figure 5: The algorithm for subgoal ordering by an exhaustive check of all permutations.

Algorithm 3
Order(S )
let P0 f;g; n jSj
loop for kn= 1 tofi n
o
Pk0 nP~ kB fifi P~fi 2 Pk,1; B h2 S n P~
io
~ P~ 0 ) ) Cost(P~ )  Cost(P~ 0 )
Pk P~ 2 Pk0 fifi 8P~ 0 2 Pk0 ; permutation(P;
Return the single member of Pn .
Figure 6: The ordering algorithm which checks permutations of ordered prefixes.
the right ends of ordered prefixes. By Lemma 1, if a cheaper permutation of a prefix exists,
then this prefix cannot belong to a minimal ordering. The ordering algorithm can build
prefixes with increasing lengths, at each step adding to the right end of each prefix one of
the subgoals that do not appear in it already, and for each subset keeping only its cheapest
permutation (if several permutations have equal cost, any one of them can be chosen). The
listing for this algorithm is shown in Figure 6. At each step k, Pk0 stores the set of prefixes
from step k , 1 extended by every subgoal not appearing there already. Pk  Pk0 , and
in Pk each subset of subgoals is represented only by its cheapest permutation. Obviously,
jPk j = (nk) (one prefix is kept for every subset of S of size k). For each prefix of length k , 1,
there are n , (k , 1) possible continuations of length k. The size of Pk0 is as follows:
!
k
n!
n
jPk0 j = (k,n1)(n,(k,1)) = (n , (k ,n1))!(
k , 1)! (n,(k,1)) = k  (n , k)!(k , 1)! = k (k ):
For each prefix, we compute its cost in  time. The permutation test can be completed
in O(n) time, by using, for example, a trie structure (Aho et al., 1987), where subgoals in
prefixes are sorted lexicographically. Each step k takes O((n +  )  k  (nk )) time, and the
52

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

whole algorithm runs in
n
X

k=1

O((n +  )  k  (nk )) = O(n  (n +  ) 

n
X
n
k=1

(k )) = O(n  (n +  )  2n ):

If  = O(n), this makes O(n2  2n ).
Smith and Genesereth (1985) and Natarajan (1987) point out that in a minimal ordered
sequence every adjacent pair of subgoals must satisfy an adjacency restriction. The most
general form of such a restriction in our notation says that two adjacent subgoals Ak and
Ak+1 in a minimal ordering hA1; A2 : : :Ani must satisfy
cost(hAk ; Ak+1i)jfA :::Ak, g  cost(hAk+1 ; Ak i)jfA :::Ak, g:
1

1

1

1

(8)

The restriction follows immediately from Lemma 1. However, it can only help to find a
locally minimal ordering, i.e., an ordering that cannot be improved by transpositions of
adjacent subgoals. It is possible that all adjacent subgoal pairs satisfy Equation 8, but the
ordering is still not minimal. The following example illustrates this statement.

Example 3 Let the unordered set be fp(X ); q(X ); r(X )g, where the predicates are defined
by the following program:

p(c1):
q (c1):
r(c1):
p(c2) f: q (c2):
r(c1):
q (c3) f:
f fails after 50 unifications.
The ordering hp(X ); q (X ); r(X )i satisfies the adjacency restriction (Equation 8):
cost(q (X ); r(X ))jp(X ) = 5
cost(p(X ); q (X ))j; = 55
cost(q (X ); p(X ))j; = 107
cost(r(X ); q (X ))jp(X ) = 8
But it is not minimal:

cost(hp(X ); q(X ); r(X )i) = 57
cost(hr(X ); p(X ); q (X )i) = 12

To find a globally minimal ordering, it seems beneficial to combine the prefix algorithm
with the adjacency restriction: if a prefix does not satisfy the adjacency restriction, then
there is a cheaper permutation of this prefix. The adjacency test can be performed faster
than the permutation test, since it must only consider the two last subgoals of each prefix. Nevertheless, the number of prefixes remaining after each step of Algorithm 3 is not
reduced: if a prefix is rejected due to a violation of the adjacency restriction, it would have
also been rejected by the permutation test. Furthermore, if the adjacency restriction test
does not fail, we should still perform the permutation test to avoid local minima (as in
Example 3). The adjacency test succeeds in at least half of the cases: if we examine a
prefix hA1 ; : : :Ak ; B1; B2 i, we shall also examine hA1; : : :Ak ; B2 ; B1i, and the adjacency test
cannot fail in both. Consequently, addition of the adjacency test can only halve the total
running time of the ordering algorithm, leaving it O(n2  2n ) in the worst case.
53

fiLedeniov & Markovitch

Smith and Genesereth propose performing a best-first search in the space of ordered
prefixes, preferring prefixes with lower cost. The best-first search can be combined with
the permutation test and the adjacency restriction. In addition, when the subgoals not
in a prefix are independent under its binding, they can be sorted, and the sorted result
concatenated to the prefix. By Lemma 1 and Corollary 1, this produces the cheapest
completion of this prefix. When we perform completion, there is no need to perform the
adjacency or permutation test: if a complete sequence is not minimal, it will never be chosen
as the cheapest prefix; even if it is added to the list of prefixes, it will never be extracted
therefrom. The resulting algorithm is shown in Figure 7.

Algorithm 4
Order(S )

let prefix-list ;, prefix ;, rest S
loop until empty(rest)
if Independent(restjprefix)
then
let completion prefixkSort-by-cn(restjprefix)
Insert-By-Cost(completion, prefix-list)
else
loop for subgoal 2 rest
let extension prefixksubgoal
if Adjacency-Restriction-Test(extension)
and Permutation-Test(extension)
then
Insert-By-Cost(extension, prefix-list)
prefix Cheapest(prefix-list)
Remove-from-list(prefix, prefix-list)
rest Snprefix
Return prefix

Figure 7: An algorithm for subgoal ordering, incorporating the ideas of earlier researchers.
The advantage of using best-first search is that it avoids expanding prefixes whose cost
is higher than the cost of the minimal ordering. The policy used by the algorithm may,
however, be suboptimal or even harmful. It often happens that the best completion of a
cheaper prefix is much more expensive than the best completion of a more expensive prefix.
When the number of solutions is large, it is better to place subgoals with high costs closer
to the beginning of the ordering to reduce the number of times that their cost is multiplied.
For example, let the set be fa(X ); b(X )g, with cost(a(X )) = 10, cost(b(X )) = nsols(a(X ))
= nsols(b(X )) = 2. Then a minimal ordering starts with the most expensive prefix:
Cost(ha(X ); b(X )i) = 10 + 2  2 = 14
54

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Cost(hb(X ); a(X )i) = 2 + 2  10 = 22
If there are many prefixes whose cost is higher than the cost of the minimal ordering, then
best-first search saves time. But if the number of such prefixes is small, using best-first
search can increase the total time, due to the need to perform insertion of a prefix into a
priority queue, according to its cost.
A sample run of Algorithm 4 will be shown later (in Section 4.7).

4. The Divide-and-Conquer Subgoal Ordering Algorithm

Algorithm 1 presented in Section 3.2 is very ecient, but is applicable only when the entire
set of subgoals is independent. Algorithm 3 can handle a dependent set of subgoals but is
very inecient. Algorithm 4, a combination of the two, can exploit independence of subgoals for better eciency. However, the obtained benefit is quite limited. In this section,
we present the Divide-and-Conquer (dac) algorithm, which is able to exploit subgoal independence in a more elaborate way. The algorithm divides the set of subgoals into smaller
subsets, orders these subsets recursively and combines the results.

4.1 Divisibility Trees of Subgoal Sets

In this subsection we define a structure that represents all the ways of breaking a subgoal
set into independent parts. Our algorithm will work by traversing this structure.
Definition: Let S and B be sets of subgoals. The divisibility tree of S under B, DTree(S ; B),
is an AND-OR tree defined as follows:
8 leaf(S ; B)
, S is independent under B
>
>

>
<
DTree(S ; B) = > OR(S ; B; fDTree(S n fBi g; B [ fBi g) j Bi 2 Sg) , S is indivisible under B
>
: AND(S ; B; fDTree(Si; B) j Si 2 DPart(S ; B)g) , S is divisible under B

Each node N in the tree DTree(S0; B0) has an associated set of subgoals S (N )  S0 and
an associated binding set B(N )  B0. For the root node, S (N ) = S0, B(N ) = B0 . If the
binding set of the root is not specified explicitly, we assume it to be empty. For AND-nodes
and OR-nodes we also define the sets of children.

 If S (N ) is independent under B(N ), then N is a leaf.
 If S (N ) is indivisible under B(N ), then N is an OR-node. Each subgoal Bi in S (N )
defines a child node whose set of subgoals is S (N ) n fBi g and the binding set is
B(N ) [ fBi g. We call Bi the binder of the generated child. Note that the binding
set of every node in a divisibility tree is the union of the binders of all its indivisible
ancestors and of the root's binding set.

 If S (N ) is divisible under B(N ), then N is an AND-node. Each subset Si in the
divisibility partition DPart(S (N ); B(N )) defines a child node with associated set of
subgoals Si and binding set B(N ). Divisibility partition was defined in Section 3.2.1.
55

fiLedeniov & Markovitch

= {a, b, c(X), d(X), e(X)}
n1 S(n1)
B(n1) = O

S(n2) = {a, b}
B(n2) = O

n2

S(n3) = {c(X), d(X), e(X)}

n3 B(n3) = O

S(n4) = {d(X), e(X)}
S(n6) = {c(X), d(X)}
n5
n6 B(n6) = {e(X)}
B(n4) = {c(X)} n4
S(n5) = {c(X), e(X)}
B(n5) = {d(X)}

Figure 8: The divisibility tree of fa; b; c(X ); d(X ); e(X )g under empty initial binding set. The set
associated with node n1 is divisible, and is represented by an AND-node. Its children
correspond to its divisibility subsets { one independent, S (n2) = fa; bg, and one indivisible, S (n3) = fc(X ); d(X ); e(X )g. n3 is an OR-node, whose children correspond to its
three subgoals (each subgoal serves as a binder in one of the children). The sets S (n2),
S (n4), S (n5) and S (n6) are independent under their respective binding sets, and their
nodes are leaves. Here we assumed that the subgoals c(X ), d(X ) and e(X ) bind X as a
result of their proof.

It is easy to show that the divisibility tree of a set of subgoals is unique up to the order of
children of each node. Figure 8 shows the divisibility tree of the set fa; b; c(X ); d(X ); e(X )g
under empty initial binding set. The associated sets and binding sets are written next to
the nodes.
The following lemma expresses an important property of divisibility trees: subgoals of
each node are independent of the rest of subgoals under the binding set of the node.

Lemma 3 Let S0 be a set of subgoals. Then for every node N in DTree(S0; ;), for every
subgoal A 2 S (N ), and for every subgoal Y 2 S0 n (S (N ) [B(N )), A and Y are independent
under B(N ).
Proof: by induction on the depth of N in the divisibility tree.
Inductive base: N is the root node, S0 n S (N ) is empty, and no such Y exists.
Inductive hypothesis: The lemma holds for M , the parent node of N .
Inductive step: Let A 2 S (N ), Y 2 S0 n (S (N ) [ B(N )). A 2 S (M ), and for M the
lemma holds, thus either A and Y are independent under B(M ), or Y 2 S (M ).
If A and Y are independent under B(M ), then they are also independent under B(N ),
since B(M )  B(N ). Otherwise, A and Y are dependent under B(M ), and Y 2 S (M ).
56

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

 If M is an AND-node, and A and Y are dependent under B(M ), then A and
Y belong to the same element of DPart(S (M ); B(M )), and Y 2 S (N ) { a

contradiction.
 If M is an OR-node and Y 2 S (M ) n S (N ), then Y must be the binder of N .
But then B(N ) = B(M ) [ fY g and Y 2 B(N ) { a contradiction again.
2
The lemma relates to subgoal independence inside divisibility trees. We shall sometimes
need to argue about independence inside ordered sequences of subgoals. The following
corollary provides the necessary connecting link.

Corollary 2 Let S0 be a set of subgoals, N be a node in the divisibility tree of S0, S~ an
ordering of S0, S~ = S~1 kS~2, where B(N )  S~1 and S (N )  S~2 . Then S (N ) is mutually
independent of S~2 n S (N ) under S~1.
Proof: Let A 2 S (N ), Y 2 S~2 n S (N ). A and Y are independent under B(N ), by the
preceding lemma. Since B(N )  S~1 , A and Y are independent under S~1. Every subgoal of
S (N ) is independent of every subgoal of S~2 nS (N ) under S~1; therefore, S (N ) and S~2 nS (N )
are mutually independent under S~1 .

2

4.2 Valid Orderings in Divisibility Trees

The aim of our ordering algorithm is to find a minimal ordering of a given set of subgoals.
We construct orderings following a divide-and-conquer policy: larger sets are split into
smaller ones, and orderings of the smaller sets are combined to produce an ordering of the
larger set. To implement this policy, we perform a post-order traversal of the divisibility
tree corresponding to the given set of subgoals under an empty initial binding set. When
orderings of child nodes are combined to produce an ordering of the parent node, the inner
order of their subgoals is not changed: smaller orderings are consistent with larger orderings.

Definition: Let S and G  S be sets of subgoals. An ordering O~ G of G and an ordering
O~ S of S are consistent (denoted Cons(O~ G; O~ S )), if the order of subgoals of G in O~ G and in
O~ S is the same.

The divide-and-conquer process described above seems analogous to Merge Sort (Knuth,
1973). There, the set of numbers is split into two (or more) subsets, each subset is independently ordered to a sequence consistent with the global order, and these sequences are
merged. Is it possible to use a similar method for subgoal ordering? Assume that a set
of subgoals is partitioned into two mutually independent subsets, A and B. Can we build
an algorithm that, given A, produces its ordering consistent with a minimal ordering of
A [ B, independently of B? Unfortunately, the answer is negative. An ordering of A may
be consistent with a minimal ordering of A [ B1 but at the same time not be consistent
with a minimal ordering of A [ B2 for some B1 6= B2.
For example, let A = fa1(X ); a2(X )g, B1 = fbg, B2 = fdg and the control values be as
specified in Figure 9. The single minimal ordering of A [ B1 is ha2(X ); b; a1(X )i, while the
single minimal ordering of A[B2 is hd; a1(X ); a2(X )i. There is no ordering of A consistent
with both these minimal global orderings.
57

fiLedeniov & Markovitch

The program:
a1(c1).
a1(c1).
a2(c1).
a2(c1).
a2(c2)

b
b
d.

a1(X).
d.

a1(c2).

The control values:
a1(X )j; a1(X )jfa2(X )g a2(X )j; a2(X )jfa1(X )g b d
cost
2
2
5
3
5 1
nsols
2
2
2
2
3 1

Cost(b; a1(X ); a2(X )) = 5 + 3  2 + 3  2  3 = 29
Cost(b; a2(X ); a1(X )) = 5 + 3  5 + 3  2  2 = 32
Cost(a1(X ); b; a2(X )) = 2 + 2  5 + 2  3  3 = 30
Cost(a1(X ); a2(X ); b) = 2 + 2  3 + 2  2  5 = 28
Cost(a2(X ); b; a1(X )) = 5 + 2  5 + 2  3  2 = 27
Cost(a2(X ); a1(X ); b) = 5 + 2  2 + 2  2  5 = 29

Cost(d; a1(X ); a2(X )) = 1 + 1  2 + 1  2  3 = 9
Cost(d; a2(X ); a1(X )) = 1 + 1  5 + 1  2  2 = 10
Cost(a1(X ); d; a2(X )) = 2 + 2  1 + 2  1  3 = 10
Cost(a1(X ); a2(X ); d) = 2 + 2  3 + 2  2  1 = 12
Cost(a2(X ); d; a1(X )) = 5 + 2  1 + 2  1  2 = 11
Cost(a2(X ); a1(X ); d) = 5 + 2  2 + 2  2  1 = 13

Figure 9: We show a small program and the control values it defines. Then we compute costs of all
permutations of the sets fb; a1(X ); a2(X )g and fd; a1(X ); a2(X )g. Different orderings of
fa1(X ); a2(X )g are consistent with minimal orderings of these sets.
Since, unlike the case of Merge Sort, we cannot always identify a single ordering of the
subset consistent with a minimal ordering of the whole set, our algorithm will deal with
sets of candidate orderings. Our requirement from such a set is that it contain at least
one local ordering consistent with a global minimal ordering, if such a local ordering exists
(\local" ordering is an ordering of the set of the node, \global" ordering is an ordering of
the set of the root). Such a set will be called valid. The following definition defines valid
sets formally, together with several other concepts.
Definition: Let S0 be a set of subgoals and N be a node in the divisibility tree of S0.
Recall that  (S ) denotes the set of all permutations of S .
1. O~ S 2  (S0) is binder-consistent with O~ N 2  (S (N )) (denoted BCN (O~ N ; O~ S )), if they
are consistent, and all subgoals of B(N ) appear in O~ S before all subgoals of O~ N :
BCN (O~ N ; O~ S ) () 9O~ B 2 (B(N )) : Cons(O~ B kO~ N ; O~ S ):
O~ S 2 (S0) is binder-consistent with the node N (denoted BCN (O~ S )), if it is binderconsistent with some ordering of S (N ):
BCN (O~ S ) () 9O~ N 2 (S (N )) : BCN (O~ N ; O~ S ):
2. O~ N 2  (S (N )) is min-consistent with O~ S 2  (S0) (denoted MCN;S (O~ N ; O~ S )), if they
are binder-consistent, and O~ S is minimal:
MCN;S (O~ N ; O~ S ) () BCN (O~ N ; O~ S ) ^ Min(O~ S ; S0):
O~ N 2 (S (N )) is min-consistent (denoted MCN;S (O~ N )), if it is min-consistent with
some ordering of S0:
MCN;S (O~ N ) () 9O~ S 2 (S0) : MCN;S (O~ N ; O~ S ):
0

0

0

0

0

58

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

3. An ordering O~ N 2  (S (N )) is MC-contradicting, if it is not min-consistent:
MCCN;S (O~ N ) () :MCN;S (O~ N ):
0

0

4. Two orderings O~ 1; O~ 2 2  (S (N )) are MC-equivalent, if one of them is min-consistent
iff the other one is:
MCEN;S (O~ 1; O~ 2) () [MCN;S (O~ 1) () MCN;S (O~ 2)]:
0

0

0

5. A set of orderings CN   (S (N )) is valid, if CN contains a min-consistent ordering
(when at least one min-consistent ordering of S (N ) exists):

V alidN;S (CN ) () [9O~ N0 2  (S (N )) : MCN;S (O~ N0 )] ! [9O~ N 2 CN : MCN;S (O~ N )]:
0

0

0

An important property of valid sets is that a valid set of orderings of the root of

DTree(S0; ;) must contain a minimal ordering of S0. Indeed, in the root S (N ) = S0,
and consistency becomes identity. Also, B(N ) = ;, so that binder-consistency becomes

consistency, and min-consistency becomes minimality. Since there always exists a minimal
ordering of S0 , a valid set of orderings of the root must contain a minimal ordering of S0.

4.3 The Outline of the Divide-and-Conquer Algorithm

We propose an algorithm that is based on producing valid sets of orderings. Each node in
a divisibility tree produces a valid set for its associated set of subgoals, and passes it to
its parent node. After the valid set of the root node is found, we compare costs of all its
members, and return the cheapest one.
The set of orderings produced by the algorithm for a node N is called a candidate set
of N . Its members are called candidate orderings of N , or simply candidates. To find
a candidate set of N , we first consider the set of all possible orderings of S (N ) that are
consistent with candidates of N 's children. This set is called the consistency set of N .
Given the candidate sets of N 's children, the consistency set of N is defined uniquely. A
candidate set of N is usually not unique.
Definition: Let N be a node in a divisibility tree of S0. The consistency set of N , denoted
ConsSet(N ), and the candidate set of N , denoted CandSet(N ), are defined recursively:

 If N is a leaf, its consistency set contains all permutations of S (N ):
ConsSet(N ) = (S (N )):

 If N is an AND-node, and its child nodes are N1; N2; : : :Nk , we define the consistency
set of N as the set of all possible orderings of S (N ) consistent with candidates of
N1; N2; : : :Nk :

fi
n
o
ConsSet(N ) = O~ N 2 (S (N )) fifi 8i (1  i  k); 9O~ i 2 CandSet(Ni) : Cons(O~ i ; O~ N ) :
59

fiLedeniov & Markovitch

 If N is an OR-node, and its child node corresponding to every binder A 2 S (N ) is

NA , then the consistency set of N is obtained by adding binders as the first elements
to the candidates of the children:
fi
n
o
ConsSet(N ) = AkO~ A fifi A 2 S (N ); O~ A 2 CandSet(NA) :

 A candidate set of N is any set of orderings produced by removing MC-contradicting

and MC-equivalent orderings out of the consistency set of N , while keeping at least
one representative for each group of MC-equivalent orderings:
CandSet(N )  ConsSet(N );
~ON 2 (ConsSet(N ) n CandSet(N )) ) MCCN;S (O~ N ) _
h 0
i
9O~ N 2 CandSet(N ) : MCEN;S (O~ N ; O~ N0 ) :
0

0

(In other words, if some ordering is rejected, it is either MC-contradicting, or MCequivalent to some other ordering, which is not rejected.)
There are two kinds of orderings which can be removed from ConsSet(N ) while retaining its validity: MC-contradicting and MC-equivalent orderings. Removal of an MCcontradicting ordering cannot change the number of min-consistent orderings in the set; if
we remove an MC-equivalent ordering, then even if it is min-consistent, some other minconsistent ordering is retained in the set. If there exists a min-consistent ordering of the set
of the node, then its candidate set must contain a min-consistent ordering, and therefore
the candidate set is valid.
Note that when our algorithm treats an OR-node, the binder of each child is always
placed as the first subgoal of the produced ordering of this node. On higher levels the inner
order of subgoals in the ordering does not change (consistency is preserved). Therefore,
our algorithm can only produce binder-consistent orderings. This explains the choice of
the names \binder" and \binding set": the subgoals of B(N ) bind some common variables
of S (N ), since they stand to the left of them in any global ordering that our algorithm
produces. In particular, if S (N ) is independent under B(N ), then the subgoals of B(N )
bind all the shared free variables of S (N ).
To implement the DPart function, we can use the Union-Find data structure (Cormen,
Leiserson, & Rivest, 1991, Chapter 22), where subgoals are elements, and indivisible sets
are groups. In the beginning, every subgoal constitutes a group by itself. Whenever we
discover that two subgoals share a free variable not bound by subgoals of the binding set,
we unite their groups into one. To complete the procedure, we need a way to determine
which variables are bound by the given binding set. Section 7.1 contains a discussion of
this problem and proposes some practical solutions. Finally, we collect all the indivisible
subgoals into a separate group. These operations can be implemented in O(nff(n; n)) amortized time, where ff(n; n) is the inverse Ackermann function, which can be considered O(1)
for all values of n that can appear in realistic logic programs. Thus, the whole process of
finding the divisibility partition of n subgoals can be performed in O(n) average time.
The formal listing of the ordering algorithm discussed above is shown in Figure 10.
The algorithm does not specify explicitly how candidate sets are created from consistency sets. To complete this algorithm, we must provide the three filtering procedures
60

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Algorithm 5
Order(S0)

RootCandSet

CandidateSet(S0 ; ;)

Return the cheapest member of RootCandSet

CandidateSet(S ; B)
case (S under B)

independent:
let ConsSetN  (S )
let CandSetN ValidLeafFilter(ConsSetN )
divisible:
let fS1; S2; : : : Sk g DPart(S ; B)
loop for i = 1 to k
let Ci CandidateSet
(Si ; B)
n~
o
let ConsSetN
ON 2 (S (N )) j 8i = 1 : : :k; 9O~ i 2 Ci : Cons(O~ i; O~ N )
let CandSetN ValidANDFilter(ConsSetN ; fS1; : : : Sk g; fC1; : : : Ck g)
indivisible:
loop for A 2 S
let C (A) CandidateSet
n ~ ~ (S n foAg; B [ fAg)
0
let C (A)
ASkOA j OA 2 C (A)
0
let ConsSetN
A2S C (A)
let CandSetN ValidORFilter(ConsSetN )
Return CandSetN

Figure 10: The skeleton of the dac ordering algorithm. For each type of node in a divisibility tree,

a consistency set is created and refined through validity filters. The produced candidate
set of the root is valid; hence, its cheapest member is a minimal ordering of the given
set.

{ ValidLeafFilter , ValidANDFilter and ValidORFilter . Trivially, we can define them
all as null filters that return the sets they receive unchanged. In this case the candidate
set of every node will contain all the permutations of its subgoals, and will surely be valid.
This will, however, greatly increase the ordering time. Our intention is to reduce the sizes
of candidate sets as far as possible, while keeping them valid.
In the following two subsections we discuss the filtering procedures. Section 4.4 discusses detection of MC-contradicting orderings, and Section 4.5 discusses detection of MCequivalent orderings. Finally, in Section 4.6 we present the complete ordering algorithm,
incorporating the filters into the skeleton of Algorithm 5.
61

fiLedeniov & Markovitch

4.4 Detection of MC-Contradicting Orderings

In this subsection we show sucient conditions for an ordering to be MC-contradicting.
Such orderings can be safely discarded, leaving the set of orderings valid, but reducing its
size. The subsection is divided into three parts, one for each type of node in a divisibility
tree.
4.4.1 Detection of MC-Contradicting Orderings in Leaves

The following lemma shows that subgoals in a min-consistent ordering of a leaf node must
be sorted by cn.

Lemma 4
Let S0 be a set of subgoals, N be a leaf in the divisibility tree of S0. Let O~ N be an ordering of
S (N ). If the subgoals of O~ N are not sorted by cn under B(N ), then O~ N is MC-contradicting.
Proof: Let O~ S be any ordering of S0, binder-consistent with O~ N . We show that O~ S cannot
be a minimal ordering of S0, thus O~ N is not min-consistent.
O~ N is not sorted by cn, i.e., it contains an adjacent cn-inverted pair of subgoals hA1; A2i.

(Recall that a pair is cn-inverted if the first element has a larger cn value than the second
one { Section 3.2.3). Since O~ S is consistent with O~ N , we can write O~ S = X~ kA1 kY~ kA2kZ~ ,
where X~ , Y~ and Z~ are (possibly empty) sequences of subgoals. Since O~ S is binder-consistent
with O~ N , B(N )  X~ .
If Y~ is empty, then A1 and A2 are adjacent in O~ S . Since B(N )  X~ , A1 and A2 are
independent under X~ . Therefore, the cost of the whole ordered sequence can be reduced
by transposing A1 and A2 , according to Lemma 2 (they are adjacent, independent and
cn-inverted).
If Y~ is not empty, then no subgoal of Y~ belongs to S (N ), since otherwise it would appear
in O~ N between A1 and A2 . By Corollary 2, Y~ is mutually independent of both A1 and A2
under X~ .

 If cn(Y~ )jX~ < cn(A1)jX~ then, by Lemma 2, a transposition of Y~ with A1 produces an
ordering with lower cost.
 Otherwise, cn(Y~ )jX~  cn(A1)jX~ . Since the pair hA1; A2i is cn-inverted, cn(A1)jX~ >
cn(A2)jX~ . Hence, cn(Y~ )jX~ > cn(A2)jX~ , and transposition of Y~ with A2 reduces the
cost, by Lemma 2.
In either case, there is a way to reduce the cost of O~ S . Therefore, O~ S cannot be minimal,
and O~ N is MC-contradicting.
2
4.4.2 Detection of MC-Contradicting Orderings in AND-nodes

Every member of the consistency set of an AND-node is consistent with some combination
of candidates of its child nodes. If there are k child nodes, and for each child Ni the sizes
of subgoal and candidate sets are jS (Ni)j = ni and jCandSet(Ni)j = ci , then the total
number of possible consistent orderings is c1  c2  : : :ck  (nn+!nn +!::::::+nnk !k )! . Fortunately, most
of these orderings are MC-contradicting and can be discarded from the candidate set. The
1

2

1

62

2

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

following lemma states that it is forbidden to insert other subgoals between two cn-inverted
sub-sequences. If such insertion takes place, the ordering is MC-contradicting and can be
safely discarded.

Lemma 5
Let S0 be a set of subgoals, N a node in the divisibility tree of S0 , and O~ S an ordering of
S0, binder-consistent with an ordering O~ N of S (N ).
If O~ N contains an adjacent cn-inverted pair of sub-sequences hA~ 1 ; A~ 2i, A~ 1 and A~ 2 appear

in O~ S not mixed with other subgoals, and A~ 1 and A~ 2 are not adjacent in O~ S , then O~ S is
not minimal.

Proof: Let O~ S be such an ordering of S0, binder-consistent with O~ N :
O~ S = X~ kA~ 1kY~ kA~ 2kZ~ ;
where Y~ is not empty. No subgoal of Y~ belongs to S (N ), since otherwise it would stand
in O~ N between A~ 1 and A~ 2 . O~ S is binder-consistent with O~ N ; therefore, B(N )  X~ . By
Corollary 2, Y~ must be mutually independent of both A~ 1 and A~ 2 under X~ , and by Lemma 2
a transposition of Y~ with either A~ 1 or A~ 2 reduces the cost { exactly as in the proof of
Lemma 4.
2

If a pair of adjacent subgoals hAi ; Ai+1i is cn-inverted, then by the previous lemma any
attempt to insert subgoals inside it results in a non-minimal global ordering. Thereupon
we may join Ai and Ai+1 into a block Ai;i+1 , which can further participate in a larger block.
The formal recursive definition of a block follows. For convenience, we consider separate
subgoals to be blocks of length 1.

Definition:

1. A sub-sequence A~ of an ordered sequence of subgoals is a block if it is either a single
subgoal, or A~ = A~ 1 kA~ 2, where hA~ 1; A~ 2i is a cn-inverted pair of blocks.
2. A block is maximal (max-block) if it is not a sub-sequence of a larger block.
3. Let N be a node in a divisibility tree, M be some descendant of N , O~ N 2  (S (N ))
and O~ M 2  (S (M )) be two consistent orderings of these nodes. A block A~ of O~ M is
violated in O~ N if there are two adjacent subgoals in A~ that are not adjacent in O~ N (in
other words, alien subgoals are inserted between the subgoals of the block).
4. Let N be a node, M be its descendant, O~ N 2  (S (N )) and O~ M 2  (S (M )) be two
consistent orderings of these nodes. O~ M is called the projection of O~ N on M . We
shall usually speak about projection of an ordering on a child node.
The concept of max-block is similar to the maximal indivisible block introduced by Simon
and Kadane (1975) in the context of satisficing search. The following corollary presents the
result of Lemma 5 in a more convenient way.

Corollary 3 Let N be a node in a divisibility tree, M be one of its children, O~ N be an

ordering of N , and O~ M be the projection of O~ N on M . If O~ M contains a block that is
violated in O~ N , then O~ N is MC-contradicting.
63

fiLedeniov & Markovitch

Proof: Let A~ be the smallest block of O~ M violated in O~ N . According to the definition
of a block, A~ = A~ 1 kA~ 2 , where A~ 1 and A~ 2 are not violated in O~ N , and the pair hA~ 1; A~ 2i

is cn-inverted. Let O~ S be any ordering of the root node binder-consistent with O~ N . O~ S
violates A~ , since O~ N violates A~ . To show that O~ N is MC-contradicting, we must prove that
O~ S is not minimal.
 If A~1 and A~ 2 are not violated in O~ S , then they are not adjacent in O~ S , and O~ S is not
minimal, by Lemma 5.
 Otherwise, A~ 1 or A~2 is violated in O~ S . Without loss of generality, let it be A~1. Let A~0
be the smallest sub-block of A~ 1 violated in O~ S . According to the definition of a block,
A~ 0 = A~01 kA~ 02 , where the pair hA~ 01; A~ 02i is cn-inverted, A~ 1 and A~ 2 are not violated and
not adjacent in O~ S . By Lemma 5, O~ S is not minimal.
2
For example, if control values of subgoals are as shown in Figure 9, then ha1(X ); a2(X )i
is a block, since cn(a1(X ))j; = 2,2 1 = 12 , cn(a2(X ))jfa1(X )g = 2,3 1 = 13 . As one can see from
the figure, insertion of b or d inside this block results in a non-minimal ordering.
As was already noted above, the consistency set of an AND-node can be large. In
many of its orderings, however, blocks of projections are violated, and we can discard
these orderings as MC-contradicting. In the remaining orderings, no block of a projection
is violated, and each such ordering can be represented as a sequence of max-blocks of the
projections. In each projection, its max-blocks stand in cn-ascending order (otherwise, there
is an adjacent cn-inverted pair of blocks, and a larger block can be formed, which contradicts
their maximality). As the following lemma states, in the parent AND-node these blocks
must also be ordered by their cn values; otherwise, the ordering is MC-contradicting.
Lemma 6 If an ordering of an AND-node contains an adjacent cn-inverted pair of maxblocks of its projections on the children, then this ordering is MC-contradicting.
Proof: If these blocks are violated in the binder-consistent global ordering, the global
ordering is not minimal by Corollary 3. If the blocks are not violated, the proof is similar
to the proof of Lemma 4.
2
The two sucient conditions for detection of MC-contradicting orderings expressed in
Corollary 3 and Lemma 6 allow us to reduce the size of the candidate set significantly.
Assume, for example, that the set of our current node N is split into two mutually independent subsets whose candidates are ha1; a2i and hb1; b2i (one candidate for each child). There
are six possible orderings of S (N ), all shown in Figure 11. Assume that both ha1; a2i and
hb1; b2i are blocks, and cn(ha1; a2i)jB(N ) < cn(hb1; b2i)jB(N ). Out of six consistent orderings,
four (2{5) can be rejected due to block violation, and one of the remaining two (number 6)
puts the blocks in the wrong order. So, only one ordering (number 1) can be left in the candidate set of N . Even if neither ha1; a2i nor hb1; b2i are blocks, Lemma 6 dictates a unique
interleaving of their elements (max-blocks), assuming that cn(a1 )jB(N ) 6= cn(a2 )jB(N )[fa g
6= cn(b1)jB(N ) 6= cn(b2)jB(N )[fb g.
1

1

4.4.3 Detection of MC-Contradicting Orderings in OR-nodes

The following lemma states that if a block has a cheaper permutation, then the ordering is
MC-contradicting (and can be discarded from the candidate set).
64

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

1.

a1 a2

b1 b2

2.

a1 a2

b1 b2

3.

a1 a2

4.

b1 b2

a1 a2

b1 b2

5.

a1 a2

b1 b2

6.

a1 a2

b1 b2

Figure 11: The possible ways to combine ha1, a2 i and hb1, b2i

Lemma 7 Let N be a node in the divisibility tree of S~0, O~ N 2 (S~(N )). Let A~ be a leading
block of O~ N : O~ N = A~ kR~ . If there is a permutation of A~ , A~ 0 , such that cost(A~ 0 )jB(N ) <
cost(A~ )jB(N ), then O~ N is MC-contradicting.
Proof: Let O~ S 2 (S~0) be binder-consistent with O~ N . If A~ is violated in O~ S , O~ S cannot

be minimal (Corollary 3). Otherwise, A~ occupies a continuous segment in O~ S , and its
replacement by a cheaper permutation reduces the cost of the global ordering (Lemma 1).
Thus, O~ S cannot be minimal.
2
This check should be done only for leading blocks of OR-nodes:

 Every ordering of a leaf node that has not been rejected due to Lemma 4 must be

sorted by cn. Consequently, it contains no cn-inverted adjacent pair of subgoals, and
no block of size  2 can be formed.

 Every ordering of an AND-node that has not been rejected due to Corollary 3 or
Lemma 6 must have its blocks unbroken and in cn-ascending order. Consequently,
new blocks cannot be formed here either.

 In OR-nodes, new blocks can be formed when we add a binder as the first element of
an ordering, if the cn value of the binder is greater than that of the subsequent block.
All new blocks start from the binder, and we must perform the permutation test only
on the leading max-block of an ordering.

4.5 Detection of MC-Equivalent Orderings

In the previous subsection we presented sucient conditions for detecting MC-contradicting
orderings. In this subsection we specify sucient conditions for identifying MC-equivalent
orderings. Recall that two orderings of a node are MC-equivalent if minimal consistency
of one implies minimal consistency of the other. Finding such sucient conditions will
allow us to eliminate orderings without loss of validity of the candidate set. We start
with defining a specialization of the MC-equivalence relation: blockwise equivalence. We
then show that orderings whose max-blocks are sorted by cn are blockwise-equivalent, and
therefore MC-equivalent.
65

fiLedeniov & Markovitch

Definition: Let S0 be a set of subgoals and N be a node in the divisibility tree of S0. Let
O~ 1 and O~ 2 be two orderings of S (N ) with an equal number of max-blocks. Let O~ S be an
ordering of S0 , binder-consistent with O~ 1, where blocks of O~ 1 are not violated.
O~ S jOO~~ is the ordering obtained by replacing in O~ S every max-block of O~ 1 with a max2

block of O~ 2, while preserving the order of max-blocks (the i-th max-block of O~ 1 is replaced
by the i-th max-block of O~ 2).
O~ 1 and O~ 2 are blockwise-equivalent if the following condition holds: O~ 1 is min-consistent
with O~ S iff O~ 2 is min-consistent with O~ S jOO~~ .
As can be easily seen, if two orderings are blockwise-equivalent, then they are MCequivalent. Now we show that a transposition of adjacent, mutually independent cn-equal
max-blocks in an ordering of a node produces a blockwise-equivalent ordering. The proof
of the following lemma is found in Appendix A.
1

2
1

Lemma 8
Let S0 be a set of subgoals, N be a node in the divisibility tree of S0, O~ N = Q~ kA~ 1 kA~ 2kR~ be
an ordering of S (N ), where A~ 1 and A~ 2 are max-blocks, mutually independent and cn-equal
under the bindings of B(N ) [ Q~ . Then O~ N is blockwise-equivalent with O~ N0 = Q~ kA~ 2 kA~ 1kR~ .
Corollary 4 All sorted by cn orderings of a leaf node are blockwise-equivalent.
For example, if S (N ) = fA; B; C; Dg, cn(A)jB(N ) = 0:1, cn(B )jB(N ) = cn(C )jB(N ) = 0:3,
cn(D)jB(N ) = 0:5, then the orderings hA; B; C; Di and hA; C; B; Di are blockwise-equivalent,
and we can remove from the candidate set any one of them (but not both).

Corollary 5 All orderings of an AND-node, where blocks of projections are not violated

and adjacent max-blocks from different children projections are cn-ordered, are blockwiseequivalent.

~ B;
~ C~ ; D~ are
For example, if the candidates of the children are A~ kB~ and C~ kD~ , where A;
max-blocks, cn(A~ )jB(N ) = 0:1, cn(B~ )jB(N )[A~ = cn(C~ )jB(N ) = 0:3 and cn(D~ )jB(N )[C~ = 0:5,
then the orderings A~ kB~ kC~ kD~ and A~ kC~ kB~ kD~ are blockwise-equivalent, and we can remove
from the candidate set any one of them (but not both).
To prove both Corollaries 4 and 5, we note that in each case one of the mentioned
orderings can be obtained from the other by a finite number of transpositions of adjacent,
mutually independent and cn-equal max-blocks. According to Lemma 8, each such transposition yields a blockwise-equivalent ordering. It is easy to show that blockwise equivalence
is transitive.
The following corollary states that subgoals within a block can be permuted, provided
that the cost of the block is not changed.

Corollary 6 All orderings of a node, identical up to cost-preserving permutations of subgoals inside blocks, are blockwise-equivalent.

The proof of the corollary follows immediately from Lemma 1. For example, if the set
is fa(X ); b(X )g, and the control values are as in the first counter-example of Proposition 1,
66

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Node Set
MC-contradicting
Leaf Independent Subgoals not sorted by cn
| Lemma 4
Contains violated blocks
AND Divisible
| Corollary 3
Max-blocks not sorted by cn
| Lemma 6
The leading max-block has
OR Indivisible a cheaper permutation
| Lemma 7

blockwise-equivalent
Subgoals sorted by cn
| Corollary 4
Max-blocks not violated,
sorted by cn
| Corollary 5
Cost-preserving permutations
of blocks
| Corollary 6

Table 1: Summary of sucient conditions for detection of MC-contradicting and blockwiseequivalent orderings.

i.e. cn(a(X )j;) = cn(b(X )j;) = 12 , and cn(a(X )jfb(X )g) = cn(b(X )jfa(X )g) = 0, then in both
possible orderings, ha(X ); b(X )i and hb(X ); a(X )i, the two subgoals are united into a block,
and these blocks have equal cost. In any global ordering containing the block ha(X ); b(X )i,
we can replace this block with hb(X ); a(X )i without changing the total cost. Therefore
ha(X ); b(X )i is blockwise-equivalent to hb(X ); a(X )i.
The sucient condition expressed in Corollary 6 should be checked only in OR-nodes,
since in leaves and AND-nodes no new blocks are created, as was argued in Section 4.4.3.

4.6 The Revised Ordering Algorithm

In the two preceding subsections we saw several sucient conditions of MC-contradiction
and MC-equivalence, summarized in Table 1. These results permit us to close the gaps
in Algorithm 5 by providing the necessary validity filters. Each filter tests the sucient
conditions of MC-contradiction and MC-equivalence on every ordering in the consistency
set. If some of these sucient conditions hold, the ordering is rejected. The formal listing
of these procedures is shown in Figure 12.
While the generate-and-test approach described above served us well for methodological
purposes, it is obviously not practical because of its computational limitations. For example,
for an independent set of size n, the algorithm creates n! orderings, then rejects n! , 1
and keeps only one. This process takes O(n!  n) time and produces an ordering which
is sorted by cn. The same result could be obtained in just O(n log n) time, by a single
sorting. So, instead of uncontrolled creation of orderings and selective rejection, we want to
perform a selective creation of orderings. In other words, we want to revise our algorithm to
deal directly with candidate sets, instead of generating large consistency sets. The revised
algorithm produces the candidate set of a node N as follows:
 If N is a leaf, the subgoals of S (N ) are sorted by cn under the bindings of B(N ), and
the produced ordering is the sole candidate of N .
 If N is an AND-node, then for each combination of its children's candidates a candidate of N is created, where the max-blocks of the children's candidates are ordered
67

fiLedeniov & Markovitch

ValidLeafFilter(ConsSetN )
let CandSetN ;
loop for O~ N 2 ConsSetN

if O~ N is sorted by cn
and there is no O~ N0 2 CandSetN which is sorted by cn
then CandSetN CandSetN [ fO~ N g
Return CandSetN

ValidANDFilter(ConsSetN ; fS1; : : : Sk g; fC1; : : : Ck g)
let CandSetN ;
loop for O~ N 2 ConsSetN

loop for i = 1 to k
let O~ i be the projection of O~ N on Si
if 8i O~ i 2 Ci
and max-blocks of O~ i -s are not violated in O~ N ,
and max-blocks of O~ i -s are ordered by cn in O~ N ,
and there is no O~ N0 2 CandSetN consistent with all O~ i-s,
then CandSetN CandSetN [ fO~ N g
Return CandSetN

ValidORFilter(ConsSetN )
let CandSetN ;
loop for O~ N 2 ConsSetN

if O~ N does not start with a block having a cheaper permutation,
and there is no O~ N0 2 CandSetN , identical to O~ N up to
cost-preserving permutations in blocks,
then CandSetN CandSetN [ fO~ N g
Return CandSetN

Figure 12: The three filter procedures that convert a consistency set into a candidate set. Together
with Algorithm 5, they form a complete ordering algorithm. The eciency of the
algorithm can be improved, as we shall see in Algorithm 6.

by cn. The candidate is produced by merging: moving in parallel on the candidates
of the children and extracting max-blocks that are minimal by cn.

 If N is an OR-node, then for each candidate of its child an ordering of N is created
by adding the binder to the left end of the child candidate. If this results in creation
of a block that has a cheaper permutation, the ordering is rejected; otherwise, it is
added to the candidate set. It suces to check only the leading max-block.
68

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Note that the revised algorithm does not include a test for cost-preserving permutations
of blocks in different orderings (expressed in Corollary 6), because of the high expense of
such a test.
The revised algorithm described above contains manipulations of blocks. For this purpose, we need an easy and ecient way to detect blocks in orderings. Since we do not
permit block violation (by Corollary 3), we can unite all the subgoals of a max-block into
one entity, and treat it as an ordinary subgoal. The procedure of joining subgoals into
blocks is called folding, and the resulting sequence of max-blocks { a folded sequence. After
subgoals are folded into a block, there is no need to unfold this block back to separate
subgoals: on upper levels of the tree, these subgoals will again be joined into a block, unless
the block is violated. The unfolding operation is carried out only once before returning the
cheapest ordering of the set (of the root node). The candidate sets of the nodes are now
defined as sets of folded orderings.
As was already stated, new blocks can only be created in the candidates of OR-nodes,
when the binder is added as the first element of the ordering, if the cn value of the binder
is greater than the cn value of the first max-block of the child projection. Therefore, in the
revised algorithm we only build new blocks that start from the binder: the max-blocks in
the rest of the ordering remain from the child's candidate. First we try to make a block
out of the binder and the first max-block of the child's candidate. If they are cn-ordered,
we stop the folding. If they are cn-inverted, we unite them into a larger block, and try
to unite it with the second max-block of the child's candidate, and so on. The produced
folded ordering contains only maximal blocks: the first block is maximal, since we could not
expand it further to the right, and the other blocks are maximal, since they were maximal
in the child's candidate.
Lemma 7 states that an ordering whose leading max-block has a cheaper permutation
is MC-contradicting. One way to detect such a block is to exhaustively test all its permutations, computing and comparing their costs. This procedure is very expensive. Instead,
in our revised algorithm we employ the adjacency restriction test (Equation 8). The test is
applied to every pair of adjacent subgoals of a block, and if some adjacent pair has a cheaper
transposition, then the whole block has a cheaper permutation, by Lemma 1. Since blocks
are created by concatenation of smaller blocks, it suces to test the adjacency restriction
only at the points where blocks are joined (for other adjacent pairs of subgoals, the tests
were performed on the lower levels, when smaller blocks were formed). The adjacency restriction test does not guarantee detection of all not-cheapest permutations (as was shown
in Example 3), but it detects such blocks in many cases, and works in linear time.
The final version of the dac subgoal ordering algorithm is presented in Figure 13. The
complete correctness proof of Algorithm 6 is found in Appendix B.

4.7 Sample Run and Comparison of Ordering Algorithms
We illustrate the work of the dac algorithm, using the subgoal set shown in Figure 8,
S0 = fa; b; c(X ); d(X ); e(X )g. After proving c(X ), d(X ) or e(X ), we can assume that X is
bound. Let the control values for the subgoals be as shown in Table 2. The column c(free)
contains control values for the subgoal c(X ) when X is not yet bound by the preceding
subgoals (i.e., the binding set does not contain d(X ) or e(X )). The column c(bound)
69

fiLedeniov & Markovitch

Algorithm 6 : The Divide-and-Conquer Algorithm
Order(S0)
let RootCandSet CandidateSet(S0 ; ;)
Return Unfold(the cheapest element of RootCandSet)
CandidateSet(S ; B)
let fS1; S2; : : : Sk g DPart(S ; B)
case

 k = 1, shared-vars(S1) = ; (S is independent under B):
Return fSort-by-cn(S ; B)g
 k = 1, shared-vars(S1) =6 ; (S is indivisible under B):
loop for A 2 S
let C (A) CandidateSet
(S fin fAg; B [ fAog)
n
0
~
let C (A)
Fold
(AkOA ; B) fifi O~ A 2 C (A)
S
Return A2S C 0(A)
 k > 1 (S is divisible under B):
loop for i = 1 to k
let Cin CandidateSet(Si ; B)
Return Merge(fO~ 1; O~ 2; : : : O~ k g; B)

fifi
o
fi O~ 1 2 C1; O~ 2 2 C2; : : : O~ k 2 Ck

Merge(fO~ 1; O~ 2; : : : O~ k g; B)

let min-cn-candidate O~ i that minimizes cn(first-max-block(O~ i ))jB , 1  i  k
let min-cn-block first-max-block(min-cn-candidate)
remove-first-max-block(min-cn-candidate)
Return min-cn-blockkMerge(fO~ 1; O~ 2; : : : O~ k g; B [ min-cn-block)

Fold(hA1; A2 : : :Ak i; B)
if k  1 or cn(A1)jB  cn(A2)jBkA
then Return hA1; A2 : : :Ak i

1

else
if the last subgoal of A1 and the first subgoal of A2 satisfy the adjacency restriction
then
let A0 block(A1 ; A2)
Return Fold(hA0 ; A3 : : :Ak i; B)
else Return ;

Figure 13: The revised version of the dac algorithm. The candidate sets are built selectively,

without explicit creation of consistency sets. Candidate sets contain folded orderings,
and unfolding is performed only on the returned global ordering. The code of the
Unfold and Sort-by-cn procedures is not listed, due to its straightforwardness. The
merging procedure recursively extracts from the given folded orderings max-blocks that
are minimal by cn. The folding procedure joins two leading blocks into a larger one, as
long as they are cn-inverted.
70

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

a
b c(free) c(bound) d(free) d(bound) e(free) e(bound)
cost
10 5
5
5
10
5
20
10
nsols 0.8 2
2
0.5
4
1
0.4
0.1
cn
-0.02 0.2 0.2
-0.1
0.3
0
-0.03
-0.09
Table 2: Control values for the sample runs of the ordering algorithms.
contains cost values of c(X ) when d(X ) or e(X ) have already bound X . For example,
cost(c(X ))jfa;d(X )g = cost(c(bound)) = 5. The dac algorithm traverses the divisibility tree
of S0 as follows. (The names of the nodes are as in Figure 8.)
1. The root of the divisibility tree, n1, has empty binding set B(n1) = ;, and the
associated subgoal set S (n1) = fa; b; c(X ); d(X ); e(X )g. The set S (n1) is partitioned into two subsets under B(n1): one independent { fa; bg, and one indivisible {
fc(X ); d(X ); e(X )g. These two subsets correspond to two child nodes of the ANDnode n1: n2 and n3, both with empty binding sets.
2. S (n2) is independent under B(n2). Therefore, n2 is a leaf, and its sole candidate
ordering is obtained by sorting its subgoals by cn under B(n2). cn(a)j; = ,0:02,
cn(b)j; = 0:2, thus CandSet(n2) = fha; big.
3. S (n3) is indivisible under B(n3). Therefore, n3 is an OR-node, and its three children
are created { one for each subgoal of S (n3) serving as the binder.

 Binder c(X ) yields the child node n4 with the associated set S (n4) = fd(X ); e(X )g
and the binding set B(n4) = fc(X )g. S (n4) is independent under B(n4). There-

fore, n4 is a leaf, and its sole candidate is obtained by sorting its subgoals by
cn:
cn(d(X ))jfc(X)g = 0; cn(e(X ))jfc(X )g = ,0:09;
thus, the candidate of n4 is he(X ); d(X )i.
 Binder d(X ) yields the child node n5 with the associated set S (n5) = fc(X ); e(X )g
and the binding set B(n5) = fd(X )g. S (n5) is independent under B(n5), and its
sorting by cn produces the candidate hc(X ); e(X )i.
 Binder e(X ) yields the child node n6 with the associated set S (n6) = fc(X ); d(X )g
and the binding set B(n6) = fe(X )g. S (n6) is independent under B(n6), and its
sorting by cn produces the candidate hc(X ); d(X )i.
4. We now add each binder to its corresponding child's candidate and obtain three orderings of the OR-node n3: hc(X ); e(X ); d(X )i, hd(X ); c(X ); e(X )i, he(X ); c(X ); d(X )i.
5. We now perform folding of these orderings and check violations of the adjacency
restriction, in order to determine whether a block has a cheaper permutation.
71

fiLedeniov & Markovitch

 First, we perform the folding of hc(X ); e(X ); d(X )i. The pair hc(X ); e(X )i is
cn-inverted: cn(c(X ))j; = 0:2, cn(e(X ))jfc(X )g = ,0:09. We thus unite it into a
block. This block does not pass the adjacency restriction test (Equation 8):
cost(hc(X ); e(X )i)j; = 5 + 2  10 = 25;
cost(he(X ); c(X )i)j; = 20 + 0:4  5 = 22:

Therefore, this ordering is MC-contradicting and can be discarded.
 We perform the folding of hd(X ); c(X ); e(X )i. cn(d(X ))j; = 0:3, cn(c(X ))jfd(X )g =
,0:1, the pair is cn-inverted, and we unite it into a block. This block does not
pass the adjacency restriction test:
cost(hd(X ); c(X )i)j; = 10 + 4  5 = 30;
cost(hc(X ); d(X )i)j; = 5 + 2  5 = 15:

This ordering is rejected too, even before its folding is finished. If we continue
the folding process, we shall see that the subgoal e(X ) must also be added to this
block, since cn(hd(X ); c(X )i)j; = 4030:5,1 = 0:0333, and cn(e(X ))jhd(X );c(X )i =
,0:09.
 We perform the folding of he(X ); c(X ); d(X )i. cn(e(X ))j; = ,0:03, cn(c(X ))jfe(X)g
= ,0:1, the pair is cn-inverted, and we form a block ec(X ) = he(X ); c(X )i, which
passes the adjacency restriction test:
cost(he(X ); c(X )i)j; = 20 + 0:4  5 = 22;
cost(hc(X ); e(X )i)j; = 5 + 2  10 = 25:

We compute the control values of the new block:
cost(ec(X ))j; = 20 + 0:4  5 = 22
nsols(ec(X ))j; = 0:4  0:5 = 0:2
cn(ec(X ))j; = 0:222, 1 = ,0:0363636
cn(d(X ))jfec(X )g = 0, thus the pair hec(X ); d(X )i is cn-ordered, no more folding
is needed, and we add the folded candidate hec(X ); d(X )i to the candidate set
of n3.

6. We now perform merging of the candidate set of n2, fha; big, with the candidate set
of n3, fhec(X ); d(X )ig. In the resulting sequence max-blocks must be sorted by cn.

cn(a) = ,0:02; cn(b) = 0:2; cn(ec(X ))j; = ,0:0363636; cn(d(X ))jfec(X )g = 0:
The merged ordering, hec(X ); a; d(X ); bi, is added to the candidate set of n1.
7. We compare the costs of all candidates of n1, and output the cheapest one. In our case,
there is only one candidate, hec(X ); a; d(X ); bi. The algorithm returns this candidate
unfolded, he(X ); c(X ); a; d(X ); bi.
72

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Extension/Completion
Cost
h ai
10
hbi
5
hc(X )i
5
hd(X )i
10
he(X )i
20
hbi
hb; ai
the adjacency restriction test fails
hb; c(X )i
5 + 2  5 = 15
hb; d(X )i
5 + 2  10 = 25
hb; e(X )i
the adjacency restriction test fails
hc(X )i
hc(X ); e(X ); a; d(X ); bi
5 + 2(10 + 0:1(10 + 0:8(5 + 1  5))) = 28:6
hai
ha; bi
10 + 0:8  5 = 14
ha; c(X )i
10 + 0:8  5 = 14
ha; d(X )i
10 + 0:8  10 = 18
ha; e(X )i
the adjacency restriction test fails
hd(X )i
hd(X ); c(X ); e(X ); a; bi 10 + 4(5 + 0:5(10 + 0:1(10 + 0:8  5))) = 52:8
ha; bi
ha; b; c(X )i
14 + 0:8  2  5 = 22
ha; b; d(X )i
14 + 0:8  2  10 = 30
ha; b; e(X )i
the adjacency restriction test fails
ha; c(X )i
ha; c(X ); e(X ); d(X ); bi
14 + 0:8  2(10 + 0:1(5 + 1  5)) = 31:6
hb; c(X )i
hb; c(X ); e(X ); a; d(X )i
15 + 2  2(10 + 0:1(10 + 0:8  5)) = 60:6
ha; d(X )i
ha; d(X ); c(X ); e(X ); bi
18 + 0:8  4(5 + 0:5(10 + 0:1  5)) = 50:8
he(X )i
he(X ); c(X ); a; d(X ); bi 20 + 0:4(5 + 0:5(10 + 0:8(5 + 1  5))) = 25:6
ha; b; c(X )i
ha; b; c(X ); e(X ); d(X )i
22 + 0:8  2  2(10 + 0:1  5) = 55:6
hb; d(X )i
hb; d(X ); c(X ); e(X ); ai
25 + 2  4(5 + 0:5(10 + 0:1  10)) = 109
he(X ); c(X ); a; d(X ); bi complete ordering

;

Cheapest prefix

Table 3: A trace of a sample run of Algorithm 4 on the set of Figure 8. The left column shows the
cheapest prefix extracted from the list on each step, the middle column { its extensions
or completions that are added to the list, and the right column { their associated costs.

For comparison, we now show how the same task is performed by Algorithm 4. The
algorithm maintains a list of prefixes, sorted by their cost values, and which initially contains
an empty sequence. On each step the algorithm extracts from the list its cheapest element,
and adds to the list the extensions or completions of this prefix. Extensions are created when
the set of remaining subgoals is dependent, by appending each of the remaining subgoals
to the end of the prefix. Completions are created when the set of remaining subgoals is
independent, by sorting them and appending the entire resulting sequence to the prefix. An
extension is added to the list only when the adjacency restriction test succeeds on its two
last subgoals. To make the list operations faster, we can implement it as a heap structure
(Cormen et al., 1991).
The trace of Algorithm 4 on the set S0 is shown in Table 3. The left column shows the
cheapest prefix extracted from the list on each step, the middle column { its extensions or
completions that are added to the list, and the right column { their associated costs.
It looks as if the dac algorithm orders the given set S0 more eciently than Algorithm 4.
We can compare several discrete measurements to show this. For example, Algorithm 6
73

fiLedeniov & Markovitch

p2(X2,X5,X7,X9)
p1(X1,X2,X3,X4)
p5(X4,X8,X9,X10)
p3(X1,X5,X6,X8)
p4(X6,X3,X7,X10)

Figure 14: An example of the worst case for ordering. When all variables are initially free, every

subset of subgoals is indivisible under the binding of the rest of subgoals, and the overall
complexity of ordering by Algorithm 6 is O(n!).

performs 4 sorting sessions, each one with 2 elements, while Algorithm 4 performs 5 sortings
with 2 elements, and 3 sortings with 3 elements. The adjacency restriction is tested only 3
times by Algorithm 6, and 11 times by Algorithm 4. Algorithm 6 creates totally 8 different
ordered sub-sequences, with total length 22, while Algorithm 4 creates 24 ordered prefixes,
with total length 55.

4.8 Complexity Analysis

Both Algorithm 4 and Algorithm 6 find a minimal ordering, and both sort independent
subsets of subgoals whenever possible. Algorithm 6, however, offers several advantages due
to its divide-and-conquer strategy.
Let n be the number of subgoals in the initial set. For convenience, we assume that
the time of computing the control values for one subgoal is O(1); otherwise, if this time
is  , all the complexities below must be multiplied by  . The worst case complexity of
Algorithm 6 is O(n!). Figure 14 shows an example of such a case for n = 5. In this set
every two subgoals share a variable that does not appear in other subgoals. Thus, other
subgoals cannot bind it. The set of the root is indivisible, and no matter which binder is
chosen, the sets of the children are indivisible. So, in each child of the root, we must select
every remaining subgoal as the binder, and so on. The overall complexity of this execution
is O(n!). This is indeed the worst-case complexity: presence of AND-nodes in the tree can
only reduce it.
Note that even when n is small, such a complex rule body with (n2 ) free variables is
very improbable in practical programs. Also, the worst-case complexity can be reduced
to O(n2  2n ), if we move from divisibility trees to divisibility graphs (DAGs), where all
identical nodes of a divisibility tree (same subgoal set, same binding set) are represented
by a single vertex. The equivalence test of the tree nodes can be performed eciently with
the help of trie structures (Aho et al., 1987), where subgoals are sorted lexicographically.
Let there be n subgoals, with v shared variables appearing in m subgoals. As was
already noted in Section 4.3, the partition of subgoals into subsets can be performed in
74

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

O(n) average time, using a Union-Find data structure (Cormen et al., 1991, Chapter 22).

In the worst possible case, there are no AND-nodes in the divisibility tree, apart from the
root node (whose set is divisible into a dependent set of size m and an independent set of
size n , m). The overall complexity of the dac algorithm in such a case is
T (n; m; v ) = O(n)
| divisibility partition
+ O((nQ, m) log(n , m))
| ordering of independent subgoals
k
+ O(( i=0
ordering of dependent subgoals
Q (,m1(,mi)), i))log(m , k)) |
+ O(m Q ki=0
| folding
,1 (m , i))
+ O(n  ki=0
| merging
where k is the maximal possible number of bindings performed before the remaining subset
is independent. If we assume that every subgoal binds all its free variables (which happens
very frequently in practical logic programs), then k = minfv; m , 1g; otherwise k = m , 1.
k is equal to the maximal number of OR-nodes on a path from the root to a leaf of the
divisibility tree. Therefore, the height of the divisibility tree is limited by k + 1. Actually,
the tree can be shallower, since some binders can bind more than one shared variable each.
This means that the number of shared variables can decrease by more than 1 in each ORnode. Below we simplify the above formula for several common cases, when k is small and
when the abovementioned assumption holds (every subgoal binds all its free variables after
its proof terminates).
 If v < m  n: T (n; m; v) = O(n  mv + n  log n)
 If m  v  n: T (n; m; v) = O(n  mm,1 + n  log n)
 If v  m ' n: T (n; m; v) = O(nv+1  log n)
 If m  v ' n: T (n; m; v) = O(n  m! + n  log n)
Generally, for a small number v of shared variables, the complexity of the algorithm is
roughly bounded by O(nv+1  log n). In particular, if all subgoals are independent (v = 0),
the complexity is O(n log n). In most practical cases, the number of shared free variables
in a rule body is relatively small, and every subgoal binds all its free variables; therefore,
the algorithm has polynomial complexity. Note that even if a rule body in the program
text contains many free variables, most of them usually become bound after the rule head
unification is performed (i.e., before we start the ordering of the instantiated body).

5. Learning Control Knowledge for Ordering

The ordering algorithms described in the previous sections assume the availability of correct
values of average cost and number of solutions for various predicates under various argument
bindings. In this section we discuss how this control knowledge can be obtained by learning.
Instead of static exploration of the program text (Debray & Lin, 1993; Etzioni, 1993),
we adopt the approach of Markovitch and Scott (1989) and learn the control knowledge
by collecting statistics on the literals that were proved in the past. This learning can be
performed on-line or off-line. In the latter case, the ordering system first works with a
training set of queries, while collecting statistics. This training set can be built on the
75

fiLedeniov & Markovitch

distribution of user queries seen in the past. We assume that the distribution of queries
received by the system does not change significantly with time; hence, the past distribution
directs the system to learn relevant knowledge for the future queries.
While proving queries, the learning component accumulates information about the control values (average cost and number of solutions) of various literals. Storing a separate
value for each literal is not practical, for two reasons. The first is the large space required
by this approach. The second is the lack of generalization: the ordering algorithm is quite
likely to encounter literals which have not been seen before, and whose control values are
unknown. Recall that when we transformed Equation 2 into Equation 5, we moved from
control values of single literals to average control values over sets of literals. To obtain the
precise averages for these sets, we still needed the control values of individual literals. Here,
we take a different approach, that of learning and using control values for more general
classes of literals. The estimated cost (nsols) value of a class can be defined as the average
real cost (nsols) value of all examples of this class that were proved in the past.
The more refined the classes, the smaller the variance of real control values inside each
class, the more precise the cost and nsols estimations that the classes assign to their members, and the better orderings we obtain. One easy way to define classes is by modes
or binding patterns (Debray & Warren, 1988; Ullman & Vardi, 1988): for each argument we denote whether it is free or bound. For example, for the predicate father the
possible classes are father(free,free), father(bound,free), father(free,bound) and
father(bound,bound). Now, if we receive a literal (for example, father(abraham,X)),
we can easily determine its binding pattern (in this case, father(bound,free)) and retrieve the control information stored for this class. Of course, to find the binding pattern
of a subgoal with a given binding set, we need a method to determine which variables are
bound by the subgoals of the binding set. The same problem arose in DPart computation
(Section 4.3). We shall discuss some practical ways to solve this problem in Section 7.1.
For the purpose of class definition we can also use regression trees { a type of decision tree
that classifies to continuous numeric values and not to discrete classes (Breiman et al., 1984;
Quinlan, 1986). Two separate regression trees can be stored for every program predicate,
one for its cost values, and one for the nsols. The tests in the tree nodes can be defined
in various ways. If we only use the test \is argument i bound?", then the classes of literals
defined by regression trees coincide with the classes defined by binding patterns. But we
can also apply more sophisticated tests, both syntactic (e.g., \is the third argument a term
with functor f?") and semantic (e.g., \is the third argument female?"), which leads to
more refined classes and better estimations. A possible regression tree for estimating the of
number of solutions for predicate father is shown in Figure 15.
Semantic tests about the arguments require logic inference (in the example of Figure 15
{ invoking the predicate female on the first argument of the literal). Therefore, they must
be as ecient as possible. Otherwise the retrieval of control values will take too much time.
The problem of ecient learning of control values is further considered elsewhere (Ledeniov
& Markovitch, 1998a).
Several researchers applied machine learning techniques for accelerating logic inference
(Cohen, 1990; Dejong & Mooney, 1986; Langley, 1985; Markovitch & Scott, 1993; Minton,
1988; Mitchell, Keller, & Kedar-Cabelli, 1986; Mooney & Zelle, 1993; Prieditis & Mostow,
1987). Some of these works used explanation-based learning or generalized caching tech76

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Average: 3.1416
Test: bound(arg1)?

yes

no

Average: 0.3
Test: female(arg1)?

yes
Average: 0.0

Average: 5
Test: bound(arg2)?

no

yes

Average: 0.5
Test: bound(arg2)?

Average: 0.98

yes

no

Average: 0.0001

Average: 1.0

no
Average: 50

Figure 15: A regression tree that estimates the number of solutions for father(arg1,arg2).
niques to avoid repeated computation. Others utilized the acquired knowledge for the problem of clause selection. None of these works, however, dealt with the problem of subgoal
reordering.

6. Experimentation
To test the effectiveness of our ordering algorithm, we experimented with it on various
domains, and compared its performance to other ordering algorithms. Most experiments
were performed on randomly created artificial domains. We also tested the performance of
the system on several real domains.

6.1 Experimental Methodology
All experiments described below consist of a training session, followed by a testing session.
Training and testing sets of queries are randomly drawn from a fixed distribution. In the
training session we collect the control knowledge for literal classes. In the testing session we
prove the queries of the testing set using different ordering algorithms, and compare their
performance using various measurements.
The goal of ordering is to reduce the time spent by the Prolog interpreter when it
proves queries of the testing set. This time is the sum of the time spent by the ordering
procedure (ordering time) and the time spent by the interpreter (inference time). Since the
CPU time is known to be very sensitive to irrelevant factors such as hardware, software
and programming quality, we also show two alternative discrete measurements: the total
number of clause unifications, and the total number of clause reductions performed. The
number of reductions reects the size of the proof tree.
For experimentation we used a new version of the lassy system (Markovitch & Scott,
1989), using regression trees for learning, and the ordering algorithms discussed in this
paper.
77

fiLedeniov & Markovitch

6.2 Experiments with Artificial Domains

In order to ensure the statistical significance of the results of comparing different ordering
algorithms, we experimented with many different domains. For this purpose, we created a
set of 100 artificial domains, each with a small fixed set of predicates, but with a random
number of clauses in each predicate, and with random rule lengths. Predicates in the
rule bodies, and arguments in both rule heads and bodies are randomly drawn from fixed
distributions. Each domain has its own training and testing sets (these two sets do not
intersect).
The more training examples are fed into the system on the learning phase, the better
estimations of control values it produces. On the other hand, the learning time must be limited, because after seeing a certain number of training examples, new examples do not bring
much new information, and additional learning becomes wasteful. We have experimentally
built a learning curve which shows the dependence of the quality of the control knowledge
on the amount of training. The curve suggests that after control values were learned for
approximately 400 literals, there is no significant improvement in the quality of ordering
with new training examples. Therefore, in the subsequent experiments we stopped training
after 600 cost values were learned. The training time was always small: one learned cost
value corresponds to a complete proof of a literal. Thus, if every predicate in a program has
four clauses that define it, then 600 cost values are learned after 2400 unifications, which is
a very small time.
The control values were learned by means of regression trees (Section 5), with simple
syntactic tests that only checked whether some argument is bound or whether some argument is a term with a certain functor (the list of functors was created automatically when
the domain was loaded). However, as we shall see, even these simple tests succeeded in
making good estimations of control values.
We tested the following ordering methods:
 Random: The subgoals are permuted randomly and the control knowledge is not
used.
 Algorithm 3: Building ordered prefixes. Out of all prefixes that are permutation of
one another, only the cheapest one is retained.
 Algorithm 3a: As Algorithm 3, but with best-first search method used to define the
next processed prefix. A similar algorithm was used in the lassy system of Markovitch
and Scott (1989).
 Algorithm 3b: As Algorithm 3a, but with adjacency restriction test added. A
similar algorithm was described by Smith and Genesereth (1985).
 Algorithm 4: As Algorithm 3b, but whenever all the subgoals that are not in the
prefix are independent (under the binding of the prefix), they are sorted and the result
is appended to the prefix as one unit.
 Algorithm 6: The dac algorithm.
In our experiments we always used the Bubble-Sort algorithm to sort literals in independent sets. This algorithm is easy to implement, and it is known to be ecient for small
78

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Ordering
Method
Random
Algorithm 3
Algorithm 3.a
Algorithm 3.b
Algorithm 4
Algorithm 6

Unifications Reductions Ordering Inference Total Ord.Time
Time
Time Time Reductions
86052.06
27741.52
8.1910
27.385 35.576 0.00029
2600.32
911.04 504.648
1.208 505.856 0.55
2829.00
978.59 347.313
1.178 348.491 0.35
2525.34
881.12 203.497
1.137 204.634 0.23
2822.27
976.02
40.284
1.191 41.475 0.04
2623.82
914.67
2.3620
1.102 3.464 0.0025

Table 4: The effect of ordering on the tree sizes and the CPU time (mean results over 100 artificial
domains).

sets, when the elements are already ordered, or nearly ordered. In practice, programmers
order most program rules optimally, and the sorting stops early.
Since the non-deterministic nature of the random method introduces additional noise,
we performed on each artificial domain 20 experiments with this method, and the table
presents the average values of these measurements.
Table 4 shows the obtained results over 100 domains: the rows correspond to the ordering
methods used, and the columns to the measurements taken. The rightmost column shows
the ratio of the ordering time and the number of reductions performed, which reects the
average ordering time of one rule body. The inference time was not measured separately,
but was set as the difference of the total time and the ordering time.
Several observations can be made:
1. Using the dac ordering algorithm helps to reduce the total time of proving the testing
set of queries by a factor of 10, compared to the random ordering. The inference time
is reduced by a factor of 25.
2. All deterministic ordering methods have similar number of unifications and reductions,
and similar inference time, which is predictable, since they all find minimal orderings.
Small uctuations of these values can be explained by the fact that some rules have
several minimal orderings under the existing control knowledge, and different ordering
algorithms select different minimal orderings. Since the control knowledge is not
absolutely precise, the real execution costs of these orderings may be different, which
leads to the differences. The random ordering method builds much larger trees, with
larger inference time.
3. When we compare the performance of the deterministic algorithms (3 { 6), we see
that the dac algorithm performs much better than the algorithms that build ordered
prefixes. In the latter ones, the ordering is expensive, and smaller inference time
cannot compensate for the increase in ordering time. Only Algorithm 4, a combination
of several ideas of previous researchers, has total time comparable with the time of
the random method (though still greater).
79

fiLedeniov & Markovitch

4. It may seem strange that the simple random ordering method has larger ordering time
than the sophisticated Algorithm 6. To explain this, note that the random method
creates much larger proof trees (on average), therefore the number of ordered rules
increases, and even the cheap operations, like random ordering of a rule, sum up to
a considerable time. The average time spent on ordering of one rule is shown in the
last column of Table 4; this value is very small in the random method.

6.3 Experiments with Real Domains

We tested our ordering algorithm also on real domains obtained from various sources. These
domains allow us to compare orderings performed by our algorithm with orderings performed by human programmers.
The following domains were used:
 Moral-reasoner: Taken from the Machine Learning Repository at the University
of California, Irvine1 . The domain qualitatively simulates moral reasoning: whether
a person can be considered guilty, given various aspects of his character and of the
crime performed.
 Depth-first planner: Program 14.11 from the book \The Art of Prolog" (Sterling
& Shapiro, 1994). The program implements a simple planner for the blocks world.
 Biblical Family Database: A database similar to that described in Example 1.
 Appletalk: A domain describing the physical layout of a local computer network
(Markovitch, 1989).
 Benchmark: A Prolog benchmark taken from the CMU Artificial Intelligence Repository2. The predicate names are not informative: it is an example of a program where
manual ordering is dicult.
 Slow reverse: Another benchmark program from the same source.
 Geography: Also a benchmark program from the CMU Repository. The domain
contains many geographical facts about countries.
Table 5 shows the results obtained. For ordering we used the dac algorithm, with literal
classes defined by binding patterns. It can be seen that the dac algorithm was able to speed
up the logic inference in real domains as well. Note that in the Slow Reverse domain the
programmer's ordering was already optimal; thus, applying the ordering algorithm did not
reduce the tree sizes. Still, the overhead of the ordering is not significant.

7. Discussion

In this concluding section we discuss several issues concerning the practical implementation
of the dac algorithm and several ways to increase its eciency. Then we survey some
related areas of logic programming and propose the use of the dac algorithm there.
1. URL: http://www.ics.uci.edu/~mlearn/MLRepository.html
2. URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/html/air.html

80

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Domain

Without ordering
With ordering
Gain ratio
unifications seconds unifications seconds (time/time)
Moral-reasoner
352180 98.39
87020 23.53
4.2
Depth-first planner
10225 19.01
9927 18.16
1.05
Biblical Family
347827 112.68
120701 46.08
2.5
Appletalk
5036167 1246.30
640092 221.73
5.6
Benchmark
62012 554.31
46012 395.04
1.4
Slow reverse
6291 10.33
6291 11.92
0.9
Geography
428480 141.47
226628 82.76
1.7
Table 5: Experiments on real domains.

7.1 Practical Issues

In this subsection we would like to address several issues related to implementation and
applications of the dac algorithm.
The computation of the DPart function (Section 3.2.1) requires a procedure for computing the set of variables bound by a given binding set of subgoals. The same procedure
is needed for computing control values (Section 5). There are several possible ways to
implement such a procedure. For example:
1. The easiest way is to assume that every subgoal binds all the variables appearing in its
arguments. This simplistic assumption is sucient for many domains, especially the
database-oriented ones. However, it is not appropriate when logic programs are used
to manipulate complex data structures containing free variables (such as difference
lists). This assumption was used for the experiments described in Section 6.
2. Some dialects of Prolog and other logic languages support mode declarations provided
by the user (Somogyi et al., 1996b). When such declarations are available, it is easy
to infer the binding status of each variable upon exiting a subgoal.
3. Even when the user did not supply enough mode declarations, they can often be
inferred from the structure of the program by means of static analysis (Debray &
Warren, 1988). Note, however, that as was pointed out by Somogyi et al. (1996b),
no-one has yet demonstrated a mode inference algorithm that is guaranteed to find
accurate mode information for every predicate in the program.
4. We can learn the sets of variables bound by classes of subgoals using methods similar
to those described in Section 5 for learning control values.
Several researchers advocate user declarations of available (permitted) modes. Such
declarations can be elegantly incorporated into our algorithm to prune branches that violate
available modes. When we fix a binder in an OR-node, we compute the set of variables
that become bound by it. If this results in a violation of an available mode for one of the
subgoals of the corresponding child, then the whole subtree of this child is pruned. Note
that we can detect violations even when the mode of the subgoal is partially unknown
81

fiLedeniov & Markovitch

CandidateSet(S ; B)
let fS1; S2; : : : Sk g DPart(S ; B)
case

:::
 k = 1, shared-vars(S1) 6= ; (S is indivisible under B):
loop for A 2 S
if B [ fAg does not violate available modes
in any subgoal of S n fAg
then
let C (A) CandidateSet
(S fin fAg; B [ fAog)
n
0
let C (A)
Fold(AkO~ A; B) fifi O~ A 2 C (A)
else let
C 0(A) ; (don't enter the branch)
S
Return A2S C 0 (A)

Figure 16: Changes to Algorithm 6 that make use of available mode declarations.
The rest of the algorithm remains unchanged.

at the moment. For example, if all the available modes require that the first argument
be unbound, then binding of the argument by the OR-node binder will trigger pruning,
even if the binding status of the other arguments is not yet known. Figure 16 shows how
Algorithm 6 can be changed in order to incorporate declarations of available modes. Any
other correctness requirement can be treated in a similar manner: a candidate ordering will
be rejected whenever we see that it violates the requirement.
The experiments described in Section 6 were performed with a Prolog interpreter. Is
it possible to combine the dac algorithm with a Prolog compiler? There are several ways
to achieve this goal. One way is to allow the compiler to insert code for on-line learning.
The compiled code will contain procedures for accumulating control values and for the dac
algorithm. Alternatively, off-line learning can be implemented, with training as a part of
the compilation process.
Another method for combining our algorithm with existing Prolog compilers is to use
it for program transformation, and to process the transformed program by a standard
compiler. Elsewhere (Ledeniov & Markovitch, 1998a) we describe the method for classifying
the orderings produced by the dac algorithm. For each rule we build a classification tree,
where classes are the different orderings of the rule body, and the tests are applied to the
rule head arguments. These are the same type of tests described in Section 5 for learning
control values. Figure 17 shows two examples of such trees.
Given such a classification tree, we can write a set of Prolog rules, where each rule has
the same head as the original rule, and has a body built of all the tests on the path from
the tree root to a leaf node followed by the ordering at the leaf. For example, the second
tree in Figure 17 yields the following set of rules:
82

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

A classification tree for the rule
uncle(X,Y) of Example 1.

nonvar(Y) ?
yes

no

[parent(Z,Y),brother(Z,X)]
[brother(Z,X),parent(Z,Y)]
nonvar(X) ?
yes

A possible classification tree for the rule
head(X,Y)

male(X) ?

p1(X), p2(Y), p3(X,Y).

yes
[p1(X),p3(X,Y),p2(Y)]

no

nonvar(Y) ?

no

yes

no

[p2(Y),p3(X,Y),p1(X)]

[p1(X),p2(Y),p3(X,Y)]

[p3(X,Y),p1(X),p2(Y)]

Figure 17: Examples of classification trees that learn rule body orderings.

head(X,Y)
head(X,Y)
head(X,Y)
head(X,Y)

nonvar(X), male(X), p1(X), p3(X,Y), p2(Y).
nonvar(X), not(male(X)), p1(X), p2(Y), p3(X,Y).
var(X), nonvar(Y), p2(Y), p3(X,Y), p1(X).
var(X), var(Y), p3(X,Y), p1(X), p2(Y).

From Table 4 we can see that while the dac algorithm helped to reduce the inference
time by a factor of 25, the total time was reduced only by a factor of 10. This difference
is caused by the additional computation of the ordering procedure. There is a danger that
the benefit obtained by ordering will be outweighed by the cost of the ordering process.
This is a manifestation of the so-called utility problem (Minton, 1988; Markovitch & Scott,
1993). In systems that are strongly-moded (such as Mercury { Somogyi et al., 1996b) we can
employ the dac algorithm statically at compilation time for each one of the available modes,
thus reducing the run-time ordering time to zero. The mode-based approach performs only
syntactic tests of the subgoal arguments. The classification tree method, described above,
is a generalization of the mode-based approach, allowing semantic tests as well.
Due to insucient learning experience or lack of meaningful semantic tests, it is quite
possible that the classification trees contain leaves with large degrees of error. In such cases
we still need to perform the ordering dynamically. To reduce the harmfulness of the utility
problem in the case of dynamic ordering, we can use a cost-sensitive variation of the dac
algorithm (Ledeniov & Markovitch, 1998a, 1998b). This modified algorithm deals with the
problem by explicit reasoning about the economy of the control process. The algorithm is
anytime, that is, it can be stopped at any moment and return its currently best ordering
(Boddy & Dean, 1989). We learn a resource-investment function to compute the expected
return in speedup time for additional control time. This function is used to determine a
stopping condition for the anytime procedure. We have implemented this framework and
found that indeed we have succeeded in reducing ordering time, without significant increase
of inference time.
83

fiLedeniov & Markovitch

7.2 Relationship to Other Works

The work described in this paper is a continuation of the line of research initiated by Smith
and Genesereth (1985) and continued by Natarajan (1987) and Markovitch and Scott (1989).
This line of research aims at finding the most ecient ordering of a set of subgoals. The
search for minimal-cost ordering is based on cost analysis that utilizes available information
about the cost and number-of-solutions of individual subgoals.
Smith and Genesereth (1985) performed an exhaustive search over the space of all
permutations of the given set of subgoals, using the adjacency restriction to reduce the
size of the search space (Equation 8). This restriction was applied on pairs of adjacent
subgoals in the global ordering of the entire set. When applied to an independent set of
subgoals, the adjacency restriction is easily transformed into the sorting restriction: the
subgoals in a minimal ordering must be sorted by their cn values. Natarajan (1987) arrived
at this conclusion and presented an ecient ordering algorithm for independent sets.
The dac algorithm uses subgoal dependence to break the set into smaller subsets. Independent subsets are sorted. Dependent subsets are recursively ordered, and the resulting
orderings are merged using a generalization of the adjacency restriction that manipulates
blocks of subgoals. Therefore the dac algorithm is a generalization of both algorithms.
During the last decade, a significant research effort went into static analysis (SA) of
logic programs. There are three types of SA that can be exploited by the dac algorithm to
reduce the ordering time.
A major part of the SA research deals with program termination (De Schreye & Decorte,
1994). The dac algorithm solves the termination problem, as a special case of the eciency
problem (it always finds a terminating ordering, if such orderings exist). During learning,
we set limits on the computation resources available for subgoal execution. If a subgoal is
non-terminating (in a certain mode), the learning module will associate a very high cost
with this particular mode. Consequently, the dac algorithm will not allow orderings with
this mode of the subgoal. Nevertheless, while the use of static termination analysis is
not mandatory for a proper operation of the dac algorithm, we can exploit such analysis
to increase the eciency of both the learning process and the ordering process. During
learning, the limit that we set on the computation resources devoted to the execution of
a subgoal must be high, to increase the reliability of the cost estimation. However, such
a high limit can lead to a significant increase in learning time when many subgoals are
non-terminating. If termination information obtained by SA is available, we can use it to
avoid entering infinite branches of proof trees. During ordering, termination information can
serve to reduce the size of space of orderings searched by the algorithm. If the termination
information comes in the form of allowed modes (Somogyi et al., 1996b), orderings that
violate these modes are filtered out, as in the modified algorithm shown in Figure 16. If the
termination information comes in the form of a partial order between subgoals, orderings
that violate this partial order can be filtered out in a similar manner.
The second type of SA research that can be combined with the dac algorithm is correctness analysis, where the program is tested against specifications given by the user.
The folon environment (Henrard & Le Charlier, 1992) was designed to support the
methodology for logic program construction that aims at reconciling the declarative semantics with an ecient implementation (Deville, 1990). The construction process starts with
84

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

a specification, converts it into a logic description and finally, into a Prolog program. If
the rules of the program are not correct with respect to the initial specification, the system performs transformations such as reordering literals in a clause, adding type checking
literals and so on. De Boeck and Le Charlier (1990) mention this reordering, but do not
specify an ordering algorithm different from the simple generate-and-test method. Cortesi,
Le Charlier, and Rossi (1997) present an analyzer for verifying the correctness of a Prolog
program relative to a specification which provides a list of input/output annotations for the
arguments and parameters that can be used to establish program termination. Again, no
ordering algorithm is given explicitly. The purpose of the dac algorithm is complementary
to the purpose of folon, and it could serve as an auxiliary aid to make the resulting Prolog
program more ecient.
Recently, the Mercury language was developed at the University of Melbourne (Somogyi
et al., 1996a, 1996b). Mercury is a strongly typed and strongly moded language. Type and
mode declarations should be supplied by the programmer (though recent releases of the
Mercury system already support partial inference of types and modes { Somogyi et al.,
1996a). The compiler checks that mode declarations for all predicates are satisfied; if
necessary, it reorders subgoals in the rule body to ensure mode correctness (and rejects the
program if neither ordering satisfies the mode declaration constraints). When the compiler
performs this reordering, it does not consider the eciency issue. It often happens that
several orderings of a rule body satisfy the mode declaration constraints: in such cases
the Mercury compiler could call the static version of the dac algorithm to select the most
ecient ordering. Another alternative is to augment the dac algorithm by mode declaration
checks, as was shown in Figure 16.
Note that Mercury is a purely declarative logic programming language, and is therefore
more suitable for subgoal reordering than Prolog. It has no non-logical constructs that
could destroy the declarative semantics which give logic programs their power; in Mercury
even I/O is declarative.
The third type of relevant SA is the cost analysis of logic programs (Debray & Lin,
1993; Braem et al., 1994; Debray et al., 1997). Cortesi et al. (1997) describe a cost formula
similar to Equation 5 to select a lowest-cost ordering. However, they used a generate-andtest approach which can sometimes be prohibitively expensive. Static analysis of cost and
number of solutions can be used to obtain the control values, instead of learning them.
The eciency of logic programs can also be increased by methods of program transformation (Pettorossi & Proietti, 1994, 1996). One of the most popular approaches is the
\rules+strategies" approach, which consists in starting from an initial program and then
applying one or more elementary transformation rules. Transformation strategies are metarules which prescribe suitable sequences of applications of transformation rules.
One of the possible transformation rules is the goal rearrangement rule which transforms
a program by transposing two adjacent subgoals in a rule body. Obviously, any ordering
of a rule body can be transformed into any other ordering by a finite number of such
transpositions. Thus, static subgoal ordering can be considered a special case of program
transformation where only the goal rearrangement rule is used. On the other hand, dynamic
and semi-dynamic ordering methods cannot be represented by simple transformation rules,
since they make use of run-time information (expressed in bindings that rule body subgoals
85

fiLedeniov & Markovitch

obtain through unifications of rule heads), and may order the same rule body differently
under different circumstances.
A program transformation technique called compiling control (Bruynooghe, De Schreye,
& Krekels, 1989; Pettorossi & Proietti, 1994) follows an approach different from that of
trying to improve the control strategy of logic programs. Instead of enhancing the naive
Prolog evaluator using a better (and often more complex) computation rule, the program is
transformed so that the derived program behaves under the naive evaluator exactly as the
initial program would behave under an enhanced evaluator. Most forms of compiling control
first translate the initial program into some standard representation (for example, into an
unfolding tree), while the complex computation rule is used, and then the new program is
constructed from this representation, with the naive computation rule in mind.
Reordering of rule body subgoals can be regarded as moving to a complex computation
rule which selects subgoals in the order dictated by the ordering algorithm. In the case of
the dac algorithm, this computation rule may be too complex for simple use of compiling
control methods. Nevertheless, it can be easily incorporated into a special compiling control
method. In Section 7.1 we described a method of program rewriting which first builds
classification trees based on the orderings that were performed in the past, and then uses
these classification trees for constructing clauses of a derived program. The derived program
can be eciently executed under the naive computation rule of Prolog. This technique is
in fact a kind of compiling control. Its important property is the use of knowledge collected
from experience (the orderings that were made in the past).
One transformation method that can significantly benefit from the dac algorithm is
unfolding (Tamaki & Sato, 1984). During the unfolding process subgoals are replaced by
their associated rule bodies. Even if the initial rules were ordered optimally by a human
programmer or a static ordering procedure, the resulting combined sequence may be far from
optimal. Therefore it could be very advantageous to use the dac algorithm for reordering
of the unfolded rule. As the rules become longer, the potential benefit of ordering grows.
The danger of high complexity of the ordering procedure can be overcome by using the
cost-sensitive version of the dac algorithm (Section 7.1).

7.3 Conclusions
In this work we study the problem of subgoal ordering in logic programs. We present both a
theoretical base and a practical implementation of the ideas, and show empirical results that
confirm our theoretical predictions. We combine the ideas of Smith and Genesereth (1985),
Simon and Kadane (1975) and Natarajan (1987) into a novel algorithm for ordering of
conjunctive goals. The algorithm is aimed at minimizing the time which the logic interpreter
spends on the proof of the given conjunctive goal.
The main algorithm described in this paper is the dac algorithm (Algorithm 6, Section 4.6). It works by dividing the sets of subgoals into smaller sets, producing candidate
sets of orderings for the smaller sets, and combining these candidate sets to obtain orderings
of the larger sets. We prove that the algorithm finds a minimal ordering of the given set
of subgoals, and we show its eciency under practical assumptions. The algorithm can
be employed statically (to reorder rule bodies in the program text before the execution
86

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

starts), semi-dynamically (to reorder the rule body before the reduction is performed) or
dynamically (to reorder the resolvent after every reduction of a subgoal by a rule body).
Several researchers (Minker, 1978; Warren, 1981; Naish, 1985a, 1985b; Nie & Plaisted,
1990) proposed various heuristics for subgoal ordering. Though fast, these methods do
not guarantee finding minimal-cost orderings. Our algorithm provably finds a minimal-cost
ordering, though the ordering itself may take more time than with the heuristic methods. In
the future it seems promising to incorporate heuristics into the dac algorithm. For example,
heuristics can be used to grade binders in OR-nodes: rather than exhaustively trying all
subgoals as binders, we could try just one, or several binders, thus reducing the ordering
time. Also, the current version of our ordering algorithm is suitable only for finding all
solutions to a conjunctive goal. We would like to extend it to the problem of finding one
solution, or a fixed number of solutions.
Another interesting issue for further research is the adaptation of the dac algorithm to
interleaving ordering methods (Section 2.3). There, if subgoals of a rule body are added
to an ordered resolvent, it seems wasteful to start a complete ordering process; we should
use the information stored in the existing ordering of the resolvent. Perhaps the whole
divisibility tree of the resolvent should be stored, and its nodes updated when subgoals of
a rule body are added to the resolvent.
The ordering algorithm needs control knowledge for its work. This control knowledge is
the average cost and number of solutions of literals, and it can be learned by training and
collecting statistics. We make an assumption that the distribution of queries received by
the system does not change with time; thus, if the training set is based on the distribution
seen in the past, the system learns relevant knowledge for future queries. We consider the
issue of learning control values more thoroughly in another paper (Ledeniov & Markovitch,
1998a), together with other issues concerning the dac algorithm (such as minimizing the
total time, instead of minimizing the inference time only).
Ullman and Vardi (1988) showed that the problem of ordering subgoals to obtain termination is inherently exponential in time. The problem we work with is substantially
harder: we must not only find an order whose execution terminates in finite time, but one
that terminates in minimal finite time. It is impossible to find an ecient algorithm for
all cases. The dac algorithm, however, is ecient in most practical cases, when the graph
representing the subgoal dependence (Figure 3) is sparsely connected.
We have implemented the dac algorithm and tested it on artificial and real domains.
The experiments show a speedup factor of up to 10 compared with random ordering, and
up to 13 compared with some alternative ordering algorithms.
The dac algorithm can be useful for many practical applications. Formal hardware
verification has become extremely important in the semiconductor industry. While model
checking is currently the most widely used technique, it is generally agreed that coping with
the increasing complexity of VLSI design requires methods based on theorem proving. The
main obstacle preventing the use of automatic theorem proving is its high computational
demands. The dac algorithm may be used for speeding up logic inference, making the use
of automatic theorem provers more practical.
Logic has gained increasing popularity for representation of common-sense knowledge.
It has several advantages, including exibility and well-understood semantics. Indeed, the
CYC project (Lenat, 1995) has recently moved from frame-based representation to logic87

fiLedeniov & Markovitch

based representation. However, the large scale of such knowledge bases is likely to present
significant eciency problems to the inference engines. Using automatic subgoal ordering
techniques, such as those described here, may help to solve these problems.
The issue of subgoal ordering obtains a new significance with the development of Inductive Logic Programming (Lavrac & Dzeroski, 1994; Muggleton & De Raedt, 1994). Systems
using this approach, such as FOIL (Quinlan & Cameron-Jones, 1995), try to build correct
programs as fast as possible, without considering the eciency of the produced programs.
Combining the dac algorithm with Inductive Logic Programming and other techniques for
the synthesis of logic programs (such as the deductive and the constructive approaches)
looks like a promising direction.

Appendix A. Proof of Lemma 8

In this appendix we present the proof of a lemma which was omitted from the main text of
the paper for reasons of compactness. Before we prove it we show two auxiliary lemmas.

Lemma 9

Let A~ 1 and A~ 2 be two ordered sequences of subgoals, and B a set of subgoals. The value of
cn(A~ 1kA~2)jB lies between the values cn(A~ 1)jB and cn(A~ 2)jB[A~ .
1

Proof:

Denote c1 = cost(A~1 )jB
n1 = nsols(A~1)jB
cn1 = cn(A~1)jB
c2 = cost(A~2 )jB[A~ n2 = nsols(A~2)jB[A~ cn2 = cn(A~2)jB;[A~
c1;2 = cost(A~1 kA~ 2)jB n1;2 = nsols(A~ 1kA~ 2 )jB cn1;2 = cn(A~ 1 kA~ 2)jB
cn = n1;2 , 1 = n1 n2 , 1 = (n1 , 1) + n1 (n2 , 1) =
1

1;2

1

1

c1;2

c1;2
c1;2
n
n
1 ,1
2 ,1
c
+n c
= 1 c1 c 1 2 c2 = c1 cn1 +c n1 c2cn2 = cc1  cn1 + nc1 c2  cn2
1;2
1;2
1;2
1;2
n
c
1 c2
1
So, cn1;2 always lies between cn1 and cn2 (because c1;2 and c1;2 are positive and
to 1). More exactly, the point cn1;2 divides the segment [cn1; cn2] with ratio

sum

(cn1;2 , cn1 ) : (cn2 , cn1;2 ) = n1 c2 : c1:

In other words, cn1;2 is a weighted average of cn1 and cn2 . Note that c1 is the amount
of resources spent in the proof-tree of B~ 1 , n1 c2 { the resources spent in the tree of B~ 2 , and
c1;2 is their sum. So, the more time (relatively) we dedicate to the proof of B~ 1 , the closer
cn1;2 is to cn1. This conclusion can be generalized to a larger number of components in a
concatenation (the proof is by induction):
cost(A~ 1 )jB
cn(A~ 1 kA~ 2 k : : : A~ k )jB =
 cn(A~ 1 )jB +
cost(A~ 1 kA~ 2 k : : : A~ k )jB
nsols(A~ 1 )jB  cost(A~ 2 )jB[A~ 1
+
 cn(A~ 2 )jB[A~ 1 + : : : +
cost(A~ 1 kA~ 2k : : : A~ k )jB
nsols(A~ 1 kA~ 2 k : : : A~ k,1)jB  cost(A~ k )jB[A~ 1 [:::A~ k,1
+
 cn(A~ k )jB[A~ 1 [:::A~ k,1
cost(A~ 1kA~ 2 k : : : A~ k )jB
88

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

2

Lemma 10
Let S0 be a set of subgoals and N be a node in the divisibility tree of S0. Let O~ N =
Q~ kA~ 1kA~2 kR~ be an ordering of S (N ), where A~ 1 and A~ 2 are cn-equal max-blocks: cn(A~ 1 )jB(N )[Q~ =
cn(A~ 2)jB(N )[Q~ [A~ .
Let M be an ancestor of N and O~ M be an ordering of S (M ) consistent with O~ N , where
1

A~ 1 and A~ 2 are not violated. Then either A~ 1 and A~ 2 are both max-blocks in O~ M and all
max-blocks that stand between them are cn-equal to them, or A~ 1 and A~ 2 belong to the same
max-block in O~ M , or O~ M is MC-contradicting.
Proof: By induction on the distance between N and M . If M = N , then A~1 and A~2
are max-blocks, and the lemma holds. Let M 6= N , and let M 0 be the child of M whose
0 be the
descendant is N . By inductive hypothesis, the lemma holds for N and M 0 . Let O~ M
0
0
~
~
~
~
projection of OM on M . A1 and A2 are not violated in OM , since they are not violated in
O~ M .
 If A~ 1 and A~2 are both max-blocks in O~ M0 , then by the inductive hypothesis all maxblocks that stand between them are cn-equal to them. If M is an OR-node, no new
subgoals can enter between A~ 1 and A~ 2 . If M is an AND-node, the insertion of new

subgoals is possible, but if it violates blocks, or places max-blocks not ordered by cn,
then O~ M is MC-contradicting, by Corollary 3 or Lemma 6. So, if O~ M is not MCcontradicting, then all new max-blocks inserted between A~ 1 and A~ 2 must be cn-equal
to them both.
Assume that A~ 1 and A~ 2 are not both max-blocks in O~ M . Without loss of generality,
let A~ 1 be member of a larger max-block in O~ M . We show that A~ 2 must also participate
in the same max-block.
Since A~ 1 joined a larger block, there must exist another block, B~ , adjacent to A~ 1,
such that their pair is cn-inverted. Let B~ stand to the left of A~ 1 (in the opposite case,
~ A~ 1i is cn-inverted, i.e.,
the proof is similar): O~ M = X~ kB~ kA~ 1kY~ kA~ 2 kZ~ . The pair hB;
~
~
~
~
cn(B )jB(M )[X~ > cn(A1)jB(M )[X~ [B~ . From Lemma 9, cn(B kA1)jB(M )[X~ > cn(A~ 1)jB(M )[X~ [B~ ,
and we must add to the block B~ kA~ 1 all blocks from Y~ , because they are all cn-equal
to A~ 1 . Also, cn(A~ 1)jB(M )[X~ [B~ = cn(A~ 2 )jB(M )[X~ [B~ [A~ , and A~ 2 must also be added
to the block. Thus, A~ 1 and A~ 2 belong to the same max-block in O~ M .
 If A~1 and A~ 2 belong to the same max-block in O~ M0 , then this block is either violated
in O~ M , or not. In the former case, O~ M is MC-contradicting, by Corollary 3. In the
latter case, A~ 1 and A~ 2 belong to the same max-block in O~ M .
 If O~ M0 is MC-contradicting, then O~ M is MC-contradicting too (the proof is easy). 2
1

Now we can prove Lemma 8:

Lemma 8
Let S0 be a set of subgoals, N be a node in the divisibility tree of S0 and O~ N = Q~ kA~ 1 kA~ 2kR~
89

fiLedeniov & Markovitch

be an ordering of S (N ), where A~ 1 and A~ 2 are max-blocks, mutually independent and
cn-equal under the bindings of B(N ) [ Q~ . Then O~ N is blockwise-equivalent with O~ N0 =
Q~ kA~ 2kA~1 kR~ .

Proof:

Let S~ be a minimal ordering of S0 binder-consistent with O~ N . By Corollary 3, S~ does not
~0
violate the blocks of O~ N , in particular A~ 1 and A~ 2 : S~ = X~ kA~ 1kY~ kA~ 2 kZ~ . Let S~ 0 = S~ jOO~ NN =
X~ kA~ 2 kY~ kA~ 1 kZ~ . We must show that S~ 0 is minimal, which implies blockwise equivalence of
O~ N and O~ N0 .
If Y~ is empty, then Cost(S~ ) = Cost(S~ 0 ) by Lemma 2 (A~ 1 and A~ 2 are adjacent, mutually
independent and cn-equal; thus, their transposition does not change the cost).
If Y~ is not empty, then by Corollary 2 Y~ is mutually independent of both A~ 1 and A~ 2
(S~ is binder-consistent with O~ N , therefore B(N )  X~ , and consequently Y~ \ B(N ) = ;).
Y~ can be divided into several blocks, each one of them cn-equal to A~ 1 and A~ 2: since S~
is minimal, O~ N cannot be MC-contradicting, and the claim follows from Lemma 10. By
Lemma 9, cn(Y~ )jX~ = cn(A~ 1 )jX~ = cn(A~ 2)jX~ . By Lemma 2:

Cost(S~ ) = Cost(X~ kA~ 1kY~ kA~ 2kZ~ )
= Cost(X~ kA~ 1kA~ 2 kY~ kZ~ )
= Cost(X~ kA~ 2kA~ 1 kY~ kZ~ )
= Cost(X~ kA~ 2kY~ kA~ 1kZ~ )

=
=
=
=

== swap(Y; A2)
== swap(A1; A2 )
== swap(A1; Y )
Cost(S~ 0 )

Minimality of S~ 0 implies blockwise equivalence of O~ N and O~ N0 .

2

Appendix B. Correctness of the dac Algorithm

In this section we show that the dac algorithm is correct, i.e., given a set of subgoals S0,
it returns its minimal ordering. It suces to show that the candidate set of the root node
of DTree(S0; ;) is valid. In such a case, as follows from the definition of valid sets, it must
contain a minimal ordering. The algorithm returns one of the cheapest candidates of the
root. Therefore, if the candidate set of the root is valid, the dac algorithm must return a
minimal ordering of S0.
We start by defining strong validity of sets of orderings. We then prove that strong
validity implies validity. Finally, we use induction to prove a theorem, showing that the
candidate set produced for each node in the divisibility tree is strongly valid.
Definition: Let S0 be a set of subgoals, N be a node in the divisibility tree of S0. The set
CN  (S (N )) is strongly valid, if every ordering in (S (N )) nCN is either MC-contradicting
or blockwise-equivalent to some member of CN , unless no ordering of S (N ) is min-consistent.

StronglyV alidN;S (CN ) ()
[9O~ N0 2  (S (N )) : MCN;S (O~ N0 )] ! [O~ N 2  (S (N )) n CN ! MCCN;S (O~ N )_
(9O~ N00 2 CN ^ MCEN;S (O~ N ; O~ N00 ))]
0

0

0

0

Lemma 11 A strongly valid set of orderings is valid.
90

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Proof: Let S0 be a set of subgoals, N be a node in the divisibility tree of S0, C (N ) be a
strongly valid set of orderings of N .
If there is no min-consistent ordering of N , then C (N ) is valid, by the definition of a
valid set (Section 4.2).
Otherwise, there exists at least one minimal ordering of S0, binder-consistent with N .
Every ordering in  (S (N )) n C (N ) is either MC-contradicting or blockwise-equivalent to
some member of C (N ). To prove that C (N ) is valid, we must show that it contains an
ordering O~ N , which is binder-consistent with some minimal ordering S~ of S0 .
Let S~ 0 be a minimal ordering of S0, binder-consistent with N . Let O~ N0 be the projection
of S~ 0 on N . If O~ N0 2 C (N ), we are done (O~ N = O~ N0 , S~ = S~ 0). Otherwise, O~ N0 2  (S (N )) n
C (N ). O~ N0 cannot be MC-contradicting (it is min-consistent to S~ 0), therefore it must be
blockwise-equivalent to some O~ N00 2 C (N ). Blocks of O~ N0 are not violated in S~ 0, since S~ 0 is
~ 00
minimal (Corollary 3). Therefore the substitution S~ 00 = S~ 0jOO~ N0 is well defined. S~ 00 is minimal,
N
since S~ 0 is minimal and O~ N0 and O~ N00 are blockwise-equivalent. S~ 00 is binder-consistent with
O~ N00 , since S~ 0 was binder-consistent with O~ N0 . Thereupon S~ 00 and O~ N00 satisfy the requirements
of validity (O~ N = O~ N00 , S~ = S~ 00).
2
Theorem 3
Let S0 be a set of subgoals. For each node N of the divisibility tree of S0, Algorithm 6
creates a strongly valid candidate set of orderings.

Proof: By induction on the height of N 's subtree.
Inductive base: N is a leaf node, which means that S (N ) is independent under B(N ).

The candidate set of N contains one element, whose subgoals are sorted by cn. All
orderings that belong to  (S (N )) n CandSet(N ) are either not sorted by cn, and
hence are MC-contradicting (Lemma 4), or are sorted by cn, and hence are blockwiseequivalent to the candidate (Corollary 4). Consequently, CandSet(N ) is strongly
valid.
Inductive hypothesis: For all children of N , Algorithm 6 produces strongly valid candidate sets.
Inductive step: An internal node in a divisibility tree is either an AND-node or an ORnode.
1. N is an AND-node. Let N1; N2; : : :Nk be the children of N . First we show that
ConsSet(N ) is strongly valid.
Let O~ N 2  (S (N )) n ConsSet(N ). For all 1  i  k, let O~ i be the projection of
O~ N on Ni. The set of projections fO~ 1; O~ 2; : : : O~ k g can belong to one of the three
following types, with regard to O~ N .
(a) The sets of the first type contain at least one MC-contradicting projection. In
such a case O~ N is MC-contradicting too. Assume the contrary: there exists
a minimal ordering S~ of S0, binder-consistent with O~ N . Let O~ i be an MCcontradicting projection. Since O~ i is consistent with O~ N , it is also consistent
91

fiLedeniov & Markovitch

with S~ . Since B(Ni ) = B(N ), all subgoals of B(Ni) appear in S~ before
subgoals of S (Ni ). Therefore, O~ i is binder-consistent with S~ , and since S~ is
minimal, O~ i is min-consistent and not MC-contradicting { a contradiction.
(b) The sets of the second type do not contain MC-contradicting projections, but
in O~ N some block of some projection is violated, or max-blocks from different
projections are not ordered by cn. In such a case, O~ N is MC-contradicting,
by Corollary 3 and Lemma 6.
(c) The sets of the third type do not contain MC-contradicting projections, and
max-blocks of the projections are not violated in O~ N and are sorted by cn.
Every projection O~ i either belongs to CandSet(Ni), or not. If O~ i 62 CandSet(Ni),
then there exists O~ i0 2 CandSet(Ni) such that O~ i is blockwise-equivalent to
O~ i0 (because CandSet(Ni) is strongly valid by the inductive hypothesis, and
O~ i is not MC-contradicting). If O~ i 2 CandSet(Ni), we can set O~ i0 = O~ i.
~0
~0 ~0
Let O~ N0 = O~ N jOO~ jOO~ : : : jOO~ kk . This substitution is well defined, since each O~ i
has the same number of max-blocks as O~ i0 , and max-blocks of the projections
are not violated in O~ N . Let S~ be a minimal ordering of S0, binder-consistent
with O~ N . Since S~ is minimal, blocks of O~ 1 are not violated in S~ . Since O~ 1
~0
is blockwise-equivalent to O~ 10 , the ordering S~1 = S~ jOO~ is well-defined and
minimal. In S~1 the positions of the subgoals from B(N ) did not change;
thus, O~ 2 is min-consistent with S~1, and blockwise equivalence of O~ 2 and O~ 20
~0
~0 ~0
entails minimality of the ordering S~2 = S~1 jOO~ = S~ jOO~ jOO~ . We continue with
~0
~0 ~0
other O~ i -s, and finally obtain that S~ 0 = S~ jOO~ jOO~ : : : jOO~ kk is minimal. From the
~0
definition of O~ N0 , S~ 0 = S~ jOO~ NN (note that we introduced blockwise equivalence
and strong validity only to be able to perform this transition). S~ 0 is minimal,
therefore O~ N is blockwise-equivalent to O~ N0 . O~ N0 2 ConsSet(N ), since all its
projections are candidates of the child nodes. Thereupon, O~ N is blockwiseequivalent to a member of ConsSet(N ).
So, ConsSet(N ) is strongly valid. To prove that CandSet(N ) is strongly valid,
it suces to show that all the members of ConsSet(N ) that are not included in
CandSet(N ) by Algorithm 6, are either MC-contradicting or blockwise-equivalent
to members of CandSet(N ). Such orderings can be of three types:
(a) Orderings that violate blocks of the children projections. They are MCcontradicting by Corollary 3.
(b) Orderings that do not violate blocks, but where max-blocks of children projections are not ordered by cn. They are MC-contradicting by Lemma 6.
(c) Orderings that do not violate blocks and have them sorted by cn. For each
combination of projections, one consistent ordering of N is retained in the
candidate set, and all the other are rejected. By Corollary 5, the rejected
orderings are blockwise-equivalent to the retained candidate.
Consequently, CandSet(N ) is strongly valid.
1

2

1

2

1
1

92

2

1

2

2

1

2

1

2

1

2

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

2. N is an OR-node. Again, we start with showing that ConsSet(N ) is strongly
valid.
Let O~ N 2  (S (N )) n ConsSet(N ). O~ N is constructed from a binder H and a
\tail" sequence T~ : O~ N = H kT~ . Let NH be the child of N that corresponds
to the binder H . By the inductive hypothesis, CandSet(NH ) is strongly valid.
T~ 62 CandSet(NH ), since otherwise O~ N 2 ConsSet(N ). Therefore, T~ is either MC-contradicting, or blockwise-equivalent to some T~ 0 2 CandSet(NH ).
If T~ is MC-contradicting, O~ N is MC-contradicting too (proof by contradiction, as for AND-nodes). If T~ is blockwise-equivalent to T~ 0 , then O~ N = H kT~
is blockwise-equivalent to H kT~ 0 2 ConsSet(N ) (the proof is easy). Hence,
ConsSet(N ) is strongly valid. The only orderings of ConsSet(N ) that are not included in CandSet(N ) by the dac algorithm have cheaper permutations of their
leading max-blocks, and therefore are MC-contradicting, by Lemma 7. Hence,
CandSet(N ) is strongly valid.
2

Corollary 7 The candidate set found by Algorithm 6 for the root node is valid.
Corollary 8 Algorithm 6 finds a minimal ordering of the given set of subgoals.

References

Aho, A. V., Hopcroft, J. E., & Ullman, J. D. (1987). Data Structures and Algorithms.
Addison-Wesley.
Boddy, M., & Dean, T. (1989). Solving time-dependent planning problems. In Sridharan, N. S. (Ed.), Proceedings of the 11th International Joint Conference on Artificial
Intelligence, pp. 979{984, Detroit, MI, USA. Morgan Kaufmann.
Bol, R. N., Apt, K. R., & Klop, J. W. (1991). An analysis of loop checking mechanisms for
logic programs. Theoretical Computer Science, 86 (1), 35{79.
Braem, C., Le Charlier, B., Modar, S., & Van Hentenryck, P. (1994). Cardinality Analysis
of Prolog. In Bruynooghe, M. (Ed.), Logic Programming - Proceedings of the 1994
International Symposium, pp. 457{471, Massachusetts Institute of Technology. The
MIT Press.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and
Regression Trees. Wadsworth International Group, Belmont, CA.
Bruynooghe, M., De Schreye, D., & Krekels, B. (1989). Compiling control. The Journal of
Logic Programming, 6, 135{162.
Clark, K. L., & McCabe, F. (1979). The control facilities of IC-Prolog. In Michie, D. (Ed.),
Expert Systems in The Microelectronic Age., pp. 122{149. University of Edinburgh,
Scotland.
Clocksin, W. F., & Mellish, C. S. (1987). Programming in Prolog (Third edition). SpringerVerlag, New York.
93

fiLedeniov & Markovitch

Cohen, W. W. (1990). Learning approximate control rules of high utility. In Proceedings of
the Seventh International Machine Learning Workshop, pp. 268{276, Austin, Texas.
Morgan Kaufmann.
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1991). Introduction To Algorithms. MIT
Press, Cambridge, Mass.
Cortesi, A., Le Charlier, B., & Rossi, S. (1997). Specification-based automatic verification
of Prolog programs. In Gallagher, J. (Ed.), Proceedings of the 6th International Workshop on Logic Program Synthesis and Transformation, Vol. 1207 of LNCS, pp. 38{57,
Stockholm, Sweden. Springer-Verlag.
De Boeck, P., & Le Charlier, B. (1990). Static type analysis of Prolog procedures for ensuring
correctness. In Deransart, P., & Maluszynski, J. (Eds.), Programming Languages
Implementation and Logic Programming, Vol. 456 of LNCS, pp. 222{237, Linkoping,
Sweden. Springer-Verlag.
De Schreye, D., & Decorte, S. (1994). Termination of logic programs: The never-ending
story. The Journal of Logic Programming, 19 & 20, 199{260.
Debray, S., Lopez-Garca, P., Hermenegildo, M., & Lin, N.-W. (1997). Lower bound cost
estimation for logic programs. In Maluszynski, J. (Ed.), Proceedings of the International Symposium on Logic Programming (ILPS-97), pp. 291{306, Cambridge. MIT
Press.
Debray, S. K., & Lin, N.-W. (1993). Cost analysis of logic programs. ACM Transactions
on Programming Languages and Systems, 15 (5), 826{875.
Debray, S. K., & Warren, D. S. (1988). Automatic mode inference for logic programs. The
Journal of Logic Programming, 5, 207{229.
Dejong, G., & Mooney, R. (1986). Explanation-based learning: An alternative view. Machine Learning, 1, 145{176.
Deville, Y. (1990). Logic Programming: Systematic Program Development. International
Series in Logic Programming, Addison-Wesley.
Etzioni, O. (1991). STATIC: A problem-space compiler for PRODIGY. In Dean, Thomas
L.; McKeown, K. (Ed.), Proceedings of the 9th National Conference on Artificial
Intelligence, pp. 533{540, Anaheim, California. MIT Press.
Etzioni, O. (1993). Acquiring search-control knowledge via static analysis. Artificial Intelligence, 62, 255{301.
Greiner, R., & Orponen, P. (1996). Probably approximately optimal satisficing strategies.
Artificial Intelligence, 82 (1-2), 21{44.
Henrard, J., & Le Charlier, B. (1992). FOLON: An environment for declarative construction
of logic programs. In Bruynooghe, M., & Wirsing, M. (Eds.), Proceedings of the Fourth
International Symposium on Programming Language Implementation and Logic Programming, Vol. 631 of LNCS, pp. 217{231, Leuven, Belgium. Springer-Verlag.
94

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Itai, A., & Makowsky, J. A. (1987). Unification as a complexity measure for logic programming. The Journal of Logic Programming, 4, 105{117.
Knuth, D. E. (1973). The Art Of Computer Programming, Vol. 3. Addison-Wesley, Reading,
Mass.
Kowalski, R. A. (1979). Algorithm = Logic + Control. Communications of the ACM, 22(7),
424{436.
Laird, P. D. (1992). Ecient dynamic optimization of logic programs. In Proceedings of
the ML92 Workshop on Knowledge Compilation and Speedup Learning Aberdeen,
Scotland.
Langley, P. (1985). Learning to search: From weak methods to domain-specific heuristics.
Cognitive Science, 9, 217{260.
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming: Techniques and Applications. Artificial Intelligence. Ellis Harwood, New York.
Ledeniov, O., & Markovitch, S. (1998a). Controlled utilization of control knowledge for
speeding up logic inference. Tech. rep. CIS9812, Technion, Haifa, Israel.
Ledeniov, O., & Markovitch, S. (1998b). Learning investment functions for controlling the
utility of control knowledge. In Proceedings of the Fifteenth National Conference on
Artificial Intelligence, pp. 463{468, Madison, Wisconsin. Morgan Kaufmann.
Lenat, D. B. (1995). CYC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38 (11), 33{38.
Lloyd, J. W. (1987). Foundations of Logic Programming (Second edition). Springer-Verlag,
Berlin.
Markovitch, S., & Scott, P. D. (1989). Automatic ordering of subgoals | a machine learning
approach. In Lusk, E. L., & Overbeek, R. A. (Eds.), Proceedings of the North American
Conference on Logic Programming, pp. 224{242, Cleveland, Ohio. MIT Press.
Markovitch, S. (1989). Information Filtering: Selection Mechanisms in Learning Systems.
Ph.D. thesis, EECS Department, University of Michigan.
Markovitch, S., & Scott, P. D. (1993). Information filtering: Selection mechanisms in
learning systems. Machine Learning, 10, 113{151.
Minker, J. (1978). Search strategy and selection function for an inferential relational system.
In ACM Transactions on Database Systems, Vol. 3, pp. 1{31.
Minton, S. (1988). Learning Search Control Knowledge: An Explanation-Based Approach.
Kluwer, Boston, MA.
Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). Explanation-based generalization: A unifying view. Machine Learning, 1, 47{80.
95

fiLedeniov & Markovitch

Mooney, R. J., & Zelle, J. M. (1993). Combining FOIL and EBG to speed-up logic programs.
In Bajcsy, R. (Ed.), Proceedings of The Thirteenth International Joint Conference for
Artificial Intelligence, pp. 1106{1111, Chambery, France. Morgan Kaufmann.
Morris, K. A. (1988). An algorithm for ordering subgoals in NAIL!. In Proceedings of the
Seventh ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, pp.
82{88, Austin, TX. ACM Press, New York.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods.
The Journal of Logic Programming, 19 & 20, 629{680.
Naish, L. (1984). MU-Prolog 3.1db Reference Manual. Dept. of Computer Science, Univ.
of Melbourne.
Naish, L. (1985a). Automatic control for logic programs. The Journal of Logic Programming,
3, 167{183.
Naish, L. (1985b). Prolog control rules. In Joshi, A. (Ed.), Proceedings of the 9th International Joint Conference on Artificial Intelligence, pp. 720{723, Los Angeles, CA.
Morgan Kaufmann.
Natarajan, K. S. (1987). Optimizing backtrack search for all solutions to conjunctive problems. In McDermott, J. (Ed.), Proceedings of the 10th International Joint Conference
on Artificial Intelligence, pp. 955{958, Milan, Italy. Morgan Kaufmann.
Nie, X., & Plaisted, D. A. (1990). Experimental results on subgoal ordering. In IEEE
Transactions On Computers, Vol. 39, pp. 845{848.
Pettorossi, A., & Proietti, M. (1994). Transformation of logic programs: Foundations and
techniques. The Journal of Logic Programming, 19 & 20, 261{320.
Pettorossi, A., & Proietti, M. (1996). Rules and strategies for transforming functional and
logic programs. ACM Computing Surveys, 28 (2), 360{414.
Porto, A. (1984). Epilog: A language for extended programming. In Campbell, J. (Ed.),
Implementations of Prolog. Ellis Harwood.
Prieditis, A. E., & Mostow, J. (1987). PROLEARN: Towards a prolog interpreter that
learns. In Forbus, Kenneth; Shrobe, H. (Ed.), Proceedings of the 6th National Conference on Artificial Intelligence, pp. 494{498, Seattle, WA. Morgan Kaufmann.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81{106.
Quinlan, J. R., & Cameron-Jones, R. M. (1995). Induction of logic programs: FOIL and
related systems. New Generation Computing, Special Issue on Inductive Logic Programming, 13 (3-4), 287{312.
Simon, H. A., & Kadane, J. B. (1975). Optimal problem-solving search: All-or-none solutions. Artificial Intelligence, 6, 235{247.
Smith, D. E. (1989). Controlling backward inference. Artificial Intelligence, 39 (1), 145{208.
96

fiThe Divide-and-Conquer Subgoal-Ordering Algorithm

Smith, D. E., & Genesereth, M. R. (1985). Ordering conjunctive queries. Artificial Intelligence, 26, 171{215.
Smith, D. E., Genesereth, M. R., & Ginsberg, M. L. (1986). Controlling recursive inference.
Artificial Intelligence, 30 (3), 343{389.
Somogyi, Z., Henderson, F., Conway, T., Bromage, A., Dowd, T., Jeffery, D., & al. (1996a).
Status of the Mercury system. In Proc. of the JICSLP '96 Workshop on Parallelism
and Implementation Technology for (Constraint) Logic Programming Languages, pp.
207{218, Bonn, Germany.
Somogyi, Z., Henderson, F., & Conway, T. (1996b). The execution algorithm of Mercury,
an ecient purely declarative logic programming language. Journal of Logic Programming, 29 (1{3), 17{64.
Sterling, L., & Shapiro, E. (1994). The Art of Prolog (Second edition). MIT Press, Cambridge, MA.
Tamaki, H., & Sato, T. (1984). Unfold/fold transformation of logic programs. In Tarnlund,
S.-
A. (Ed.), Proceedings of the Second International Conference on Logic Programming, pp. 127{138, Uppsala, Sweden.
Ullman, J. D., & Vardi, M. Y. (1988). The complexity of ordering subgoals. In Proceedings of
the Seventh ACM SIGACT-SIGMOD Symposium on Principles of Database Systems,
pp. 74{81, Austin, TX. ACM Press, New York.
Ullman, J. D. (1982). Principles of Database Systems. Computer Science Press, Rockville,
MD.
Vasak, T., & Potter, J. (1985). Metalogical control for logic programs. Journal of Logic
Programming, 2 (3), 203{220.
Warren, D. H. D. (1981). Ecient processing of interactive relational database queries
expressed in logic. In Zaniola, & Delobel (Eds.), Proceedings of the 7th International
Conference on Very Large Data Bases, pp. 272{281, Cannes, France. IEEE Computer
Society Press.

97

fi