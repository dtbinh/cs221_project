Journal of Artificial Intelligence Research 9 (1998) 167-217

Submitted 6/98; published 10/98

Probabilistic Inference from Arbitrary Uncertainty
using Mixtures of Factorized Generalized Gaussians
Alberto Ruiz
Pedro E. Lpez-de-Teruel
M. Carmen Garrido
Universidad de Murcia, Facultad de Informtica,
Campus de Espinardo, 30100, Murcia, Spain

ARUIZ@DIF.UM.ES
PEDROE@DITEC.UM.ES
MGARRIDO@DIF.UM.ES

Abstract
This paper presents a general and efficient framework for probabilistic inference and learning from
arbitrary uncertain information. It exploits the calculation properties of finite mixture models, conjugate families and factorization. Both the joint probability density of the variables and the likelihood
function of the (objective or subjective) observation are approximated by a special mixture model, in
such a way that any desired conditional distribution can be directly obtained without numerical integration. We have developed an extended version of the expectation maximization (EM) algorithm to
estimate the parameters of mixture models from uncertain training examples (indirect observations). As
a consequence, any piece of exact or uncertain information about both input and output values is consistently handled in the inference and learning stages. This ability, extremely useful in certain situations, is not found in most alternative methods. The proposed framework is formally justified from
standard probabilistic principles and illustrative examples are provided in the fields of nonparametric
pattern classification, nonlinear regression and pattern completion. Finally, experiments on a real application and comparative results over standard databases provide empirical evidence of the utility of the
method in a wide range of applications.

1. Introduction
The estimation of unknown magnitudes from available information, in the form of sensor measurements or subjective judgments, is a central problem in many fields of science and engineering.
To solve this task, the domain must be accurately described by a model able to support the desired
range of inferences. When satisfactory models cannot be derived from first principles, approximations must be obtained from empirical data in a learning stage.
Consider a domain Z composed by a collection of objects z =(z1, z2, ..., zn), represented by
vectors of n attributes. Given some partial knowledge S (expressed in a general form explained
later) about a certain object z, we are interested in computing a good estimate z ( S ) , close to the
true z. We allow heterogeneous descriptions; any attribute zi may be continuous, discrete, or symbolic valued, including mixed types. If there is a specific subset of unknown or uncertain attributes
to be estimated, the attribute vector can be partitioned as z = (x, y), where y  z denotes the target
or output attributes. The target attributes can be different for different objects z. This scenario includes several usual inference paradigms. For instance, when there is a specific target symbolic
attribute, the task is called pattern recognition or classification; when the target attribute is continuous, the inference task is called regression or function approximation. In general, we are interested in a general framework for pattern completion from partially known objects.
Example 1: To illustrate this setting, assume that the preprocessor of a hypothetical computer
vision system obtains features of a segmented object. The instances of the domain are described

1998 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

by the following n=7 attributes: AREA: z1  , COLOR: z2  {white, black, red, ...}, DISTANCE:
z3  , SHAPE: z4  {circular, rectangular, triangular, ...}, TEXTURE: z5  {soft, rough, ...},
OBJECTTYPE: z6  {door, window, ...} and ANGLE: z7  . A typical instance may be z = (78,
blue, 3.4, triangular, soft, window, 45). If the object is partially occluded or 3-dimensional,
some attributes will be missing or uncertain. For instance, the available information S about z
could be expressed as (748, blue OR black, 3.4, triangular, ?, window 70% door 30%, ?),
where z1, z2, z6 are uncertain, z3, z4 are exact and z5, z7 are missing. In this case we could be interested in estimates for y = {z5, z6, z7} and even in improving our knowledge on z1 and z2.

The non-deterministic nature of many real world domains suggests a probabilistic approach,
where attributes are considered as random variables. Objects are assumed to be drawn independently and identically distributed from p(z) = p(z1, ..., zn) = p(x, y), the multivariate joint probability
density function of the attributes, which completely characterizes the n-dimensional random variable z. To simplify notation, we will use the same function symbol p() to denote different p.d.f.s if
they can be identified without risk of confusion.
According to Statistical Decision Theory (Berger 1985), optimum estimators for the desired attributes are obtained through minimization of a suitable expected loss function:

y OPT ( S ) = argmin y E{ L( y , y )| S}


where L(y, y ) is the loss incurred when the true y is estimated by y . Estimators are always features of the conditional or posterior distribution p(y|S) of the target variables given the available
information. For instance, the minimum squared error (MSE) estimator is the posterior mean, the
minimum linear loss estimator is the posterior median and the minimum error probability (EP, 0-1
loss) estimator is the posterior mode.
Example 2: A typical problem is the prediction of an unknown attribute y from the observed attributes x. In this case the available information can be written as S = (x, ?). If y is continuous, it
is reasonable to use the MSE estimator: y MSE ( S ) = E{ y | x} , the general regression function. If y
is symbolic and the same loss is associated to all errors, the EP estimator is adequate:
y EP ( S ) = argmaxy p(y|x) = argmaxy p(x|y)p(y). It corresponds to the Maximum A Posteriori rule
or Bayes Test, widely used in Statistical Pattern Recognition.

The joint density p(z) = p(x, y) plays an essential role in the inference process. It implicitly
includes complete information about attribute dependences. In principle, any desired conditional
distribution or estimator can be computed from the joint density by adequate integration. Probabilistic Inference is the process of computing the desired conditional probabilities from a (possibly
implicit) joint distribution. From p(z) (the prior, model of the domain, comprising implications) and
S (a known event, somewhat related to a certain z), we could obtain the posterior p(z|S) and the
desired target marginal p(y|S) (the probabilistic consequent).
Example 3: If we observe an exact value xo in attribute x, i.e. S = { x = xo}, we have:

p( y| S )  p(y| xo) =



Y

p ( xo , y )
p( xo , y )dy

If we know that instance z is in a certain region R in the attribute space, i.e. S = {z  R}, we
compute the marginal density of y from the joint p(z) = p(x, y) restricted to region R (Fig. 1):

168

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

p( y| S ) =



X

p( x, y|{( x , y )  R}) dx =

 p( x, y) dx
 p( x, y) dxdy
R

R

More general types of uncertain information S about z will be discussed later.

y
p(y|R)
R

p(x,y)
x

Figure 1. The conditional probability density of y, assuming z = (x,y)  R.

In summary, from the joint density p(z) of a multivariate random variable, any subset of variables y  z may be, in principle, estimated given the available information S about the whole z = (x,
y). In practical situations, two steps are required to solve the inference problem. First, a good model
of the true joint density p(z) must be obtained. Second, the available information S must be efficiently processed to improve our knowledge about future, partially specified instances z. These two
complementary aspects, learning and inference, are approached from many scientific fields, providing different methodologies to solve practical applications.
From the point of view of Computer Science, the essential goal of Inductive Inference is to
find an approximate intensional definition (properties) of an unknown concept (subset of the domain) from an incomplete extensional definition (finite sample). Machine Learning techniques
(Michalski, Carbonell & Mitchell 1977, 1983, Hutchinson 1994) provide practical solutions (e.g.
automatic construction of decision trees) to solve many situations where explicit programming
must be avoided. Computational Learning Theory (Valiant 1993, Wolpert 1994, Vapnik 1995)
studies the feasibility of induction in terms of generalization ability and resource requirements of
different learning paradigms.
Under the general setting of Statistical Decision Theory, modeling techniques and the operational aspects of inference (based in numerical integration, Monte Carlo simulation, analytic approximations, etc.) are extensively studied from the Bayesian perspective (Berger 1985, Bernardo
& Smith 1994). In the more specific field of Statistical Pattern Recognition (Duda & Hart 1973,
Fukunaga 1990), standard parametric or nonparametric density approximation techniques (Izenman
1991) are used to learn from training data the class-conditional p.d.f.s required by the optimum
decision rule. For instance, if the class-conditional densities p(x|y) are Gaussian, the required parameters are the mean vector and covariance matrix of the feature vector in each class and the decision regions for y in x have quadratic boundaries. Among the nonparametric classification techniques, the Parzen method and the K-N Nearest Neighbors rule must be mentioned. Analogously, if
the target attribute is continuous and the statistical dependence between input and output variables
p(x,y) can be properly modeled by joint normality, we get multivariate linear regression: y MSE(x) =
A x + B, where the required parameters are the mean values and the covariance matrix of the attrib-

169

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

utes. Nonlinear regression curves can be also derived from nonparametric approximation techniques. Nonparametric methods present slower convergence rates, requiring significantly larger
sample sizes to obtain satisfactory approximations; they are also strongly affected by the dimensionality of the data and the selection of the smoothing parameter is a crucial step. In contrast, they
only require some kind of smoothness assumption on the target density.
Neural Networks (Hertz et. al 1991) are computational models trainable from empirical data
that have been proposed to solve more complex situations. Their intrinsic parallel architecture is
especially efficient in the inference stage. One of the most widely used neural models is the Multilayer Perceptron, a universal function approximator (Hornik et al. 1989) that breaks the limitations
of linear decision functions. The Backpropagation learning algorithm (Rumelhart et al. 1986) can,
in principle, adjust the network weights to implement arbitrary mappings, and the network outputs
show desirable probabilistic properties (Wan 1990, Rojas 1996). There are also unsupervised networks for probability density function approximation (Kohonen 1989). However, neural models
usually contain a large number of adjustable parameters, which is not convenient for generalization
and, frequently, long times are required for training in relatively easy tasks. The input / output role
of attributes cannot be changed in runtime and missing and uncertain values are poorly supported.
Bayesian Networks, based in the concept of conditional independence, are among the most
relevant probabilistic inference technologies (Pearl 1988, Heckerman & Wellman 1995). The joint
density of the variables is modeled by a directed graph which explicitly represents dependence
statements. A wide range of inferences can be performed under this framework (Chang & Fung
1995, Lauritzen & Spiegelhalter 1988) and there are significant results on inductive learning of
network structures (Bouckaert 1994, Cooper & Herskovits 1992, Valiveti & Oomen 1992). This
approach is adequate when there is a large number of variables showing explicit dependences and
simple cause-effect relations. Nevertheless, solving arbitrary queries is NP-Complete, automatic
learning algorithms are time consuming and the allowed dependences between variables are relatively simple.
In an attempt to mitigate some of the above drawbacks, we have developed a general and efficient inference and learning framework based on the following considerations. It is well known
(Titterington et al. 1985, McLachlan & Basford 1988, Dalal & Hall 1983, Bernardo & Smith 1994,
Xu & Jordan 1996) that any reasonable probability density function p(z) can be approximated up to
the desired degree of accuracy by a finite mixture of simple components Ci, i = 1..l:

p( z )   P{Ci } p( z| Ci )

(1)

i

The superposition of simple densities is extensively used to approximate arbitrary data dependences (Fig. 2). Maximum Likelihood estimators of the mixture parameters can be efficiently obtained from samples by the Expectation Maximization (EM) algorithm (Dempster, Laird & Rubin
1977, Redner & Walker 1984) (see Section 4).

170

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

(a)
(b)
(c)
Figure 2. Illustrative example of density approximation using a mixture model. (a) Samples from a p.d.f. p(x,y) showing a nonlinear dependence. (b) Mixture model for p(x,y)
with 6 gaussian components obtained by the standard EM algorithm. (c) Location of
components.

The decomposition of probability distributions using mixtures has been frequently applied to
unsupervised learning tasks, especially Cluster Analysis (McLachlan & Basford 1988, Duda &
Hart 1973, Fukunaga 1990): the a posteriori probabilities of each postulated category are computed
for all the examples, which are labeled according to the most probable source density. However,
mixture models are specially useful in nonparametric supervised learning situations. For instance,
the class conditional densities required in Statistical Pattern Recognition were individually approximated in (Priebe & Marchette 1991, Traven 1991) by finite mixtures; hierarchical mixtures of
linear models were proposed in (Jordan & Jacobs 1994, Peng et. al 1995); mixtures of factor analyzers have been developed in (Ghahramani & Hinton 1996, Hinton, Dayan, & Revow 1997) and
mixture models have been also useful for feature selection (Pudil et al. 1995). Mixture modeling is
a growing semiparametric probabilistic learning methodology with applications in many research
areas (Weiss & Adelson 1995, Fan et al. 1996, Moghaddam & Pentland 1997).
This paper introduces a framework for probabilistic inference and learning from arbitrary uncertain data: any piece of exact or uncertain information about both input and output values is consistently handled in the inference and learning stages. We approximate both the joint density p(z)
(model of the domain) and the relative likelihood function p(S|z) (describing the available information) by a specific mixture model with factorized conjugate components, in such a way that numerical integration is avoided in the computation of any desired estimator, marginal or conditional
density.
The advantages of modeling arbitrary densities using mixtures of natural conjugate components were already shown in (Dalal & Hall 1983), and, recently, inference procedures based in a
similar idea have been proposed in (Ghahramani & Jordan 1994, Cohn et al. 1996, Peng et al. 1995,
Palm 1994). However, our method efficiently handles uncertain data using explicit likelihood
functions, which has not been extensively used before in Machine Learning, Pattern Recognition or
related areas. We will follow standard probabilistic principles, providing natural statistical validation procedures.
The organization of the paper is as follows. Section 2 reviews some elementary results and
concepts used in the proposed framework. Section 3 addresses the inference stage. Section 4 is
concerned with learning, extending the EM algorithm to manage uncertain information. Section 5
discusses the method in relation to alternative techniques and presents experimental evaluation.
The last section summarizes the conclusions and future directions of this work.

171

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

2. Preliminaries
2.1 A Calculus of Generalized Normals
In many applications, the instances of the domain are represented simultaneously by continuous
and symbolic or discrete variables (as in Wilson & Martinez 1997). To simplify notation, we will
denote both probability impulses and Gaussian densities by means of a common formalism. The
generalized normal
(x,,) denotes a probability density function with the following properties:

T

T(x,,) 

If  > 0,

  ( x  ) 2 
1
exp

2  22 

T(x,,) = T(x,,0)  T(x,)  (x)

If  = 0,

Tzero,(x,,)
is a Gaussian density with mean  and standard deviation   0. When the dispersion is
T reduces
to a Diracs delta function located at . In both cases T is a proper p.d.f.:
T(x,,) > 0
 T(x,,) dx = 1
X

The product of generalized normals can be elegantly expressed (Papoulis 1991 pp. 258, Berger
1985) by:
for 1+2 >0:

T(x,1,1)  T(x,2,2) = T(x,,)  T(1,2, 12 + 22 )

(2)

where the mean  and dispersion  of the new normal are given by:

122 + 22 1
=
12 + 22

1222
 = 2
1 + 22
2

This relation is useful for computing the integral of the product of two generalized normals:
for 1+2 >0:



X

T(x,1,1)  T(x,2,2) dx = T(1, 2, 12 + 22 )

(3)

And, for consistency, we define



for 1 = 2 = 0:

X

T(x,1) T(x,2) dx = T(1,2)  I{1=2}

where I{predicate} = 1 if predicate is true and zero otherwise. Virtually any reasonable univariate
probability distribution or likelihood function can be accurately modeled by an appropriate mixture
of generalized normals. In particular, p.d.f.s over symbolic variables are mixtures of impulses.
Without loss of generality, symbols may be arbitrarily mapped to specific numbers and represented
over numeric axes. Integrals over discrete domains become sums.
Example 4: Let us approximate the p.d.f. p(x) of a mixed continuous and symbolic valued random variable x by a mixture of generalized normals. Assume that x takes with probability 0.4
the exact value 10 (with a special meaning), and with probability 0.6 a random value continuously distributed following the triangular shape shown in Fig. 3. The density p(x) can be accurately approximated (see Section 4) using 4 generalized normals:

T(x,10) + .21T(x,.04,.23) + .28T(x,.45,.28) + .11T(x,.99,.21)

p(x)  .40

172

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

Figure 3. The p.d.f. of a mixed random variable approximated by a mixture of generalized normals.

2.2 Modeling Uncertainty: The Likelihood Principle
Assume that the value of a random variable z must be inferred from a certain observation or subjective information S. If z has been drawn from p(z) and the measurement or judgment process is
characterized by a conditional p(S|z), our knowledge on z is updated according to p(z|S)=p(z) p(S|z)
/ p(S), where p(S) = Z p(S|z) p(z) dz (see Fig. 4).
The likelihood function fS(z)  p(S|z) is the probability density ascribed to S by each possible
z. It is an arbitrary nonnegative function over z that can be interpreted in two alternative ways. It
can be the objective conditional distribution p(S|z) of a physical measurement process (e.g. a
model of sensor noise, specifying bias and variance of the observable S for every possible true
value z), also known as error model. It can also be a subjective judgment about the chance of the
different z values (e.g. intervals, more likely regions, etc.), based on vague or difficult to formalize
information. The dispersion of fS(z) is directly related to the uncertainty associated to the measurement process. Following the likelihood principle (Berger 1985), we explicitly assume that all the
experimental information required to perform probabilistic inference is contained in the likelihood
function fS(z).

p(z)
z1

z2

z3

z

p(s|z2)

p(s|z1)

p(s|z3)
s

so

prior
model of
measurement

p(s)
s
fSo(z)= p(so|z)

z

observable

likelihood of
observation so

p(z|so)
posterior
z
Figure 4. Illustration of the elementary Bayesian univariate inference process.

173

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

2.3 Inference Using Mixtures of Conjugate Densities
The computation of p(z|S) may be hard, unless p(z) and p(S|z) belong to special (conjugate) families (Berger 1985, Bernardo & Smith 1994). In this case the posterior density can be analytically
obtained from the parameters of the prior and the likelihood, avoiding numeric integration. The
prior, the likelihood and the posterior are in the same mathematical family. The belief structure is
closed under the inference process.

T

Example 5: In the univariate case, assume that z is known to be normally distributed around r
with dispersion r, i.e. p(z) = (z, r, r). Assume also that our measurement device has Gaussian noise, so the observed values are distributed according to p(s|z) =
(s, z, s). Therefore,
if we observe a certain value so, from the property of the product of generalized normals in eq.
(2), the posterior knowledge on z becomes another normal
(z, , ). The new expected location of z can be expressed as a weighted average of r and so:  =  so + (1-)r and the uncertainty is reduced to 2 =  S2 . The coefficient  =  2r / (  2r + S2 ) quantifies the relative im-

T

T

portance of the experiment with respect to the prior.

This computational advantage can be extended to the general case by using mixtures of conjugate families (Dalal & Hall 1983) to approximate the desired joint probability distribution and the
likelihood function.
Example 6: If the domain and the likelihood are modeled respectively by

p(z) 


i

Pi

T(z,  ,  )
i

p(so|z) 

i



r

r

T(z,  ,  )
r

r

(where  r , r and  r depend explicitly on the observed so), then the posterior can be also
written as the following mixture:



p(z|so) 

i, r

From properties (2) and (3), the parameters

i , r

i ,r

T(z, 

,  i ,r )

(4)

i ,r and  i ,r and the weights i ,r are given by:

i2 r +  2r  i
=
i2 +  r2

 i ,r 

i ,r

 i ,r =

i  r
 i2 +  r2

Pi  r T( i , r , i2 +  r2 )

P 
k

l

T( k , l , 2k +  2l )

k ,l

2.4 The Role of Factorization
Given a multivariate observation z partitioned into two subvectors, z = (x, y), assume that we are
interested in inferring the value of the unknown attributes y from the observed attributes x. Note
that if x and y are statistically independent, the joint density is factorizable: p(z) = p(x, y) = p(x)
p(y) and, therefore, the posterior p(y|x) equals the prior marginal p(y). The observed x carries no
predictive information about y and the optimum estimators do no depend on x. For instance,
y MSE ( x ) = E{y|x} = E{y} and y EP ( x ) = argmax y p( y ) . This is the simplest estimation task. No
runtime computations are required for the optimum solution, which may be precalculated.

174

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

In realistic situations the variables are statistically dependent. In general, the joint density cannot be factorized and the required marginal densities may be hard to compute. However, interesting
consequences arise if the joint density is expressed as a finite mixture of factorized (with independent variables) components C1, C2, ..., Cl :

p(z) = p( z 1 ,..., z n ) =  P{Ci } p( z| Ci ) =  P{Ci }  p( z j | Ci )
i

i

(5)

j

This structure is convenient for inference purposes. In particular, in terms of the desired partition of z = (x, y):

p(z) = p(x, y) =

 P{C } p( x| C ) p( y| C )
i

i

i

i

so the marginal densities are mixtures of the marginal components:

p( x ) =  p( x , y )dy =  P{Ci } p( x| Ci )
Y

i

p( y ) =  P{Ci } p( y| Ci )
i

and the desired conditional densities are also mixtures of the marginal components:

p( y| x ) =   i ( x ) p( y| Ci )

(6)

i

where the weights  i (x) are the probabilities that the observed x has been generated by each component Ci :

i ( x) =

P{Ci } p( x|Ci )

 P{C } p( x|C )
j

= P{Ci | x}

j

j

The p.d.f. approximation capabilities of mixture models with factorized components remain
unchanged, at the cost of a possibly higher number of components to obtain the desired degree of
accuracy, avoiding artifacts (see Fig. 5). Section 5.2 discusses the implications of factorization in
relation with alternative model structures.

(a)
(b)
Figure 5. (a) Density approximation for the data in Fig. 2, using a mixture with 8 factorized components. (b) Location of components. Note how an arbitrary dependence can be
represented as a mixture of components which itself have independent variables (observe
that a somewhat smoother solution could be obtained increasing the number of components).

175

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

3. The MFGN Framework
The previous concepts will be integrated in a general probabilistic inference framework that we call
MFGN (Mixtures of Factorized Generalized Normals). Fig. 6 shows the abstract dependence relations among attributes in a generic domain (upper section of the figure) and between the attributes
and the observed information (lower section). In the MFGN framework, both relations are modeled
by finite mixtures of products of generalized normals. The key idea is using factorization to cope
with multivariate domains and heterogeneous attribute vectors, and conjugate densities to efficiently perform inferences given arbitrary uncertain information. In this section, we will derive the
main inference expressions. The learning stage will be described in Section 4.

p(z)
z1

zn
z2

model of
domain
p(z)

zj
model of
measurement
p(S|z)

S

Figure 6. Generic dependences in the inference process.

3.1 Modeling Attribute Dependences in the Domain
In the MFGN framework the attribute dependencies in the domain are modeled by a joint density in
the form of a finite mixture of factored components, as in expression (5), where the component
marginals p( z j | Ci )  T( z j ,  ij ,  ij ) are generalized normals:

p( z ) =  Pi
i

 T( z

j

,  ij ,  ij )

i=1..l, j=1..n,

(7)

j

If desired, the terms associated to the pure symbolic attributes z j (with all the  ij = 0) can be
collected in such a way that the component marginals are expressed as mixtures of impulses:

p( z j | Ci )   ti j, T( z j ,  )

(8)



where ti j,  P{z j = | Ci } is the probability that z j takes its -th value in component Ci . This
manipulation reduces the number l of global components in the mixture. The adjustable parameters
of the model are the proportions Pi = P{Ci } and the mean value  ij and dispersion  ij of the j-th
j
attribute in the i-th component (or, for the symbolic attributes, the probabilities ti , ). While the

structure (8) will be explicitly used for symbolic attributes in applications and illustrative examples, most of the mathematical derivations will be made over the concise expression (7).

176

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

When all variables are continuous, the MFGN architecture reduces to a mixture of gaussians
with diagonal covariance matrices. The proposed factorized structure extends the properties of
diagonal covariance matrices to heterogeneous attribute vectors. We are interested in joint models,
which support inferences from partial information about any subset of variables. Note that there is
not an easy way to define a measure of statistical depencence between symbolic and continuous
attributes, to be used as a parameter of some probability density function1. The required "hetereogeneous" dependence model can be conveniently captured by superposition of simple factorized
(with independent variables) densities.
Example 7: Figure 7 shows an illustrative 3-attribute data set (x and y are continuous and z is
symbolic) and the components of the MFGN approximation obtained by the EM algorithm
(see Section 4) for their joint density. The parameters of the mixture are shown in Table 1.
Note that, because of the overlapped structure of the data, some components (5 and 6) are assigned to both values of the symbolic attribute z.

(a)
(b)
Figure 7. (a) Simple data set with two continuous and one symbolic attribute.
(b) Location of the mixture components.

i

Pi

 ix

 ix

 iy

 iy

tiz,white

1
.14
-.40
.24
-.27
.20
0
2
.09
-.76
.19
-.68
.18
0
3
.20
.55
.23
.66
.24
0
4
.17
-.71
.27
.76
.22
1
5
.13
.21
.17
-.14
.19
.74
6
.18
-.14
.18
.26
.17
.55
7
.09
.65
.16
-.64
.19
1
Table 1. Parameters of the Mixture Model for the Data Set in Fig. 7.

tiz,black
1
1
1
0
.26
.45
0

3.2 Modeling Arbitrary Information about Instances
The available information about a particular instance z is denoted by S. Following the likelihood
principle, we are not concerned with the true nature of S, whether it is some kind of physical meas1

For this reason, in pattern classification tasks separate models are typically built for each class-conditional density.

177

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

urement or a more subjective judgment about the location of z in the attribute space. All we need to
update our knowledge about z, in the form of the posterior p(z|S), is the relative likelihood function
p(S|z) of the observed S. In general, p(S|z) can be any nonnegative multivariable function fS(z) over
the domain. In the objective case, statistical studies of the measurement process can be used to
determine the likelihood function. In the subjective case, it may be obtained by standard distribution elicitation techniques (Berger 1985). In either case, under the MFGN framework, the likelihood function of the available information to be used in the inference process will be approximated, up to the desired degree of accuracy, by a sum of products of generalized normals:

p( S| z ) =  P{S | s r } p( s r | z ) =  P{S | s r } p( srj | z j )
r

r

=

   T( z
r

j

j

, srj ,  rj )

(9)

j

r

Without loss of generality, the available knowledge is structured as a weighted disjunction S =
1
2
{1s1 OR 2s2 ... OR  R s R } of conjunctions sr = { sr AND sr ... AND srn ) of elementary uncertain
observations in the form of generalized normal likelihoods T( z j , srj ,  rj ) centered at srj with
uncertainty  rj . The measurement process can be interpreted as the result of R (objective or subjective) sensors sr , providing conditionally independent information p( srj | z j ) about the attributes
(each srj only depends on z j ) with relative strength  r . Note that any complex uncertain information about an instance z, expressed as a nested combination of elementary uncertain beliefs srj
about z j using probabilistic connectives, can be ultimately expressed by structure (9) (OR translates to addition, AND translates to product and the product of two generalized normals over the
same attribute becomes a single, weighted normal).
Example 8: Consider the hypothetical computer vision domain in Example 1. Assume that the
information about an object z is the following: AREA is around a and DISTANCE is around b or,
more likely, SHAPE is surely triangular or else circular and AREA is around c and ANGLE is
around d or equal to e. This structured piece of information can be formalized as:

T(z , a,  ) T(z , b,  )]
+ .7 [ (.9T(z ,triang)+.1T(z ,circ)) T(z , c,  ) (T(z , d,  )+T(z ,e)) ]

p(S|z) = .3 [

1

3

a

4

b

4

1

7

c

7

d

which, expanded, becomes the mixture of 5 factorized components operationally represented by
the parameters shown in Table 2.
In a simpler situation, the available information about z could be a conjunction of uncertain attributes similar to {Color = red 0.8 green 0.2} and {Area = 3  .5} and {Shape = rectangular 0.6
circular 0.3 triangular 0.1}. The likelihood of Shape values can be obtained from the output of a
simple pattern classifier (e.g. K-N-nearest neighbors) over moment invariants, while attributes
as Color and Area are directly extracted from the image. In this case we could be interested in
the distribution of values for other attributes as Texture and ObjectType. Alternatively, we
could start from {ObjectType = door 0.6 window 0.4} and {Texture = rough} in order to determine the probabilities of Color and Angle values for selecting a promising search region.

178

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

r

sr1 , 1r

sr3 ,  r3

sr2 , 2r

sr4 , 4r

sr5 , 5r

sr6 , 6r

sr7 , 7r

.30
a, a
-, 
b, b
-, 
-, 
-, 
.63
triang, 0
c, c
-, 
-, 
-, 
-, 
.63
triang, 0
c, c
-, 
-, 
-, 
-, 
.07
circ, 0
c, c
-, 
-, 
-, 
-, 
.07
circ
,
0
c, c
-, 
-, 
-, 
-, 
Table 2. Parameters of the Uncertain Information Model in Example 8.

-, 
d,d
e, 0
d,d
e, 0

3.3 The Joint Model-Observation Density
The generic dependence structure in Fig. 6 is implemented by the MFGN framework as shown in
Fig. 8. The upper section of the figure is the model of nature, obtained in a previous learning stage
and used for inference without further changes. Dependences among attributes are conducted
through an intermediary hidden or latent component Ci. The lower section represents the available
uncertain information, measurement model or query structure associated to each particular inference operation.

Ci

p( z 1 | Ci )
1

Domain:

p( z)   P{Ci } p( z j | Ci )
2

z

z

s21

s22

...

zn

...

s2n

j

i

p( s11 | z1 )
s11

s12

...

s1n

s1

...

s1R

sR2

...

sRn

sR

s2
P{S|s1}

Measurement:
p( S| z )   P{S | s r } p( srj | z j )

S

r

j

Figure 8. Structure of the MFGN model. The attributes are conditionally independent.
The measurement process is modeled by a collection of independent virtual sensors
p( srj | z j ) .

The joint density of the relevant variables becomes:

p(Ci , z , sr , S ) = P{S | s r } p( s r | z ) p( z| Ci ) P{Ci }

 p( s

= P{Ci } P{S | s r }

j
r

| z j ) p( z j | Ci )

j

= Pi  r

 T( z

j

, srj ,  rj ) T( z j ,  ij ,  ij )

j

179

(10)

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Now we will derive an alternative expression for eq. (10) which is more convenient for computing the marginal densities of any desired variable. Using the following relation:

p( srj | z j ) p( z j | Ci ) = p( z j , srj | Ci ) = p( z j | srj , Ci ) p( srj | Ci )
and properties (2) and (3), we define the dual densities of the model:

 ij,r  p( srj | Ci ) =



Zj

p( srj | z j ) p( z j | Ci ) dz j =T( srj ,  ij ,  ij,r )

(11)

p( srj | z j )
p( z j | Ci ) = T( z j , ij,r ,  ji ,r )
p( srj | Ci )

(12)

 ij,r ( z j )  p( z j | srj , Ci ) =

where the parameters ij,r , ij,r and  ji ,r are given by:

 ij,r  ( ij ) 2 + ( rj ) 2
(  ij ) 2 srj + (  rj ) 2  ij
 
(ij,r ) 2
j
i ,r



j
i ,r

 ij  rj
 j
i ,r

ij,r is the likelihood of the r-th elementary observation srj of the j-th attribute z j in each
component Ci and  ij,r ( z j ) is the effect of the r-th elementary information srj about the j-th attribute z j over the marginal component p( z j | Ci ) in each component Ci . Using the above notation, the MFGN model structure can be conveniently written as:

p(Ci , z , sr , S ) = Pi  r  ij,r  ij,r ( z j )

(13)

j

3.4 The Posterior Density
In the inference process the available information is combined with the model of the domain to
update our knowledge about a particular object. Given a new piece of information S we must compute the posterior distribution p( y| S ) of the desired target attributes y  z. Then, estimators
y ( S )  y can be obtained from p( y| S ) to minimize any suitable average loss function. This is
efficiently supported under the MFGN framework regardless of the complexity of the domain p(z)
and the structure of the available information S = { r sr } .
The attributes are partitioned into two subvectors z = (x, y), where y = { z d } are the desired
target attributes and x = { z o } are the rest of attributes. Accordingly, each component sr of the
available information S is partitioned as s r = ( s rx , s ry ) . The information about the target attributes
y in the r-th observation, independent from the model p(z), is denoted by sry (often y is just missing
and there are no such pieces of information) and srx represents the information about the rest of
attributes x. Using this convention we can write:

180

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

p( z , S ) = p( x , y , S ) =  Pi  r i ,r i ,r ( x ) i ,r ( y )
i ,r

Where i ,r is the likelihood of the r-th conjunction sr of S in component Ci :

i ,r   ij,r

(14)

j

and the terms  ij,r ( z j ) are grouped according to the partition of z = (x, y):

i ,r ( x )    io,r ( z o )

i ,r ( y )    id,r ( z d )

o

d

The desired posterior p(y|S) = p(y,S) / p(S) can be computed from the joint p(z,S) by marginalization: along x to obtain p(y,S) and along all z to obtain p(S). Note that each univariate marginalization of p(z,S) along attribute z j eliminates all the terms  ij,r ( z j ) in the sum (13):

p( y , S ) =  p( x , y , S ) dx =  Pi  r i ,r i ,r ( y )
X

i ,r

p( S ) =  p( z , S ) dz =  Pi  r i ,r
Z

i ,r

Therefore, the posterior density can be compactly written as:

p( y| S ) =   i ,r i ,r ( y )

(15)

i ,r

where  i ,r is the probability that the object z has been generated by component Ci and the elementary information sr is true, given the total information S:

 i ,r  P{Ci , sr | S} =

Pi  r i ,r

P

k

 l  k ,l

(16)

k ,l

and i ,r ( y ) = p( y| sry , Ci ) is the marginal density p( y| Ci ) of the desired attributes in the i-th
component, modified by the contribution of all the associated sry . Since p( y| sry , Ci ) =
p( y| sr , Ci ) , the expression (16) also follows from the expansion:

p( y| S ) =  p( y| sr , Ci ) P{Ci , sr | S}
i ,r

In summary, when the joint density and the likelihood function are approximated by mixture
models with the proposed structure, the computation of conditional densities given events of arbitrary geometry is notably simplified. Factorized components reduce multidimensional integration
to simple combination of univariate integrals and conjugate families avoid numeric integration.
This property is illustrated in Fig. 9.

181

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

S

y

p(y|C2)

1y

E{y|S}

C2

p(y,x1,x2)

 2y

C1

p(y|C1)
p(x2|C2)

p(y|S)
p(x2|C1)

22,1 22,2
1

p(x |C2)

21,1

x2

p(x1|C1)

11,2

11,1

s1
s2

x

1

S

Figure 9. Graphical illustration of the essential property of the MFGN framework. Consider the MSE estimate for y, conditioned to the event that (y, x1, x2) is in the cylindrical
region S. The required multidimensional integrations are computed analytically in terms
of the marginal likelihoods ji,r associated to each attribute and each pair of components
Ci and sr of the models for p(y, x1, x2) and for S, respectively. In this case i,r(y)=p(y|Ci)
because no information about y is supplied in S.

Example 9: Fig. 10.a shows the joint density of two continuous variables x and y. It is modeled
as a mixture with 30 factorized generalized normals. Fig. 10.b shows the likelihood function
of the event S1 = {(x  y OR x  -y) AND y>0}. Fig. 10.c shows the posterior joint density
p(x,y|S1). Fig. 10.d shows the likelihood function of the event S2 = {(x,y)  (0,0) OR x3}.
Fig. 10.e shows the posterior joint density p(x,y|S2). Fig. 10.f and 10.g show respectively the
posterior marginal density p(x|S2) and p(y|S2). These complex inferences are analytically computed under the MFGN framework, without any numeric integration.

182

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

(a)

(b)

(c)

(d)
(e)
Figure 10. Illustrative examples of probabilistic inference from arbitrary uncertain information in the MFGN framework (see Example 9).

183

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

p(y|S)

0.4

0.8

0.3

0.6

0.2

0.4

0.1

p(x|S)

0.2

y

x

-4

-2

2

4

-4

-2

2

(f)
Figure 10. (cont.).

4

(g)

3.5 Expressions for the Estimators
Approximations to the optimum estimators can be easily obtained by taking advantage of the
mathematically convenient structure of the posterior density. Under the MFGN framework, the
conditional expected value of any function g(y) becomes a linear combination of constants:

E{g ( y )| S} =  g ( y ) p( y| S ) dy =
Y

=

 
i ,r

Y

i ,r

g ( y ) i ,r ( y ) dy =



i ,r

E i ,r {g ( y )}

(17)

i ,r

where E i ,r {g ( y )}  E{g ( y )| s ry , Ci } is the expected value of g(y) in the i-th component2 modified3 by the r-th observation sry :

E i ,r {g ( y )} 



Y

g ( y )  T( z d ,  id,r , di ,r ) dy
d

We can now analytically compute the desired optimum estimators. For instance, the MSE estimator for a single continuous attribute y = z d requires the mean values E i ,r {z d } =  id,r :

y MSE ( S ) = E{ y| S} =   i ,r  id,r
i ,r

From our explicit expression for p(y|S) we can also compute the conditional cost:

{

}

e 2MSE ( S ) = E ( y  y MSE ( S ) ) | S = E{y 2 | S}  y 2MSE ( S ) =
=


i ,r

i ,r

2

[( 

d
i ,r

) + ( )
2

d
i ,r

2

2

]



    i ,r  id,r 
 i ,r


2

Note that computing the conditional expected value of an arbitrary function g(y) of several variables may be difficult. In
general g(y) can be expanded as a power series to obtain E{g(y)|S} in terms of moments of p(y|S).
3
When S is just sx (there is no information about the target attributes) the constants Ei,r{g(y)} can be precomputed from
the model of nature p(z) after the learning stage.

184

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

Therefore, given S, from Tchevichev inequality we can answer y  y MSE ( S )  2e MSE ( S )
with a confidence level above 75%. When the shape of p(y|S) is complex it must be reported explicitly (the point estimator y  y MSE ( S ) only makes sense if p(y|S) is unimodal).
Example 10: Nonlinear regression. Fig. 11 shows the mixture components and regression
lines (with a confidence band of two standard deviations) obtained in a simple example of
nonlinear dependence between two variables. In this case the joint density can be adequately
approximated by 3 or 4 components: MSE (1 component) = 0.532, MSE (2 comp.) = 0.449,
MSE (3 comp.) = 0.382, MSE (4 comp.) = 0.381.

(a)
(b)
Figure 11. Nonlinear regression example: (a) 2 components, (b) 4 components.

When the target y is symbolic we must compute the posterior probability of each value. In this
case all the di ,r = 0 and the id,r =  id are the possible values  taken by y = z d . Collecting together all the id,r = , as in (8), eq. (15) can be written as:

p ( y| S ) =    i ,r , T ( y ,  )


i ,r

where  i ,r , are the coefficients of the impulses located at . The posterior probability of each
value is:

q   P{ y = | S} =   i ,r ,
i ,r

For instance, the minimum error probability estimator (EP) is:

y EP ( S ) = argmax



q

and any desired rejection threshold can be easily established. We can reject the decision if the entropy of the posterior, H =  q log q, or the estimated error probability, E = 1- max q, are too
high.
Example 11: Nonparametric Pattern Recognition. Fig. 12 shows a bivariate data set with elements from two different categories, represented as the value of an additional symbolic attribute. The joint density can be satisfactorily approximated by a 6-component mixture (Fig.
12.a). The decision regions when the rejection threshold was set to 0.9 are shown in Fig. 12.b.

185

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Note that Statistical Pattern Classification usually start from an (implicit or explicit) approximation to the class-conditional densities. In contrast, we start from the joint density, from
which the class-conditional densities can be easily derived (Fig. 12.c).

(a)
(b)
(c)
Figure 12. Simple nonparametric 2feature pattern recognition task and its 3attribute
joint mixture model: (a) Feature space and mixture components. (b) Decision boundary.
(c) One of the class-conditional densities.

The computation of the optimum estimators for other loss functions is straightforward. Observe that the estimators are based on the combination of different rules, weighted by their degree
of applicability. This is a typical structure used by many other decision methods. In our case, since
the components of the joint density have independent variables the rules reduce to constants, the
simplest type of rule.
3.6 Examples of Elementary Pieces of Information
Some important types of elementary observations srj about z j are shown, including the corresponding likelihoods ij,r and modified marginals  ij,r ( z j ) (j=d) required in expression (15).
Exact information: srj = z j . The observation is modeled by an impulse:

p( srj | z j ) =

T( srj , z j ) = ( srj  z j ) . Therefore:
 ij,r = T( srj ,  ij ,  ij )
 ij,r ( z j ) = T( z j , srj )
The contribution ij,r of exact information about the input attribute z j is the standard likelihood p( z j | Ci ) of the observed value z j in each component. On the other hand, if we acquire
exact information about a target attribute z j (when there is only one (R=1) elementary observation
and s j = z j ) then the inference process is trivially not required: p( z j | S ) = ( z j  s j ) .
Gaussian noise with bias rj and standard deviation  rj : The observation is modeled by a 1component mixture: p( srj | z j ) = T( srj , z j + rj ,  rj ) , which can also be expressed as a 95% confidence interval z j  srj + rj  2 rj . From property (2-2):

186

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

 ij,r = T( srj ,  ij  rj , ( ij ) 2 + (  rj ) 2 )
The effect of a noisy input z j  srj  2 rj is equivalent to the effect of an exact input z j = srj
in a mixture with components of larger variance:  ij  ( ij ) 2 + (  rj ) 2 . Uncertainty spreads the
effect of the observation, increasing the contribution of distant components.
Example 12: Fig. 13.a shows a simple two-attribute domain approximated by a 3-component
mixture. We are interested in the marginal density of attribute x given different degrees of uncertainty  in the input attribute y  .4  2, modeled by

p( s y | y ) = T( s y ,.4,  ) . If  = 0

we have the sharpest density (A) in Fig. 13.b, providing x.4.5. If  = .25 we obtain density
(B) and x.3.7. Finally, if  = .5 we obtain density (C) and x.2.8. Obviously, as the uncertainty in y increases, so does the uncertainty in x. The expected value of x moves towards
more distant components, which become more likely as the probability distribution of y expands. In this situation an interesting effect appears: the mode of the marginal density does not
change at the same rate than the mean. Uncertainty in y skews p(x). This effect suggests that
the optimum estimators for different loss functions are not equally robust against uncertainty.

A
B
C

(a)
(b)
Figure 13. Effect of the amount of uncertainty (see text). (a) Data set and 3-component
model. (b) p(x | uncertain ys around 0.4).

j
j
For the output role,  i ,r ( z ) becomes the original marginal, modified in location and disper2
2
2
sion towards srj according to the factor  = ( ij ) / [( ij ) + (  rj ) ] , which quantifies the relative
importance of the observation:

 ij,r ( z j ) = T(z j ,  ( srj   rj ) + (1   ) ij ,  1/ 2  rj )
Missing data. When there is no information about the j-th attribute, srj = {z j = ?} , the observation can be modeled by p( srj | z j ) = constant or, equivalently, p( srj | z j ) = T( srj , a , b) with a
arbitrary and b  . All the components contribute with the same weight:

 ij,r = p( z j = anything| Ci )  constant  1
If the target is missing the  ij,r ( z j ) reduce to the original marginal components:

187

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

 ij,r ( z j ) = T( z j ,  ij ,  ij ) = p( z j | Ci )
Arbitrary uncertainty. In general, any unidimensional relative likelihood function can be approximated by a mixture of generalized normals, as shown in Example 6, where ij,r and  ij,r ( z j )
are given respectively by eqs. (11) and (12).
Intervals. Some useful functions cannot be accurately approximated by a small number of normal
components. A typical example is the indicator function of an interval, used to model an uncertain
observation where all the values are equally likely: srj = {z j  (a, b)} . If z j is only considered as
input, we can use the shortcut  ij,r = Fi j (b)  Fi j (a ) , where Fi j ( z j ) is the cumulative distribution of the normal marginal component p( z j | Ci ) . Unfortunately, the expression for  ij,r ( z j ) ,
required for z j considered as output, may not be so useful for computing certain optimum estimaj
j
j
tors.  i ,r ( z ) is the restriction of p(z j|Ci) to the interval (a,b) and normalized by i ,r .
Disjunction and conjunction of events. Finally, standard probability rules are used to build
structured information from simple observations: if from subjective judgments or objective evidence we ascribe relative degrees of credibility  rj to several observations srj about z j , the overall
j
j j
likelihood becomes  i =  r  r  i ,r . In particular, if s j = {z j =  1 OR z j =  2 } and the two

possibilities are equiprobable then  ij = p( 1 | Ci ) + p( 2 | Ci ) . Analogously, conjunctions of
events translate to multiplication of likelihood functions.
3.7 Summary of the Inference Procedure
Once the domain p(z) has been adequately modeled in the learning process (as explained in Section
4), the system enters the inference stage over new, partially specified objects. From the parameters
of the domain p(z) ( Pi ,  ij and  ij ) and the parameters of the model of the observation p(S|z)
(  r , srj and  rj ), we must obtain the parameters  ij,r ,  id,r and di ,r of the desired marginal posterior densities and estimators. The inference procedure comprises the following steps:


Compute the elementary likelihoods ij,r , using eq. (11).



Obtain the product i ,r for each conjunction sr and component Ci , using eq. (14).



Normalize Pi  r  i ,r to obtain the coefficients  i ,r of the posterior, using eq. (16).



Choose the desired target attributes y = { z d } and compute the parameters id,r , and

di ,r of the modified component marginal densities  id,r ( z d ) using eq. (12).


Report the joint posterior density of y. Show graphs of the posterior marginal densities
of the desired attributes z d using eq. (15). Provide optimum (point, interval, etc.) estimators using eq. (17).

Example 13: Iris Data. The inference procedure is illustrated over the well known Iris benchmark: 150 objects represented by four numeric features (x, y, z and w) and one symbolic category U  {U1 (setosa), U2 (versicolor), U3 (virginica)}. The whole data set was divided into
two disjoints subsets for training and validation. The joint density can be satisfactorily approxi-

188

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

mated (see Section 4) by a 6component mixture (the error rate classifying U in the validation
set without rejection was 2.67%). Fig. 14 shows two projections of the 150 examples and the location of the mixture components learned from the training subset. The parameters of the mixture are shown in Table 3.

(a)
(b)
Figure 14. Two views of the Iris examples and the components of the joint density mixture model. U1: white, U2: black, U3: gray. (a) Attributes x, y (b) Attributes z, w.

i

Pi

1
2
3
4
5
6

0.15
0.13
0.21
0.18
0.15
0.17

 ix

 ix

 iy

 iy

 iz

 iz

7.13 0.48 3.12 0.34 6.17 0.45
5.48 0.41 2.50 0.28 3.87 0.32
6.29 0.39 2.93 0.27 4.59 0.20
4.75 0.23 3.25 0.23 1.42 0.21
5.36 0.26 3.76 0.29 1.51 0.16
6.16 0.42 2.77 0.28 5.22 0.30
Table 3. Parameters of the Iris Data Joint Density Model

 iw

 iw

2.18
1.20
1.45
0.19
0.32
1.94

0.20
0.21
0.14
0.05
0.10
0.23

P{U1|Ci} P{U2|Ci} P{U3|Ci}

0
0
0
1
1
0

0
0.93
1
0
0
0.00

Table 4 shows the results of the inference process in the following illustrative situations:
Case 1: Attribute z is known: S = {z = 5}.
Case 2: Attributes x and U are known: S = {(x = 5.5) AND (U=U2)}.
Case 3: Attribute x is uncertain: S = {x  71}.
Case 4: Attributes x and w are uncertain: S = {(x  71) AND (w  10.5)}. Note that uncertainty decreases when more information is supplied (compare with Case 3).
Case 5: Structured query expressed in terms of logical connectives over uncertain elementary
events: S = {[(z  13) OR (z  73)] AND [(U = U1) OR (U = U2)]}.

189

1
0.07
0.00
0
0
1

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

CASE
1
2
3
4
5

INPUT
OUTPUT
INPUT
OUTPUT
INPUT
OUTPUT
INPUT
OUTPUT
INPUT
OR

OUTPUT

X
?
6.20.9
5.5
5.5
71
6.70.9
71
6.50.7
?
?

y
?
2.80.6
?
2.60.6
?

z
5.0
5.0
?
4.00.8
?

w
?
1.80.6
?
1.30.4
?

3.00.7
?

5.31.8
?

2.90.6
?
?

1.80.8
10.5
1.30.3
?
?

5.31.2

3.30.9

4.50.8
13
73
23

(approx.
unimodal)

(unimodal)

(bimodal)

0.51

U
?

U2: 22% U3: 78%
U2: 100%
U2: 100%
?
U2: 36% U3: 63%
?
U2: 95% U3: 5%
U1: 50% U2: 50%
U1: 50% U2: 50%
U1: 75% U2: 25%

(bimodal)

Table 4. Some Inference Results Over the IRIS Domain

The consistency of the results can be visually checked in Fig. 14. Finally, Table 5 shows the
elementary likelihoods

i
1
2
3
4
5
6

ix,1

iy,1

i,j r of Case 5, illustrating the essence of the method.

iz,1

iw,1

Ui ,1

i ,1

ix,2

1
1
1
0
1
.001
0
1
1
1
1
.045
.47 .02
1
1
1
1
.016
.50 .01
1
1
1
1
.254
.50 .13
1
1
1
1
.250
.50 .13
1
1
1
0
1
.006
0
Table 5. Elementary likelihoods in Case 5 from Table 4.

iy,2
1
1
1
1
1
1

iz,2

iw,2

Ui ,2

i ,2

.221
.032
.074
3E-4
4E-4
.132

1
1
1
1
1
1

0
.47
.50
.50
.50
0

0
.02
.04
.00
.00
0

3.8 Independent Measurements
One of the key features of the MFGN framework is the ability to infer over arbitrary relational
knowledge about the attributes, in the form of a likelihood function adequately approximated by a
mixture model with the structure of eq. (9). For instance, we could answer questions as: what happens to z d when z i tends to be less than z j ? (i.e., when p(S|z) is high in the region z i  z j < 0 ).
However, there are situations where the observations over each single attribute z j are statistically
independent: we have information about attributes (e.g. z i is around a and z j is around b) but not
about attribute relations. We will pay attention to this particular case because it illustrates the role
of the main MFGN framework elements. Furthermore, many practical applications can be satisfactorily solved under the assumption of independent measurements or judgments. In this case, the
likelihood of the available information can be expressed as the conjunction of n marginal observations s j about z j :

p ( S | z ) =  p( s j | z j )
j

190

(18)

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

This means that the sum of products in equation (9) is complete, i.e., it includes all the elements
in the N-fold cartesian product of attributes:

p( S | z ) =    rj T( z j , srj ,  rj )
j

r

where  j  rj =  r . This factored likelihood function can be considered also as a 1-component
mixture (with R=1 in (9) and s j  s1j ) where the marginal observation models are allowed to be
mixtures of generalized normals: p( s j | z j ) =  r '  rj' T( z j , srj' ,  rj ' ) . In this case we can even think
1
1
of function valued attributes z  [ f ( z ),..., f n ( z n )] , where f j ( z j )  p( s j | z j ) models the

range and relative likelihood of the values of z j . Loosely speaking, attributes with concentrated
f j ( z j ) may be considered as inputs, and attributes with high dispersion play the role of outputs.
Since y is conditionally independent of s x given Ci , the posterior can be obtained from the expansion:

p( y| S ) =  p( y| S , Ci ) P{Ci | S} =  p( y| Ci , s y ) P{Ci | S}
i

(19)

i

The interpretation of (19) is straightforward. The effect of sx over y = {zd} must be computed
through x = {zo} and the components Ci . Then, a simple Bayesian update of p( y| s x ) as a new
prior is made using s y (see Fig. 15).

Ci
z1

i

p( z j | Ci )

zd
 id ( z d )

s1

...


...

sd

j
i

zj
p( s j | z j )

sj

...

Figure 15. Structure of the MFGN inference process from independent pieces of information. In this case, the likelihood function is also factorizable. The data flow in the inference process is shown by dotted arrows.

191

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

4. Learning from Uncertain Information
In the previous section, we have described the inference process from uncertain information under
the MFGN framework. Now we will develop a learning algorithm for the model of the domain,
where the training examples will be also uncertain. Specifically, we must find the parameters Pi ,

 ij ,  ij (or ti j, ) of a mixture with structure (7) to approximate the true joint density p(z) from a
training i.i.d. random sample {z(k)}, k=1..M, partially known through the associated likelihood
functions {S(k)} with structure (9).
4.1 Overview of the EM Algorithm
Maximum Likelihood estimates for the parameters of mixture models are usually computed by the
well-known Expectation-Maximization (EM) algorithm (Dempster, Laird and Rubin 1997, Redner
and Walker 1984, Tanner 1996), based in the following idea. In principle, the maximization of the
training sample likelihood J =  k p( z ( k ) ) is a mathematically complex task due to the product of
sums structure. However, note that J could be conveniently expressed for maximization if the components that generated each example were known (this is called complete data in EM terminology). The underlying credit assignment problem disappears and the estimation task reduces to several uncoupled simple maximizations. The key idea of EM is the following: instead of maximizing
the complete data likelihood (which is unknown), we can iteratively maximize its expected value
given the training sample and the current mixture parameters. It can be shown that this process
eventually achieves a local maximum of J.
Instead of a rigorous derivation of the EM algorithm, to be found in the references (see especially McLachlan and Krishnan, 1997), we will present here a more heuristic justification which
provides insight for generalizing the EM algorithm to accept uncertain examples. We will review
first the simplest case, where no missing or uncertain values are allowed in the training set. The
parameters of the mixture are conditional expectations:

E z |Ci {g ( z )| Ci )} =  g ( z ) p( z| Ci ) dz
Z

(20)

2
2
In particular,  ij = E{z j | Ci } , ( ij ) = E{( z j   ij ) | Ci } and ti j, = E{I {z j = }| Ci } . The

mixture proportions are Pi = E{ P{Ci | z} } .
We rewrite the conditional expectation (20) using Bayes Theorem in the form of an unconditional expectation:

E z |Ci {g ( z )| Ci )} =  g ( z ) P{Ci | z} p( z ) / P{Ci } dz =

(21)

= E z {g ( z ) P{Ci | z}} / Pi

(22)

Z

The EM algorithm can be interpreted as a method to iteratively update the mixture parameters
using expression (22) in the form of an empirical average over the training data4. Starting from a
tentative, randomly chosen set of parameters, the following E and M steps are repeated until the
4

Expression (21) can be also used for iterative approximation of explicit functions which are not indirectly known by
i.i.d. sampling (e.g., subjective likelihood functions sketched by the human user, as in Example 4). In this case p(z) is set
to the target function and P{Ci| z} is computed from the current mixture model.

192

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

total likelihood J no longer improves (the notation (expression)(k) means that (expression) is computed with the parameters of example z(k)):
( )
( )
(E) Expectation step. Compute the probabilities qi k  P{Ci | z k } that the k-th example has been
generated by the i-th component of the mixture:

qi( k )  p( z ( k ) | Ci ) P{Ci } / p( z ( k ) )
(M) Maximization step. Update the parameters of each component using all the examples, weighted
by their probabilities qi( k ) . First, the a priori probabilities of each component:

Pi 

1
 q (k )
M k i

Then, for continuous variables, the mean values and standard deviations in each component:

1
MPi

 ij 
( ij ) 2 

1
MPi

 [q

z j ]( k )

i

k

 [q

( z j ) 2 ]( k )  ( ij ) 2

i

(23)

k

and for symbolic variables, the probabilities of each value:

ti j, 

1
MPi

 [q

I {z j = }]( k )

i

k

4.2 Extension to Uncertain Values
In general, in the MFGN framework we do not know the true values z j of the attributes in the
training examples, required to compute g ( z ) P{Ci | z} in the (empirical) expectation (22). Instead,
we will start from uncertain observations S ( k ) about the true training examples z
likelihood functions expressed as mixtures of generalized normals:

(k )

, in the form of

p( S ( k ) z ( k ) ) =  P{S ( k ) sr( k ) } p( sr( k ) z ( k ) )
r

Therefore, we must express the expectation (22) over p(z) as an unconditional expectation
over p(S), the distribution which generates the available information about the training set. This
can be easily done by expanding p( z| Ci ) in terms of S:

E z |Ci {g ( z )| Ci )} =  g ( z ) p( z| Ci ) dz
Z

[  p ( z | S , C ) p ( S | C ) dS ] dz
=  [ g ( z ) p( z| S , C ) dz ] P{C | S} p( S ) dS / P{C }
=

S

Z



Z

g ( z)

i

S

i

i

i

If we define

193

i

(24)

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

i ( S )  E z |S ,Ci {g ( z )| S , Ci )} =  g ( z ) p( z| Ci ) p( S | z ) dz / p( S | Ci )
Z

then the parameters of p(z) can be finally written5 as an unconditional expectation over the observable p(S) in a form similar to eq. (22):

E z |Ci {g ( z )| Ci )} = E S {i ( S ) P{Ci | S}} / Pi

(25)

This expression justifies an extended form of the EM algorithm to iteratively update the parameters of p(z) by averaging i ( S ) P{Ci | S} over the available training information { S ( k ) }
drawn from p(S). This can be considered as a numerical/statistical method for solving p(z) in the
integral equation:



Z

p ( S | z ) p ( z ) dz = p ( S )

Note that we cannot approximate p(S) as a fixed mixture in terms of p( S | Ci ) and then computing back the corresponding p( z| Ci ) because, in general, p( S | z ) will be different for the different training examples. For the same reason, elementary deconvolution methods are not directly
applicable.
This kind of problem is addressed by Vapnik (1982, 1995) to perform inference from the result of
indirect measurements. This is an ill-posed problem, requiring regularization techniques. The
proposed extended EM algorithm can be considered as a method for empirical regularization, in
which the solution is restricted to be in the family of mixtures of (generalized) gaussians. EM is
also proposed by You and Kaveh (1996) for regularization in the context of image restoration.
The interpretation of (25) is straightforward. Since we do not know the exact z required to
approximate the parameters of p(z) by empirically averaging g ( z ) P{Ci | z} , we obtain the same
result by averaging the corresponding i ( S ) P{Ci | S} in the S domain, where i ( S ) plays the
role of g(z) in (22). As z is uncertain, g(z) is replaced by its expected value in each component
given the information about S. In particular, if there is exact knowledge about the training set at( )
tributes ( S ( k ) = z k , i.e., R = 1 and the marginal likelihoods are impulses) then (25) reduces to
(22). Fig. 16. illustrates the approximation process performed by the extended version of the EM
algorithm in a simple univariate situation.
It is convenient to develop a version of the proposed Extended EM algorithm for uncertain
training sets, structured as tables of (sub)cases  (uncertainly valued) variables (see Fig. 17).
First, let us write eq. (24) expanding S in terms of its components sr:

p( z| Ci , S ) P{Ci | S ) = p( z , Ci | S )
=

 p( z, C | s ) P{s | S} =  p( z| C , s ) P{C | s } P{s | S}
i

r

r

r

i

r

i

r

r

r

Therefore

i ( S ) P{Ci | S} =  i ,r ( s r ) P{Ci , s r | S}
r

This result can be also obtained from the relation Ez{w(z)} = ES{ Ez|S{w(z)| S} } for w(z)  g(z) P{Ci|z} and Bayes Theorem.

5

194

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

(a)

(b)
(d)

(c)

Figure 16. The extended EM algorithm iteratively reduces the (large) difference between
(a) the true density p(z), and (b) the mixture model p ( z ) , indirectly through the (small)
discrepancies between (c) the true observation density p(S) and (d) the modeled observation density p ( S ) . In real cases p(S) must be estimated from a finite i.i.d. sample
{S(k)}.

S(1)

s1(1)
s2(1)
s(2)
s1(3)
s2(3)
s3(3)
...

.4
.6
1
.2
.5
.3
...

S(2)
S(3)

 (r k )

S(r)

( sr1 ,  1r ) ( k )

( srj ,  rj ) ( k )

...

...

...
...
Figure 17. Structure of the uncertain training information for the Extended EM Algorithm. The coefficients

 (r k ) are normalized for easy detection of the rows included in

each uncertain example. When z



(k )

= 1 and all the

(k)

(k)

is not uncertain, S

reduces to a single row with

 = 0.
j

Using the notation introduced in (12),

i ,r ( s r )  E z |sr ,Ci {g ( z )| s r , Ci } =



Z

g ( z ) p( z| s r , Ci ) dz =



Z

P{Ci , s r | S} = P{Ci | s r } P{s r | S} =  i ,r
we can write (25) as:

195

g ( z )  ij,r ( z j ) dz
j

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO



E z|Ci {g (z ) | Ci } = E S   i ,r  g (z )  ij,r ( z j )dz / Pi
Z
j
 r

In the MFGN framework the contributions i ,r ( s r ) P{Ci , s r | S} to the empirical expected
values required by the Extended EM algorithm can be obtained again without numeric integration.
We only need to consider the case g(z) = z j to compute the means  ij and probabilities ti j, , and
g(z) = ( z j )2 for the deviations  ij . From (12) we already know an explicit expression for the parameters of  ij,r ( z j ) = T( z j ,  ij,r ,  ji ,r ) . Hence:



Z

z j i ,r ( z ) dz =

 (z
Z

j



Z

z j  ij,r ( z j )dz j =  ij,r

) 2 i ,r ( z ) dz = (  ij,r ) 2 + (  ji ,r ) 2

In conclusion, the steps of the Extended EM algorithm are as follows:
(E) Expectation step. Compute all the elementary likelihoods of the training set:

(

 ij,r( k ) = T srj ,  ij , ( ij ) 2 + (  rj ) 2

)

(k )

(26)

( )
( )
Obtain the likelihood of each conjunction sr k of example S k in component Ci:

 i(,kr) =   ij,r( k )
j

( )
Obtain the total likelihood of example S k :

 ( k )  p( S ( k ) ) =   Pi  r( k )  i(,kr )
i

r

( )
( )
( )
Compute the probabilities qi ,kr  P{Ci , s r k | S k } that the r-th component of the k-th exam-

ple has been generated by the i-th component of the mixture:

qi(,kr )   i(,kr) = Pi  r ( k )  i(,kr) /  ( k )
(M) Maximization step. Update the parameters of each component Ci using all the components

s r( k ) of all the examples weighted with their probabilities qi(,kr ) . First, the prior probabilities of each
component:

Pi 

1
M

 q
k

(k )
i ,r

r

Then the mean value and standard deviation in each component:

 ij 

1
MPi

  [q
k

r

196

i ,r

ij,r

]

(k )

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

( ij ) 2 

1
MPi

  [q
k

i ,r

]

[( ij,r ) 2 + ( ij ,r ) 2 ]

r

(k )

 ( ij ) 2

(27)

For symbolic variables under representation (8) we may use:

ij,r( k ) =  P{srj = }( k ) ti j,

(26)



ti j, 

1
MPi

  [q
k

i ,r

P{srj = } t i j, /  ij,r

r

]

(k )

(27)

Consider the particular case in which the attributes in the training examples are contaminated
with unbiased Gaussian noise. The likelihood of the uncertain observations is modeled by 1( )
( )
( )
( )
( )
( ) 2
component mixtures: p( s k | z k ) =  j T( z j k , s j k ,  j k ) , where ( j k ) is the variance of
the measurement process over z j ( k ) which obtains the observed value s j ( k ) . This can be also ex( )
( )
( )
pressed as a confidence interval z j k  s j k  2 j k . In this case, the basic EM algorithm (23)
( )
can be easily modified to take into account the effect of the uncertainties  j k . In the E step, com( )
pute qi k using the following deviations:

 ij  ( ij ) 2 + (  j ( k ) ) 2
and, in the M step, apply the substitution:

z j ( k )   s j ( k ) + (1   )  ij

[

( z j ( k ) ) 2   s j ( k ) + (1   ) ij

] +  [ ]
2

j( k ) 2

where

( ij ) 2
= j 2
( i ) + (  j ( k ) ) 2
measures the relative importance of the observed s j k for computing the new  ij and  ij .
The previous situation illustrates how missing values must be processed in the learning stage.
( )
j (k )
If z
is exact then  j k = 0 and  = 1, so the original algorithm (23) is not changed. In the other
( )
extreme, if z j ( k ) is missing, which can be modeled by  j k  , we get  = 0 and therefore the
( )
observation s j k does not contribute to the new parameters at all. The correct procedure to deal
with missing values in the MFGN framework is simply omitting them in the empirical averages.
Note that this fact arises from the factorized structure of the mixture components, providing conditionally independent attributes. Alternative learning methods require a careful management of
missing data to avoid biased estimators (Ghahramani & Jordan 1994).
( )

4.3 Evaluation of the Extended EM Algorithm
We have studied the improvement of the parameter estimations when the uncertainty of the observations, modeled by likelihood functions, is explicitly taken into account. The proposed Extended

197

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

EM is compared with the EM algorithm over the "raw" observations (Basic EM), which ignores the
likelihood function and typically uses just its average value (e.g., given x82, Basic EM uses
x=8). We considered a synthetic 3-attribute domain with the following joint density:
p(x,y,w) = 0.5 (x,0,2) (y,0,1) (w,white)
+ 0.5 (x,2,1) (y,2,2) (w,black)

TT

TT

TT

Different learning experiments were performed with varying degrees of uncertainty. In all
cases the training sample size was 250. All trained models had the same structure as the true density (2 components), since the goal of this experiment is to measure the quality of the estimation
with respect to the amount of uncertainty, without regard of other sources of variability such as
local minima, alternative solutions, etc., which are empirically studied in Section 5. Table 6 shows
the mixture parameters obtained by the learning algorithms. Fig. 18 graphically shows the difference between Extended and Basic EM in some illustrative cases.
Case 0: Exact Data (Fig. 18.a).
Cases m %: Results of the Extended EM learning algorithm when there is a m % rate of missing
values in the training data.
Case 1: Basic EM when attribute y is biased +3 units with probability 0.7. Case 2: Extended EM
algorithm over Case 1 (see Fig. 18.b). Here, the observed value is sy=y+3 in 70% of the samples
and sy=y in the rest. In all samples, Basic EM uses the observed value sy and Extended EM uses
the explicit likelihood function f(y) = 0.3 (ysy) + 0.7 (y(sy3)).
Case 3: Basic EM when attributes x and y have Gaussian noise with  = 0.5 and w is changed with
probability 0.1. Case 4: Extended EM algorithm over Case 3.
Case 5: Basic EM when x and y have Gaussian noise with  = 1 and w is changed with probability
0.2. Case 6: Extended EM algorithm over Case 5 (see Fig. 18.c).
Case 7: Basic EM when x and y have Gaussian noise with  = 2 and w is changed with probability
0.3. Case 8: Extended EM algorithm over Case 7 (see Fig. 18.d).

T

Case 9: Extended EM when values y>3 are missing (censoring). Case 10: Extended EM over Case
9 when the missing y values are assumed to be distributed as
(y, 4, 1), providing some additional information on the data generation mechanism.
Table 6 and Fig. 18 confirm that for small amounts of deterioration in relation to the sample
size, the estimates computed by the basic EM Algorithm over the raw observed data are similar
to those obtained by the Extended EM algorithm (e.g., Cases 3 and 4). However, when the data sets
are moderately deteriorated the true joint density can be correctly recovered by Extended EM using
the likelihood functions of the attributes instead of the raw observed data (e.g., Cases 5 and 6, Fig.
18.c). Finally, when there is a very large amount of uncertainty with respect to the training sample
size the true joint density cannot be adequately recovered (e.g., Cases 7 and 8, Fig. 18.d).

198

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

(a)

(b)

(c)
(d)
Figure 18. Illustration of the advantages of the Extended EM algorithm (see text). (a)
Case 0 (exact data). (b) Cases 1 and 2 (biased data). (c) Cases 5 and 6 (moderated noise).
(d) Cases 7 and 8 (large noise). All figures show the true mixture components (gray ellipses), the available raw observations (black and white squares), the components estimated by Basic EM from the raw observations (dotted ellipses) and the components estimated by Extended EM taking into account the likelihood functions of the uncertain
values (black ellipses).

Note that the ability to learn from uncertain information suggests a method to manage non
random missing attributes (e.g., censoring) (Ghahramani & Jordan 1994) and other complex
mechanisms of uncertain data generation. As illustrated in Case 9, if the missing data generation
mechanism depends on the value of the hidden attribute, it is not correct to assign equal likelihood
to all components. In principle, statistical studies or other kind of knowledge may help to ascertain
the likelihood of the true values as a function of the available observations. For instance, in Case 10
we replaced the missing attributes of Case 9 by normal likelihoods y  42 (i.e, y is high), improving the estimates of the mixture parameters.

199

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Case

P1

1x

1y

1x

1y

t1,wwhite

 2x

 2y

true
.5
0
0
2
1
1
2
2
0
.48 -.04 .03 2.11 1.00 1.00 2.09 1.83
20%
.48 .16 -.03 1.91 1.01 0.96 2.31 2.09
40%
.49 .14 -.16 1.78 .99 0.95 2.39 2.29
60%
.45 .02 -.29 2.50 .78 1.00 1.86 2.01
80%
.49 -.11 1.69 2.21 1.73 .50 1.91 0.31
1
.48 .06
.90 1.88 1.73 1.00 1.88 2.98
2
.48 -.04 .05 1.90 .92 1.00 1.98 1.97
3
.47 .02
.09 1.60 1.02 .87 2.00 1.78
4
.49 .27 -.08 1.97 .90
.70 2.10 2.06
5
.43 -.04 -.18 2.40 1.48 .82 1.85 1.52
6
.54 -.07 -.02 1.97 1.09 .56 1.93 2.11
7
.46 .15 -.16 2.73 2.53 .31 1.94 1.62
8
.79 .87
.08 2.09 1.52 .51 1.96 3.61
9
.48 .32 -.02 1.77 1.10 1.00 1.92 0.67
10
.45 .00
.03 2.20 1.01 1.00 2.13 1.55
Table 6. Parameter Estimates from Uncertain Information (see text)

 2x

 2y

t2,wblack

1
1.00
.88
.94
1.03
1.14
.96
1.01
1.14
1.01
1.52
.85
2.47
0.87
0.94
1.04

2
2.08
2.10
2.14
1.77
0.68
2.40
1.90
2.07
1.94
2.33
1.69
2.90
1.21
1.22
1.77

1
1.00
1.00
1.00
1.00
0.47
1.00
1.00
0.85
0.71
.80
.62
.29
.54
1.00
1.00

Example 14: Learning from examples with missing attributes has been performed over the IRIS
domain to illustrate the behavior of the MFGN framework. The whole data set was randomly
divided into two subsets of equal size for training and testing. 5-component mixture models
were obtained and evaluated, combining missing data proportions of 0% and 50%. The error
prediction on attribute U (plant class) was the following:

missing attributes

training set
0%
0%
50%
50%

test set
0%
50%
0%
50%

prediction error
2.7%
12.0%
4.0%
18.7%

In the relatively simple IRIS domain, the performance degradation due to 50% missing attributes is much greater in inference than in learning stage. The Extended EM algorithm is able
to correctly recover the overall structure of the domain from the available information.

4.4 Comments
Convergence of the EM Algorithm is very fast, requiring no adjustable parameters such as learning
rates. The algorithm is robust with respect to the random initial mixture parameters: bad local
maxima are not frequent and alternative solutions are usually equally acceptable. All the examples
contribute to all the components, which are never wasted by unfortunate initialization. For a fixed
number of components, the algorithm progressively increases the likelihood J of the training data
until a maximum is reached. When the number of components is incremented the maximum J also
increases, until a limit value is obtained that cannot be improved using extra components (Fukunaga 1990). Some simple heuristics can be incorporated to the standard Expectation-Maximization
scheme to control the value of certain parameters (e.g., lower bounds can be established for variances) or the quality of the model (e.g., mixture components can be eliminated if their proportions
are too small).
In our case, factorized components are specially convenient because matrix inversions are not
required and, what is more important, uncertain and missing values can be correctly handled in a

200

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

simple and unified way, for heterogeneous attribute sets. It is not necessary to provide models for
uncertain attribute correlations since no covariance parameters must be estimated. Finally, the
training sample size must be large enough in relation to both the degree of uncertainty of the examples and the complexity of the joint density model in order to obtain satisfactory approximations.
On the other hand, the number of mixture components required for a satisfactory approximation to the joint density must be specified. A pragmatic option is the minimization of the experimental estimation cost over the main inference task, if it exists. For instance, in regression we
could increase the number of components until an acceptable estimation error is obtained over an
independent data set (cross-validation). The same idea applies to pattern classification: use the
number of components that minimizes the error rate over an independent test set. However, one of
the main advantages of the proposed method is the independence between the learning stage and
the inference stage, where we can freely choose and dynamically modify the input and output role
of the attributes. Therefore, a global validity criterion is desirable. Some typical validation methods
for mixture models are reviewed in McLachlan & Basford (1988); the standard approach is based
on likelihood ratio tests on the number of components. Unfortunately, this method does not validate
the mixture itself, only selects the best number of components (DeSoete 1993).
Since the MFGN framework provides an explicit expression for the model p(z), we can apply
statistical tests of hypothesis over an independent sample T taken from the true density (e.g. a subset of the examples reserved for testing) to find out if the obtained approximation is compatible
with test data. If the hypothesis H = {T comes from p(z)} is rejected, then the learning process must
continue, possibly increasing the number of components. It is not difficult to build some statistical
tests, e.g. over moments of p(z), because their sample means and variances can be directly obtained.
However, as data sets usually include symbolic and numeric variables, we have also developed a
test on the expected likelihood of the test sample, which measures how well p(z) covers the examples. The mean and variance of p(z) can be easily obtained using the properties of generalized
normals. Some experiments over simple univariate continuous densities show that this test is not
very powerful for small sample sizes, i.e. incompatibility is not always detected, while other standard tests significantly evidence rejection. Nevertheless, clearly inaccurate approximations are
detected, results improve as the sample size increases and the test is valid for data sets with uncertain values.
The Minimum Description Length (Li & Vitnyi 1993) principle can be also invoked to select
the optimum number of components by trading-off the complexity of the model and the accuracy in
the description of the data.

201

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

5. Discussion and Experimental Results
5.1 Advantages of Joint Models
Most inductive inference methods compute a direct approximation of the conditional densities of
interest, or even obtain empirical decision rules without explicit models of the underlying conditional densities. In these cases, both the model and the learning stage depend on the selected input /
output role of the variables. In contrast, we have presented an inference and learning method based
on an approximation to the joint probability density function of the attributes by a convenient
parametric family (a special mixture model). The MFGN framework works as a pattern completion
machine operating over possibly uncertain information. For example, given a pattern classification
problem, the same learning stage suffices for predicting class labels from feature vectors and for
estimating the value of missing features from the observed information in incomplete patterns. The
joint density approach finds the regions occupied by the training examples in the whole attribute
space. The attribute dependences are captured at a higher abstraction level than the one provided by
strictly empirical rules for pre-established target variables. This property is extremely useful in
many situations, as shown in the following examples.
Example 15: Hints can be provided for inference over multivalued relations. Given the data
set and model from Example 10, assume that we are interested in the value of x for y = 0. We
obtain the bimodal marginal density shown in Fig. 19.a and the corresponding estimator x 
0.2  1.4 which is, in some sense, meaningless. However, if we specify the branch of interest
of the model, inferring from y = 0 AND x  -11 (i.e., x is small), we obtain the unimodal
marginal density in Fig. 19.b and the reasonable estimator x  0.80.5.

(a)
(b)
Figure 19. The desired branch in multivalued relations can be selected by providing
some information about the output values. (a) Bimodal posterior density inferred from
y=0. (b) Unimodal posterior density inferred from y = 0 and the hint x is small.

Example 16: Image Processing. The advantages of a joint model supporting inferences from
partial information on both inputs and outputs can be illustrated in the following application
with natural data (see Fig. 20). The image in Fig. 20.a is characterized by a 5-attribute density
(x, y, R, G, B) describing position and color of the pixels. A random sample of 5000 pixels was
used to build a 100-component mixture model. We are interested in the location of certain ob-

202

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

jects in the image. Figs. 20.b-f show the posterior density of the desired attributes given the following queries:


"Something light green". Fig. 20.b. Two groups can be easily identified in the posterior
density, corresponding to the upper faces of the green objects6. S = C1={x, y unknown;
R=11050, G=24510, B=16050}.



"Something light green OR dark red". Fig. 20.c. We find the same groups as above and an
additional, more scattered group, corresponding to the red object. This greater dispersion
arises from the larger size of the red object and also from the fact that the R component of
dark red is more disperse than the G component of light green. S =Two equiprobable components with C1 as above and C2={x, y unknown; R=11010, G=B=3050}.



"Something light green on the right". Fig. 20.d. Here we provide partial information on the
output: S = C3={x=24030; y unknown; R=11050, G=24510, B=16050}



"Something white". Fig. 20.e. S = C4={x,y unknown; R=24510, G=24510, B=24510}



"Something white, in the lower-left region, under the main diagonal (y<240-x)". Fig. 20.f.
Here we provide relational information on the attributes that can be modeled by S = 6 equiprobable components (note that in this case the posterior distribution contains 600 components, but it is still computationally manageable) =

{x=6030, y=18030, R=24510, G=24510, B=24510}+
{x=6030, y=12030, R=24510, G=24510, B=24510}+
{x=6030, y=6030, R=24510, G=24510, B=24510}+
{x=12030, y=12030, R=24510, G=24510, B=24510}+
{x=12030, y=6030, R=24510, G=24510, B=24510}+
{x=18030, y=6030, R=24510, G=24510, B=24510}
In all cases, the posterior density is consistent with the structure of the original image. The time
required to compute the posterior distribution is always lower than one second. Learning time
was of order of hours in a Pentium 100 system. Simpler models (25-component, obtained from
1000 random pixels) produced also acceptable results with much lower learning time. Furthermore, the EM algorithm can be efficiently parallelized.

On the other hand, when there is a large number of irrelevant attributes, the joint model strategy wastes resources to capture a proper probability density function along unnecessary dimensions. (This problem does not arise in the specification of a likelihood function, since only the relevant attributes explicitly appear in the model.) Joint modeling is appropriate for domains with a
moderated number of "meaningful" variables without fixed input / output roles.

6

Note that a sharp peak (a component with small dispersion) was obtained in the learning process, which also "transmits"
to the posterior density. This kind of artifacts are inocuous and can be easily removed by post processing.

203

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

(a)

(b)

(c)

(d)

(e)
(f)
Figure 20. Inference results for the image domain in Example 16. (a) source image.
(b) posterior density of attributes x-y given "Something light green". (c) the same for
"Something light green OR dark red". (d) for "Something light green on the right".
(e) for "Something white". (f) for "Something white, in the lower-left region, under the
main diagonal of the image (Y<240-X)"

5.2 Advantages of Factorization
The proposed methodology is supported by the general density approximation property of mixture
models. We use components with independent variables in order to make computations feasible in

204

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

the inference and learning stage. Factorized components can be imposed to a mixture model without loss of generality. Any statistical dependence between variables can be still captured, at the cost
of a possibly larger number of components in the mixture to achieve the required accuracy in the
approximation.
The simplicity of the building block structure is entirely compensated by an important saving in computation time. High-dimensional integrals are analytically computed from univariate
integrals and matrix inversions are avoided in the learning stage. Additionally, high-dimensional
domains can be easily modeled using a small number of parameters in each mixture component.
From the viewpoint of Computational Learning Theory (Vapnik 1995), models with a small number of adjustable parameters (actually, with low expressive power) have favorable consequences
for generalization.
Mixtures of factorized components are also used in Latent Class Analysis (DeSoete 1993), a
well-known unsupervised classification technique. It is assumed that the statistical dependences
between attributes can be fully explained by a hidden variable specifying the latent class of each
example. This method is similar to the Gaussian decomposition clustering algorithm mentioned in
Section 1, constrained to component-conditional attribute independence. However, our goal is not
unsupervised classification but obtaining an accurate and mathematically convenient expression for
the joint density of the variables, required to derive the desired estimators. The meaning of the
components is irrelevant, as long as the whole mixture is a good approximation to the joint density.
More expressive architectures, which combine mixture models with local dimensionality reduction, have been also considered: Mixtures of Linear Experts (Jordan & Jacobs 1994), Mixtures
of Principal Component Analyzers (Sung & Poggio 1998) or Mixtures of Factor Analyzers (Ghahramani & Hinton 1996, Hinton, Dayan, & Revow 1997). Unfortunately, the general kind of inference and learning from uncertain data considered in this work cannot be directly incorporated into
these architectures with the computational advantages demonstrated by the MFGN model.
The restriction to factorized components may produce undesirable artifacts in the approximations of certain domains learned from small training samples. Nevertheless, this problem always
occurs to any approximator when the structure of the building block does not match the shape of
the target function. In this case, many terms (or components, units, etc.) are required for a good
approximation and the associated parameters can be correctly adjusted only from a large training
sample. However, note that the complexity of the model should not be measured uniquely in terms
of the number of mixture components. The number of adjustable parameters is probably a better
measure of complexity. For instance, full covariance models show a quadratic growth of the number of free parameters with respect to the dimension of the attribute vector. For factorized components the growth is linear, so the amount of training data need not be unreasonably high even if the
number of mixture components is large.
In real applications, the nature of the target function is unknown, so little can be said a priori
about the best building block structure to be used by a universal approximator. We have chosen a
very simple component structure to make inference and learning feasible from uncertain information. Section 5.4 provides experimental evidence that in realistic problems the proposed model is
not inferior to other popular approaches.
5.3 Qualitative Comparison with Alternative Approaches
Instead of the proposed methodology, based on mixture models and the EM algorithm, other alternative nonparametric density approximation methods could also be used (either for the joint density
or for specific conditional densities). For instance, the nearest neighbor rule locally approximates
the target density using a certain number of training samples near to the point of interest. Symbolic

205

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

attributes are directly estimated by a voting scheme and continuous attributes can be also estimated
by averaging the observed values of training instances which are near, in the subspace of observed
attributes, to the point of interest. However, for small sample sizes, the above estimators are not
smooth and show strong sensitivity to random fluctuations in the training set, which penalizes the
estimation cost. For large sample size, the time required to find the nearest neighbors becomes very
long. As an example, consider the regression problem in Example 10, Section 3.5. Fig. 11.b shows
the MFGN solution with 4 components and MSE=0.381. Fig. 21.a shows the regression line obtained by 5-nearest-neighbors average, with a higher MSE=0.522.
Parzen windows and similar kernel approximation methods are used to smooth the results of
the simple nearest neighbors rule (Duda & Hart 1973, Izenman 1991). They are actually mixtures
of simple conventional densities located at the training samples. In principle, the properties of the
MFGN framework could be adapted to that kind of approximation (Ruiz et al. 1998). Learning
becomes trivial, but strong run time computation effort is required since a concise model of the
domain is not extracted from the training set. This kind of rote learning has also negative consequences on generalization according to the Occam Razor Principle (Li & Vitnyi 1993). An adequately cross-validated mixture model with a small number of components in relation to the training sample size reasonably guarantees that probably the true attribute dependencies are correctly
captured.

C2

C2
C1

C1

(a)
(b)
(c)
Figure 21. Alternative solutions in regression and classification (see text for details).

The nature of the solutions obtained by Backpropagation Multilayer Perceptrons (Rumelhart et
al. 1986) in pattern classification is also illustrative. In general, each decision region can be geometrically expressed as the union of intersections of several halfspaces defined by the units in the
first hidden layer. However, backprop networks often require very long learning times, many adjustable parameters and, what is worse, apparently simple distributions of patterns are hard to learn.
For instance, the solution to the circle-ring classification problem in Fig. 21.b, obtained by a network with 6 hidden units requires hundreds of standard backprop epochs. The decision regions are
not very satisfactory, even though the network has extra flexibility for this task (3 hidden units
suffice to separate the training examples). Better solutions exist using all the resources in the network architecture, but backprop learning does not find them. In contrast, the solution obtained by
the MFGN approach using 7 components (Fig. 21.c) requires a learning time orders of magnitude
shorter than backprop optimization. All the components in the mixture contribute to synthesize
reliable decision regions and acceptable solutions can be also obtained with a smaller number of
components.

206

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

The proposed approach is closely related to a well-known family of approximation techniques
which, essentially, distribute (using some kind of clustering or self-organizing algorithm) detectors over the relevant regions of the input space and then combine their responses for computing
the desired outputs. This is the case of Radial Basis Functions (RBF) (Hertz et al.), the classification and regression trees proposed in (Breiman et al. 1984) and the topological maps used in (Cherkassky & Najafi 1992) to locate the knots required for piecewise linear regression.
A relevant methodology is proposed in (Jordan & Jacobs 1994, Peng et al. 1995), where the
EM algorithm is used to learn hierarchical mixtures of experts in the form of linear rules in such a
way that the desired posterior densities can be explicitly obtained. The properties of the EM algorithm are also satisfactorily used in (Ghahramani & Jordan 1994) to obtain unbiased approximations from missing data in a mixture-based framework similar to ours. Our framework extends this
successful approach by exploiting the conjugate properties of the chosen universal approximation
model: uncertain information of arbitrary complexity can be efficiently processed in the inference
and learning stages.
The MFGN framework is appropriate for a moderated number of variables showing relatively
complex dependencies. In contrast, Bayesian Networks satisfactorily addresses the case of a large
number of variables with clear conditional independence relations. There are situations in which a
certain subset of the variables in a Bayesian Network shows no explicit causal structure. This subdomain could be empirically modeled by a mixture model in order to be considered later as a composite node embedded in the whole network. If the subdomain can be conditionally isolated from
the rest of variables through a set of communication nodes, the MFGN framework can be used to
perform the required inferences.
Finally, mixture models are typically used for unsupervised classification: the examples are
labeled with the index of the component with highest posterior probability. In fact, the MFGN
framework explicitly finds clusters in the training set. Furthermore, continuous and symbolic attributes are allowed in the joint density, so the examples are clustered using an implicit probabilistic metric which automatically weighs all the (heterogeneous) attributes, even with missing and
uncertain values. However, this method is effective only when the groups of interest have the same
structure as the component densities. In order to simplify inference the mixture components have
been selected with constraints (Gaussian, independent variables) which are not necessarily verified
by the natural groups found in real applications.
A tentative possibility (inspired in a common heuristic clustering technique) consists of joining overlapping components (e.g., according to the Battachariya distance, a well-known bound on
the Bayes error used in Statistical Pattern Recognition (Fukunaga 1990)). Unfortunately, our experiments indicate that the overlapping threshold is a free parameter that strongly determines the
quality of the results. A universal threshold, independent of the application, does not seem to exist.
In principle, clusters of arbitrary geometry may be discovered, but this cannot be easily automated.
Therefore, other nonparametric cluster analysis methods (e.g. density valley seeking) are suggested
for labeling complex groups.
5.4 Experimental Evaluation
The MFGN method has been evaluated on standard benchmarks from the Machine Learning database repository at the University of California, Irvine (Merz and Murphy 1996). It contains inductive learning problems which are representative of real world situations. We have experimented
with the following databases: Ionosphere, Pima Indians, Monk's Problems, and Horse Colic, which
illustrate different properties of the proposed methodology. In most cases MFGN has been compared to alternative learning methods with respect to the inference task considered of interest in

207

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

each problem (typically, prediction of a specific attribute given the rest of them). We usually give
the error rate over both the training and the test set to indicate the amount of overfitting obtained by
the learning algorithms.

(a) Ionosphere
(b) Pima Indians
Figure 22. Most discriminant 2D projections of two representative databases.

5.4.1 IONOSPHERE DATABASE
Two classes of radar returns from the ionosphere must be discriminated from vectors of 32 continuous attributes7. There are 351 examples, randomly partitioned into two disjoint subsets of approximately equal size for training and testing. The prevalence of the minoritary class (random
prediction rate) is 36%. Figure 22.a and Table 7 show that this is a typical statistical pattern recognition problem, easily solvable by standard methods. The results suggest that the Bayes (optimum)
error probability is around 5%.
error rate
PE
(training set)
METHOD
(test set)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
2-3 Nearest Neighbor
Parzen Model
Backprop Multilayer Perceptron 2 hidden units
Support Vector Machine, RBF kernel, width 1, (105 s.v.)
Support Vector Machine, RBF kernel, width 3, (35 s.v.)
Support Vector Machine, polinomial kernel, order 2, (41 s.v.)
Support Vector Machine, polinomial kernel, order 3, (45 s.v.)
Support Vector Machine, polinomial kernel, order 4, (42 s.v.)
Full covariance gaussian mixture, 1 component/class
Full covariance gaussian mixture, 2 component/class
Full covariance gaussian mixture, 3 component/class
MFGN 4 components (average)
MFGN 8 components (average)
MFGN 15 components (average)
MFGN, best result by cross-validation (8 components)
Table 7. Ionosphere Database Results

7

.11

.05
.00

.03
.01
.005
.22.15
.11.06
.10.05
.07

Originally the database contains 34 attributes. Two of them, meaningless or ill behaved, were eliminated.

208

.14
.13
.18
.08
.08
.05
.09
.13
.17
.20
.11
.19
.26
.21.08
.13.06
.13.06
.06

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

In this problem, the plain MFGN method, without special heuristics in the learning stage, is
comparable in average to the alternative methods. The best solution on the training set (crossvalidation) is entirely satisfactory.
For the Ionosphere database we also present an exhaustive study of performance given varying
proportions of missing values in the training and testing examples. A value of x % means that in all
training or test examples the value of each attribute is deleted with probability x. The basic experiment consists of learning a MFGN model with the prescribed number of components (4, 8 and 15)
and computing the error rate on the training and test sets. Table 8 shows the mean value  2 standard deviations of the error rates obtained in 10 repetitions of the basic experiment in each configuration. Column M contains the error rate of each configuration over its own training set. The training/test partition is kept fixed to analyze the variability of the solutions due to random initialization
of the EM.
LEARNING
M

0%

INFERENCE
10%
25%

50%

4 COMP. - 0%
8 COMP. - 0%
15 COMP. - 0%

22  15
11  6
10  5

21  8
13  6
13  6

21  8
12  6
13  6

22  8
13  5
13  5

22  9
12  6
13  4

4 COMP. - 10%
8 COMP. - 10%
15 COMP. - 10%

21  14
11  3
10  3

23  11
13  5
12  6

23  11
12  5
12  7

23  11
13  5
12  6

23  11
13  4
12  3

4 COMP. - 25%
8 COMP. - 25%
15 COMP. - 25%

18  7
12  7
95

19  5
14  10
12  9

19  5
14  9
13  11

18  6
15  8
13  9

18  6
14  7
13  7

4 COMP. - 50%
27  18 26  15 27  15 27  14 26  13
8 COMP. - 50%
16  12 21  15 21  15 21  13 20  11
15 COMP. - 50% 13  6
26  17 25  15 25  14 23  13
Table 8. Evaluation of MFGN on Ionosphere Database given
different proportions of missing data in the training and testing subsets.

As expected, the MFGN model is robust with respect to large proportions of missing values in
the test patterns, and to moderated proportions of missing data in the training set. We have compared the above behavior with a standard algorithm for Decision Tree construction inspired in
(Quinlan 1993), which is also able to support missing values8. Table 9 shows the error rates of the
decision trees for the same experimental setting as in Table 8. This kind of Decision Tree obtains
error rates that are better than the averages obtained by MFGN. However, MFGN's best solutions
(selected by cross-validation) are better than the ones obtained by Decision Tree. Furthermore,
Decision Tree performance degrades faster than MFGN, especially with respect to the proportion
of missing values in the inference stage.

8

Essentially, missing values are handled as follows. In the learning stage, when an attribute is selected, examples with
missing values are sent to all the partitions with appropriate weights. In the inference stage, if a node asks for a missing
value, it follows all the branches with appropriate weights and finally the outputs are combined.

209

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

LEARNING
M

0%

INFERENCE
10 %
25 %

50 %

1%
0%
9%
10 %
11 %
12 %
5%
10 %
14 %
15 %
19 %
18 %
6%
25 %
15 %
17 %
17 %
18 %
8%
50 %
17 %
18 %
18 %
19 %
Table 9. Evaluation of basic Decision Tree on Ionosphere Database given different proportions of missing data in the training and testing subsets.

5.4.2 PIMA INDIANS DATABASE
In this problem we must discriminate between two possible results of a diabetes test given to Pima
Indians. There are 8 continuous attributes, and 768 examples, randomly partitioned into two disjoint subsets of equal size for training and testing. The prevalence of the minority class is 35%. The
attribute vector has been normalized. Table 10 presents comparative results.
error rate
PE
(training set)
METHOD
(test set)
Linear MSE (pseudoinverse)
Oblique Decision Tree 8 decision nodes
1-1 Nearest Neighbor
2-3 Nearest Neighbor
Full covariance gaussian mixture, 1 component/class
Full covariance gaussian mixture, 2 component/class
Full covariance gaussian mixture, 3 component/class
Full covariance gaussian mixture, 4 component/class
Backprop Multilayer Perceptron 2 hidden units
Backprop Multilayer Perceptron 4 hidden units
Backprop Multilayer Perceptron 8 hidden units
Support Vector Machine, RBF kernel, width 1 (297 s.v.)
Support Vector Machine, RBF kernel, width 3 (176 s.v.)
Support Vector Machine, polynomial kernel, order 4 (138 s.v.)
Support Vector Machine, polynomial kernel, order 5 (131 s.v.)
MFGN 4 components
MFGN 6 components
MFGN 8 components
Table 10. Pima Indians Database Results

.22
.18

.24
.19
.17
.17
.17
.14
.05

.28
.25
.29

.23
.24
.30
.25
.26
.29
.30
.31
.25
.24
.29
.30
.35
.36
.34
.35
.32
.35

Despite of low dimensionality and large number of examples, this classification problem is
hard (see Figure 22.b). Even sophisticated learners such as backpropagation networks, decision
trees or support vector machines, which are able to store a reasonable proportion of the training set,
do not achieve significant generalization. MFGN shows a similar behavior, although it is slightly
less prone to overfitting (the error rate on the training set is not misleading).
5.4.3 HORSE COLIC DATABASE
This database contains a classification task from a heterogeneous attribute vector including symbolic, discrete and continuous variables, with 30% missing values. It illustrates the problem of

210

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

feature selection in the context of joint modeling, mentioned in Section 5.1. Table 11 shows the
error rates obtained by MFGN using different attribute subsets9. To take advantage of its general
inference properties, the MFGN model must be applied to the attribute subset of interest. If the
inference task is fixed and the number of attributes is very large, alternative methods should be
used.
METHOD

PE

PE

(distribution, 10 initializations)

(best)

6 Selected attributes
MFGN 2 components
MFGN 3 components
MFGN 4 components
MFGN 5 components
MFGN 6 components
MFGN 7 components
MFGN 10 components
MFGN 12 components
MFGN 15 components

.32.00
.20.05
.19.02
.20.02
.19.03
.20.04
.22.04
.19.03
.19.04

.32
.18
.18
.18
.16
.18
.18
.16
.15

.22.01
.21.02
.21.03
.23.02
.21.02
.21.02

.21
.19
.18
.18
.18
.18

.28.02
.29.03
.34.08
.34.06

.25
.25
.25
.28

8 Selected attributes
MFGN 4 components
MFGN 6 components
MFGN 8 components
MFGN 10 components
MFGN 12 components
MFGN 15 components

23 Selected attributes
MFGN 6 components
MFGN 8 components
MFGN 10 components
MFGN 15 components
Table 11. Horse Colic Database Results (random rate = .5)

5.4.4 MONK'S PROBLEMS
The Monk's problems are three concept learning tasks from 6 symbolic attributes, widely used as
benchmarks for inductive learning algorithms (Thrun et al. 1991). As seen in Table 12, MFGN fails
on MONK1 (where acceptable generalization is not obtained) and MONK2 (where the training
examples cannot even be stored). In contrast, MFGN correctly solves MONK3. This behavior is
related to the fact that the MONK's problems are based on deterministic or abstract concepts which
may lack the kind of geometric regularities in the attribute space required by probabilistic models10.
9

Features were individually selected using a simple discrimination index related to the Kolmogorov-Smirnov statistic
(Ruiz 1995).
10
A typical example is the parity problem: acceptable off-training-set generalization cannot be achieved if the inductive
bias of the learning machine is biased towards "smooth" solutions.

211

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Fig. 23 shows the most discriminant 2D projections of the datasets and illustrates the fact that
MONK2 cannot be easily captured by statistic techniques. In this benchmark, MFGN performance
is similar to that of other popular probabilistic methods (Thrun et al. 1991).

(a) MONK1
(b) MONK2
Figure 23. Most Discriminant 2D Projections of the Monk's Datasets.

(c) MONK3

error rate
(training set)

METHOD

PE
(test set)

MONK1 (random rate = .5)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
Support Vector Machine, RBF kernel, width 1 (78 s.v.)
Cascade Correlation
MFGN 4 components
MFGN 8 components

.29

.06
.00

.34
.17
.08
0
.40
.33

MONK2 (random rate  .4)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
Support Vector Machine, RBF kernel, width 1 (117 s.v.)
Cascade Correlation
MFGN 4 components
MFGN 8 components
MFGN 15 components

.40

.31
.26
.14

.37
.19
.20
0
.38
.44
.50

MONK3 (random rate  .5)
Linear MSE (pseudoinverse)
1-1 Nearest Neighbor
Support Vector Machine, RBF kernel, width 1 (69 s.v.)
Cascade Correlation
MFGN 2 components
MFGN 4 components
MFGN 8 components
Table 12. Monk's Problems Results

212

.19

.07
.04
.03

.19
.18
.08
.03
.03
.03
.08

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

5.4.5 COMMENTS
The above experiments demonstrate that the MFGN model is able to obtain acceptable results on
many real world applications. In particular, the error rates obtained in standard classification tasks
are comparable to those obtained by other popular learners. Additionally, MFGN is able to perform
inferences over any other attribute given uncertain or partial information, which is not possible for
most of the alternative methods. This property makes MFGN a very attractive alternative for many
inference problems such as the one illustrated in Example 16. The experiments have also contributed to characterize the kind of problems for which the MFGN model is best suited. Essentially, the
relationship among attributes must be of a true probabilistic nature, and the attribute vector must be
of a moderated size containing "relevant" variables. A previous feature selection / accommodation
stage is recommended in certain applications.

6. Conclusions
We have developed an efficient methodology for probabilistic inference and learning from uncertain information. Under the proposed MFGN framework, the joint probability density function of
the attributes and the likelihood function of the available information are approximated by Mixtures of Factorized Generalized Normals. This mathematical structure allows efficient computation,
without numerical integration, of posterior densities and expectations of the desired variables given
events of arbitrary geometry. An extended version of the EM learning algorithm has been developed to estimate the parameters of the required mixture models from uncertain training examples.
Different paradigms as pattern recognition, regression or pattern completion are subsumed under a
common framework.
A comprehensive collection of examples illustrates the methodology, which has been critically
compared with alternative techniques. The Extended EM algorithm is able to learn satisfactory
domain models from a reasonable number of examples with uncertain values, taking into account
the explicit likelihood functions of the available information. Results are satisfactory whenever the
sample size is large in relation to the amount of (known) degradation of the training set. The experiments also characterized the kind of situations that the model manages better: Domains described by a moderate number of heterogeneous attributes with complex probabilistic dependences,
problems in which the output variables are not necessarily known in the learning stage (i.e. pattern
completion), and, finally, problems in which an explicit management of uncertainty is needed, either in the learning or in the inference stage (or even in both). The MFGN framework has obtained
a very favorable trade-off between useful features and model complexity in the solutions to different applications and benchmarks.
Future developments of our work include improving the learning stage with some heuristic
steps that are combined with the standard E and M steps to control the adequacy of the acquired
models. Additional studies are required on validation tests, generalization, scalability, robustness
and data preprocessing. The essential idea of working with explicit likelihood functions will be
incorporated into the Parzen approximation scheme and we are also interested in more expressive
model structures such as mixtures of factor analyzers, principal component analyzers or linear experts. Finally, the methodology can be developed in a pure Bayesian framework or subsumed under
the Dempster-Shafer Evidence Theory.

213

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Acknowledgments
The authors would like to thank the anonymous reviewers for their careful reading and helpful suggestions. This work has been supported by the Spanish CICYT grants TIC95-1019, TIC97-1343C02-02 and TIC97-0897-C04-03.

References
Berger, J., (1985). Statistical Decision Theory and Bayesian Analysis. Springer-Verlag.
Bernardo, J.M., Smith, A.F.M. (1994). Bayesian Theory. Wiley.
Bouckaert, R.R. (1994). Properties of Bayesian Belief Network Learning Algorithms. Proceedings of Uncertainty in AI, pp. 102-109.
Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J. (1984). Classification and Regression
Trees. Wadsworth International Group, Belmont, CA.
Chang, K. & Fung, R. (1995). Symbolic Probabilistic Inference with Both Discrete and Continuous Variables. IEEE Tran. on Systems, Man, and Cybernetics, Vol. 25, No. 6, june, pp. 910916.
Cherkassky, V. and Lari-Najafi, H. (1992). Nonparametric Regression Analyisis Using SelfOrganizing Topological Maps in H. Wechsler (ed.), Neural Networks for Perception. Vol.2,
Computation, Learning and Architectures, San Diego: Academic Press.
Cohn, D.A., Ghahramani, Z. & Jordan, M.I. (1996). Active Learning with Statistical Models.
Journal of Artificial Intelligence Research 4, pp. 129-145.
Dalal, S.R. & Hall, W.J. (1983). Approximating Priors by Mixtures of Natural Conjugate Priors.
J. R. Statist. Soc. B, Vol. 45, No. 2, pp. 278-286.
De Soete, G. (1993). Using Latent Class Analysis in Categorization Research in I. V. Mechelen,
J. Hampton, R.S. Michalski, P. Theuns (eds.), Categories and Concepts: Theoretical Views
and Inductive Data Analysis, San Diego: Academic Press.
Dempster, A.P., Laird, N.M., Rubin, D.B., (1977). Maximum Likelihood Estimation from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, Series B, Vol. 39:
pp. 1-38.
Duda, R.O. and Hart, P.E. (1973). Pattern Classification and Scene Analysis. John Wiley & Sons.
Fan, C.M., Namazi, N.M. and Penafiel, P.B. (1996). A New Image Motion Estimation Algorithm
Based on the EM Technique. IEEE Transactions on Pattern Analisys and Machine Intelligence, Vol.18, No.3, March, pp. 348-352.
Fukunaga, K. (1990). Introduction to Statistical Pattern Recognition. Academic Press.
Ghahramani, Z. and Jordan, M.I. (1994) Supervised learning from Incomplete data via an EM
approach In Cowan, J.D., Tesauro, G., and Alspector, J. (eds.). Advances in Neural Information Processing Systems 6. Morgan Kauffman
Ghahramani, Z. and Hinton, G.E. (1996) The EM algorithm for mixtures of factor analyzers.
Tech. Rep. Univ. Toronto. CRG-TR-96-1.

214

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

Heckerman, D. & Wellman, M.P. (1995). Bayesian Networks. Communications of the ACM, Vol.
28, No.3, pp. 27-30, March.
Hertz, J., Krogh, A., Palmer, R.G., (1991). Introduction to the Theory of Neural Computation.
Addison Wesley.
Hinton, G.E., Dayan, P. and Revow, M. (1997). Modeling the manifold of images of handwritten
digits. IEEE T. on Neural Networks 8, pp. 65-74.
Hornik, K., Stinchcombe, M., White, H., (1989). Multilayer FeedForward Networks are Universal
Approximators. Neural Networks, No.2.
Hutchinson, A. (1994). Algorithmic Learning. New York: Oxford Univ. Press.
Izenman, A.J. (1991). Recent Developments in Nonparametric Density Estimation. J. Amer. Statist. Assoc. Vol. 86, No. 413, pp. 205-224.
Jordan, M.I., Jacobs, R.A., (1994). Hierarchical Mixtures of Experts and the EM Algorithm.
Neural Computation, 6, pp. 181-214.
Kohonen, T., (1989). Self-Organization and Associative Memory. Springer-Verlag.
Lauritzen, S.L. & Spiegelhalter, D. J. (1988). Local Computations with Probabilities on Graphical
Structures and their Application to Expert Systems. J. R. Statist. Soc. B. 50, No. 2, pp. 157224.
Li, M. and Vitnyi, P. (1993). An Introduction to Kolmogorov Complexity and Its Applications.
New York: Springer-Verlag.
McLachlan, G.J., Basford, K.E., (1988). Mixture Models. New York: Marcel Dekker.
McLachlan, G.J. and Krishnan, T. (1997). The EM Algorithm and Extensions. John Wiley and
Sons.
Michalski, R.S., Carbonell, J. and Mitchell, T.M., eds. (1983). Machine Learning: An Artificial
Intelligence approach. Palo Alto, CA: Tioga Press. Also reprinted by Morgan Kaufmann
(Los Altos, CA).
Michalski, R.S., Carbonell, J. and Mitchell, T.M., eds. (1986). Machine Learning: An Artificial
Intelligence approach, Vol. II. Los Altos, CA: Morgan Kaufmann.
Mohgaddam, B. and Pentland, A. (1997). Probabilistic Visual Learning for Object Representation. IEEE T PAMI, Vol. 19, No.7, 710. pp. 696-.
Merz, C.J. and Murphy, P.M. (1996). UCI Repository of machine learning databases.
[http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: University of California,
Department of Information and Computer Science.
Palm, H.C. (1994). New method for generating statistical classifiers assuming linear mixtures of
Gaussian densities, Proceedings 12th IAPR International Conference on Pattern Recognition (Jerusalem, October 9-13, 1994), vol.2, IEEE, Piscataway, NJ, USA,
Papoulis, A., (1991). Probability, Random Variables and Stochastic Processes. MCGraw-Hill.
Pearl, J., (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference.
Morgan Kaufmann.

215

fiRUIZ, LPEZ-DE-TERUEL & GARRIDO

Peng, F., Jacobs, R.A., Tanner, M.A. (1995). Bayesian Inference in Mixtures-of-Experts and Hierchical Mixtures-of-Experts Models With an Application to Speech Recognition. Accepted
in the Journal of the American Statistical Association.
Priebe, C.E., Marchette, D.J., (1991). Adaptive mixtures: recursive nonparametric pattern recognition. Pattern Recognition, V24 N12, pp. 1197-1209.
Pudil, P., Novovicova, J., Choakjarernwanit, N., Kittler, J. (1995). Feature Selection Based on the
Approximation of Class Densities by Finite Mixtures of Special Type. Pattern Recognition
Vol.28 No.9 pp. 1389-1398.
Quinlan, J.R., (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann.
Redner, R.A., Walker, H.F., (1984). Mixture densities, maximum likelihood estimation and the
EM algorithm. SIAM Review, Vol. 26, pp. 195-239.
Rojas, R. (1996). A Short Proof of the Posterior Probability Property of Classifier Neural Networks. Neural Computation Vol.8 Issue 1, January.
Rumelhart, D.E., Hinton, G. E. and Williams, R. (1986). Learning Internal Representations by
Error Propagation in Rumelhart, McClelland & the PDP Group (1986), pp. 319-362.
Rumelhart, D.E., McClelland, J.L. & the PDP Research Group. (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1, Foundations. Cambrigde
MA, Bradford Books/MIT Press.
Ruiz, A. (1995). A nonparametric bound for the Bayes Error. Pattern Recognition, Vol. 28, No.
6, pp. 921-930.
Ruiz, A., Lpez-de-Teruel, P.E. and Garrido, M.C. (1998). Kernel Density Estimation from Indirect Observations. In preparation.
Sung, K.-K. and Poggio, T. (1998),. "Example Based Learning for View-Based Human Face Detection". IEEE Trans. Pattern Analyisis and Machine Intelligence. Vol.20, N.1. January, pp.
39-51.
Tanner, M.A. (1996). Tools for statistical inference. (3rd ed.). Springer.
Thrun, S. et al (1991). "The MONK's Problems. A performance Comparison of Different Learning
Algorithms". Technical Report CMU-CS-91-197.
Titterington, D.M., A.F.M. Smith and U.E. Makov (1985). Statistical Analysis of Finite Mixture
Distributions, Wiley, New York.
Traven, H.G.C., (1991). A Neural Network Approach to Statistical Pattern Classification by
"Semiparametric" Estimation of Prob. Den. Func.. IEEE T Neural Networks, V2 N3.
Valiant, L.G. (1993). A View of Computational Learning Theory in Meyrowitz and Chipman,
eds. (1993). Foundations of Knowledge Acquisition: Machine Learning. Kluwer Acad. Pub.
Valiveti, R.S., Oommen, B.J., (1992). On using the chi-squared metric for determining stochastic
dependence. Pattern Recognition, V25 N11 pp. 1389-1400.
Vapnik, V.N. (1982). Learning Dependencies based on Empirical Data. Springer, New York.
Vapnik, V.N. (1995). The Nature of Statistical Learning Theory. Springer, New York.

216

fiPROBABILISTIC INFERENCE FROM UNCERTAIN DATA USING MIXTURES

Wan, E.A., (1990). Neural Networks Classification: A Bayesian Interpretation. IEEE Trans. on
Neural Networks, V1 N4.
Weiss, Y. and Adelson E.H. (1995). Perceptually organized EM: A framework for motion segmentation that combines information about form and motion. TR MIT MLPCS TR 315.
Wilson, D.R. and Martinez, T.R. (1997). Improved Heterogeneous Distance Functions. JAIR, V6,
pp. 1-34.
Wolpert, D.H., ed. (1994a). The Mathematics of Generalization. Proc. SFI/CLNS workshop on
Formal Approaches to Supervised Learning.
Xu, L. & Jordan, M.I. (1996). On the Convergence Properties of the EM Algorithm for Gaussian
Mixtures. Neural Computation Vol.8 Issue 1, January.
You, Y.L. and Kaveh, M. (1996). A regularization approach to joint blur identification and image
restoration. IEEE T on Image Processing, vol 5.no.3, pp. 416-428.

217

fi