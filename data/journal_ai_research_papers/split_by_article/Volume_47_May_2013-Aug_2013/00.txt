Journal of Artificial Intelligence Research 47 (2013) 1-34

Submitted 10/2012; published 05/2013

A Feature Subset Selection Algorithm Automatic
Recommendation Method
Guangtao Wang
Qinbao Song
Heli Sun
Xueying Zhang

gt.wang@stu.xjtu.edu.cn
qbsong@mail.xjtu.edu.cn
hlsun@mail.xjtu.edu.cn
zhangxueying.725@stu.xjtu.edu.cn

Department of Computer Science & Technology,
Xian Jiaotong University, 710049, China

Baowen Xu
Yuming Zhou

bwxu@nju.edu.cn
zhouyuming@nju.edu.cn

Department of Computer Science & Technology,
Nanjing University, China

Abstract
Many feature subset selection (FSS) algorithms have been proposed, but not all of them
are appropriate for a given feature selection problem. At the same time, so far there is
rarely a good way to choose appropriate FSS algorithms for the problem at hand. Thus,
FSS algorithm automatic recommendation is very important and practically useful. In
this paper, a meta learning based FSS algorithm automatic recommendation method is
presented. The proposed method first identifies the data sets that are most similar to the
one at hand by the k -nearest neighbor classification algorithm, and the distances among
these data sets are calculated based on the commonly-used data set characteristics. Then,
it ranks all the candidate FSS algorithms according to their performance on these similar
data sets, and chooses the algorithms with best performance as the appropriate ones.
The performance of the candidate FSS algorithms is evaluated by a multi-criteria metric
that takes into account not only the classification accuracy over the selected features, but
also the runtime of feature selection and the number of selected features. The proposed
recommendation method is extensively tested on 115 real world data sets with 22 wellknown and frequently-used different FSS algorithms for five representative classifiers. The
results show the effectiveness of our proposed FSS algorithm recommendation method.

1. Introduction
Feature subset selection (FSS) plays an important role in the fields of data mining and
machine learning. A good FSS algorithm can effectively remove irrelevant and redundant
features and take into account feature interaction. This not only leads up to an insight
understanding of the data, but also improves the performance of a learner by enhancing the
generalization capacity and the interpretability of the learning model (Pudil, Novovicova,
Somol, & Vrnata, 1998a; Pudil, Novovicova, Somol, & Vrnata, 1998b; Molina, Belanche,
& Nebot, 2002; Guyon & Elisseeff, 2003; Saeys, Inza, & Larranaga, 2007; Liu & Yu, 2005;
Liu, Motoda, Setiono, & Zhao, 2010).
c
2013
AI Access Foundation. All rights reserved.

fiWang, Song, Sun, Zhang, Xu & Zhou

Although a large number of FSS algorithms have been proposed, there is no single
algorithm which performs uniformly well on all feature selection problems. Experiments
(Hall, 1999; Zhao & Liu, 2007) have confirmed that there could exist significant differences
of performance (e.g., classification accuracy) among different FSS algorithms over a given
data set. That means for a given data set, some FSS algorithms outperform others.
This raises a practical and very important question: which FSS algorithms should be
picked up for a given data set? The common solution is to apply all candidate FSS algorithms to the given data set, and choose one with the best performance by the crossvalidation strategy. However, this solution is quite time-consuming especially for highdimensional data (Brodley, 1993).
For the purpose of addressing this problem in a more efficient way, in this paper, an FSS
algorithm automatic recommendation method is proposed. The assumption underlying our
proposed method is that the performance of an FSS algorithm on a data set is related to
the characteristics of the data set. The rationality of this assumption can be demonstrated
as follows:
1) Generally, when a new FSS algorithm is proposed, its performance needs to be extensively evaluated at least on real world data sets. However, the published FSS algorithms
are rarely tested on the identical group of data sets (Hall, 1999; Zhao & Liu, 2007; Yu &
Liu, 2003; Dash & Liu, 2003; Kononenko, 1994). That is, for any two algorithms, they
are usually tested on the different data. This implies that the performance of an FSS
algorithm biases to some data sets.
2) At the same time, the famous NFL (No Free Lunch) (Wolpert, 2001) theory tells us
that, for a particular data set, different algorithms have different data-conditioned performance, and the performance differences vary with data sets.
The above evidences imply that there is a relationship between the performance of an
FSS algorithm and the characteristics of data sets. In this paper, we intend to explore this
relationship and utilize it to automatically recommend appropriate FSS algorithm(s) for
a given data set. The recommendation process can be viewed as a specific application of
meta-learning (Vilalta & Drissi, 2002; Brazdil, Carrier, Soares, & Vilalta, 2008) that has
been used to recommend algorithms for classification problems (Ali & Smith, 2006; King,
Feng, & Sutherland, 1995; Brazdil, Soares, & Da Costa, 2003; Kalousis, Gama, & Hilario,
2004; Smith-Miles, 2008; Song, Wang, & Wang, 2012).
To model this relationship, there are three fundamental issues to be considered: i) which
features (often are referred to as meta-features) are used to characterize a data set; ii) how
to evaluate the performance of an FSS algorithm and identify the applicable one(s) for a
given data set; iii) how to recommend FSS algorithm(s) for a new data set.
In this paper, the meta-features, which are frequently used in meta-learning (Vilalta &
Drissi, 2002; Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Castiello, Castellano,
& Fanelli, 2005), are employed to characterize data sets. At the same time, a multi-criteria
metric, which takes into account not only the classification accuracy of a classifier with an
FSS algorithm but also the runtime of feature selection and the number of selected features,
is used to evaluate the performance of the FSS algorithm. Meanwhile, a k -NN (k -Nearest
Neighbor) based method is proposed to recommend FSS algorithm(s) for a new data set.
2

fiSubset Selection Algorithm Automatic Recommendation

Our proposed FSS algorithm recommendation method has been extensively tested on
115 real world data sets with 22 well-known and frequently-used different FSS algorithms
for five representative classifiers. The results show the effectiveness of our proposed recommendation method.
The rest of this paper is organized as follows. Section 2 introduces the preliminaries.
Section 3 describes our proposed FSS algorithm recommendation method. Section 4 provides the experimental results. Section 5 conducts the sensitivity analysis of the number
of the nearest data sets on the recommendation results. Finally, section 6 summarizes the
work and draws some conclusions.

2. Preliminaries
In this section, we first describe the meta-features used to characterize data sets. Then,
we introduce the multi-criteria evaluation metric used to measure the performance of FSS
algorithms.
2.1 Meta-features of Data Sets
Our proposed FSS algorithm recommendation method is based on the relationship between
the performance of FSS algorithms and the meta-features of data sets.
The recommendation can be viewed as a data mining problem, where the performance
of FSS algorithms and the meta-features are the target function and the input variables,
respectively. Due to the ubiquity of Garbage In, Garbage Out (Lee, Lu, Ling, & Ko, 1999)
in the field of data mining, the selection of the meta-features is crucial for our proposed
FSS recommendation method.
The meta-features are measures that are extracted from data sets and can be used to
uniformly characterize different data sets, where the underlying properties are reflected. The
meta-features should be not only conveniently and efficiently calculated, but also related to
the performance of machine learning algorithms (Castiello et al., 2005).
There has been 15 years of research to study and improve on the meta-features proposed
in the StatLog project (Michie, Spiegelhalter, & Taylor, 1994). A number of meta-features
have been employed to characterize data sets (Brazdil et al., 2003; Castiello et al., 2005;
Michie et al., 1994; Engels & Theusinger, 1998; Gama & Brazdil, 1995; Lindner & Studer,
1999; Sohn, 1999), and have been demonstrated working well in modeling the relationship
between the characteristics of data sets and the performance (e.g., classification accuracy) of
learning algorithms (Ali & Smith, 2006; King et al., 1995; Brazdil et al., 2003; Kalousis et al.,
2004; Smith-Miles, 2008). As these meta-features do characterize data sets themselves, and
have no connection with learning algorithms and their types, so we use them to model the
relationship between data sets and FSS algorithms.
The most commonly used meta-features are established focusing on the following three
aspects of a data set: i) general properties, ii) statistic-based properties, and iii) informationtheoretic based properties (Castiello et al., 2005). Table 11 shows the details.
1. In order to compute the information-theoretic features, for data sets with continuous-valued features, if
needed, the well-known MDL (Minimum Description Length) method with Fayyad & Irani criterion was
used to discretize the continuous values.

3

fiWang, Song, Sun, Zhang, Xu & Zhou

Category
General properties

Statistical based properties

Information-theoretic properties

Notation
I
F
T
D
(X, Y )
Skew(X)
Kurt(X)
H(C)norm
H(X)norm
M I(C, X)
M I(C, X)max
ENattr
N Sratio

Measure description
Number of instances
Number of features
Number of target concept values
Data set dimensionality, D = I/F
Mean absolute linear correlation coefficient of all possible pairs of features
Mean skewness
Mean kurtosis
Normalized class entropy
Mean normalized feature entropy
Mean mutual information of class and attribute
Maximum mutual information of class and attribute
Equivalent number of features, ENattr = H(C)/M I(C, X)
Noise-signal ratio, N Sratio = (H(X)  M I(C, X))/M I(C, X)

Table 1: Meta-features used to characterize a data set
2.2 Multi-criteria Metric for FSS Algorithm Evaluation
In this section, first, the classical metrics evaluating the performance of FSS algorithm are
introduced. Then, by analyzing the user requirements in practice application, based on
these metrics, a new and user-oriented multi-criteria metric is proposed for FSS algorithm
evaluation by combining these metrics together.
2.2.1 Classical Performance Metrics
When evaluating the performance of an FSS algorithm, the following three metrics are
extensively used in feature selection literature: i) classification accuracy , ii) runtime of
feature selection, and iii) number of selected features.
1) The classification accuracy (acc) of a classifier with the selected features can be used to
measure how well the selected features describe a classification problem. This is because
for a given data set, different feature subsets generally result in different classification
accuracies. Thus, it is reasonable that the feature subset with higher classification accuracy has stronger capability of depicting the classification problem. The classification
accuracy also reflects the ability of an FSS algorithm in identifying the salient features
for learning.
2) The runtime (t) of feature selection measures the efficiency of an FSS algorithm for
picking up the useful features. It is also viewed as a metric to measure the cost of feature
selection. The longer the runtime, the higher the expenditure of feature selection.
3) The number of selected features (n) measures the simplicity of the feature selection
results. If the classification accuracies with two FSS algorithms are similar, we usually
favor the algorithm with fewer features.
Feature subset selection aims to improve the performance of learning algorithms which
usually is measured with classification accuracy. The FSS algorithms with higher classification accuracy are in favor. However, this does not mean that the runtime and the
number of selected features could be ignored. This can be explained by the following two
considerations:
1) Suppose there are two different FSS algorithms Ai and Aj , and a given data set D. If
the classification accuracy with Ai on D is slightly greater than that with Aj , but the
4

fiSubset Selection Algorithm Automatic Recommendation

runtime of Ai and the number of features selected by Ai are much greater than those of
Aj , then Aj is often chosen.
2) Usually, we do not prefer to use the algorithms with higher accuracy but longer runtime,
so is those with lower accuracy but shorter runtime. Therefore, we need a tradeoff between classification accuracy and the runtime of feature selection/the number of selected
features. For example, in real-time systems, it is impossible to choose the algorithm with
high time-consumption even if its classification accuracy is high.
Therefore, it is necessary to allow users making a user-oriented performance evaluation
for different FSS algorithms. For this purpose, it is needed to address the problem of how
to integrate classification accuracy with the runtime of feature selection and the number of
selected features to obtain a unified metric. In this paper, we resort to the multi-criteria
metric to explore this problem. The underlying reason lies that the multi-criteria metric
has been successfully used to evaluate data mining algorithms by considering the positive
properties (e.g. classification accuracy) and the negative ones (e.g. runtime and number of
selected features) simultaneously (Nakhaeizadeh & Schnabl, 1997, 1998).
When comparing two algorithms, besides the metrics used to evaluate their performance,
the ratio of the metric values can also be used. For example, suppose A1 and A2 are two
different FSS algorithms, if A1 is better than A2 in terms of classification accuracy, i.e.,
acc1 > acc2 2 , then ratio acc1 /acc2 > 1 can be used to show A1 is better than A2 as well.
On the contrary, for the negative metrics runtime of feature selection and number of of the
selected features, the corresponding ratio < 1 means a better algorithm.
Actually, a multi-criteria metric adjusted ratio of ratios (ARR) (Brazdil et al., 2003),
which combines classification accuracy and runtime together as a unified metric, has been
proposed to evaluate the performance of a learning algorithm. We extend ARR by integrating it with the runtime of feature selection and the number of selected features, so a new
multi-criteria metric EARR (extened ARR) is proposed. In the following discussion, we will
show that the new metric EARR is more inclusive, very flexible, and easy to understand.
2.2.2 Multi-Criteria Metric EARR
Let DSet = {D1 , D2 ,    , DN } be a set of N data sets, and ASet = {A1 , A2 ,    , AM } be a
set of M FSS algorithms. Suppose accji is the classification accuracy of a classifier with FSS
algorithm Ai on data set Dj (1  i  M , 1  j  N ), and tji and nji denote the runtime
and the number of selected features of FSS algorithm Ai on data set Dj , respectively. Then
the EARR of Ai to Aj over Dk is defined as
k
EARRD
Ai ,Aj =

accki /acckj
1 +   log (tki /tkj ) +   log (nki /nkj )

(1  i 6= j  M, 1  k  N ),

(1)

where  and  are the user-predefined parameters which denote the relative importance of
the runtime of feature selection and the number of selected features, respectively.
The computation of the metric EARR is based on the ratios of the classical FSS algorithm performance metrics, the classification accuracy, the runtime of feature selection and
2. Where acc1 and acc2 are the corresponding classification accuracies of algorithms A1 and A2 , respectively.

5

fiWang, Song, Sun, Zhang, Xu & Zhou

the number of selected features. From the definition we can know that EARR evaluates an
FSS algorithm by comparing it with another algorithm. This is reasonable since it is more
objective to assert an algorithm is good or not by comparing it with another one instead of
just focusing on its own performance. For example, suppose there is a classifier with 70%
classification accuracy on a data set, we will get confused on whether the classifier is good
or not. However, if we compare it with another classifier that can obtain 90% classification
accuracy on the same data set, then we can definitely say that the first classifier is not good
compared with the second one.
Noted that, in practice, the runtime difference between two different FSS algorithms
usually can be quite great. Meanwhile, for high-dimensional data sets, the difference of the
number of selected features for two different FSS algorithms can be great as well. Thus,
the ratio of runtime and the ratio of the number of selected features usually have much
more wide ranges than that of the classification accuracy. If the simple ratio of runtime and
the simple ratio of the number of selected features are employed, they would dominate the
value of EARR, and the ratio of the classification accuracy will be drowned. In order to
avoid this situation, the common logarithm (i.e., the logarithm with base 10) of the ratio
of runtime and the common logarithm of the ratio of the number of selected features are
employed.
The parameters  and  represent the amount of classification accuracy that a user is
willing to trade for a 10 times speedup/reduction on the runtime of feature selection/the
number of selected features, respectively. This allows users to choose the algorithms with
shorter runtime and less features but acceptable accuracy. This can be illustrated by the
following example. Suppose that accki = (1 +  + )  acckj , the runtime of algorithm Ai on
a given data set is 10 times of that of Aj (i.e., tki = 10  tkj ), and the number of selected
features of algorithm Ai is 10 times of that of Aj (i.e., nki = 10  nkj ). Then, according to
Dk
1
3
k
Eq. (1), EARR D
Ai ,Aj = 1, and EARR Aj ,Ai = 1(+)2 > 1. In this case, Aj outperforms
Ai . If a user prefers fast algorithms with less features, Aj will be his/her choice.
k
The value of EARR varies around 1. The value of EARR D
Ai ,Aj is greater than (or equal
k
to, or smaller than) that of EARR D
Aj ,Ai indicates that Ai is better than (or equal to, or
worse than) Aj .
Eq. (1) can be directly used to evaluate the performance of two different FSS algorithms.
When comparing multiple FSS algorithms, the performance of any algorithm Ai  ASet on
a given data set D can be evaluated by the metric EARR D
Ai defined as follows:

EARRD
Ai =

1
M 1

M
X

EARRD
Ai ,Aj .

(2)

j=1j6=i

This equation shows that the EARR of an FSS algorithm Ai on D is the arithmetic
mean of the EARRD
Ai ,Aj of Ai to other algorithm Aj on D. That is, the performance of any
FSS algorithm Ai  ASet is evaluated based on the comparisons with other algorithms in
{ASet {Ai }}. The larger the value of EARR, the better the corresponding FSS algorithm
on the given data set D.
3. Since log (x/y) =  log (y/x) and ( + )2 > 0

6

fiSubset Selection Algorithm Automatic Recommendation

3. FSS Algorithm Recommendation Method
In this section, we first give the framework of the FSS algorithm recommendation. Then,
we introduce the nearest neighbor based recommendation method in detail.
3.1 Framework
Based on the assumption that there is a relationship between the performance of an FSS
algorithm on a given data set and the data set characteristics (aka meta-features), our
proposed recommendation method firstly constructs a meta-knowledge database consisting
of data set meta-features and FSS algorithm performance. After that, with the help of
the meta-knowledge database, a k-NN based method is used to model this relationship and
recommend appropriate FSS algorithms for a new data set.
Therefore, our proposed recommendation method consists of two parts: Meta-knowledge
Database Construction and FSS Algorithm Recommendation. Fig. 1 shows the details.
Meta-knowledge database construction
Feature selection
algorithms

Historical
data sets

Performance metric
aquirement
Meta-features
extraction

Performance metrics

Meta-features

FSS algorithm recommendation

New data set
Recommended
FSS algorithms

Metaknowledge
database
Performance
metrics

Meta-features

Meta-features
extraction

Meta-features

Nearest data sets
identification

Top r algorithms
recommendation

Ranks

FSS algorithms
ranking

Nearest
data sets

Metric
collection

Performance
metrics

Figure 1: Framework of feature subset selection algorithm recommendation
1) Meta-Knowledge Database Construction
As mentioned previously, the meta-knowledge database consists of the meta-features of
a set of historical data sets and the performance of a group of FSS algorithms on them.
It is the foundation of our proposed recommendation method, and the effectiveness of the
recommendations depends heavily on this database.
The meta-knowledge database is constructed by the following three steps. Firstly, the
meta-features in Table 1 are extracted from each historical data set by the module Metafeatures extraction. Then, each candidate FSS algorithm is applied on each historical data
set. The classification accuracy, the runtime of feature selection and the number of selected
features are recorded, and the corresponding value of the performance metric EARR is
calculated. This is accomplished by the module Performance metric calculation. Finally,
for each data set, a tuple, which is composed of the meta-features and the values of the
performance metric EARR for all the candidate FSS algorithms, is obtained and added into
the knowledge database.
2) FSS Algorithm Recommendation
7

fiWang, Song, Sun, Zhang, Xu & Zhou

Based on the introduction of the first part Meta-knowledge Database Construction
we presented above, the learning target of the meta-knowledge data is a set of EARR values
instead of an appropriate FSS algorithm. In this case, it has been demonstrated that the
researchers usually resort to the instance-based or k -NN (nearest neighbors) methods or
their variations (Brazdil et al., 2003, 2008) for algorithm recommendation. Thus, a k -NN
based FSS algorithm recommendation procedure is proposed.
When recommending FSS algorithms for a new data set, firstly, the meta-features of
this data set are extracted. Then, the distance between the new data set and each historical
data set is calculated according to the meta-features. After that, its k nearest data sets
are identified, and the EARR values of the candidate FSS algorithms on these k data sets
are retrieved from the meta-knowledge database. Finally, all the candidate FSS algorithms
are ranked according to these EARR values, where the algorithm with the highest EARR
achieves the top rank, the one with the second highest EARR gets second rank, and so
forth, and the top r algorithms are recommended.
3.2 Recommendation Method
To recommend appropriate FSS algorithms for a new data set Dnew based on its k nearest
data sets, there are two foundational issues to be solved: i) how to identify its k nearest
data sets, and ii) how to recommend appropriate FSS algorithms based on these k data
sets.
1) k nearest data sets identification
The k nearest data sets of Dnew are identified by calculating the distance between Dnew
and each historical data set based on their meta-features. The smaller the distance, the
more similar the corresponding data set to Dnew .
In order to effectively calculate the distance between two data sets, the L1 norm distance
(Atkeson, Moore, & Schaal, 1997) is adopted since it is easy to understand and calculate,
and its ability in measuring the similarity between two data sets has been demonstrated by
Brazdil et al. (2003).
Let Fi = <fi,1 , fi,2 ,    , fi,h > be the meta-features of data set Di , where fi,p is the
value of pth feature of Fi and h is the length of the meta-features. The L1 norm distance
between data sets Di and Dj can be formulated as
dist(Di , Dj ) = kFi  Fj k1 =

h
X

|fi,p  fj,p |.

(3)

p=1

It is worth noting that the ranges of different meta-features are quite different. For example,
of the meta-features introduced in Table 1, the value of normalized class entropy varies from
0 to 1, while the number of instances can be millions. Thus, if these meta-features with
different ranges are directly used to calculate the distance between two data sets, the metafeatures with large range would dominate the distance, and the meta-features with small
range will be ignored. In order to avoid this problem, the 0-1 standardized method (Eq.
(4)) is employed to make all the meta-features have the same range [0, 1].
fi,p  min (f,p )
,
max (f,p )  min (f,p )
8

(4)

fiSubset Selection Algorithm Automatic Recommendation

where fi,p (1  p  h) is the value of the pth meta-feature of data set Di , min (f,p ) and
max (f,p ) denote the minimum and maximum values of the pth meta-feature over historical
data sets, respectively.
2) FSS algorithm recommendation
Once getting the k nearest data sets of Dnew , the performance of the candidate FSS
algorithms on Dnew is evaluated according to those on the k nearest data sets. Then, the
algorithms with the best performance are recommended.
D
Let Dknn = {D1 , D2 ,    , Dk } be the k nearest data sets of Dnew and EARR Aij be the
performance metric of the FSS algorithm Ai on data set Dj  Dknn (1  j  k). Then the
performance of Ai on the new data set Dnew can be evaluated by
knn
EARRD
Ai

=

k
X

j 

D
EARRAij ,

where j = dj

1

k
X
dt 1 , dj = dist(Dnew , Dj ).
/

(5)

t=1

j=1

Eq. (5) indicates that the performance of the FSS algorithm Ai on Dnew is evaluated by
its performance on the Dknn of Dnew . For a data set Dj  Dknn , the smaller the distance
dj between itself and Dnew , the more similar the two data sets. This means for two data
sets Dp and Dq , if dp < dq then the data set Dp is more similar to Dnew , so the EARR
of Ai on Dp is more important for evaluating the performance of Ai on Dnew . Thus, the
weighted average, which takes into account the relative importance of each data set in Dknn
rather than treating each data set equally, is employed. Moreover, in the domain of machine
learning, the reciprocal of the distance usually is used to measure the similarity. So the
k
P
j = dj 1 / dt 1 is used as the weight of the EARR of Ai on Dj  Dknn .
t=1

According to the EARR of each candidate FSS algorithm in ASet on Dnew , a rank of
these candidate FSS algorithms can be obtained. The greater the EARR, the higher the
rank. Then, the top r (e.g., r = 3 in this paper) FSS algorithms are picked up as the
appropriate ones for the new data set Dnew .
Procedure FSSAlgorithmRecommendation shows the pseudo-code of the recommendation.
Time complexity. The recommendation procedure consists of two parts. In the first part
(lines 1-3), the k nearest data sets of the given new data set D are identified. Firstly,
the meta-features F of D are extracted by function MetaFeatureExtraction(). Then, the
k-nearest historical data sets are identified by function NeighborRecognition() based on
the distance between F and the meta-features Fi of each historical data set Di . Suppose
that P is the number of instances and Q is the number of features in the given data set
D, the time complexity of function MetaFeatureExtraction() is O(P + Q). For function
NeighborRecognition(), the time complexity is O(n) which depends on the number of the
historical data sets n. Consequently, the time complexity of the first part is O(P +Q)+O(n).
In the second part (lines 4-8), the r FSS algorithms are recommended for the data set
D. Since the weights and EARRs of the k nearest data sets can be obtained directly, the
time complexity of these two steps is O(1). The time complexity for estimating and ranking
the EARRs of the algorithms in ASet is O(k  m) + O(m  log(m)), where k is the preassigned
number of the nearest data sets and m is the number of the candidate FSS algorithms.
9

fiWang, Song, Sun, Zhang, Xu & Zhou

Procedure FSSAlgorithmRecommendation
Inputs :
D - a new given data set;
DSet - {D1 , D2 ,    , Dn }, historical data sets;
ASet - {A1 , A2 ,    , Am }, candidate FSS algorithms;
MetaDataBase - {<Fi , EARRsi >|1  i  n} where Fi and EARRs i are the
meta-features and the EARRs of ASet on Di , respectively;
k - the predefined number of the nearest neighbors;
r - the number of recommended FSS algorithms.
Output: RecAlgs - Recommended FSS algorithms for D
//Part 1: Recognition of the k nearest data sets for D
1 F = MetaFeatureExtraction (D);//Extract meta-features from D
2 MetaFeatureSet = {F1 , F2 ,    , Fn };//Meta-feature of each data set in DSet
3 Neighbors = NeighborRecognition (k, F, MetaFeatureSet);
//Part 2: Appropriate FSS algorithm recommendation
4 WeightSet = calculate the weight for each data set in Neighbors //See Eq. (5)
5 EARRSet = the corresponding EARRs for each data set in Neighbors from MetaDataBase;
6 Estimate the EARR of each FSS algorithm  ASet on D according to WeightSet and EARRSet

by Eq. (5) and rank the algorithms in ASet based on these EARRs;
7 RecAlgs = top r FSS algorithms;
8 return RecAlgs;

To sum up, the time complexity of the recommendation procedure is O(P + Q) + O(n) +
O(km)+O(mlog(m)). In practice, for a data set D that needs to conduct feature selection,
the number of the instances P and/or the number of the features Q in D are much greater
than the number of the nearest data sets k and the number of the candidate FSS algorithms
m, so the major time consumption of this recommendation procedure is determined by the
first part.

4. Experimental Results and Analysis
In this section, we experimentally evaluate the proposed feature subset selection (FSS)
algorithm recommendation method by recommending algorithms over the benchmark data
sets.
4.1 Benchmark Data Sets
115 extensively-used real world data sets, which come from different areas such as Computer,
Image, Life, Biology, Physical and Text 4 , are employed in our experiment. The sizes of
these data sets vary from 10 to 24863 instances, and the numbers of their features are
between 5 and 27680.
The statistical summary of these data sets is shown in Table 2 in terms of the number
of instances (denoted as I), the number of features (denoted as F) and the number of target
concepts (denoted as T).
4. These data sets are available from http://archive.ics.uci.edu/ml/datasets.html, http://
featureselection.asu.edu/datasets.php, http://sci2s.ugr.es/keel/datasets.php, http://www.
upo.es/eps/bigs/datasets.html, and http://tunedit.org/repo/Data, respectively.

10

fiSubset Selection Algorithm Automatic Recommendation

Data ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58

Data Name
ada agnostic
ada prior
anneal
anneal ORIG
AR10P 130 674
arrhythmia
audiology
autos
balance-scale
breast-cancer
breast-w
bridges version1
bridges version2
car
CLL-SUB-111 111 2856
cmc
colic
colic.ORIG
colon
credit-a
credit-g
cylinder-bands
dermatology
diabetes
ECML90x27679
ecoli
Embryonaldataset C
eucalyptus
flags
GCM Test
gina agnostic
gina prior
gina prior2
glass
grub-damage
heart-c
heart-h
heart-statlog
hepatitis
hypothyroid
ionosphere
iris
kdd ipums la 97-small
kdd ipums la 98-small
kdd ipums la 99-small
kdd JapaneseVowels test
kdd JapaneseVowels train
kdd synthetic control
kr-vs-kp
labor
Leukemia
Leukemia 3c
leukemia test 34x7129
leukemia train 38x7129
lung-cancer
lymph
Lymphoma45x4026+2classes
Lymphoma96x4026+10classes

I
4562
4562
898
898
130
452
226
205
625
286
699
107
107
1728
111
1473
368
368
62
690
1000
540
366
768
90
336
60
24863
194
46
3468
3468
3468
214
155
303
294
270
155
3772
351
150
7019
7485
8844
5687
4274
600
3196
57
72
72
34
38
32
148
45
96

F
49
15
39
39
675
280
70
26
5
10
10
13
13
7
2857
10
23
28
2001
16
21
40
35
9
27680
8
7130
249
30
16064
971
785
785
10
9
14
14
14
20
30
35
5
61
61
61
15
15
62
37
17
7130
7130
7130
7130
57
19
4027
4027

T
2
2
6
6
10
16
24
7
3
2
2
6
6
4
3
3
2
2
2
2
2
2
6
2
43
8
2
12
8
14
2
2
10
7
4
5
5
2
2
4
2
3
9
10
9
9
9
6
2
2
2
3
2
2
3
4
2
11

Data ID
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115

Data Name
Lymphoma96x4026+9classes
mfeat-fourier
mfeat-morphological
mfeat-pixel
mfeat-zernike
molecular-biology promoters
monks-problems-1 test
monks-problems-1 train
monks-problems-2 test
monks-problems-2 train
monks-problems-3 test
monks-problems-3 train
mushroom
oh0.wc
oh10.wc
oh15.wc
oh5.wc
pasture
pendigits
PIE10P 210 1520
postoperative-patient-data
primary-tumor
segment
shuttle-landing-control
sick
SMK-CAN-187 187 1815
solar-flare 1
solar-flare 2
sonar
soybean
spectf test
spectf train
spectrometer
spect test
spect train
splice
sponge
squash-stored
squash-unstored
sylva agnostic
sylva prior
TOX-171 171 1538
tr11.wc
tr12.wc
tr23.wc
tr31.wc
tr41.wc
tr45.wc
trains
vehicle
vote
vowel
wap.wc
waveform-5000
white-clover
wine
zoo

I
96
2000
2000
2000
2000
106
432
124
432
169
432
122
8124
1003
1050
913
918
36
10992
210
90
339
2310
15
3772
187
323
1066
208
683
269
80
531
187
80
3190
76
52
52
14395
14395
171
414
313
204
927
878
690
10
846
435
990
1560
5000
63
178
101

F
4027
77
7
241
48
59
7
7
7
7
7
7
23
3183
3239
3101
3013
23
17
1521
9
18
20
7
30
1816
13
13
61
36
45
45
103
23
23
62
46
25
24
217
109
1538
6430
5805
5833
10129
7455
8262
33
19
17
14
8461
41
32
14
18

T
9
10
10
10
10
2
2
2
2
2
2
2
2
10
10
10
10
3
10
10
3
22
7
2
2
2
2
3
2
19
2
2
48
2
2
3
3
3
3
2
2
4
9
8
6
7
10
10
2
4
2
11
20
3
4
3
7

Table 2: Statistical summary of the 115 data sets

4.2 Experimental Setup
In order to evaluate the performance of the proposed FSS algorithm recommendation
method, further verify whether the proposed method is potentially useful in practice, and
confirm the reproducibility of our experiments, we set the experimental study as follows.
4.2.1 FSS Algorithms
FSS algorithms can be grouped into two broad categories: Wrapper and Filter (Molina et al.,
2002; Kohavi & John, 1997). The Wrapper method uses the error rate of the classification
algorithm as the evaluation function to measure a feature subset, while the evaluation
function of the Filter method is independent of the classification algorithm. The accuracy
of the Wrapper method is usually high; however, the generality of the result is limited,
and the computational complexity is high. In comparison, Filter method is of generality,
and the computational complexity is low. Due to the fact that the Wrapper method is
computationally expensive (Dash & Liu, 1997), the Filter method is usually a good choice
11

fiWang, Song, Sun, Zhang, Xu & Zhou

when the number of features is very large. Thus, we focus on the Filter method in our
experiment.
A number of Filter based FSS algorithms have been proposed to handle feature selection
problems. These algorithms can be significantly distinguished by i) the search method used
to generate the feature subset being evaluated, and ii) the evaluation measures used to assess
the feature subset (Liu & Yu, 2005; de Souza, 2004; Dash & Liu, 1997; Pudil, Novovicova,
& Kittler, 1994).
In order to guarantee the generality of our experimental results, twelve well-known or
the latest search methods and four representative evaluation measures are employed. The
brief introduction of these search methods and evaluation measures is as follows.
1) Search methods
i) Sequential forward search (SFS): Starting from the empty set, sequentially add the
feature which results in the highest value of objective function into the current
feature subset.
ii) Sequential backward search (SBS): Starting from the full set, sequentially eliminate
the feature which results in smallest or no decrease in the value of objective function
from the current feature subset.
iii) Bi-direction search (BiS): A parallel implementation of SFS and SBS. It searches
the feature subset space in two directions.
iv) Genetic search (GS): A randomized search method which performs using a simple
genetic algorithm (Goldberg, 1989). The genetic algorithm finds the feature subset
to maximize special output function using techniques inspired by natural evolution.
v) Linear search (LS): An extension of BestFirst search (Gutlein, Frank, Hall, & Karwath, 2009) which searches the space of feature subsets by greedy hill-climbing
augmented with a backtracking facility.
vi) Rank search (RS) (Battiti, 1994): It uses a feature evaluator (such as gain ratio)
to rank all the features. After a feature evaluator is specified, a forward selection
search is used to generate a ranking list.
vii) Scatter search (SS) (Garcia Lopez, Garcia Torres, Melian Batista, Moreno Perez,
& Moreno-Vega, 2006): This method performs a scatter search through the feature
subset space. It starts with a population of many significant and diverse feature
subsets, and stops when the assessment criteria is higher than a given threshold or
does not have improvement any longer.
viii) Stepwise search (SWS) (Kohavi & John, 1997): A variation of the forward search.
At each step in the search process, after a new feature is added, a test is performed
to check if some features can be eliminated without significant reduction in the
output function.
ix) Tabu search (TS) (Hedar, Wang, & Fukushima, 2008): It is proposed for combinatorial optimization problems. It is an adaptive memory and responsive exploration
by combining a local search process with anti-cycling memory-based rules to avoid
trapping in local optimal solutions.
12

fiSubset Selection Algorithm Automatic Recommendation

x) Interactive search (Zhao & Liu, 2007): It traverses the feature subset space for
maximizing the target function while taking consideration the interaction among
features.
xi) FCBF search (Yu & Liu, 2003): It evaluates features via the relevance and redundancy analysis, and uses the analysis results as guideline to choose features.
xii) Ranker (Kononenko, 1994; Kira & Rendell, 1992; Liu & Setiono, 1995): It evaluates
each feature individually and ranks the features by the values of their evaluation
metrics.
2) Evaluation measures
i) Consistency (Liu & Setiono, 1996; Zhao & Liu, 2007): This kind of measure evaluates the worth of a feature subset by the level of consistency in the target concept
when the instances are projected onto the feature subset. The consistency of any
feature subset can never be lower than that of the full feature set.
ii) Dependency (Hall, 1999; Yu & Liu, 2003): This kind of measure evaluates the worth
of a subset of features by considering the individual predictive ability of each feature
along with the degree of redundancy among these features. The FSS methods based
on this kind of measure assume that good feature subsets contain features closely
correlated with the target concept, but uncorrelated with each other.
iii) Distance (Kira & Rendell, 1992; Kononenko, 1994): This kind of measure is proposed based on the assumption that the distance of instances from different target
concepts is greater than that from same target concepts.
iv) Probabilistic significance (Zhou & Dillon, 1988; Liu & Setiono, 1995): This measure
evaluates the worth of a feature by calculating the probabilistic significance as a
two-way function, i.e., the association between feature and the target concept. A
good feature should have significant association with the target concept.
We should pay attention to that, besides the above four evaluation measures, there is
another basic kind of measure: information-based measure (Liu & Yu, 2005; de Souza, 2004;
Dash & Liu, 1997), which is not contemplated in the experiments. The reason is demonstrated as follow. The information-based measure is usually in conjunction with ranker
search method. Thus, the FSS algorithms based on this kind of measure usually provide a
rank list of the features instead of telling us which features are relevant to the learning target. In this case, we should preassign particular thresholds for these FSS algorithms to pick
up the relevant features. However, there is not any effective method to set the thresholds
or any acknowledged default threshold for these FSS algorithms. Moreover, it is unfair to
conclude that these information measure based FSS algorithms with any assigned threshold
are not appropriate when comparing to the other FSS algorithms. Therefore, this kind of
FSS algorithm is not employed in our experiments.
Based on the search methods and the evaluation measures introduced above, 22 different
FSS algorithms are obtained. Table 3 shows the brief introduction of these FSS algorithms.
Where all these algorithms are available in the data mining toolkit Weka5 (Hall, Frank,
5. http://www.cs.waikato.ac.nz/ml/weka/

13

fiWang, Song, Sun, Zhang, Xu & Zhou

Holmes, Pfahringer, Reutemann, & Witten, 2009), and the search method INTERACT is
implemented based on Weka and its source codes are available online6 .
ID
1
2
3
4
5
6
7
8
9
10
11

Search Method
BestFirst + Sequential Forward Search
BestFirst + Sequential Backward Search
BestFirst + Bi-direction Search
Genetic Search
Linear Search
Rank Search
Scatter Search
Stepwise Search
Tabu Search
Interactive Search
BestFirst + Sequential Forward Search

Evaluation Measure
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Dependency
Consistency

Notation
CFS-SFS
CFS-SBS
CFS-BiS
CFS-GS
CFS-LS
CFS-RS
CFS-SS
CFS-SWS
CFS-TS
INTERACT-D
Cons-SFS

ID
12
13
14
15
16
17
18
19
20
21
22

Search Method
BestFirst + Sequential Backward Search
BestFirst + Bi-direction Search
Genetic Search
Linear Search
Rank Search
Scatter Search
Stepwise Search
Interactive Search
FCBFsearch
Ranker
Ranker

Evaluation Measure
Consistency
Consistency
Consistency
Consistency
Consistency
Consistency
Consistency
Consistency
Dependency
Distance
Probabilistic Significance

Notation
Cons-SBS
Cons-BiS
Cons-GS
Cons-LS
Cons-RS
Cons-SS
Cons-SWS
INTERACT-C
FCBF
Relief-F
Signific

Table 3: Introduction of the 22 FSS algorithms
It is noted that some of these algorithms require particular settings of certain parameters. For the purpose of allowing other researchers to confirm our results, we introduce the
parameter settings of these FSS algorithms. Such as, for FSS algorithms INTERACT-D
and INTERACT-C, there is a parameter, c-contribution threshold, used to identify the
irrelevant features. We set this threshold as 0.0001 suggested by Zhao and Liu (2007). For
FSS algorithm FCBF, we set the relevance threshold to be the SU (Symmetric Uncertainty) value of the bN/ log N cth ranked feature suggested by Yu and Liu (2003). For FSS
algorithm Relief-F, we set the significance threshold to 0.01 used by Robnik-Sikonja and
Kononenko (2003). For FSS algorithm Signific, there is a threshold, statistical significance level , used to identify the irrelevant features. We set  as the commonly-used value
0.01 in our experiment. The other FSS algorithms are conducted in the Weka environment
with the default setting(s).
4.2.2 Classification Algorithms
Since the actual relevant features of real world data sets are usually not known in advance, it
is impracticable to directly evaluate an FSS algorithm by the selected features. Classification
accuracy is an extensively used metric for evaluating the performance of FSS algorithms,
and also plays an important role in our proposed performance metric EARR for assessing
different FSS algorithms.
However, different classification algorithms have different biases. An FSS algorithm may
be more suitable for some classification algorithms than others (de Souza, 2004). This fact
affects the performance evaluation of FSS algorithms.
With this in mind, in order to demonstrate that our proposed FSS algorithm recommendation method is not limited to any particular classification algorithm, five representative
classification algorithms based on different hypotheses are employed. They are bayes-based
Naive Bayes (John & Langley, 1995) and Bayes Network (Friedman, Geiger, & Goldszmidt,
1997), information gain-based C4.5 (Quinlan, 1993), rule-based PART (Frank & Witten,
1998), and instance-based IB1 (Aha, Kibler, & Albert, 1991), respectively.
Although Naive Bayes and Bayes Net are both bayes-based classification algorithms,
they are quite different from each other since Naive Bayes is proposed based on the hypoth6. http://www.public.asu.edu/huanliu/INTERACT/INTERACTsoftware.html

14

fiSubset Selection Algorithm Automatic Recommendation

esis that the features are conditional independent (John & Langley, 1995), while Bayes Net
takes into account the feature interaction (Friedman et al., 1997).
4.2.3 Measures to Evaluate the Recommendation Method
FSS algorithm recommendation is an application of meta-learning. So far as we know,
there are no unified measures to evaluate the performance of the meta-learning methods.
In order to assess our proposed FSS algorithm recommendation method, two measures,
recommendation hit ratio and recommendation performance ratio, are defined.
Let D be a given data set and Arec be an FSS algorithm recommended by the recommendation method for D, these two measures can be introduced as follows.
1) Recommendation hit ratio
An intuitive evaluation criterion is whether the recommended FSS algorithm Arec meets
users requirements. That is, whether Arec is the optimal FSS algorithm for D, or the performance of Arec on D has no significant difference with that of the optimal FSS algorithm.
Suppose Aopt represents the optimal FSS algorithm for D, and ASetopt denotes the FSS
algorithm set in which each algorithm has no significant difference with Aopt (of course it
includes Aopt as well). Then, a measure named recommendation hit can be defined to assess
whether the recommended algorithm Arec is effective on D.
Definition 1 (Recommendation hit). If an FSS algorithm Arec is recommended to a data
set D, then the recommendation hit Hit(Arec , D) is defined as
(
1, if Arec  ASetopt
Hit(Arec , D) =
.
(6)
0, otherwise
Where Hit(Arec , D) = 1 means the recommendation is effective since the recommended
FSS algorithm Arec is one of the algorithms in ASetopt for D, while hit(Arec , D) = 0 indicates
the recommended FSS algorithm Arec is not a member of ASetopt , i.e., Arec is significantly
worse than the optimal FSS algorithm Aopt on D, thus the recommendation is bad.
From Definition 1 we know that the recommendation hit Hit(Arec , D) is used to evaluate
the recommendation method for a single data set. Thus, it is extended as recommendation
hit ratio to evaluate the recommendation for a set of data sets, and is defined as follows.
Definition 2 Recommendation hit ratio
G

1 X
Hit Ratio(Arec ) =
Hit(Arec , Di ).
G

(7)

i=1

Where G is the number of the historical data sets, e.g., G = 115 in our experiment.
Definition 2 represents the percentage of data sets on which the appropriate FSS algorithms are effectively recommended by our recommendation method. The larger this value,
the better the recommendation method.
2) Recommendation performance ratio
The recommendation hit ratio reveals that whether or not an appropriate FSS algorithm
is recommended for a given data set, but it cannot tell us the margin of the recommended
algorithm to the best one. Thus, a new measure, the recommendation performance ratio
for a recommendation, is defined.
15

fiWang, Song, Sun, Zhang, Xu & Zhou

Definition 3 (Recommendation performance ratio). Let EARRrec and EARRopt be the
performance of the recommended FSS algorithm Arec and the optimal FSS algorithm on D,
respectively. Then, the recommendation performance ratio (RPR) for Arec is defined as
RPR(Arec , D) = EARRrec /EAARopt .

(8)

In this definition, the best performance EARR opt is employed as a benchmark. Without
the benchmark, it is hard to determine the recommended algorithms are good or not.
For example, suppose the EARR of Arec on D is 0.59. If EARR opt = 0.61, then the
recommendation is effective since EARR of Arec is very close to EARR opt . However, the
recommendation is poor if EARR opt = 0.91.
RPR is the ratio of EARR of a recommended FSS algorithm to that of the optimal one.
It measures how close the recommended FSS algorithm to the optimal one, and reveals the
relative performance of the recommended FSS algorithm. Its value varies from 0 to 1, and
the larger the value of RPR, the closer the performance of the recommended FSS algorithm
to that of the optimal one. The recommended algorithm is the optimal one if and only if
RPR = 1.
4.2.4 Values of the Parameters  and 
In this paper, a multi-criteria metric EARR is proposed to evaluate the performance of an
FSS algorithm. For the proposed metric EARR, two parameters  and  are established
for users to express their requirements on algorithm performance.
In the experiment, when presenting the results, two representative value pairs of parameters  and  are used as follows:
1)  = 0 and  = 0. This setting represents the situation where the classification
accuracy is most important. The higher the classification accuracy over the selected features,
the better the corresponding FSS algorithms.
2)  6= 0 and  6= 0. This setting represents the situation where the user can tolerate
an accuracy attenuation and favor the FSS algorithms with shorter runtime and fewer
selected features. In the experiment, both  and  are set to 10% that is quite different
from  =  = 0. This allows us can explore the impact of these two parameters on the
recommendation results.
Moreover, in order to explore how parameters  and  affect the recommended FSS
algorithms in terms of classification accuracy, runtime and the number of selected features,
different parameters settings are provided. Specifically, the values of  and  vary from 0
to 10% with an increase of 1%.
4.3 Experimental Process
In order to make sure the soundness of our experimental conclusion and guarantee the
experiments reported being reproducible, in this part, we introduce the four crucial processes
used in our experiments. They are i) meta-knowledge database construction, ii) optimal
FSS algorithm set identification for a given data set, iii) Recommendation method validation
and iv) sensitivity analysis of the number of the nearest data sets on recommendations.
1) Meta-knowledge database construction
16

fiSubset Selection Algorithm Automatic Recommendation

Procedure PerformanceEvaluation
Inputs : data = a given data set, i.e, one of the 115 data sets;
learner = a given classification algorithm, i.e., one of {Naive Bayes, C4.5, PART,
IB1 or Bayes Network};
FSSAlgSet = {FSSAlg 1 , FSSAlg 2 ,    , FSSAlg 22 }, the set of the 22 FSS
algorithms;
Output: EARRset = {EARR 1 , EARR 2 ,    , EARR 22 }, the EARRs of the 22 FSS
algorithms on data;
1 M = 5; FOLDS = 10;
2 for i = 1 to 22 do
3
EARR i = 0;
4 for i = 1 to M do
5
randomized order from data;
6
generate FOLDS bins from data;
7
for j = 1 to FOLDS do
8
TestData = bin[j ];
9
TrainData = data- TestData;
10
numberList = Null , runtimeList = Null , accuracyList = Null ;
11
for k = 1 to 22 do
12
(Subset, runtime) = apply FSSAlg k on TrainData;
13
number = |Subset |;
14
RedTestData = reduce TestData according to selected Subset;
15
RedTrainData = reduce TrainData according to selected Subset;
16
classifier = learner (RedTrainData);
17
accuracy = apply classifier to RedTestData;
18
numberList [k ] = number , runtimeList [k ] = runtime, accuracyList [k ] =

accuracy;
19
20
21

for k = 1 to 22 do
EARR = EARRCompution(accuracyList, runtimeList, numberList, k );
//Compute EARR of FSSAlg k on jth bin of pass i according Eqs. (1) and (2)
EARR k = EARR k + EARR;

22 for i  1 to 22 do
23
EARR i = EARR i /(M FOLDS );
24 return EARRset;

For each data set Di (1  i  115), we i) extract its meta-features Fi ; ii) calculate the
EARRs for the 22 candidate FSS algorithms with the stratified 510-fold cross-validation
strategy (Kohavi, 1995), and iii) combine the meta-features Fi and the EARR of each FSS
algorithm together to form a tuple, which is finally added to the meta-knowledge database.
Since the extraction of meta-features and the combination of the meta-features and the
EARRs are straightforward, we just present the calculation of EARRs, procedure PerformanceEvaluation shows the details.
2) Optimal FSS algorithm set identification
The optimal FSS algorithm set for a given data set Di consists of the optimal FSS algorithm for this data set and those algorithms that have no significant performance difference
with the optimal one on Di .
The optimal FSS algorithm set for a given data set Di is obtained via a non-parametric
Friedman test (1937) followed by a Holm procedure test (1988) on the performance, which
17

fiWang, Song, Sun, Zhang, Xu & Zhou

is estimated by the 510 cross validation strategy, of the 22 FSS algorithms. If the result
of the Friedman test shows that there is no significant performance difference among the
22 FSS algorithms, these 22 FSS algorithms are added to the optimal FSS algorithm set.
Otherwise, the FSS algorithm with the highest performance is viewed as the optimal one
and added to the optimal FSS algorithm set. Then, the Holm procedure test is performed
to identify the algorithms from the rest 21 FSS algorithms. The algorithms that have no
significant performance differences with the optimal one are added into the optimal FSS
algorithm set.
The reason why the non-parametric test is employed lies in that it is difficult for the
performance values to follow the normal distribution and satisfy variance homogeneous
condition.
Note that the optimal FSS algorithm sets for different settings of parameters  and  are
different, since the values of these two parameters directly affect the required performance
values.
3) Recommendation method validation
The leave-one-out strategy is used to empirically evaluate our proposed FSS algorithm recommendation method as follows: for each data set Di (1  i  115) that
is viewed as the test data, i) identify its k nearest data sets from the training data =
{D1 ,    , Di1 , Di+1 ,    , D115 } excluding Di ; ii) calculate the performance of the 22 candidate FSS algorithms according to Eq. (5) based on the k nearest data sets where the value
of k is determined by the standard cross-validation strategy, and recommend the top three
to Di ; and iii) evaluate the recommendations by the measures introduced in section 4.2.3.
4) Sensitivity analysis of the number of the nearest data sets on recommendations
In order to explore the effect of the number of the nearest data sets on the recommendations and provide users an empirical method to choose its value, for each data set, all
the possible numbers of the nearest data sets are tested. That is, when identifying the k
nearest data sets for a given data set, k is set from 1 to the number of the historical data
sets minus 1 (e.g., 114 in this experiment).
4.4 Results and Analysis
In this section, we present the recommendation results in terms of recommended FSS algorithms, hit ratio and performance ratio , respectively. Due to the space limit of the
paper, we do not list all the recommendations, but instead present the results under two
significantly different pairs of  and , i.e., ( = 0,  = 0) and ( = 10%,  = 10%).
Afterward, we also provide the experimental results of the influence of the user-oriented
parameters  and  on recommendations in terms of classification accuracy, runtime, and
the number of selected features, respectively.
4.4.1 Recommended Algorithms and Hit Ratio
Figs. 2, 3, 4, 5 and 6 show the first recommended FSS algorithms for the 115 data sets
when the classification algorithms Naive Bayes, C4.5, PART, IB1 and Bayes Network are
used, respectively.
18

fiSubset Selection Algorithm Automatic Recommendation

In each figure, there are two sub-figures corresponding to the recommendation results
for ( = 0,  = 0) and ( = 10%,  = 10%), respectively. In each sub-figure,  and 
denote the correctly and incorrectly recommended algorithms, respectively.

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 2: FSS algorithms recommended for the 115 data sets when Naive Bayes is used

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 3: FSS algorithms recommended for the 115 data sets when C4.5 is used
From these figures, we observe that:
1) For all the five classifiers, the proposed method can effectively recommend appropriate
FSS algorithms for most of the 115 data sets.
In the case of ( = 0,  = 0), the number of data sets, whose appropriate FSS
algorithms are correctly recommended, is 109 out of 115 for Naive Bayes, 111 out of 115
for C4.5, 109 out of 115 for PART, 108 out of 115 for IB1, and 109 out of 115 for Bayes
19

fiWang, Song, Sun, Zhang, Xu & Zhou

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 4: FSS algorithms recommended for the 115 data sets when PART is used

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 5: FSS algorithms recommended for the 115 data sets when IB1 is used

Network, respectively. This states that the recommendation method is effective when
classification accuracy is most important.
In the case of ( = 10%,  = 10%), the number of data sets, whose appropriate FSS
algorithms are correctly recommended, is 104 out of 115 for Naive Bayes, 109 out of 115
for C4.5, 110 out of 115 for PART, 106 out of 115 for IB1, and 104 out of 115 for Bayes
Network, respectively. This indicates that the recommendation method also works well
when tradeoff is required among classification accuracy, runtime, and the number of
selected features.
2) The distribution of the recommended FSS algorithms for the 115 data sets is different for
different parameters settings. The distribution is relatively uniform for ( = 0,  = 0),
20

fiSubset Selection Algorithm Automatic Recommendation

FSS algorithm ID

Correctly recommended algorithm

Incorrectly recommended algorithm

22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

97

99 101 103 105 107 109 111 113 115
98 100 102 104 106 108 110 112 114

96

Data set ID

FSS algorithm ID

(a)  = 0,  = 0
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

96

Data set ID

(b)  = 10%,  = 10%

Figure 6: FSS algorithms recommended for the 115 data sets when Bayes Network is used

while it is seriously biased to some algorithm (e.g., the 22th FSS algorithm) for ( =
10%,  = 10%).
This phenomenon is similar for all the five classifiers. This can be explained as follows.
The FSS algorithms with the best classification accuracy distribute on the 115 data sets
uniformly. Thus, in the case of ( = 0,  = 0) where users favor accurate classifiers, the
distribution of the recommended FSS algorithms is relatively uniform as well. However,
there exist some FSS algorithms that run faster (e.g., the 22th algorithm Signific)
or select fewer features (e.g., the 8th algorithm CFS-SWS, the 18th algorithm ConsSWS, and the 22th algorithm Signific) on most of the 115 data sets. For this reason, in
the case of ( = 10%,  = 10%) where users prefer the FSS algorithms with less runtime
and fewer features, the distribution of the FSS algorithms with the best performance on
the 115 data sets is biased to some algorithms, so is the recommended FSS algorithms.
3) The 22th FSS algorithm performs well on about 85 out of 115 data sets for all classifiers
when ( = 10%,  = 10%). It seems that this FSS algorithm is a generally wellperformed FSS algorithm that can be adopted by all FSS tasks and there is no need
for FSS algorithm recommendation. Unfortunately, this is not the case. The 22th FSS
algorithm is still failing to perform well over about a quarter of the 115 data sets.
Yet, our recommendation method can distinguish these data sets and further effectively
recommend appropriate FSS algorithms for them. This indicates our recommendation
method is still necessary in this case.
Compared with ( = 0,  = 0), we can know that this case is due to the larger  and
 values and can be explained as follows. For all the 22 FSS algorithms, although the
classification accuracies of a classifier over the features selected by them are different,
the differences are usually bounded. Meanwhile, from Eq. (1) we know that when /
is set to be greater than the bound value, the value of EARR will be dominated by
the runtime/the number of selected features. This means that if  or  is set to be
a relatively large value, the algorithm with a lower time complexity or the algorithm
that chooses smaller number of features will be recommended, and the classification
21

fiWang, Song, Sun, Zhang, Xu & Zhou

accuracy over the selected features will be ignored. However, as we know, one of the
most important targets of feature selection is to improve the performance of learning
algorithms. So it is unreasonable to ignore the classification accuracy and just focus on
the speed and the simplicity of an FSS algorithm.
Thus, in real applications, the values of  and  should be set under the limit of classification accuracies. Generally, the / should be bounded by (accmax  accmin )/accmin ,
where accmax and accmin denote the maximum and the minimum classification accuracies, respectively.
Parameter setting
 = 0,  = 0

 = 10%,  = 10%


Recommendation
Alg1st
Alg2nd
Alg3rd
Top 3
Alg1st
Alg2nd
Alg3rd
Top 3

Naive Bayes
94.78
83.48
74.78
99.13
90.43
71.30
38.26
99.13

C4.5
96.52
79.13
80.87
98.26
94.78
69.57
45.22
100.0

PART
94.78
92.17
84.35
99.13
94.78
70.43
42.61
100.0

IB1
93.91
77.39
75.65
99.13
92.17
64.35
43.48
100.0

Bayes Network
94.78
83.48
73.91
98.26
90.43
71.30
36.52
99.13

Algx denotes only the x -th algorithm in the ranking list is recommended while Top 3 means the top three
algorithms are recommended.

Table 4: Hit ratio (%) of the recommendations for the five classifiers under different settings
of (, )
Table 4 shows the hit ratio of the recommended FSS algorithms for the five classifiers.
From it we can observe that:
1) If a single FSS algorithm is recommended, the hit ratio of the first recommended algorithm Alg1st is the highest, its value is up to 96.52% and at least is 90.43% for all the
five classifiers. Thus, Alg1st should be the first choice.
2) If the top three algorithms are recommended, the hit ratio is up to 100% and at least
is 98.62%. That indicates that the confidence of the top three algorithms including an
appropriate one is very high. This is the reason why only the top three algorithms
are recommended. Moreover, the proposed recommendation method has reduced the
number of candidate algorithms to three, users can further pick up the one that fits
his/her specific requirement from them.
4.4.2 Recommendation Performance Ratio
Figs. 7 and 8 show the recommendation performance ratio RPR of the first recommended
FSS algorithm for the five classifiers with ( = 0,  = 0) and ( = 10%,  = 10%),
respectively. From these two figures we can observe that, for most data sets and the two
settings of  and , the RPRs of the recommended FSS algorithms are greater than 95%
and some of them are up to 100% no matter which classifier is used. This indicates that the
FSS algorithms recommended by our proposed method are very close to the optimal one.
Table 5 shows the average RPRs over the 115 data sets for the five classifiers under
different settings of (, ). In this table, for each classifier, columns Rec and Def
shows the RPR value corresponding to the recommended FSS algorithms and default FSS
algorithms, respectively. Where the default FSS algorithm is the most frequent best one on
the 115 data sets under the classifier.
22

fiSubset Selection Algorithm Automatic Recommendation

RPR (%)

Naive Bayes
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

C4.5
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

PART
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

IB1
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

Bayes Network
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

Figure 7: RPR of the 1 st recommended FSS algorithm with ( = 0,  = 0) for the five
classifiers
RPR (%)

Naive Bayes
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

C4.5
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

PART
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

IB1
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

RPR (%)

Bayes Network
100
95
90
85
80
75
70

1

3
2

5
4

7
6

9
8

11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 109 111 113 115
10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114

Data set ID

Figure 8: RPR of the 1 st recommended FSS algorithm with ( = 10%,  = 10%) for the
five classifiers

23

fiWang, Song, Sun, Zhang, Xu & Zhou

From it we observe that the average RPRs range from 97.32% to 98.8% for ( = 0,
 = 0), and from 97.82% to 98.99% for ( = 10%,  = 10%), respectively. Moreover,
the average RPR of the recommended FSS algorithms surpasses that of the default FSS
algorithms for all the five different classifiers. This means our proposed recommendation
method works very well and greatly fits users performance requirement.
Parameter setting
 = 0,  = 0
 = 10%,  = 10%

Naive bayes
Rec
Def
98.24 96.42
98.69 91.95

C4.5
Rec
Def
98.80
98.18
97.82
92.40

PART
Rec
Def
97.61 94.79
97.89 92.40

IB1
Rec
Def
97.32
96.43
98.11
92.35

Bayes Network
Rec
Def
98.37
96.63
98.99
92.43

Table 5: Average RPR (%) over 115 data sets for the five classifiers

Data ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58


NB
0.0443
0.0227
0.0118
0.0079
0.0244
0.019
0.0091
0.0111
0.011
0.0091
0.0086
0.0062
0.0068
0.0077
0.0616
0.0099
0.0074
0.0083
0.0102
0.007
0.008
0.0103
0.008
0.0066
0.0088
0.0061
0.0097
0.0083
0.007
0.0273
0.2236
0.2602
0.3691
0.008
0.0084
0.0103
0.0065
0.0084
0.0098
0.0278
0.007
0.0138
0.1219
0.1453
0.1937
0.0225
0.0149
0.0101
0.0228
0.0069
0.0195
0.0202
0.0128
0.0128
0.0075
0.0084
0.0146
0.0432

C4.5
0.0425
0.0131
0.0124
0.0092
0.0253
0.019
0.0093
0.0062
0.0076
0.0087
0.007
0.0068
0.0085
0.0087
0.0582
0.0083
0.0126
0.0077
0.0078
0.0071
0.01
0.0079
0.0083
0.0064
0.0158
0.0068
0.0078
0.0069
0.0132
0.0272
0.2231
0.2616
0.3722
0.0103
0.0068
0.0068
0.0088
0.007
0.0352
0.0243
0.0099
0.006
0.1228
0.1427
0.1955
0.0232
0.0142
0.0125
0.0245
0.0075
0.0201
0.0207
0.0135
0.013
0.0073
0.0073
0.008
0.0464

PART
0.0499
0.0147
0.0123
0.0082
0.0257
0.0221
0.0093
0.0076
0.0072
0.0084
0.0072
0.0065
0.0064
0.0086
0.0575
0.0081
0.0076
0.0128
0.0105
0.007
0.009
0.011
0.0079
0.0067
0.0101
0.0075
0.0113
0.008
0.0093
0.0291
0.2236
0.2605
0.3689
0.0083
0.0065
0.0066
0.0086
0.0084
0.0069
0.0245
0.011
0.0106
0.1214
0.144
0.1972
0.0242
0.0125
0.0101
0.0191
0.0084
0.0196
0.0165
0.0116
0.0197
0.0069
0.007
0.0095
0.0449

IB1
0.0423
0.0137
0.0117
0.0114
0.0246
0.0192
0.0114
0.0066
0.0074
0.0138
0.0075
0.0063
0.0093
0.0084
0.058
0.0082
0.0084
0.0117
0.0067
0.0069
0.0075
0.0135
0.0093
0.0069
0.0082
0.0083
0.0117
0.0082
0.0072
0.0273
0.2239
0.262
0.3689
0.0113
0.0064
0.0066
0.0059
0.0093
0.0067
0.0246
0.007
0.0116
0.1231
0.145
0.194
0.0219
0.0139
0.0092
0.0198
0.0068
0.0209
0.0192
0.0156
0.0138
0.0065
0.006
0.0082
0.0436

BNet
0.0464
0.0129
0.0116
0.0096
0.0241
0.0208
0.0113
0.0088
0.0076
0.0073
0.0083
0.011
0.0096
0.0077
0.0586
0.009
0.0085
0.007
0.0091
0.0081
0.0079
0.0088
0.0072
0.0093
0.0094
0.0077
0.0123
0.0074
0.007
0.0272
0.2255
0.2596
0.3714
0.007
0.009
0.0069
0.0061
0.0085
0.0077
0.0249
0.0086
0.0063
0.1241
0.1434
0.1935
0.0228
0.015
0.0104
0.024
0.009
0.0197
0.0165
0.0158
0.0134
0.0068
0.0069
0.0103
0.0445

Data ID
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
Average

NB
0.0446
0.0361
0.0087
0.0845
0.0225
0.0108
0.0117
0.0089
0.0092
0.0082
0.0061
0.0069
0.0611
0.203
0.1854
0.1195
0.1246
0.0147
0.0576
0.0685
0.0081
0.0086
0.0203
0.0095
0.0244
0.0683
0.0074
0.0084
0.0066
0.0096
0.0095
0.006
0.0158
0.0068
0.0097
0.0365
0.0108
0.0058
0.0082
0.4402
0.4591
0.0504
0.1012
0.04
0.0633
0.9103
0.6484
0.5864
0.0067
0.0091
0.0082
0.0088
0.7746
0.0267
0.0082
0.0106
0.0086
0.0658

C4.5
0.0439
0.0351
0.0096
0.0843
0.0177
0.0077
0.0105
0.0075
0.0067
0.0063
0.0058
0.0077
0.0496
0.1993
0.1689
0.1112
0.1208
0.0064
0.0526
0.0704
0.006
0.0085
0.0144
0.0109
0.0244
0.0671
0.0069
0.0077
0.0101
0.0091
0.0095
0.0069
0.0152
0.0073
0.0076
0.0396
0.0081
0.0062
0.0078
0.4429
0.4532
0.0496
0.095
0.0393
0.0618
0.9096
0.6448
0.5884
0.0056
0.0075
0.0074
0.0131
0.7759
0.0255
0.0075
0.0095
0.0079
0.0651

PART
0.0443
0.0407
0.0073
0.0835
0.0211
0.0065
0.0074
0.0058
0.0079
0.0071
0.006
0.0079
0.051
0.1976
0.1631
0.1105
0.1217
0.0068
0.0509
0.0641
0.0082
0.0087
0.0141
0.0054
0.0276
0.0674
0.0086
0.0411
0.0071
0.0124
0.0078
0.0085
0.0164
0.0066
0.008
0.0378
0.0064
0.0062
0.0064
0.444
0.4557
0.0502
0.0954
0.0424
0.0644
0.9091
0.6443
0.5861
0.0065
0.0102
0.0087
0.0131
0.7736
0.025
0.0066
0.0095
0.0073
0.0652

IB1
0.0477
0.034
0.0078
0.0853
0.02
0.0076
0.0068
0.008
0.0065
0.0085
0.0067
0.0097
0.0485
0.1994
0.1652
0.1103
0.1209
0.0067
0.0519
0.066
0.008
0.0072
0.0118
0.0091
0.0257
0.0692
0.0086
0.0074
0.0069
0.014
0.0124
0.0066
0.0183
0.0081
0.0065
0.0374
0.0094
0.0061
0.0068
0.4401
0.4551
0.0539
0.0966
0.0416
0.0635
0.9142
0.6464
0.5851
0.0075
0.0131
0.0064
0.0093
0.7739
0.0266
0.0063
0.0055
0.0062
0.0649

BNet
0.0427
0.0354
0.0085
0.0859
0.0194
0.0098
0.0071
0.0079
0.0064
0.0116
0.0067
0.007
0.0514
0.1983
0.1638
0.1096
0.1226
0.006
0.0533
0.0646
0.013
0.0089
0.0184
0.0082
0.0248
0.0664
0.0071
0.0132
0.0077
0.0113
0.0092
0.0079
0.0165
0.0061
0.008
0.0375
0.0077
0.0064
0.0083
0.4413
0.4545
0.0526
0.0968
0.0394
0.0625
0.9122
0.6461
0.5854
0.0055
0.0103
0.0095
0.0078
0.7746
0.0282
0.0135
0.0073
0.0064
0.0650

NB and BNet denote Naive Bayes and Bayes Network, respectively.

Table 6: Recommendation time over 115 data sets for the five classifiers (in second )

24

fiSubset Selection Algorithm Automatic Recommendation

4.4.3 Recommendation Time
When recommending FSS algorithms for a feature selection problem, the recommendation
time is contributed by meta-features extraction, k nearest data sets identification, and the
candidate algorithm ranking according to their performance on the k data sets.
Of these three recommendation time contributors, only the candidate algorithm ranking
is related with the parameters  and  of the performance metric EARR.
However, the computation of performance EARR is the same whatever the values of 
and  are. This means recommendation time is independent of the specific settings of 
and . Thus, we just present the recommendation time with ( = 0,  = 0), and Table 6
shows the details.
From Table 6 we observe that for a given data set, the recommendation time differences
for the five classifiers are small. The reason is that the recommendation time is mainly
contributed by the extraction of meta-features, which has no relation with classifiers. This
is consistent with the time complexity analysis in Section 3.2. We also observe that for most
of the data sets, the recommendation time is less than 0.1 second, and its average value on
the 115 data sets is around 0.65 second for each of the five classifiers. This is much faster
than the conventional cross validation method.
4.4.4 Impact of the Parameters  and 
Figs. 9, 10, 11, 12 and 13 show the impact of the settings of  and  on the classification
accuracy, the runtime of feature selection, the number of selected features, the Hit Ratio
and the RPR value, respectively.
NaiveBayes

C4.5

PART

Average accuracy

0.845

Average accuracy

IB1

Bayes Network

0.845

0.84
0.835
0.83
0.825
0.82
0.815
0.81

0.84
0.835
0.83
0.825
0.82
0.815
0.81
0.805

0.805
0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 9: Classification accuracies of the five classifiers with the recommended FSS algorithms under different values of  and 
Fig. 9 shows the classification accuracies of the five classifiers under the different values
of  and . From it we observe that, with the increase of either  or , the classification
accuracies of the five classifiers with the recommended FSS algorithms decrease. This is
because the increase of  or  indicates that users much prefer faster FSS algorithms or the
FSS algorithms that can get less features. Thus, the proportion of classification accuracy
in performance is decreased. This means the ranks of the FSS algorithms that run faster
and/or get less features are improved and the corresponding FSS algorithms are finally
selected.
25

fiWang, Song, Sun, Zhang, Xu & Zhou

C4.5

PART

IB1

Bayes Network

2400

Average runtime (ms)

Average runtime (ms)

NaiveBayes
2400
2200
2000
1800
1600
1400
1200
1000
800
600
400
200

2200
2000
1800
1600
1400
1200
1000
800
600
400

0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 10: Runtime of the FSS algorithms recommended to the five classifiers under different
values of  and 
Fig. 10 shows the runtime of the FSS algorithms that recommended to the five classifiers
under the different values of  and  for the five classifiers. From it we observe that:
1) With the increase of , the average runtime of the recommended FSS algorithms for
each classifier decreases. Note a larger value of  means users favor faster FSS algorithms. Thus, this indicates that users performance requirement is met since faster FSS
algorithms were recommended.
2) With the increase of , the average runtime of the recommended FSS algorithms increases
as well. This is because in our proposed recommendation method, the appropriate
FSS algorithms for a given data set are recommended based on its nearest data sets.
Moreover, in the experiment, for more than half (i.e., 69) of the 115 data sets, there is
a negative correlation between the number of selected features and the runtime of the
22 FSS algorithms. Thus, the more data sets with this kind of negative correlation, the
more possible the nearest neighbors of a given data set have the negative correlation.
Therefore, a larger  means longer runtime. Another possible reason is that a larger
value of  means users favor the FSS algorithms that choose fewer features, and in order
to get fewer features, the FSS algorithms need to consume relatively more time.
C4.5

PART

Average number of features

Average number of features

NaiveBayes

100
90
80
70
60
50
40
0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

IB1

Bayes Network

120
100
80
60
40
20
0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 11: Number of features selected by the FSS algorithms that recommended to the
five classifiers under different values of  and 
Fig. 11 shows the number of features selected by the FSS algorithms that recommended
to the five classifiers under different values of  and . From it we observe that:
26

fiSubset Selection Algorithm Automatic Recommendation

1) With the increase of , the average number of selected features increases as well. This
is because in our proposed recommendation method, the appropriate FSS algorithms
for a given data set are recommended based on its nearest data sets. Moreover, in the
experiment, for more than half (i.e., 69) of the 115 data sets, there is a negative correlation between the number of selected features and the runtime of the 22 FSS algorithms.
Thus, the more data sets with this kind of negative correlation, the more possible the
nearest neighbors of a given data set have the negative correlation. Therefore, a larger
 means more features. Another possible reason is that a larger value of  means users
favor faster FSS algorithms. It is possible that shorter computation time can be obtained
via filter out less features so more features are remained.
Note that there is an exception. That is, the average number of selected features for
C4.5 decreases when the value of  is small. However, the decrement comes up in a quite
small range of  (i.e., < 0.005).
2) With the increase of , the average number of features selected by the recommended
FSS algorithm decreases. Note a larger value of  means users favor the FSS algorithms
that can get fewer features. Thus, this indicates that users requirement is met since the
FSS algorithms that can get fewer features were recommended.
NaiveBayes

C4.5

PART

Average Hit Ratio (%)

Average Hit Ratio (%)

IB1

Bayes Network

100

100
99
98
97
96
95
94
93
92

99
98
97
96
95
94
93
92

91
0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 12: Average Hit Ratio of the FSS algorithms that recommended to the five classifiers
under different values of  and 
C4.5

PART
100

99.5

99.5

Average RPR (%)

Average RPR (%)

NaiveBayes
100

99
98.5
98
97.5

IB1

Bayes Network

99
98.5
98
97.5
97

97
0

0

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1

!

Figure 13: Average RPR of the FSS algorithms that recommended to the five classifiers
under different values of  and 
Figs. 12 and 13 show the average hit ratio and RPR of the recommended FSS algorithms
under different values of  and  for the five classifiers.
27

fiWang, Song, Sun, Zhang, Xu & Zhou

From them we observe that, the average hit ratio falls in the intervals [91.74%, 100%]
under  and [92.56%, 99.13%]) under . The average RPR varies in the intervals [97.69%,
98.82%] under  and [97.68%, 98.73%] under . With the change of the  and , the hit
ratio and RPR of the recommended FSS algorithms vary as well. However, the change
intervals fall in a relative small interval and the lower bound stands at a fairly high level.
The minimum average hit ratio is up to 91.74% and the minimum average RPR is up
to 97.68%. This indicates that the proposed FSS algorithm recommendation method has
general application and works well for different settings of  and .

5. Sensitivity Analysis of the Number of Nearest Data Sets on
Recommendation Results
In this section, we analyze how the number of the nearest data sets affects the recommendation performance. Based on the experimental results, we provide some guidelines for
selecting the appropriate number of nearest data sets in practice.
5.1 Experimental Method
Generally, different numbers of the nearest data sets (i.e., k) will result in different recommendations. Thus, when recommending FSS algorithms to a feature selection problem, an
appropriate k value is very important.
The k value that results in higher recommendation performance is preferred. However,
the recommendation performance difference under two different k values sometimes might
be random and not significant. Thus, in order to identify an appropriate k value from
alternatives, we should first determine whether or not the performance differences among
them are statistically significant. Non-parametric statistical test, Friedman test followed by
Holm procedure test as suggested by Demsar (2006), can be used for this purpose.
In the experiment, we conducted FSS algorithm recommendation with all possible k
values (i.e., from 1 to 114) over the 115 data sets. When identifying the appropriate k
values, the non-parametric statistical test is conducted as follows.
Firstly, the Friedman test is performed over the 114 recommendation performance at
the significance level 0.05. Its null hypothesis is that the 114 k values perform equivalently
well in the proposed recommendation method over the 115 data sets.
If the Friedman test rejects the null hypothesis, that is, there exists significant difference
among these 114 k values, then we choose one under which the recommendation has the best
performance as the reference. After that, the Holm procedure test is performed to find out
the k values under which the recommendation performance has no significant difference with
that of the reference. The identified k values including the reference are the appropriate
numbers of the nearest data sets.
5.2 Results Analysis
Fig. 14 shows how the number of the nearest data sets (i.e., k) affects the performance
of the recommendation method under different settings of  and , where  denotes the
k under which the recommendation performance is significantly worse than others at the
significance level of 0.05. From it we observe that:
28

fiSubset Selection Algorithm Automatic Recommendation

Naive Bayes

C4.5

PART

IB1

Bayes Network

Inappropriate number of neighbors

1
0.995

RPR

0.99
0.985
0.98
0.975
0.97
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97
96

99 101 103 105 107 109 111 113
98 100 102 104 106 108 110 112 114

Number of nearest data sets

(a)  = 0,  = 0
Naive Bayes

C4.5

PART

IB1

Bayes Network

Inappropriate number of neighbors

1

RPR

0.995
0.99
0.985
0.98
0.975
1

3
2

5
4

7
6

9
8

11
10

13
12

15
14

17
16

19
18

21
20

23
22

25
24

27
26

29
28

31
30

33
32

35
34

37
36

39
38

41
40

43
42

45
44

47
46

49
48

51
50

53
52

55
54

57
56

59
58

61
60

63
62

65
64

67
66

69
68

71
70

73
72

75
74

77
76

79
78

81
80

83
82

85
84

87
86

89
88

91
90

93
92

95
94

97
96

99 101 103 105 107 109 111 113
98 100 102 104 106 108 110 112 114

Number of nearest data sets

(b)  = 10%,  = 10%

Figure 14: Number of the nearest data sets vs. RPR
1) When  =  = 0 (Fig. 14(a)), for each of the five classifiers, the RPR varies with
different k values. Specifically, the RPR is fluctuant when k is smaller than 20, while it
is relatively flat in the middle part, and it decreases when k is larger than 79 except for
C4.5. However, the increment of C4.5 is very small (< 0.002). This might be due to that
C4.5 picks up useful features to build the tree by itself, so the impact of other feature
selection methods is less. Moreover, the difference among accuracies of C4.5 on most
data sets is relatively small, while the performance metric EARR that used to evaluate
different FSS algorithms depends only on classification accuracy when  =  = 0. Thus,
the RPR of C4.5 is relatively stable for the different values of k.
2) In the case of  =  = 10% (Fig. 14(b)), the variation of RPR is different from that of
 =  = 0. For each of the five classifiers, the RPR first decreases with fluctuations, then
increases, and finally decreases slowly and steadily. This could be due to that, when the
parameters  and  are set to be a relatively large value (such as 10% in our experiment),
the runtime of ( or the number of features selected by) an FSS algorithm will play a
more important role in evaluating the performance of the FSS algorithm. Thus, for a
given data set, the FSS algorithms with lower time complexity (or the smaller number of
selected features) will be more possibly higher ranked and have larger RPR. Therefore,
with the increasing of k, these algorithms are more possibly recommended. Meanwhile,
for most data sets, these algorithms are either the real appropriate algorithms or with
larger RPR, so the RPR averaged over all data sets is relatively stable with the increasing
of k.
29

fiWang, Song, Sun, Zhang, Xu & Zhou

3) Comparing the cases of  =  = 0 and  =  = 10%, we found that  appears when
k < 21 for the former and k < 29 for the latter, while it emerges again when k > 76 for
the former. This means we cannot choose the k values falling into these ranges. At the
same time, we also found that the peak values of RPR for  =  = 10% appear in the
range of [32, 54], which is also one of the peak value ranges for  =  = 0 except C4.5.
This means if we set k to 28% to 47% of the number of data sets, better recommendation
performance can be obtained.

6. Conclusion
In this paper, we have presented an FSS algorithm recommendation method with the aim
to support the automatic selection of appropriate FSS algorithms for a new feature selection
problem from a number of candidates.
The proposed recommendation method consists of meta-knowledge database construction and algorithm recommendation. The former obtains the meta-features and the performance of all the candidate FSS algorithms, while the latter models the relationship between
the meta-features and the FSS algorithm performance based on a k -NN method and recommends appropriate algorithms for a feature selection problem with the built up model.
We have thoroughly tested the recommendation method with 115 real world data sets, 22
different FSS algorithms, and five representative classification algorithms under two typical
users performance requirements. The experimental results show that our recommendation
method is effective.
We have also conducted a sensitivity analysis to explore how the number of the nearest
data sets (k) impacts the FSS algorithm recommendation, and suggest to set k as the 28%
to 47% of the number of the historical data sets.
In this paper, we have utilized the well-known and commonly-used meta-features to
characterize different data sets. Which meta-features are informative? and Are there
any other more informative meta-features? are still open questions. To our knowledge,
there still does not exist any effective method to answer these questions. Thus, for future
work, we plan to explore further that how to measure the information of the meta-features
and whether there are some more informative meta-features that can lead to further improvements for FSS algorithm recommendation.

Acknowledgements
This work is supported by the National Natural Science Foundation of China under grant
61070006.

References
Aha, D. W., Kibler, D., & Albert, M. K. (1991). Instance-based learning algorithms. Machine learning, 6 (1), 3766.
Ali, S., & Smith, K. A. (2006). On learning algorithm selection for classification. Applied
Soft Computing, 6 (2), 119138.
30

fiSubset Selection Algorithm Automatic Recommendation

Atkeson, C. G., Moore, A. W., & Schaal, S. (1997). Locally weighted learning. Artificial
intelligence review, 11 (1), 1173.
Battiti, R. (1994). Using mutual information for selecting features in supervised neural net
learning. IEEE Transactions on Neural Networks, 5 (4), 537550.
Brazdil, P., Carrier, C., Soares, C., & Vilalta, R. (2008). Metalearning: Applications to data
mining. Springer.
Brazdil, P. B., Soares, C., & Da Costa, J. P. (2003). Ranking learning algorithms: Using IBL
and meta-learning on accuracy and time results. Machine Learning, 50 (3), 251277.
Brodley, C. E. (1993). Addressing the selective superiority problem: Automatic algorithm/model class selection. In Proceedings of the Tenth International Conference
on Machine Learning, pp. 1724. Citeseer.
Castiello, C., Castellano, G., & Fanelli, A. (2005). Meta-data: Characterization of input
features for meta-learning. Modeling Decisions for Artificial Intelligence, 457468.
Dash, M., & Liu, H. (1997). Feature selection for classification. Intelligent data analysis,
1 (3), 131156.
Dash, M., & Liu, H. (2003). Consistency-based search in feature selection. Artificial Intelligence, 151 (1-2), 155176.
de Souza, J. T. (2004). Feature selection with a general hybrid algorithm. Ph.D. thesis,
University of Ottawa.
Demsar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of
Machine Learning Research, 7, 130.
Engels, R., & Theusinger, C. (1998). Using a data metric for preprocessing advice for data
mining applications..
Frank, E., & Witten, I. H. (1998). Generating accurate rule sets without global optimization.
In Proceedings of the 25th international conference on Machine learning, pp. 144151.
Morgan Kaufmann, San Francisco, CA.
Friedman, M. (1937). The use of ranks to avoid the assumption of normality implicit in
the analysis of variance. Journal of the American Statistical Association, 32 (200),
675701.
Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian network classifiers. Machine
learning, 29 (2), 131163.
Gama, J., & Brazdil, P. (1995). Characterization of classification algorithms. Progress in
Artificial Intelligence, 189200.
Garcia Lopez, F., Garcia Torres, M., Melian Batista, B., Moreno Perez, J. A., & MorenoVega, J. M. (2006). Solving feature subset selection problem by a parallel scatter
search. European Journal of Operational Research, 169 (2), 477489.
Goldberg, D. E. (1989). Genetic algorithms in search, optimization, and machine learning.
Addison-Wesley Professional.
31

fiWang, Song, Sun, Zhang, Xu & Zhou

Gutlein, M., Frank, E., Hall, M., & Karwath, A. (2009). Large-scale attribute selection
using wrappers. In Proceedings of IEEE Symposium on Computational Intelligence
and Data Mining, pp. 332339. IEEE.
Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection. The
Journal of Machine Learning Research, 3, 11571182.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009). The
weka data mining software: an update. ACM SIGKDD Explorations Newsletter, 11 (1),
1018.
Hall, M. A. (1999). Correlation-based Feature Selection for Machine Learning. Ph.D. thesis,
The University of Waikato.
Hedar, A. R., Wang, J., & Fukushima, M. (2008). Tabu search for attribute reduction
in rough set theory. Soft Computing-A Fusion of Foundations, Methodologies and
Applications, 12 (9), 909918.
Hommel, G. (1988). A stagewise rejective multiple test procedure based on a modified
bonferroni test. Biometrika, 75 (2), 383386.
John, G. H., & Langley, P. (1995). Estimating continuous distributions in Bayesian classifiers. In Proceedings of the eleventh conference on uncertainty in artificial intelligence,
Vol. 1, pp. 338345. Citeseer.
Kalousis, A., Gama, J., & Hilario, M. (2004). On data and algorithms: Understanding
inductive performance. Machine Learning, 54 (3), 275312.
King, R. D., Feng, C., & Sutherland, A. (1995). Statlog: comparison of classification algorithms on large real-world problems. Applied Artificial Intelligence, 9 (3), 289333.
Kira, K., & Rendell, L. (1992). A practical approach to feature selection. In Proceedings of the ninth international workshop on Machine learning, pp. 249256. Morgan
Kaufmann Publishers Inc.
Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and
model selection. In International joint Conference on artificial intelligence, Vol. 14,
pp. 11371145. Citeseer.
Kohavi, R., & John, G. (1997). Wrappers for feature subset selection. Artificial intelligence,
97 (1), 273324.
Kononenko, I. (1994). Estimating attributes: analysis and extensions of RELIEF. In Proceedings of the European conference on machine learning on Machine Learning, pp.
171182. Springer-Verlag New York.
Lee, M., Lu, H., Ling, T., & Ko, Y. (1999). Cleansing data for mining and warehousing.
In Proceedings of the 10th International Conference on Database and Expert Systems
Applications, pp. 751760. Springer.
Lindner, G., & Studer, R. (1999). AST: Support for algorithm selection with a CBR approach. Principles of Data Mining and Knowledge Discovery, 418423.
Liu, H., Motoda, H., Setiono, R., & Zhao, Z. (2010). Feature Selection: An Ever Evolving
Frontier in Data Mining. In The Fourth Workshop on Feature Selection in Data
Mining, pp. 314. Citeseer.
32

fiSubset Selection Algorithm Automatic Recommendation

Liu, H., & Setiono, R. (1995). Chi2: Feature selection and discretization of numeric attributes. In Proceedings of the Seventh International Conference on Tools with Artificial Intelligence, pp. 388391. IEEE.
Liu, H., & Setiono, R. (1996). A probabilistic approach to feature selection-a filter solution..
pp. 319327. Citeseer.
Liu, H., & Yu, L. (2005). Toward integrating feature selection algorithms for classification
and clustering. IEEE Transactions on Knowledge and Data Engineering, 17 (4), 491
502.
Michie, D., Spiegelhalter, D. J., & Taylor, C. C. (1994).
statistical classification..

Machine learning, neural and

Molina, L. C., Belanche, L., & Nebot, A. (2002). Feature selection algorithms: A survey and
experimental evaluation. In Proceedings of IEEE International Conference on Data
Mining, pp. 306313. IEEE.
Nakhaeizadeh, G., & Schnabl, A. (1997). Development of multi-criteria metrics for evaluation of data mining algorithms. In Proceedings of the 3rd International Conference
on Knowledge Discovery and Data mining, pp. 3742.
Nakhaeizadeh, G., & Schnabl, A. (1998). Towards the personalization of algorithms evaluation in data mining. In Proceedings of the 4th International Conference on Knowledge
Discovery and Data mining, pp. 289293.
Pudil, P., Novovicova, J., & Kittler, J. (1994). Floating search methods in feature selection.
Pattern recognition letters, 15 (11), 11191125.
Pudil, P., Novovicova, J., Somol, P., & Vrnata, R. (1998a). Conceptual base of feature
selection consulting system. Kybernetika, 34 (4), 451460.
Pudil, P., Novovicova, J., Somol, P., & Vrnata, R. (1998b). Feature selection expertuser
oriented approach. Advances in Pattern Recognition, 573582.
Quinlan, J. R. (1993). C4.5: programs for machine learning. Morgan Kaufmann.
Robnik-Sikonja, M., & Kononenko, I. (2003). Theoretical and empirical analysis of relieff
and rrelieff. Machine learning, 53 (1), 2369.
Saeys, Y., Inza, I., & Larranaga, P. (2007). A review of feature selection techniques in
bioinformatics. Bioinformatics, 23 (19), 25072517.
Smith-Miles, K. A. (2008). Cross-disciplinary perspectives on meta-learning for algorithm
selection. ACM Computing Surveys, 41 (1), 125.
Sohn, S. Y. (1999). Meta analysis of classification algorithms for pattern recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 21 (11), 11371144.
Song, Q. B., Wang, G. T., & Wang, C. (2012). Automatic recommendation of classification
algorithms based on data set characteristics. Pattern Recognition, 45 (7), 26722689.
Vilalta, R., & Drissi, Y. (2002). A perspective view and survey of meta-learning. Artificial
Intelligence Review, 18 (2), 7795.
33

fiWang, Song, Sun, Zhang, Xu & Zhou

Wolpert, D. H. (2001). The supervised learning no-free-lunch theorems. In Proceedings
of 6th Online World Conference on Soft Computing in Industrial Applications, pp.
2542. Citeseer.
Yu, L., & Liu, H. (2003). Feature selection for high-dimensional data: A fast correlationbased filter solution. In Proceedings of The Twentieth International Conference on
Machine Leaning, Vol. 20, pp. 856863.
Zhao, Z., & Liu, H. (2007). Searching for interacting features. In Proceedings of the 20th
International Joint Conference on Artifical Intelligence, pp. 11561161. Morgan Kaufmann Publishers Inc.
Zhou, X., & Dillon, T. (1988). A heuristic-statistical feature selection criterion for inductive machine learning in the real world. In Proceedings of the IEEE International
Conference on Systems, Man, and Cybernetics, Vol. 1, pp. 548552. IEEE.

34

fi