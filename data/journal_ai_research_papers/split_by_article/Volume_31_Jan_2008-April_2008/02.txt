Journal of Artificial Intelligence Research 31 (2008) 83-112

Submitted 06/07; published 01/08

CUI Networks: A Graphical Representation for Conditional
Utility Independence
Yagil Engel
Michael P. Wellman

yagil@umich.edu
wellman@umich.edu

University of Michigan, Computer Science & Engineering
2260 Hayward St, Ann Arbor, MI 48109-2121, USA

Abstract
We introduce CUI networks, a compact graphical representation of utility functions
over multiple attributes. CUI networks model multiattribute utility functions using the
well-studied and widely applicable utility independence concept. We show how conditional
utility independence leads to an effective functional decomposition that can be exhibited
graphically, and how local, compact data at the graph nodes can be used to calculate
joint utility. We discuss aspects of elicitation, network construction, and optimization, and
contrast our new representation with previous graphical preference modeling.

1. Introduction
Modern AI decision making is based on the notion of expected utility, in which probability distributions are used to weigh the utility values for each of the possible outcomes.
The representation of probability distribution functions by Markov or Bayesian networks
(Pearl, 1988)exploiting conditional independence to achieve compactness and computational efficiencyhas led to a plethora of new techniques and applications. Despite their
equal importance to decision making, preferences and utilities have generally not received
the level of attention AI researchers have devoted to beliefs and probabilities. Nor have the
(increasing) efforts to develop representations and inference methods for utility achieved a
degree of success comparable to the impact of graphical models on probabilistic reasoning.
Recognizing that utility functions over multidimensional domains may also be amenable to
factoring based on independence (Keeney & Raiffa, 1976), several have aimed to develop
models with analogous benefits (Bacchus & Grove, 1995; Boutilier, Bacchus, & Brafman,
2001; La Mura & Shoham, 1999; Wellman & Doyle, 1992). This is our goal as well, and we
compare our approach to these and other methods in the Related Work section (2.2).
The development of compact representations for multiattribute utility begins with the
notion of preferential independence (PI), or separability of subdomains in the outcome space.
A subdomain of outcomes is separable in the PI sense if the preference order over this subdomain does not depend on the rest of the domain. When all subsets of attributes induce
separable subdomains, the ordinal utility (value) function decomposes additively over its
variables (Debreu, 1959; Fishburn, 1965; Gorman, 1968). A cardinal utility function represents not only preferences over outcomes but also a notion of strength of preferences, most
notably to represent preferences over actions with uncertain outcomes, or lotteries. Direct
adaptation of the PI concept to cardinal utility requires a generalization of this notion:
a set of attributes is utility independent (UI) if the preference order over lotteries on the

c
2008
AI Access Foundation. All rights reserved.

fiEngel & Wellman

induced subdomain does not depend on the values of the rest of the attributes. A stronger
judgement is to assert that the preference order over the joint domain depends only on its
margins over some attribute subsets. The latter leads to powerful additive decompositions,
either fully additive (when the subsets of attributes are disjoint), or generalized, which is an
additive decomposition over overlapping subsets (Fishburn, 1967; Bacchus & Grove, 1995).
Utility independence leads to less convenient decompositions, such as multilinear (Keeney
& Raiffa, 1976) or hierarchical (Von Stengel, 1988; Wellman & Doyle, 1992). Most previous
efforts in the AI community to adapt modern graphical modeling to utility functions employ the generalized additive decomposition (Bacchus & Grove, 1995; Boutilier et al., 2001;
Gonzales & Perny, 2004). In contrast, our work continues the other thread, based on the
weaker utility independence assumption. We elaborate on the difference between the types
of independence following the presentation of formal definitions.

2. Background
Our utility-theoretic terminology follows the definitive text by Keeney and Raiffa (1976).
In the multiattribute utility framework, an outcome is represented by a vector of values
for n variables, called attributes. The decision makers preferences are represented by a
total pre-order, , over the set of outcomes. In common applications decision makers do
not have the ability to choose a certain outcome, but rather an action that results in a
probability distribution over outcomes, also called a lottery. The decision maker is hence
 over the set of possible lotteries. Given a standard set
assumed to have a preference order 

of axioms,  can be represented by a real-valued utility function over outcomes, U (), such
that numeric ranking of probabilistic outcomes by expected utility respects the ordering by
 The utility function is unique up to positive affine transformations. A positive linear
.
transform of U () represents the same preferences, and is thus strategically equivalent.
The ability to represent utility over probability distributions by a function over outcomes
provides some structure, but in multiattribute settings the outcome space is n-dimensional.
Unless n is quite small, therefore, an explicit (e.g., tabular) representation of U () will
generally not be practical. Much of the research in multiattribute utility theory aims to
identify structural patterns that enable more compact representations. In particular, when
subsets of attributes respect various independence relationships, the utility function may
be decomposed into combinations of modular subutility functions of smaller dimension.
Let S = {x1 , . . . , xn } be a set of attributes. In the following definitions (and the rest of
the work) capital letters denote subsets of attributes, small letters (with or without numeric
subscripts) denote specific attributes, and X denotes the complement of X with respect to
S. We denote the (joint) domain of X by D(X), and indicate specific attribute assignments
with prime signs or superscripts. To represent an instantiation of subsets X and Y at the
same time we use a sequence of instantiation symbols, as in X 0 Y 0 .
In order to meaningfully discuss preferences over subsets of attributes, we need a notion
of preferences over a subset given fixed values for the rest of the attributes.
0

0

Definition 1. Outcome Y 0 is conditionally preferred to outcome Y 00 given Y , if Y 0 Y 
0
0
Y 00 Y . We denote the conditional preference order over Y given Y by Y 0 .

84

fiCUI networks

 0
Similarly we define conditional preference order over lotteries. The preference order 
Y
0
over lotteries on Y is represented by a conditional utility function, U (Y, Y ).
Definition 2. Y is Preferential Independent (PI) of Y if Y 0 does not depend on the value
0
chosen for Y .
Preferential independence can be very useful for qualitative preference assessment. Firstorder preferential independence (i.e., independence of a single attribute from the rest) is a
natural assumption in many domains. For example, in typical purchase decisions greater
quantity or higher quality is more desirable regardless of the values of other attributes. Preferential independence of higher order, however, requires invariance of the tradeoffs among
some attributes with respect to variation in others, a more stringentthough still often
satisfiableindependence condition. The standard PI condition applies to a subset with respect to the full complement of remaining attributes. The conditional version of PI specifies
independence with respect to a subset of the complement, holding the remaining attributes
fixed.
Definition 3. Y is Conditionally Preferential Independent (CPI) of X given Z (Z = XY ),
if for any Z 0 , X 0 Z 0 does not depend on the value chosen for X 0 . We denote this relationship
by CPI(Y, X | Z).
The counterpart of preferential independence that considers probability distributions
over outcomes is called utility independence.
Definition 4. Y is Utility Independent (UI) of Y , if the conditional preference order for
 0 , does not depend on value chosen for Y 0 .
lotteries over Y , 
Y
In our notations, we apply UI and the conditions defined below to sets of attributes or
to specific attributes.
Given UI(Y, X), taking X = Y , the conditional utility function over Y given X 0 is
invariant up to positive affine transformations, for any fixed value X 0 . This fact can be
expressed by the decomposition
U (X, Y ) = f (X) + g(X)U (X 0 , Y ),

g() > 0.

Note that the functions f () and g() may be different for each particular choice of X 0 . Since
U (X 0 , Y ) is a function only of Y , we sometimes use the notation UX 0 (Y ).
Utility independence has a conditional version as well.
Definition 5. Y is Conditionally Utility Independent (CUI) of X given Z (Z = XY ) if for
 X 0 Z 0 does not depend on the value chosen for X 0 . We denote this relationship by
any Z 0 , 
CUI(Y, X | Z).
CUI also supports functional decomposition. For any Z 0 , the conditional utility function
over Y given X 0 Z 0 is strategically equivalent to this function given a different instantiation
of X. However, the transformation depends not only on X, but also on Z 0 . Hence we can
write:
U (X, Y, Z) = f (X, Z) + g(X, Z)U (X 0 , Y, Z), g() > 0.
(1)
85

fiEngel & Wellman

That is, we can fix X on some arbitrary level X 0 and use two transformation functions f and
g to get the value of U () for other levels of X. A stronger, symmetric form of independence
which leads to additive decomposition of the utility function is called additive independence.
We provide the definition for its conditional version.
Definition 6. X and Y are Conditionally Additive Independent given Z, CAI(X, Y | Z),
 Z 0 depends only on the marginal conditional probability disif for any instantiation Z 0 , 
0
tributions over XZ and Y Z 0 .
This means that for any value Z 0 , and for any two probability distributions p, q such
that p(X, , Z 0 )  q(X, , Z 0 ), and p(, Y, Z 0 )  q(, Y, Z 0 ), the decision maker is indifferent
between p and q. A necessary (but not always sufficient) condition for this to hold is that
the utility differences U (X 0 , Y, Z 0 )  U (X 00 , Y, Z 0 ) (for any X 0 , X 00 ) do not depend on the
value of Y .
CAI leads to the following decomposition (Keeney & Raiffa, 1976):
U (X, Y, Z) = f (X, Z) + g(Y, Z).
Other variations of utility independence were considered in the theoretical literature,
leading to various decomposition results (Fishburn, 1975; Krantz, Luce, Suppes, & Tversky,
1971; Fuhrken & Richter, 1991).
2.1 Motivation
The most obvious benefit of a model based on (conditional) utility independence is the
generality admitted by a weaker independence condition, in comparison to additive independence. Whereas additivity practically excludes any interaction between utility of one
attribute or subset (X in Definition 6) to the value of another (Y ), utility independence
allows substitutivity and complementarity relationships, as long as the risk attitude towards one variable is not affected by the value of another. One could also argue that UI is
particularly intuitive, based as it is on an invariance condition on the preference order. In
contrast, (conditional) additive independence requires a judgment about the effects of joint
versus marginal probability distributions. Moreover, additive independence is symmetric,
whereas the condition U I(X, Y ) does allow the preference order over Y to depend on X.
Bacchus and Grove exemplify the difference between additive and utility independence
on a simple state space of two boolean attributes: Health and Wealth. In their example,
shown in Table 1, the attributes are not additive independent (it can be immediately seen
using preference differences), because H and W are complements: having both is worth
more than the sum of having each one without the other. We would have considered the
two attributes substitutes if, for example, U (W, H) = 4 and U (W, H) = 3. In both cases
H and W are nonetheless preferential independent, since we always prefer to be richer (all
else being equal) and healthier (all else being equal). For boolean variables, preferential and
utility independence are equivalent (we always prefer lotteries that give higher probability
to the preferred level) and therefore Health and Wealth are also UI of each other.
(Conditional) additive independence and its resulting additive decomposition can be
generalized to multiple subsets that are not necessarily disjoint. This condition is called

86

fiCUI networks

W
W

H
5
2

H
1
0

Table 1: Utility values for the Health and Wealth example (Bacchus & Grove, 1995).
generalized additive independence (GAI). If GAI holds, U () decomposes to a sum of independent functions fi () over the GAI subsets Xi . As shown by Bacchus and Grove, CAI
conditions can be accumulated to a global GAI decomposition (see Section 2.2). The latter
may also exist without any CAI conditions leading to it, but such a GAI condition is hard
to identify: whereas each CAI condition corresponds to the independence of two attributes
or two subsets, a global GAI condition does not have such an intuitive interpretation.
In the next example, no cardinal independence condition exists, except for a non symmetric CUI. The example also shows the difference between PI and UI, and hence requires
the domains of H and W to include at least three values each. We also add a third attribute to the outcome space, location (L), indicating whether we live in the city or in the
countryside (Table 2). In order to show that U I(H, {W, L}) does not hold it is enough
to find that it is violated for one pair of lotteries. Given the partial outcome Wr , Lci we
prefer the equal chance lottery over < Hf , Hs >, whose expected utility is 12+5
2 , to the sure
outcome Hg (value 8), whereas given Wp , Lci we are indifferent (expected utility of 2 to
both lotteries). Intuitively, it may be the case that the additional value we get from fitness
(over good health) is higher if we are also rich, making it more significant than the value
Hg adds over Hs . Similarly, U I(W, {H, L}) does not hold, by comparing the even-chance
gamble over < Wr , Wp > and the sure outcome Wm , first given Hf , Lci and then given
Hs , Lci .
W and H are therefore not utility independent, but they are preferential independent.
L, however, is not: when we are rich we would rather live in the city, and the other way
round when we are poor, except for the case of being poor and sick under which we prefer
the city.

Wr
Wm
Wp

Hf
12
6
3

Lci
Hg
8
4
2

Hs
5
3
1

Hf
10
6
4

Lco
Hg
6
3
1.5

Hs
4
2
0

Table 2: Utility values for the Health, Wealth and Location example. Wr means rich, Wm is
medium income, Wp means poor. Hf is healthy and at top fitness, Hg means good health,
and Hs means sick. Lci stands for city location, and Lco means countryside location.
Therefore, no symmetric independence condition exists here, and that rules out any additive or multiplicative independence, conditional or not, between any subsets of attributes.
Also, since no single variable is unconditionally UI, then no subset can be unconditionally
UI. Further, the fact that preferences over L depend on the combination of H and W rules
out a GAI decomposition of the form {W, L}, {W, H}, {H, L}.
87

fiEngel & Wellman

We can, however, achieve decomposition using CUI. It is the case that CUI(W, L|H),
since each column on the left matrix (Lci ) is an affine transformation of its counterpart on
the right side (Lco ). For example, to transform the first column (Hf ), multiply by 23 and
add 2.
This example illustrates the subtlety of utility independence. In particular, whereas
preferences over L depend on W , W may still be (conditionally) UI of L. A CAI assumption for the same attributes must inevitably ignore the reversal of preferences over L for
different values of W , hence a decision maker that will be queried for preferences under this
assumption may not be able to provide meaningful answers.
The interaction with a system that requires preference representation normally requires
the identification of structure, and then the population of the utility values that are required
by the compact representation. It is therefore most important that these two aspects are
simplified as possible, whereas the functional form handled by the system may be more
sophisticated. This is exactly the tradeoff made by CUI nets, compared to a GAI-based
representation: if the GAI condition is based on CAI, CUI nets achieve lower dimensionality
(Section 7), and therefore easier elicitation. If a GAI condition is not based on a collection
of CAI conditions, it is hard to identify. CUI nets simplify these bottleneck aspects, by
driving the complexity into the algorithms and into the functional form that is handled
behind the scenes.
2.2 Related Work
Perhaps the earliest effort to exploit separable preferences in a graphical model was the extension of influence diagrams by Tatman and Shachter (1990) to decompose value functions
into sums and products of multiple value nodes. This structure provided computational
advantages, enabling the use of dynamic programming techniques exploiting value separability.
Bacchus and Grove (1995) were first to develop a graphical model based on conditional
independence structure. In particular, they establish that the CAI condition has a perfect
map (Pearl & Paz, 1989); that is, a graph with attribute nodes S such that node separation
reflects exactly the set of CAI conditions on S. More specifically, for any two sets of nodes
X, Y  S, CAI(X, Y |XY ) holds if and only if there is no direct edge between a node in X
and a node in Y . We use the term CAI map of S when referring to the graph which reflects
the perfect map of CAI conditions, in the context of a preference order over D(S). Bacchus
and Grove go on to show that the utility function has a GAI decomposition over the set of
maximal cliques of the CAI map. As we show in Section 7, the CUI network representation
developed here achieves weakly better dimensionality than CAI maps due to the greater
generality of the independence assumption.
Initiating another important line of work, Boutilier et al. (1999) introduced CP networks, an efficient representation for ordinal preferences over multiple attributes. In a
CP network, each variable is conditionally PI of the rest given its parents. Ordinal multiattribute preference representation schemes (for decision making under certainty), and
especially CP networks, can dramatically simplify the preference elicitation process, based
as they are on intuitive relative preference statements that avoid magnitude considerations.
However, the limited expressive power of CP networks may not suffice for complex decision

88

fiCUI networks

problems, in which tradeoff resolution may hinge in a complicated way on attribute settings
over rich domains. This problem is particularly acute when continuous or almost continuous
attributes are involved, such as money or time.
Boutilier et al. (2001) subsequently extended this approach to numeric, cardinal utility
in UCP networks, a graphical model that utilizes the GAI decomposition combined with
a CP-net topology. This requires dominance relations between parents and their children,
somewhat limiting the applicability of the representation. The GAI structure was also
applied for graphical models by Gonzales and Perny (2004), who employ the clique graph
of the CAI map (the GAI network ) for elicitation purposes.
In earlier work, La Mura and Shoham (1999) redefine utility independence as a symmetric multiplicative condition, taking it closer to its probability analog, and supporting a
Bayes-net like representation. Although multiplicative independence is different from additive independence, it is not necessarily weaker. Recent work by Abbas (2005) defines
a subclass of utility functions on which a multiplicative notion of UI obeys an analog of
Bayess rule.
The only graphical decomposition suggested in the past for utility functions that is based
on the original, non-symmetric notion of utility independence is the utility tree (Von Stengel,
1988, see also Wellman and Doyle, 1992, for discussion in an AI context). The utility tree
decomposes the utility function using multilinear or multiplicative decomposition (Keeney
& Raiffa, 1976), and then further tries to decompose each subset similarly. Using these
hierarchical steps the utility function becomes a nested expression over functions of its
smallest separable subsets and their complements.
2.3 Graphical Models of CUI
In their concluding remarks, Bacchus and Grove (1995) suggest investigating graphical
models of other independence concepts, in particular utility independence. Founding a
graphical model on UI is more difficult, however, as utility independence does not decompose
as effectively as does additive independence. In particular, the condition U I(Y, X) ensures
that Y has a subutility function, but since X does not have one it is harder to carry on
the decomposition into X. Hence in the case that X is large the dimensionality of the
representation may remain too high. Our approach therefore employs CUI conditions on
large subsets Y , in which case the decomposition can be driven further by decomposing the
conditional utility function of Y using more CUI conditions.
In the sequel we show how serial application of CUI leads to functional decomposition.
The corresponding graphical model, a CUI network, provides a lower-dimension representation of the utility function in which the function for any vertex depends only on the node
and its parents. We demonstrate the use of CUI networks by constructing an example
for a relatively complex domain. Next we elaborate on the technical and semantic properties of the model and knowledge required to construct it. Subsequent technical sections
present optimization algorithms and techniques for further reducing the complexity of the
representation.

89

fiEngel & Wellman

3. CUI Networks
We begin by constructing a DAG representing a set of CUI conditions, followed by a derivation of the functional decomposition over the nodes of the DAG.
3.1 CUI DAG
Suppose that we obtain a set  of CUI conditions on the variable set S = {x1 , . . . , xn }, such
that for each x  S,  contains a condition of the form
CUI(S \ ({x}  P (x)) , x | P (x)).
In other words, there exists a set P (x) that separates the rest of the variables from x. Such
P (x) always exists, because for P (x) = S \ {x} the condition above trivially holds. The set
 can be represented graphically by the following procedure, which we name procedure C.
1. Define an order on the set S (for convenience we assume the ordering x1 , . . . , xn ).
2. Define the set of parents of x1 as P a(x1 ) = P (x1 ).
3. For each i = 2, . . . , n
 i ), as the set of nodes in
 Define the set of intermediate descendants of xi , Dn(x
x1 , . . . , xi1 that turned out to be descendants of xi , that is those for which xi is
 i ) is the smallest
a parent or another descendant of xi is a parent. Formally, Dn(x
set that satisfies the following condition:
 i )  P a(xj )
j  {1, . . . , i  1}, [xi  P a(xj ), or k  {1, . . . , i  1}.xk  Dn(x
 i ).] (2)
 xj  Dn(x
 Define the parents of xi to be the nodes in P (xi ) which are not already descendants of xi ,
 i ).
P a(xi ) = P (xi ) \ Dn(x
This procedure defines a DAG. We denote Dn(x) as the final set of descendants of x. It
is the set defined by Equation (2), when replacing {1, . . . , i  1} with {1, . . . , n}). By their

definitions, Dn(x)  Dn(x),
hence

P a(x)  Dn(x)  P a(x)  Dn(x)
= P (x).

(3)

Proposition 1. Consider the DAG defined by procedure C for a set of attributes S. For
any x  S,
CUI(S \ ({x}  P a(x)  Dn(x)) , x | P a(x)  Dn(x)).
(4)

Proof. By the definitions of P a(x) and P (x), (4) holds when replacing Dn(x) with Dn(x).
From the definition of CUI, it is straightforward that
CUI(S \ (Y  W ) , Y | W )  CUI(S \ (Y  W  Z) , Y | W  Z),
because invariance of preference order over S \ (Y  W ) implies invariance of preference
order over its subset S \ (Y  W  Z), when the difference set Z is fixed. Given (3), and


taking W = P a(x)  Dn(x)
and Z = Dn(x) \ Dn(x),
we get (4).
90

fiCUI networks

As an example, we show the construction of the structure on a small set of variables
S = {x1 , x2 , x3 , x4 , x5 , x6 }, for which we are given the following set of CUI conditions:
 = {CUI({x4 , x5 , x6 }, x1 | {x2 , x3 }), CUI({x4 , x3 , x6 }, x2 | {x1 , x5 }),
CUI({x2 , x4 , x6 }, x3 | {x1 , x5 }), CUI({x1 , x3 , x5 }, x4 | {x2 , x6 }),
CUI(x6 , x5 | {x1 , x2 , x3 , x4 }), CUI({x1 , x2 , x3 , x5 }, x6 | x4 )}.
Construction of the network using the order implied by the indices results in the CUI
DAG illustrated in Figure 1. The minimal separating set for x1 is {x2 , x3 }. For x2 , we get
 2 ) = {x1 }, and the only non-descendant variable that is required to separate it from
Dn(x
the rest is x5 , which is therefore its only parent. The rest of the graph is constructed in
a similar way. When x4 is placed, we find that P (x4 ) = {x2 , x6 }. Therefore, x4 becomes
 2 )  {x4 } = {x1 , x4 }.
descendant of x2 after x2 is placed, in other words Dn(x2 ) = Dn(x
ix6

ix5
 Z

Z
=x

~ ix
Z
i
2



?
=x

i
4

3

Z

Z

~i
Z
=x

1

Figure 1: CUI DAG given  and order x1 ,. . . ,x6 .

Definition 7. Let U (S) be a utility function representing cardinal preferences over D(S).
A CUI DAG for U () is a DAG, such that for any x  S, (4) holds.
Procedure C yields a CUI DAG by Proposition 1. As the other direction, any given CUI
DAG G (in which parents and descendants are denoted by P aG (), DnG (), respectively)
can be constructed using C, as follows. Define P (x) = P aG (x)  DnG (x) and a variable
ordering according to the reverse topological order of G, and complete the execution of C.
It is straightforward to show that the set of parents selected for each xi is exactly P aG (xi ),
hence the result is a DAG which is identical to G.
3.2 CUI Decomposition
We now show how the CUI conditions, guaranteed by Proposition 1, can be applied iteratively to decompose U () to lower dimensional functions. We first pick a variable ordering
that agrees with the reverse topological order of the CUI DAG. To simplify the presentation,
we rename the variables so the ordering is again x1 , . . . , xn . The CUI condition (4) on x1
implies the following decomposition, according to (1):
U (S) = f1 (x1 , P a(x1 ), Dn(x1 )) + g1 (x1 , P a(x1 ), Dn(x1 ))Ux01 (S \ {x1 }).

(5)

Note that Dn(x1 ) = .
We assume that we have specified a reference point S 0 , which is an arbitrary value chosen
for each attribute x  S, denoted by x0 . Ux01 () on the right hand side is the conditional
91

fiEngel & Wellman

utility function on S given that x1 is fixed at a reference point x01 . For convenience we omit
attributes whose values are fixed from the list of arguments.
By applying the decomposition based on the CUI condition of x2 on the conditional
utility function Ux01 (), we get
Ux01 (S \ {x1 }) = f2 (x, P a(x2 ), Dn(x2 )) + g2 (x2 , P a(x2 ), Dn(x2 ))Ux01 ,x02 (S \ {x1 , x2 }). (6)
Note that Dn(x2 )  {x1 }, and x1 is fixed to x01 , hence f2 and g2 effectively depend only on
x2 and P a(x2 ). This point is exploited below.
Substituting Ux01 () in (5) according to (6) yields:
U (S) = f1 + g1 (f2 + g2 Ux01 ,x02 (S \ {x1 , x2 })) = f1 + g1 f2 + g1 g2 Ux01 ,x02 (S \ {x1 , x2 }).
The list of arguments to the functions fj , gj are always (xj , P a(xj ), Dn(xj )), and we omit
it for readability.
We continue in this fashion and get
U (S) =

i1
X

k1
Y

(fk

j=1

k=1

gj ) +

i
Y

gj Ux01 ,...,x0 (xi , . . . , xn ),
i1

j=1

and apply the CUI condition of xi ,
Ux01 ,...,x0 (xi , xi+1 , . . . xn ) =
i1

fi (xi , P a(xi ), Dn(xi )) + gi (xi , P a(xi ), Dn(xi ))Ux01 ,...,x0 (xi+1 , . . . , xn ). (7)
i

For convenience, we define the constant function fn+1  Ux01 ,...,x0n (). Ultimately we obtain
U (S) =

n+1
X

i1
Y

i=1

j=1

(fi (xi , P a(xi ), Dn(xi ))

gj (xj , P a(xj )), Dn(xj )).

(8)

The variable ordering is restricted to agree with the reverse topological order of the graph,
hence in (7), Dn(xi )  {x1 , . . . , xi1 }. Therefore, all the variables in Dn(xi ) on the righthand side of (7) are fixed on their reference points, so fi and gi only depend on xi and
P a(xi ). Formally, let y1 , . . . , yk be the variables in Dn(xi ). With some abuse of notation,
we define:
fi (xi , P a(xi )) = fi (xi , P a(xi ), y10 , . . . , yk0 ),
gi (xi , P a(xi )) = gi (xi , P a(xi ), y10 , . . . , yk0 ).

(9)

Now (8) becomes
U (S) =

n+1
X

i1
Y

i=1

j=1

(fi (xi , P a(xi ))

gj (xj , P a(xj ))).

(10)

This term is a decomposition of the multiattribute utility function to lower dimensional
functions, whose dimensions depend on the number of variables of P a(x). As a result, the
92

fiCUI networks

dimensionality of the representation is reduced (as in Bayesian networks) to the maximal
number of parents of a node plus one.
We illustrate how the utility function is decomposed in the example of Figure 1. We
pick the ordering x4 , x1 , x6 , x3 , x2 , x5 that agrees with the reverse topological order of
the graph (note that we are not renaming the variables here). To simplify notation we
denote the conditional utility function in which xi is fixed on the reference point by adding
a subscript i to U ().
U (S) = f4 (x4 x2 x6 ) + g4 (x4 x2 x6 )U4 (S \ {x4 })
U4 (S \ {x4 }) = f1 (x1 x2 x3 ) + g1 (x1 x2 x3 )U1,4 (S \ {x4 x1 })
U1,4 (S \ {x4 x1 }) = f6 (x6 ) + g6 (x6 )U1,4,6 (x2 x3 x5 )
U1,4,6 (x2 x3 x5 ) = f3 (x3 x5 ) + g3 (x3 x5 )U1,3,4,6 (x2 x5 )
U1,3,4,6 (x2 x5 ) = f2 (x2 x5 ) + g2 (x2 x5 )U1,2,3,4,6 (x5 )
U1,2,3,4,6 (x5 ) = f5 (x5 ) + g5 (x5 )U1,2,3,4,5,6 ()
Note that each fi and gi depends on xi and its parents. Merging the above equations, and
using the definition f7  U1,2,3,4,5,6 () produces
U (S) = f4 + g4 f1 + g4 g1 f6 + g4 g1 g6 f3 + g4 g1 g6 g3 f2 + g4 g1 g6 g3 g2 f5 + g4 g1 g6 g3 g2 g5 f7 . (11)
We established that U (S) can be represented using a set of functions F, that includes,
for any x  S, the functions (fx , gx ) resulting from the decomposition (1) based on the CUI
condition (4). This means that to fully specify U (S) it is sufficient to obtain the data for
functions in F (this aspect is discussed in Section 5).
Definition 8. Let U (S) be a utility function representing cardinal preferences over D(S).
A CUI network for U () is a triplet (G, F, S 0 ). G = (S, E) is a CUI DAG for U (S), S 0 is
a reference point, and F is the set of functions {fi (xi , P a(xi )), gi (xi , P a(xi )) | i = 1 . . . , n}
defined above.
The utility value for any assignment to S can be calculated from the CUI network
according to (10), using any variable ordering that agrees with the reverse topological order
of the DAG. In our example, we can choose a different variable ordering than the one used
above, such as x1 , x3 , x4 , x2 , x5 , x6 , leading to the following expression.
U (S) = f1 + g1 f3 + g1 g3 f4 + g1 g3 g4 f2 + g1 g3 g4 g2 f5 + g1 g3 g4 g2 g5 f6 + g1 g3 g4 g2 g5 g6 f7 .
This sum of product is different than the one in (11). However, it is based on the same CUI
decompositions and therefore the same functions (fi , gi ).
3.3 Properties of CUI Networks
Based on Procedure C and the decomposition following it, we conclude the following.
Proposition 2. Let S be a set of attributes, and  a set of CUI conditions on S. If 
includes a condition of the form CUI(S \ (x  Zx ), x | Zx ) for each x  S, then  can be
represented by a CUI network whose dimensionality does not exceed maxx (|Zx | + 1).
93

fiEngel & Wellman

Note that Zx denotes here a minimal set of attributes (variables) that renders the rest
CUI of x. This bound on the dimensionality will be obtained regardless of the variable
ordering. We can expect the maximal dimension to be lower if the network is constructed
using a good variable ordering. A good heuristic in determining the ordering would be to
use attributes with smaller dependent sets first, so that the attributes with more dependents
would have some of them as descendants. Based on such an ordering we would expect the
less important attributes to be lower in the topology, while the more crucial attributes
would either be present higher or have a larger number of parents.
From this point we usually omit the third argument when referring to a CUI condition,
as in CUI(X, Y ), which is taken equivalent to CUI(X, Y | S \ (X  Y )).
In order to achieve a low dimensional CUI networks, we are required to detect CUI
conditions over large sets. This may be a difficult task, and we address it through an
example in Section 4. The task is made somewhat easier by the fact that the set has to be
CUI of a single variable; note that the condition CU I(Y, x) is weaker than the condition
CU I(Y, X) when x  X. Furthermore, Section 7 shows how the dimensionality can be
reduced if the initial CUI decomposition is not sufficiently effective.
Based on properties of CUI, we can read additional independence conditions off the
graph. First, we observe that CUI has a composition property at the second argument.
Lemma 3. Let CUI(X, Y ) [X, Y  S], and CUI(A, B) [A, B  S]. Then
CUI(A  X , Y  B).
This property leads to the following claim, which allows us to derive additional CUI
conditions once the graph is constructed.
Proposition
4. Consider S
a CUI network for a set of attributes S. Define P a(X) =
S
P
a(x)
and
Dn(X)
=
xX Dn(x). Then for any X  S,
xX
CUI(S \ (X  P a(X)  Dn(X)) , X).
Proof. By recursion on X, using Lemma 3 and Proposition 1.
We also consider the other direction, by defining the set of nodes that renders a set CUI
of the rest. This dual perspective becomes particularly useful for optimization (Section 6),
because optimization based on the preference order over an attribute is meaningful only
when holding enough other attributes fixed to make it CPI or CUI of the rest. Let Ch(X)
denote the union of children of nodes in X, and let An(X) denote all of the ancestors of
nodes in X, in both cases excluding nodes which are themselves in X.
Proposition 5. Consider a CUI network for a set of attributes S. CUI(X, S \(X An(X)
Ch(X))) for any X  S.
Proof. Let y 
/ X  Ch(X)  An(X). Then clearly x  X, x 
/ P a(y)  Dn(y). Hence from
Proposition 1, CUI(X, y). We apply Lemma 3 iteratively for each y 
/ X  Ch(X)  An(X)
(note that the first argument is X for each CUI condition, so it is X in the result as well),
and get the desired result.
We conclude this section by relating CUI networks to CAI maps.
94

fiCUI networks

Proposition 6. Let G = (X, E) be a CAI map, and x1 , . . . , xn an ordering over the nodes
in X. Let G0 = (X, E 0 ) be the DAG such that there is a directed arc (xi , xj ) in E 0 iff i < j
and (xi , xj )  E. Then G0 is a CUI network.
We note, however, that CAI maps decompose the utility function over the maximal
cliques, whereas CUI networks decompose over nodes and their parents. Section 7 bridges
this gap. In addition, this result is used in Section 6.3.

4. CUI Modeling Example
To demonstrate the potential representational advantage of CUI networks we require a domain that is difficult to simplify otherwise. The example we use is the choice of a software
package by an enterprise that wishes to automate its sourcing (strategic procurement) process. We focus on the softwares facilities for running auction or RFQ (request for quotes)
events, and tools to select winning suppliers either manually or automatically.
We identified nine key features of these kinds of software packages. In our choice scenario,
the buyer evaluates each package on these nine features, graded on a discrete scale (e.g.,
one to five). The features are, in brief:
Interactive Negotiations (IN ) allows a separate bargaining procedure with each supplier.
Multi-Stage (MS ) allows a procurement event to be comprised of separate stages of different types.
Cost Formula (CF ) buyers can formulate their total cost of doing business with each
supplier.
Supplier Tracking (ST ) allows long-term tracking of supplier performance.
MultiAttribute (MA) bidding over multiattribute items, potentially using a scoring function.1
Event Monitoring (EM ) provides an interface to running events and real-time graphical
views.
Bundle Bidding (BB ) bidding for bundles of goods.
Grid Bidding (GB ) adds a bidding dimension corresponding to an aspect such as time
or region.
Decision Support (DS ) tools for optimization and for aiding in the choice of the best
supplier(s).
We observe first that additive independence does not widely apply in this domain. For
example, Multi-Stage makes several other features more useful or important: Interactive
Negotiations (often useful as a last stage), Decision Support (to choose which suppliers
1. We hope the fact that the software itself may include facilities for multiattribute decision making does
not cause undue confusion. Naturally, we consider this an important feature.

95

fiEngel & Wellman

proceed to the next stage), and Event Monitoring (helps keep track of how useful was each
stage in reducing costs). Conversely, in some circumstances Multi-Stage can substitute for
the functionality of other features: MultiAttribute (by bidding on different attributes in different stages), Bundle Bidding (bidding on separate items in different stages), Grid Bidding
(bidding on different time/regions in different stages) and Supplier Tracking (by extracting
supplier information in a Request for Information stage). The potential dependencies for
each attribute are shown in Table 3.
Attr
EM
IN
CF
ST
MA
MS
DS
GB
BB

Complements
CF ST MS
ST MS MA
EM MS DS MA GB BB
EM MS IN DS
IN DS CF
DS EM IN ST
CF MA GB ST BB MS
CF DS
CF DS

Substitutes

DS
MA
MS BB ST GB
GB BB MA CF
MA MS BB
MA MS GB

CUI set
IN,DS,MA,GB,BB
EM,CF,ST,DS,GB,BB
MA,GB,BB
IN,CF,GB,BB
GB,BB
MA,GB,BB
IN,EM
MA,BB
MA,GB

Table 3: Dependent and independent sets for each attribute.
The presence of a complement or substitute relation precludes additive independence.
From this fact we can identify a set of six attributes that must be mutually (additive) dependent: {BB , GB , DS , MA, MS , CF }. In consequence, the best-case dimensionality achieved
by a CAI map (and other CAI-based representations, see Section 2.2), for this domain would
be six, the size of the largest maximal clique.
In order to construct a CUI network we first identify, for each attribute x, a set Y that is
CUI of it. We can first guess such a set according to the complement/substitute information
in Table 3; typically, the set of attributes that are neither complements nor substitutes would
be CUI. This is the approach taken for the attributes EM and DS . However, attributes that
are complements or substitutes may still be CUI of each other, and we therefore attempt
to detect and verify potentially larger CUI sets. Keeney and Raiffa (1976) provide several
useful results that can help in detection of UI, and those results can be generalized to CUI.
In particular they show that we can first detect a conditional preferential independence
(CPI) condition in which one element is also CUI. Based on this result, in order to verify
for example that
CUI({BB , GB , MA}, CF | S \ {BB , GB , MA, CF }),

(12)

the following two conditions are sufficient:
CPI({BB , GB , MA}, CF | S \ {BB , GB , MA, CF }),

(13)

CUI(BB , {GB , MA, CF } | S \ {BB , GB , MA, CF }).

(14)

Detection and verification of these conditions are also discussed by Keeney and Raiffa (1976).
For our example, we observe that the features BB , GB , and MA each add a qualitative
96

fiCUI networks

element to the bidding. Each bidding element is best exploited when cost formulation
is available, so complements CF . The complementarity is similar for each feature, thus
implying (13). Moreover, BB is a crucial feature and therefore the risk attitude towards it
is not expected to vary with the level of CF , MA, and GB , and that implies (14), together
leading to (12).
In a similar fashion, we observe that the nature of the substitutivity of the three
mechanisms BB , GB , MA in MS is similar: each can be simulated using multiple stages.
That means that the tradeoffs among the three do not depend on MS , meaning that
CPI({BB , GB , MA}, MS ) holds. Next, the dependency among the triplet {BB , GB , MA} is
also a result of the option to substitute one by another. As a result, each pair is CPI of the
third. Finally, we find that the complementarity of ST and IN is marginal and does not
affect the tradeoffs with other attributes. We can therefore verify the following conditions:
CUI({BB , GB , MA}, MS ), CUI({BB , GB }, MA), CUI({ST , EM , CF , DS , GB , BB }, IN ),
and CUI({GB , BB , CF , IN }, ST ). The resulting maximal CUI sets for each attribute are
shown in Table 3.
To construct the network we start with the variable with the largest CUI set, IN , which
needs only MS and MA as parents, after which it is EM that gets CF , MS , and ST as
parents. Next, we consider ST which needs four attributes in its conditional set, but EM is
a descendant, therefore only DS , MS , and MA are needed as parents. The next variable to
choose is MS , which needs only CF and DS as parents since the other dependant variables
are descendants. Had we chosen CF before MS it would have needed four parents: IN , MS ,
ST , and DS (note that although IN is CUI of CF and so is the set {BB , GB , MA}, this
is not the case for the union {BB , GB , MA, IN }). Now that we choose CF after MS it has
MS , ST , and IN as descendants and therefore only DS is a parent. The complete variable
ordering is IN , EM , ST , MS , CF , DS , MA, GB , BB , and the resulting CUI network is
depicted in Figure 2. The maximal dimension is four.
The structure we obtained over the utility function in the above example is based largely
on objective domain knowledge, and may be common to various sourcing departments.
This demonstrates an important aspect of graphical modeling captured by CUI networks:
encoding qualitative information about the domain, thus making the process of extracting
the numeric information easier. This structure in some cases differs among decision makers,
but in other cases (as above) it makes sense to extract such data from domain experts and
reuse this structure across decision makers.

5. Representation and Elicitation
In this section, we derive an expression for local node data in terms of conditional utility functions, and discuss how to elicit utility information from judgments about relative
preference differences.
5.1 Node Data Representation
Representing U by a CUI network requires that we determine the f and g functions for each
CUI condition. At any node y the functions f, g represent the affine transformation of the
conditional utility function U (x0 , Y, Z) (here Z = P a(x)) to strategically equivalent utility
functions for other values of x. Like the transformation functions for UI (Keeney & Raiffa,
97

fiEngel & Wellman

Figure 2: CUI network for the example. The maximal number of parents is 3, leading to
dimension 4.

1976), the transformation functions for CUI can be represented in terms of the conditional
utility functions U (x, Y 1 , Z) and U (x, Y 2 , Z) for suitable values Y 1 and Y 2 (see below).
We can determine f and g by solving the system of two equations below, both based on
applying (1) for these specific values of Y :
U (x, Y 1 , Z) = f (x, Z) + g(x, Z)U (x0 , Y 1 , Z),
U (x, Y 2 , Z) = f (x, Z) + g(x, Z)U (x0 , Y 2 , Z),
yielding
U (x, Y 2 , Z)  U (x, Y 1 , Z)
,
U (x0 , Y 2 , Z)  U (x0 , Y 1 , Z)
f (x, Z) = U (x, Y 1 , Z)  g(x, Z)U (x0 , Y 1 , Z).
g(x, Z) =

(15)
(16)

The only restriction on the choice of Y 1 , Y 2 is that the decision maker must not be
indifferent between them given x0 and the current assignment to Z. For example, Y 1 , Y 2
may differ on any single attribute y  Y that is strictly essential.
5.2 Elicitation of Measurable Value Functions
A utility function that is a used for choosing an action that leads to a known probability
distribution over the outcomes, should be obtained through elicitation of preferences over
lotteries, for example using even-chance gambles and their certainty equivalents (Keeney &
Raiffa, 1976). Based on the preceding discussion, to fully specify U () via a CUI network,
we need to obtain the numeric values for the conditional utility functions U (x, Y 1 , P a(x))
and U (x, Y 2 , P a(x)) for each node x. This is significantly easier than obtaining the full
n-dimensional function, and in general can be done using methods described in preference
98

fiCUI networks

elicitation literature (Keeney & Raiffa, 1976). In this section we show how elicitation can
be conducted in cases when the choice is assumed to be done over certain outcomes, but a
cardinal representation is nevertheless useful.
For particular applications we can point out specific attributes that can be used as a
measurement for others. The most common example is preferences that are quasi-linear in
a special attribute such as money or time. These kind of preferences can be represented by
a measurable value function, or MVF (Krantz et al., 1971; Dyer & Sarin, 1979). An MVF
is a cardinal utility function defined under certainty and represents preference differences.
It has been shown (Dyer & Sarin, 1979) that UI has an analogous interpretation for MVF
with similar resulting decomposition. The extension to CUI is straightforward.
For the case of monetary scaling, the preference difference over a pair of outcomes
represents the difference in willingness to pay (wtp) for each. A potential way to elicit the
MVF is by asking the decision maker to provide her wtp to improve from one outcome to
another, particularly when these outcomes differ over a single attribute.
Under this interpretation, we first observe from (15) that g(x, Z) can be elicited in terms
of preference differences, between outcomes that possibly differ over a single attribute. The
result can convey qualitative preference information. Assume Y 2  Y 1 and that x.x0  x.
Then g(x, Z) is the ratio of the preference difference between Y 1 and Y 2 given x to the
same difference given x0 (Z is fixed in all outcomes). Hence, if Y and x are complements
then g(x, Z) > 1 and increasing in x. If Y and x are substitutes, g(x, Z) < 1 and decreasing
in x. This holds regardless of the choice for Y 1 , Y 2 , since by CUI(Y, x | Z) all attributes in
Y maintain the same complementarity or substitutivity relationship to x. Note also that
g(x, Z) = 1 iff CAI(Y, x | Z). Another important observation is that though both Y and x
may depend on Z, in practice we do not expect the level of dependency between Y and x to
depend on the particular value of Z. In that case g becomes a single-dimensional function,
independent of Z.
f (x, Z), intuitively speaking, is a measurement of wtp to improve from x0 to x. The
value U (x0 , Y 1 , Z) is multiplied by g(x, Z) to compensate for the interaction between Y
and x, allowing f () to be independent of Y . If we perform the elicitation obeying the
topological order of the graph, the function U (x0 , Y 1 , Z) can be readily calculated for each
new node from data stored at its predecessors. Choose Y 1 = Y 0 , and let Z = {z1 , . . . , zk },
ordered such that children precede parents. Since Y, x are fixed on the reference point,
k
i1
X
Y
U (x , Y , Z) =
(fzi
gzj )fn+1 ().
0

0

i=1

j=1

Now we can obtain f (x, Z) as follows: first we elicit the preference difference function
e(x, Z) = U (x, Y 1 , Z)  U (x0 , Y 1 , Z). Then, assuming g(x, Z) was already obtained, calculate:
f (x, Z) = e(x, Z)  (g(x, Z)  1)U (x0 , Y 1 , Z).

6. Optimization
One of the primary uses of utility functions is to support optimal choices, as in selecting an
outcome or action. The complexity of the choice depends on the specific properties of the
99

fiEngel & Wellman

environment. When the choice is among a limited set of definite outcomes, we can recover
the utility of each outcome using the compact representation and choose the one with the
highest value. For instance, in the software example of Section 4 we would normally choose
among an enumerated set of vendors or packages. In this procurement scenario we assume
the utility is an MVF, and we usually choose the outcome that yields the highest utility
net of price. In case of decision under uncertainty, when the choice is among actions that
lead to probability distributions over outcomes, the optimal choice is selected by computing
the expected utility of each action. If each action involves a reasonably bounded number of
outcomes with non-zero probability, this again can be done by exhaustive computation.
Nevertheless, it is often useful to directly identify the maximal utility outcome given a
quantitative representation of utility. In case of a direct choice over a constrained outcome
space, the optimization algorithm serves as a subroutine for systematic optimization procedures, and such can be adapted from the probabilistic reasoning literature (Nilsson, 1998).
The algorithm may also be useful as a heuristic aid for optimization of expected utility or
net utility mentioned above, when the set of possible outcomes is too large for an explicit,
exhaustive choice.
In this section, we develop optimization algorithms for discrete domains, and show how
in many cases CUI networks can provide leverage for optimization of CAI maps. As is
typical for graphical models, our optimization algorithm is particularly efficient when the
graph is restricted to a tree.
6.1 Optimization Over CUI Trees
Definition 9. A CUI tree is a CUI network in which no node has more than one child.
Note that this type of graph corresponds to an upside-down version of a standard directed tree (or a forest).
Let T be a CUI tree. We assume WLOG that T is connected (a forest can be turned
into a tree by adding arcs). As an upside-down sort of tree, it has any number of roots, and
a single leaf. We denote the root nodes by ai  {a1 , . . . , ak }, the child of ai by bi , and so
on. For each root node ai , we define the function
hai (bi ) = arg 0 max U (bi , a0i ),
ai D(ai )

denoting the selection of an optimal value of ai corresponding to a given value of its child.
From Proposition 5, hai does not depend on the reference values chosen for S \ {ai , bi }. The
function hai (), which we call the optimal value function (OVF) of ai , is stored at node ai
since it is used by its descendants as described below.
Next, each bi has no children or a single child ci , and any number of parents. For simplicity of exposition we present the case that bi has two parents, ai and aj . The maximization
function for bi is defined as
hbi (ci ) = arg 0 max U (ci , b0i , hai (b0i ), haj (b0i )).
bi D(bi )

In words, we pick the optimal value of bi for each assignment to its child and its parents.
But since we already know the optimum of the parents for each value of bi , we need only
consider this optimum for each evaluation on the domain of bi .
100

fiCUI networks

(a)

(b)

Figure 3: CUI networks in optimization examples: (a) Tree (b) Non-tree
The only external child of the set {ai , aj , bi } is ci , and it has no external ancestors,
hence {ai , aj , bi } is CUI of the rest given ci , therefore the maximization above again does
not depend on the reference values of the rest of the attributes. Similarly, when computing
hci (di ) for the child ci of bi , each value for ci fixes bi (and any other parents of ci ), and that
fixes ai and aj (and the other ancestors of ci ). The last computation, at the leaf x, evaluates
each value of x. Each value x0 causes this cascade of fixed values to all of the ancestors,
meaning we finally get the optimal choice by comparing |D(x)| complete assignments.
We illustrate the execution of the algorithm on the CUI tree of Figure 3a. We compute
ha (c) which is the optimal value of a for each value of c, and similarly hb (c). Next, to
compute hc (e), for each value e0 of e we compare all outcomes (e0 , c0 , ha (c0 ), hb (c0 )), c0  D(c).
At node d we compute hd (f ), which is independent of the other nodes. At node e we compute
he (f ) = arg maxe0 U (f, e0 , hc (e0 ), hb (hc (e0 )), ha (hc (e0 )) (node d can be ignored here) and at
node f it is
hf () = arg max U (f 0 , he (f 0 ), hd (f 0 ), hc (he (f 0 )), hb (hc (he (f 0 ))), ha (hc (he (f 0 ))).
f 0 D(f )

Note how each candidate value of f causes the cascade of optimal values to all of its
ancestors. The solution is then hf () and the resulting values of all the ancestors.
This optimization algorithm iterates over the nodes in topological order, and for each xi
it calculates the OVF hxi (xj ), where xj is the child of xi . This calculation uses the values
of the OVF stored for its parents, and therefore involves comparison of |D(xi )||D(xj )|
outcomes. In case the numeric data at the nodes is available, factoring in the time it takes
to recover the utility value for each outcome (which is O(n)), the algorithm runs in time
O(n2 maxi |D(xi )|2 ).
6.2 Optimization Over General DAGs
A common way in graphical models to apply tree algorithms to non-trees is by using the
junction graph. However, the common notion of a junction graph for DAG is a polytree,

101

fiEngel & Wellman

whereas our algorithm above is specialized to a (unit) tree. Instead, we optimize the CUI
network directly by generalizing the tree algorithm.
In the tree case, fixing the value of the child of a node x is sufficient in order to separate
x from the rest of the graph, excluding ancestors. We consider each value of the child at a
time, so it also determines the values for all the ancestors. In a general DAG it is no longer
sufficient for the OVF to depend on the children, because they do not provide sufficient
information to determine the values of An(x). Hence we generalize this notion to be the
scope of x (Sc(x), defined below), which is a set of nodes on which the OVF of x must
depend, in order for an iterative computation of the OVF to be sound.
With this generalization, the DAG algorithm is similar to the tree algorithm. Let G be
a CUI network, and x1 , . . . , xn a variable ordering that agrees with the topological order of
G (parents precede children). For each xi (according to the ordering), compute hxi (Sc(xi ))
for any instantiation of Sc(xi ). The optimal instantiation can now be selected backwards
from hxn (), since for each node xi that is reached the values for Sc(xi ) are already selected.
Sc(xi ) is computed as follows: scan variables xi+1 , . . . , xn in this order. When scanning
xj , add xj to Sc(xi ) if the following conditions hold:
1. There is an undirected path between xj and xi .
2. The path is not blocked by a node already in Sc(xi ).
By these conditions, Sc(xi ) includes all the children of xi , but non of xi s ancestor since
they precede xi in the ordering. In addition, Sc(xi ) includes all nodes that are needed to
block the paths that reach xi through its ancestors. For example, if xk , xj are children of
an ancestor xa of xi , and k < i < j, then xj must be in Sc(xi ), because of the path through
xa . The children of xj are blocked by xj , so unless they have another path to xi they will
not be in Sc(xi ). The children of xk , if ordered later than xi , will be in Sc(xi ) (but their
children will not), and so on.
Figure 3b is an example of a CUI network that is not a tree. We consider the scopes
under the variable ordering a, b, . . . , j. The scope of roots always equals their set of children
(because there is no other path reaching them), meaning Sc(a) = {d, e}, Sc(b) = {d, e, f },
Sc(c) = {e, f, h}, Sc(i) = {j}. The scope of d must include its child g and its siblings e
and f . All paths of h, j, and i to d are blocked by g, e, f therefore Sc(d) = {g, e, f }. For e,
we must include its child g, and its younger sibling f . h has a blocked path to e through
f  Sc(e), but also a non-blocked one through c 
/ Sc(e), therefore Sc(e) = {g, f, h}.
Similarly, g and h are in the scope of f due to paths through b and c respectively, hence
Sc(f ) = {g, h, j}. For g, in addition to its child h we add j whose path to g through f, b, e
is not blocked (Sc(g) = {h, j}) and finally Sc(h) = Sc(i) = {j} and Sc(j) = {}.
The next step, computing the OVF, requires that we compare a set of outcomes that
differ on xi  Co(xi ), where Co(xi ) is a set of nodes whose OVF can be determined by
xi  Sc(xi ) (hence they are covered by xi ). For this maximization to be valid, the condition
CUI(xi  Co(xi ), S \ (xi  Co(xi )  Sc(xi ))) must hold. We formally define Co(xi ), and
establish this result which is proved in the appendix.
Definition 10. Co(xi ) is the smallest set of nodes that satisfied the following condition
j < i, Sc(xj )  ({xi }  Sc(xi )  Co(xi ))  xj  Co(xi ).
102

(17)

fiCUI networks

Intuitively, xj is covered by xi if each node xk 6= xi in its scope, is either in the scope of xi
or was determined (according to its own scope) to be covered by xi . In Figure 3b, f  Co(g)
because Sc(f ) = {g}  Sc(g). e  Co(g) because Sc(e)  {g}  Sc(g)  {f }. Moreover,
Sc(d) = {g, e, f } hence d  Co(g) as well, and similarly we find that a, b  Co(g). In this
example all the nodes preceding g in the ordering are covered, but this is not necessarily
always the case.
Lemma 7. An assignment to xi and Sc(xi ) is sufficient to determine hxj () for each xj 
Co(xi ).
Lemma 8. For any node xi , CUI({xi }  Co(xi ), S \ ({xi }  Co(xi )  Sc(xi ))). Meaning
that xi and the nodes it covers are CUI of the rest given Sc(xi ).
When the algorithm reaches node xi , every choice of assignment to Sc(xi )  {xi } determines optimal values for Co(xi ) (Lemma 7). We compare the |D(xi )| assignments which
differ over the values of xi and Co(xi ), and select an optimal one as the value of hxi (Sc(xi )).
This optimum does not depend on the nodes in S \({xi }Co(xi )Sc(xi ))) due to Lemma 8.
To illustrate, we examine what happens when the algorithm reaches node g in Figure 3b.
At this point hx (Sc(x)) is known for any x that precedes g. As showed, all these nodes
are in Co(g). Indeed, the assignment to Sc(g) = {g, h, j} directly determines the value for
hf (), and then together with hf () it determines the value for he (), and further it cascades
to the rest of the nodes. The CUI network shows that CUI({a, b, c, d, e, f, g}, {i}) (given
{h, j}) and therefore the maximization operation (over the choice of value for g) is valid
regardless of the value of i.
The performance of the optimization algorithm is exponential in the size of the largest
scope (plus one). Note that this would be seriously affected by the choice of variable ordering. Also note, that in the case of a tree this algorithm specializes to the tree optimization
above, since there is no node that has a path to an ancestor of xi , except for other ancestors of xi which must precede xi in the ordering. Therefore it is always the case that
Sc(xi ) = Ch(xi ), meaning that hxi () is a function of its single child. Based on that, we
expect the algorithm to perform better the more similar the CUI network is to a tree.
6.3 CUI Tree for Optimization of CAI Maps
The optimization procedure for CUI trees is particularly attractive due to the relatively
low amount of preference information it requires. In some cases the comparison can be
done directly, without even having the data that comprises the utility function. Aside
from the direct benefit to CUI networks, we are interested in applying this structure to the
optimization of CAI maps. In some domains a CAI map is a simple and effective way to
decompose the utility function. However, the optimization of CAI maps is exponential in
the size of its tree width, and it requires the full data in terms of utility functions over its
maximal cliques. If a CAI map happens to have a simple structure, such as a tree, or the
CP condition, faster optimization algorithms can be used. However, it could be the case
that a CAI map is not a tree, but more subtle CUI conditions might exist which cannot be
captured by CAI conditions. If enough such conditions could be detected to turn the CAI
map into a CUI tree (or close enough to a tree), we could take advantage of our simple
optimization procedure.
103

fiEngel & Wellman

(a)

(b)

(c)

Figure 4: (a) A CAI map containing a cycle. (b) Enhanced CAI map, expressing CUI of
{a, d, f } in b. (c) An equivalent CUI tree.
Definition 11. Let G = (V, E) be a CAI map. An enhanced CAI map is a directed graph
G0 = (V, A), in which a pair of arcs (u, v), (v, u)  A implies the same dependency as an
edge (u, v)  E, and in addition for any node x, CUI(S \ ({x}  In(x)), x) (In(x) denoting
the set of nodes y for which (y, x)  A). We call the pair of arcs (u, v), (v, u)  A a hard
link and an arc (u, v)  A s.t. (v, u) 
/ A a weak link.
For any CAI map, an enhanced CAI map can be generated by replacing each edge (u, v)
with the arcs (u, v) and (v, u). This does not require any additional CUI conditions because
these are entailed by the CAI map. However, if additional CUI conditions as above can be
detected, we might be able to remove one (or both) of the directions. Figure 4a shows a
CAI map which contains a cycle. If we could detect that CUI({a, d, f }, b), we could remove
the direction (a, b) and get the enhanced CAI map in Figure 4b. The set of CUI conditions
implied by the enhanced CAI map can now be expressed by a CUI tree, as in Figure 4c.
Proposition 9. Consider an enhanced CAI map G. Let  be an ordering on the nodes of
G, and G0 a DAG which is the result of removing all arcs (u, v) whose direction does not
agree with . If for any such removed arc, v is an ancestor of u in G0 , then G0 is a CUI
network.
For hard links, the removal of (u, v) leaves v as a parent of u, so the condition trivially
holds. To obtain a CUI tree, the key is therefore to find a variable ordering under which
enough weak links can be removed to turn the graph into a tree, maintaining the condition of
Proposition 9. For a large number of variables, an exhaustive search over variable orderings
may not be feasible. However in many cases it can be effectively constrained, restricting
the number of orderings that we need to consider. For example, in order to break the cycle
in Figure 4b it is clear that the weak link (b, a) must be implied by the ordering, so that a
could be an ancestor of b. The only way for this to happen (given the existing hard links),
is that c is a parent of b, d is a parent of c, and a a parent of d.
Proposition 10. Let c = (y1 , . . . , yk ) be a cycle in an enhanced CAI map G. Assume that
c contains exactly one weak link: (yi , yi+1 ) for some i < k, or (yk , y1 ). Let  be a variable
104

fiCUI networks

ordering that does not agree with the order of the path p = (yi+1 , yi+2 , . . . , yk , y1 , . . . , yi ).
Then any CUI network constructed from G and  (by Proposition 9), is not a tree.
Therefore any cycle that contains one weak link leads to a constraint on the variable
ordering. Cycles with more than one weak link also lead to constraints. If c above has
another weak link (yj , yj+1 ), one of the two links must be removed, and the ordering must
agree with either the path p above or the path p0 = (yj+1 , yj+2 , . . . , yk , y1 , . . . , yj ). Assuming
WLOG that j > i, the paths (yi+1 , . . . , yj ) and (yj+1 , . . . , yi ) are required for both p and
p0 , and therefore can be used as constraints. Similarly we can find the intersection of the
paths implied by any number of weak links in a cycle.
Sometimes the constraint set can lead to an immediate contradiction, and in such case
search is redundant. If it does not, it can significantly reduce the search space. However,
the major bottleneck in preference handling is usually elicitation, rather than computation.
Therefore, given that a good variable ordering may lead to the reduction of the optimization
problem to a simpler, qualitative task, eliminating the need for a full utility elicitation, it
would be worthwhile to invest the required computation time.

7. Nested Representation
From Section 5.1 we conclude that node data can be represented by conditional utility functions depending on the node and its parents. But this may not be the best dimensionality
that can be achieved by a network. Perhaps the set Z = P a(x) has some internal structure,
in the sense that the subgraph induced by Z has maximal dimension lower than |Z|. In such
a case we could recursively apply CUI decomposition to the conditional utility functions for
this subgraph. This approach somewhat resembles the hierarchical decomposition done for
the utility trees (Keeney & Raiffa, 1976; Von Stengel, 1988). For example, to represent f1 for
the network of Figure 1, we require the conditional utility function U (x1 , x14 , x15 , x16 , x2 , x3 ).
However from the network we can see that CUI(x3 , x2 | x1 , x4 , x5 , x6 ). Hence we can decompose this conditional utility:
U (x1 , x14 , x15 , x16 , x2 , x3 ) = f 0 (x1 , x2 ) + g 0 (x1 , x2 )U (x1 , x3 , x02 , x14 , x15 , x16 ).
We use the notation f 0 and g 0 since these are not the same as the f and g functions of the
top level decomposition.
A nested representation can be generated systematically (Algorithm 1), by decomposing
each local function at node x (xs utility factors) whose argument set Z  P a(x) does not
form a clique. We do that by performing a complete CUI decomposition over the subgraph
induced by Z (keeping in mind that all the resulting factors depend also on x).
Proposition 11. Let G be a CUI network for utility function U (S). Then U (S) can be
represented by a set of conditional utility functions, each depending on a set of attributes
corresponding to (undirected) cliques in G.
7.1 Discussion
This result reduces the maximal dimensionality of the representation to the size of the
largest maximal clique of the CUI network. For instance, applying it to the example in
105

fiEngel & Wellman

Data: CUI Utility factors U (x, P a(x), Y ), U (x, P a(x), Y ) at each node x
/* note: Y , Y  D(Y ) */
Determine order x1 , . . . , xn ;
for j = 1, . . . , n do /* initialization */
Kj1 = {xj }  P a(xj ) /* scope of utility factors */ ;
Yj1 = S \ Kj1 /* rest of variables */ ;
Q1j = P a(xj );
A1j = , dj = 1;
end
for j = 1, . . . , n do
for i = 1, . . . , dj do /* loop on factors in node j*/
if Qij 6=  and Kji is not a clique then
Let Gij be the subgraph induced by Qij ;
Decompose Uji (Kji ) according the CUI network on Gij ;
foreach xr  Qij do
Let dr = dr + 1 (current num. of factors at xr ) and denote d = dr ;
Adr = Aij  {xj }, Qdr = P a(xr )  Qij ;
Krd = Adr  {xr }  Qdr , Yrd = S \ Krd ;
Store new CUI factors of xr : U (Krd , Yrd ), U (Krd , Yrd );
/*Yrd , Yrd are fixed assignments to Yrd */
end
Remove factors U (Kji , Yji ), (Kji , Yji );
end
end
end
Algorithm 1: Recursive CUI decomposition. Process node in reverse topological
order (outermost loop). Decompose each factor stored at current node, whose parents
do not form a clique. At each such parent xr (innermost loop) store resulting new
factors. These are defined over xr , those of xr s parents that are also in P a(xi ) (this
is Qdr ), and a clique Adr on which the original factor depends. Each time a factor is
decomposed its set Q shrinks. When it is empty, K is a clique.

106

fiCUI networks

Section 4 reduces dimensionality from four to three. An important implication is that we
can somewhat relax the requirement to find very large CUI sets. If some variables end up
with many parents, we can reduce dimensionality using this technique. As the example
below illustrates, this technique aggregates lower order CUI conditions to a more effective
decomposition.
The procedure may generate a complex functional form, decomposing a function multiple
times before the factors become restricted to a clique. The ultimate number of factors
required to represent U (S) is exponential in the number of such nesting levels. However,
each decomposition is based on a CUI network on a subgraph, and therefore typically
reduces the number of entries that are maintained.
We expect the typical application of this technique to be for composition rather than
decomposition. We execute Algorithm 1 without the actual data, resulting in a list of factors
per node (that are conditional utility functions over cliques of the graph). That means
that for elicitation purposes we can restrict attention to conditional utility functions over
maximal cliques. Once these are obtained, we have sufficient data for all the factors. We can
then recover the original, more convenient CUI-network representation of the function and
store it as such (more on that in the example below). Therefore, the effective dimensionality
for elicitation is that of the maximal cliques. The storage for efficient usage requires the
potentially higher dimension of the original CUI network, but typically this is less of a
concern.
With this result and Proposition 6, CUI networks are shown to always achieve weakly
better dimensionality than CAI maps, since both representations reduce the dimensionality
to the size of the maximal clique.
7.2 Example
We illustrate this result using a simple example. Consider a domain with four attributes
(a, b, c, d), and the following CUI conditions:
CUI(b, c), CUI(c, b), CUI(d, a)
The CUI network corresponding to the variable ordering a, b, c, d is depicted in Figure 5.
Since the CUI sets are small (a single variable each), for any variable ordering there must
be a node with two parents, meaning dimensionality of three. The nesting operation below
combines these lower order conditions to reduce the dimensionality to two.
Initially, the utility function is represented using the conditional utility functions listed
according to their corresponding nodes in the column Level 0 in Table 4. To remove the
three-dimensional factors, we need to decompose the functions of node a according to the
CUI network on {b, c}, which contains no arcs. This proceeds as follows:
U (a, b, c, d1 ) =
fb1 (a, c) + gb1 (a, c)U (a, b, c0 , d1 ) = fb1 (a, c) + gb1 (a, c)(fc1 (a, b) + gc1 (a, b)U (a, b0 , c1 , d1 ))
U (a, b, c, d2 ) =
fb2 (a, c) + gb2 (a, c)U (abc0 d2 ) = fb2 (a, c) + gb2 (a, c)(fc2 (a, b) + gc2 (a, b)U (a, b0 , c0 , d2 ))
107

fiEngel & Wellman

Figure 5: Nesting example

The resulting functions are fbi (a, c), gbi (a, c), fci (a, b), gci (a, b), i = 1, 2. The functions
and gbi (a, c) can be represented using the conditional utility functions U (a, b1 , c, di )
and U (a, b2 , c, di ), and similarly the other two functions. We can delete the factors of a,
U (a, b, c, di ), and add the new lower dimensional factors as a second column to as parents
b and c. Though we had to multiply the number of factors we store by four, all the new
factors are conditional utility functions over subdomains of the (deleted) higher dimensional
factors. The algorithm continues to node b, and loops over its six factors. If there are factors
that are defined over a set of parents of b that are not a clique it decomposes them and store
the new factors in the next table column. In case a factor from column level 1 could be
decomposed, we would add a level 2 column to store the result. In our simple example
no further decomposition is possible.
fbi (a, c)

Attr
a
b
c
d

Level 0 (CUI net)
U (a, b, c, d1 ), U (a, b, c, d1 )
U (a0 , b, c1 , d),
U (a0 , b, c2 , d)
U (a0 , b1 , c, d),
U (a0 , b2 , c, d)
U (a0 , b0 , c0 , d)

Level 1
U (a, b, c1 , d1 ),
U (a, b, c1 , d2 ),
U (a, b1 , c, d1 ),
U (a, b1 , c, d2 ),

U (a, b, c2 , d1 )
U (a, b, c2 , d2 )
U (a, b2 , c, d1 )
U (a, b2 , c, d2 )

Table 4: Nested CUI decomposition
The reverse direction mentioned above is done as follows: we run Algorithm 1 without
any data, resulting in a table such as Table 4 (without the actual utility values). We then
elicit the data for the non deleted factors (all are limited to maximal cliques). Next, we
recover the more convenient level 0 CUI representation using the table, by computing
each deleted factor (going from rightmost columns to the left) as a function of the factors
stored at its parents.

8. Conclusions
We present a graphical representation for multiattribute utility functions, based on conditional utility independence. CUI networks provide a potentially compact representation of
the multiattribute utility function, via functional decomposition to lower-dimensional functions that depend on a node and its parents. CUI is a weaker independence condition than
108

fiCUI networks

those previously employed as a basis for graphical utility representations, allowing common
patterns of complementarity and substitutivity relations disallowed by additive models.
We proposed techniques to obtain and verify structural information, and use it to construct the network and elicit the numeric data. In addition, we developed an optimization
algorithm that performs particularly well for the special case of CUI trees. In some cases it
can also be leveraged for efficient optimization of CAI maps. Finally, we show how functions
can be further decomposed over the set of maximal cliques of the CUI network. With this
technique, CUI networks can achieve the same dimensionality of graphical models based on
CAI and GAI decompositions, yet with more broadly applicable independence conditions.

Acknowledgments
A preliminary version of this paper was published in the proceedings of AAAI-06. The
work was supported in part by NSF grant IIS-0205435, and the STIET program under NSF
IGERT grant 0114368. We are grateful to the thorough work of the anonymous reviewers,
whose suggestions provided valued help in finalizing this paper.

Appendix A. Proofs
A.1 Lemma 3
Proof. Let Z = S \ (X  Y ) and C = S \ (A  B). We simply apply the two independence
conditions consequentially, and we can define f, g such that:
U (S) = U (XY Z) = f (Y Z) + g(Y Z)UY (S \ Y ) = f (Y Z) + g(Y Z)(f 0 ((BC) \ Y )
+ g 0 ((BC) \ Y )UY B (S \ (Y B))) = f(ZBY C) + g(ZBY C)U (S \ (Y B)).
Since Z Y B C = S \(AX), the last decomposition is equivalent to the decomposition
(1) for the condition CUI(A  X , Y  B).
A.2 Proposition 6
Proof. A CAI condition is stronger than a CUI condition, in that CAI(x, y)  CUI(x, y) 
CUI(y, x). To be a CUI network, for each node xi it must be the case that all other nodes
are CUI of it given its parents and descendants. This is obvious since xi is CAI of all other
nodes given its parents and children.
A.3 Lemma 7
Proof. To determine hxj (), any y  Sc(xj ) needs to be determined. If y  {xi }  Sc(xi )
we are done, if not its own scope is covered and therefore recursively determined by the
assignment to {xi }  Sc(xi ).
A.4 Lemma 8
We first introduce two additional lemmas.
Lemma 12. Ch({xi }  Co(xi ))  ({xi }  Sc(xi )  Co(xi ))
109

fiEngel & Wellman

Proof. Let xj  {xi }  Co(xi ), and y  Ch(xj ). If xj = xi the proof is immediate because
Ch(xi )  Sc(xi ). Assume xj  Co(xi ). We know from Definition 10 that Ch(xj ) 
Sc(xj )  ({xi }  Sc(xi )  Co(xi )), and this proves the lemma.
Lemma 13. An({xi }  Co(xi ))  Co(xi )
Proof. Let xj  An(xi ) (clearly j < i, therefore xj 
/ Sc(xi )). Let xj1  Sc(xj ). Then
j1 > j and there is an undirected path from xj1 to xj , not blocked by Sc(xj ). If j1  i, then
xj1  Sc(xi ){xi } because it has an unblocked path to xj (and from there to xi ). Otherwise,
let xj2  Sc(xj1 ), and apply the same argument to xj2 . We continue until xjk such that
xy  Sc(xjk ), y > i at which point xy  Sc(xi )  {xi } by the path xy , xjk , . . . , xj1 , xj , xi
and the recursion halts (note that it includes empty scopes), proving that xj  Co(xi ).
It is left to prove that An(Co(xi ))  Co(xi ). Let xj  Co(xi ), y  An(xj ). Applying
the first part of the proof on xj , we get that y  Co(xj ). From Definition of Co(xj ), we get
y < j and w  Sc(y), either w = xj , w  Sc(xj ) or w  Co(xj ). To show that y  Co(xi ),
we need to prove for each of the cases that w  {xi }  Sc(xi )  Co(xi ).
1. If w = xj immediately w  Co(xi ).
2. If w  Sc(xj ), from xj  Co(xi ) we get that either w = xi , w  Sc(xi ) or w  Co(xi ).
3. If w  Co(xj ), we repeat the argument recursively z  Sc(w). Note that z precedes
w therefore the recursion will halt at some point.

Lemma 8. Let X = {xi }  Co(xi ). From Lemma 13, X has no external ancestors. From
Lemma 12, all external children of X are in Sc(xi ). Therefore S \ (X  An(X)  Ch(X)) =
S \ (X  Sc(xi )) and the result is immediate from Proposition 5.
A.5 Proposition 9
Proof. Let x be a node in G0 . Let Y = S \ (x  In(x)) in G and Y = S \ (x  P a(x)  Dn(x))
in G0 . By definition of G, we know that CAI(Y, x), so also CUI(Y, x). Let y 
/ Y, y 6= x (so
0
y  In(x) in G). If y  Y , then y 
/ P a(x) = In(x) in G . Then the arc (y, x) was removed,
meaning that y  Dn(x). It therefore must be the case that y 
/ Y . Therefore Y  Y hence
CUI(Y , x).
A.6 Proposition 10
Proof. For G to become a CUI tree, for each cycle at least one weak link must be removed.
Since (yi , yi+1 ) is the only weak link for c, it must be removed. By Proposition 9, the
variable ordering must ensure that yi+1 is an ancestor of yi . This can be done through the
path according to the order of p, or there might be another path from yi+1 to yi . Let p1
be such path. Then the combination of p1 and p is another cycle c1 , which therefore must
be broken. Since p comprises of strong links, there must be at least one weak link (u, v) in
p1 . For (u, v) to be removed, v must be an ancestor of u. This can be done through the
path in the cycle c1 , and this path includes p, or through another path if such exists, for
which we can repeat the argument. At each stage we get a larger cycle ci , and a larger path
110

fiCUI networks

pi  pi1 . Therefore at some point there will be just one path pi that must be guaranteed
by the variable ordering, and this path includes p.
A.7 Proposition 11
Proof. We show that Algorithm 1 leads to a functional decomposition over cliques. The
outer loop in the algorithm maintains the following iteration properties:
1. a  Aij , Qij  P a(a)
2. Uij is defined over Kji
3. Aij  xj is a clique
These properties hold trivially after initialization. Assume they are valid for all factors
stored in the network until outer iteration j and inner iteration i, we next show that they
remain valid for each factor Urd that is created in iteration j, i:
1. By definition Adr = Aij  xj . From previous iteration and definition of Qdr , a 
Aij , Qdr  Qij  P a(a). From definitions of Qij and Qdr we get P a(xj )  Qij  Qdr , and
together it yields the result.
2. Urd is a factor in the CUI decomposition of Uji (Kji ) over Gij . Its scope contains: (i)
the nodes that are not affected by the last CUI decomposition, i.e. in Kji \ Qij =
Aij  xj = Adr , (ii) its node xr , and (iii) the parents P a(xr ) which were not fixed
in Uji (i.e. P a(xr  Kji )). We know Kji = Aij  xj  Qij , and xj 
/ P a(xr ) (because
i
xr  P a(xj )), and also P a(xr )  Aj =  (using a similar argument and property 1).
Therefore (P a(xr )  Kji )  Qij , and from (i),(ii),(iii) we get that Krd = Adr  xr  Qdr .
3. Adr is a clique by its definition and the same property of previous iteration. xr  Qij ,
therefore from property 1 of previous iteration xr  P a(a) for each a  Aij . Also xr 
Qij  P a(xj ) (the last containment is immediate from definition of Qij ). Therefore xr
is a parent of all members of Adr , and as a result Adr  xr is a clique.
By the iteration properties, either Krd is a clique, or Qdr is non empty and decomposition
can be applied once we reach node r in the outer loop. At the end of the process all factors
which are not defined on cliques where removed. All the factors that remained are defined
on cliques. U (S) can be still represented by the new set of factors since we only applied
valid decompositions to its factors.

References
Abbas, A. (2005). Attribute dominance utility. Decision Analysis, 2, 185206.
Bacchus, F., & Grove, A. (1995). Graphical models for preference and utility. In Eleventh
Conference on Uncertainty in Artificial Intelligence, pp. 310, Montreal.
Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: A directed graphical
representation of conditional utilities. In Seventeenth Conference on Uncertainty in
Artificial Intelligence, pp. 5664, Seattle.
111

fiEngel & Wellman

Boutilier, C., Brafman, R. I., Hoos, H. H., & Poole, D. (1999). Reasoning with conditional ceteris paribus preference statements. In Fifteenth Conference on Uncertainty
in Artificial Intelligence, pp. 7180, Stockholm.
Debreu, G. (1959). Topological methods in cardinal utility theory. In Arrow, K., Karlin, S.,
& Suppes, P. (Eds.), Mathematical Methods in the Social Sciences. Stanford University
Press.
Dyer, J. S., & Sarin, R. K. (1979). Measurable multiattribute value functions. Operations
Research, 27, 810822.
Fishburn, P. C. (1965). Independence in utility theory with whole product sets. Operations
Research, 13, 2845.
Fishburn, P. C. (1967). Interdependence and additivity in multivariate, unidimensional
expected utility theory. International Economic Review, 8, 335342.
Fishburn, P. C. (1975). Nondecomposable conjoint measurement for bisymmetric structures.
Journal of Mathematical Psychology, 12, 7589.
Fuhrken, G., & Richter, M. K. (1991). Polynomial utility. Economic Theory, 1 (3), 231249.
Gonzales, C., & Perny, P. (2004). GAI networks for utility elicitation. In Ninth International
Conference on Principles of Knowledge Representation and Reasoning, pp. 224234,
Whistler, BC, Canada.
Gorman, W. M. (1968). The structure of utility functions. Review of Economic Studies,
35, 367390.
Keeney, R. L., & Raiffa, H. (1976). Decisions with Multiple Objectives: Preferences and
Value Tradeoffs. Wiley.
Krantz, D. H., Luce, R. D., Suppes, P., & Tversky, A. (1971). Foundations of Measurement,
Vol. 1. Academic Press, New York.
La Mura, P., & Shoham, Y. (1999). Expected utility networks. In Fifteenth Conference on
Uncertainty in Artificial Intelligence, pp. 366373, Stockholm.
Nilsson, D. (1998). An efficient algorithm for finding the M most probable configurations
in probabilistic expert systems. Statistics and Computing, 8 (2), 159173.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Pearl, J., & Paz, A. (1989). Graphoids: A graph based logic for reasoning about relevance
relations. In Du Boulay, B. (Ed.), Advances in Artificial Intelligence II. North-Holland,
New York.
Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming and influence diagrams..
20, 365379.
Von Stengel, B. (1988). Decomposition of multiattribute expected utility functions. Annals
of Operations Research, 16, 161184.
Wellman, M. P., & Doyle, J. (1992). Modular utility representation for decision-theoretic
planning. In First International Conference on Artificial Intelligence Planning Systems, pp. 236242, College Park, MD.

112

fi