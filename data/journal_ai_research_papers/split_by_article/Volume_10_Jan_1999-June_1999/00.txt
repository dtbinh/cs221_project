Journal of Artificial Intelligence Research 10 (1999) 1-38

Submitted 4/98; published 1/99

Order of Magnitude Comparisons of Distance
Ernest Davis

davise@cs.nyu.edu

Courant Institute
New York, NY 10012 USA

Abstract
Order of magnitude reasoning | reasoning by rough comparisons of the sizes of quantities | is often called \back of the envelope calculation", with the implication that the
calculations are quick though approximate. This paper exhibits an interesting class of constraint sets in which order of magnitude reasoning is demonstrably fast. Specifically, we
present a polynomial-time algorithm that can solve a set of constraints of the form \Points
a and b are much closer together than points c and d." We prove that this algorithm can be
applied if \much closer together" is interpreted either as referring to an infinite difference in
scale or as referring to a finite difference in scale, as long as the difference in scale is greater
than the number of variables in the constraint set. We also prove that the first-order theory
over such constraints is decidable.
1. Introduction
Order of magnitude reasoning | reasoning by rough comparisons of the sizes of quantities |
is often called \back of the envelope calculation", with the implication that the calculations
are quick though approximate. Previous AI work on order of magnitude reasoning, however,
has focussed on its expressive power and inferential structure, not on its computational
leverage (Raiman, 1990; Mavrovouniotis and Stephanopoulos, 1990; Davis, 1990; Weld,
1990).
In this paper we exhibit an interesting case where solving a set of order of magnitude
comparisons is demonstrably much faster than solving the analogous set of simple order
comparisons. Specifically, given a set of constraints of the form \Points a and b are much
closer together than points c and d," the consistency of such a set can be determined in
low-order polynomial time. By contrast, it is easily shown that solving a set of constraints
of the form \The distance from a to b is less than or equal to the distance from c to d" in
one dimension is NP-complete, and in higher dimensions is as hard as solving an arbitrary
set of algebraic constraints over the reals.
In particular, the paper presents the following results:
1. The algorithm \solve constraints(S )" solves a system of constraints of the form \Points
a and b are infinitely closer than points c and d" in polynomial time (Section 5).
2. An improved version of the algorithm runs in time O(max(n2 ff(n)); ne; s) where n is
the number of variables, ff(n) is the inverse Ackermann's function, e is the number of
edges mentioned in the constraint set, and s is the size of the constraint set. (Section
6.1).
3. An extended version of the algorithm allows the inclusion of non-strict constraints of
the form \Points a and b are not infinitely further apart than points c and d." The
c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiDavis

running time for this modified algorithm is slower than that of solve constraints, but
still polynomial time. (Section 6.2)
4. A different extension of the algorithm allows the combination of order of magnitude
constraints on distances with order comparisons on the points of the form \Point a
precedes point b." (Section 6.3)
5. The same algorithm can be applied to constraints of the form \The distance from a
to b is less than 1=B times the distance from c to d," where B is a given finite value,
as long as B is greater than the number of variables in the constraint set. (Section 7)
6. The first-order theory over such constraints is decidable. (Section 8)
As preliminary steps, we begin with a small example and an informal discussion (Section
2). We then give a formal account of order-of-magnitude spaces (Section 3) and present
a data structure called a cluster tree, which expresses order-of-magnitude distance comparisons (Section 4). We conclude the paper with a discussion of the significance of these
results (Section 9).

2. Examples
Consider the following inferences:

Example 1: I wish to buy a house and rent oce space in a suburb of Metropolis. For
obvious reasons, I want the house to be close to the school, the house to be close to the
oce, and the oce to be close to the commuter train station. I am told that in Elmville
the train station is quite far from the school, but in Newton they are close together.
Infer that I will not be able to satisfy my constraints in Elmville, but may be able to
in Newton.
Example 2: The Empire State Building is much closer to the Washington Monument than
to Versailles. The Statue of Liberty is much closer both to the Empire State Building and
to Carnegie Hall than to the Washington Monument.
Infer that Carnegie Hall is much closer to the Empire State Building than to Versailles.
Example 3: You have to carry out a collection of computational tasks covering a wide
range of diculty. For instance
a.
b.
c.
d.
e.
f.

Add up a column of 100 numbers.
Sort a list of 10,000 elements.
Invert a 100  100 matrix.
Invert a 1000  1000 matrix.
Given the O.D.E. x = cos(et x), x(0) = 0, find x(20) to 32-bit accuracy.
Given a online collection of 1,000 photographs in GIF format, use state-ofthe-art image recognition software to select all those that show a man on
horseback.

2

fiOrder of Magnitude Comparisons of Distance

g. Do a Web search until you have collected 100 pictures of men on horseback,
using state-of-the-art image recognition software.
h. Using state-of-the-art theorem proving software, find a proof that the medians of a triangle are concurrent.
i. Using state-of-the-art theorem proving software, find a proof of Fermat's
little theorem.
It is plausible to suppose that, in many of these cases, you can say reliably that one
task will take much longer than another, either by a human judgment or using an expert
system. For instance, task (a) is much shorter than any of the others. Task (b) is much
shorter than any of the others except (a) and possibly (h). Task (c) is certainly much
shorter than (d), (f), (g), or (i). However, with certain pairs such as (c) and (h) or (c) and
(e) it would be dicult to guess whether one is much shorter than another, or whether they
are of comparable diculty.
You have a number of independent identical computers, of unknown vintage and characteristics, on which you will schedule tasks of these kinds. Note that, under these circumstances, there is no way to predict the absolute time required by any of these tasks within a
couple of orders of magnitude. Nonetheless, the comparative lengths presumably still stand.
Given: a particular schedule of tasks on machines, infer what you can about the relative
order of completion times. For example, given the following schedule
Machine M1: tasks a,b,h,d.
Machine M2: tasks c,i.
it should be possible to predict that (a) and (b) will complete before (c); that (c) will
complete before (d); and that (d) will complete before (i); but it will not be possible to
predict the order in which (c) and (h) will complete.
In all three examples, the given information has the form \The distance between points
W and X is much less than the distance between Y and Z ". In examples 1 and 2, the
points are geometric. In example 3, the points are the start and completion times of the
various tasks, and the constraints on relative lengths can be put in the form \The distance
from start(a) to end(a) is much less than the distance from start(c) to end(c)", and so on.
In example 3, there is also ordering information: the start of each task precedes its end; the
end of (a) is equal to the start of (b); and so on. The problem is to make inferences based
on this weak kind of constraint.
It should be noted that these examples are meant to be illustrative, rather than serious applications. Example 1 does not extend in any obvious way to a class of natural,
large problems. Example 2 is implausible as a state of knowledge; how does the reasoner
find himself knowing just the order-of-magnitude relations among distances and no other
geometric information? Example 3 is contrived. Nonetheless, these illustrate the kinds of
situations where order-of-magnitude relations on distance do arise; where they express a
substantial part of the knowledge of the reasoner; and where inferences based purely on the
order-of-magnitude comparisons can yield useful conclusions.
The methods presented in this paper involve construing the relation \Distance D is much
shorter than distance E" as if it were \Distance D is infinitesimal as compared to distance
E." As we shall see, under this interpretation, systems of constraints over distances can be

3

fiDavis

solved eciently. The logical foundations for dealing with infinitesimal quantities lie in the
non-standard model of the real line with infinitesimals, developed by Abraham Robinson
(1965). (A more readable account is given by Keisler, 1976.) Reasoning with quantities of
infinitely different scale is known as \order of magnitude" reasoning.
The reader may ask, \Since infinitesimals have no physical reality, what is the value
of developing techniques for reasoning about them?" In none of the examples, after all, is
the smaller quantity truly infinitesimal or the larger one truly infinite. In example 1 and
2, the ratio between successive sizes is somewhere between 10 and 100; in example 3, it
is between 100 and a rather large number dicult to estimate; but one can always give
some kind of upper bound. It is essentially certain, for instance, that the ratio between the
times required for tasks (a) and (i) is less than 10100;000 . Why not use the best real-valued
estimate instead?
The first answer is that this is an idealization. Practically all physical reasoning and
calculation rest on one idealization or another: the idealization in the situation calculus
that time is discrete; the idealization that solid objects are rigid, employed in most mechanics programs; the idealization that such physical properties as density, temperature, and
pressure are continuous rather than local averages over atoms, which underlies most uses of
partial differential equations; the idealization involved in the use of the Dirac delta function;
and so on. Our idealization here that a very short distance is infinitesimally smaller than a
long one simplifies reasoning and yields useful results as long as care is taken to stay within
an appropriate range of application.
The second answer is that this is a technique of mathematical approximation, which we
are using to turn an intractable problem into a tractable one. This would be analogous to
linearizing a non-linear equation over a small neighborhood; or to approximating a sum by
an integral.
There are circumstances where we can be sure that the approximation gives an answer
that is guaranteed exactly correct; namely if the actual ratio implicit in the comparison
\D is much smaller than E " is larger than the number of points involved in the system of
constraints. This will be proven in Section 7. There is also a broader, less well-defined, class
of problems where the approximation, though not guaranteed correct, is more reliable than
some of the other links in the reasoning. For instance, suppose that one were to consider
an instance of example 3 involving a couple of hundred tasks, apply order-of-magnitude
reasoning, and come up with an answer that can be determined to be wrong. It is possible
that the error would be due to the order-of-magnitude reasoning. However, it seems safe
to say that, in most cases, the error is more likely to be due to a mistake in estimating the
comparative sizes.

3. Order-of-magnitude spaces
An order-of-magnitude space, or om-space, is a space of geometric points. Any two points
are separated by a distance. Two distances d and e are compared by the relation d  e,
meaning \Distance d is infinitesimal compared to e" or, more loosely, \Distance d is much
smaller than e."
For example, let < be the non-standard real line with infinitesimals. Let <m be the
corresponding m-dimensional space. Then we can let a point of the om-space be a point in

4

fiOrder of Magnitude Comparisons of Distance

Rm . The distance between two points a; b is the Euclidean distance, which is a non-negative
value in < . The relation d  e holds for two distances d; e, if d=e is infinitesimal.
The distance operator and the comparator are related by a number of axioms, specified
below. The most interesting of these is called the om-triangle inequality: If ab and bc are
both much smaller than xy, then ac is much smaller than xy. This combines the ordinary
triangle inequality \The distance ac is less than or equal to distance ab plus distance bc"
together with the rule from order-of-magnitude algebra, \If p  r and q  r then p+q  r."
It will simplify the exposition below if, rather than talking about distances, we talk about
orders of magnitude. These are defined as follows. We say that two distances d and e have
the same order of magnitude if neither d  e nor e  d. In < this is the condition that d=e
is finite: neither infinitesimal nor infinite. (Raiman, 1990 uses the notation \d Co e" for this
relation.) By the rules of the order-of-magnitude calculus, this is an equivalence relation.
Hence we can define an order of magnitude to be an equivalence class of distances under
the relation \same order of magnitude". For two points a; b, we define the function od(a; b)
to be the order of magnitude of the distance from a to b. For two orders of magnitude p; q,
we define p  q if, for any representatives d 2 p and e 2 q, d  e. By the rules of the orderof-magnitude calculus, if this holds for any representatives, it holds for all representatives.
The advantage of using orders-of-magnitude and the function \od", rather than distances
and the distance function, is that it allows us to deal with logical equality rather than the
equivalence relation \same order of magnitude".
For example, in the non-standard real line, let  be a positive infinitesimal value. Then
values such as f1; 100; 2 50 + 1002 : : :g, are all of the same order of magnitude, o1. The
values f; 1:001; 3 + e 1= : : :g are of a different order of magnitude o2  o1. The values
f1=; 10= + 5 : : :g are of a third order of magnitude o3  o1.
Definition 1: An order-of-magnitude space (om-space) 
 consists of:







A set of points P ;
A set of orders of magnitude D;
A distinguished value 0 2 D;
A function \od(a; b)" mapping two points a; b 2 P to an order of magnitude;
A relation \d  e" over two orders of magnitude d; e 2 D

satisfying the following axioms:
A.1 For any orders of magnitude d; e
e  d, d = e.

2 D, exactly one of the following holds: d  e,

A.2 For d; e; f 2 D, if d  e and e  f then d  f .
(Transitivity. Together with A.1, this means that  is a total ordering on orders of
magnitude.)
A.3 For any d 2 D, not d  0.
(0 is the minimal order of magnitude.)

5

fiDavis

A.4 For points a; b 2 P , od(a; b) = 0 if and only if a = b.
(The function od is positive definite.)
A.5 For points a; b 2 P , od(a; b) = od(b; a).
(The function od is symmetric.)
A.6 For points a; b; c 2 P , and order of magnitude d 2 D,
if od(a; b)  d and od(b; c)  d then od(a; c)  d.
(The om-triangle inequality.)
A.7 There are infinitely many different orders of magnitude.
A.8 For any point a1 2 P and order of magnitude d
a2 ; a3 : : : such that od(ai ; aj ) = d for all i 6= j .

2 D, there exists an infinite set

The example we have given above of an om-space, non-standard Euclidean space, is wild
and woolly and hard to conceptualize. Here are two simpler examples of om-spaces:
I. Let  be an infinitesimal value. We define a point to be a polynomial in  with integer
coecients, such as 3 + 5 85 . We define an order-of-magnitude to be a power of . We
define m  n if m > n; for example, 6  4 . We define od(a; b) to be the smallest power
of  in a b. For example, od(1 + 2 33 ; 1 52 + 44 ) = 2 .
II. Let N be an infinite value. We define a point to be a polynomial in N with integer
coecients. We define an order of magnitude to be a power of N . We define N p  N q if
p < q; for example, N 4  N 6 . We define od(a; b) to be the largest power of N in a b. For
example, od(1 + N 2 3N 3 ; 1 5N 2 + 4N 4 ) = N 4 .
It can be shown that any om-space either contains a subset isomorphic to (I) or a subset
isomorphic to (II). (This is just a special case of the general rule that any infinite total
ordering contains either an infinite descending chain or an infinite ascending chain.)
We will use the notation \de" as an abbreviation for \d  e or d = e".

4. Cluster Trees
Let P be a finite set of points in an om-space. If the distances between different pairs of
points in P are of different orders of magnitude, then the om-space imposes a unique treelike hierarchical structure on P . The points will naturally fall into clusters, each cluster C
being a collection of points all of which are much closer to one another than to any point in
P outside C . The collection of all the clusters over P forms a strict tree under the subset
relation. Moreover, the structure of this tree and the comparative sizes of different clusters
in the tree captures all of the order-of-magnitude relations between any pair of points in P .
The tree of clusters is thus a very powerful data structure for reasoning about points in an
om-space, and it is, indeed, the central data structure for the algorithms we will develop in
this paper. In this section, we give a formal definition of cluster trees and prove some basic
results as foundations for our algorithms.

Definition 2: Let P be a finite set of points in an om-space. A non-empty subset C  P is
called a cluster of P if for every x; y 2 C , z 2 P C , od(x; y)  od(x; z ). If C is a cluster,
the diameter of C , denoted \odiam(C )", is the maximum value of od(x; y) for x; y 2 C .

6

fiOrder of Magnitude Comparisons of Distance

n1
5
n2

n3

4

3

n4

n5

a

d

0

3

0

0

e

g

f

b

c

0

0

0

0

0

Figure 1: Cluster tree
Note that the set of any single element of P is trivially a cluster of P . The entire set P
is likewise a cluster of P . The empty set is by definition not a cluster of P .

Lemma 1: If C and D are clusters of P , then either C
disjoint.

 D, D  C , or C and D are

Proof: Suppose not. Then let x 2 C \ D, y 2 C D, z 2 D C . Since C is a cluster,
od(x; y)  od(x; z ). Since D is a cluster, od(x; z )  od(x; y). Thus we have a contradiction.

2

By virtue of lemma 1, the clusters of a set P form a tree. We now develop a representation of the order of magnitude relations in P by constructing a tree whose nodes correspond
to the clusters of P , labelled with an indication of the relative size of each cluster.

Definition 3: A cluster tree is a tree T such that

 Every leaf of T is a distinct symbol.
 Every internal node of T has at least two children.
 Each internal node of T is labelled with a non-negative value. Two or more nodes
may be given the same value. (For the purposes of Sections 5-7, labels may be taken
to be non-negative integers; in Section 8, it will be useful to allow rational labels.)

 Every leaf of the tree is labelled 0.
 The label of every internal node in the tree is less than the label of its parent.
For any node N of T , the field \N .symbols" gives the set of symbols in the leaves in the
subtree of T rooted at N , and the field \N .label" gives the integer label on node N .

7

fiDavis

Thus, for example, in Figure 1, n3.label=3 and n3.symbols = fa; dg; n1.label = 5 and
n1.symbols = fa; b; c; d; e; f; gg.
As we shall see, the nodes of the tree T represent the clusters of a set of points, and the
labels represent the relative sizes of the diameters of the clusters.

Definition 4: A valuation over a set of symbols is a function mapping each symbol to a
point in an om-space. If T is a cluster tree, a valuation over T is a valuation over T .symbols.
If N is any node in T and is a valuation over T , we will write (N ) as an abbreviation
for (N .symbols).
We now define how a cluster tree T expresses the order of magnitude relations over a
set of points P .
Definition 5: Let T be a cluster tree and let be a valuation over T . Let P = (T ), the
set of points in the image of T under . We say that j=T (read satisfies or instantiates
T ) if the following conditions hold:
i. For any internal node N of T , (N ) is a cluster of P .
ii. For any cluster C of P , there is a node N such that C = (N ).
iii. For any nodes M and N , if M .label < N .label then odiam( (M ))  odiam( (N )).
iv. If label(M ) = 0, then odiam(M ) = 0. (That is, all children of M are assigned the
same value under .)
The following algorithm generates an instantiation
procedure

variable

given a cluster tree T :

instantiate(in T : cluster tree; 
 : an om-space)
return : array of points indexed on the symbols of T

G[N ] : array of points indexed on the nodes of T ;

Let k be the number of internal nodes in T ;
Choose 0 = 0  1  2  : : :  k to be k + 1 different orders of magnitude;
/* Such values can be chosen by virtue of axiom A.7 */
pick a point x 2 
;
G[root of T ] := x;
instantiate1(T; 
; 1 : : : k ; G);
return the restriction of G to the symbols of T .
end instantiate.
instantiate1(in N : a node in a cluster tree; 
 : an om-space; 1 : : : k : orders of magnitude;
in out G : array of points indexed on the nodes of T )
if N is not a leaf then
let C1 : : : Cp be the children of N ;
x1 := G[N ];
q := N .label;
pick points x2 : : : xp such that
for all i; j 2 1 : : : p, if i 6= j then od(xi ; xj ) = q ;
/* Such points can be chosen by virtue of axiom A.8 */
8

fiOrder of Magnitude Comparisons of Distance

for

i = 1 : : : p do
G[Ci ] := xi ;

instantiate1(Ci ; 
; 1 : : : k ; G);

endfor
endif end

instantiate1.

Thus, we begin by picking orders of magnitude corresponding to the values of the labels.
We pick an arbitrary point for the root of the tree, and then recurse down the nodes of the
tree. For each node N , we place the children at points that all lie separated by the desired
diameter of N . The final placement of the leaves is then the desired instantiation.
Lemma 2: If T is a cluster tree and 
 is an om-space, then instantiate(T; 
) returns an
instantiation of T .
The proof is given in the appendix.
Moreover, it is clear that any instantiation of T can be generated as a possible output
of instantiate(T; 
). (Given an instantiation , just pick G[N ] at each stage to be of some
symbol of N .)
Note that, given any valuation over a finite set of symbols S , there exists a cluster
tree T such that T .symbols = S and satisfies T . Such a T is essentially unique up to an
isomorphism over the set of labels that preserves the label 0 and the order of labels.

5. Constraints
In this section, we develop the first of our algorithms. Algorithm solve constraints tests
a collection of constraints of the form \a is much closer to b than c is to d," for consistency. If the set is consistent, then the algorithm returns a cluster tree that satisfies the
constraints. The algorithm builds the cluster tree from top to bottom dealing first with the
large distances, and then proceeding to smaller and smaller distances.
Let S be a system of constraints of the form od(a; b)  od(c; d); and let T be a cluster
tree. We will say that T `S (read \T satisfies S ") if every instantiation of T satisfies S . In
this section, we develop an algorithm for finding a cluster tree that satisfies a given set of
constraints.
The algorithm works along the following lines: Suppose we have a solution satisfying S .
Let D be the diameter of the solution. If S contains a constraint od(a; b)  od(c; d) then,
since od(c; d) is certainly no more than D, it follows that od(a; b) is much smaller than D.
We label ab as a \short" edge.
If two points u and v are connected by a path of short edges, then by the triangle
inequality the edge uv is also short (i.e. much shorter than D). Thus, if we compute the
connected components H of all the edges that have been labelled short, then all these edges
in H can likewise be labelled short. For example, in table 3, edges vz , wx, and xy can all
be labelled \short".
On the other hand, as we shall prove below, if an edge is not in the set H , then there is
no reason to believe that it is much shorter than D. We can, in fact, safely posit that it is
the same o.m. as D. We label all such edges \long".
We can now assume that any connected component of points connected by short edges
is a cluster, and a child of the root of the cluster tree. The root of the cluster tree is then
given the largest label. Its children will be given smaller labels. Each \long" edge now

9

fiDavis

connects symbols in two different children of the root. Hence, any instantiation of the tree
will make any long edge longer than any short edge.
If no edges are labelled \long" | that is, if H contains the complete graph over the
symbols | then there is an inconsistency; all edges are much shorter than the longest edge.
For instance, in table 4, since vw, wx, and xy are all much smaller than zy, it follows
by the triangle inequality that vy is much smaller than zy. But since we also have the
constraints that zy is much smaller than vz and that vz is much smaller than vy, we have
an inconsistency.
The algorithm then iterates, at the next smaller scale. Since we have now taken care of
all the constraints od(a; b)  od(c; d), where cd was labelled \long", we can drop all those
from S . Let D now be the greatest length of all the edges that remain in S . If a constraint
od(a; b)  od(c; d) is in the new S , then we know that od(a; b) is much shorter than D, and
we label it \short". We continue as above. The algorithm halts when all the constraints in
S have been satisfied, and S is therefore empty; or when we encounter a contradiction, as
above.
We now give the formal statement of this algorithm. The algorithm uses an undirected
graph over the variable symbols in S . Given such a graph G, and a constraint C of the
form od(a; b)  od(c; d), we will refer to the edge ab as the \short" of C , and to the edge
cd as the \long" of C . The shorts of the system S is the set of all shorts of the constraints
of S and the longs of S is the set of all the longs of the constraints. An edge may be both a
short and a long of S if it appears on one side in one constraint and on the other in another
constraint.
procedure

type:

solve constraints(in S : a system of constraints of the form od(a; b)  od(c; d))
return either a cluster tree T satisfying S if S is consistent;
or false if S is inconsistent.

A node N of the cluster tree contains
pointers to the parent and children of N ;
the field N.label, holding the integer label;
and the field N.symbols, holding the list of symbols in the leaves of N .

variables:

begin if

m is an integer;
C is a constraint in S ;
H; I are undirected graphs;
N; M are nodes of T ;

S contains any constraint of the form, \od(a; b)  od(c; c)" then return false;

m := the number of variables in S ;
initialize T to consist of a single node N ;
N .symbols:= the variables in S ;
repeat

H := the connected components of the shorts of S ;
if H contains all the edges in S then return(false) endif;
for each leaf N of T do
if not all vertices of N are connected in H then
N .label := m;
for each connected component I of N .symbols in H

10

do

fiOrder of Magnitude Comparisons of Distance

construct node M as a new child of N in T ;
M .symbols:= the vertices of I ;

endfor endif endfor

S := the subset of constraints in S whose long is in H ;
m := m 1;
until
for

S is empty;

each leaf N of T
N .label := 0;
if N .symbols has more than one symbol
then create a leaf of N for each symbol in N .symbols;
label each such leaf 0;
endif endfor end solve constraints.

Tables 3 and 4 give two examples of the working of procedure solve constraints. Table
3 shows how the procedure can be used to establish that the following constraints are
consistent:
The Empire State Building (x) is much closer to the Washington Monument (w)
than to Notre Dame Cathedral (v).
Bunker Hill (y) is much closer to the Empire State Building than to the Eiffel
Tower (z ).
The distance from the Eiffel Tower to Notre Dame is much less than the distance
from the Washington Monument to Bunker Hill.
Table 4 shows that the following inference can be justified:

Given: The distances from the Statue of Liberty (v) to the World Trade Center
(w), from the World Trade Center to the Empire State Building (x), and from
the Empire State Building to the Chrysler Building (y) are all much less than
the distance from the Chrysler Building to the Washington Monument (z ).
Infer: The Washington Monument is not much nearer to the Chrysler Building
than to the Statue of Liberty.
This inference is carried out by asserting the negation of the consequent, \The Washington Monument is much nearer to the Chrysler Building than to the Statue of Liberty," and
showing that that collection of constraints is inconsistent. Note that if we change \much
less" and \much nearer" in this example to \less" and \nearer", then the inference no longer
valid.
Theorem 1 states the correctness of algorithm solve constraints. The proof is given in
the appendix.
Theorem 1: The algorithm solve constraints(S ) returns a cluster tree satisfying S if S is
consistent, and returns false if S is inconsistent.
There may be many cluster trees that satisfy a given set of constraints. Among these,
the cluster tree returned by the algorithm solve constraints has an important property: it
has the fewest possible labels consistent with the constraints. In other words, it uses the
minimum number of different orders of magnitude of any solution. Therefore, the algorithm
can be used to check the satisfiability of a set of constraints in an om-space that violates

11

fiDavis

S contains the constraints
1. od(w; x)  od(x; v).
2. od(x; y)  od(y; z ).
3. od(v; z )  od(w; y).
The algorithm proceeds as follows:
Initialization:
The tree is initializes to a single node with n1.
n1.symbols := f v; w; x; y; z g.
First iteration:
The shorts of S are f wx; xy; vz g.
Computing the connected components, H is set to f wx; xy; wy; vz g.
n1.label := 5;
Two children of n1 are created:
n11.symbols := w; x; y ;
n12.symbols := v; z ;
As xv is not in H , delete constraint #1 from S .
As yz is not in H , delete constraint #2 from S .
S now contains just constraint #3.
Second iteration:
The shorts of S are f vz g.
The connected components H is just fvz g.
n11.label := 4;
Three children of n11 are created:
n111.symbols := w;
n112.symbols := x;
n113.symbols := z ;
As wy is not in H , delete constraint #3 from S .
S is now empty.
Cleanup:
n12.label := 0;
Two children of n12 are created:
n121.symbols := v;
n122.symbols := z ;
(See Figure 2.)
Table 1: Example of computing a cluster tree

12

fiOrder of Magnitude Comparisons of Distance

n1
0th iteration
v,w,x,y,z

n1

1st iteration

5
v,w,x,y,z
n11

n12

w,x,y

v,z

n1
2nd iteration
5
v,w,x,y,z
n11
4

n12
w,x,y
v,z

w

x

y

n1
Cleanup
5
v,w,x,y,z
n11
4

w

x

n12
w,x,y

0

y

v

Figure 2: Building a cluster tree

13

v,z

z

fiDavis

S contains the constraints
od(v; w)  od(z; y).
od(w; x)  od(z; y).
od(x; y)  od(z; y).
od(z; y)  od(v; z ).
The algorithm proceeds as follows:
Initialization:
The tree is initializes to a single node with n1.
n1.symbols := f v; w; x; y; z g.
First iteration:
The shorts of S are f vw; wx; xy; zy; vz g.
H is set to its connected components, which is the complete graph over v; w; x; y; z .
The algorithm exits returning false
Table 2: Example of determining inconsistency
axiom A.7 and has only finitely many different orders of magnitude. If the algorithm returns
T and T has no more different labels than the number of different orders of magnitude in
the space, then the constraints are satisfiable. If T uses more labels than the space has
orders of magnitude, then the constraints are unsatisfiable.
The proof is easier to present if we rewrite algorithm solve constraints in the following
form, which returns only the number of different non-zero labels used, but does not actually
construct the cluster tree.1

num labels(S );

function
if
then return
else return

S is empty

(0)
(1 + num labels(reduce constraints(S )))
function reduce constraints(S )
H := connected components of the shorts of S ;
if H contains all the edges in S then return(false) to top-level
else return(the set of constraints in S whose long is in H )

It is easily verified that the sequence of values of S in successive recursive calls to
num labels is the same as the sequence of values of S in the main loop of solve constraints.
Therefore num labels returns the number of different non-zero labels in the tree constructed
by solve constraints.
1. The reader may wonder why this simpler algorithm was not presented before the more complicated
algorithm solve constraints. The reason is that the only proof we have found that the system of constraints is consistent if num labels does not return
the constructive solve constraints.

14

false

relies on the relation between num labels and

fiOrder of Magnitude Comparisons of Distance

Theorem 2: Out of all solutions to the set of constraints S , the instantiations of
solve constraints(S ) have the fewest number of different values of od(a; b), where a; b range
over the symbols in S . This number is given by num labels(S ).
The proof is given in the appendix.

6. Extensions and Consequences
We next present a number of modifications of the algorithm solve constraints. The first
is a more ecient implementation. The second extends the algorithm to handle non-strict
comparisons. The third extend the algorithm to handle a combination of order-of-magnitude
comparisons on distance with order comparisons, in a one-dimensional space.

6.1 An Ecient Implementation of Solve constraints
It is possible to implement algorithm solve constraints somewhat more eciently than the
naive encoding of the above description. The key is to observe that the graph H of connected
components does not have to be computed explicitly; it suces to compute it implicitly using
merge-find sets (union-find sets). Combining this with suitable back pointers from edges to
constraints, we can formulate a more ecient version of the algorithm.
We use the following data structures and subroutines:

 Each node N of the cluster tree contains pointers to its parents and children; a field

N .label, holding the integer label; a field N .symbols, holding the list of symbols in
the leaves of N ; and a field N .mfsets, holding a list of the connected components of
the symbols in N . As described below, each connected component is implemented as
an merge-find set (MFSET).

 An edge E in the graph over symbols contains its two endpoints, each of which is a

symbol; a field E .shorts, a list of the constraints in which E appears as a short; and
a field E .longs, a list of the constraints in which E appears as a long.

 A constraint C has two fields, C .short and C .long, both of them edges. It also has

pointers into the lists C .short.shorts and C .long.longs, enabling C to be removed in
constant time from the constraint lists associated with the individual edges.

 We will use the disjoint-set forest implementation of MFSETs (Cormen, Leiserson,

and Rivest, 1990, p. 448) with merging smaller sets into larger and path-compression.
Thus, each MFSET is a upward-pointing tree of symbols, each node of the tree being
a symbol. The tree as a whole is represented by the symbol at the root. A symbol A
has then the following fields:

{
{
{
{

A.parent is a pointer to the parent in the MFSET tree.
A.cluster leaf is a pointer to the leaf in the cluster tree containing A.
If A is the root of the MFSET then A.size holds the size of the MFSET.
If A is the root of the MFSET, then A.symbols holds all the elements of the
MFSET.

15

fiDavis

{ If A is the root of the MFSET then A.leaf ptr holds a pointer to the pointer to
A in N .mfsets where N = A.cluster leaf.
We can now describe the algorithm.
procedure

variables:

0.
1.
2.
3.
4.
5.
6.
7.

begin if

8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.

solve constraints1(in S : a system of constraints of the form od(a; b)  od(c; d)).
return either a cluster tree T satisfying S if S is consistent;
or false if S is inconsistent.
m is an integer;
a; b are symbols;
C is a constraint in S ;
H is an undirected graph;
E; F are edges;
P is an MFSET;
N; M are nodes of T ;

S contains any constraint of the form, \od(a; b)  od(c; c)" then return false;

H := ;;

each constraint C in S with short E and long F
add E and F to H ;
add C to E .shorts and to F .longs endfor;
m := the number of variables in S ;
initialize T to contain the root N ;
N .symbols := the variables in S ;
for

do

each leaf N of T , INITIALIZE MFSETS(N );
each edge E = ab in H do
if E .shorts is non-empty and FIND(a) 6= FIND(b) then
MERGE(FIND(a), FIND(b)) endif endfor
if every edge E = ab in H satisfies FIND(a) = FIND(b)

repeat for
for

then return(false) endif

for

each current leaf N of T do
if N .mfsets has more than one element then
for each mfset P in N .mfsets do
construct node M as a new child of N in T ;
M .symbols:= P .symbols;
endfor endif endfor

each edge E = ab in H do
if FIND(a) 6= FIND(b) then
for each constraint C in E .longs do
delete C from S ;
delete C from E .longs;
delete C from C .short.shorts endfor
delete E from H endif endfor
m := m 1;
until S is empty;
for

for

each leaf N of T
N .label := 0;
if N .symbols has more than one symbol
16

fiOrder of Magnitude Comparisons of Distance

32.
33.

create a leaf of N with label 0 for each symbol in N .symbols;
solve constraints1.

then
endif endfor end

INITIALIZE MFSETS(N : node)
: symbol;
N .mfsets := ;;
for A in N .symbols do
A.parent := null;
A.cluster leaf := N ;
A.symbols := fAg;
A.size := 1;
N .mfsets := cons(A,N .mfsets);
A.leaf ptr := N .mfsets;
endfor end INITIALIZE MFSETS.
procedure
var A

MERGE(in A; B : symbol)
.size
.size then swap(A; B );
A.parent := B ;
B .size := B .size + A.size;
B .symbols := B .symbols [ A.symbols;
Using A.leaf ptr, delete A from A.cluster leaf.mfsets;
end MERGE.
procedure
if A
>B

FIND(in A : symbol) return symbol;
: symbol;
.parent = null then return A
:= FIND(A.parent);
A.parent := R; /* Path compression */
return(R)
end FIND.
procedure
var R
if A
else R

Let n be the number of symbols in S ; let e be the number of edges; and let s be the
number of constraints. Note that n=2  e  n(n 1)=2 and that e=2  s  e(e 1)=2.
The running time of solve constraints1 can be computed as follows. As each iteration of the
main loop 8-28 splits at least one of the connected components of H , there can be at most
n 1 iterations. The MERGE-FIND operations in the for loop 9-11 take together time
at most O(max(nff(n); e)) where ff(n) is the inverse Ackermann's function. Each iteration
of the inner for loop lines 16-18 creates one node M of the tree. Therefore, there are
only O(n) iterations of this loop over the entire algorithm. Lines 14, 15 of the outer for
loop require at most n iterations in each iteration of the main loop. The for loop 22-26
is executed exactly once in the course of the entire execution of the algorithm for each
constraint C , and hence takes at most time O(s) over the entire algorithm. Steps 20-21
require time O(e) in each iteration of the main loop. It is easily verified that the remaining
operations in the algorithm take no more time than these. Hence the overall running time
is O(max(n2 ff(n); ne; s)).

17

fiDavis

6.2 Adding Non-strict Comparisons
The algorithm solve constraints can be modified to deal with non-strict comparisons of the
form od(a; b)  od(c; d) by, intuitively, marking the edge ab as \short" on each iteration if
the edge cd has been found to be short.
Specifically, in algorithm solve constraints, we make the following two changes. First,
the revised algorithm takes two parameters: S , the set of strict constraints, and W , the set
of non-strict constraints. Second, we replace the line
H := the connected components of the shorts of S

with the following code:

1.
2.
3.
4.
5.

H := the shorts of S ;
repeat H := the connected components of H ;
for each weak constraint od(a; b)  od(c; d)
if cd is in H then add ab to H endif endfor
until no change has been made to H in the last iteration.

The proof that the revised algorithm is correct is only a slight extension of the proof of
theorem 1 and is given in the appendix.
Optimizing this algorithm for eciency is a little involved, not only because of the new
operations that must be included, but also because there are now four parameters | n, the
number of symbols; e, the number of edges mentioned; s, the number of strict comparison;
and w, the number of non-strict comparisons | and the optimal implementation varies
depending on their relative sizes. In particular, either s or w, though not both, may be
much smaller than n, and each of these cases requires special treatment for optimal eciency.
The best implementation we have found for the case where both s and w are 
(n) has a
running time of O(max(n3 ; nw; s)). The details of the implementation are straightforward
and not of sucient interest to be worth elaborating here.
An immediate consequence of this result is that a couple of problems of inference are
easily computed:

 To determine whether a constraint C is the consequence of a set of constraints S ,
form the set S [ :C and check for consistency. If S [ :C is inconsistent then Sj=C .
Note that the negation of the constraint od(a; b)  od(c; d) is the constraint
od(c; d)  od(a; b).
 To determine whether two sets of constraints are logically equivalent, check that each
constraint in the first is a consequence of the second, and vice versa.

6.3 Adding Order Constraints
Example 3 of Section 2 involves a combination of order-of-magnitude constraints on distances together with simple ordering on points, where the points lie on a one-dimensional
line. We next show how to extend algorithm solve constraints to deal with this more complex situation.

18

fiOrder of Magnitude Comparisons of Distance

In terms of the axiomatics, adding an ordering on points involves positing that the
relation p < q is a total ordering and that the ordering of points is related to order of
magnitude comparisons of distances through the following axiom.
A.9 For points a; b; c 2 P , if a < b < c then od(a; b)  od(a; c).
The following rule is easily deduced: If C and D are disjoint clusters, then either every
point in C is less than all the points in D, or vice versa.
In extending our algorithm, we begin by defining an ordered cluster tree to be a cluster
tree where, for every internal node N , there is a partial order on the children of N . If A
and B are children of N and A is ordered before B , then in an instantiation of the tree,
every leaf of A must precede every leaf of B . Procedure instantiate1 can then be modified
to deal with ordered cluster trees as follows:

instantiate1(in N : a node in a cluster tree; 
 : an om-space; 1 : : : k : orders of magnitude;
in out G : array of points indexed on the nodes of T )
if N is not a leaf then
let C1 : : : Cp be the children of N in topologically sorted order;
x0 := G[N ];
q := N .label;
pick points x1 : : : xp in increasing order such that
for all i; j 2 0 : : : p, if i 6= j then od(xi ; xj ) = q ;
/* Such points can be chosen by virtue of axiom A.8 */
for i = 1 : : : p do
G[Ci ] := xi ;
instantiate1(Ci ; 
; 1 : : : k ; G)
endfor
endif end

instantiate1.

Algorithm solve constraints is modified as follows:
procedure

fNEWg

variables:

begin if

solve constraints2(in S : a system of constraints of the form od(a; b)  od(c; d) ;
O : a system of constraints of the form a < b)
return either an ordered cluster tree T satisfying S
if S is consistent;
or false if S is inconsistent.
m is an integer;
C is a constraint in S ;
H; I are undirected graphs;
M; N; P are nodes of T ;
a; b; c; d are symbols;

S contains any constraint of the form, \od(a; b)  od(c; c)"

then return false

;

fNEWg if O is internally inconsistent (contains a cycle) then return false;
m := the number of variables in S ;
initialize T to consist of a single node N ;
N .symbols:= the variables in S ;
repeat

H := the connected component of the shorts of S ;

19

fiDavis

fNEWg

H := incorporate order(H; O);
if H contains all the edges in S
for

then return false

each leaf N of T do
if not all vertices of N are connected in H then
N .label := m;
for each connected component I of N .symbols in H do
construct node M as a new child of N in T ;
M .symbols:= the vertices of I ;
endfor endif

fNEWg
fNEWg
fNEWg
fNEWg
fNEWg

for

each constraint a < b 2 O
if a is in M .symbols and b is in P .symbols
where M and P are different children of N
then add an ordering arc from M to P ;
endif endfor

endfor

S := the subset of constraints in S whose long is in H ;
m := m 1;
until
for

S is empty;

each leaf N of T
N .label := 0;
if N .symbols has more than one symbol
then create a leaf of N for each symbol in N .symbols;
label each such leaf 0;
endif endfor

end

solve constraints2.

fNEWg

function

incorporate order(in H : undirected graph;
O : a system of constraints of the form a < b)
return undirected graph;

variables:

G : directed graph;
a; b : vertices in H ;
A; B : connected components of H ;
V [A] : array of vertices of G indexed on connected components of H ;
I : subset of vertices of G;

connected component A of H create a vertex V [A] in G;
constraint a < b 2 O
let A and B be the connected components of H containing a and b respectively;
if A 6= B then add an arc in G from V [A] to V [B ] endif endfor;
for each strongly connected component I of G do
for each pair of distinct vertices V [A]; V [B ] 2 I do
for each a 2 A and b 2 B add the edge ab to H endfor endfor

for each
for each

endfor

20

fiOrder of Magnitude Comparisons of Distance

end

incorporate order.

Function incorporate order serves the following purpose. Suppose that we are in the
midst of the main loop of solve constraints2, we have a partially constructed cluster tree,
and we are currently working on finding the sub-clusters of a node N . As in the original
form of solve constraints, we find the connected components of the shorts of the order-ofmagnitude constraints. Let these be C1 : : : Cq ; then we know that the diameter of each Ci
is much smaller than the diameter of N . Now, suppose, for example, that we have in O the
constraints a1 < a5 ; b5 < b2 ; c2 < c1 , where a1 ; c1 2 C1 ; b2 ; c2 2 C2 ; and a5 ; b5 2 C5 . Then
it follows from axiom A.9 that C1 , C2 , and C5 must all be merged into a single cluster,
whose diameter will be less than the diameter of N . Procedure incorporate order finds all
such loops by constructing a graph G whose vertices are the connected components of H
and whose arcs are the ordering relations in O and then computing the strongly connected
components of G. (Recall that two vertices u; v in a directed graph are in the same strongly
connected component if there is a cycle from u to v to u.) It then merges together all of
the connected components of H that lie in a single strongly connected component of G.
The proof of the correctness of algorithm solve constraints2 is again analogous in structure to the proof of theorem 1, and is given in the appendix.
By implementing this in the manner of Section 6.1, the algorithm can be made to run
in time O(max(n2 ff(n); ne; no; s)), where o is the number of constraints in O.

7. Finite order of magnitude comparison
In this section, it is demonstrated that algorithm solve constraints can be applied to systems
of constraints of the form \dist(a; b) < dist(c; d) / B " for finite B in ordinary Euclidean
space as long as the number of symbols in the constraint network is smaller than B .
We could be sure immediately that some such result must apply for finite B . It is
a fundamental property of the non-standard real line that any sentence in the first-order
theory of the reals that holds for all infinite values holds for any suciently large finite
value, and that any sentence that holds for some infinite value holds for arbitrarily large
finite values. Hence, since the answer given by algorithm solve constraints works over a
set of constraints S when the constraint \od(a; b)  od(c; d)" is interpreted as \od(a; b)
< od(c; d)/B for infinite B ", the same answer must be valid for suciently large finite B .
What is interesting is that we can find a simple characterization of B in terms of S ; namely,
that B is larger than the number of symbols in S .
We begin by modifying the form of the constraints, and the interpretation of a cluster
tree. First, to avoid confusion, we will use a four-place predicate \much closer(a; b; c; d)"
rather than the form \od(a; b)  od(c; d)" as we are not going to give an interpretation to
\od" as a function. We fix a finite value B > 1, and interpret \much closer(a; b; c; d)" to
mean \dist(a; b) < dist(c; d) / B ."
We next redefine what it means for a valuation to instantiate a cluster tree:

Definition 6: Let T be a cluster tree and let be a valuation on the symbols in T . We say
that `T if the following holds: For any symbols a; b; c; d in T , let M be the least common
ancestor of a; b and let N be the least common ancestor of c; d. If M .label < N .label then
much closer(a; b; c; d).

21

fiDavis

Procedure \instantiate", which generates an instantiation of a cluster tree, is modified
as follows:
procedure

instantiate(in T : cluster tree; 
 : Euclidean space; B : real);
return : array of points indexed on the symbols of T ;

Let n be the number of nodes in T ;
ff := 2 + 2n + Bn;
Choose 1 ; 2 : : : n such that i < i+1 =ff;
pick a point x 2 
;
G[T ] := x;
instantiate1(T; 
; 1 : : : n ; G);
return the restriction of G to the symbols of T .
end instantiate.
instantiate1(in N : a node in a cluster tree; 
 : a Euclidean space;
1 : : : n : orders of magnitude;
in out G : array of points indexed on the nodes of T )
if N is not a leaf then
let C1 : : : Cp be the children of N ;
x1 := G[N ];
q := N .label;
pick points x2 : : : xp such that
for all i; j 2 1 : : : p, if i 6= j then q  dist(xi ; xj ) < nq
/* This is possible since p  n. */
for i = 1 : : : p do
G[Ci ] := xi ;
instantiate1(Ci ; 
; 1 : : : n ; G)
endfor
endif end

instantiate1.

The analogue of lemma 2 holds for the revised algorithm:
Lemma 22: Any cluster tree T has an instantiation in Euclidean space <m of any dimensionality m.
We can now state theorem 3, which asserts the correctness of algorithm \solve constraints"
in this new setting:

Theorem 3: Let S be a set of constraints over n variables of the form \dist(a; b) <
dist(c; d) / B ", where B > n. The algorithm solve constraints(S ) returns a cluster tree
satisfying S if S is consistent over Euclidean space, and returns false if S is inconsistent.
The proofs of lemma 22 and theorem 3 are given in the appendix.
An examination of the proof of lemma 22 shows that this result does not depend on
any relation between n and B . Therefore, if solve constraints(S ) returns a tree T , then S
is consistent and T satisfies S regardless of the relation between n and B . However, it is
possible for S to be consistent and solve constraints(S ) to return false if n  B . On the
other hand, one can see from the proof of theorem 3 (particularly lemma 23) that if B > n
and solve constraints(S ) returns false then S is inconsistent in any metric space. However,
there are metric spaces other than <m in which the cluster tree returned by solve constraints
may have no instantiation.

22

fiOrder of Magnitude Comparisons of Distance

8. The first-order theory
Our final result asserts that if the om-space is rich enough then the full first-order language
of order-of-magnitude distance comparisons is decidable. Specifically, if the collection of
orders of magnitude is dense and unbounded above, then there is a decision algorithm for
first-order sentences over the formula, \od(W; X )  od(Y; Z )" that runs in time O(4n (n!)2 s)
where n is the number of variables in the sentence and s is the length of the sentence.
The basic reason for this is the following: As we have observed in corollary 4, a cluster
tree T determines the truth value of all constraints of the form \od(a; b)  od(c; d)" where
a; b; c; d are symbols in the tree. That is, any two instantiations of T in any two omspaces agree on any such constraint. If we further require that the om-spaces are dense
and unbounded, then a much stronger statement holds: Any two instantiations of T over
such om-spaces agree on any first-order formula free in the symbols of T over the relation
\od(W; X )  od(Y; Z )". Hence, it suces to check the truth of a sentence over all possible
cluster trees on the variables in the sentence. Since there are only finitely many cluster
trees over a fixed set of variables (taking into account only the relative order of the labels
and not their numeric values), this is a decidable procedure.
Let L be the first-order language with equality with no constant or function symbols,
and the single predicate symbol \much closer(a; b; c; d)". It is easily shown that L is as
expressive as the language with the function symbol \od" and the relation symbol .

Definition 7: An om-space 
 with orders of magnitude
following axiom:

D is dense if it satisfies the

A.9 For all orders of magnitude 1  3 in D, there exists a order of magnitude 2 in D
such that 1  2  3 .

 is unbounded above if it satisfies the following:
A.10 For every order of magnitude 1 in D there exists 2 in D such that 1  2 .
If D is the collection of orders of magnitude in the hyperreal
line, then both of these
p
are satisfied. In axiom [A.9], if 0  1  3 , choose 2 = 1 3 , the geometric mean.
If 0 = 1  3 , choose 2 = 3  where   1. In axiom [A.10] choose 2 = 1 = where
0 <   1.

Definition 8: Let T be a cluster tree. Let l0 = 0; l1 ; l2 : : : lk be the distinct labels in T
in ascending order. An extending label for T is either (a) li for some i; (b) lk + 1 (note that
lk is the label of the root); (c) (li 1 + li )=2 for some i between 1 and k.
Note that if T has k distinct non-zero labels, then there are 2k + 2 different extending
labels for T .

Definition 9: Let T be a cluster tree. Let x be a symbol not in T . The cluster tree
0
T extends T with x if T 0 is formed from T by applying one of the following operations (a
single application of a single operation).
1. T is the null tree and T 0 is the tree containing the single node x.

23

fiDavis

2. T consists of the single node for symbol y. Make a new node M , make both x and y
children of M , and set the label of M to be either 0 or 1.
3. For any internal node N of T (including the root), make x a child of N .
4. Let y be a symbol in T , and let N be its father. If N .label 6= 0, create a new node M
with an extending label for T such that M .label < N .label. Make M a child of N ,
and make x and y children of M .
5. Let C be an internal node of T other than the root, and let N be its father. Create
a new node M with an extending label for T such that C .label < M .label < N .label.
Make M a child of N and make x and C children of M .
6. Let R be the root of T . Create a new node M such that M .label = R.label + 1. Make
R and x children of M . Thus M is the root of the new tree T 0 .
(See Figure 3.)
Note that if T is a tree of n symbols and at most n 1 internal nodes then

 There are n 1 ways to carry out step 3.
 There are n possible ways to choose symbol y in step 4, and at most 2n 2 for the
label on M in each.

 There are at most n 2 different choices for C in step 5, and at most 2n 3 choices
for the label on M in each.

 There is only one way to carry out step 6.
Hence, there are less than 4n2 different extensions of T by x. (This is almost certainly
an overestimate by at least a factor of 2, but the final algorithm is so entirely impractical
that it is not worthwhile being more precise.)

Definition 10: Let T be a cluster tree, and let  be a formula of L open in the variables
of T . T satisfies  if every instantiation of T satisfies .
Theorem 4: Let T be a cluster tree. Let  be an open formula in L, whose free variables
are the symbols of T . Let 
 be an om-space that is dense and unbounded above. Algorithm
decide(T; ) returns true if T satisfies  and false otherwise.

decide(T : cluster tree;  : formula) return boolean
convert  to an equivalent form in which the only logical symbols in  are
: (not), ^ (and), 9 (exists), = (equals) and variable names,
and the only non-logical symbol is the predicate \much closer".
function

case

 has form X = Y : return (distance(X; Y; T ) = 0);
 has form \much closer(W; X; Y; Z )": return distance(W; X; T ) < distance(Y; Z; T ));
 has form : : return not(decide(T; ))
 has form ^ : return(decide(T; ) and decide(T; ))
 has form 9X ff;

24

fiOrder of Magnitude Comparisons of Distance

P

P

P

2

2

2
w

Q

Q

1

w

Q

w

x

1

1

u

u

v

u

v

v

x

Operation 3:

Operation 3:
Original T

N = P

N = Q

P
2
w

Q
1

P

P

2

2
w

Q
1

Operation 4:

Operation 4:
y=v

y=u

M

Q

0/0.5/1/1.5

1
u

v

M

M
0/0.5

0/0.5

u

u

v

x

w

Operation 4:

x

y=w

M
P

3

2
x
M

w

P
2

1.5
Operation 5:
x

w

Q
1

C=Q, N=P

Q
1
u

u

v

v

v
Operation 6:
R=P

Figure 3: Extensions of a cluster tree

25

x

fiDavis

extension

of by , decide(T ; ff) = true

if for some
T0 T X
then return true
else return false endif endcase

end

0

decide

distance(X; Y : symbol; T : cluster tree) return
N := the common ancestor of X and Y in T ;
return(N .label)
function

end

integer

distance

The proof of theorem 4 is given in the appendix.
Running time: As we have remarked above, for a tree T of size k there are at most 4k2
extensions of T to be considered. The total number of cluster trees considered is therefore
bounded by nk=1 4k2 = 4n (n!)2 . It is easily verified that the logical operators other than
quantifiers add at most a factor of s where s is the length of the sentence. Hence the running
time is bounded by O(4n (n!)2 s).
A key lemma, of interest in itself, states the following:
Lemma 28: Let T be a cluster tree. Let  be an open formula in L, whose free variables
are the symbols of T . Let 
 be an om-space that is dense and unbounded above. If one
instantiation of T in 
 satisfies  then every instantiation of T in 
 satisfies .
That is, either  is true for all instantiations of T or for none. The proof is given in the
appendix.
It should be observed that the above conditions on 
 in lemma 28 are necessary, and
that the statement is false otherwise. For example, let 
 be the om-space described in
example I, Section 3, of polynomials over an infinitesimal . Then 
 is not unbounded
above; there is a maximum order-of-magnitude O(1). Let T be the starting tree of Figure
3 (upper-left corner). Let  be the formula \9X od(V; W )  od(W; X )", free in V and W .
Then the valuation fU ! ; V ! 0; W ! 1g satisfies T but not , whereas the valuation
fU ! 2 ; V ! 22 ; W ! g satisfies both T and .

9. Conclusions
The applications of the specific algorithms above are undoubtedly limited; we are not aware
of any practical problems where solving systems of order-of-magnitude relations on distances
is the central problem. However, the potential applications of order-of-magnitude reasoning
generally are very widespread. Ordinary commonsense reasoning involves distances spanning a ratio of about 108 , from a fraction of an inch to thousands of miles, and durations
spanning a ratio of about 1010 , from a fraction of a second to a human lifetime. Scientific
reasoning spans much greater ranges. Explaining the dynamics of a star combines reasoning
about nuclear reactions with reasoning about the star as a whole; these differ by a ratio
of about 1057 . The techniques needed to compute with quantities of such vastly differing
sizes are quite different from the techniques needed to compute with quantities all of similar
sizes. This paper is a small step in the development and analysis of such computational
techniques.
The above results are also significant in the encouragement that they give to the hope
that order-of-magnitude reasoning specifically, and qualitative reasoning generally, may lead

26

fiOrder of Magnitude Comparisons of Distance

to useful quick reasoning strategies in a broader range of problems. It has been often found
in AI that moving from greater to lesser precision in the mode of inference or type of
knowledge does not lead to quick and dirty heuristic techniques, but rather to slow and
dirty techniques. Nonmonotonic reasoning is the most notorious example of this, but it
arises as well in many other types of automated reasoning, including qualitative spatial and
physical reasoning. The algorithms developed in this paper are a welcome exception to
this rule. We are currently studying algorithmic techniques for other order-of-magnitude
problems, and are optimistic of finding similar favorable results.

Acknowledgements
This research has been supported by NSF grant #IRI-9625859. Thanks to Ji-Ae Shin,
Andrew Gelsey, and the reviewers for helpful comments.

Appendix A. Proofs
In this appendix, we give the proofs of the various results asserted in the body of the paper.

Proof of Lemma 2
Lemma 2: If T is a cluster tree and 
 is an om-space, then instantiate(T; 
) returns an
instantiation of T .
Proof: Let 0 = 0. For any node N , if i=N .label, we define (N ) = i . The proof then
proceeds in the following steps:
i. For any nodes M ,N , if M is a descendant of N in T then od(G[M ]; G[N ])  (N ).
Proof: If M is a child of N , then this is immediate from the construction of x2 : : : xp
in instantiate1. Else, let N = N1 ; N2 : : : Nq = M be the path from N to M through
T . By the definition of a cluster tree, it follows that Ni .label < N .label, for i > 1 and
therefore (Ni )  (N ). Thus od(G[M ]; G[N ])  (by the o.m.-triangle inequality)
maxi=1:::q 1 (od(G[Ni+1 ]; G[Ni ]))  maxi=1:::q 1 ((Ni )) (since Ni+1 is the child of
Ni )  (N ).
ii. Let N be a node in T ; let C1 and C2 be two distinct children of N ; and let M1
and M2 be descendants of C1 and C2 respectively. Then od(G[M1 ]; G[M2 ]) = (N ).
Proof: By the construction of x2 : : : xp in instantiate1(N ), od(G[C1 ]; G[C2 ]) = (N ).
By part (i.), od(G[M1 ]; G[C1 ])  (C1 )  (N ) and likewise od(G[M2 ]; G[C2 ]) 
(N ). Hence, by axiom A.6, od(G[M1 ]; G[M2 ]) = (N ).
iii. Let a and b be any two leaves in T , and let N be the least common ancestor in T of
a and b. Then od(G[a]; G[b]) = (N ). Proof: Immediate from (ii).
iv. For any node N , odiam( (N )) = (N ). Proof: From (iii), any two leaves descending
from different children of N are at a distance of order (N ), and no two leaves of N
are at a distance of order greater than (N ).

27

fiDavis

v. For any node N , (N ) is a cluster of (T ). Proof: Let a and b be leaves of N ,
and let c be a leaf of T N . Let I be the common ancestor of a and b in T and
let J be the common ancestor of a and c. Then I is either N or a descendant of N
and J is a proper ancestor of N . Therefore by part (i), (I )  (J ). But by (iii),
od( (a); (b)) = (I )  (J ) = od( (a); (c)).
vi. For any internal nodes N; M if M .label < N .label then odiam( (M ))  odiam( (N )).
Proof: Immediate from (iv) and the construction of .
vii. If C is a cluster of (T ) then there is a node N in T such that C = (N ). Proof: Let
S be the set of symbols corresponding to C and let N be the least common ancestor
of all of S . Let a and b be two symbols in S that are in different subtrees of N . Then
by (iii), od(G[a]; G[b]) = (N ). Let x be any symbol in N .symbols. Then by (iii)
od(G[a]; G[x])  (N ). Hence G[x] 2 C .

2
Proof of Theorem 1
We here prove the correctness of algorithm solve constraints. We will assume throughout
that the two variables in the long of any constraint in S are distinct.
Lemma 3: Let T be a cluster tree and let be an instantiation of T . Let a and b be
symbols of T . Let N be the least common ancestor of a and b in T . Then od( (a); (b)) =
odiam( (N )).
Proof: Since (a) and (b) are elements of (N ), it follows from the definition of odiam that
od( (a); (b))  odiam( (N )). Suppose the inequality were strict; that is, od( (a); (b))
 odiam( (N )). Then let C be the set of all the symbols c of T such that od( (a); (c))
 od( (a); (b)). Then odiam( (C )) = od( (a); (b))  odiam( (N )). It is easily shown
that (C ) is a cluster in (T ). Therefore, by property (ii) of definition 5, there must be
a node M such that M .symbols = C . Now, M is certainly not an ancestor of N , since
odiam( (M ))  odiam( (N )) but M .symbols contains both a and b. But this contradicts
the assumption that N was the least common ancestor of a and b. 2
Corollary 4: Let T be a cluster tree and let be an instantiation of T . Let a; b; c; d be
symbols of T . Let N be the least common ancestor of c and d in T , and let M be the
least common ancestor of a and b in T . Then od( (a); (b))  od( (c); (d)) if and only if
M .label < N .label.
Proof: Immediate from lemma 3 and property (iii) of definition 5 of instantiation. 2

Lemma 5: Let S be any set of constraints of the form od(a; b)  od(c; d). Let H be the
connected components of the shorts of S . If S is consistent, then not every edge of S is in
H.
Proof: Let be a valuation satisfying S . Find an edge pq in S for which od( (p); (q)) is
maximal. Now, if ab is a short of S | that is, there is a constraint od(a; b)  od(c; d) in
S | then od( (a); (b))  od( (c); (d))  od( (p); (q)).

28

fiOrder of Magnitude Comparisons of Distance

Now, let ab be any edge in H , the connected components of the shorts of S . Then there
is a path a1 = a; a2 : : : ak = b such that the edge ai ai+1 is a short of S for i = 1 : : : k 1.
Thus, by the om-triangle inequality, od( (a); (b))  maxi=1::k 1(od( (ai ); (ai+1 ))) 
od( (p); (q)). Hence pq 6= ab, so pq is not in H . 2

Lemma 6: The values of S and H in any iteration are supersets of their values in any later
iteration.

Proof: S is reset to a subset of itself at the end of each iteration. H is defined in terms of
S in a monotonic manner. 2

S cannot be the same in two successive iterations of the main loop.
Proof: by contradiction. Suppose that S is the same in two successive iterations. Then H
will be the same, since it is defined in terms of S . H is constructed to contain all the shorts
of S , Since the resetting of S at the end of the first iteration does not change S , H must
contain all the longs as well. Thus, H contains all the edges in S . But that being the case,
the algorithm should have terminated with failure at the beginning of the first iteration. 2
Lemma 7:

Lemma 8: Algorithm solve constraints always terminates.
Proof: By lemma 7, if the algorithm does not exit with failure, then on each iteration some
constraints are removed from S . Hence, the number of iterations of the main loop is at
most the original size of S . Everything else in the algorithm is clearly bounded. (Note that
this bound on the number of iterations is improved in Section 6.1 to n 1, where n is the
number of symbols.) 2

Lemma 9: If algorithm solve constraints returns false, then S is inconsistent.

Proof: If the algorithm returns false, then the transitive closure of the shorts of S contains
all the edges in S . By lemma 5, S is inconsistent.

Lemma 10: If constraint C of form od(a; b)  od(c; d) is in the initial value of S , and
edge cd is in H in some particular iteration, then constraint C is in S at the start of that
iteration.
Proof: Suppose that C is deleted from S on some particular iteration. Then edge cd, the
long of C , cannot be in H in that iteration. That is, it is not possible for edge cd to persist
in H in an iteration after C has been deleted from S . Note that, by lemma 6, once cd is
eliminated from H , it remains out of H . 2
Lemma 11: The following loop invariant holds: At the end of each loop iteration, the
values of L.symbols, where L is a leaf in the current state of the tree, are exactly the
connected components of H .

Proof: In the first iteration, T is initially just the root R, containing all the symbols, and
a child of R is created for each connected component of H .
Let Ti and Hi be the values of T and H at the end of the ith iteration. Suppose that
the invariant holds at the end of the kth iteration. By lemma 6, Hk+1 is a subset of Hk .
Hence, each connected component of Hk+1 is a subset of a connected component of Hk .

29

fiDavis

Moreover, each connected component J of Hk is either a connected component of Hk+1 or
is partitioned into several connected components of Hk+1 . In the former case, the leaf of
Tk corresponding to J is unchanged and remains a leaf in Tk+1 . In the latter case, the leaf
corresponding to J gets assigned one child for each connected component of Hk+1 that is a
subset of J . Thus, the connected components of Hk+1 correspond to the leaves of Tk+1 . 2

Lemma 12: If procedure solve constraints does not return false, then it returns a wellformed cluster tree T .
Proof: Using lemma 11, and the cleanup section of solve constraints which creates the final
leaves for symbols, it follows that every symbol in S ends up in a single leaf of T . As m is
decremented on each iteration, and as no iteration adds both a new node and children of
that node, it follows that the label of each internal node is less than the label of its father.
Hence the constraints on cluster trees (definition 3) are satisfied. 2
Lemma 13: Let a; b be two distinct symbols in S and let T be the cluster tree returned
by solve constraints for S . Let N be the least common ancestor of a; b in T . Then either N
is assigned its label on the first iteration when the edge ab is not in H , or the edge ab is in
the final value of H when the loop is exited and N is assigned its label in the final cleanup
section.
Proof: As above, let Hi be the value of H in the ith iteration.
If N is the root, then it is assigned its label in the first iteration. Clearly, a and b, being
in different subtrees of N , must be in different connected components of H1 .
Suppose N is assigned its label in the kth iteration of the loop for k > 1. By lemma 11,
at the end of the previous iteration, N .symbols was a connected component of Hk 1 , and it
therefore contained the edge ab. Since N is the least common ancestor of a; b, it follows that
a and b are placed in two different children of N ; hence, they are in two different connected
components of Hk . Thus the edge ab cannot be in Hk .
Suppose N is assigned its label in the cleanup section of the algorithm. Then by lemma
11, N .symbols is a connected component of the final value of H . Hence the edge ab was in
the final value of H . 2
Lemma 14: Let S initially contain constraint C of form od(a; b)  od(c; d). Suppose that
solve constraints(S ) returns a cluster tree T . Let M be the least common ancestor of a; b
in T and let N be the least common ancestor of c; d. Then M .label < N .label.
Proof: Suppose N is given a label in a given iteration. By lemma 13, cd is eliminated
from H in that same iteration. By lemma 10, constraint C must be in S at the start of the
iteration. Hence ab is a short of S in the iteration, and is therefore in H . Hence M is not
given a label until a later iteration, and therefore is given a lower label.
It is easily seen that cd cannot be in H in the final iteration of the loop, and hence N
is not assigned its label in the cleanup section. 2
Lemma 15: Suppose that solve constraints(S ) returns a cluster tree T . Then any instantiation of T satisfies the constraints S .
Proof: Immediate from lemma 14 and corollary 4.

30

fiOrder of Magnitude Comparisons of Distance

Theorem 1: The algorithm solve constraints(S ) returns a cluster tree satisfying S if S is
consistent, and returns false if S is inconsistent.
Proof: If solve constraints(S ) returns false, then it is inconsistent (lemma 9). If it does
not return false, then it returns a cluster tree T (lemma 12). Since T has an instantiation
(lemma 2) and since every instantiation of T is a solution of S (lemma 15), it follows that
S is consistent and T satisfies S . 2
Proof of Theorem 2
Lemma 16: If S1 and S2 are consistent sets of constraints, and S1  S2 then
reduce constraints(S1 )  reduce constraints(S2 ).
Proof: Immediate by construction. The value of H in the case of S1 is a superset of its value
in the case of S2 , and hence reduce constraints(S1 ) is a superset of reduce constraints(S2 ).
Lemma 17: If S1 and S2 are consistent sets of constraints, and S1  S2 then num labels(S1)
 num labels(S2).
Proof by induction on num labels(S2). If num labels(S2) = 0, the statement is trivial.
Suppose that the statement holds for all S 0 , where num labels(S 0) = k.
Let num labels(S2) = k + 1.
Then k + 1 = num labels(S2 ) = 1 + num labels(reduce constraints(S2 )), so
k =num labels(reduce constraints(S2 )). Now, suppose S1  S2 . By lemma 16
reduce constraints(S1 )  reduce constraints(S2 ). But then by the inductive hypothesis
num labels(reduce constraints(S1 ))  num labels(reduce constraints(S2 )), so
num labels(S1)  num labels(S2). 2
Lemma 18: Let S be a set of constraints, and let be a solution of S . For any graph G
over the symbols of S , let nd(G; ) be the number of different non-zero values of od(a; b)
where edge ab is in G. Let edges(S ) be the set of edges in S . Then nd(edges(S ), ) 
num labels(S ).
Proof: by induction on num labels(S ). If num labels(S ) = 0, then the statement is trivial.
Suppose for some k, the statement holds for all S 0 where num labels(S 0) = k, and suppose
num labels(S ) = k + 1. Let pq be the edge in S of maximal length. For any set of edges E ,
let small-edges(E; ) be the set of all edges ab in E for which
od( (a); (b))  od( (p); (q)). Since small-edges(E ) contains edges of every order of
magnitude in E except the order of magnitude of pq, it follows that
nd(small-edges(E; ), ) = nd(E; ) 1. Let G be the complete graph over all the symbols
in S . By the same argument as in lemma 5, small-edges(G; )  H , where H is the connected
components of the shorts of S , as computed in reduce constraints(S ). Let S 0 be the set of
constraints whose longs are in small-edges(G; ). It follows that S 0  reduce constraints(S ).
Now small-edges(G; )  edges(S 0 )  edges(reduce constraints(S )).
Hence nd(edges(S ), ) = nd(G; ) = nd(small-edges(G; ), ) + 1
 nd(edges(reduce constraints(S ))) + 1  (by the inductive hypothesis)
num labels(reduce constraints(S )) + 1 = num labels(S ). 2

31

fiDavis

Theorem 2: Out of all solutions to the set of constraints S , the instantiations of
solve constraints(S ) have the fewest number of different values of od(a; b), where a; b range
over the symbols in S . This number is given by num labels(S ).
Proof: Immediate from lemma 18.
Corollary 19: Let 
 have all the properties of an om-space except that it has only k
different orders of magnitude. A system of constraints S has a solution in 
 if and only if
the tree returned by solve constraints(S ) uses no more than k different labels.
Proof: Immediate from theorems 1 and 2. 2
Proof of Algorithm for Non-strict Comparisons
We now prove that the revised algorithm presented in Section 6.2 for non-strict comparisons
is correct. The proof is only a slight extension of the proof of theorem 1, given above.
Recall that the revised algorithm in Section 6.2 replaces the line of solve constraints
H := the connected components of the shorts of S
with the following code:

1.
2.
3.
4.
5.

H := the shorts of S ;
repeat H := the connected components of H ;
for each weak constraint od(a; b)  od(c; d)
if cd is in H then add ab to H endif endfor
until no change has been made to H in the last iteration.

We need the following new lemmas and proofs:

Lemma 20: Let S be a set of strict comparisons, and let W be a set of non-strict comparisons. Let H be the set of edges output by the above code. If S [ W is consistent, then
there is an edge in S that is not in H .
Proof: As in the proof of lemma 5, let be a valuation satisfying S [ W and let pq be
an edge in S such that od( (p); (q)) is maximal. We wish to show that, for every edge
ab 2 H , od( (a); (b))  od( (p); (q)), and hence ab 6= pq. Proof by induction: suppose
that this holds for all the edges in H at some point in the code, and that ab is now to be
added to H . There are three cases to consider.

 ab is added in step [1]. Then, as in lemma 5, there is a constraint od(a; b)  od(c; d)
in S . Hence od( (a); (b))  od( (c); (d))  od( (p); (q)).
 ab is added in step [2]. Then there is a path a1 = a; a2 : : : ak = b such that the edge
ai ai+1 is in H for i = 1 : : : k 1. By the inductive hypothesis, od( (ai ); (ai+1 )) 
od( (p); (q)). By the om-triangle inequality,
od( (a); (b))  maxi=1::k 1(od( (ai ); (ai+1 )))  od( (p); (q)).

 ab is added in step [4]. Then there is a constraint od(a; b)  od(c; d) in W such that
cd is in H . By the inductive hypothesis, od( (c); (d))  od( (p); (q)).
32

fiOrder of Magnitude Comparisons of Distance

2

Lemma 21: Let W contain the constraint od(a; b)  od(c; d). Suppose that the algorithm
returns a cluster tree T . Let M be the least common ancestor of a and b in T , and let N
be the least common ancestor of c and d. Then M .label  N .label.
Proof: By lemma 13, N is assigned a label in the first iteration where H does not include
the edge cd. In all previous iterations, since cd is in H , ab will likewise be put into H .
Hence M does not get assigned a label before N , so M .label  N .label.
The remainder of the proof of the correctness of the revised algorithm is exactly the
same as the proof of theorem 1.
Validation of Algorithm Solve constraints2
The proof of the correctness of algorithm solve constraints2 is again analogous in structure
to the proof of theorem 1. We sketch it below: the details are not dicult to fill in.
1. (Analogue of lemma 2:) If T is an ordered cluster tree, then the revised version of
instantiate(T ) returns an instantiation of T . The proof is exactly the same as lemma
2, with the additional verification that instantiate2 preserves the orderings in T .
2. (Analogue of lemma 5:) Let S be a set of order-of-magnitude constraints on distances,
and let O be a set of ordering constraints on points. Let H be the graph given by the
two statements

H := the connected components of the shorts of S ;
H := incorporate order(H; O);
If S and O are consistent, then H does not contain all the edges of S .
Proof: As in the proof of lemma 5, choose a valuation satisfying S ; O and let pq be
an edge in S for which od( (p); (q)) is maximal. Following the informal argument
presented in Section 6.3, it is easily shown that pq is longer than any of the edges
added in these two statements, and hence it is not in H .
3. (Analogue of lemma 9:) If solve constraints2 returns false, then S ; O is inconsistent.
Proof: Immediate from (2).
4. (Analogue of lemma 12:) If solve constraints2(S ; O) does not return false, then it
returns a well-formed ordered cluster tree.
Proof: By merging the strongly connected components of G, incorporate order always
ensures that the ordering arcs between connected components of H form a DAG. These
arcs are precisely the same ones that are later added among the children of node N as
ordering arcs. Thus, the ordering arcs over the children of a node in the cluster tree
form a DAG. Otherwise, the construction of the tree T is the same as in lemma 12.
The remainder of the proof is the same as the proof of theorem 1.

33

fiDavis

Proof of Theorem 3
We begin by proving lemma 22, that the revised version of \instantiate", given in Section
6.3, gives an instantiation of a cluster tree in Euclidean space.
Lemma 22: Any cluster tree T has an instantiation in Euclidean space <m of any dimensionality m.
The proof is essentially the same as the proof of Lemma 2, except that we now have
to keep track of real quantities. For any node N , if i=N .label, we define (N ) = i . The
proof then proceeds in the following steps:
i. For any i < j , i < j =ffj i . Immediate by construction.
ii. For any nodes M ,C , if M is a descendant of C in T then
dist(G[M ]; G[C ]) < ffn(C )=(ff 1).
Proof: Let C = C0 ; C1 : : : Cr = M be the path from P
C to M through T . Then
dist(
the triangle inequality) ri=01 dist(G[Ci+1]; G[Ci ]) 
Pr 1G(n[M(];CG)[=ffC ])i) < (by
(ff=(ff 1))(n(C )).
i=0
iii. Let N be a node in T ; let C1 and C2 be two children of N ; and let M1 and M2 be
descendants of C1 and C2 respectively. Then
(N )(1 2n=(ff 1)) < dist(G[M1]; G[M2 ]) < n(N )(1 + 2=(ff 1))
Proof: By the triangle inequality,
dist(G[C1 ]; G[C2 ])  dist(G[C1 ]; G[M1 ]) + dist(G[M1 ]; G[M2 ]) + dist(G[M2 ]; G[C2 ]).
Thus, dist(G[C1]; G[C2 ]) dist(G[C1]; G[M1 ]) dist(G[M2 ]; G[C2 ])  dist(G[M1 ]; G[M2 ]).
Also, by the triangle inequality,
dist(G[M1 ]; G[M2 ])  dist(G[C1]; G[C2 ]) + dist(G[C1]; G[M1 ]) + dist(G[M2 ]; G[C2 ]).
By construction, (N )  dist(G[C1]; G[C2 ]) < n(N ),
and by part (ii), for i = 1; 2, dist(G[Mi]; G[Ci ]) < ffn(C )=(ff 1) < n(N )=(ff 1)
as (C ) < (N )=ff.
iv. For any symbols a; b; c; d in T , let P be the least common ancestor of a; b and let N
be the least common ancestor of c; d. If P .label < N .label then
much closer(G[a]; G[b]; G[c]; G[d]).
Proof: By part (iii), dist(G[a]; G[b]) < n(P )(1 + 2=(ff 1))
and dist(G[c]; G[d]) > (N )(1 2n=(ff 1)). Since (P ) < (N )=ff and since
ff = 2 + 2n + Bn, it follows by straightforward algebra that
dist(G[a]; G[b]) < dist(G[c]; G[d]) / B .

2

We next prove the analogue of lemma 5.

Lemma 23: Let S be a set of constraints over n variables of the form
\dist(a; b) < dist(c; d) / B ", where B > n. If S is consistent, then there is some edge in S
which is not in the connected components of the shorts of S .
Proof: Let be a valuation satisfying S . Let pq be the edge in S for which dist( (p); (q))
is maximal. Now, if ab is a short of S | that is, there is a constraint much closer(a; b; c; d)
in S | then dist( (a); (b)) < dist( (c); (d))/B  dist( (p); (q))/B .

34

fiOrder of Magnitude Comparisons of Distance

Now, let ab be any edge in H , the connected components of the shorts of S . Then
there is a simple path a1 = a; a2 : : : ak = b such that the edge ai ai+1 is a short of S for
i = 1 : : : k 1. Note that k  n. Then, by the triangle inequality,
dist( (a); (b)) 
dist( (a1 ); (a2 )) + dist( (a2 ); (a3 )) + . . . + dist( (ak 1 ); (ak )) 
(k 1)dist( (p); (q)) / B < dist( (p); (q))

Hence pq 6= ab, so pq is not in H . 2

Theorem 3: Let S be a set of constraints over n variables of the form \dist(a; b) < dist(c; d)
/ B ", where B > n. The algorithm solve constraints(S ) returns a cluster tree satisfying S
if S is consistent over Euclidean space, and returns false if S is inconsistent.
Proof: Note that the semantics of the constraints \much closer(a; b; c; d)" enters into the
proof of Theorem 1 only in lemmas 2 and 5. The remainder of the proof of Theorem 1 has to
do purely with the relation between the structure of S and the structure of the tree. Hence,
since we have shown that the analogues of lemmas 2 and 5 hold in a set of constraints of
this kind, the same proof can be completed in exactly the same way. 2
Proof of Theorem 4
Lemma 24: Let T be a cluster tree and let be a valuation over om-space 
 satisfying T .
Let x be a symbol not in T , let a be a point in 
, and let 0 be the valuation [ fx ! ag.
Then there exists an extension T 0 of T by x such that 0 satisfies T 0 .
Proof: If T is the empty tree, the statement is trivial. If T contains the single symbol y,
then if a = (y) then operation (2) applies with M .label=0; if a 6= (y) then operation (2)
applies with M .label=1.
Otherwise, let y be the symbol in T such that od( (y); a) is minimal. (We will deal
with the case of ties in step (D) below.) Let F be the father of y in T .
Let D=od( (y); a). Let V be the set of all orders of magnitude od( (p); (q)), where
p and q range over symbols in T . We define L to be the suitable label for D as follows: If
D 2 V , then L is the label in T corresponding to D. If D is larger than any value in V
then L is the label of the root of T plus 1. If D 62 V , but some value in V is larger than D,
then let D1 be the largest value in V less than D; let D2 be the smallest value in V greater
than D; let L1 , L2 be the labels in T corresponding to D1 , D2 ; and let L = (L1 + L2 )=2.
One of the following must hold:
A. (y) = a, and F .label=0. Then apply operation (3) with N = F .
B. (y) = a and F .label 6= 0. Then apply operation (4) with M .label = 0.
C. (y) 6= a, but od( (y); a) is less than od( (z ); a) for any other symbol z =
6 y in T .
Apply operation (4) with M .label set to the suitable value for D in T .
D. There is more than one value y1 : : : yk for which od( (yi ); a) = D. It is easily shown
that in this case there is an internal node Q such that y1 : : : yk is just the set of symbols
in the subtree of Q. There are three cases to consider:

35

fiDavis

D.i D=odiam( (Q.symbols)). Then apply operation (3) with N = Q.
D.ii D > odiam( (Q.symbols)), and Q is not the root. Then apply operation (5)
with C = Q. Set M .label to be the suitable value for D. (It is easily shown that
D < odiam( (N .symbols)), where N is the father of Q.)
D.iii D > odiam( (Q.symbols)), and Q is the root. Apply operation (6).

2

Lemma 25: Let A = fa1 : : : ak g be a finite set of points whose diameter has order-ofmagnitude D. Then there exists a point u such that, for i = 1 : : : k, od(u; ai ) = D.
Proof: Let b1 = a1 . By axiom A.8 there exists an infinite collection of points b2 ; b3 : : :
such that od(bi ; bj ) = D for i 6= j . Now, for any value ai there can be at most one value bj
such that od(ai ; bj )  D; if there were two such values bj 1 and bj 2, then by the om-triangle
inequality, od(bj 1; bj 2 )  D. Hence, all but k different values of bj are at least D from any
of the ai . Let u be any of these values of bj . Then since od(u; a1 ) = D and od(a1 ; ai )  D
for all i, it follows that od(u; ai )  D for all ai . Thus, since od(u; ai )  D but not od(u; ai )
 D, it follows that od(u; ai) = D. 2
Lemma 26: Let T be a cluster tree; let be a valuation over om-space 
 satisfying T ;
and let T 0 be an extension of T by x. If 
 is dense and unbounded above, then there is a
value a such that the valuation [ fx ! ag satisfies T 0 .
Proof: For operations (1) and (2) the statement is trivial.
Otherwise, let L be an extending label of T . If L = 0, then set D = 0. If L is in T ,
then let D be the order of magnitude corresponding to L in T under . If L1 < L < L2
where L1 and L2 are labels of consecutive values in T , then let D1 and D2 be the orders of
magnitude corresponding to L1 , L2 in T under . Let D be chosen so that D1  D  D2 .
If L is greater than any label in the tree, then choose D to be greater than the diameter of
the tree under .
If T 0 is formed from T by operation (3), then using lemma 25 let a be a point such that
od(a; (y)) = odiam(N ) for all y in N .symbols.
If T 0 is formed from T by operation (4), then let a be a point such that od(a; (y)) =
D.
If T 0 is formed from T by operation (5), then let a be a point such that od(a; (y)) =
D for all y in C .symbols. (Note that, since M .label < N .label, D < odiam(N .symbols).)
If T 0 is formed from T by operation (6), then let a be a point such that od(a; (y)) =
D for all y in R.symbols.
In each of these cases, it is straightforward to verify that [ fx ! ag satisfies T 0 . 2
As we observed in Section 8 regarding lemma 28, the conditions on 
 in lemma 26
are necessary, and the statement is false otherwise. For example, let 
 be the om-space
described in example I, Section 3, of polynomials over an infinitesimal . Then 
 is not
unbounded above; there is a maximum order-of-magnitude O(1). Let T be the starting tree
of Figure 3 (upper-left corner), and let T 0 be the result of applying operation 6 (middle
bottom). Let be the valuation fu ! ; v ! 2; w ! 1g. Then satisfies T , but it cannot
be extended to a valuation that satisfies T 0 , as that would require x to be given a value
such that od(v; w)  od(x; w), and no such value exists within 
. The point of the lemma

36

fiOrder of Magnitude Comparisons of Distance

is that, if 
 is required to be both dense and unbounded above, then we cannot get \stuck"
in this way.

Lemma 27: Let T be a cluster tree. Let X be a variable not among the symbols of T .
Let ff be an open formula in L, whose free variables are the symbols of T and the variable
X . Let  be the formula 9X ff. Let 
 be an om-space that is dense and unbounded above.
Then there exists an instantiation of T in 
 that satisfies  if and only if there exists an
extension T 0 of T and an instantiation 0 of T 0 that extends and satisfies ff.
Proof: Suppose that there exists an instantiation of T that satisfies 9X ff. Then, by
definition, there is a point a in 
 such that satisfies ff(X=a). That is, the instantiation
[ fX ! ag satisfies ff. Let 0 = [ fX ! ag. By lemma 24, the cluster tree T 0
corresponding to 0 is an extension of T .
Conversely, suppose that there exists an extension T 0 of T and an instantiation 0 of T 0
satisfying ff. Let be the restriction of 0 to the symbols of T . Then clearly satisfies the
formula 9X ff. 2
Lemma 28: Let T be a cluster tree. Let  be an open formula in L, whose free variables
are the symbols of T . Let 
 be an om-space that is dense and unbounded above. If one
instantiation of T in 
 satisfies  then every instantiation of T in 
 satisfies .
Proof: We can assume without loss of generality that the only logical symbols in  are :
(not), ^ (and), 9 (exists), = (equals) and variables names, and that the only non-logical
symbol is the predicate \much closer". We now proceed using structural induction on the
form of . Note that an equivalent statement of the inductive hypothesis is, \For any formula
, either is true under every instantiation of T , or is false under every instantiation of
T ."
Base case: If  is an atomic formula \X = Y " or \much closer(W; X; Y; Z )" then this
follows immediately from corollary 4.
Let  have the form : . If  is true under , then is false under . By the inductive hypothesis, is false under every instantiation of T . Hence  is true under every
instantiation of T .
Let  have the form ^ . If  is true under then both and  are true under . By
the inductive hypothesis, both and  are true under every instantiation of T . Hence  is
true under every instantiation of T .
Let  have the form 9X ff. If  is true under then by lemma 27, there exists an
extension T 0 of T and a instantiation 0 of T 0 such that ff is true under 0 . By the inductive
hypothesis, ff is true under every instantiation of T 0 . Now, if 0 is an instantiation of T 0
that satisfies ff, and  is the restriction of 0 to the variables in T , then clearly  satisfies
9X ff. But by lemma 26, every instantiation  of T can be extended to an instantiation 0
of T 0 . Therefore, every instantiation of T satisfies . 2
Theorem 4: Let T be a cluster tree. Let  be an open formula in L, whose free variables
are the symbols of T . Let 
 be an om-space that is dense and unbounded above. Algorithm
decide(T; ) returns true if T satisfies  and false otherwise.
Proof: Immediate from the proof of lemma 28. 2

37

fiDavis

References
Cormen, T.H., Leiserson, C.E., and Rivest. R.L. (1990). Introduction to Algorithms. Cambridge, MA: MIT Press
Davis, E. (1990). Order of Magnitude Reasoning in Qualitative Differential Equations. In
D. Weld and J. de Kleer (Eds.) Readings in Qualitative Reasoning about Physical Systems.
San Mateo, CA: Morgan Kaufmann. 422-434.
Keisler, J. (1976). Foundations of Infinitesimal Calculus. Boston, MA: Prindle, Webber,
and Schmidt.
Mavrovouniotis, M. and Stephanopoulos, G. (1990). \Formal Order-of-Magnitude Reasoning in Process Engineering." In D. Weld and J. de Kleer (Eds.) Readings in Qualitative
Reasoning about Physical Systems. San Mateo, CA: Morgan Kaufmann. 323-336.
Raiman, O. (1990). \Order of Magnitude Reasoning." In D. Weld and J. de Kleer (Eds.)
Readings in Qualitative Reasoning about Physical Systems. San Mateo, CA: Morgan Kaufmann. 318-322.
Robinson, A. (1965). Non-Standard Analysis. Amsterdam: North-Holland Publishing Co.
Weld, D. (1990). \Exaggeration." In D. Weld and J. de Kleer (Eds.) Readings in Qualitative
Reasoning about Physical Systems. San Mateo, CA: Morgan Kaufmann. 417-421.

38

fi