Journal of Artificial Intelligence Research 5 (1996) 301-328

Submitted 4/96; published 12/96

Exploiting Causal Independence in Bayesian Network Inference
Nevin Lianwen Zhang

LZHANG @ CS . UST. HK

Department of Computer Science,
University of Science & Technology, Hong Kong

David Poole

POOLE @ CS . UBC . CA

Department of Computer Science, University of British Columbia,
2366 Main Mall, Vancouver, B.C., Canada V6T 1Z4

Abstract
A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability
into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even
smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new
formulation of causal independence lets us specify the conditional probability of a variable given its
parents in terms of an associative and commutative operator, such as or, sum or max, on the
contribution of each parent. We start with a simple algorithm VE for Bayesian network inference
that, given evidence and a query variable, uses the factorization to find the posterior distribution of
the query. We show how this algorithm can be extended to exploit causal independence. Empirical
studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient
than previous methods and allows for inference in larger networks than previous algorithms.

1. Introduction
Reasoning with uncertain knowledge and beliefs has long been recognized as an important research
issue in AI (Shortliffe & Buchanan, 1975; Duda et al., 1976). Several methodologies have been
proposed, including certainty factors, fuzzy sets, Dempster-Shafer theory, and probability theory.
The probabilistic approach is now by far the most popular among all those alternatives, mainly due
to a knowledge representation framework called Bayesian networks or belief networks (Pearl, 1988;
Howard & Matheson, 1981).
Bayesian networks are a graphical representation of (in)dependencies amongst random variables.
A Bayesian network (BN) is a DAG with nodes representing random variables, and arcs representing
direct influence. The independence that is encoded in a Bayesian network is that each variable is
independent of its non-descendents given its parents.
Bayesian networks aid in knowledge acquisition by specifying which probabilities are needed.
Where the network structure is sparse, the number of probabilities required can be much less than the
number required if there were no independencies. The structure can be exploited computationally to
make inference faster (Pearl, 1988; Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990; Shafer &
Shenoy, 1990).
The definition of a Bayesian network does not constrain how a variable depends on its parents.
Often, however, there is much structure in these probability functions that can be exploited for knowledge acquisition and inference. One such case is where some dependencies depend on particular
values of other variables; such dependencies can be stated as rules (Poole, 1993), trees (Boutilier

c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiZ HANG & P OOLE

et al., 1996) or as multinets (Geiger & Heckerman, 1996). Another is where the the function can be
described using a binary operator that can be applied to values from each of the parent variables. It
is the latter, known as causal independencies, that we seek to exploit in this paper.
Causal independence refers to the situation where multiple causes contribute independently to
a common effect. A well-known example is the noisy OR-gate model (Good, 1961). Knowledge
engineers have been using specific causal independence models in simplifying knowledge acquisition (Henrion, 1987; Olesen et al., 1989; Olesen & Andreassen, 1993). Heckerman (1993) was the
first to formalize the general concept of causal independence. The formalization was later refined by
Heckerman and Breese (1994).
Kim and Pearl (1983) showed how the use of noisy OR-gate can speed up inference in a special
kind of BNs known as polytrees; DAmbrosio (1994, 1995) showed the same for two level BNs with
binary variables. For general BNs, Olesen et al. (1989) and Heckerman (1993) proposed two ways
of using causal independencies to transform the network structures. Inference in the transformed
networks is more efficient than in the original networks (see Section 9).
This paper proposes a new method for exploiting a special type of causal independence (see Section 4) that still covers common causal independence models such as noisy OR-gates, noisy MAXgates, noisy AND-gates, and noisy adders as special cases. The method is based on the following
observation. A BN can be viewed as representing a factorization of a joint probability into the multiplication of a list of conditional probabilities (Shachter et al., 1990; Zhang & Poole, 1994; Li &
DAmbrosio, 1994). The type of causal independence studied in this paper leads to further factorization of the conditional probabilities (Section 5). A finer-grain factorization of the joint probability
is obtained as a result. We propose to extend exact inference algorithms that only exploit conditional
independencies to also make use of the finer-grain factorization provided by causal independence.
The state-of-art exact inference algorithm is called clique tree propagation (CTP) (Lauritzen &
Spiegelhalter, 1988; Jensen et al., 1990; Shafer & Shenoy, 1990). This paper proposes another algorithm called variable elimination (VE ) (Section 3), that is related to SPI (Shachter et al., 1990; Li
& DAmbrosio, 1994), and extends it to make use of the finer-grain factorization (see Sections 6, 7,
and 8). Rather than compiling to a secondary structure and finding the posterior probability for each
variable, VE is query-oriented; it needs only that part of the network relevant to the query given the
observations, and only does the work necessary to answer that query. We chose VE instead of CTP
because of its simplicity and because it can carry out inference in large networks that CTP cannot
deal with.
Experiments (Section 10) have been performed with two CPCS networks provided by Pradhan.
The networks consist of 364 and 421 nodes respectively and they both contain abundant causal independencies. Before this paper, the best one could do in terms of exact inference would be to first
transform the networks by using Jensen et al.s or Heckermans technique and then apply CTP. In
our experiments, the computer ran out of memory when constructing clique trees for the transformed
networks. When this occurs one cannot answer any query at all. However, the extended VE algorithm has been able to answer almost all randomly generated queries with twenty or less observations
(findings) in both networks.
One might propose to first perform Jensen et al.s or Heckermans transformation and then apply
VE . Our experiments show that this is significantly less efficient than the extended VE algorithm.
We begin with a brief review of the concept of a Bayesian network and the issue of inference.
302

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

2. Bayesian Networks
We assume that a problem domain is characterized by a set of random variables. Beliefs are represented by a Bayesian network (BN)  an annotated directed acyclic graph, where nodes represent
the random variables, and arcs represent probabilistic dependencies amongst the variables. We use
the terms node and variable interchangeably. Associated with each node is a conditional probability of the variable given its parents.
In addition to the explicitly represented conditional probabilities, a BN also implicitly represents
conditional independence assertions. Let x1 , x2 , ..., xn be an enumeration of all the nodes in a BN
such that each node appears after its children, and let xi be the set of parents of a node xi . The
Bayesian network represents the following independence assertion:
Each variable xi is conditionallyindependent of the variables in fx1 ; x2; : : :; xi,1 g given
values for its parents.
The conditional independence assertions and the conditional probabilities together entail a joint probability over all the variables. By the chain rule, we have:

P (x1; x2; : : :; xn)

=

=

n
Y
i=1
n
Y
i=1

P (xi jx1; x2; : : :; xi,1)
P (xi jx );
i

(1)

where the second equation is true because of the conditional independence assertions. The conditional probabilities P (xi jxi ) are given in the specification of the BN. Consequently, one can, in
theory, do arbitrary probabilistic reasoning in a BN.
2.1 Inference
Inference refers to the process of computing the posterior probability P (X jY =Y0 ) of a set X of
query variables after obtaining some observations Y =Y0 . Here Y is a list of observed variables and
Y0 is the corresponding list of observed values. Often, X consists of only one query variable.
In theory, P (X jY =Y0 ) can be obtained from the marginal probability P (X; Y ), which in turn
can be computed from the joint probability P (x1 ; x2; : : :; xn) by summing out variables outside
X [Y one by one. In practice, this is not viable because summing out a variable from a joint probability requires an exponential number of additions.
The key to more efficient inference lies in the concept of factorization. A factorization of a joint
probability is a list of factors (functions) from which one can construct the joint probability.
A factor is a function from a set of variables into a number. We say that the factor contains a variable if the factor is a function of that variable; or say it is a factor of the variables on which it depends.
Suppose f1 and f2 are factors, where f1 is a factor that contains variables x1 ; : : :; xi; y1; : : :; yj 
we write this as f1 (x1 ; : : :; xi; y1; : : :; yj )  and f2 is a factor with variables y1 ; : : :; yj ; z1; : : :; zk ,
where y1 ; : : :; yj are the variables in common to f1 and f2 . The product of f1 and f2 is a factor that
is a function of the union of the variables, namely x1 ; : : :; xi; y1; : : :; yj ; z1; : : :; zk , defined by:

f1 f2)(x1; : : :; xi; y1; : : :; yj ; z1; : : :; zk) = f1(x1; : : :; xi; y1; : : :; yj )f2(y1; : : :; yj ; z1; : : :; zk )

(

303

fiZ HANG & P OOLE

c

b

a

e

e

2

1

e3

Figure 1: A Bayesian network.
Let f (x1 ; : : :; xi ) be a function of variable x1; : : :; xi . Setting, say x1 in f (x1 ; : : :; xi) to a particular
value ff yields f (x1 =ff; x2; : : :; xi), which is a function of variables x2 ; : : :; xi.
If f (x1; : : :; xi) is a factor, we can sum out a variable, say x1 , resulting in a factor of variables
x2 ; : : :; xi, defined

X

(

x1

f )(x2; : : :; xi) = f (x1 =ff1 ; x2; : : :; xi) +    + f (x1=ffm; x2; : : :; xi)

where ff1 ; : : :; ffm are the possible values of variable x1.
Because of equation (1), a BN can be viewed as representing a factorization of a joint probability.
For example, the Bayesian network in Figure 1 factorizes the joint probability P (a; b; c; e1; e2; e3)
into the following list of factors:

P (a); P (b); P (c); P (e1ja; b; c); P (e2ja; b; c); P (e3je1; e2 ):
Multiplying those factors yields the joint probability.
Suppose a joint probability P (z1 ; z2; : : :; zm ) is factorized into the multiplication of a list of factors f1 , f2 , ..., fm . While obtaining P (z2 ; : : :; zm ) by summing out z1 from P (z1 ; z2; : : :; zm ) requires an exponential number of additions, obtaining a factorization of P (z2 ; : : :; zm ) can often be
done with much less computation. Consider the following procedure:
Procedure sum-out(F ; z ):




Inputs: F  a list of factors; z  a variable.
Output: A list of factors.

1. Remove from the F all the factors, say f1 , ..., fk , that contain z ,
2. Add the new factor

P Qk f to F and return F .
z i=1 i

Theorem 1 Suppose a joint probability P (z1 ; z2; : : :; zm) is factorized into the multiplication of a
list F of factors. Then sum-out(F ; z1 ) returns a list of factors whose multiplicationis P (z2 ; : : :; zm ).
304

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

Proof: Suppose F consists of factors f1 , f2 , ..., fm and suppose z1 appears in and only in factors
f1, f2, ..., fk . Then

P (z2 ; : : :; zm)

=

=

X
z1

P (z1 ; z2; : : :; zm )

m
XY
z1 i=1

fi = [

k
XY
z1 i=1

m
Y

fi ][

i=k+1

fi ]:

The theorem follows. 2
Only variables that appear in the factors f1 , f2 , ..., fk participated in the computation of sum-out(F ; z1 ),
and those are often only a small portion of all the variables. This is why inference in a BN can be
tractable in many cases, even if the general problem is NP-hard (Cooper, 1990).

3. The Variable Elimination Algorithm
Based on the discussions of the previous section, we present a simple algorithm for computing P (X jY =Y0 ).
The algorithm is based on the intuitions underlying DAmbrosios symbolic probabilistic inference
(SPI) (Shachter et al., 1990; Li & DAmbrosio, 1994), and first appeared in Zhang and Poole (1994).
It is essentially Dechter (1996)s bucket elimination algorithm for belief assessment.
The algorithm is called variable elimination (VE ) because it sums out variables from a list of
factors one by one. An ordering  by which variables outside X [Y to be summed out is required as
an input. It is called an elimination ordering.
Procedure VE (F ; X; Y; Y0; )





Inputs: F  The list of conditional probabilities in a BN;
X  A list of query variables;
Y  A list of observed variables;
Y0  The corresponding list of observed values;
  An elimination ordering for variables outside X [Y .
Output: P (X jY =Y0 ).

1. Set the observed variables in all factors to their corresponding observed values.
2. While  is not empty,

(a) Remove the first variable z from ,
(b) Call sum-out(F ; z ). Endwhile

3. Set h = the multiplication of all the factors on F .
/* h is a function of variables in X . */

4. Return h(X )=

P h(X ). /* Renormalization */
X

Theorem 2 The output of VE(F ; X; Y; Y0; ) is indeed P (X jY =Y0 ).
Proof: Consider the following modifications to the procedure. First remove step 1. Then the factor
h produced at step 3 will be a function of variables in X and Y . Add a new step after step 3 that sets
the observed variables in h to their observed values.
305

fiZ HANG & P OOLE

Let f (y; A) be a function of variable y and of variables in A. We use f (y; A)jy=ff to denote
f (y=ff; A). Let f (y; ,), g (y; ,), and h(y; z; ,) be three functions of y and other variables. It is
evident that

f (y; ,)g (y; ,)jy=ff = f (y; ,)jy=ff g(y; ,)jy=ff;
X
X
[
h(y; z; ,)]jy=ff = [h(y; z; ,)jy=ff ]:
z

z

Consequently, the modifications do not change the output of the procedure.
According to Theorem 1, after the modifications the factor produced at step 3 is simply the marginal
probability P (X; Y ). Consequently, the output is exactly P (X jY =Y0 ). 2
The complexity of VE can be measured by the number of numerical multiplications and numerical summations it performs. An optimal elimination ordering is one that results in the least complexity. The problem of finding an optimal elimination ordering is NP-complete (Arnborg et al., 1987).
Commonly used heuristics include minimum deficiency search (Bertele & Brioschi, 1972) and maximum cardinality search (Tarjan & Yannakakis, 1984). Kjrulff (1990) has empirically shown that
minimum deficiency search is the best existing heuristic. We use minimum deficiency search in our
experiments because we also found it to be better than the maximum cardinality search.
3.1

VE

versus Clique Tree Propagation

Clique tree propagation (Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990; Shafer & Shenoy,
1990) has a compilation step that transforms a BN into a secondary structure called clique tree or
junction tree. The secondary structure allows CTP to compute the answers to all queries with one
query variable and a fixed set of observations in twice the time needed to answer one such query in
the clique tree. For many applications this is a desirable property since a user might want to compare
the posterior probabilities of different variables.
CTP takes work to build the secondary structure before any observations have been received.
When the Bayesian network is reused, the cost of building the secondary structure can be amortized
over many cases. Each observation entails a propagation though the network.
Given all of the observations, VE processes one query at a time. If a user wants the posterior
probabilities of several variables, or for a sequence of observations, she needs to run VE for each of
the variables and observation sets.
The cost, in terms of the number of summations and multiplications, of answering a single query
with no observations using VE is of the same order of magnitude as using CTP. A particular clique
tree and propagation sequence encodes an elimination ordering; using VE on that elimination ordering results in approximately the same summations and multiplications of factors as in the CTP (there
is some discrepancy, as VE does not actually form the marginals on the cliques, but works with conditional probabilities directly). Observations make VE simpler (the observed variables are eliminated
at the start of the algorithm), but each observation in CTP requires propagation of evidence. Because
VE is query oriented, we can prune nodes that are irrelevant to specific queries (Geiger et al., 1990;
Lauritzen et al., 1990; Baker & Boult, 1990). In CTP, on the other hand, the clique tree structure is
kept static at run time, and hence does not allow pruning of irrelevant nodes.
CTP encodes a particular space-time tradeoff, and VE another. CTP is particularly suited to the
case where observations arrive incrementally, where we want the posterior probability of each node,
306

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

and where the cost of building the clique tree can be amortized over many cases. VE is suited for
one-off queries, where there is a single query variable and all of the observations are given at once.
Unfortunately, there are large real-world networks that CTP cannot deal with due to time and
space complexities (see Section 10 for two examples). In such networks, VE can still answer some
of the possible queries because it permits pruning of irrelevant variables.

4. Causal Independence
Bayesian networks place no restriction on how a node depends on its parents. Unfortunately this
means that in the most general case we need to specify an exponential (in the number of parents)
number of conditional probabilities for each node. There are many cases where there is structure in
the probability tables that can be exploited for both acquisition and for inference. One such case that
we investigate in this paper is known as causal independence.
In one interpretation, arcs in a BN represent causal relationships; the parents c1; c2; : : :; cm of a
variable e are viewed as causes that jointly bear on the effect e. Causal independence refers to the
situation where the causes c1 ; c2; : : :; cm contribute independently to the effect e.
More precisely, c1; c2; : : :; cm are said to be causally independent w.r.t. effect e if there exist
random variables 1; 2; : : :; m that have the same frame, i.e., the same set of possible values, as e
such that
1. For each i, i probabilistically depends on ci and is conditionally independent of all other cj s
and all other j s given ci , and
2. There exists a commutative and associative binary operator
e = 1 2 : : : m .
Using the independence notion of Pearl (1988), let
given Z , the first condition is:

 over the frame of e such that

I (X; Y jZ ) mean that X is independent of Y

I (1; fc2; : : :; cm; 2; : : :; mgjc1)
and similarly for the other variables. This entails I (1; cj jc1) and I (1; j jc1) for each cj and j
where j 6= 1.
We refer to i as the contribution of ci to e. In less technical terms, causes are causally independent w.r.t. their common effect if individual contributions from different causes are independent and
the total influence on the effect is a combination of the individual contributions.
We call the variable e a convergent variable as it is where independent contributions from different sources are collected and combined (and for the lack of a better name). Non-convergent variables
will simply be called regular variables. We call  the base combination operator of e.
The definition of causal independence given here is slightly different than that given by Heckerman and Breese (1994) and Srinivas (1993). However, it still covers common causal independence
models such as noisy OR-gates (Good, 1961; Pearl, 1988), noisy MAX-gates (Dez, 1993), noisy
AND-gates, and noisy adders (Dagum & Galper, 1993) as special cases. One can see this in the following examples.
Example 1 (Lottery) Buying lotteries affects your wealth. The amounts of money you spend on
buying different kinds of lotteries affect your wealth independently. In other words, they are causally
307

fiZ HANG & P OOLE

independent w.r.t. the change in your wealth. Let c1; : : :; ck denote the amounts of money you spend
on buying k types of lottery tickets. Let 1; : : :; k be the changes in your wealth due to buying the
different types of lottery tickets respectively. Then, each i depends probabilistically on ci and is
conditionally independent of the other cj and j given ci . Let e be the total change in your wealth
due to lottery buying. Then e=1 +    +k . Hence c1 ; : : :; ck are causally independent w.r.t. e. The
base combination operator of e is numerical addition. This example is an instance of a causal independence model called noisy adders.
If c1 ; : : :; ck are the amounts of money you spend on buying lottery tickets in the same lottery,
then c1 ; : : :; ck are not causally independent w.r.t. e, because winning with one ticket reduces the
chance of winning with the other. Thus, 1 is not conditionally independent of 2 given c1. However,
if the ci represent the expected change in wealth in buying tickets in the same lottery, then they would
be causally independent, but not probabilistically independent (there would be arcs between the ci s).
Example 2 (Alarm) Consider the following scenario. There are m different motion sensors each
of which are connected to a burglary alarm. If one sensor activates, then the alarm rings. Different
sensors could have different reliability. We can treat the activation of sensor i as a random variable.
The reliability of the sensor can be reflected in the i . We assume that the sensors fail independently1.
Assume that the alarm can only be caused by a sensor activation2. Then alarm=1 _   _m ; the
base combination operator here is the logical OR operator. This example is an instance of a causal
independence model called the noisy OR-gate.
The following example is not an instance of any causal independence models that we know:
Example 3 (Contract renewal) Faculty members at a university are evaluated in teaching, research,
and service for the purpose of contract renewal. A faculty members contract is not renewed, renewed without pay raise, renewed with a pay raise, or renewed with double pay raise depending on
whether his performance is evaluated unacceptable in at least one of the three areas, acceptable in
all areas, excellent in one area, or excellent in at least two areas.
Let c1 , c2, and c3 be the fractions of time a faculty member spends on teaching, research, and
service respectively. Let i represent the evaluation he gets in the ith area. It can take values 0, 1,
and 2 depending on whether the evaluation is unacceptable, acceptable, or excellent. The variable
i depends probabilistically on ci. It is reasonable to assume that i is conditionally independent of
other cj s and other j s given ci .
Let e represent the contract renewal result. The variable can take values 0, 1, 2, and 3 depending
on whether the contract is not renewed, renewed with no pay raise, renewed with a pay raise, or
renewed with double pay raise. Then e=1 23, where the base combination operator  is given
in this following table:

0
1
2
3

0
0
0
0
0

1
0
1
2
3

2
0
2
3
3

3
0
3
3
3

1. This is called the exception independence assumption by Pearl (1988).
2. This is called the accountability assumption by Pearl (1988). The assumption can always be satisfied by introducing
a node that represent all other causes (Henrion, 1987).

308

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

So, the fractions of time a faculty member spends in the three areas are causally independent
w.r.t. the contract renewal result.
In the traditional formulation of a Bayesian network we need to specify an exponential, in the
number of parents, number of conditional probabilities for a variable. With causal independence,
the number of conditional probabilities P (i jci ) is linear in m. This is why causal independence
can reduce complexity of knowledge acquisition (Henrion, 1987; Pearl, 1988; Olesen et al., 1989;
Olesen & Andreassen, 1993). In the following sections we show how causal independence can also
be exploited for computational gain.
4.1 Conditional Probabilities of Convergent Variables
VE allows us to exploit structure in a Bayesian network by providing a factorization of the joint probability distribution. In this section we show how causal independence can be used to factorize the
joint distributioneven further. The initial factors in the VE algorithm are of the form P (ejc1; : : :; cm).
We want to break this down into simpler factors so that we do not need a table exponential in m. The
following proposition shows how causal independence can be used to do this:

Proposition 1 Let e be a node in a BN and let c1 ; c2; : : :; cm be the parents of e. If c1; c2; : : :; cm
are causally independent w.r.t. e, then the conditional probability P (ejc1; : : :; cm) can be obtained
from the conditional probabilities P (i jci) through

P (e=ffjc1; : : :; cm) =

X

ff1 :::ffk =ff

P (1 =ff1 jc1): : :P (m =ffmjcm );

(2)

for each value ff of e. Here  is the base combination operator of e.
Proof:3 The definition of causal independence entails the independence assertions

I (1; fc2; : : :; cmgjc1) and I (1; 2jc1):
By the axiom of weak union (Pearl, 1988, p. 84), we have I (1; 2jfc1; : : :; cmg). Thus all of the i
mutually independent given fc1; : : :; cmg.
Also we have, by the definition of causal independence I (1; fc2; : : :; cm gjc1), so
P (1jfc1; c2; : : :; cmg) = P (1jc1)
Thus we have:

P (e=ffjc1; : : :; cm)
= P (1   m =ffjc1; : : :; cm)
X
=
P (1=ff1 ; : : :; m=ffm jc1; : : :; cm)
ff1 :::ff =ff
X
=
P (1=ff1 jc1; : : :; cm )P (2=ff2 jc1; : : :; cm)    P (m=ffm jc1; : : :; cm)
ff1 :::ff =ff
X
=
P (1=ff1 jc1)P (2=ff2jc2)    P (m =ffm jcm)
ff1 :::ff =ff
2
k

k

k

The next four sections develop an algorithm for exploiting causal independence in inference.
3. Thanks to an anonymous reviewer for helping us to simplify this proof.

309

fiZ HANG & P OOLE

5. Causal Independence and Heterogeneous Factorizations
In this section, we shall first introduce an operator for combining factors that contain convergent
variables. The operator is a basic ingredient of the algorithm to be developed in the next three sections. Using the operator, we shall rewrite equation (2) in a form that is more convenient to use in
inference and introduce the concept of heterogeneous factorization.
Consider two factors f and g . Let e1 , ..., ek be the convergent variables that appear in both f and
g , let A be the list of regular variables that appear in both f and g , let B be the list of variables that
appear only in f , and let C be the list of variables that appear only in g . Both B and C can contain
convergent variables, as well as regular variables. Suppose i is the base combination operator of
ei. Then, the combination f 
g of f and g is a function of variables e1, ..., ek and of the variables
in A, B , and C . It is defined by:4

f 
g (e1=X
ff1 ; : : :; ek =ffkX
; A; B; C )
=
:::
f (e1=ff11; : : :; ek =ffk1 ; A; B)
ff11 1 ff12 =ff1

ffk1 k ffk2 =ffk

g (e1=ff12; : : :; ek=ffk2; A; C );

(3)

for each value ffi of ei . We shall sometimes write f 
g as f (e1 ; : : :; ek ; A; B )
g (e1; : : :; ek ; A; C )
to make explicit the arguments of f and g .
Note that base combination operators of different convergent variables can be different.
The following proposition exhibits some of the basic properties of the combination operator 
.
Proposition 2 1. If f and g do not share any convergent variables, then f 
g is simply the multiplication of f and g . 2. The operator 
 is commutative and associative.
Proof: The first item is obvious. The commutativity of 
 follows readily from the commutativity of
multiplication and the base combination operators. We shall prove the associativity of 
 in a special
case. The general case can be proved by following the same line of reasoning.
Suppose f , g , and h are three factors that contain only one variable e and the variable is convergent. We need to show that (f 
g )
h=f 
(g 
h). Let  be the base combination operator of e. By
the associativity of , we have, for any value ff of e, that

f 
g )
h(e=ff)

(

=
=
=
=

X

f 
g (e=ff4)h(e=ff3)
X
X
[
f (e=ff1 )g(e=ff2)]h(e=ff3 )
ff4 ff3 =ff ff1 ff2 =ff4
X
f (e=ff1)g (e=ff2 )h(e=ff3 )
ff1 ff2 ff3 =ff
X
X
f (e=ff1)[
g (e=ff2 )h(e=ff3)]
ff4 ff3 =ff

ff1 ff4 =ff

ff2ff3 =ff4

4. Note that the base combination operators under the summations are indexed. With each convergent variable is an associated operator, and we always use the binary operator that is associated with the corresponding convergent variable.
In the examples, for ease of exposition, we will use one base combination operator. Where there is more than one type
of base combination operator (e.g., we may use or, sum and max for different variables in the same network), we
have to keep track of which operators are associated with which convergent variables. This will, however, complicate
the description.

310

fiE XPLOITING C AUSAL I NDEPENDENCE

X

=

ff1 ff4 =ff

IN

B AYESIAN N ETWORK I NFERENCE

f (e=ff1)g 
h(e=ff4)

f 
(g 
h)(e=ff):

=

The proposition is hence proved.2
The following propositions give some properties for 
 that correspond to the operations that we
exploited for the algorithm VE . The proofs are straight forward and are omitted.
Proposition 3 Suppose f and g are factors and variable z appears in f and not in g , then

X

Xz
z

X

fg)

=

(

f 
g )

=

(

(

(

z
X
z

f )g; and
f )
g:

Proposition 4 Suppose f , g and h are factors such that g and h do not share any convergent variables, then

g (f 
h) = (gf )
h:

(4)

5.1 Rewriting Equation 2
Noticing that the contribution variable i has the same possible values as e, we define functions

fi(e; ci) by

fi(e=ff; ci ) = P (i =ffjci);
for any value ff of e. We shall refer to fi as the contributing factor of ci to e.
By using the operator 
, we can now rewrite equation (2) as follows
P (ejc1; : : :; cm) = 
mi=1 fi (e; ci):

(5)

It is interesting to notice the similarity between equation (1) and equation (5). In equation (1)
conditional independence allows one to factorize a joint probability into factors that involve less
variables, while in equation (5) causal independence allows one to factorize a conditional probability
into factors that involve less variables. However, the ways by which the factors are combined are
different in the two equations.
5.2 Heterogeneous Factorizations
Consider the Bayesian network in Figure 1. It factorizes the joint probability P (a; b; c; e1; e2; e3)
into the following list of factors:

P (a); P (b); P (c); P (e1ja; b; c); P (e2ja; b; c); P (e3je1; e2 ):
We say that this factorization is homogeneous because all the factors are combined in the same way,
i.e., by multiplication.
Now suppose the ei s are convergent variables. Then their conditional probabilities can be further factorized as follows:

P (e1ja; b; c)
P (e2ja; b; c)
P (e3 je1 ; e2)

=
=
=

f11 (e1; a)
f12 (e1 ; b)
f13 (e1; c);
f21 (e2; a)
f22 (e2 ; b)
f23 (e2; c);
f31 (e3; e1)
f32 (e3; e2 );
311

fiZ HANG & P OOLE

where the factor f11(e1 ; a), for instance, is the contributing factor of a to e1 .
We say that the following list of factors

f11(e1 ; a); f12(e1; b); f13(e1; c); f21(e2 ; a); f22(e2 ; b); f23(e2 ; c); f31(e3 ; e1); f32(e3 ; e2);
P (a); P (b); and P (c)
(6)
constitute a heterogeneous factorization of P (a; b; c; e1; e2; e3) because the joint probability can be
obtained by combining those factors in a proper order using either multiplication or the operator 
.
The word heterogeneous is to signify the fact that different factor pairs might be combined in different ways. We call each fij a heterogeneous factor because it needs to be combined with the other
fik s by the operator 
 before it can be combined with other factors by multiplication. In contrast,
we call the factors P (a), P (b), and P (c) homogeneous factors.
We shall refer to that heterogeneous factorization as the heterogeneous factorization represented
by the BN in Figure 1. It is obvious that this heterogeneous factorization is of finer grain than the
homogeneous factorization represented by the BN.

6. Flexible Heterogeneous Factorizations and Deputation
This paper extends VE to exploit this finer-grain factorization. We will compute the answer to a query
by summing out variables one by one from the factorization just as we did in VE .
The correctness of VE is guaranteed by the fact that factors in a homogeneous factorization can
be combined (by multiplication) in any order and by the distributivity of multiplication over summations (see the proof of Theorem 1).
According to Proposition 3, the operator 
 is distributive over summations. However, factors in
a heterogeneous factorization cannot be combined in arbitrary order. For example, consider the heterogeneous factorization (6). While it is correct to combine f11(e1 ; a) and f12 (e1; b) using 
, and to
combine f31 (e3; e1 ) and f32 (e3; e2 ) using 
, it is not correct to combine f11 (e1; a) and f31 (e3; e1)
with 
. We want to combine these latter two by multiplication, but only after each has been combined with its sibling heterogeneous factors.
To overcome this difficulty, a transformation called deputation will be performed on our BN.
The transformation does not change the answers to queries. And the heterogeneous factorization
represented by the transformed BN is flexible in the following sense:
A heterogeneous factorization of a joint probability is flexible if:
The joint probability
=

multiplication of all homogeneous factors

 combination (by 
) of all heterogeneous factors:

(7)

This property allows us to carry out multiplication of homogeneous factors in arbitrary order,
and since 
 is associative and commutative, combination of heterogeneous factors in arbitrary order. If the conditions of Proposition 4 are satisfied, we can also exchange a multiplication with a
combination by 
. To guarantee the conditions of Proposition 4, the elimination ordering needs to
be constrained (Sections 7 and 8).
The heterogeneous factorization of P (a; b; c; e1; e2; e3) given at the end of the previous section is
not flexible. Consider combining all the heterogeneous factors. Since the operator 
 is commutative
312

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

c

b

a
e1

e2

e1

e2
e3
e3

Figure 2: The BN in Figure 1 after the deputation of convergent variables.
and associative, one can first combine, for each i, all the fik s, obtaining the conditional probability
of ei , and then combine the resulting conditional probabilities. The combination

P (e1 ja; b; c)
P (e2 ja; b; c)
P (e3je1 ; e2)
is not the same as the multiplication

P (e1 ja; b; c)P (e2ja; b; c)P (e3je1; e2)
because the convergent variables e1 and e2 appear in more than one factor. Consequently, equation
(7) does not hold and the factorization is not flexible. This problem arises when a convergent variable is shared between two factors that are not siblings. For example, we do not want to combine
f11 (e1; a) and f31 (e3 ; e1) using 
. In order to tackle this problem we introduce a new deputation
variable so that each heterogeneous factor contains a single convergent variable.
Deputation is a transformation that one can apply to a BN to make the heterogeneous factorization represented by the BN flexible. Let e be a convergent variable. To depute e is to make a copy
e0 of e, make the parents of e be parents of e0 , replace e with e0 in the contributing factors of e, make
e0 the only parent of e, and set the conditional probability P (eje0 ) as follows:

P (eje0 ) =

(

1
0

if e = e0
otherwise

(8)

We shall call e0 the deputy of e. The deputy variable e0 is a convergent variable by definition. The
variable e, which is convergent before deputation, becomes a regular variable after deputation. We
shall refer to it as a new regular variable. In contrast, we shall refer to the variables that are regular
before deputation as old regular variables. The conditional probability P (e0 je) is a homogeneous
factor by definition. It will sometimes be called the deputing function and written as I (e0; e) since it
ensures that e0 and e always take the same value.
A deputation BN is obtained from a BN by deputing all the convergent variables. In a deputation
BN, deputy variables are convergent variables and only deputy variables are convergent variables.
313

fiZ HANG & P OOLE

Figure 2 shows the deputation of the BN in Figure 1. It factorizes the joint probability

P (a; b; c; e1; e01; e2; e02; e3; e03)
into homogeneous factors

P (a); P (b); P (c); I1(e01; e1); I2(e02; e2); I3(e03; e3);
and heterogeneous factors

f11(e01 ; a); f12(e01; b); f13(e01 ; c); f21(e02 ; a); f22(e02; b); f23(e02; c); f31(e03; e1); f32(e03; e2):
This factorization has three important properties.
1. Each heterogeneous factor contains one and only one convergent variable. (Recall that the ei s
are no longer convergent variables and their deputies are.)
2. Each convergent variable e0 appears in one and only one homogeneous factor, namely the
deputing function I (e0; e).
3. Except for the deputing functions, none of the homogeneous factors contain any convergent
variables.
Those properties are shared by the factorization represented by any deputation BN.
Proposition 5 The heterogeneous factorization represented by a deputation BN is flexible.
Proof: Consider the combination, by 
, of all the heterogeneous factors in the deputation BN. Since
the combination operator 
 is commutative and associative, we can carry out the combination in following two steps. First for each convergent (deputy) variable e0 , combine all the heterogeneous factors that contain e0 , yielding the conditional probability P (e0 je ) of e0 . Then combine those resulting
conditional probabilities. It follows from the first property mentioned above that for different convergent variables e01 and e02, P (e01 je1 ) and P (e02 je2 ) do not share convergent variables. Hence the
combination of the P (e0 je )s is just the multiplication of them. Consequently, the combination,
by 
, of all heterogeneous factors in a deputation BN is just the multiplication of the conditional
probabilities of all convergent variables. Therefore, we have
0

0

0

0

The joint probability of variables in a deputation BN
= multiplication of conditional probabilities of all variables
=

multiplication of conditional probabilities of all regular variables

=

multiplication of all homogeneous factors

multiplication of conditional probabilities of all convergent variables

combination (by 
) of all heterogeneous factors:
The proposition is hence proved. 2
Deputation does not change the answer to a query. More precisely, we have
Proposition 6 The posterior probability P (X jY =Y0 ) is the same in a BN as in its deputation.
314

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

Proof: Let R, E , and E 0 be the lists of old regular, new regular, and deputy variables in the deputation BN respectively. It suffices to show that P (R; E ) is the same in the original BN as in the
deputation BN. For any new regular variable e, let e0 be its deputy. It is easy to see that the quantity
P I (e0; e)P (e0j ) in the deputation BN is the same as P (ej ) in the original BN. Hence,
e
e
e
0

0

P (R; EX) in the deputation BN
=
P (R; E; E 0)
E Y
X
Y
=
P (rjr ) [P (eje)P (e0 je )]
E r2R
2E
Y
Y eX
=
P (rjr ) [ I (e0; e)P (e0je )]
r2R
e2E e
Y
Y
=
P (rjr ) P (eje )
0

0

0

0

0

r2R

=

The proposition is proved.

2

e2 E

P (R; E ) in the original BN:

7. Tidy Heterogeneous Factorizations
So far, we have only encountered heterogeneous factorizations that correspond to Bayesian networks.
In the following algorithm, the intermediate heterogeneous factorizations do not necessarily correspond by BNs. They do have the property that they combine to form the appropriate marginal probabilities. The general intuition is that the heterogeneous factors must combine with their sibling heterogeneous factors before being multiplied by factors containing the original convergent variable.
In the previous section, we mentioned three properties of the heterogeneous factorization represented by a deputation BN, and we used the first property to show that the factorization is flexible.
The other two properties qualify the factorization as a tidy heterogeneous factorization, which is defined below.
Let z1 , z2 , ..., zk be a list of variables in a deputation BN such that if a convergent (deputy)
variable e0 is in fz1; z2 ; : : :; zk g, so is the corresponding new regular variable e. A flexible heterogeneous factorization of P (z1 ; z2; : : :; zk ) is said to be tidy If

1. For each convergent (deputy) variable e02fz1; z2; : : :; zk g, the factorization contains the deputing function I (e0; e) and it is the only homogeneous factor that involves e0 .

2. Except for the deputing functions, none of the homogeneous factors contain any convergent
variables.
As stated earlier, the heterogeneous factorization represented by a deputation BN is tidy.
Under certain conditions, to be given in Theorem 3, one can obtain a tidy factorization of P (z2 ; : : :; zk )
by summing out z1 from a tidy factorization of P (z1 ; z2; : : :; zk ) using the the following procedure.
Procedure sum-out1(F1 ; F2; z )



Inputs: F1  A list of homogeneous factors,
F2  A list of heterogeneous factors,
z  A variable.
315

fiZ HANG & P OOLE



Output: A list of heterogeneous factors and a list of homogeneous factors.

1. Remove from F1 all the factors that contain z , multiply them resulting in, say, f .
If there are no such factors, set f =nil.

2. Remove from F2 all the factors that contain z , combine them by using 
 resulting
in, say, g . If there are no such factors, set g =nil.

P f to F .
1
z
P
Else add the new (heterogeneous) factor z fg to F2.
Return (F1; F2).

3. If g =nil, add the new (homogeneous) factor
4.
5.

Theorem 3 Suppose a list of homogeneous factors F1 and a list of heterogeneous factors F2 constitute a tidy factorization of P (z1 ; z2; : : :; zk ). If z1 is either a convergent variable, or an old regular
variable, or a new regular variable whose deputy is not in the list fz2; : : :; zk g, then the procedure
sum-out1(F1 ; F2; z1) returns a tidy heterogeneous factorization of P (z2 ; : : :; zk ).
The proof of this theorem is quite long and hence is given in the appendix.

8. Causal Independence and Inference
Our task is to compute P (X jY =Y0 ) in a BN. According to Proposition 6, we can do this in the
deputation of the BN.
An elimination ordering consisting of the variables outside X [Y is legitimate if each deputy
variable e0 appears before the corresponding new regular variable e. Such an ordering can be found
using, with minor adaptations, minimum deficiency search or maximum cardinality search.
The following algorithm computes P (X jY =Y0 ) in the deputation BN. It is called VE 1 because
it is an extension of VE .
Procedure VE 1 (F1; F2; X; Y; Y0; )





Inputs: F1  The list of homogeneous factors
in the deputation BN;
F2  The list of heterogeneous factors
in the deputation BN;
X  A list of query variables;
Y  A list of observed variables;
Y0  The corresponding list of observed values;
  A legitimate elimination ordering.
Output: P (X jY =Y0 ).

1. Set the observed variables in all factors to their observed values.
2. While  is not empty,

 Remove the first variable z from .
 (F1; F2) = sum-out1(F1; F2; z). Endwhile
316

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

3. Set h=multiplication of all factors in F1
 combination (by 
) of all factors in F2.
/* h is a function of variables in X . */
4. Return h(X )=

P h(X ). /* renormalization */
X

Theorem 4 The output of VE 1 (F1; F2; X; Y; Y0; ) is indeed P (X jY =Y0 ).
Proof: Consider the following modifications to the algorithm. First remove step 1. Then the factor
h produced at step 3 is a function of variables in X and Y . Add a new step after step 3 that sets
the observed variables in h to their observed values. We shall first show that the modifications do
not change the output of the algorithm and then show that the output of the modified algorithm is
P (X jY =Y0 ).
Let f (y; ,), g (y; ,), and h(y; z; ,) be three functions of y and other variables. It is evident that

f (y; ,)g (y; ,)jy=ff = f (y; ,)jy=ff g(y; ,)jy=ff;
X
X
[
h(y; z; ,)]jy=ff = [h(y; z; ,)jy=ff ]:
z

z

If y is a regular variable, we also have

f (y; ,)
g (y; ,)jy=ff = f (y; ,)jy=ff 
g (y; ,)jy=ff :
Consequently, the modifications do not change the output of the procedure.
Since the elimination ordering  is legitimate, it is always the case that if a deputy variable e0 has
not been summed out, neither has the corresponding new regular variable e. Let z1 , ..., zk be the remaining variables in  at any time during the execution of the algorithm. Then, e0 2fz1 ; : : :; zk g implies e2fz1 ; : : :; zk g. This and the fact that the factorization represented by a deputation BN is tidy
enable us to repeatedly apply Theorem 3 and conclude that, after the modifications, the factor created
at step 3 is simply the marginal probability P (X; Y ). Consequently, the output is P (X jY =Y0 ). 2
8.1 An Example
This subsection illustrates VE 1 by walking through an example. Consider computing the P (e2 je3=0)
in the deputation Bayesian network shown in Figure 2. Suppose the elimination ordering  is: a, b,
c, e01, e02, e1, and e03 . After the first step of VE1 ,

F1 = fP (a); P (b); P (c); I1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff11(e01; a); f12(e01; b); f13(e01; c); f21(e02; a); f22(e02; b); f23(e02; c); f31(e03; e1); f32(e03; e2)g:
Now the procedure enters the while-loop and it sums out the variables in  one by one.
After summing out a,
F1 = fP (b); P (c); I1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff12(e01; b); f13P(e01; c); f22(e02; b); f23(e02; c); f31(e03; e1); f32(e03; e2); 1(e01; e02)g;
where 1 (e01; e02) = a P (a)f11(e01 ; a)f21(e02 ; a).
After summing out b,
F1 = fP (c); I1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff13(e01; c); f23P(e02; c); f31(e03; e1); f32(e03; e2); 1(e01; e02); 2(e01; e02)g;
where 2 (e01; e02) = b P (b)f12(e01 ; b)f22(e02; b).
317

fiZ HANG & P OOLE

After summing out c,
F1 = fI1(e01; e1); I2(e02; e2); I3(e03; e3=0)g;
F2 = ff31(e03; e1); fP
32(e03 ; e2); 1(e01 ; e02); 2(e01 ; e02); 3(e01 ; e02)g;
0
0
where 3 (e1; e2) = c P (c)f23 (e02; c)f13(e01 ; c).
After summing out e01 ,
F1 = fI2(e02; e2); I3(e03; e3=0)g;
F2 = ff31(e03; e1); fP
32(e03 ; e2); 4(e1 ; e02)g;
where 4 (e1; e02) = e I1(e01 ; e1)[ 1(e01; e02)
 2 (e01; e02)
 3 (e01; e03)].
1
After summing out e02 ,
F1 = fI3(e03; e3=0)g;
F2 = ff31(e03; e1); fP
32(e03 ; e2); 5(e1 ; e2)g;
where 5 (e1; e2) = e I2(e02 ; e2) 4(e1 ; e02).
2
After summing out e1 ,
F1 = fI3(e03; e3=0)g;
F2 = ff32(e03; e2); P6(e03; e2)g;
where 6 (e03; e2) = e1 f31(e03 ; e1) 5(e1 ; e2).
Finally, after summing out e03 ,
F 1 = ;;
F2 = f 7(e2)g; P
where 7 (e2) = e I3(e03 ; e3=0)[f32(e03; e2 )
 6(e03; e2 )]. Now the procedure enters step 3, where
3
P
there is nothing to do in this example. Finally, the procedure returns 7(e2 )= e2 7(e2 ), which is
P (e2je3 =0), the required probability.
0

0

0

8.2 Comparing VE and VE 1
In comparing VE and VE 1 , we notice that when summing out a variable, they both combine only
those factors that contain the variable. However, the factorization that the latter works with is of
finer grain than the factorization used by the former. In our running example, the latter works with a
factorization which initially consists of factors that contain only two variables; while the factorization the former uses initially include factors that contain five variables. On the other hand, the latter
uses the operator 
 which is more expensive than multiplication. Consider, for instance, calculating
f (e; a)
g(e; b). Suppose e is a convergent variable and all variables are binary. Then the operation requires 24 numerical multiplications and 24 , 23 numerical summations. On the other hand,
multiplying f (e; a) and g (e; b) only requires 23 numerical multiplications.

Despite the expensiveness of the operator 
, VE 1 is more efficient than VE . We shall provide
empirical evidence in support of this claim in Section 10. To see a simple example where this is true,
consider the BN in Figure 3(1), where e is a convergent variable. Suppose all variables are binary.
Then, computing P (e) by VE using the elimination ordering c1 , c2, c3, and c4 requires 25 + 24 +
23 + 22=60 numerical multiplications and (25 , 24) + (24 , 23 ) + (23 , 22 ) + (22 , 2)=30
numerical additions. On the other hand, computing P (e) in the deputation BN shown in Figure 3(2)
by VE 1 using the elimination ordering c1, c2, c3 , c4, and e0 requires only 22 + 22 + 22 + 22 +
(322 + 22)=32 numerical multiplications and 2 + 2 + 2 + 2 + (32 + 2)=16 numerical additions.
Note that summing out e0 requires 322 + 22 numerical multiplications because after summing out
ci s, there are four heterogeneous factors, each containing the only argument e0 . Combining them
318

fiE XPLOITING C AUSAL I NDEPENDENCE

c1

c3

c2

IN

B AYESIAN N ETWORK I NFERENCE

c1

c4

c3

c2

c4

e

e

e
(1)

c1

(2)

c3

c2
e1

c4
e2

c1

c2

c3

c4

e1

e2

e

e

3

e
(4)

(3)

Figure 3: A BN, its deputation and transformations.
pairwise requires 322 multiplications. The resultant factor needs to be multiplied with the deputing
factor I (e0; e), which requires 22 numerical multiplications.

9. Previous Methods
Two methods have been proposed previously for exploiting causal independence to speed up inference in general BNs (Olesen et al., 1989; Heckerman, 1993). They both use causal independence to
transform the topology of a BN. After the transformation, conventional algorithms such as CTP or
VE are used for inference.
We shall illustrate those methods by using the BN in Figure 3(1). Let  be the base combination
operator of e, let i denote the contribution of ci to e, and let fi (e; ci) be the contributing factor of ci
to e.
The parent-divorcing method (Olesen et al., 1989) transforms the BN into the one in Figure 3(3).
After the transformation, all variables are regular and the new variables e1 and e2 have the same
possible values as e. The conditional probabilities of e1 and e2 are given by

P (e1 jc1; c2)=f1(e; c1)
f2 (e; c2);
P (e2 jc3; c4)=f3(e; c3)
f4 (e; c4):

The conditional probability of e is given by

P (e=ffje1=ff1; e2=ff2 ) = 1 if ff=ff1 ff2,
for any value ff of e, ff1 of e1 , and ff2 of e2 . We shall use PD to refer to the algorithm that first
performs the parent-divorcing transformation and then uses VE for inference.
319

fiZ HANG & P OOLE

The temporal transformation by Heckerman (1993) converts the BN into the one in Figure 3(4).
Again all variables are regular after the transformation and the newly introduced variables have the
same possible values as e. The conditional probability of e1 is given by

P (e1=ffjc1) = f1(1=ff; c1);
for each value ff of e1 . For i=2; 3; 4, the conditional probability of ei (e4 stands for e) is given by

P (ei =ffjei,1 =ff1 ; ci) =

X

ff1 ff2 =ff

fi(e=ff2; ci);

for each possible value ff of ei and ff1 of ei,1 . We shall use TT to refer to the algorithm that first
performs the temporal transformation and then uses VE for inference.
The factorization represented by the original BN includes a factor that contain five variables,
while factors in the transformed BNs contain no more than three variables. In general, the transformations lead to finer-grain factorizations of joint probabilities. This is why PD and TT can be more
efficient than VE .
However, PD and TT are not as efficient as VE 1 . We shall provide empirical evidence in support
of this claim in the next section. Here we illustrate it by considering calculating P (e). Doing this in
Figure 3(3) by VE using the elimination ordering c1, c2, c3 , c4, e1 , and e2 would require 23 + 22 +
23 + 22 + 23 + 22 =36 numerical multiplications and 18 numerical additions.5 Doing the same in
Figure 3(4) using the elimination ordering c1 , e1 , c2, e2 , c3, e3 , c4 would require 22 + 23 + 22 +
23 + 22 + 23 + 22 =40 numerical multiplications and 20 numerical additions. In both cases, more
numerical multiplications and additions are performed than VE 1 . The differences are more drastic
in complex networks, as will be shown in the next section.
The saving for this example may seem marginal. It may be reasonable to conjecture that, as
Olesons method produces families with three elements, this marginal saving is all that we can hope
for; producing factors of two elements rather than cliques of three elements. However, interacting
causal variables can make the difference more extreme. For example, if we were to use Olesons
method for the BN of Figure 1, we produce6 the network of Figure 4. Any triangulation for this
network has at least one clique with four or more elements, yet VE 1 does not produce a factor with
more than two elements.
Note that as far as computing P (e) in the networks shown in Figure 3 is concerned, VE 1 is more
efficient than PD, PD is more efficient than TT, and TT is more efficient than VE . Our experiments
show that this is true in general.
5. This is exactly the same number of operations required to determine P (e) using clique-tree propagation on the same
network. The clique tree for Figure 3(3) has three cliques, one containing fc1 ; c2 ; e1 g, one containing fc3 ; c4 ; e2 g, and
once containing fe1 ; e2 ; eg. The first clique contains 8 elements; to construct it requires 22 + 23 = 12 multiplications.
The message that needs to be sent to the third clique is the marginal on e1 which is obtained by summing out c1 and
c2 . Similarly for the second clique. The third clique again has 8 elements and requires 12 multiplications to construct.
In order to extract P (e) from this clique, we need to sum out e1 and e2 . This shown one reason why VE 1 can be more
efficient that CTP or VE; VE 1 never constructs a factor with three variables for this example. Note however, that an
advantage of CTP is that the cost of building the cliques can be amortized over many queries.
6. Note that we need to produce two variables both of which represent noisy a  b. We need two variables as the noise
applied in each case is independent. Note that if there was no noise in the network  if e1 = a  b  c  we only
need to create one variable, but also e1 and e2 would be the same variable (or at least be perfectly correlated). In this
case we would need a more complicated example to show our point.

320

fiE XPLOITING C AUSAL I NDEPENDENCE

a

IN

B AYESIAN N ETWORK I NFERENCE

b

e

c

11

e21

e

e

2

1

e3

Figure 4: The result of Applying Olesons method to the BN of Figure 1.

10. Experiments
The CPCS networks are multi-level, multi-valued BNs for medicine. They were created by Pradhan
et al. (1994) based on the Computer-based Patient Case Simulation system (CPCS-PM) developed
by Parker and Miller (1987). Two CPCS networks7 were used in our experiments. One of them
consists of 422 nodes and 867 arcs, and the other contains 364 nodes. They are among the largest
BNs in use at the present time.
The CPCS networks contain abundant causal independencies. As a matter of fact, each non-root
variable is a convergent variable with base combination operator MAX. They are good test cases for
inference algorithms that exploit causal independencies.
10.1 CTP-based Approaches versus VE -based Approaches
As we have seen in the previous section, one kind of approach for exploiting causal independencies
is to use them to transform BNs. Thereafter, any inference algorithms, including CTP or VE , can be
used for inference.
We found the coupling of the network transformation techniques and CTP was not able to carry
out inference in the two CPCS networks used in our experiments. The computer ran out memory
when constructing clique trees for the transformed networks. As will be reported in the next subsection, however, the combination of the network transformation techniques and VE was able to answer
many queries.
This paper has proposed a new method of exploiting causal independencies. We have observed
that causal independencies lead to a factorization of a joint probability that is of finer-grain than
the factorization entailed by conditional independencies alone. One can extend any inference algorithms, including CTP and VE , to exploit this finer-grain factorization. This paper has extended
VE and obtained an algorithm called VE 1 . VE 1 was able to answer almost all queries in the two
CPCS networks. We conjecture, however, that an extension of CTP would not be able to carry out
inference with the two CPCS networks at all. Because the resources that VE 1 takes to answer any
query in a BN can be no more than those an extension of CTP would take to construct a clique tree
7. Obtained from ftp://camis.stanford.edu/pub/pradhan.
V1.0.txt and CPCS-networks/std1.08.5.

321

The file names are CPCS-LM-SM-K0-

fi50
45
40
35
30
25
20
15
10
5
0

Number of queries

Number of queries

Z HANG & P OOLE

"5ve1"
"5pd"
"5tt"
"5ve"

Number of queries

0 1 2 3 4 5 6 7 8 9
CPU time in seconds

50
45
40
35
30
25
20
15
10
5
0

"10ve1"
"10pd"
"10tt"
"10ve"

0 1 2 3 4 5 6 7 8 9
CPU time in seconds

50
45
40
35
30
25
20
15
10
5
0

"15ve1"
"15pd"
"15tt"

0 1 2 3 4 5 6 7 8 9 10
CPU time in seconds

Figure 5: Comparisons in the 364-node BN.

for the BN and there are, as will be seen in the next subsection, queries in the two CPCS networks
that VE 1 was not able to answer.
In summary, CTP based approaches are not or would not be able to deal with the two CPCS
networks, while VE -based approaches can (to different extents).
10.2 Comparisons of VE -based Approaches
This subsection provides experimental data to compare the VE -based approaches namely PD, TT,
and VE 1 . We also compare those approaches with VE itself to determine how much can be gained
by exploiting causal independencies.
In the 364-node network, three types of queries with one query variable and five, ten, or fifteen
observations respectively were considered. Fifty queries were randomly generated for each query
type. A query is passed to the algorithms after nodes that are irrelevant to it have been pruned. In general, more observations mean less irrelevant nodes and hence greater difficulty to answer the query.
The CPU times the algorithms spent in answering those queries were recorded.
In order to get statistics for all algorithms, CPU time consumption was limited to ten seconds
and memory consumption was limited to ten megabytes.
The statistics are shown in Figure 5. In the charts, the curve 5ve1, for instance, displays the
time statistics for VE 1 on queries with five observations. Points on the X-axis represent CPU times
322

fi50
45
40
35
30
25
20
15
10
5
0

IN

B AYESIAN N ETWORK I NFERENCE

Number of queries

Number of queries

E XPLOITING C AUSAL I NDEPENDENCE

"5ve1"
"5pd"
"5tt"
"5ve"

0 1 2 3 4 5 6 7 8 9
CPU time in seconds

40
35
30
25
20
15
10
5
0

"10ve1"
"10pd"
"10tt"

0 1 2 3 4 5 6 7 8 9 10
CPU time in seconds

Figure 6: Comparisons in the 422-node BN.
in seconds. For any time point, the corresponding point on the Y-axis represents the number of fiveobservation queries that were each answered within the time by VE 1 .
We see that while VE 1 was able to answer all the queries, PD and TT were not able to answer
some of the ten-observation and fifteen-observation queries. VE was not able to answer a majority
of the queries.
To get a feeling about the average performances of the algorithms, regard the curves as representing functions of y , instead of x. The integration, along the Y -axis, of the curve 10PD, for instance,
is roughly the total amount of time PD took to answer all the ten-observation queries that PD was
able to answer. Dividing this by the total number of queries answered, one gets the average time PD
took to answer a ten-observation query.
It is clear that on average, VE 1 performed significantly better than PD and TT, which in turn
performed much better than VE . The average performance of PD on five- or ten-observation queries
are roughly the same as that of TT, and slightly better on fifteen-observation queries.
In the 422-node network, two types of queries with five or ten observations were considered and
fifty queries were generated for each type. The same space and time limits were imposed as in the
364-node networks. Moreover, approximations had to be made; real numbers smaller than 0.00001
were regarded as zero. Since the approximations are the same for all algorithms, the comparisons
are fair.
The statistics are shown in Figure 6. The curves 5ve1 and 10ve1 are hardly visible because
they are very close to the Y -axis.
Again we see that on average, VE 1 performed significantly better than PD, PD performed significantly better than TT, and TT performed much better than VE .
One might notice that TT was able to answer thirty nine ten-observation queries, more than that
VE 1 and PD were able to. This is due to the limit on memory consumption. As we will see in the
next subsection, with the memory consumption limit increased to twenty megabytes, VE 1 was able
to answer forty five ten-observation queries exactly under ten seconds.
10.3 Effectiveness of VE 1
We have now established that VE 1 is the most efficient VE -based algorithm for exploiting causal
independencies. In this section we investigate how effective VE 1 is.
323

fiZ HANG & P OOLE

The 422-node BN
Number of queries

Number of queries

The 364-node BN
50
45
40
35
30
25
20
15
10
5
0

"5ve1"
"10ve1"
"15ve1"
"20ve1"

0 1 2 3 4 5 6 7 8 9 10
CPU time in seconds

50
45
40
35
30
25
20
15
10
5
0

"5ve1"
"10ve1"
"15ve1"

0

5 10 15 20 25 30 35 40
CPU time in seconds

Figure 7: Time statistics for VE 1 .
Experiments have been carried out in both of the two CPCS networks to answer this question. In
the 364-node network, four types of queries with one query variable and five, ten, fifteen, or twenty
observations respectively were considered. Fifty queries were randomly generated for each query
type. The statistics of the times VE 1 took to answer those queries are given by the left chart in Figure
7. When collecting the statistics, a ten MB memory limit and a ten second CPU time limit were
imposed to guide against excessive resource demands. We see that all fifty five-observation queries
in the network were each answered in less than half a second. Forty eight ten-observation queries,
forty five fifteen-observation queries, and forty twenty-observation queries were answered in one
second. There is, however, one twenty-observation query that VE 1 was not able to answer within
the time and memory limits.
In the 364-node network, three types of queries with one query variable and five, ten, or fifteen,
observations respectively were considered. Fifty queries were randomly generated for each query
type. Unlike in the previous section, no approximations were made. A twenty MB memory limit
and a forty-second CPU time limit were imposed. The time statistics is shown in the right hand side
chart. We see that VE 1 was able to answer all most all queries and a majority of the queries were
answered in little time. There are, however, three fifteen-observation queries that VE 1 was not able
to answer.

11. Conclusions
This paper has been concerned with how to exploit causal independence in exact BN inference. Previous approaches (Olesen et al., 1989; Heckerman, 1993) use causal independencies to transform
BNs. Efficiency is gained because inference is easier in the transformed BNs than in the original
BNs.
A new method has been proposed in this paper. Here is the basic idea. A Bayesian network
can be viewed as representing a factorization of a joint probability into the multiplication of a list of
conditional probabilities. We have studied a notion of causal independence that enables one to further
factorize the conditional probabilities into a combination of even smaller factors and consequently
obtain a finer-grain factorization of the joint probability.
We propose to extend inference algorithms to make use of this finer-grain factorization. This
paper has extended an algorithm called VE . Experiments have shown that the extended VE algo324

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

rithm, VE 1 , is significantly more efficient than if one first performs Olesen et al.s or Heckermans
transformation and then apply VE .
The choice of VE instead of the more widely known CTP algorithm is due to its ability to work in
networks that CTP cannot deal with. As a matter of fact, CTP was not able to deal with the networks
used in our experiments, even after Olesen et al.s or Heckermans transformation. On the other hand,
VE 1 was able to answer almost all randomly generated queries with and a majority of the queries
were answered in little time. It would be interesting to extend CTP to make use of the finer-grain
factorization mentioned above.
As we have seen in the previous section, there are queries, especially in the 422-node network,
that took VE 1 a long time to answer. There are also queries that VE 1 was not able to answer. For
those queries, approximation is a must. We employed an approximation technique when comparing
algorithms in the 422-node network. The technique captures, to some extent, the heuristic of ignoring
minor distinctions. In future work, we are developing a way to bound the error of the technique and
an anytime algorithm based on the technique.

Acknowledgements
We are grateful to Malcolm Pradhan and Gregory Provan for sharing with us the CPCS networks.
We also thank Jack Breese, Bruce DAmbrosio, Mike Horsch, Runping Qi, and Glenn Shafer for
valuable discussions, and Ronen Brafman, Chris Geib, Mike Horsch and the anonymous reviewers for very helpful comments. Mr. Tak Yin Chan has been a great help in the experimentations.
Research was supported by NSERC Grant OGPOO44121, the Institute for Robotics and Intelligent
Systems, Hong Kong Research Council grant HKUST658/95E and Sino Software Research Center
grant SSRC95/96.EG01.

Appendix A. Proof of Theorem 3
Theorem 3 Suppose a list of homogeneous factors F1 and a list of heterogeneous factors F2 constitute a tidy factorization of P (z1 ; z2; : : :; zk ). If z1 is either a convergent variable, or an old regular
variable, or a new regular variable whose deputy is not in the list fz2 ; : : :; zk g, then the procedure
sum-out1(F1 ; F2; z1) returns a tidy heterogeneous factorization of P (z2 ; : : :; zk ).
Proof: Suppose f1 , ..., fr are all the heterogeneous factors and g1, ..., gs are all the homogeneous
factors. Also suppose f1 , ..., fl , g1, ..., gm are all the factors that contain z1 . Then

P (z2 ; : : :; zk )

=

=

=

=

X

P (z1 ; z2; : : :; zk)

z1

X
z1


rj=1fj

X

s
Y
i=1

gi


lj=1fj ) 
 (
rj=l+1 fj )]

m
Y

s
Y

gi
i=1 i=m+1
m
s
X l
Y
Y
[(
j =1 fj
gi) 
 (
rj=l+1 fj )]
gi
z1
i=1
i=m+1
z1

[(

325

gi

(9)

fiZ HANG & P OOLE

=

X

m
s
Y
Y
l
r
[(

j=1 fj gi) 
 (
j=l+1fj )]
gi ;
z1
i=1
i=m+1

(10)

where equation (10) is due to Proposition 3. Equation (9) is follows from Proposition 4. As a matter
Q g due to the
of fact, if z1 is a convergent variable, then it is the only convergent variable in m
i=1 i
first condition of tidiness. The condition of Proposition 4 is satisfied because z1 does not appear
in fl+1 , ..., fr . On the other hand, if z1 is an old regular variable or a new regular variable whose
Q g contains no convergent variables due to the
deputy does not appear in the list z2 , ..., zk , then m
i=1 i
second condition of tidiness. Again the condition of Proposition 4 is satisfied. We have thus proved
that sum-out1(F1 ; F2; z1) yields a flexible heterogeneous factorization of P (z2 ; : : :; zk ).
Let e0 be a convergent variable in the list z2 , ..., zk . Then z1 cannot be the corresponding new regular variable e. Hence the factor I (e0; e) is not touched by sum-out1(F1 ; F2; z1). Consequently, if
we can show that the new factor created by sum-out1(F1 ; F2; z1) is either a heterogeneous factor
or a homogeneous factor that contain no convergent variable, then the factorization returned is tidy.
Suppose sum-out1(F1 ; F2; z1) does not create a new homogeneous factor. Then no heterogeneous factors in F1 contain z1 . If z1 is a convergent variable, say e0, then I (e0; e) is the only homoP
geneous factor that contain e0 . The new factor is e I (e0; e), which does contain any convergent
variables. If z1 is an old regular variable or a new regular variable whose deputy is not in the list z2 ,
..., zk , all the factors that contain z1 do not contain any convergent variables. Hence the new factor
again does not contain any convergent variables. The theorem is thus proved. 2
0

References
Arnborg, S., Corneil, D. G., & Proskurowski, A. (1987). Complexity of finding embedding in a
k-tree. SIAM J. Alg. Disc. Meth., 8(2), 277284.
Baker, M., & Boult, T. (1990). Pruning Bayesian networks for efficient computation. In Proc. Sixth
Conf. on Uncertainty in Artificial Intelligence, pp. 257264 Cambridge, Mass.
Bertele, U., & Brioschi, F. (1972). Nonserial dynamic programming, Vol. 91 of Mathematics in
Science and Engineering. Academic Press.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence in
Bayesian networks. In E. Horvitz and F. Jensen (Ed.), Proc. Twelthth Conf. on Uncertainty in
Artificial Intelligence, pp. 115123 Portland, Oregon.
Cooper, G. F. (1990). The computational complexity of probabilistic inference using Bayesian belief
networks. Artificial Intelligence, 42(2-3), 393405.
Dagum, P., & Galper, A. (1993). Additive belief-network models. In D. Heckerman and A. Mamdani
(Ed.), Proc. Ninth Conf. on Uncertainty in Artificial Intelligence, pp. 9198 Washington D.C.
DAmbrosio (1995). Local expression languages for probabilistic dependence. International Journal of Approximate Reasoning, 13(1), 6181.
DAmbrosio, B. (1994). Symbolic probabilistic inference in large BN2O networks. In R. Lopez de
Mantaras and D. Poole (Ed.), Proc. Tenth Conf. on Uncertainty in Artificial Intelligence, pp.
128135 Seattle.
326

fiE XPLOITING C AUSAL I NDEPENDENCE

IN

B AYESIAN N ETWORK I NFERENCE

Dechter, R. (1996). Bucket elimination: A unifying framework for probabilistic inference. In E.
Horvits and F. Jensen (Ed.), Proc. Twelthth Conf. on Uncertainty in Artificial Intelligence, pp.
211219 Portland, Oregon.
Dez, F. J. (1993). Parameter adjustment in bayes networks. the generalized noisy or-gate. In D.
Heckerman and A. Mamdani (Ed.), Proc. Ninth Conf. on Uncertainty in Artificial Intelligence,
pp. 99105 Washington D.C.
Duda, R. O., Hart, P. E., & Nilsson, N. J. (1976). Subjective Bayesian methods for rule-based inference systems. In Proc. AFIPS Nat. Comp. Conf., pp. 10751082.
Geiger, D., & Heckerman, D. (1996). Knowledge representation and inference in similarity networks
and Bayesian multinets. Artificial Intelligence, 82, 4574.
Geiger, D., Verma, T., & Pearl, J. (1990). d-separation: from theorems to algorithms. In M. Henrion
et. al. (Ed.), Uncertainty in Artificial Intelligence 5, pp. 139148. North Holland, New York.
Good, I. (1961). A causal calculus (i). British Journal of Philosophy of Science, 11, 305318.
Heckerman, D. (1993). Causal independence for knowledge acquisition and inference. In Proc. of
the Ninth Conference on Uncertainty in Artificial Intelligence, pp. 122127.
Heckerman, D., & Breese, J. (1994). A new look at causal independence. In Proc. of the Tenth
Conference on Uncertainty in Artificial Ingelligence, pp. 286292.
Henrion, M. (1987). Some practical issues in constructing belief networks. In L. Kanal and T. Levitt
and J. Lemmer (Ed.), Uncertainty in Artificial Intelligence, pp. 161174. North-Holland.
Howard, R. A., & Matheson, J. E. (1981). Influence diagrams. In Howard, R. A., & Matheson,
J. (Eds.), The Principles and Applications of Decision Analysis, pp. 720762. Strategic Decisions Group, CA.
Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. (1990). Bayesian updating in causal probabilistic
networks by local computations. Computational Statistics Quaterly, 4, 269282.
Kim, J., & Pearl, J. (1983). A computational model for causal and diagnostic reasoning in inference
engines. In Proc. of the Eighth International Joint Conference on Artificial Intelligence, pp.
190193 Karlsruhe, Germany.
Kjrulff, U. (1990). Triangulation of graphs - algorithms giving small total state space. Tech. rep.
R 90-09, Department of Mathematics and Computer Science, Strandvejen, DK 9000 Aalborg,
Denmark.
Lauritzen, S. L., Dawid, A. P., Larsen, B. N., & Leimer, H. G. (1990). Independence properties of
directed markov fields. Networks, 20, 491506.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical Society,
Series B, 50(2), 157224.
327

fiZ HANG & P OOLE

Li, Z., & DAmbrosio, B. (1994). Efficient inference in Bayes networks as a combinatorial optimization problem. International Journal of Approximate Reasoning, 11(1), 5581.
Olesen, K. G., & Andreassen, S. (1993). Specification of models in large expert systems based on
causal probabilistic networks. Artificial Intelligence in Medicine, 5, 269281.
Olesen, K. G., Kjrulff, U., Jensen, F., Falck, B., Andreassen, S., & Andersen, S. K. (1989). A
munin network for the median nerve - a case study on loops. Applied Artificial Intelligence,
3, 384403.
Parker, R., & Miller, R. (1987). Using causal knowledge to creat simulated patient cases: the CPSC
project as an extension of Internist-1. In Proc. 11th Symp. Comp. Appl. in Medical Care, pp.
473480 Los Alamitos, CA. IEEE Comp Soc Press.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, San Mateo, CA.
Poole, D. (1993). Probabilistic Horn abduction and Bayesian networks. Artificial Intelligence, 64(1),
81129.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering for large
belief networks. In R. Lopez de Mantaras and D. Poole (Ed.), Proc. Tenth Conf. on Uncertainty
in Artificial Intelligence, pp. 484490 Seattle.
Shachter, R. D., DAmbrosio, B. D., & Del Favero, B. D. (1990). Symbolic probabilistic inference
in belief networks. In Proc. 8th National Conference on Artificial Intelligence, pp. 126131
Boston. MIT Press.
Shafer, G., & Shenoy, P. (1990). Probability propagation. Annals of Mathematics and Artificial
Intelligence, 2, 327352.
Shortliffe, E., & Buchanan, G. B. (1975). A model of inexact reasoning in medicine. Math. Biosci.,
23, 351379.
Srinivas, S. (1993). A generalization of noisy-or model. In Proc. of the Ninth Conference on Uncertainty in Artificial Intelligence, pp. 208215.
Tarjan, R. E., & Yannakakis, M. (1984). Simple linear time algorithm to test chordality of graphs,
test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. SIAM J. Comput.,
13, 566579.
Zhang, N. L., & Poole, D. (1994). A simple approach to Bayesian network computations. In Proc.
of the Tenth Canadian Conference on Artificial Intelligence, pp. 171178.

328

fi