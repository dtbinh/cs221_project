Journal of Articial Intelligence Research 15 (2001) 163-187

Submitted 10/00; published 09/01

An Analysis of Reduced Error Pruning
elomaa@cs.helsinki.fi
matti.kaariainen@cs.helsinki.fi

Tapio Elomaa
Matti Kriinen
Department of Computer Science
P. O. Box 26 (Teollisuuskatu 23)
FIN-00014 University of Helsinki, Finland

Abstract

Top-down induction of decision trees has been observed to suer from the inadequate
functioning of the pruning phase. In particular, it is known that the size of the resulting
tree grows linearly with the sample size, even though the accuracy of the tree does not
improve. Reduced Error Pruning is an algorithm that has been used as a representative
technique in attempts to explain the problems of decision tree learning.
In this paper we present analyses of Reduced Error Pruning in three dierent settings.
First we study the basic algorithmic properties of the method, properties that hold independent of the input decision tree and pruning examples. Then we examine a situation that
intuitively should lead to the subtree under consideration to be replaced by a leaf node,
one in which the class label and attribute values of the pruning examples are independent
of each other. This analysis is conducted under two dierent assumptions. The general
analysis shows that the pruning probability of a node tting pure noise is bounded by a
function that decreases exponentially as the size of the tree grows. In a specic analysis
we assume that the examples are distributed uniformly to the tree. This assumption lets
us approximate the number of subtrees that are pruned because they do not receive any
pruning examples.
This paper claries the dierent variants of the Reduced Error Pruning algorithm,
brings new insight to its algorithmic properties, analyses the algorithm with less imposed
assumptions than before, and includes the previously overlooked empty subtrees to the
analysis.
1. Introduction
Decision tree learning is usually a two-phase process (Breiman, Friedman, Olshen, & Stone,
1984; Quinlan, 1993).

training examples

First a tree reecting the given sample as faithfully as possible is

constructed. If no noise prevails, the accuracy of the tree is perfect on the

that were used to build the tree. In practice, however, the data tends to be noisy, which
may introduce contradicting examples to the training set.

overtted

necessarily be obtained even on the training set.
is

Hence, 100% accuracy cannot

In any case, the resulting decision tree

to the sample; in addition to the general trends of the data, it encodes the

pruned

peculiarities and particularities of the training data, which makes it a poor predictor of the
class label of future instances. In the second phase of induction, the decision tree is

in order to reduce its dependency on the training data. Pruning aims at removing from the
tree those parts that are likely to only be due to the chance properties of the training set.
The problems of the two-phased top-down induction of decision trees are well-known
and have been extensively reported (Catlett, 1991; Oates & Jensen, 1997, 1998). The size

c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiElomaa & Kriinen
of the tree grows linearly with the size of the training set, even though after a while no
accuracy is gained through the increased tree complexity. Obviously, pruning is intended to
ght this eect. Another defect is observed when the data contains no relevant attributes;
i.e., when the class labels of the examples are independent of their attribute values. Clearly,
a single-node tree predicting the majority label of the examples should result in this case,
since no help can be obtained by querying the attribute values. In practice, though, often
large decision trees are built from such data.
Many alternative pruning schemes exist (Mingers, 1989a; Esposito, Malerba, & Semeraro, 1997; Frank, 2000).

pruning examples

They dier, e.g., on whether a single pruned tree or a series of

pruned trees is produced, whether a separate set of

is used, which aspects

(classication error and tree complexity) are taken into account in pruning decisions, how
these aspects are determined, and whether a single scan through the tree suces or whether
iterative processing is required.

The basic pruning operation that is applied to the tree

is the replacement of an internal node together with the subtree rooted at it with a leaf.
Also more elaborated tree restructuring operations are used by some pruning techniques

majority leaf
pruning of a tree

(Quinlan, 1987, 1993). In this paper, the only pruning operation that is considered is the
replacement of a subtree by the
examples reaching it. Hence, a

, i.e., a leaf labeled by the majority class of the
is a subtree of the original tree with just

zero, one, or more internal nodes changed into leaves.

Reduced Error Pruning

(subsequently

rep for short) was introduced by Quinlan (1987) in

the context of decision tree learning. It has subsequently been adapted to rule set learning as
well (Pagallo & Haussler, 1990; Cohen, 1993).
In practical decision tree pruning

overprunes

rep

rep is one of the simplest pruning strategies.

is seldom used, because it has the disadvantage of

requiring a separate set of examples for pruning. Moreover, it is considered too aggressive a
pruning strategy that

the decision tree, deleting relevant parts from it (Quinlan,

1987; Esposito et al., 1997). The need for a pruning set is often considered harmful because
of the scarceness of the data. However, in the data mining context the examples are often
abundant and setting a part of them aside for pruning purposes presents no problem.
Despite its shortcomings

rep

is a baseline method to which the performance of other

pruning algorithms is compared (Mingers, 1989a; Esposito, Malerba, & Semeraro, 1993;
Esposito et al., 1997).

It presents a good starting point for understanding the strengths

and weaknesses of the two-phased decision tree learning and oers insight to decision tree
pruning.

rep has the advantage of producing the smallest pruning among those that are the

most accurate with respect to the pruning set. Recently, Oates and Jensen (1999) analyzed

rep in an attempt to explain why and when decision tree pruning fails to control the growth

of the tree, even though the data do not warrant the increased size. We approach the same
subject, but try to avoid restricting the analysis with unnecessary assumptions.

We also

consider an explanation for the unwarranted growth of the size of the decision tree.

rep in three dierent settings. First, we explore the basic algorep, which apply regardless of the distribution of examples presented

In this paper we analyze
rithmic properties of

to the learning algorithm. Second, we study, in a probabilistic setting, the situation in which
the attribute values are independent of the classication of an example. Even though this
pure noise tting situation is not expected to arise when the whole pruning set is considered,
it is encountered at lower levels of the tree, when all relevant attributes have already been
exhausted. We further assume that all subtrees receive at least one pruning example, so that

164

fiAn Analysis of Reduced Error Pruning
none of them can be directly pruned due to not receiving any examples. The class value is
also assigned at random to the pruning examples. In our third analysis it is assumed that
each pruning example has an equal chance to end up in any one of the subtrees of the tree
being pruned. This rather theoretical setting lets us take into account those subtrees that
do not receive any examples. They have been left without attention in earlier analyses.
The rest of this paper is organized as follows. The next section discusses the dierent
versions of the

rep

algorithm and xes the one that is analyzed subsequently. In Section

3 we review earlier analyses of

rep.

Basic algorithmic properties of

Section 4. Then, in Section 5, we carry out a probabilistic analysis of
any assumptions about the distribution of examples.

rep are examined in
rep, without making

We derive a bound for the pruning

probability of a tree which depends exponentially on the relation of the number of pruning
examples and the size of the tree. Section 6 presents an analysis, which assumes that the
pruning examples distribute uniformly to the subtrees of the tree. This assumption lets us
sharpen the preceding analysis on certain aspects. However, the bounds of Section 5 hold
with certainty, while those of Section 6 are approximate results. Further related research is
briey reviewed in Section 7 and, nally, in Section 8 we present the concluding remarks of
this study.

2. Reduced Error Pruning Algorithm

rep

was never introduced algorithmically by Quinlan (1987), which is a source of much

confusion. Even though

rep

is considered and appears to be a very simple, almost trivial,

algorithm for pruning, there are many dierent algorithms that go under the same name.
No consensus exists whether

rep is a bottom-up algorithm

or an iterative method. Neither

is it obvious whether the training set or pruning set is used to decide the labels of the leaves
that result from pruning.

2.1 High-Level Control
Quinlan's (1987, p. 225226) original description of

rep does not clearly specify the pruning

algorithm and leaves room for interpretation. It includes, e.g., the following characterizations.
For every non-leaf subtree

S

of

T

we examine the change in misclassications

over the test set that would occur if

S

were replaced by the best possible leaf.

If the new tree would give an equal or fewer number of errors and
subtree with the same property,

S is replaced by the leaf.

S contains no

The process continues

until any further replacements would increase the number of errors over the test
set.
[...] the nal tree is the most accurate subtree of the original tree with respect
to the test set and the smallest tree with that accuracy.
Quinlan (1987, p. 227) also later continues to give the following description.
This method [pessimistic pruning] has two advantages. It is much faster than
either of the preceding methods [cost-complexity and reduced error pruning]
since each subtree is examined at most once.

165

fiElomaa & Kriinen
On one hand this description requires the nodes to be processed in a bottom-up manner,
since subtrees must be checked for the same property before pruning a node but, on the
other hand, the last quotation would indicate

rep

rep

to be an iterative method.

We take

to have the following single-scan bottom-up control strategy like in most other studies

(Oates & Jensen, 1997, 1998, 1999; Esposito et al., 1993, 1997; Kearns & Mansour, 1998).
Nodes are pruned in a single bottom-up sweep through the decision tree, pruning each node is considered as it is encountered.

The nodes are processed in

postorder.
By this order of node processing, any tree that is a candidate for pruning itself cannot
contain a subtree that could still be pruned without increasing the tree's error.
Due to the ambiguity of
1989a; Mitchell, 1997).

rep's denition, a dierent version of rep also lives on (Mingers,

It is probably due to Mingers' (1989) interpretation of Quinlan's

ambiguous denition.
Nodes are pruned iteratively, always choosing the node whose removal most
increases the decision tree accuracy over the pruning set. The process continues
until further pruning is harmful.
However, this algorithm appears to be incorrect. Esposito et al. (1993, 1997) have shown
that a tree produced by this algorithm does not meet the objective of being the most accurate
subtree with respect to the pruning set.

Moreover, this algorithm overlooks the explicit

requirement of checking whether a subtree would lead to reduction of the classication
error.
Other iterative algorithms could be induced from Quinlan's original description. However, if the explicit requirement of checking whether a subtree could be pruned before pruning a supertree is obeyed, then these versions of

rep

will all reduce to the more ecient

bottom-up algorithm.

2.2 Leaf Labeling
Another source of confusion in Quinlan's (1987) description of

rep

is that it is not clearly

specied how to choose the labels for the leaves that are introduced to the tree through

training

pruning. Oates and Jensen (1999) interpreted that the intended algorithm would label the
new leaves according to the majority class of the

pruning

examples, but themselves analyzed

a version of the algorithm where the new leaves obtain as their labels the majority of the
examples. Oates and Jensen motivated their choice by the empirical observation

that in practice there is very little dierence between choosing the leaf labels in either way.
However, choosing the labels of pruned leaves according to the majority of pruning examples
will set such leaves into a dierent status than the original leaves, which have as their label
the majority class of training examples.

Example

Figure 1 shows a decision tree that will be pruned into a single leaf if the

training examples are used to label pruned leaves. A negative leaf replaces the root of the
tree and makes two mistakes on the pruning examples, while the original tree makes three
mistakes.

With this tree we can illustrate an important dierence in using training and

166

fiAn Analysis of Reduced Error Pruning





+



+



+

0/1

0/1

2/0

2/0

Figure 1: A (part of a) decision tree. The labels inside the nodes denote the majority classes
of training examples arriving to these nodes. For leaves the numbers of pruning
examples from the two classes are also given.

x=y

means that

x negative

and

y

positive instances reach the leaf.

pruning examples to label pruned leaves. Using training examples and proceeding bottomup, observe that neither subtree is pruned, since the left one replaced with a negative leaf
would make two mistakes instead of the original one mistake. Similarly, the right subtree
replaced with a positive leaf would result in an increased number of classication errors.
Nevertheless, the root node  even though its subtrees have not been pruned  can still be
pruned.
When pruning examples are used to label pruned leaves, a node with two non-trivial
subtrees cannot be pruned unless both its subtrees are collapsed into leaves.

The next

section will prove this. In the tree of Figure 1 both subtrees would be collapsed into zeroerror leaves. However, in this case the root node will not be pruned.

A further possibility for labeling the leaf nodes would be to take both training and
pruning examples into account in deciding the label of a pruned leaf.

Depending on the

relation of the numbers of training and pruning examples this strategy resembles one or the
other of the above-described approaches. Usually the training examples are more numerous
than the pruning examples, and will thus dominate. In practice it is impossible to discern
this labeling strategy from that of using the majority of training examples.

2.3 Empty Subtrees
Since

rep

uses dierent sets of examples to construct and to prune a decision tree, it is

possible that some parts of the tree do not receive any examples in the pruning phase. Such
parts of the decision tree, naturally, can be replaced with a single leaf without changing
the number of classication errors that the tree makes on the pruning examples. In other
words, subtrees that do not obtain any pruning examples are always pruned. Quinlan (1987)
already noted that the parts of the original tree that correspond to rarer special cases, which
are not represented in the pruning set, may be excised.

167

fiElomaa & Kriinen
DecisionTree REP( DecisionTree T, ExampleArray S )
{ for ( i = 0 to S.length-1 ) classify( T, S[i] );
return prune( T ); }
void classify( DecisionTree T, Example e )
{ T.total++; if ( e.label == 1 ) T.pos++;
// update node counters
if ( !leaf(T) )
if ( T.test(e) == 0 ) classify( T.left, e );
else classify( T.right, e ); }
int prune( DecisionTree T ) // Output classification error after pruning T
{ if ( leaf(T) )
if ( T.label == 1 ) return T.total - T.pos;
else return T.pos;
else
{ error = prune( T.left ) + prune( T.right );
if ( error < min( T.pos, T.total - T.pos ) )
return error;
else
{ replace T with a leaf;
if ( T.pos > T.total - T.pos )
{ T.label = 1; return T.total - T.pos; }
else
{ T.label = 0; return T.pos; } } } }
Table 1: The

rep

algorithm. The algorithm rst classies the pruning examples in a top-

down pass using method
tree using method

prune.

classify

and then during a bottom-up pass prunes the

Intuitively, it is not clear which is the best-founded strategy for handling

empty subtrees

,

those that do not receive any examples. On one hand they obtain support from the training
set, which usually is more numerous than the pruning set but, on the other hand, the fact
that no pruning example corresponds to these parts of the tree would justify drawing the
conclusion that these parts of the decision tree were built by chance properties of the training
data. In

rep,

consistently with preferring smaller prunings also otherwise, the latter view

is adopted.
The problem of empty subtrees is connected to the problem of
learning algorithms (Holte, Acker, & Porter, 1989).
number of the training examples.

small disjuncts

in machine

A small disjunct covers only a small

Collectively the small disjuncts are responsible for a

small number of classication decisions, but they accumulate most of the error of the whole
concept. Nevertheless, small disjuncts cannot be eliminated altogether, without adversely
aecting other disjuncts in the concept.

168

fiAn Analysis of Reduced Error Pruning
2.4 The Analyzed Pruning Algorithm
Let us briey reiterate the details of the

rep algorithm that is analyzed subsequently.

As al-

ready stated, the control strategy of the algorithm is the single-sweep bottom-up processing.
First, a top-down traversal drives the pruning examples through the tree to the appropriate
leaves. The counters of the nodes en route are updated. Second, during a bottom-up traversal the pruning operations indicated by the classication errors are executed.

The errors

can be determined on the basis of the node counter values. In the bottom-up traversal each
node is visited only once. The pruned leaves are labeled by the majority of the pruning set
(see Table 1).

3. Previous Work
Pruning of decision trees has recently received a lot of analytical attention; existing pruning
methods have been analyzed (Esposito et al., 1993, 1997; Oates & Jensen, 1997, 1998,
1999) and new analytically-founded pruning techniques have been developed (Helmbold &
Schapire, 1997; Pereira & Singer, 1999; Mansour, 1997; Kearns & Mansour, 1998).

Also

many empirical comparisons of pruning have appeared (Mingers, 1989a; Malerba, Esposito,
& Semeraro, 1996; Frank, 2000). In this section we review earlier work that concerns the

rep

algorithm. Further related research is considered in Section 7.

Esposito et al. (1993) viewed the
search process in the state space.

rep

algorithm, among other pruning methods, as a

In addition to noting that the iterative version of

rep

cannot produce the optimal result required by Quinlan (1987), they also observed that even
though
the tree

rep is a linear-time algorithm in the size of the tree, with respect to the height of
rep requires exponential time in the worst case. In their subsequent comparative

analysis Esposito et al. (1997) sketched a proof for Quinlan's (1987) claim that the pruning
produced by

rep

tree.
The bias of

is the smallest among the most accurate prunings of the given decision

rep was briey examined by Oates and Jensen (1997, 1998).

They observed

rL , of the best majority leaf that could replace a subtree T only depends on
(the class distribution of ) the examples that reach the root N of T . In other words, the tree
structure above T and N decides the error rL . Let rT denote the error of the subtree T at
the moment when the pruning sweep reaches N ; i.e., when some pruning may already have
taken place in T . All pruning operations performed in T have led either rT to decrease from
the initial situation or to stay unchanged. In any case, pruning that has taken place in T
potentially decreases rT , but does not aect rL . Hence, the probability that rT < rL  i.e.,
that T will not be pruned  increases through pruning in T . This error propagation bias
that the error,

is inherent to

rep.

Oates and Jensen (1997, 1998) conjecture that the larger the original

tree and the smaller the pruning set, the larger this eect, because a large tree provides
more pruning opportunities and the high variance of a small pruning set oers more random
chances for

rL  rT .

Subsequently we study some of these eects exactly.

In a follow-up study Oates and Jensen (1999) used

rep

as a vehicle for explaining the

problems that have been observed in the pruning phase of top-down induction of decision
trees. They analyzed

rep in a situation in which

the decision node under consideration ts

noise  i.e., when the class of the examples is independent of the value of the attribute tested
in the node at hand  and built a statistical model of

169

rep

in this situation. It indicates,

fiElomaa & Kriinen
consistently with their earlier considerations, that even though the probability of pruning a
node that ts noise prior to pruning beneath it is close to 1, pruning that occurs beneath the
node reduces its pruning probability close to 0. In particular, this model shows that if even
one descendant of node

N

at depth

d is not pruned, then N

will not be pruned (assuming

d +1). The consequence of this result is that increasing depth
d leads to an exponential decrease of the node's pruning probability.
there are no leaves until depth

The rst part of Oates and Jensen's (1999) analysis is easy to comprehend, but its significance is uncertain, because this situation does not rise in any bottom-up pruning strategy.
The statistical model is based on the assumption that the number,

n, of pruning instances

that pass through the node under consideration is large, in which case  independence assumptions prevailing  the errors committed by the node can be approximated by the normal
distribution. The expected error of the original tree is the mean of the distribution, while, if
pruned to a leaf, the tree would misclassify a proportion of the

n examples that corresponds

to that of the minority class. Oates and Jensen show that the latter number is always less
than the mean of the standard distribution of errors. Hence, the probability of pruning is
over 0.5 and approaches 1 as

n grows.

In the second part of the analysis, in considering the pruning probability of a node

N

after pruning has taken place beneath it, Oates and Jensen assume that the proportion of
positive examples in any descendant of
assuming further that

N

N

at depth

d is the same as in N .

In this setting,

d also have a
d are pruned, they are

has a positive majority, all its descendants at level

positive majority. It directly follows that if all descendants at level

all replaced by a positive leaf. Hence, the function represented by this pruning is identically
positive. The majority leaf that would replace
smaller than the above pruning. Therefore,

N

also represents the same function and is

rep will choose the single leaf pruning.

On the

N at depth d are not pruned, then the
N , in which these subtrees are maintained and all other nodes

other hand, if one or more of the descendants of
pruning of the tree rooted at
at level

d are

pruned into positive leaves, is more accurate than the majority leaf. In this

case the tree will not be pruned.
Oates and Jensen (1999) also assume that starting from any node at level

d the proba-

bility of routing an example to a positive leaf is the same. In the following analyses we try
to rid all unnecessary assumptions; the same results can be obtained without any knowledge
of the example distribution.

4. Basic Properties of

rep

Before going to the detailed probabilistic analysis of the
of its basic algorithmic properties.

rep

algorithm, we examine some

Throughout this paper we review the binary case for

simplicity. The results, however, also apply with many-valued attributes and several classes.
Now that the processing control of the

rep

algorithm has been settled, we can actually

prove Quinlan's (1987) claim of the optimality of the pruning produced by

rep.

Observe

that the following result holds true independent of the leaf labeling strategy.

Theorem 1 Applying rep with a set of pruning examples, S , to a decision tree T produces
T 0 a pruning of T such that it is the smallest of those prunings of T that have minimal
error with respect to the example set S.
170

fiAn Analysis of Reduced Error Pruning
Proof We prove the claim by induction over the size of the tree. Observe that a decision
T is a full binary tree, which has 2L(T ) 1 nodes, where L(T ) is the number of leaves

tree

in the tree.

Base case

.

L(T ) = 1,

If

then the original tree

T

consists of a single leaf node.

T

is

the only possible pruning of itself. Thus, it is, trivially, also the smallest among the most
accurate prunings of

T.

Inductive hypothesis
Inductive step L(T ) = k

. The claim holds when

.

. Let

N

right subtree, respectively. Subtrees
the pruning decision for

N

L(T ) < k.

T1

the left and the

must have strictly less than

When

be the root of the tree and

T0

and

T1

prunings of these trees.

T00

and

T10 ,

k leaves.

By the inductive hypothesis,

are the smallest possible among the most accurate

(i): Accuracy
N

and

is taken, then  by the bottom-up recursive control strategy of

rep  T0 and T1 have already been processed by the algorithm.
the subtrees after pruning,

T0

. The pruning decision for the node

N consists of choosing whether to collapse

and the tree rooted at it into a majority leaf, or whether to maintain the whole

tree. If both alternatives make the same number of errors, then

N

is collapsed and the

original accuracy with respect to the pruning set is retained. Otherwise, by the

rep

algorithm, the pruning decision is based on which of the resulting trees would make

S . Hence, whichever choice is made, the
T 0 will make the smaller number of errors with respect to S .
00
Let us now assume that a pruning T of T makes even less errors with respect to S
0
00
00
00
than T . Then T must consist of the root N and two subtrees T0 and T1 , because the
0
00
majority leaf cannot be more accurate than T . Since T is a more accurate pruning of
T than T 0 , it must be that either T000 is a more accurate pruning of T0 than T00 or T100 is
0
a more accurate pruning of T1 than T1 . By the inductive hypothesis both possibilities
0
are false. Therefore, T is the most accurate pruning of T .

less errors with respect to the pruning set
resulting tree

(ii): Size
0
T

. To see that the chosen alternative is also as small as possible, rst assume that

consists of a single leaf. Such a tree is the smallest pruning of

claim follows.

T00 and T10 .

0
Otherwise, T consists of the root node N

T , and in this case the

and the two pruned subtrees

Since this tree was not collapsed, the tree must be more accurate than the

tree consisting of a single majority leaf. Now assume that there exists a pruning

T

T 0 , but smaller. Because the majority leaf is less accurate
0



than T , T must consist of the root node N and two subtrees T0 and T1 . Then, either
T0 is a smaller pruning of T0 than T00 , but as accurate, or T1 is a smaller pruning of
T1 than T10 , but as accurate. Both cases contradict the inductive hypothesis. Hence,
T 0 is the smallest among the most accurate prunings of T .
of

T

that is as accurate as

Thus, in any case, the claim follows for

T.

2

We consider next the situation in an internal node of the tree, when the bottom-up
pruning sweep reaches the node.

From now on we are committed to leaf labeling by the

majority of the pruning examples.

171

fiElomaa & Kriinen

An internal node, which prior to pruning had no leaves as its children, will not
be pruned by rep if it has a non-trivial subtree when the bottom-up pruning sweep reaches
it.
Theorem 2
Proof

For an internal node

N

we have two possible cases in which it has non-trivial

subtrees; either both its subtrees are non-trivial (non-leaf ) or one of them is trivial. Let us
review these cases.
Let

rT

denote the error of (sub)tree

T

with respect to the part of the pruning set that

T . By rL we denote the misclassication rate of the majority leaf L that
T , if T was chosen to be pruned.

reaches the root of
would replace

Case I: Let the two subtrees of

T , T0 and T1 , be non-trivial.

Hence, both of them have been

rT0 < rL0 and rT1 < rL1 ,
T0 and T1 , respectively,
if pruned. Because rT = rT0 + rT1 , it must be that rT < rL0 + rL1 .
If T0 and T1 have the same the majority class, then it is also the majority class of T .
Then rL = rL0 + rL1 , where L is the majority leaf corresponding to T . Otherwise,
rL  rL0 + rL1 . In any case, rL  rL0 + rL1 . Combining this with the fact that
rT < rL0 + rL1 means that rT < rL . Hence, T is not pruned.

retained when the pruning sweep has passed them. Thus,

L0

where

Case II: Let

T

and

L1

are the majority leaves that would replace

have one trivial subtree, which was produced by pruning, and one non-

T0 is non-trivial and L1 is
T1 in the pruning process. Then, rT0 < rL0 . Hence,
we have that rT = rT0 + rL1 < rL0 + rL1 .
In the same way as in the Case I, we can deduce that rL  rL0 + rL1 . Therefore,
rT < rL and T will be retained in the pruned tree.

trivial subtree. We assume, without loss of generality, that
a majority leaf which has replaced

T

cannot be pruned in either case, and the pruning process can be stopped on the branch

T

containing
If node

unless an original leaf appears along the path from the root to

N

2

T.

has an original leaf, then it may be pruned even if the other subtree of

is non-trivial. Also when

N

N

has two trivial subtrees, it may be pruned. Whether pruning

takes place depends on the class distribution of examples reaching

N

and its subtrees.

In the analysis of Oates and Jensen (1999) it was shown that the prerequisite for pruning
a node

N

from the tree is that all its descendants at depth

d

have been pruned.

depth just above the rst (original) leaf in the subtree rooted at
result to this situation, we can corroborate their nding that
more of its descendants at depth

d are retained.

N

N.

d

is the

If we apply the above

will not be pruned if one or

Applying Theorem 2 recursively gives the

result.

A tree T rooted at node N will be retained by rep if one or more of the
descendants of N at depth d are not pruned.
Corollary 3

To avoid the analysis being restricted by the leaf globally closest to the root, we need to

fringe

be able to consider the set of leaves closest to the root on all branches of the tree. Let us
dene that the

of a decision tree contains any node that prior to pruning had a leaf

172

fiAn Analysis of Reduced Error Pruning

Figure 2: The fringe (black and gray nodes), interior (white nodes), and the safe nodes
(black ones) of a decision tree. The triangles denote non-trivial subtrees.

as its child. Furthermore, any node that is in a subtree rooted at a node belonging to the

interior

Safe nodes

fringe of the tree is also in the fringe. Those nodes not belonging to the fringe make up the
of the tree.

themselves belong to the fringe of the tree, but have their

parent in the interior of the tree (see Figure 2). Because the fringe of a decision tree is closed
downwards, the safe nodes of a tree correspond to the leaves of some pruning of it. Observe
also that along the path from the root to a safe node there are no leaves. Therefore, if the
pruning process ever reaches a safe node, Theorem 2 applies on the corresponding branch
from there on.
If the decision tree under consideration will be pruned into a single majority leaf, safe
nodes also need to be turned into leaves at some point, not necessarily simultaneously. If
the pruning sweep continues to the safe nodes, from then on the question whether a node is
pruned is settled solely on the basis of whether all nodes on the path to the root have the
same majority class. The pruning of the whole tree can be characterized as below.
Let

T

be the tree to be pruned and

S the set of pruning examples, jS j = n.

We assume,

without loss of generality, that at least half of the pruning examples are positive. Let
the proportion of positive examples in

S ; p  0:5.

If

T

p be

was to be replaced by a majority

leaf, that leaf would have a positive class label. Under these assumptions we can prove the
following.

A tree T will be pruned into a single leaf if and only if
 all subtrees rooted at the safe nodes of T are pruned and
 at least as many positive as negative pruning examples reach each safe node in T .

Theorem 4

173

fiElomaa & Kriinen
Proof

To begin we show that the two conditions are necessary for the pruning of

we show that if the former condition is not fullled, then
leaf. Second, we prove that neither will
latter not.
hold, then

T

T

T.

First,

cannot be pruned into a single

be pruned if the former condition holds, but the

Third, we show the suciency of the conditions; i.e., prove that if they both

T

will be pruned into a single leaf.

T

(i): Let us rst assume that in

there is a safe node

the denition of a safe node, the parent
Therefore, by Theorem 2,
neither will the root of

T

P

P

of

N

N

such that it will not be pruned. By

originally had no leaves as its children.

will not be pruned.

It is easy to see, inductively, that

be pruned.

(ii): Let us then assume that all subtrees rooted at safe nodes get pruned and that there are
one or more safe nodes in

T

into which more negative than positive pruning examples

fall. Observe that all safe nodes cannot be such. Let us now consider the pruning of

T

in which the leaves are situated in place of the safe nodes; the leaves receive the same
examples as the original safe nodes.

Because safe nodes are internal nodes, in

rep

the corresponding pruned leaves are labeled by the majority of the pruning examples.
In particular, the safe nodes that receive more negative than positive examples are
replaced by negative leaves. All other leaves are labeled positive. This pruning of the
original tree is more accurate than the majority leaf. Hence, by Theorem 1,
not prune

T

rep

will

into a single-leaf tree.

(iii): Let us now assume that all subtrees rooted at the safe nodes of

T

are pruned and that

at least as many positive as negative pruning examples reach each safe node. Then
all interior nodes must also have a majority of positive pruning examples. Otherwise,

T that has more negative than positive examples. Thus,
N has a majority of negative examples. Carrying the
induction all the way to the safe nodes shows that no such node N can exist in T .
Hence, all interior prunings of T represent the same function (identically positive) and
all of them have the same error with respect to S . The majority leaf is the unique,

there is an interior node

N

in

at least one of the children of

smallest of these prunings and will, by Theorem 1, be chosen.

2
5. A Probabilistic Analysis of

rep

Let us now turn our attention to the question of what the prerequisites for pruning a decision
tree

T

into a single majority leaf are. Since, by Theorem 1,

rep

produces a pruning of

T

which is the most accurate with respect to the pruning set and such that it is as small as
possible, to show that

T

does not reduce to a single leaf it suces to nd its pruning that

has a better prediction accuracy on the pruning examples than the majority leaf has.
In the following the class of an example is assumed to be independent of its attribute
values. Obviously, if in a decision tree there is a node where this assumption holds for the
examples arriving to it, we would like the pruning algorithm to turn it into a majority leaf.
We do not make any assumptions about the decision tree. However, similar to the analysis
of Oates and Jensen (1999), for the obtained bounds to be tight, the shortest path from the
root of the tree to a leaf should not be too short.

174

fiAn Analysis of Reduced Error Pruning
5.1 Probability Theoretical Preliminaries
Let us recall some basic probabilistic concepts and results that are used subsequently. We
denote the probability of an event

and p

X
X  B (n; p), if

(integer-valued) random variable
, denoted by

E

by

PrfE g

EE .

binomially distributed with parameters n
and its expectation by

is said to be

A discrete

!

n k
Prf X = k g =
p (1 p)n k ; k = 0; 1; : : : ; n:
k
If

X  B (n; p), then its expected
p value or mean is EX =  = np, variance varX = np(1 p),
 = np(1 p).

indicator variable

and standard deviation
An

1. An indicator variable
If

A1 ; : : : ; An

is is a discrete random variable that takes on only the values 0 and

I

is used to denote the occurrence or non-occurrence of an event.

Pn

are independent events with

X=

IA

PrfAi g = p and IA1 ; : : : ; IA

n

are the respective

i=1
Bernoulli
p
density function fX : IN ! [0; 1]
X
fX (x) = Prf X = x g
cumulative
distribution
function
F
:
IN
!
[0; 1]
X
P
indicator variables, then

IA

i

is called a

i

is binomially distributed with parameters

random variable with parameter

The

for a discrete random variable

.

n and p.

.

The

is dened as

FX (y) = Prf X  y g = xy fX (x).
Let X  B (n; p) be a random variable with mean  = np and standard
p
 = np(1 p). The normalized random variable corresponding to X is

for

X

is

dened as

X

Xe =





deviation

:

By the central limit theorem we can approximate the cumulative distribution function

e
of X

by the

normal Gaussian distribution
or

n

FXe (y) = Pr Xe  y

o

 (y):

 is the cumulative distribution function of the bell curve density function e
Respectively, we can apply the
able

X

normal approximation

FX (y) = Prf X  y g = FXe

x2 =2 =

FXe

p

2 .

to the corresponding random vari-

y 
y 


:



5.2 Bounding the Pruning Probability of a Tree
Now, the pruning set is considered to be a sample from a distribution in which the class
attribute is independent of the other attributes.

We assume that the class attribute is

(p) distribution; i.e., the class is positive with probability
p and negative with probability 1 p. We assume that p > 0:5.
distributed according to Bernoulli

In the following we will analyze the situation in which the subtrees rooted at safe nodes
have already been pruned into leaves. We bound the pruning probability of the tree starting
from this initial conguration.

Since the bottom-up pruning may already have come to

a halt before that situation, the following results actually give too high a probability for
pruning. Hence, the following upper bounds are not as tight as possible.

175

fiElomaa & Kriinen
We consider pruning a decision tree by

rep

as a trial whose result is decided by the set

all

of pruning examples. By Theorem 4 we can approximate the probability that a tree will
be pruned into a majority leaf by approximating the probability that

safe nodes get a

positive majority or a negative majority. The latter alternative is not very probable under
the assumption

p > :5.

It is safe to assume that it never happens.

We can consider sampling the pruning examples in two phases. First the attribute values
are assigned.

This decides the leaf into which the example falls.

In the second phase we

independently assign the class label for the example.

Z (T ) = fz1 ; : : : ; zk g and let the number of examples in
the pruning set S be jS j = n. The number of pruning examples falling to a safe node zi is
Pk
denoted by ni ;
i=1 ni = n. For the time being we assume that ni > 0 for all i. The number
of positive examples falling to safe node zi is the sum of independent Bernoulli variables
and, thus, it is binomially distributed with parameters ni and p. Respectively, the number
of negative pruning examples in safe node zi is Xi  B (ni ; 1
p). The probability that there
is a majority of negative examples in safe node zi is Prf Xi > ni =2 g. We can bound this
Let the safe nodes of tree

T

be

probability from below by using the following inequality (Slud, 1977).

Lemma 5 (Slud's inequality)
for m(1 q)  h  mq,

Let X  B(m; q) be a random variable with q  1=2. Then
!

h mq
:
Prf X  h g  1  p
mq(1 q)
p > :5 and the random variable corresponding to the number of negative examples
zi is Xi  B (ni; 1 p), the rst condition of Slud's inequality holds. Furthermore,
to see that condition m(1
q)  h  mq holds in safe node zi substitute h = ni =2, m = ni ,
and q = 1
p to obtain ni p  ni=2  ni(1 p). Thus,
Since

in safe node



Pr Xi >
As

ni 

2

!

!

=2 ni(1 p)
= 1  p(p 1=2)ni :
 1  nip
ni p(1 p)
ni p(1 p)

(1)

ni , the number of pruning instances reaching safe node zi , grows, then the standard

normal distribution term in the above bound also grows. Hence, the bound on the probability
that the majority of the pruning examples reaching

zi

is negative is the smaller the more

pruning examples reach it. The probability of a negative majority also reduces through the
growing probability of positive class for an example,

p.

These both are also reected in the

pruning probabilities of the whole tree.
We can now roughly approximate the probability that
majority leaf as follows. By Theorem 4,
node in
are

T

T

T

will be pruned into a single

will be pruned into a leaf if and only if each safe

receives a majority of positive examples. Because

T

has

k

safe nodes and there

n pruning examples, then according to the pigeon-hole principle at least half of the safe
r = 2n=k examples. Each safe node zi with ni  r examples has, by

nodes receive at most

Inequality 1, a negative majority at least with probability

!

1  p(p 1=2)r :
rp(1 p)
176

fiAn Analysis of Reduced Error Pruning
Observe that Inequality 1 also holds when

ni < r, becausepthe cumulative

distribution

 is an increasing function. The argument ni (p 1=2)= ni p(1 p) canpbe rewritten
p
as
ni cp , where cp ispa positive constant depending on the value of p. Since ( ni cp ) grows
as ni grows, 1
( ni cp ) grows with decreasing ni . Hence, the lower bound of Inequality
1 also applies for values 0 < ni < r .

function

Thus, the probability that the half of the safe nodes that receive at most

r

examples

have a positive majority is at most

 p(p 1=2)r
rp(1 p)

!!k=2

:

(2)

This is an upper bound for the probability that the whole tree

T

will be pruned into a single

leaf. The only distribution assumption that was made to reach the result is that

p > :5.

order to obtain tighter bounds, one has to make assumptions about the shape of the tree
and the distribution of examples.
The bound of Equation 2 depends on the size of the decision tree (reected by

n

p

In

T

k), the

number ( ) and the class distribution ( ) of the pruning examples. Keeping other parameters
constant and letting

k

grow reduces the pruning probability exponentially. If the number

of pruning examples grows in the same proportion so that

r = 2n=k

stays constant, the

pruning probability still falls exponentially. Class distribution of the pruning examples also
aects the pruning probability which is the smaller, the closer

p is to value .5.

5.3 Implications of the Analysis
It has been empirically observed that the size of the decision tree grows linearly with the
training set size, even when the trees are pruned (Catlett, 1991; Oates & Jensen, 1997,
1998). The above analysis gives us a possibility to explain this behavior. However, let us
rst prove that when there is no correlation between the attribute values and the class label
of an example, the size of the tree that perfectly ts the training data depends linearly on
the size of the sample.
Our setting is as simple as can be. We only have one real-valued attribute
attribute

y, whose value is independent

of that of

x.

As before,

y

x and the class

has two possible values,

0 and 1. The tree is built using binary splits of a numerical value range; i.e., propositions
of type 

x < r

are assigned to the internal leaves of the tree.

In this analysis duplicate

instances occur with probability 0.

Let the training examples (x; y) be drawn from a distribution, where x is uniformly distributed in the range [0; 1) and y obtains value 1, independent of x, with probability
p, and value 0 with probability 1 p. Then the expected size of the decision tree that ts the
data is linear in the size of the sample.
Theorem 6

S = h(x1 ; y1 ); : : : ; (xt ; yt )i be a sample of the above described distribution. We
xi 6= xj , when i 6= j , because the probability of the complement event is 0.
Let us, further, assume that the examples of S have been indexed so that x1 < x2 < : : : < xt .
Let Ai be the indicator variable for the event that instances i and i + 1 have dierent class
labels; i.e., yi 6= yi+1 , 1  i  t 1. Then EAi = Prf Ai = 1 g = p(1 p)+(1 p)p = 2p(1 p),
Proof

Let

may assume that

177

fiElomaa & Kriinen
yi = 1 has probability p, at the same time the event yi+1 = 0 has
1 p, and vice versa. Now the number of class alternations is A = Pti=11 Ai and

because when the event
probability

its expectation is

EA =
Let

T

t 1
X
i=1

EAi =

t 1
X
i=1

2p(1 p) = 2p(1 p)

t 1
X
i=1

1 = 2(t 1)p(1 p):

be a decision tree that has been grown on the sample

continued until the training error is 0. Each leaf in

[a; b) in [0; 1).

If

yi 6= yy+1 ,

then

xi

and

xi+1

T

S.

(3)

The growing has been

corresponds to a half open interval

must fall into dierent leaves of

T,

because

T . Thus, the upper boundary b of
xi falls in must have a value less than xi+1 .

otherwise one or the other example is falsely classied by
the interval corresponding to the leaf into which

Repetitively applying this observation when scanning through the examples from left to

T must at least have one leaf for x1 and one leaf for each class alternation;
A + 1 leaves in total. By using Equation 3 we see that the expected number of leaves

right, we see that
i.e.,
in

T

is

EA + 1 = 2(t 1)p(1 p) + 1:
In particular, this is linear in the size of the sample S ; jS j = t.

2

The above theorem only concerns zero training error trees built in the rst phase of
decision tree induction. The empirical observations of Catlett (1991) and Oates and Jensen
(1997, 1998), however, concern decision trees that have been pruned in the second phase of
induction. We come back to the topic of pruned trees shortly.
Consider how

rep is used in practice.

There is some amount of (classied) data available

from the application domain. Let there be a total of

t examples available. Some part ff of
1 ff of it is reserved as the

the data is used for tree growing and the remaining portion
separate pruning set;

0 < ff < 1.

Quite a common practice is to use two thirds of the data

for growing and one third for pruning or nine tenths for growing and one tenth for pruning
when (ten-fold) cross-validation is used. In the decision tree construction phase the tree is
tted to the

fft examples as perfectly as possible.

If we hypothesize that the previous result

holds for noisy real-world data sets, which by empirical evidence would appear to be the
case, and that the number of safe nodes also grows linearly with the number of leaves, then
the tree grown will contain

t

safe nodes, where

 > 0. Since the pruning set size also is
r = 2n=k stays constant in this setting.

a linear fraction of the training set size, the ratio

Hence, by Equation 2, the growing data set size forces the pruning probability to zero, even
quite fast, because the reduction in the probability is exponential.

5.4 Limitations of the Analysis
Empty subtrees, which do not receive any pruning examples, were left without attention
above; we assumed that

ni > 0

for each

i.

Empty subtrees, however, decisively aect the

analysis; they are automatically pruned away. Unfortunately, one cannot derive a non-trivial
upper bound for the number of empty subtrees. In the worst case all pruning examples are
routed to the same safe node, which leaves

k 1 empty safe nodes to the tree.

Subsequently

we review the case where the examples are distributed uniformly to the safe nodes. Then
better approximations can be obtained.

178

fiAn Analysis of Reduced Error Pruning
Even though we assume that each pruning example is positive with a higher probability
than .5, there are no guarantees that the majority of all examples is positive.

However,

the probability that the majority of all examples changes is very small, even negligible, by
Cherno 's inequality (Cherno, 1952; Hagerup & Rb, 1990) when the number of pruning

n, is high and p is not extremely close to one half.
Prf X  h g, but above we used it to bound
the probability Prf X > h g. Some continuity correction could be used to compensate this.
examples,

Slud's inequality bounds the probability

In practice, the inexactness does not make any dierence.
Even though it would appear that the number of safe nodes increases in the same proportion as that of leaves when the size of the training set grows, we have not proved this
result. Theorem 6 essentially uses leaf nodes, and does not lend itself to modication, where
safe nodes could be substituted in place of leaves.
The relation between the number of safe nodes and leaves in a decision tree depends on
the shape of the tree. Hence, the splitting criterion that was used in tree growing decisively
aects this relation. Some splitting criteria aim at keeping the produced split as balanced as
possible, while others aim at separating small class coherent subsets from the data (Quinlan,
1986; Mingers, 1989b). For example, the common entropy-based criteria have a bias that
favors balanced splits (Breiman, 1996). Using a balanced splitting criterion would seem to
imply that the number of safe nodes in a tree depends linearly on the number of leaves in
the tree.

In that case the above reasoning would explain the empirically observed linear

growth of pruned decision trees.

6. Pruning Probability Under Uniform Distribution
We now assume that all
of the

k

n

pruning examples have an equal probability to end up in each

safe nodes; i.e., a pruning example falls to the safe node

zi

with probability

1=k.

Contrary to the normal uniform distribution assumption analysis, for our analysis this is not
the best case. Here the best distribution of examples into safe nodes would have one pruning
example in each of the safe nodes except one, into which all remaining pruning instances
would gather.

Nevertheless, the uniformity lets us sharpen the general approximation by

using standard techniques.

n=k. Let us calculate
cn=k examples, where c is an
for the event safe node zi receives at

The expected number of examples falling into any safe node is
the expected number of those safe nodes that receive at most

QPi be the indicator
k Q is the number of those safe nodes that receive less
i=1 i
Pk
than
By the linearity of expectation EQ =
i=1 EQi = kEQ1 , in which
the last equality follows from the fact that the Qi -s are identically distributed.
Let Y1 be the number of examples reaching safe node z1 . Because each of the n examples reaches z1 with probability 1=k independent of the other examples, Y1 is binomially
distributed with parameters n and 1=k . Clearly EQ1 = Prf Y1  cn=k g. We can approxiarbitrary positive constant. Let
most

cn=k examples.
cn=k examples.

Then

Q=

mate the last probability by the normal approximation, from which we obtain

!
!


cn=k
n=k
(
c
1)
n=k
cn
  pn  1=k  (1 1=k) =  pn=k(1 1=k) :
Pr Y1 
k
179

fiElomaa & Kriinen
Hence, by the above observation,

!

(c 1)n=k :
EQ = kEQ1  k p
n=k(1 1=k)
T

(4)

We now use Approximation 4 to determine the probability that the whole decision tree
will be pruned into a single leaf. Let

denote by

T

P

be a random variable that represent the number

cn=k examples and at least one example. If we
R the number of empty safe nodes, we have P = Q R. Hence, EP = E(Q R) =

of those safe nodes in

that receive at most

EQ ER.

The following result (Kamath, Motwani, Palem, & Spirakis, 1994; Motwani & Raghavan,
1995) lets us approximate the number of empty safe nodes when

Theorem 7

bins. Then

n  k.

Let Z be the number of empty bins when m balls are thrown randomly into h


 = EZ = h 1

and for  > 0,

Prf jZ

1 m  he

h

j   g  2 exp

m=h

!

2 (h 1=2)
:
h2 2

By this result the expected number of empty safe nodes is approximately
number is small when

ke

n=k ;

this

k is relatively small compared to n.
EQ (Equation 4) and using the pre-

Substituting the above obtained approximation for
vious result, we get

(c 1)n=k
EP = EQ ER  k  p
n=k(1 1=k)

!

e

n=k

!

:

Applying Slud's inequality we can, as before, bound from above the probability that
the majority class does not change in a safe node that receives
Since there are

P

cn=k

pruning examples.

such safe nodes and the class distribution of examples within them is

independent, the event majority class does not change in any safe node that receives at
least one and at most

cn=k examples

has the upper bound

 p(p :5)r
rp(1 p)
where

r = cn=k.

Replacing

P

!!P

;

(5)

with its expected value in this equation we have an approxi-

mation for the pruning probability. This approximation is valid if
from its expected value. We consider the deviation of

P

P

does not deviate a lot

from its expected value below.

The above upper bound for the pruning probability is similar to the upper bound that
was obtained without any assumptions about the distribution of the examples. However, the
earlier constant 2 has been replaced by a new, controllable parameter
are now explicitly taken into account. If

c, and empty subtrees

c is chosen suitably, this upper bound is more strict

than the one obtained in the general case.

180

fiAn Analysis of Reduced Error Pruning

Upper bound for the pruning probability
0.5
0.25

1

0.5

0

0.9
0.8
0.7

0.5
1

Figure 3: The eect of parameters

p

0.6

1.5

c

0.5

p and c on the upper bound of the pruning probability

of a tree with 100 safe nodes when 500 pruning examples are used. The curves
depicting the 0.25 and 0.5 upper bounds are also shown.

6.1 An Illustration of the Upper Bound
Figure 3 plots the upper bound of the pruning probability of a tree with 100 safe nodes when
500 pruning examples are used. The value of the parameter

c varies from 0 to 2 and p varies

from 0.5 to 1. We can observe that the surface corresponding to the upper bound stays very
close to 0 when the class distribution is not too skewed and when the parameter

c does not

have a very small value. When the probability of an example having a positive class label

c approaches 0, the upper bound climbs very steeply. At least
parameter c this is due to the inexactness of the approximation on the

hits value 0.75 or the value of
on the part of the
extreme values.
When the probability

p

that an example has a positive class approaches 1, the error

committed by a single positive leaf falls to 0. Hence, the accuracy of a non-trivial pruning
has to be better, the closer

p is to 1 for it to beat the majority leaf.

Intuitively, the probability

that such a pruning exists  i.e., that the root node is not pruned  should drop to zero as

p increases.

The bound reects this intuition.

When the value of parameter

c falls close to 0, the safe nodes that are taken into account

in the upper bound only receive very few pruning examples. The number of such nodes is

181

fiElomaa & Kriinen
small. On the other hand, when

c

is increased, the number of nodes under consideration

grows together with the upper limit on the number of examples reaching each single one of
them. Thus, both small and large values of
the value of

c is somewhere

c

yield loose bounds. In the strictest bounds

in the middle, in our example around values 1.01.5. In the

bound of Equation 5 the argument of the cumulative distribution function
zero when the value of

c is very small,

 tends towards

but at the same time the exponent decreases. The

 approaches 1/2, when its argument goes to zero. On the other hand, when c has
a large value,  approaches value 1 and the exponent P also increases.
value of

6.2 On the Exactness of the Approximation
Above we used the expected value of P in the analysis; EP = EQ
ER. We now probe into
the deviation of P from its expected value. The deviation of R is directly available from
Theorem 7:

For

!

2 (k 1=2)
:
k2 E2 R

Prf jR ERj   g  2 exp

Q we do not have a similar result yet.

Lipschitz condition

In this section we provide one.

Let us rst recapitulate the denition of the

.

f : D1      Dm ! IR be a real-valued function with m arguments from
f is said to satisfy the Lipchitz condition if for any
x1 2 D1 ; : : : ; xm 2 Dm , any i 2 f1; : : : ; mg, and any yi 2 Di ,
Denition

Let

possibly distinct domains. The function

jf (x1 ; : : : ; xi 1 ; xi ; xi+1; : : : ; xm ) f (x1; : : : ; xi 1 ; yi; xi+1 ; : : : ; xm )j  1:

Hence, a function satises the Lipschitz condition if an arbitrary change in the value of
any one argument does not change the value of the function more than 1.

martingales

The following result (McDiarmid, 1989) holds for functions satisfying the Lipschitz condition. More general results of the same kind can be obtained using
(Motwani & Raghavan, 1995)).

(see e.g.,

Theorem 8 (McDiarmid) Let X1 ; : : : ; Xm be independent random variables taking values
in a set V . Let f : V m ! IR be such that, for i = 1; : : : ; m:

sup

x1 ;:::;xm ;yi 2V

jf (x1; : : : ; xi 1; xi ; xi+1; : : : ; xm ) f (x1; : : : ; xi 1 ; yi; xi+1 ; : : : ; xm )j  ci :

Then for  > 0,
Prf jf (X1 ; : : : ; Xm ) Ef (X1 ; : : : ; Xm )j   g  2 exp

2
P2

m c2
i=1 i

!

:

Wi , i = 1; : : : ; n, be a random variable such that Wi = j if the i-th example is
directed to the safe node zj . By the uniform distribution assumption Wi -s are independent.
They have their values within the set f1; : : : ; k g. Let us dene the function f so that
f (w1 ; : : : ; wn ) is the number of those safe nodes that receive at most r = cn=k examples,
when the i-th example is directed to the safe node zw . That is,
Let

i

f (w1 ; : : : ; wn ) = jf i 2 f 1; : : : ; k g j jSi j  r gj;
182

fiAn Analysis of Reduced Error Pruning
where

Si is the set of those examples that are directed to safe node zi ;
Si = f h 2 f 1; : : : ; n g j wh = i g:
Q = f (W1 ; : : : ; Wn ).

Hence,

Moving any one example from one safe node to another (chang-

ing the value of any one argument
dition
of

f

Pn

wi ), can change one more safe node zi

jSij  r, one less safe node to fulll it, or both at the same time.

to fulll the conThus, the value

changes by at most 1. Hence, the function fullls the Lipschitz condition. Therefore,

we can apply McDiarmid's inequality to it by substituting

c2 = n:

i=1 i

ci = 1 and observing that then

2
Prf jf (W1 ; : : : ; Wn ) Ef (W1 ; : : : ; Wn )j   g  2e 2 =n ;
or equally

2
Prf jQ EQj   g  2e 2 =n :

Unfortunately, this concentration bound is not very tight. Nevertheless, combining the
concentration bounds for

Q and R we have for P

the following deviation from its expected

value.

Since jP
EP j = jQ R E(Q R)j = jQ EQ + ER Rj  jQ EQj + jR ERj,
jQ R E(Q R)j   implies that jQ EQj  =2 or jR ERj  =2. Thus,
Prf jP EP j   g = Prf jQ R E(Q R)j   g




 Pr jQ EQj  2 + Pr jR ERj  2

!

2
2n + 2 exp

 2 exp

!

2 (k 1=2)
:
4(k2 E2 R)

7. Related Work
Traditional pruning algorithms  like cost-complexity pruning (Breiman et al., 1984), pessimistic pruning (Quinlan, 1987), minimum error pruning (Niblett & Bratko, 1986; Cestnik
& Bratko, 1991), critical value pruning (Mingers, 1989a), and error-based pruning (Quinlan,
1993)  have already been covered extensively in earlier work (Mingers, 1989a; Esposito
et al., 1997; Frank, 2000). Thus we will not touch on these methods any further. Instead,
we review some of the more recent work on pruning.

rep

produces an optimal pruning of the given decision tree with respect to the pruning

set. Other approaches for producing optimal prunings have also been presented (Breiman
et al., 1984; Bohanec & Bratko, 1994; Oliver & Hand, 1995; Almuallim, 1996). However,
often optimality is measured over the training set. Then it is only possible to maintain the
initial accuracy, assuming that no noise is present. Neither is it usually possible to reduce
the size of the decision tree without sacricing the classication accuracy. For example, in
the work of Bohanec and Bratko (1994) it was studied how to eciently nd the optimal
pruning in the sense that the output decision tree is the smallest pruning which satises
a given accuracy requirement. A somewhat improved algorithm for the same problem was
presented subsequently by Almuallim (1996).

183

fiElomaa & Kriinen
The high level control of Kearns and Mansour's (1998) pruning algorithm is the same

cost-complexity

bottom-up sweep as in

rep.

However, the pruning criterion in their method is a kind of a

condition (Breiman et al., 1984) that takes both the observed classication

error and (sub)tree complexity into account.

Moreover, their pruning scheme does not

pessimistic

require the pruning set to be separate from the training set. Both Mansour's (1997) and
Kearns and Mansour's (1998) algorithms are
of a (sub)tree by its training error.

: they try to bound the true error

Since the training error is by nature optimistic, the

pruning criterion has to compensate it by being pessimistic about the error approximation.
Consider yet another variant of

rep, one which

is otherwise similar to the one analyzed

above, with the exception that the original leaves are not put to a special status, but can
be relabeled by the majority of the pruning examples just like internal nodes. This version
of

rep

produces the optimal pruning with respect to which the performance of Kearns and

Mansour's (1998) algorithm is measured. Their pessimistic pruning produces a decision tree
that is smaller than that produced by

rep.

Kearns and Mansour (1998) are able to prove that their algorithm has a strong performance guarantee. The generalization error of the produced pruning is bounded by that of
the best pruning of the given tree plus a complexity penalty.
local in the same sense as those of

rep

The pruning decisions are

and only the basic pruning operation of replacing a

subtree with a leaf is used in this pruning algorithm.

8. Conclusion
In this paper the

rep

algorithm has been analyzed in three dierent settings.

First, we

rep alone, without assuming anything about the input
this setting it is possible to prove that rep fullls its

studied the algorithmic properties of
decision tree nor pruning set.

In

intended task and produces an optimal pruning of the given tree. The algorithm proceeds
to prune the nodes of a branch as long as both subtrees of an internal node are pruned and
stops immediately if even one subtree is kept. Moreover, it prunes an interior node only if
all its descendants at level

d

have been pruned. Furthermore,

rep

either halts before the

safe nodes are reached or prunes the whole tree only in case all safe nodes have the same
majority class.
In the second setting the tree under consideration was assumed to t noise; i.e., it
was assumed that the class label of the pruning examples is independent of their attribute
values. In this setting the pruning probability of the tree could be bound by an equation
that depends exponentially on the size of the tree and linearly on the number and class
distribution of the pruning examples. Thus, our analysis corroborates the main nding of
Oates and Jensen (1999) that

rep fails to control the growth of a decision tree in the extreme

case that the tree ts pure noise. Moreover, our analysis opened a possibility to initially
explain why the learned decision tree grows linearly with an increasing data set. Our bound
on the pruning probability of a tree is based on bounding the probability that all safe nodes
have the same majority class. Surprisingly, essentially the same property, whose probability
we try to bound close to 0, is assumed to hold with probability 1 in the analysis of Oates
and Jensen (1999).
In

rep

it may happen that no pruning examples are directed to a given subtree. Such

subtrees have not been taken into account in earlier analyses.

184

In our nal analysis we

fiAn Analysis of Reduced Error Pruning
included empty subtrees in the equation for a tree's pruning probability.

Taking empty

subtrees into account gives a more realistic bound for the pruning probability of a tree.
Unfortunately, one cannot draw very denite general conclusions on the two-phased topdown induction of decision trees on the basis of analyses on the
bias is quite unique among pruning algorithms.

The fact that

rep algorithm, because its
rep does not penalize the

size of a tree, but only rests on the classication error on the pruning examples makes the
method sensitive to small changes in the class distribution of the pruning set. Other decision
tree pruning algorithms also have their individual characteristics. Therefore, unied analysis
of decision tree pruning may be impossible.
The version of

rep,

in which one is allowed to relabel original leaves, as well, is used

as the performance objective in Kearns and Mansour's (1998) pruning algorithm.

Thus,

the performance of pruning algorithms that use both error and size penalty is related to
those that use only error estimation. In the version of

rep

used by Kearns and Mansour

our analysis based on safe nodes applies with leaves in place of safe nodes. Hence for this
algorithm the derived bounds are stricter.
We leave the detailed analysis of other important pruning algorithms as future work.
Only through such investigation is it possible to disclose the dierences and similarities of
pruning algorithms.

Empirical examination has not managed to reveal clear performance

dierences between the methods.

Also, the relationship of the number of safe nodes and

leaves of a tree ought to be examined analytically and empirically. In particular, one should
study whether the number of safe nodes does increase linearly with a growing training set,
as conjectured in this paper. Deeper understanding of existing pruning algorithms may help
to overcome the problems associated with the pruning phase of decision tree learning.

References

Intelligence 83

Almuallim, H. (1996). An ecient algorithm for optimal pruning of decision trees.
,

Learning 15

, 347362.

Bohanec, M., & Bratko, I. (1994). Trading accuracy for simplicity in decision trees.
,

(3), 223250.

Regression Trees

Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984).
. Wadsworth, Pacic Grove, CA.

tional Joint Conference on Articial Intelligence

Machine

Classication and

Machine Learning 24
Proceedings of the Twelfth Interna-

Breiman, L. (1996). Some properties of splitting criteria.
Catlett, J. (1991). Overpruning large decision trees. In

Articial

,

(1), 4147.

, pp. 764769, San Mateo, CA. Morgan

Kaufmann.

Machine LearningEWSL-91: Proceedings of the Fifth European Working
Lecture Notes in Articial Intelligence

Cestnik, B., & Bratko, I. (1991). On estimating probabilities in tree pruning. In Kodrato,

Session

Y. (Ed.),

, Vol. 482 of

, pp. 138150, Berlin, Hei-

delberg, New York. Springer-Verlag.

Annals of Mathematical Statistics 23

Cherno, H. (1952). A measure of asymptotic eciency for tests of a hypothesis based on
the sum of observations.

,

185

(4), 493507.

fiElomaa & Kriinen

Proceedings of the Thirteenth International Joint Conference on Articial

Cohen, W. W. (1993).

Intelligence

systems. In

Ecient pruning methods for separate-and-conquer rule learning

, pp. 988994, San Mateo, CA. Morgan Kaufmann.

Machine Learning: ECML-93, Proceedings of
Lecture Notes in Articial Intelligence

Esposito, F., Malerba, D., & Semeraro, G. (1993).

the Sixth European Conference

the state space. In Brazdil, P. B. (Ed.),

Decision tree pruning as a search in

, Vol. 667 of

, pp.

165184, Berlin, Heidelberg, New York. Springer-Verlag.

IEEE Transactions on Pattern Analysis and Machine Intelligence 19
Pruning Decision Trees and Lists
Information Processing
Letters 33
Machine Learning 27
Proceedings of the Eleventh International Joint Conference on Articial
Intelligence
Proceedings of the Thirty-Fifth Annual
IEEE Symposium on Foundations of Computer Science

Esposito, F., Malerba, D., & Semeraro, G. (1997). A comparative analysis of methods for
pruning decision trees.
,

(5), 476491.

Frank, E. (2000).

. Ph.D. thesis, University of Waikato,

Department of Computer Science, Hamilton, New Zealand.

Hagerup, T., & Rb, C. (1990). A guided tour of Cherno bounds.
,

(6), 305308.

Helmbold, D. P., & Schapire, R. E. (1997). Predicting nearly as well as the best pruning of
a decision tree.

,

(1), 5168.

Holte, R. C., Acker, L., & Porter, B. (1989).

Concept learning and the problem of small

disjuncts. In

, pp. 813818, San Mateo, CA. Morgan Kaufmann.

Kamath, A., Motwani, R., Palem, K., & Spirakis, P. (1994).

Tail bounds for occupancy

and the satisability threshold conjecture. In

, pp. 592603, Los Alamitos,

CA. IEEE Press.

Proceedings of the Fifteenth Inter-

Kearns, M., & Mansour, Y. (1998). A fast, bottom-up decision tree pruning algorithm with

national Conference on Machine Learning

near-optimal generalization. In Shavlik, J. (Ed.),

, pp. 269277, San Francisco, CA. Morgan

Kaufmann.

Learning from

Malerba, D., Esposito, F., & Semeraro, G. (1996). A further comparison of simplication

Data: AI and Statistics V
Proceedings of the Fourteenth International Conference on Machine Learning

methods for decision-tree induction. In Fisher, D., & Lenz, H.-J. (Eds.),

, pp. 365374, Berlin, Heidelberg, New York. Springer-Verlag.

Mansour, Y. (1997). Pessimistic decision tree pruning based on tree size. In Fisher, D. H.
(Ed.),

,

pp. 195201, San Francisco, CA. Morgan Kaufmann.

Surveys in Combinatorics: Invited Papers of the 12th British Combinatorial Conference
Machine Learning 4
Machine Learning 3
Machine Learning

McDiarmid, C. J. H. (1989). On the method of bounded dierences. In Siemons, J. (Ed.),
, pp. 148188, Cambridge, U.K. Cambridge University Press.

Mingers, J. (1989a). An empirical comparison of pruning methods for decision tree induction.
,

(2), 227243.

Mingers, J. (1989b). An empirical comparison of selection measures for decision-tree induction.

Mitchell, T. M. (1997).

,

(4), 319342.

. McGraw-Hill, New York.

186

fiAn Analysis of Reduced Error Pruning
Motwani, R., & Raghavan, P. (1995).
New York.

Randomized Algorithms

. Cambridge University Press,

Research and Development in Expert Systems III

Niblett, T., & Bratko, I. (1986). Learning decision rules in noisy domains. In Bramer, M. A.
(Ed.),

, pp. 2534, Cambridge, UK.

Cambridge University Press.

Proceedings of the Fourteenth International Conference on

Oates, T., & Jensen, D. (1997). The eects of training set size on decision tree complexity.

Machine Learning

In Fisher, D. H. (Ed.),

, pp. 254261, San Francisco, CA. Morgan Kaufmann.

Oates, T., & Jensen, D. (1998). Large datasets lead to overly complex models: An expla-

Proceedings of the Fourth International Conference on Knowledge Discovery and Data
Mining
Proceedings of the Sixteenth National Conference on Articial Intelligence
nation and a solution.

In Agrawal, R., Stolorz, P., & Piatetsky-Shapiro, G. (Eds.),

, pp. 294298, Menlo Park, CA. AAAI Press.

Oates, T., & Jensen, D. (1999).

Toward a theoretical understanding of why and when

decision tree pruning algorithms fail. In

, pp. 372378, Menlo Park, CA/Cambridge, MA. AAAI

Press/MIT Press.

Proceedings of the Twelfth International Conference on Machine
Machine

Oliver, J. J., & Hand, D. J. (1995). On pruning and averaging decision trees. In Prieditis, A.,

Learning
Learning 5

& Russell, S. (Eds.),

, pp. 430437, San Francisco, CA. Morgan Kaufmann.

Pagallo, G., & Haussler, D. (1990). Boolean feature discovery in empirical learning.
,

(1), 7199.

Machine Learning 36

Pereira, F., & Singer, Y. (1999). An ecient extension to mixture techniques for prediction
and decision trees.

,

(3), 183199.

Machine Learning 1
International Journal of Man-Machine
C4.5: Programs for Machine Learning
The Annals of Probability

Quinlan, J. R. (1986). Induction of decision trees.

Studies 27

Quinlan, J. R. (1987).
,

,

, 81106.

Simplifying decision trees.

(3), 221248.

Quinlan, J. R. (1993).

.

Morgan Kaufmann, San

Slud, E. V. (1977). Distribution inequalities for the binomial law.

,

Mateo, CA.

5

(3), 404412.

187

fi