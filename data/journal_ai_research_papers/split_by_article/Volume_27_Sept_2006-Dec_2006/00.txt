Journal of Artificial Intelligence Research 27 (2006) 1-23

Submitted 02/06; published 09/06

A Variational Inference Procedure Allowing Internal
Structure for Overlapping Clusters and Deterministic
Constraints
Dan Geiger

dang@cs.technion.ac.il

Computer Science Dept., Technion,
Haifa, 32000, Israel

Christopher Meek

meek@microsoft.com

Microsoft Research, Microsoft Corporation,
Redmond, WA 98052, USA

Ydo Wexler

ywex@cs.technion.ac.il

Computer Science Dept., Technion,
Haifa, 32000, Israel

Abstract
We develop a novel algorithm, called VIP*, for structured variational approximate
inference. This algorithm extends known algorithms to allow efficient multiple potential
updates for overlapping clusters, and overcomes the difficulties imposed by deterministic
constraints. The algorithms convergence is proven and its applicability demonstrated for
genetic linkage analysis.

1. Introduction
Probabilistic graphical models are an elegant framework to represent joint probability distributions in a compact manner. The independence relationships between random variables
which are nodes in the graph are represented through the absence of arcs in the model.
This intuitively appealing presentation also naturally enables the design of efficient generalpurpose algorithms for computing marginal probabilities, called inference algorithms.
The general inference problem is NP-hard (Cooper, 1990; Dagum & Luby, 1993), and
although there are many cases where the model is small (or, more precisely, has a small
treewidth) and exact inference algorithms are feasible, there are others in which the time
and space complexity makes the use of such algorithms infeasible. In these cases fast yet
accurate approximations are desired.
We focus on variational algorithms: a powerful tool for efficient approximate inference
that offers guarantees in the form of a lower bound on the marginal probabilities. This
family of approaches aims to minimize the KL divergence between a distribution Q and the
target distribution P by finding the best distribution Q from some family of distributions
for which inference is feasible. In particular, we have a joint distribution P (X) over a set of
discrete variables X and our goal is to compute the marginal probability P (Y = y) where
Y  X. Further assume that this exact computation is not feasible. The idea is to replace
P with a distribution Q which can be used to compute a lower bound for P (Y = y). We
c
2006
AI Access Foundation. All rights reserved.

fiGeiger, Meek & Wexler

let H = X \ Y . Then, by using Jensens inequality we get the following bound:
log P (y) = log

X
h

Q(h)

P (y, h) X
P (y, h)

Q(h) log
= D(Q(H) || P (Y = y, H))
Q(h)
Q(h)
h

where D( || ) denotes the KL divergence between two probability distributions. The quantity D(Q || P ) is often called the free-energy where P and Q are possibly un-normalized
distributions. Variational techniques aim to choose a distribution Q such that the lower
bound is as high as possible, or equivalently, such that the KL divergence between Q(h)
and P (h|Y = y) is minimized.
Variational approaches such as the mean field, generalized mean field, and structured
mean field differ only with respect to the family of approximating distributions that can
be used, with the structural mean field approach subsuming the remaining approaches as
special cases. The research of several authors guided our work: Saul & Jordan (1996),
Ghahramani & Jordan (1997), Wiegerinck (2000) and Bishop & Winn (2003).
The contributions of this paper are threefold. First we develop an extension to the
algorithm by Wiegerinck (2000), which we call vip? , that allows for a set potentials of the
approximating distribution Q to be updated simultaneously even if the clusters of Q overlap. Algorithm vip? is N -fold faster than Wiegerincks algorithm for N N grid-like models
and yields two orders of magnitude improvement for large graphs such as genetic linkage
analysis model of large pedigrees. Note that simultaneous updates were first presented for
phylogenic trees by Jojic et al. (2004). Second, we prove the convergence of vip? and of previous variational methods via a novel proof method, using properties of the KL divergence.
Third, we extend vip? to allow deterministic constraints in the model and demonstrate the
applicability of this extension to genetic linkage analysis.

2. Background
This background section is based primarily on the paper by Weigerinck (2000), which in
turn builds on pioneering works such as the papers of Saul & Jordan (1996) and Ghahramani
& Jordan (1997). Our review provides a new exposition of this material.
We denote distributions by P (x) and Q(x) and related un-normalized distributions by
P (x)  P (x) and Q(x)  Q(x). Let
X be a finite set of variables and x be an instantiation
1 Q
of these variables. Let P (x) = ZP i i (di ) where di is the projection of the instantiation
x to the variables in Di  X and where i is a non-negative function, commonly called a
potential. The constant ZP normalizes the product of potentials and the subsets {Di }Ii=1
are allowed to overlap. We often suppress the arguments of a potential and of a distribution,
using i instead of i (di ) and P instead of P (X).
Our goal is to find a distribution Q that minimizes Q
the KL divergence between Q and P .
We further constrain Q to be of the form Q(x) = Z1Q j j (cj ) where ZQ is a normalizing
constant and where C1 , . . . , CJ are possibly overlapping subsets of X, which we call clusters. Finding an optimum Q, however, can be difficult. A more modest and common goal
is devising iterative converging algorithms such that in each iteration the KL divergence
between an approximating distribution Q and P decreases unless Q is a stationary point.
Throughout, we define Q(w|u) = |W1\U | for instantiations U = u for which Q(u) = 0.
Consequently, all terms in the equality Q(w, u) = Q(u)Q(w|u) are well defined even if
2

fiA Variational Inference Procedure

P
Q(u) = 0. Moreover, this convention maintains properties such as
W \U Q(w|u) = 1
1
1
1
and Q(w, z|u) = Q(w|z, u)Q(z|u) = |W \{U Z}|  |Z\U | = |{W Z}\U | . We also note that
Q(x) log Q(x) = 0 whenever Q(x) = 0 and thus the KL divergence
D(Q || P ) =

X

Q(x) log

x

Q(x)
P (x)

is not finite if and only if P (x) = 0 and Q(x) > 0 for some instance x.
Our starting point is the algorithm developed by Wiegerinck (2000). The algorithm finds
such a distribution Q as follows: it iterates over the clusters Cj and their instantiations cj
to update the potentials j (cj ) = ej (cj ) via the following update equation:

j (cj )  

X

X

Q(ck |cj ) log k (ck ) +

{k:gkj =1} Ck \Cj

X

X

Q(di |cj ) log i (di )

(1)

{i:fij =1} Di \Cj

where gkj and fij are two indicator functions defined via gkj = 0 if Q(Ck |cj ) = Q(Ck ) for
every instance cj of Cj and 1 otherwise, and fij = 0 if Q(Di |cj ) = Q(Di ) for every instance
cj of Cj and 1 otherwise. Wiegerinck (2000) proved convergence of this algorithm to a stationary point using Lagrangians. Throughout we call this iterative procedure, Wiegerincks
algorithm.
Wiegerincks algorithm relies at each step on an algorithm to compute the conditional
probabilities Q(ck |cj ) and Q(di |cj ) from an un-normalized distribution Q represented by
a set of potentials j (cj ). This can be accomplished by any inference algorithm such as
bucket elimination algorithm or the sum-product algorithm described by Dechter (1999)
and
Q
Kschischang, Frey & Loeliger (2001) . It is important to note that for Q(x) = j j (cj )
the computation of these conditionals is not affected by multiplying any j by a constant
.
Wiegerincks algorithm generalizes the mean field (MF) algorithm and the generalized
mean field (GMF) algorithm (Xing, Jordan & Russell, 2003, 2004). The mean field algorithm is the special case of Wiegerincks algorithm in which each Cj contains a single
variable. Similarly, the generalized mean field algorithm is the special case in which the Cj
are disjoint subsets of variables. When Cj are disjoint clusters, the formula for j in Eq. 1
simplifies to the GMF equations as follows (first term drops out):
j (cj ) 

X

X

Q(di |cj ) log i (di ).

(2)

{i:fij =1} Di \Cj

The term Q(di |cj ) can be made more explicit when Cj are disjoint clusters (Bishop & Winn
2003). In particular, the set Di \ Cj partitions into Dik = (Di \ Cj )  Ck Q
for k = 1, . . . , J
where k 6= j. Note that Dik = Di  Ck . Using this notation, Q(di |cj ) = k Q(dki ) where
Q(dki ) = 1 whenever Dik = . This factorization further simplifies the formula for j as
follows:
X
X X
j (cj ) 
Q(d1i ) . . .
Q(dJi ) log i (di ).
(3)
{i:fij =1} Di1

DiJ

3

fiGeiger, Meek & Wexler

This simplification is achieved automatically when using bucket elimination for computing
j . The iterated sums in Eq. 3 are in fact the buckets formed by bucket elimination when
Cj are disjoint.
Eq. 1 requires repeated computation of the quantities Q(ck |cj ) and Q(di |cj ). This repetition can be significant because there could be many indices k such that Q(Ck |cj ) 6= Q(Ck ),
and many indices i such that Q(Di |cj ) 6= Q(Di ). As these computations share many subcomputations it is therefore reasonable to add a data structure to facilitate a more efficient
implementation for these function calls. In particular, it is possible to save computations if
the sets C1 , . . . , CJ form a junction tree.
A set of clusters C1 , . . . , CJ forms a junction tree iff there exists a set of trees JT having one node, called Cj , for each cluster of variables Cj , and for every two nodes Ci and
Cj of JT, which are connected with a path in JT, and for each node Ck on this path,
Ci  Cj  Ck holds. By a set of trees we mean an undirected graph, not necessarily
connected, with no cycles. Note that this definition allows a junction tree to be a disconnected graph.
When
Q
Q C1 , . . . , CJ form a junction tree, Q(x) has the decomposable form
Q(x) = j j (cj )/ e e (se ), where j are marginals on the subsets Cj of X, and where
e are the marginals on intersections Se = Ci  Cj , one for each two neighboring clusters
in the junction tree (Jensen 1996).
Wiegerinck (2000) enhanced his basic algorithm so that it maintains
a consistent
junction
P
P
tree JT for the distribution Q(x). Consistency means that Cj \Ck j = Ck \Cj k for
every two clusters. In a consistent junction tree, each potential j (Cj ) is proportional to
Q(Cj ). An update of a potential during the algorithm may yield an inconsistent junction
tree, however, consistency is maintained by applying DistributeEvidence(j ) (Jensen
1996) after each update of a potential. The procedure DistributeEvidence(0j ) accepts
as input a consistent junction tree and a new cluster marginal 0j for Cj , and updates the
potential of every neighboring cluster Ck of Cj via
Cj \Ck

0j (cj )

Cj \Ck

j (cj )

P
0k (ck )

 k (ck ) P

(4)

and each neighboring cluster recursively propagates the update by applying Eq. 4 to all its
neighbors except the one from which the update came. The output of this procedure is a
consistent junction tree, having the same clusters, where 0j is the (possibly un-normalized)
marginal probability of Q on Cj , and where the conditional probability Q(X|Cj ) remains
unchanged (Jensen 1996, pp. 74).
Wiegerincks enhanced algorithm, which uses a junction tree, iteratively updates the
potential of each cluster (node in the junction tree), using the potentials of other clusters
and separators. However, since the junction tree may not be consistent after the update,
the algorithm applies the procedure DistributeEvidence(j ) to the junction tree, after
each update. Note that our description omits a normalization step in Wiegerinck (2000)
that is not needed for convergence.
The most time consuming computation in variational algorithms is computing conditional probabilities of the form Q(ck |cj ) and Q(di |cj ). We distinguish among these conditional probabilities as follows.

4

fiA Variational Inference Procedure

Definition: A conditional probability Q(A|cj ) is subsumed by Q if the set of target variables A is a subset of some cluster Ck of Q (i.e., (A \ Cj )  Ck ).
Wiegerincks enhanced algorithm has substantial computational benefits when the conditional probabilities are subsumed. In such cases the needed quantities in Eq. 1, Q(di |cj )
and Q(ck |cj ), are obtained by a mere lookup in the junction tree, and only one call to
DistributeEvidence is made for each update.
Weigerincks basic and enhanced algorithms do not assume any structure for j , namely,
the algorithms hold tables j with an explicit entry for every instantiation of Cj . Since
the computations Q(ck |cj ) and Q(di |cj ) grow exponentially in the size of Di and Ck , the
algorithms become infeasible for large cliques or clusters. For simplification, additional
structure to j was suggested by Wiegerinck (2000, Section 4) of the form,

j (cj ) =

nj
Y

jl (cjl ),

(5)

l=1

where the sets Cjl , l = 1, . . . , nj , are possibly overlapping subsets of Cj , and cjl is the
projection of the instantiation cj on the variables in Cjl . Using such structure it is sufficient
to hold tables for the subsets Cjl which are considerably smaller. Note that when j has
an entry to each instantiation cj , then nj = 1 and j (cj ) = j1 (cj1 ). Weigerinck uses this
structure for the potentials j under the following assumptions:
Definition [SelfQ
compatibility]: A distribution Q with clusters Cj and subsets Cjl of the
1
form Q(x) = ZQ j j (cj ), with clusters that factor according to Eq. 5, is self compatible
if for every Cj and Ck the set of indices Njk = {l : Q(Ck |cj ) = Q(Ck |cjl )} is non-empty
regardless of the values of the potentials j , where cj is an arbitrary instantiation of Cj
and cjl is the projection of cj on Cjl .
Definition [Compatibility
wrt P ]: A distribution Q with clusters Cj and subsets Cjl of
Q
the form Q(x) = Z1Q j j (cj ), with clusters that factor according to Eq. 5, is compatible
Q
wrt a distribution P with sets Di of the form P (x) = Z1P i i (di ) if for every Di and Cj
the set of indices Mij = {l : Q(Di |cj ) = Q(Di |cjl )} is non-empty, where cj is an arbitrary
instantiation of Cj and cjl is the projection of cj on Cjl .
Note that self-compatibility and compatibility wrt P depend on the form of Q and not on
a particular realization of the potentials j .
Under these assumptions Weigerinck states that considerable simplifications can be deduced, and provides some examples for this statement.
We note that the algorithms of Bishop & Winn (2003) and Jojic et al. (2004) use a
stronger assumption that the clusters Cj of the approximating distribution Q are disjoint
and that Q(Ck |cj ) = Q(Ck ). This assumption, which implies that Q(Ck |cj ) = Q(Ck |cjl )
and Q(Di |cj ) = Q(Di |cjl ) for every index l, is relaxed by requiring each of these equalities
to hold for a single index l (but possibly for multiple indices).
5

fiGeiger, Meek & Wexler

3. Multiple Potential Update using Overlapping Clusters
In this section we develop a new algorithm, called vip? , that uses the additional structure
of potentials offered by Eq. 5 to speed up computations. In particular, rather than updatnj
ing each potential jl separately, we offer a way to update the set of potentials {jl }l=1
simultaneously, saving considerable computations. Furthermore, this simultaneous update
is enhanced by using a junction tree, despite the fact that the sets {Cjl } need not form a
junction tree, and only {Cj } form a junction tree.
Our algorithm uses the definitions of self compatibility and compatibility wrt P , defined
earlier, and the following definition of indices.
Definition: Let the indicator function gjk (l) equal 1 for a single fixed index l  Njk and
0 for all other indices in Njk when Q(Ck |cj ) 6= Q(Ck ), and equal 0 otherwise. Let the
indicator function fij (l) equal 1 for a single fixed index l  Mij and 0 for all other indices
in Mij when Q(Di |cj ) 6= Q(Di ), and equal 0 otherwise.
Algorithm vip? is given in Figure 1. Its convergence is proved in Section 4. The proof
requires Q to be self-compatible, compatible wrt P , and in addition, to satisfy (P (x) = 0) 
(Q(x) = 0). Note that D(Q || P ) =  for distributions Q which do not satisfy the last
assumption.
The main improvement of the algorithm is an efficient update of potentials. For potentials j that factorize into smaller potentials jl according to Eq. 5, algorithm vip? only
updates jl instead of updating the whole potential j , as done in Weigerincks algorithms.
The update of the potentials jl as done by vip? is equivalent to updating j according to
Eq. 1, up to an irrelevant constant, but does not require to compute the update equation for
each instance of the cluster Cj . The proposed change considerably speeds up the previous
algorithms.
The algorithm gets as input a target distribution P with sets Di of the form P (x) =
1 Q
i i (di ) and an approximating distribution Q with clusters Cj which is self-compatible,
ZP
compatible wrt P and satisfies
the condition (P (x) = 0)  (Q(x) = 0). Distribution Q is
Q
of the form Q(x) = Z1Q j j (cj ) where the potential of every cluster Cj factors according
Qnj
jl (cjl ) and the clusters form a consistent junction tree. The algorithm
to j (cj ) = l=1
iterates over the clusters, updating the potential of every instantiation of each of the subsets
Cjl according to Eq. 6. To apply the update equation, the quantities Q(di |cjl ) are computed
via variable propagation (Jensen, pp 69-80) on the junction tree. When these quantities are
subsumed, they are obtained by a mere lookup in the junction tree. Then, after updating
the potentials of all subsets Cjl for a cluster Cj , procedure DistributeEvidence is applied
once to make the junction tree consistent with respect to j . Since the clusters Cj form a
junction tree only via their subsets Cjl , Eq. 4 is replaced with Eq. 7. After convergence,
algorithm vip? outputs the approximating distribution Q with its revised potentials.
Example 1 The target distribution P is an N N grid of pairwise potentials (see Figure 2a)
and the approximating family is defined by a single row and the set of columns in the grid,
each augmented with edges to the middle vertex (see Figure 2b) where C7 is a row of the
grid and Ci (i = 1, . . . , N = 6) are the columns. Using the notation Xi,j to denote the
vertex at row i and column j in the grid, cluster C7 is associated with N  1 subsets
6

fiA Variational Inference Procedure

Algorithm VIP? (Q,P)
Q
Q
Input: Two probability distributions P (x) = Z1P i i (di ) and Q(x) = Z1Q j j (cj ) where
Qnj
the initial potentials j (cj ) = l=1
jl (cjl ) form a consistent junction tree, and where Q is
self-compatible, compatible wrt P , and satisfies (P (x) = 0)  (Q(x) = 0).
Output:
A revised set of potentials jl (cj ) defining a probability distribution Q via Q(x) 
Q

(c
j,l jl jl ) such that Q is a stationary point of D(Q || P ).
Iterate over all clusters Cj until convergence
Step 1.
For l = 1, . . . , nj :
For every instantiation cjl of Cjl apply the following update equation:
jl (cjl )  

X

X

X

Q(ck |cjl ) log k (ck ) +

{k:gjk (l)=1} Ck \Cjl

X

Q(di |cjl ) log i (di ) (6)

{i:fij (l)=1} Di \Cjl

jl (cjl )  ejl (cjl )
Note: Q(di |cjl ) is computed via variable propagation (Jensen, pp 69-80) on the junction
tree JT. However, when these quantities are subsumed, they are obtained by a mere lookup
in JT.
Step 2. Make JT consistent with respect to j :

DistributeEvidence(JT, j )

DistributeEvidence(JT, 0j )
Input: A junction tree JT with nodes Ck and potentials k (ck ) =
node Cj with a revised potential 0j .
Output: A consistent junction tree.

Qnk

l=1 kl (ckl ).

A starting

initialization source(j)  0; updated {j}
While (updated6= )
  first element in updated; updated updated\{}
For all neighboring nodes Ck of C in JT such that k 6= source()
P
Qn 0
C
\C
l=1 l (cl )
0km (ckm )  km (ckm ) P  k Qn
C \Ck
l=1 l (cl )
for a single subset m of Ck for which (C  Ck )  Ckm
source(k)  
updated updated{k}
Figure 1: Algorithm vip?
7

(7)

fiGeiger, Meek & Wexler

(a)

(b)

(c)

Figure 2: (a) Grid-like P distribution (b) & (c) Approximating distributions Q.
C7l = {X3,l , X3,l+1 }. Each column cluster Cj is associated with 2N-4=8 subsets Cjl from
which Cjl = {Xl,j , Xl+1,j } for N-1 subsets (l = 1, . . . , 5), Cjl = {X1,j , X3,j } for l = N , and
Cjl = {XlN +4,j , X3,j } for each of the additional N-4 subsets (l = 7, 8).
This choice induces a self-compatible approximating distribution Q; every column cluster
Cj is independent of another cluster given a subset that contains X3,j (such as Cj2 ). In
addition, the row cluster C7 is independent of every column cluster Cj given C7j . The
induced distribution is also compatible wrt P ; for each vertical edge Dv = {Xi,j , Xi+1,j }
of P , distribution Q satisfies Q(Dv |ck ) = Q(Dv |ck2 ) for a column cluster Ck such that
k 6= j, and Q(Dv |c7 ) = Q(Dv |c7j ). In addition, for each horizontal edge Dh = {Xi,j , Xi,j+1 }
in P , distribution Q satisfies Q(Dh |c7 ) = Q(Dh |c7j ), and Q(Dh |ck ) = Q(Dh |ck2 ) for k 6=
j, j + 1. Finally, for the edge Dh and k = j, j + 1, the approximating distribution satisfies
Q(Dh |ck ) = Q(Dh |ckl ) where Ckl = {Xi,k , X3,k }, due to the additional N  3 edges added
to each column cluster.
Like Wiegerincks enhanced algorithm, algorithm vip? has substantial computational
benefits when the conditional probabilities Q(di |cjl ) are subsumed. In such cases the needed
quantities, Q(di |cjl ) and Q(ck |cjl ), are obtained by a mere lookup in the junction tree in
step 1 of the algorithm, and only one call to DistributeEvidence is made in step 2,
as demonstrated in the next paragraph. The computational efficiency of vip? is achieved
even if the quantities Q(di |cjl ) are not subsumed but only factor to subsumed probabilities.
Disjoint clusters is one such special case, in which the quantities Q(di |cjl ) can factor into
subsumed probabilities Q(dki |cjl ), where Dik = Di  Ck , which are obtainable by lookup in
a junction tree.
Consider Example 1 to compare the computational cost of vip? versus Wiegerincks
basic and enhanced algorithms. Assume Wiegerincks basic algorithm (Eq. 1), uses the
distribution Q given in Figure 2c, with 35 clusters Cj0 and 60 sets Di . Therefore, when a
junction tree is not used, 4  (60 + 34) = 376 conditionals are computed for each cluster Cj0
(edge) not on the boundary of the grid, 94 for each of the four possible values for the edge
cluster Cj0 . Clearly, if additional clusters are introduced, as shown for example by Figure 2b,
then the computational cost grows. By using a junction tree, as done by Wiegerincks enhanced algorithm, the subsumed conditional probabilities, which are computed separately
by Wiegerincks basic algorithm, are computed with a single call to DistributeEvidence.
These computation covers all subsets in Figure 2c. The only conditionals that are not sub8

fiA Variational Inference Procedure

(a)

(b)

(c)

Figure 3: The target distribution P is a grid of pairwise potentials as in (a). Two different
partitions of the grid into clusters are shown in Figures (b) and (c), both contain
the same subsets.

sumed are Q(di |cj ) of horizontal edges Di that are not contained in a single cluster, namely,
edges in 2a but not in 2c. These factor to two subsumed probabilities, one computed by the
single call described earlier and the other requires a second call to DistributeEvidence.
For example, let Di = {X1 , X2 } be a horizontal edge in P which does not overlap with Cj0 ,
then Q(x1 , x2 |c0j ) = Q(x1 |c0j , x2 )Q(x2 |c0j ). The two conditionals are subsumed, but a second
call to DistributeEvidence is needed to obtain Q(x1 |c0j , x2 ). This yields 25 calls to DistributeEvidence. However, in Example 1, one call to DistributeEvidence is sufficient
to compute the conditionals for two adjacent horizontal edges, yielding the need only for
15 calls. Therefore, since there are 4  15 = 60 calls to DistributeEvidence, and since
the cost of the junction tree algorithm is typically twice the cost of computing conditional
probabilities without using a junction tree, this yields a 3-fold speedup for Wiegerincks
enhanced algorithm versus Wiegerincks basic algorithm. For edges on a boundary, the
speedup factor is less than 3. As the size of the grid grows, a smaller fraction of the edges
are on the boundary, and, thus, the speedup approaches a 3-fold speedup.
A significant speedup is obtained by using algorithm vip? with clusters Cj and subsets
as described in Figure 2b. Note that the additional subsets are needed to meet the compatibility assumption of vip? . Algorithm vip? makes one call to DistributeEvidence
per cluster Cj for each non-subsumed conditional, rather than for every edge cluster Cj0 .
Since vip? uses N + 1 clusters Cj , the speedup when compared to Wiegerincks enhanced
algorithm approaches N as the the N  N grid grows. This O(N ) speedup is confirmed in
the experiments section (Figure 9).
Another potential benefit of vip? is the possibility of alternating between different
choices of clusters which contain identical subsets Cjl . A simple example is the grid in
Figure 3a, where two such choices are illustrated in Figures 3b and 3c. The two sets of clusters update of the potentials j differently and therefore can yield better approximations as
the distance D(Q || P ) is reduced with every alternation. In general, we can iterate through
a set of choices for clusters and execute vip? for one choice using as initial potentials the
potentials jl found for an earlier choice of clusters. The practical benefit of this option of
added flexibility remains to be tested in an application.
9

fiGeiger, Meek & Wexler

4. Proof of Convergence
We develop several lemmas that culminate with a proof of convergence of algorithm vip? .
Lemma 1 states that for two un-normalized probability distributions P (x) and Q(x) the
KL divergence is minimized when Q(x) is proportional to P (x). Lemma 2 rewrites the
KL divergence D(Q || P ) in terms of the potentials P
of Q and P using the quantity j (cj )
which, according to Lemma 3, differs from j (cj ) = l jl (cjl ) only by a constant. Finally,
Theorem 1 asserts that the KL divergence between Q and P decreases with each iteration
of Algorithm vip? unless Q is a stationary point. The proof exploits the new form of
D(Q || P ) provided by Lemma 2, and replaces the term j (cj ) with the terms jl (cjl ) used
in the update equation of vip? . A final observation, which uses Lemma 1, closes the proof
showing that when the potentials are updated as in algorithm vip? , the KL divergence is
minimized wrt j .
The first lemma provides a variant of a well known property of KL. Recall that for
every two probability distributions Q(x) and P (x), the KL divergence D(Q(x) || P (x))  0
and equality holds if and only if Q(x) = P (x) (Cover & Thomas 1991; Theorem 2.6.3). A
similar result holds also for un-normalized probability distributions.
Lemma 1 Let Q(x) and P (x) be non-negative functions such that
let
Q(x) =
D(Q(x) || P (x))
P min
{Q|

x

P

x P (x)

= ZP > 0, and

Q(x)=ZQ }

where ZQ is a positive constant. Then Q(x) =

ZQ
ZP P (x).

Proof. We observe that
P (x)
D(Q(x) || P (x)) = ZQ  D( Q(x)
ZQ || ZP ) + ZQ log

ZQ
ZP

which implies, using the cited result about normalized distributions, that the minimum is
P (x)
obtained when Q(x)
ZQ = ZP , yielding the desired claim. 
The next lemma rewrites the KL divergence so that an optimizing update equation for
cluster Cj becomes readily available.
Lemma 2 Let P (x) =
tions. Then,

1
ZP

Q

i i (di )

D(Q || P ) =

X
Cj

and Q(x) =

Q(cj ) log

1
ZQ

Q

j

j (cj ) be two probability distribu-

j (cj )
+ log(ZP )  log(ZQ )
j (cj )

(8)

where j (cj ) = ej (cj ) , and where
j (cj ) = 

X X

Q(ck |cj ) log k (ck ) +

X X
i

k Ck \Cj

10

Di \Cj

Q(di |cj ) log i (di )

(9)

fiA Variational Inference Procedure

Proof: Recall that
D(Q || P ) =

X

Q(x) log

X

Q(x)
=  [H(Q) + EQ [log P (x)]]
P (x)

(10)

where H(Q) denotes the entropy of Q(x) and EQ denotes expectation with respect to Q.
The entropy term can be written as
X X
H(Q) = 
Q(cj )Q(x|cj ) [log Q(cj ) + log Q(x|cj )]
Cj X\Cj

=

P

Cj

Q(cj ) log Q(cj ) 

P

Cj

Q(cj )

P

X\Cj

Q(x|cj ) log Q(x|cj ).

This is a variation of a well known form of H(Q) which is derived by splitting
summation
P
over X into summation over Cj and X \ Cj , and using the fact that Q(cj ) X\Cj Q(x|cj ) =
Q(cj ) which holds for every distribution. To split the sum over X \ Cj for Q(cj ) > 0 we use
1 Q
X
k k (ck )
ZQ
=
log k (ck )  log Q(cj )  log(ZQ )
log Q(x|cj ) = log
Q(cj )
k

Thus,
X

Q(x|cj ) log Q(x|cj ) =

X\Cj

P P

P
Q(ck |cj )Q(x|ck , cj ) log k (ck )  X\Cj Q(x|cj ) [log Q(cj ) + log(ZQ )]
P
and by using Q(ck , cj ) X\{Ck Cj } Q(x|ck , cj ) = Q(ck , cj ) this term is further rewritten as
X
H(Q) = 
Q(cj ) log Q(cj )
k

X\Cj

Cj



P

Cj Q(cj ) 

hP

k6=j

P

Ck \Cj Q(ck |cj ) log k (ck ) + log

j (cj )
Q(cj )

i

+ log(ZQ )

Note that when Q(cj ) = 0 the bracketed term is multiplied by zero, and due to the equality
0 log 0 = 0, the product is also zero.

The second term of Eq. 10 is similarly written as
XX
X
EQ [log P (x)] =
Q(cj )
Q(x|cj ) log i (di )  log(ZP )
i

=

X

Q(cj )

Cj

X X
i

Cj

(11)

X\Cj

Q(di |cj ) log i (di )  log(ZP )

Di \Cj

Hence Eq. 10 is rewritten as
X
D(Q || P ) =
Q(cj ) log Q(cj )
Cj




X
Cj

Q(cj ) 


X X

Q(ck |cj ) log k (ck ) +

X X
i

k Ck \Cj

11

Di \Cj

Q(di |cj ) log i (di )

fiGeiger, Meek & Wexler



X
Cj

Q(cj ) log

Q(cj )
+ log(ZP )  log(ZQ )
j (cj )

Denoting the bracketed term by j (cj ), and letting j (cj ) = ej (cj ) , we get
D(Q || P ) =

X
Cj

Q(cj ) log

j (cj )
+ log(ZP )  log(ZQ ).
j (cj )



P
The next lemma shows that j (cj ), defined in Eq. 9, and j (cj ) = l jl (cjl ), used to
update potentials of Q in vip? , differ only by an additive constant which does not depend
on cj . As argued in Theorem 1, the fact that this difference is a constant enables vip? to
use the latter form, which is a more efficient representation.
Q
Q
Q
Lemma 3 Let P (x) = Z1P i i (di ) and Q(x) = Z1Q j j (cj ) where j (cj ) = l jl (cjl ),
be two probability distributions such that Q is self-compatible and compatible wrt P . Let
X
X
X
X
jl (cjl ) = 
Q(ck |cjl ) log k (ck ) +
Q(di |cjl ) log i (di ) (12)
{k:gjk (l)=1} Ck \Cjl

{i:fij (l)=1} Di \Cjl

Then, the difference between j (cj ) defined by Eq. 9 and j (cj ) =
that does not depend on cj .

P

l

jl (cjl ) is a constant

P
Proof: We first argue that each term of the form Ck \Cj Q(ck |cj ) log k (ck ) and each term
P
of the form Di \Cj Q(di |cj ) log i (di ) in Eq. 9 that depends on cj appears exactly once for
a single subset Cjl in Eq. 12. We then argue that every term in Eq. 12 appears once in
Eq. 9.
Since Q is self-compatible, it follows that for every cluster Ck that depends on Cj the
function gkj (l) equalsPone for a single subset Cjl , namely Q(ck |cj ) = Q(ck |cjl ), in which
case the expression
Ck \Cj Q(ck |cjl ) log k (ck ) appears in Eq. 12. Similarly, since Q is
compatible wrt P it follows that for every set Di that depends on Cj the function fij (l)
equals
one for a single subset Cjl , namely Q(di |cj ) = Q(di |cjl ), in which case the expression
P
Di \Cj Q(di |cjl ) log i (di ) appears in the second term of Eq. 12.
It remains to show that every term in Eq. 12 appears once in Eq. 9. Since Q is selfcompatible, it is implied that Ck  Cj  Cjl and thus summing over Ck \ Cj is equivalent
to summing over Ck \ Cjl . Therefore, for every k such that gjk (l) = 1, the first term of
Eq. 12 appears once in Eq. 9. Similarly, since Q is compatible wrt P , it is implied that
Di  Cj  Cjl and thus summing over Di \ Cj is equivalent to summing over Di \ Cjl and
therefore for every i such that fij (l) = 1 the second term of Eq. 12 appears once in Eq. 9.


Theorem 1 (Convergence of vip? ) Let the initial approximating distribution Q be selfcompatible and compatible wrt a given distribution P , and assume that (P (x) = 0) 
(Q(x) = 0). Then, the revised distribution Q retains these properties, and in each iteration
of Algorithm vip? the KL divergence between Q and P decreases unless Q is a stationary
point.
12

fiA Variational Inference Procedure

Q
Proof. Let Q(x) = Z1Q jl jl (cjl ) where jl (cjl ) = ejl (cjl ) . We need to show that
0
?
at the start
Q of0 each iteration of vip the function Q defined by the revised potentials
0
j (cj ) = l jl (cjl ) is a probability distribution with the listed properties and that it is
closer to P in KL divergence than Q at the start of the previous iteration.
First, we show that Q0 maintains the properties listed in the theorem throughout the
updates done in vip? . The properties of self-compatibility and compatibility wrt P are
derived from the form of Q and thus are not affected by the updates done in vip? . For
the property
(P (x) = 0)  (Q(x) = 0), consider an instance x for which P (x) = 0. Since
1 Q
P (x) = ZP i i (di ) there exists a potential i of P for which i (di ) = 0, where di is the
Q
projection of x on the set Di . Since Q(x) = 0 and Q(x) = Z1Q jl jl (cjl ) there exists a
subset Cjl for which Q(cjl ) = 0, where cjl is the projection of x on Cjl . Algorithm vip?
1
by convention,
updates jl (cjl ) to  because log i (di ) =  and Q(di |cjl ) = |Di \C
jl |
0
yielding Q (x) = 0, as claimed.
Now, we show that no additional zeroes are introduced into Q whenever (P (x) = 0) 
(Q(x) = 0). Hence, the normalizing constant ZQ > 0 and therefore the revised Q0 is a
probability distribution. For instances cjl for which Q(cjl ) > 0 the terms Q(ck |cjl ) log (ck )
and Q(di |cjl ) log (di ) are finite as long as (P (x) = 0)  (Q(x) = 0). This implies jl (cjl )
is updated to a positive value and thus, no additional zeroes are introduced into Q.
Using the given form of Q, we have


1 X Y
Q(cj ) =
k (ck ) j (cj ).
(13)
ZQ
X\Cj k6=j

We denote the bracketed coefficient of j (cj ) by Bj (cj ) and note that it is constant in the
sense that it does not depend on the quantity j being optimized.
We now use Eq. 13 to rewrite the KL divergence as justified by Eq. 8 in Lemma 2:


j (cj )Bj (cj ) 
1 X
D(Q || P ) =
j (cj )Bj (cj ) log
+ log(ZP )  log(ZQ ).
(14)
ZQ
j (cj )Bj (cj )
Cj

Due to Lemma 3, the distance D(Q || P
j (cj )
P) only changes by a constant when replacing

(c
)
?
j
j
with j (cj ) = e
, where j (cj ) = l jl (cjl ) as computed via Eq. 6 of vip . Note that
j (cj ) does not depend on (cj ) and is a function of Q(x) only through the conditional
distribution of X \ Cj given Cj (via Q(ck |cj )). Hence, Lemma 1 states that the minimum
of D(Q || P ) wrt j is achieved when j (cj ) is proportional to j (cj ). As the potential j
is only held implicitly through the partial potentials jl , step 1 of vip? updates j (cj ) via
Eq. 6 to be proportional to j (cj ) by setting each potential jl (cjl ) to be proportional to
jl (cjl ). The proportionality constant does not matter because if j is multiplied by , and
the arbitrary constraining constant ZQ is also multiplied by , these influences cancel in
Eq. 14. For simplicity, the algorithm uses  = 1 and therefore j (cj )  ej (cj ) . Algorithm
vip? implicitly computes j (cj ) according to this formula and hence decreases D(Q || P ) at
each iteration by improving j (cj ) while holding all other cluster potentials fixed. Since
the KL divergence is lower bounded by zero, vip? converges. 

13

fiGeiger, Meek & Wexler

The properties of Q required by Theorem 1 of self-compatibility and compatibility wrt
P are derived from the form of Q and are satisfied by setting the clusters Cj appropriately.
In addition, the condition (P (x) = 0)  (Q(x) = 0) is trivially satisfied for strictly positive
distributions P .
Note that the difference between j (cj ) defined by Eq. 9 and j (cj ) defined by Eq. 1 is
a constant that does not depend on cj . Consequently, our convergence proof also applies to
Wiegerincks algorithm, because this algorithm is a special case of vip? where every cluster
Cj has a single subset Cj1 = Cj .

5. Handling Deterministic Potentials
When the distribution P is not strictly positive the property (P (x) = 0)  (Q(x) = 0)
must hold for the convergence proof of vip? to apply. In this section we provide a sufficient
condition for Q to retain this property.
Definition: An instantiation W = w is feasible (wrt to a distribution P ) if P (W = w) > 0.
Otherwise, the instantiation is infeasible.
Q
Definition: A constraining set wrt a distribution P (x) = Z1P i i (di ) with sets Di is a
minimal set of variables   Di which has an infeasible instantiation .
Q
Definition: A distribution Q(x) = Z1Q j j (cj ) with clusters Cj is containable wrt a
Q
distribution P (x) = Z1P i i (di ), if for every constraining set  of P there exists at least
one cluster Cj such that   Cj .
Q
Q
Theorem 2 Let P (x) = Z1P i i (di ) and Q(x) = Z1Q j j (cj ) be two distributions where
Q
j (cj ) = l jl (cjl ) and where Q is containable and compatible wrt P and strictly positive. Then, after vip? iterates once over all clusters Cj , the revised distribution Q satisfies
(P (x) = 0)  (Q(x) = 0).
Proof: From the definition of a constraining set, for every infeasible instantiation x, there
exists an infeasible instantiation  of a constraining set  such that  is a projection of x
on . We show that vip? updates Q(x) = 0 for such instantiations. Since Q is containable
wrt P there exists a cluster Cj which contains . Furthermore, since   Di where Di is
a set of P and since Q is compatible wrt P , there exists a subset Cjl that contains . For
every instantiation cjl which is a projection of  on Cjl the expression jl (cjl ) is updated
to  according Q
to Eq. 6 of vip? . This is true because Q(di |cjl ) > 0 and log (di ) = .
1
Since Q(x) = ZQ j,l jl (cjl ) this update implies Q(x) = 0.
Whenever the first two compatibility conditions of Theorem 1 hold, it follows that vip?
converges for containable distributions. Note that since every iteration of vip? decreases the
KL divergence, following iterations can not change Q to be greater than zero for instantiation
which are infeasible wrt P , as this leads to an infinite distance. However, containability
implies a stronger property stated in the next theorem.

14

fiA Variational Inference Procedure

Q
Q
Theorem 3 Let P (x) = Z1P i i (di ) and Q(x) = Z1Q j j (cj ) be two distributions where
Q
j (cj ) = l jl (cjl ) and where Q is containable wrt P and (P (x) = 0)  (Q(x) = 0). Then,
after vip? iterates once over all clusters Cj , the revised distribution Q satisfies (Q(x) = 0) 
(P (x) = 0).
Proof: Consider an instantiation x for which P (x)
> 0. We show that Eq. 6 of vip? upQ
dates Q(x) to a positive value. Since Q(x) = Z1Q j,l jl (cjl ), it is sufficient to show that
the revised potential jl (cjl ) is positive for each subset Cjl and an instance cjl which is
the projection of x on the Cjl . For such instances cjl the value jl (cjl ) is set by Eq. 6 to
a finite value since i (di ) > 0 for every instance di which is a projection of x on a set Di ,
and k (ck ) = 0 implies Q(ck |cjl ) = 0. Therefore, jl (cjl ) = ejl (cjl ) > 0 and Q(x) > 0.
The consequence of Theorem 3 is that Q(x) = 0 iff P (x) = 0. Conditions weaker than
containability may be sufficient to ensure the requirement needed for convergence, however,
containability is easily satisfiable in applications of variational techniques as explicated in
the next section.

6. Genetic Linkage Analysis via Variational Algorithms
Genetic linkage analysis takes as input a family pedigree in which some individuals are
affected with a genetic disease, affection status of members of the pedigree, marker readings
across the genome, and mode of inheritance. The output is the likelihood of data as a function of the location of a disease gene and the given pedigree. Locations yielding maximum
or close to maximum likelihood are singled out as suspect regions for further scrutiny. The
exact computation of this likelihood is often too complex and approximations are needed.
Algorithm vip? has been developed to facilitate such likelihood computations. In particular, vip? allows there to be overlapping clusters which minimizes the loss of valuable
information and, more importantly, can handle the deterministic constraints that are common in these models. In this section, we describe the standard probabilistic model for
genetic linkage, several approximate distributions that we use when applying vip? to the
genetic linkage model, and demonstrate vip? on a real-world data set of a large pedigree
with 115 individuals.
The standard probabilistic model for genetic linkage is based on a pedigree which contains several variables for each person at each location and conditional probability tables
for each variable Xm given a set of variables called parents of Xm and denoted by (Xm ).
The distribution P (x) that represents the joint distribution of the variables in the pedigree
is written using multiple indices; one set of indices for persons (i), one for loci (j), and
another for the type of variable (t) as follows:
P (x) =

YY
j

i,t
P (xi,t
j |(xj )) =

Y

i t{ps,ms,pg,mg,f }

1 YY
ZP
j

Y

i t{ps,ms,pg,mg,f }

15

i,t
i,t
i,t
j (xj |(xj ))

(15)

fiGeiger, Meek & Wexler

where the five possible types of variables are: paternal selector (ps), maternal selector (ms),
paternal genotype (pg), maternal genotype (mg) and phenotype (f). Thus, the set Dji,t
equals {Xji,t , (Xji,t )}.
We denote the variables of the different types, ps, ms, pg, mg and f, of individual i at
i,m
locus j by Sji,p , Sji,m , Gi,p
and Fji respectively. In this notation the possible potentials
j , Gj
a,p
a,m
i,p
i,m
b,p
b,m
i,m
i,p
i,p
i,m
i,m
of P are (Gi,p
j , Gj , Gj , Sj ), (Gj , Gj , Gj , Sj ), (Sj , Sj1 ), (Sj , Sj1 ) and
i,m
(Fji , Gi,p
j , Gj ) where a and b are is father and mother in the pedigree, respectively. Some
i,p
a,p
a,m
i,ms
i,m
exemplifying sets are Dji,pg = {Gi,p
= {Sji,m , Sj1
}, and Dji,f =
j , Sj , Gj , Gj }, Dj
i,m
{Fji , Gi,p
j , Gj }, where a is the father of individual i in the pedigree. We note that the first
two types of potentials and possibly the last one are deterministic potentials which equal
zero for some instantiations.
A directed
Q acyclic graph along with a probability distribution R that factors according
to R(Z) = i R(zi |(zi )) is called a Bayesian network. A Bayesian network defined by
Eq. 15, that describes parents-offspring interaction in a simple genetic analysis problem
with two siblings and their parents across 3 loci, is given in Figure 4. The dashed boxes
contain all variables that describe the variables in a single location. In this example we
assume that each phenotype variable depends on the genotype at a single locus. This is
reflected by the fact that only edges from a single locus point into each phenotype variable.
The full semantics of all variables and more details regarding the conditional probability
tables can be found in the paper by Fishelson & Geiger (2002); these details are not needed
here.
We use several choices to cluster the Bayesian network for P (x) such that Q is selfcompatible and compatible wrt P . In addition, since some potentials in P are constrained
(e.g. i,pg
j ), we choose clusters such that Q is containable wrt P . According to Theorem 2
this choice ensures that Q satisfies all conditions necessary for convergence of vip? , and in
particular (P (x) = 0)  (Q(x) = 0).
Consider a partition of the network into slots, each containing a set of consecutive loci.
In the most simple case every slot is a single locus and each of the subsets Cji contains
variables related to one individual i and the genotypes of his parents in that slot. We set
S
(i)
Cji = {Gj , Sji } and Cj = i {Cji } where (i) denotes the union of i and his parents.
An illustration of such a setting in a pedigree of two siblings and their parents over three
loci is given in Figure 5. In this setting, self-compatibility is trivially satisfied because the
clusters Cj of Q are disjoint, and Q is containable wrt P since only sets Dji,ps and Dji,ms , the
potentials of which are not constrained, span across more than a single locus. It remains to
show that compatibility of Q wrt P is satisfied. For sets Dji,t contained in a single subset

Cji this is trivial as Q(Dji,t |cj ) = Q(Dji,t |cji ) = 1. Otherwise, t equals ps or ms and without
i,p
i,p
i,p
i,p
loss of generality, Q(Sj1
, Sji,p |cj ) = Q(Sj1
|cj ) = Q(Sj1
|cji ) = Q(Sj1
, Sji,p |cji ).
In a more complex setup, which is similar to Example 1, we add a cluster CJ+1 which
cuts across the loci for selector variables of an individual r, as shown in Figure 6. The
S
(i)
subset Cji are set to Cji = {GjS , Sji , Sjr } and the clusters are Cj = i {Cji }, for j = 1 . . . J.
r
In addition, we set CJ+1 = l {CJ+1,l } and the subsets CJ+1,l = {Sl,l+1
}, for a single
chosen individual r. We verify that Q still satisfies the conditions of Theorem 1. Selfcompatibility is maintained since Q(Cj |cJ+1 ) = Q(Cj |cJ+1,j ), Q(CJ+1 |cj ) = Q(CJ+1 |cjr ),

16

fiA Variational Inference Procedure

Figure 4: A Bayesian network representation of a pedigree of two siblings and their parents
in a 3-loci model. The circles, squares and diamond shapes represent genotype,
phenotype and selector variables respectively.

and Q(Ck |cj ) = Q(Ck |cjr ) for every two clusters Cj , Ck such that j, k  J. Sets Dji,t
such that t 6= ms, ps are contained in cluster Cj and thus maintain independence given a
subset. For t = ms, ps the sets Dji,t that connect two adjacent loci are independent of CJ+1
given CJ+1,j , and are independent of other clusters Cj given the subset Cji , maintaining
compatibility of Q wrt P . Finally, Q is containable wrt P since all clusters from the previous
option remain.
Immediate extensions of the above clustering schemes allow every slot to contain several
consecutive loci and a set of possibly more than one individual R to cut across the loci. To
maintain the compatibility of Q wrt P in the latter extension, the subsets CjR are set to
(R)
CjR = {Gj , SjR }, where (R) denotes the union of individuals in R and their parents.
We now describe the experiments performed using a large pedigree with 115 individuals
spanning across 21 locations, which was studied by Narkis et al. (2004) to locate an area
that contains a gene that causes a fatal neurological disorder (LCCS type 2). First, the
proximity of the disease gene to each of the markers was tested through two-point analysis
- a model-selection method in which only two loci are considered simultaneously, with one
of them being the disease locus. In this method, loci which yield likelihood maxima suggest
probable locations of the disease locus. Two-point analysis of the abovementioned pedigree
took several days using the exact inference software superlink v1.5 designed for genetic
linkage analysis.
17

fiGeiger, Meek & Wexler

Figure 5: A schematic division of a pedigree into clusters, where each locus is a cluster.
In cluster C3 the variables of every individual are in a separate ellipse and the
number of that individual is written. In the other two clusters the marked areas
are C14 and C23 .

The log-likelihood probabilities obtained by vip? using the abovementioned extended
clustering scheme with 5 clusters across all loci and with no cluster cutting across the
loci, are shown in Figure 7. The figure shows clearly that the exact and approximate loglikelihood curves have a similar shape and almost identical extremum points, but have a
major difference in absolute value.
Next, we tested vip? on three-point analysis problems on the same pedigree, where
two markers are considered simultaneously along with one disease locus. We note that
exact inference for this task on the specified pedigree is hard on a single PC, taking several
weeks. Since the exact location of the disease gene is known with high probability, we
wished to test whether or not the lower-bounds found by vip? indicate this location. We
considered two nearby markers (number 4 and 6) and seven models which differ by the
location of the speculated disease gene: in the first two, the disease locus was positioned
left to marker 4 at distances 0.01 and 0.02 centi-Morgan (cM), and in the remaining five it
was positioned to the right of marker 4 at distances 0.01 to 0.05 cM with 0.01 cM difference
between locations. The location of the disease gene is 0.01cM to the right of marker 4. The
algorithm was run three times on each model with random initial potentials, taking into
account only the maximum value obtained. The results of this test are plotted in Figure 8
versus the approximation found by the sampling software simwalk2 introduced by Sobel,
Papp & Lange (2002) which is designed for approximate pedigree analysis. As shown, the
probabilities found by vip? are higher as we approach the location of the disease gene. The
same in not true for the probabilities found by simwalk2. However, we note that vip? is
18

fiA Variational Inference Procedure

ln-likelihood

Figure 6: A pedigree partitioned into clusters, where each locus is a cluster and an additional cluster C4 , in the striped area, contains the variables of one of the individuals.

0
-20
-40
-60
-80
-100
-120
-140
-160
-180
-200
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

Exact

locus

VIP* - 5 clusters across all loci
VIP* - disjoint clusters

Figure 7: Approximations to the log-likelihood of two-point analysis using vip? .

much slower on this problem than simwalk2, taking several hours for each run. In addition,
19

fiGeiger, Meek & Wexler

we note that the ln-likelihood probabilities in Figures 8(a) and (b) are drawn on different
scales due to the markedly different output of the two methods.

-182

-77
-78

-186

ln-likelihood

ln-likelihood

-184

-188
-190
-192
-194

-79
-80
-81
-82

-0.02

-0.01

0.01

0.02

0.03

0.04

0.05

-0.02

location (relative to marker 4)

-0.01

0.01

0.02

0.03

0.04

0.05

location (relative to marker 4)

(a) using vip?

(b) using simwalk2

Figure 8: Approximations to the log-likelihood of three-point analysis.
We compared the convergence times of vip? and Wiegerincks algorithm on various size
problems of genetic linkage analysis. The original network on which we performed the test
includes 332 variables and represents a pedigree with 28 individuals over four loci. To create
various sized problems, subsets of the original pedigree with increasing number of individuals
were considered. In addition, for a fair comparison, the clusters of Wiegerincks algorithm
were chosen to be a subset of the subsets used by vip? . Since the number of iterations until
convergence did not vary significantly between the two algorithms, we report the ratio of
iteration times which also tests the theoretical speedup predicted in Section 3 of vip? over
Wiegerincks algorithm. Figure 9 illustrates the ratio of time for an update iteration of the
two algorithms, where it is evident that the ratio increases linearly with the problem size.
The ratios indicated are averaged over 5 runs each with 10 iterations for every problem size.
Finally we examine the convergence of vip? in Figure 10 using six representatives of
the original 21 two-point analysis runs described earlier, each on a different network. The
algorithm is halted when the change in the lower-bound is smaller than 105 for more than
3 iterations. Although not all runs converge at the same rate, it seems that they obey a
certain pattern of convergence where the first few iterations show significant improvements
in the lower-bound, followed by slow convergence to a local maximum, and then another
moderate improvement to a better maximum point.

7. Discussion
In this paper we present an efficient algorithm called vip? for structured variational approximate inference. This algorithm, which extends known algorithms, can handle overlapping
clusters and overcome the difficulties imposed by deterministic constraints. We show that
for N  N grid-like models, algorithm vip? is N fold faster than Wiegerincks algorithm,
20

fiA Variational Inference Procedure

20
18

ratio of iteration times

16
14
12
10
8
6
4
2
0
10

12

14

16

18

20

22

24

26

28

number of individuals

Figure 9: Speedup of vip? over Wiegerincks algorithm.

-70
-90

ln-likelihood

-110
-130
-150
-170
-190
-210
-230
1

3

5

7

9

11

13

15

17

19

21

23

iteration

Figure 10: Convergence of vip? for 6 runs of two-point analysis.
when a junction tree is used. In addition, we prove the convergence of vip? and of previous
variational methods via a novel proof method, using properties of the KL divergence.
Finally, algorithm vip? is tested on Bayesian networks that model genetic linkage analysis problems. These graphs resemble grid-like models and are notoriously difficult to
approximate due to the numerous deterministic constraints. The results show a linear
improvement in speed of vip? versus Wiegerincks algorithm, and that the approximation
21

fiGeiger, Meek & Wexler

follows the shape of the real likelihood probabilities. Nevertheless, Figure 7 shows that variational methods such as Wiegerincks algorithm or vip? are still not appropriate to produce
accurate approximation of the likelihood for genetic linkage analysis.

Acknowledgments
This paper is an extension of a paper that originally appeared at the 10th workshop on
Artificial Intelligence and Statistics (Geiger & Meek 2005). We thank D. Heckerman, N.
Jojic and V. Jojic for helpful discussions. We also thank the two anonymous reviewers
for correcting several errors that appeared in the early version as well as improving the
presentation. Part of the work was done while the first author was a visitor at Microsoft
Research. This work is supported by the Israeli Science Foundation and the Israeli Science
Ministry.

References
Bishop, C. & Winn, J. (2003). Structured variational distributions in VIBES. In Artificial
Intelligence and Statistics. Society for Artificial Intelligence and Statistics.
Cooper, G. (1990). Probabilistic inference using belief networks is NP-hard. Artificial
Intelligence, 42, 393405.
Cover, T. M. & Thomas, J. A. (1991). Elements of Information Theory. Wiley.
Dagum, P. & Luby, M. (1993). Approximating probabilistic inference in Bayesian belief
networks is NP-hard. Artificial Intelligence, 60 (1), 141153.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113 (1-2), 4185.
Fishelson, M. & Geiger, D. (2002). Exact genetic linkage computations for general pedigrees.
Bioinformatics, 18, S189S198.
Geiger, D. & Meek, C. (2005). Structured variational inference procedures and their realizations. In Proceedings of Tenth International Workshop on Artificial Intelligence
and Statistics, The Barbados. The Society for Artificial Intelligence and Statistics.
Ghahramani, Z. & Jordan, M. I. (1997). Factorial hidden Markov models. Machine Learning, 29, 245273.
Jensen, F. V. (1996). An Introduction to Bayesian Networks. Springer.
Jojic, V., Jojic, N., Meek, C., Geiger, D., Siepel, A., Haussler, D., & Heckerman, D.
(2004). Efficient approximations for learning phylogenetic HMM models from data.
Bioinformatics, 20, 161168.
Kschischang, F. R., Frey, B. J., & Loeliger, H. A. (2001). Factor graphs and the sum-product
algorithm. IEEE Transactions on information theory, 47 (2), 498519.
22

fiA Variational Inference Procedure

Narkis, G., Landau, D., Manor, E., Elbedour, K., Tzemach, A., Fishelson, M., Geiger, D.,
Ofir, R., Carmi, R., & Birk, O. (2004). Homozygosity mapping of lethal congenital
contractural syndrome type 2 (LCCS2) to a 6 cM interval on chromosome 12q13.
American Journal of Medical Genetics, 130 (3), 272276.
Saul, L. & Jordan, M. I. (1996). Exploiting tractable substructures in intractable networks.
In Advances in Neural Information Processing Systems (NIPS). MIT Press.
Sobel, E., Papp, J., & Lange, K. (2002). Detection and integration of genotyping errors in
statistical genetics. American Journal of Human Genetics, 70, 496508.
Wiegerinck, W. (2000). Variational approximations between mean field theory and the junction tree algorithm. In Uncertainty in Artificial Intelligence, (pp. 626633). Morgan
Kaufmann.
Xing, E. P., Jordan, M. I., & Russell, S. (2003). A generalized mean field algorithm for
variational inference in exponential families. In Uncertainty in Artificial Intelligence,
(pp. 583591). Morgan Kaufmann.
Xing, E. P., Jordan, M. I., & Russell, S. (2004). Graph partition strategies for generalized
mean field inference. In Uncertainty in Artificial Intelligence, (pp. 602  610). Morgan
Kaufmann.

23

fi