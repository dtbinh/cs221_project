Journal of Artificial Intelligence Research 50 (2014) 487-533

Submitted 12/13; published 06/14

Improving Delete Relaxation Heuristics Through Explicitly
Represented Conjunctions
Emil Keyder

emilkeyder@gmail.com

Jorg Hoffmann

hoffmann@cs.uni-saarland.de

Saarland University
66123 Saarbrucken, Germany

Patrik Haslum

patrik.haslum@anu.edu.au

The Australian National University & NICTA
Canberra ACT 0200, Australia

Abstract
Heuristic functions based on the delete relaxation compute upper and lower bounds
on the optimal delete-relaxation heuristic h+ , and are of paramount importance in both
optimal and satisficing planning. Here we introduce a principled and flexible technique
for improving h+ , by augmenting delete-relaxed planning tasks with a limited amount of
delete information. This is done by introducing special fluents that explicitly represent
conjunctions of fluents in the original planning task, rendering h+ the perfect heuristic h
in the limit. Previous work has introduced a method in which the growth of the task is
potentially exponential in the number of conjunctions introduced. We formulate an alternative technique relying on conditional effects, limiting the growth of the task to be linear
in this number. We show that this method still renders h+ the perfect heuristic h in the
limit. We propose techniques to find an informative set of conjunctions to be introduced in
different settings, and analyze and extend existing methods for lower-bounding and upperbounding h+ in the presence of conditional effects. We evaluate the resulting heuristic
functions empirically on a set of IPC benchmarks, and show that they are sometimes much
more informative than standard delete-relaxation heuristics.

1. Introduction
Planning as heuristic search is one of the most successful approaches to planning. Some
of the most informative heuristic functions for domain-independent planning are obtained
as the estimated cost of the delete relaxation of the original planning task. The delete
relaxation simplifies planning tasks by assuming that every variable value, once achieved,
persists during the execution of the rest of the plan. The cost of an optimal plan for the
resulting relaxed planning task, denoted h+ , is NP-complete to compute, however whether
some plan for the delete-relaxed task exists can be checked in polynomial time (Bylander,
1994). For satisficing planning, where the heuristic does not have to be admissible, the
latter fact can be exploited to upper-bound h+ , by generating some not necessarily optimal
plan for the delete-relaxed task (Hoffmann & Nebel, 2001). For optimal planning, lowerbounding methods have been devised based on the analysis of landmarks, logical formulas
over the set of actions that state necessary properties of delete-relaxed plans (Karpas &
c
2014
AI Access Foundation. All rights reserved.

fiKeyder, Hoffmann, & Haslum

Domshlak, 2009; Helmert & Domshlak, 2009). These cost estimates for the delete-relaxed
task can then be used to guide heuristic search in the state space of the original task.
Since delete relaxation heuristics were first proposed (Bonet & Geffner, 2001), much
work has been done to improve them. One approach focuses on better approximation
schemes for h+ , obtaining tighter upper bounds and thus better non-admissible estimates
(Hoffmann & Nebel, 2001; Keyder & Geffner, 2008, 2009), or tighter lower bounds that
correspond to more informative admissible heuristics (Helmert & Domshlak, 2009; Bonet
& Helmert, 2010). In many domains, however, it is important that the heuristic be able to
take into account delete information (Hoffmann, 2005), and indeed there is a long tradition
of works proposing heuristics that do so. Several of these extend the delete relaxation to
capture strictly more information (Fox & Long, 2001; Helmert, 2006; Helmert & Geffner,
2008; Cai, Hoffmann, & Helmert, 2009; Katz, Hoffmann, & Domshlak, 2013), while some
consider only the delete relaxation but attempt to find low-conflict relaxed plans (Baier &
Botea, 2009), or generate modified heuristic values based on taking conflicts into account
to some extent (Do & Kambhampati, 2001; Gerevini, Saetti, & Serina, 2003). Here, we
approach this problem by taking inspiration from the admissible hm family of heuristics
(Haslum & Geffner, 2000). An important property of the heuristics we introduce, shared
with some other recent work in this direction, is that our technique renders h+ the perfect
heuristic h in the limit. In other words, the technique offers a trade-off between the amount
of delete information considered and the computational overhead of doing so. At one end
of that continuum, delete-relaxed plans become plans for the original task.
The hm heuristic function considers the cost of making true simultaneously sets of fluents
of size  m. The cost of the planning task is then estimated by recursively taking the cost
of a set of fluents, such as the goal or a set of action preconditions, to be the cost of its
most costly subset of size  m, and ignoring the cost of achieving the remaining fluents in
the set. As each possible subset of size m of the fluents in the task must be considered, the
size of the representation required to compute hm is exponential in m. The hm heuristics
provide the guarantee that there exists an m such that hm = h (trivially satisfied when m
is the total number of fluents in the task). However, the value of m required to achieve this
is usually so large as to make computing h with this method infeasible in practice.
The hm heuristic has recently been recast as the hmax = h1 cost of a planning task m
with no deletes (Haslum, 2009). This is achieved by representing conjunctions of fluents c
of size  m in the original task with new fluents c , here called -fluents, and modifying
the initial state, goal, and operators of the planning task so as to capture the reasoning
performed by hm over these sets within the computation of hmax . However, h+ (m ) is
not admissible (since a separate copy of the same action may be needed to establish each
-fluent), and thus the m compilation is not useful for obtaining admissible estimates that
are more informative than h+ . The more recent C construction (Haslum, 2012) fixes this
issue (introducing an action copy for every subset of -fluents that may be established), at
the cost of growing the task representation exponentially in the number of -fluents rather
than linearly as in the m representation. On the other hand, C offers the possibility of a
more fine-grained tradeoff between representation size and heuristic accuracy, by allowing
the choice of an arbitrary set of conjunctions C and corresponding -fluents (which need not
all be of the same size). This stands in contrast to the hm heuristic and the m compilation,
in which all sets or conjunctions of size  m are represented.
488

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Haslum (2012) proposed to repeatedly solve C optimally, within an iterative procedure
that adds new conjunctions to the set C in each iteration. The relaxed plans that are
computed therefore gradually become closer, in some sense, to being plans for the original
task. We instead explore the idea of using this kind of construction for obtaining heuristic
functions for guiding search.
C
We introduce a related construction C
ce that is similar to  , but that makes use of
conditional effects to limit the growth of the task to be worst case linear, rather than
exponential, in |C|. This gain in size comes at the price of some information loss relative to
C . However, as we show, this information loss does not affect the fundamental property
of tending towards the perfect heuristic h if enough conjunctions are introduced: Like
C ,  C
ce is perfect in the limit, i. e. there always exists a set of conjunctions C such that

h+ (C
ce ) = h . Furthermore, while information may be lost, this is not always the case.
Indeed, it is possible to construct families of planning tasks for which C
ce can represent the
+
C
same heuristic function as C for the same set of conjunctions C, i. e., h+ (C
ce ) = h ( ),
C
but for which the representation of ce occupies exponentially less space.
Having said that, the theoretical advantage of C
ce does not tend to materialize in practice
(or at least in the commonly used benchmarks): While without further optimizations C
indeed grows too quickly to be practical, it turns out that mutex pruning techniques (eliminating compiled actions with conflicting preconditions) are extremely effective at keeping
C
the size of C at bay. We therefore consider both C
ce and  , evaluating their usefulness
for devising improved heuristic functions. We focus on two main questions:
(A) How to obtain upper and lower bounds for h+ in compiled tasks?
(B) How to choose a set of conjunctions C so as to maximize the information gained from
their addition to the planning task?
In response to question (A), we analyze and extend three state-of-the-art methods for
estimating h+ . In the satisficing setting (upper-bounding h+ ), we consider the problem
of finding low-cost relaxed plans that can be scheduled so as to minimize the cost of the
sequence of actions required to trigger a given set of conditional effects, avoiding unnecessary
repeated applications of the same action. This problem has been only scantily addressed
in previous work. Here we show that the problem of optimal action scheduling for a given
set of effects is NP-complete, and generalize the approximation technique used in the FF
planner (Hoffmann & Nebel, 2001).
In the optimal setting (lower-bounding h+ ), we consider the LM-cut heuristic (Helmert
& Domshlak, 2009) as well as admissible heuristics based on fluent landmarks (Karpas
& Domshlak, 2009). For the former, our findings are mostly negative: First, we show
that even though the introduction of -fluents cannot decrease hmax or h+ , which lowerand upper-bound LM-cut, respectively, the LM-cut heuristic value can decrease. Second,
we show that neither of the two straightforward adaptations of the LM-cut algorithm to
problems with conditional effects maintains both admissibility and domination of hmax .1 For
the latter, we show that C
ce can be used to generate more informative fluent landmarks.
Recent work (Keyder, Richter, & Helmert, 2010) extracts landmarks from the m task.
1. A more sophisticated adaptation of LM-cut, based on the idea of context splitting, has recently been
proposed by ? (?). It maintains both properties.

489

fiKeyder, Hoffmann, & Haslum

This allows the discovery of -fluent landmarks corresponding to conjunctive landmarks in
the original task, but suffers due to the large number of -fluents that must be considered.
The C
ce compilation offers the possibility of discovering interesting conjunctive landmarks
of unbounded size, while avoiding growing the size of the compilation unnecessarily.
In response to question (B), we devise a range of strategies depending on the purpose
C
for which the C
ce or  compilation is to be used. Most of these are parameterized in terms
of the allowed growth of the compiled task relative to the original task, and thus allow a
trade-off between the informativeness of the heuristic and its computational overhead. We
evaluate the resulting heuristics on a wide range of benchmarks from the International Planning Competition, varying the relevant algorithm parameters to determine their individual
effect on performance. As the results show, in several domains our heuristics are much more
informative than previous ones, leading to significantly improved performance.
We next define the basic concepts (Section 2), before moving on to the formal definition
m
C
of the C
ce compilation and the previously introduced  and  compilations (Section 3).
C
m
C
In Section 4, we analyze ce and its relation to  and  from a theoretical perspective.
Section 5 discusses the practical issues that arise in using the compilations for the purpose
of satisficing planning, and describes the obtained experimental results, while Section 6 does
the same for the case of optimal planning. Finally, Section 7 summarizes the main points
of the paper and indicates some possible future research directions.

2. Preliminaries
Our planning model is based on the propositional STRIPS formalization, to which we add
action costs and conditional effects. States and operators are defined in terms of a set F
of propositional variables, or fluents. A state s  F is given by the set of fluents that are
true in that state. A planning task is described by a 4-tuple  = hF, A, I, Gi, where F
is a set of such variables, A is the set of actions, I  F is the initial state, and G  F
describes the set of goal states, given by {s | G  s}. Each action a  A consists of
a 4-tuple hpre(a), add(a), del(a), ce(a)i, where pre(a), add(a), and del(a) are subsets of F .
The action has a cost cost(a)  R+
0 . By ce(a) = {ce(a)1 , . . . , ce(a)n }, we denote the set of
conditional effects of action a, each of which is a triple hc(a)i , add(a)i , del(a)i i of subsets of
F . To simplify some of our notations, we require that add(a)  del(a) = ; we do not need
to impose any restrictions on the deletes del(a)i of conditional effects, because conditional
effects will be used only within the delete relaxation. If ce(a) =  for all a  A,  has no
conditional effects, and we say that it is a STRIPS planning task.
An action a is applicable in s if pre(a)  s. The result of applying it is given by
[
[
s[a] = (s \ (del(a) 
del(a)i ))  (add(a) 
add(a)i )
{i|c(a)i s}

{i|c(a)i s}

A plan for s is a sequence
Pn of actions  = a1 , . . . , an whose application in s results in a goal
state. The cost of  is i=1 cost(ai ).  is optimal if its cost is minimal among all plans for
s; we will often denote optimal plans with   . A plan for I is also called a plan for , or
simply a plan.
A heuristic for  is a function h mapping states of  into R+
0 . The perfect heuristic

h maps each state s to the cost of an optimal plan for s. A heuristic h is admissible if
490

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

h(s)  h (s) for all s. By h(0 ), we denote a heuristic function for  whose value in s is
given by estimating the cost of the corresponding state s0 in a modified task 0 . We specify
0 in terms of the transformation of  = hF, A, I, Gi into 0 = hF 0 , A0 , I 0 , G0 i; s0 is obtained
by applying to s the same transformation used to obtain I 0 from I. It is sometimes useful
to make explicit that h is a heuristic computed on  itself; we will denote that by h().
Note that the modified task 0 is used only for the computation of the heuristic function.
In particular, the actual search for a plan is performed in the state space of the original
planning task .
The delete relaxation + of a planning task  is obtained by discarding all delete
effects. Formally, + = hF, A+ , I, Gi, where A+ = {hpre(a), add(a), , ce+ (a)i | a  A}, and
ce+ (a) = {hc(a)i , add(a)i , i | ce(a)i  ce(a)}. The cost of each action a+  A+ is the same
as the cost of the corresponding action cost(a). The optimal delete relaxation heuristic h+
is defined as the cost h (+ ) of an optimal plan for + .
We denote the power set of F with P(F ) = {c | c  F }. In the context of hm ,
C
 , and C
ce , we refer to fluent subsets c  P(F ) as sets or conjunctions interchangeably.
Throughout the paper, we assume that conjunctions are non-unit, i. e., |c| > 1.
A landmark in a planning task is a logical formula  over the set of fluents F such that
every valid plan  makes  true in some state (Hoffmann, Porteous, & Sebastia, 2004).
Orderings between landmarks are statements about the order in which these states occur.
A natural ordering 1 n 2 means that if a state sj satisfies 2 , there is some state si
occurring before sj in which 1 is satisfied. A necessary ordering 1 nec 2 means that 1
is always true in the state immediately before the state in which 2 becomes true, while a
greedy necessary ordering 1 gn 2 means that this relationship holds the first time that
2 is made true. Note that the necessary ordering 1 nec 2 implies the greedy necessary
ordering 1 gn 2 , but not vice versa. A landmark graph G is a directed graph whose
nodes are landmarks, and whose labelled edges correspond to the known orderings between
these landmarks.

3. The m , C and C
ce Compilations
The m compilation (Haslum, 2009) was the first technique proposed that made use of
the idea of -fluents that explicitly represent conjunctions in the original task. Given a
conjunction c  F , c is a new fluent c 6 F unique to c, i. e., if c 6= c0 then c 6= c0 .
In defining m and the other compilations that we discuss, we use the shorthand X C =
X  {c | c  C  c  X}, where X  F is a set of fluents, and C  P(F ) is a set of
conjunctions. In other words, X C consists of the set of fluents X itself, together with new
fluents c whose intention is to represent the conjunctions c  C that are contained in X,
c  X.
Definition 1 (The m compilation) Given a STRIPS planning task  = hF, A, I, Gi
and a parameter m  Z+ , m is the planning task hF C , AC , I C , GC i, where C = {c | c 
F  1 < |c|  m}, and AC contains A as well as an action ac for each pair a  A, c  C
such that del(a)  c =  and add(a)  c 6= , and ac is given by del(ac ) = , ce(ac ) = , and
pre(ac ) = (pre(a)  (c \ add(a)))C
add(ac ) = add(a)  {c0 | c0  C  c0  (add(a)  c)}
491

fiKeyder, Hoffmann, & Haslum

The parameter m here indicates the maximum size of the conjunctions to be represented
explicitly in the resulting compiled task. A -fluent is inserted (by definition of F C , cf.
above) for each c  F where 1 < |c|  m. These c are then added to all fluent sets in the
task (such as the initial state, action preconditions, and goals) containing the associated
set c. Furthermore, a linear (in |C|) number of representatives of each action a are added
to the task to model the situation in which the elements of c that are not made true by a
are already true before a is applied, and a adds the remaining fluents in c while deleting
none of them, thereby making every fluent in c, and therefore c , true. This compilation
allows the admissible hm cost of the original task to be computed as the hmax cost of this
compiled task.
The non-admissibility of h (m ) = h+ (m ) is due to the construction of the action representatives ac : Sets of fluents that are simultaneously made true with a single application
of an action a in  may require several representatives of a to explicitly achieve the same
effect in m . Consider for example an action a adding a fluent p in a state in which q and
r are already true. In  this makes the fluents p, q, and r true simultaneously, whereas in
2 , two different representatives of a are required: one with c = {p, q} adding {p,q} , and
one with c = {p, r} adding {p,r} .
The C compilation solves this problem by instead creating a number of representatives of a exponential in the number of -fluents which may be made true by a. Each
of these representatives corresponds to an application of a that makes a set of -fluents
true (Haslum, 2012). Following the above example, separate representatives of a would be
introduced for each of the -fluent sets , {{p,q} }, {{p,r} }, and {{p,q} , {p,r} }, and the
representative resulting from the last of these could be applied to make the two -fluents
true simultaneously. C also differs from m in that it allows the choice of a set C  P(F ),
and introduces fluents c for only those c  C, rather than for all subsets of size at most
m:2
Definition 2 (The C compilation) Given a STRIPS planning task  = hF, A, I, Gi
and a set of non-unit conjunctions C  P(F ), C is the planning task hF C , AC , I C , GC i,
0
where AC contains an action aC for each pair a  A, C 0  C such that c0  C 0 ,
(1) del(a)  c0 =   add(a)  c0 6= , and
(2) c  C((c  c0  add(a)  c 6= ) = c  C 0 ),
0

0

0

and aC is given by del(aC ) = , ce(aC ) = , and
[
0
pre(aC ) = (pre(a) 
(c0 \ add(a)))C
c0 C 0
C0

add(a ) = (add(a)  (pre(a) \ del(a)))C  {c0 | c0  C 0 }
2. There are three differences between our definition and Haslums (2012) definition of the the actions in
C . First, Haslums definition features delete effects, ensuring that real (non-relaxed) plans correspond
to plans in the original task. Since we only consider delete relaxations of the compiled task, we can safely
omit these. Second, we allow the sets C 0 used in the construction of actions to contain conjunctions c
0
with c  add(a)  (pre(a) \ del(a)); and third, add(aC ) contains the -fluents c where c  pre(a) \ del(a).
These latter two differences keep our definitions simpler. The redundant action representatives and
redundant add effects that they cause are easily pruned in practice.

492

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

0

The representatives aC of a enforce, for every c0  C 0 , that no part of c0 is deleted,
0
and that the non-added part of c0 is true already before aC is executed. Constraint (2)
0
ensures a form of non-redundancy: if aC adds a -fluent c0 , then it also adds all -fluents
c such that c  c0 , as all fluents in c necessarily become true with the application of
the action. Note that, differently than m , add effects in C include -fluents representing
conjunctions of fluents added by the action with prevail fluents (non-deleted preconditions).
This is necessary for admissibility of h+ (the primary purpose of C ), but not needed for
the computation of h1 (the primary purpose of m ).
C enumerates all possible subsets of C in constructing the representatives of each
action and therefore grows exponentially in |C|. This exponentiality is reminiscent of the
canonical conditional effects compilation used to convert planning tasks with conditional
effects into classical STRIPS planning tasks with exponentially more actions (Gazen &
Knoblock, 1997). The C
ce compilation that we introduce here is the result of applying
roughly the reverse transformation to C , resulting in a closely related planning task with
a linear (in |C|) number of conditional effects:
Definition 3 (The C
ce compilation) Given a STRIPS planning task  = hF, A, I, Gi
C
C
C
C
and a set of non-unit conjunctions C  P(F ), C
ce is the planning task hF , Ace , I , G i
where
C
C
C
C
AC
ce = {hpre(a ), add(a ), del(a ), ce(a )i | a  A},

and aC is given by
pre(aC ) = pre(a)C
add(aC ) = (add(a)  (pre(a) \ del(a)))C
del(aC ) = 
ce(aC ) = {h(pre(a)  (c \ add(a)))C , {c }, i
| c  C  c  del(a) =   c  add(a) 6= }
Rather than enumerating the sets of -fluents that may be made true by an action, C
ce
uses conditional effects to implicitly describe the conditions under which each is made true.
The only information lost in doing so is the information encoded by cross-context -fluents
in preconditions, which appear in action representatives in C , but not in the preconditions
C 0 in
or effect conditions of the corresponding actions in C
ce . For action representatives a
0
C , these are -fluents y  pre(aC ) where there exists no c  C 0 s.t. y  (c \ add(a)) 
pre(a). In the situation discussed above, for example, {q,r} is a precondition for the action
representative that adds both {p,q} and {p,r} in C , but does not appear in the condition
of any conditional effects of the corresponding action in C
ce . Since effect conditions are
determined individually for each c , such conditions are never included. We will return to
this below when discussing the theoretical relationship between C and C
ce .
Example 1 Consider the STRIPS planning task (adapted from Helmert & Geffner, 2008)
with variables {x0 , . . . , xn , y}, initial state I = {x0 , y}, goal G = {xn }, and unit-cost actions
a : h, {y}, , i

bi : h{xi , y}, {xi+1 }, {y}, i
493

fiKeyder, Hoffmann, & Haslum

for i = 0, . . . , n  1.
The optimal solution to this planning task takes the form b0 , a, b1 , a, . . . , bn1 , and has
cost 2n1. In the delete relaxation of the task, the fact that y is deleted after each application
of bi is ignored, and the optimal plan has cost n.
When a -fluent xi ,y is introduced in the C
ce compilation, it is added to the precondition
of the action bi , and new conditional effects ce(a)i of the form h{xi }, {{xi ,y} }, i are created
for action a. No conditional effects are added to any of the b-actions, as each deletes y and
therefore cannot be an achiever of any -fluent. This increases the optimal delete relaxation
cost of the task by 1, as a new instance of a must be added to the relaxed plan to achieve
the newly introduced precondition of bi . If all -fluents of the form {xi ,y} are introduced,
the delete relaxation cost of C
ce becomes 2n  1, the optimal cost.
The same set of conjunctions renders the delete relaxation cost of C perfect (i. e.,
2n  1). However, the size of C given that conjunction set is exponential in n: As action
a may in principle achieve any subset of the conjunctions, every such subset C 0 induces a
0
separate representative aC in AC .
Regarding the m compilation, h2 = hmax (2 ) also gives the optimal cost of this task.
However, its computation requires the consideration of (n2 ) fluent pairs, rather than the
linear number of -fluents that need to be introduced in C
ce . As we shall see below (Theorem 6), the example can be easily extended so that m must scale with n for hm to become
C
m
perfect, thus showing an exponential separation between C
ce and both  and  .
An important practical optimization for both C and C
ce is mutex pruning. If mutex
information about the original planning task is available, specifically if we are given (some)
m-tuples of fluents that are not reachable in conjunction, then we can discard from the
compiled task any action representatives and conditional effects which require such an
m-tuple, without losing admissibility of the compilation. Namely, the value of h+ (C )
(respectively h+ (C
ce )) after this mutex pruning is bounded from above by the value of
0
0
0
h+ (C ) (respectively h+ (C
ce )) for a larger set C  C of conjunctions: If we include all
m
-fluents of size at most m, then all h mutexes are found, i. e., none of the respective fluents is reachable in the compiled task. Exploiting available mutex information allows us
to make the compilation more informed without having to add all these additional -fluents,
helping to keep the compilation small.
Another optimization we use is to eliminate dominated preconditions. Whenever we add
a fluent c to the precondition of an action, or to the condition of a conditional effect, we
remove from that condition all fluents p  c and -fluents {c0 | c0  c}. That is because
achieving c implies achieving these fluents as well, and methods that count their cost
separately (such as, for example, hadd and related heuristics) would incur an overestimation.
Note, however, that this does not eliminate the duplication caused by -fluents representing different fluent sets that have a non-empty intersection. Consider, for example, an
action a with pre(a) = {p, q, r}. If C = {{p, q}, {q, r}}, then pre(a) = {{p,q} , {q,r} }, and
the cost of achieving q will implicitly be counted twice in the hadd estimate of the cost of
applying a. As a possible solution, we considered replacing overlapping -fluents c , c0 with
cc0 . This, however, did not consistently improve any of the heuristics that we compute
from the compiled tasks.
494

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

4. Theoretical Properties of C
ce
+
C
We now discuss some theoretical properties of C
ce , considering the cost h (ce ) of its
optimal solutions instead of more practical approximations (note that for C
ce and the version
of C considered here, h+ = h as no delete effects are present). Where only proof sketches
are shown, the full proofs can be found in Appendix A. We first show the fundamental and
expected property:

Theorem 1 (Consistency and admissibility) h+ (C
ce ) is consistent and admissible.
Proof: Regarding consistency, given s, a such that s[a] = s0 in , we need to show that
+
C
0
 0C
0C in C . Then
h+ (C
ce )(s)  cost(a) + h (ce )(s ). Let  (s ) be an optimal plan for s
ce
0C  sC [aC ] and C is a task with no
aC    (s0C ) is necessarily a plan for sC in C
ce , as s
ce
deletes. Admissibility follows from consistency together with the fact that h+ (C
ce )(s) = 0
on goal states s.
Furthermore, the (ideal) delete relaxation lower bound can only improve as we add -fluents:
Theorem 2 (h+ (C
ce ) grows monotonically with C) Given a planning task  and sets
0
+
C0
C  C of non-unit conjunctions, h+ (C
ce )  h (ce ).
C
C
0
Proof: This follows from the fact that given any plan  = aC
1 , . . . , an0 for0 ce , 0 =
0
0
0
C
C
C
C
C
C
a1 , . . . , an constitutes a plan for ce . We show by induction that I [a1 ] . . . [ai ] 
C
0
C0
C0 =
I C [aC
1 ] . . . [ai ] \ {c | c  C \ C }, which shows the result since the goal of ce is G
GC \ {c | c  C \ C 0 }, and GC  sC [] if  is a valid plan.
0
For i = 0, the induction hypothesis holds since I C = I C \ {c | c  C \ C 0 } by definition.
0
0
0
0
C
C
C
C0
C
0
For i > 0, aC
i is applicable in I [a1 ] . . . [ai1 ] since pre(ai ) = pre(ai ) \ {c | c  C \ C },
0
0
0
C
0
C
C C
and I C [aC
1 ] . . . [ai1 ]  I [a1 ] . . . [ai ] \ {c | c  C \ C } by the induction hypothesis.
C
C
C
C
C
C
For {c | c  (I [a1 ] . . . [ai ] \ I [a1 ] . . . [ai1 ])  c  C 0 }, either c  add(ai )C , which
0
C0
implies c  add(aC
i ) due to the definition of ce , or there exists some conditional effect
C
0
cej (aC
i ) = h(pre(a)  (c \0 add(a))) , {c }, i. Since c  C , there must exist a 0corresponding
0
C
C0
conditional effect in ce by definition, and its condition must be true in I C [aC
1 ] . . . [ai1 ]
by the induction hypothesis.

For the special case where C 0 = , Theorem 2 gives us:
+
Corollary 1 (h+ (C
ce ) dominates h ()) Given a planning task  and a set of non-unit
+
C
+
conjunctions C, h (ce )  h ().

The domination can be strict, as follows trivially from convergence to h (Theorem 5 below).
We now consider the relationship between the C and C
ce compilations. As mentioned
above, information encoded by cross-context preconditions is lost when moving from the
C
exponential C to the linear C
ce . Estimates obtained from ce may therefore be inferior to
those obtained from C :
Theorem 3 (h+ (C ) dominates h+ (C
ce )) Given a planning task  and a set of nonunit conjunctions C, h+ (C )  h+ (C
).
There are cases in which the inequality is strict.
ce
495

fiKeyder, Hoffmann, & Haslum

Proof sketch: The standard conditional effects compilation into STRIPS (Gazen & Knoblock,
C
1997), applied to C
ce , is equivalent to  except for the presence of cross-context preconditions in C . Given this, any plan for C is also a plan for C
ce , yet the inverse is not the
C C
C
1
n
,
.
. . , aC
case. To show the first part, we show by induction that I C [aC
n ]  I [a1 , . . . , an ]ce ,
1
where I[. . . ]ce denotes the result of applying a sequence of actions to the initial state I C in
C
C
ce . Since the goal of both tasks is defined as G , this shows the desired result.
The strictness result follows from the fact that it is possible to construct tasks in which
the cross-context preconditions discussed above play a role, leading to situations in which
C
there exist plans for C
ce that are shorter than the minimum-length plans for  .
In the proof of strictness (Appendix A), we show a planning task for which the h+ (C )
value is strictly larger than the h+ (C
ce ) value when C is chosen to be all conjunctions
of size 2. This implies that there exist tasks in which it is necessary to consider strictly
larger conjunctions in C
ce to obtain equally good heuristic estimates as are obtained with
C
C . This is not necessarily problematic however, as differently from hm , C
ce and  do not
introduce all conjunctions of a given size, and are therefore not exponential in the maximum
size of the conjunctions considered.
C
The advantage of C
ce over  is that it is potentially exponentially smaller in |C|; the
above domination therefore must be qualified against this reduction in size. Furthermore,
C
ce preserves the ability to compute a perfect heuristic given a sufficiently large set C of
conjunctions. We first consider the equivalent result for C , already proved by Haslum
(2012). We provide an alternative proof here that can be conveniently adapted to show the
C
same property for C
ce . The key to our proof for  is the following equivalence between
h1 (m ) and h1 (C ):
Lemma 1 Given a planning task , and C = {c  P(F ) | 1 < |c|  m}, h1 (m ) = h1 (C ).
Proof sketch: m and C are identical except for their action sets. h1 values are computed
by considering only a single add effect at a time. The inequality h1 (m )  h1 (C ) is
0
then easy to see by verifying that, for every add effect c of an action aC in C (unless
0
c  pre(aC ) and thus is redundant), the action ac in m dominates it, i. e., c  add(ac )
0
and pre(ac )  pre(aC ). The proof is similar for the inequality h1 (C )  h1 (m ), observing
that for any action ac in m and non-redundant add effect, there exists a dominating action
0
aC in C .
Theorem 4 (h+ (C ) is perfect in the limit) Given a planning task , there exists C
such that h+ (C ) = h ().
Proof: It is known that h () = hm () for sufficiently high values of m (Haslum &
Geffner, 2000), and as shown by Haslum (2009), hm () = h1 (m ). By Lemma 1, for
C = {c  P(F ) | 1 < |c|  m}, we have h1 (m ) = h1 (C ). Choosing an appropriate
m and the corresponding C, we thus have that h () = hm () = h1 (m ) = h1 (C ).
Together with the fact that h1 (C )  h+ (C ), and since h+ (C )  h () by admissibility
of h+ (C ), the claim follows.
1
C
+
C
To show the same claim for C
ce , all that remains is to relate h ( ) to h (ce ):

496

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Lemma 2 Given a planning task  and a set of non-unit conjunctions C, h1 (C ) 
h+ (C
ce ).
C
Proof sketch: Consider a planning task C
no-cc identical to  except that it drops cross1
context -fluents from preconditions. We show that (A) h (C )  h1 (C
no-cc ), and (B)
1
C
+
C
h (no-cc )  h (ce ).
Similarly to the proof of Lemma 1, (A) is easy to see by showing that every add effect
0
C 00 in C : we simply set C 00 to
c of an action aC in C
no-cc is dominated by an action a
the minimal subset of C 0 that contains c and satisfies condition (2) of Definition 2 (in other
words, we reduce C 0 to get rid of any cross-context -fluents).
+
C
For (B), it suffices to show that h+ (C
no-cc )  h (ce ). This holds because, for any action
C
0
a in a relaxed plan for ce , if C is the set of conjunctions that are added by conditional
0
effects of a when it is applied in the plan, then the action representative aC in C
no-cc has
the same preconditions as a, and can be used to achieve the same set of fluents.

Theorem 5 (h+ (C
ce ) is perfect in the limit) Given a planning task , there exists C
such that h+ (C
)
=
h ().
ce
Proof: Choosing an appropriate m and C, we have h () = hm () = h1 (m ), and,
by Lemma 1, h1 (m ) = h1 (C ). With Lemma 2, we get h1 (C )  h+ (C
ce ). Since, by
 (), this shows the claim.
Theorem 1, h+ (C
)

h
ce
Note that, with Theorem 3, Theorem 4 is actually a corollary of Theorem 5. Our presentation is chosen to make the relation between the two results, and the role of the two lemmas,
clearer.
The proofs of Theorems 4 and 5 rely on obtaining perfect hm , which is clearly unfeasible
in general since this involves enumerating all subsets of fluents (and hence all possible states)
in the worst case. However, C and C
ce offer flexibility in allowing us to choose the set
C: while selecting all subsets guarantees a perfect heuristic, this may be achieved with
much less effort, which is especially beneficial when using C
ce whose growth in |C| is linear.

Indeed, there are task families for which obtaining h takes exponential effort with hm , and
requires exponentially-sized C , yet for which C
ce remains small:
m and C ) There exist parameterized task
Theorem 6 (Expressive power of C
ce vs. h
families k such that

1. if hm (k ) = h (k ) then m  k,

C
2. h+ (C
k ) = h (k ) implies that the number of action representatives in k is exponential in k, and

3. for any k there exists Ck such that |Ck | (and therefore the number of conditional effects
+
C

in (k )C
ce ) is polynomial in k, and (b) h ((k )ce ) = h (k ).
Proof: Members of one such family are given by the combination of k planning tasks of
the type shown in Example 1, each of size k, that share among them the action a and the
fluent y that needs to be made true after each step. k then has k goals, and hm = h iff
m  k.
497

fiKeyder, Hoffmann, & Haslum

C
For both C
k and (k )ce to be perfect, k -fluents {xi1 , y}, . . . , {xik , y} must be introduced for each of the individual subtasks i, leading to a total of k 2 -fluents. If any one of
these -fluents is not present, then the precondition for the action bij in (k )C
ce , or similarly
its representative with C 0 = {} in C
,
have
only
the
individual
fluent
preconditions
y, xij ,
k
and in consequence one of the a actions reestablishing y can be left out of the plan. The
number of conditional effects created in (k )C
ce is linear in the number of -fluents added.
However, the number of action representatives in (k )C is exponential in k: the action a
adds the fluent y, that belongs to all -fluents, and hence a has one representative for each
subset of the -fluents.

When using h+ (C
ce ) in practice, we will not typically be able to choose a C that results
in a perfect heuristic. Instead, we try to pick a set C that yields an informative heuristic
without making the size of the representation impractical to work with.

5. Heuristics for Satisficing Planning
We now consider the practical issues involved in using C
ce for satisficing planning. Section 5.1 deals with the extraction of relaxed plans, and Section 5.2 deals with strategies for
choosing the set of conjunctions C. Section 5.3 presents our experiments with the resulting
setup.
5.1 Relaxed Planning with Conditional Effects
Techniques for extracting relaxed plans in the presence of conditional effects have long been
known (Hoffmann & Nebel, 2001). Here, we refine and extend those techniques. They are
particularly important in our context as, unlike in most IPC benchmarks, the structure
of the conditional effects in C
ce can be rather complex, involving multiple dependencies
between different actions, and even between different executions of the same action.3
Non-admissible delete-relaxation heuristics are typically obtained with a relaxed plan
extraction algorithm (Keyder & Geffner, 2008). The different variants of this algorithm
are characterized by the best-supporter function bs : F 7 A they use. In all cases, bs(p)
is an action adding p that minimizes some estimate of the cost of making p true. When
no conditional effects are present, the algorithms compute a set of actions  that can be
scheduled to form a relaxed plan for the planning task. Formally, the algorithms construct
a relaxed plan  according to the following equations (Keyder & Geffner, 2008):
(
{}
if p  s
(p) =
bs(p)  (pre(bs(p))) otherwise
[
(P ) =
(p)
pP

Existing methods for choosing best supporters, such as hadd or hmax , can easily be
extended to conditional effects by treating each conditional effect in the task as a separate
3. We remark that similar issues arise in approaches compiling uncertainty into classical planning with
conditional effects (Palacios & Geffner, 2009; Bonet, Palacios, & Geffner, 2009), so our techniques may
turn out to be useful there as well.

498

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

action. In particular, this is the method employed, using hmax , to compute the FF heuristic
function (Hoffmann & Nebel, 2001). More precisely, for each relaxed conditional effect
ce(a)+
i with condition c(a)i and add add(a)i , an action ai with the same add effect add(ai ) =
add(a)  add(a)i and precondition pre(ai ) = pre(a)  c(a)i is created. The set of effects (G)
as defined by the rules above then forms a relaxed plan. The presence of conditional effects,
however, implies that there is a problem of how to schedule that relaxed plan: different
schedules may require different numbers of action applications, as multiple applications of
a single action a can be avoided by making the conditions of multiple desired effects true
before a given application of a.
For illustration, consider a planning task where an action move-briefcase has n conditional effects, each of which conditionally transports an object from location A to location
B if it is inside the briefcase. Using the representation above, a distinct moving action is
generated for each conditional effect. So one possible schedule of the relaxed plan repeatedly
puts an object in the briefcase, applies move-briefcase, then proceeds to the next object.
This plan is n  1 steps longer than the optimal relaxed plan, which first places all of the
objects in the briefcase and then applies move-briefcase once.
In other words, as a single action execution may trigger several conditional effects at
once, there may exist a relaxed plan of length less than |(G)|. The question then arises of
how to optimally schedule the relaxed plan, minimizing the number of action applications
required. FF uses a simple approximate solution to this problem, that we outline and
improve upon below. But we first note that the problem of scheduling conditional relaxed
plans (SCRP) is actually NP-complete:
Theorem 7 (Scheduling conditional relaxed plans) Let + be a relaxed planning task
with conditional effects and (G) a set of effects that, viewed as a set of independent actions,
constitutes a plan for + . Deciding whether there exists a sequence of actions of length  k
such that all conditional effects in (G) are triggered is NP-complete.
Proof: Membership follows from the fact that given a sequence of k actions, it can easily be
checked in polynomial time whether all conditional effects in (G) are triggered. Hardness
follows by reduction from the shortest common supersequence problem (SCS) (Garey &
Johnson, 1979). A supersequence of a string x = d0 . . . dm over the alphabet  is a string
over the same alphabet that belongs to the language L =  d0  . . .  dm  . Given an
instance of the SCS problem with strings x0 , . . . , xn over the alphabet {0, 1} that asks
whether there exists a supersequence of these strings with length  k, we construct a
planning task with conditional effects  = hF, A, I, Gi, where
S
 F = ni=0 {yij | 0  j  |xi |}
 A = {a0 , a1 }, where az = {, , , ce(az )}, and ce(az ) is given by the set of conditional
effects
n |x[
i |1
[
{hyij , yij+1 , i | xij = z}
i=0 j=0

 I = {y00 , . . . , yn0 }
 G = {y0|x0 | , . . . , yn|xn | }
499

fiKeyder, Hoffmann, & Haslum

The two actions a0 and a1 correspond to the addition of the symbols 0 and 1 respectively
to the supersequence that is implicitly being constructed, and a fluent yij encodes the fact
that the current string constitutes a supersequence for a prefix xi0 , . . . , xij1 . It can then
be seen that a valid plan for the planning task must trigger all of the conditional effects
of the task, yet such a sequence of actions with length  k exists iff there is a common
supersequence of x0 , . . . , xn with length  k. This transformation of the SCS problem into
a planning task with conditional effects is polynomial, which shows the claim.
Note that Theorem 7 does not relate to the (known) hardness of optimal relaxed planning:
we wish only to schedule effects that we have already selected and which we know to form
a relaxed plan. This source of complexity has, as yet, been overlooked in the literature.
Given this hardness result, we employ a greedy minimization technique that we call
conditional effect merging. Starting with the trivial schedule containing one action execution
for each effect in (G), we consider pairs of effects e, e0  (G) that are conditional effects of
the same action a. The two effects are merged into a single execution of a if their conditions
can be achieved without the use of either of their add effects. FFs approximation method
applies similar reasoning, but captures only a special case in which this condition holds:
when both e and e0 appear in the same layer of the relaxed planning graph, which trivially
implies that the conditions of these effects are independently achievable. However, the
same may also be the case for effects in different layers of the relaxed planning graph. Here
we devise a strictly more general technique, capturing this form of independence between
effects using what we call the best supporter graph (BSG) representation of the relaxed plan
(for simplicity, we assume here that the task has a single goal fluent G0 , which if needed can
be achieved by introducing a new action end whose preconditions are the original goals,
and that adds G0 ):
Definition 4 (Best supporter graph) Given a relaxed planning task + and a best supporter function bs, the best supporter graph is a directed acyclic graph  = hV, Ei, where
V = (G), with (G) as above, E = {hv, v 0 i | p  pre(v 0 )  v = bs(p)}, each vertex is
labeled with the action whose conditional effect it represents, and each edge is labelled with
the set of preconditions {p | p  pre(v 0 )  v = bs(p)}.
The nodes of this graph represent conditional effects that appear in the relaxed plan, and
there exists an edge hv, v 0 i between two nodes if the effect represented by v is the best
supporter of a (pre)condition of the effect represented by v 0 .4 bs being a valid best supporter
function (i. e., the relaxed plan (G) generated by bs being sound) is a sufficient condition
for  being acyclic, and it can easily be shown that any topological sort of  is a sound
relaxed plan. This implies that, if there is no path in  between two conditional effects of
the same action, they can occur as the result of the same action application, and therefore
can be merged into a single occurrence of the action. These nodes are then removed from
the BSG, and a new node is added that represents both effects, combining their incoming
and outgoing edges. This process can be repeated until no further node merges are possible.
The algorithm runs in polynomial time and is sound in that it results in a BSG of which
any topological sort constitutes a relaxed plan for . It does not, however, guarantee an
optimal scheduling of the original plan.
4. The edge labels will be used in our procedure choosing the conjunction set C, described in Section 5.2.

500

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

For example, consider again the task where move-briefcase has n conditional effects
transporting an object from location A to location B if it is inside the briefcase. The nodes
in the BSG are n put-into-briefcase(oi ) actions (one for each object oi ), as well as n copies
of move-briefcase(A, B) (one for the conditional effect regarding each object oi ). There is
one edge from put-into-briefcase(oi ) to the respective copy of move-briefcase(A, B), labeled
with in-briefcase(oi ). There is therefore no path in the graph from any move-briefcase(A, B)
node to another, and they will be merged into a single node by the conditional effect merging
algorithm. All topological sorts of that merged BSG correspond to optimal relaxed plans.
5.2 Choosing C for Relaxed Planning
Algorithm 1 shows the main procedure for computing the set of conjunctions C used to form
the C
ce task. The algorithm is applied once, before the start of the search, to the initial
C
state of the planning task. The resulting C
ce (or  ) task is then used for all subsequent
heuristic evaluations. Conditional effect merging is not used during the conflict extraction
phase in any configuration discussed below, i. e., we use the original non-merged BSG as
stated in Definition 4.
Algorithm 1: Choosing C for relaxed plan heuristics.
C=
 = RelaxedPlan(C
ce )
while  not a plan for  and size(C
ce ) < bound do
C = C  FindConflicts()
 = RelaxedPlan(C
ce )
Algorithm 1 is, at a high level, very similar to the procedure previously introduced for
computing incremental cost lower bounds based on the C construction (Haslum, 2012).
The algorithm repeatedly generates relaxed plans for the initial state of the current compiled
task. It adds new conjunctions to C based on the conflicts that are found in the current
plan, i. e., based on how the current relaxed plan fails when executed in the original planning
task . The process stops when either no further conflicts can be found, implying that the
current relaxed plan for C
ce is a plan for the original planning task, or when a user-specified
C
bound on the size of ce is reached. We will express this bound in terms of the size of C
ce
compared to  (see below). We will also sometimes impose a bound on the runtime of the
algorithm.
If no bound is specified, and if FindConflicts() returns at least one new conjunction as
long as  is not a plan for , Algorithm 1 is a complete planning algorithm in its own right.
We report results for this usage of the algorithm in our experiments below. If the relaxed
plan generated in each iteration is optimal, Algorithm 1 can be used to compute a sequence
of admissible cost estimates that converges to the optimal plan cost (Haslum, 2012). Our
focus, however, is on the use of C
ce for generating inadmissible heuristic functions. We
therefore use a tractable, non-optimal, relaxed planning procedure, and impose a bound
that typically stops Algorithm 1 before a plan for the original task has been found.
It remains to specify the FindConflicts procedure: Given a relaxed plan that fails to
execute in the original planning task , how to select the set of new conjunctions C? One
501

fiKeyder, Hoffmann, & Haslum

answer to this question has been provided by the previous use of Algorithm 1 to compute
plan cost lower bounds (Haslum, 2012). Because our aim is different  computing heuristics
for satisficing search-based planning  we make a number of changes to the previously
proposed version of FindConflicts. Section 5.2.1 summarizes the original procedure, and
Section 5.2.2 describes the changes we make to it.
5.2.1 Conflict Extraction for Incremental Plan Cost Lower Bounds
Given an optimal relaxed plan that is not a plan for the original planning task, Haslums
(2012) version of FindConflicts returns a set of conjunctions C that prevents the same relaxed
plan from being a solution in the next iteration. This ensures progress, in the sense that
the cost of the relaxed plan will eventually increase, or prove to be the real plan cost. To
describe the conflict extraction procedure, we need two definitions:
Definition 5 (Relaxed Plan Dependency Graph) Let  be a non-redundant plan for
the relaxed planning task + . Construct a directed graph G () with one node va for each
action a in , plus a node vG representing the goal. Let pre(v) denote the precondition of
node v, which is pre(a) for a node va and G for node vG . G (S) has a directed edge from
va to v 0 iff pre(v 0 ) is not relaxed reachable using the set of actions in  minus {a}. The
edge is labelled with the subset of pre(v 0 ) that is relaxed unreachable with these actions. The
relaxed plan dependency graph, RPDG(), is the transitive reduction of G ().
The RPDG is similar to the BSG above (Definition 4), but encodes the necessary dependencies between actions in a relaxed plan. A path from a node va to a node vb in the
RPDG implies that a precedes b in every valid sequencing of ; in this case, vb is said
to be ordered after va . In contrast, the BSG encodes the intentions of the relaxed plan
heuristic, in the form of the chosen best supporters, and may impose orderings that need
not be respected in every valid sequencing of the plan (e. g. if a fluent p is added by another
action in the relaxed plan that is not the best supporter of p). Because the relaxed plan is
non-redundant, meaning that no action can be removed without invalidating it, there is a
path from every action node in the RPDG to the goal node.
Definition 6 (Dependency Closure) Let  be a non-redundant plan for the relaxed planning task + , and let v and v 0 be nodes in RPDG(), where v 0 is ordered after v. A simple
q1
q2
qm
dependency path is a path v  v1  . . .  v 0 from v to v 0 in RPDG(), where each edge
is labelled by one fluent, chosen arbitrarily, from the edge label in RPDG(). (Whenever
v 0 is ordered after v, a simple dependency path from v to v 0 exists.) A dependency closure
from v to v 0 is a minimal, w.r.t. subset, union of paths, such that (1) it contains a simple
dependency path from v to v 0 , and (2) if q is the fluent that labels an edge from a node v 00
in the closure, and a is an action with q  add(a), where a is not the action associated with
v 00 , then the closure contains a simple dependency path from v to the node corresponding to
a. (Such a path is guaranteed to exist.)
Recall that input to FindConflicts is a plan, , that is valid for the delete relaxation +
but not for the original planning task  when delete effects are considered. Because  is
valid for + , the preconditions of all actions in , as well as all goals, must be made true at
502

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

...
vd
p1
r
...
vf
q1

p
vd

q1

...

qn

vf

pn
vj
qm

(b)

(a)

Figure 1: Relaxed plan failure scenarios. Wavy edges show deletions of a precondition.
some point. Thus, if  fails to solve the original task  it must be the case that some action
d, which we call the deleter, deletes a precondition of some other action f , which we called
the failed action. Note that the failed action here can also be the goal. Let p  pre(f )
be the deleted fluent. The procedure distinguishes two cases, based on the relation between
nodes vd and vf in the RPDG:
In the first case, illustrated in Figure 1 (a), vf is ordered after vd . Choose a dependency
closure from vd to vf , and let L be the set of fluents labelling the edges in this closure: the
set of conflicts generated is {{p, q} | q  L}. (Note that p 6 L, and thus each conflict is a
proper conjunction.)
If the first case does not hold, then vd and vf are unordered. They must have a nearest
common descendant node, vj , in the RPDG, so we are in the situation illustrated in Figure 1
(b). Choose a dependency closure from vd to vj , and let L1 be the set of fluents labelling
the edges in this closure. Likewise, choose a dependency closure from vf to vj , and let L2
be the set of fluents labelling the edges in this closure. The set of conflicts generated is then
{{q, q 0 } | q  L1 , q 0  L2  {p}}.
Theorem 8 (Haslum, 2012, Theorem 6) Let  = a1 , . . . , an be a non-redundant plan
for the delete relaxed task + that is not valid for the original task , and let C be a set of
conjunctions extracted by the procedure described above. No action sequence  0 = a01 , . . . , a0n
such that each a0i is a representative of ai is a valid plan for C .
5.2.2 Changes to Conflict Extraction for Satisficing Planning
There are a number of differences between our setting and that of Haslum (2012). In
particular, although -fluents are collected only in the initial state, the resulting C
ce task
will be used for heuristic evaluations of all states encountered in the search, and growth
in the size of the C
ce task will incur an overhead on each heuristic evaluation. Thus, our
objective is to find a set C that will make the heuristic more accurate across all states,
while keeping the size of C limited. On the other hand, computing non-optimal relaxed
plans is computationally far cheaper than optimal relaxed planning, so we can afford more
iterations in Algorithm 1.
Therefore, we make the following modifications to the strategy: First, we use the BSG
instead of the RPDG. The necessity of orderings in the latter does not extend beyond
the current (initial) state, and therefore is not useful for our purpose. The BSG is more
representative of the relaxed plans found by the non-optimal relaxed planning procedure.
Second, we introduce just a single -fluent in each iteration of Algorithm 1. Most of the
time, this does cause a new relaxed plan to be found, which allows the algorithm to focus
on finding a small number conflicts that are useful in a wide range of states. The chosen
conflict is {p, qn } in the case depicted in Figure 1 (a), and {pn , qm } in that of Figure 1 (b).
503

fiKeyder, Hoffmann, & Haslum

Intuitively, this works better in our setting because the set of all conflicts generated by the
same plan failure tends to be redundant, and thus needlessly grows the size of the task
leading to slow evaluation times without much gain in informativeness.
These changes do not affect the fundamental property of Algorithm 1, that it converges
to a real plan. To show convergence, the only property that FindConflicts must have is that
it returns at least one new conjunction whenever  fails to solve the original task. Our
variant still gives that guarantee:
Lemma 3 Assume we eliminate dominated preconditions 5 in C . Let  = a1 , . . . , an be
a non-redundant plan for C that is not valid for the original task , and let c be the
conjunction extracted by the procedure described above. Then c 6 C.
Proof: This is simply because, in both possible relaxed plan failure scenarios (Figure 1), the
chosen conjunction c = {x, y} ({x, y} = {p, qn } respectively {x, y} = {pn , qm }) is contained
in the precondition of the failed action f . Assuming that c = {x, y}  C, as we eliminate
dominated preconditions, no action precondition in C contains both x and y. Hence, in
that case, c cannot be the chosen conjunction.
Theorem 9 (Convergence of conflict extraction) Assume we eliminate dominated preconditions in C , and Algorithm 1 is run without a size bound. Then eventually  will be
a plan for .
Proof: Follows from Lemma 3 as the set of possible conjunctions is finite.
Contrasting Theorem 9 with Haslums variant (Theorem 8), the latter gives a stronger
convergence guarantee (only) in the sense that it guarantees a certain minimum progress
is made in each iteration.
Lemma 3 (and thus Theorem 9) holds in the same way for C
ce , i. e., when  is a sequence
C
of conditional effects in ce , that, viewed as a set of independent actions, constitutes a nonredundant plan for C
ce . We rely on eliminating dominated preconditions here as that
makes the proof very simple, and we use the technique in practice anyway. We did not
verify whether or not convergence holds also if dominated preconditions are not eliminated;
we conjecture that it does.
Since there can be multiple conflicts in the BSG of a relaxed plan, in our experiments
we choose (arbitrarily) one that minimizes the number of conditional effects (or STRIPS
actions, in the case of C ) created. We place a bound on the factor x by which C
ce exceeds
the size of the original planning task . Precisely, when x = 1, no -fluents or conditional
+
effects are added, and C
ce =  , resulting in a standard relaxed plan heuristic. For growth
bounds x > 1, -fluents are added until the number of conditional effects in the task reaches
(x  1)  |A|. For C , x limits the total number of actions in the task as a multiple of |A|.

5. Recall that eliminating dominated preconditions means that, whenever we add a fluent c to the precondition of an action, or to the condition of a conditional effect, we remove from that condition all fluents
p  c and -fluents {c0 | c0  c}.

504

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Example 2 Consider again the STRIPS planning task from Example 1, with variables
{x0 , . . . , xn , y}, initial state I = {x0 , y}, goal G = {xn }, and unit-cost actions
a : h, {y}, , i

bi : h{xi , y}, {xi+1 }, {y}, i

for i = 0, . . . , n  1.
As previously discussed, setting C = {x1 ,y , . . . , xn1 ,y } renders the delete relaxation
perfect, i. e., results in the relaxed plan having to re-establish y in between every two bactions. Exactly that set C is iteratively selected by our procedure.
Assuming a best supporter function based on either of hadd or hmax , in the first iteration
of Algorithm 1 the BSG will be:

b0

x1

b1

x2

...

b2

bn2

xn1

bn1

The relaxed plan fails to execute when trying to apply the second action, b1 . The corresponding failure scenario matches Figure 1 (a):

y
b0

b1

x1

The chosen conflict thus is {y, x0 }. With the now non-empty set of conjunctions C containing just that single conjunction, the precondition of b1 contains {x1 ,y} which must be
established using action a, so that the BSG now takes the form (note that the dominated
preconditions y and xi of b1 are eliminated):

b0

x1

a

{x1 ,y}

b1

b2

x2

...

bn2

xn1

bn1

The relaxed plan now fails to execute when trying to apply the fourth action, b2 . The
corresponding failure scenario is:

y
b1

x2

b2

The chosen conflict now is {y, x2 }. Iterating the procedure will, in the same manner, select
exactly the set C above one-by-one, at the end of which the relaxed plan will solve the original
planning task.
5.3 Experiments
We evaluate the impact of using the C
ce compilation in a relaxed plan heuristic in the context of a greedy search. The expected impact of using a heuristic based on the improved
relaxation is two-fold. On the one hand, it should make the heuristic more informative,
505

fiKeyder, Hoffmann, & Haslum

enabling the search to find plans with fewer node evaluations. On the other hand, there is a
computational overhead associated with the growth of the problem, slowing down heuristic
evaluations. We examine both of these effects individually, as well as their combined influence on coverage, or the set of problems that the planner is able to solve within given time
and memory bounds, which we take to be the main measure of performance.
In this study, we do not consider the objective of producing plans of high quality (as
measured by plan length or cost). That is not because plan quality is unimportant. Rather,
the rationale for this decision is methodological: Seeking a high quality plan is not the
same problem as seeking to find a plan with minimum search effort  particularly when
quality is measured by non-unit action costs  and the requirements on heuristics for the
two problems are quite different. Here, we have chosen to focus on one, viz. search efficiency,
as measured by coverage and node evaluations, rather than conflate the two. The choice of
a plain greedy search algorithm is also motivated by this decision. As a consequence, we
treat all actions as having a unit cost of 1. Previous experiments have shown that in the
context of greedy search, distinguishing action costs in heuristic calculation tends to result
in lower coverage (Richter & Westphal, 2010). However, to at least assess the impact of our
heuristics on plan quality, we do report data regarding plan length.
We next describe the experiment setup and baseline. We then discuss heuristic informativeness, computational overhead, the impact of conditional effect merging, the impact on
plan length of using C
ce heuristics, a comparison with other state-of-the-art heuristics for
the same problem, the difference between using the C and C
ce compilations, and finding
plans with no search.
5.3.1 Experiment Setup and Baseline
The compilation and associated heuristics were implemented in the Fast Downward planner
(Helmert, 2006), and used in a greedy best-first search, with lazy evaluation and a second
open list (with boosting) for states resulting from preferred operators. The planners were
tested on all of the STRIPS domains from the 19982011 editions of the International
Planning Competition (IPC). For domains from the last two IPCs, only the most recent
sets of instances were used. All experiments were run on Opteron 2384 processors with the
settings used in the competition: a memory limit of 2Gb and a time limit of 30 minutes.
The baseline planner configuration uses the relaxed plan heuristic, with best supporters
identified by hadd , on the unmodified planning task (i. e., with the growth bound x = 1).
It is a known fact that greedy search, and in particular greedy search with lazy evaluation
and a strong bias towards preferred operators, can be highly sensitive to small changes
in the relaxed plan, even changes that do not alter the heuristic value but rather only the
operators that are preferred. Unfortunately, this fact is very rarely taken into account when
heuristics are compared in the context of greedy search. Since the introduction of -fluents
alters the structure of the relaxed plan, we believe it is particularly important to determine
whether the resulting differences in planner performance are really due to the relaxed plan
being more (or less) informative.
Therefore, as a first step towards accounting for the brittleness of experiments with
greedy heuristic search, we introduce a simple variance measure and use it to decide when the
results of our experiments should be considered significant. Variance in the performance
506

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

of the baseline planner is measured by randomizing the choice of supporters with equal
hadd values in the construction of the relaxed plan and measuring the maximum deviation
from the results of the baseline planner over five repeated runs. The results are shown in
columns labeled MaD (Tables 1 and 4). For each domain and for the problem set as a
whole, the deviation is defined by the differences in coverage and in the median number of
heuristic evaluations. Note that we are not interested in whether randomization helps
or hurts the search, but rather in the magnitude of the variation that it causes. When
C
comparing the results of the planner using heuristics based on C
ce or  under different
growth bounds with the results of the baseline planner, we consider the difference between
them to be significant if it is greater in magnitude than the maximum deviation observed
with randomization of the baseline. This should not be interpreted as significance in the
statistical sense (although, if we assumed that randomization affects all heuristics equally,
we could estimate the probability of the hypothesis of no difference), but simply as setting
a reasonable threshold for what counts as a substantial difference in search performance.

5.3.2 Heuristic Informativeness
The comparison of heuristic informativeness is summarized in the right half of Table 1,
which shows the ratio of the median (per domain, over tasks solved by both planners)
number of heuristic evaluations for the baseline planner to that of the planners using the
C
ce -based heuristics. In just under half the domains, the difference in informativeness of the
C
ce -based heuristics compared to the baseline does not exceed the threshold for significance
set by the sensitivity study (shown in the MaD column). Among the domains where
there is a significant difference, in the majority using the C
ce -based heuristics reduces the
number of node evaluations, indicating that the augmented heuristics are more informative.
In most of these cases, the ratio grows as more -fluents are added, i.e., as the growth bound
x is increased. The most drastic example can be seen in the Floortile domain, where all
C
ce -based heuristics evaluate four or more orders of magnitude fewer nodes, compared to
the standard delete relaxation heuristic. This allows them to easily solve all of the instances
in the domain. By comparison, no planner in IPC 2011 was able to solve more than 9 of
the 20 instances in this domain. In the Woodworking domain, C
ce heuristics are more than
two orders of magnitude more informative, but there is no associated increase in coverage
as all tasks are solved by all configurations.
In roughly a third of the domains there is a consistent (or nearly consistent) loss of
informativeness, though in most of them it is not significant. Note that this loss of informativeness does not always correlate with a loss in coverage. This can be attributed to different
factors, including the small magnitudes of loss, as well as the fact that the ratio of node
evaluations is taken only over tasks solved by both the planners compared. Another issue is
that dramatic coverage losses are often due to the computational overhead incurred by the
C
ce compilation. In particular, in the Openstacks and Satellite domains, the decrease in the
number of tasks solved with the C
ce -based heuristics matches almost exactly the number of
tasks for which the conflict selection and compilation process fails to complete within the
1800 seconds allocated per task. We get back to this in the next subsection.
507

fiKeyder, Hoffmann, & Haslum

It is worth noting that the quality of C
ce -based heuristics can be highly sensitive to the
precise choice of -fluents used in the compilation.6 Hence, there may exist better policies
for making this choice than the relatively simple one we have used here.
The HO and PO colums in Table 1 (coverage only) examine the effect of the new
heuristic function, respectively the new preferred operators returned by that function, in
separation. PO corresponds to a configuration that uses the relaxed plan from the C
ce task
(built with x = 1.5 and a timeout of t = 60s, discussed in Section 5.3.3) only to identify
preferred operators, together with the heuristic value from the baseline (x = 1) heuristic.
HO, on the other hand, uses the heuristic values obtained with x = 1.5 and the preferred
operators from x = 1. Interestingly, either heuristic values or preferred operators alone are
sufficient to greatly improve coverage in Floortile, the domain in which our techniques have
the greatest impact. Both the HO and PO configurations are able to solve every instance
of this domain.7 The effect in other domains is mixed, with both configurations solving
sometimes more, sometimes fewer instances.
5.3.3 Computational Overhead
The computational overhead of the C
ce -based heuristics, compared to the standard relaxed
plan heuristic, stems from two sources: (1) the time spent on computing the set of -fluents
to add to the problem, and (2) the greater overhead of heuristic evaluation in the C
ce task.
Table 2 shows three measures of their impact.
The first four columns (under Timeouts) show the number of instances in which the
construction of the C
ce task does not finish within 1800 seconds, while the second set of
four columns (under > 60 sec) shows the number of instances for which the construction
time exceeds 60 seconds (inclusive of those instances in the first set of columns). Note that
this behavior  spending a large amount of time on the C
ce construction without reaching
the growth bound  is partly due to our strategy for selecting -fluents, since we purposely
choose those -fluents which will increase the size of the compiled task the least. While
there are several domains in which construction time frequently exceeds 60 seconds, this
does not happen in those domains where the C
ce -based heuristic are most informative, such
as Floortile and Woodworking. This suggests that imposing a time limit on the construction
of the C
ce task will incur only a small loss of informativeness. We present coverage results
for such a strategy (using a 60 second time limit) in Table 4 below. It is significantly better
than the baseline planner, and compares favourably with other state of the art heuristics.
As expected, in most domains evaluating heuristics on the C
ce task is slower than on the
standard delete relaxation, and tends to slow down more as the growth bound x increases,
due to the larger number of fluents and actions in the compiled planning task. The median
slowdown per domain is typically of the same order as x itself, and exceeds one order of
6. Indeed, the results reported in our earlier paper (Keyder, Hoffmann, & Haslum, 2012) show an increase
in informativeness in the Barman and Parcprinter domains.
7. A plausible explanation for this is the behavior with respect to dead-end states (intuitively, where the
robot has painted itself into a corner) that are unrecognized under the standard delete relaxation
heuristic, i. e., where a relaxed plan for  exists. It appears that C
ce is highly effective at fixing this
issue: While under x = 1 search encounters millions of states with hFF () = , HO encounters only
FF
few states with hFF (C
(C
ce ) =  (suggesting that h
ce ) prunes dead-ends early on), and PO encounters
no such states at all (suggesting that hFF (C
)
preferred
operators prevent the search from entering the
ce
dead-end regions in the first place).

508

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Domain
x=1 MaD
Airport (50)
36
Barman (20)
13
Blocksworld (35)
35
Depots (22)
19
Driverlog (20)
20
Elevators (20)
19
Floortile (20)
6
FreeCell (80)
79
Grid (5)
5
Gripper (20)
20
Logistics00 (28)
28
Logistics98 (35)
34
Miconic (150)
150
Mprime (35)
35
Mystery (30)
16
Nomystery (20)
9
Openstacks (20)
20
Parcprinter (20)
16
Parking (20)
20
Pathways (30)
30
Pegsol (20)
20
Pipes-NoTk (50)
42
Pipes-Tank (50)
38
PSR (50)
50
Rovers (40)
40
Satellite (36)
35
Scanalyzer (20)
18
Sokoban (20)
19
Tidybot (20)
16
TPP (30)
30
Transport (20)
11
Trucks (30)
14
Visitall (20)
19
Woodwork (20)
20
Zenotravel (20)
20
Total (1126)
1002

+2

+5

+0

+1

+0

+3

+0

+2

+0

+0

+0

+1

+0

+0

+0

+2

+0

+7

+1

+2

+0

+1

+4

+0

+0

+1

+1

+1

+3

+0

+1

+0

+1

+0

+1

+19


Coverage
x =
PO HO
1.5 2 2.5 3 1.5 1.5
+0
+1 +3 +1 +2 +1 
+6 +3 +1 3 3 +5
+0 
+0 
+0 
+0 
+0 
+0

+0 1
+2 +2 +2 +2 
+0 
+0 
+0 
+0 
+0 
+0

+1 +1 +1 +1 +1 +1
+14 +14 +14 +14 +14 +14
+0 +1
+1 3 4 2 
+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 
+0 
+0 
+0

+0 1 
+0 +1 +1
+1 
+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 
+0 
+0 
+0

+3 +3 +3 +3 +3 +1
2 3 3 3 1 2
9 9 9 9 1 1
11 8 7 7 4 10
+0 6
2 8 7 5 
+0
1 2 5 1 1 
+0 
+0 
+0 
+0 
+0 
+0

+0 1 
+0 
+0 1 
+0

+0 +2 1 +3
+3 +2 
+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 1 
+0
1 
+0
2 5 6 8 +1 
+2 +2 +2 +2 1 +2
+0 2
2 2 3 3 
+0 
+0 1 1 
+0
2 
+0 
+0 
+0 
+0 
+0 
+0

+2 2 +1 +1 7 3
+0 +2
+1 +3 +4 +2 
+1 +1 2 2 2 2
+0 
+0 
+0 
+0 
+0 
+0

+0 
+0 
+0 
+0 
+0 
+0

+6 9 17 14 3 +3

Median Node Evaluations Ratio
x =
1.5
2
2.5
3
4.32
1.36 : 1
1.34 : 1
1.40 : 1
1.50 : 1
9.91
1 : 1.63
1 : 2.22
1 : 8.33
1 : 25
5
2.84 : 1
2.90 : 1
2.90 : 1
3.02 : 1
12.13
2.84 : 1
3.26 : 1
3.65 : 1
8.55 : 1
1.16 2.17 : 1
2.64 : 1
2.65 : 1
2.31 : 1
1.31
1.25 : 1
1.47 : 1
1.34 : 1
1.22 : 1
6.42 15013 : 1
15110 : 1
14757 : 1
19674 : 1
1.28
1 : 1.11
1 : 1.28
1 : 1.17
1 : 1.29
1.21 1.27 : 1
2.83 : 1
1.27 : 1
1.27 : 1
1
1 : 1.05
1 : 1.51
1 : 1.49 1.23 : 1
1.64
1.47 : 1
1.47 : 1
1.53 : 1
1.60 : 1
1.45 2.53 : 1
2.57 : 1
2.83 : 1
2.70 : 1
1 1.16 : 1
1.22 : 1
1.29 : 1
1.36 : 1
1.09 2.33 : 1
2.33 : 1
2.33 : 1
2.33 : 1
1.07 1.21 : 1
1.27 : 1
1.29 : 1
1.29 : 1
13.93
1 : 1.44
2.58 : 1
9.95 : 1
10.13 : 1
1.08 2.52 : 1
1.31 : 1
1 : 1.23
1 : 1.11
1.58
1 : 1.04
1 : 1.04
1 : 1.08
1 : 1.09
7.06
1 : 1.12
3.02 : 1
1.61 : 1
1.18 : 1
1.21
1 : 1.03
1:1
1 : 1.05
1 : 1.11
3.42
1.44 : 1
1.09 : 1
1.66 : 1
1.41 : 1
16.47
1 : 1.07
1.08 : 1
1.14 : 1
1.29 : 1
2.46
1:1
1:1
1:1
1 : 1.05
1
1:1
1.02 : 1
1.12 : 1
1.12 : 1
1.33
1.12 : 1
1.15 : 1
1.12 : 1
1.20 : 1
1.36
1.15 : 1
1.36 : 1
1.30 : 1
1.29 : 1
1.85
1.17 : 1
1.80 : 1
3.43 : 1
3.25 : 1
1.46
1.17 : 1
1 : 1.09
1.01 : 1
1.06 : 1
1.22
1.15 : 1
1 : 1.25
1 : 1.38
1 : 1.08
2.29
1 : 1.20
1 : 1.13
1 : 1.19
1 : 1.01
2.73
1.20 : 1
1 : 1.11
1 : 1.29
1 : 1.01
1.66
1.16 : 1
1.97 : 1
7.27 : 1
4.57 : 1
1.34
1.01 : 1
1 : 1.08
1 : 1.09
1 : 1.49
52.78 245.72 : 1
263.3 : 1
245.72 : 1
245.72 : 1
1.28
1.22 : 1
1.24 : 1
1.36 : 1
1.42 : 1
MaD

Table 1: Planner coverage and heuristic informativeness using C
ce with varying growth bounds,
without conditional effect merging. Coverage shows the number of problems solved for the baseline
configuration (x = 1), and the difference (increase/decrease) relative to the baseline for the other
configurations; PO uses only the preferred operators obtained from the C
ce compilation with
x = 1.5 and t = 60s, returning the x = 1 heuristic value, while HO uses the heuristic values
obtained with x = 1.5 and t = 60s, and the preferred operators from x = 1. Heuristic informativeness
is measured by the ratio of the per-domain median number of node evaluations, comparing the
baseline to our configurations (across those instances solved by both configurations), normalized so
that the smaller value is 1. That is, an entry m : 1 means that the baseline planner requires m times
as many heuristic evaluations as the other planner. Columns labeled MaD show the magnitude
of the maximum deviation (in coverage and ratio) from the baseline in our sensitivity study: values
in bold are those that exceed this threshold, and which we therefore consider to be significant.

magnitude only in the Floortile domain when x = 1.5. Somewhat surprisingly, there are
domains in which heuristic evaluations become faster as more -fluents are added. A possible
explanation for this is that because we eliminate dominated preconditions (cf. Section 3),
the number of action preconditions decreases and the delete-relaxation hypergraph of the
C
ce becomes more graph-like as a result.
509

fiKeyder, Hoffmann, & Haslum

Domain
Airport (50)
Barman (20)
Blocksworld (35)
Depots (22)
Driverlog (20)
Elevators (20)
Floortile (20)
FreeCell (80)
Grid (5)
Gripper (20)
Logistics00 (28)
Logistics98 (35)
Miconic (150)
Mprime (35)
Mystery (30)
Nomystery (20)
Openstacks (20)
Parcprinter (20)
Parking (20)
Pathways (30)
Pegsol (20)
Pipes-NoTank (50)
Pipes-Tank (50)
PSR (50)
Rovers (40)
Satellite (36)
Scanalyzer (20)
Sokoban (20)
Tidybot (20)
TPP (30)
Transport (20)
Trucks (30)
Visitall (20)
Woodwork (20)
Zenotravel (20)
Total (1126)

Timeouts
x =
1.5 2 2.5 3
1
1
1
1

1.5
15

> 60 sec
x =
2
2.5
20
23

3
26

1

9

3

9

6

16

10

10

12

1

1

3

6

9

10

9

9

16

16

16

16

14

16
3

16
6

16
7

5
9
3

5
11
3

5
13
3

5
13
4

6

1
5
12
2
8

1
5
16
2
9

1
6
17
4
10

96

118

134

148

7

2

13

10

20

9

3

23

4
11

Ratio of Median Evaluations/sec
x =
1.5
2
2.5
3
1.08 : 1
1.19 : 1
1.30 : 1
1.26 : 1
1.35 : 1
1.88 : 1
2.42 : 1
3.09 : 1
1.50 : 1
1.60 : 1
1.56 : 1
1.42 : 1
1.65 : 1
1.81 : 1
2.03 : 1
2.31 : 1
1.56 : 1
2.38 : 1
3.17 : 1
4.07 : 1
1.87 : 1
2.92 : 1
3.99 : 1
5.45 : 1
17.67 : 1
8.28 : 1
5.97 : 1
5.49 : 1
1.20 : 1
1.31 : 1
1.39 : 1
1.66 : 1
1.27 : 1
2.55 : 1
3.04 : 1
3.04 : 1
1 : 2.13
1 : 2.04
1 : 2.13
1 : 1.69
1.09 : 1
1 : 1.76
1 : 1.35
1 : 1.22
1.20 : 1
1.64 : 1
2.55 : 1
3.72 : 1
1.06 : 1
1.10 : 1
1.18 : 1
1.27 : 1
1.60 : 1
1.80 : 1
1.60 : 1
1.80 : 1
1.28 : 1
1.35 : 1
1.42 : 1
1.42 : 1
1.43 : 1
2.20 : 1
2.53 : 1
3.26 : 1
1.16 : 1
1.73 : 1
2.49 : 1
3.17 : 1
1 : 8.33
1 : 8.33
1 : 6.25
1 : 1.81
2.32 : 1
3.61 : 1
4.84 : 1
6.20 : 1
1.36 : 1
2.04 : 1
1.01 : 1
1.64 : 1
1.25 : 1
1.08 : 1
1.48 : 1
1.11 : 1
1.41 : 1
1.94 : 1
2.31 : 1
2.78 : 1
1.61 : 1
2.37 : 1
2.42 : 1
3.39 : 1
1:1
1:1
1.02 : 1
1 : 1.05
1.11 : 1
1.43 : 1
1.48 : 1
1.83 : 1
1.14 : 1
1.07 : 1
1 : 1.01 1.30 : 1
1.09 : 1
2.22 : 1
2.42 : 1
2.73 : 1
1.23 : 1
1.38 : 1
1.62 : 1
1.82 : 1
1.48 : 1
2.03 : 1
3.36 : 1
2.22 : 1
1.59 : 1
1.70 : 1
2.38 : 1
2.84 : 1
2.38 : 1
4.04 : 1
6.50 : 1
8.80 : 1
1.62 : 1
2.29 : 1
2.52 : 1
3.30 : 1
1 : 1.49
1 : 1.33
1 : 1.15
1 : 1.07
1 : 2.63
1 : 2.08 3.29 : 1
6.32 : 1
1.09 : 1
1:1
1.06 : 1
1.10 : 1

Table 2: Computational overhead of C
ce . The first set of columns (Timeouts) shows the number

of tasks for which the C
ce construction does not finish within the 1800 second time limit, and the
second set (> 60 sec) shows the number of tasks for which construction time exceeds 60 seconds
(inclusive of those in the first set of columns). To improve readability, only non-zero entries are
shown (i.e., the blank cells in these columns are zeroes). The last set of columns shows the median
(per domain, over commonly solved tasks) ratio of heuristic evaluations per second for the baseline
planner (x = 1) to that of the other planner (x > 1). An entry m : 1 means that the baseline planner
performs m times as many heuristic evaluations per second as the other planner.

5.3.4 Conditional Effect Merging
In a majority of domains, conditional effect merging slightly increases or does not change
the informativeness of the C
ce -based heuristics. Some exceptions to this are the Logistics00
and Gripper domains, where merging results in a heuristic that is twice as informative
(using the same ratio of median number of evaluations metric as presented in Table 1), the
Nomystery domain where it is an order of magnitude more informative (for the problems
that are solved by both heuristics), and the Barman domain where it is four times less
informative. In general, higher informativeness occurs in domains in which all tasks are
510

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

solved by all planners, so it does not result in increased coverage. Indeed, as shown in
Table 4 below, conditional effect merging proves to be detrimental to the overall coverage
C
of the planner using the C
ce -based heuristic: the best ce configuration with conditional
effect merging solves, in total, only 4 more tasks than the standard relaxed plan heuristic,
while the same configuration without conditional effect merging solves 20 more tasks.
The runtime overhead of the merging procedure is quite small, as the transitive closure
operation required to check whether there is a path between two nodes of the BSG can be
implemented very efficiently when the graph is known to be directed acyclic, as is the case
here. For x = 1.5, comparing the C
ce -based heuristic with conditional effect merging to
that without, the ratio of the median number of heuristic evaluations per second (the same
metric as used on the right-hand side of Table 2) shows a maximal per-domain slow-down
of 2.04, and an across-domain average of 1.11. Coverage decreases in domains other than
Barman therefore appear to be due to the sensitivity of search to small changes in the
heuristic function (rather than due to the time taken to compute that function).
5.3.5 Plan Length with C
ce
To determine the effect of using C
ce heuristics on plan quality, we compare the length of the
plans found with C
heuristics
to
those found with x = 1, the standard delete relaxation
ce
heuristic. The plan length measure is equivalent to plan quality in the unit-cost setting.
We consider the median ratio of plan length found with the standard delete relaxation
heuristic to that found with the C
ce heuristic, over the set of instances that are solved by
both configurations (Table 3). In general, we do not observe large differences, with the
median ratio staying close to 1. One notable exception is the blocksworld domain, in which
heuristics based on the C
ce compilation consistently find significantly shorter plans. This
C
results from the ce heuristics ability to deduce implicit ordering constraints in the domain,
avoiding actions that lead to temporary improvements during greedy search but that later
need to be reversed, adding to plan length. C
ce also leads to shorter plans in the Gripper,
Mprime, and Woodworking domains, but tends to result in longer plans in the Barman and
Grid domains.
5.3.6 Comparison to the State of the Art
Table 4 shows coverage for a variety of heuristics and planners. The best configurations
for each of the two compilations with x > 1 achieve better overall coverage results than
the standard relaxed plan heuristic. The best-performing heuristics obtained with the C
ce
compilation without conditional effect merging, and the C compilation, give coverages of
1022 and 1023 respectively. The difference between these and the coverage of the baseline
planner is greater than the significance threshold. These numbers also far exceed the coverage obtained with the hcea heuristic, but fall short of the 1039 instances solved with the
dual heuristic approach used by LAMA. However, combining LAMA and the best C
ce -nm
configuration in a portfolio planner that runs LAMA for 1500 seconds and search with the
C
ce -nm heuristic for 300 seconds results in a coverage of 1063 out of 1115 solvable problems.
Almost all of this difference results from the C
ce -based heuristics superior performance in
the Floortile, and to a lesser extent, Airport domains.
511

fiKeyder, Hoffmann, & Haslum

Domain
Airport
Barman
Blocksworld
Depots
Driverlog
Elevators
Floortile
FreeCell
Grid
Gripper
Logistics00
Logistics98
Miconic
Mprime
Mystery
Nomystery
Openstacks
Parcprinter
Parking
Pathways
Pegsol
Pipesworld
Pipesworld
PSR
Rovers
Satellite
Scanalyzer
Sokoban
Tidybot
TPP
Transport
Trucks
Visitall
Woodwork
Zenotravel

x = 1.5
1 : 1.00
1 : 1.11
1.65 : 1
1.08 : 1
1.04 : 1
1 : 1.06
1 : 1.10
1 : 1.02
1 : 1.24
1.04 : 1
1 : 1.14
1.07 : 1
1 : 1.00
1.12 : 1
1 : 1.00
1 : 1.00
1 : 1.02
1 : 1.00
1.26 : 1
1 : 1.01
1 : 1.04
1 : 1.09
1 : 1.12
1 : 1.00
1 : 1.01
1 : 1.00
1 : 1.00
1 : 1.00
1 : 1.06
1 : 1.04
1 : 1.10
1 : 1.00
1.01 : 1
1.21 : 1
1 : 1.00

x
1
1
1.68
1.03
1
1
1
1
1
1.16
1
1.05
1
1.17
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1.21
1

=
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

2
1.00
1.12
1
1
1.00
1.05
1.15
1.03
1.17
1
1.13
1
1.00
1
1.00
1.02
1.02
1.00
1.01
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.03
1.19
1.02
1.16
1.00
1.00
1
1.00

x = 2.5
1 : 1.00
1 : 1.23
1.69 : 1
1.04 : 1
1 : 1.00
1 : 1.05
1 : 1.05
1 : 1.04
1 : 1.17
1.31 : 1
1 : 1.13
1.05 : 1
1 : 1.00
1.12 : 1
1 : 1.00
1 : 1.02
1 : 1.02
1 : 1.00
1.04 : 1
1 : 1.01
1 : 1.03
1 : 1.17
1 : 1.04
1 : 1.00
1.01 : 1
1 : 1.00
1 : 1.00
1.01 : 1
1 : 1.16
1 : 1.00
1 : 1.06
1 : 1.00
1 : 1.04
1.21 : 1
1 : 1.00

x
1
1
1.64
1.06
1
1
1
1
1
1.31
1
1.06
1.01
1.17
1
1
1
1.04
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1.21
1

=
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

3
1.00
1.32
1
1
1.00
1.12
1.03
1.06
1.17
1
1.12
1
1
1
1.00
1.02
1.01
1
1.00
1.02
1.02
1.14
1.06
1.00
1.00
1.01
1.00
1.03
1.13
1.00
1.13
1.00
1.08
1
1.00

Table 3: Median ratio of length of plans found with x = 1, to the length of plans found with C
ce
with different values of x, over instances solved by both planners. An entry m : 1 means that the
baselines plans are m times longer than those of the other planner. No conditional effect merging
was used.

5.3.7 C vs. C
ce
Given a fixed number of -fluents, the difference in size between the C and the C
ce
compilations is exponential in the worst case. Mutex pruning, however, can mitigate much
of the growth of C . Consider, for example, an action in the C
ce compilation that has n
different conditional effects. If mutexes are not considered, one would expect the number of
action representatives generated for the same set of -fluents in C to be 2n . If, however,
each of the n -fluents generating the conditional effects can be shown to be mutex with
one another, the number of action representatives generated in C is also only n.
We have found in our experiments that this effect leads to much slower growth in C
than what might be expected. Consider Figure 2. Each point on the graph represents a
512

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Domain
Airport (50)
Barman (20)
Blocksworld (35)
Depots (22)
Driverlog (20)
Elevators (20)
Floortile (20)
FreeCell (80)
Grid (5)
Gripper (20)
Logistics00 (28)
Logistics98 (35)
Miconic (150)
Mprime (35)
Mystery (30)
Nomystery (20)
Openstacks (20)
Parcprinter (20)
Parking (20)
Pathways (30)
Pegsol (20)
Pipes-NoTank (50)
Pipes-Tank (50)
PSR (50)
Rovers (40)
Satellite (36)
Scanalyzer (20)
Sokoban (20)
Tidybot (20)
TPP (30)
Transport (20)
Trucks (30)
Visitall (20)
Woodwork (20)
Zenotravel (20)
Total (1126)

x=1
36
13
35
19
20
19
6
79
5
20
28
34
150
35
16
9
20
16
20
30
20
42
38
50
40
35
18
19
16
30
11
14
19
20
20
1002

MaD
+2

+5

+0

+1

+0

+3

+0

+2

+0

+0

+0

+1

+0

+0

+0

+2

+0

+7

+1

+2

+0

+1

+4

+0

+0

+1

+1

+1

+3

+0

+1

+0

+1

+0

+1

+
19

C
ce
37
14
35
22
20
19
20
80
4
20
28
35
150
35
18
9
20
9
12
24
20
42
41
50
40
36
19
18
14
30
11
16
18
20
20
1006

Coverage
C
-nm
C
ce
36
38
19
18
35
35
21
21
20
20
20
19
20
20
80
79
5
5
20
20
28
28
35
35
150
150
35
35
19
19
7
11
20
20
5
10
18
15
29
29
20
20
42
42
41
40
50
50
40
40
36
36
20
20
17
17
14
16
30
30
15
11
15
16
20
18
20
20
20
20
1022
1023

hFF
34
20
35
18
20
19
6
79
5
20
28
33
150
35
16
10
20
20
20
30
20
43
39
50
40
36
18
19
14
30
11
19
3
20
20
1000

hcea
42
0
35
18
20
20
6
79
5
20
28
35
150
35
19
7
19
12
20
28
20
40
32
50
40
36
20
3
16
29
17
15
3
8
20
947

LAMA
31
20
35
21
20
20
5
79
5
20
28
35
150
35
19
13
20
20
20
30
20
44
43
50
40
36
20
19
17
30
19
15
20
20
20
1039

PF
36
20
35
22
20
20
20
79
5
20
28
35
150
35
19
13
20
20
20
30
20
44
44
50
40
36
20
19
17
30
19
16
20
20
20
1062

Table 4: Comparison of state-of-the-art-heuristics for satisficing planning. Columns x = 1 and
MaD are as in Table 1. Column C
ce shows coverage for the best configuration (in terms of overall
coverage) with that compilation when using conditional effect merging (namely x = 1.5, t = 60);
C
C
ce -nm is the best ce configuration without conditional effect merging (which happens to use the
C
same x and t);  is the best C configuration where x > 1 (which again happens to use the same
x and t). Entries in bold in these columns are those where the difference from the baseline planner
exceeds our threshold for significance (given by the MaD column). Column PF shows coverage
for a portfolio planner that runs LAMA for 1500 seconds and C
ce for 300 seconds.
single problem instance (from the same instance set as before), paired with a value of x.
C
For each of C
ce (x-axis) and  (y-axis), we measure the ratio of growth in |A| over growth
in |F |, i. e., the factor by which the compilation increased the size of the action set encoding
(measured by the number of actions in C and by the number of conditional effects in C
ce ),
divided by the factor by which the compilation increased the number of fluents. In other
513

fiKeyder, Hoffmann, & Haslum

1e+08
Action set growth/uent set growth, C

Action set growth/uent set growth, C

4.5
4
3.5
3
2.5
2
1.5
1
0.5
0

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

Action set growth/uent set growth, Cce

1e+07
1e+06
100000
10000
1000
100
10
1

0

50

100

150

200

250

300

350

Action set growth/uent set growth, Cce

(a)

(b)

Figure 2: Growth in problem size as ratio of growth in |A| to growth in |F |, with (a) and without
(b) mutex pruning. Each point corresponds to a single instance and value of x. f (x) = x is also
shown for reference.

words, we assess the growth of the encoding over the number of conjunctions |C|, which in
theory is worst-case exponential for C but linear for C
ce .
When mutex pruning is not used (Figure 2 (b)), the growth of A in C is rapid and the
ratio quickly increases to millions; with mutex pruning (Figure 2 (a)), the growth in C is
still faster than in C
ce , but the difference is much smaller.
5.3.8 Finding Plans With No Search
When no growth or time limit is imposed on the construction of the C or C
ce tasks,
Algorithm 1 can be used as a complete planning algorithm. While it is not competitive with
heuristic search methods, it is nevertheless interesting to observe some performance details
of this algorithm in various domains. The coverage obtained with this algorithm using
the C and C
ce compilations, as well as some statistics about the growth of the compiled
C
tasks, are shown in Table 5. The difference between C
ce and  is much more visible here,
since the number of -fluents added is, in general, much larger than in the growth-bounded
constructions we have used for heuristic computation. C
ce is able to rapidly add a much
larger number of -fluents, and can therefore find relaxed plans that are solutions to the
C
original planning task as well. Overall, C
ce solves 568 tasks compared to 404 for  , and
solves an equal or greater number of tasks in all except 4 domains.
Considering individual domains, it can be seen that C and C
ce are able to solve all or
almost all of the tasks in certain domains such as Logistics00, Mprime, Mystery, Parcprinter,
PSR, and Woodworking. In some of these domains, even the addition of a small amount of
information is sufficient to obtain relaxed plans that are plans for the original task, and the
maximum x values required to solve all tasks are quite low. This is the case in the Mprime,
Mystery, and Woodworking domains, where the maximum required x values are 4.07, 4.62,
and 1.35, respectively. In others such as Elevators, Openstacks, Transport, and Visitall,
where even the smallest tasks are quite large and many different plans are possible, it is not
possible to introduce enough -fluents to disqualify all of the possible relaxed plans that do
not constitute real plans, and no tasks can be solved.
514

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Domain
Airport (50)
Barman-sat (20)
Blocksworld (35)
Depots (22)
Driverlog (20)
Elevators-sat11 (20)
Floortile-sat11 (20)
FreeCell (80)
Grid (5)
Gripper (20)
Logistics00 (28)
Logistics98 (35)
Miconic (150)
Mprime (35)
Mystery (30)
Nomystery-sat11 (20)
Openstacks-sat11 (20)
Parcprinter-sat11 (20)
Parking-sat11 (20)
Pathways-noneg (30)
Pegsol-sat11 (20)
Pipes-NoTank (50)
Pipes-Tank (50)
PSR (50)
Rovers (40)
Satellite (36)
Scanalyzer-sat11 (20)
Sokoban-sat11 (20)
Tidybot-sat11 (20)
TPP (30)
Transport-sat11 (20)
Trucks (30)
Visitall-sat11 (20)
Woodwork-sat11 (20)
Zenotravel (20)
Total (1126)

Cov.
34
0
33
16
15
0
4
1
4
16
27
24
106
35
19
5
0
20
0
5
19
10
9
50
21
15
9
0
1
20
0
15
0
20
15
568

Min
1.00
1.21
1.82
1.00
31.47
91.41
1.29
5.37
1.27
1.22
1.20
1.02
1.02
16.72
3.66
1.77
69.36
3.51
2.69
1.03
1.35
4.22
1.90
22.42
1.17
5.54
1.00
1.00

C
ce
Max
44.50
141.79
90.93
37.11
216.18
91.41
22.53
208.75
91.10
41.16
175.11
4.07
4.62
59.80
76.04
51.72
707.06
187.41
94.33
77.38
459.66
43.61
47.75
22.42
43.12
21.36
1.35
92.26

C
Med
2.31
11.44
8.48
8.92
112.50
91.41
1.41
67.69
6.79
31.14
55.01
1.15
1.25
41.14
9.95
7.26
137.98
11.60
13.46
7.29
7.40
19.16
13.93
22.42
13.75
13.56
1.24
7.25

Cov.
31
0
30
14
14
0
9
2
4
6
27
21
36
35
19
3
0
9
0
3
0
5
8
49
14
7
10
1
0
7
0
8
0
20
12
404

Min
1.00
1.21
1.82
1.00
48.92
212.24
1.31
6.77
1.27
1.22
1.20
1.02
1.02
21.91
1.60
2.32
12.61
2.56
1.05
1.70
5.22
2.38
2.89
1.17
6.32
1.00
1.00

Max
168.45
400.94
221.41
74.97
633.11
272.12
41.22
501.28
585.53
149.91
2706.86
13.22
8.34
43.50
276.81
203.41
738.88
110.51
1052.26
645.00
399.10
161.42
2.89
560.95
100.15
1.40
191.54

Med
3.56
15.49
12.45
9.96
107.21
242.18
1.54
76.12
14.11
5.80
54.47
1.14
1.20
33.27
88.23
89.79
242.54
11.71
16.04
26.04
22.09
44.21
2.89
11.27
41.05
1.25
11.14

Table 5: Solving planning tasks with no search. Table shows Coverage for the C and C
ce compilations, and the Minimum, Maximum, and Median values of x for solved tasks.

6. Heuristics for Optimal Planning
We now consider admissible heuristics, for optimal planning. Section 6.1 considers the
LM-cut heuristic, showing that there are certain complications that make it difficult to
obtain improved heuristic estimates from both C and C
ce . In Section 6.2, we consider an
alternative method to lower-bound h+ , namely admissible cost-partitioning heuristics based
on the conjunctive landmarks that can be obtained from C
ce . We detail how to choose C
in that setting, and present our experimental results in Section 6.3.
515

fiKeyder, Hoffmann, & Haslum

a1
i

a2
a3

g2

aC
3

g1

g1
i

a1
a2
a3

g2

aC
2

0

g3

g3
(a)

0

{g1 ,g2 ,g3 }
aC
1

0

(b)

Figure 3: LM-cut in , and in the C compilation, for Example 3.
6.1 LM-cut
The state-of-the-art admissible approximation of h+ is computed by the LM-cut algorithm
(Helmert & Domshlak, 2009). A logical approach to obtaining admissible heuristics from
C and C
ce is therefore to apply LM-cut to these compilations. Unfortunately, it turns out
that there are several serious obstacles to this. Before discussing these issues, we first give
a brief description of the LM-cut algorithm, and then present the simpler case of the C
compilation, where the additional complication of conditional effects is not present.
Before LM-cut can be computed on a planning task with no deletes, a simple transformation is first applied that replaces the goal set G with a single goal that can only be achieved
with a goal-achievement action whose precondition set is G, and adds a dummy precondition to all actions whose precondition set is empty. LM-cut then initializes hLM-cut := 0
and, repeats the following steps until hmax (G) becomes 0: (1) Compute hmax ; (2) apply a
precondition choice function (PCF) to each action precondition pre(a) that removes from
pre(a) all but one of the fluents p  pre(a) for which hmax (p) is maximal; (3) construct the
justification graph whose vertices are the fluents and whose arcs are the precondition/effect
pairs according to the PCF; (4) find a cut L between the initial state and the goal in the
justification graph, given by the set of actions that enters the goal zone, i. e., the set of
fluents from which the goal can be reached at 0 cost; and (5) add costmin := minaL cost(a)
to the heuristic value hLM-cut , and reduce the cost of each a  L by costmin . As proved by
Helmert and Domshlak (2009), this algorithm has two fundamental properties, namely (i)
admissibility, hLM-cut  h+ , and (ii) domination of hmax , hmax  hLM-cut .
While it would be expected that the heuristic obtained in this manner from C would
be strictly more informative than that obtained from the original planning task , this
turns out not to be the case. Indeed, the heuristic can become strictly less informative:
Example 3 Let  = hF, A, I, Gi be given by F = {g1 , g2 , g3 }, A = {a1 , a2 , a3 }, where
ai = h, {gi }, , i, I = , and G = {g1 , g2 , g3 } (Figure 3). In words,  has three goals, each
of which is achievable by a single action. Valid plans for  apply each action once in any
order, to make all of the goals true. The cuts found by the LM-cut algorithm for this task are
{a1 }, {a2 }, and {a3 }, regardless of the PCFs chosen, and the LM-cut algorithm therefore
always computes the optimal cost 3. Consider now the C compilation that results from the
set C = {{g1 , g2 , g3 }}. F C then contains the single -fluent {g1 ,g2 ,g3 } , and a representative
0
0
of each action aC
i constructed with the sole non-empty subset C = {{g1 , g2 , g3 }} of C. The
first cut found by LM-cut then contains these three representatives, as each adds the most
expensive goal {g1 ,g2 ,g3 } . For any of the possible PCFs, the next cut is then the last, and
the final heuristic estimate is only 2 as only two cuts have been found. If, for example, the
516

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

0

0

C
precondition choice function chooses g1 as the hmax justifier for aC
2 and a3 , and g2 as the
0
max
C
h
justifier for a1 , the cut is {a1 , a2 }. After that cut, the goal can be reached at 0 cost
{{g ,g ,g }}
via a1 , a2 , a3 1 2 3 , so hmax is 0 and LM-cut stops.

+
C
Note that (similarly to h+ (C
ce ), cf. Theorem 2) it is not possible for either h ( ) or
hmax (C ) to decrease with the addition of -fluents, and that in this example the hmax
cost of the task actually increases (from 1 to 2) with the addition of the -fluent {g1 ,g2 ,g3 } .
However, the type of interactions that are introduced are difficult for the LM-cut algorithm
to reason about, resulting in worse admissible bounds in practice. LM-cut of course continues to dominate hmax , proving that if a sufficient number of -fluents are added, LM-cut
will eventually tend towards the optimal cost of the task.

The weakness pointed out by Example 3 is inherited in the application of the LM-cut
algorithm to the C
ce compilation. Furthermore, that application involves an additional complication that proves formidable: LM-cut is not defined for conditional effects, and therefore
cannot be directly applied to the C
ce task. It turns out that of the two straightforward
adaptations of the algorithm to problems with conditional effects, neither preserves both
properties (i) admissibility and (ii) domination of hmax .
To see why (ii) is at stake, consider a planning task  with a single action a that
has two conditional effects ce(a)1 = h{p}, {q}, i and ce(a)2 = h{q}, {r}, i, initial state
{p}, and goal {r}. We have h+ () = hmax () = 2 due to the critical path ha, ai, and the
justification graph considered by LM-cut consists of this same sequence. The first cut found
is {a}. When the cost of a is reduced, the remaining task has hmax cost 0, resulting in the
cost estimate hLM-cut = 1.
The issue here is that different conditional effects of an action may be part of the same
critical path. A natural approach is therefore to reduce costs per individual conditional
effect, rather than for all of the effects of the action at once. Unfortunately, it turns out that
this does not preserve admissibility (i). Indeed, as we detail in Example 4 (Appendix A),
there exist STRIPS tasks  whose C
ce compilations have the following property: there
exists an action a such that reducing its cost globally when it is first encountered in a cut
leads to a heuristic estimate that is less than hmax (C
ce ), while treating each of its effects
+
C
separately leads to an estimate greater than h (ce ) = h ().
There is therefore no simple strategy for dealing with conditional effects that preserves
both (i) and (ii) on all planning tasks. Since admissibility cannot be sacrificed, we must
reduce costs globally and give up on dominating hmax . As a particular implication of doing
1
C

so, despite Theorem 5 which shows that hmax (C
ce ) = h (ce ) converges to h (), such
convergence is not guaranteed for hLM-cut (C
ce ). This could of course be fixed by using
max(hmax , hLM-cut ) as the heuristic value, yet as hmax is typically not informative, this
strategy is not useful in practice.
As we detail in Section 6.3 below, on the IPC benchmarks, using A with LM-cut computed on either C or C
ce often results in larger search spaces as more -fluents are introduced. In all but a few cases, overall performance is worse with hLM-cut (C ) or hLM-cut (C
ce )
than with hLM-cut (). It remains an open question whether this can be improved.
517

fiKeyder, Hoffmann, & Haslum

6.2 C
ce Landmarks
Landmarks in planning tasks are formulas  over the set of fluents F that have the property that they are made true in some state during the execution of any valid plan. As
the problem of checking whether even a single fluent is a landmark for a planning task
is PSPACE-complete, approaches to finding landmarks have in the past focused on the
delete relaxation, in which setting whether a fluent is a landmark or not can be checked
in polynomial time. It has recently been shown that the maximum fixpoint solution to a
set of simple recursive equations defines the complete set of single fact delete-relaxation
landmarks, in other words those landmark formulas that consist of only a single literal
 = p (Keyder et al., 2010). This solution can be computed by an algorithm that repeatedly updates the set of such landmarks for each fluent and action in the planning task,
until convergence. This method can naturally handle conditional effects by treating them
as independent actions, as described in Section 5.1.
It has also been shown that these equations can be applied to any AND/OR graph
structure, not necessarily corresponding to the delete relaxation of a planning task. This
insight has been used to obtain landmarks from the m task. Single -fluent landmarks
in m correspond to conjunctive landmarks in  that are not necessarily landmarks of the
delete relaxation + . This approach suffers, however, from the large number of -fluents
that are considered in m , rendering landmark generation impractical for the m compilations of larger tasks. Here we aim to take advantage of the flexibility of the C
ce compilation
to obtain non-delete-relaxation landmarks for the original task, while considering only a
focused set of -fluents and not all those of a given size m. As before, this allow us to
consider larger conjunctions while keeping the size of the delete relaxation task low.
When using C
ce for landmark finding, to focus the technique and keep its overhead at
bay, we choose the set of conjunctions C so as to guarantee that every -fluent is a landmark
in C
ce (and therefore in the original planning task). This is accomplished by extracting from
the landmark graph sets of landmarks that are simultaneously achieved :
Definition 7 (Simultaneously achieved landmarks) A set of landmarks Ls = {1 ,
. . . , n } is simultaneously achieved if Lc = 1  ...  n is a landmark in .
Maximal sets of simultanously achieved landmarks can easily be extracted from a set of
landmarks and orderings. Given an initial set of landmarks L and a set of orderings, the
following are sets of sets of simultaneously achieved landmarks:
 LG = {{ | G |= }}
 Lnec = {{ |  nec } |   L}
 Lgn = {{ |  gn } |   L}
LG contains a single set that is made up of the landmarks in L that are entailed by G. Since
any valid plan must make all goals true in its final state, they are necessarily simultaneously
achieved. Given a landmark , Lnec contains a set which has as its elements all those landmarks  that are ordered necessarily before . Due to the definition of necessary orderings,
all of these  must be simultaneously true in every state that immediately precedes a state
in which  becomes true. Lgn is a similar set, yet since greedy necessary orderings are
518

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

weaker than necessary orderings, can sometimes contain sets that do not appear in Lnec ,
and therefore result in a larger overall set of conjunctive landmarks. Note that all necessary orderings are also greedy necessary orderings, and each conjunctive landmark that
results from a set of necessary orderings is therefore a subset of some conjunctive landmark
that results from greedy necessary orderings. We include conjunctive landmarks that result
from necessary orderings as they result in stronger necessary orderings between the added
conjunctive landmark and . Landmark heuristics can then sometimes infer that these
conjunctive landmarks must be reachieved if a landmark that they are ordered necessarily
before has to be reachieved. This is not the case for the conjunctive landmarks derived
from greedy-necessary orderings, as they only need to be achieved to make the landmarks
they are ordered before true for the first time.
Algorithm 2: Choosing C for landmark generation.
C=
L = FindLandmarks(C
ce )
repeat
C = C  SimultaneouslyAchieved(L)
L = FindLandmarks(C
ce )
until SimultaneouslyAchieved(L)  C

Our strategy for choosing C for landmark generation is shown in Algorithm 2. While
new conjunctive landmarks L = p1      pn can be discovered, the corresponding fluents
{p1 ,...,pn } are added to C
ce and the landmark computation step is repeated. Note that the
process may go through several iterations, and is run until a fixpoint is reached, as the
addition of the new -fluents to the C
ce task can result in the discovery of new landmarks.
The process terminates when all the new conjunctive landmarks that are discovered already
exist as -fluents in C
ce . We note that this method of choosing C does have the desired
C
property mentioned above: all -fluents introduced in C
ce represent fact landmarks in ce
and conjunctive landmarks in the original task .
The above strategy works especially well in domains in which many landmarks have several landmarks that are necessarily or greedy necessarily ordered before them. One domain
where this occurs is in Blocksworld (see an illustration in Figure 4), where the method is
able to find extremely informative conjunctive landmarks that allow it to optimally solve
more tasks than any other heuristic we tested.
6.3 Experiments
We consider the performance of the LM-cut heuristic hLM-cut on the C and C
ce compilations, and that of the admissible landmark cost-partitioning heuristic hLM introduced by
Karpas and Domshlak (2009) with different landmark generation schemes, including the
LM-cut is used with the A search algorithm, while for hLM
landmarks obtained from C
ce . h

we use LM-A , a variant which is more effective when there are known fluent landmarks
(Karpas & Domshlak, 2009). The benchmarks, computers, and time/memory limits are the
same as those used in in Section 5.3.
519

fiKeyder, Hoffmann, & Haslum

Informativeness
Coverage
Domain
C , 1.5
C
,
1.5
Orig.
x
=
1
C , 1.5 C
ce
ce , 1.5
Airport
1 : 49.02
1 : 52.91
28
28
19
18
Barman-opt
1 : 1.06
1 : 1.12
4
4
4
4
Blocksworld
4.22 : 1
1 : 1.71
28
28
28
27
Depots
1 : 1.96
1 : 6.49
7
5
4
4
Driverlog
1 : 23.7
1 : 48.31
13
13
10
10
Elevators-opt11
1.65 : 1
1 : 1.03
18
18
16
15
Floortile-opt11
19.23 : 1
13.93 : 1
7
6
12
12
FreeCell
1.07 : 1
1 : 2.12
15
15
13
9
Grid
3.47 : 1
1 : 1.35
2
2
2
1
Gripper
1:1
1:1
7
7
6
6
Logistics00
1 : 9.38
1 : 10.47
20
20
16
15
Logistics98
1 : 7.69
1 : 18.87
6
6
2
3
Miconic
1 : 232.55
1 : 769.23 141 141
50
45
Mprime
13.08 : 1
1 : 1.13
22
22
28
22
Mystery
1.03 : 1
1.07 : 1
16
16
17
17
Nomystery-opt11
1 : 126.58
1 : 588.24
14
14
8
8
Openstacks-opt11
1:1
1:1
14
14
14
14
Parcprinter-opt11
1 : 1.14
1 : 4.23
13
13
13
12
Parking-opt11
2
1
1
0
Pathways-noneg
1 : 15.72
1 : 51.81
5
5
4
4
Pegsol-opt11
1.08 : 1
1.08 : 1
17
17
17
17
Pipes-NoTank
1 : 1.48
1 : 2.09
17
17
15
14
Pipes-Tank
1 : 1.30
1 : 2.25
11
10
8
7
PSR
1.04 : 1
1 : 1.03
49
49
49
49
Rovers
1 : 1.72
1 : 3.56
7
7
7
6
Satellite
1 : 3.77
1 : 33.33
7
7
6
6
Scanalyzer-opt11
1 : 1.36
1 : 1.06
11
11
4
5
Sokoban-opt11
1 : 1.28
1 : 1.31
20
20
20
20
Tidybot-opt11
1 : 4.79
1 : 13.19
13
13
7
6
TPP
1 : 5.14
1 : 1.70
6
6
6
6
Transport-opt11
1 : 1.87
1.35 : 1
6
6
6
7
Trucks
1 : 5.94
1 : 10.26
10
10
7
6
Visitall-opt11
1 : 3.86
1 : 3.32
10
10
10
10
Woodwork-opt11
4.07 : 1
1 : 2.36
11
11
7
5
Zenotravel
1 : 39.37
1 : 153.85
12
12
8
8
Total
589 584
444
418
C
Table 6: LM-cut with C
ce and  . The two columns on the left show the ratio of the summed

number of heuristic evaluations for tasks solved by both configurations, comparing the standard
LM-cut that results from x = 1 to LM-cut computed on C and C
ce with x = 1.5. For example,
the first entry in the table, 1 : 49.02, shows that LM-cut computed on C with a growth bound of
x = 1.5 evaluates, in sum over the commonly solved tasks, nearly 50 times as many states as LM-cut
computed on the standard delete relaxation. The last 4 columns show coverage. Column Original
shows results obtained with Fast Downwards implementation of LM-cut (which applies only to the
standard delete relaxation), while column x = 1 shows the results of our implementation of LMcut on the unmodified delete relaxation (with any differences between the two being purely due to
implementation details). Entries in bold indicate the highest coverage in each domain, and in total.

520

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

clear(a)
gn
clear(b)
handempty
ontable(b)

clear(a)
holding(b)

nat

nat

clear(b)
handempty
on(b, a)
clear(c)
handempty
ontable(c)

clear(b)
clear(d)
gn
on(b, a)
holding(c)
nat ontable(d)
gn

on(b, a)
on(c, b)
on(d, c)

nec

clear(c)
clear(c)
clear(d)
on(b, a) gn handempty
on(c, b)
on(b, a)
holding(d)
on(c, b)
ontable(d)

Figure 4: Landmarks graph found with the C
ce compilation for a small Blocksworld task, in which
all blocks are initially on the table and G = {on(b, a), on(c, b), on(d, c)}. Some smaller conjunctive
landmarks and single fluent landmarks are omitted.

6.3.1 LM-cut with C and C
ce
To evaluate the impact of using the C and C
ce compilations on LM-cut, we constructed
C
C
the  and ce tasks following the same procedure as described in Section 5.3, repeatedly
selecting conflicts until the increase in the size of the compiled task reached a fixed growth
bound x. Conflict selection was based on hmax supporters rather than hadd supporters, as
hmax plays a key role in the computation of LM-cut, and also resulted in better performance.
Other than that, the procedure used to generate the C and C
ce tasks was the same. We
C
tested each value of x from the set {1.5, 2, 2.5, 3} for both  and C
ce . We observed that
x = 1.5 dominated the larger values of x on a domain-by-domain and overall basis, and
therefore report results only for these two configurations. The only exception to this is the
Mystery domain, in which C with x = 2.5 and x = 3 solved 18 tasks compared to 17 for
x = 1.5.
Overall, the heuristic computed by the LM-cut algorithm on the standard delete relaxation + dominates that computed on C and C
ce , both in terms of informativeness and
in terms of coverage. The first two columns of Table 6 show that for the large majority
of domains, search using LM-cut computed on C and C
ce performs many more heuristic
evaluations in tasks that are solved by both configurations. In the Airport domain, for
instance, LM-cut on the standard delete relaxation requires approximately 50 times fewer
heuristic evaluations to solve the same set of tasks as either C or C
ce . In most other domains, the situation is less extreme, but the standard delete relaxation continues to give the
better heuristic estimates. The exceptions to this are the Blocksworld, Elevators, Floortile,
FreeCell, Grid, Mprime, Mystery, Pegsol, PSR, Transport and Woodworking domains, in
+
which at least one of C or C
ce yields more informative heuristic estimates than  . Most
impressively, C and C
ce give estimates that are respectively 19 and 14 times more informative than the estimates obtained from + in the Floortile domain, and estimates using
C are 13 times more informative than those of + on the Mprime domain. In terms of
coverage, this translates into 12 tasks solved with both C and C
ce in the Floortile domain,
compared to 7 for the standard version of LM-cut, and 28 tasks solved with C in the
521

fiKeyder, Hoffmann, & Haslum

Mprime domain, compared to 22. In the Mystery domain, coverage is increased by 1. In all
other domains, the coverage achieved with C and C
ce -based LM-cut is less than or equal
to the coverage achieved with standard LM-cut. Overall, the standard version of LM-cut
solves 589 planning tasks as compared to 445 for C and 418 for C
ce . Though a large part
of this difference (90 tasks) comes from the Miconic domain, the difference in the remaining
domains is still significant.
Comparing C and C
ce , it can be seen that the additional loss of information resulting
from the treatment of conditional effects in LM-cut leads to worse heuristic estimates when
C
using C
ce . As expected from the theoretical result that ce grows only linearly with the
number of -fluents, the number of -fluents that are added to the task when using the
C
C
ce compilation is almost always higher than when using  . However the treatment of
conditional effects in LM-cut (described above) turns out to greatly degrade performance,
C
and LM-cut using C
ce is more informative than LM-cut using  in only 4 domains.
6.3.2 Admissible Landmark Heuristics with C
ce Landmarks
The admissible landmark heuristic, hLM , uses action cost partitioning to derive heuristic
values from a collection of (ordered) landmarks, distributing the cost of each action over the
set of landmarks it achieves (Karpas & Domshlak, 2009). Cost partitioning can be done in
different ways: optimal cost partitioning is tractable, and yields the best possible heuristic
value for a given set of landmarks, but in practice is so slow that coverage suffers; uniform
partitioning generally achieves a better time/informativeness trade-off, and therefore better
coverage.
To evaluate the potential informativeness of the landmarks obtained with C
ce using the
iterative technique described in Section 6.2, we used these landmarks in the optimal cost
partitioning setting, since this setting makes the best possible use of information present
in the given landmarks. We compared their informativeness to that of the heuristic using
landmarks obtained from the m compilation with m = 1 and m = 2 and the sound and
complete landmark generation algorithm (Keyder et al., 2010). The results are shown in the
first two columns of Table 7. They show the ratio of total number of heuristic evaluations,
per domain, over all tasks solved by all configurations, for hLM using landmarks from 1 only
to the heuristic using landmarks from 2 and C
ce , respectively. Note that the landmarks
2 compilations contain the landmarks obtained from 1
generated from both the C
and

ce
as a subset, and that hLM with optimal partitioning over the 2 or C
ce landmarks therefore
dominates hLM with optimal partitioning over 1 landmarks. Hence the ratio is always
greater than 1.
In 9 of the 35 domains considered, neither the addition of 2 landmarks nor C
ce landmarks leads to a more informative heuristic (cases in which both columns show the value 1).
Of the remaining 26 domains, both schemes improve over 1 landmarks to an equal degree
in 7, and 2 improves over 1 to a greater degree than C
ce in 17. In one case, Blocksworld,
C
landmarks
are
much
more
informative
than
landmarks
found by both the other methce
ods, and improve informativeness over the baseline heuristic using only 1 landmarks by a
factor of 122.
Uniform cost partitioning divides the cost of each action evenly over the set of landmarks
that it achieves, rather than searching for a partitioning that maximizes the heuristic value
522

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Domain

Airport
Barman-opt
Blocksworld
Depots
Driverlog
Elevators-opt11
Floortile-opt11
FreeCell
Grid
Gripper
Logistics00
Logistics98
Miconic
Mprime
Mystery
Nomystery-opt11
Openstacks-opt11
Parcprinter-opt11
Parking-opt11
Pathways-noneg
Pegsol-opt11
Pipes-NoTank
Pipes-Tank
PSR
Rovers
Satellite
Scanalyzer-opt11
Sokoban-opt11
Tidybot-opt11
TPP
Transport-opt11
Trucks
Visitall-opt11
Woodwork-opt11
Zenotravel
Total

Informativeness
Coverage
(optimal partitioning) (uniform partitioning)
2
C
1
2
C
ce
ce
1.03
1.03
27
11
27
4
4
4
25.65
122.31
26
28
32
4.28
1.11
7
7
7
1.02
1.01
10
9
9
1.54
1.54
12
12
12
3.28
1.02
2
2
2
1.32
1.01
60
38
39
1.12
1.12
2
2
2
1
1
7
6
7
13.65
13.65
20
22
22
1.25
1.25
3
3
3
3.91
3.91
142
142
143
1.39
1
20
20
20
2.17
1
15
15
15
3.08
1.69
20
20
18
1
1
12
7
11
4.09
1
10
8
10
9.84
1
3
0
0
1
1
4
4
4
1.25
1
17
15
17
1.37
1.27
16
16
16
1.64
1.13
13
10
11
2.68
1.23
49
49
49
1
1
6
6
5
1.06
1.06
6
6
6
1.54
1.01
6
3
6
1.02
1
20
14
18
1.18
1.05
14
9
14
1
1
6
6
6
1
1
6
6
6
1
1
8
6
7
1
1
16
9
9
3.04
1.16
7
4
5
1
1
8
8
8
604
527
570

Table 7: hLM with landmarks generated from the delete relaxation (1 ), 2 (Keyder et al., 2010)
and C
ce . The two columns on the left show the ratio of the summed number of heuristic evaluations
for tasks solved by both configurations, comparing 2 and C
ce to the baseline of using only landmarks
from 1 . Using optimal cost partitioning in hLM , more landmarks can only yield better lower bounds,
and indeed all the ratios are  1 (which is why we do not use a m : 1 presentation, differently from
the previous tables). The right-most three columns show coverage, using uniform cost partitioning.
The heuristic with uniform partitioning solves more tasks than with optimal cost partitioning under
all the landmark generation schemes considered. Entries in bold indicate best results, per domain
and in total.

523

fiKeyder, Hoffmann, & Haslum

in each state. This can make the hLM heuristic weaker, though typically not by much, but
also makes it much faster to compute, leading to better coverage in general. We confirmed
that uniform cost partitioning results in higher coverage than optimal cost partitioning in
all domains for all three of the landmark generation schemes we considered.
The three right-most columns in Table 7 show the coverage achieved with hLM and the
three landmark generation schemes in this setting. It can be seen that using only 1 landmarks results in greater coverage than combining them with either 2 or C
ce landmarks.
2
C
Compared to the heuristic using  landmarks, using ce landmarks solve as many or more
tasks in every domain except the Nomystery and Rovers domains. C
ce landmarks outperform 1 landmarks in two domains: Blocksworld, where using C
landmarks
the planner
ce
finds optimal solutions to 32 out of the 35 tasks, more than any other tested heuristic, and
in Miconic, by 1 instance. In the other domains, the use of C
ce landmarks either has no
1
effect or worsens coverage compared to  . Interestingly, while the informativeness of the
LM-cut heuristic increases greatly with the C and C
ce compilations in the Floortile domain, there is no corresponding increase when the compilations are used to find landmarks.
This is because few conjunctive landmarks, besides the goal, are found.

7. Conclusions and Open Questions
There is a long tradition of works attempting to devise heuristics taking into account some
delete effects. However, techniques rendering h+ perfect in the limit  and thus allowing
to smoothly interpolate between h+ and h  have been proposed only quite recently, by
Haslum (2012) and Katz et al. (2013) respectively. We have extended Haslums approach
by introducing a new compilation method with linear (vs. worst-case exponential) growth,
and demonstrated the machinery needed for using the approach to generate heuristics. Our
evaluation shows that, in some domains, informedness can be dramatically improved at a
small cost in terms of computational overhead.
The main open issue lies in our use of the words in some domains here. In most
domains, the gain in informativeness is small, and in some domains overall performance
suffers dramatically. While no domain-independent planning technique can work well in
every domain, and while a simple portfolio approach (cf. column PF in Table 4) suffices
to improve the state of the art in satisficing planning, the extent of the per-domain performance variation of our technique is dramatic. Can we obtain an understanding of what
causes these phenomena, and ultimately exploit that understanding to devise more reliable/effective practical methods? Is the unchanged or worse performance in many domains
due to fundamental limitations of the technique, or only due to its particular instantiation
(especially the selection of -fluents) as run in our experiments?
From a practical perspective, answering these questions comes down to the exploration
of techniques for predicting the impact of adding -fluents, and for making more informed
decisions about which -fluents to add. We have observed that changes in domain formulation, random reorderings, and small changes to the heuristic criteria used in -fluent
selection can have a large impact on both heuristic informativeness and coverage. Further research to formulate new heuristic criteria and improve existing ones therefore could,
potentially, provide better performance across a wide range of domains. It might also be in524

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

teresting to systematically explore the impact of random/arbitrary changes, and to attempt
building complementary-strength compilations to be combined into effective portfolios.
From a theoretical perspective, we are currently approaching the above questions in
terms of analyzing the conditions under which a small (polynomial-size) set of -fluents
suffices to render h+ perfect. Applied to individual domains, this analysis offers a way
of answering the question of whether a lack of performance improvement is due to an
essential limitation or only due to choosing the wrong set of -fluents. Our hope is
to eventually obtain syntactic criteria (e. g., based on causal graph structure) that can be
automatically applied to arbitrary planning task descriptions, serving to select the -fluents
(or to exclude subsets of -fluents from consideration) in a targeted manner. Our first
results in that direction have already been published in HSDIP14 (Hoffmann, Steinmetz,
& Haslum, 2014).
Our observations in optimal planning pose many questions for future work. A simple one
is whether more effective C
ce landmarks could be extracted by not restricting our techniques
to adding only -fluents guaranteed to be landmarks. The more daunting challenges regard
LM-cut. Our observations suggest that the methods we use suffer greatly from suboptimal
choices of precondition choice functions (PCFs). It would therefore be worthwhile to investigate new methods for obtaining better PCFs. Another important direction is to develop
extensions of LM-cut to conditional effects that guarantee both admissibility and domination of hmax . A simple yet impractical method is to multiply out the conditional effects
(enumerating all subsets thereof). A more sophisticated method based on context splitting, where distinctions between different occurences of the same action are introduced in
a targeted manner only where necessary, has recently been proposed (Roger, Pommerening,
& Helmert, 2014).
In summary, explicitly represented conjunctions clearly exhibit the potential to dramatically improve delete relaxation heuristics. But much remains to be done in order to
understand and use them effectively.

Acknowledgments
Part of the work leading to this publication was carried out while Emil Keyder and Jorg
Hoffmann were working at INRIA Grand Est, Nancy, France. NICTA is funded by the
Australian Government through the Department of Communications and the Australian
Research Council through the ICT Centre of Excellence Program. We thank the University
of Freiburg for allowing us to use their computional resources.

Appendix A. Proofs
Theorem 3 (h+ (C ) dominates h+ (C
ce )) Given a planning task  and a set of con+
C
+
C
junctions C, h ( )  h (ce ). There are cases in which the inequality is strict.
Proof: This follows from the fact that any plan for C is also a plan for C
ce , yet the
Cn i be a plan for C .
1
inverse is not the case. To show the first part, let  = haC
,
.
.
.
,
a
n
1
We show that the same sequence of actions constitutes a plan for C
ce , by showing by
Cn ]  I C [aC , . . . , aC ] , where I[. . . ]
1
induction that I C [aC
,
.
.
.
,
a
denotes
the result of
ce
n
n ce
1
1
525

fiKeyder, Hoffmann, & Haslum

applying a sequence of actions to the initial state I C in C
ce . Since the goal in both tasks
is defined to be GC , this shows the desired result. For the base case, the initial state in
C
both C and C
ce is I , and the subset relation holds. For the inductive case, assume
Ci1
C1
C
C
C
C
C
I [a1 , . . . , ai1 ]  I [aC
1 , . . . , ai1 ]ce . Since the precondition of ai in ce is a subset of
C
C
C C
C
i
the precondition of aC
i in  for all a and all Ci , ai can be applied in I [a1 , . . . , ai1 ]ce by
C
i
the induction hypothesis. We then need to show that all fluents added by aC
i in  are also
Ci
C
C C
C
C
added by aC
i in ce when applied in I [a1 , . . . , ai1 ]ce . The add effect of ai in  consists
C
of the union of two sets, (add(a)  (pre(a) \ del(a))) , which is also the add effect of aC
i in
Ci
C
0
C
0
ce and therefore added, and {c | c  Ci }. Since ai was applicable in  , each of its
S
Ci1
1
preconditions (pre(a) c0 Ci (c0 \add(a)))C must be true in I C [aC
1 , . . . , ai1 ], and therefore
C
0
in I C [aC
1 , . . . , ai1 ]ce , by the induction hypothesis. For c  C (and therefore c  Ci , as Ci 
C
C
C), aC
i in ce has a conditional effect with effect c and condition (pre(a)  (c \ add(a))) ,
Ci
C
which applies because its condition is a subset of the precondition of ai in  . This shows
the desired property.
For strictness, consider the planning task with fluent set F = {p1 , p2 , r, g1 , g2 }, initial
state I = {p1 }, goal G = {g1 , g2 }, and actions
ap2 : h{p1 }, {p2 }, {r, p1 }, i ar : h, {r}, , i
ag1 : h{p1 , r}, {g1 }, , i ag2 : h{p2 , r}, {g2 }, , i
Let C = {c  F | |c| = 2}. The only optimal plan for both  and C is the sequence
har , ag1 , ap2 , ar , ag2 i. In the case of C this follows from the fact that the plan must include
ag1 and ag2 as they are the only actions achieving the two goals, and therefore must achieve
their precondition -fluents {p1 ,r} and {p2 ,r} , respectively. Each of these -fluents can
be achieved only with ar , as no action achieves either of the p fluents without deleting
r. There is no single representative of ar that achieves both {p1 ,r} and {p2 ,r} , as such a
representative would have the precondition {p1 ,p2 } , which is unreachable, since the only
action achieving p2 deletes p1 . A plan in C therefore must contain ag1 , ag2 , at least two
instances of ar , and ap2 .
This no longer holds, however, when considering C
ce , for which the action sequence
hap2 , ar , ag1 , ag2 i is a plan that contains only 4 actions. In C
ce , the two possible -fluents
added by ar , {p1 ,r} and {p2 ,r} , are treated independently, and a separate conditional effect
is created for each, with the conditions p1 and p2 respectively. Once p1 and p2 have been
achieved separately, a single application of the action ar is then sufficient to achieve the
two -fluents, without making true the (unreachable) cross-context -fluent {p1 ,p2 } . In this
and similar cases, there exist plans for C
ce that are shorter than the minimum length plans
for C .
Given a STRIPS task  = hF, A, I, G, costi, the h1 heuristic for a set P of fluents, is
defined as follows (Bonet & Geffner, 2001):

0
if p  s
1
h (p) =
min{a|padd(a)} h1 (pre(a)) + cost(a) otherwise
h1 (P ) = max h1 (p)
pP

526

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

The value of the heuristic for a given planning task is taken to be the h1 cost of the goal
G, h1 () = h1 (G).
Lemma 1 Given a planning task  and C = {c  P(F ) | 1 < |c|  m}, h1 (C ) = h1 (m ).
Proof: Let  = hF, A, I, Gi. m and C are identical except for the action sets. We
denote the action set of m by AC (m ) and that of C by AC (C ). There are no deletes
and conditional effects in either of AC (m ) and AC (C ), so we will ignore these in what
follows.
We first show that h1 (C )  h1 (m ), then that h1 (m )  h1 (C ). Each direction
is based on the following two observations. First, for any STRIPS planning task, we can
split up the actions over their singleton add effects, without affecting h1 . Precisely, given
an action a and p  add(a) \ pre(a), we denote by a[p] the action where pre(a[p]) = pre(a)
and add(a[p]) = {p}. Replacing each a with all its split-up actions a[p] (i. e. generating a
split-up action a[p] for every non-redundant add effect of a), h1 remains the same. Second,
say that every split-up action a[p] in action set A is dominated by an action a0 in action set
A0 , i. e., pre(a0 )  pre(a[p]) and add(a0 )  add(a[p]). Then h1 using A0 is a lower bound on
h1 using A.
We now prove that h1 (C )  h1 (m ). For every a  A and c  F so that 1 < |c|  m,
del(a)  c = , and add(a)  c 6= , AC (m ) contains the action ac given by pre(ac ) =
(pre(a)  (c \ add(a)))C , and add(ac ) = add(a)  {c0 | c0  C  c0  (add(a)  c)}. Let
0
p  add(ac ). If p  add(a), then aC for C 0 =  dominates ac [p]. Say p = c0 where
c0 6 pre(ac ). To obtain a dominating action in AC (C ), we define:
C 0 := {c00  C | del(a)  c00 = , add(a)  c00 6= , c00  c0 }
We have C 0  C, and for all c00  C 0 the conditions (1) del(a)  c00 =   add(a)  c00 6=  and
(2) c  C : ((c  c00  add(a)  c 6= ) = c  C 0 ) of Definition 2 are
S obviously satisfied.
0
0
Thus AC (C ) contains the action aC given by pre(aC ) = (pre(a)  c00 C 0 (c00 \ add(a)))C
0
and add(aC ) = (add(a)  (pre(a) \ del(a)))C  {c00 | c00  C 0 }. We now prove that (a)
0
0
pre(aC )  pre(ac ) and (b) p = c0  add(aC ).
Regarding (a), for every c00  C 0 we have
S
c00 \ add(a)  c0 \ add(a)  c \ add(a), and thus c00 C 0 (c00 \ add(a))  c0 \ add(a)  c \ add(a).
Regarding (b), we need to prove that c0  C, del(a)c0 = , add(a)c0 6= , and c0  c0 . The
first and the last of these properties are obvious, the second one is direct from construction.
As for the third one, add(a)  c0 6= , this is true because otherwise we would have c0 
c \ add(a) implying in contradiction to construction that c0  pre(ac ).
It remains to prove that h1 (m )  h1 (C ). For every a  A and C 0  C with conditions
0
0
(1) and (2) as stated above, AC (C ) contains the action aC . Let p  add(aC ). If p is not
a -fluent, then either p  add(a) or p  pre(a) \ del(a). The latter case is irrelevant (and
0
no split-up action is generated); in the former case, setting c := add(a) we get that aC [p]
is dominated by ac in AC (m ). Say p = c . Then at least one of the following cases
must hold: (a) c  C 0 or (b) c  (pre(a) \ del(a)) or (c) c  (add(a)  (pre(a) \ del(a)))
and c  add(a) 6= . In case (a), it follows directly by definition that ac  AC (m ) which
0
0
dominates aC [p]. In case (b), p = c  pre(aC ) so that case is irrelevant. In case (c),
0
c  add(a) 6=  and c  del(a) =  so again ac  AC (m ) which dominates aC [p]. This
concludes the proof.
527

fiKeyder, Hoffmann, & Haslum

Lemma 2 Given a planning task  and a set of non-unit conjunctions C, h1 (C ) 
h+ (C
ce ).
C
Proof: Consider the planning task C
no-cc that is identical to  except that it does not
include cross-context preconditions. That is, the precondition of each action representative
0
aC is modified to be the following:

0

pre(aC ) = pre(a)C 

[

(pre(a)  (c0 \ add(a)))C

c0 C 0

1
C
+
C
We show that (A) h1 (C )  h1 (C
no-cc ), and (B) h (no-cc )  h (ce ).

We first prove (A). As in the proof of Lemma 1, it suffices to prove that, for every split0
C
up action aC [p] of C
no-cc , there exists a dominating action in  . If p is not a -fluent,
00
0
then aC in C for C 00 =  dominates aC [p]. Otherwise, say p = c0 . Then at least one of
the following cases must hold: (a) c0  C 0 or (b) c0  (add(a)  (pre(a) \ del(a))). In case (b),
00
0
again aC in C for C 00 =  dominates aC [p]. In case (a), to obtain a dominating action
00
aC in C , we define
C 00 := {c00  C | del(a)  c00 = , add(a)  c00 6= , c00  c0 }
All c00  C 00 satisfy the conditions (1) del(a)  c00 =   add(a)  c00 6=  and (2) c  C :
00
((c  c00  add(a)  c 6= ) = c  C 0 ) of Definition 2, so indeed aC is an action in C .
00
00
We obviously have c0  C 00 and thus p  add(aC ). It remains to prove that pre(aC ) 
0
pre(aC [p]). This is so, intuitively, because C 00 corresponds to the single conjunction c0 (plus
subsumed conjunctions) and hence no cross-context fluents arise.S Specifically, for every
00
c00  C 00 we have c00 \add(a)  c0 \add(a). Thus pre(aC ) = (pre(a) c00 C 00 (c00 \add(a)))C =
0
(pre(a)  (c0 \ add(a)))C . The latter is obviously contained in pre(aC [p]), concluding the
proof of (A).
+
C
+
C
It remains to prove (B). Since h1 (C
no-cc )  h (no-cc ), it suffices to prove that h (no-cc )
+
C
+
C
h (ce ). Consider a state s, and a relaxed plan ce for s in ce . For each action ace in
+ , representing action a of the original task , let C 0 be the set of conjunctions c whose
ce
+ in C . C 0 obviously qualifies for
-fluents are added by ace during the execution of ce
ce

constraint (1) in Definition 2; it qualifies for constraint (2) because any conditional effect
for c with that property will be triggered by ace if the conditional effect for a suitable c0 is
C 0 of a. Define the action sequence  +
triggered. Thus C
no-cc includes the representative a
C 0 in  + adds the same fluents as a ,
C0
in C
ce
no-cc to be the sequence of these a . Obviously, a
and its precondition is the union of that of ace and of its conditional effects that fire. Thus
+
C
+
C
 + is a relaxed plan for C
no-cc . It follows that h (no-cc )  h (ce ) as desired.

Example 4 Consider the STRIPS planning task  with variables {i, p, q, r, z, g1 , g2 , g3 },
initial state I = {i}, goal G = {g1 , g2 , g3 }, and actions A as follows:
528



fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Name
pre
add del ce cost
aqz
{i}
{q,
z}
 
4
i
ari
{i}
{r}
 
1
pz
{i, q}
{p} {z} 
1
aiq
apz
{r}
{p}
{z}

4
r
g1
{p, z} {g1 }
 
1
apz
agiq2
{i, q} {g2 }
 
1
g3
ar
{r} {g3 }
 
1
We set C = {{i, q}, {p, z}}. The only operator adding part of {i, q} is aqz
i which adds q.
qz
pz pz
The only operators adding part of {p, z} are ai which adds z, and aiq , ar which add p;
pz
since aiq
and apz
both delete z, they cannot be used to establish the conjunction {p, z}.
r
Thus the actions of C
ce are:
Name
pre
add del
ce cost
qz
qz
ai
{i} {q, z}
 ce(ai )
4
ari
{i}
{r}


1
{i,
q,

}
{p}


1
apz
i,q
iq
pz
ar
{r}
{p}


4
{p, z, p,z } {g1 }


1
agpz1
g2
aiq
{i, q, i,q } {g2 }


1
{r} {g3 }


1
agr3
where ce(aqz
i ) contains two conditional effects:
Name
c
add del
i,q
ei
{i} {i,q }


ep p,z
{p} {p,z }

pz
Clearly, with respect to hmax , the -fluents in the preconditions of aiq
, agpz1 , and agiq2 dominate the respective other preconditions of these actions (as pointed out in Section 5.1, in our
implementation we actually remove the other preconditions). Thus LM-cuts justification
graph on C
ce would have the structure shown in Figure 5.
+
C
We have hmax (C
ce ) = 10 due to the cost of achieving g1 . As for h (ce ), to construct
qz pz
a plan for C
ce the only choice we have is how to establish p. We can use ai , aiq , or we
can use ari , apz
r . In the latter case, we make do with a single application of the conditionalg2 g3
r pz qz g1
effects action aqz
i , by the relaxed plan 1 = hai , ar , ai , apz , aiq , ar i, whose cost is 12. In
qz
the former case, we must use ai twice  first for i,q , then for p,z  yielding the relaxed
pz qz g1
g2 r g3
+
C
plan 2 = haqz
i , aiq , ai , apz , aiq , ai , ar i, whose cost is 13. Thus h (ce ) = 12. Since, in
pz
the execution of 1 , the only delete is that of ar , which is not true anyhow in the state of
execution, 1 also solves the original task  and we get h () = h+ (C
ce ) = 12.
Now consider LM-cut, and say that we produced the cut for the conditional-effects action
p,z
aqz
i that connects p to p,z via the conditional effect ep . The two options discussed in
Section 6.1 are to (A) reduce the cost of aqz
i globally, sticking to the original definition of


LM-cut; or to (B) reduce the cost only of ep p,z , because the other conditional effect ei i,q is
p,z
part of an optimal-cost path to ep and thus serves to justify its hmax value. Each of these
options violates one of the essential properties of LM-cut:

529

fiKeyder, Hoffmann, & Haslum



z

p



i,q
aqz
i : ei

aqz
i

i

q

aqz
i

:


ei i,q

:

r

arg3

p,z

apz
iq


ei i,q

i,q

apz
r

ari

p,z
aqz
i : ep

agiq2

agpz1

g1

g2

g3

Figure 5: Illustration of LM-cut justification graphs for C
ce in Example 4. The dashed
edges correspond to preconditions that are not critical (hmax -maximizing) at the start, but
that become critical at some point during the execution of LM-cut.


p,z
(A) In this configuration, LM-cut produces the cuts {agpz1 } [cost 1], {aqz
i : ep } [cost 4],
pz pz
g2
[cost 1], {ar , aiq } [cost 1], {aiq } [cost 1], {ari } [cost 1]. Note here that, after the
i,q
p,z
qz
qz
cut {aqz
i : ep }, the cost of ai is reduced to 0 globally; in particular, the cut {ai : ei }
is not produced. We get the heuristic value hLM-cut = 9 < hmax (C
ce ) = 10, so here LM-cut
does not dominate hmax .
(B) In this configuration, LM-cut can produce the following cuts. At the start, for every
p,z
possible precondition choice function (pcf ), we get the cuts {agpz1 } [cost 1] and {aqz
i : ep }
[cost 4]. Now hmax is 5 for each of g1 , g2 ; say the pcf selects g2 , and we get the cut {agiq2 }
pz
max is 4
[cost 1]. Now, the pcf has to select g1 , getting the cut {apz
r , aiq } [cost 1]. Then, h
g1
for each of g1 , g2 ; say the pcf selects g1 . Say further that the pcf selects p,z for apz (another
choice would be z), and selects i,q for apz
(another choice would be q), thus remaining
iq
i,q pz
in the non-dashed part of Figure 5. Then we get the cut {aqz
i : ei , ar } [cost 3] because
g1 can be reached at 0 cost from both p and i,q (we would get the same cut for any pcf
selecting g1 , at this point). Now, hmax is 2 for g3 and 1 for each of g1 , g2 , so we get the cut
{agr3 } [cost 1]. At this point, hmax is 1 for all goal facts; say LM-cut selects g3 , and thus
we get the cut {ari } [cost 1] because that is the only way to achieve r. Then finally hmax
i,q
is 1 for g2 only, yielding the cut {aqz
i : ei } [cost 1]. Overall, we get the heuristic value
hLM-cut = 13 > h () = h+ (C
ce ) = 12, so here LM-cut is not admissible.

{agr3 }

Bibliography
Baier, J. A., & Botea, A. (2009). Improving planning performance using low-conflict relaxed
plans. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the
530

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

19th International Conference on Automated Planning and Scheduling (ICAPS09),
pp. 1017, Thessaloniki, Greece. AAAI Press.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (1
2), 533.
Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets. In
Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings of the 19th European
Conference on Artificial Intelligence (ECAI10), pp. 329334, Lisbon, Portugal. IOS
Press.
Bonet, B., Palacios, H., & Geffner, H. (2009). Automatic derivation of memoryless policies
and finite-state controllers using classical planners. In Gerevini, A., Howe, A., Cesta,
A., & Refanidis, I. (Eds.), Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS09), pp. 3441, Thessaloniki, Greece. AAAI
Press.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69 (12), 165204.
Cai, D., Hoffmann, J., & Helmert, M. (2009). Enhancing the context-enhanced additive
heuristic with precedence constraints. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the 19th International Conference on Automated Planning
and Scheduling (ICAPS09), pp. 5057, Thessaloniki, Greece. AAAI Press.
Do, M. B., & Kambhampati, S. (2001). Sapa: A domain-independent heuristic metric temporal planner. In Cesta, A., & Borrajo, D. (Eds.), Recent Advances in AI Planning. 6th
European Conference on Planning (ECP01), Lecture Notes in Artificial Intelligence,
pp. 109120, Toledo, Spain. Springer-Verlag.
Fox, M., & Long, D. (2001). STAN4: A hybrid planning strategy based on subproblem
abstraction. The AI Magazine, 22 (3), 8184.
Garey, M. R., & Johnson, D. S. (1979). Computers and IntractabilityA Guide to the
Theory of NP-Completeness. Freeman, San Francisco, CA.
Gazen, B. C., & Knoblock, C. (1997). Combining the expressiveness of UCPOP with
the efficiency of Graphplan. In Steel, S., & Alami, R. (Eds.), Recent Advances in AI
Planning. 4th European Conference on Planning (ECP97), Lecture Notes in Artificial
Intelligence, pp. 221233, Toulouse, France. Springer-Verlag.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs. Journal of Artificial Intelligence Research, 20, 239290.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Chien, S.,
Kambhampati, R., & Knoblock, C. (Eds.), Proceedings of the 5th International Conference on Artificial Intelligence Planning Systems (AIPS00), pp. 140149, Breckenridge, CO. AAAI Press.
Haslum, P. (2009). hm (P ) = h1 (P m ): Alternative characterisations of the generalisation
from hmax to hm . In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the 19th International Conference on Automated Planning and Scheduling
(ICAPS09), pp. 354357, Thessaloniki, Greece. AAAI Press.
531

fiKeyder, Hoffmann, & Haslum

Haslum, P. (2012). Incremental lower bounds for additive cost planning problems. In
Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the 22nd
International Conference on Automated Planning and Scheduling (ICAPS12), pp.
7482, Sao Paulo, Brasil. AAAI Press.
Helmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence
Research, 26, 191246.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: Whats
the difference anyway?. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.),
Proceedings of the 19th International Conference on Automated Planning and Scheduling (ICAPS09), pp. 162169, Thessaloniki, Greece. AAAI Press.
Helmert, M., & Geffner, H. (2008). Unifying the causal graph and additive heuristics. In
Rintanen, J., Nebel, B., Beck, J. C., & Hansen, E. (Eds.), Proceedings of the 18th
International Conference on Automated Planning and Scheduling (ICAPS08), pp.
140147, Sydney, Australia. AAAI Press.
Hoffmann, J. (2005). Where ignoring delete lists works: Local search topology in planning
benchmarks. Journal of Artificial Intelligence Research, 24, 685758.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. Journal
of Artificial Intelligence Research, 22, 215278.
Hoffmann, J., Steinmetz, M., & Haslum, P. (2014). What does it take to render h+ ( c )
perfect?. In Proceedings of the 6th Workshop on Heuristics and Search for Domain
Independent Planning, at ICAPS14.
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In Boutilier, C.
(Ed.), Proceedings of the 21st International Joint Conference on Artificial Intelligence
(IJCAI09), pp. 17281733, Pasadena, California, USA. Morgan Kaufmann.
Katz, M., Hoffmann, J., & Domshlak, C. (2013). Who said we need to relax all variables?.
In Borrajo, D., Fratini, S., Kambhampati, S., & Oddi, A. (Eds.), Proceedings of the
23rd International Conference on Automated Planning and Scheduling (ICAPS13),
pp. 126134, Rome, Italy. AAAI Press.
Keyder, E., & Geffner, H. (2008). Heuristics for planning with action costs revisited. In
Ghallab, M. (Ed.), Proceedings of the 18th European Conference on Artificial Intelligence (ECAI08), pp. 588592, Patras, Greece. Wiley.
Keyder, E., & Geffner, H. (2009). Trees of shortest paths vs. Steiner trees: Understanding
and improving delete relaxation heuristics. In Boutilier, C. (Ed.), Proceedings of the
21st International Joint Conference on Artificial Intelligence (IJCAI09), pp. 1734
1749, Pasadena, California, USA. Morgan Kaufmann.
Keyder, E., Hoffmann, J., & Haslum, P. (2012). Semi-relaxed plan heuristics. In Bonet, B.,
McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the 22nd International Conference on Automated Planning and Scheduling (ICAPS12), pp. 128136,
Sao Paulo, Brasil. AAAI Press.
532

fiImproving Delete Relaxation Heuristics Through Explicit Conjunctions

Keyder, E., Richter, S., & Helmert, M. (2010). Sound and complete landmarks for And/Or
graphs. In Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings of the 19th
European Conference on Artificial Intelligence (ECAI10), pp. 335340, Lisbon, Portugal. IOS Press.
Palacios, H., & Geffner, H. (2009). Compiling uncertainty away in conformant planning
problems with bounded width. Journal of Artificial Intelligence Research, 35, 623
675.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime
planning with landmarks. Journal of Artificial Intelligence Research, 39, 127177.
Roger, G., Pommerening, F., & Helmert, M. (2014). Optimal planning in the presence of
conditional effects: Extending LM-Cut with context-splitting. In Schaub, T. (Ed.),
Proceedings of the 21st European Conference on Artificial Intelligence (ECAI14),
Prague, Czech Republic. IOS Press. To appear.

533

fi