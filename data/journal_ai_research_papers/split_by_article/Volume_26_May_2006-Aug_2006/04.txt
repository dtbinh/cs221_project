Journal of Artificial Intelligence Research 26 (2006) 153-190

Submitted 10/05; published 06/06

Convexity Arguments for Efficient Minimization of the
Bethe and Kikuchi Free Energies
Tom Heskes

t.heskes@science.ru.nl

IRIS, Faculty of Science, Radboud University Nijmegen
Toernooiveld 1, 6525 ED, Nijmegen, The Netherlands

Abstract
Loopy and generalized belief propagation are popular algorithms for approximate inference in Markov random fields and Bayesian networks. Fixed points of these algorithms
have been shown to correspond to extrema of the Bethe and Kikuchi free energy, both of
which are approximations of the exact Helmholtz free energy. However, belief propagation does not always converge, which motivates approaches that explicitly minimize the
Kikuchi/Bethe free energy, such as CCCP and UPS.
Here we describe a class of algorithms that solves this typically non-convex constrained
minimization problem through a sequence of convex constrained minimizations of upper
bounds on the Kikuchi free energy. Intuitively one would expect tighter bounds to lead to
faster algorithms, which is indeed convincingly demonstrated in our simulations. Several
ideas are applied to obtain tight convex bounds that yield dramatic speed-ups over CCCP.

1. Introduction
Pearls belief propagation (Pearl, 1988) is a popular algorithm for inference in Bayesian
networks. It is known to be exact in special cases, e.g., for tree-structured (singly connected)
networks with just Gaussian or just discrete nodes. But also on networks containing cycles,
so-called loopy belief propagation empirically often leads to good performance (approximate
marginals close to exact marginals) (Murphy, Weiss, & Jordan, 1999; McEliece, MacKay,
& Cheng, 1998). The notion that fixed points of loopy belief propagation correspond to
extrema of the so-called Bethe free energy (Yedidia, Freeman, & Weiss, 2001) is an important
step in the theoretical understanding of this success.
The Kikuchi free energy (Kikuchi, 1951) is a generalization of the Bethe free energy that
can lead to better approximations of the exact Helmholtz free energy. Just like fixed points
of loopy belief propagation correspond to extrema of the Bethe free energy, fixed points
of an algorithm called generalized belief propagation (Yedidia et al., 2001) correspond to
extrema of the Kikuchi free energy.
A problem with loopy and generalized belief propagation is that they do not always
converge to a stable fixed point. New algorithms (Yuille, 2002; Teh & Welling, 2002) have
been derived that therefore explicitly minimize the Bethe and Kikuchi free energy. As we will
describe in Section 2, minimization of the Kikuchi free energy corresponds to a usually nonconvex constrained minimization problem. Non-convex constrained minimization problems
are known to be rather difficult to solve, so in Section 3 we will first derive sufficient
conditions for the Kikuchi free energy to be convex (over the set of constraints). In Section 4
we will then derive a class of converging double-loop algorithms, in which each inner loop
corresponds to constrained minimization of a convex bound on the Kikuchi free energy,
c
2006
AI Access Foundation. All rights reserved.

fiHeskes

and each outer-loop step to a recalculation of this bound. Based on the intuition that the
tightest bound yields the fastest algorithm, we come up with several ideas to construct
tight bounds. We will see that Yuilles (2002) CCCP algorithm corresponds to a special
case of a rather loose bound and discuss the relationship with the UPS algorithm by Teh
and Welling (2002) in Section 4.5. The simulations in Section 5 illustrate the use of tight
convex bounds on several inference problems. Implications and other issues are discussed
in Section 6. Technical details are treated in the appendices.

2. The Kikuchi Approximation
Exact inference in graphical models is often intractable. In this section we will introduce
the Kikuchi approximation as a particular example of a variational approach towards approximate inference.
2.1 Graphical Models
An undirected graph G = (V, E) consists of set of nodes or vertices V = {1, . . . , N } that
are joined by a set of edges E. We place at each node i a variable xi which takes values in
a finite discrete alphabet. The vector containing all variables is denoted x  (x1 , . . . , xn ).
Let  be a subset of V ; we call  a region. A clique is any fully connected subset of V ; C is
a set of cliques. The potential, also referred to as compatibility or kernel function,  (x )
is a strictly positive function that only depends on the variables that are part of the clique
. We define the probability distribution or probability mass function
pexact (x) 

1 Y
 (x ) ,
Z

(1)

C

where Z is the normalizing constant, often called partition function. The HammersleyClifford theorem (Besag, 1974) guarantees us that the underlying probability process is
Markov with respect to the graph and, vice versa, that the distribution of any Markov random field over G that is strictly positive can be expressed in this form. Through the process
of moralization, any directed graphical model (Bayesian network) can be transformed into a
corresponding undirected model. Consequently, the probability distribution corresponding
to a Bayesian network can also be written in the form (1) (Lauritzen, 1996).
Computing the partition function Z, as well as computing marginals on subsets of variables, in principle requires summation over an exponential number of states. To circumvent
this exponential summation there are two kinds of approaches: sampling techniques and
variational methods. With sampling, one draws samples from the exact probability distribution. The variational methods try to find an approximation to the exact probability
distribution.
2.2 Variational Methods
Variational methods are often derived from an approximation of the so-called free energy
X
XX
p(x ) log  (x ) +
p(x) log p(x)  E(p)  S(p) .
(2)
F (p) = 
C x

x

154

fiEfficient minimization of the Kikuchi free energy

The first term, E(p), is referred to as the energy, the second term S(p) as the entropy.
Functional minimization of F (p) with respect to functions p(x) under the constraint that
p(x) is properly normalized yields pexact (x). Furthermore, the partition function Z then
follows from
 log Z = F (pexact ) .
When we stick to the exact free energy (2), we do not really gain anything: the entropy
part S(p) still consists of a sum over exponentially many terms. Variational methods are
based on a tractable approximation of the free energy. They can be roughly divided into two
classes, the mean-field and the Kikuchi approximations. In the mean-field approach one
confines the minimization of the free energy to a restricted class T of (tractable) probability
distributions instead of considering the class P of all probability distributions:
 log Z = F (pexact ) = min F (p)  min F (p) .
pP

pT

The crux is to choose the class T such that the entropy S(p) becomes tractable for all p  T .
Note however that this restriction typically also affects the energy term E(p) (Jordan,
Ghahramani, Jaakkola, & Saul, 1998; Jaakkola & Jordan, 1999).
The Kikuchi approximation of the free energy (2) leaves the energy term as is and
approximates the entropy S(p) through a combination of marginal entropies:
X
X
S(p) =
p(x) log p(x)  
c S (p)
R

x

=

X

c

R

X

p(x ) log p(x ) .

(3)

x

Here R denotes a collection of so-called regions; the parameters c are called Moebius or
overcounting numbers.
2.2.1 Partially Ordered Sets
Following Pakzad and Anantharam (2002, 2005), we will use the language of partially ordered
sets or posets. Specifically, the collection R of regions can be viewed as such a poset where
the ordering is defined with respect to the inclusion operator . A region  includes a
region   , written     , if all variables in   are also part of . We use     to denote
strict inclusion, i.e.,     and   6 . We say that  covers   in R, written     , if
    and there exists no    R such that        . We can visualize a poset with a
so-called Hasse diagram or region graph (see the examples below). Given a particular poset
R, its Hasse diagram GR is a directed acyclic graph, whose vertices are the elements of R,
and whose edges corresponds to the cover relationships. That is, there is an edge from  to
  iff     .
2.3 The Cluster Variation Method
In Kikuchis (1951) original cluster variation method (CVM), the collections of regions and
overcounting numbers are constructed as follows. We start by defining a collection O of
outer regions. The minimal choice is the original set of cliques C, but we can also choose to
155

fiHeskes

combine cliques and construct larger ones, similar to the process of triangulation (Lauritzen,
1996). For convenience, we redefine the potentials correspondingly, i.e., such that there is
precisely one potential  (x ) per outer region  (see the example below).
Given these outer regions, we construct new regions by taking the intersections of these
outer regions, the intersections of intersections, and so on, until no more intersections can
be made. We will refer to the regions constructed in this way as inner regions, combined
in the collection I. The collection of all regions R in (3) is now the union of the outer and
inner regions: R = O  I.
The overcounting or Moebius numbers in the original CVM follow from the Moebius
formula
X
(4)
c = 1 
c  .
  

By definition we have c = 1 for all outer regions   O.
The Bethe free energy can be considered a special case of the Kikuchi free energy. In the
Bethe free energy there are no intersectionsPof intersections, i.e., there is only one level of
inner regions with c = 1  n where n  O; 1 equals the number of outer regions
covering inner region .
2.3.1 Alternatives
Several alternatives to the original CVM, with weaker constraints and/or other constraints
on the choice of regions and overcounting numbers, have been proposed recently. Yedidia,
Freeman, and Weiss (2005) present an overview. The particular choice of inner regions
subsets and overcounting numbers in junction graphs (Aji & McEliece, 2001) and join
graphs (Dechter, Kask, & Mateescu, 2002) leads to an entropy approximation in which
all overcounting numbers for the inner regions are negative. The resulting algorithms are
very similar to the junction tree algorithm, but then applied to a graph with loops. The
entropy approximation that follows from the original cluster variation method takes into
account all entropy contributions up to the level of the outer regions in a consistent manner
and, on theoretical grounds, there seems to be no reason to deviate from that (Pakzad
& Anantharam, 2005). In this paper, we therefore focus on the original cluster variation
method, but our analysis holds much more generally for any poset or region graph.
2.4 Constrained Minimization
The Kikuchi approximation of the free energy only depends on the marginals p(x ) for
all   R. We now replace the minimization of the exact free energy over the complete
distribution p(x) by minimization of the Kikuchi free energy
XX
X X
FKikuchi (q) = 
q (x ) log  (x ) +
c
q (x ) log q (x )
(5)
O x

R

156

x

fiEfficient minimization of the Kikuchi free energy

over all pseudo-marginals q  {q ;   R} under the consistency and normalization constraints
q (x )  0 R x
X

q (x ) = 1 R

(positive)

(6a)

(normalized)

(6b)

(consistent)

(6c)

x

X

q  (x  ) = q (x ) ,  R;  

x  \

Referring to the class of pseudo-marginals satisfying these constraints as Q, we have the
approximation
 log Z  min FKikuchi (q) .
qQ

Furthermore, the hope is that the pseudo-marginals q (x ) corresponding to this minimum are accurate approximations of the exact marginals pexact (x ). The Kikuchi free
energy and corresponding marginals are exact if the Hasse diagram turns out to be singlyconnected (Pakzad & Anantharam, 2005).
2.5 Illustration
For illustration of the main concepts, we consider a probability model with 4 variables
(nodes) and pairwise interactions between each of the nodes as visualized in Figure 1(a).
In obvious shorthand notation, the exact distribution is of the form
1 Y
1
pexact (x) =
ij (xi , xj ) = 12 13 14 23 24 34 .
Z
Z
{i,j}

Note here that potentials originally defined on single nodes can always be incorporated in
the definition of the two-node potentials. The region graph corresponding to the minimal
choice of outer regions, i.e., equivalent to the potential subsets, is given in Figure 1(b).
With the outer regions all pairs of nodes, the inner regions subsets are all single nodes. In
fact, in this case the region graph is equivalent to a so-called factor graph (Kschischang,
Frey, & Loeliger, 2001) and the Kikuchi approximation of the free energy boils down to a
Bethe approximation:
XX
qij (xi , xj ) log ij (xi , xj )
FKikuchi (q) = 
{i,j} xi ,xj

+

XX

qij (xi , xj ) log qij (xi , xj ) +

{i,j} xi ,xj

X
X
(1  ni )
qi (xi ) log qi (xi ) ,
i

xi

where ni = 3 is the number of outer regions containing the inner region i.
The cluster variation method allows us to choose larger outer regions, for example,
consisting of all triples {i, j, k}. We redefine the factorization of the potentials such that
pexact (x) =

1 Y
ijk (xi , xj , xk ) = 123 124 134 234 ,
Z
{i,j,k}

157

fiHeskes

1
x1

x3

EE
EE yyy
EyEy
yy EEE
y
E
yy

1

1

1

1

1

1, 2 I 1, 3 I 1, 4 I 2, 3
2, 4
3, 4
66 II
II 
II 
u
u
u
u
II
II
u
66 III
uu
uu 
66 III  IIII  IuIuIuIu
uu 
u
66
I
II uu II uu

IuIu
Iu
6 IIII

uuuII
uu I
1
2
3
4

x2

x4

-2

(a) Markov random field.

-2

-2

-2

(b) Hasse diagram for the Bethe approximation.

1

1

1

1

1, 2, 3I 1, 2, 4I 1, 3, 4I 2, 3, 4

II  66
Iu
Iu

II
uuII
uuII 
 uuu III uuu III
 II 666


II 6
I
I
u
u



I
I
u
u I
II 66
I
u
u


II
II
 uuu
II 6
uu



I
I
u
I
 uu
 I
 I
uu

1, 2 I 1, 3 I 1, 4 I 2, 3
2, 4
3, 4
66 II
II 
II 
u
u
u
u
I
I
I
u
u
II
II uu -1 uu -1
II
-1 666 -1
-1
-1
u
66 IIII  IIII  uuIuIuII
uu 
II
II uuu
II uu
66



uI

 II
uuuII
uu I
1
2
3
4
1

1

1

1

(c) Region graph for the Kikuchi approximation.
Figure 1: Region graphs for the Bethe and Kikuchi approximations. Lines between nodes
in the Markov random field (a) indicate edges. In the region graphs (b) and (c),
the outer regions are drawn at the highest level. Lines indicate the covering
relationship, where lower regions are covered by the higher regions. The oblique
numbers are the overcounting numbers that follow from the Moebius formula.
The Bethe approximation (b) corresponds to the minimal approximation with
the outer regions equivalent to the cliques in the graph; here all pairs of nodes.
The particular Kikuchi approximation (c) follows by taking for the outer regions
all node triples.

158

fiEfficient minimization of the Kikuchi free energy

for example through (distribute symmetrically)
1

123  [12 13 23 ] 2

1

124  [12 14 24 ] 2

1

134  [13 14 34 ] 2

1

234  [23 24 34 ] 2 ,
or through (assign to the first outer region)
123  12 13 23
124  14 24
134  34
234  1 .
The corresponding region graph is given in Figure 1(c). Now the first-level inner regions
are all pairs of nodes and the second-level inner regions are all single nodes, with overcounting numbers -1 and 1, respectively. The Kikuchi approximation of the entropy boils down
to
X
X
X
SKikuchi (q) =
Sijk 
Sij +
Si .
{i,j,k}

{i,j}

i

The intuitive reasoning behind this approximation is as follows. The sum over all threenode entropies overcounts the two-node interactions (each combination {i, j} appears twice
rather than once), which therefore have to be discounted once. But now the single-node
interactions are too much discounted (overcounting number -1 times 3 appearances, compared with the 3 appearances with overcounting number 1 in the three-node entropies),
yielding the overcounting number 1  3  (1)  3  (1) = 1.
2.6 Generalized and Loopy Belief Propagation
To summarize, finding the Kikuchi approximation of the partition function Z boils down
to minimization of the Kikuchi free energy with respect to a set of pseudo-marginals under
linear constraints between them. Introducing Lagrange multipliers for these constraints, it
can be shown that fixed points of a popular algorithm called loopy belief propagation correspond to extrema of the Bethe free energy and, more generally, fixed points of generalized
belief propagation to extrema of the Kikuchi free energy (Yedidia et al., 2001). However,
these algorithms are not guaranteed to converge to a minimum and in practice do get stuck
in for example limit cycles. This explains the search for convergent alternatives that directly
minimize the Kikuchi free energy, which will be the topic of the rest of this paper.

3. Convexity of the Kikuchi Free Energy
In this section we will derive sufficient conditions for the Kikuchi free energy to be convex
over the set of consistency constraints (6). This is relevant because if the Kikuchi free
energy is indeed convex over the constraint set, it must have a unique minimum and the
minimization problem is relatively straightforward. Furthermore, the argument that we will
159

fiHeskes

use in deriving these conditions will play an important role in the construction of efficient
minimization algorithms later on.
3.1 Sufficient Conditions
We have to consider the Kikuchi free energy (5) as a function of the pseudo-marginals q.
In reasoning about convexity, we can disregard the energy term being linear in q. The
entropy terms give either a convex or a concave contribution, depending on whether the
corresponding overcounting numbers are positive or negative, respectively. Ignoring the
constraints (6), the free energy (5) is convex if and only if all concave contributions vanish,
i.e., c = 0 for all   R .
However, we really only care about the subspace induced by the constraints (6). Therefore we introduce the notion of convexity over the set of constraints. We call the free energy
convex over the set of constraints (6) if
F (q1 + (1  )q2 )  F (q1 ) + (1  )F (q2 ) 0<<1 q1 ,q2 Q .
Note that, since the constraints are all linear, if q1 and q2 satisfy the constraints (6), then
so does q1 + (1  )q2 . In the following, when we talk about convexity of the Kikuchi free
energy, the conditioning on the constraint set is implicitly assumed.
One way to proceed is to make use of the (consistency) constraints to express the Kikuchi
free energy in terms of the outer region pseudo-marginals only and then study its convexity.
Our approach is along these lines. In particular, we will replace inner region pseudomarginals that correspond to concave contributions by outer region pseudo-marginals. The
pseudo-marginals corresponding to convex contributions are of no concern. In fact, we may
be able to use these convex contributions as well to compensate for some of the concave
contributions.
To make this reasoning more precise, we define positive regions (or perhaps better,
nonnegative)   R+ , with R+  {  R; c  0}  O  I+ and negative regions   R ,
with R  {  R; c < 0}  I . The idea, formulated in the following theorem, is then
that the Kikuchi free energy is convex if we can compensate the concave contributions of
the negative regions R by the convex contributions of the positive regions R+ .
Theorem 3.1. The Kikuchi free energy is convex over the set of constraints (6) if there
exists an allocation matrix A between positive regions   R+ and negative regions
  R satisfying
A 6= 0 only if   

( can be used to compensate )

(7a)

A  0
X
A  c

(positivity)

(7b)

(sufficient amount of resources)

(7c)

(sufficient compensation)

(7d)

R+



X

A  |c |

R



160

fiEfficient minimization of the Kikuchi free energy

Proof First of all, we note that we do not have to worry about the energy terms that are
linear in q. In other words, to prove the theorem we can restrict ourselves to showing that
minus the entropy


X
X
S(q) =  
c S (q ) 
|c |S (q )
R+

R

is convex over the set of constraints.
As an intermediate step, let us consider the combination of a convex entropy contribution
of a positive region   R+ with the concave entropy contribution of a negative inner region
  R , where  is a subset of :
X
X
 (q)  [S (q)  S (q)] =
q (x ) log q (x ) 
q (x ) log q (x )
x

=

X

q (x ) log q (x ) 

=

x

q (x ) log q (x )

x

x

X

x

X



q (x ) 

X

x\



q (x\ |x ) log q (x\ |x ) ,

where we used the standard definitions
X
q (x )
q (x ) and q (x\ |x ) 
q (x ) 
.
q (x )
x
\

In the first step, we applied the constraint q (x ) = q (x ) and extended the summation
over x in the second term to a summation over x . In the second step we basically turned
the difference between two entropies into (a weighted sum of) conditional entropies. The
difference  , which now only depends on q , is, from Lemma A.1 in Appendix A, convex
in q . In other words, the concave contribution from S is fully compensated by the convex
contribution S , yielding an overall convex term in the relevant set of constraints.
The resulting operation is now a matter of resource allocation. For each concave contribution |c |S we have to find convex contributions S to compensate for it. Let A denote
the amount of resources that we take from positive region   R+ to compensate for
negative region   R . Obviously, a positive region can only compensate negative regions
that it contains, so A = 0 when  is not a subset of , which explains condition (7a).
Now, in shorthand notation and with a little bit of rewriting


X
X
|c |S 
c S 
S(q) =  
R+

=

X

R+

=

X

R+



c 



c 

R

X

A +



X



X







A  S 

A  S 

X X

R+ 

161

X

R





X

A +



A [S  S ] 

X



X

R






A  |c | S

X





A  |c | S .

fiHeskes

P
Convexity of the first term is guaranteed
P if c   A  0 (7c), of the second term if
A  0 (7b), and of the third term if  A  |c |  0 (7d).

3.2 Checking the Conditions

Checking the conditions of Theorem 3.1 can be cast in the form of a linear programming
problem, for example as follows. We define an auxiliary variable  replacing condition (7c)
by
X
(8)
A = |c | R (variable compensation)


Then we solve the linear programming problem that attempts to maximize the single variable  under all constraints implied by the four conditions. The interpretation is that we
try to use the available resources to compensate for as much of the concave contributions
as we can. If we find a solution   1 all conditions are satisfied: the Kikuchi free energy
is convex over the set of constraints and has a unique minimum. If the optimal  turns
out to be smaller than 1, there is no matrix A satisfying all constraints and convexity of
the Kikuchi free energy is not guaranteed by Theorem 3.1.
Instead of solving the linear program, we can often get away with simpler checks. For
example, we can guess a particular A and check whether the conditions (7) hold. An obvious
choice is
X
c
A =  with n
1,
 
n
R ,


which satisfies condition (7c) and when substituted into (7d) yields the condition
X

c +

R+ ,

c
0
n


R .

(9)

Similarly, the choice
A =

X
|c |
+
with
n

1

n+

R ,
+

satisfies condition (7d) and yields the condition
X

R ,

c
+ c  0 R+
n+


(10)

when substituted into (7c). If (9) or (10) holds, Theorem 3.1 guarantees convexity of the
Kikuchi free energy.
The above two conditions are sufficient, but not necessary for Theorem 3.1 to apply. A
necessary condition is
X
X
c  0
(11)
c +
R

R+

which is easily derived by summing condition (7d) over all   R and substituting condition (7c). If condition (11) fails, we cannot use Theorem 3.1 to prove convexity of the
Kikuchi free energy.

162

fiEfficient minimization of the Kikuchi free energy

We would like to conjecture that the conditions in Theorem 3.1 are not only sufficient,
but also necessary for convexity of the Kikuchi free energy. We will not pursue this any
further here, because it is irrelevant for our current purposes. Furthermore, it may not
be that relevant in practice either, since convexity by itself is a sufficient but not necessary condition for a unique minimum. Tatikonda and Jordan (2002), Heskes (2004), Ihler,
Fisher, and Willsky (2005) give conditions for convergence of loopy belief propagation and
uniqueness of the minimum of the corresponding Bethe free energy. These conditions do not
only depend on the graphical structure, but also on the (strength of the) kernels  (x ).
3.3 Related Work
Chiang and Forney (2001) present similar ideas, about convex entropy terms compensating
concave terms in the set of constraints, and derive conditions for convexity of the Bethe
free energy with pairwise potentials. The resulting conditions are formulated in terms of
single-node marginals, which may be difficult both to validate in practice and to generalize
to the Kikuchi case.
Closely related to our Theorem 3.1 is the following theorem of Pakzad and Anantharam
(2002, 2005).
Theorem 3.2. (Pakzad & Anantharam, 2002, 2005) The Kikuchi free energy (5) is convex
over the set of consistency constraints imposed by a collection of regions R (and hence the
constrained minimization problem has a unique solution) if the overcounting numbers c
and c  satisfy:
X
X
S  R,
c +
c   0 .
(12)
  R\S:
S, 

S

In words, for any subset S of R, the sum of overcounting numbers of elements of S and all
their ancestors in R must be nonnegative.
In fact, using Halls (1935) matching theorem, it can be shown that the conditions (7)
in our Theorem 3.1 are equivalent to the conditions (12) in Theorem 3.2. The latter are
more direct and do not require the solution of a linear program.
Both Theorem 3.1 and Theorem 3.2 can be used to show that the Bethe free energy for
graphs with a single loop is convex over the set of constraints (Heskes, 2004; McEliece &
Yildirim, 2003; Pakzad & Anantharam, 2002, 2005).
3.4 Minimization of the Convex Kikuchi Free Energy
If the Kikuchi free energy is convex, it is not only guaranteed to have a unique minimum,
but this minimum is also relatively easy to find with a message-passing algorithm similar
to standard (loopy) belief propagation.
The basic idea is as follows. We here focus on the case in which all overcounting numbers
are positive. The case with negative overcounting numbers is more involved and worked
out in Appendix B. Furthermore, here and in the rest of this paper we ignore the positivity
constraints (6a). It is easy to check that these are satisfied at the solutions we obtain. We
introduce Lagrange multipliers    (x ) for the consistency constraints as well as  for the

163

fiHeskes

normalization constraints and construct the Lagrangian


XX
X
L(q, ) = FKikuchi (q) +
q  (x  )
   (x ) q (x ) 
  ,
 

+

X




x  \

x

 1 

X
x



q (x ) .

(13)

Minimization of the Kikuchi free energy under the appropriate consistency and normalization constraints is, in terms of this Lagrangian, equivalent to
min FKikuchi (q) = min max L(q, ) ,
q

qQ



where the minimization over q is now unconstrained. Standard results from constrained
optimization (e.g., Luenberger, 1984) tell us that
min max L(q, )  max min L(q, ) ,
q





q

with equality for convex problems under linear equality constraints. That is, for convex
problems we are allowed to interchange the maximum over  and the minimum over q.
Furthermore, the optimal q () corresponding to the minimum of the Lagrangian (13) as
a function of  is unique, since L(q, ) is convex in q for all . Substitution of the solution
then yields the so-called dual
L ()  min L(q, ) = L(q (), ) .

(14)

q

This dual is concave in  and has a unique maximum.
Many algorithms can be used to find the maximum of the dual (14). A particular
one, derived in Appendix B, is given in Algorithm 1. It slightly differs from those presented
by Yedidia et al. (2005) and Yuille (2002) by sending messages (messages are directly related
to Lagrange multipliers) only between inner regions and outer regions, i.e., never between
inner regions subsets and other inner regions. The price one has to pay is that the update in
line 7 depends on the overcounting number c . For the Bethe free energy, with c = 1  n ,
we obtain the standard (loopy) belief propagation update rules. The particular ordering
in Algorithm 1, running over inner regions and updating the messages between an inner
region and all its neighboring outer regions, guarantees that the dual (14) increases at each
iteration1 . The local partition functions Z and Z in lines 10 and 7 are chosen such as
to normalize the pseudo-marginals q (x ) and q (x ). This normalization is not strictly
necessary, but helps to prevent numerical instability. Algorithm 1 can be initialized by
setting all messages  (x ) = 1 and skipping lines 3 to 6 at the first iteration.
1. For positive overcounting numbers c . The argumentation with negative overcounting numbers is more
complicated and may require damping of the updates to achieve convergence. See Appendix B for details.

164

fiEfficient minimization of the Kikuchi free energy

Algorithm 1 Message-passing algorithm for constrained minimization of a Kikuchi free
energy.
1:

while converged do

2:

for all   I do

3:
4:

for all   O,    do
X
q (x ) =
q (x )
x\

5:

 (x ) =

q (x )
 (x )

6:

end for

7:

q (x ) =

8:

for all   O,    do
q (x )
 (x ) =
 (x )
Y
1
q (x ) =
 (x )
 (x )
Z
I,

1
n +c
1 Y
 (x )
Z O,


9:
10:



11:

end for

12:

end for

13:

end while

4. Double-Loop Algorithms for Guaranteed Convergence
Even when the Kikuchi free energy is not convex, we can still run Algorithm 1 in the
hope that it converges to a fixed point. This fixed point then must correspond to an
extremum of the Kikuchi free energy under the appropriate constraints (Yedidia et al.,
2001). Even better, empirically for the general Kikuchi free energy and provably for the
Bethe free energy (Heskes, 2003), this extremum is in fact a minimum. However, in practice
this single-loop2 algorithm does not always converge and we have to resort to double-loop
algorithms to guarantee convergence to a minimum of the Kikuchi free energy.
4.1 The General Procedure
We introduce a class of such double-loop algorithms based on the following theorem.
2. Note that single loop here refers to the message-passing algorithm and has nothing to do with the
notion of a single loop in the graphical model.

165

fiHeskes

Theorem 4.1. Given a function Fconvex (q; q ) with properties
Fconvex (q; q )  FKikuchi (q)

q,q Q

Fconvex (q; q) = FKikuchi (q) and
fi
Fconvex (q; q ) fifi
FKikuchi (q)
fi  =
q
q
q =q

Fconvex (q; q ) is convex in q  Q

(bound)

(15a)

qQ

(touching)

(15b)

q Q

(convex)

(15c)

the algorithm
qn+1 = argmin Fconvex (q; qn ) ,

(16)

qQ

with qn the pseudo-marginals at iteration n, is guaranteed to converge to a local minimum
of the Kikuchi free energy FKikuchi (q) under the appropriate constraints.
Proof It is immediate that the Kikuchi free energy decreases with each iteration:
FKikuchi (qn+1 )  Fconvex (qn+1 ; qn )  Fconvex (qn ; qn ) = FKikuchi (qn ) ,
where the first inequality follows from condition (15a) (upper bound) and the second from
the definition of the algorithm. The gradient property (15b) ensures that the algorithm is
only stationary in points where the gradient of FKikuchi is zero. By construction qn  Q for
all n.
See Figure 2 for an illustration of the algorithm and the proof. In fact, the convexity
of Fconvex has not been used to establish the proof. But, as argued in Section 3.4, from an
algorithmic point of view constrained minimization of a convex functional is much simpler
than constrained minimization of a non-convex functional. This general idea, replacing
the minimization of a complex functional by the consecutive minimization of an easier
to handle upper bound of this functional, forms the basis of popular algorithms such as
the EM algorithm (Dempster, Laird, & Rubin, 1977; Neal & Hinton, 1998) and iterative
scaling/iterative proportional fitting (Darroch & Ratcliff, 1972; Jirousek & Preucil, 1995).
Intuitively, the tighter the bound, the faster the algorithm.
4.2 Bounding the Concave Terms
As a first step, to lay out the main ideas, we build a convex bound by removing all concave
entropy contributions for   I . To do so, we will make use of the linear bound
X
X

q (x ) log q (x )  
(17)
q (x ) log q (x ) ,
x

x

which directly follows from
0

KL(q , q )

=

X
x

"

q (x )
q (x ) log 
q (x )

166

#

fiEfficient minimization of the Kikuchi free energy

(1)
(2)

(3)

Figure 2: Illustration of the proposed algorithm and corresponding convergence proof. At
iteration n, Fconvex (q; qn ) (dashed line) is a convex bound of the non-convex
FKikuchi (q) (solid line). They touch at qn , point (1), where Fconvex (qn ; qn ) =
FKikuchi (qn ).
At the minimum, point (2), we have Fconvex (qn+1 ; qn ) 
Fconvex (qn ; qn ). The corresponding Kikuchi free energy, point (3), obeys
FKikuchi (qn+1 )  Fconvex (qn+1 ; qn ) because of the bounding property.

with KL the Kullback-Leibler divergence. Our choice Fconvex then reads

 X
XX
X
q (x )
(1)

Fconvex (q; q ) =
q (x ) log
q (x ) log q (x )
c
+
 (x )
 x
x
I+


X
X
X
X
|c | 1 
q (x ) . (18)
|c |

q (x ) log q (x ) +
I

I

x

x

It is easy to check that this functional has properties (15a) and (15c). The last term has
been added to fulfill property (15b). Next we make the crucial observation that, using the
(1)
constraints (6) and for fixed q , we can rewrite Fconvex in the normal form (5):

 X X
XX
q (x )
(1)
Fconvex
(q; q ) =
q (x ) log
+
c
q (x ) log q (x ) + C(q) , (19)

(x
)


 x
x


where C(q) evaluates to zero for all q  Q and where , which implicitly depends on q ,
and c are defined through

X |c |
0 I

log q (x ) and c 
.
(20)
log  (x )  log  (x ) +
c I+
n
I ,



That is, we can always to incorporate the terms now linear in q in the energy term by
redefinition of the potentials. Here we have chosen to distribute each of these terms equally
over the n neighboring outer regions, but other choices are possible as well.
167

fiHeskes

The term C(q) in (19) evaluates to zero for all q  Q and is thus irrelevant to the
optimization in the inner loop. It consists of terms such as the last one in (18) that only
(1)
serve to make the bound Fconvex satisfy (15b). In the construction of other bounds below,
we will ignore such terms: they do not affect the algorithm in any way3 .
(1)
Now that we have Fconvex both convex and in normal form, we can use Algorithm 1 to
solve the constrained problem (16). The resulting double-loop algorithm can be described
in two lines.
Outer loop: recompute  from (20) with q = qn .
Inner loop: run Algorithm 1 with  for  and c for c, yielding qn+1 .
In each inner loop, we can initialize the messages to the converged values of the previous
inner loop.
4.3 Bounding the Convex Terms
In this section we will show that in many cases we can make the algorithm both better
and simpler. The idea is to bound not only the concave, but also the convex entropy
contributions from inner regions. That is, we enforce c  0   I and set


XX
q (x )
(2)

Fconvex (q; q ) =
q (x ) log
,
(21)
 (x )
 x
with now
log  (x )  log  (x ) 

X c
log q (x ) .
n

(22)



(2)

Let us first explain why the algorithm based on Fconvex is simpler than the one based on
In (21), all reference to inner regions has disappeared. In fact, the only constraints
that we have to care about are that the outer regions pseudo-marginals should agree on
their intersections. Consequently, in the inner loop (Algorithm 1), we only have to run over
those inner regions  that are direct intersections of the outer regions, that is, those  for
which there exist outer regions  and  such that x = x  x . Similar arguments can be
used for the algorithm based on (19) as well, neglecting all negative inner regions   I
that do not correspond to direct intersections of outer regions. In practice, however, most
negative inner regions are direct intersections of the outer regions, whereas many positive
inner regions arise at the next level, from intersections of intersections. See for instance the
example of Figure 1, where all six negative inner regions are direct intersections of outer
regions, in contrast with all four positive inner regions.
(2)
From (17), but now applied to the positive inner regions, it is clear that Fconvex (q; q ) 
(1)
(2)
(1)
Fconvex (q; q ): when it is a bound, Fconvex is a tighter bound than Fconvex and we can expect
(2)
the algorithm based on Fconvex to perform better. It remains to be shown under which
(2)
conditions FKikuchi (q)  Fconvex (q; q ). This is where the following theorem comes in.
(1)
Fconvex .

3. Alternatively, we could relax condition (15b) to the statement that the gradients of Fconvex and FKikuchi
only have to be equal in the subspace orthogonal to the constraints. With this milder condition, C(q)
as well as the last term (18) are no longer needed.

168

fiEfficient minimization of the Kikuchi free energy

Theorem 4.2. The functional Fconvex in (21) is a convex bound of the Kikuchi free energy (5) if there exists an allocation matrix A between negative inner regions   I
and positive inner regions   I+ satisfying
A 6= 0 only if   

( can be used to compensate )

(23a)

A  0
X
A  |c |

(positivity)

(23b)

(sufficient amount of resources)

(23c)

(sufficient compensation)

(23d)

I



X

A  c

I+



Proof Not surprisingly, the proof follows the same line of reasoning as the proof of Theorem 3.1. First we consider the combination of a concave entropy contribution from   I
as in (17) with a convex entropy contribution from   I+ ,   :
X
X

q (x ) log q (x ) +
q (x ) log q (x ) 
x

x



X

q (x ) log q (x ) +

x

X

q (x ) log q (x ) ,

(24)

x

which follows from

q (x\ |x )
0 
q (x ) q (x\ |x ) 
q (x\ |x )
x



X
q (x )
q (x ) q (x )
=
q (x )
log
,
 (x )
q
(x
)
q
(x
)
q






x
X







where we recognize the term between braces as a Kullback-Leibler divergence between two
probability distributions.
(2)
To show that the difference between Fconvex and FKikuchi is nonnegative, we should be
able to compensate each of the concave contributions c for all   I+ with convex contributions from   I with   , without exceeding the available amount of resources
|c |. In shorthand notation, with
#
"
X
q (x )
K 
,
q (x ) log 
q (x )
x


we have the decomposition
X
X
X
(2)
Fconvex
 FKikuchi = 
c K =
c K
|c |K 
I

=

X

I



|c | 

X





A  K +

I

X X

I+

A (K  K ) +

I 

X

I+

169




X





A  c  K  0 ,

fiHeskes

123456
1

2

3

4

5

1237

12457

13467

23567

4567

6

1

2

123
3

1245

1346

2356

456

127

137

237

147

257

457

367

467

567

1 1 1 1 1 1 1 1 1 1 1 1 1 1

4

5

6

12

13

23

14

25

45

36

46

56

17

27

37

47

57

67

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

7

(a) Outer regions.

7

1

2

3

4

5

6

1

1

1

1

1

1

1

(b) Region graph.
Figure 3: Smallest example showing that the conditions for Theorem 4.2 need not always
hold for a region graph and overcounting numbers constructed with the cluster
variation method. (a) Visualization of the outer regions: black means that the
variable (1 to 7) is part of the outer region (1 to 6).
(b) Region graph with overcounting numbers in boldface. The positive overcounting numbers at the third level just outweigh the negative overcounting numbers at the second level.

where the inequality follows since each of the terms itself is guaranteed to be nonnegative
when the conditions (23) are satisfied.
As above, the conditions of Theorem 4.2 can be checked with a linear program. Having
generated many different sets of overcounting numbers resulting from the Moebius formula (4), we started wondering whether the conditions (23) are perhaps automatically satisfied. However, exhaustively checking all possible outer region combinations given a fixed
number of variables, we did come up with a counterexample. The smallest counterexample
that violates the conditions for Theorem 4.2, is illustrated in Figure 3.
Even if, as in this counterexample, not all positive inner regions can be compensated for
by negative inner regions, it will pay to get rid of as many as possible. Finding the optimal
assignment may be a complex problem, but heuristics are easy to find (see Appendix C).
4.4 Pulling Out a Tree or More
(1)

In the previous section we tightened the convex bound Fconvex of the Kikuchi free energy
FKikuchi by bounding convex contributions from positive regions as well. Another way to
get a tighter bound is to bound only part of the concave contributions from the negative

170

fiEfficient minimization of the Kikuchi free energy

inner regions. We will first illustrate this by considering the Bethe free energy, i.e., just
non-overlapping negative inner regions (nodes) with c = 1  n .
The Bethe free energy is convex for singly-connected structures. Inspired by Teh and
Welling (2002), we choose a set of nodes   Ibound such that the remaining nodes   Ifree
become singly-connected and take


XX
X
X
q (x )
(3)

Fconvex (q; q ) =
q (x ) log
+
(1  n )
q (x ) log q (x )
 (x )
 x
x
Ifree
X
X
+
(1  n )
(25)
q (x ) log q (x ) .
Ibound

x

That is, we bound the entropy terms corresponding to the bounded nodes   Ibound and
simply keep the entropy terms correspond to the free nodes   Ifree . By construction
Fconvex satisfies all conditions (15). Furthermore, it can be rewritten in the normal form (5)
with definitions

X 1  n
0
Ibound

log  (x )  log  (x ) 
.
log q (x ) and c 
1

n
n
 Ifree
I
,
bound



Note that the resulting inner-loop algorithm is not completely equivalent to running standard belief propagation on the tree of all free nodes: we do have to send messages to and
from the bounded nodes   Ibound as well to enforce the constraints q (x ) = q (x ) for
,   .
Rather than pulling out a single tree, we can also pull out a convex combination of
trees. That is, suppose that we have several bounds, each of them the result of pulling
out a particular tree and with a corresponding set of overcounting numbers ci . Then any
convex combination
X
X
c =
wi ci with wi  0 and
wi = 1
i

i

also corresponds to a convex bound. More generally, we can combine the ideas in this
and the previous section by choosing c such that the resulting bound is just convex. A
procedure for doing so is given in Appendix C. Basically, we first try to shield as much of
the concave entropy contributions by convex entropy contributions as we can. Next, we
tighten the bound further by incorporating convex contributions in the linear bounds of the
concave contributions that we did not manage to shield in the first step. Both steps can be
cast in the form of an easy to solve linear programming problem.
4.5 Related Work
(1)

The double-loop algorithm described in Section 4.2 and based on Fconvex is closely related
to Yuilles (2002) CCCP (concave-convex procedure) algorithm. Although originally formulated in a completely different way, CCCP applied for minimization of the Kikuchi free
energy can also be understood as a particular case of the general procedure outlined in
Theorem 4.1. More specifically, it is based on bounding the concave contributions with
X
X
X
|c |
q (x ) log q (x ) 
q (x ) log q (x )  (|c |  1)
q (x ) log q (x ) , (26)
x

x

x

171

fiHeskes

which is to be compared with (17). That is, before bounding the concave entropy contributions, part of these concave terms are taken over to the convex side. The reason for
doing so is that the CCCP algorithm requires the functional to be convex, independent of
the constraints involved4 . Our procedure, on the other hand, makes use of the fact that
the functional only has to be convex over the set of constraints. This allows us to use
tighter bounds, yielding more efficient and sometimes simpler algorithms. On a less important note, the inner-loop algorithm and in particular the message-passing scheme applied
by Yuille (2002) is somewhat different.
(3)
The double-loop algorithm based on Fconvex in (25) is inspired by Teh and Wellings
(2002) UPS (unified propagation and scaling) algorithm. The difference is that where we
bound the entropy contributions from nodes on the tree, in UPS these nodes (and thus
the entropy contributions) are clamped to the values resulting from the previous inner loop.
That is, each inner loop in the UPS algorithm corresponds to minimizing


XX
X
X
q (x )
UPS

Fconvex (q; q ) =
q (x ) log
+
(1  n )
q (x ) log q (x )
 (x )
 x
x
Ifree
X
X

(1  n )
q (x ) log q (x ) .
Iclamped

x

under the constraints
q (x ) = q (x ) Ifree , , yet q (x ) = q (x ) Iclamped , .
This boils down to an iterative scaling algorithm, which is also relatively easy to solve.
At each outer-loop iteration, a different choice is made for Ifree and Iclamped . The UPS
algorithm can be understood as coordinate descent and is guaranteed to converge to a
local minimum of Bethe free energy (under appropriate conditions on the choices made for
(3)
Ifree and Iclamped ). The inner loop that results from Fconvex also allows for changes in the
marginals q (x ) for   Ibound , i.e., is more flexible and can make larger steps. Loosely
(3)
UPS . Furthermore, in our approach we
speaking, Fconvex is again a tighter bound than Fconvex
can but do not have to choose different subdivisions between bounded and free nodes
within each inner loop.
Wainwright, Jaakkola, and Willsky (2002b, 2002a) present similar ideas, exploiting the
convexity of the Bethe free energy on tree structures. Wainwright et al. (2002b) use the
tree structure to obtain a more efficient implementation of loopy belief propagation, without
however guaranteeing convergence. Wainwright et al. (2002a) show that particular convex
combinations of convex Bethe free energies lead to convex bounds on the exact Helmholtz
free energy (2). In these bounds, the overcounting numbers of the inner regions still follow
the Moebius relation (4), but the overcounting numbers for the outer regions are smaller
than or equal to 1. Constrained minimization of such a bound is very similar to constrained
(3)
minimization of Fconvex and the algorithm used by Wainwright, Jaakkola, and Willsky (2003)
is indeed closely related to Algorithm 1.
4. The procedure described by Yuille (2002) often even moves part of the convex terms to the concave side.
This makes the (implicit) bound even worse and the corresponding algorithm slower. In the following
we will stick to the more favorable interpretation of the CCCP algorithm that is based on the implicit
bound (26).

172

fiEfficient minimization of the Kikuchi free energy

5. Simulations
Intuitively, we would expect the algorithms based on the tightest bound to converge the
fastest in terms of outer-loop iterations. However, with larger steps in the outer loop,
we might need more inner-loop iterations to achieve convergence in the inner loop. The
following simulations are designed to check this.
5.1 General Set-up
In the simulations we compare four different algorithms, each of them based on a different
bound.
just convex The tightest bound of the Kikuchi free energy that is just convex. Based on
the ideas described in Section 4.4 and Appendix C.
negative to zero The bound obtained by setting all negative overcounting numbers to
zero, as explained in Section 4.2.
all to zero The bound described in Section 4.3 that follows by setting all overcounting
numbers, both negative and positive, to zero. In all models considered below, the
overcounting numbers satisfy the conditions of Theorem 4.2, i.e., setting them to zero
indeed yields a bound on the Kikuchi free energy. Note further that all to zero is
equivalent to negative to zero for the Bethe free energy.
cccp The (rather favorable interpretation of the) bound implicit in Yuilles (2002) CCCP
algorithm, as explained in Section 4.5.
Algorithm 1 is applied in the inner loop of all these algorithms: the only difference
between them is the setting of the overcounting numbers c implied by the bound. Each
inner loop runs until a preset convergence criterion is met. Specifically, we end the inner loop
when all inner region marginals change less then 104 . With this criterion all algorithms
happened to converge, which probably would also have been the case with looser criteria.
For example, Yuille (2002) reports that two inner-loop iterations were sufficient to obtain
convergence.
In all simulations we report on the Kullback-Leibler (KL) divergence between exact and
approximate marginals, either summed over all nodes or over a subset of nodes. Plots for
the different error functions all look very much the same. The Kikuchi/Bethe free energy
itself is somewhat less illustrative: when it is very close to its minimum, the marginals and
thus KL divergence can still change considerably. We visualize the KL divergence both as
a function of outer-loop iterations and as a function of floating point operations, where we
count only the necessary operations involved in the inner-loop and outer-loop updates (i.e.,
not those involved in convergence checks, computing the KL divergence, and so on). In
comparing the number of inner-loop iterations used by the different algorithms to meet the
convergence criterion, we scale the outer-loop iterations relative to the outer-loop iterations
of the just convex algorithm. That is, for each number of outer-loop iterations used by an
algorithm to reach a particular level of accuracy, we consider the corresponding number of
outer-loop iterations used by the just convex algorithm to reach the same level.

173

fiHeskes

(a)

(b)
just_convex
negative_to_zero
cccp
kldivergence

kldivergence

just_convex
negative_to_zero
cccp
0

10

2

0

10

2

10

10
0

20

40
60
80
outerloop iterations

100

0

1

2
flops

3

4
6

x 10

Figure 4: Bethe approximation on a 9  9 Boltzmann grid. Kullback-Leibler divergence
between exact and approximate single-node marginals as a function of the outerloop iterations (a) and floating point operations (b) for three different algorithms.

We have done simulations on quite a number of different problems and problem instances, involving both Markov random fields and Bayesian networks. The results shown
here are exemplary and meant to illustrate the more general findings that we will summarize
below.
5.2 Bethe Free Energy on a Boltzmann Grid
Our first set of simulations concerns the minimization of the Bethe free energy on a Boltzmann grid of 9  9 nodes with pairwise interactions of the form


tj
ti
(27)
ij (xi , xj ) = exp wij (2xi  1)(2xj  1) + (2xi  1) + (2xj  1)
ni
nj
where ni is the number of neighbors of node i, i.e., 2 for a corner node, 3 for other nodes
on the boundary, and 4 for nodes in the middle. Weights wij and biases ti are drawn at
random from a normal distribution with mean zero and standard deviation 0.5. In the
Bethe approximation the outer regions are all pairs of neighboring nodes.
Figure 4 shows the summed KL divergence between exact and approximate single-node
marginals as a function of the number of outer loop iterations (a) and as a function of
the number of floating point operations (b) for the just convex, negative to zero, and
cccp algorithms. It can be seen that, as expected, the just convex algorithms converges
faster than the negative to zero algorithm, which itself converges faster than the cccp algorithm. The speed-up in terms of outer-loop iterations translates into an almost equivalent
speed-up in terms of flops. Indeed, as can be seen in Figure 5(a), the number of inner-loop
iterations required by the just convex algorithm is just slightly higher than that of the
other two algorithms.
The curves in Figure 4(a) can be mapped onto each other with a rough linear scaling
of the number of outer-loop iterations. This is also suggested by the straight lines in
174

fiEfficient minimization of the Kikuchi free energy

2

outerloop iterations

10

(b)
number of innerloop iterations

(a)
just_convex
negative_to_zero
cccp

1

10

0

10
0
10

8
6
4
2
0

1

just_convex
negative_to_zero
cccp

10
outerloop iterations

10
20
30
40
outerloop iterations (scaled)

Figure 5: Bethe approximation on a 9  9 Boltzmann grid. (a) Outer loop iterations of
the just convex algorithm versus the corresponding outer-loop iterations of the
other two algorithms. (b) Number of inner loop iterations needed to meet the
convergence criterion as a function of the outer-loop iterations, scaled according
to (a).

Figure 5(a). The slope of these lines relate to each other as 0.34, 1 (by definition), and 1.35
for just convex, negative to zero and cccp, respectively (see also the convergence rates
in Table 1). The following argumentation shows that there is a striking correspondence
between these numbers and the respective bounds.
The negative overcounting numbers
P
for the Bethe free energy FKikuchi
P add up to I c = 207. For the respective convex
bounds Fconvex , these sums are I c = 144, 0, and 81. If we now translate these into
the fraction of negative overcounting mass that is bounded, i.e.,
P
P
I c 
I c
P
,
I c

we obtain, respectively 0.30, 1 (by definition), and 1.39. That is, there appears to be an
almost linear relationship between the tightness of the bound (here expressed in the fraction
of concave entropy contributions that is bounded linearly) and the speed of convergence.
We have noticed the same almost linear relationship in all other simulations involving a
Bethe free energy (no positive overcounting numbers).
5.3 Kikuchi Free Energy on a Boltzmann Grid

Our second set of simulations is also on a 99 Boltzmann grid, where now the outer regions
are chosen to be all squares of four neighboring nodes. Potentials are of the form (27) with
weights and biases drawn from a normal distribution with standard deviation 4 and 0.5,
respectively. Note that the size of the weights is much larger than in the previous set of
simulations, to make the problem still a bit of a challenge for the Kikuchi approximation.
With these weights, the Bethe approximation does very badly (summed Kullback-Leibler
175

fiHeskes

(a)

(b)

2

2

10
just_convex
negative_to_zero
all_to_zero
cccp

1

10

kldivergence

kldivergence

10

0

10

just_convex
negative_to_zero
all_to_zero
cccp

1

10

0

10

0

200
400
600
outerloop iterations

800

0

5

10
flops

15
7

x 10

Figure 6: Kikuchi approximation on a 9  9 Boltzmann grid. Kullback-Leibler divergence
between exact and approximate single-node marginals as a function of the outerloop iterations (a) and floating point operations (b) for four different algorithms.

divergence larger than 10). Both for the Bethe and for the Kikuchi algorithm, the singleloop algorithm has convergence problems: for the Bethe approximation it typically gets
stuck in a limit cycle and for the Kikuchi approximation it tends to diverge. In total there
are 8  8 = 64 outer regions and (8  7)  2 = 122 negative inner regions (all node pairs
that correspond to intersections of the outer regions) and 7  7 = 49 positive inner regions
(all single nodes that correspond to intersections of the node pairs).
Figure 6 shows the KL divergence between approximate and exact single-node marginals
for the four different algorithms in terms of the outer-loop iterations (a) and floating point
operations (b). It can be seen that the ordering in (a) is again as expected: the tighter the
bound, the faster the algorithm. In terms of floating point operations, the just convex and
all to zero algorithm get much closer together.
Part of the explanation is given in Figure 7: the just convex algorithm requires considerably more inner-loop iterations to meet the same convergence criterion. The other
effect is that the all to zero algorithm in its inner loop only runs over the 112 negative
inner regions instead of all 161 positive and negative inner regions. This makes that each
inner-loop iteration of all to zero requires a factor 1.8 less floating point operations than
an inner-loop iteration of the other three algorithms.
Here it is more difficult to find a quantitative relationship between the tightness of
the bounds and the (asymptotic) convergence rates. One of the complications is that not
only the negative, but also the positive overcounting numbers play a role. In any case, all
algorithms still seem to converge linearly, with faster convergence rates for tighter bounds.
These convergence rates, expressed as the time scale of the corresponding exponential decay
(KL(t)  KL()  exp[t/ ], with t and  in outer-loop iterations), are summarized in
Table 1.

176

fiEfficient minimization of the Kikuchi free energy

3

outerloop iterations

10

(b)
number of innerloop iterations

(a)
just_convex
negative_to_zero
all_to_zero
cccp

2

10

1

10

0

10
0
10

25
20
15
10
5
0

1

10
outerloop iterations

just_convex
negative_to_zero
all_to_zero
cccp

5

10
15
20
25
outerloop iterations (scaled)

Figure 7: Kikuchi approximation on a 9  9 Boltzmann grid. (a) Outer loop iterations of
the just convex algorithm versus the corresponding outer-loop iterations of the
other three algorithms. (b) Number of inner loop iterations needed to meet the
convergence criterion as a function of the outer-loop iterations, scaled according
to (a).

Figure 8: Graphical structure of the QMR-like network.
5.4 A QMR Network
Our third set of simulations concerns a QMR-like (Quick Medical Reference) Bayesian
network (Heckerman, 1989; Jaakkola & Jordan, 1999): a bipartite graph with a layer of
disease nodes and a layer of findings. The particular network used in these simulations has
been generated with the Bayes Net Toolbox (Murphy, 2001). It contains 20 finding nodes,
of which 18 are observed (positive), and 10 hidden disease nodes; see Figure 8. The diseases
have Bernoulli probability distributions with a prior drawn at random between 0 and 0.01.
The findings have noisy-or conditional probability distributions without leakage. Diseases
and findings are linked randomly with probability 0.5. The absence of leakage, large amount
of findings, and strong connectivity make this a relatively difficult inference problem. As
outer regions we take the subsets implied by the conditional probability distribution, i.e.,
each outer region consists of a disease and all findings linked to it. Figure 9 gives the
corresponding region graph.

177

fiHeskes

11111111111111111111111111111111
1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0

0

1

1

0

1

1

1

1

0
2

1

2

2

0

1

1

1

1

1

0

1

1

1

1

1

0

1
1

Figure 9: Region graph resulting from the QMR-like network.

(a)
just_convex
negative_to_zero
all_to_zero
cccp

10

10

2

10

0

200
400
600
outerloop iterations

just_convex
negative_to_zero
all_to_zero
cccp

0

kldivergence

0

kldivergence

(b)

800

2

10

0

2

4

6
flops

8

10
8

x 10

Figure 10: Kikuchi approximation on a QMR-like network. Kullback-Leibler divergence
between exact and approximate single-node marginals as a function of the outerloop iterations (a) and floating point operations (b) for four different algorithms.

178

fiEfficient minimization of the Kikuchi free energy

(b)
number of innerloop iterations

outerloop iterations

(a)
just_convex
negative_to_zero
all_to_zero
cccp

2

10

1

10

0

10
0
10

35

25
20
15
10
5
0

1

10
outerloop iterations

just_convex
negative_to_zero
all_to_zero
cccp

30

5
10
15
outerloop iterations (scaled)

20

Figure 11: Kikuchi approximation on a QMR-like network. (a) Outer loop iterations of the
just convex algorithm versus the corresponding outer-loop iterations of the
other three algorithms. (b) Number of inner loop iterations needed to meet the
convergence criterion as a function of the outer-loop iterations, scaled according
to (a).

original
just convex
negative to zero
all to zero
cccp

-207
-144
0
0
81

Bethe
+

0
0
3.8
0 11.3
0 11.3
0 15.3

Kikuchi
+

-112 49
-64
1
11
0 49
41
0
0
29
112 49 153

-54
-34
0
0
52

QMR
+

35
15
6
35
67
0
17
35 166

Table 1: Summary of asymptotic convergence ( is the time constant, with time in outerloop iterations, in the exponential decay) and sums of negative and positive overcounting numbers in the original Kikuchi/Bethe free energy and the convex bounds
used by the different algorithms.

The results can be found in Figure 10 and 11. They are comparable with those for the
Kikuchi approximation on the Boltzmann grid. Also here the single-loop algorithm fails
to converge. The just convex algorithm converges much faster than the other three algorithms, but requires more inner-loop iterations and is less efficient than the all to zero algorithm, which makes the latter preferable in terms of floating point operations. However,
it is relatively straightforward to speed-up the just convex algorithm. First, we probably
do not need that many inner-loop iterations for the outer loop to converge properly. And
secondly, where we now bound part of each entropy contribution, a more efficient choice
would have as many zero overcounting numbers as possible.

179

fiHeskes

5.5 General Findings
Here we summarize some of the points that have been illustrated above and that we have
encountered in many other simulations as well.
 The tighter the (convex) bound used in the inner loop, the faster the convergence in
terms of outer-loop iterations.
 The number of outer-loop iterations needed to meet a prespecified convergence criterion tends to decrease with a looser bound, but never nearly enough to compensate
for the slower convergence in the outer loop.
 In fact, we have only observed a strong dependency between this number of inner-loop
iterations and the tightness of the bound if the bound is just convex and the problem
is hard in the sense that a single-loop algorithm would fail to converge.
 In terms of floating point operations, a looser bound that sets all overcounting numbers
in the inner loop to zero, can beat a tighter bound with negative overcounting numbers:
the slower convergence in terms of outer-loop iterations is compensated by a more
efficient inner loop.
Pelizzola (2005) tests several convergent algorithms on Kikuchi approximations of problems in statistical physics and reports similar findings. Also in this study, the just convex algorithm, described for the first time by Heskes, Albers, and Kappen (2003), clearly outperforms all competitors.

6. Discussion
This article is based on the perspective that we are interested in minima of the Kikuchi
free energy under appropriate constraints. Finding such a minimum then becomes a possibly non-convex constrained minimization problem. Here, as well as in other studies, the
approach has been to solve this non-convex problem through sequential constrained minimization of convex bounds on the Kikuchi free energy. On the presumption that tighter
bounds yield faster algorithms, we have worked out several ideas to construct tight convex
bounds. The simulation results in this article as well as those obtained by Pelizzola (2005)
clearly validate this presumption and show that the speed-ups can be very significant.
Heskes, Zoeter, and Wiegerinck (2004) apply these bounds for (approximate) parameter
learning in directed graphical models.
The double-loop algorithms considered in this article are all based on convex bounds
of the Kikuchi free energy. In principle, this is not necessary: our only concern is that
the inner-loop algorithm converges and this might well be the case for tighter bounds. One
practical solution is to simply choose a (tight) bound on the Kikuchi free, check whether the
inner-loop algorithm does converge, and restart with a looser bound if not. Alternatively,
we can construct tighter bounds making use of conditions for guaranteed convergence of
belief propagation such as those derived by Tatikonda and Jordan (2002), Heskes (2004),
Ihler et al. (2005) for the Bethe approximation.
It has been suggested that non-convergence of single-loop generalized/loopy belief propagation by itself is an indication that the Kikuchi/Bethe approximation is inaccurate. The
180

fiEfficient minimization of the Kikuchi free energy

results in Section 5.3 and 5.4 show that this need not always be the case. Apparently, there
does exist a middle range of problems where the Kikuchi free energy is not easy to minimize, but does yield decent approximations. It is on these problems that the algorithms
described in this article are useful.

Acknowledgments
The author would like to thank Wim Wiegerinck, Onno Zoeter, Kees Albers, and Bert Kappen for fruitful discussions and the anonymous reviewers for their constructive comments.
This work has been supported in part by the Dutch Technology Foundation STW.

Appendix A: Convexity of the Difference between Two Entropies
This appendix treats two lemmas on the convexity of the difference between two entropies.
The first one is used in the proof of Theorem 3.1. A similar lemma is used by McEliece and
Yildirim (2003).
Lemma A.1. The difference between two entropies
X
X
q (x ) log q (x )
q (x ) log q (x ) 
 (q ) 
x

x

=

X
x

is convex in q .



q (x ) 

X

x\



q (x\ |x ) log q (x\ |x )

Proof We take a step backwards and write  out as

 (q ) =

X
x





 q (x ) 
.
q (x ) log 
X

q (x ) 

x\

When taking derivatives, we best interpret the table q , specifying the value q (x ) for each
possible realization x , as a vector with x playing the role of an index. Taking second
derivatives, we then obtain
Hx ,x (q ) 

 2  (q )
1
1
 .
=
Ix ,x 
I
q (x )q (x )
q (x )   q (x ) x ,x

with Ix,x  1 if all elements of x and x are equal and zero otherwise.

181

fiHeskes

Next we would like to show that this matrix is positive semi-definite, i.e., that for all
tables q, again to be interpreted as vectors with indices x ,
0 

X

q(x )Hx ,x (q )q(x ) =

x ,x

x


 X q 2 (x , x )
X

\

=

q (x\ , x )
x x


\

X q(x )q(x )

I
q (x )
q (x ) x ,x

x ,x



q(x\ , x )q(x\ , x ) 

X q 2 (x )

X

x\ ,x\



q (x )


i2 
hP



X  X q 2 (x\ , x )
x\ q(x\ , x )
=
 P
.

q (x\ , x )

x\ q (x\ , x ) 
x x





\

From Cauchys inequality,

X

a2k

k

X

b2k 

k

"

X
k

ak bk

#2

,

it follows that the term between braces is indeed semi-positive q
for each realization of x .
To see this, we make the substitutions x\  k, q(x\ , x )/ q (x\ , x )  ak , and
q
q (x\ , x )  bk to find
{. . .} 

X
k

a2k

P
2
k ak bk ]
 P
0.
2
k bk
[

The following related lemma is used in Appendix B.

Lemma A.2. The difference between two entropies
X
X
 (q , q ) 
q (x ) log q (x ) 
q (x ) log q (x )
x

x

is convex in {q , q }.
Proof The Hessian matrix has components
Hx ,x



 2  (q )
1
=
Ix ,x

q (x )q (x )
q (x )  

Hx ,x



 2  (q )
1

=
I

q (x )q (x )
q (x ) x ,x

Hx ,x



q (x )
 2  (q )
 .
I
= 2

q (x )q (x )
q (x ) x ,x

182

fiEfficient minimization of the Kikuchi free energy

Convexity requires that for any q = (q (x ), q (x )),
q (x ) q (x )

0 
=

X q2 (x )

2

Hx ,x
Hx ,x

Hx ,x
Hx ,x

X q (x )q (x )

q (x )


X
q (x ) q (x ) 2

.
q (x )
=
q (x ) q (x )
x
x

q (x )



x

+

!

q (x )
q (x )



X q (x )q2 (x )
q2 (x )

x



Appendix B: Minimizing a Convex Kikuchi Free Energy
In this appendix, we derive Algorithm 1 for minimizing a convex Kikuchi free energy under
appropriate linear constraints. To simplify notation, we will use the convention that  runs
over outer regions, and  over inner regions.
First, we note that in principle it is not necessary to explicitly take into account all
constraints (6), since some constraints are implied by others. Obviously, the constraint
between two inner region marginals,
q  (x ) = q (x ) for some     ,
is implied by corresponding constraints between the inner region marginals and an outer
region subsuming both inner regions,
q (x  ) = q  (x  ) and q (x ) = q (x ) for some       .
That is, we do not have to take into account constraints between inner regions and other
inner regions. Similarly, normalization constraints on outer region pseudo-marginals follow
from normalization constraints on the inner region pseudo-marginals. So, a sufficient set of
constraints is
X
q (x )

q (x ) = q (x ) with q (x ) =
x\

X

q (x ) = 1

 .

x

Introducing Lagrange multipliers  (x ) and  for the corresponding constraints, we
obtain the Lagrangian
 X X

XX
q (x )
q (x ) log q (x )
+
c
L(q, ) =
q (x ) log
 (x )
x
 x





XXX
X
X
X
+
 (x ) q (x ) 
q (x ) +
 1 
q (x ) . (B-1)
  x

x\

183



x

fiHeskes

Convex Independent of the Constraints
Let us first consider the case that all overcounting numbers c are strictly positive (c >
0). Then, the Lagrangian is not just convex over the set of constraints, but convex in
q independent of the constraints. Minimization of the Lagrangian with respect to these
pseudo-marginals follows by setting its derivatives to zero, yielding
Y
e (x ) 
(B-2)
q (x ) =  (x )e1


q (x )

 /c 1

= e

Y

e (x )/c  ,

(B-3)



where here and in the following it should be noted that q and q are functions of the
Lagrange multipliers . Substituting this solution back into the Lagrangian, we obtain the
dual
X
XX
X X
L ()  L(q (), ) =
 
q (x ) 
c
q (x ) .
(B-4)




x



x

Now, consider optimizing L () with respect to the subset of the components corresponding
to the inner region , collected in   ( ,  (x ) ,x ), keeping all other   for
  6=  fixed. Because of the concavity of the dual L (), we can find the maximum in the
direction  by setting the corresponding derivatives to zero. This yields
fi
L () fifi
= qnew (x )  qnew (x ) = 0 x ;
 (x ) fi=new
fi
X
L () fifi
=
1

qnew (x ) = 0 ,
(B-5)
 fi=new
x
where q new refers to the solution (B-2) and (B-3) with  (x ) replaced by new
 (x ) and
 by new
.
Since
from
(B-2)

new

qnew (x )

e (x )
=  (x ) q (x ) ,
e  

the solution for new
 (x ) must obey
new
new
 (x ) =  log q (x ) +  (x ) + log q (x ) ,

where we still have to solve for qnew (x ). Summing this expression over all   , substituting (B-3), and solving for qnew (x ) we get
log qnew (x ) =

X
1
1
[log q (x )   (x )] +
(new  c ) .
n  + c
n  + c 


Now, we obtain exactly the updates in Algorithm 1 if we define
 (x ) = e (x ) and  (x ) = q (x )e (x ) ,
184

fiEfficient minimization of the Kikuchi free energy

and properly normalize q (x ), as in line 7. The normalization of q (x ) in line 10 is then
in fact unnecessary, since by construction the updates ensure that q (x ) = q (x ) with
Z = 1.
The bottom line is that with the particular ordering in Algorithm 1 the joint update of
all messages for a particular subset  can be interpreted as doing coordinate-wise gradient
ascent in the dual L (), updating the Lagrange multipliers  and  (x ) for a particular
 and all    at the same time. Therefore Algorithm 1 is guaranteed to converge to the
unique maximum in the case of all positive overcounting numbers c .
Convex over the Set of Constraints
Next, let us consider the more general case in which (some of) the overcounting numbers are
negative, but such that the Kikuchi free energy is still convex over the set of constraints.
We consider the case in which all inner region overcounting numbers are negative5 . We
will show that, with sufficient damping of the updates, Algorithm 1 is still guaranteed to
converge to the unique minimum of the Kikuchi free energy over the set of constraints.
Note that direct application of the above argumentation fails, because the solution (B-3)
for q (x ) with negative c corresponds to a maximum rather than a minimum. Consequently, the dual L () in (B-4) need not be concave. The updates in Algorithm 1 that
follow by setting derivatives to zero can be interpreted as fixed-point iterations, not as coordinate ascent in L (). Still, in practice they do seem to work just fine and indeed without
always increasing L (). In the following we will explain why: we will argue that the updates of Algorithm 1 do not correspond to coordinate ascent, but rather to something like
coordinate descent-ascent on a convex-concave saddle function. With sufficient damping,
such an algorithm will converge to the unique saddle point, which then corresponds to the
minimum of the Kikuchi free energy over the set of constraints.
Convexity over the set
P according to Theorem 3.1, that there exists
P of constraints implies,
a matrix A such that  A = |c | and  A  1. Using q (x ) = q (x ), we replace
the Lagrangian (B-1) by

 XX
XX
X
q (x )
L(q, ) =
q (x ) log
q (x ) log q (x )

A
 (x )
 x
x
 




XXX
X
X
X
X
1
+
 (x ) 
q (x ) 
q (x ) +
 1 
q (x ) . (B-6)
n 
x
x
x
 

 





\



Now since, from Lemma A.2 in Appendix A,
X
X
q (x ) log q (x ) 
q (x ) log q (x )
x

x

is convex in {q (x ), q (x )}, the Lagrangian (B-6) is indeed convex in q independent of the
constraints. Thus we could apply the same argumentation as above: find the minimum of
5. Our argumentation does not hold if some of the negative inner region entropy contributions have to be
compensated by positive inner region subset entropy contributions to prove convexity of the Kikuchi free
energy. In that case, we might need a slightly different algorithm to guarantee convergence.

185

fiHeskes

the convex Lagrangian with respect to q, substitute the corresponding solution q () back
into the Lagrangian to obtain the concave dual L (), and maximize this dual with respect
to . The problem is that we do not have a closed-form expression for the optimal q ()
and thus also no closed-form expression for the dual L (), which makes this procedure
rather awkward.
Instead, we distinguish between the outer region marginals, collected in qO , and the
inner region marginals, collected in qI . Having rewritten the consistency constraint in terms
of outer region marginals alone, we only replace the constrained minimization with respect
to qO by unconstrained maximization with respect to corresponding Lagrange multipliers
O , leaving the minimization with respect to qI under the normalization constraint as
is. This gives us a saddle-point problem of the type minqI maxO . Even without explicitly
writing out the equations, we can tell that maximization with respect to  for a particular
 and all    corresponds to finding  such that
qnew (x ) = qnew
 (x )

,  .

Then, minimization with respect to q given fixed qnew (x ) immediately yields
X
A qnew (x ) ,
qnew (x ) 


properly normalized to sum to 1. This is exactly what the updates for a particular inner
region  in Algorithm 1 amount to: they yield the unique maximum with respect to 
and minimum with respect to q , while keeping all other    and q  for   6=  fixed.
Such a coordinate descent-ascent procedure works fine if the saddle function is convex in the minimizing parameter and concave in the maximizing parameter (e.g., Seung,
Richardson, Lagarias, & Hopfield, 1998). The concavity in  is immediate, the convexity
in qI follows from the convexity of the Lagrangian (B-6) in q = (qO , qI ): minimizing
an overall convex function over some of its parameters, here qO , yields a convex function
over its remaining parameters, qI . Technically, convergence to the unique solution of the
saddle-point problem can be proven through the construction of a Lyapunov function that
decreases under infinitesimal updates of the parameters in the descent and ascent direction
to zero at the unique saddle point (Seung et al., 1998). Convergence can be guaranteed for
sufficiently damped updates, not the full ones in Algorithm 1. Empirically the full updates, that correspond to full maximization and minimization for one inner region  before
moving on the next one, work fine in most cases, but occasionally indeed require a little
damping. Wainwright et al. (2003) successfully apply damping to a very similar algorithm
in an attempt to minimize a convexified Bethe free energy.

Appendix C: Constructing a Tight Convex Bound
In this appendix, we describe a procedure for constructing a tight convex bound Fconvex
of the Kikuchi free energy FKikuchi . It combines ideas from Section 4.3 and 4.4. That is,
we first convexify the Kikuchi free energy, bounding as little concave contributions from
negative inner regions as possible. Next, in the terms that we have to bound anyways, we
try to incorporate as many convex contributions as we can. This leads to the following
procedure.
186

fiEfficient minimization of the Kikuchi free energy

 Consider minus the entropy
S = 


X

S +



O

X

c S +

X

c S

,



I+

I




and choose c  c for   I such that the first term in
 


 X
X

X
X
c S 
c S +
(c  c )S ,
S = 
S +
 
 

I+

I

I

is (just) convex.

 With A the corresponding allocation matrix of Theorem 3.1, define the used resources
X
A |c |  c ,
c 
I

and rewrite
S = 


X


S +





c S +

I


X


X

X

c S

I+

(c  c )S +

X

I+

I

By construction, the first term is still convex.





(c  c )S




.



 To guarantee convexity, we have to bound the entropy contributions S in the second
term for each   I . To make this bound tighter, we include as many of the convex
contributions S as we can, while still satisfying the conditions in Theorem 4.2. Call
the corresponding overcounting numbers c  c  c  c and put the remaining
c  c back into the first term:


X

X
X
c S
S = 
c S +
S +
 

I+
I


X

X

(c  c )S .
(c  c )S +


I+

I

 Choose Fconvex to be the first term plus a linear bound of the second term.

To find c in the first step and similarly c in the third, we can use a linear program
similar to the one described in Section 3.2 for checking the conditions of Theorem 3.1. We
introduce slack variables  and replace condition (7d) by
X
A =  I (variable compensation) ,


187

fiHeskes

similar in spirit to (8). Furthermore, we add the inequality constraints   |cP
 | I
(no need to compensate for more than |c |) and search for the maximum of   I 
(compensate as much as possible). In terms of the corresponding solution  , we set c =
c   .

References
Aji, S., & McEliece, R. (2001). The generalized distributive law and free energy minimization. In Proceedings of the Allerton Conference on Communication, Control, and
Computing.
Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems. Journal
of the Royal Statistical Society Series B, 36, 192236.
Chiang, M., & Forney, G. (2001). Statistical physics, convex optimization and the sum
product algorithm. Tech. rep., Stanford University.
Darroch, J., & Ratcliff, D. (1972). Generalized iterative scaling. Annals of Mathematical
Statistics, 43, 14701480.
Dechter, R., Kask, K., & Mateescu, R. (2002). Iterative join-graph propagation. In Darwiche, A., & Friedman, N. (Eds.), Proceedings UAI-2002, pp. 128136.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society B, 39, 138.
Hall, P. (1935). On representatives of subsets. Journal of the London Mathematical Society,
10, 2630.
Heckerman, D. (1989). A tractable inference algorithm for diagnosing multiple diseases. In
Kanal, L., Henrion, M., Shachter, R., & Lemmer, J. (Eds.), Proceedings of the Fifth
Workshop on Uncertainty in Artificial Intelligence, pp. 163171, Amsterdam. Elsevier.
Heskes, T. (2003). Stable fixed points of loopy belief propagation are minima of the Bethe
free energy. In Becker, S., Thrun, S., & Obermayer, K. (Eds.), Advances in Neural
Information Processing Systems 15, pp. 359366, Cambridge. MIT Press.
Heskes, T. (2004). On the uniqueness of loopy belief propagation fixed points. Neural
Computation, 16, 23792413.
Heskes, T., Albers, K., & Kappen, B. (2003). Approximate inference and constrained optimization. In Uncertainty in Artificial Intelligence: Proceedings of the Nineteenth
Conference (UAI-2003), pp. 313320, San Francisco, CA. Morgan Kaufmann Publishers.
Heskes, T., Zoeter, O., & Wiegerinck, W. (2004). Approximate Expectation Maximization. In Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Advances in Neural Information
Processing Systems 16, pp. 353360, Cambridge. MIT Press.
Ihler, A., Fisher, J., & Willsky, A. (2005). Loopy belief propagation: Convergence and
effects of message errors. Journal of Machine Learning Research, 6, 905936.
Jaakkola, T., & Jordan, M. (1999). Variational probabilistic inference and the QMR-DT
network. Journal of Artificial Intelligence Research, 10, 291299.
188

fiEfficient minimization of the Kikuchi free energy

Jirousek, R., & Preucil, S. (1995). On the effective implementation of the iterative proportional fitting procedure. Computational Statistics and Data Analysis, 19, 177189.
Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. (1998). An introduction to variational
methods for graphical models. In Jordan, M. (Ed.), Learning in Graphical Models,
pp. 183233. Kluwer Academic Publishers, Dordrecht.
Kikuchi, R. (1951). The theory of cooperative phenomena. Physical Review, 81, 9881003.
Kschischang, F., Frey, B., & Loeliger, H. (2001). Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 47 (2), 498519.
Lauritzen, S. (1996). Graphical models. Oxford University Press, Oxford.
Luenberger, D. (1984). Linear and Nonlinear Programming. Addison-Wesley, Reading,
Massachusetts.
McEliece, R., MacKay, D., & Cheng, J. (1998). Turbo decoding as as an instance of Pearls
belief propagation algorithm. IEEE Journal on Selected Areas in Communication,
16 (2), 140152.
McEliece, R., & Yildirim, M. (2003). Belief propagation on partially ordered sets. In
Gilliam, D., & Rosenthal, J. (Eds.), Mathematical Systems Theory in Biology, Communications, Computation, and Finance, pp. 275300. Springer, New York.
Murphy, K. (2001). The Bayes Net toolbox for Matlab. Computing Science and Statistics,
33, 331350.
Murphy, K., Weiss, Y., & Jordan, M. (1999). Loopy belief propagation for approximate
inference: An empirical study. In Laskey, K., & Prade, H. (Eds.), Proceedings of
the Fifteenth Conference on Uncertainty in Articial Intelligence, pp. 467475, San
Francisco, CA. Morgan Kaufmann Publishers.
Neal, R., & Hinton, G. (1998). A view of the EM algorithm that justifies incremental,
sparse, and other variants. In Jordan, M. (Ed.), Learning in Graphical Models, pp.
355368. Kluwer Academic Publishers, Dordrecht.
Pakzad, P., & Anantharam, V. (2002). Belief propagation and statistical physics. In 2002
Conference on Information Sciences and Systems, Princeton University.
Pakzad, P., & Anantharam, V. (2005). Estimation and marginalization using Kikuchi approximation methods. Neural Computation, 17, 18361873.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent systems: Networks of Plausible Inference. Morgan Kaufmann, San Francisco, CA.
Pelizzola, A. (2005). Cluster variation method in statistical physics and graphical models.
Journal of Physics A, 38, R309R339.
Seung, S., Richardson, T., Lagarias, J., & Hopfield, J. (1998). Minimax and Hamiltonian
dynamics of excitatory-inhibitory networks. In Jordan, M., Kearns, M., & Solla, S.
(Eds.), Advances in Neural Information Processing Systems 10, pp. 329335. MIT
Press.
Tatikonda, S., & Jordan, M. (2002). Loopy belief propagation and Gibbs measures. In Darwiche, A., & Friedman, N. (Eds.), Uncertainty in Artificial Intelligence: Proceedings
189

fiHeskes

of the Eighteenth Conference (UAI-2002), pp. 493500, San Francisco, CA. Morgan
Kaufmann Publishers.
Teh, Y., & Welling, M. (2002). The unified propagation and scaling algorithm. In Dietterich,
T., Becker, S., & Ghahramani, Z. (Eds.), Advances in Neural Information Processing
Systems 14, pp. 953960, Cambridge. MIT Press.
Wainwright, M., Jaakkola, T., & Willsky, A. (2002a). A new class of upper bounds on the log
partition function. In Darwiche, A., & Friedman, N. (Eds.), Uncertainty in Artificial
Intelligence: Proceedings of the Eighteenth Conference (UAI-2002), pp. 536543, San
Francisco, CA. Morgan Kaufmann Publishers.
Wainwright, M., Jaakkola, T., & Willsky, A. (2002b). Tree-based reparameterization for
approximate estimation on loopy graphs. In Dietterich, T., Becker, S., & Ghahramani,
Z. (Eds.), Advances in Neural Information Processing Systems 14, pp. 10011008,
Cambridge. MIT Press.
Wainwright, M., Jaakkola, T., & Willsky, A. (2003). Tree-reweighted belief propagation
algorithms and approximate ML estimation via pseudo-moment matching. In Bishop,
C., & Frey, B. (Eds.), Proceedings of the Ninth International Workshop on Artificial
Intelligence and Statistics. Society for Artificial Intelligence and Statistics.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. In Leen,
T., Dietterich, T., & Tresp, V. (Eds.), Advances in Neural Information Processing
Systems 13, pp. 689695, Cambridge. MIT Press.
Yedidia, J., Freeman, W., & Weiss, Y. (2005). Constructing free energy approximations
and generalized belief propagation algorithms. IEEE Transactions on Information
Theory, 51, 22822312.
Yuille, A. (2002). CCCP algorithms to minimize the Bethe and Kikuchi free energies:
Convergent alternatives to belief propagation. Neural Computation, 14, 16911722.

190

fi