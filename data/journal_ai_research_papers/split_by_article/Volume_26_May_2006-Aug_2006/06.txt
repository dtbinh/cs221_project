Journal of Artificial Intelligence Research 26 (2006) 247287

Submitted 01/06; published 07/06

How the Landscape of Random Job Shop Scheduling
Instances Depends on the Ratio of Jobs to Machines
Matthew J. Streeter
Stephen F. Smith

matts@cs.cmu.edu
sfs@cs.cmu.edu

Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA, 15213 USA

Abstract
We characterize the search landscape of random instances of the job shop scheduling
problem (JSP). Specifically, we investigate how the expected values of (1) backbone size,
(2) distance between near-optimal schedules, and (3) makespan of random schedules vary
N
N
N
as a function of the job to machine ratio ( M
). For the limiting cases M
 0 and M

N
we provide analytical results, while for intermediate values of M we perform experiments.
N
N
We prove that as M
 0, backbone size approaches 100%, while as M
  the backbone
N
N
vanishes. In the process we show that as M  0 (resp. M  ), simple priority rules
almost surely generate an optimal schedule, providing theoretical evidence of an easyhard-easy pattern of typical-case instance difficulty in job shop scheduling. We also draw
connections between our theoretical results and the big valley picture of JSP landscapes.

1. Introduction
1.1 Motivations
The goal of this work is to provide a picture of the typical landscape of a random instance
of the job shop scheduling problem (JSP), and to determine how this picture changes as
N
a function of the job to machine ratio ( M
). Such a picture is potentially useful in (1)
N
understanding how typical-case instance difficulty varies as a function of M
and (2) designing
or selecting search heuristics that take advantage of regularities in typical instances of the
JSP.

1.1.1 Understanding instance difficulty as a function of

N
M

The job shop scheduling literature contains much empirical evidence that square JSPs (those
N
with M
= 1) are more difficult to solve than rectangular instances (Fisher & Thompson,
1963). This work makes both theoretical and empirical contributions toward understanding
this phenomenon. Empirically, we show that both random schedules and random local
N
 1. Analytically, we prove that in the two
optima are furthest from optimality when M
N
N
limiting cases ( M  0 and M  ) there exist simple priority rules that almost surely
produce an optimal schedule, providing theoretical evidence of an easy-hard-easy pattern
of instance difficulty in the JSP.
c
2006
AI Access Foundation. All rights reserved.

fiStreeter & Smith

1.1.2 Informing the design of search heuristics
Heuristics based on local search, for example tabu search (Glover & Laguna, 1997; Nowicki
& Smutnicki, 1996) and iterated local search (Lourenco, Martin, and Stutzle, 2003), have
shown excellent performance on benchmark instances of the job shop scheduling problem
(Jain & Meeran, 1998; Jones & Rabelo, 1998). In order to design an effective heuristic, one
must (explicitly or implicitly) make assumptions about the search landscape of instances
to which the heuristic will be applied. For example, Nowicki and Smutnicki motivate the
use of path relinking in their state-of-the-art i-TSAB algorithm by citing evidence that the
JSP has a big valley distribution of local optima (Nowicki & Smutnicki, 2005). One of
the conclusions of our work is that the typical landscape of random instances can only be
N
N
thought of as a big valley for values of M
close to 1; for larger values of M
(including values
common in benchmark instances), the landscape breaks into many big valleys, suggesting
that modifications to i-TSAB may allow it to better handle this case (we discuss i-TSAB
further in 9.3).
1.2 Contributions
The contributions of this paper are twofold. First, we design a novel set of experiments and
run these experiments on random instances of the JSP. Second, we derive analytical results
that confirm and provide insight into the trends suggested by our experiments.
The main contributions of our empirical work are as follows.
N
, we show that low-makespan schedules are clustered in a small
 For low values of M
region of the search space and many attributes (i.e., directed disjunctive graph edges)
N
are common to all low-makespan schedules. As M
increases, low-makespan schedules
become dispersed throughout the search space and there are no attributes common
to all low-makespan schedules.

 We introduce a statistic (neighborhood exactness) that can be used to quantitatively
measure the smoothness of a search landscape, and estimate the expected value of
this statistic for random instances of the JSP. These results, in combination with the
results on clustering, suggest that the landscape of typical instances of the JSP can
N
N
be described as a big valley only for low values of M
; for high values of M
there are
many separate big valleys.
For the limiting cases
we prove that

N
M

 0 and

N
M

 , we derive analytical results. Specifically,

N
 as M
 0, the expected size of the backbone (i.e., the set of problem variables that
N
have a common value in all global optima) approaches 100%, while as M
 , the
expected backbone size approaches 0%; and
N
N
 as M
 0 (resp. M
 ), a randomly generated schedule will almost surely (a) be
located close in the search space to an optimal schedule and (b) have near-optimal
makespan.

248

fiThe Landscape of Random Job Shop Scheduling Instances

2. Related Work
There are at least three threads of research that have conducted search space analyses
related to the ones we conduct here. These include literature on the big valley distribution
common to a number of combinatorial optimization problems, studies of backbone size in
Boolean satisfiability, and a statistical mechanical analysis of the TSP. We briefly review
these three areas below, as well as relevant work on phase transitions and the easy-hardeasy pattern of instance difficulty.
2.1 The Big Valley
The term big valley originated in a paper by Boese et al. (1994) that examined the
distribution of local optima in the Traveling Salesman Problem (TSP). Based on a sample
of local optima obtained by next-descent starting from random TSP tours, Boese calculated
two correlations:
1. the correlation between the cost of a locally optimal tour and its average distance to
other locally optimal tours, and
2. the correlation between the cost of a locally optimal tour and the distance from that
tour to the best tour in the sample.
The distance between two TSP tours was defined as the total number of edges minus
the number of edges that are common to the two tours. Based on the fact that both of
these correlations were surprisingly high, Boese conjectured that local optima in the TSP
are arranged in a big valley. Adapted from the work of Boese et al. (1994), Figure 1
gives an intuitive picture of the big valley, in which the set of local minima appears convex
with one central global minimum (Boese et al., 1994). We offer a more formal definition
of a big valley landscape in 6.
Boeses analysis has been applied to other combinatorial problems (Kim & Moon, 2004),
including the permutation flow shop scheduling problem (Watson, Barbulescu, Whitley, &
Howe, 2002; Reeves & Yamada, 1998) and the JSP (Nowicki & Smutnicki, 2001). Correlations observed for the JSP are generally weaker than those observed for the TSP.
In a related study, Mattfeld (1996) examined cost-distance correlations in the famous
JSP instance ft10 (Beasley, 1990) and found evidence of a Massif Central. . . where many
near optimal solutions reside laying closer together than other local optima. 4 contains
related results on the backbone size of ft10.
2.2 Backbone Size
The backbone of a problem instance is the set of variables that are assigned a common value
in all globally optimal solutions of that instance. For example, in the Boolean satisfiability
problem (SAT), the backbone is the set of variables that are assigned a fixed truth value
in all satisfying assignments. In the JSP, the backbone has been defined as the number of
disjunctive edges (3.2) that have a common orientation in all globally optimal schedules
(a formal definition is given in 4).
There is a large literature on backbones in combinatorial optimization problems, including many empirical and analytical results (Slaney & Walsh, 2001; Monasson, Zecchina,
249

fiStreeter & Smith

Figure 1: An intuitive picture of a big valley landscape.
Kirkpatrick, Selman, & Troyansky, 1999). In an analysis of problem difficulty in the JSP,
Watson et al. (2001) present histograms of backbone size for random 6x6 (6 job, 6 machine)
and 6x4 (6 job, 4 machine) JSP instances. Summarizing experiments not reported in their
paper, Watson et al. note that For [job:machine ratios] > 1.5, the bias toward small backbones becomes more pronounced, while for ratios < 1, the bias toward larger backbones
is further magnified. 4 generalizes these observations and proves two theorems that give
insight into why this phenomenon occurs.
2.3 Statistical Mechanical Analyses
A large and growing literature applies techniques from statistical mechanics to the analysis
of combinatorial optimization problems (Martin, Monasson, & Zecchina, 2001). At least
one result obtained in this literature concerns clustering of low-cost solutions. In a study of
the TSP, Mezard and Parisi (1986) obtain an expression for the expected overlap (number
of common edges) between random TSP tours drawn from a Boltzmann distribution. They
show that as the temperature parameter of the Boltzmann distribution is lowered (placing
more probability mass on low-cost TSP tours), expected overlap approaches 100%. Though
we do not use a Boltzmann weighting, 5 of this paper examines how expected overlap
between random JSP schedules changes as more probability mass is placed on low-makespan
schedules.
2.4 Phase Transitions and the Easy-hard-easy Pattern
Loosely speaking, a phase transition occurs in a system when the expected value of some
statistic varies discontinuously (asymptotically) as a function of some parameter. As an
example, for any  > 0 it holds that random instances of the 2-SAT problem are satisfiable
with probability asymptotically approaching 1 when the clause to variable ratio ( m
n ) is 1,
but are satisfiable with probability approaching 0 when the clause to variable ratio is 1 + .
A similar statement is conjectured to hold for 3-SAT; the critical value k of m
n (if it exists)
must satisfy 3.42  k  4.51 (Achlioptas & Peres, 2004).
For some problems that exhibit phase transitions (notably 3-SAT), average-case instance
difficulty (for typical solvers) appears to first increase and then decrease as one increases
the relevant parameter, with the hardest instances appearing close to the threshold value
250

fiThe Landscape of Random Job Shop Scheduling Instances

(A) JSP instance

J1 :

J11

J 12

J 2 : J12









J 13

J 22


(B) JSP schedule

J 14

J 32

J 42





J11





J12

J 22



 time

(C) Disjunctive

graph




J11

J 12

J 13

J 32

J 14
J 42



J 14
o*

o

J12


J 12 J 13



J 22


J 32


J 42

Figure 2: (A) A JSP instance,

(B) afeasibleschedule for the instance, and (C) the disjunctive graph representation of the schedule. Boxes represent operations; operation
durations are proportional to the width of a box; and the machine on which an
operation is performed is represented by texture. In (C), solid arrows represent
conjunctive arcs and dashed arrows represent disjunctive arcs (arc weights are
proportional to the duration of the operation the arc points out of).

(Cheeseman, Kanefsky, & Taylor, 1991; Yokoo, 1997). This phenomenon has been referred
to as an easy-hard-easy pattern of instance difficulty (Mammen & Hogg, 1997). In 7.4
we discuss evidence of an easy-hard-easy pattern of instance difficulty in the JSP, though
(to our knowledge) it is not associated with any phase transition.
The results in 4-5 and the empirical results in 6 were previously presented in a conference paper (Streeter & Smith, 2005a).

3. The Job Shop Scheduling Problem
We adopt the notation [n]  {1, 2, . . . , n}.
3.1 Problem Definition
Definition (JSP instance). An N by M JSP instance I = {J 1 ,J 2 , . . . , J N } is a set of N
k ) is a sequence of M operations. Each operation
jobs , where each job J k = (J1k , J2k , . . . , JM
k
o = Ji has an associated duration  (o)  (0, max ] and machine m(o)  [M ]. We require
that each job uses each machine exactly once (i.e., for each J k  I and m  [M ], there is
exactly one i  [M ] such that m(Jik ) = m). We define
1. ops(I)  {Jik : k  [N ], i  [M ]},
2.  (J k ) 

PM

k
i=1  (Ji ),

and
251

fiStreeter & Smith

3. the job-predecessor J (Jik ) of an operation Jik as
J (Jik )




k
Ji1
o

if i > 1
otherwise

where o is a fictitious operation with  (o ) = 0 and m(o ) undefined.
Definition (JSP schedule). A JSP schedule for an instance I is a function S : ops(I) 
<+ that associates with each operation o  ops(I) a start time S(o) (operation o is performed
on machine m(o) from time S(o) to time S(o) +  (o); preemption is not allowed). We make
the following definitions.
1. The completion time of an operation o is S + (o)  S(o) +  (o).
2. The machine-predecessor M(o) of an operation o  ops(I) is

M(o) 

arg maxoOprev (o) S(o)
o

if Oprev (o) 6= 
otherwise.

where Oprev (o) = {o  ops(I) : m(o) = m(o), S(o) < S(o)} is the set of operations
scheduled to run before o on os machine.
3. S is a feasible schedule if S(o)  max(S + (J (o)), S + (M(o))) o  ops(I).
4. The quantity
`(S)  max S + (o)
oops(I)

is called the makespan of S.
We consider the makespan-minimization version of the JSP, in which the goal is to find
a schedule that minimizes the makespan.
For the remainder of the paper, whenever we refer to a JSP schedule S we shall adopt
the convention that S(o ) = 0 and we shall assume that
S(o) = max(S + (J (o)), S + (M(o))) o  ops(I)

(3.1)

(i.e., S is a so-called semi-active schedule, French, 1982). In other words, we ignore schedules
with superfluous idle time at the start of the schedule or between the end of one operation
and the start of another.
Figure 2 (A) and (B) depict, respectively, a JSP instance and a feasible schedule for
that instance.
3.2 Disjunctive Graphs
A schedule satisfying (3.1) can be uniquely represented by a weighted, directed graph called
its disjunctive graph. In the disjunctive graph representation of a schedule S for a JSP
instance I, each operation o  ops(I) is a vertex and a directed edge (o1 , o2 ) indicates that
operation o1 completes before o2 starts.
252

fiThe Landscape of Random Job Shop Scheduling Instances

Definition (disjunctive graph). The disjunctive graph G = G(I, S) of a schedule S for
~ w) defined as follows.
a JSP instance I is the weighted, directed graph G = (V, E,
 V = ops(I)  {o , o }, where o (like o ) is a fictitious operation with  (o ) = 0 and
m(o ) undefined.
~ =C
~  D,
~ where
 E

	
~ = {(J (o), o) : o  ops(I)} (J k , o ) : k  [N ] is called the set of conjunctive
 C
M
arcs (which specify that o cannot start until J (o) completes), and
~ = {(o1 , o2 ) : {o1 , o2 }  ops(I), m(o1 ) = m(o2 ), S(o1 ) < S(o2 )} is called the set
 D
of disjunctive arcs (which specify, for each pair of operations performed on the
same machine, which of the two operations is to be performed first).
 w((o1 , o2 )) =  (o1 ).
Figure 2 (C) depicts the disjunctive graph for the schedule depicted in Figure 2 (B).
The connection between a schedule and its disjunctive graph is established by the following
proposition (Roy & Sussmann, 1964).
Proposition 1. Let S be a feasible schedule for I satisfying (3.1), and let G = G(I, S) be
the corresponding disjunctive graph. Then `(S) is equal to the length of the longest weighted
path from o to o in G.
Proof. For any operation o, let L(o) denote the length of the longest weighted path from
o to o in G. It suffices to show that for any o  ops(I), S(o) = L(o). This follows by
induction on the number of edges in the path, with the base case S(o ) = L(o ) = 0.
The undirected version of a disjunctive arc is called a disjunctive edge.
Definition (disjunctive edge). Let I be a JSP instance. A disjunctive edge is a set
{o1 , o2 }  ops(I) with m(o1 ) = m(o2 ). We define the following notation.
 E(I) is the set of disjunctive edges for I.
 Let S be a schedule for I and let e = {o1 , o2 } be a disjunctive edge. We denote by
~e(S) the unique arc in {(o1 , o2 ), (o2 , o1 )} that appears in the disjunctive graph G(I, S)
(this arc is called the orientation of e in S).
We measure the distance between two schedules S1 and S2 for a JSP instance I by
counting the number of disjunctive edges that are oriented in opposite directions in G(I, S1 )
and G(I, S2 ).
Definition (disjunctive graph distance). The disjunctive graph distance kS1  S2 k
between two schedules S1 and S2 for a JSP instance I is defined by
kS1  S2 k  |{e  E(I) : ~e(S1 ) 6= ~e(S2 )}| .
253

fiStreeter & Smith

3.3 Random Schedules and Instances
We define a uniform distribution over JSP instances as follows. Our distribution is identical
to the one used by Taillard (1993).
Definition (random JSP instance). A random N by M JSP instance I is generated as
follows.
1. Let 1 , 2 , . . . , N be random permutations of [M ].
2. Let G be a probability distribution over (0, max ] with mean  and variance  2 > 0.
3. Define I = {J 1 , J 2 , . . . , J N }, where m(Jik ) = k (i) and each  (Jik ) is drawn (independently at random) from G.
Note that this definition (and likewise, our theoretical results) assumes a maximum
operation duration max , but makes no assumptions about the form of the distribution of
operation durations. For the empirical results reported in this paper, we choose operation
durations from a uniform distribution over {1, 2, . . . , 100}.
Our proofs will frequently make use of priority rules. A priority rule is a greedy schedulebuilding algorithm that assigns a priority to each operation and, at each step of the greedy
algorithm, assigns the earliest possible start time to the operation with minimum priority.
Definition (priority rule). A priority rule  is a function that, given an instance I and
an operation o  ops(I), returns a priority (I, o)  <. The schedule S = S(, I) associated
with  is defined by the following procedure.
1. U nscheduled  ops(I), S(o )  0.
2. While |U nscheduled| > 0 do:
(a) Ready  {o  U nscheduled : J (o) 
/ U nscheduled}.
(b) o  the element of Ready with least priority.
(c) S(o)  max(S + (J (o)), S + (M(o))).
(d) Remove o from U nscheduled.
A priority rule is called instance-independent if, for any N by M JSP instance I and
integers k  [N ], i  [M ], the value (I, Jik ) depends only on k, i, N , and M .
We obtain a random schedule by assigning random priorities to each operation. The
resulting distribution is equivalent to the one used by Mattfeld (1996).
Definition (random schedule). A random schedule for an N by M JSP instance I is
generated by performing the following steps.
1. Create a list L containing M occurrences of the integer k for each k  [N ] (we think
of the M occurrences of k as representing the operations in the job J k ).
2. Shuffle L (obtaining each permutation with equal probability).
3. Return the schedule S(rand , I) where rand (I, Jik ) = the index of the ith occurrence
of k in L.
254

fiThe Landscape of Random Job Shop Scheduling Instances

4. Number of Common Attributes as a Function of Makespan
The backbone of a JSP instance is the set of disjunctive edges that have a common orientation in all schedules whose makespan is globally optimal. For   1, we define the
 backbone to be the set of disjunctive edges that have a common orientation in all schedules whose makespan is within a factor  of optimal (a related definition appears in Slaney
& Walsh, 2001).
Definition ( backbone). Let I be a JSP instance with optimal makespan `min (I). For
  1, let  opt(I)  {S : `(S)    `min (I)} be the set of schedules whose makespan is
within a factor  of optimal. Then
 backbone(I)  {e  E(I) : ~e(S1 ) = ~e(S2 ) {S1 , S2 }   opt(I)} .
In this section we compute the expected value of | backbone| as a function of  for
random N by M JSP instances, and examine how the shape of this curve changes as a
N
function of M
.
4.1 Computing the  backbone
To compute the  backbone we use the following proposition.
Proposition 2. Let I be a JSP instance with optimal makespan `min (I). Let e = {o1 , o2 }
be a disjunctive edge with orientations a1 = (o1 , o2 ) and a2 = (o2 , o1 ). For any disjunctive
arc a, let `min (I|a) denote the optimum makespan among schedules whose disjunctive graph
contains the arc a. Then
e   backbone(I)  max {`min (I|a1 ), `min (I|a2 )} >   lmin (I) .
Proof. If e   backbone, then e must have a common orientation (say a1 ) in all schedules
S with `(S)    `min (I), which implies `min (I|a2 ) >   `min (I). If e 
/  backbone, then
there must be some {S1 , S2 }   opt(I) with ~e(S1 ) = a1 and ~e(S2 ) = a2 , which implies
max{`min (I|a1 ), `min (I|a2 )}    `min (I).

Thus to compute  backbone(I) we need only to compute `min (I|a) for the 2M N2
possible choices of a. Given a disjunctive arc a, we compute `min (I|a) using branch and
bound. In branch and bound algorithms for the JSP, nodes in the search tree represent
choices of orientations for a subset of the disjunctive edges. By constructing a root search
tree node that has a as a fixed arc, we can determine `min (I|a). We use a branch and bound
algorithm due to Brucker et al. (1994) because it is efficient and because the code for it is
freely available via ORSEP (Brucker, Jurisch,
& Sievers, 1992).


N
Computing `min (I|a) for the 2M 2 possible choices of a requires only 1 + M N2 runs
of branch and bound. The first run is used to find a globally optimal schedule,
which gives

N
N
the value of `min (I|a) for M 2 possible choices of a (namely, the M 2 disjunctive arcs
that are
 present in the globally optimal schedule). A separate run is used for each of the
M N2 remaining choices of a.
Figure 3 graphs the fraction of disjunctive edges that belong to the  backbone as a
function of  for instance ft10 (a 10 job, 10 machine instance) from the OR library (Beasley,
255

fiStreeter & Smith

Instance ft10
Normalized |-backbone|

1
0.8
0.6
0.4
0.2
0
1.00

1.02

1.04

1.06

1.09

1.11

1.13



Figure 3: Normalized | backbone| as a function of  for OR library instance ft10.
1990). Note that by definition the curve is non-increasing with respect to , and that the
curve is exact for all . It is noteworthy that among schedules whose makespan is within a
factor 1.005 of optimal, 80% of the disjunctive edges have a fixed orientation. We will see
N
that this behavior is typical of JSP instances with M
= 1.
4.2 Results
We plotted | backbone| as a function of  for all instances in the OR library having 10 or
fewer jobs and 10 or fewer machines. The results are available online (Streeter & Smith,
2005b). Inspection of the graphs revealed that the shape of the curve is largely a function
of the job:machine ratio. To investigate this further, we repeat these experiments on a large
number of randomly generated JSP instances.
We use randomly generated instances with 7 different combinations of N and M to study
N
N
instances with M
equal to 1, 2, or 3. For M
= 1 we use 6x6, 7x7, and 8x8 instances; for
N
N
=
2
we
use
8x4
and
10x5
instances;
and
for
M
M = 3 we use 9x3 and 12x4 instances. We
generate 1000 random instances for each combination of N and M .
Figure 4 parts (A), (B), and (C) graph the expected fraction of edges belonging to the
N
-backbone as a function of  for each combination of N and M , grouped according to M
.
N
Figure 4 (D) compares the curves for different values of M , and plots the 0.25 and 0.75
quantiles. For the purposes of this study the two most important observations about Figure
4 are as follows.
 The curves depend on both the size of the instance (i.e., N M ) and the shape (i.e.,
N
N
M ). Of these two factors, M has by far the stronger influence on the shape of the
curves.
 For all values of , the expected fraction of edges belonging to the  backbone decreases
N
as M
increases.
256

fiThe Landscape of Random Job Shop Scheduling Instances

(A) Job:machine ratio 1:1

(B) Job:machine ratio 2:1
1

0.8

E[frac. edges in -backbone]

E[frac. edges in -backbone]

1

6x6 instances
7x7 instances

0.6

8x8 instances
0.4

0.2

0.8

8x4 instances
10x5 instances

0.6
0.4
0.2
0

0
1

1.1

1.2



1.3

1.4

1

1.5

1.1

1.3

1.4

1.5



(C) Job:machine ratio 3:1

(D) Comparison

1

1

0.8

Frac. edges in -backbone

E[frac. edges in -backbone]

1.2

9x3 instances

0.6

12x4 instances

0.4
0.2
0

0.8

8x8 instances
10x5 instances

0.6

12x4 instances
0.4

0.2

0

1

1.1

1.2

1.3

1.4

1.5



1

1.1

1.2



1.3

1.4

1.5

Figure 4: Expected fraction of edges in -backbone as a function of  for random JSP
N
instances. Graphs (A), (B), and (C) depict curves for random instances with M
= 1, 2, and 3, respectively. Graph (D) compares the curves depicted in (A), (B),
and (C) (only the curves for the largest instance sizes are shown in (D)). In (D),
top and bottom error bars represent 0.75 and 0.25 quantiles, respectively.

257

fiStreeter & Smith

4.3 Analysis
We now give some insight into Figure 4 by analyzing two limiting cases. We prove that as
N
M 0, the expected fraction of disjunctive edges that belong to the backbone approaches
N
1, while as M
 this expected fraction approaches 0.
N
Intuitively, what happens is as follows. As M
0 (i.e., N is held constant and M )
each of the jobs becomes very long. Individual disjunctive edges then represent precedence
relations among operations that should be performed very far apart in time. For example,
if there are 10,000 machines (and so each job consists of 10,000 operations), a disjunctive
edge might specify whether operation 1,200 of job A is to be performed before operation
8,500 of job B. Clearly, waiting for job B to complete 8,500 of its operations before allowing
job A to complete 12% of its operations is likely to produce an inefficient schedule. Thus,
orienting a single disjunctive edge in the wrong direction is likely to prevent a schedule
from being optimal, and so any particular edge will likely have a common orientation in all
globally optimal schedules.
N
In contrast, when M
, it is the workloads of the machines that become very long.
The order in which the jobs are processed on a particular machine does not matter much as
long as the machine with the longest workload is kept busy, and so the fact that a particular
edge is oriented a particular way is unlikely to prevent a schedule from being optimal. All
of this is formalized below.
We will make use of the following well-known definition.
Definition (whp). A sequence of events n occurs with high probability (whp) if limn
P[n ] = 1.
Lemma 1 and Theorem 1 show that for constant N , a randomly chosen edge of a random
N by M JSP instance will be in the backbone whp (as M ). Lemma 2 and Theorem 2
show that for constant M , a randomly chosen edge of a random N by M JSP instance will
not be in the backbone whp (as N ).
Lemma 1. Let I be a random N by M JSP instance, and let S = S(, I) be the schedule
for I obtained using some instance-independent priority rule . For an arbitrary job J  I,
define SJ  S + (JM )   (J). Then E[SJ ] is O(N ).
Proof. We assume N = 2 and M > 1. The generalization to larger N is straightforward,
while the cases N = 1 and M = 1 are trivial. Let I = {J 1 , J 2 } and let J = J 1 .
Let T = (o1 , o2 , . . . , oN M ) be the sequence of operations selected from Ready (in line
2(b) of the definition of a priority rule in 3.3) in constructing S. We say that an operation
Ji1 overlaps with an operation Jj2 if
1. Jj2 appears before Ji1 in T , and
1 ), S + (J 1 ) +  (J 1 )] 6=  .
2. [S(Jj2 ), S + (Jj2 )]  [S + (Ji1
i1
i

If additionally m(Ji1 ) = m(Jj2 ), we say that Ji1 contends with Jj2 . Intuitively, if o  Ji1
overlaps with o0  Jj2 then the start time of o might have been delayed because os machine
was being used by o0 . If o contends with o0 , then the start time of o actually was delayed.
258

fiThe Landscape of Random Job Shop Scheduling Instances

Let i,j (resp. i,j ) be an indicator for the event that Ji1 overlaps (resp. contends) with
2
2
1
Let
S Ci  {Jj : i,j = 1} be the set of operations in J that Ji overlaps with. Then
|Ci  i0 >i Ci0 |  1. Thus

Jj2 .

X
i

|Ci | =

X
i

|Ci \

[

Ci0 | +

i0 >i

X
i

|Ci 

[

Ci0 |  2M .

(4.1)

i0 >i

Let I = IN,M 1 be a random N by M  1 JSP instance, and define i,j , i,j , and Ci
analogously to the above. Then for i, j  M  1,




P i,j = 1|m(Ji1 ) = m(Jj2 ) = P i,j = 1 .
This is true because P[i,j = 1] is a function of the joint distribution of the operations in
the set {Ji10 : i0 < i}  {Jj20 : j 0 < j}; and, as far as this joint distribution is concerned,
conditioning on the event m(Ji1 ) = m(Jj2 ) is like deleting the operations that use the
machine m(Ji1 ).
h
i


 
1
1
1
P i,j = 1|m(Ji1 ) = m(Jj2 ) = M
P i,j = 1 = M
E i,j .
Thus E [i,j ] = P [i,j = 1] = M
Therefore,
PM 1 PM 1
PM PM
j=1 E[i,j ]
i=1
j=1 E[i,j ]  2 +
i=1
1 PM 1 PM 1
= 2 + M i=1
j=1 E[i,j ]
1 PM 1
= 2 + M i=1 E[|Ci |]
4
where in the last step we have used (4.1). It follows that E[SJ ]  4max (max is the
maximum operation duration defined in 3). When we consider arbitrary N , we get E[SJ ] 
4max (N  1).
As a corollary of Lemma 1, we can show that a simple priority rule (0 ) almost surely
N
 0.
generates an optimal schedule in the case M
Definition (priority rule 0 ). Given an N by M JSP instance I, let k  = arg maxk[N ]
 (J k ) be the index of the longest job. The priority rule 0 first schedules the operations in

J k , then schedules the remaining operations in a fixed order.

i
if k = k 
k
0 (I, Ji ) =
M k + i otherwise.
Corollary 1. Let I be a random N by M JSP instance. Then for fixed N , it holds
whp (as M  ) that the schedule S = S(0 , i) is optimal and has makespan `(S) =
maxk[N ]  (J k ).
Proof. Define the priority rule k by k (I, Jik ) = i if k = k; M k + i otherwise. Then k is
instance-independent, and 0 is equivalent to k . Thus for any J  I we have
E[J0 ] 

X

E[Jk ] = O(N 2 )

k

259

fiStreeter & Smith

S(,I)

where we define J  J
, and the second step uses Lemma 1. By Markovs inequal1
0
ity, J < M 4 J  I whp. By the Central Limit Theorem, each
  (J) is asymptotically
normally distributed with mean M and standard deviation  M . It follows that whp,
1



 (J k )   (J k ) > M 4 k 6= k  . This implies `(S) =  (J k ). Because  (J k ) is a lower
bound on the makespan of any schedule, the corollary follows.
Theorem 1. Let I be a random N by M JSP instance, and let e be a randomly selected
element of E(I). Then for fixed N , it holds whp (as M ) that e  1 backbone(I).
Proof. Let e = {Ji , Jj0 } with i  j and let a = (Jj0 , Ji ). By Proposition 1 and Corollary 1,
it suffices to show that whp, all disjunctive graphs containing a contain a path from o to
o with weighted length > maxk[N ]  (J k ).
3

Assume j  i  M 4 (this holds whp because both i and j are selected uniformly at
random from [M ]), and consider the path
P = (o , J10 , J20 , . . . , Jj0 , Ji , Ji+1 , . . . , JM , o )
3

which passes through |P |  3+M +M 4 vertices and has weighted length w(P ). We want
to show that w(P ) > maxJI  (J) whp. By the Central Limit Theorem, (1) for any fixed
i and j, w(P
p) is asymptotically normally distributed with mean (|P |  2) and standard
deviation  (|P |  2) and (2) for each 
J,  (J) is asymptotically normally distributed
with mean M and standard deviation  M . That w(P ) > maxJI  (J) whp follows by
Chebyshevs inequality.
N
 , a simple priority rule ( ) almost surely generates
Lemma 2 shows that as M
a schedule in which no machine is idle until all the operations performed on that machine
have been completed (a schedule with this property is clearly optimal).

Definition (priority rule  ). Given an N by M JSP instance I, the priority rule 
first schedules the first operation of each job (taking the jobs in order of ascending indices),
then the second operation of each job, and so forth. It is defined by  (I, Jik ) = iN + k.
Lemma 2. Let I be a random N by M JSP instance. Then for fixed M , it holds whp (as
N  ) that the schedule S = S( , I) has the property that
S(o) = S + (M(o)) o  ops(I) .
Proof. Suppose that when executing  we replace the line S(o)  max(S + (J (o)),
S + (M(o))) (line 2(c) in the definition of a priority rule given in 3.3) with S(o)  S + (M(o)).
If the resulting S is feasible then the replacement must have had no effect. Thus it suffices
to show that the resulting S is feasible whp. Equivalently, we want to show that whp,
S(o)  S + (J (o)) o  ops(I) when S is constructed using the modified version of line 2
(c).
Let ops2+ (I) = {Jik  ops(I) : i > 1} be the set of operations that are not first in their
job. It suffices to show that S(o)  S(J (o))  max o  ops2+ (I). To this end, consider
an arbitrary operation o = Jik  ops2+ (I). Under  , the number of operations with lower
260

fiThe Landscape of Random Job Shop Scheduling Instances

priority than o is (i  1)N + (k  1). The number of operations that have lower priority
1
than Jik and run on machine m(o) is, in expectation, equal to M
[(i  1)(N  1) + (k  1)]
(where the switch from N to N  1 is due to the fact that o is the only operation in job J k
that uses machine m(o)). It follows that
E[S(o)] =


[(i  1)(N  1) + (k  1)]
M

so that
k
)] = 
E[S(o)  S(J (o))] = E[S(Jik )  S(Ji1

N 1
.
M

In Appendix A we use a martingale tail inequality to establish the following claim.
Claim 2.1. With high probability, for all o  ops2+ (I) we have
1
S(o)  S(J (o))  E[S(o)  S(J (o))] .
2
The Lemma then follows from the fact that 21 E[S(o)  S(J (o))] > max for N sufficiently
large.
Based on the results of computational experiments, Taillard (1994) conjectured that as
  the optimal makespan is almost surely equal to the maximum machine workload.
The following corollary of Lemma 2 confirms this conjecture.
N
M

Corollary 2. Let I be a random N by M JSP instance with optimal makespan `min (I).
Let  (m)   ({o  ops(I) : m(o) = m}) denote the workload of machine m. Then for fixed
M , it holds whp (as N ) that `min (I) = maxm[M ]  (m).
Theorem 2. Let I be a random N by M JSP instance, and let e be a randomly selected
element of E(I). Then for fixed M , it holds whp (as N ) that e 
/ 1 backbone(I).
Proof. Let e = {Ji , Jj0 }. Remove both J and J 0 from I to create an N  2 by M instance
 which comes from the same distribution as a random N  2 by M JSP instance. Lemma
I,
2 shows that whp there exists an optimal schedule S for I with the property described in
the statement of the lemma.
 : m(o) = m}) denote the workload of machine m in the
Let  (m)   ({o  ops(I)

instance I. By the Central Limit Theorem, each
  (m) is asymptotically normally distributed
with mean (N 2) and standard deviation  N  2. It follows that whp, | (m) (m0 )| >
1
N 4 m 6= m0 .
Thus whp there will be only one machine still processing operations during the interval
1
[`(S)  N 4 , `(S)]. Because max( (J),  (J 0 ))  M max = O(1), we can use this interval
to construct optimal schedules containing the disjunctive arc (Ji , Jj0 ) as well as optimal
schedules containing the disjunctive arc (Jj0 , Ji ).
261

fiStreeter & Smith

5. Clustering as a Function of Makespan
In this section we estimate the expected distance between random schedules whose makespan
is within a factor  of optimal, as a function of  for various combinations of N and M . We
N
then examine how the shape of this curve changes as a function of M
. More formally, if
 I is a random N by M JSP instance with optimal makespan `min (I),
  opt(I)  {S : `(S)    `min (I)}, and
 S1 and S2 are drawn independently at random from  opt(I),
we wish to compute E[kS1  S2 k].
Note that the experiments of 4 provide an upper bound on this quantity:
 
N
 E [| backbone|]
E [kS1  S2 k]  M
2
but provide no lower bound (a low backbone size is not evidence that the mean distance
between global optima is large). The experiments in this section can be viewed as a test of
the degree to which the upper bound provided by 4 is tight.
5.1 Methodology
We generate random samples from  opt(I) by running the simulated annealing algorithm
of van Laarhoven et al. (1992) until it finds such a schedule. More precisely, our procedure
for sampling distances is as follows.
1. Generate a random N by M JSP instance I.
2. Using the branch and bound algorithm of Brucker et al. (1994), determine the optimal
makespan of I.
3. Perform k runs, R1 , R2 , . . . , Rk , of the van Laarhoven et al. (1992) simulated annealing
algorithm. Restart each run as many times as necessary for it to find a schedule whose
makespan is optimal.
4. For each   {1, 1.01, 1.02, . . . , 1.5}, find the first schedule, call it Si (), in each run
Ri whose makespan is within a factor  of optimal. For each of the k2 pairs of
runs (Ri , Rj ), add the distance between Si () and Sj () to the sample of distances
associated with .
We ran this procedure on random JSP instances for the same 7 combinations of N and
M that were used in 4.2. For the smallest instance sizes for each ratio (i.e., 6x6, 8x4 and
9x3 instances) we generate 100 random JSP instances and run the procedure with k = 100.
Setting k = 100 allows us to measure the variation in instance-specific expected values. For
the other 4 combinations of N and M , performing 10,000 simulated annealing runs is too
computationally expensive, so we instead generate 1000 random JSP instances and run the
procedure with k = 2.
262

fiThe Landscape of Random Job Shop Scheduling Instances

Figure 5 (A), (B), and (C) plot the expected distance between random -optimal schedN
ules as a function of  for each of the three values of M
. Figure 5 (D) shows the 0.75
and 0.25 quantiles of the 100 instance-specific sample means for each of the three smallest
instance sizes. Examining Figure 5 (D), we see that the variation among random instances
with the same N and M is small relative to the differences between the curves for different
N
values of M
.
5.2 Discussion
By examining Figure 5 we see that for any , the expected distance between random N
optimal schedules increases as M
increases. Indeed, global optima are dispersed widely
N
N
throughout the search space for M
= 3, and this is true to a lesser extent for M
= 2.
An immediate implication of Figure 5 is that whether or not they exhibit the two
correlations that are the operational definition of a big valley, typical landscapes for JSP
N
= 3 cannot be expected to be big valleys in the sense of having a central
instances with M
cluster of optimal or near-optimal solutions. If anything, one might posit the existence of
multiple big valleys, each leading to a separate global optimum. The next section expands
upon these observations.

6. The Big Valley
In this section we define some formal properties of a big valley landscape, conduct experiments to determine the extent to which random JSP instances exhibit these properties as
N
N
N
we vary M
, and present analytical results for the limiting cases M
 0 and M
 .
Considering again the intuitive picture given in Figure 1, we take the following to be
necessary (though perhaps not sufficient) conditions for a function f (x) to be a big valley.
1. Small improving moves. If x is not a global minimum of f , there must exist a nearby
x0 with f (x0 ) < f (x).
2. Clustering of global optima. The maximum distance between any two global minima
of f is small.
Note that there is no direct relationship between these two properties and the cost-distance
correlations considered by Boese et al. (1994).
6.1 Formalization
The following four definitions allow us to formalize the notion of a big valley landscape.
Definition (Neighborhood Nr ). Let I be an arbitrary JSP instance, and let U be the set
of all schedules for I. Let r be a positive integer. The neighborhood Nr : U  2U is defined
by
Nr (S)  {S 0  U : kS  S 0 k  r} .
Definition (local optimum L(S, N )). Let I and U be as above; let N : U  2U be
an arbitrary neighborhood function; and let S be a schedule for I. L(S, N ) is the schedule
returned by the following procedure (which finds a local optimum by performing next-descent
starting from S using the neighborhood N ).
263

fiStreeter & Smith

(A) Job:machine ratio 1:1

(B) Job:machine ratio 2:1
0.5

E[dist. between schedules]

E[dist. between schedules]

0.5
0.4
0.3
0.2

6x6 instances
7x7 instances

0.1

8x8 instances
0

0.4
0.3
0.2

8x4 instances

0.1

10x5 instances

0
1

1.1

1.2

1.3

1.4

1.5

1

1.1

1.2



(C) Job:machine ratio 3:1

1.4

1.5

(D) Comparison

0.5

0.5

E[dist. between schedules]

E[dist. between schedules]

1.3



0.4
0.3
0.2

9x3 instances

0.1

12x4 instances

0.4
0.3
0.2

6x6 instances
8x4 instances

0.1

9x3 instances
0

0
1

1.1

1.2

1.3

1.4

1.5

1

1.1

1.2

1.3

1.4

1.5





Figure 5: Expected distance between random schedules within a factor  of optimal, as a
function of . Graphs (A), (B), and (C) depict curves for random instances with
N
M = 1, 2, and 3, respectively. Graph (D) compares the curves depicted in (A),
(B), and (C) (only the curves for the smallest instance sizes are shown in (D)). In
(D), top and bottom error bars represent 0.75 and 0.25 quantiles (respectively)
of instance-specific sample means.

264

fiThe Landscape of Random Job Shop Scheduling Instances

(A) An (r,)-valley

(B) Three (r,)-valleys

r

r

r

r









Figure 6: Two landscapes comprised of (r, )-valleys. (A) is a single (r, ) valley (for the
values of r and  shown in the figure), while (B) can either be viewed as three
distinct (r, ) valleys or as a single (r,  0 )-valley. (The values of r shown in the
figure are slightly larger than necessary.)

1. Let N (S) = {S1 , S2 , . . . , S|N (S)| } (where the elements of N (S) are indexed in a fixed
but arbitrary manner).
2. Find the least i such that `(Si ) < `(S). If no such i exists, return S; otherwise set
S  Si and go to 1.
Definition ((r, )-valley). Let I and U be as above, and let r and  be non-negative
integers. A set V  U is an (r, )-valley if V has the following two properties.
1. For any S  V , the schedule L(S, Nr ) is in V and is globally optimal.
2. For any two globally optimal schedules S1 and S2 that are both in V , kS1  S2 k  .
Figure 6 illustrates the definition of an (r, )-valley. We would say that the landscape
depicted in Figure 6 (A) is a big valley, while that depicted in 6 (B) is comprised of three
big valleys.
Definition ((r, , p) landscape). Let I and U be as above, and let S be a random schedule
for I. Then I has an (r, , p) landscape if there exists a V  U such that
1. V is an (r, )-valley, and
2. P[S  V ]  p.



Any JSP instance trivially has an (M N2 , M N2 , 1) landscape (because if r = M N2

then Nr includes all possible schedules). If a JSP instance I has an (r, M N2 , 1) landscape,
then a globally optimal schedule for I can always be found by starting at a random schedule
and applying next-descent using the neighborhood Nr .
We say that a JSP instance I has a big valley landscape if I has an (r, , p) landscape for
small r and  in combination with p near 1. In contrast, if we have small r in combination
with p near 1 but require large , we say that the landscape consists of multiple big valleys.
265

fiStreeter & Smith

6.2 Neighborhood Exactness
In this section we seek to determine the extent to which random JSP instances have the
small improving moves property. We require the following definition.
Definition (neighborhood exactness). Let I, U , and N be as above, and let S be a
random schedule for I. The exactness of the neighborhood N on the instance I is the
probability that L(S, N ) is a global optimum.

If the exactness of Nr is p, then I has an (r, M N2 , p) landscape (let V consist of all
schedules S such that L(S, N ) is a global optimum). We will estimate the expected exactness
of Nr as a function of r for various combinations of N and M . By examining the resulting
curves, we will be able to draw conclusions about the extent to which the landscapes of a
random N by M JSP instance typically has the small improving moves property. We can
N
then determine how the presence or absence of this property depends on M
.

For fixed N and M , we compute the expected exactness of Nr for 1  r  M N2 by
repeatedly executing the following procedure.
1. Generate a random N by M JSP instance I.
2. Using the algorithm of Brucker et al. (1994), compute the optimal makespan of I.
3. Repeat k times:
(a) S  a random feasible schedule, r  1, opt  f alse.
(b) While opt = f alse do:
 S  L(S, Nr ).
 If S is a global optimum, opt  true.
 Record the pair (r, opt).
 r  r + 1.
(c) For all r0 such that r  r0  M

N
2



record the pair (r0 , true).

The pairs recorded by the procedure (in step 3(c) and the third bullet point of 3 (b))
are used in the obvious way to estimate expected exactness. Specifically, for each r the
estimated expected exactness of Nr is the fraction of pairs (r, x) for which x = true.
The implementation of the first bullet point in step 3 (b) deserves further discussion. To
determine L(S, Nr ), each step of next-descent must be able to determine the best schedule
in {S 0 : kS S 0 k  r}. For large r it is impractical to do this by brute force. Instead we have
developed a radius-limited branch and bound algorithm that, given an arbitrary center
schedule Sc and radius r, finds the schedule arg min{S 0 :kSc S 0 kr} `(S 0 ). Our radius-limited
branch and bound algorithm uses the branching rule of Balas (1969) combined with the
lower bounds and branch ordering heuristic of Brucker et al. (1994).
266

fiThe Landscape of Random Job Shop Scheduling Instances

6.3 Results
N
We use three combinations of N and M with M
= 15 (3x15, 4x20, and 5x25 instances), three
N
N
combinations with M = 1 (6x6, 7x7, and 8x8 instances) and two combinations with M
=5
(15x3 and 20x4 instances). For the smallest instance sizes for each ratio (i.e., 3x15, 6x6,
and 15x3 instances) we generate 100 random JSP instances and run the above procedure
with k = 100. Otherwise, we generate 1000 random JSP instances and run the procedure
with k = 1.

Figure 7 (A), (B), and (C) plot expected exactness as a function of neighborhood radius
N
(normalized by the number of disjunctive edges) for each of these three values of M
. Figure
7 (D) shows the 0.75 and 0.25 quantiles of the 100 instance-specific sample means for each
of the three smallest instance sizes.
6.4 Discussion
Examining Figure 7, we see that for any normalized neighborhood radius, the neighborhood
N
exactness is lowest for instances with M
= 1 and higher for the two more extreme ratios
1
N
N
( M = 5 and M = 5). If we view neighborhood exactness as measuring the smoothness
of a landscape, the data suggest that typical JSP landscapes are least smooth at some
N
N
N
intermediate value of M
, but become more smooth as M
 0 or M
 . This in
itself suggests an easy-hard-easy pattern of typical-case instance difficulty in the JSP, a
phenomenon explored more fully in the next section.
Using the methodology of 4-5, we found that the expected proportions of backbone
edges for 3x15, 4x20, and 5x25 instances are 0.94, 0.93, and 0.92, respectively, while the
expected distance between global optima was 0.02 in all three cases. In contrast, the
expected proportions of backbone edges for 15x3 and 20x4 instances are near-zero, while
the expected distances between global optima are 0.33 and 0.28, respectively. We conclude
that landcapes of random N by M JSP instances typically have the clustering of global
N
N
optima property for M
= 51 but not for M
= 5. However, Figure 7 suggests that the small
N
N
improving moves property is present for both M
= 51 and M
= 5. Accordingly, we would
N
1
N
say that typical landscapes for M = 5 are big valleys, while for M
= 5 the landscape is
comprised of many big valleys rather than just one.
N
The data from 4-5 show that for M
= 1, typical landscapes have the clustering of
global optima property. Examining Figure 7 (B), we see that we are able to descend from
a random schedule to a globally optimal schedule with probability 12 when the (normalized)
neighborhood radius is about 6%. For this reason, we think of the landscapes of random
N
JSP instances with M
= 1 as having the small improving moves property to some extent.
This, in combination with the curve in Figure 5 (A) (which shows expected distance between
random -optimal schedules as a function of ) leads us to say that typical landscapes of
N
random JSP instances with M
= 1 can still be roughly described as big valleys. However,
the valley is much rougher (meaning that larger steps are required to move from a random
schedule to a global optimum via a sequence of improving moves) than for the more extreme
N
values of M
.

Table 1 summarizes the empirical findings just discussed.
267

fiStreeter & Smith

(A) Job:machine ratio 1:5

(B) Job:machine ratio 1:1

1

1
0.8

3x15 instances
0.6

E[exactness]

E[exactness]

0.8

4x20 instances

0.4

5x25 instances

6x6 instances
0.6

7x7 instances
8x8 instances

0.4
0.2

0.2

0

0
0

0.1

0.2

0

0.3

0.2

0.3

Normalized radius

Normalized radius

(C) Job:machine ratio 5:1

(D) Comparison

1

1
0.8

E[exactness]

0.8

E[exactness]

0.1

15x3 instances
0.6

20x4 instances
0.4

3x15 instances

0.6

6x6 instances
0.4

15x3 instances

0.2

0.2

0

0
0

0.1

0.2

0.3

0

0.1

0.2

0.3

Normalized radius

Normalized radius

Figure 7: Expected exactness of Nr as a function of the (normalized) neighborhood radius
N
r. Graphs (A), (B), and (C) depict curves for random instances with M
= 15 ,
1, and 5, respectively. Graph (D) compares the curves depicted in (A), (B), and
(C) (only the curves for the largest instances are shown in (D)). In (D), top and
bottom error bars represent 0.75 and 0.25 quantiles (respectively) of instancespecific exactness.

268

fiThe Landscape of Random Job Shop Scheduling Instances

N
M
1
5

N
Table 1. Landscape attributes for three values of M
.
Clustering
of Small
improving
Description
global optima? moves?
Yes
Yes
Big valley

1

Yes

Somewhat

(Rough) big valley

5

No

Yes

Multiple big valleys

6.5 Analysis
We first establish the behavior of the curves depicted in Figure 7 in the limiting cases
N
N
M  0 and M  . We then use these results to characterize the landscapes of random
JSP instances using the (r, , p) notation introduced in 6.1.
N
N
The following two lemmas show that as M
 0 (resp. M
 ), a random schedule
will almost surely be close to an optimal schedule. The proofs are given in Appendix A.
Lemma 3. Let I be a random N by M JSP instance, and let S be a random schedule for
I. Let S be an optimal schedule for I such that kS  Sk is minimal. Let f (M ) be any
unbounded, increasing function of M . Then for fixed N , it holds whp (as M  ) that
kS  Sk < f (M ).
Lemma 4. Let I be a random N by M JSP instance, let S be a random schedule for I,
and let S be an optimal schedule for I such that kS  Sk is minimal. Then for fixed M and
 > 0, it holds whp (as N  ) that kS  Sk < N 1+ .
The following are immediate corollaries of Lemmas 3 and 4.
Corollary 3. For fixed N , the expected exactness of Nf (M ) approaches 1 as M  , where
f (M ) is any unbounded, increasing function of M .
Corollary 4. For fixed M and  > 0, the expected exactness of NN 1+ approaches 1 as
N  .

Because the total number of disjunctive edges is M N2 , these two corollaries imply that
N
N
 0 (resp. M
 ), the curve depicted in Figure 7 approaches a horizontal line at
as M
a height of 1.
Using Lemmas 3 and 4, Theorems 3 and 4 characterize the landscape of random JSP
instances using the (r, , p) notation of 6.1. Before presenting these theorems, a slight
disclaimer is in order. Lemmas 3 and 4 (the proofs of which are fairly involved) indicate
N
N
that in the extreme cases M
 0 and M
  we can jump from a random schedule to a
globally optimal schedule via a single small move. We strongly believe that in these cases
it is also possible to go from a random schedule to a global optimum by a sequence of many
(smaller) improving moves, although proving this seems difficult. Nevertheless, it should
be understood that our theoretical results do not strictly imply the existence of landscapes
like those depicted in Figure 6 (where for most starting points there is a sequence of two or
more small improving moves leading to a global optimum).
N
Theorem 3 shows that as M
 0, a random JSP instance almost certainly has an

(r, , p) landscape where r grows arbitrarily slowly as a function of M ,  is o(M N2 ), and
269

fiStreeter & Smith

N
p is arbitrarily close to 1. In other words, as M
 0 the landscape has both the small
improving move(s) property and the clustering of global optima property. In contrast,
N
Theorem 4 shows that as M
 , a random JSP instance almost surely does not have an
(r, , p) landscape unless  is (N 2 ). Instead, the landscape contains (N !) (r, 1)-valleys,
N
where r is o(M N2 ). Thus, as M
 , the landscape has the small improving move(s)
property but not the clustering of global optima property. These analytical results confirm
the trend suggested by Figure 7 and discussed in 6.4.

Theorem 3. Let I be a random N by M JSP instance. Let f (M ) be any unbounded,
increasing function of M . For fixed N and  > 0, it holds whp (as M  ) that I has a
(r, , p) landscape for r = f (M ),  = M N2 and p = 1  .
Proof. Let V be the set of all schedules S such that L(S, Nr ) is a global optimum. It follows
by Corollary 3 that whp, the exactness of I on r is at least p, which means S  V with
probability at least p. It remains to show that V is an (r, )-valley whp. Part 1 of the
definition of an (r, )-valley is satisfied by the definition of V . Part 2 follows from Theorem
1.
Theorem 4. Let I be a random N by M JSP instance, and let S be a random schedule for
I. There exists a set V (I) = ni=1 Vi of schedules for I such that for fixed M and  > 0, V
has the following properties whp:
1. S  V ;
2. Vi is an (r, )-valley with r = N 1+ and  = 1 i  [n];
3. n > N !(1  ); and
4. max{S1 ,S2 }V kS1  S2 k > (N 2 ).
Proof. Let {S1 , S2 , . . . , Sn } be the set of globally optimal schedules for I, and define Vi 
{S : L(S, N 1+ ) = Si }. Property 1 holds whp by Lemma 4. Property 2 holds by definition
of Vi .
The fact that property 3 holds whp is a consequence of Lemma 2. Recall that Lemma
N
2 showed that as M
 , the priority rule  generates an optimal schedule whp, where
k
 (I, Ji ) = iN + k. Because the indices assigned to the jobs are arbitrary, Lemma 2 also
applies to the priority rule   (I, Jik ) = iN + (k), where  is any permutation of [N ]. There
are N ! possible choices of . Let f be the number of choices that fail to yield a globally
optimal schedule. Property 3 can only fail to hold if f  N !. But by Lemma 1, E[f ] is
o(1)N !; hence f < N ! whp by Markovs inequality.
To establish property 4, choose permutations 1 and 2 that list the elements of [N ]
in reverse order (i.e., 1 (i) = 2 (N  i) i  [N ]). By Lemma 2, the schedules S1 =
S( 1 , I) and S2 = S( 2 , I) are both globally optimal whp. But for any disjunctive edge
e = {J1 , J10 } we must have ~e(S1 ) 6= ~e(S2 ), hence kS1  S2 k  |{{J, J 0 }  I : m(J1 ) =
1 
1 
m(J10 )}|  N M2
= (N 2 ), where we obtain the expression N M2
using the pigeonhole
principle.
270

fiThe Landscape of Random Job Shop Scheduling Instances

7. Quality of Random Schedules
7.1 Methodology
In this section we examine how the quality of randomly generated schedules changes as a
function of the job:machine ratio. Specifically, for various combinations of N and M , we
estimate the expected value of the following four quantities:
(A) the makespan of a random schedule,
(B) the makespan of a locally optimal schedule obtained by starting at a random schedule
and applying next-descent using the N1 move operator,
(C) the makespan of an optimal schedule, and
(D) the lower bound on the makespan of an optimal schedule given by the maximum of
the maximum job duration and the maximum machine workload:


X
max max  (J), max
 (o) .
JI

m[M ]

oops(I):m(o)=m

N
considered in our experiments are those in the set R = { 17 , 16 , 51 , 14 ,
The values of M
1 1 2
3
3 , 2 ,S3 , 1, 2 , 2, 3, 4, 5, 6, 7 }. We consider all combinations of N and M in the set
N
S  rR Sr , where Sr  {(N, M ) : M
= r, min(N, M )  2, max(N, M )  6, N M < 1000}.
For each (N, M )  S, we estimate the expected value of (A) (resp. (B)) by generating 100
random N by M JSP instances and, for each instance, generating 100 random schedules
(resp. local optima). We estimate (D) by generating 1000 random JSP instances for each
(N, M )  S. For some combinations (N, M )  Ssmall  S, it was also practical to compute
N
quantity (C). Let nr = |Ssmall  Sr | be the number of combinations (N, M ) with M
= r for
3
which we computed (C). We chose Ssmall so that nr  4 for r 6= 2 while n 3 = 3. For each
2
(N, M )  Ssmall , we estimate (C) using 1000 random JSP instances.

7.2 Results
Figure 8 plots the mean values of (A), (B), and (C), respectively, against the mean value of
(D), for various combinations of N and M . The data points for each combination of N and
N
M are assigned a symbol based on the value of M
. Top and bottom error bars represent 0.75
and 0.25 quantiles (respectively) of instance-specific sample means. Note that the width of
these error bars is small relative to the differences between the curves for different values
N
of M
.
N
Examining Figure 8, we see that the set of data points for each value of M
are approximately (though not exactly) collinear. Furthermore, in all three graphs the slope of the line
N
formed by the data points with M
= r is maximized when r = 1, and decreases as r gets
further away from 1 (see also Figure 9 (A)).
To further investigate this trend, we performed least squares linear regression on the set
N
of data points for each value of M
. The slopes of the resulting lines are shown as a function
N
of M in Figure 9 (A).
From examination of Figure 9 (A), it is apparent that
271

fiStreeter & Smith

(A) Random schedules
Mean makespan

7000
6000
5000
4000

Ratio 1:5

3000

Ratio 1:3
Ratio 1:1

2000

Ratio 3:1

1000

Ratio 5:1

0
0

1000 2000 3000 4000 5000 6000 7000

Mean lower bound

(B) Random local optima
Mean makespan

7000
6000
5000
4000

Ratio 1:5

3000

Ratio 1:3
Ratio 1:1

2000

Ratio 3:1

1000

Ratio 5:1

0
0

1000 2000 3000 4000 5000 6000 7000

Mean lower bound

(C) Optimal schedules
Mean makespan

7000
6000
5000
4000

Ratio 1:5

3000

Ratio 1:3
Ratio 1:1

2000

Ratio 3:1

1000

Ratio 5:1

0
0

1000 2000 3000 4000 5000 6000 7000

Mean lower bound

Figure 8: Expected makespan of (A) random schedules, (B) random local optima, and (C)
optimal schedules vs. expected lower bound, for various combinations of N and
N
M (grouped by symbol according to M
). Top and bottom error bars represent
0.75 and 0.25 quantiles (respectively) of instance-specific sample means.
272

fiThe Landscape of Random Job Shop Scheduling Instances

(A) Results of least squares regression

Slope of E[makespan]
vs. E[lower bound]

4

Random schedules
Random local optima
Optimal schedules

3

2

1
0.1

1

10

Job:machine ratio

(B) Branch and bound search cost
2:1

Num. tree nodes

10000

3:2

1:1
2:3

1000
1:2

100

1:3

3:1
4:1
5:1

10

1:4

1:5
1:6

6:1

1:7
7:1

1
0

500

1000

1500

log(search space size)

1:7
1:6
1:5
1:4
1:3
1:2
2:3
1:1
3:2
2:1
3:1
4:1
5:1
6:1
7:1

2000

Figure 9: (A) graphs the slope of the least squares fits to the data in Figure 8 (A), (B),
N
N
and (C) as a function of M
(includes values of M
not depicted in Figure 8). (B)
th
graphs the number of search tree nodes (90 percentile) used by the branch and
bound algorithm of Brucker et al. (1994) to find an optimal schedule.

273

fiStreeter & Smith

N
 as the value of M
becomes more extreme (i.e., approaches either 0 or ), the expected makespan of random schedules (resp. random local optima) comes closer to
the expected value of the lower bound on makespan; and

 the difference between the expected makespan of random schedules (resp. random
local optima) and the expected value of the lower bound on makespan is maximized
N
at a value of M
 1.
N
The first of these two observations suggests that as M
approaches either 0 or , a
random schedule is almost certainly near-optimal. 7.3 contains two theorems that confirm
this.
The second of these two observations suggests that the expected difference between the
makespan of a random schedule and the makespan of an optimal schedule is maximized at a
N
value of M
somewhere in the neighborhood of 1. This observation is particularly interesting
N
in light of the empirical fact that square instances of the JSP (i.e., those with M
= 1) are
harder to solve than rectangular ones (Fisher & Thompson, 1963).
Figure 9 (B) graphs the number of search tree nodes (90th percentile) required by the
branch and bound algorithm of Brucker et al. (1994) to optimally solve random N by M
instances, as a function of the log (base 10) of search space size. We take the size of the
search space for an N by M JSP instance to be the number of possible disjunctive graphs,
M
namely 2N ( 2 ) . Note that some of these disjunctive graphs contain cycles and therefore do
not correspond to feasible schedules, so this expression overestimates the size of the search
space. Data points are given for each combination of N and M for which we could afford to
run branch and bound (i.e., each combination of N and M for which we computed quantity
N
(C)). The data points are grouped into curves according to M
.
Examining Figure 9 (B), we see that the curves are steepest for the ratios 23 , 1, 32 , 2,
N
and 3, and that the curves are substantially less steep for extreme values of M
such as 17
and 7. Thus, at least from the point of view of this particular branch and bound algorithm,
random JSP instances exhibit an easy-hard-easy pattern of instance difficulty. We discuss
this pattern further in 7.4.

7.3 Analysis
The following two theorems show that, as
will almost surely be near-optimal.

N
M

approaches either 0 or , a random schedule

Theorem 5. Let I be a random N by M JSP instance with optimal makespan `min (I) and
let S be a random schedule for I. Then for fixed N and  > 0, it holds whp (as M  )
that `(S)  (1 + )`min (I).
Proof. The priority rule rand associates a priority with each operation o  ops(I). Let
the sequence T contain the elements of ops(I), sorted in ascending order of priority. The
schedule S = S(rand , I) depends only on T , and there are N M ! possible choices of T . Thus
rand can be seen as choosing at random from a set of N M ! instance-independent priority
rules. Because each of these instance-independent priority rules is subject to Lemma 1, rand
is
to Lemma 1 and thus for each J, E[SJ ] is O(N ). Thus E[`(S)  `min (I)] 
Palso subject
S
2
J E[J ] = O(N ), so `(S)  `min (I) does not exceed `min (I) = (M ) whp by Markovs
inequality.
274

fiThe Landscape of Random Job Shop Scheduling Instances

Theorem 6. Let I be a random N by M JSP instance with optimal makespan `min (I) and
let S be a random schedule for I. Then for fixed M and  > 0, it holds whp (as N  )
that `(S)  (1 + )`min (I).
Proof. See Appendix A.
The idea behind the proof of Theorem 6 is the following. As shown in Lemma 2, the
priority rule  almost surely generates an optimal schedule. The relevant property of 
was that, when the operations were sorted in order of ascending priority, the number of
operations in between J (o) and o was (N ). The key to the proof of Theorem 6 is that in
expectation, rand shares this property for most of the operations o  ops(I).
7.4 Easy-hard-easy Pattern of Instance Difficulty
N
N
The proofs of Corollary 1 (resp. Lemma 2) show that as M
 0 (resp. M
 ) there exist
simple priority rules that almost surely produce an optimal schedule. Moreover, Theorems
5 and 6 show that in these two limiting cases, even a random schedule will almost surely
N
N
have makespan that is very close to optimal. Thus, both as M
 0 and as M
 , almost
all JSP instances are easy.
N
In contrast, for M
 1, Figure 9 (A) suggests that random schedules (as well as random
local optima) are far from optimal. The literature on the JSP (as well the results depicted
N
in Figure 9 (B)) attests to the fact that random JSP instances with M
 1 are hard.
Thus we conjecture that, as in 3-SAT, typical instance difficulty in the JSP follows an easyhard-easy pattern as a function of a certain parameter. In contrast to 3-SAT, the easyhard-easy pattern in the JSP is not (to our knowledge) associated with a phase transition
N
(i.e., we have not identified a quantity that undergoes a sharp threshold at M
 1).
Furthermore, although the empirical results in Figures 9 (A) and (B) support the idea
that typical-case instance difficulty in the JSP follows as easy-hard-easy pattern, we
N
do not claim to have isolated any particular value of M
as being the point of maximum
difficulty. As shown in Figure 9 (B), random JSP N by M JSP instances are most difficult
N
for the branch and bound algorithm of Brucker et. al (1994) when M
 2, but this may not
be true of other branch and bound algorithms or of JSP heuristics based on local search.
We leave the task of characterizing the easy-hard-easy pattern more precisely as future
work.
In related work, Beck (1997) studied a constraint-satisfaction (as opposed to makespanminimization) version of the JSP, and gave empirical evidence that the probability that a
random JSP instance is satisfiable undergoes a sharp threshold as a function of a quantity
called the constrainedness of the instance.

8. Limitations and Extensions
The primary limitation of the work reported in this paper is that both our theoretical and
empirical results apply only to random instances of the job shop scheduling problem. There
is no guarantee that our observations will generalize to instances drawn from distributions
with more interesting structure (Watson et al., 2002). The difficulty in extending our
analysis to other distributions is that analytical results similar to the ones presented in
275

fiStreeter & Smith

this paper may become much more difficult to derive. However, there are at least three
distributions that have been studied in the scheduling literature for which we believe it
should be not too difficult to adapt our proofs (the conclusions may change as part of the
adaptation process).
 Random workflow JSP instances. In a workflow JSP instance, the set of machines
is partitioned into sets (say M1 , M2 , . . . , Mk ). For i < j, each job must use all the
machines in Mi before using any machines in Mj . Mattfeld et al. (1999) define
a random distribution over workflow JSPs which generalizes in a natural way the
distribution defined in 3.3 (the difference is that the permutations 1 , 2 , . . . , N are
chosen uniformly at random from the set of permutations that satisfy the workflow
constraints).
 Random instances of the (permutation) flow shop scheduling problem. An instance of
the flow shop scheduling problem (FSP) is a JSP instance in which all jobs use the
machines in the same order (equivalently, a FSP instance is a workflow JSP instance
with k = M ). The permutation flow shop problem (PFSP) is a special case of the FSP
in which, additionally, each machine must process the jobs in the same order. There
is a large literature on the (P)FSP; Framinan et al. (2004) and Hejazi and Saghafian
(2005) provide relevant surveys.
 Job-correlated and machine-correlated JSP instances. In a job-correlated JSP instance,
the distribution from which operation durations are drawn depends on the job to which
an operation belongs. Similarly, in machine-correlated JSP instance the distribution
depends on the machine on which the operation is performed. Watson et al. (2002)
have studied job-correlated and machine-correlated instances of the PFSP.
Regarding the difficulty of instances drawn from these three distributions, computational
experience shows that (i) random workflow JSPs are harder than random JSPs; (ii) random PFSPs are easier than random JSPs; and (iii) job-correlated and machine-correlated
PFSPs are easier than random PFSPs. Extending our theoretical analysis to each of these
distributions may give some insight into the relevant differences between them.
8.1 The Big Valley vs. Cost-Distance Correlations
In 6, we defined a big valley landscape as one that exhibits two properties: small improving moves and clustering of global optima. Our analytical and experimental results
were based on this definition. Although we believe this definition captures properties of JSP
landscapes that are important for designers of heuristics to understand, other properties
(e.g., cost-distance correlations) are likely to be important as well. In particular, it may be
possible for algorithms to exploit cost-distance correlations on landscapes that have neither
the small improving moves nor the clustering of global optima properties.
In the existing literature, the term big valley is used amorphously to mean either
(1) a landscape like that depicted in Figure 1 or (2) a landscape that exhibits high costdistance correlations. By making a sharper distinction between these two distinct concepts,
we can only improve our understanding of JSP landscapes as well as the landscapes of other
combinatorial problems.
276

fiThe Landscape of Random Job Shop Scheduling Instances

9. Conclusions
9.1 Summary of Experimental Results
N
Empirically, we demonstrated that for low values of the job to machine ratio ( M
), lowmakespan schedules are clustered in a small region of the search space and the backbone
N
size is high. As M
increases, low-makespan schedules become dispersed throughout the
N
search space and the backbone vanishes. As a function of M
, the smoothness of the
landscape (as measured by a statistic called neighborhood exactness) starts out small for
N
N
N
low values of M
(e.g., M
= 15 ), is relatively high for M
 1, and becomes small again for
N
N
N
high values of M (e.g., M = 5). For both extremely low and extremely high values of M
,
the expected makespan of random schedules comes very close to that of optimal schedules.
The quality of random schedules (resp. random local optima) appears to be the worst at a
N
value of M
 1.
6.4 discussed the implications of our results for the big valley picture of JSP search
N
 1, we concluded that a typical landscape can be described as a big
landscapes. For M
N
N
valley, while for larger values of M
(e.g., M
 3) there are many big valleys. 7.4 discussed
how our data support the idea that JSP instance difficulty exhibits an easy-hard-easy
N
.
pattern as a function of M

9.2 Summary of Theoretical Results
Table 2 shows the asymptotic expected values of various attributes of a random N by M
N
N
JSP instance in the limiting cases M
 0 and M
 .
Table 2. Attributes of random JSP instances.
Fixed N , M   Fixed M , N  
Optimum makespan

Max. job length
(Corollary 1)

Max. machine workload
(Corollary 2)

Normalized backbone size

1 (Theorem 1)

0 (Theorem 2)

Normalized maximum distance between global optima
Normalized distance between random
schedule and nearest global optimum
Ratio of makespan of random schedule
to optimum makespan

0 (Theorem 1)

(1) (Theorem 4)

0 (Lemma 3)

0 (Lemma 4)

1 (Theorem 5)

1 (Theorem 6)

9.3 Rules of Thumb for Designing JSP Heuristics
Though we do not claim to have any deep insights into how to solve random instances of
the JSP, our results suggest two general rules of thumb:
N
N
 when M
is low (say, M
 1 or lower), an algorithm should attempt to locate the
cluster of global optima and exploit it; while

277

fiStreeter & Smith

N
N
 when M
is high (say, M
 3) an algorithm should attempt to isolate one or more
clusters of global optima and deal separately with each of them.

We briefly discuss these ideas in relation to two recent algorithms: backbone-guided local
search (Zhang, 2004) and i-TSAB (Nowicki & Smutnicki, 2005).
9.3.1 Backbone-guided local search
Several recent algorithms attempt to use backbone information to bias the move operator employed by local search. For example, Zhang (2004) describes an approach called
backbone-guided local search in which the frequency with which an attribute (e.g., an assignment of a particular value to a particular variable in a Boolean formula) appears in
random local optima is used as a proxy for the frequency with which the attribute appears in global optima. The approach improved the performance of the WalkSAT algorithm
(Selman, Kautz, & Cohen, 1994) on large instances from SATLIB (Hoos & Stutzle, 2000).
A similar algorithm has been successfully applied to the TSP (Zhang & Looks, 2005) to
improve the performance of an iterated Lin-Kernighan algorithm (Martin, Otto, & Felten,
1991). Zhang writes:
This method is built upon the following working hypothesis: On a problem
whose optimal and near optimal solutions form a cluster, if a local search algorithm can reach close vicinities of such solutions, the algorithm is effective
in finding some information of the solution structures, backbone in particular.
(Zhang, 2004, p. 3)
Based on the results of 4-5, this working hypothesis is satisfied for random JSPs with
 1 or lower. It seems plausible that backbone-guided local search could be used to boost
the performance of early local search heuristics for the JSP such as those of van Laarhoven
et al. (1992) and Taillard (1994) (whether the results would be competitive with those of
recent algorithms such as i-TSAB is a separate question).
N
The hypothesis is typically violated for random JSP instances with larger values of M
.
In these cases it makes more sense to attempt to exploit local clustering of optimal and
near-optimal schedules.
N
M

9.3.2 i-TSAB
Nowicki and Smutnicki (2005) present a JSP heuristic called i-TSAB which employs multiple
runs of the tabu search algorithm TSAB (Nowicki & Smutnicki, 1996). i-TSAB employs path
relinking to localize the center of BV [big valley], probably close to the global minimum
(Nowicki & Smutnicki, 2005). In other words, i-TSAB was designed based on the intuitive
picture depicted in Figure 6 (A), which is inaccurate for typical random JSP instances with
N
N
M  3. Note that although random JSP instances become easy as M  , instances
N
with M
 3 are by no means easy, as evidenced by Figure 9 (B).
For concreteness, we briefly describe how i-TSAB works. Initially, i-TSAB performs a
number of independent runs of TSAB and adds each best-of-run schedule to a pool of elite
solutions. It then performs additional runs of TSAB and uses the best-of-run schedules
from these additional runs to replace schedules in the pool of elite solutions. Starting points
278

fiThe Landscape of Random Job Shop Scheduling Instances

for the additional TSAB runs are either (i) random elite solutions or (ii) schedules obtained
by performing path relinking on a random pair of elite solutions. Given two schedules S1
and S2 , path relinking uses a move operator to generate a new schedule that is midway
(in terms of disjunctive graph distance) between S1 and S2 . The pool of elite solutions
can be thought of as a cloud of particles that hovers over the search space and (hopefully)
converges to a region of the space containing a global optimum.
N
For random JSP instances with M
 1, our results are consistent with the idea that
the cloud of elite solutions converges to the center of the big valley. For random JSP
N
instances with M
 3, however, the cloud must either converge to one of many big valleys
or not converge at all. As an alternate approach one can imagine using multiple clouds,
with the intention that each cloud specializes on a particular big valley. It seems plausible
that such ideas could improve the performance of i-TSAB on random JSP instances with
N
larger values of M
.

Appendix A: Additional Proofs
P
For the proofs in this section, we define  (O)  oO  (o), where O is any set of operations.
We make use of the following inequality (Spencer, 2005).
Azumas Perimetric Inequality (A.P.I.). Let X = (X1 , X2 , . . . , Xn ) be a vector of n independent random variables. Let the function f (x) take as input a vector x = (x1 , x2 , . . . , xn ),
where xi is a realization of Xi for i  [n], and produce as output a real number. Suppose
that for some  > 0 it holds that for any two vectors x and x0 that differ on at most one
component,
|f (x)  f (x0 )|   .
Then for any  > 0,



 
2
P X > E[X] +  n  exp  2 .
2

The same inequality holds for P [X  E[X]   n].
Lemma 2. Let I be a random N by M JSP instance. Then for fixed M , it holds whp (as
N  ) that the schedule S = S( , I) has the property that
S(o) = S + (M(o)) o  ops(I) .
Proof. It remains only to prove Claim 2.1 from the proof in 4, which says that whp, for
all o  ops2+ (I) we have
1
S(o)  S(J (o))  E[S(o)  S(J (o))] .
2
Pick some arbitrary operation o  ops2+ (I), and suppose that the random choices used
to construct I were made in the following order:
1. Randomly choose m1 = m(o) and m2 = J (o).
2. For k from 1 to N :
279

fiStreeter & Smith

(a) Randomly choose the order in which job J k uses the machines (if o  J k then
part of this choice has already been made in step 1).
(b) Randomly choose  (Jik ) i  [M ].
Let the random variable Xk denote the sequence of random bits used in steps (a) and
(b) of the k th iteration of the loop. Define o  S(o)  S(J (o)). Then, for any fixed choices
of m1 and m2 , o is a function of the N independent events X1 , X2 , . . . , XN , and it is easy
to check that altering a particular Xi changes the value of o by at most 2max . Thus
h
i


1)
P o < 12 E[o ] = P o < E[o ]  (N
2M
h
i
N
 P o < E[o ]  2M


2
 exp  2(4M N
2
max )
where in the first step we have used the fact (from the proof in 4) that E[o ] = (NM1) and
in the last step we have used A.P.I. Taking a union bound over the N (M  1) operations
in ops2+ (I) proves the claim.

Lemma 3. Let I be a random N by M JSP instance, and let S be a random schedule for
I. Let S be an optimal schedule for I such that kS  Sk is minimal. Let f (M ) be any
unbounded, increasing function of M . Then for fixed N , it holds whp (as M  ) that
kS  Sk < f (M ).
Proof. Let S = S(0 , I). The proof of Corollary 1 showed that for any J, E[SJ ] is O(N 2 ).
Thus it holds whp that SJ < log(f (M )) J. As in the proof of Theorem 5, the procedure
used to produce S is a mixture of instance-independent priority rules, each subject to
Lemma 1. Thus for any J, E[SJ ] is O(N ), so whp SJ < log(f (M )) J.
P
P
Let Onear (Ji ) = {Jj0 : J 0 6= J, | i0 <i  (Ji0 )  j 0 <j  (Jj0 0 )| < log(f (M ))}. (Onear (Ji ) is
the set of operations that would be scheduled near in time to Ji if we ignored the fact
that a machine may only perform one operation at a time.) Let Enear = {e = {Ji , Jj0 } 
E(I) : Jj0  Onear (Ji )}. Under the assumptions of the previous paragraph (each of which
hold whp), kS  Sk  |Enear |. For any Ji , E[|Onear (Ji )|] is O(N log f (M )). Thus


E kS  Sk  E [|Enear |] =

X
oops(I)


1
E [|Onear (o)|] = O N 2 log(f (M ))
M

so kS  Sk < f (M ) whp by Markovs inequality.
For the purpose of the remaining proofs, it is convenient to introduce some additional
notation. Let T = (T1 , T2 , . . . , T|T | ) be a sequence of operations. We define
 T(i1 ,i2 ]  {Ti : i1 < i  i2 }, and
 T(im1 ,i2 ]  {Ti  T(i1 ,i2 ] : m(Ti ) = m} .
280

fiThe Landscape of Random Job Shop Scheduling Instances

Lemma 4. Let I be a random N by M JSP instance, let S be a random schedule for I,
and let S be an optimal schedule for I such that kS  Sk is minimal. Then for fixed M and
 > 0, it holds whp (as N  ) that kS  Sk < N 1+ .
Proof. Let T be the sequence of operations o  ops(I), sorted in ascending order by priority
rand (I, o) (where rand is the random priority rule used to create S). Note that for any
o  ops(I) with J (o) 6= o , J (o) must appear before o in T . Let Ti denote the ith operation
in T .
Consider the schedule S defined by the following procedure:
1. S(o)   o  ops(I).
2. Q  (). Let Qj denote the j th operation in Q.
3. Let the function ready(o) return true if S + (M(o))  S + (J (o)), false otherwise.
4. For i from 1 to N M do:
(a) If ready(Ti ), then set S(o)  S + (M(Ti )). Otherwise append Ti onto Q.
(b) For j from 1 to |Q| do:
i. If ready(Qj ), then set S(Qj )  S + (M(Qj )) and remove Qj from Q.
5. Schedule any remaining operations of Q in a manner to be specified (in the last
paragraph of the proof).
The construction of S is just like the construction of S, except for the manipulations
involving Q. The purpose of Q is to delay the scheduling of any operation o that, if
scheduled immediately, might produce a schedule in which S(o) > S + (M(o)). We first
show that kS  Sk < N 1+ whp; then we show that S is optimal whp.
Let P
Qi denote Q as it exists after i iterations of step 4 have been performed. Let
NM
i
of iterations during which o  Q. We claim that
q(o) =
i=1
P |o  Q | be the number
N
M
|. Letting E 6= = {e  E(I) : ~e(S) 6= ~e(S)}, we have
kS  Sk  oops(I) q(o) + (N  1)|Q
kS  Sk = |{e  E 6= : e  QN M = }| + |{e  E 6= : e  QN M 6= }|
 |{e  E 6= : e  QN M = }| + (N  1)|QN M |
P
so it suffices to show |{e  E 6= : e  QN M = }| 
oops(I) q(o). To see this, let
=
6
N
M
e = {o1 , o2 }  E be such that e  Q
= . We must have q(o1 ) + q(o2 ) > 0. We charge e
to the operation in {o1 , o2 } that was inserted into Q first. It is easy to see that an operation
can be charged for at most one edge perPiteration it spends in Q, establishing our claim.
Thus it suffices to show that kS  Sk  oops(I) q(o) + (N  1)|QN M |  N 1+ whp.
1

0

1

0

We divide the construction of S into n = M N 2  epochs, each consisting of N 2 +
iterations of step 4, for a to-be-specified 0 > 0. Let zj denote the number of iterations of
step 4 that occur before the end of the j th epoch, with zj = 0 for j  0 by convention. Let
m
 Cjm  T(0,z
\ Qzj be the set of operations that have been scheduled to run on m by
j]

the end of the j th epoch; and
281

fiStreeter & Smith

S
 Onear  j[n] {o  T(zj1 ,zj ] : J (o)  T(zj(M +2) ,zj ] } be the set of operations whose
job-predecessor belongs to a nearby epoch.
1

0

For any i  [N M ], P[Ti  Onear ]  (M + 2)N  2 + . Thus for any j  [n], E[|Onear 
0
T(zj1 ,zj ] |]  (M + 2)N 2 . Using A.P.I. it is straightforward to show that whp,
|Onear  T(zj1 ,zj ] |  N

1+0
2

j  [n] .

(9.1)

We claim that whp, the following statements hold j  [n]:
[

Qi  Onear ,

(9.2)

izj

J  Qzj1 6=   |J  Qzj1  Qzj | < |J  Qzj1 |
zj

zjM

Q Q

|Qzj |  M N

J  I ,

(9.3)

=  , and
1+0
2

(9.4)

.

(9.5)

We prove this by induction, where each step of the induction fails with exponentially
small probability. For j = 0, (9.3) and (9.4) hold trivially. (9.2) is true because the
operations in T(0,z1 ] \ Onear are the first operations in their jobs, hence cannot be added to
Q. (9.5) then follows from (9.2) and (9.1).
Consider the case j > 0. To show (9.2), let o be an arbitrary operation in T(zj1 ,zj ] \Onear .
m(J (o))

By the induction hypothesis (specifically, equation (9.4)), J (o)  Cj2




m(J (o))
m(o)
0   Cj2
>  Cj1 . By the induction hypothesis,

. Thus q(o) >









1+0
m(o)
m(J (o))
m(o)
m(J (o))
.
 Cj1   Cj2
  T(0,zj1 ]  M N 2   T(0,zj2 ]
Letting  denote the right hand side of this inequality, we have E[] =
1+0
2

1
+0
1
2
MN



, and A.P.I. can be used to show that for some K > 0 independent of N , P[ <
MN
0
0
0]  exp( K1 N  ). Thus (9.2) holds with probability at least 1  exp( K1 N  ).
To show (9.3), let J be such that J  Qzj1 6= , and let Ji  Qzj1 be
 chosenso that i is
m(J (Ji ))
m(J (Ji ))
m(J )
minimal. Then J (Ji )  Cj1
. Thus Ji  Qzj   Cj1
>  Cj i . By (9.1),
1+0

(9.2), and the induction hypothesis (equation (9.5)), |Qzj |  (M + 1)N 2 . Using the same
0
technique as above, we can show that (9.3) holds with probability at least 1  exp( K1 N  )
for some K > 0 independent of N .
(9.3) implies (9.4). (9.2) and (9.4) together with (9.1) imply (9.5). Thus whp, (9.2)
through (9.5) hold j  [n].
By (9.2) and (9.4), we have


X
1
0
0
E
q(o)  E[|Onear |]M N 2 +  M 2 (M + 2)N 1+2
oops(I)

and also
282

fiThe Landscape of Random Job Shop Scheduling Instances

0

E[|QN M |]  E[|T(znM ,zn ]  Onear |]  (M + 2)N 2
P
so setting 0 = 3 gives kS  Sk  oops(I) q(o) + (N  1)|QN M |  N 1+ whp.
It remains to show that S is optimal whp. We first prove the following claim.
Claim 4.1. For any non-negative integers a and b, the probability that T(a,b] contains
two operations from the same job is at most

(ba)2
N .

Proof of Claim 4.1. Let X denote the number of pairs of operations in T(a,b] that belong to
1
(ba)2
the same job. Then P[X > 0]  E[X]  ba
2 N 
N .
To see that S is optimal whp, note that the operations scheduled prior to step 5 do not
cause any idle time on any machine, so it is only the operations in QN M that can cause S
to be sub-optimal. Let  (m)   ({o  ops(I) : m(o) = m}) denote the workload of machine
m. Let m = arg maxm[M ]  (m). Then the following hold whp.
 The set Z m  T m
last. (It holds

1

consists of operations belonging to jobs that use m

(N M 2M N 4 ,N M ]
whp that Z m  Z,

where Z  T

1

(N M N 3 ,N M ]

. So if Z m contains an

operation from a job that does not use m last, then Z must contain two operations
from the same job. But by Claim 4.1, the probability that this happens is at most
1
(N 3 )2 N1 = o(1).)
1

 N 4   (Z m ) and  (Z m )   (m)   (m) m 6= m. (This follows by applying the
Central Limit Theorem to  (Z m ),  (m), and  (m)).
Thus whp it holds that prior to the execution of step 5, S contains a period of length
1
at least  (Z m )  N 4 during which the only operations being processed are those in Z m ,
0
where {o  ops(I) : J (o)  Z m } = . Assuming |QN M | < N 3 (holds whp), we can always
schedule the operations in QN M so as to guarantee `(S) =  (m), which implies S is optimal.

Theorem 6. Let I be a random N by M JSP instance with optimal makespan `min (I) and
let S be a random schedule for I. Then for fixed M and  > 0, it holds whp (as N  )
that `(S)  (1 + )`min (I).
Proof. As in the proof of Lemma 4, let T be the sequence of operations o  ops(I), sorted
in ascending order by priority rand (I, o) (where rand is the random priority rule used to
create S). Note that for any o  ops(I) with J (o) 6= o , J (o) must appear before o in T .
Let Ti denote the ith operation in T .
Rather than analyze S directly, we analyze a schedule S defined by the following procedure:
1. t  0.
2. For i from 1 to N M do:
283

fiStreeter & Smith

(a) Set S(Ti ) = max(t, S + (J (Ti )), S + (M(Ti ))) .
(b) If S + (J (Ti )) > S + (M(Ti )), set t = maxi0 i S + (Ti0 ).
The procedure is identical to the one used to construct S, except that, whenever an
operation Ti is assigned a start time S(Ti ) > S + (M(Ti )), the procedure inserts artificial
delays into the schedule in order to re-synchronize the machines. For any T , it is clear that
`(S)  `(S). Thus, it suffices to show that `(S)  (1 + )`min (I) whp.
We divide the construction of S into n epochs, where each update to t (in step 2(b)) defines the beginning of a new epoch. Let zi be the number of operations scheduled before the
end of the ith epoch, with z0 = 0 by convention. Let ti = maxi0 zi S + (oi0 ) be the (updated)
P
+
0
value of t at the end ofPthe ith epoch. Define i  M
m=1
Ptni  maxi0 <i,m(Ti0 )=m S (Ti ).
n
Then `(S)  `min (I)  i=1 i , so it suffices to show that i=1 i  `min (I) whp.
P
2
Let I P
= [n], and let L = {i  I : zi  zi1  N 7 }. We first consider iL i ; then we
consider iI\L i .
2

Let i1 and i2 be arbitrary integers with 0  i1 , i2  N M and i2  i1  N 7 . Let
i1
. For any T ,  is a function of the outcome of at most
 =  (T(im1 ,i2 ] ). Then E[ ] =  i2M
i2  i1 events (namely, the definition of each of the jobs in {J : J  T(i1 ,i2 ] 6= }), each of
which alters the value of  by at most max . It follows by A.P.I. that
!
0

N 2
0
P[|  E[ ]| > N
i2  i1 ]  2 exp  2
2max

for any 0 > 0. Thus, it holds whp that |  E[ ]|  N  i2  i1 for all possible choices
0
of i1 and i2 . In particular, whp
we have i  2M N  zi  zi1  i  L, which implies
p
P
5 P
6
2
0
0
7
2M N  N 7 = 2M N 7 + .
iL i  N
iL
P
Now consider
iI\L i . As shown in the proof of Lemma 4 (Claim 4.1), for any
non-negative integers a and b the probability that T(a,b] contains two operations from the
2

2

7
same job is at most (ba)
N . Thus the probability that an arbitrary subsequence of size N
4
 37
contains two operations from the same job is at most N , so E[|I \ L|]  N 7 . Clearly
P
2
6
i  max N 7 i  I \ L, so E[ iI\L i ] is O(N 7 ).
P
P
6
6
0
0
Thus E[ iI i ] is O(N 7 + ) for any 0 > 0, so iI i  N 7 +2 whp, while it is easy
to see that `min (I)   N2 whp.

References
Achlioptas, D., & Peres, Y. (2004). The threshold for random k-SAT is 2k log 2  O(k).
Journal of the AMS, 17, 947973.
Balas, E. (1969). Machine sequencing via disjunctive graphs: An implicit enumeration
algorithm. Operations Research, 17, 110.
Beasley, J. E. (1990). OR-library: Distributing test problems by electronic mail. Journal of
the Operational Research Society, 41(11), 10691072.
284

fiThe Landscape of Random Job Shop Scheduling Instances

Beck, J. C., & Jackson, W. K. (1997). Constrainedness and the phase transition in job
shop scheduling. Tech. rep. CMPT97-21, School of Computing Science, Simon Fraser
University.
Boese, K. D., Kahng, A. B., & Muddu, S. (1994). A new adaptive multi-start technique for
combinatorial global optimizations. Operations Research Letters, 16, 101113.
Brucker, P., Jurisch, B., & Sievers, B. (1992). Job-shop (C-codes). European Journal of Operational Research, 57, 132133. Code available at http://optimierung.
mathematik.uni-kl.de/ORSEP/contents.html.
Brucker, P., Jurisch, B., & Sievers, B. (1994). A branch and bound algorithm for the
job-shop scheduling problem. Discrete Applied Mathematics, 49(1-3), 107127.
Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). Where the really hard problems are.
In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence,
IJCAI-91, Sidney, Australia, pp. 331337.
Fisher, H., & Thompson, G. L. (1963). Probabilistic learning combinations of local job-shop
scheduling rules. In Muth, J. F., & Thompson, G. L. (Eds.), Industrial Scheduling,
pp. 225251. Prentice-Hall, Englewood Cliffs, NJ.
Framinan, J. M., Gupta, J. N. D., & Leisten, R. (2004). A review and classification of
heuristics for permutation flow-shop scheduling with makespan objective. Journal of
the Operational Research Society, 55(12), 12431255.
French, S. (1982). Sequencing and Scheduling: An Introduction to the Mathematics of the
Job-Shop. Wiley, New York.
Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer Academic Publishers, Boston, MA.
Hejazi, S. R., & Saghafian, S. (2005). Flowshop-scheduling problems with makespan criterion: a review. International Journal of Production Research, 43(14), 28952929.
Hoos, H. H., & Stutzle, T. (2000). SATLIB: An online resource for research on SAT. In
Gent, I. P., v. Maaren, H., & Walsh, T. (Eds.), Proceedings of SAT 2000, pp. 283292.
SATLIB is available online at www.satlib.org.
Jain, A., & Meeran, S. (1998). A state-of-the-art review of job-shop scheduling techniques.
Tech. rep., Department of Applied Physics, Electronic and Mechanical Engineering,
University of Dundee, Dundee, Scotland.
Jones, A., & Rabelo, L. C. (1998). Survey of job shop scheduling techniques. Tech. rep.,
National Institute of Standards and Technology, Gaithersburg, MD.
Kim, Y.-H., & Moon, B.-R. (2004). Investigation of the fitness landscapes in graph bipartitioning: An empirical study. Journal of Heuristics, 10, 111133.
Lourenco, H., Martin, O., & Stutzle, T. (2003). Iterated local search. In Glover, F., &
Kochenberger, G. (Eds.), Handbook of Metaheuristics. Kluwer Academic Publishers,
Boston, MA.
Mammen, D. L., & Hogg, T. (1997). A new look at the easy-hard-easy pattern of combinatorial search difficulty. Journal of Artificial Intelligence Research, 7, 4766.
285

fiStreeter & Smith

Martin, O. C., Otto, S. W., & Felten, E. W. (1991). Large-step Markov chains for the
traveling salesman problem. Complex Systems, 5, 299326.
Martin, O. C., Monasson, R., & Zecchina, R. (2001). Statistical mechanics methods and
phase transitions in combinatorial problems. Theoretical Computer Science, 265(1-2),
367.
Mattfeld, D. C. (1996). Evolutionary Search and the Job Shop: Investigations on Genetic
Algorithms for Production Scheduling. Physica-Verlag, Heidelberg.
Mattfeld, D. C., Bierwirth, C., & Kopfer, H. (1999). A search space analysis of the job shop
scheduling problem. Annals of Operations Research, 86, 441453.
Mezard, M., & Parisi, G. (1986). A replica analysis of the traveling salesman problem.
Journal de Physique, 47, 12851296.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computational complexity from characteristic phase transitions. Nature, 400,
133137.
Nowicki, E., & Smutnicki, C. (1996). A fast taboo search algorithm for the job-shop problem.
Management Science, 42(6), 797813.
Nowicki, E., & Smutnicki, C. (2001). Some new ideas in TS for job shop scheduling. Tech.
rep. 50/2001, University of Wroclaw.
Nowicki, E., & Smutnicki, C. (2005). An advanced tabu search algorithm for the job shop
problem. Journal of Scheduling, 8, 145159.
Reeves, C. R., & Yamada, T. (1998). Genetic algorithms, path relinking, and the flowshop
sequencing problem. Evolutionary Computation, 6, 4560.
Roy, B., & Sussmann, B. (1964). Les problemes dordonnancement avec contraintes disjonctives. Note D.S. no. 9 bis, SEMA, Paris, France, Decembre.
Selman, B., Kautz, H., & Cohen, B. (1994). Noise strategies for local search. In Proceedings
of AAAI-94, pp. 337343.
Slaney, J., & Walsh, T. (2001). Backbones in optimization and approximation. In Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI2001), pp. 254259.
Spencer, J. (2005). Modern probabilistic methods in combinatorics. http://www.cs.nyu.
edu/cs/faculty/spencer/papers/stirlingtalk.pdf.
Streeter, M. J., & Smith, S. F. (2005a). Characterizing the distribution of low-makespan
schedules in the job shop scheduling problem. In Biundo, S., Myers, K., & Rajan, K.
(Eds.), Proceedings of ICAPS 2005, pp. 6170.
Streeter, M. J., & Smith, S. F. (2005b). Supplemental material for ICAPS 2005 paper
Characterizing the distribution of low-makespan schedules in the job shop scheduling
problem. http://www.cs.cmu.edu/~matts/icaps_2005.
Taillard, E. (1993). Benchmarks for basic scheduling problems. European Journal of Operational Research, 64, 278285.
286

fiThe Landscape of Random Job Shop Scheduling Instances

Taillard, E. (1994). Parallel taboo search techniques for the job shop scheduling problem.
ORSA Journal on Computing, 6, 108117.
van Laarhoven, P., Aarts, E., & Lenstra, J. (1992). Job shop scheduling by simulated
annealing. Operations Research, 40(1), 113125.
Watson, J.-P., Barbulescu, L., Whitley, L. D., & Howe, A. (2002). Contrasting structured
and random permutation flow-shop scheduling problems: search-space topology and
algorithm performance. INFORMS Journal on Computing, 14(2), 98123.
Watson, J.-P., Beck, J. C., Howe, A. E., & Whitley, L. D. (2001). Toward an understanding
of local search cost in job-shop scheduling. In Cesta, A. (Ed.), Proceedings of the Sixth
European Conference on Planning.
Yokoo, M. (1997). Why adding more constraints makes a problem easier for hill-climbing
algorithms: Analyzing landscapes of CSPs. In Principles and Practice of Constraint
Programming, pp. 356370.
Zhang, W. (2004). Configuartion landscape analysis and backbone guided local search: Part
I: Satisfiability and maximum satisfiability. Artificial Intelligence, 158(1), 126.
Zhang, W., & Looks, M. (2005). A novel local search algorithm for the traveling salesman problem that exploits backbones. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence, pp. 343350.

287

fi