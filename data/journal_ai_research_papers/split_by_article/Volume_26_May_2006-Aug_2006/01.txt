Journal of Artificial Intelligence Research 26 (2006) 35-99

Submitted 8/05; published 5/06

Planning Graph Heuristics for Belief Space Search
Daniel Bryce
Subbarao Kambhampati,

DAN . BRYCE @ ASU . EDU
RAO @ ASU . EDU

Department of Computer Science and Engineering
Ira A. Fulton School of Engineering
Arizona State University, Brickyard Suite 501
699 South Mill Avenue, Tempe, AZ 85281

David E. Smith

DE 2 SMITH @ EMAIL . ARC . NASA . GOV

NASA Ames Research Center
Intelligent Systems Division, MS 269-2
Moffett Field, CA 94035-1000

Abstract
Some recent works in conditional planning have proposed reachability heuristics to improve
planner scalability, but many lack a formal description of the properties of their distance estimates.
To place previous work in context and extend work on heuristics for conditional planning, we
provide a formal basis for distance estimates between belief states. We give a definition for the
distance between belief states that relies on aggregating underlying state distance measures. We
give several techniques to aggregate state distances and their associated properties. Many existing
heuristics exhibit a subset of the properties, but in order to provide a standardized comparison we
present several generalizations of planning graph heuristics that are used in a single planner. We
compliment our belief state distance estimate framework by also investigating efficient planning
graph data structures that incorporate BDDs to compute the most effective heuristics.
We developed two planners to serve as test-beds for our investigation. The first, CAltAlt,
is a conformant regression planner that uses A* search. The second, P ON D, is a conditional
progression planner that uses AO* search. We show the relative effectiveness of our heuristic
techniques within these planners. We also compare the performance of these planners with several
state of the art approaches in conditional planning.

1. Introduction
Ever since CGP (Smith & Weld, 1998) and SGP (Weld, Anderson, & Smith, 1998) a series of planners have been developed for tackling conformant and conditional planning problems  including
GPT (Bonet & Geffner, 2000), C-Plan (Castellini, Giunchiglia, & Tacchella, 2001), PKSPlan (Petrick & Bacchus, 2002), Frag-Plan (Kurien, Nayak, & Smith, 2002), MBP (Bertoli, Cimatti, Roveri,
& Traverso, 2001b), KACMBP (Bertoli & Cimatti, 2002), CFF (Hoffmann & Brafman, 2004), and
YKA (Rintanen, 2003b). Several of these planners are extensions of heuristic state space planners
that search in the space of belief states (where a belief state is a set of possible states). Without
full-observability, agents need belief states to capture state uncertainty arising from starting in an
uncertain state or by executing actions with uncertain effects in a known state. We focus on the
first type of uncertainty, where an agent starts in an uncertain state but has deterministic actions.
We seek strong plans, where the agent will reach the goal with certainty despite its partially known
state. Many of the aforementioned planners find strong plans, and heuristic search planners are
c
2006
AI Access Foundation. All rights reserved.

fiB RYCE , K AMBHAMPATI , & S MITH

currently among the best. Yet a foundation for what constitutes a good distance-based heuristic for
belief space has not been adequately investigated.
Belief Space Heuristics: Intuitively, it can be argued that the heuristic merit of a belief state depends
on at least two factorsthe size of the belief state (i.e., the uncertainty in the current state), and the
distance of the individual states in the belief state from a destination belief state. The question of
course is how to compute these measures and which are most effective. Many approaches estimate
belief state distances in terms of individual state to state distances between states in two belief
states, but either lack effective state to state distances or ways to aggregate the state distances. For
instance the MBP planner (Bertoli et al., 2001b) counts the number of states in the current belief
state. This amounts to assuming each state distance has unit cost, and planning for each state can be
done independently. The GPT planner (Bonet & Geffner, 2000) measures the state to state distances
exactly and takes the maximum distance, assuming the states of the belief state positively interact.
Heuristic Computation Substrates: We characterize several approaches to estimating belief state
distance by describing them in terms of underlying state to state distances. The basis of our investigation is in adapting classical planning reachability heuristics to measure state distances and
developing state distance aggregation techniques to measure interaction between plans for states in
a belief state. We take three fundamental approaches to measure the distance between two belief
states. The first approach does not involve aggregating state distance measures, rather we use a
classical planning graph to compute a representative state distance. The second retains distinctions
between individual states in the belief state by using multiple planning graphs, akin to CGP (Smith
& Weld, 1998), to compute many state distance measures which are then aggregated. The third
employs a new planning graph generalization, called the Labelled Uncertainty Graph (LU G), that
blends the first two to measure a single distance between two belief states. With each of these techniques we will discuss the types of heuristics that we can compute with special emphasis on relaxed
plans. We present several relaxed plan heuristics that differ in terms of how they employ state distance aggregation to make stronger assumptions about how states in a belief state can co-achieve
the goal through action sequences that are independent, positively interact, or negatively interact.
Our motivation for the first of the three planning graph techniques for measuring belief state
distances is to try a minimal extension to classical planning heuristics to see if they will work for us.
Noticing that our use of classical planning heuristics ignores distinctions between states in a belief
state and may provide uninformed heuristics, we move to the second approach where we possibly
build exponentially many planning graphs to get a better heuristic. With the multiple planning
graphs we extract a heuristic from each graph and aggregate them to get the belief state distance
measure. If we assume the states of a belief state are independent, we can aggregate the measures
with a summation. Or, if we assume they positively interact we can use a maximization. However,
as we will show, relaxed plans give us a unique opportunity to measure both positive interaction and
independence among the states by essentially taking the union of several relaxed plans. Moreover,
mutexes play a role in measuring negative interactions between states. Despite the utility of having
robust ways to aggregate state distances, we are still faced with the exponential blow up in the
number of planning graphs needed. Thus, our third approach seeks to retain the ability to measure
the interaction of state distances but avoid computing multiple graphs and extracting heuristics
from each. The idea is to condense and symbolically represent multiple planning graphs in a single
planning graph, called a Labelled Uncertainty Graph (LU G). Loosely speaking, this single graph
unions the causal support information present in the multiple graphs and pushes the disjunction,
36

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

describing sets of possible worlds (i.e., initial literal layers), into labels. The planning graph
vertices are the same as those present in multiple graphs, but redundant representation is avoided.
For instance an action that was present in all of the multiple planning graphs would be present only
once in the LU G and labelled to indicate that it is applicable in a planning graph projection from
each possible world. We will describe how to extract heuristics from the LU G that make implicit
assumptions about state interaction without explicitly aggregating several state distances.
Ideally, each of the planning graph techniques considers every state in a belief state to compute
heuristics, but as belief states grow in size this could become uninformed or costly. For example,
the single classical planning graph ignores distinctions between possible states where the heuristic
based on multiple graphs leads to the construction of a planning graph for each state. One way to
keep costs down is to base the heuristics on only a subset of the states in our belief state. We evaluate
the effect of such a sampling on the cost of our heuristics. With a single graph we sample a single
state and with multiple graphs and the LU G we sample some percent of the states. We evaluate
state sampling to show when it is appropriate, and find that it is dependent on how we compute
heuristics with the states.
Standardized Evaluation of Heuristics: An issue in evaluating the effectiveness of heuristic techniques is the many architectural differences between planners that use the heuristics. It is quite hard
to pinpoint the global effect of the assumptions underlying their heuristics on performance. For
example, GPT is outperformed by MBPbut it is questionable as to whether the credit for this efficiency is attributable to the differences in heuristics, or differences in search engines (MBP uses a
BDD-based search). Our interest in this paper is to systematically evaluate a spectrum of approaches
for computing heuristics for belief space planning. Thus we have implemented heuristics similar
to GPT and MBP and use them to compare against our new heuristics developed around the notion
of overlap (multiple world positive interaction and independence). We implemented the heuristics
within two planners, the Conformant-AltAlt planner (CAltAlt) and the Partially-Observable NonDeterministic planner (P ON D). P ON D does handle search with non-deterministic actions, but
for the bulk of the paper we discuss deterministic actions. This more general action formulation, as
pointed out by Smith and Weld (1998), can be translated into initial state uncertainty. Alternatively,
in Section 8.2 we discuss a more direct approach to reason with non-deterministic actions in the
heuristics.
External Evaluation: Although our main interest in this paper is to evaluate the relative advantages of a spectrum of belief space planning heuristics in a normalized setting, we also compare
the performance of the best heuristics from this work to current state of the art conformant and
conditional planners. Our empirical studies show that planning graph based heuristics provide effective guidance compared to cardinality heuristics as well as the reachability heuristic used by GPT
and CFF, and our planners are competitive with BDD-based planners such as MBP and YKA, and
GraphPlan-based ones such as CGP and SGP. We also notice that our planners gain scalability with
our heuristics and retain reasonable quality solutions, unlike several of the planners we compare
against.
The rest of this paper is organized as follows. We first present the CAltAlt and P ON D planners
by describing their state and action representations as well as their search algorithms. To understand
search guidance in the planners, we then discuss appropriate properties of heuristic measures for
belief space planning. We follow with a description of the three planning graph substrates used to
compute heuristics. We carry out an empirical evaluation in the next three sections, by describing
37

fiB RYCE , K AMBHAMPATI , & S MITH

our test setup, presenting a standardized internal comparison, and finally comparing with several
other state of the art planners. We end with related research, discussion, prospects for future work,
and various concluding remarks.

2. Belief Space Planners
Our planning formulation uses regression search to find strong conformant plans and progression
search to find strong conformant and conditional plans. A strong plan guarantees that after a finite
number of actions executed from any of the many possible initial states, all resulting states are goal
states. Conformant plans are a special case where the plan has no conditional plan branches, as in
classical planning. Conditional plans are a more general case where plans are structured as a graph
because they include conditional actions (i.e. the actions have causative and observational effects).
In this presentation, we restrict conditional plans to DAGs, but there is no conceptual reason why
they cannot be general graphs. Our plan quality metric is the maximum plan path length.
We formulate search in the space of belief states, a technique described by Bonet and Geffner
(2000). The planning problem P is defined as the tuple D, BSI , BSG , where D is a domain
description, BSI is the initial belief state, and BSG is the goal belief state (consisting of all states
satisfying the goal). The domain D is a tuple F, A, where F is a set of fluents and A is a set of
actions.
Logical Formula Representation: We make extensive use of logical formulas over F to represent
belief states, actions, and LU G labels, so we first explain a few conventions. We refer to every
fluent in F as either a positive literal or a negative literal, either of which is denoted by l. When
discussing the literal l, the opposite polarity literal is denoted l. Thus if l = at(location1), then
l = at(location1). We reserve the symbols  and  to denote logical false and true, respectively.
Throughout the paper we define the conjunction of an empty set equivalent to , and the disjunction
of an empty set as .
Logical formulas are propositional sentences comprised of literals, disjunction, conjunction, and
negation. We refer to the set of models of a formula f as M(f ). We consider the disjunctive normal
 ), and the conjunctive normal form of f , (f ). The DNF is seen as
form of a logical formula f , (f
a disjunction of constituents S each of which is a conjunction of literals. Alternatively the CNF
is seen as a conjunction of clauses C each of which is a disjunction of literals.1 We find it useful
to think of DNF and CNF represented as sets  a disjunctive set of constituents or a conjunctive set
of clauses. We also refer to the complete representation (f ) of a formula f as a DNF where every
constituent  or in this case state S  is a model of f .
Belief State Representation: A world state, S, is represented as a complete interpretation over
fluents. We also refer to states as possible worlds. A belief state BS is a set of states and is symbolically represented as a propositional formula over F . A state S is in the set of states represented by
a belief state BS if S  M(BS), or equivalently S |= BS.
For pedagogical purposes, we use the bomb and toilet with clogging and sensing problem,
BTCS, as a running example for this paper.2 BTCS is a problem that includes two packages, one of
 ) are readily related. Specifically each constituent contains k of the |F | literals,
1. It is easy to see that M(f ) and (f
corresponding to 2|F |k models.
2. We are aware of the negative publicity associated with the B&T problems and we do in fact handle more interesting
problems with difficult reachability and uncertainty (e.g. Logistics and Rovers), but to simplify our discussion we
choose this small problem.

38

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

which contains a bomb, and there is also a toilet in which we can dunk packages to defuse potential
bombs. The goal is to disarm the bomb and the only allowable actions are dunking a package in
the toilet (DunkP1, DunkP2), flushing the toilet after it becomes clogged from dunking (Flush), and
using a metal-detector to sense if a package contains the bomb (DetectMetal). The fluents encoding
the problem denote that the bomb is armed (arm) or not, the bomb is in a package (inP1, inP2) or
not, and that the toilet is clogged (clog) or not. We also consider a conformant variation on BTCS,
called BTC, where there is no DetectMetal action.
The belief state representation of the BTCS initial condition, in clausal representation is:
(BSI ) = arm clog  (inP1  inP2)  (inP1 inP2),
and in constituent representation is:

(BS
I ) = (arm  clog  inP1 inP2)  (arm  clog inP1  inP2).
The goal of BTCS has the clausal and constituent representation:

(BSG ) = (BS
G ) = arm.
However, the goal has the complete representation:
(BSG ) = (arm  clog  inP1 inP2)  (arm  clog inP1  inP2) 
(arm clog  inP1 inP2)  (arm clog inP1  inP2) 
(arm  clog inP1 inP2)  (arm  clog  inP1  inP2) 
(arm clog inP1 inP2)  (arm clog  inP1  inP2).
The last four states (disjuncts) in the complete representation are unreachable, but consistent with
the goal description.
Action Representation: We represent actions as having both causative and observational effects.
All actions a are described by a tuple e (a), (a), (a) where e (a) is an execution precondition,
(a) is a set of causative effects, and (a) is a set of observations. The execution precondition,
e (a), is a conjunction of literals that must hold for the action to be executable. If an action is executable, we apply the set of causative effects to find successor states and then apply the observations
to partition the successor states into observational classes.
Each causative effect j (a)  (a) is a conditional effect of the form j (a) = j (a), where
the antecedent j (a) and consequent j (a) are both a conjunction of literals. We handle disjunction
in e (a) or a j (a) by replicating the respective action or effect with different conditions, so with out
loss of generality we assume conjunctive preconditions. However, we cannot split disjunction in the
effects. Disjunction in an effect amounts to representing a set of non-deterministic outcomes. Hence
we do not allow disjunction in effects thereby restricting to deterministic effects. By convention
0 (a) is an unconditional effect, which is equivalent to a conditional effect where 0 (a) = .
The only way to obtain observations is to execute an action with observations. Each observation
formula oj (a)  (a) is a possible sensor reading. For example, an action a that observes the
truth values of two fluents p and q defines (a) = {p  q, p  q, p  q, p  q}. This differs
slightly from the conventional description of observations in the conditional planning literature.
Some works (e.g., Rintanen, 2003b) describe an observation as a list of observable formulas, then
define possible sensor readings as all boolean combinations of the formulas. We directly define the
possible sensor readings, as illustrated by our example. We note that our convention is helpful in
problems where some boolean combinations of observable formulas will never be sensor readings.
The causative and sensory actions for the example BTCS problem are:
39

fiB RYCE , K AMBHAMPATI , & S MITH

DunkP1: e = clog,  = {0 = clog, 1 = inP1 = arm},  = {},
DunkP2: e = clog,  = {0 = clog, 1 = inP2 = arm},  = {},
Flush: e = ,  = {0 = clog},  = {}, and
DetectMetal: e = ,  = ,  = {o0 = inP1, o1 = inP1}.
2.1 Regression
We perform regression in the CAltAlt planner to find conformant plans by starting with the goal
belief state and regressing it non-deterministically over all relevant actions. An action (without
observations) is relevant for regressing a belief state if (i) its unconditional effect is consistent with
every state in the belief state and (ii) at least one effect consequent contains a literal that is present in
a constituent of the belief state. The first part of relevance requires that every state in the successor
belief state is actually reachable from the predecessor belief state and the second ensures that the
action helps support the successor.
Following Pednault (1988), regressing a belief state BS over an action a, with conditional
effects, involves finding the execution, causation, and preservation formulas. We define regression
in terms of clausal representation, but it can be generalized for arbitrary formulas. The regression
of a belief state is a conjunction of the regression of clauses in (BS). Formally, the result BS  of
regressing the belief state BS over the action a is defined as:3




BS  = Regress(BS, a) = (a)  




((a, l)  IP (a, l))

C(BS) lC

Execution formula ((a)) is the execution precondition e (a). This is what must hold in BS  for
a to have been applicable.
Causation formula ((a, l)) for a literal l w.r.t all effects i (a) of an action a is defined as the
weakest formula that must hold in the state before a such that l holds in BS. The intuitive meaning
is that l already held in BS  , or the antecedent i (a) must have held in BS  to make l hold in BS.
Formally (a, l) is defined as:

i (a)
(a, l) = l 
i:li (a)

Preservation formula (IP (a, l)) of a literal l w.r.t. all effects i (a) of action a is defined as the
formula that must be true before a such that l is not violated by any effect i (a). The intuitive
meaning is that the antecedent of every effect that is inconsistent with l could not have held in BS  .
Formally IP (a, l) is defined as:
IP (a, l) =



i (a)

i:li (a)

Regression has also been formalized in the MBP planner (Cimatti & Roveri, 2000) as a symbolic
pre-image computation of BDDs (Bryant, 1986). While our formulation is syntactically different,
both approaches compute the same result.
3. Note that BS  may not be in clausal form after regression (especially when an action has multiple conditional effects).

40

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

BSG
Flush

BS1
Flush

BS4
Flush

BS7

DunkP1

BS8

DunkP1

BS2
DunkP1

BS5

DunkP2

BS3
DunkP2

BS6

DunkP2

BS9

Figure 1: Illustration of the regression search path for a conformant plan in the BT C problem.
2.2 CAltAlt
The CAltAlt planner uses the regression operator to generate children in an A* search. Regression
terminates when search node expansion generates a belief state BS which is logically entailed by
the initial belief state BSI . The plan is the sequence of actions regressed from BSG to obtain the
belief state entailed by BSI .
For example, in the BTC problem, Figure 1, we have:
BS2 =Regress(BSG , DunkP1) = clog  (arm  inP1).
The first clause is the execution formula and the second clause is the causation formula for the
conditional effect of DunkP1 and arm.
Regressing BS2 with Flush gives:
BS4 = Regress(BS2 , Flush) = (arm  inP1).
For BS4 , the execution precondition of Flush is , the causation formula is   clog = , and
(arm  inP1) comes by persistence of the causation formula.
Finally, regressing BS4 with DunkP2 gives:
BS9 = Regress(BS4 , DunkP2) = clog  (arm  inP1  inP2).
We terminate at BS9 because BSI |= BS9 . The plan is DunkP2, Flush, DunkP1.
2.3 Progression
In progression we can handle both causative effects and observations, so in general, progressing the
action a over the belief state BS generates the set of successor belief states B. The set of belief
states B is empty when the action is not applicable to BS (BS 
|= e (a)).
Progression of a belief state BS over an action a is best understood as the union of the result of
applying a to each model of BS but we in fact implement it as BDD images, as in the MBP planner
41

fiB RYCE , K AMBHAMPATI , & S MITH

(Bertoli et al., 2001b). Since we compute progression in two steps, first finding a causative successor, and second partitioning the successor into observational classes, we explain the steps separately.
The causative successor BS  is found by progressing the belief state BS over the causative effects
of the action a. If the action is applicable, the causative successor is the disjunction of causative
progression (Progressc ) for each state in BS over a:



: BS 
|= e (a)



BS = Progressc (BS, a) =
 SM(BS) Progressc (S, a) : otherwise
The progression of an action a over a state S is the conjunction of every literal that persists (no
applicable effect consequent contains the negation of the literal) and every literal that is given as an
effect (an applicable effect consequent contains the literal).


S  = Progressc (S, a) =

l:lS and
j S|=j (a) and
lj (a)



l
l:j

S|=j (a)
lj (a)

l
and

Applying the observations of an action results in the set of successors B. The set is found (in
Progresss ) by individually taking the conjunction of each sensor reading oj (a) with the causative
successor BS  . Applying the observations (a) to a belief state BS  results in a set B of belief
states, defined as:

: BS  =
 


{BS }
: (a) = 
B = Progresss (BS , a) =



j

{BS |BS = o (a)  BS } : otherwise
The full progression is computed as:
B = Progress(BS, a) = Progresss (Progressc (BS, a), a).
2.4 P ON D
We use top down AO* search (Nilsson, 1980), in the P ON D planner to generate conformant and
conditional plans. In the search graph, the nodes are belief states and the hyper-edges are actions.
We need AO* because applying an action with observations to a belief state divides the belief state
into observational classes. We use hyper-edges for actions because actions with observations have
several possible successor belief states, all of which must be included in a solution.
The AO* search consists of two repeated steps: expand the current partial solution, and then
revise the current partial solution. Search ends when every leaf node of the current solution is a
belief state that satisfies the goal and no better solution exists (given our heuristic function). Expansion involves following the current solution to an unexpanded leaf node and generating its children.
Revision is a dynamic programming update at each node in the current solution that selects a best
hyper-edge (action). The update assigns the action with minimum cost to start the best solution
rooted at the given node. The cost of a node is the cost of its best action plus the average cost of its
children (the nodes connected through the best action). When expanding a leaf node, the children
of all applied actions are given a heuristic value to indicate their estimated cost.
42

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

The main differences between our formulation of AO* and that of Nilsson (1980) are that we
do not allow cycles in the search graph, we update the costs of nodes with an average rather than a
summation, and use a weighted estimate of future cost. The first difference is to ensure that plans
are strong (there are a finite number of steps to the goal), the second is to guide search toward plans
with lower average path cost, and the third is to bias our search to trust the heuristic function. We
define our plan quality metric (maximum plan path length) differently than the metric our search
minimizes for two reasons. First, it is easier to compare to other competing planners because they
measure the same plan quality metric. Second, search tends to be more efficient using the average
instead of the maximum cost of an actions children. By using average instead of maximum, the
measured cost of a plan is lower  this means that we are likely to search a shallower search graph
to prove a solution is not the best solution.
Conformant planning, using actions without observations, is a special case for AO* search,
which is similar to A* search. The hyper-edges that represent actions are singletons, leading to a
single successor belief state. Consider the BTC problem (BTCS without the DetectMetal action)
with the future cost (heuristic value) set to zero for every search node. We show the search graph in
Figure 2 for this conformant example as well as a conditional example, described shortly. We can
expand the initial belief state by progressing it over all applicable actions. We get:
B1 = {BS10 } = Progress(BSI , DunkP1)
= {(inP1 inP2  clog arm)  (inP1  inP2  clog  arm)}
and
B3 = {BS20 } = Progress(BSI , DunkP2)
= {(inP1 inP2  clog  arm)  (inP1  inP2  clog arm)}.
Since clog already holds in every state of the initial belief state, applying Flush to BSI leads to
BSI creating a cycle. Hence, a hyper-edge for Flush is not added to the search graph for BSI . We
assign a cost of zero to BS10 and BS20 , update the internal nodes of our best solution, and add
DunkP1 to the best solution rooted at BSI (whose cost is now one).
We expand the leaf nodes of our best solution, a single node BS10 , with all applicable actions.
The only applicable action is Flush, so we get:
B3 = {BS30 } = Progress(BS10 , Flush)
= {(inP1 inP2 clog arm)  (inP1  inP2 clog  arm)}.
We assign a cost of zero to BS30 and update our best solution. We choose Flush as the best action
for BS10 (whose cost is now one), and choose DunkP2 as the best action for BSI (whose cost is
now one). DunkP2 is chosen for BSI because its successor BS20 has a cost of zero, as opposed to
BS10 which now has a cost of one.
Expanding the leaf node BS20 with the only applicable action, Flush, we get:
B4 = {BS40 } = Progress(BS20 , Flush)
= {(inP1  inP2 clog arm)  (inP1 inP2 clog  arm)}.
We update BS40 (to have cost zero) and BS20 (to have a cost of one), and choose Flush as the best
action for BS20 . The root node BSI has two children, each with cost one, so we arbitrarily choose
DunkP1 as the best action.
We expand BS30 with the relevant actions to get BSG with the DunkP2 action. DunkP1 creates
a cycle back to BS10 so it is not added to the search graph. We now have a solution where all leaf
nodes are terminal. While it is only required that a terminal belief state contains a subset of the
43

fiB RYCE , K AMBHAMPATI , & S MITH

BSI
DunkP1

Detect
Metal

DunkP2

:inP1

inP1

B1

B2

BS10

B5

BS50

BS20

Flush

Flush

B3

B4

BS30

DunkP2

DunkP1
DunkP2

DunkP1

B6

BS40
DunkP2

BS51

B7

BS60

BS70

DunkP1

BSG

Figure 2: Illustration of progression search for a conformant plan (bold dashed edges) and a conditional plan (bold solid edges) in the BTCS problem.

states in BSG , in this case the terminal belief state contains exactly the states in BSG . The cost of
the solution is three because, through revision, BS30 has a cost of one, which sets BS10 to a cost
of two. However, this means now that BSI has cost of three if its best action is DunkP1. Instead,
revision sets the best action for BSI to DunkP2 because its cost is currently two.
We then expand BS40 with DunkP1 to find that its successor is BSG . DunkP2 creates a cycle
back to BS20 so it is not added to the search graph. We now have our second valid solution because
it contains no unexpanded leaf nodes. Revision sets the cost of BS40 to one, BS20 to two, and
BSI to three. Since all solutions starting at BSI have equal cost (meaning there are now cheaper
solutions), we can terminate with the plan DunkP2, Flush, DunkP1, shown in bold with dashed lines
in Figure 2.
As an example of search for a conditional plan in P ON D, consider the BTCS example whose
search graph is also shown in Figure 2. Expanding the initial belief state, we get:
B1 = {BS10 } = Progress(BSI , DunkP1),
B2 = {BS20 } = Progress(BSI , DunkP2),
and
B5 = {BS50 , BS51 } = Progress(BSI ,DetectMetal)
= {inP1 inP2 clog  arm, inP1  inP2 clog  arm}.
Each of the leaf nodes is assigned a cost of zero, and DunkP1 is chosen arbitrarily for the best
solution rooted at BSI because the cost of each solution is identical. The cost of including each
hyper-edge is the average cost of its children plus its cost, so the cost of using DetectMetal is (0+0)/2
+ 1 = 1. Thus, our root BSI has a cost of one.
44

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

As in the conformant problem we expand BS10 , giving its child a cost of zero and BS10 a cost
of one. This changes our best solution at BSI to use DunkP2, and we expand BS20 , giving its child
a cost of zero and it a cost of one. Then we choose DetectMetal to start the best solution at BSI
because it gives BSI a cost of one, where using either Dunk action would give BSI a cost of two.
We expand the first child of DetectMetal, BS50 , with DunkP1 to get:
{inP1 inP2  clog arm},
which is a goal state, and DunkP2 to get:
B6 = {BS60 } = Progress(BS50 ,DunkP2) = {inP1 inP2 clog  arm}.
We then expand the second child, BS51 , with DunkP2 to get:
{inP1  inP2  clog arm},
which is also a goal state and DunkP1 to get:
B7 = {BS70 } = Progress(BS51 ,DunkP1) = {inP1  inP2 clog  arm}.
While none of these new belief states are not equivalent to BSG , two of them entail BSG , so we
can treat them as terminal by connecting the hyper-edges for these actions to BSG . We choose
DunkP1 and DunkP2 as best actions for BS50 and BS51 respectively and set the cost of each node
to one. This in turn sets the cost of using DetectMetal for BSI to (1+1)/2 + 1 = 2. We terminate
here because this plan has cost equal to the other possible plans starting at BSI and all leaf nodes
satisfy the goal. The plan is shown in bold with solid lines in Figure 2.

3. Belief State Distance
In both the CAltAlt and P ON D planners we need to guide search node expansion with heuristics
that estimate the plan distance dist(BS, BS  ) between two belief states BS and BS  . By convention, we assume BS precedes BS  (i.e., in progression BS is a search node and BS  is the goal
belief state, or in regression BS is the initial belief state and BS  is a search node). For simplicity,
we limit our discussion to progression planning. Since a strong plan (executed in BS) ensures that
every state S  M(BS) will transition to some state S   M(BS  ), we define the plan distance
between BS and BS  as the number of actions needed to transition every state S  M(BS) to a
state S   M(BS  ). Naturally, in a strong plan, the actions used to transition a state S1  M(BS)
may affect how we transition another state S2  M(BS). There is usually some degree of positive
or negative interaction between S1 and S2 that can be ignored or captured in estimating plan distance.4 In the following we explore how to perform such estimates by using several intuitions from
classical planning state distance heuristics.
We start with an example search scenario in Figure 3. There are three belief states BS1 (containing states S11 and S12 ), BS2 (containing state S21 ), and BS3 (containing states S31 and S32 ).
The goal belief state is BS3 , and the two progression search nodes are BS1 and BS2 . We want to
expand the search node with the smallest distance to BS3 by estimating dist(BS1 , BS3 )  denoted
by the bold, dashed line  and dist(BS2 , BS3 )  denoted by the bold, solid line. We will assume for
now that we have estimates of state distance measures dist(S, S  )  denoted by the light dashed and
solid lines with numbers. The state distances can be represented as numbers or action sequences. In
our example, we will use the following action sequences for illustration:
4. Interaction between states captures the notion that actions performed to transition one state to the goal may interfere
(negatively interact) or aid with (positively interact) transitioning other states to goals states.

45

fiB RYCE , K AMBHAMPATI , & S MITH

BS1
S11

BS3

14
5

S12

S31

3
7

BS2

S32

8

S21

10

Figure 3: Conformant Plan Distance Estimation in Belief Space
dist(S11 , S32 ) : ({a1 , a2 }, {a5 }, {a6 , a7 }),
dist(S12 , S31 ) : ({a1 , a7 }, {a3 }),
dist(S21 , S31 ) : ({a3 , a6 }, {a9 , a2 , a1 }, {a0 , a8 }, {a5 }).
In each sequence there may be several actions in each step. For instance, dist(S21 , S31 ) has a3 and
a6 in its first step, and there are a total of eight actions in the sequence  meaning the distance is
eight. Notice that our example includes several state distance estimates, which can be found with
classical planning techniques. There are many ways that we can use similar ideas to estimate belief
state distance once we have addressed the issue of belief states containing several states.
Selecting States for Distance Estimation: There exists a considerable body of literature on estimating the plan distance between states in classical planning (Bonet & Geffner, 1999; Nguyen,
Kambhampati, & Nigenda, 2002; Hoffmann & Nebel, 2001), and we would like to apply it to estimate the plan distance between two belief states, say BS1 and BS3 . We identify four possible
options for using state distance estimates to compute the distance between belief states BS1 and
BS3 :
 Sample a State Pair: We can sample a single state from BS1 and a single state from BS3 ,
whose plan distance is used for the belief state distance. For example, we might sample S12
from BS1 and S31 from BS3 , then define dist(BS1 , BS3 ) = dist(S12 , S31 ).
 Aggregate States: We can form aggregate states for BS1 and BS3 and measure their plan
distance. An aggregate state is the union of the literals needed to express a belief state formula,
46

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

which we define as:
S(BS) =

ff

l

l:lS,S(BS)

Since it is possible to express a belief state formula with every literal (e.g., using (q  q)  p
to express the belief state where p is true), we assume a reasonably succinct representation,
such as a ROBDD (Bryant, 1986). It is quite possible the aggregate states are inconsistent, but many classical planning techniques (such as planning graphs) do not require consistent states. For example, with aggregate states we would compute the belief state distance
dist(BS1 , BS3 ) = dist(S(BS1 ), S(BS3 )).
 Choose a Subset of States: We can choose a set of states (e.g., by random sampling) from
BS1 and a set of states from BS3 , and then compute state distances for all pairs of states
from the sets. Upon computing all state distances, we can aggregate the state distances (as we
will describe shortly). For example, we might sample both S11 and S12 from BS1 and S31
from BS3 , compute dist(S11 , S31 ) and dist(S12 , S31 ), and then aggregate the state distances
to define dist(BS1 , BS3 ).
 Use All States: We can use all states in BS1 and BS3 , and, similar to sampling a subset of
states (above), we can compute all distances for state pairs and aggregate the distances.
The former two options for computing belief state distance are reasonably straightforward, given
the existing work in classical planning. In the latter two options we compute multiple state distances.
With multiple state distances there are two details which require consideration in order to obtain a
belief state distance measure. In the following we treat belief states as if they contain all states
because they can be appropriately replaced with the subset of chosen states.
The first issue is that some of the state distances may not be needed. Since each state in BS1
needs to reach a state in BS3 , we should consider the distance for each state in BS1 to a state in
BS3 . However, we dont necessarily need the distance for every state in BS1 to every state in
BS3 . We will explore assumptions about which state distances need to be computed in Section 3.1.
The second issue, which arises after computing the state distances, is that we need to aggregate
the state distances into a belief state distance. We notice that the popular state distance estimates
used in classical planning typically measure aggregate costs of state features (literals). Since we
are planning in belief space, we wish to estimate belief state distance with the aggregate cost of
belief state features (states). In Section 3.2, we will examine several choices for aggregating state
distances and discuss how each captures different types of state interaction. In Section 3.3, we
conclude with a summary of the choices we make in order to compute belief state distances.
3.1 State Distance Assumptions
When we choose to compute multiple state distances between two belief states BS and BS  ,
whether by considering all states or sampling subsets, not all of the state distances are important.
For a given state in BS we do not need to know the distance to every state in BS  because each
state in BS need only transition to one state in BS  . There are two assumptions that we can make
about the states reached in BS  which help us define two different belief state distance measures in
terms of aggregate state distances:
47

fiB RYCE , K AMBHAMPATI , & S MITH

 We can optimistically assume that each of the earlier states S  M(BS) can reach the closest
of the later states S   M(BS  ). With this assumption we compute distance as:
dist(BS, BS  ) = ffSM(BS)

min

S  M(BS  )

dist(S, S  ).

 We can assume that all of the earlier states S  M(BS) reach the same later state S  
M(BS  ), where the aggregate distance is minimum. With this assumption we compute distance as:
dist(BS, BS  ) =

min

S  M(BS  )

ffSM(BS) dist(S, S  ),

where ff represents an aggregation technique (several of which we will discuss shortly).
Throughout the rest of the paper we use the first definition for belief state distance because it is
relatively robust and easy to compute. Its only drawback is that it treats the earlier states in a more
independent fashion, but is flexible in allowing earlier states to transition to different later states.
The second definition measures more dependencies of the earlier states, but restricts them to reach
the same later state. While the second may sometimes be more accurate, it is misinformed in cases
where all earlier states cannot reach the same later state (i.e., the measure would be infinite). We do
not pursue the second method because it may return distance measures that are infinite when they
are in fact finite.
As we will see in Section 4, when we discuss computing these measures with planning graphs,
we can implicitly find for each state in BS the closest state in BS  , so that we do not enumerate the
states S  in the minimization term of the first belief state distance (above). Part of the reason we can
 ) rather than actual states.

do this is that we compute distance in terms of constituents S   (BS
Also, because we only consider constituents of BS  , when we discuss sampling belief states to include in distance computation we only sample from BS. We can also avoid the explicit aggregation
ff by using the LU G, but describe several choices for ff to understand implicit assumptions made
by the heuristics computed on the LU G.
3.2 State Distance Aggregation
The aggregation function ff plays an important role in how we measure the distance between belief
states. When we compute more than one state distance measure, either exhaustively or by sampling
a subset (as previously mentioned), we must combine the measures by some means, denoted ff.
There is a range of options for taking the state distances and aggregating them into a belief state
distance. We discuss several assumptions associated with potential measures:
 Positive Interaction of States: Positive interaction assumes that the most difficult state in BS
requires actions that will help transition all other states in BS to some state in BS  . In our
example, this means that we assume the actions used to transition S11 to S32 will help us
transition S12 to S31 (assuming each state in BS1 transitions to the closest state in BS3 ).
Inspecting the action sequences, we see they positively interact because both need actions a1
and a7 . We do not need to know the action sequences to assume positive interaction because
we define the aggregation ff as a maximization of numerical state distances:
dist(BS, BS  ) =

max

min

SM(BS) S  M(BS  )

dist(S, S  ).
48

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

The belief state distances are dist(BS1 , BS3 ) = max(min(14, 5), min(3, 7)) = 5 and
dist(BS2 , BS3 ) = max(min(8, 10)) = 8. In this case we prefer BS1 to BS2 . If each
state distance is admissible and we do not sample from belief states, then assuming positive
interaction is also admissible.
 Independence of States: Independence assumes that each state in BS requires actions that are
different from all other states in BS in order to reach a state in BS  . Previously, we found
there was positive interaction in the action sequences to transition S11 to S32 and S12 to S31
because they shared actions a1 and a7 . There is also some independence in these sequences
because the first contains a2 , a5 , and a6 , where the second contains a3 . Again, we do not need
to know the action sequences to assume independence because we define the aggregation ff
as a summation of numerical state distances:
fi
min dist(S, S  ).
dist(BS, BS  ) =


SM(BS) S M(BS )

In our example, dist(BS1 , BS3 ) = min(14, 5) + min(3, 7) = 8, and dist(BS2 , BS3 ) =
min(8, 10) = 8. In this case we have no preference over BS1 and BS2 .
We notice that using the cardinality of a belief state |M(BS)| to measure dist(BS, BS  ) is
a special case of assuming state independence, where S, S  dist(S, S  ) = 1. If we use cardinality to measure distance in our example, then we have dist(BS1 , BS3 ) = |M(BS1 )| = 2,
and dist(BS2 , BS3 ) = |M(BS2 )| = 1. With cardinality we prefer BS2 over BS1 because
we have better knowledge in BS2 .
 Overlap of States: Overlap assumes that there is both positive interaction and independence
between the actions used by states in BS to reach a state in BS  . The intuition is that some
actions can often be used for multiple states in BS simultaneously and we should count these
actions only once. For example, when we computed dist(BS1 , BS3 ) by assuming positive
interaction, we noticed that the action sequences for dist(S11 , S32 ) and dist(S12 , S31 ) both
used a1 and a7 . When we aggregate these sequences we would like to count a1 and a7 each
only once because they potentially overlap. However, truly combining the action sequences
for maximal overlap is a plan merging problem (Kambhampati, Ihrig, & Srivastava, 1996),
which can be as difficult as planning. Since our ultimate intent is to compute heuristics,
we take a very simple approach to merging action sequences. We introduce a plan merging
operator  for ff that picks a step at which we align the sequences and then unions the aligned
steps. We use the size of the resulting action sequence to measure belief state distance:
dist(BS, BS  ) = SM(BS)

min

S  M(BS  )

dist(S, S  ).

Depending on the type of search, we define  differently. We assume that sequences used in
progression search start at the same time and those used in regression end at the same time.
Thus, in progression all sequences are aligned at the first step before we union steps, and in
regression all sequences are aligned at the last step before the union.
For example, in progression dist(S11 , S32 )  dist(S12 , S31 ) = ({a1 , a2 }, {a5 }, {a6 , a7 }) 
({a1 , a7 }, {a3 }) = ({a1 , a2 , a7 }, {a5 , a3 }, {a6 , a7 }) because we align the sequences at their
first steps, then union each step. Notice that this resulting sequence has seven actions, giving
49

fiB RYCE , K AMBHAMPATI , & S MITH

dist(BS1 , BS3 ) = 7, whereas defining ff as maximum gave a distance of five and as summation gave a distance of eight. Compared with overlap, positive interaction tends to under
estimate distance, and independence tends to over estimate distance. As we will see during our empirical evaluation (in Section 6.5), accounting for overlap provides more accurate
distance measures for many conformant planning domains.
 Negative Interaction of States: Negative interaction between states can appear in our example
if transitioning state S11 to state S32 makes it more difficult (or even impossible) to transition
state S12 to state S31 . This could happen if performing action a5 for S11 conflicts with action
a3 for S12 . We can say that BS1 cannot reach BS3 if all possible action sequences that start
in S11 and S12 , respectively, and end in any S  M(BS3 ) negatively interact.
There are two ways negative interactions play a role in belief state distances. Negative interactions can allow us to prove it is impossible for a belief state BS to reach a belief state
BS  , meaning dist(BS, BS  ) = , or they can potentially increase the distance by a finite
amount. We use only the first, more extreme, notion of negative interaction by computing
cross-world mutexes (Smith & Weld, 1998) to prune belief states from the search. If we
cannot prune a belief state, then we use one of the aforementioned techniques to aggregate
state distances. As such, we do not provide a concrete definition for ff to measure negative
interaction.
While we do not explore ways to adjust the distance measure for negative interactions, we
mention some possibilities. Like work in classical planning (Nguyen et al., 2002), we can
penalize the distance measure dist(BS1 , BS3 ) to reflect additional cost associated with serializing conflicting actions. Additionally in conditional planning, conflicting actions can be
conditioned on observations so that they do not execute in the same plan branch. A distance
measure that uses observations would reflect the added cost of obtaining observations, as
well as the change in cost associated with introducing plan branches (e.g., measuring average
branch cost).
The above techniques for belief state distance estimation in terms of state distances provide the
basis for our use of multiple planning graphs. We will show in the empirical evaluation that these
measures affect planner performance very differently across standard conformant and conditional
planning domains. While it can be quite costly to compute several state distance measures, understanding how to aggregate state distances sets the foundation for techniques we develop in the
LU G. As we have already mentioned, the LU G conveniently allows us to implicitly aggregate state
distances to directly measure belief state distance.
3.3 Summary of Methods for Distance Estimation
Since we explore several methods for computing belief state distances on planning graphs, we provide a summary of the choices we must consider, listed in Table 1. Each column is headed with a
choice, containing possible options below. The order of the columns reflects the order in which we
consider the options.
In this section we have covered the first two columns which relate to selecting states from belief
states for distance computation, as well as aggregating multiple state distances into a belief state
distance. We test options for both of these choices in the empirical evaluation.
50

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

State
Selection
Single
Aggregate
Subset
All

State Distance
Aggregation
+ Interaction
Independence
Overlap
- Interaction

Planning
Graph
SG
MG
LU G

Mutex
Type
None
Static
Dynamic
Induced

Mutex
Worlds
Same
Intersect
Cross

Heuristic
Max
Sum
Level
Relaxed Plan

Table 1: Features for a belief state distance estimation.
In the next section we will also expand upon how to aggregate distance measures as well as
discuss the remaining columns of Table 1. We will present each type of planning graph: the single
planning graph (SG), multiple planning graphs (M G), and the labelled uncertainty graph (LU G).
Within each planning graph we will describe several types of mutex, including static, dynamic,
and induced mutexes. Additionally, each type of mutex can be computed with respect to different
possible worlds  which means the mutex involves planning graph elements (e.g., actions) when
they exist in the same world (i.e., mutexes are only computed within the planning graph for a single
state), or across worlds (i.e., mutexes are computed between planning graphs for different states)
by two methods (denoted Intersect and Cross). Finally, we can compute many different heuristics
on the planning graphs to measure state distances  max, sum, level, and relaxed plan. We focus
our discussion on the planning graphs, same-world mutexes, and relaxed plan heuristics in the next
section. Cross-world mutexes and the other heuristics are described in appendices.

4. Heuristics
This section discusses how we can use planning graph heuristics to measure belief state distances.
We cover several types of planning graphs and the extent to which they can be used to compute
various heuristics. We begin with a brief background on planning graphs.
Planning Graphs: Planning graphs serve as the basis for our belief state distance estimation. Planning graphs were initially introduced in GraphPlan (Blum & Furst, 1995) for representing an optimistic, compressed version of the state space progression tree. The compression lies in unioning
the literals from every state at subsequent steps from the initial state. The optimism relates to underestimating the number of steps it takes to support sets of literals (by tracking only a subset of the
infeasible tuples of literals). GraphPlan searches the compressed progression (or planning graph)
once it achieves the goal literals in a level with no two goal literals marked infeasible. The search
tries to find actions to support the top level goal literals, then find actions to support the chosen
actions and so on until reaching the first graph level. The basic idea behind using planning graphs
for search heuristics is that we can find the first level of a planning graph where a literal in a state
appears; the index of this level is a lower bound on the number of actions that are needed to achieve
a state with the literal. There are also techniques for estimating the number of actions required to
achieve sets of literals. The planning graphs serve as a way to estimate the reachability of state literals and discriminate between the goodness of different search states. This work generalizes such
literal estimations to belief space search by considering both GraphPlan and CGP style planning
graphs plus a new generalization of planning graphs, called the LU G.
Planners such as CGP (Smith & Weld, 1998) and SGP (Weld et al., 1998) adapt the GraphPlan
idea of compressing the search space with a planning graph by using multiple planning graphs, one
51

fiB RYCE , K AMBHAMPATI , & S MITH

Overlap

n-distances

hMG
RPU

hLUG
RP

State Distance Aggregation

CFF

Independence

Positive
Interaction

None

h card
MBP
KACMBP
YKA

hMG
s-RP

GPT

hMG
m-RP

h0
NG

1

hSG
RP
U
hSG
RP
SG

MG

LUG

Planning Graph Type

Figure 4: Taxonomy of heuristics with respect to planning graph type and state distance aggregation. Blank entries indicate that the combination is meaningless or not possible.

for each possible world in the initial belief state. CGP and SGP search on these planning graphs,
similar to GraphPlan, to find conformant and conditional plans. The work in this paper seeks to
apply the idea of extracting search heuristics from planning graphs, previously used in state space
search (Nguyen et al., 2002; Hoffmann & Nebel, 2001; Bonet & Geffner, 1999) to belief space
search.
Planning Graphs for Belief Space: This section proceeds by describing four classes of heuristics
to estimate belief state distance N G, SG, M G, and LU G. N G heuristics are techniques existing in
the literature that are not based on planning graphs, SG heuristics are techniques based on a single
classical planning graph, M G heuristics are techniques based on multiple planning graphs (similar
to those used in CGP) and LU G heuristics use a new labelled planning graph. The LU G combines
the advantages of SG and M G to reduce the representation size and maintain informedness. Note
that we do not include observations in any of the planning graph structures as SGP (Weld et al.,
1998) would, however we do include this feature for future work. The conditional planning formulation directly uses the planning graph heuristics by ignoring observations, and our results show that
this still gives good performance.
In Figure 4 we present a taxonomy of distance measures for belief space. The taxonomy also
includes related planners, whose distance measures will be characterized in this section. All of the
related planners are listed in the N G group, despite the fact that some actually use planning graphs,
because they do not clearly fall into one of our planning graph categories. The figure shows how
52

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

different substrates (horizontal axis) can be used to compute belief state distance by aggregating
state to state distances under various assumptions (vertical axis). Some of the combinations are
not considered because they do not make sense or are impossible. The reasons for these omissions
will be discussed in subsequent sections. While there are a wealth of different heuristics one can
compute using planning graphs, we concentrate on relaxed plans because they have proven to be the
most effective in classical planning and in our previous studies (Bryce & Kambhampati, 2004). We
provide additional descriptions of other heuristics like max, sum, and level in Appendix A.
Example: To illustrate the computation of each heuristic, we use an example derived from BTC
called Courteous BTC (CBTC) where a courteous package dunker has to disarm the bomb and
leave the toilet unclogged, but some discourteous person has left the toilet clogged. The initial
belief state of CBTC in clausal representation is:
(BSI ) = arm  clog  (inP1  inP2)  (inP1 inP2),
and the goal is:
(BSG ) = clog arm.
The optimal action sequences to reach BSG from BSI are:
Flush, DunkP1, Flush, DunkP2, Flush,
and
Flush, DunkP2, Flush, DunkP1, Flush.
Thus the optimal heuristic estimate for the distance between BSI and BSG , in regression, is
h (BSG ) = 5 because in either plan there are five actions.
We use planning graphs for both progression and regression search. In regression search the
heuristic estimates the cost of the current belief state w.r.t. the initial belief state and in progression
search the heuristic estimates the cost of the goal belief state w.r.t. the current belief state. Thus,
in regression search the planning graph(s) are built (projected) once from the possible worlds of
the initial belief state, but in progression search they need to be built at each search node. We
introduce a notation BSi to denote the belief state for which we find a heuristic measure, and BSP
to denote the belief state that is used to construct the initial layer of the planning graph(s). In the
following subsections we describe computing heuristics for regression, but they are generalized for
progression by changing BSi and BSP appropriately.
In the previous section we discussed two important issues involved in heuristic computation:
sampling states to include in the computation and using mutexes to capture negative interactions in
the heuristics. We will not directly address these issues in this section, deferring them to discussion
in the respective empirical evaluation sections, 6.4 and 6.2. The heuristics below are computed
once we have decided on a set of states to use, whether by sampling or not. Also, as previously
mentioned, we only consider sampling states from the belief state BSP because we can implicitly
find closest states from BSi without sampling. We only explore computing mutexes on the planning
graphs in regression search. We use mutexes to determine the first level of the planning graph where
the goal belief state is reachable (via the level heuristic described in Appendix A) and then extract a
relaxed plan starting at that level. If the level heuristic is  because there is no level where a belief
state is reachable, then we can prune the regressed belief state.
We proceed by describing the various substrates used for computing belief space distance estimates. Within each we describe the prospects for various types of world aggregation. In addition to
our heuristics, we mention related work in the relevant areas.
53

fiB RYCE , K AMBHAMPATI , & S MITH

4.1 Non Planning Graph-based Heuristics (N G)
We group many heuristics and planners into the N G group because they are not using SG, M G,
or LU G planning graphs. Just because we mention them in this group does not mean they are not
using planning graphs in some other form.
No Aggregation: Breadth first search uses a simple heuristic, h0 where the heuristic value is set
to zero. We mention this heuristic so that we can gauge the effectiveness of our search substrates
relative to improvements gained through using heuristics.
Positive Interaction Aggregation: The GPT planner (Bonet & Geffner, 2000) measures belief
state distance as the maximum of the minimum state to state distance of states in the source and
destination belief states, assuming optimistic reachability as mentioned in Section 3. GPT measures
state distances exactly, in terms of the minimum number of transitions in the state space. Taking
the maximum state to state distance is akin to assuming positive interaction of states in the current
belief state.
Independence Aggregation: The MBP planner (Bertoli et al., 2001b), KACMBP planner (Bertoli
& Cimatti, 2002), YKA planner (Rintanen, 2003b), and our comparable hcard heuristic measure
belief state distance by assuming every state to state distance is one, and taking the summation of
the state distances (i.e. counting the number of states in a belief state). This measure can be useful
in regression because goal belief states are partially specified and contain many states consistent
with a goal formula and many of the states consistent with the goal formula are not reachable from
the initial belief state. Throughout regression, many of the unreachable states are removed from
predecessor belief states because they are inconsistent with the preconditions of a regressed action.
Thus, belief states can reduce in size during regression and their cardinality may indicate they are
closer to the initial belief state. Cardinality is also useful in progression because as belief states
become smaller, the agent has more knowledge and it can be easier to reach a goal state.
In CBTC, hcard (BSG ) = 4 because BSG has four states consistent with its complete representation:
(BSG ) = (inP1 inP2clog arm)  (inP1  inP2 clog arm) 
(inP1 inP2 clog arm)  (inP1  inP2 clog arm).
Notice, this may be uninformed for BSG because two of the states in (BSG ) are not reachable,
like: (inP1  inP2 clog arm). If there are n packages, then there would be 2n1 unreachable
states represented by (BSG ). Counting unreachable states may overestimate the distance estimate
because we do not need to plan for them. In general, in addition to the problem of counting unreachable states, cardinality does not accurately reflect distance measures. For instance, MBP reverts to
breadth first search in classical planning problems because state distance may be large or small but
it still assigns a value of one.
Overlap Aggregation: Rintanen (2004) describes n-Distances which generalize the belief state
distance measure in GPT to consider the maximum n-tuple state distance. The measure involves,
for each n-sized tuple of states in a belief state, finding the length of the actual plan to transition the
n-tuple to the destination belief state. Then the maximum n-tuple distance is taken as the distance
measure.
For example, consider a belief state with four states. With an n equal to two, we would define
six belief states, one for each size two subset of the four states. For each of these belief states we
find a real plan, then take the maximum cost over these plans to measure the distance for the original
54

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

L0

A0

E0

L1

A1

E1

L2

inP1

inP1

inP1

inP1

inP1

inP1

inP2

inP2

inP2

inP2

inP2

inP2
DunkP1

1(DunkP1)
0(DunkP1)

arm
DunkP2
arm

arm

1(DunkP2)
0(DunkP2)

clog

clog

clog
Flush

0(Flush)

clog

arm

Flush

0(Flush)

clog

Figure 5: Single planning graph for CBTC, with relaxed plan components in bold. Mutexes omitted.

four state belief state. When n is one, we are computing the same measure as GPT, and when n is
equal to the size of the belief state we are directly solving the planning problem. While it is costly
to compute this measure for large values of n, it is very informed as it accounts for overlap and
negative interactions.
The CFF planner (Hoffmann & Brafman, 2004) uses a version of a relaxed planning graph to
extract relaxed plans. The relaxed plans measure the cost of supporting a set of goal literals from all
states in a belief state. In addition to the traditional notion of a relaxed planning graph that ignores
mutexes, CFF also ignores all but one antecedent literal in conditional effects to keep their relaxed
plan reasoning tractable. The CFF relaxed plan does capture overlap but ignores some subgoals and
all mutexes. The way CFF ensures the goal is supported in the relaxed problem is to encode the
relaxed planning graph as a satisfiability problem. If the encoding is satisfiable, the chosen number
of action assignments is the distance measure.
4.2 Single Graph Heuristics (SG)
The simplest approach for using planning graphs for belief space planning heuristics is to use a
classical planning graph. To form the initial literal layer from the projected belief state, we could
either sample a single state (denoted SG1 ) or use an aggregate state (denoted SGU ). For example,
in CBTC (see Figure 5) assuming regression search with BSP = BSI , the initial level L0 of the
planning graph for SG1 might be:
55

fiB RYCE , K AMBHAMPATI , & S MITH

L0 = {arm, clog, inP1, inP2}
and for SGU it is defined by the aggregate state S(BSP ):
L0 = {arm, clog, inP1, inP2, inP1, inP2}.
Since these two versions of the single planning graph have identical semantics, aside from the initial
literal layer, we proceed by describing the SGU graph and point out differences with SG1 where
they arise.
Graph construction is identical to classical planning graphs (including mutex propagation) and
stops when two subsequent literal layers are identical (level off). We use the planning graph formalism used in IPP (Koehler, Nebel, Hoffmann, & Dimopoulos, 1997) to allow for explicit representation of conditional effects, meaning there is a literal layer Lk , an action layer Ak , and an effect
layer Ek in each level k. Persistence for a literal l, denoted by lp , is represented as an action where
e (lp ) = 0 (lp ) = l. A literal is in Lk if an effect from the previous effect layer Ek1 contains the
literal in its consequent. An action is in the action layer Ak if every one of its execution precondition
literals is in Lk . An effect is in the effect layer Ek if its associated action is in the action layer Ak and
every one of its antecedent literals is in Lk . Using conditional effects in the planning graph avoids
factoring an action with conditional effects into a possibly exponential number of non-conditional
actions, but adds an extra planning graph layer per level. Once our graph is built, we can extract
heuristics.
No Aggregation: Relaxed plans within a single planning graph are able to measure, under the
most optimistic assumptions, the distance between two belief states. The relaxed plan represents a
distance between a subset of the initial layer literals and the literals in a constituent of our belief
state. In the SGU , the literals from the initial layer that are used for support may not hold in a
single state of the projected belief state, unlike the SG1 . The classical relaxed plan heuristic hSG
RP
finds a set of (possibly interfering) actions to support the goal constituent. The relaxed plan RP is a
RP
RP
RP
RP
RP
subgraph of the planning graph, of the form {ARP
0 , E0 , L1 , ..., Ab1 , Eb1 , Lb }. Each of the
layers contains a subset of the vertices in the corresponding layer of the planning graph.

More formally, we find the relaxed plan to support the constituent S  (BS
i ) that is reached
SG
earliest in the graph (as found by the hlevel (BSi ) heuristic in Appendix A). Briefly, hSG
level (BSi )
returns the first level b where a constituent of BSi has all its literals in Lb and none are marked
pair-wise mutex. Notice that this is how we incorporate negative interactions into our heuristics.
We start extraction at the level b, by defining LRP
as the literals in the constituent used in the level
b
RP
heuristic. For each literal l  Lb , we select a supporting effect (ignoring mutexes) from Eb1
RP . We prefer persistence of literals to effects in supporting literals. Once a
to form the subset Eb1
RP
supporting set of effects is found, we create ARP
b1 as all actions with an effect in Eb1 . Then the
RP
RP are added
needed preconditions for the actions and antecedents for chosen effects in Ab1 and Eb1
to the list of literals to support from LRP
b2 . The algorithm repeats until we find the needed actions
from A0 . A relaxed plans value is the summation of the number of actions in each action layer. A
literal persistence, denoted by a subscript p, is treated as an action in the planning graph, but in a
|. The single graph relaxed plan
relaxed plan we do not include it in the final computation of | ARP
j
heuristic is computed as
hSG
RP (BSi )

=

b1

j=0

56

| ARP
|
j

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

For the CBTC problem we find a relaxed plan from the SGU , as shown in Figure 5 as the bold
edges and nodes. Since arm and clog are non mutex at level two, we can use persistence to
RP we can use persistence for inP1, and
support clog and DunkP1 to support arm in LRP
2 . In L1
SG
Flush for clog. Thus, hRP (BSG ) = 2 because the relaxed plan is:
= {inP1p , Flush},
ARP
0
E0RP = {0 (inP1p ), 0 (Flush)},

= {inP1, clog},
LRP
1
= {clogp , DunkP1},
ARP
1

E1RP = {0 (clogp ), 1 (DunkP1)},
= {arm, clog}.
LRP
2

The relaxed plan does not use both DunkP2 and DunkP1 to support arm. As a result arm is
not supported in all worlds (i.e. it is not supported when the state where inP2 holds is our initial
state). Our initial literal layer threw away knowledge of inP1 and inP2 holding in different worlds,
and the relaxed plan extraction ignored the fact that arm needs to be supported in all worlds. Even
with an SG1 graph, we see similar behavior because we are reasoning with only a single world. A
single, unmodified classical planning graph cannot capture support from all possible worlds  hence
there is no explicit aggregation over distance measures for states. As a result, we do not mention
aggregating states to measure positive interaction, independence, or overlap.
4.3 Multiple Graph Heuristics (M G)
Single graph heuristics are usually uninformed because the projected belief state BSP often corresponds to multiple possible states. The lack of accuracy is because single graphs are not able to
capture propagation of multiple world support information. Consider the CBTC problem where the
projected belief state is BSI and we are using a single graph SGU . If DunkP1 were the only action
we would say that arm and clog can be reached at a cost of two, but in fact the cost is infinite
(since there is no DunkP2 to support arm from all possible worlds), and there is no strong plan.
To account for lack of support in all possible worlds and sharpen the heuristic estimate, a set of
multiple planning graphs  is considered. Each    is a single graph, as previously discussed.
These multiple graphs are similar to the graphs used by CGP (Smith & Weld, 1998), but lack the
more general cross-world mutexes. Mutexes are only computed within each graph, i.e. only sameworld mutexes are computed. We construct the initial layer L0 of each graph  with a different state
S  M(BSP ). With multiple graphs, the heuristic value of a belief state is computed in terms of
all the graphs. Unlike single graphs, we can compute different world aggregation measures with the
multiple planning graphs.
While we get a more informed heuristic by considering more of the states in M(BSP ), in
certain cases it can be costly to compute the full set of planning graphs and extract relaxed plans.
We will describe computing the full set of planning graphs, but will later evaluate (in Section 6.4)
the effect of computing a smaller proportion of these. The single graph SG1 is the extreme case of
computing fewer graphs.
To illustrate the use of multiple planning graphs, consider our example CBTC. We build two
graphs (Figure 6) for the projected BSP . They have the respective initial literal layers:
L10 = {arm, clog, inP1, inP2} and
L20 = {arm, clog, inP2, inP2}.

57

fiB RYCE , K AMBHAMPATI , & S MITH

L0

inP1

A0

E0

inP2

A1

L1

inP1

E1

inP2

inP2
DunkP1

1

1(DunkP1)
0(DunkP1)

arm
clog

1(DunkP2)

arm
Flush

0(Flush)

DunkP2

clog

Flush

 clog
inP1

inP1

inP2

inP2

L2

inP1

arm
arm

0(DunkP2)
0(Flush)

clog
 clog
inP1

DunkP1

1(DunkP1)
0(DunkP1)

2

inP2

arm
arm
clog

1(DunkP2)

arm
Flush

0(Flush)

DunkP2

clog

Flush

clog

0(DunkP2)
0(Flush)

arm
clog
clog

Figure 6: Multiple planning graphs for CBTC, with relaxed plan components bolded. Mutexes
omitted.

In the graph for the first possible world, arm comes in only through DunkP1 at level 2. In the
graph for the second world, arm comes in only through DunkP2 at level 2. Thus, the multiple
graphs show which actions in the different worlds contribute to support the same literal.
A single planning graph is sufficient if we do not aggregate state measures, so in the following we consider how to compute the achievement cost of a belief state with multiple graphs by
aggregating state distances.
Positive Interaction Aggregation: Similar to GPT (Bonet & Geffner, 2000), we can use the worstG
case world to represent the cost of the belief state BSi by using the hM
mRP heuristic. The difference
with GPT is that we compute a heuristic on planning graphs, where they compute plans in state
space. With this heuristic we account for the number of actions used in a given world, but assume
positive interaction across all possible worlds.
G
The hM
mRP heuristic is computed by finding a relaxed plan RP on each planning graph   ,
exactly as done on the single graph with hSG
RP . The difference is that unlike the single graph relaxed
plan SGU , but like SG1 , the initial levels of the planning graphs are states, so each relaxed plan
will reflect all the support needed in the world corresponding to . Formally:


b 1

RP
G

| Aj  |
hM
mRP (BSi ) = max


j=0

where b is the level of  where a constituent of BSG was first reachable.
58

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Notice that we are not computing all state distances between states in BSP and BSi . Each
planning graph  corresponds to a state in BSP , and from each  we extract a single relaxed plan.
We do not need to enumerate all states in BSi and find a relaxed plan for each. We instead support a
set of literals from one constituent of BSi . This constituent is estimated to be the minimum distance
state in BSi because it is the first constituent reached in .
G
For CBTC, computing hM
mRP (BSG ) (Figure 6) finds:
RP 1 =
1
= {inP1p , Flush},
ARP
0

E0RP1 = {0 (inP1p ), 0 (Flush)},
1
= {inP1, clog},
LRP
1
1
= {clogp , DunkP1},
ARP
1

E1RP1 = {0 (clogp ), 1 (DunkP1)},

1
= {arm, clog}
LRP
2

and RP 2 =
2
= {inP2p , Flush},
ARP
0

E0RP2 = {0 (inP2p ), 0 (Flush)},

2
= {inP2, clog},
LRP
1
2
= {clogp , DunkP2},
ARP
1

E1RP2 = {0 (clogp ), 1 (DunkP2)},

2
= {arm, clog}.
LRP
2

Each relaxed plan contains two actions and taking the maximum of the two relaxed plan values
G
gives hM
mRP (BSG ) = 2. This aggregation ignores the fact that we must use different Dunk actions
each possible world.
G
Independence Aggregation: We can use the hM
sRP heuristic to assume independence among the
worlds in our belief state. We extract relaxed plans exactly as described in the previous heuristic
and simply use a summation rather than maximization of the relaxed plan costs. Formally:


 1
 b
RP
G

| Aj  |
hM
sRP (BSi ) =


j=0

where b is the level of  where a constituent of BSG was first reachable.
G
MG
For CBTC, if computing hM
sRP (BSG ), we find the same relaxed plans as in the hmRP (BSG )
heuristic, but sum their values to get 2 + 2 = 4 as our heuristic. This aggregation ignores the fact
that we can use the same Flush action for both possible worlds.
State Overlap Aggregation: We notice that in the two previous heuristics we are either taking a
maximization and not accounting for some actions, or taking a summation and possibly accounting
G
for extra actions. We present the hM
RP U heuristic to balance the measure between positive interaction
and independence of worlds. Examining the relaxed plans computed by the two previous heuristics
for the CBTC example, we see that the relaxed plans extracted from each graph have some overlap.
1
2
Notice, that both ARP
and ARP
contain a Flush action irrespective of which package the bomb is
0
0
1
2
contains DunkP1, and ARP
contains DunkP2
in  showing some positive interaction. Also, ARP
1
1
59

fiB RYCE , K AMBHAMPATI , & S MITH

 showing some independence. If we take the layer-wise union of the two relaxed plans, we would
get a unioned relaxed plan:
RPU =
U
= {inP1p , Flush},
ARP
0

E0RPU = {0 (inP1p ), 0 (inP2p ), 0 (Flush)},
U
LRP
= {inP1, inP2, clog},
1
U
= {clogp , DunkP1, DunkP2},
ARP
1

E1RPU = {0 (clogp ), 1 (DunkP1), 1 (DunkP2)},
U
LRP
= {arm, clog}.
2

This relaxed plans accounts for the actions that are the same between possible worlds and the
actions that differ. Notice that Flush appears only once in layer zero and the Dunk actions both
appear in layer one.
In order to get the union of relaxed plans, we extract relaxed plans from each   , as in the
two previous heuristics. Then if we are computing heuristics for regression search, we start at the
last level (and repeat for each level) by taking the union of the sets of actions for each relaxed plan at
each level into another relaxed plan. The relaxed plans are end-aligned, hence the unioning of levels
proceeds from the last layer of each relaxed plan to create the last layer of the RPU relaxed plan,
then the second to last layer for each relaxed plan is unioned and so on. In progression search, the
relaxed plans are start-aligned to reflect that they all start at the same time, whereas in regression
we assume they all end at the same time. The summation of the number of actions of each action
level in the unioned relaxed plan is used as the heuristic value. Formally:
G
hM
RP U (BSi ) =

b1


U
| ARP
|
j

j=0

where b is the greatest level b where a constituent of BSG was first reachable.
For CBTC, we just found RPU , so counting the number of actions gives us a heuristic value of
G (BS ) = 3.
hM
G
RP U
4.4 Labelled Uncertainty Graph Heuristics (LU G)
The multiple graph technique has the advantage of heuristics that can aggregate the costs of multiple
worlds, but the disadvantage of computing some redundant information in different graphs (c.f.
G
Figure 6) and using every graph to compute heuristics (c.f hM
RP U ). Our next approach addresses
these limitations by condensing the multiple planning graphs to a single planning graph, called a
labelled uncertainty graph (LU G). The idea is to implicitly represent multiple planning graphs by
collapsing the graph connectivity into one planning graph, but use annotations, called labels (), to
retain information about multiple worlds. While we could construct the LU G by generating each
of the multiple graphs and taking their union, instead we define a direct construction procedure.
We start in a manner similar to the unioned single planning graph (SGU ) by constructing an initial
layer of all literals in our source belief state. The difference with the LU G is that we can prevent
loss of information about multiple worlds by keeping a label for each literal the records which
of the worlds is relevant. As we will discuss, we use a few simple techniques to propagate the
60

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

labels through actions and effects and label subsequent literal layers. Label propagation relies on
expressing labels as propositional formulas and using standard propositional logic operations. The
end product is a single planning graph with labels on all graph elements; labels indicate which of
the explicit multiple graphs (if we were to build them) contain each graph element.
We are trading planning graph structure space for label storage space. Our choice of BDDs to
represent labels helps lower the storage requirements on labels. The worst-case complexity of the
LU G is equivalent to the M G representation. The LU Gs complexity savings is not realized when
the projected possible worlds and the relevant actions for each are completely disjoint; however, this
does not often appear in practice. The space savings comes in two ways: (1) redundant representation of actions and literals is avoided, and (2) labels that facilitate non-redundant representation
are stored as BDDs. A nice feature of the BDD package (Brace, Rudell, & Bryant, 1990) we use
is that it efficiently represents many individual BDDs in a shared BDD that leverages common substructure. Hence, in practice the LU G contains the same information as M G with much lower
construction and usage costs.
In this section we present construction of the LU G without mutexes, then describe how to
introduce mutexes, and finally discuss how to extract relaxed plans.
4.4.1 L ABEL P ROPAGATION
Like the single graph and multiple graphs, the LU G is based on the IP P (Koehler et al., 1997)
planning graph. We extend the single graph to capture multiple world causal support, as present in
multiple graphs, by adding labels to the elements of the action A, effect E, and literal L layers. We
denote the label of a literal l in level k as k (l). We can build the LU G for any belief state BSP ,
and illustrate BSP = BSI for the CBTC example. A label is a formula describing a set of states (in
BSP ) from which a graph element is (optimistically) reachable. We say a literal l is reachable from
a set of states, described by BS, after k levels, if BS |= k (l). For instance, we can say that arm
is reachable after two levels if L2 contains arm and BSI |= 2 (arm), meaning that the models of
worlds where arm holds after two levels are a superset of the worlds in our current belief state.
The intuitive definition of the LU G is a planning graph skeleton, that represents causal relations,
over which we propagate labels to indicate specific possible world support. We show the skeleton
for CBTC in Figure 7. Constructing the graph skeleton largely follows traditional planning graph
semantics, and label propagation relies on a few simple rules. Each initial layer literal is labelled,
to indicate the worlds of BSP in which it holds, as the conjunction of the literal with BSP . An
action is labelled, to indicate all worlds where its execution preconditions can be co-achieved, as
the conjunction of the labels of its execution preconditions. An effect is labelled, to indicate all
worlds where its antecedent literals and its actions execution preconditions can be co-achieved, as
the conjunction of the labels of its antecedent literals and the label of its associated action. Finally,
literals are labelled, to indicate all worlds where they are given as an effect, as the disjunction over
all labels of effects in the previous level that affect the literal. In the following we describe label
propagation in more detail and work through the CBTC example.
Initial Literal Layer: The LU G has an initial layer consisting of every literal with a non false ()
label. In the initial layer the label 0 (l) of each literal l is identical to lBSP , representing the states
of BSP in which l holds. The labels for the initial layer literals are propagated through actions and
effects to label the next literal layer, as we will describe shortly. We continue propagation until no
label of any literal changes between layers, a condition referred to as level off.
61

fiB RYCE , K AMBHAMPATI , & S MITH

L0

A0

E0

L1

A1

E1

L2

inP1

inP1

inP1

: inP1

: inP1

: inP1

inP2

inP2

inP2

: inP2

: inP2

DunkP1

1(DunkP1)
0(DunkP1)

DunkP2

: inP2
: arm

1(DunkP2)
0(DunkP2)

arm

arm

arm

clog

clog

clog

Flush

0(Flush)

: clog

Flush

0(Flush)

: clog

G
Figure 7: The LU G skeleton for CBTC, with no mutexes. The relaxed plan for hLU
RP is shown in
bold.

The LU G for CBTC, shown in Figure 7 (without labels), using BSP =BSI has the initial literal
layer:
L0 = {inP1, inP2, inP2, inP1, clog, arm}
0 (inP1) = 0 (inP2) = (arm  clog  inP1  inP2),
0 (inP2) = 0 (inP1) = (arm  clog  inP1  inP2),
0 (clog) = 0 (arm) = BSP
Notice that inP1 and inP2 have labels indicating the respective initial states in which they hold,
and clog and arm have BSP as their label because they hold in all states in BSP .
Action Layer: Once the previous literal layer Lk is computed, we construct and label the action
layer Ak . Ak contains causative actions from the action set A, plus literal persistence. An action is
included in Ak if its label is not false (i.e. k (a) 
=). The label of an action at level k, is equivalent
to the extended label of its execution precondition:
k (a) = k (e (a))
Above, we introduce the notation for extended labels k (f ) of a formula f to denote the worlds
of BSP that can reach f at level k. We say that any propositional formula f is reachable from BS
62

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

after k levels if BSi |= k (f ). Since we only have labels for literals, we substitute the labels of
literals for the literals in a formula to get the extended label of the formula. The extended label of a
propositional formula f at level k, is defined:
k (f  f  ) = k (f )  k (f  ),
k (f  f  ) = k (f )  k (f  ),

k ((f  f  )) = k (f  f  ),
k ((f  f  )) = k (f  f  ),
k () = BSP ,
k () =,
k (l) = k (l)
The zeroth action layer for CBTC is:
A0 = {Flush, inP1p , inP2p , inP2p , inP1p , clogp , armp }
0 (Flush) = BSP ,
0 (inP1p ) = 0 (inP2p ) = (arm  clog  inP1  inP2),
0 (inP2p ) = 0 (inP1p ) = (arm  clog  inP1  inP2),
0 (clogp ) = 0 (armp ) = BSP
Each literal persistence has a label identical to the label of the corresponding literal from the
previous literal layer. The Flush action has BSP as its label because it is always applicable.
Effect Layer: The effect layer Ek depends both on the literal layer Lk and action layer Ak . Ek
contains an effect j (a) if the effect has a non false label (i.e. k (j (a)) 
=). Because both the
action and an effect must be applicable in the same world, the label of the effect at level k is the
conjunction of the label of the associated action with the extended label of the antecedent
k (j (a)) = k (a)  k (j (a))
The zeroth effect layer for CBTC is:
E0 = {0 (Flush), 0 (inP1p ), 0 (inP2p ), 0 (inP2p ),
0 (inP1p ), 0 (clogp ), 0 (armp )}
0 (0 (Flush)) = BSP
0 (0 (inP1p )) = 0 (0 (inP2p )) = (arm  clog  inP1  inP2),
0 (0 (inP2p )) = 0 (0 (inP1p )) = (arm  clog  inP1  inP2),
0 (0 (clogp )) = 0 (0 (armp )) = BSP
Again, like the action layer, the unconditional effect of each literal persistence has a label identical to the corresponding literal in the previous literal layer. The unconditional effect of Flush has
a label identical to the label of Flush.
Literal Layer: The literal layer Lk depends on the previous effect layer Ek1 , and contains only
literals with non false labels (i.e. k (l) 
=). An effect j (a)  Ek1 contributes to the label of a
literal l when the effect consequent contains the literal l. The label of a literal is the disjunction of
the labels of each effect from the previous effect layer that gives the literal:

k1 (j (a))
k (l) =
j (a):lj (a),
j (a)Ek1

63

fiB RYCE , K AMBHAMPATI , & S MITH

The first literal layer for CBTC is:
L1 = {inP1, inP2, inP2, inP1, clog, clog, arm}
1 (inP1) = 1 (inP2) = (arm  clog  inP1  inP2),
1 (inP2) = 1 (inP1) = (arm  clog  inP1  inP2),
1 (clog) = 1 (clog) = 1 (arm) = BSP
This literal layer is identical to the initial literal layer, except that clog goes from having a false
label (i.e. not existing in the layer) to having the label BSP .
We continue to the level one action layer because L1 does not indicate that BSG is reachable
from BSP (arm 
 L1 ). Action layer one is defined:
A1 = {DunkP1, DunkP2, Flush, inP1p , inP2p , inP2p , inP1p , clogp , armp , clogp }
1 (DunkP1) = 1 (DunkP2) = 1 (Flush) = BSP ,
1 (inP1p ) = 1 (inP2p ) = (arm  clog  inP1  inP2),
1 (inP2p ) = 1 (inP1p ) = (arm  clog  inP1  inP2),
1 (clogp ) = 1 (armp ) = 1 (clogp ) = BSP
This action layer is similar to the level zero action layer. It adds both Dunk actions because they
are now executable. We also add the persistence for clog. Each Dunk action gets a label identical
to its execution precondition label.
The level one effect layer is:
E1 = {0 (DunkP1), 0 (DunkP2), 1 (DunkP1), 1 (DunkP2), 0 (Flush), 0 (inP1p ),
0 (inP2p ), 0 (inP2p ), 0 (inP1p ), 0 (clogp ), 0 (armp ), 0 (clogp )}
1 (0 (DunkP1)) = 1 (0 (DunkP2)) = 1 (0 (Flush)) = BSP
1 (1 (DunkP1)) = (arm  clog  inP1  inP2),
1 (1 (DunkP2)) = (arm  clog  inP1  inP2),
1 (0 (inP2p )) = 1 (0 (inP1p )) = (arm  clog  inP1  inP2),
1 (0 (inP1p )) = 1 (0 (inP2p )) = (arm  clog  inP1  inP2),
1 (0 (clogp )) = 1 (0 (armp )) = 1 (0 (clogp )) = BSP
The conditional effects of the Dunk actions in CBTC (Figure 7) have labels that indicate the
possible worlds in which they will give arm because their antecedents do not hold in all possible
worlds. For example, the conditional effect 1 (DunkP1) has the label found by taking the conjunction of the actions label BSP with the antecedent label 1 (inP1) to obtain (arm  clog  inP1 
inP2).
Finally, the level two literal layer:
L2 = {inP1, inP2, inP2, inP1, clog, clog, arm, arm}
2 (inP1) = 2 (inP2) = (arm  clog  inP1  inP2),
2 (inP2) = 2 (inP1) = (arm  clog  inP1  inP2),
2 (clog) = 2 (clog) = 2 (arm) = 2 (arm) = BSP
The labels of the literals for level 2 of CBTC indicate that arm is reachable from BSP because its label is entailed by BSP . The label of arm is found by taking the disjunction of
the labels of effects that give it, namely, (arm  clog  inP1  inP2) from the conditional
64

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

effect of DunkP1 and (arm  clog  inP1  inP2) from the conditional effect of DunkP2,
which reduces to BSP . Construction could stop here because BSP entails the label of the goal
k (armclog)= k (arm)  k (clog) = BSP  BSP = BSP . However, level off occurs at
the next level because there is no change in the labels of the literals.
When level off occurs at level three in our example, we can say that for any BS, where BS |=
BSP , that a formula f is reachable in k steps if BS |= k (f ). If no such level k exists, then f is not
reachable from BS. If there is some level k, where f is reachable from BS, then the first such k is
a lower bound on the number of parallel plan steps needed to reach f from BS. This lower bound
is similar to the classical planning max heuristic (Nguyen et al., 2002). We can provide a more
informed heuristic by extracting a relaxed plan to support f with respect to BS, described shortly.
4.4.2 S AME -W ORLD L ABELLED M UTEXES
There are several types of mutexes that can be added to the LU G. To start with, we only concentrate
on those that can evolve in a single possible world because same-world mutexes are more effective
as well as relatively easy to understand. We extend the mutex propagation that was used in the
multiple graphs so that the mutexes are on one planning graph. The savings of computing mutexes
on the LU G instead of multiple graphs is that we can reduce computation when a mutex exits in
several worlds. In Appendix B we describe how to handle cross-world mutexes, despite their lack of
effectiveness in the experiments we conducted. Cross-world mutexes extend the LU G to compute
the same set of mutexes found by CGP (Smith & Weld, 1998).
Same-world mutexes can be represented with a single label, k (x1 , x2 ), between two elements
(actions, effect, or literals). The mutex holds between elements x1 and x2 in all worlds S where
S |= k (x1 , x2 ). If the elements are not mutex in any world, we can assume the label of a mutex
between them is false . We discuss how the labelled mutexes are discovered and propagated for
actions, effect relations, and literals.
By using mutexes, we can refine what it means for a formula f to be reachable from a set of
worlds BSP . We must ensure that for every state in BSP , there exists a state of f that is reachable.
A state S  of f is reachable from a state S of BSP when there are no two literals in S  that are
mutex in world S and BSP |= k (S).
In each of the action, effect, and literal layers there are multiple ways for the same pair of
elements to become mutex (e.g. interference or competing needs). Thus, the mutex label for a pair
is the disjunction of all labelled mutexes found for the pair by some means.
Action Mutexes: The same-world action mutexes at a level k are a set of labelled pairs of actions.
Each pair is labelled with a formula that indicates the set of possible worlds where the actions are
mutex. The possible reasons for mutex actions are interference and competing needs.

 Interference Two actions a, a interfere if (1) the unconditional effect consequent 0 (a) of
one is inconsistent with the execution precondition e (a ) of the other, or (2) vice versa.
They additionally interfere if (3) both unconditional effect consequents 0 (a) and 0 (a ) are
inconsistent, or (4) both execution preconditions e (a) and e (a ) are inconsistent. The mutex
will exist in all possible world projections k (a, a ) = BSP . Formally, a and a interfere if
65

fiB RYCE , K AMBHAMPATI , & S MITH

one of the following holds:
(1) 0 (a)  e (a ) =
(2) e (a)  0 (a ) =
(3) 0 (a)  0 (a ) =
(4) e (a)  e (a ) =
 Competing Needs Two actions a, a have competing needs in a world when a pair of literals
from their execution preconditions are mutex in the world. The worlds where a and a are
mutex because of competing needs are described by:


k (a)  k (a ) 

k (l, l )

lj (a),l j (a )

In the above formula we find all worlds where a pair of execution preconditions l  e (a), l 
e (a ) are mutex and both actions are reachable.
Effect Mutexes: The effect mutexes are a set of labelled pairs of effects. Each pair is labelled with
a formula that indicates the set of possible worlds where the effects are mutex. The possible reasons
for mutex effects are associated action mutexes, interference, competing needs, or induced effects.
 Mutex Actions Two effects i (a)  (a), j (a )  (a ) are mutex in all worlds where
their associated actions are mutex, k (a, a ).
 Interference Like actions, two effects i (a), j (a ) interfere if (1) the consequent i (a) of
one is inconsistent with the antecedent j (a ) of the other, or (2) vice versa. They additionally interfere if (3) both effect consequents i (a) and j (a ) are inconsistent, or (4) both
antecedents i (a) and j (a ) are inconsistent. The mutex will exist in all possible world projections, so the label of the mutex is k (i (a), j (a )) = BSP . Formally, i (a) and j (a )
interfere if one of the following holds:
(1) i (a)  j (a ) =
(2) i (a)  j (a ) =
(3) i (a)  j (a ) =
(4) i (a)  j (a ) =
 Competing Needs Like actions, two effects have competing needs in a world when a pair of
literals from their antecedents are mutex in a world. The worlds where i (a) and j (a ) have
a competing needs mutex are:
k (i (a))  k (j (a )) 



k (l, l )

li (a),l j (a )

In the above formula we find all worlds where a pair of execution preconditions l  i (a), l 
j (a ) are mutex and both actions are reachable.
66

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Lk

lk(p)
p

Ek

Ak

lk(a)
a

h(a)

lk(h(a))



lk(p, q)
Induced mutex in worlds:

lk(j(a),h(a))lk(i(a))



lk(j(a), h(a))

lk(q)
q
lk(a)
a

j(a)

lk(r)
r

lk(j(a))
i(a) induces j(a) in:
lk(i(a))lk(j(a))

i(a) lk(i(a))

Figure 8: Effect i (a) induces effect j (a). j (a) is mutex with h (a ), so i (a) is induced mutex
with h (a ).

 Induced An induced effect j (a) of an effect i (a) is an effect of the same action a that
may execute at the same time. An effect is induced by another in the possible worlds where
they are both reachable. For example, the conditional effect of an action always induces the
unconditional effect of the action.
Induced mutexes, involving the inducing effect i (a), come about when an induced effect
j (a) is mutex with another effect h (a ) (see Figure 8). The induced mutex is between
(a) the effect h (a ) that is mutex with the induced effect j (a) and (b) the inducing effect
i (a). The label of the mutex is the conjunction of the label of the mutex k (j (a), h (a ))
and the label of the induced effect j (a). For additional discussion of the methodology behind
induced mutexes we refer to Smith and Weld (1998).

Literal Mutexes: The literal mutexes are a set of labelled pairs of literals. Each pair is labelled with
a formula that indicates the set of possible worlds where the literals are mutex. The only reason for
mutex literals is inconsistent support.
 Inconsistent Support Two literals have inconsistent support in a possible world at level k
when there are no two non-mutex effects that support both literals in the world. The label of
the literal mutex at level k is a disjunction of all worlds where they have inconsistent support.
The worlds for an inconsistent support mutex between l and l are:
67

fiB RYCE , K AMBHAMPATI , & S MITH



S

S:i (a),j (a )E

k1 ,
where li (a),l j (a ),
S|=k1 (i (a),j (a ))

The meaning of the above formula is that the two literals are mutex in all worlds S where all
pairs of effects that support the literals in S are mutex in S.
4.4.3 LU G H EURISTICS
The heuristics computed on the LU G can capture measures similar to the M G heuristics, but there
exists a new opportunity to make use of labels to improve heuristic computation efficiency. A single
planning graph is sufficient if there is no state aggregation being measured, so we do not mention
such measures for the LU G.
Positive Interaction Aggregation: Unlike M G heuristics, we do not compute positive interaction
based relaxed plans on the LU G. The M G approach to measure positive interaction across each
state in a belief state is to compute multiple relaxed plans and take their maximum value. To get the
same measure on the LU G we would still need to extract multiple relaxed plans, the situation we are
trying to avoid by using the LU G. While the graph construction overhead may be lowered by using
the LU G, the heuristic computation could take too long. Hence, we do not compute relaxed plans
on the LU G to measure positive interaction alone, but we do compute relaxed plans that measure
overlap (which measures positive interaction).
Independence Aggregation: Like positive interaction aggregation, we need a relaxed plan for every
state in the projected belief state to find the summation of the costs. Hence, we do not compute
relaxed plans that assume independence.
G
State Overlap Aggregation: A relaxed plan extracted from the LU G to get the hLU
RP heuristic
M
G
M
G
resembles the unioned relaxed plan in the hRP U heuristic. Recall that the hRP U heuristic extracts
a relaxed plan from each of the multiple planning graphs (one for each possible world) and unions
the set of actions chosen at each level in each of the relaxed plans. The LU G relaxed plan heuristic
is similar in that it counts actions that have positive interaction in multiple worlds only once and
accounts for independent actions that are used in subsets of the possible worlds. The advantage of
G
hLU
RP is that we find these actions with a single pass on one planning graph.
We are trading the cost of computing multiple relaxed plans for the cost of manipulating LU G
labels to determine what lines of causal support are used in what worlds. In the relaxed plan we
want to support the goal with every state in BSP , but in doing so we need to track which states in
BSP use which paths in the planning graph. A subgoal may have several different (and possibly
overlapping) paths from the worlds in BSP .
RP
RP
RP
RP
RP
RP
A LU G relaxed plan is a set of layers: {ARP
0 , E0 , L1 , ..., Ab1 , Eb1 , Lb }, where Ar
RP
RP
is a set of actions, Er is a set of effects, and Lr+1 is a set of clauses. The elements of the layers
are labelled to indicate the worlds of BSP where they are chosen for support. The relaxed plan is
G
extracted from the level b = hLU
level (BSi ) (i.e., the first level where BSi is reachable, also described
in Appendix A).
Please note that we are extracting the relaxed plan for BSi in terms of clauses, and not literals, which is different than the SG and M G versions of relaxed plans. Previously we found the

68

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

constituent of BSi that was first reached on a planning graph and now we do not commit to any
one constituent. Our rationale is that we were possibly using different constituents in each of the
multiple graphs, and in this condensed version of the multiple graphs we still want to be able to
support different constituents of the BSi in different worlds. We could also use the constituent representation of BSi in defining the layers of the relaxed plan, but choose the clausal representation
of BSi instead because we know that we have to support each clause. However with constituents
we know we only need to support one (but we dont need to know which one).
The relaxed plan, shown in bold in Figure 7, for BSI to reach BSG in CBTC is listed as follows:
= {inP1p , inP2p , Flush},
ARP
0
RP
0 (inP1p ) = (arm  clog  inP1  inP2),
RP
0 (inP2p ) = (arm  clog  inP1  inP2),
RP
0 (Flush) = BSP ,
E0RP = {0 (inP1p ), 0 (inP2p ), 0 (Flush)},
0
RP
0 ( (inP1p )) = (arm  clog  inP1  inP2),
0
RP
0 ( (inP2p )) = (arm  clog  inP1  inP2),
RP
0 (0 (Flush)) = BSP ,
= {inP1, inP2, clog},
LRP
1
RP
1 (inP1) = (arm  clog  inP1  inP2),
RP
1 (inP2) = (arm  clog  inP1  inP2),
RP
1 (clog) = BSP ,
= {DunkP1, DunkP2, clogp },
ARP
1
RP
1 (DunkP1) = (arm  clog  inP1  inP2),
RP
1 (DunkP2) = (arm  clog  inP1  inP2),
RP
1 (clogp ) = BSP ,
E1RP = {1 (DunkP1), 1 (DunkP2), 0 (clogp )},
1
RP
1 ( (DunkP1)) = (arm  clog  inP1  inP2),
1
RP
1 ( (DunkP2)) = (arm  clog  inP1  inP2),
RP
1 (0 (clogp )) = BSP ,
= {arm, clog},
LRP
2
RP
2 (arm) = BSP ,
RP
2 (clog) = BSP
We start by forming LRP
with the clauses in (BSG ), namely arm and clog; we label the
2
clauses with BSP because they need to be supported by all states in our belief state. Next, we
support each clause in LRP
with the relevant effects from E1 to form E1RP . For clog we use
2
persistence because it supports clog in all worlds described by BSP (this is an example of positive
interaction of worlds). For arm the relevant effects are the respective 1 from each Dunk action.
We choose both effects to support arm because we need to support arm in all worlds of BSP , and
each effect gives support in only one world (this is an example of independence of worlds). We then
with the appropriate label indicating
insert the actions associated with each chosen effect into ARP
1
69

fiB RYCE , K AMBHAMPATI , & S MITH

the worlds where it was needed, which in general is fewer worlds than where it is reachable (i.e.
RP with the execution preconditions of
it is always the case that RP
r () |= r ()). Next we form L1
actions in ARP
and antecedents of effects in E1RP , which are clog, inP1, and inP2, labelled with
1
all worlds where an action or effect needed them. In the same fashion as level two, we support the
literals at level one, using persistence for inP1 and inP2, and Flush for clog. We stop here, because
we have supported all clauses at level one.
For the general case, extraction starts at the level b where BSi is first reachable from BSP .
RP
RP
RP contains all clauses
The first relaxed plan layers we construct are ARP
b1 , Eb1 , Lb , where Lb
RP
C  (BSi ), labelled as k (C) = BSP .
by choosing relevant effects from
For each level r, 1  r  b, we support each clause in LRP
r
RP . An effect j (a) is relevant if it is reachable in some of the worlds where we
Er1 to form Er1
need to support C (i.e. r1 (j (a))  RP
r (C) 
=) and the consequent gives a literal l  C. For
each clause, we have to choose enough supporting effects so that the chosen effect worlds are a
superset of the worlds we need to support the clause, formally:









RP
RP
j

(C)
|=

(
(a))
CLRP


r
r1
r
 j

 (a):lj (a),

lC,
j (a)Er1

We think of supporting a clause in a set of worlds as a set cover problem where effects cover
subsets of worlds. Our algorithm to cover the worlds of a clause with worlds of effects is a variant
of the well known greedy algorithm for set cover (Cormen, Leiserson, & Rivest, 1990). We first
choose all relevant persistence effects that can cover worlds, then choose action effects that cover
RP and labelled with the new
the most new worlds. Each effect we choose for support is added to Er1
RP
worlds it covered for C. Once all clauses in Lr are covered, we form the action layer ARP
r1 as all
RP . The actions in ARP are labelled to indicate all worlds where
actions that have an effect in Er1
r1
RP .
any of their effects were labelled in Er1
We obtain the next subgoal layer, LRP
r1 , by adding literals from the execution preconditions of
RP
RP . Each literal l  LRP is labelled to indicate all
actions in Ar1 and antecedents of effects in Er1
r1
worlds where any action or effect requires l. We support the literals in LRP
r1 in the same fashion
.
We
continue
to
support
literals
with
effects,
insert
actions,
and
insert action and effect
as LRP
r
RP
preconditions until we have supported all literals in L1 .
G
Once we get a relaxed plan, the relaxed plan heuristic, hLU
RP (BSi ), is the summation of the
number of actions in each action layer, formally:
G
hLU
RP (BSi )

=

b1


| ARP
|
i

i=0
G
Thus in our CBTC example we have hLU
RP (BSG ) = 3. Notice that if we construct the LU G
without mutexes for CBTC we reach the goal after two layers. If we had included mutexes the
LU G, then it would reach the goal after three layers. The way we use mutexes will not change our
relaxed plan because we do not use mutexes to influence relaxed plan extraction. Mutexes only help
to identify when a the belief state BSi is not reachable from BSP .

70

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Problem

PDDL Parser
(IPC)
Actions

Belief
States

Search Engine
(HSP-r: CAltAlt/
LAO*: POND)

Heuristics

BDDs
(CUDD)

Labels

(POND only)

Planning
Graph(s)
(IPP)

Figure 9: The implementations of CAltAlt and P ON D rely on many existing technologies. The
search engine is guided by heuristics extracted from planning graphs.

5. Empirical Evaluation: Setup
This section presents our implementation of the CAltAlt and P ON D planners and the domains we
use in the experiments. All tests were run in Linux on an x86 machine with a 2.66GHz P4 processor
and 1GB RAM with a timeout of 20 minutes. Both CAltAlt and P ON D used a heuristic weight
of five in the, respective, A* and AO* searches. We compare with the competing approaches (CGP,
SGP, GPT v1.40, MBP v0.91, KACMBP, YKA, and CFF) on several domains and problems. Our
planners and all domain and problem files for all of the compared planners can be found in the
online appendix.
5.1 Implementation
The implementation of CAltAlt uses several off-the-shelf planning software packages. Figure 9
shows a diagram of the system architecture for CAltAlt and P ON D. While CAltAlt extends the
name of AltAlt, it relies on a limited subset of the implementation. The components of CAltAlt
are the IPC parser for PDDL 2.1 (slightly extended to allow uncertain initial conditions), the HSPr search engine (Bonet & Geffner, 1999), the IPP planning graph (Koehler et al., 1997), and the
CUDD BDD package (Brace et al., 1990) to implement the LU G labels. The custom parts of the
implementation include the action representation, belief state representation, regression operator,
and the heuristic calculation.
The implementation of P ON D is very similar to CAltAlt aside from the search engine, and
state and action representation. P ON D also uses the IPP source code for planning graphs. P ON D
uses modified LAO* (Hansen & Zilberstein, 2001) source code from Eric Hansen to perform AO*
71

fiB RYCE , K AMBHAMPATI , & S MITH

Problem
Rovers1
Rovers2
Rovers3
Rovers4
Rovers5
Rovers6
Logistics1
Logistics2
Logistics3
Logistics4
Logistics5
BT(n)
BTC(n)
CubeCenter(n)
Ring(n)

Initial
States
1
2
3
4
16
12
2
4
2
4
8
n
n
n3
n3n

Goal
Literals
1
1
1
1
3
3
1
2
1
2
3
1
1
3
n

Fluents
66
66
66
66
71
119
29
36
58
68
78
n+1
n+2
3n
4n

Causative
Actions
88
88
88
88
97
217
70
106
282
396
510
n
n+1
6
4

Observational
Actions
0 {12}
0 {12}
0 {12}
0 {12}
0 {12}
0 {18}
0 {10}
0 {20}
0 {21}
0 {42}
0 {63}
0 {n}
0 {n}
0
0

Optimal
Parallel
5 {5}
8 {7}
10 {?}
13 {?}
? {?}
? {?}
6 {6}
6 {?}
8 {?}
8 {?}
? {?}
1 {1}
2n-1 {2}
(3n-3)/2
3n-1

Optimal
Serial
5 {5}
8 {7}
10 {8}
13 {10}
20 {?}
? {?}
9 {7}
15 {12}
11 {8}
18 {?}
28 {?}
n {n-1}
2n-1 {n-1}
(9n-3)/2
3n-1

Table 2: Features of test domains and problems - Number of initial states, Number of goal literals, Number of fluents, Number of causative actions, Number of Observational Actions,
Optimal number of parallel plan steps, Optimal number of serial plan steps. Data for conditional versions of domains is in braces; plan lengths for conditional plans are maximum
conditional branch length.

search, and CUDD (Brace et al., 1990) to represent belief states and actions. Even with deterministic
actions it is possible to obtain cycles from actions with observations because we are planning in
belief space. P ON D constructs the search graph as a directed acyclic graph by employing a cyclechecking algorithm. If adding a hyper-edge to the search graph creates a cycle, then the hyper-edge
cannot represent an action in a strong plan and is hence not added to the graph.
5.2 Domains
Table 2 shows some of the relative features of the different problems we used to evaluate our approach. The table shows the number of initial states, goal literals, fluents, actions, and optimal
plan lengths. This can be used as a guide to gauge the difficulty of the problems, as well as our
performance.
Conformant Problems In addition to the standard domains used in conformant planningsuch
as Bomb-in-the-Toilet, Ring, and Cube Center, we also developed two new domains Logistics and
Rovers. We chose these new domains because they have more difficult subgoals, and have many
plans of varying length.
The Ring domain involves a ring of n rooms where each room is connected to two adjacent
rooms. Each room has a window which can be open, closed, or locked. The goal is to have every
window locked. Initially, any state is possible  we could be in any room and each window could be
in any configuration. There are four actions: move right, move left, close the window in the current
72

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

room, and lock the window in the current room. Closing a window only works if the window is
open, and locking a window only works if the window is closed. A good conformant plan involves
moving in one direction closing and locking the window in each room.
The Cube Center domain involves a three-dimensional grid (cube) where there are six actions 
it is possible to move in two directions along each dimension. Each dimension consists of n possible
locations. Moving in a direction along which there are no more grid points leaves one in the same
position. Using this phenomena, it is possible to localize in each dimension by repeatedly moving
in the same direction. Initially it is possible to be at any location in the cube and the goal is to reach
the center. A good conformant plan involves localizing in a corner and then moving to the center.
The Rovers domain is a conformant adaptation of the analogous domain of the classical planning
track of the International Planning Competition (Long & Fox, 2003). The added uncertainty to
the initial state uses conditions that determine whether an image objective is visible from various
vantage points due to weather, and the availability of rock and soil samples. The goal is to upload an
image of an objective and some rock and soil sample data. Thus a conformant plan requires visiting
all of the possible vantage points and taking a picture, plus visiting all possible locations of soil and
rock samples to draw samples.
The first five Rovers problems have 4 waypoints. Problems one through four have one through
four locations, respectively, at which a desired imaging objective is possibly visible (at least one will
work, but we dont know which one). Problem 5 adds some rock and soil samples as part of the goal
and several waypoints where one of each can be obtained (again, we dont know which waypoint
will have the right sample). Problem 6 adds two more waypoints, keeps the same goals as Problem
5 and changes the possible locations of the rock and soil samples. In all cases the waypoints are
connected in a tree structure, as opposed to completely connected.
The Logistics domain is a conformant adaptation of the classical Logistics domain where trucks
and airplanes move packages. The uncertainty is the initial locations of packages. Thus, any actions
relating to the movement of packages have a conditional effect that is predicated on the package
actually being at a location. In the conformant version, the drivers and pilots cannot sense or communicate a packages actual whereabouts. The problems scale by adding packages and cities.
The Logistics problems consist of one airplane, and cities with an airport, a post office, and a
truck. The airplane can travel between airports and the trucks can travel within cities. The first
problem has two cities and one package that could start at either post office, and the goal is to get
the package to the second citys airport. The second problem adds another package at the same
possible starting points and having the same destination. The third problem has three cities with
one package that could be at any post office and has to reach the third airport. The fourth problem
adds a second package to the third problem with the same starting and ending locations. The fifth
problem has three cities and three packages, each at one of two of the three post offices and having
to reach different airports.
Conditional Problems For conditional planning we consider domains from the literature: Bombin-the-Toilet with sensing BTS, and Bomb-in-the-Toilet with clogging and sensing BTCS. We also
extend the conformant Logistics and Rovers to include sensory actions.
The Rovers problem allows for the rover, when it is at a particular waypoint, to sense the availability of image, soil, or rock data at that location. The locations of the collectable data are expressed
as one-of constraints, so the rover can deduce the locations of collectable data by failing to sense
the other possibilities.
73

fiB RYCE , K AMBHAMPATI , & S MITH

Logistics has observations to determine if a package at a location exists, and the observation is
assumed to be made by a driver or pilot at the particular location. Since there are several drivers and
a pilot, different agents make the observations. The information gained by the agents is assumed to
be automatically communicated to the others, as the planner is the agent that has all the knowledge.5

6. Empirical Evaluation: Inter-Heuristic Comparison
We start by comparing the heuristic approaches within our planners. In the next section, we continue
by describing how our planners, using the best heuristics, compare against other state of the art
approaches. In this section we intend to validate our claims that belief space heuristics that measure
overlap perform well across several domains. We further justify using the LU G over multiple
planning graphs and applying mutexes to improve heuristics in regression through pruning belief
states.
We compare many techniques within CAltAlt and P ON D on our conformant planning domains, and in addition we test the heuristics in P ON D on the conditional domains. Our performance metrics include the total planning time and the number of search nodes expanded. Additionally, when discussing mutexes we analyze planning graph construction time. We proceed by
showing how the heuristics perform in CAltAlt and then how various mutex computation schemes
for the LU G can affect performance. Then we present how P ON D performs with the different
heuristics in both conformant and conditional domains, explore the effect of sampling a proportion
of worlds to build SG1 , M G, and LU G graphs, and compare the heuristic estimates in P ON D
to the optimal plan length to gauge heuristic accuracy. We finish with a summary of important
conclusions.
We only compute mutexes in the planning graphs for CAltAlt because the planning graph(s) are
only built once in a search episode and mutexes help prune the inconsistent belief states encountered
in regression search. We abstain from computing mutexes in P ON D because in progression we
build new planning graphs for each search node and we want to keep graph computation time low.
With the exception of our discussion on sampling worlds to construct the planning graphs, the
planning graphs are constructed deterministically. This means that the single graph is the unioned
single graph SGU , and the M G and LU G graphs are built for all possible worlds.
6.1 CAltAlt
The results for CAltAlt in the conformant Rovers, Logistics, BT, and BTC domains, in terms of
total time and number of expanded search nodes, are presented in Table 3. We show the number of
expanded nodes because it gives an indication of how well a heuristic guides the planner. The total
time captures the amount of time computing the heuristic and searching. A high total time with a
high number of search nodes indicates a poor heuristic, and a high total time and low number of
search nodes indicates an expensive but informed heuristic.
We do not discuss the Ring and Cube Center domains for CAltAlt because it cannot solve
even the smallest instances. Due to implementation details the planner performs very poorly when
domains have actions with several conditional effects and hence does not scale. The trouble stems
5. This problem may be interesting to investigate in a multi-agent planning scenario, assuming no global communication
(e.g. no radio dispatcher).

74

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70

h0
2255/5
49426/8
TO
1108/9
TO
19/2
4837/10
TO
30/3
15021/19
TO
-

hcard
18687/14
TO
4268/9
TO
14/2
56/10
418/20
1698/30
5271/40
12859/50
26131/60
48081/70
82250/80
16/3
161/19
1052/39
3823/59
11285/79
26514/99
55687/119
125594/140

hSG
RP
543/5
78419/8
91672/10
TO
198/9
7722/15
3324/14
141094/19
TO
18/2
5158/10
TO
16/3
15679/19
TO
-

G
hM
mRP
542/5
8327/8
20162/10
61521/16
TO
183/9
15491/15
70882/14
TO
20/2
8988/10
TO
33/3
41805/19
TO
-

G
hM
RP U
185/5
29285/9
2244/11
3285/15
TO
1109/9
69818/19
TO
21/2
342/10
2299/20
9116/30
44741/40
TO
23/3
614/19
2652/39
9352/59
51859/79
TO
-

LU G(F X)

hRP
15164/5
32969/8
16668/10
31584/13
TO
1340/9
18535/15
16458/15
178068/19
TO
12/2
71/10
569/20
2517/30
7734/40
18389/50
37820/60
70538/70
188603/80
18/3
1470/19
51969/39
484878/59
TO
-

Table 3: Results for CAltAlt for conformant Rovers, Logistics, BT, and BTC. The data is Total
Time / # Expanded Nodes, TO indicates a time out (20 minutes) and - indicates no
attempt.

from a weak implementation for bringing general propositional formulas (obtained by regression
with several conditional effects) into CNF.
We describe the results from left to right in Table 3, comparing the different planning graph
structures and relaxed plans computed on each planning graph. We start with the non-planning
graph heuristics h0 and hcard . As expected, h0 , breadth-first search, does not perform well in a
large portion of the problems, shown by the large number of search nodes and inability to scale to
solve larger problems. We notice that with the hcard heuristic performance is very good in the BT
and BTC problems (this confirms the results originally seen by Bertoli, Cimatti, & Roveri, 2001a).
However, hcard does not perform as well in the Rovers and Logistics problems because the size of a
belief state, during planning, does not necessarily indicate that the belief state will be in a good plan.
Part of the reason hcard works so well in some domains is that it measures knowledge, and plans
for these domains are largely based on increasing knowledge. The reason hcard performs poorly
on other domains is that finding causal support (which it does not measure) is more important than
knowledge for these domains.
75

fiB RYCE , K AMBHAMPATI , & S MITH

Next, for a single planning graph (SGU ), CAltAlt does reasonably well with the hSG
RP heuristic in
the Rovers and Logistics domains, but fails to scale very well on the BT and BTC domains. Rovers
and Logistics have comparatively fewer initial worlds than the BT and BTC problems. Moreover
the deterministic plans, assuming each initial state is the real state, are somewhat similar for Rovers
and Logistics, but mostly independent for BT and BTC. Therefore, approximating a fully observable plan with the single graph relaxed plan is reasonable when plans for achieving the goal from
each world have high positive interaction. However, without high positive interaction the heuristic
degrades quickly when the number of initial worlds increases.
With multiple planning graphs, CAltAlt is able to perform better in the Rovers domain, but takes
quite a bit of time in the Logistics, BT, and BTC domains. In Rovers, capturing distance estimates
for individual worlds and aggregating them by some means tends to be better than aggregating
worlds and computing a single distance estimate (as in a single graph). In Logistics, part of the
reason computing multiple graphs is so costly is that we are computing mutexes on each of the
planning graphs. In BT and BTC, the total time increases quickly because the number of planning
graphs, and number of relaxed plans for every search node increase so much as problems get larger.
G
MG
Comparing the two multiple graph heuristics6 in CAltAlt namely hM
mRP and hRP U , we can
M
G
see the effect of our choices for state distance aggregation. The hmRP relaxed plan heuristic
aggregates state distances, as found on each planning graph, by taking the maximum distance. The
G
hM
RP U unions the relaxed plans from each graph, and counts the number of actions in the unioned
G
relaxed plan. As with the single graph relaxed plan, the hM
mRP relaxed plan essentially measures
one state to state distance; thus, performance suffers on the BT and BTC domains. However, using
the unioned relaxed plan heuristic, we capture the independence among the multiple worlds so that
we scale up better in BT and BTC. Despite the usefulness of the unioned relaxed plan, it is costly to
compute and scalability is limited, so we turn to the LU G version of this same measure.
LU G(F X)

With the LU G, we use the hRP
heuristic in CAltAlt. This heuristic uses a LU G with
G
full cross-world mutexes (denoted by F X). As in the similar hM
RP U heuristic, measuring overlap is
important, and improving the speed of computing the heuristic tends to improve the scalability of
CAltAlt. While CAltAlt is slower in the Rovers and BTC domains when using the LU G, we note
that it is because of the added cost of computing cross-world mutexes  we are able to improve the
speed by relaxing the mutexes, as we will describe shortly.
6.2 Mutexes
Mutexes are used to help determine when a belief state is unreachable. Mutexes improve the pruning
power of heuristics by accounting for negative interactions. The mutexes are only used to improve
our heuristics, so it is reasonable to compute only a subset of the mutexes. We would like to know
which mutexes are the most cost effective because the number of possible mutexes we can find is
quite large.
We can use several schemes to compute a subset of the mutexes. The schemes combine different
types of mutexes with types of cross-world checking. The mutex types are: computing no mutexes
(NX), computing only static interference mutexes (StX), computing (StX) plus inconsistent support and competing needs mutexes  dynamic mutexes (DyX), and computing (DyX) plus induced
mutexes  full mutexes (FX). The cross-world checking (see appendix B) reduction schemes are:
G
6. We show hM
sRP with P ON D.

76

fiProblem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70

LU G(N X)

hRP
13/1112/51
20/904/41
13/8704/384
TO
5/868/81
10/63699/1433
TO
1/34/2
4/72/10
19/452/20
62/1999/30
130/6130/40
248/14641/50
430/30140/60
680/55202/70
1143/135760/80
0/62/3
4/93/19
21/546/39
58/2311/59
133/6889/79
260/15942/99
435/32201/119
742/62192/139

LU G(StX)

hRP
19/1119/51
16/903/41
17/8972/384
TO
10/868/81
88/78448/1433
TO
0/13/2
4/56/10
22/448/20
59/1981/30
132/6170/40
255/14760/50
440/29891/60
693/55372/70
1253/140716/80
1/16/3
4/77/19
32/545/39
61/2293/59
149/6879/79
261/16452/99
443/32923/119
745/61827/139

LU G(DyX)

hRP
15453/89/6
13431/138/8
17545/185/10
32645/441/14
698575/3569/45
TO
1250/117/9
16394/622/15
17196/1075/15
136702/1035/19
TO
0/13/2
13/57/10
120/453/20
514/1999/30
1534/6432/40
3730/14711/50
7645/30127/60
15019/55417/70
26478/132603/80
0/15/3
14/78/19
139/553/39
543/2288/59
1564/6829/79
TO
-

LU G(F X)

hRP
15077/87/6
32822/147/8
16481/187/10
31293/291/14
TO
1242/98/9
18114/421/15
16085/373/15
176995/1073/19
TO
0/12/2
13/58/10
120/449/20
509/2008/30
1517/6217/40
3626/14763/50
7656/30164/60
14636/55902/70
26368/162235/80
4/14/3
1388/82/19
51412/557/39
482578/2300/59
TO
-

LU G(DyXSX)

hRP
15983/87/6
10318/139/8
10643/185/10
14988/291/14
61373/3497/45
217507/3544/37
791/116/9
2506/356/15
10407/403/15
24214/648/19
52036/2690/41
0/16/2
12/59/10
102/450/20
421/1994/30
1217/6326/40
2866/14707/50
5966/30017/60
11967/55723/70
21506/136149/80
0/16/3
13/76/19
105/546/39
427/2294/59
1211/6798/79
2890/16184/99
6045/32348/119
TO

LU G(DyXIX)

hRP
15457/87/6
10625/134/8
11098/209/10
16772/291/14
379230/3457/45
565013/3504/37
797/117/9
7087/428/15
10399/408/15
71964/871/19
328114/4668/52
0/15/2
14/59/10
139/454/20
600/2007/30
1822/6163/40
4480/14676/50
9552/30337/60
18475/55572/70
32221/105654/80
1/14/3
16/75/19
140/549/39
606/2300/59
1824/6816/79
4412/16414/99
9492/32350/119
TO

LU G(F XSX)

hRP
15098/86/6
10523/138/8
10700/191/10
14726/290/14
60985/3388/45
225213/3408/37
796/115/9
2499/352/15
10214/387/15
23792/642/19
52109/2672/41
0/25/2
13/59/10
105/444/20
413/1986/30
1196/6113/40
2905/14867/50
5933/30116/60
11558/55280/70
21053/139079/80
1/13/3
14/75/19
110/555/39
444/2287/59
1253/6830/79
2926/16028/99
6150/32876/119
TO

LU G(F XIX)

hRP
15094/85/6
14550/138/8
11023/184/10
16907/290/14
378869/3427/45
588336/3512/37
808/115/9
6968/401/15
10441/418/15
71099/858/19
324508/4194/52
0/13/2
14/56/10
137/454/20
596/2002/30
1797/6127/40
4392/14683/50
9234/29986/60
18081/55403/70
32693/109508/80
2/14/3
440/81/19
19447/568/39
199601/2401/59
1068019/6940/79
TO
-

P LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

G
Table 4: Results for CAltAlt using hLU
RP with mutex schemes. The data is Graph Construction
Time (ms)/All Other Time (ms)/# Expanded Nodes, TO indicates a time out (20 minutes)
and - indicates no attempt.

77

fiB RYCE , K AMBHAMPATI , & S MITH

computing mutexes across same-worlds (SX) and computing mutexes across pairs of worlds in the
intersection (conjunction) of element labels (IX).
Table 4 shows that within CAltAlt, using the relaxed plan heuristic and changing the way we
compute mutexes on the LU G can drastically alter performance. Often, the cross-world mutexes
are so numerous that building the LU G takes too much time. To see if we could reduce graph
G
construction overhead without hindering performance, we evaluated hLU
RP when the LUG is built
(a) considering all cross-world relations, for the schemes (NX), (StX), (DyX), and (FX); and (b)
same-world relations for the schemes (DyX-SX) and (FX-SX), and (c) cross-world relations for all
possible worlds pairs in the intersection of elements labels (DyX-IX) and (FX-IX).
The results show that simpler problems like BT and BTC do not benefit as much from advanced
computation of mutexes beyond static interference. However, for the Rovers and Logistics problems, advanced mutexes play a larger role. Mainly, interference, competing needs, and inconsistent
support mutexes are important. The competing needs and inconsistent support mutexes seem to
have a large impact on the informedness of the guidance given by the LU G, as scalability improves
most here. Induced mutexes dont improve search time much, and only add to graph computation
time. A possible reason induced mutexes dont help as much in these domains is that all the actions
have at most two effects, an unconditional and conditional effect. Reducing cross-world mutex
checking also helps quite a bit. It seems that only checking same-world mutexes is sufficient to
solve large problems. Interestingly, the M G graphs compute same-world interference, competing
needs, and inconsistent support mutexes within each graph, equating to the same scenario as (DyXSX), however, the LUG provides a much faster construction time, evidenced by the LU Gs ability
to out-scale M G.
6.3 P ON D
We show the total time and the number of expanded nodes for P ON D solving the conformant
problems (including Ring and Cube Center) in Table 5, and for P ON D solving the conditional
problems in Table 6. As with CAltAlt we show the total time and number of expanded nodes for
G
each test. We also add the hM
sRP heuristic, not implemented in CAltAlt, that takes the summation
of the values of relaxed plans extracted from multiple planning graphs. We do not compute mutexes
on any of the planning graphs used for heuristics in P ON D mainly because we build planning
graphs for each search node. We proceed by first commenting on the performance of P ON D, with
the different heuristics, in the conformant domains, then discuss the conditional domains.
In the conformant domains, P ON D generally does better than CAltAlt. This may be attributed
in part to implementation-level details. P ON D makes use of an existing (highly optimized) BDD
package for belief state generation in progression, but as previously mentioned, CAltAlt relies on a
less optimized implementation for belief state generation in regression. As we will see in the next
section, regression planners that employ a more sophisticated implementation perform much better,
but could still benefit from our heuristics. Aside from a few differences that we will mention, we see
similar trends in the performance of the various heuristics in both CAltAlt and P ON D. Namely,
the N G and SG heuristics have limited ability to help the planner scale, the M G heuristics help
the planner scale better but are costly, and the LU G provides the best scalability. The difference
between the M G and the LU G are especially pronounced in Cube Center and Ring, where the size
of the initial belief state is quite large as the instances scale. Interestingly in Ring, breadth first
search and the single graph relaxed plan are able to scale due to reduced heuristic computation time
78

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70
CubeCenter 3
5
7
9
11
13
Ring 2
3
4
5
6
7
8
9
10

h0
540/36
940/249
3340/1150
TO
560/169
TO
450/3
760/1023
TO
460/5
1090/2045
TO
10/184
180/3198
1940/21703
TO
20/15
20/59
30/232
160/973
880/4057
5940/16299
39120/64657
251370/261394
TO

hcard
520/21
790/157
2340/755
14830/4067
TO
530/102
TO
460/2
590/428
TO
460/4
970/1806
TO
30/14
20/58
40/203
70/363
230/1010
700/2594
20/7
20/11
20/15
20/19
30/23
40/27
40/31
50/35
70/39

hSG
RP
590/6
700/15
3150/230
13480/1004
TO
680/46
TO
460/3
1560/1023
TO
450/5
3160/2045
TO
90/34
3510/1342
46620/10316
333330/46881
TO
30/15
70/59
350/232
2270/973
14250/4057
83360/16299
510850/64657
TO
-

G
hM
mRP
580/6
1250/32
3430/77
10630/181
85370/452
180890/416
970/58
2520/32
27820/927
5740/27
42980/59
450/2
6200/428
TO
460/4
18250/1806
TO
1050/61
60460/382
TO
80/8
1500/41
51310/77
TO
-

G
hM
sRP
580/6
750/10
1450/24
7000/163
12470/99
15780/38
730/21
6420/105
4050/83
29180/211
51380/152
450/2
820/10
6740/20
41320/30
179930/40
726930/50
TO
460/3
980/19
TO
370/9
11060/55
852630/359
TO
80/7
500/8
6370/11
283780/16
TO
-

G
hM
RP U
580/6
830/13
1370/23
2170/34
31480/73
31950/73
650/9
2310/20
2000/15
53470/382
471850/988
500/2
880/10
6870/20
44260/30
183930/40
758140/50
TO
470/3
990/19
9180/39
54140/59
251140/79
1075250/99
TO
0430/11
14780/82
1183220/444
TO
80/8
920/19
19300/40
TO
-

G
hLU
RP
590/6
680/11
850/16
1130/28
2050/36
9850/147
560/9
910/15
1130/14
3180/46
6010/42
460/2
520/10
1230/20
4080/30
11680/40
28420/50
59420/60
113110/70
202550/80
460/3
540/19
1460/39
4830/59
14250/79
34220/99
71650/119
134880/139
70/11
1780/205
27900/1774
177790/7226
609540/17027
TO
30/8
70/10
250/24
970/44
4080/98
75020/574
388300/902
TO
-

Table 5: Results for P ON D for conformant Rovers, Logistics, BT, BTC, Cube Center, and Ring.
The data is Total Time (ms)/# Expanded Nodes, TO indicates a time out and - indicates
no attempt.

and the low branching factor in search. The LU G is able to provide good search guidance, but tends
to take a long time computing heuristics in Ring.
We are also now able to compare the choices for aggregating the distance measures from reG
laxed plans for multiple graphs. We see that taking the maximum of the relaxed plans, hM
mRP , in
assuming positive interaction among worlds is useful in Logistics and Rovers, but loses the independence of worlds in the BT and BTC domains. However, taking the summation of the relaxed plan
79

fiB RYCE , K AMBHAMPATI , & S MITH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70

h0
550/36
1030/262
1700/467
5230/1321
TO
530/118
TO
460/5
TO
450/6
TO
-

hcard
480/21
550/36
590/48
620/58
TO
TO
460/3
470/19
510/39
620/59
850/79
1310/99
2240/119
24230/139
45270/159
460/3
480/19
510/39
660/59
970/79
1860/99
4010/119
7580/139

hSG
RP
580/6
780/15
3930/248
6760/387
TO
740/46
TO
450/3
111260/7197
TO
470/5
271410/10842
TO
-

G
hM
mRP
570/6
760/14
830/15
1020/20
16360/175
31870/173
580/10
1630/30
1360/20
4230/59
27370/183
460/3
970/19
9070/39
52410/59
207890/79
726490/99
TO
470/3
1150/19
11520/39
62060/59
251850/79
941220/99
TO
-

G
hM
sRP
570/6
710/12
830/15
1040/21
11100/232
24840/159
570/10
1300/36
1250/19
3820/57
19620/178
450/3
970/19
9060/39
52210/59
206830/79
719000/99
TO
460/3
1140/19
TO
-

G
hM
RP U
580/6
730/12
910/17
1070/21
12810/209
30250/198
600/10
1360/36
1290/19
3940/57
20040/178
470/3
1020/19
9380/39
55750/59
233720/79
TO
470/3
1200/19
11610/39
64290/59
274610/79
TO
-

G
hLU
RP
580/6
730/13
810/16
910/21
7100/174
13560/174
570/10
1250/36
1210/19
4160/57
20170/178
460/3
550/19
1610/39
5970/59
17620/79
43020/99
91990/119
170510/139
309940/159
470/3
590/19
1960/39
6910/59
19830/79
49080/99
103480/119
202040/139

Table 6: Results for P ON D for conditional Rovers, Logistics, BTS, BTCS. The data is Total Time
(ms)/# Expanded Nodes, TO indicates a time out (20 minutes) and - indicates no
attempt.

G
values for different worlds, hM
sRP is able to capture the independence in the BT domain. We notice
that the summation does not help P ON D in the BTC domain; this is because we overestimate the
heuristic value for some nodes by counting the Flush action once for each world when it in fact
G
only needs to be done once (i.e. we miss positive interaction). Finally, using the hM
RP U heuristic
we do well in every domain, aside from the cost of computing multiple graph heuristics, because
we account for both positive interaction and independence by taking the overlap of relaxed plans.
Again, with the LU G relaxed plan, analogous to the multiple graph unioned relaxed plan, P ON D
scales well because we measure overlap and lower the cost of computing the heuristic significantly.

The main change we see in using P ON D versus CAltAlt is that the direction of search is
different, so the hcard heuristic performs unlike before. In the BT and BTC domains cardinality
does not work well in progression because the size of belief states does not change as we get closer
to the goal (it is impossible to ever know which package contains the bomb). However, in regression
we start with a belief state containing all states consistent with the goal and regressing actions limits
80

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

our belief state to only those states that can reach the goal through those actions. Thus in regression
the size of belief states decreases, but in progression remains constant.
The performance of P ON D in the conditional domains exhibits similar trends to the conformant domains, with a few exceptions. Like the conformant domains, the M G relaxed plans tend to
outperform the SG relaxed plan, but the LU G relaxed plan does best overall. Unlike the conformant
G
domains, The hM
mRP performs much better in BTS and BTCS over BT and BTC partly because
the conditional plans have a lower average cost. The hcard heuristic does better in BTS and BTCS
over BT and BTC because the belief states actually decrease in size when they are partitioned by
sensory actions.
6.4 Sampling Worlds
Our evaluations to this point have considered the effectiveness of different heuristics, each computed with respect to all possible worlds of a belief state. While we would like to use as many of
the possible worlds as we can, we can reduce computation cost and hopefully still get reasonable
heuristics by considering a subset of the worlds. Our scheme for considering subsets of worlds in
the heuristics is to sample a single world (SG1 ), or sample a given percentage of the worlds and
build multiple graphs, or the LU G.
MG
LU G
With these sampling approaches, we use the hSG
RP , hRP U , and hRP relaxed plans. We build
the M G and LU G for 10%, 30%, 50%, 70%, and 90% of the worlds in each belief state, sampled
randomly. In Figure 10, we show the total time taken (ms) to solve every problem in our test set
(79 problems over 10 domains). Each unsolved problem contributed 20 minutes to the total time.
For comparison we show the previously mentioned heuristics: hSG
RP computed on a unioned single
U
graph SG , denoted as Unioned compared to the sampled single graph SG1 denoted as Single,
G
LU G
and hM
RP U and hRP computed for all worlds denoted as 100%. The total time for any heuristic
that samples worlds is averaged over ten runs.
There are two major points to see in Figure 10. First, the hSG
RP heuristic is much more effective
1
U
when computed on SG versus SG . This is because the SG1 is less optimistic. It builds a
planning graph for a real world state, as opposed to the union of literals in all possible world states,
as in SGU . Respecting state boundaries and considering only a single state is better than ignoring
state boundaries to naively consider all possible states. However, as we have seen with the M G
and LU G heuristics, respecting state boundaries and considering several states can be much better,
bringing us to the second point.
We see very different performance when using more possible worlds to build multiple graphs
compared to the LU G. We are better off using fewer worlds if we have to build multiple graphs
because they can become very costly as the number of worlds increases. In contrast, performance
improves with more possible worlds when we use the LU G. Using more possible worlds to compute
heuristics is a good idea, but it takes a more efficient substrate to exploit them.
6.5 Accuracy
The heuristics that account for overlap in the possible worlds should be more accurate than the
heuristics that make an assumption of full positive interaction or full independence. To check our
intuitions, we compare the heuristic estimates for the distance between the initial belief state and
the goal belief state for all the heuristics used in conformant problems solved by P ON D. Figure
11 shows the ratio of the heuristic estimate for h(BSI ) to the optimal serial plan length h (BSI ) in
81

fiB RYCE , K AMBHAMPATI , & S MITH

SG
MG
LUG

16

14

12

10

8

6

4

2

0
Unioned

Single

10%

30%

50%

70%

90%

100%

Figure 10: Total Time (hours) for P ON D to solve all conformant and conditional problems when
sampling worlds to use in heuristic computation.

several problems. The points below the line (where the ratio is one) are under-estimates, and those
above are over-estimates. Some of the problem instances are not shown because no optimal plan
length is known.
G
MG

We note that in all the domains the hLU
RP and hRP U heuristics are very close to h , confirming
M
G
M
G

our intuitions. Interestingly, hsRP and hmRP are both close to h in Rovers and Logistics;
whereas the former is close in the BT and BTC problems, and the latter is close in CubeCenter
and Ring. As expected, assuming independence (using summation) tends to over-estimate, and
assuming positive interaction (using maximization) tends to under-estimate. The hSG
RP heuristic
tends to under-estimate, and in some cases (CubeCenter and Ring) gives a value of zero (because
there is an initial state that satisfies the goal). The hcard heuristic is only accurate in BT and BTC,
under-estimates in Rovers and Logistics, and over-estimates in Cube Center and Ring.

The accuracy of heuristics is in some cases disconnected from their run time performance. For
instance hcard highly overestimates in Ring and Cube Center, but does well because the domains
G
MG
exhibit special structure and the heuristic is fast to compute. On the other hand, hLU
RP and hRP U
82

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

10000
1000
100
10
1
0.1
0.01
0.001

3
3
3
3

3
 33
3
3
3
3


3
3


3


 

 



 


+





33333333
2
2

2

22
2
2++
   
2

2
2






+
 

33
22
222 ++
33333
+
33
+
3
33
2
33 + 3 3
+
2
+
+
2
2+
2+
+
2+
2+
2+
2+
2+
2+
2
2+
2+
2+
2
hcard 3
hSG
RP +
M
G
hmRP
2
M
G
hsRP 
G
hM
RP U 
G 
hLU
RP

Rv1 Rv4 L1

L5 B10

B80 BC10
BC70 C3
Problem

C13 R2

R10

Figure 11: Ratio of heuristic estimates for distance between BSI and BSG to optimal plan length.
Rv = Rovers, L = Logistics, B = BT, BC = BTC, C = Cube Center, R = Ring.

are very accurate in many domains, but suffer in Ring and Cube Center because they can be costly
to compute.
6.6 Inter-Heuristic Conclusions
Our findings fall into two main categories: one, what are effective estimates for belief state distances
in terms of state to state distances, and two, how we can exploit planning graphs to support the
computation of these distance measures.
In comparing ways to aggregate state distance measures to compute belief state distances, we
found that measuring no interaction as in single graph heuristics tends to poorly guide planners,
measuring independence and positive interaction of worlds works well in specific domains, and
measuring overlap (i.e. a combination of positive interaction and independence) tends to work well
in a large variety of instances. By studying the accuracy of our heuristics we found that in some
cases the most accurate were not the most effective. We did however find that the most accurate did
best over the most cases.
Comparing graph structures that provide the basis for belief state distance measures, we found
that the heuristics extracted from the single graph fail to systematically account for the independence or positive interaction among different possible worlds. Despite this lack in the distance
measure, single graphs can still identify some structure in domains like Rovers and Logistics. To
more accurately reflect belief state distances, multiple graphs reason about reachability for each
world independently. This accuracy comes at the cost of computing a lot of redundant M G structure and is limiting in instances with large belief states. We can reduce the cost of the M G structure
83

fiB RYCE , K AMBHAMPATI , & S MITH

Planner
CAltAlt
P ON D
MBP
KACMBP
CGP
SGP
GPT
YKA
CFF

Search Space
Belief Space
Belief Space
Belief Space
Belief Space
Planning Graph
Planning Graph
Belief Space
Belief Space
Belief Space

Search Direction
Backward
Forward
Forward/Backward
Forward
Backward
Backward
Forward
Backward
Forward

Conditional






Heuristic
Planning Graph
Planning Graph
Cardinality
Cardinality
Planning Graph
Planning Graph
State Space Plans
Cardinality
Planning Graph

Implementation
C
C
C
C
Lisp
Lisp
C
C
C

Table 7: Comparison of planner features.
by sampling worlds used in its construction. However planners are able to exhibit better scalability
by considering more worlds through optimizing the representation of the redundant structure as in
the LU G. The improvement in scalability is attributed to lowering the cost of heuristic computation, but retaining measures of multiple state distances. The LU G makes a trade-off of using an
exponential time algorithm for evaluation of labels instead of building an exponential number of
planning graphs. This trade-off is justified by our experiments.

7. Empirical Evaluation: Inter-Planner Comparison
We first compare CAltAlt and P ON D with several planners on our conformant domains, then
compare P ON D with the conditional planners on the conditional domains. Our purpose in this
section is to identify the advantages of our techniques over the state of the art planners. We end the
section with a discussion of general conclusions drawn from the evaluation.
7.1 Conformant Planning
Although this work is aimed at giving a general comparison of heuristics for belief space planning,
we also present a comparison of the best heuristics within CAltAlt and P ON D to some of the
other leading approaches to conformant planning. Table 7 lists several features of the evaluated
planners, such as their search space, their search direction, whether they are conditional, the type of
heuristics, and the implementation language. Note, since each approach uses a different planning
representation (BDDs, GraphPlan, etc.), not all of which even use heuristics, it is hard to get a
standardized comparison of heuristic effectiveness. Furthermore, not all of the planners use PDDLlike input syntax; MBP, and KACMBP use AR encodings which may give them an advantage in
reducing the number of literals and actions. We gave the MBP planners the same grounded and
filtered action descriptions that we used in CAltAlt and P ON D. We also tried, but do not report
results, giving the MBP planners the full set of ground actions without filtering irrelevant actions. It
appears that the MBP planners do not use any sort of action pre-processing because performance was
much worse with the full grounded set of actions. Nevertheless, Table 8 compares MBP, KACMBP,
LU G(DyXSX)
G
GPT, CGP, YKA, and CFF with hRP
in CAltAlt and hLU
RP in P ON D with respect to
run time and plan length.
MBP: The MBP planner uses a cardinality heuristic that in many cases overestimates plan distances
(as per our implementation with hcard ). MBP uses regression search for conformant plans, but
progression search for conditional plans. It is interesting to note that in the more difficult problem
84

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70
CubeCenter 3
5
7
9
11
Ring 2
3
4
5
6
7
8

CAltAlt
LU G(DyXSX)
hRP U
16070/5
10457/8
10828/10
15279/13
64870/29
221051/25
907/9
2862/15
10810/15
24862/19
54726/34
16/2
71/10
552/20
2415/30
7543/40
17573/50
35983/60
67690/70
157655/80
16/3
89/19
651/39
2721/59
8009/79
19074/99
38393/119
65448/139
TO
TO
-

POND
G
hLU
RP
590/5
680/9
850/11
1130/16
2050/25
8370/25
560/9
910/15
1130/14
3180/22
6010/29
460/2
520/10
1230/20
4080/30
11680/40
28420/50
59420/60
113110/70
202550/80
460/3
540/19
1460/39
4820/59
14250/79
34220/99
71650/119
134880/139
70/9
1780/18
27900/29
177790/36
609540/47
30/6
70/8
250/13
970/17
4080/22
75020/30
388300/29

MBP

KACMBP

GPT

CGP

YKA

CFF

66/5
141/8
484/10
3252/13
OoM
727/32
37/9
486/24
408/14
2881/27
OoM
6/2
119/10
80/20
170/30
160/40
300/50
480/60
730/70
1080/80
8/3
504/19
98/39
268/59
615/79
1287/99
2223/119
3625/139
10/9
16/18
35/27
64/36
130/45
0/5
0/8
10/11
20/14
30/17
80/20
160/23

9293/5
9289/15
9293/16
9371/18
39773/40
TO
127/12
451/19
1578/18
8865/22
226986/42
10/2
16/10
84/20
244/30
533/40
1090/50
2123/60
3529/70
1090/80
18/3
45/19
211/39
635/59
1498/79
10821/99
5506/119
2640/139
20/9
20/18
70/27
120/36
230/45
0/5
40/8
30/11
50/14
120/18
230/21
600/24

3139/5
4365/8
5842/10
7393/13
399525/20
TO
916/9
1297/15
1711/11
9828/18
543865/28
487/2
627/10
472174/20
TO
465/3
715/19
40/9
363/18
4782/27
42258/36
26549/45
31/5
35/8
60/11
635/14
51678/17
TO
-

70/5
180/8
460/10
1860/13
OoM
60/6
290/6
400/8
1170/8
TO
20/1
520/1
3200/1
10330/1
24630/1
49329/1
87970/1
145270/1
TO
0/3
39370/19
28990/3
TO
TO
-

1220/7
2050/10
1740/12
2010/16
7490/27
24370/26
250/13
670/19
20280/21
17530/27
141910/40
0/2
0/10
20/20
80/30
160/40
250/50
420/60
620/70
3310/80
10/3
30/19
240/39
1210/59
3410/79
8060/50
15370/119
27400/139
0/9
0/19
20/34
80/69
190/68
0/5
0/8
20/11
80/14
110/17
300/20
480/23

70/5
30/8
10/10
10/13
18/22
21/23
10/9
12/15
14/12
12/18
25/28
0/2
30/10
4400/20
4500/30
26120/40
84730/50
233410/60
522120/70
979400/80
10/3
57/19
2039/39
23629/59
116156/79
334879/99
TO
20/15
28540/45
TO
360/12
TO
-

LU G(DyXSX)

G
Table 8: Results for CAltAlt using hRP
, P ON D using hLU
RP , MBP, KACMBP, GPT,
CGP, YKA, and CFF for conformant Rovers, Logistics, BT, BTC, Cube Center, and Ring.
The data is Total Time / # Plan Steps, TO indicates a time out (20 minutes), OoM
indicates out of memory (1GB), and - indicates no attempt.

instances in the Rovers and Logistics domains MBP and KACMBP tend to generate much longer
plans than the other planners. MBP does outperform P ON D in some cases but does not find
solutions in certain instances (like Rovers 5), most likely because of its heuristic. We note that
KACMBP and MBP are quite fast on the Cube Center and Ring domains, but have more trouble on
domains like Rovers and Logistics. This illustrates how a heuristic modeling knowledge as opposed
to reachability can do well in domains where the challenge is uncertainty not reachability.
85

fiB RYCE , K AMBHAMPATI , & S MITH

Optimal Planners: The optimal approaches (CGP and GPT) tend not to scale as well, despite their
good solutions. CGP has trouble constructing its planning graphs as the parallel conformant plan
depth increases. CGP spends quite a bit of time computing mutexes, which increases the planning
cost as plan lengths increase. CGP does much better on shallow and parallel domains like BT, where
it can find one step plans that dunk every package in parallel.
GPT performs progression search that is guided by a heuristic that measures the cost of fully
observable plans in state space. GPT finds optimal serial plans but is not as effective when the size
of the search space increases. GPT fails to scale with the search space because it becomes difficult
to even compute its heuristic (due to a larger state space as well).
YKA: YKA, like CAltAlt is a regression planner, but the search engine is very different and YKA
uses a cardinality heuristic. YKA performs well on all the domains because of its search engine
based on BDDs. We notice a difference in progression and regression by comparing P ON D to
YKA, similar to trends found in the comparison between P ON D and CAltAlt. Additionally, it
seems YKA has a stronger regression search engine than CAltAlt. P ON D is able to do better than
YKA in the Rovers and Logistics domains, but it is unclear whether that it is because of the search
direction or heuristics.
CFF: Conformant FF, a progression planner using a relaxed plan similar to the LU G relaxed plan,
does very well in the Rovers and Logistics domains because it uses the highly optimized FF search
engine as well as a cheap to compute relaxed plan heuristic. However, CFF does not do as well in
the BT, BTC, Cube Center, and Ring problems because there are not as many literals that will be
entailed by a belief state. CFF relies on implicitly representing belief states in terms of the literals
that are entailed by the belief state, the initial belief state, and the action history. When there are
very few literals that can be entailed by the belief state, reasoning about the belief state requires
inference about the action history. Another possible reason CFF suffers is our encodings. The
Cube Center and Ring domains are naturally expressed with multi-valued state features, and in our
transformation to binary state features we describe the values that must hold but also the values that
must not hold. This is difficult for CFF because the conditional effect antecedents contain several
literals and its heuristic is restricted to considering only one such literal. It may be that CFF is
choosing the wrong literal or simply not enough literals to get effective heuristics. However in BT
and BTC where we used only one literal in effect antecedents CFF still performs poorly.
7.2 Conditional Planning
Table 9 shows the results for testing the conditional versions of the domains on P ON D, MBP, GPT,
SGP, and YKA.
MBP: The P ON D planner is very similar to MBP in that it uses progression search. P ON D
uses an AO* search, whereas the MBP binary we used uses a depth first And-Or search. The depth
first search used by MBP contributes to highly sub-optimal maximum length branches (as much
as an order of magnitude longer than P ON D). For instance, the plans generated by MBP for
the Rovers domain have the rover navigating back and forth between locations several times before
doing anything useful; this is not a situation beneficial for actual mission use. MBP tends to not scale
as well as P ON D in all of the domains we tested. A possible reason for the performance of MBP
is that the Logistics and Rovers domains have sensory actions with execution preconditions, which
prevent branching early and finding deterministic plan segments for each branch. We experimented
86

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70

POND
G
hLU
RP
580/5
730/8
810/8
910/10
7100/19
13560/22
570/7
1250/12
1210/9
4160/15
20170/22
460/2
550/10
1610/20
5970/30
17620/40
43020/50
91990/60
170510/70
309940/80
470/2
590/10
1960/20
6910/30
19830/40
49080/50
103480/60
202040/70

MBP

GPT

SGP

YKA

3312/11
4713/75
5500/119
5674/146
16301/76
OoM
41/16
22660/177
2120/45
OoM
0/2
240/10
OoM
20/2
280/10
OoM
-

3148/5
5334/7
7434/8
11430/10
TO
1023/7
5348/12
2010/8
TO
510/2
155314/10
OoM
529/2
213277/10
TO
-

70/5
760/7
TO
5490/6
TO
0/1
70/1
950/1
4470/1
13420/1
32160/1
90407/1
120010/1
TO
10/2
TO
-

3210/5
6400/7
7490/8
11210/10
TO
1390/8
TO
TO
0/2
20/10
60/20
200/30
400/40
810/50
1350/60
2210/70
3290/80
0/4
210/12
2540/22
13880/32
46160/42
109620/52
221460/62
41374/72

G
Table 9: Results for P ON D using hLU
RP , MBP, GPT, SGP, and YKA for conditional Rovers, Logistics, BT, and BTC. The data is Total Time / # Maximum possible steps in a execution,
TO indicates a time out (20 minutes), OoM indicates out of memory (1GB), and -
indicates no attempt.

with MBP using sensory actions without execution preconditions and it was able to scale somewhat
better, but plan quality was much longer.
Optimal Planners: GPT and SGP generate better solutions but very slowly. GPT does better on
the Rovers and Logistics problems because they exhibit some positive interaction in the plans, but
SGP does well on BT because its planning graph search is well suited for shallow, yet broad (highly
parallel) problems.
YKA: We see that YKA fares similar to GPT in Rovers and Logistics, but has trouble scaling for
other reasons. We think that YKA may be having trouble in regression because of sensory actions
since it was able to scale reasonably well in the conformant version of the domains. Despite this,
YKA proves to do very well in the BT and BTC problems.
87

fiB RYCE , K AMBHAMPATI , & S MITH

7.3 Empirical Evaluation Conclusions
In our internal comparisons of heuristics within CAltAlt and P ON D, as well as external comparisons with several state of the art conformant and conditional planners we have learned many
interesting lessons about heuristics for planning in belief space.
 Distance based heuristics for belief space search help control conformant and conditional plan
length because, as opposed to cardinality, the heuristics model desirable plan quality metrics.
 Planning graph heuristics for belief space search scale better than planning graph search and
admissible heuristic search techniques.
 Of the planning graph heuristics presented, relaxed plans that take into account the overlap
of individual plans between states of the source and destination belief states are the most
accurate and tend to perform well across many domains.
 The LUG is an effective planning graph for both regression and progression search heuristics.
 In regression search, planning graphs that maintain only same-world mutexes provide the best
trade-off between graph construction cost and heuristic informedness.
 Sampling possible worlds to construct planning graphs does reduce computational cost, but
considering more worlds by exploiting planning graph structure common to possible worlds
(as in the LU G), can be more efficient and informed.
 The LUG heuristics help our conditional planner, P ON D, to scale up in conditional domains,
despite the fact that the heuristic computation does not model observation actions.

8. Related Work & Discussion
We discuss connections with several related works that involve heuristics and/or conditional planning in the first half of this section. In the second part of the section we discuss how we can extend
our work to directly handle non-deterministic outcomes of actions in heuristic computation.
8.1 Related Work
Much interest in conformant and conditional planning can be traced to CGP (Smith & Weld, 1998), a
conformant version of GraphPlan (Blum & Furst, 1995), and SGP (Weld et al., 1998), the analogous
conditional version of GraphPlan. Here the graph search is conducted on several planning graphs,
each constructed from one of the possible initial states. More recent work on C-plan (Castellini
et al., 2001) and Frag-Plan (Kurien et al., 2002) generalize the CGP approach by ordering the
searches in the different worlds such that the plan for the hardest to satisfy world is found first,
and is then extended to the other worlds. Although CAltAlt and P ON D utilize planning graphs
similar to CGP and Frag-plan it only uses them to compute reachability estimates. The search itself
is conducted in the space of belief states.
Another strand of work models conformant and conditional planning as a search in the space
of belief states. This started with Genesereth and Nourbakhsh (1993), who concentrated on formulating a set of admissible pruning conditions for controlling search. There were no heuristics for
choosing among unpruned nodes. GPT (Bonet & Geffner, 2000) extended this idea to consider a
88

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

simple form of reachability heuristic. Specifically, in computing the estimated cost of a belief state,
GPT assumes that the initial state is fully observable. The cost estimate itself is done in terms of
reachability (with dynamic programming rather than planning graphs). GPTs reachability heuristic
G
is similar to our hM
mRP heuristic because they both estimate the cost of the farthest (maximum distance) state by looking at a deterministic relaxation of the problem. In comparison to GPT, CAltAlt
and P ON D can be seen as using heuristics that do a better job of considering the cost of the belief
state across the various possible worlds.
Another family of planners that search in belief states is the MBP-family of plannersMBP
(Bertoli et al., 2001b), and KACMBP (Bertoli & Cimatti, 2002). In contrast to CAltAlt but similar to P ON D, the MBP-family of planners all represent belief states in terms of binary decision
diagrams. Action application is modeled as modifications to the BDDs. MBP supports both progression and regression in the space of belief states, while KACMBP is a pure progression planner.
Before computing heuristic estimates, KACMBP pro-actively reduces the uncertainty in the belief
state by preferring uncertainty reducing actions. The motivation for this approach is that applying
cardinality heuristics to belief states containing multiple states may not give accurate enough direction to the search. While reducing the uncertainty seems to be an effective idea, we note that (a)
not all domains may contain actions that reduce belief state uncertainty and (b) the need for uncertainty reduction may be reduced when we have heuristics that effectively reason about the multiple
worlds (viz., our multiple planning graph heuristics). Nevertheless, it could be very fruitful to integrate knowledge goal ideas of KACMBP and the reachability heuristics of CAltAlt and P ON D to
handle domains that contain both high uncertainty and costly goals.
In contrast to these domain-independent approaches that only require models of the domain
physics, PKSPlan (Petrick & Bacchus, 2002) is a forward-chaining knowledge-based planner that
requires richer domain knowledge. The planner makes use of several knowledge bases, as opposed
to a single knowledge base taking the form of a belief state. The knowledge bases separate binary
and multi-valued variables, and planning and execution time knowledge.
YKA (Rintanen, 2003b) is a regression conditional planner using BDDs that uses a cardinality heuristic. Recently Rintanen has also developed related reachability heuristics that consider
distances for groups of states, which do not rely on planning graphs (Rintanen, 2004).
More recently, there has been closely related work on heuristics for constructing conformant
plans within the CFF planner (Hoffmann & Brafman, 2004). The planner represents belief states
implicitly through a set of known facts, the action history (leading to the belief state), and the initial
belief state. CFF builds a planning graph forward from the set of known literals to the goal literals
and backwards to the initial belief state. In the planning graph, conditional effects are restricted
to single literals in their antecedent to enable tractable 2-cnf reasoning. From this planning graph,
CFF extracts a relaxed plan that represents supporting the goal belief state from all states in the
initial belief state. The biggest differences between the LU G and the CFF technique are that the
LU G reasons only forward from the source belief state (assuming an explicit, albeit symbolic, belief
state), and the LU G does not restrict the number of literals in antecedents. As a result, the LU G
does not lose the causal information nor perform backward reasoning to the initial belief state.
Our handling of uncertainty through labels and label propagation is reminiscent of and related to
de Kleers assumption based truth maintenance system (ATMS) (de Kleer, 1986). Where an ATMS
uses labels to identify the assumptions (contexts) where a particular statement holds, a traditional
truth maintenance system requires extensive backtracking and consistency enforcement to identify
other contexts. Similarly, where we can reason about multiple possible worlds (contexts) with the
89

fiB RYCE , K AMBHAMPATI , & S MITH

LUG simultaneously, the MG approach requires, not backtracking, but reproduction of planning
graphs for other possible worlds.
Finally, CAltAlt and P ON D are also related to, and an adaptation of the work on reachability
heuristics for classical planning, including AltAlt (Nguyen et al., 2002), FF (Hoffmann & Nebel,
2001) and HSP-r (Bonet & Geffner, 1999). CAltAlt is the conformant extension to AltAlt that uses
regression search (similar to HSP-r) guided by planning graph heuristics. P ON D is similar to FF
in that it uses progression search with planning graph heuristics.
8.2 Extension to Non-Deterministic Actions
While the scope of our presentation and evaluation is restricted to planning with initial state uncertainty and deterministic actions, some of the planning graph techniques can be extended to include
non-deterministic actions of the type described by Rintanen (2003a). Non-deterministic actions
have effects that are described in terms of a set of outcomes. For simplicity, we consider Rintanens
conditionality normal form, where actions have a set of conditional effects (as before) and each
consequent is a mutually-exclusive set of conjunctions (outcomes)  one outcome of the effect will
result randomly. We outline the generalization of our single, multiple, and labelled planning graphs
to reason with non-deterministic actions.
Single Planning Graphs: Single planning graphs, that are built from approximate belief states or
a sampled state, do not lend themselves to a straight-forward extension. A single graph ignores
uncertainty in a belief state by unioning its literals or sampling a state to form the initial planning
graph layer. Continuing with the single graph assumptions about uncertainty, it makes sense to treat
non-deterministic actions as deterministic. Similar to how we approximate a belief state as a set of
literals to form the initial literal layer or sample a state, we can assume that a non-deterministic effect
adds all literals appearing in the effect or samples an outcome as if the action were deterministic
(i.e. gives a set of literals). Single graph relaxed plan heuristics thus remain unchanged.
Multiple Planning Graphs: Multiple planning graphs are very much like Conformant GraphPlan
(Smith & Weld, 1998). We can generalize splitting the non-determinism in the current belief state
into multiple initial literal layers to splitting the outcomes of non-deterministic effects into multiple
literal layers. The idea is to root a set of new planning graphs at each level, where each has an
initial literal layer containing literals supported by an interpretation of the previous effect layer. By
interpretations of the effect layer we mean every possible set of joint effect outcomes. A set of effect
outcomes is possible if no two outcomes are outcomes of the same effect. Relaxed plan extraction
still involves finding a relaxed plan in each planning graph. However, since each planning graph is
split many times (in a tree-like structure) a relaxed plan is extracted from each path of the tree.
We note that this technique is not likely to scale because of the exponential growth in redundant
planning graph structure over time. Further, in our experiments CGP has enough trouble with initial
state uncertainty. We expect that we should be able to do much better with the LU G.
Labelled Uncertainty Graph: With multiple planning graphs we are forced to capture non
determinism through splitting the planning graphs not only in the initial literal layer, but also each
literal layer that follows at least one non-deterministic effect. We saw in the LU G that labels can
capture the non-determinism that drove us to split the initial literal layer in multiple graphs. As
such, these labels took on a syntactic form that describes subsets of the states in our source belief
state. In order to generalize labels to capture non-determinism resulting from uncertain effects, we
90

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

need to extend their syntactic form. Our objective is to have a label represent which sources of
uncertainty (arising from the source belief state or effects) causally support the labelled item. We
also introduce a graph layer Ok to represent outcomes and how they connect effects and literals.
It might seem natural to describe the labels for outcomes in terms of their affected literals, but
this can lead to trouble. The problem is that the literals in effect outcomes are describing states at a
different time than the literals in the projected belief state. Further, an outcome that appears in two
levels of the graph is describing a random event at different times. Using state literals to describe
all labels will lead to confusion as to which random events (state uncertainty and effect outcomes at
distinct steps) causally support a labelled item. A pathological example is when we have an effect
whose set of outcomes matches one-to-one with the states in the source belief state. In such a case,
by using labels defined in terms of state literals we cannot distinguish which random event (the state
uncertainty or the effect uncertainty) is described by the label.
We have two choices for describing effect outcomes in labels. In both choices we introduce a
new set of label variables to describe how a literal layer is split. These new variables will be used
to describe effect outcomes in labels and will not be confused with variables describing initial state
uncertainty. In the first case, these variables will have a one-to-one matching with our original set
of literals, but can be thought of as time-stamped literals. The number of variables we add to the
label function is on the order of 2F per level (the number of fluent literals  assuming boolean
fluents). The second option is to describe outcomes in labels with a new set of fluents, where each
interpretation over the fluents is matched to a particular outcome. In this case, we add on the order
of log |Ok | variables, where Ok is the k th outcome layer. It would actually be lower if many of
the outcomes were from deterministic effects because there is no need to describe them in labels.
The former approach is likely to introduce fewer variables when there are a lot of non-deterministic
effects and they affect quite a few of the same literals. The latter will introduce fewer variables
when there are relatively few non-deterministic effects whose outcomes are fairly independent.
With the generalized labelling, we can still say that an item is reachable from the source belief
state when its label is entailed by the source belief state. This is because even though we are adding
variables to labels, we are implicitly adding the fluents to the source belief state. For example, say
we add a fluent v to describe two outcomes of an effect. One outcome is labelled v, the other v.
We can express the source belief state BSP that is projected by the LU G with the new fluent as
BSP  (v  v) = BSP . An item labelled as BSP  v will not be entailed by the projected belief
state (i.e. is unreachable) because only one outcome causally supports it. If both outcomes support
the item, then it will be reachable.
Given our notion of reachability, we can determine the level from which to extract a relaxed
plan. The relaxed plan procedure does not change much in terms of its semantics other than having
the extra graph layer for outcomes. We still have to ensure that literals are causally supported in all
worlds they are labelled with in a relaxed plan, whether or not the worlds are from the initial state
uncertainty or supporting non-deterministic effects.

9. Conclusion
With the intent of establishing a basis for belief state distance estimates, we have:
 Discussed how heuristic measures can aggregate state distance measures to capture positive
interaction, negative interaction, independence, and overlap.
91

fiB RYCE , K AMBHAMPATI , & S MITH

 Shown how to compute such heuristic measures on planning graphs and provided empirical
comparisons of these measures.
 Found that exploiting planning graph structure to reduce the cost of considering more possible
states of a belief state is preferable to sampling a subset of the states for the heuristics.
 Shown that a labelled uncertainty graph can capture the same support information as multiple
graphs, and reduces the cost of heuristic computation.
 Shown that the labelled uncertainty graph is very useful for conformant planning and, without
considering observational actions and knowledge, can perform well in conditional planning.
Our intent in this work was to provide a formal basis for measuring the distance between belief
states in terms of underlying state distances. We investigated several ways to aggregate the state
distances to reflect various assumptions about the interaction of state to state trajectories. The best
of these measures turned out to measure both positive interaction and independence, what we call
overlap. We saw that planners using this notion of overlap tend to do well across a large variety of
domains and tend to have more accurate heuristics.
Weve also shown that planning with a Labelled Uncertainty planning Graph LU G, a condensed
version of the multiple graphs is useful for encoding conformant reachability information. Our main
innovation is the idea of labels  labels are attached to all literals, actions, effect relations, and
mutexes to indicate the set of worlds in which those respective elements hold. Our experimental
results show that the LU G can outperform the multiple graph approach. In comparison to other
approaches, weve also been able to demonstrate the utility of structured reachability heuristics in
controlling plan length and boosting scalability for both conformant and conditional planning.
We intend to investigate three additions to this work. The first, is to incorporate sensing and
knowledge into the heuristics. We already have some promising results without using these features
in the planning graphs, but hope that they will help the approaches scale even better on conditional
problems. The second addition will be to consider heuristics for stochastic planning problems. The
major challenges here are to associate probabilities with labels to indicate the likelihood of each
possible world and integrate reasoning about probabilistic action effects.
Lastly, we have recently extended the LU G within the framework of state agnostic planning
graphs (Cushing & Bryce, 2005), and hope to improve the technique. A state agnostic planning
graph is essentially a multiple source planning graph, where by analogy a conventional planning
graph has a single source. Planning graphs are already multiple destination, so in our generalization
the state agnostic planning graph allows us to compute the distance measure between any pair of
states or belief states. The LU G seeks to avoid redundancy across the multiple planning graphs
built for states in the same belief state. We extended this notion to avoid redundancy in planning
graphs built for every belief state. We have shown that the state agnostic LU G (SLU G) which is
built once per search episode (as opposed to a LU G at each node) can reduce heuristic computation
cost without sacrificing informedness.
Acknowledgments We would like to thank Minh B. Do, Romeo Sanchez, Terry Zimmermam,
Satish Kumar Thittamaranahalli, and Will Cushing for helpful discussions and feedback, Jussi Rintanen for help with the YKA planner, and Piergiorgio Bertoli for help with the MBP planner. This
work was supported in part by NASA grants NCC2-1225 and NAG2-1461, the NSF grant IIS0308139, the 2003 NASA RIACS SSRP, the ARCS Foundation, and an IBM faculty award.
92

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Appendix A. Additional Heuristics
For completeness, we present some additional heuristics adapted from classical planning to reason
about belief state distances in each type of planning graph. Many of these heuristics appeared in our
previous work (Bryce & Kambhampati, 2004). We show how to compute the max, sum, and level
heuristics on the single graph SG, multiple graphs M G, and the labelled uncertainty graph LU G.
While these heuristics tend to be less effective than the relaxed plan heuristics, we provide them as
reference. As with Section 4, we describe the heuristics in terms of regression search.
A.1 Single Planning Graph Heuristics (SG)
Like, the relaxed plan for the single unmodified planning graph, we cannot aggregate state distances
because all notion of separate states is lost in forming the initial literal layer, thus we only compute
heuristics that do not aggregate state distances.
No State Aggregation:
 Max In classical planning, the maximum cost literal is used to get a max heuristic, but we use
formulas to describe our belief states, so we take the maximum cost clause as the cost of the
belief state to find the max heuristic hSG
max . The maximum cost clause of the belief state, with
respect to a single planning graph, is:
hSG
max (BSi ) =

max

C(BSi )

cost(C)

where the cost of a clause is:
cost(C) = min min k
lC k:lLk

Here we find the cheapest literal as the cost of each clause to find the maximum cost clause.
This is an underestimate of the closest state to our current belief state.
 Sum Like the classical planning sum heuristic, we can take the sum hSG
sum of the costs of the
clauses in our belief state to estimate our belief state distance

cost(C)
hSG
sum (BSi ) =
C(BSi )

This heuristic takes the summation of costs of literals in the closest estimated state in the
belief state, and is inadmissible because there may be a single action that will support every
clause, and we could count it once for each clause.
 Level When we have mutexes on the planning graph, we can compute a level heuristic hSG
level
(without mutexes the level heuristic is equivalent to the max heuristic). The level heuristic
maintains the admissibility of the max heuristic but improves the lower bound by considering
what level of the planning graph all literals in a constituent are non-pairwise mutex. The

level heuristic is computed by taking the minimum among the S  (BS
i ), of the first level
(lev(S)) in the planning graph where literals of S are present with none of them marked
pairwise mutex. Formally:
hSG
level (BSi ) =

93

min

S(BSi )

lev(S)

fiB RYCE , K AMBHAMPATI , & S MITH

A.2 Multiple Planning Graph Heuristics (M G)
Similar to the various relaxed plan heuristics for the multiple graphs, we can compute a max, sum,
or level heuristic on each of the multiple planning graphs and aggregate them with a maximum
or summation to respectively measure positive interaction or independence. The reason we cannot
aggregate the individual graph heuristics to measure overlap is that they are numbers, not sets of
actions. Measuring overlap involves taking the union of heuristics from each graph and the union
of numbers is not meaningful like the union of action sets from relaxed plans. Like before, there is
no reason to use multiple graphs if there is no state distance aggregation.
Positive Interaction Aggregation:
G
 Max The max heuristic hM
mmax is computed with multiple planning graphs to measure posM
G
itive interaction in the hmmax heuristic. This heuristic computes the maximum cost clause
in (BSi ) for each graph   , similar to how hSG
mmax (BSi ) is computed, and takes the
maximum. Formally:
G

hM
mmax (BSi ) = max (hmax (BSi ))


G
The hM
mmax heuristic considers the minimum cost, relevant literals of a belief state (those that
are reachable given a possible world for each graph ) to get state measures. The maximum
is taken because the estimate accounts for the worst (i.e., the plan needed in the most difficult
world to achieve the subgoals).

 Sum The sum heuristic that measures positive interaction for multiple planning graphs is
G
hM
msum . It computes the summation of the cost of the clauses in (BSi ) for each graph
   and takes the maximum. Formally:
G

hM
msum (BSi ) = max (hsum (BSi ))


The heuristic considers the minimum cost, relevant literals of a belief state (those that are
reachable given the possible worlds represented for each graph ) to get state measures. As
G
with hM
mmax , the maximum is taken to estimate for the most costly world.

G
MG
MG
 Level Similar to hM
mmax and hmsum , the hmlevel heuristic is found by first finding hlevel
for each graph    to get a state distance measure, and then taking the maximum across

the graphs. hlevel (BSi ) is computed by taking the minimum among the S  (BS
i ), of the

first level lev (S) in the planning graph  where literals of S are present with none of them
marked mutex. Formally:

hlevel (BSi ) =

min

S(BSi )

lev  (S)

and

G
hM
mlevel (BSi ) = max(hlevel (BSi ))


Note that this heuristic is admissible. By the same reasoning as in classical planning, the first
level where all the subgoals are present and non-mutex is an underestimate of the true cost of
a state. This holds for each of the graphs. Taking the maximum accounts for the most difficult
94

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

world in which to achieve a constituent of BSi and is thus a provable underestimate of h .
G
GPTs max heuristic (Bonet & Geffner, 2000) is similar to hM
mlevel , but is computed with
dynamic programming in state space rather than planning graphs.
Independence Aggregation: All heuristics mentioned for Positive Interaction Aggregation can
be augmented to take the summation of costs found on the individual planning graphs rather than
G
MG
MG
the maximum. We denote them as: hM
smax , hssum , and hslevel . None of these heuristics are
admissible because the same action may be used in all worlds, but we count its cost for every world
by using summation.
A.3 Labelled Uncertainty Graph (LU G)
The max, sum, and level heuristics for the LU G are similar to the analogous multiple graph heuristics. The main difference with these heuristics for the LU G is that it is much easier to compute
positive interaction measures than independence measures. The reason positive interaction is easier
to compute is that we find the cost of a clause for all states in our belief state at once, rather than on
each of multiple planning graphs. Like before, we do not consider heuristics that do not aggregate
state distances.
Positive Interaction Aggregation:
G
 Max The max heuristic hLU
mmax for the LU G finds the maximum clause cost across clauses
of the current belief state BSi . The cost of a clause is the first level it becomes reachable.
Formally:


G
hLU
mmax (BSi )

=

max

C(BSi )


min

k:BSP |=k (C)

k

G
 Sum The sum heuristic hLU
msum for the LU G sums the individual levels where each clause
in (BSi ) is first reachable. Formally:

 
G
min
hLU
(BS
)
=
k
i
msum

C(BSi )

k:BSP |=k (C)

G
 Level The level heuristic hLU
mlevel is the index of the first level where BSi is reachable.
Formally:
G
hLU
mlevel (BSi ) =

min

k:BSP |=k (BSi )

i

Independence Aggregation: All heuristics mentioned for positive interaction aggregation can be
augmented to take the summation of costs for each state in our belief state. This may be inefficient
due to the fact that we lose the benefit of having a LU G by evaluating a heuristic for each state of
our BSP , rather than all states at once as in the positive interaction aggregation. In such a case we
are doing work similar to the multiple graph heuristic extraction, aside from the improved graph
construction time. The positive interaction aggregation is able to implicitly calculate the maximum
over all worlds for most of the heuristics, whereas for the sum heuristic we need to explicitly find a
G
LU G
LU G
cost for each world. We denote the sum heuristics as: hLU
smax , hssum , and hslevel .
95

fiB RYCE , K AMBHAMPATI , & S MITH

Appendix B. Cross-World Mutexes
Mutexes can develop not only in the same possible world but also between two possible worlds,
as described by Smith and Weld (1998). Cross-world mutexes are useful to capture negative interactions in belief state distance measures (mentioned in Section 3). The representation of crossworld mutexes requires another generalization for the labelling of mutexes. Same world mutexes
require keeping only one label for the mutex to signify all same possible worlds for which the mutex holds. The extended representation keeps a pair of labels, one for each element in the mutex;
if x in possible world S is mutex with x in possible world S  , we denote the mutex as the pair
(k (x) = S, k (x ) = S  ).
We can compute cross-world mutexes between several worlds of elements x and x . For example, if k (x) = S1 S2 S3 and k (x ) = S2 S3 , then to check for all cross-world mutexes we need
to consider mutexes for the world pairs (S1 , S2 ), (S1 , S3 ), (S2 , S2 ), (S2 , S3 ), (S3 , S2 ), and (S3 , S3 ).
We can also check for mutexes in the intersection of the element labels k (x)  k (x ) = S2  S3 ,
meaning the only cross world pairs we check for mutexes are (S2 , S2 ), (S2 , S3 ), (S3 , S2 ), and
(S3 , S3 ).
We can say that a formula f is reachable from our projected belief state BSP , when considering
cross-world mutexes, if for every pair of states in BSP , f is reachable. For a pair of states S and
S  , f is reachable if S  S  |= k (f ) and for every pair of constituents S , S  f such that
S |= k (S ) and S  |= k (S ), there are no two literals in either S or S that are same-world
mutex when S = S  , and there is not a mutex between literals in S and S , across the respective
worlds S and S  when S 
= S  . There is a mutex between a pair literals l and l , respectively from
S and S if there is a mutex (k (l), k (l )) such that S |= k (l) and S  |= k (l ).
The computation of cross-world mutexes requires changes to some of the mutex formulas, as
outlined next. The major change is to check, instead of all the single possible worlds S, all pairs of
possible worlds S and S  for mutexes.
Action Mutexes: The action mutexes can now hold for actions that are executable in different
possible worlds.
 Interference Interference mutexes do not change for cross-world mutexes, except that there
is a pair of labels where (k (a) = BSP , k (a ) = BSP ), instead of a single label.
 Competing Needs Competing needs change mutexes for cross-world mutexes because two
actions a and a , in worlds S and S  respectively, could be competing. Formally, a crossworld competing needs mutex ((k (a) = S, k (a ) = S  ) exists between a and a in worlds S
and S  if:
le (a),l e (a ) (k (l) = S, k (l ) = S  )
Effect Mutexes: The effect mutexes can now hold for effects that occur in different possible worlds.
 Interference Effect interference mutexes do not change for cross-world mutexes, except that
there is a pair of labels where (k (i (a)) = BSP , k (j (a )) = BSP ), instead of a single
label.
96

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Lk

lk(p)
p


Ek

Ak

lk(a)
a

h(a)

lk(h(a))



(lk(p), lk(q))
Induced mutex across worlds:


(lk(j(a))lk(i(a)), lk(h(a)))





(lk(j(a)), lk(h(a)))

lk(q)
q
lk(a)
a

j(a)

lk(r)
r

lk(j(a))
i(a) induces j(a) in:
lk(i(a))lk(j(a))

i(a) lk(i(a))

Figure 12: Example of a cross-world induced effect mutex.
 Competing Needs Effect competing needs mutexes change for cross-world mutexes because
two effects i (a) and j (a ), in worlds S and S  respectively, could be competing. Formally,
a cross-world competing needs mutex (k (i (a)) = S, k (j (a )) = S  ) exists between i (a)
and j (a ) in worlds S and S  if:
li (a),l j (a ) (k (l) = S, k (l ) = S  )
 Induced Induced mutexes change slightly for cross-world mutexes. The worlds where one
effect induces another, remains the same, but the mutex changes slightly.
If j (a) in k (j (a)) is mutex with h (a ) in k (h (a )), and i (a) induces effect j (a)
in the possible worlds described by k (i (a))  k (j (a)), then there is an induced mutex
between i (a) in k (j (a))  k (i (a)) and h (a ) in k (h (a )) (see Figure 12).

Literal Mutexes: The literal mutexes can now hold for literals that are supported in different possible worlds.
 Inconsistent Support changes for cross-world mutexes. A mutex (k (l) = S, k (l ) = S  )
holds for l in S and l in S  if i (a), j (a )  Ek1 where l  i (a), l  j (a ), there is a
mutex k1 (i (a)) = S, k1 (j (a )) = S  ).

97

fiB RYCE , K AMBHAMPATI , & S MITH

References
Bertoli, P., & Cimatti, A. (2002). Improving heuristics for planning as search in belief space. In
Proceedings of AIPS02.
Bertoli, P., Cimatti, A., & Roveri, M. (2001a). Heuristic search + symbolic model checking =
efficient conformant planning. In Proceedings of IJCAI01.
Bertoli, P., Cimatti, A., Roveri, M., & Traverso, P. (2001b). Planning in nondeterministic domains
under partial observability via symbolic model checking. In Proceedings of IJCAI01.
Blum, A., & Furst, M. (1995). Fast planning through planning graph analysis. In Proceedings of
IJCAI95.
Bonet, B., & Geffner, H. (1999). Planning as heuristic search: New results. In Proceedings of
ECP99.
Bonet, B., & Geffner, H. (2000). Planning with incomplete information as heuristic search in belief
space. In Proceedings of AIPS00.
Brace, K., Rudell, R., & Bryant, R. (1990). Efficient implementation of a bdd package. In Proceedings of the 27th ACM/IEEE design automation conference.
Bryant, R. (1986). Graph-based algorithms for Boolean function manipulation. IEEE Transactions
on Computers, C-35(8), 677691.
Bryce, D., & Kambhampati, S. (2004). Heuristic guidance measures for conformant planning. In
Proceedings of ICAPS04.
Castellini, C., Giunchiglia, E., & Tacchella, A. (2001). Improvements to sat-based conformant
planning. In Proceedings of ECP01.
Cimatti, A., & Roveri, M. (2000). Conformant planning via symbolic model checking. Journal of
Artificial Intelligence Research, 13, 305338.
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1990). Introduction to Algorithms. McGraw-Hill.
Cushing, W., & Bryce, D. (2005). State agnostic planning graphs. In Proceedings of AAAI05.
de Kleer, J. (1986). An Assumption-Based TMS. Artificial Intelligence, 28(2), 127162.
Genesereth, M. R., & Nourbakhsh, I. R. (1993). Time-saving tips for problem solving with incomplete information. In Proceedings of AAAI93.
Hansen, E., & Zilberstein, S. (2001). LAO: A heuristic-search algorithm that finds solutions with
loops. Artificial Intelligence, 129(12), 3562.
Hoffmann, J., & Brafman, R. (2004). Conformant planning via heuristic forward search: A new
approach. In Proceedings of ICAPS04.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence Research, 14, 253302.
Kambhampati, S., Ihrig, L., & Srivastava, B. (1996). A candidate set based analysis of subgoal
interactions in conjunctive goal planning. In Proceedings of AIPS96.
Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphs to an
adl subset. In Proceedings of ECP97.
98

fiP LANNING G RAPH H EURISTICS FOR B ELIEF S PACE S EARCH

Kurien, J., Nayak, P., & Smith, D. (2002). Fragment-based conformant planning. In Proceedings of
AIPS02.
Long, D., & Fox, M. (2003). The 3rd international planning competition: Results and analysis.
Journal of Artificial Intelligence Research, 20, 159.
Nguyen, X., Kambhampati, S., & Nigenda, R. (2002). Planning graph as the basis for deriving
heuristics for plan synthesis by state space and CSP search. Artificial Intelligence, 135(1-2),
73123.
Nilsson, N. (1980). Principles of Artificial Intelligence. Morgan Kaufmann.
Pednault, E. P. D. (1988). Synthesizing plans that contain actions with context-dependent effects.
Computational Intelligence, 4, 356372.
Petrick, R., & Bacchus, F. (2002). A knowledge-based approach to planning with incomplete information and sensing. In Proceedings of AIPS02.
Rintanen, J. (2003a). Expressive equivalence of formalisms for planning with sensing. In Proceedings of ICAPS03.
Rintanen, J. (2003b). Product representation of belief spaces in planning under partial observability.
In Proceedings of IJCAI03.
Rintanen, J. (2004). Distance estimates for planning in the discrete belief space. In Proceedings of
AAAI04.
Smith, D., & Weld, D. (1998). Conformant graphplan. In Proceedings of AAAI98.
Weld, D., Anderson, C., & Smith, D. (1998). Extending graphplan to handle uncertainty and sensing
actions. In Proceedings of AAAI98.

99

fi