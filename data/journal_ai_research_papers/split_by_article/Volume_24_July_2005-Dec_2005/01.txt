Journal of Artificial Intelligence Research 24 (2005) 49-79

Submitted 09/04; published 07/05

A Framework for Sequential Planning in Multi-Agent Settings
Piotr J. Gmytrasiewicz
Prashant Doshi

PIOTR @ CS . UIC . EDU
PDOSHI @ CS . UIC . EDU

Department of Computer Science
University of Illinois at Chicago
851 S. Morgan St
Chicago, IL 60607

Abstract
This paper extends the framework of partially observable Markov decision processes (POMDPs)
to multi-agent settings by incorporating the notion of agent models into the state space. Agents
maintain beliefs over physical states of the environment and over models of other agents, and they
use Bayesian updates to maintain their beliefs over time. The solutions map belief states to actions.
Models of other agents may include their belief states and are related to agent types considered in
games of incomplete information. We express the agents autonomy by postulating that their models are not directly manipulable or observable by other agents. We show that important properties
of POMDPs, such as convergence of value iteration, the rate of convergence, and piece-wise linearity and convexity of the value functions carry over to our framework. Our approach complements a
more traditional approach to interactive settings which uses Nash equilibria as a solution paradigm.
We seek to avoid some of the drawbacks of equilibria which may be non-unique and do not capture
off-equilibrium behaviors. We do so at the cost of having to represent, process and continuously
revise models of other agents. Since the agents beliefs may be arbitrarily nested, the optimal solutions to decision making problems are only asymptotically computable. However, approximate
belief updates and approximately optimal plans are computable. We illustrate our framework using
a simple application domain, and we show examples of belief updates and value functions.

1. Introduction
We develop a framework for sequential rationality of autonomous agents interacting with other
agents within a common, and possibly uncertain, environment. We use the normative paradigm of
decision-theoretic planning under uncertainty formalized as partially observable Markov decision
processes (POMDPs) (Boutilier, Dean, & Hanks, 1999; Kaelbling, Littman, & Cassandra, 1998;
Russell & Norvig, 2003) as a point of departure. Solutions of POMDPs are mappings from an
agents beliefs to actions. The drawback of POMDPs when it comes to environments populated by
other agents is that other agents actions have to be represented implicitly as environmental noise
within the, usually static, transition model. Thus, an agents beliefs about another agent are not part
of solutions to POMDPs.
The main idea behind our formalism, called interactive POMDPs (I-POMDPs), is to allow
agents to use more sophisticated constructs to model and predict behavior of other agents. Thus,
we replace flat beliefs about the state space used in POMDPs with beliefs about the physical
environment and about the other agent(s), possibly in terms of their preferences, capabilities, and
beliefs. Such beliefs could include others beliefs about others, and thus can be nested to arbitrary
levels. They are called interactive beliefs. While the space of interactive beliefs is very rich and
updating these beliefs is more complex than updating their flat counterparts, we use the value
c
2005
AI Access Foundation. All rights reserved.

fiG MYTRASIEWICZ & D OSHI

function plots to show that solutions to I-POMDPs are at least as good as, and in usual cases superior
to, comparable solutions to POMDPs. The reason is intuitive  maintaining sophisticated models of
other agents allows more refined analysis of their behavior and better predictions of their actions.
I-POMDPs are applicable to autonomous self-interested agents who locally compute what actions they should execute to optimize their preferences given what they believe while interacting
with others with possibly conflicting objectives. Our approach of using a decision-theoretic framework and solution concept complements the equilibrium approach to analyzing interactions as used
in classical game theory (Fudenberg & Tirole, 1991). The drawback of equilibria is that there could
be many of them (non-uniqueness), and that they describe agents optimal actions only if, and when,
an equilibrium has been reached (incompleteness). Our approach, instead, is centered on optimality
and best response to anticipated action of other agent(s), rather then on stability (Binmore, 1990;
Kadane & Larkey, 1982). The question of whether, under what circumstances, and what kind of
equilibria could arise from solutions to I-POMDPs is currently open.
Our approach avoids the difficulties of non-uniqueness and incompleteness of traditional equilibrium approach, and offers solutions which are likely to be better than the solutions of traditional
POMDPs applied to multi-agent settings. But these advantages come at the cost of processing and
maintaining possibly infinitely nested interactive beliefs. Consequently, only approximate belief
updates and approximately optimal solutions to planning problems are computable in general. We
define a class of finitely nested I-POMDPs to form a basis for computable approximations to infinitely nested ones. We show that a number of properties that facilitate solutions of POMDPs carry
over to finitely nested I-POMDPs. In particular, the interactive beliefs are sufficient statistics for the
histories of agents observations, the belief update is a generalization of the update in POMDPs, the
value function is piece-wise linear and convex, and the value iteration algorithm converges at the
same rate.
The remainder of this paper is structured as follows. We start with a brief review of related
work in Section 2, followed by an overview of partially observable Markov decision processes in
Section 3. There, we include a simple example of a tiger game. We introduce the concept of
agent types in Section 4. Section 5 introduces interactive POMDPs and defines their solutions. The
finitely nested I-POMDPs, and some of their properties are introduced in Section 6. We continue
with an example application of finitely nested I-POMDPs to a multi-agent version of the tiger game
in Section 7. There, we show examples of belief updates and value functions. We conclude with
a brief summary and some current research issues in Section 8. Details of all proofs are in the
Appendix.

2. Related Work
Our work draws from prior research on partially observable Markov decision processes, which
recently gained a lot of attention within the AI community (Smallwood & Sondik, 1973; Monahan,
1982; Lovejoy, 1991; Hausktecht, 1997; Kaelbling et al., 1998; Boutilier et al., 1999; Hauskrecht,
2000).
The formalism of Markov decision processes has been extended to multiple agents giving rise to
stochastic games or Markov games (Fudenberg & Tirole, 1991). Traditionally, the solution concept
used for stochastic games is that of Nash equilibria. Some recent work in AI follows that tradition
(Littman, 1994; Hu & Wellman, 1998; Boutilier, 1999; Koller & Milch, 2001). However, as we
mentioned before, and as has been pointed out by some game theorists (Binmore, 1990; Kadane &
50

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

Larkey, 1982), while Nash equilibria are useful for describing a multi-agent system when, and if,
it has reached a stable state, this solution concept is not sufficient as a general control paradigm.
The main reasons are that there may be multiple equilibria with no clear way to choose among them
(non-uniqueness), and the fact that equilibria do not specify actions in cases in which agents believe
that other agents may not act according to their equilibrium strategies (incompleteness).
Other extensions of POMDPs to multiple agents appeared in AI literature recently (Bernstein,
Givan, Immerman, & Zilberstein, 2002; Nair, Pynadath, Yokoo, Tambe, & Marsella, 2003). They
have been called decentralized POMDPs (DEC-POMDPs), and are related to decentralized control
problems (Ooi & Wornell, 1996). DEC-POMDP framework assumes that the agents are fully cooperative, i.e., they have common reward function and form a team. Furthermore, it is assumed that
the optimal joint solution is computed centrally and then distributed among the agents for execution.
From the game-theoretic side, we are motivated by the subjective approach to probability in
games (Kadane & Larkey, 1982), Bayesian games of incomplete information (see Fudenberg &
Tirole, 1991; Harsanyi, 1967, and references therein), work on interactive belief systems (Harsanyi,
1967; Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Fagin, Halpern, Moses, & Vardi,
1995; Aumann, 1999; Fagin, Geanakoplos, Halpern, & Vardi, 1999), and insights from research on
learning in game theory (Fudenberg & Levine, 1998). Our approach, closely related to decisiontheoretic (Myerson, 1991), or epistemic (Ambruster & Boge, 1979; Battigalli & Siniscalchi, 1999;
Brandenburger, 2002) approach to game theory, consists of predicting actions of other agents given
all available information, and then of choosing the agents own action (Kadane & Larkey, 1982).
Thus, the descriptive aspect of decision theory is used to predict others actions, and its prescriptive
aspect is used to select agents own optimal action.
The work presented here also extends previous work on Recursive Modeling Method (RMM)
(Gmytrasiewicz & Durfee, 2000), but adds elements of belief update and sequential planning.

3. Background: Partially Observable Markov Decision Processes
A partially observable Markov decision process (POMDP) (Monahan, 1982; Hausktecht, 1997;
Kaelbling et al., 1998; Boutilier et al., 1999; Hauskrecht, 2000) of an agent i is defined as
POMDPi = hS, Ai , Ti , i , Oi , Ri i

(1)

where: S is a set of possible states of the environment. Ai is a set of actions agent i can execute. Ti is
a transition function  Ti : S Ai S  [0, 1] which describes results of agent is actions. i is the
set of observations the agent i can make. Oi is the agents observation function  Oi : S Ai i 
[0, 1] which specifies probabilities of observations given agents actions and resulting states. Finally,
Ri is the reward function representing the agent is preferences  R i : S  Ai  <.
In POMDPs, an agents belief about the state is represented as a probability distribution over S.
Initially, before any observations or actions take place, the agent has some (prior) belief, b 0i . After
some time steps, t, we assume that the agent has t + 1 observations and has performed t actions 1 .
t
These can be assembled into agent is observation history: h ti = {o0i , o1i , .., ot1
i , oi } at time t. Let
Hi denote the set of all observation histories of agent i. The agents current belief, b ti over S, is
continuously revised based on new observations and expected results of performed actions. It turns
1. We assume that action is taken at every time step; it is without loss of generality since any of the actions maybe a
No-op.

51

fiG MYTRASIEWICZ & D OSHI

out that the agents belief state is sufficient to summarize all of the past observation history and
initial belief; hence it is called a sufficient statistic.2
t1
The belief update takes into account changes in initial belief, b t1
i , due to action, ai , executed
t
t
at time t  1, and the new observation, oi . The new belief, bi , that the current state is st , is:
bti (st ) = Oi (oti , st , at1
i )

X

bit1 (st1 )Ti (st , ati , st1 )

(2)

st1 S

where  is the normalizing constant.
It is convenient to summarize the above update performed for all states in S as
t
bti = SE(bit1 , at1
i , oi ) (Kaelbling et al., 1998).
3.1 Optimality Criteria and Solutions
The agents optimality criterion, OCi , is needed to specify how rewards acquired over time are
handled. Commonly used criteria include:
 A finite horizon criterion,P
in which the agent maximizes the expected value of the sum of the
following T rewards: E( Tt=0 rt ). Here, rt is a reward obtained at time t and T is the length
of the horizon. We will denote this criterion as fhT .
 AnP
infinite horizon criterion with discounting, according to which the agent maximizes

t
E( 
t=0  rt ), where 0 <  < 1 is a discount factor. We will denote this criterion as ih .

 An infinite horizon criterion with averaging, according to which the agent maximizes the
average reward per time step. We will denote this as ihAV .

In what follows, we concentrate on the infinite horizon criterion with discounting, but our approach can be easily adapted to the other criteria.
The utility associated with a belief state, bi is composed of the best of the immediate rewards
that can be obtained in bi , together with the discounted expected sum of utilities associated with
belief states following bi :

U (bi ) = max

ai Ai

X

bi (s)Ri (s, ai ) + 

X

P r(oi |ai , bi )U (SEi (bi , ai , oi ))

oi i

sS



(3)

Value iteration uses the Equation 3 iteratively to obtain values of belief states for longer time
horizons. At each step of the value iteration the error of the current value estimate is reduced by the
factor of at least  (see for example Russell & Norvig, 2003, Section 17.2.) The optimal action, a i ,
is then an element of the set of optimal actions, OP T (bi ), for the belief state, defined as:

OP T (bi ) = argmax
ai Ai

X

bi (s)Ri (s, ai ) + 

X

oi i

sS

2. See (Smallwood & Sondik, 1973) for proof.

52

P r(oi |ai , bi )U (SE(bi , ai , oi ))



(4)

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

L
OR

OL

10

Value Function(U)

8

6

4

2

0

0

0.2

0.4

0.6

0.8

1

pp_i(TL)
i (TL)
POMDP with noise

POMDP

Figure 1: The value function for single agent tiger game with time horizon of length 1, OC i = fh1 .
Actions are: open right door - OR, open left door - OL, and listen - L. For this value of
the time horizon the value function for a POMDP with noise factor is identical to single
agent POMDP.

3.2 Example: The Tiger Game
We briefly review the POMDP solutions to the tiger game (Kaelbling et al., 1998). Our purpose is
to build on the insights that POMDP solutions provide in this simple case to illustrate solutions to
interactive versions of this game later.
The traditional tiger game resembles a game-show situation in which the decision maker has
to choose to open one of two doors behind which lies either a valuable prize or a dangerous tiger.
Apart from actions that open doors, the subject has the option of listening for the tigers growl
coming from the left, or the right, door. However, the subjects hearing is imperfect, with given
percentages (say, 15%) of false positive and false negative occurrences. Following (Kaelbling et al.,
1998), we assume that the value of the prize is 10, that the pain associated with encountering the
tiger can be quantified as -100, and that the cost of listening is -1.
The value function, in Figure 1, shows values of various belief states when the agents time
horizon is equal to 1. Values of beliefs are based on best action available in that belief state, as
specified in Eq. 3. The state of certainty is most valuable  when the agent knows the location of
the tiger it can open the opposite door and claim the prize which certainly awaits. Thus, when the
probability of tiger location is 0 or 1, the value is 10. When the agent is sufficiently uncertain, its
best option is to play it safe and listen; the value is then -1. The agent is indifferent between opening
doors and listening when it assigns probabilities of 0.9 or 0.1 to the location of the tiger.
Note that, when the time horizon is equal to 1, listening does not provide any useful information
since the game does not continue to allow for the use of this information. For longer time horizons
the benefits of results of listening results in policies which are better in some ranges of initial belief.
Since the value function is composed of values corresponding to actions, which are linear in prob53

fiG MYTRASIEWICZ & D OSHI

L\();L\(GL),OL\(GR)

L\();L\(*)

L\();OR\(GL),L\(GR)
L\();OR\(*)
OR\();L\(*)

L\();OL\(*)
OL\();L\(*)

8

Value Function(U)

6

4

2

0

-2
0

0.2

0.4

0.6

0.8

1

pp_i(TL)
i (TL)
POMDP with noise

POMDP

Figure 2: The value function for single agent tiger game compared to an agent facing a noise factor, for horizon of length 2. Policies corresponding to value lines are conditional plans.
Actions, L, OR or OL, are conditioned on observational sequences in parenthesis. For
example L\();L\(GL),OL\(GR) denotes a plan to perform the listening action, L, at the
beginning (list of observations is empty), and then another L if the observation is growl
from the left (GL), and open the left door, OL, if the observation is GR.  is a wildcard
with the usual interpretation.

ability of tiger location, the value function has the property of being piece-wise linear and convex
(PWLC) for all horizons. This simplifies the computations substantially.
In Figure 2 we present a comparison of value functions for horizon of length 2 for a single
agent, and for an agent facing a more noisy environment. The presence of such noise could be
due to another agent opening the doors or listening with some probabilities. 3 Since POMDPs do
not include explicit models of other agents, these noise actions have been included in the transition
model, T .
Consequences of folding noise into T are two-fold. First, the effectiveness of the agents optimal
policies declines since the value of hearing growls diminishes over many time steps. Figure 3 depicts
a comparison of value functions for horizon of length 3. Here, for example, two consecutive growls
in a noisy environment are not as valuable as when the agent knows it is acting alone since the noise
may have perturbed the state of the system between the growls. For time horizon of length 1 the
noise does not matter and the value vectors overlap, as in Figure 1.
Second, since the presence of another agent is implicit in the static transition model, the agent
cannot update its model of the other agents actions during repeated interactions. This effect becomes more important as time horizon increases. Our approach addresses this issue by allowing
explicit modeling of the other agent(s). This results in policies of superior quality, as we show in
Section 7. Figure 4 shows a policy for an agent facing a noisy environment for time horizon of 3.
We compare it to the corresponding I-POMDP policy in Section 7. Note that it is slightly different
3. We assumed that, due to the noise, either door opens with probabilities of 0.1 at each turn, and nothing happens with
the probability 0.8. We explain the origin of this assumption in Section 7.

54

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

L\();L\(*);OL\(GR;GR),L\(?)
L\();L\(GL),OL\(GR);OL\(GL;GR),L\(?)

L\();L\(*);OR\(GL;GL),L\(?)
L\();OR\(GL),L\(GR);OR\(GR;GL),L\(?)
L\();L\(*);OR\(GL;GL),OL\(GR;GR),L\(?)
OR\();L\(*);L\(*)
L\();L\(*);OR\(*)

OL\();L\(*);L\(*)
L\();L\(*);OL\(*)
8

Value Function(U)

7
6
5
4
3
2
1
0

0.2

0.4

0.6

0.8

1

p (TL)
p_i(TL)
i

POMDP with noise

POMDP

Figure 3: The value function for single agent tiger game compared to an agent facing a noise factor,
for horizon of length 3. The ? in the description of a policy stands for any of the
perceptual sequences not yet listed in the description of the policy.

[00.045)
OL
*

[0.0450.135) [0.1350.175) [0.1750.825)
L

L

GR

GL

GR

L
GR

GL

*

OL

L
GR

L
GL

GR

OR
GL

*

GL

L
GL

[0.8650.955) [0.9551]

L

GR

OL

[0.8250.865)

L
*

L

GR

OR
GL

*

OR

Figure 4: The policy graph corresponding to value function of POMDP with noise depicted in
Fig. 3.

55

fiG MYTRASIEWICZ & D OSHI

than the policy without noise in the example by Kaelbling, Littman and Cassandra (1998) due to
differences in value functions.

4. Agent Types and Frames
The POMDP definition includes parameters that permit us to compute an agents optimal behavior, 4
conditioned on its beliefs. Let us collect these implementation independent factors into a construct
we call an agent is type.
Definition 1 (Type). A type of an agent i is, i = hbi , Ai , i , Ti , Oi , Ri , OCi i, where bi is agent is
state of belief (an element of (S)), OCi is its optimality criterion, and the rest of the elements are
as defined before. Let i be the set of agent is types.
Given type, i , and the assumption that the agent is Bayesian-rational, the set of agents optimal
actions will be denoted as OP T (i ). In the next section, we generalize the notion of type to situations which include interactions with other agents; it then coincides with the notion of type used in
Bayesian games (Fudenberg & Tirole, 1991; Harsanyi, 1967).
It is convenient to define the notion of a frame, bi , of agent i:

b i be the set of
Definition 2 (Frame). A frame of an agent i is, bi = hAi , i , Ti , Oi , Ri , OCi i. Let 
agent is frames.

For brevity one can write a type as consisting of an agents belief together with its frame:  i =
hbi , bi i.
In the context of the tiger game described in the previous section, agent type describes the
agents actions and their results, the quality of the agents hearing, its payoffs, and its belief about
the tiger location.
Realistically, apart from implementation-independent factors grouped in type, an agents behavior may also depend on implementation-specific parameters, like the processor speed, memory
available, etc. These can be included in the (implementation dependent, or complete) type, increasing the accuracy of predicted behavior, but at the cost of additional complexity. Definition and use
of complete types is a topic of ongoing work.

5. Interactive POMDPs
As we mentioned, our intention is to generalize POMDPs to handle presence of other agents. We
do this by including descriptions of other agents (their types for example) in the state space. For
simplicity of presentation, we consider an agent i, that is interacting with one other agent, j. The
formalism easily generalizes to larger number of agents.
Definition 3 (I-POMDP). An interactive POMDP of agent i, I-POMDPi , is:
I-POMDPi = hISi , A, Ti , i , Oi , Ri i

(5)

4. The issue of computability of solutions to POMDPs has been a subject of much research (Papadimitriou & Tsitsiklis,
1987; Madani, Hanks, & Condon, 2003). It is of obvious importance when one uses POMDPs to model agents; we
return to this issue later.

56

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

where:
 ISi is a set of interactive states defined as ISi = S  Mj ,5 interacting with agent i, where
S is the set of states of the physical environment, and Mj is the set of possible models of agent
j. Each model, mj  Mj , is defined as a triple mj = hhj , fj , Oj i, where fj : Hj  (Aj )
is agent js function, assumed computable, which maps possible histories of js observations to
distributions over its actions. hj is an element of Hj , and Oj is a function specifying the way the
environment is supplying the agent with its input. Sometimes we write model m j as mj = hhj , m
b j i,
where m
b j consists of fj and Oj . It is convenient to subdivide the set of models into two classes.
The subintentional models, SMj , are relatively simple, while the intentional models, IMj , use the
notion of rationality to model the other agent. Thus, Mj = IMj  SMj .
Simple examples of subintentional models include a no-information model and a fictitious play
model, both of which are history independent. A no-information model (Gmytrasiewicz & Durfee,
2000) assumes that each of the other agents actions is executed with equal probability. Fictitious
play (Fudenberg & Levine, 1998) assumes that the other agent chooses actions according to a fixed
but unknown distribution, and that the original agents prior belief over that distribution takes a form
of a Dirichlet distribution.6 An example of a more powerful subintentional model is a finite state
controller.
The intentional models are more sophisticated in that they ascribe to the other agent beliefs,
preferences and rationality in action selection.7 Intentional models are thus js types, j = hbj , bj i,
under the assumption that agent j is Bayesian-rational.8 Agent js belief is a probability distribution
over states of the environment and the models of the agent i; b j  (S  Mi ). The notion of a type
we use here coincides with the notion of type in game theory, where it is defined as consisting of
all of the agent is private information relevant to its decision making (Harsanyi, 1967; Fudenberg
& Tirole, 1991). In particular, if agents beliefs are private information, then their types involve
possibly infinitely nested beliefs over others types and their beliefs about others (Mertens & Zamir,
1985; Brandenburger & Dekel, 1993; Aumann, 1999; Aumann & Heifetz, 2002). 9 They are related
to recursive model structures in our prior work (Gmytrasiewicz & Durfee, 2000). The definition of
interactive state space is consistent with the notion of a completely specified state space put forward
by Aumann (1999). Similar state spaces have been proposed by others (Mertens & Zamir, 1985;
Brandenburger & Dekel, 1993).
 A = Ai  Aj is the set of joint moves of all agents.
 Ti is the transition model. The usual way to define the transition probabilities in POMDPs
is to assume that the agents actions can change any aspect of the state description. In case of IPOMDPs, this would mean actions modifying any aspect of the interactive states, including other
agents observation histories and their functions, or, if they are modeled intentionally, their beliefs
and reward functions. Allowing agents to directly manipulate other agents in such ways, however,
violates the notion of agents autonomy. Thus, we make the following simplifying assumption:
1
5. If there are more agents, say N > 2, then ISi = S N
j=1 Mj
6. Technically, according to our notation, fictitious play is actually an ensemble of models.
7. Dennet (1986) advocates ascribing rationality to other agent(s), and calls it assuming an intentional stance towards
them.
8. Note that the space of types is by far richer than that of computable models. In particular, since the set of computable
models is countable and the set of types is uncountable, many types are not computable models.
9. Implicit in the definition of interactive beliefs is the assumption of coherency (Brandenburger & Dekel, 1993).

57

fiG MYTRASIEWICZ & D OSHI

Model Non-manipulability Assumption (MNM): Agents actions do not change the other
agents models directly.
Given this simplification, the transition model can be defined as T i : S  A  S  [0, 1]
Autonomy, formalized by the MNM assumption, precludes, for example, direct mind control,
and implies that other agents belief states can be changed only indirectly, typically by changing the
environment in a way observable to them. In other words, agents beliefs change, like in POMDPs,
but as a result of belief update after an observation, not as a direct result of any of the agents
actions.10
 i is defined as before in the POMDP model.
 Oi is an observation function. In defining this function we make the following assumption:
Model Non-observability (MNO): Agents cannot observe others models directly.
Given this assumption the observation function is defined as O i : S  A  i  [0, 1].
The MNO assumption formalizes another aspect of autonomy  agents are autonomous in that
their observations and functions, or beliefs and other properties, say preferences, in intentional
models, are private and the other agents cannot observe them directly. 11
 Ri is defined as Ri : ISi  A  <. We allow the agent to have preferences over physical
states and models of other agents, but usually only the physical state will matter.
As we mentioned, we see interactive POMDPs as a subjective counterpart to an objective external view in stochastic games (Fudenberg & Tirole, 1991), and also followed in some work in
AI (Boutilier, 1999) and (Koller & Milch, 2001) and in decentralized POMDPs (Bernstein et al.,
2002; Nair et al., 2003). Interactive POMDPs represent an individual agents point of view on the
environment and the other agents, and facilitate planning and problem solving at the agents own
individual level.
5.1 Belief Update in I-POMDPs
We will show that, as in POMDPs, an agents beliefs over their interactive states are sufficient
statistics, i.e., they fully summarize the agents observation histories. Further, we need to show how
beliefs are updated after the agents action and observation, and how solutions are defined.
t1
The new belief state, bti , is a function of the previous belief state, bt1
i , the last action, ai ,
t
and the new observation, oi , just as in POMDPs. There are two differences that complicate belief
update when compared to POMDPs. First, since the state of the physical environment depends on
the actions performed by both agents the prediction of how the physical state changes has to be
made based on the probabilities of various actions of the other agent. The probabilities of others
actions are obtained based on their models. Thus, unlike in Bayesian and stochastic games, we do
not assume that actions are fully observable by other agents. Rather, agents can attempt to infer what
actions other agents have performed by sensing their results on the environment. Second, changes in
the models of other agents have to be included in the update. These reflect the others observations
and, if they are modeled intentionally, the update of the other agents beliefs. In this case, the agent
has to update its beliefs about the other agent based on what it anticipates the other agent observes
10. The possibility that agents can influence the observational capabilities of other agents can be accommodated by
including the factors that can change sensing capabilities in the set S.
11. Again, the possibility that agents can observe factors that may influence the observational capabilities of other agents
is allowed by including these factors in S.

58

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

and how it updates. As could be expected, the update of the possibly infinitely nested belief over
others types is, in general, only asymptotically computable.
Proposition 1. (Sufficiency) In an interactive POMDP of agent i, is current belief, i.e., the probability distribution over the set S  Mj , is a sufficient statistic for the past history of is observations.
t1
The next proposition defines the agent is belief update function, b ti (ist ) = P r(ist |oti , at1
i , bi ),
where ist  ISi is an interactive state. We use the belief state estimation function, SE i , as an abt1 t
breviation for belief updates for individual states so that bti = SEi (bt1
i , ai , oi ).
t1 t1 t t
t1
t1
i (bi , ai , oi , bi ) will stand for P r(bti |bi , ai , oti ). Further below we also define the set of
type-dependent optimal actions of an agent, OP T (i ).

Proposition 2. (Belief Update) Under the MNM and MNO assumptions, the belief update function
for an interactive POMDP hISi , A, Ti , i , Oi , Ri i, when mj in ist is intentional, is:
bti (ist ) = 
Ti

P

t1 )
bt1
i (is

ist1 :m
b t1
=bjt
j
P
t1
t1
(s , a , st ) 
otj

P

t1
t t1 , ot )
P r(at1
i
j |j )Oi (s , a

at1
j
t1 t1 t t
t t1 , ot )
t
(b
j j , aj , oj , bj )Oj (s , a
j

(6)

=m
b tj ,
When mj in ist is subintentional the first summation extends over ist1 : m
b t1
j
t1
t1
t1
t1 t1 t t
P r(at1
j |j ) is replaced with P r(aj |mj ), and jt (bj , aj , oj , bj ) is replaced with the
t
t
Kronecker delta function K (APPEND(ht1
j , oj ), hj ).

Above, bt1
and btj are the belief elements of jt1 and jt , respectively,  is a normalizing constant,
j
t1
t1
and P r(at1
is Bayesian rational for agent described by type
j |j ) is the probability that aj
t1
t1
1
j . This probability is equal to |OP T (j )| if aj  OP T (j ), and it is equal to zero otherwise.
We define OP T in Section 5.2.12 For the case of js subintentional model, is = (s, mj ), ht1
and
j
t respectively, O is the observation
htj are the observation histories which are part of mt1
,
and
m
j
j
j
t1
t1
t1
function in mtj , and P r(at1
|m
)
is
the
probability
assigned
by
m
to
a
j
j
j
j . APPEND returns
a string with the second argument appended to the first. The proofs of the propositions are in the
Appendix.
Proposition 2 and Eq. 6 have a lot in common with belief update in POMDPs, as should be
expected. Both depend on agent is observation and transition functions. However, since agent is
observations also depend on agent js actions, the probabilities of various actions of j have to be
included (in the first line of Eq. 6.) Further, since the update of agent js model depends on what
j observes, the probabilities of various observations of j have to be included (in the second line of
Eq. 6.) The update of js beliefs is represented by the j term. The belief update can easily be
generalized to the setting where more than one other agents co-exist with agent i.
P
12. If the agents prior belief over ISi is given by a probability density function then the
ist1 is replaced by
:
, otj , btj ) takes the form of Dirac delta function over argument bt1
, at1
an integral. In that case jt (bt1
j
j
j
, otj )  btj ).
, at1
D (SEjt (bt1
j
j

59

fiG MYTRASIEWICZ & D OSHI

5.2 Value Function and Solutions in I-POMDPs
Analogously to POMDPs, each belief state in I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state:


P
P
b
ERi (is, ai )bi (is) + 
U (i ) = max
P r(oi |ai , bi )U (hSEi (bi , ai , oi ), i i)
(7)
ai Ai

oi i

is

P
where, ERi (is, ai ) =
aj Ri (is, ai , aj )P r(aj |mj ). Eq. 7 is a basis for value iteration in IPOMDPs.
Agent is optimal action, ai , for the case of infinite horizon criterion with discounting, is an
element of the set of optimal actions for the belief state, OP T (i ), defined as:


P
P
OP T (i ) = argmax
ERi (is, ai )bi (is) + 
P r(oi |ai , bi )U (hSEi (bi , ai , oi ), bi i)
ai Ai

oi i

is

(8)
As in the case of belief update, due to possibly infinitely nested beliefs, a step of value iteration
and optimal actions are only asymptotically computable.

6. Finitely Nested I-POMDPs
Possible infinite nesting of agents beliefs in intentional models presents an obvious obstacle to
computing the belief updates and optimal solutions. Since the models of agents with infinitely
nested beliefs correspond to agent functions which are not computable it is natural to consider
finite nestings. We follow approaches in game theory (Aumann, 1999; Brandenburger & Dekel,
1993; Fagin et al., 1999), extend our previous work (Gmytrasiewicz & Durfee, 2000), and construct
finitely nested I-POMDPs bottom-up. Assume a set of physical states of the world S, and two
agents i and j. Agent is 0-th level beliefs, bi,0 , are probability distributions over S. Its 0-th level
types, i,0 , contain its 0-th level beliefs, and its frames, and analogously for agent j. 0-level types
are, therefore, POMDPs.13 0-level models include 0-level types (i.e., intentional models) and the
subintentional models, elements of SM . An agents first level beliefs are probability distributions
over physical states and 0-level models of the other agent. An agents first level types consist of
its first level beliefs and frames. Its first level models consist of the types upto level 1 and the
subintentional models. Second level beliefs are defined in terms of first level models and so on.
Formally, define spaces:
ISi,0 = S,
j,0 = {hbj,0 , bj i : bj,0  (ISj,0 )}, Mj,0 = j,0  SMj
ISi,1 = S  Mj,0 ,
j,1 = {hbj,1 , bj i : bj,1  (ISj,1 )}, Mj,1 = j,1  Mj,0
.
.
.
.
.
.
ISi,l = S  Mj,l1 , j,l = {hbj,l , bj i : bj,l  (ISj,l )}, Mj,l = j,l  Mj,l1
Definition 4. (Finitely Nested I-POMDP) A finitely nested I-POMDP of agent i, I-POMDP i,l , is:
I-POMDPi,l = hISi,l , A, Ti , i , Oi , Ri i
13. In 0-level types the other agents actions are folded into the T , O and R functions as noise.

60

(9)

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

The parameter l will be called the strategy level of the finitely nested I-POMDP. The belief update,
value function, and the optimal actions for finitely nested I-POMDPs are computed using Equation 6
and Equation 8, but recursion is guaranteed to terminate at 0-th level and subintentional models.
Agents which are more strategic are capable of modeling others at deeper levels (i.e., all levels
up to their own strategy level l), but are always only boundedly optimal. As such, these agents
could fail to predict the strategy of a more sophisticated opponent. The fact that the computability
of an agent function implies that the agent may be suboptimal during interactions has been pointed
out by Binmore (1990), and proved more recently by Nachbar and Zame (1996). Intuitively, the
difficulty is that an agents unbounded optimality would have to include the capability to model the
other agents modeling the original agent. This leads to an impossibility result due to self-reference,
which is very similar to Godels incompleteness theorem and the halting problem (Brandenburger,
2002). On a positive note, some convergence results (Kalai & Lehrer, 1993) strongly suggest that
approximate optimality is achievable, although their applicability to our work remains open.
As we mentioned, the 0-th level types are POMDPs. They provide probability distributions
over actions of the agent modeled at that level to models with strategy level of 1. Given probability
distributions over other agents actions the level-1 models can themselves be solved as POMDPs,
and provide probability distributions to yet higher level models. Assume that the number of models
considered at each level is bound by a number, M . Solving an I-POMDP i,l in then equivalent to
solving O(M l ) POMDPs. Hence, the complexity of solving an I-POMDPi,l is PSPACE-hard for
finite time horizons,14 and undecidable for infinite horizons, just like for POMDPs.
6.1 Some Properties of I-POMDPs
In this section we establish two important properties, namely convergence of value iteration and
piece-wise linearity and convexity of the value function, for finitely nested I-POMDPs.
6.1.1 C ONVERGENCE

OF

VALUE I TERATION

For an agent i and its I-POMDPi,l , we can show that the sequence of value functions, {U n }, where
n is the horizon, obtained by value iteration defined in Eq. 7, converges to a unique fixed-point, U  .
Let us define a backup operator H : B  B such that U n = HU n1 , and B is the set of all
bounded value functions. In order to prove the convergence result, we first establish some of the
properties of H.
Lemma 1 (Isotonicity). For any finitely nested I-POMDP value functions V and U , if V  U , then
HV  HU .
The proof of this lemma is analogous to one due to Hauskrecht (1997), for POMDPs. It is
also sketched in the Appendix. Another important property exhibited by the backup operator is the
property of contraction.
Lemma 2 (Contraction). For any finitely nested I-POMDP value functions V , U and a discount
factor   (0, 1), ||HV  HU ||  ||V  U ||.
The proof of this lemma is again similar to the corresponding one in POMDPs (Hausktecht,
1997). The proof makes use of Lemma 1. ||  || is the supremum norm.
14. Usually PSPACE-complete since the number of states in I-POMDPs is likely to be larger than the time horizon
(Papadimitriou & Tsitsiklis, 1987).

61

fiG MYTRASIEWICZ & D OSHI

Under the contraction property of H, and noting that the space of value functions along with
the supremum norm forms a complete normed space (Banach space), we can apply the Contraction
Mapping Theorem (Stokey & Lucas, 1989) to show that value iteration for I-POMDPs converges
to a unique fixed point (optimal solution). The following theorem captures this result.
Theorem 1 (Convergence). For any finitely nested I-POMDP, the value iteration algorithm starting from any arbitrary well-defined value function converges to a unique fixed-point.
The detailed proof of this theorem is included in the Appendix.
As in the case of POMDPs (Russell & Norvig, 2003), the error in the iterative estimates, U n , for
finitely nested I-POMDPs, i.e., ||U n  U  ||, is reduced by the factor of at least  on each iteration.
Hence, the number of iterations, N , needed to reach an error of at most  is:
N = dlog(Rmax /(1  ))/ log(1/)e

(10)

where Rmax is the upper bound of the reward function.
6.1.2 P IECEWISE L INEARITY

AND

C ONVEXITY

Another property that carries over from POMDPs to finitely nested I-POMDPs is the piecewise
linearity and convexity (PWLC) of the value function. Establishing this property allows us to decompose the I-POMDP value function into a set of alpha vectors, each of which represents a policy
tree. The PWLC property enables us to work with sets of alpha vectors rather than perform value
iteration over the continuum of agents beliefs. Theorem 2 below states the PWLC property of the
I-POMDP value function.
Theorem 2 (PWLC). For any finitely nested I-POMDP, U is piecewise linear and convex.
The complete proof of Theorem 2 is included in the Appendix. The proof is similar to one
due to Smallwood and Sondik (1973) for POMDPs and proceeds by induction. The basis case is
established by considering the horizon 1 value function. Showing the PWLC for the inductive step
requires substituting the belief update (Eq. 6) into Eq. 7, followed by factoring out the belief from
both terms of the equation.

7. Example: Multi-agent Tiger Game
To illustrate optimal sequential behavior of agents in multi-agent settings we apply our I-POMDP
framework to the multi-agent tiger game, a traditional version of which we described before.
7.1 Definition
Let us denote the actions of opening doors and listening as OR, OL and L, as before. TL and
TR denote states corresponding to tiger located behind the left and right door, respectively. The
transition, reward and observation functions depend now on the actions of both agents. Again, we
assume that the tiger location is chosen randomly in the next time step if any of the agents opened
any doors in the current step. We also assume that the agent hears the tigers growls, GR and GL,
with the accuracy of 85%. To make the interaction more interesting we added an observation of
door creaks, which depend on the action executed by the other agent. Creak right, CR, is likely due
62

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

to the other agent having opened the right door, and similarly for creak left, CL. Silence, S, is a good
indication that the other agent did not open doors and listened instead. We assume that the accuracy
of creaks is 90%. We also assume that the agents payoffs are analogous to the single agent versions
described in Section 3.2 to make these cases comparable. Note that the result of this assumption is
that the other agents actions do not impact the original agents payoffs directly, but rather indirectly
by resulting in states that matter to the original agent. Table 1 quantifies these factors.

hai , aj i
hOL, i
hOR, i
h, OLi
h, ORi
hL, Li
hL, Li

State
*
*
*
*
TL
TR

TL
0.5
0.5
0.5
0.5
1.0
0

TR
0.5
0.5
0.5
0.5
0
1.0

hai , aj i
hOR, ORi
hOL, OLi
hOR, OLi
hOL, ORi
hL, Li
hL, ORi
hOR, Li
hL, OLi
hOL, Li

Transition function: Ti = Tj

TL
10
-100
10
-100
-1
-1
10
-1
-100

TR
-100
10
-100
10
-1
-1
-100
-1
10

hai , aj i
hOR, ORi
hOL, OLi
hOR, OLi
hOL, ORi
hL, Li
hL, ORi
hOR, Li
hL, OLi
hOL, Li

TL
10
-100
-100
10
-1
10
-1
-100
-1

TR
-100
10
10
-100
-1
-100
-1
10
-1

Reward functions of agents i and j

hai , aj i
hL, Li
hL, Li
hL, OLi
hL, OLi
hL, ORi
hL, ORi
hOL, i
hOR, i

State
TL
TR
TL
TR
TL
TR



h GL, CL i
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
1/6
1/6

h GL, CR i
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
1/6
1/6

h GL, S i
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
1/6
1/6

h GR, CL i
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
1/6
1/6

h GR, CR i
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
1/6
1/6

h GR, S i
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
1/6
1/6

hai , aj i
hL, Li
hL, Li
hOL, Li
hOL, Li
hOR, Li
hOR, Li
h, OLi
h, ORi

State
TL
TR
TL
TR
TL
TR



h GL, CL i
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
1/6
1/6

h GL, CR i
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
1/6
1/6

h GL, S i
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
1/6
1/6

h GR, CL i
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
1/6
1/6

h GR, CR i
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
1/6
1/6

h GR, S i
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
1/6
1/6

Observation functions of agents i and j.
Table 1: Transition, reward, and observation functions for the multi-agent Tiger game.
When an agent makes its choice in the multi-agent tiger game, it considers what it believes
about the location of the tiger, as well as whether the other agent will listen or open a door, which in
turn depends on the other agents beliefs, reward function, optimality criterion, etc. 15 In particular,
if the other agent were to open any of the doors the tiger location in the next time step would be
chosen randomly. Thus, the information obtained from hearing the previous growls would have to
be discarded. We simplify the situation by considering is I-POMDP with a single level of nesting,
assuming that all of the agent js properties, except for beliefs, are known to i, and that js time
horizon is equal to is. In other words, is uncertainty pertains only to js beliefs and not to its
frame. Agent is interactive state space is, ISi,1 = S  j,0 , where S is the physical state, S={TL,
15. We assume an intentional model of the other agent here.

63

fiG MYTRASIEWICZ & D OSHI

TR}, and j,0 is a set of intentional models of agent js, each of which differs only in js beliefs
over the location of the tiger.
7.2 Examples of the Belief Update
In Section 5, we presented the belief update equation for I-POMDPs (Eq. 6). Here we consider
examples of beliefs, bi,1 , of agent i, which are probability distributions over S   j,0 . Each 0-th
level type of agent j, j,0  j,0 , contains a flat belief as to the location of the tiger, which can be
represented by a single probability assignment  bj,0 = pj (T L).
0.506
0.504

0.504

Pr(TL,p
Pr(TL,b_j) )
j

Pr(TL,p
Pr(TL,b_j)

j

)

0.506
0.502
0.5
0.498

0.502
0.5
0.498

0.496
0.496

0.494
0

0.2

0.4

0.6

0.8

1

0.494
0

0.2

0.4

0.6

0.8

1

0.8

1

pb_j
j (TL)

0.506

0.506

0.504

0.504

j

)

0.502

Pr(TR,p
Pr(TR,b_j)

Pr(TR,p
Pr(TR,b_j)

j

)

pjb_j
(TL)
j(TL)

0.5
0.498

0.502
0.5
0.498

0.496
0.494

0.496

0

0.2

0.4 0.6 0.8
ppb_j
(TL)
(TR)
(TL)

1
0.494

jj

0

0.2

0.4

0.6

p j (TL)
b_j

(i)

(ii)

Figure 5: Two examples of singly nested belief states of agent i. In each case i has no information
about the tigers location. In (i) agent i knows that j does not know the location of the
tiger; the single point (star) denotes a Dirac delta function which integrates to the height
of the point, here 0.5 . In (ii) agent i is uninformed about js beliefs about tigers location.

In Fig. 5 we show some examples of level 1 beliefs of agent i. In each case i does not know
the location of the tiger so that the marginals in the top and bottom sections of the figure sum up to
0.5 for probabilities of TL and TR each. In Fig. 5(i), i knows that j assigns 0.5 probability to tiger
being behind the left door. This is represented using a Dirac delta function. In Fig. 5(ii), agent i is
uninformed about js beliefs. This is represented as a uniform probability density over all values of
the probability j could assign to state TL.
To make the presentation of the belief update more transparent we decompose the formula in
Eq. 6 into two steps:
64

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

t1
 Prediction: When agent i performs an action at1
i , and given that agent j performs aj , the
predicted belief state is:

bbt (ist ) = P r(ist |at1 , at1 , bt1 ) = P t1 bt1 bt bt1 (ist1 )P r(at1 |t1 )
i
j
j
i
j
i
is
|j =j i
P
T (st1 , at1 , st ) Oj (st , at1 , otj )

(11)

otj

t1 t t
jt (bt1
j , a j , o j , bj )

 Correction: When agent i perceives an observation, o ti , the predicted belief states,
t1 t1
P r(|at1
i , aj , bi ), are combined according to:
bti (ist ) = P r(ist |oti , ait1 , bt1
i )=

X

t1 t1
Oi (st , at1 , oti )P r(ist |at1
i , a j , bi )

(12)

at1
j

where  is the normalizing constant.

t1

t

0.496
0.494
0

0.2

0.4 0.6 0.8
pb_j(TL)

1

j

0.496
0.494
0.4 0.6 0.8
pb_j(TL)

1

0.8

1

0.7
0.6
0.5
0.4
0.3
L,<GL,S>
0.2
0.1
L,<GL,S> 0
0
0.8
1

L,<GL,S>

0

0.2

0.4

0.6

pjb_j(TL)

<GL,S>

<GL,S>
0.2

0.4

0.6

0.8

L,<GL,S>

0.1

L,<GL,S>

0.06

0.8

1

0.4 0.6 0.8
pb_j(TL)

1

0.02

0.01

0.005

0.04

L,<GL,S>
0.2

0.4

pjb_j
(TL)

0.6

pjb_j
(TL)

(b)

0.6

pjb_j(TL)

0.015

0.08

0

1

0.4

0.025

0.12

0.02
0

0.2

L,<GL,S>

0.14

0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05

j

(a)

0.6

pjb_j
(TL)

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

j)

j)
0.4

Pr(TR,pj )

0.498

Pr(TR,b_j)

j)

Pr(TR,p
Pr(TR,b_j)

L,(L,GL)

0.5

0.2

0.2

L,(L,GR)

0.504

0

<GL,S>

0

0.506
0.502

L,<GL,S>

0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05

Pr(TL,p
Pr(TL,b_j)

L,(L,GR)

0.5
0.498

bi

Pr(TR,b_j)
Pr(TR,p
)
j

0.502

t+1

bi

<GL,S>

Pr(TL,p
Pr(TL,b_j)

0.504

Pr(TL,p
Pr(TL,b_j) )
j

Pr(TL,p
Pr(TL,b_j)

j)

0.506

t

bi

L,(L,GL)

Pr(TR,p
Pr(TR,b_j) )
j

bi

(c)

0.8

1

0
0

0.2

j

(d)

Figure 6: A trace of the belief update of agent i. (a) depicts the prior. (b) is the result of prediction
given is listening action, L, and a pair denoting js action and observation. i knows that
j will listen and could hear tigers growl on the right or the left, and that the probabilities
j would assign to TL are 0.15 or 0.85, respectively. (c) is the result of correction after
i observes tigers growl on the left and no creaks, hGL,Si. The probability i assigns to
TL is now greater than TR. (d) depicts the results of another update (both prediction and
correction) after another listen action of i and the same observation, hGL,Si.
Each discrete point above denotes, again, a Dirac delta function which integrates to the height of
the point.
In Fig. 6, we display the example trace through the update of singly nested belief. In the first
column of Fig. 6, labeled (a), is an example of agent is prior belief we introduced before, according
65

fiG MYTRASIEWICZ & D OSHI

to which i knows that j is uninformed of the location of the tiger. 16 Let us assume that i listens and
hears a growl from the left and no creaks. The second column of Fig. 6, (b), displays the predicted
belief after i performs the listen action (Eq. 11). As part of the prediction step, agent i must solve
js model to obtain js optimal action when its belief is 0.5 (term P r(a t1
j |j ) in Eq. 11). Given the
value function in Fig. 3, this evaluates to probability of 1 for listen action, and zero for opening of
any of the doors. i also updates js belief given that j listens and hears the tiger growling from either
t1 t t
the left, GL, or right, GR, (term jt (bt1
j , aj , oj , bj ) in Eq. 11). Agent js updated probabilities
for tiger being on the left are 0.85 and 0.15, for js hearing GL and GR, respectively. If the tiger is
on the left (top of Fig. 6 (b)) js observation GL is more likely, and consequently js assigning the
probability of 0.85 to state TL is more likely (i assigns a probability of 0.425 to this state.) When
the tiger is on the right j is more likely to hear GR and i assigns the lower probability, 0.075, to
js assigning a probability 0.85 to tiger being on the left. The third column, (c), of Fig. 6 shows
the posterior belief after the correction step. The belief in column (b) is updated to account for is
hearing a growl from the left and no creaks, hGL,Si. The resulting marginalised probability of the
tiger being on the left is higher (0.85) than that of the tiger being on the right. If we assume that in
the next time step i again listens and hears the tiger growling from the left and no creaks, the belief
state depicted in the fourth column of Fig. 6 results.
In Fig. 7 we show the belief update starting from the prior in Fig. 5 (ii), according to which
agent i initially has no information about what j believes about the tigers location.
The traces of belief updates in Fig. 6 and Fig. 7 illustrate the changing state of information agent
i has about the other agents beliefs. The benefit of representing these updates explicitly is that, at
each stage, is optimal behavior depends on its estimate of probabilities of js actions. The more
informative these estimates are the more value agent i can expect out of the interaction. Below, we
show the increase in the value function for I-POMDPs compared to POMDPs with the noise factor.
7.3 Examples of Value Functions
This section compares value functions obtained from solving a POMDP with a static noise factor,
accounting for the presence of another agent,17 to value functions of level-1 I-POMDP. The advantage of more refined modeling and update in I-POMDPs is due to two factors. First is the ability to
keep track of the other agents state of beliefs to better predict its future actions. The second is the
ability to adjust the other agents time horizon as the number of steps to go during the interaction
decreases. Neither of these is possible within the classical POMDP formalism.
We continue with the simple example of I-POMDPi,1 of agent i. In Fig. 8 we display is
value function for the time horizon of 1, assuming that is initial belief as to the value j assigns
to TL, pj (T L), is as depicted in Fig. 5 (ii), i.e. i has no information about what j believes about
tigers location. This value function is identical to the value function obtained for an agent using
a traditional POMDP framework with noise, as well as single agent POMDP which we described
in Section 3.2. The value functions overlap since agents do not have to update their beliefs and
the advantage of more refined modeling of agent j in is I-POMDP does not become apparent. Put
another way, when agent i models j using an intentional model, it concludes that agent j will open
each door with probability 0.1 and listen with probability 0.8. This coincides with the noise factor
we described in Section 3.2.
16. The points in Fig. 7 again denote Dirac delta functions which integrate to the value equal to the points height.
17. The POMDP with noise is the same as level-0 I-POMDP.

66

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

t1

t
bi

bi

L(L,GR)
L(L,GL)
0.5

0.5

L(L,GL)
0.4

0.4

L(L,GR)

0.2

0.5

0.3

0.2

0.498
0.1

0.1

L(L,GR)

0.496
0.494
0

0.2

0.4

0.6

0.8

0

L(OL/OR,*)

1

pjb_j(TL)

0

0.504
0.502

0.494
0

0.2

0.4

0.6

0.8

1

0.8

0

1

0

Pr(TL,b_j)

L(L,GR)

pjb_j(TL)

0.4

0.6

0.8

1

pjb_j
(TL)
0.02275
0.0227
0.02265

0.0226

0.0226

0.02255
0.0225

0.02245

0.02255
0.0225
0.02245

0.0224

0.0224

0.02235

0.02235

0.0223

L(OL/OR,*)

0.2

0.02265

Pr(TL, pj )

L(OL/OR,*)

0.496

0.6

0.0227

L(OL/OR,*)

0.498

0.4

0.02275

L(L,GL)

0.5

0.2

pjb_j
(TL)

L(L,GL)

0.506

Pr(TR,
pj )
Pr(TR,b_j)

0.3

Pr(TR,b_j)
Pr(TR,p
j)

Pr(TL,p
)
Pr(TL,b_j)
j

0.502

Pr(TR,b_j)
Pr(TR,p
j)

0.504

Pr(TL,b_j)
Pr(TL,
pj )

0.506

0.0223

0.02225

0.02225
0

0.2

0.4

0.6

0.8

1

0

pjb_j
(TL)

(a)

0.2

0.4

0.6

0.8

1

pjb_j
(TL)

(b)
<GL,S>

t

bi

0.3

1.8

1.6
0.25

Pr(TR,pj )

1.2

Pr(TR,b_j)

Pr(TL,p
)
Pr(TL,b_j)
j

1.4

1

0.8

0.2

0.15

0.1

0.6

0.4
0.05
0.2
0

0
0

0.2

0.4

0.6

pjb_j
(TL)

0.8

0

1

0.2

0.4

0.6

0.8

1

pj b_j
(TL)

(c)

Figure 7: A trace of the belief update of agent i. (a) depicts the prior according to which i is
uninformed about js beliefs. (b) is the result of the prediction step after is listening
action (L). The top half of (b) shows is belief after it has listened and given that j also
listened. The two observations j can make, GL and GR, each with probability dependent
on the tigers location, give rise to flat portions representing what i knows about js belief
in each case. The increased probability i assigns to js belief between 0.472 and 0.528 is
due to js updates after it hears GL and after it hears GR resulting in the same values in
this interval. The bottom half of (b) shows is belief after i has listened and j has opened
the left or right door (plots are identical for each action and only one of them is shown). i
knows that j has no information about the tigers location in this case. (c) is the result of
correction after i observes tigers growl on the left and no creaks hGL,Si. The plots in (c)
are obtained by performing a weighted summation of the plots in (b). The probability i
assigns to TL is now greater than TR, and information about js beliefs allows i to refine
its prediction of js action in the next time step.

67

fiG MYTRASIEWICZ & D OSHI

L
OR

OL

10

Value Function (U)

8

6

4

2

0

0

0.2

0.4

0.6

0.8

1

pp_i(TL)
i (TL)
Level 1 I-POMDP

POMDP with noise

Figure 8: For time horizon of 1 the value functions obtained from solving a singly nested I-POMDP
and a POMDP with noise factor overlap.
L\();OL\(<GR,S>),L\(?)

L\();OR\(<GL,S>),L\(?)

L\();L\(<GL,*>),OL\(<GR,*>)

OL\();L\(*)

L\();L\(*)

L\();OR\(<GL,*>),L\(<GR,*>)

L\();L\(GL),OL\(GR)

L\();OR\(GL),L\(GR)

OR\();L\(*)

8

Value Function (U)

6

4

2

0

-2
0

0.2

0.4

0.6

0.8

1

pp_i(TL)
i (TL)
Level 1 I-POMDP

POMDP with noise

Figure 9: Comparison of value functions obtained from solving an I-POMDP and a POMDP with
noise for time horizon of 2. I-POMDP value function dominates due to agent i adjusting
the behavior of agent j to the remaining steps to go in the interaction.

68

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

8

Value Function (U)

7
6
5
4
3
2
1
0

0.2

0.4

0.6

0.8

1

p i (TL)
p_i(TL)
Level 1 I-POMDP

POMDP with noise

Figure 10: Comparison of value functions obtained from solving an I-POMDP and a POMDP with
noise for time horizon of 3. I-POMDP value function dominates due to agent is adjusting js remaining steps to go, and due to is modeling js belief update. Both factors
allow for better predictions of js actions during interaction. The descriptions of individual policies were omitted for clarity; they can be read off of Fig. 11.

In Fig. 9 we display is value functions for the time horizon of 2. The value function of
I-POMDPi,1 is higher than the value function of a POMDP with a noise factor. The reason is
not related to the advantages of modeling agent js beliefs  this effect becomes apparent at the time
horizon of 3 and longer. Rather, the I-POMDP solution dominates due to agent i modeling js time
horizon during interaction: i knows that at the last time step j will behave according to its optimal
policy for time horizon of 1, while with two steps to go j will optimize according to its 2 steps to go
policy. As we mentioned, this effect cannot be modeled using a POMDP with a static noise factor
included in the transition function.
Fig. 10 shows a comparison between the I-POMDP and the noisy POMDP value functions for
horizon 3. The advantage of more refined agent modeling within the I-POMDP framework has
increased.18 Both factors, is adjusting js steps to go and is modeling js belief update during
interaction are responsible for the superiority of values achieved using the I-POMDP. In particular,
recall that at the second time step is information as to js beliefs about the tigers location is as
depicted in Fig. 7 (c). This enables i to make a high quality prediction that, with two steps left to
go, j will perform its actions OL, L, and OR with probabilities 0.009076, 0.96591 and 0.02501,
respectively (recall that for POMDP with noise these probabilities remained unchanged at 0.1, 0,8,
and 0.1, respectively.)
Fig. 11 shows agent is policy graph for time horizon of 3. As usual, it prescribes the optimal
first action depending on the initial belief as to the tigers location. The subsequent actions depend
on the observations received. The observations include creaks that are indicative of the other agents
18. Note that I-POMDP solution is not as good as the solution of a POMDP for an agent operating alone in the environment shown in Fig. 3.

69

fiG MYTRASIEWICZ & D OSHI

[0  0.029)
OL

[0.029  0.089)

[0.089  0.211)

L
*

L

<GR,S>

<GL,CL/CR>
<GR,*>

[0.211  0.789)

[0.789  0.911)

L

*

OL

OR
*

<GR,S>
<GL,CL\CR>

L

L
<GR,*>

[0.971  1]

L

<GR,CL\CR>
<GR,CL\CR>
<GL,CL\CR>
<GL,S>
<GR,*> <GL,*>
<GL,*>
<GR,S>
<GL,S>

<GL,S>
<GR,CL\CR>

OL

[0.911  0.971)

L

<GL,*>

L
*

<GR,*>

OR
<GL,*>

*

OR

L

Figure 11: The policy graph corresponding to the I-POMDP value function in Fig. 10.
having opened a door. The creaks contain valuable information and allow the agent to make more
refined choices, compared to ones in the noisy POMDP in Fig. 4. Consider the case when agent i
starts out with fairly strong belief as to the tigers location, decides to listen (according to the four
off-center top row L nodes in Fig. 11) and hears a door creak. The agent is then in the position to
open either the left or the right door, even if that is counter to its initial belief. The reason is that the
creak is an indication that the tigers position has likely been reset by agent j and that j will then
not open any of the doors during the following two time steps. Now, two growls coming from the
same door lead to enough confidence to open the other door. This is because the agent is hearing
of tigers growls are indicative of the tigers position in the state following the agents actions,
Note that the value functions and the policy above depict a special case of agent i having no
information as to what probability j assigns to tigers location (Fig. 5 (ii)). Accounting for and
visualizing all possible beliefs i can have about js beliefs is difficult due to the complexity of the
space of interactive beliefs. As our ongoing work indicates, a drastic reduction in complexity is
possible without loss of information, and consequently representation of solutions in a manageable
number of dimensions is indeed possible. We will report these results separately.

8. Conclusions
We proposed a framework for optimal sequential decision-making suitable for controlling autonomous
agents interacting with other agents within an uncertain environment. We used the normative
paradigm of decision-theoretic planning under uncertainty formalized as partially observable Markov
decision processes (POMDPs) as a point of departure. We extended POMDPs to cases of agents
interacting with other agents by allowing them to have beliefs not only about the physical environment, but also about the other agents. This could include beliefs about the others abilities, sensing
capabilities, beliefs, preferences, and intended actions. Our framework shares numerous properties
with POMDPs, has analogously defined solutions, and reduces to POMDPs when agents are alone
in the environment.
In contrast to some recent work on DEC-POMDPs (Bernstein et al., 2002; Nair et al., 2003),
and to work motivated by game-theoretic equilibria (Boutilier, 1999; Hu & Wellman, 1998; Koller
70

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

& Milch, 2001; Littman, 1994), our approach is subjective and amenable to agents independently
computing their optimal solutions.
The line of work presented here opens an area of future research on integrating frameworks for
sequential planning with elements of game theory and Bayesian learning in interactive settings. In
particular, one of the avenues of our future research centers on proving further formal properties of
I-POMDPs, and establishing clearer relations between solutions to I-POMDPs and various flavors
of equilibria. Another concentrates on developing efficient approximation techniques for solving
I-POMDPs. As for POMDPs, development of approximate approaches to I-POMDPs is crucial for
moving beyond toy problems. One promising approximation technique we are working on is particle
filtering. We are also devising methods for representing I-POMDP solutions without assumptions
about whats believed about other agents beliefs. As we mentioned, in spite of the complexity of the
interactive state space, there seem to be intuitive representations of belief partitions corresponding
to optimal policies, analogous to those for POMDPs. Other research issues include the suitable
choice of priors over models,19 and the ways to fulfill the absolute continuity condition needed for
convergence of probabilities assigned to the alternative models during interactions (Kalai & Lehrer,
1993).

Acknowledgments
This research is supported by the National Science Foundation CAREER award IRI-9702132, and
NSF award IRI-0119270.

Appendix A. Proofs
Proof of Propositions 1 and 2. We start with Proposition 2, by applying the Bayes Theorem:

t1
bti (ist ) = P r(ist |oti , at1
i , bi ) =

)
,bt1
P r(ist ,oti |at1
i
i
t1 t1
t
P r(oi |ai ,bi )

P
t1 )
=  ist1 bit1 (ist1 )P r(ist , oti |at1
i , is
P
P
t1
t1 )P r(at1 |at1 , ist1 )
=  ist1 bit1 (ist1 ) at1 P r(ist , oti |at1
i , aj , is
j
i
j
P
P
(13)
t1
t1 )P r(at1 |ist1 )
,
is
,
a
=  ist1 bit1 (ist1 ) at1 P r(ist , oti |at1
j
j
i
j
P
P
t1
i
t t1 , ist1 )P r(ist |at1 , ist1 )
=  ist1 bit1 (ist1 ) at1 P r(at1
j |mj )P r(ot |is , a
j
P
P
t1
i
t t1 )P r(ist |at1 , ist1 )
=  ist1 bit1 (ist1 ) at1 P r(at1
j |mj )P r(ot |is , a
j
P
P
t1
t t1 , ot )P r(ist |at1 , ist1 )
=  ist1 bit1 (ist1 ) at1 P r(at1
i
j |mj )Oi (s , a
j

19. We are looking at Kolmogorov complexity (Li & Vitanyi, 1997) as a possible way to assign priors.

71

fiG MYTRASIEWICZ & D OSHI

To simplify the term P r(ist |at1 , ist1 ) let us substitute the interactive state ist with its components. When mj in the interactive states is intentional: ist = (st , jt ) = (st , btj , bjt ).

P r(ist |at1 , ist1 ) = P r(st , btj , bjt |at1 , ist1 )
= P r(btj |st , bjt , at1 , ist1 )P r(st , bjt |at1 , ist1 )
= P r(btj |st , bjt , at1 , ist1 )P r(bjt |st , at1 , ist1 )P r(st |at1 , ist1 )
= P r(btj |st , bjt , at1 , ist1 )I(bjt1 , bjt )Ti (st1 , at1 , st )
(14)
b tj ).
When mj is subintentional: ist = (st , mtj ) = (st , htj , m

P r(ist |at1 , ist1 ) = P r(st , htj , m
b tj |at1 , ist1 )
b tj |at1 , ist1 )
b tj , at1 , ist1 )P r(st , m
= P r(htj |st , m
b tj , at1 , ist1 )P r(bjt |st , at1 , ist1 )P r(st |at1 , ist1 )
= P r(htj |st , m
t
t
= P r(hj |s , m
b tj , at1 , ist1 )I(m
b tj )Ti (st1 , at1 , st )
b t1
(14)
j ,m

The joint action pair, at1 , may change the physical state. The third term on the right-hand
side of Eqs. 14 and 140 above captures this transition. We utilized the MNM assumption to replace
the second terms of the equations with boolean identity functions, I( bjt1 , bjt ) and I(m
b t1
b tj )
j ,m
respectively, which equal 1 if the two frames are identical, and 0 otherwise. Let us turn our attention
to the first terms. If mj in ist and ist1 is intentional:
P
P r(btj |st , bjt , at1 , ist1 ) = ot P r(btj |st , bjt , at1 , ist1 , otj )P r(otj |st , bjt , at1 , ist1 )
Pj
= ot P r(btj |st , bjt , at1 , ist1 , otj )P r(otj |st , bjt , at1 )
Pj
t1 t t
t1 , ot )
= ot jt (bt1
j
j , aj , oj , bj )Oj (st , a

(15)

j

Else if it is subintentional:

P r(htj |st , m
b tj , at1 , ist1 ) =

=

=

P

t

Po j
t

Po j
otj

b tj , at1 , ist1 )
b tj , at1 , ist1 , otj )P r(otj |st , m
P r(htj |st , m

b tj , at1 )
b tj , at1 , ist1 , otj )P r(otj |st , m
P r(htj |st , m

t
t
t1 , ot )
K (APPEND(ht1
j
j , oj ), hj )Oj (st , a

(15)

t1 t
In Eq. 15, the first term on the right-hand side is 1 if agent js belief update, SE j (bt1
j , a j , oj )
generates a belief state equal to btj . Similarly, in Eq. 150 , the first term is 1 if appending the otj
to ht1
results in htj . K is the Kronecker delta function. In the second terms on the right-hand
j
side of the equations, the MNO assumption makes it possible to replace P r(o t |st , bt , at1 ) with
j

j

Oj (st , at1 , otj ), and P r(otj |st , m
b tj , at1 ) with Oj (st , at1 , otj ) respectively.
Let us now substitute Eq. 15 into Eq. 14.
P
t1 t t
t t1 , ot )I(
bt1 , bt )Ti (st1 , at1 , st )
P r(ist |at1 , ist1 ) = ot jt (bt1
j
j
j , aj , oj , bj )Oj (s , a
j
j
(16)
0
0
Substituting Eq. 15 into Eq. 14 we get,
P
t
t
t t1 , ot )I(m
P r(ist |at1 , ist1 ) = ot K (APPEND(ht1
b t1
b tj )
j
j , oj ), hj )Oj (s , a
j ,m
j

Ti (st1 , at1 , st )

(16)

72

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

Replacing Eq. 16 into Eq. 13 we get:
P
P
t1 t t
t1
t1 )
P r(at1
|jt1 )Oi (st , at1 , oti ) ot jt (bt1
ist1 bi (is
j , a j , o j , bj )
j
at1
j
j
Oj (st , at1 , otj )I(bjt1 , bjt )Ti (st1 , at1 , st )

bti (ist ) = 

P

(17)

Similarly, replacing Eq. 160 into Eq. 13 we get:

P
P
t1
t t1 , ot )
t1 )
P r(at1
bti (ist ) =  ist1 bt1
i
j |mj )Oi (s , a
i (is
at1
j
P
t1 t
t1
t
t
t1
t
bj ,m
b tj )Ti (st1 , at1 , st )
 ot K (APPEND(hj , oj ), hj )Oj (s , a , oj )I(m
j

We arrive at the final expressions for the belief update by removing the terms
I(m
b t1
b tj ) and changing the scope of the first summations.
j ,m
When mj in the interactive states is intentional:

I( bjt1 , bjt )

P
P
t1
t t1 , ot )
bt1 (ist1 ) at1 P r(at1
bti (ist ) =  ist1 :m
i
j |j )Oi (s , a
b t1
=bjt i
j
j
P
t1 t1 t t
t1
t1
t
t
t1
t
 ot jt (bj , aj , oj , bj )Oj (s , a , oj )Ti (s , a , s )

(170 )
and

(18)

j

Else, if it is subintentional:
P
P
t1
t1
t t1 , ot )
(ist1 ) at1 P r(at1
bti (ist ) =  ist1 :m
t bi
i
j |mj )Oi (s , a
=
m
b
b t1
j
j
j
P
t ), ht )O (st , at1 , ot )T (st1 , at1 , st )
,
o
 ot K (APPEND(ht1
j
j
j
j i
j

(19)

j

Since proposition 2 expresses the belief bti (ist ) in terms of parameters of the previous time step
only, Proposition 1 holds as well.
Before we present the proof of Theorem 1 we note that the Equation 7, which defines value
iteration in I-POMDPs, can be rewritten in the following form, U n = HU n1 . Here, H : B  B
is a backup operator, and is defined as,
HU n1 (i ) = max h(i , ai , U n1 )
ai Ai

where h : i  Ai  B  R is,
h(i , ai , U ) =

P
is

bi (is)ERi (is, ai ) + 

P

oi

P r(oi |ai , bi )U (hSEi (bi , ai , oi ), i i)

and where B is the set of all bounded value functions U . Lemmas 1 and 2 establish important
properties of the backup operator. Proof of Lemma 1 is given below, and proof of Lemma 2 follows
thereafter.
Proof of Lemma 1. Select arbitrary value functions V and U such that V ( i,l )  U (i,l ) i,l 
i,l . Let i,l be an arbitrary type of agent i.
73

fiG MYTRASIEWICZ & D OSHI





P

P

HV (i,l ) = max
oi P r(oi |ai , bi )V (hSEi,l (bi , ai , oi ), i i)
is bi (is)ERi (is, ai ) + 
ai Ai
P
P
= is bi (is)ERi (is, ai ) +  oi P r(oi |ai , bi )V (hSEi,l (bi , ai , oi ), i i)
P
P



 is b
i (is)ERi (is, ai ) + 
oi P r(oi |ai , bi )U (hSEi,l (bi , ai , oi ), i i)

P
P
 max
oi P r(oi |ai , bi )U (hSEi,l (bi , ai , oi ), i i)
is bi (is)ERi (is, ai ) + 
ai Ai

= HU (i,l )

Since i,l is arbitrary, HV  HU .
Proof of Lemma 2. Assume two arbitrary well defined value functions V and U such that V  U .
From Lemma 1 it follows that HV  HU . Let i,l be an arbitrary type of agent i. Also, let ai be
the action that optimizes HU (i,l ).
0  HU (i,l )  HV (i,l )



P

= max sumis bi (is)ERi (is, ai ) +  oi P r(oi |ai , bi )U (SEi,l (bi , ai , oi ), hi i) 
ai Ai

P
P
max
is bi (is)ERi (is, ai ) + 
oi P r(oi |ai , bi )V (SEi,l (bi , ai , oi ), hi i)
ai Ai
P
P
 is bi (is)ERi (is, ai ) +  oi P r(oi |ai , bi )U (SEi,l (bi , ai , oi ), hi i) 
P
P



oi P r(oi |ai , bi )V (SEi,l (bi , ai , oi ), hi i)
is bi (is)ERi (is, ai )  
P


=  oi P r(oi |ai , bi )U (SEi,l (bi , ai , oi ), hi i)
P

 oi P r(oi |ai , bi )V (SE

 i,l (bi , ai , oi ), hi i)
P



=  oi P r(oi |ai , bi ) U (SEi,l (bi , ai , oi ), hi i)  V (SEi,l (bi , ai , oi ), hi i)
P
  oi P r(oi |ai , bi )||U  V ||
= ||U  V ||

As the supremum norm is symmetrical, a similar result can be derived for HV ( i,l )  HU (i,l ).
Since i,l is arbitrary, the Contraction property follows, i.e. ||HV  HU ||  ||V  U ||.
Lemmas 1 and 2 provide the stepping stones for proving Theorem 1. Proof of Theorem 1 follows
from a straightforward application of the Contraction Mapping Theorem. We state the Contraction
Mapping Theorem (Stokey & Lucas, 1989) below:
Theorem 3 (Contraction Mapping Theorem). If (S, ) is a complete metric space and T : S  S
is a contraction mapping with modulus , then
1. T has exactly one fixed point U  in S, and
2. The sequence {U n } converges to U  .
Proof of Theorem 1 follows.
74

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

Proof of Theorem 1. The normed space (B, ||  ||) is complete w.r.t the metric induced by the supremum norm. Lemma 2 establishes the contraction property of the backup operator, H. Using Theorem 3, and substituting T with H, convergence of value iteration in I-POMDPs to a unique fixed
point is established.
We go on to the piecewise linearity and convexity (PWLC) property of the value function.
We follow the outlines of the analogous proof for POMDPs in (Hausktecht, 1997; Smallwood &
Sondik, 1973).
Let  : IS  R be a real-valued and bounded function. Let the space of such real-valued
bounded functions be B(IS). We will now define an inner product.
Definition 5 (Inner product). Define the inner product, h, i : B(IS)  (IS)  R, by
X
h, bi i =
bi (is)(is)
is

The next lemma establishes the bilinearity of the inner product defined above.
Lemma 3 (Bilinearity). For any s, t  R, f, g  B(IS), and b,   (IS) the following equalities
hold:
hsf + tg, bi = shf, bi + thg, bi
hf, sb + ti = shf, bi + thf, i
We are now ready to give the proof of Theorem 2. Theorem 4 restates Theorem 2 mathematically, and its proof follows thereafter.
Theorem 4 (PWLC). The value function, U n , in finitely nested I-POMDP is piece-wise linear and
convex (PWLC). Mathematically,
U n (i,l ) = max
n


X

bi (is)n (is)

n = 1, 2, ...

is

Proof of Theorem 4. Basis Step: n = 1
From Bellmans Dynamic Programming equation,
U 1 (i ) = max
ai

X

bi (is)ER(is, ai )

(20)

is

P
where ERi (is, ai ) = aj R(is, ai , aj )P r(aj |mj ). Here, ERi () represents the expectation of
R w.r.t. agent js actions. Eq. 20 represents an inner product and using Lemma 3, the inner product
is linear in bi . By selecting the maximum of a set of linear vectors (hyperplanes), we obtain a PWLC
horizon 1 value function.
Inductive Hypothesis: Suppose that U n1 (i,l ) is PWLC. Formally we have,
U n1 (i,l ) = max
n1

=

P

max

n1 ,

is bi (is)

n1



P

n1 (is)

is:mj IMj bi

(is)n1 (is)
75

+

P

is:mj SMj bi

(is)n1 (is)



(21)

fiG MYTRASIEWICZ & D OSHI

Inductive Proof: To show that U n (i,l ) is PWLC.

U n (i,l ) = max
at1
i

(

X

t1
bt1
)ERi (ist1 , at1
i (is
i )+

X

t1
n1
P r(oti |at1
(i,l )
i , bi )U

oti

ist1

From the inductive hypothesis:
(
P
t1
t1 )ER (ist1 , at1 )
U n (i,l ) = max
i
ist1 bi (is
i
at1
i

+

P

oti

t1
P r(oti |at1
i , bi )

max

n1 n1

P

t
t n1 (ist )
ist bi (is )

)

)

t1 t
t1 t1 t
t
Let l(bt1
i , ai , oi ) be the index of the alpha vector that maximizes the value at b i = SE(bi , ai , oi ).
Then,
(
P
t1
t1 )ER (ist1 , at1 )
U n (i,l ) = max
i
ist1 bi (is
i
t1
ai
)
P
P
t1
t
t n1
+ ot P r(oti |at1
ist bi (is )l(bt1 ,at1 ,ot )
i , bi )
i

i

i

i

From the second equation in the inductive hypothesis:
(
P
P
t1
t1 )ER (ist1 , at1 ) + 
n
t t1 t1
U (i,l ) = max
i
ot P r(oi |ai , bi )
ist1 bi (is
i
at1
i

i





P

t
t n1
ist :mtj IMj bi (is )l(bt1 ,at1 ,ot )
i
i
i

+

P

t
t n1
ist :mtj SMj bi (is )l(bt1 ,at1 ,ot )
i
i
i

Substituting bti with the appropriate belief updates from Eqs. 17 and 17 0 we get:
(
P
P
t1
t t1 t1
t1 )ER (ist1 , at1 ) + 
U n (i,l ) = max
i
oti P r(oi |ai , bi )
ist1 bi (is
i
t1
ai
"


P
P
P
t1
t1 t1
t1

)
P r(aj |j ) Oi (st , at1 , oti )
ist :mtj IMj
ist1 bi (is
at1
j


P
t1 t1 t t
t1
t
t
t1
t
t
t1
t1
t
 ot Oj (s , a , oj ) jt (bj , aj , oj , bj )I(bj , bj )Ti (s , a , s )

)

j

n1
t
l(b
t1 t1 t (is )
,ai ,oi )
i


P
P
P
t1
t1
t1
t1
+ ist :mt SMj ist1 bi (is )
P r(aj |mj ) Oi (st , at1 , oti )
at1
j
j


P
t1 t
t1
t
t
t1
t
t
t
t1
t1
t
 ot Oj (s , a , oj ) K (APPEND(hj , oj )  hj )I(m
bj ,m
b j )Ti (s , a , s )
j
#)
n1
t
l(b
t1 t1 t (is )
,a
,o )
i

i

i

76

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

and further
U n (i,l ) = max
at1
i




P

P

(

P

t1
t1 )ER (ist1 , at1 )
i
ist1 bi (is
i

t1
t1 )
ist1 bi (is



t t t1 , ot )
j
otj Oj (s , a

P



at1
j

+

P

oti

"

P

ist :mtj IMj


t1
P r(at1
|
)
Oi (st , at1 , oti )
j
j

t1 t t
t1 , at1 , st )
bt1 bt
jt (bt1
j , aj , oj , bj )I(j , j )Ti (s



n1
t
l(b
t1 t1 t (is )
,ai ,oi )
i


P
P
P
t1
t1
t1
t1
P r(aj |mj ) Oi (st , at1 , oti )
+ ist :mt SMj ist1 bi (is )
at1
j
j


P
t1
t )  ht )I(m
t )T (st1 , at1 , st )
 ot Ojt (st , at1 , otj ) K (APPEND(ht1
,
o
b
,
m
b
j
j
j i
j
j
j
#)
n1
t
l(b
t1 t1 t (is )
,a
,o )
i

i

i

Rearranging the terms of the equation:
U n (

(


P
P P
t1
t1 ) ER (ist1 , at1 ) + 
)
=
max
b
(is
t1
t1 :m
i
i,l
oti
ist :mtj IMj
i
i
is
IM
j
j
at1
i


P
P
t1 t1

P r(aj |j ) Oi (st , at1 , oti ) ot Ojt (st , at1 , otj )
at1
j
j



n1
t1 t1 t t
t1
t
t1
t1
t
t
 jt (bj , aj , oj , bj )I(bj , bj )Ti (s , a , s )
l(bt1 ,at1 ,ot ) (is )
i
i
i

P
P
P
P
+ ist1 :mt1 SMj bit1 (ist1 ) ERi (ist1 , at1
oti
ist :mtj SMj
oti
i )+
j


P
P
P r(ajt1 |mt1
) Oi (st , at1 , oti ) ot Ojt (st , at1 , otj )

j
at1
j
j

)

n1
t
t
t
l(b
b t1
b tj )Ti (st1 , at1 , st )
 K (APPEND(ht1
t1 t1 t (is )
j ,m
j , oj )  hj )I(m
,ai ,oi )
i

P
t1
t1 )n (ist1 )
= max
ai
IMj bi (is
ist1 :mt1
j
at1
i

P
t1
t1
t1
n
+ ist1 :mt1 SMj bi (is )ai (is )
j

Therefore,
U n (

i,l )

= max
n
n
 , 

+
=

P



P

t1
t1 )n (ist1 )
ist1 :mt1
IMj bi (is
j



t1
t1 )n (ist1 )
SMj bi (is
ist1 :mt1
j
P
t1
t1 )n (ist1 ) = maxhbt1 , n i
max
ist1 bi (is
i
n

n

77

(22)

fiG MYTRASIEWICZ & D OSHI

where, if mjt1 in ist1 is intentional then n = n :
n (ist1 )

ERi (ist1 , at1
i )



P P

P

t1
P r(at1
j |j )



Oi (ist , at1 , oti )
+  ot ist :mt IMj
at1
j
j
 i

P
t1 t1 t t
t1
t
t
t1
t
t
t1
t1
t
 ot Oj (isj , a , oj ) jt (bj , aj , oj , bj )I(bj , bj )Ti (s , a , s )

=

j

n1
t
l(b
t1 t1 t (is )
,o )
,a
i

i

i

and, if mjt1 is subintentional then n = n :
n (ist1 )

ERi (ist1 , at1
i )



P P

P

t1
P r(at1
j |j )



Oi (ist , at1 , oti )
+  ot ist :mt SMj
at1
i
j
j


P
t1
t1 t
t1
t1
t
t
t
t1
t
t
t
bj ,m
b j )Ti (s , a , s )
 ot Oj (isj , a , oj ) K (APPEND(hj , oj )  hj )I(m

=

j

n1
t
l(b
t1 t1 t (is )
,ai ,oi )
i

Eq. 22 is an inner product and using Lemma 3, the value function is linear in b t1
i . Furthermore,
maximizing over a set of linear vectors (hyperplanes) produces a piecewise linear and convex value
function.

References
Ambruster, W., & Boge, W. (1979). Bayesian game theory. In Moeschlin, O., & Pallaschke, D. (Eds.), Game
Theory and Related Topics. North Holland.
Aumann, R. J. (1999). Interactive epistemology i: Knowledge. International Journal of Game Theory, pp.
263300.
Aumann, R. J., & Heifetz, A. (2002). Incomplete information. In Aumann, R., & Hart, S. (Eds.), Handbook
of Game Theory with Economic Applications, Volume III, Chapter 43. Elsevier.
Battigalli, P., & Siniscalchi, M. (1999). Hierarchies of conditional beliefs and interactive epistemology in
dynamic games. Journal of Economic Theory, pp. 188230.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity of decentralized control
of markov decision processes. Mathematics of Operations Research, 27(4), 819840.
Binmore, K. (1990). Essays on Foundations of Game Theory. Blackwell.
Boutilier, C. (1999). Sequential optimality and coordination in multiagent systems. In Proceedings of the
Sixteenth International Joint Conference on Artificial Intelligence, pp. 478485.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial intelligence Research, 11, 194.
Brandenburger, A. (2002). The power of paradox: Some recent developments in interactive epistemology.
Tech. rep., Stern School of Business, New York University, http://pages.stern.nyu.edu/ abranden/.
Brandenburger, A., & Dekel, E. (1993). Hierarchies of beliefs and common knowledge. Journal of Economic
Theory, 59, 189198.
Dennett, D. (1986). Intentional systems. In Dennett, D. (Ed.), Brainstorms. MIT Press.
Fagin, R. R., Geanakoplos, J., Halpern, J. Y., & Vardi, M. Y. (1999). A hierarchical approach to modeling
knowledge and common knowledge. International Journal of Game Theory, pp. 331365.
Fagin, R. R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning About Knowledge. MIT Press.
Fudenberg, D., & Levine, D. K. (1998). The Theory of Learning in Games. MIT Press.
78

fiA F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Gmytrasiewicz, P. J., & Durfee, E. H. (2000). Rational coordination in multi-agent environments. Autonomous Agents and Multiagent Systems Journal, 3(4), 319350.
Harsanyi, J. C. (1967). Games with incomplete information played by Bayesian players. Management
Science, 14(3), 159182.
Hauskrecht, M. (2000). Value-function approximations for partially observable markov decision processes.
Journal of Artificial Intelligence Research, pp. 3394.
Hausktecht, M. (1997). Planning and control in stochastic domains with imperfect information. Ph.D. thesis,
MIT.
Hu, J., & Wellman, M. P. (1998). Multiagent reinforcement learning: Theoretical framework and an algorithm. In Fifteenth International Conference on Machine Learning, pp. 242250.
Kadane, J. B., & Larkey, P. D. (1982). Subjective probability and the theory of games. Management Science,
28(2), 113120.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in partially observable
stochastic domains. Artificial Intelligence, 101(2), 99134.
Kalai, E., & Lehrer, E. (1993). Rational learning leads to nash equilibrium. Econometrica, pp. 12311240.
Koller, D., & Milch, B. (2001). Multi-agent influence diagrams for representing and solving games. In Seventeenth International Joint Conference on Artificial Intelligence, pp. 10271034, Seattle, Washington.
Li, M., & Vitanyi, P. (1997). An Introduction to Kolmogorov Complexity and Its Applications. Springer.
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Proceedings
of the International Conference on Machine Learning.
Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observed markov decision processes.
Annals of Operations Research, 28(1-4), 4766.
Madani, O., Hanks, S., & Condon, A. (2003). On the undecidability of probabilistic planning and related
stochastic optimization problems. Artificial Intelligence, 147, 534.
Mertens, J.-F., & Zamir, S. (1985). Formulation of Bayesian analysis for games with incomplete information.
International Journal of Game Theory, 14, 129.
Monahan, G. E. (1982). A survey of partially observable markov decision processes: Theory, models, and
algorithms. Management Science, 116.
Myerson, R. B. (1991). Game Theory: Analysis of Conflict. Harvard University Press.
Nachbar, J. H., & Zame, W. R. (1996). Non-computable strategies and discounted repeated games. Economic
Theory, 8, 103122.
Nair, R., Pynadath, D., Yokoo, M., Tambe, M., & Marsella, S. (2003). Taming decentralized pomdps: Towards
efficient policy computation for multiagent settings. In Proceedings of the Eighteenth International
Joint Conference on Artificial Intelligence (IJCAI-03).
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control of a multiple access broadcast channel. In
Proceedings of the 35th Conference on Decision and Control.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of markov decision processes. Mathematics
of Operations Research, 12(3), 441450.
Russell, S., & Norvig, P. (2003). Artificial Intelligence: A Modern Approach (Second Edition). Prentice Hall.
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable markov decision
processes over a finite horizon. Operations Research, pp. 10711088.
Stokey, N. L., & Lucas, R. E. (1989). Recursive Methods in Economic Dynamics. Harvard Univ. Press.

79

fi