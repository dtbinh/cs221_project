Journal of Artificial Intelligence Research 57 (2016) 187-227

Submitted 9/15; published 10/16

Multi-objective Reinforcement Learning through
Continuous Pareto Manifold Approximation
Simone Parisi

parisi@ias.tu-darmstadt.de

Technische Universitat Darmstadt
Hochschulstr. 10, 64289 Darmstadt, Germany

Matteo Pirotta
Marcello Restelli

matteo.pirotta@polimi.it
marcello.restelli@polimi.it

Politecnico di Milano
Piazza Leonardo da Vinci 32, 20133 Milano, Italy

Abstract
Many real-world control applications, from economics to robotics, are characterized by
the presence of multiple conflicting objectives. In these problems, the standard concept
of optimality is replaced by Paretooptimality and the goal is to find the Pareto frontier,
a set of solutions representing different compromises among the objectives. Despite recent advances in multiobjective optimization, achieving an accurate representation of the
Pareto frontier is still an important challenge. In this paper, we propose a reinforcement
learning policy gradient approach to learn a continuous approximation of the Pareto frontier in multiobjective Markov Decision Problems (MOMDPs). Differently from previous
policy gradient algorithms, where n optimization routines are executed to have n solutions,
our approach performs a single gradient ascent run, generating at each step an improved
continuous approximation of the Pareto frontier. The idea is to optimize the parameters
of a function defining a manifold in the policy parameters space, so that the corresponding
image in the objectives space gets as close as possible to the true Pareto frontier. Besides
deriving how to compute and estimate such gradient, we will also discuss the nontrivial
issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally,
the properties of the proposed approach are empirically evaluated on two problems, a
linear-quadratic Gaussian regulator and a water reservoir control task.

1. Introduction
Multiobjective sequential decision problems are characterized by the presence of multiple
conflicting objectives and can be found in many real-world scenarios, such as economic
systems (Shelton, 2001), medical treatment (Lizotte, Bowling, & Murphy, 2012), control of
water reservoirs (Castelletti, Pianosi, & Restelli, 2013), elevators (Crites & Barto, 1998) and
robots (Nojima, Kojima, & Kubota, 2003; Ahmadzadeh, Kormushev, & Caldwell, 2014),
just to mention a few. Such problems are often modeled as Multiobjective Markov Decision
Processes (MOMDPs), where the concept of optimality typical of MDPs is replaced by the
one of Pareto optimality, that defines a compromise among the different objectives.
In the last decades, Reinforcement Learning (RL) (Sutton & Barto, 1998) has established
as an effective and theoretically grounded framework that allows to solve singleobjective
MDPs whenever either no (or little) prior knowledge is available about system dynamics or
the dimensionality of the system to be controlled is too high for classical optimal control
2016 AI Access Foundation. All rights reserved.

fiParisi, Pirotta, & Restelli

methods. Multiobjective Reinforcement Learning (MORL), instead, concerns MOMDPs
and tries to solve sequential decision problems with two or more conflicting objectives.
Despite the successful development in RL theory and a high demand for multiobjective
control applications, MORL is still a relatively young and unexplored research topic.
MORL approaches can be divided in two categories, based on the number of policies
they learn (Vamplew, Dazeley, Berry, Issabekov, & Dekker, 2011): single and multiple
policy. Although most of MORL approaches belong to the former category, here we present
a multiplepolicy approach, able to learn a set of policies approximating the Pareto frontier.
A representation of the complete Pareto frontier, in fact, allows a posteriori selection of a
solution and encapsulates all the trade-offs among the objectives, giving better insights into
the relationships among the objectives. Among multiplepolicy algorithms it is possible to
identify two classes: valuebased (Lizotte et al., 2012; Castelletti et al., 2013; Van Moffaert
& Nowe, 2014), that search for optimal solutions in value functions space, and policy gradient approaches (Shelton, 2001; Parisi, Pirotta, Smacchia, Bascetta, & Restelli, 2014), that
search through policy space. In practice, each approach has different advantages. Value
based methods usually have stronger guarantees of convergence, but are preferred in domains with lowdimensional state-action spaces as they are prone to suffer from the curse
of dimensionality (Sutton & Barto, 1998). On the other hand, policy gradient methods
have been very favorable in many domains such as robotics as they allow taskappropriate
prestructured policies to be integrated straightforwardly (Deisenroth, Neumann, & Peters,
2013) and experts knowledge can be incorporated with ease. By selecting a suitable policy
parametrization, the learning problem can be simplified and stability as well as robustness
can frequently be ensured (Bertsekas, 2005). Nonetheless, both approaches lack of guarantees of uniform covering of the true Pareto frontier and the quality of the approximate
frontier, in terms of accuracy (distance from the true frontier) and covering (its extent),
is related to the metric used to measure the discrepancy from the true Pareto frontier.
However, nowadays the definition of such metric is an open problem in MOO literature.
In this paper, we overcome these limitations proposing a novel gradientbased MORL
approach and alternative quality measures for approximate frontiers. The algorithm, namely
ParetoManifold Gradient Algorithm (PMGA), exploiting a continuous approximation of
the locally Paretooptimal manifold in the policy space, is able to generate an arbitrarily
dense approximate frontier. This article is an extension of a preliminary work presented
by Pirotta, Parisi, and Restelli (2015) and its main contributions are: the derivation of the
gradient approach in the general case, i.e., independent from the metric used to measure
the quality of the current solution (Section 3), how to estimate such gradient from samples
(Section 4), a discussion about frontier quality measures that can be effectively integrated
in the proposed approach (Section 5), a thorough empirical evaluation of the proposed
algorithm and metrics performance in a multiobjective discrete-time Linear-Quadratic
Gaussian regulator and in a water reservoir management domain (Sections 6 and 7).

2. Preliminaries
In this section, we first briefly summarize the terminology as used in the paper and discuss
about state-of-the-art approaches in MORL. Subsequently, we focus on describing policy
gradient techniques and we introduce the notation used in the remainder of the paper.
188

fiMORL through Continuous Pareto Manifold Approximation

2.1 Problem Formulation
A discretetime continuous Markov Decision Process (MDP) is a mathematical framework
for modeling decision making. It is described by a tuple hS, A, P, R, , Di, where S  Rn
is the continuous state space, A  Rm is the continuous action space, P is a Markovian
transition model where P(s0 |s, a) defines the transition density between state s and s0 under
action a, R : S  A  S  R is the reward function,   [0, 1) is the discount factor, and D
is a distribution from which the initial state is drawn. In this context, the behavior of an
agent is defined by a policy, i.e., a density distribution (a|s) that specifies the probability
of taking action a in state s. Given the initial state distribution D, it is possible to define
the expected return J  associated to a policy  as
"T 1
#
X

t
J =
E
 R(st , at , st+1 )|s0  D ,
st P,at 

t=0

being R(st , at , st+1 ) the immediate reward obtained when state st+1 is reached executing
action at from state st , and T the finite or infinite time horizon. The goal of the agent is
to maximize such a return.
Multiobjective Markov Decision Processes (MOMDPs) are an extension of MDPs in
which several pairs of reward functions and discount factors are defined, one for each
objective. Formally, a MOMDP is described by a tuple hS, A, P, R, , Di, where R =
[R1 , . . . , Rq ]T and  = [1 , . . . , q ]T are qdimensional column vectors of reward functions
Ri : S  A  S  R and discount factors i  [0, 1), respectively.
In MOMDPs, any policy




 is associated to q expected returns J = J1 , . . . , Jq , where
"T 1
#
X
Ji =
E
it Ri (st , at , st+1 )|s0  D .
st P,at 

t=0

Unlike what happens in MDPs, in MOMDPs a single policy dominating all the others
usually does not exist, as when conflicting objectives are considered, no policy can simultaneously maximize all of them. For this reason, in Multiobjective Optimization (MOO)
the concept of Pareto dominance is used. Policy  strongly dominates policy  0 , denoted
by    0 , if it is superior on all objectives, i.e.,
0

   0  i  {1, . . . , q} , Ji > Ji .
Similarly, policy  weakly dominates policy  0 , denoted by    0 , if it is not worse on all
objectives, i.e.,
0

0

   0  i  {1, . . . , q} , Ji  Ji  i  {1, . . . , q} , Ji = Ji .
If there is no policy  0 such that  0  , the policy  is Paretooptimal. We can also speak
of locally Paretooptimal policies, for which the definition is the same as above, except
that we restrict the dominance to a neighborhood of . In general, there are multiple
(locally) Paretooptimal policies. Solving
a MOMDP

	 is equivalent to determine the set
 =  | @ 0 ,  0   , which maps to the socalled Pareto
of all Paretooptimal
policies

 
	
frontier F = J |    .1
1. As done by Harada, Sakuma, and Kobayashi (2006), we assume that locally Paretooptimal solutions
that are not Paretooptimal do not exist.

189

fiParisi, Pirotta, & Restelli

2.2 Related Work
In Multiobjective Optimization (MOO) field, there are two common solution concepts:
multiobjective to singleobjective strategy and Pareto strategy. The former approach
derives a scalar objective from the multiple objectives and, then, uses the standard Single
objective Optimization (SOO) techniques: weighted sum (Athan & Papalambros, 1996),
normbased (Yu & Leitmann, 1974; Koski & Silvennoinen, 1987), sequential (Romero,
2001), constrained (Waltz, 1967), physical programming (Messac & Ismail-Yahaya, 2002)
and min-max methods (Steuer & Choo, 1983). The latter strategy is based on the concept of
Pareto dominance and considers Paretooptimal solutions as non-inferior solutions among
the candidate solutions. The main exponent of this class is the convex hull method (Das &
Dennis, 1998; Messac, Ismail-Yahaya, & Mattson, 2003).
Similar to MOO, current MORL approaches can be divided into two categories based
on the number of policies they learn (Vamplew et al., 2011). Singlepolicy methods aim
at finding the best policy that satisfies a preference among the objectives. The majority
of MORL approaches belong to this category and differ for the way in which preferences
are expressed. They are easy to implement, but require a priori decision about the type
of the solution and suffer of instability, as small changes on the preferences may result
in significant variations in the solution (Vamplew et al., 2011). The most straightforward
and common singlepolicy approach is the scalarization where a function is applied to the
reward vector in order to produce a scalar signal. Usually, a linear combination weighted
sum of the rewards is performed and the weights are used to express the preferences over
multiple objective (Castelletti, Corani, Rizzolli, Soncinie-Sessa, & Weber, 2002; Natarajan
& Tadepalli, 2005; Van Moffaert, Drugan, & Nowe, 2013). Less common is the use of non
linear mappings (Tesauro, Das, Chan, Kephart, Levine, Rawson, & Lefurgy, 2008). The
main advantage of scalarization is its simplicity. However, linear scalarization presents some
limitations: it is not able to find solutions that lie in the concave or linear region of the
Pareto frontier (Athan & Papalambros, 1996) and a uniform distribution of the weights may
not produce accurate and evenly distributed points on the Pareto frontier (Das & Dennis,
1997). In addition, even if the frontier is convex, some solutions cannot be achieved through
scalarization because a loss in one objective may not be compensated by an increment
in another one (Perny & Weng, 2010). Different singlepolicy approaches are based on
thresholds and lexicographic ordering (Gabor, Kalmar, & Szepesvari, 1998) or different
kinds of preferences over the objective space (Mannor & Shimkin, 2002, 2004).
Multiplepolicy approaches, on the contrary, aim at learning multiple policies in order
to approximate the Pareto frontier. Building the exact frontier is generally impractical in
real-world problems, thus, the goal is to build an approximation of the frontier that contains
solutions that are accurate, evenly distributed along the frontier and have a range similar
to Pareto one (Zitzler, Thiele, Laumanns, Fonseca, & da Fonseca, 2003). There are many
reasons behind the superiority of the multiplepolicy methods: they permit a posteriori
selection of the solution and encapsulate all the trade-offs among the multiple objectives.
In addition, a graphical representation of the frontier can give better insights into the relationships among the objectives that can be useful for understanding the problem and the
choice of the solution. However, all these benefits come at a higher computational cost,
that can prevent learning in online scenarios. The most common approach to approximate
190

fiMORL through Continuous Pareto Manifold Approximation

the Pareto frontier is to perform multiple runs of a singlepolicy algorithm by varying the
preferences among the objectives (Castelletti et al., 2002; Van Moffaert et al., 2013). It
is a simple approach but suffers from the disadvantages of the singlepolicy method used.
Besides this, few other examples of multiplepolicy algorithms can be found in literature.
Barrett and Narayanan (2008) proposed an algorithm that learns all the deterministic policies defining the convex hull of the Pareto frontier in a single learning process. Recent
works have focused on the extension of fitted Q-iteration to the multiobjective scenario.
While Lizotte, Bowling, and Murphy (2010), and Lizotte et al. (2012) have focused on a
linear approximation of the value function, Castelletti, Pianosi, and Restelli (2012) are able
to learn the control policy for all the linear combinations of preferences among the objectives in a single learning process. Finally, Wang and Sebag (2013) proposed a MonteCarlo
Tree Search algorithm able to learn solutions lying in the concave region of the frontier.
Nevertheless, these classic approaches exploit only deterministic policies that result in
scattered Pareto frontiers, while stochastic policies give a continuous range of compromises
among objectives (Roijers, Vamplew, Whiteson, & Dazeley, 2013; Parisi et al., 2014). Shelton (2001, Section 4.2.1) was the pioneer both for the use of stochastic mixture policies and
gradient ascent in MORL. He achieved two well known goals in MORL: simultaneous and
conditional objectives maximization. In the former, the agent must maintain all goals at the
same time. The algorithm starts with a mixture of policies obtained by applying standard
RL techniques to each independent objective. The policy is subsequently improved following
a convex combination of the gradients in the policy space that are nonnegative w.r.t. all
the objectives. For each objective i, the gradient gi of the expected return w.r.t. the policy
is computed and the vector vi having the highest dot product with gi and simultaneously
satisfying the nonnegativity condition for all the returns is used as improving direction
for the i-th reward. The vectors vi are combined in a convex form to obtain the direction
of the parameter improvement. The result is a policy that belongs to the Pareto frontier.
An approximation of the Pareto frontier is obtained by performing repeated searches with
different weights of the reward gradients vi . On the other hand, conditional optimization
consists in maximizing an objective while maintaining a certain level of performance over
the others. The resulting algorithm is a gradient search in a reduced policy space in which
the value of constrained objectives are greater than the desired performance.
Only a few studies followed the work of Shelton (2001) in regard to policy gradient
algorithms applied to MOMDPs. Recently Parisi et al. (2014) proposed two policy gradient
based MORL approaches that, starting from some initial policies, perform gradient ascent
in the policy parameters space in order to determine a set of nondominated policies. In
the first approach (called Radial ), given the number p of Pareto solutions that are required
for approximating the Pareto frontier, p gradient ascent searches are performed, each one
following a different (uniformly spaced) direction within the ascent simplex defined by the
convex combination of singleobjective gradients. The second approach (called Pareto
Following) starts by performing a singleobjective optimization and then it moves along the
Pareto frontier using a two-step iterative process: updating the policy parameters following
some other gradient ascent direction, and then applying a correction procedure to move the
new solution onto the Pareto frontier. Although such methods exploit stochastic policies and
proved to be effective in several scenarios, they still return scattered solutions and are not
guaranteed to uniformly cover the Pareto frontier. To the best of our knowledge, nowadays
191

fiParisi, Pirotta, & Restelli

there is no MORL algorithm returning a continuous approximation of the Pareto frontier2 .
In the following sections we present the first approach able to do that: the ParetoManifold
Gradient Algorithm (PMGA).
2.3 Policy Parametrization in PolicyGradient Approaches
In singleobjective
MDPs,

	 policygradient approaches consider parameterized policies  
 =  :     Rd , where  is a compact notation for (a|s, ) and  is the policy
parameters space. Given a policy parametrization , we assume the policy performance
J :   F  Rq to be at least of class C 2 .3 F is called objectives space and J is defined as
the expected reward over the space of all possible trajectories T
Z
p ( |) r( )d,
J () =
T

where   T is a trajectory drawn from density distribution p( |) with reward vector
r( ) that
the accumulated expected discounted reward over trajectory  , i.e.,
Prepresents
1 t
i Ri (st , at , st+1 ). Examples of parametrized policies used in this context are
ri ( ) = Tt=0
Guassian policies and Gibbs policies. In MOMDPs, q gradient directions are defined for
each policy parameter  (Peters & Schaal, 2008b), i.e.,
Z


 Ji () =
 p ( |) ri ( )d = E T  ln p ( |) ri ( )
T
"
#
T
1
X
b  Ji (),
 E T ri ( )
 ln  (at |st , ) = 
(1)
t=0

where each direction  Ji is associated to a particular discount factorreward function
b  Ji () is its sample-based estimate. As shown by Equation (1), the
pair < i , Ri > and 
differentiability of the expected return is connected to the differentiability of the policy by
 ln p ( |) =

T
1
X

 ln (at |st , ).

t=0

A remark on notation. In the following we will use the symbol DX F to denote the
derivative of a generic function F : Rmn  Rpq w.r.t. matrix X.4 Notice that the
following relationship holds for scalar functions of vector variable: x f = (Dx f )T . Finally,
the symbol Ix will be used to denote an x  x identity matrix.

3. Gradient Ascent on Policy Manifold for Continuous Pareto Frontier
Approximation
In this section we first provide a general definition of the optimization problem that we want
to solve and then we explain how we can solve it in the MOMDP scenario using a gradient
based approach. The novel contributes of this section are summarized in Lemma 3.1 where
2. A notable exception is the MOO approach by Calandra, Peters, and Deisenrothy (2014) where Gaussian
Processes are used to obtain a continuous approximation of the Pareto frontier.
3. A function is of class C 2 when it is continuous, twice differentiable and the derivatives are continuous.
4. The derivative operator is well defined for matrices, vectors and scalar functions. Refer to the work
of Magnus and Neudecker (1999) for details.

192

fiMORL through Continuous Pareto Manifold Approximation

the objective function and its gradient are described. In particular, we provide a solution
to the problem of evaluating the performance of a continuous approximation of the Pareto
frontier w.r.t. to an indicator function. This problem is non trivial in MORL because we
do not have direct access to the Pareto frontier and we can only manipulate the policy
parameters. We provide a step-by-step derivation of these results leveraging on manifold
theory and matrix calculus.
3.1 Continuous Pareto Frontier Approximation in Multiobjective
Optimization
It has been shown that locally Paretooptimal solutions locally forms a (q  1)dimensional
manifold, assuming d > q (Harada, Sakuma, Kobayashi, & Ono, 2007). It follows that in
2objective problems, Paretooptimal solutions can be described by curves both in policy
parameters and objective spaces. The idea behind this work is to parametrize the locally
Paretooptimal solution curve in the objectives space, in order to produce a continuous
representation of the Pareto frontier.
Let the generative space T be an open set in Rb with b  q. The analogous high
dimensional function of a parameterized curve is a smooth map  : T  Rq of class
C l (l  1), where t  T and   P  Rk are the free variables and the parameters,
respectively. The set F =  (T ) together with the map  constitute a parametrized
manifold of dimension b, denoted by F (T ) (Munkres, 1997). This manifold represents our
approximation of the Pareto frontier. The goal is to find the best approximation, i.e., the
parameters  that minimize the distance from the real frontier
 = arg max I  (F (T )) ,

(2)

P

where I  : Rq  R is some indicator function measuring the quality of F (T ) w.r.t. the
true Pareto frontier. Notice that Equation (2) can be interpreted as a special projection
operator (refer to Figure 1a for a graphical representation). However, since I  requires
the knowledge of the true Pareto frontier, a different indicator function is needed. The
definition of such metric is an open problem in literature. Recently, several metrics have
been defined, but each candidate presents some intrinsic flaws that prevent the definition
of a unique superior metric (Vamplew et al., 2011). Furthermore, as we will see in the
remainder of the section, the proposed approach needs a metric that is differentiable w.r.t.
policy parameters. We will investigate this topic in Section 5.
In general, MOO algorithms compute the value of the frontier as the sum of the value
of the points composing the discrete approximation. In our scenario, where a continuous
approximate frontier is available, it maps to an integration on the Pareto manifold
Z
IdV,
(3)
L () =
F (T )

where L () is the manifold value, dV denotes the integral w.r.t. the volume of the manifold
and I : F (T )  R is an indicator function measuring the Pareto optimality of each point
of F (T ). Assuming I to be continuous, the above integral is given by (Munkres, 1997)
Z
Z
L () =
IdV 
(I   ) V ol (Dt  (t)) dt,
F (T )

T

193

fiParisi, Pirotta, & Restelli

(a)

(b)

Figure 1: Transformation maps in a generic MOO setting (Figure (a)) and in MORL (Figure (b)). While in MOO it is also possible to consider parametrized solutions as in Figure (b), in MORL this is necessary, as the mapping between  i and Fi is not known in
closed form but determined by the (discounted) sum of the rewards.
 1
provided this integral exists and V ol (X) = det X T  X 2 . A standard way to maximize
the previous equation is by performing gradient ascent, updating the parameters according
to the gradient of the manifold value w.r.t. the parameters , i.e.,    +   L () .
3.2 Continuous Pareto Frontier Approximation in Multiobjective
Reinforcement Learning
While in standard multiobjective optimization the function  is free to be designed, in
MORL it must satisfy some conditions. The first thing to notice is that the direct map
between the parameters space T and the objective space is unknown, but can be easily
defined through a reparameterization involving the policy space , as shown in Figure 1b. In
the previous section we have mentioned that there is a tight relationship between the (local)
manifold in the objective space and the (local) manifold in the policy parameters space.
This mapping is well known and it is defined by the performance function J() defining the
utility of a policy   . This means that, given a set of policy parameterizations, we can define
the associated points in the objective space. As a consequence, the optimization problem
can be reformulated as the search for the best approximation of the Pareto manifold in the
policy parameter space, i.e., to the search of the manifold in the policy parameter space
that best describes the optimal Pareto frontier.
Formally, let  : T   be a smooth map of class C l (l  1) defined on the same
domain of  . We think of the map  as a parameterization of the subset  (T ) of :
each choice of a point t  T gives rise to a point  (t) in  (T )  . This means that only
a subset  (T ) of the space  can be spanned by map  , i.e.,  (T ) is a bdimensional
parametrized manifold in the policy parameters space, i.e.,
 (T ) = { :  =  (t), t  T } ,
and, as a consequence, the associated parameterized Pareto frontier is the bdimensional
open set defined as
F (T ) = {J () :    (T )} .
194

fiMORL through Continuous Pareto Manifold Approximation

3.3 Gradient Ascent in the Manifold Space
At this point we have introduced all the notation needed to derive the gradient  L ().
Lemma 3.1. (Pirotta et al., 2015) Let T be an open set in Rb , let F (T ) be a manifold
parametrized by a smooth map  expressed as composition of maps J and  , (i.e.,  =
J   : T  Rq ). Given a continuous function I defined at each point of F (T ), the
integral w.r.t. the volume is given by
Z
Z
L () =
IdV =
(I  (J   )) V ol (D J()Dt  (t)) dt,
F (T )

T

provided this integral exists. The associated gradient w.r.t. the parameters i is given by
Z

L ()
=
(I  (J   )) V ol (T) dt
i

T
i

Z

T T 

T
T
(I  (J   )) V ol (T) vec T T
+
Nb Ib  T Di Tdt, (4)
T

where T = D J()Dt  (t),  is the Kronecker product, Nb = 12 (Ib2 + Kbb ) is a symmetric
(b2  b2 ) idempotent matrix with rank 21 b(b + 1) and Kbb is a permutation matrix (Magnus
& Neudecker, 1999). Finally,


Di T = Dt  (t)T  Iq D (D J()) Di  (t) + (Ib  D J()) Di (Dt  (t)) .
Proof. The equation of the manifold value L () follows directly from the definition of
volume integral of a manifold (Munkres, 1997) and the definition of function composition.
In the following, we provide a detailed derivation of the i-th component of the gradient.
Let T = D J( t )Dt  (t), then
Z
L ()

=
(I  (J   )) V ol (T) dt
i
T i

Z
det TT T
1
+
(I  (J   ))
dt.
2V ol (T)
i
T
The indicator derivative and the determinant derivative can be respectively expanded as

(I  (J   )) = DJ I(Jt )  D J( t )  Di  (t),
i


det TT T
det TT T vec TT T T
=
,
T
T 
i
(vec
T)
(vec
T)
i
|
{z
} |
{z
} | {z } |{z}
11

1b2

b2 qb

qb1

where

det TT T
(vec T)T
TT T
(vec T)T



T T
T
,
= det T T
vec T T


T



= 2Nb Ib  TT ,

195

fiParisi, Pirotta, & Restelli

and  is the Kronecker product, Nb = 12 (Ib2 + Kbb ) is a symmetric (b2  b2 ) idempotent
matrix with rank 21 b(b + 1) and Kbb is a permutation matrix (Magnus & Neudecker, 1999).
(T)
The last term to be expanded is Di T  vec
i . We start from a basic property of the
differential, i.e.,
d (D J()Dt  (t)) = d(D J())Dt  (t) + D J() d(Dt  (t))
then, applying the vector operator,
dvec (D J()Dt  (t)) = vec (d(D J())Dt  (t)) + vec (D J() d(Dt  (t)))


= Dt  (t)T  Iq dvec (D J()) + (Ib  D J()) dvec (Dt  (t)) .
{z
} |
{z
}|
{z
}
{z
}|
|
dq1

bqdq

bqbd

bd1

Finally, the derivative is given by

 vec D J()  (t)
vec Dt  (t)


Di T = Dt  (t)T  Iq
+ (Ib  D J())
T

i

|
{z
} | {zi }
|
{z
}
dqd



d1

bd1



T

= Dt  (t)  Iq D (D J()) Di  (t) + (Ib  D J()) Di (Dt  (t)) .

It is interesting to notice that the gradient of the manifold value L () requires to
compute the second derivatives of the policy performance J(). However, D (D J()) =
vec D J()
does not denote the Hessian matrix but a transformation of it
 T
(m,n)
H
Ji

=

2
Dn,m
Ji ()


=
 n



Ji
 m



= Dp,n (D J()) ,

where p = i + q(m  1) and q (number of objectives) is the number of rows of the Jacobian
matrix. Recall that the Hessian
 matrixis defined as the derivative of the transpose of the
Jacobian, i.e., H J() = D D J()T .
Up to now, little research has been done on second-order methods5 and in particular
on Hessian formulations. A first analysis was performed by Kakade (2001), who provided a
formulation based on the policy gradient theorem (Sutton, McAllester, Singh, & Mansour,
2000). Recently, an extended comparison between Newton method, EM algorithm and
natural gradient was presented by Furmston and Barber (2012). For the sake of clarity, we
report the Hessian formulation provided by Furmston and Barber (2012) using our notation
and we introduce the optimal baseline (in terms of variance reduction) for such formulation.
Lemma 3.2. For any MOMDP, the Hessian H J() of the expected discounted reward J
w.r.t. the policy parameters  is a qd  d matrix obtained by stacking the Hessian of each
5. Notable exceptions are the natural gradient approaches that, although they do not explicitly require to
compute second-order derivatives, are usually considered second-order methods.

196

fiMORL through Continuous Pareto Manifold Approximation

component
H J() =


vec
 T



Ji ()
 T

T



H J1 ()


..
=
,
.
H Jq ()

where
Z
H Ji () =

!
p ( |) (ri ( )  bi )  ln p ( |)  ln p ( |)T + H ln p ( |) d,

(5)

T

and
 ln p ( |) =

T
1
X

 ln (at |st , ),

H ln p ( |) =

t=0

T
1
X

H ln (at |st , ).

t=0
(m,n)

The optimal baseline of the Hessian estimate H
Ji provided in Equation (5) can
be computed as done by Greensmith, Bartlett, and Baxter (2004) in order to reduce the
variance of the gradient estimate. It is given component-wise by


2 
(m,n)
E p(|) Ri ( ) G
( )
(m,n)

bi
=
2  ,
(m,n)
E p(|) G
( )
(m,n)

(m,n)

n
where G
( ) = m
 ln p ( |)  ln p ( |)+H
to Appendix A.

ln p ( |). For its derivation, we refer

4. Manifold Gradient Estimation from Sample Trajectories
In MORL, having no prior knowledge about the reward function and the state transition
model, we need to estimate the gradient  L () from trajectory samples. This section
aims to provide a guide to the estimation of the manifold gradient. In particular, we review
results related to the estimation of standard RL components (expected discounted return
and its gradient) and we provide a finite-sample analysis of the Hessian estimate.
The formulation of the gradient  L () provided in Lemma 3.1 is composed by terms
related to the parameterization of the manifold in the policy space and terms related to
the MDP. Since the map  is free to be designed, the associated terms (e.g., Dt  (t)) can
be computed exactly. On the other hand, the terms related to the MDP (J (), D J()
and H J()) need to be estimated. While the estimate of the expected discounted reward
and the associated gradient is an old topic in RL literature and several results have been
proposed (Kakade, 2001; Pirotta, Restelli, & Bascetta, 2013), literature lacks of an explicit
analysis of the Hessian estimate. Recently, the simultaneous perturbation stochastic approximation technique was exploited to estimate the Hessian (Fonteneau & Prashanth, 2014).
However, we rely on the formulation provided by Furmston and Barber (2012) where the
Hessian is estimated from trajectory samples obtained through the current policy, removing
the necessity of generating policy perturbations.
197

fiParisi, Pirotta, & Restelli

Algorithm 1 ParetoManifold Gradient Algorithm
Define policy , parametric function  , indicator I and learning rate 
Initialize parameters 
Repeat until terminal condition is reached
Collect n = 1 . . . N trajectories
Sample free variable t[n] from the generative space

Sample policy parameters  [n] =  t[n]
n
o
[n] [n] [n] T
Execute trajectory and collect data st , at , rt,
t=1

b  Ji () according to Equation (1)
Compute gradients 
b  Ji () according to Equation (6)
Compute Hessians H
Compute manifold value derivative  L () according to Equation (4)
Update parameters    +   L ()
Since p ( |) is unknown, the expectation is approximated by the empirical average.
Assuming to have access to N trajectories, the Hessian estimate is
!
N
T
1
X
X
1
b  Ji () =
H
it rnt,i  b
N
n=1
t=0
!T T 1
!
T
1
T
1
X
X
X

 ln ant ,snt
 ln ant ,snt
+
H ln ant ,snt ,
(6)
t=0

where

o
n
[n] [n] [n] T
st , at , rt,

t=1

t=0

t=0

denotes the n-th trajectory. This formulation resembles the def-

inition of REINFORCE estimate given by Williams (1992) for the gradient  J(). Such
estimates, known as likelihood ratio methods, overcome the problem of determining the perturbation of the parameters occurring in finite-difference methods. Algorithm 1 describes
the complete PMGA procedure.
In order to simplify the theoretical analysis of the Hessian estimate, we make the following assumptions.
Assumption 4.1 (Uniform boundedness). The reward function, the log-Jacobian and the
log-Hessian of the policy are uniformly bounded: i = 1, . . . , q, m = 1, . . . , d, n =
1, . . . , d, (s, a, s0 )  S  A  S ,   
fi
fi
fi
fi
fi
fi
fi
fi (m)
fi
fi (m,n)
fi
0 fi
ln (a|s, )fi  G.
fiRi (s, a, s )fi  Ri ,
fiD ln (a|s, )fi  D,
fiH
Lemma 4.2. Given a parametrized policy (a|s, ), under Assumption 4.1, the i-th component of the log-Hessian of the expected return can be bounded by
kH Ji ()kmax 


Ri T  T 
2
TD + G ,
1

where the max norm of a matrix is defined as kAkmax = maxi,j {aij }.
198

fiMORL through Continuous Pareto Manifold Approximation

Proof. Consider the definition of the Hessian in Equation (5). Under assumption 4.1, the
Hessian components can be bounded by (m, n)
fi
"
T
1
T
1
fi
fi fiZ
X
X


fi (m,n)
fi fi
ln (at |st , )
ln (aj |sj , )
Ji ()fi = fi p ( |) ri ( )
fiH
fi T
 m
 n
t=0
j=0
#fi
fi
2
fi
+
ln (at |st , ) fi
fi
 m  n


T
1
T
1
T
1

X
X
X
Ri T  T 
2
D
 Ri
 l1 
D + G =
TD + G .
1
l=0

t=0

j=0

The previous result can be used to derive a bound on the sample complexity of the
Hessian estimate.
Theorem 4.3. Given a parametrized policy (a|s, ), under Assumption 4.1, using the
following number of T -step trajectories

 2 2
1
Ri T  T 
2
N 2
TD + G
ln

2i (1  )
b  Ji () generated by Equation (6) is such that with probability 1  
the gradient estimate H


b

 i .
H Ji ()  H Ji ()
max

Proof. Hoeffdings inequality implies that m, n
N 2 2
fi
fi
i
 PN
fi b (m,n)
fi
(m,n)
(bi ai )2
i=1
=.
P fiH
Ji ()  H
Ji ()  i fi  2e

Solving the equation for N and noticing that Lemma 4.2 provides a bound on each sample,
we obtain

2 2
1
Ri T  T 
2
N= 2
TD + G
ln .

2i (1  )

The integral estimate can be computed using standard MonteCarlo techniques. Several
statistical bounds have been proposed in literature, we refer to Robert and Casella (2004)
for a survey on MonteCarlo methods.
At this point of the paper, the reader may expect an analysis of the convergence (or
convergence rate) to the optimal parametrization. Although we consider this analysis theoretically challenging and interesting, we will not provide any result related to this topic.
This analysis is hard (or even impossible) to provide in general settings since the objective
function is nonlinear and nonconcave. Moreover, an analysis of a simplified scenario (if
possible) will be almost useless in real applications.
199

fiParisi, Pirotta, & Restelli

5. Metrics for Multiobjective Optimization
In this section, we review some indicator functions proposed in literature, underlining advantages and drawbacks, and propose some alternatives. Recently, MOO has focused on
the use of indicators to turn a multiobjective optimization problem into a singleobjective
one by optimizing the indicator itself. The indicator function is used to assign to every
point of a given frontier a scalar measure that gives a rough idea of the discrepancy between the candidate frontier and the Pareto one. Since instead of optimizing the objective
functions directly indicatorbased algorithms aim at finding a solution set that maximizes
the indicator metric, a natural question arises about the correctness of this change in the
optimization procedure and on the properties the indicator functions enjoy. For instance,
the hypervolume indicator and its weighted version are among the most widespread metrics
in literature. These metrics have gained popularity because they are refinements of the
Pareto dominance relation (Zitzler, Thiele, & Bader, 2010). Recently, several works have
been proposed in order to theoretically investigate the properties of the hypervolume indicator (e.g., Friedrich, Horoba, & Neumann, 2009). Nevertheless, it has been argued that the
hypervolume indicator may introduce a bias in the search. Furthermore another important
issue when dealing with the hypervolume indicator is the choice of the reference point. From
our perspective, the main issues of this metric are the high computational complexity (the
computation of the hypervolume indicator is a #Phard problem, see Friedrich et al., 2009)
and, above all, the non differentiability. Several other metrics have been defined in the field
of MOO, we refer to the work by Okabe, Jin, and Sendhoff (2003) for a survey. However, the
MOO literature has not been able to provide a superior metric and among the candidates
no one is suited for our scenario. Again, the main issues are the non differentiability, the
capability of evaluating only discrete representations of the Pareto frontier and the intrinsic
nature of the metrics. For example, the generational distance, another widespread measure
based on the minimum distance from a reference frontier, is not available in our settings.
To overcome these issues, we mixed different indicator concepts into novel differentiable
metrics. The insights that have guided our metrics definition are related to the MOO
desiderata. Recall that the goal of MOO is to compute an approximation of the frontier
including solutions that are accurate, evenly distributed and covering a range similar to the
actual one (Zitzler et al., 2003). Note that the uniformity of the frontier is intrinsically guaranteed by the continuity of the approximation we have introduced. Having these concepts
in mind, we need to induce accuracy and extension through the indicator function.
We have not stressed but it is clear from the definition that we want the indicator to
be maximized by the real Pareto frontier. We also must ensure that the indicator function
induces a partial ordering over frontiers: manifold F2 solutions are all (weakly) dominated
by manifold F1 ones, then F1 manifold value must be better than F2 one.
Definition 5.1 (Consistent Indicator Function). Let F be the set of all (q 1)dimensional
manifolds associated to a MOMDP with q objectives. Let k   be the manifold in the
policy parameters
space mapping to Fk  F and F  be the true Pareto frontier. Let
R
LI (F) = F IdV be the manifold value. An indicator function I is consistent if
Fk 6= Fh , LI (Fh ) > LI (Fk )  Fh  F  ,

and

h , k ,  i  k ,  j  h , j  i = LI (Fh ) > LI (Fk ).
200

fiMORL through Continuous Pareto Manifold Approximation

5.1 Accuracy Metrics
Given a reference point p, a simple indicator can be obtained by computing the distance
between every point of a frontier F and the reference point, i.e.,
I = kJ  pk22 .
As mentioned for the hypervolume indicator, the choice of the reference point may be
critical. However, a natural choice is the utopia (ideal) point (pU ), i.e., the point that
optimizes all the objectives. In this case the goal is the minimization of such indicator
function, denoted by IU (utopia indicator ). Since any dominated policy is farther from the
utopia than at least one Paretooptimal solution, the accuracy can be easily guaranteed. On
the other hand, since it has to be minimized, this measure forces the solution to collapse
into a single point, thus it is not consistent. Note that this problem can be mitigated
(but not solved) by forcing the transformation  to pass through the singleobjective
optima. Although this trick can be helpful, as we will discuss in Section 6, it requires to
find the singleobjective optimal policies in order to constrain the parameters. However,
this information is also required to properly set the utopia.
Concerning the accuracy of the frontier, from a theoretical perspective, it is possible to
define another metric using the definition of Pareto optimality. A point  is Paretooptimal
when (Brown & Smith, 2005)
l(, ) =

q
X

i  Ji () = 0,

q
X

i=1

i = 1,

i  0,

i=1

that is, it is not possible to identify an ascent direction that simultaneously improves all
the objectives. As a consequence, the Paretoascent direction l of any point on the Pareto
frontier is null. Formally, a metric that respects the Paretooptimality can be defined as
follows:
q
X
I = minq kl(, )k22 ,
i = 1, i  0.
R

i=1

We denote this indicator with IPN (Pareto norm indicator ). As for the utopiabased metric,
the extent of the frontier is not taken into account and without any constraint the optimal
solution collapses into a single point on the frontier.
5.2 Covering Metrics
If the extension of the frontier is the primary concern, maximizing the distance from the
antiutopia (pAU ) results in a metric that grows with the frontier dimension. However,
on the contrary of the utopia point, the antiutopia is located in the half space that can
be reached by the solutions of the MOO problems. This means that by considering the
antiutopiabased metric the maximization problem could become unbounded by moving
solutions arbitrary far from both the Pareto frontier and the antiutopia point. Therefore
this measure, denoted by IAU (antiutopia indicator ), does not provide any guarantee about
accuracy.
201

fiParisi, Pirotta, & Restelli

5.3 Mixed Metrics
All the mentioned indicators provide only one of the desiderata. As a consequence, the
resulting approximate frontier might be arbitrary far from the actual one. In order to
consider both the desiderata we can mix the previous concepts into the following indicator:
I = IAU  w
where w is a penalization function, i.e., it is a monotonic function that decreases as the
accuracy of the input increases, e.g., w = 1  IPN or w = 1  IU . These metrics, denoted
respectively by I,PN and I,U , take advantage of the expansive behavior of the antiutopia
based indicator and the accuracy of some optimalitybased indicator. In this way all the
desiderata can be met by a single scalar measure, that is also C l (l  1) differentiable.
Another solution is to mix utopia and antiutopiabased indicators in a different way.
As we want solutions that are simultaneously far from the antiutopia and close to the utopia,
we consider the following metric I (to be maximized):
I = 1

IAU
 2 ,
IU

where 1 and 2 are free parameters.
In the next section, we will show that the proposed mixed metrics are effective in driving
PMGA close to the Pareto frontier both in exact and approximate scenarios. However, we
want to make clear that their consistency is not guaranteed as it strongly depends on the
free parameters , 1 and 2 . More insights are discussed in Section 7.

6. Experiments
In this section, we evaluate our algorithm on two problems, a Linear-Quadratic Gaussian
regulator and a water reservoir control task. PMGA is compared to state-of-the-art methods
(Peters, Mulling, & Altun, 2010; Castelletti et al., 2013; Parisi et al., 2014; Beume, Naujoks,
& Emmerich, 2007) using the hypervolume (Vamplew et al., 2011) and an extension of a
previously defined performance index (Pianosi, Castelletti, & Restelli, 2013), named loss,
measuring the distance of an approximate Pareto front from a reference one. For 2objective
problems, the hypervolume is exactly computed. For 3objective problems, given its high
computational complexity, the hypervolume is approximated with a MonteCarlo estimate
as the percentage of points dominated by the frontier in the cube defined by the utopia and
antiutopia points. For the estimate one million points were used.
}
The idea of the loss index is to compare the true Pareto frontier FW = {Jw
wW over a
M
space of weights W to the frontier JW = {Jbw }wW returned by an algorithm M over the
same weights (Jw denotes the discounted return of a new singleobjective MDP defined by
the linear combination of the objectives over w). Formally the loss function l is defined as
l(J

M

Jw  maxM Jbw

Z

J

, F, W, p) =
wW

Jw

p(dw),

(7)

where p() is a probability density over the simplex W and Jw = w  J is the normalization factor, where the i-th component of J is the difference between the best and the
202

fiMORL through Continuous Pareto Manifold Approximation

worst value of the i-th objective of the Pareto frontier, i.e., Ji = max(Ji )  min(Ji ). This
M.
means that, for each weight, the policy that minimizes the loss function is chosen in JW
If the true Pareto frontier F is not known, a reference one is used.
Since PMGA returns continuous frontiers and the two scores are designed for discrete
ones, for the evaluation all the frontiers have been discretized. Also, figures presented in
this section show discretized frontiers in order to allow a better representation. Besides the
hypervolume and the loss function, we report also the number of solutions returned by an
algorithm and the number of rollouts (i.e., the total number of episodes simulated during
the learning process). All data have been collected in simulation and results are averaged
over ten trials6 . In all the experiments, PMGA learning rate is
s

,
(8)
=
T
 L () M 1  L ()
where M is a positive definite, symmetric matrix and  is a userdefined parameter. This
stepsize rule comes from the formulation of the gradient ascent as a constrained problem
with a predefined distance metric M (Peters, 2007) and underlies the derivation of natural
gradient approaches. However, since our algorithm exploits the vanilla gradient (i.e., we
consider the Euclidean space) the metric M is the identity matrix I.
The remainder of the section is organized as follows. We start by studying the behavior
of the metrics proposed in Section 5 and the effects of the parametrization  (t) on the LQG.
Subsequently, we focus our attention on sample complexity, meant as the number of rollouts
needed to approximate the Pareto front. Finally, we analyze the quality of our algorithm
on the water reservoir control task, a more complex real world scenario, and compare it
to some state-of-the-art multiobjective techniques. For each case study, domains are first
presented and then results are reported and discussed.
6.1 Linear-Quadratic Gaussian Regulator (LQG)
The first case of study is a discrete-time Linear-Quadratic Gaussian regulator (LQG) with
multi-dimensional and continuous state and action spaces (Peters & Schaal, 2008b). The
LQG problem is defined by the following dynamics
st+1 = Ast + Bat ,

at  N (K  st , )

R(st , at ) = st T Qst  at T Rat
where st and at are n-dimensional column vectors, A, B, Q, R  Rnn , Q is a symmetric
semidefinite matrix, and R is a symmetric positive definite matrix. Dynamics are not
coupled, i.e., A and B are identity matrices. The policy is Gaussian with parameters
 = vec(K), where K  Rnn . Finally, a constant covariance matrix  = I is used.
The LQG can be easily extended to account for multiple conflicting objectives. In
particular, the problem of minimizing the distance from the origin w.r.t. the i-th axis has
been taken into account, considering the cost of the action over the other axes
X
Ri (st , at ) = s2t,i 
a2t,j .
i6=j

6. Source code available at https://github.com/sparisi/mips.

203

fiParisi, Pirotta, & Restelli

Since the maximization of the i-th objective requires to have null action on the other axes,
objectives are conflicting. As this reward formulation violates the positiveness of matrix
Ri , we change it adding a sufficiently small -perturbation




X
X
Ri (st , at ) = (1  ) s2t,i +
a2t,j    
s2t,j + a2t,i  .
i6=j

j6=i

The parameters used for all the experiments are the following:  = 0.9,  = 0.1 and initial
state s0 = [10, 10]T and s0 = [10, 10, 10]T for the 2 and 3objective case, respectively. The
following sections compare the performance of the proposed metrics under several settings.
We will made use of tables to summarize the results at the end of each set of experiments.
6.1.1 2objective Case Results
The LQG scenario is particular instructive since all terms involved in the definition of returns, gradients and Hessians can be computed exactly. We can therefore focus on studying
different policy manifold parametrizations  (t) and metrics I.
Unconstrained Parametrization. The domain is problematic since it is defined only
for control actions in the range [1, 0] and controls outside this range lead to divergence of
the system. Our primary concern was therefore related to the boundedness of the control
actions, leading to the following parametrization of the manifold in the policy space:


(1 + exp(1 + 2 t))1
,
t  [0, 1].
 =  (t) =
(1 + exp(3 + 4 t))1
Utopia and antiutopia points are [150, 150] and [310, 310], respectively, and metrics IAU and
IU are normalized in order to have 1 as reference point.7 The learning step parameter  in
Equation (8) is  = 1.
In this case, exploiting nonmixed metrics, PMGA was not able to learn a good approximation of the Pareto frontier in terms of accuracy and covering. Using utopiabased
indicator, the learned frontier collapses in one point on the knee of the front. The same
behavior occurs using IPN . Using antiutopia point as reference point the solutions are
dominated and the approximate frontier gets wider, diverging from the true frontier and
expanding on the opposite half space. These behaviors are not surprising, considering the
definition of these indicator functions, as explained in Section 5.
On the contrary, as shown in Figure 2, all mixed metrics are able to achieve both
accuracy and covering. The starting 0 was set to [1, 2, 0, 3]T , but the algorithm was also
able to learn even starting from different random parameters. The free metric parameters
were set to  = 1.5 for I,PN ,  = 1 for I,U and to 1 = 3, 2 = 1 for I .8 Although not
shown in the figure, I,U behaved very similarly to I,PN . We can notice that in both cases
first accuracy is obtained by pushing the parametrization onto the Pareto frontier, then the
frontier is expanded toward the extrema in order to attain covering.
7. Recall that we have initially defined I = kJ  pk22 . Here we slightly modify it by normalizing the policy
performance w.r.t. the reference point: I = kJ/p  1k22 , where / is a component-wise operator.
8. In Section 7 we will study the sensitivity of the proposed metrics to their parameters  and .

204

fiMORL through Continuous Pareto Manifold Approximation

Table 1: Summary of 2dimensional LQG (unconstrained)
Metrics
Nonmixed
Issues:

Accuracy
Covering
7
7
IU , IPN : frontier collapses in one point
IAU : diverging behavior and dominated solutions found
3
3

Mixed

Partial solution
Final approximation
True Pareto frontier

300

250

16
L ()

23

J2

100

20

200

50

1

21

end

0

150
150

200

250

300

0

50

J1

100

Iterations

(a) Learning process with mixed metric I,PN .

300

15

250
10

L ()

J2

1,000

1

200

500

0

5
end
150
150

200

250

500

300

0

J1

50

100

Iterations

(b) Learning process with mixed metric I .

Figure 2: Learning processes for the 2objective LQG without any constraint on the
parametrization. Numbers denote the iteration, end denotes the frontier obtained when
the terminal condition is reached. On the left, the approximated Pareto frontiers, on the
right the corresponding L (). Using both I,PN (Figure (a)) and I (Figure (b)) the approximated frontier overlaps with the true one. However, using I , PMGA converges faster.

205

fiParisi, Pirotta, & Restelli

Constrained Parametrization. An alternative approach consists in forcing the policy
manifold to pass through the extreme points of the true front by knowing the parameterizations of the singleobjective optimal policies. In general, this requires additional
optimizations and the collection of additional trajectories that must be accounted for in the
results. However, the extreme points are required to set the utopia and antiutopia. Moreover, in our case the optimal singleobjective policies were available in literature. For these
reasons, we do not count additional samples when we report the total number of rollouts.
Using a constrained parameterization, two improvements can be easily obtained. First,
the number of free parameters decreases and, as a consequence, the learning process is
simplified. Second, the approximate frontier is forced to have a sufficiently large area to
cover all the extrema. Thus, the problem of covering shown by nonmixed indicators can
be alleviated or, in some cases, completely eliminated. For the 2dimensional LQG, a
parametrization forced to pass through the extrema of the frontier is the following:


(1 + exp(2.18708  1 t2 + (3.33837 + 1 )t))1
 =  (t) =
,
t  [0, 1].
(1 + exp(1.15129  2 t2 + (3.33837 + 2 )t))1
The initial parameter vector is 0 = [2, 2]T . The constraint was able to correct the diverging
behavior of IU and IPN , which returned an accurate and wide approximation of the Pareto
frontier, as shown in Figure 2a. We also notice a much faster convergence, since the algorithm is required to learn fewer parameters (two instead of four). However, IAU still shows
the same diverging behavior for some initial parameters 0 (in Figure 2b, 0 = [6, 6]T ). On
the contrary, solutions obtained with the other metrics are independent from the initial 0 ,
as the algorithm converges close to the true frontier even starting from a parametrization
generating an initial frontier far away from the true one.
6.1.2 3objective Case Results
Unconstrained Parametrization.


(1 + exp(1 + 2 t1 + 3 t2 ))1
 =  (t) = (1 + exp(4 + 5 t1 + 6 t2 ))1  ,
(1 + exp(7 + 8 t1 + 9 t2 ))1

t  simplex([0, 1]2 ).

Utopia and antiutopia points are [195, 195, 195] and [360, 360, 360], respectively, and metrics
IAU , IU are normalized. The initial parameters are drawn from a uniform distribution 0 
U nif ((0, 0.001)) (0 = 0 causes numerical issues) and the learning rate parameter is  = 1.
As in the 2objective scenario, frontiers learned with IU and IPN collapse in a single
point, while IAU has a divergent trend (Figure 3a). However, unlike the 2objective LQR,
I,PN also failed in correctly approximate the Pareto frontier. The reason is that the tuning
of  is difficult, given the difference in magnitude between IPN and IAU On the contrary,
I,U with  = 1.5 and I with 1 = 3, 2 = 1 returned a high quality approximate frontier.
The latter is shown in Figure 3b. Although some small areas of the true Pareto frontier
are not covered by the approximate one, we stress the fact that all the policies found were
Paretooptimal. The strength of these metrics is to be found in the normalization of both
utopia and antiutopiabased indicators. This expedient, indeed, allows for an easier tuning
of the free metric parameters, as the magnitude of the single components is very similar.
More insights into the tuning of mixed metrics parameters are discussed in Section 7.
206

fiMORL through Continuous Pareto Manifold Approximation

Table 2: Summary of 2dimensional LQG (constrained)
Metrics
Nonmixed: IU , IPN
Nonmixed: IAU
Issues:
Mixed

Accuracy
Covering
3
3
7
7
IAU : diverging behavior and dominated solutions found
3
3

Partial solution
Final approximation
True Pareto frontier

300

1

120
L ()

J2

250

2

200

3

130

end
150
150

200

250

140

300

0

5

J1

10

15

20

25

100

120

Iterations

(a) Learning process with utopiabased metric IU .

300

23

250

600

J2

7

200

L ()

3
1

400
200

150

0

150

200

250

300

0

J1

20

40

60

80

Iterations

(b) Learning process with antiutopiabased metric IAU .

Figure 3: Learning process for the 2objective LQG with a parametrization forced to pass
through the extreme points of the frontier. The constraints are able to correct the behavior
of IU (Figure (a)) and the convergence is faster than the previous parametrization. However,
IAU still diverges (Figure (b)) and the returned frontier includes dominated solutions, since
the metric considers only the covering of the frontier and not the accuracy.

207

fiParisi, Pirotta, & Restelli

Table 3: Summary of 3dimensional LQG (unconstrained)
Metrics
Nonmixed
Issues:

Accuracy
Covering
7
7
IU , IPN : frontier collapses in one point
IAU : diverging behavior and dominated solutions found
7
7
I,PN : difficult tuning of 
3
3

Mixed: I,PN
Issues:
Mixed: I,U , I

True Pareto frontier
Approximate frontier

J3

1,000

500

500

500

1,000

J1

1,000

500

1,000

500

J1

J2

1,000

J2

(a) Frontier approximated with antiutopiabased metric IAU .

J3

True Pareto frontier
Approximate frontier

300
350

200
300

200
200

200
J2

300

300

J1

250
250

300
200

J1

J2

350

(b) Frontier approximated with mixed metric I .

Figure 4: Resulting frontiers for the 3objective LQG using an unconstrained parametrization. Frontiers have been discretized for better representation. With IAU the learning
diverges (Figure (a)) while I correctly approximates the Pareto frontier (Figure (b)).

208

fiMORL through Continuous Pareto Manifold Approximation

Constrained Parametrization.


(1 + exp(a + 1 t1  (b  2 )t2  1 t21  2 t22  3 t2 t1 ))1
,
(1 + exp(a  (b  4 )t1 + 5 t2  4 t21  5 t22  6 t1 t2 ))1
 =  (t) = 
2
2
1
(1 + exp(c + (7 + b)t1 + (8 + b)t2  7 t1  8 t2  9 t1 t2 ))
a = 1.151035476,

b = 3.338299811,

t  simplex([0, 1]2 ).

c = 2.187264336,

The initial parameters are 0 = 0. Numerical results are reported in Table 4, where the
hypervolume has been computed normalizing the objective w.r.t. the antiutopia. Figure 5
shows the frontiers obtained using utopia and antiutopiabased indicators. We can clearly
see that, unlike the 2objective case, even with a constrained parametrization these metrics
lead to poor solutions, failing in providing all MO desiderata. In Figure 5a, using IU the
frontier still tends to collapse towards the center of the true one, in order to minimize the
distance from the utopia point (only the constraint on  prevents that). Although not shown
in the figures, a similar but slightly broader frontier is returned using IPN . However, we
stress that all solutions belong to the Pareto frontier, i.e., only nondominated solutions are
found. Figure 5b shows the frontier obtained with IAU . As expected, the algorithm tries to
produce a frontier as wide as possible, in order to increase the distance from the antiutopia
point. This behavior leads to dominated solutions and the learning process diverges.
On the contrary, using mixed metrics I,PN ( = 30), I,U ( = 1.4) and I (1 =
2.5, 2 = 1) PMGA is able to completely and accurately cover the Pareto frontier, as shown
in Figures 6a and 6b. It is worth to notice the different magnitude of the free parameter  in
I,PN compared to the 2objective case, for which  was 1.5. As already discussed, this is due
to the substantial difference in magnitude between IAU and IPN . On the contrary, the tuning
for the other mixed metrics was easier, as similar parameters used for the unconstrained
parametrization proved to be effective. We will come back to this topic in Section 7.
Finally, as shown in Table 4, I,U and I achieve the best numerical results, as the first
attains the highest hypervolume and the lowest loss, while the latter attains the fastest
convergence. Their superiority also resides in their easy differentiability and tuning, especially compared to I,PN . For these reasons, we have chosen them for an empirical analysis
on sample complexity and for a comparison against some state-of-the-art algorithms on a
real-world MO problem, which will be discussed in the next sections.
Table 4: Performance comparison between different metrics on the 3objective LQG with
constrained parametrization. The reference frontier has a hypervolume of 0.7297.
Metric

Hypervolume

Loss

#Iterations

IU

0.6252

2.9012e-02

59

IAU

0





IPN

0.7167

1.9012e-02

133

I,PN

0.7187

5.2720e-04

47

I,U

0.7212

4.9656e-04

33

I

0.7204

5.0679e-04

15

209

fiParisi, Pirotta, & Restelli

Table 5: Summary of 3dimensional LQG (constrained)
Metrics
Nonmixed
Issues:
Mixed

Accuracy
Covering
7
7
IU , IPN : frontier collapses in one point
IAU : diverging behavior and dominated solutions found
3
3

J3

True Pareto frontier
Approximate frontier

300
350

200
300

200
200

200
J2

300

300

J1

250
250

300
200

J1

J2

350

(a) Frontier approximated with utopiabased metric IU .

J3

True Pareto frontier
Approximate frontier

500
200

350
250

300
200

200
300
J2

J1

300
250

300

J2

350
J1

200

(b) Frontier approximated with antiutopiabased metric IAU .

Figure 5: Results with a parametrization forced to pass through the extreme points of the
frontier. Using IU (Figure (a)) the frontier shrinks as much as allowed by the parametrization. The constraint is therefore not able to solve the issues of the metric as in the 2
objective scenario. On the contrary, using IAU the frontier gets wider and diverges from the
true one (in Figure (b) an intermediate frontier is shown).

210

fiMORL through Continuous Pareto Manifold Approximation

J3

True Pareto frontier
Approximate frontier

300

3

200
200

200
J2

300

300

0.4
0.6
0.8

J1

0.4

0.8

0.6
1

(a) Frontier in objectives space.

0.4

0.6
2
0.8

(b) Frontier in policy parameters space.

Figure 6: Results using I and a constrained parametrization. As shown in Figure (a),
the approximate frontier perfectly overlaps the true one, despite small discrepancies in the
policy parameters space between the learned parameters and the optimal ones (Figure (b)).
Similar frontiers are obtainable with I,PN and I,U .

6.1.3 Empirical Sample Complexity Analysis
In this section, we provide an empirical analysis of the sample complexity of PMGA, meant
as the number of rollouts needed to approximate the Pareto frontier. The goal is to identify
the most relevant parameter in the estimate of MDP terms J(), D J() and HJ().
The analysis is performed on the 2dimensional LQG domain by varying the number of
policies used to estimate the integral per iteration of PMGA and the number of episodes
for each policy evaluation. The steps of each episode are fixed to 50. We first used the
parametrization forced to pass through the extreme points of the frontier with 0 = [3, 7]T ,
that produces an initial approximate frontier far from the true one. The parameter of the
learning rate in Equation (8) was set to  = 0.5 and the parameter of I,U was set to
 = 1. As performance criterion, we choose the total number of rollouts required to reach
a loss smaller than 5  104 and a hypervolume larger than 99.5% of the reference one.
These criteria are also used as conditions for convergence (both have to be satisfied). For
the evaluation, MDP terms are computed in closed form. The terminal condition must be
reached in 100, 000 episodes otherwise the algorithm is forced to end. The symbol  is used
to represent the latter case.
From Table 6a it results that the most relevant parameter is the number of episodes
used to estimate the MDP terms. This parameter controls the variance in the estimate,
i.e., the accuracy of the estimate of  L (). By increasing the number of episodes, the
estimation process is less prone to generate misleading directions, as happens, for instance,
in the oneepisode case where parameters move towards a wrong direction. On the contrary,
the number of points used to estimate the integral (denoted in the table by #t) seems to
have no significant impact on the final performance of the algorithm, but it influences the
number of model evaluations needed to reach the prescribed accuracy. The best behavior,
211

fiParisi, Pirotta, & Restelli

Table 6: Total number of episodes needed to converge on varying the number of points #t
to approximate the integral and the number of episodes #ep per point. The symbol  is
used when the terminal condition is not reached.
(a) If the parametrization is constrained to pass through the extreme points of the frontier, only one
point t is sufficient to move the whole frontier towards the right direction.

#ep

1

5

10

25

50

1



695  578

560  172

1, 850  757

1, 790  673

5



2, 550  1, 509

3, 440  2, 060

5, 175  3, 432

8, 250  2, 479

10



4, 780  4, 623

6, 820  3, 083

10, 500  3, 365

11, 800  1, 503

25



7, 525  2, 980

15, 100  9, 500

18, 375  6, 028

24, 250  7, 097

50



8, 700  5, 719

18, 000  6, 978

26, 750  7, 483

50, 000  1, 474

#t

(b) On the contrary, using an unconstrained parametrization, PMGA needs both a sufficient number
of episodes and enough points t for a correct update step.

#ep

1

5

10

25

50

1











5









29, 350  7, 310

10







44, 100  9, 466

64, 500  1, 359

25







60, 500  1, 000

83, 500  8, 923

50





47, 875  18, 558

84, 250  1, 457



#t

from a samplebased perspective, has been obtained by exploiting only one point for the
integral estimate. Although it can be surprising, a simple explanation exists. By forcing
the parameterization to pass through the singleobjective optima, a correct estimation of
the gradient direction of a single point t is enough to move the entire frontier toward the
true one, i.e., to move the parameters towards the optimal ones.
On the contrary, if the unconstrained parametrization is used, one point is not sufficient
anymore, as shown in Table 6b. In this case, the initial parameter vector was set to 0 =
[1, 1, 0, 0]T , the learning rate parameter to  = 0.1 and the terminal condition requires a
frontier with loss smaller than 103 and hypervolume larger than 99% of the reference
frontier. Without any constraint, the algorithm needs both accuracy in the evaluation of
single points i.e., a sufficient number of episodes and enough points t to move the whole
frontier towards the right direction. The accuracy of the gradient estimate  L () therefore
depends on both the number of points t and the number of episodes, and PMGA requires
much more rollouts to converge. The best behavior, from a samplebased perspective, has
been obtained by exploiting five points for the integral estimate and 50 episodes for the
policy evaluation.
212

fiMORL through Continuous Pareto Manifold Approximation

6.2 Water Reservoir
A water reservoir can be modeled as a MOMDP with a continuous state variable s representing the water volume stored in the reservoir, a continuous action a controlling the
water release, a state-transition model depending also on the stochastic reservoir inflow ,
and a set of conflicting objectives. This domain was proposed by Pianosi et al. (2013).
Formally, the state-transition function can be described by the mass balance equation
st+1 = st + t+1  max(at , min(at , at )) where st is the reservoir storage at time t; t+1 is the
reservoir inflow from time t to t + 1, generated by a white noise with normal distribution
t+1  N (40, 100); at is the release decision; at and at are the minimum and the maximum
releases associated to storage st according to the relations at = st and at = max(st 100, 0).
In this work we consider three objectives: flooding along the lake shores, irrigation
supply and hydro-power supply. The immediate rewards are defined by
R1 (st , at , st+1 ) =  max(ht+1  h, 0),
R2 (st , at , st+1 ) =  max(  t , 0),
R3 (st , at , st+1 ) =  max(e  et+1 , 0),
where ht+1 = st+1 /S is the reservoir level (in the following experiments S = 1), h is the
flooding threshold (h = 50), t = max(at , min(at , at )) is the release from the reservoir,  is
the water demand ( = 50), e is the electricity demand (e = 4.36) and et+1 is the electricity
production
et+1 =  g  H2 0 t ht+1 ,
where  = 106 /3.6 is a dimensional conversion coefficient, g = 9.81 the gravitational
acceleration,  = 1 the turbine efficiency and H2 0 = 1, 000 the water density. R1 denotes
the negative of the cost due to the flooding excess level, R2 is the negative of the deficit in
water supply and R3 is the negative of the deficit in hydro-power production.
Like in the original work, the discount factor is set to 1 for all the objectives and the
initial state is drawn from a finite set. However, different settings are used for the learning
and evaluation phases. Given the intrinsic stochasticity of the problem, all policies are
evaluated over 1,000 episodes of 100 steps, while the learning phase requires a different
number of episodes over 30 steps, depending on the algorithm. We will discuss the details
in the results section.
Since the problem is continuous we exploit a Gaussian policy model


(a|s, ) = N  + (s)T ,  2 ,
where  : S  Rd are the basis functions, d = || and  = {, , }. As the optimal policies
for the objectives are not linear in the state variable, we use a radial basis approximation


i (s) = e

ksci k2
wi

.

We used four centers ci uniformly placed in the interval [20, 190] and widths wi of 60, for
a total of six policy parameters.
213

fiParisi, Pirotta, & Restelli

6.2.1 Results
To evaluate the effectiveness of our algorithm we have analyzed its performance against the
frontiers found by a weighted sum Stochastic Dynamic Programming (Pianosi et al., 2013),
Multi-objective FQI (Pianosi et al., 2013), the episodic version of Relative Entropy Policy
Search (Peters et al., 2010; Deisenroth et al., 2013), SMS-EMOA (Beume et al., 2007),
and two recent policy gradient approaches, i.e., Radial Algorithm and ParetoFollowing
Algorithm (Parisi et al., 2014). Since the optimal Pareto front is not available, the one
found by SDP is chosen as reference one for the loss computation. MOFQI learns only
deterministic policies (i.e., the standard deviation  of the Gaussian is set to zero) and
has been trained using 10, 000 samples with a dataset of 50, 000 tuples for the 2objective
problem and 20, 000 samples with a dataset of 500, 000 tuples for the 3objective problem.
The remaining competing algorithms all learn stochastic policies. The number of episodes
required for a policy update step is 25 for REPS, 100 for PFA and RA, 50 for SMS-EMOA.
Given its episodic formulation, REPS draws the parameters  from an upper distribution
(|) = N (, ) ,
where  is a diagonal covariance matrix, while  is set to zero. However, since the algorithm
learns the parameters  = {, }, the overall learned policy is still stochastic. SMS-EMOA
has a maximum population size of 100 and 500 for the 2 and 3objective case, respectively.
The crossover is uniform and the mutation, which has a chance of 80% to occur, adds a white
noise to random chromosomes. At each iteration, the top 10% individuals are kept in the
next generation to guarantee that the solution quality will not decrease. Finally, MOFQI
scalarizes the objectives using the same weights as SDP, i.e., 11 and 25 weights for the 2 and
3objective case, respectively. REPS uses instead 50 and 500 linearly spaced weights. RA
also follows 50 and 500 linearly spaced directions and, along with PFA, exploits the natural
gradient (Peters & Schaal, 2008a) and the adaptive learning step in Equation (8), with  = 4
and M = F , where F is the Fisher information matrix. Concerning the parametrization of
PMGA, we used a complete first degree polynomial for the 2objective case


66  1 t2 + (1  16)t
105  2 t2 + (2 + 20)t


 18  3 t2 + (3  16)t 

,
t  [0, 1].
 =  (t) = 
2

 23  4 t + (4 + 53)t 
 39  5 t2 + (5 + 121)t 
0.01  6 t2 + (6 + 0.1)t
Similarly, for the 3objective case a complete second degree polynomial is used


36 + (15  1 )t2 + (1 + 1)t1 t2 + 30t21 + (1  1)t22
 57  (27 + 2 )t2 + (2 + 1)t1 t2  48t21 + (2  1)t22 


 13 + (7  23 )t1 + (3 + 1)t1 t2 + (23  2)t21  11t22 
2

 =  (t) = 
30 + (9  24 )t1 + (4 + 1)t1 t2 + (24  2)t2 + 60t2  , t  simplex([0, 1] ).
1
2

 104 + (57  5 )t2 + (5 + 1)t1 t2  65t2 + (5  1)t2 
1

0.05 + (1  6 )t2 + (6 + 1)t1 t2 + (6  1)t22

2

Both parameterizations are forced to pass near the extreme points of the Pareto frontier,
computed through singleobjective policy search. In both cases the starting parameter
214

fiMORL through Continuous Pareto Manifold Approximation

103
9.5

L ()

J2 (Water Demand)

0

1

2

10

SDP
PMGA(0 )
PMGA(end )

10.5
11
11.5

50

100 150 200
Iterations

250

4

(a)

3.5

3

2.5 2 1.5
J1 (Flooding)

1

(b)

Figure 7: Results for the 2objective water reservoir. Even starting from an arbitrary poor
initial parametrization, PMGA is able to approach the true Pareto frontier (Figure (b)). In
Figure (a), the trend of the manifold metric L () averaged over ten trials.

vector is 0 = [0, 0, 0, 0, 0, 50]T . The last parameter is set to 50 in order to guarantee
the generation of sufficiently explorative policies, as  6 is responsible for the variance of
the Gaussian distribution. However, for a fair comparison, also all competing algorithms
take advantage of such information, as the mean of their initial policies is calculated accordingly to the behavior of the optimal ones described by Castelletti et al. (2012), i.e.,
 = [50, 50, 0, 0, 50]T . The initial standard deviation is set to  = 20 to guarantee sufficient exploration. This parametrization avoids completely random and poor quality initial
policies. Utopia and antiutopia points were set to [0.5, 9] and [2.5, 11] for the 2
objective case, [0.5, 9, 0.001] and [65, 12, 0.7] for the 3objective one.
According to the results presented in Section 6.1.3, the integral estimate in PMGA is
performed using a MonteCarlo algorithm fed with only one random point. For each instance of variable t, 50 trajectories by 30 steps are used to estimate the gradient and the
Hessian of the policy. Regarding the learning rate, the adaptive one described in Equation (8) was used with  = 2. For the evaluation, 1,000 and 2,000 points are used for the
integral estimate in the 2 and 3objective case, respectively. As already discussed, given
the results obtained for the LQG problem and in order to show the capability of the approximate algorithm, we have decided to consider only the indicator I (1 = 1 and 2 = 1).
The main reasons are its efficiency (in Table 4 it attained the fastest convergence) and its
easy differentiability. Finally, we recall that all the results are averaged over ten trials.
Figure 7b reports the initial and final frontiers when only the first two objectives are
considered. Even starting very far from the true Pareto frontier, PMGA is able to approach
it, increasing covering and accuracy of the approximate frontier. Also, as shown in Figure 7a, despite the very low number of exploited samples, the algorithm presents an almost
monotonic trend during the learning process, which converges in a few iterations.
215

fiParisi, Pirotta, & Restelli

J2 (Water Demand)

9.5

10

10.5

SDP
PFA
RA
MOFQI
REPS
SMS-EMOA
PMGA
2.6

2.4

2.2

2

1.8

1.6

J1 (Flooding)

1.4

1.2

1

0.8

Figure 8: Visual comparison for the 2objective water reservoir. PMGA frontier is comparable to the ones obtained by state-of-the-art algorithms in terms of accuracy and covering.
However, it is the only continuous one, as the others are scattered.
Table 7: Numerical algorithm comparison for the 2objective water reservoir. The SDP
reference frontier has a hypervolume of 0.0721 and nine solutions.
Algorithm

Hypervolume

Loss

#Rollouts

#Solutions

0.0620  0.0010

0.0772  0.0045

16, 250  1, 072



PFA

0.0601  0.0012

0.0861  0.0083

27, 761  4, 849

51.1  10.9

RA

0.0480  0.0005

0.1214  0.0043

59, 253  3, 542

16.1  2.9

-

0.1870  0.0090

10, 000

-

REPS

0.0540  0.0009

0.1181  0.0030

37, 525  2, 235

17.0  4.1

SMS-EMOA

0.0581  0.0022

0.0884  0.0019

149, 825  35, 460

14.2  2.4

PMGA

MOFQI

Figure 8 offers a visual comparison of the Pareto points and Tables 7 and 8 report a
numerical evaluation, including the hypervolume and the loss achieved by the algorithms
w.r.t. the SDP approximation9 . PMGA attains the best performance both in the 2 and 3
objective cases, followed by PFA. SMS-EMOA also returns a good approximation, but is the
slowest, requiring more than ten times the amount of samples used by PMGA. Only MOFQI
outperforms PMGA on sample complexity, but its loss is the highest. Finally, Figure 9
shows the hypervolume trend for PMGA and a comparison on sample complexity for the
2objective case. PMGA is substantially more sample efficient than the other algorithms,
attaining a larger hypervolume with much fewer rollouts. For example, it is capable of
generating a frontier with the same hypervolume of RA with only one tenth of the rollouts,
or it outperforms PFA with only half of the samples needed by the latter.
9. Results regarding MOFQI include only the loss and the number of rollouts as the hypervolume and the
number of solutions are not available from the original paper.

216

fiMORL through Continuous Pareto Manifold Approximation

0.065

PMGA

PFA (27,761)
SMS-EMOA (149,825)

Hypervolume

0.06

REPS (37,525)

0.055

RA (59,253)

0.05
0.045
0.04
2,000

4,000

6,000

8,000

10,000

12,000

14,000

16,000

#Rollouts
Figure 9: Comparison of sample complexity on the 2objective case using the hypervolume
as evaluation score. In brackets the number of rollouts needed by an algorithm to produce
its best frontier. PMGA clearly outperforms all the competing algorithms, as it requires
much fewer samples to generate frontiers with better hypervolume.
Table 8: Numerical algorithm comparison for the 3objective water reservoir. The SDP
reference frontier has a hypervolume of 0.7192 and 25 solutions.
Algorithm

Hypervolume

Loss

#Rollouts

#Solutions

0.6701  0.0036

0.0116  0.0022

62, 640  7, 963



PFA

0.6521  0.0029

0.0210  0.0012

343, 742  12, 749

595  32.3

RA

0.6510  0.0047

0.0207  0.0016

626, 441  35, 852

137.3  25.4

-

0.0540  0.0061

20, 000

-

REPS

0.6139  0.0003

0.0235  0.0014

187, 565  8, 642

86  9.7

SMS-EMOA

0.6534  0.0007

0.0235  0.0020

507, 211  56, 823

355.6  13.9

PMGA

MOFQI

7. Metrics Tuning
In this section we want to examine more deeply the tuning of mixed metric parameters, in
order to provide the reader with better insights for a correct use of such metrics. The performance of PMGA strongly depends on the indicator used and, thereby, their configuration
is critical. To be more precise, mixed metrics, which obtained the best approximate Pareto
frontiers in the experiments conducted in Section 6, include a trade-off between accuracy
and covering, expressed by some parameters. In the following, we analyze the fundamental
concepts behind these metrics and study how their performance is influenced by changes in
the parameters.
217

fiParisi, Pirotta, & Restelli

Approximate frontier
1,000

True Pareto frontier

300

300

250

250

200

200

500

150
500

(a)  = 1

1,000

150
150

200

250

300

(b)  = 1.5

150

200

250

300

(c)  = 2

Figure 10: Approximate frontiers for the 2objective LQG learned by PMGA using I,PN
on varying . In Figure (a) the indicator does not penalize enough for dominated solutions,
while in Figure (c) the frontier is not wide enough. On the contrary, in Figure (b) the
algorithm achieves both accuracy and covering.

7.1 I Tuning
The first indicator (to be maximized) that we analyze is
I = IAU  w,
where w is a penalization term. In the previous sections we proposed w = 1  IPN and
w = 1  IU , in order to take advantage of the expansive behavior of the antiutopiabased
indicator and the accuracy of an optimalitybased indicator. In this section we study the
performance of this mixed metric by changing , proposing a simple tuning process. The
idea is to set  to an initial value and then increase (or decrease) it if the approximate
frontier contains dominated solutions (or is not wide enough). Figure 10 shows different
approximate frontiers obtained with different values of  in the exact 2objective LQG after
50 iterations and using w = 1  IPN . Starting with  = 1 the indicator behaves mostly
like IAU , meaning that  was too small (Figure 10a). Increasing  to 2 (Figure 10c) the
algorithm converges, but the approximate frontier does not completely cover the true one,
i.e., IPN mostly condition the behavior of the metric. Finally, with  = 1.5 (Figure 10b) the
approximate frontier perfectly matches the true one and the metric correctly mixes the two
single indicators.
However, as already discussed in Section 6, the use of w = 1  IPN can be problematic
as the difference in magnitude between IAU and IPN can make the tuning of  hard up to
the point the metric becomes ineffective. Such a drawback can be solved using w = 1  IU
and normalizing the reference point indicators (i.e., IU and IAU ) by I(J, p) = kJ/p  1k22 ,
as the normalization bounds the utopia and antiutopiabased metrics in similar intervals,
i.e., (0, ) and [0, ), respectively.10
10. The ratio between two vectors a/b is a component-wise operation.

218

fiMORL through Continuous Pareto Manifold Approximation

J2

J2

U

1

10

J1

AU

1
(a)

J2

U

J1

AU

1
(b)

U

1

J1

AU

1
(c)

Figure 11: Examples of Pareto frontiers. In Figures (a) and (b) the frontiers are convex,
but in the latter objectives are not normalized. In Figure (c) the frontier is concave.

7.2 I Tuning
The second mixed indicator (to be maximized) also takes advantage of the expansive behavior of the antiutopiabased indicator and the accuracy of the utopiabased one. It is
defined as
IAU
I = 1
 2 ,
IU
where 1 and 2 are free parameters.
To better understand the insights that have guided our metric definition, we can consider
different scenarios according to the shape of the Pareto frontier. In Figure 11a the frontier
is convex and we normalized the objectives. In this case any point that is closer to the
antiutopia than the utopia is, for sure, a dominated solution. The ratio IAU /IU of any
point on the frontier will always be greater than 1 and hence it is reasonable to set 1
and 2 both to 1. Therefore, we do not need to know exactly the antiutopia point and the
drawback of the antiutopiabased metric IAU disappears, since we also take into account the
distance from the utopia point. Nevertheless, the setting of these points is critical, as their
magnitude can strongly affect PMGA performance. An example is shown in Figure 11b,
where the frontier is not normalized and the objectives have different magnitude. In this
case, setting both 1 and 2 to 1, the indicator I evaluated at the extrema of the frontier
(J1 = [1, 0]T and J2 = [0, 10]T ) is equal to 0.99 and 99, respectively. As the first value
is negative, an approximate frontier that includes all the points of the true Pareto frontier,
but J1 would perform better than the true Pareto frontier.
On the contrary, if the frontier is concave (Figure 11c) it is not true that any point that
is closer to the antiutopia than the utopia is a dominated solution, and the ratio IAU /IU
of any point on the frontier (with the exception, eventually, of its ends) will always be
smaller than one. Keeping 1 = 1 and 2 = 1, PMGA would try to collapse the frontier
into a single point, in order to maximize the indicator. Therefore, the parameters need to
be changed accordingly by trial-and-error. For instance, if the returned frontier does not
achieve accuracy, a possible solution is to decrease 1 or to increase 2 .
219

fiParisi, Pirotta, & Restelli

8. Conclusion
In this paper we have proposed a novel gradientbased approach, namely ParetoManifold
Gradient Algorithm (PMGA), to learn a continuous approximation of the Pareto frontier in
MOMDPs. The idea is to define a parametric function  that describes a manifold in the
policy parameters space, that maps to a manifold in the objectives space. Given a metric
measuring the quality of the manifold in the objectives space (i.e., the candidate frontier),
we have shown how to compute (and estimate from trajectory samples) its gradient w.r.t.
the parameters of  . Updating the parameters along the gradient direction generates a new
policy manifold associated to an improved (w.r.t. the chosen metric) continuous frontier
in the objectives space. Although we have provided a derivation independent from the
parametric function and the metric used to measure the quality of the candidate solutions,
both these terms strongly influence the final result. Regarding the former, we achieved
high quality results by forcing the parameterization to pass through the singleobjective
optima. However, this trick might require domain expertise and additional samples and
therefore could not always be applicable. Regarding the latter, we have presented different
alternative metrics, examined pros and cons of each one, shown their properties through
an empirical analysis and discussed a general tuning process for the most promising ones.
The evaluation also included a sample complexity analysis to investigate the performance
of PMGA, and a comparison to state-of-the-art algorithms in MORL. From the results, our
approach outperforms the competing algorithms both in quality of the frontier and sample
complexity. It would be interesting to study these properties from a theoretical perspective
in order to provide support to the empirical evidence. We leave as open problems the
investigation of the convergence rate and of the approximation error of the true Pareto
frontier. However, we think it will be hard to provide this analysis in the general setting.
Future research will further address the study of metrics and parametric functions that
can produce good results in the general case. In particular, we will investigate problems
with many objectives (i.e., more than three) and highdimensional policies. Since the complexity of the manifold parameterization grows with the number of objectives and policy
parameters, a polynomial parameterization could not be effective in more complex problems and alternative parameterizations have to be found. Another interesting direction of
research concerns importance sampling techniques for reducing the sample complexity in
the gradient estimate. Since the frontier is composed of a continuum of policies, it is likely
that a trajectory generated by a specific policy can be partially used also for the estimation
of quantities related to similar policies, thus decreasing the number of samples needed for
the MonteCarlo estimate of the integral. Moreover, it would be interesting to investigate automatic techniques for the tuning of the metric parameters and the applicability of
PMGA to the multi-agent scenario (e.g., Roijers, Whiteson, & Oliehoek, 2015).

220

fiMORL through Continuous Pareto Manifold Approximation

Appendix A. Optimal Baseline
Theorem A.1 (Componentdependent baseline). The optimal baseline for the (i, j)-component
(i,j)
of the Hessian estimate HRF, JD () given in Equation (6) is

(i,j)
bH,



2
(i,j)
G ( )



E T R( )

=
2 
(i,j)
E G ( )

,

where
(i,j)

G

(i,j)

( ) = i ln p ( |) j ln p ( |) + H

ln p ( |) .

Given a baseline b, the variance reduction obtained through the optimal baseline bH, is
Var (HRF, JD (, b))  Var (HRF, J (, bH, )) =


(i,j) 2

(i,j)
2 
b
 bH,
(i,j)
E
G ( )
.
 T
N
(i,j)

Proof. Let G

( ) be the (i, j)-th component of G ( )
(i,j)

G

(i,j)

( ) = i ln p ( |) j ln p ( |) + H

ln p ( |) .

(i,j)

The variance of HRF, JD () is given by11
Var



(i,j)
HRF, JD



i2
2 
2   h

(i,j)
(i,j)
(i,j)
G ( )
 E R( )  b(i,j) G ( )
() = E R( )  b





2 

2 
(i,j)
(i,j)
2
(i,j) 2
+E b
G ( )
= E R( ) G ( )





2
 h
i2
(i,j)
(i,j)
 2b(i,j) E R( ) G ( )
 E R( )G ( )
.




Minimizing the previous equation w.r.t. b(i,j) we get

(i,j)

bH,



2 
(i,j)
E R( ) G ( )

=
2  .
(i,j)
E G ( )

11. We use the compact notation E [] to denote E T [].

221

fiParisi, Pirotta, & Restelli

The excess of variance is given by




(i,j)
(i,j)
(i,j)
Var G ( )(R( )  b(i,j) )  Var G ( )(R( )  bH, )




2 
2 
2 

2 
(i,j)
(i,j)
(i,j)
(i,j)
(i,j)
2
 2b
E R( ) G ( )
+E b
G ( )
= E R( ) G ( )





 h

2 
 
2 
i2
(i,j)
(i,j) 2
(i,j)
(i,j)
2
 E bH,
 E R( )G ( )
 E R( ) G ( )
G ( )





2   h
i2
(i,j)
(i,j)
(i,j)
+ E R( )G ( )
+ 2bH, E R( ) G ( )




2  
2 

2 
(i,j)
(i,j)
(i,j)
(i,j)
= b
E G ( )
 2b
E R( ) G ( )




2 

2 
2 
(i,j)
(i,j)
(i,j)
(i,j)
 bH, E G ( )
+ 2bH, E R( ) G ( )






2 
2

2

(i,j)
(i,j)
(i,j)
2
(i,j)
E G ( )
 2b
E R( ) G ( )
= b








2  2



(i,j)

2 
 E R( ) G ( )

(i,j)




G ( )
2   E

(i,j)
E G ( )
 

2  
(i,j)


2 

 E R( ) G ( )
(i,j)
2




R( ) G ( )
+ 2

2
E

(i,j)
E G ( )

2 

2 
2 

(i,j)
(i,j)
(i,j)
(i,j)
 2b
E R( ) G ( )
E G ( )
= b




 

2 2
(i,j)
E R( ) G ( )

+
2 
(i,j)
E G ( )




2
(i,j)
G ( )



E R( )


 (i,j) 2
(i,j)

= b
 2b
2 

(i,j)
E G ( )
E







(i,j)

= b

(i,j)

G


( )

2 


(i,j) 2
bH, E




2
(i,j)
G ( )


.

222



2  2 
(i,j)
 
 E R( ) G ( )
 


+



 
2

(i,j)
E G ( )


fiMORL through Continuous Pareto Manifold Approximation

References
Ahmadzadeh, S., Kormushev, P., & Caldwell, D. (2014). Multi-objective reinforcement
learning for auv thruster failure recovery. In Adaptive Dynamic Programming and
Reinforcement Learning (ADPRL), 2014 IEEE Symposium on, pp. 18.
Athan, T. W., & Papalambros, P. Y. (1996). A note on weighted criteria methods for compromise solutions in multi-objective optimization. Engineering Optimization, 27 (2),
155176.
Barrett, L., & Narayanan, S. (2008). Learning all optimal policies with multiple criteria.
In Proceedings of the 25th International Conference on Machine Learning, ICML 08,
pp. 4147, New York, NY, USA. ACM.
Bertsekas, D. P. (2005). Dynamic programming and suboptimal control: A survey from
ADP to MPC*. European Journal of Control, 11 (4-5), 310  334.
Beume, N., Naujoks, B., & Emmerich, M. (2007). Sms-emoa: Multiobjective selection based
on dominated hypervolume. European Journal of Operational Research, 181 (3), 1653
 1669.
Brown, M., & Smith, R. E. (2005). Directed multi-objective optimization. International
Journal of Computers, Systems, and Signals, 6 (1), 317.
Calandra, R., Peters, J., & Deisenrothy, M. (2014). Pareto front modeling for sensitivity
analysis in multi-objective bayesian optimization. In NIPS Workshop on Bayesian
Optimization, Vol. 5.
Castelletti, A., Corani, G., Rizzolli, A., Soncinie-Sessa, R., & Weber, E. (2002). Reinforcement learning in the operational management of a water system. In IFAC Workshop
on Modeling and Control in Environmental Issues, Keio University, Yokohama, Japan,
pp. 325330.
Castelletti, A., Pianosi, F., & Restelli, M. (2012). Tree-based fitted q-iteration for multiobjective markov decision problems. In Neural Networks (IJCNN), The 2012 International Joint Conference on, pp. 18.
Castelletti, A., Pianosi, F., & Restelli, M. (2013). A multiobjective reinforcement learning
approach to water resources systems operation: Pareto frontier approximation in a
single run. Water Resources Research, 49 (6), 34763486.
Crites, R. H., & Barto, A. G. (1998). Elevator group control using multiple reinforcement
learning agents. Machine Learning, 33 (2-3), 235262.
Das, I., & Dennis, J. (1997). A closer look at drawbacks of minimizing weighted sums of
objectives for pareto set generation in multicriteria optimization problems. Structural
optimization, 14 (1), 6369.
Das, I., & Dennis, J. E. (1998). Normal-boundary intersection: A new method for generating
the pareto surface in nonlinear multicriteria optimization problems. SIAM Journal
on Optimization, 8 (3), 631657.
Deisenroth, M. P., Neumann, G., & Peters, J. (2013). A survey on policy search for robotics.
Foundations and Trends in Robotics, 2 (1-2), 1142.
223

fiParisi, Pirotta, & Restelli

Fonteneau, R., & Prashanth, L. A. (2014). Simultaneous perturbation algorithms for batch
off-policy search. In 53rd IEEE Conference on Decision and Control, CDC 2014, Los
Angeles, CA, USA, December 15-17, 2014, pp. 26222627. IEEE.
Friedrich, T., Horoba, C., & Neumann, F. (2009). Multiplicative approximations and the
hypervolume indicator. In Proceedings of the 11th Annual Conference on Genetic and
Evolutionary Computation, GECCO 09, pp. 571578, New York, NY, USA. ACM.
Furmston, T., & Barber, D. (2012). A unifying perspective of parametric policy search
methods for markov decision processes. In Pereira, F., Burges, C., Bottou, L., &
Weinberger, K. (Eds.), Advances in Neural Information Processing Systems 25, pp.
27172725. Curran Associates, Inc.
Gabor, Z., Kalmar, Z., & Szepesvari, C. (1998). Multi-criteria reinforcement learning.
In Shavlik, J. W. (Ed.), Proceedings of the Fifteenth International Conference on
Machine Learning (ICML 1998), Madison, Wisconsin, USA, July 24-27, 1998, pp.
197205. Morgan Kaufmann.
Greensmith, E., Bartlett, P. L., & Baxter, J. (2004). Variance reduction techniques for
gradient estimates in reinforcement learning. Journal of Machine Learning Research,
5, 14711530.
Harada, K., Sakuma, J., & Kobayashi, S. (2006). Local search for multiobjective function
optimization: Pareto descent method. In Proceedings of the 8th Annual Conference
on Genetic and Evolutionary Computation, GECCO 06, pp. 659666, New York, NY,
USA. ACM.
Harada, K., Sakuma, J., Kobayashi, S., & Ono, I. (2007). Uniform sampling of local paretooptimal solution curves by pareto path following and its applications in multi-objective
GA. In Lipson, H. (Ed.), Genetic and Evolutionary Computation Conference, GECCO
2007, Proceedings, London, England, UK, July 7-11, 2007, pp. 813820. ACM.
Kakade, S. (2001). Optimizing average reward using discounted rewards. In Helmbold, D. P.,
& Williamson, R. C. (Eds.), Computational Learning Theory, 14th Annual Conference
on Computational Learning Theory, COLT 2001 and 5th European Conference on
Computational Learning Theory, EuroCOLT 2001, Amsterdam, The Netherlands, July
16-19, 2001, Proceedings, Vol. 2111 of Lecture Notes in Computer Science, pp. 605
615. Springer.
Koski, J., & Silvennoinen, R. (1987). Norm methods and partial weighting in multicriterion optimization of structures. International Journal for Numerical Methods in
Engineering, 24 (6), 11011121.
Lizotte, D. J., Bowling, M., & Murphy, S. A. (2012). Linear fitted-q iteration with multiple
reward functions. Journal of Machine Learning Research, 13, 32533295.
Lizotte, D. J., Bowling, M. H., & Murphy, S. A. (2010). Efficient reinforcement learning with
multiple reward functions for randomized controlled trial analysis. In Furnkranz, J.,
& Joachims, T. (Eds.), Proceedings of the 27th International Conference on Machine
Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 695702. Omnipress.
224

fiMORL through Continuous Pareto Manifold Approximation

Magnus, J. R., & Neudecker, H. (1999). Matrix Differential Calculus with Applications
in Statistics and Econometrics. Wiley Ser. Probab. Statist.: Texts and References
Section. Wiley.
Mannor, S., & Shimkin, N. (2002). The steering approach for multi-criteria reinforcement
learning. In Dietterich, T., Becker, S., & Ghahramani, Z. (Eds.), Advances in Neural
Information Processing Systems 14, pp. 15631570. MIT Press.
Mannor, S., & Shimkin, N. (2004). A geometric approach to multi-criterion reinforcement
learning. J. Mach. Learn. Res., 5, 325360.
Messac, A., & Ismail-Yahaya, A. (2002). Multiobjective robust design using physical programming. Structural and Multidisciplinary Optimization, 23 (5), 357371.
Messac, A., Ismail-Yahaya, A., & Mattson, C. A. (2003). The normalized normal constraint method for generating the pareto frontier. Structural and multidisciplinary
optimization, 25 (2), 8698.
Munkres, J. R. (1997). Analysis On Manifolds. Adv. Books Classics Series. Westview Press.
Natarajan, S., & Tadepalli, P. (2005). Dynamic preferences in multi-criteria reinforcement
learning. In Raedt, L. D., & Wrobel, S. (Eds.), Machine Learning, Proceedings of
the Twenty-Second International Conference (ICML 2005), Bonn, Germany, August
7-11, 2005, Vol. 119 of ACM International Conference Proceeding Series, pp. 601608.
ACM.
Nojima, Y., Kojima, F., & Kubota, N. (2003). Local episode-based learning of multiobjective behavior coordination for a mobile robot in dynamic environments. In Fuzzy
Systems, 2003. FUZZ 03. The 12th IEEE International Conference on, Vol. 1, pp.
307312 vol.1.
Okabe, T., Jin, Y., & Sendhoff, B. (2003). A critical survey of performance indices for
multi-objective optimisation. In Evolutionary Computation, 2003. CEC 03. The 2003
Congress on, Vol. 2, pp. 878885 Vol.2.
Parisi, S., Pirotta, M., Smacchia, N., Bascetta, L., & Restelli, M. (2014). Policy gradient
approaches for multi-objective sequential decision making. In 2014 International Joint
Conference on Neural Networks, IJCNN 2014, Beijing, China, July 6-11, 2014, pp.
23232330. IEEE.
Perny, P., & Weng, P. (2010). On finding compromise solutions in multiobjective markov
decision processes. In Coelho, H., Studer, R., & Wooldridge, M. (Eds.), ECAI 2010 19th European Conference on Artificial Intelligence, Lisbon, Portugal, August 16-20,
2010, Proceedings, Vol. 215 of Frontiers in Artificial Intelligence and Applications, pp.
969970. IOS Press.
Peters, J. (2007). Machine Learning of Motor Skills for Robotics. Ph.D. thesis, University
of Southern California.
Peters, J., Mulling, K., & Altun, Y. (2010). Relative entropy policy search. In Fox, M.,
& Poole, D. (Eds.), Proceedings of the Twenty-Fourth AAAI Conference on Artificial
Intelligence (AAAI 2010), pp. 16071612. AAAI Press.
225

fiParisi, Pirotta, & Restelli

Peters, J., & Schaal, S. (2008a). Natural actor-critic. Neurocomputing, 71 (7-9), 1180  1190.
Progress in Modeling, Theory, and Application of Computational Intelligenc 15th
European Symposium on Artificial Neural Networks 2007 15th European Symposium
on Artificial Neural Networks 2007.
Peters, J., & Schaal, S. (2008b). Reinforcement learning of motor skills with policy gradients.
Neural Networks, 21 (4), 682  697. Robotics and Neuroscience.
Pianosi, F., Castelletti, A., & Restelli, M. (2013). Tree-based fitted q-iteration for multiobjective markov decision processes in water resource management. Journal of Hydroinformatics, 15 (2), 258270.
Pirotta, M., Parisi, S., & Restelli, M. (2015). Multi-objective reinforcement learning with
continuous pareto frontier approximation. In Bonet, B., & Koenig, S. (Eds.), Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30,
2015, Austin, Texas, USA., pp. 29282934. AAAI Press.
Pirotta, M., Restelli, M., & Bascetta, L. (2013). Adaptive step-size for policy gradient
methods. In Burges, C. J. C., Bottou, L., Ghahramani, Z., & Weinberger, K. Q. (Eds.),
Advances in Neural Information Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Proceedings of a meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States., pp. 13941402.
Robert, C., & Casella, G. (2004). Monte Carlo Statistical Methods. Springer Texts in
Statistics. Springer-Verlag New York.
Roijers, D. M., Vamplew, P., Whiteson, S., & Dazeley, R. (2013). A survey of multi-objective
sequential decision-making. Journal of Artificial Intelligence Research, 48, 67113.
Roijers, D. M., Whiteson, S., & Oliehoek, F. A. (2015). Computing convex coverage sets
for faster multi-objective coordination. Journal of Artificial Intelligence Research, 52,
399443.
Romero, C. (2001). Extended lexicographic goal programming: a unifying approach. Omega,
29 (1), 6371.
Shelton, C. R. (2001). Importance Sampling for Reinforcement Learning with Multiple
Objectives. Ph.D. thesis, Massachusetts Institute of Technology.
Steuer, R. E., & Choo, E.-U. (1983). An interactive weighted tchebycheff procedure for
multiple objective programming. Mathematical Programming, 26 (3), 326344.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. A Bradford
book. Bradford Book.
Sutton, R. S., McAllester, D. A., Singh, S. P., & Mansour, Y. (2000). Policy gradient
methods for reinforcement learning with function approximation. In Solla, S., Leen,
T., & Muller, K. (Eds.), Advances in Neural Information Processing Systems 12, pp.
10571063. MIT Press.
Tesauro, G., Das, R., Chan, H., Kephart, J., Levine, D., Rawson, F., & Lefurgy, C. (2008).
Managing power consumption and performance of computing systems using reinforcement learning. In Platt, J., Koller, D., Singer, Y., & Roweis, S. (Eds.), Advances in
Neural Information Processing Systems 20, pp. 14971504. Curran Associates, Inc.
226

fiMORL through Continuous Pareto Manifold Approximation

Vamplew, P., Dazeley, R., Berry, A., Issabekov, R., & Dekker, E. (2011). Empirical evaluation methods for multiobjective reinforcement learning algorithms. Machine Learning,
84 (1-2), 5180.
Van Moffaert, K., Drugan, M. M., & Nowe, A. (2013). Scalarized multi-objective reinforcement learning: Novel design techniques. In Adaptive Dynamic Programming And
Reinforcement Learning (ADPRL), 2013 IEEE Symposium on, pp. 191199.
Van Moffaert, K., & Nowe, A. (2014). Multi-objective reinforcement learning using sets of
pareto dominating policies. Journal of Machine Learning Research, 15, 34833512.
Waltz, F. M. (1967). An engineering approach: Hierarchical optimization criteria. Automatic
Control, IEEE Transactions on, 12 (2), 179180.
Wang, W., & Sebag, M. (2013). Hypervolume indicator and dominance reward based multiobjective monte-carlo tree search. Machine Learning, 92 (2-3), 403429.
Williams, R. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8 (3-4), 229256.
Yu, P., & Leitmann, G. (1974). Compromise solutions, domination structures, and salukvadzes solution. Journal of Optimization Theory and Applications, 13 (3), 362378.
Zitzler, E., Thiele, L., & Bader, J. (2010). On set-based multiobjective optimization. Evolutionary Computation, IEEE Transactions on, 14 (1), 5879.
Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., & da Fonseca, V. G. (2003). Performance assessment of multiobjective optimizers: an analysis and review. Evolutionary
Computation, IEEE Transactions on, 7 (2), 117132.

227

fi