Journal of Artificial Intelligence Research 57 (2016) 1-37

Submitted 03/16; published 09/16

Learning Continuous Time Bayesian Networks in
Non-stationary Domains
Simone Villa
Fabio Stella

villa@disco.unimib.it
stella@disco.unimib.it

Department of Informatics, Systems and Communication
University of Milano-Bicocca
Viale Sarca 336, 20126 Milan, Italy

Abstract
Non-stationary continuous time Bayesian networks are introduced. They allow the
parents set of each node to change over continuous time. Three settings are developed for
learning non-stationary continuous time Bayesian networks from data: known transition
times, known number of epochs and unknown number of epochs. A score function for each
setting is derived and the corresponding learning algorithm is developed. A set of numerical
experiments on synthetic data is used to compare the effectiveness of non-stationary continuous time Bayesian networks to that of non-stationary dynamic Bayesian networks. Furthermore, the performance achieved by non-stationary continuous time Bayesian networks
is compared to that achieved by state-of-the-art algorithms on four real-world datasets,
namely drosophila, saccharomyces cerevisiae, songbird and macroeconomics.

1. Introduction
The identification of relationships and statistical dependencies between components in multivariate time-series, and the ability of reasoning about whether and how these dependencies
change over time is crucial in many research domains such as biology, economics, finance,
traffic engineering and neurology, to mention just a few. In biology, for example, knowing
the gene regulatory network allows to understand complex biological mechanisms ruling the
cell. In such a context, Bayesian networks (BNs) (Pearl, 1989; Segal, Peer, Regev, Koller,
& Friedman, 2005; Scutari & Denis, 2014), dynamic Bayesian networks (DBNs) (Dean
& Kanazawa, 1989; Zou & Conzen, 2005; Vinh, Chetty, Coppel, & Wangikar, 2012) and
continuous time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller, 2002; Acerbi,
Zelante, Narang, & Stella, 2014) have been used to reconstruct transcriptional regulatory
networks from gene expression data. The effectiveness of discrete DBNs has been investigated to identify functional correlations among neuroanatomical regions of interest (Burge,
Lane, Link, Qiu, & Clark, 2009), while a useful primer on BNs for functional magnetic resonance imaging data analysis has been made available (Mumford & Ramsey, 2014). However,
the mentioned applications require the time-series to be generated from a stationary distribution, i.e. one which does not change over time. While stationarity is a reasonable
assumption in many situations, there are cases where the data generating process is clearly
non-stationary. Indeed, in the last years, researchers from different disciplines, ranging from
economics to computational biology, to sociology and to medicine have become interested
in representing relationships and dependencies which change over time.
c
2016
AI Access Foundation. All rights reserved.

fiVilla & Stella

Specifically, researchers have been interested in analyzing the temporal evolution of
genetic networks (Lebre, Becq, Devaux, Stumpf, & Lelandais, 2010), the flow over neural
information networks (Smith, Yu, Smulders, Hartemink, & Jarvis, 2006), heart failure (Liu,
Hommersom, van der Heijden, & Lucas, 2016), complications in type 1 diabetes (Marini,
Trifoglio, Barbarini, Sambo, Camillo, Malovini, Manfrini, Cobelli, & Bellazzi, 2015) and
the dependence structure among financial markets during crisis (Durante & Dunson, 2014).
According to the specialized literature on evolution models (Robinson & Hartemink, 2010),
they can be divided into two main categories: structurally non-stationary, i.e. those models
which are allowed to change their structure over time, and parametrically non-stationary,
i.e. those models which only allow the parameters values to change over time.
In this paper, the structurally non-stationary continuous time Bayesian network model
(nsCTBN) is introduced. A nsCTBN consists of a sequence of CTBNs which improves expressiveness over a single CTBN. Indeed, a nsCTBN allows the parents set of each node to
change over time at specific transition times and thus it allows to model non-stationary systems. To learn a nsCTBN, the Bayesian score for learning CTBNs is extended (Nodelman,
Shelton, & Koller, 2003). The nsCTBN version of the Bayesian score is still decomposable
by variable and it depends on the knowledge setting which can be: known transition times,
where transition times are known, known number of epochs, where only the number of transition times is known, and unknown number of epochs, where the number of transition times
is unknown. A learning algorithm for each knowledge setting is designed and developed.
Experiments against non-stationary dynamic Bayesian networks (nsDBNs) (Robinson &
Hartemink, 2010), i.e. the discrete time counterparts of nsCTBNs, have been performed.
The main contributions of this paper are the following:
 definition of the structurally non-stationary continuous time Bayesian network model;
 derivation of the Bayesian score decomposition under each knowledge setting;
 the design of algorithms for learning nsCTBNs under different knowledge settings.
A novel dynamic programming algorithm for learning nsCTBNs under the known
transition times setting is described, while learning nsCTBNs under the others settings
is performed by simulated annealing, exploiting the dynamic programming algorithm;
 performance comparison between nsCTBNs and nsDBNs under all knowledge settings
for a rich set of synthetic data generated by nsCTBNs and nsDBNs;
 performance comparison between nsCTBNs and state-of-the-art algorithms on realworld datasets, namely drosophila, saccharomyces cerevisiae and songbird;
 a nsCTBN learned on a macroeconomics dataset consisting of variables evolving at
different time granularities spanning from 1st January 1986 to 31st March 2015.
The rest of the paper is organized as follows. In Section 2 continuous time Bayesian networks are introduced together with their learning problem from complete data. Section 3
introduces non-stationary continuous time Bayesian networks, presents three learning settings and derives their corresponding Bayesian score functions. Algorithms for learning
nsCTBNs under different learning settings are described in Section 4. Numerical experiments on synthetic and real-world datasets are presented in Section 5. Section 6 closes the
paper by making conclusions and indicating directions for further research activities.
2

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

2. Continuous Time Bayesian Networks
Continuous time Bayesian networks combine Bayesian networks and homogeneous Markov
processes together to efficiently model discrete state continuous time dynamical systems
(Nodelman et al., 2002). They are particularly useful for modeling domains in which variables evolve at different time granularities, such as to model the presence of people at their
computers (Nodelman & Horvitz, 2003), to study reliability of dynamical systems (Boudali
& Dugan, 2006), to model failures in server farms (Herbrich, Graepel, & Murphy, 2007), to
detect network intrusion (Xu & Shelton, 2008), to analyze social networks (Fan & Shelton,
2009), to model cardiogenic heart failure (Gatti, Luciani, & Stella, 2011) and to reconstruct
gene regulatory networks (Acerbi & Stella, 2014; Acerbi, Vigano, Poidinger, Mortellaro,
Zelante, & Stella, 2016). Recently, the complexity of inference in continuous time Bayesian
networks has been studied (Sturlaugson & Sheppard, 2014).
2.1 Basics
The representation ability of continuous time Bayesian networks is inherent to the factorization of the system dynamics into local continuous time Markov processes that depend on
a limited set of states. The continuous time Bayesian network model is defined as follows:
Definition 1. Continuous time Bayesian network (Nodelman et al., 2002). Let X be a
set of random variables X = {X1 , X2 , . . . , XN }. Each X has a finite domain of values
V al(X) = {x1 , x2 , . . . , xI }. A continuous time Bayesian network over X consists of two
0 , specified as a Bayesian network over X,
components: the first is an initial distribution PX
the second is a continuous time transition model specified as: a directed (possibly cyclic)
P a(X)
graph G whose nodes are X1 , X2 , . . . , XN ; a conditional intensity matrix (CIM), QX
,
for each variable X  X, where P a(X) denotes the set of parents of X in the graph G.
P a(X)

The conditional intensity matrix QX

consists of the set of intensity matrices

qxpa1 u
 qxpa2 xu1
=

.
qxpaI xu1


u
Qpa
X

.
.
.
.


qxpa1 xuI
qxpa2 xuI 
,

.
pau
qxI

where
pau ranges over all possible configurations of the parents set P a(X), while qxpai u =
P
pau
pau
pau
xj 6=xi qxi xj . Off-diagonal elements of QX , i.e. qxi xj , are proportional to the probability
that the variable X transitions from state xi to state xj given the parents state pau . The
pau
u
intensity matrix Qpa
X can be equivalently summarized with two independent sets: q X =
pau
{qxi : 1  i  I}, i.e. the set of intensities parameterizing the exponential distributions
pau
pau
pau
u
over when the next transition occurs, and  pa
X = {xi xj = qxi xj /qxi : 1  i, j  I, j 6= i},
i.e. the set of probabilities parameterizing the multinomial distributions over where the
state transitions. Note that the CTBN model assumes that only one single variable can
change state at any specific instant, while its transition dynamics are specified by its parents
via the CIM, and they are independent of all other variables given its Markov Blanket.
3

fiVilla & Stella

2.2 Structural Learning
Given a fully observed dataset D, i.e. a dataset consisting of multiple trajectories1 whose
states and transition times are fully known, the problem of learning the structure of a CTBN
has been addressed as the problem of selecting the graph G  which maximizes the Bayesian
score computed on the dataset D (Nodelman et al., 2003):
BS (G : D) = ln P (G) + ln P (D|G).

(1)

where P (G) is the prior over the graph G and P (D|G) is the marginal likelihood.
The prior P (G) over the graph G, which allows us to prefer some CTBNs structures
over others, is usually assumed to satisfy the structure modularity property (Friedman &
Koller, 2000), i.e. to decompose into the following product of terms:
Y
P (G) =
P (P a(X) = P aG (X)),
(2)
XX

a term for each parents set P aG (X) in the graph G. A uniform prior over G is often used.
The marginal likelihood P (D|G) depends on the prior over parameters P (q G ,  G |G) which
is usually assumed to satisfy the global parameter independence, the local parameter independence and the parameter modularity properties, which are outlined below.
Global parameter independence (Spiegelhalter & Lauritzen, 1990) states that the paramP a (X)
P a (X)
eters q X G
and  X G
associated with each variable X in a graph G are independent,
thus the prior over parameters decomposes by variable as follows:
Y
P a (X) P a (X)
P (q G ,  G |G) =
P (q X G ,  X G |G).
(3)
XX

Local parameter independence (Spiegelhalter & Lauritzen, 1990) asserts that the parameters associated with each configuration pau of the parents P aG (X) of a variable X are
independent. Therefore, the parameters associated with each variable X are decomposable
by parent configuration pau as follows:
YY
P a (X) P a (X)
u
(4)
P (q X G ,  X G |G) =
P (qxpai u ,  pa
xi |G).
pau xi

Parameter modularity (Geiger & Heckerman, 1997) asserts that if a variable X has the same
parents P aG (X) = P aG 0 (X) in two distinct graphs G and G 0 , then the probability density
functions of the parameters associated with X must be identical:
P aG (X)

P (q X

P aG (X)

, X

P aG 0 (X)

|G) = P (q X

P aG 0 (X)

, X

|G 0 ).

(5)

Furthermore, we also assume that the sets of parameters characterizing the exponential distributions are independent of the sets of parameters characterizing the multinomial
distributions:
P (q G ,  G |G) = P (q G |G)P ( G |G).
(6)
1. A trajectory is defined to be a sequence of pairs (t, X(t)), where each transition time t  [0, T ] is
associated with the state X(t) of all the random variables corresponding to the nodes of the CTBN.

4

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

A Dirichlet distribution is selected as the prior for the parameters associated with the multinomial distribution, while a gamma distribution is selected as the prior for the parameters
associated with the exponential distribution, i.e.

P (qxpai u ) Gamma xpai u , xpai u ,
(7)

pau
pau
pau
P ( xi ) Dir xi x1 , . . . , xi xI ,
(8)
where xpai u , xpai u , xpai xu1 , . . . , xpai xuI are the priors hyperparameters. In particular, the  hyperparameters represent the pseudocounts for the number of transitions from state to state,
while the  parameter represents the imaginary amount of time spent in each state before
any data is observed. Note that the hyperparameter xpai u is inversely proportional to the
number of joint states of the parents of X. Conditioning on the dataset D, we obtain the
following posteriors over parameters:

u
,
(9)
P (qxpai u |D) Gamma xpai u + Mxpai u , xpai u + Txpa
i

pau
pau
pau
pau
pau
(10)
P ( xi |D) Dir xi x1 + Mxi x1 , . . . , xi xI + Mxi xI ,
u
and Mxpai xuj are the sufficient statistics of the CTBN (Nodelman et al., 2003).
where Txpa
i
u
In particular, Txpa
is the amount of time spent by the variable X in the state xi while its
i
parents P a(X) are in state pau , while Mxpai xuj is the number of times that the variable X
transitions from the state xi to the state xj while its parents P a(X) are in state pau 2 .
In the Bayesian score (1) the term P (G) does not grow with the size of dataset D.
Thus, the significant term is the marginal likelihood P (D|G). In the case of complete data,
while exploiting the parameters independence (6) and the global parameter independence
property (3), the marginal likelihood can be written as follows:
Y
P a (X)
P a (X)
P (D|G) =
M L(q X G |D) M L( X G |D),
(11)

XX
P aG (X)

where M L(q X

|D) is the marginal likelihood of q derived as follows:
YY
pau xi

P aG (X)

and M L( X

 (xpai u + Mxpai u + 1) (xpai u )
u
 (xpai u + 1) (xpai u + Txpa
i )

u
(pa
xi +1)

pau
u
(pa
xi +Mxi +1)

,

|D) is the marginal likelihood of  derived as follows:

Y Y
Y  xpai xuj + Mxpai xuj
 (xpai u )
,
pau
pau 
pau 

(
+
M
)


x
x
x
i
i
i xj
pa x =x
x 6=x
u

i

j

i

(12)

(13)

j

under the Bayesian-Dirichlet equivalent (BDe) metric version for CTBNs (Nodelman, 2007).
In this case, the BDe metric uses the priors (7) and (8), while the parameter modularity (5),
as well as the global (3) and the local (4) parameter independence properties are assumed
to be satisfied.
2. Please note that the number of times the P
variable X leaves the state xi while its parents P a(X) are in
pau
u
state pau is computed as follows Mxpa
=
xj 6=xi Mxi xj .
i

5

fiVilla & Stella

In conclusion, the Bayesian score (1) can be computed in closed form by assuming the
structure modularity property (2) is satisfied, and using the BDe metric as follows:
X
P a (X)
P a (X)
ln P (P a(X) = P aG (X)) + ln M L(q X G |D) + ln M L( X G |D). (14)
BS(G : D) =
XX

Since the graph G of a CTBN does not have acyclicity constraints, it is possible to maximize
the Bayesian score (14) by separately optimizing the parents set P a(X) for each variable
X. It is worthwhile to mention that if the maximum number of parents is set, then the
search of the optimal value of the Bayesian score (14) can be performed in polynomial time.
The search can be performed by enumerating each possible parents set or by using a greedy
hill-climbing procedure with operators to add, delete or reverse edges of the graph G.

3. Non-stationary Continuous Time Bayesian Networks
Continuous time Bayesian networks are both structurally stationary, as the graph does not
change over time, and parametrically stationary, as the conditional intensity matrices do
not change over time. These stationarity assumptions are reasonable in many situations,
but there are cases where the data generating process is intrinsically non-stationary and
thus CTBNs can no longer be used. Therefore, in this section, we extend CTBNs to become
structurally non-stationary. i.e. we allow the CTBNs structure to change over continuous
time.
3.1 Definition
In the non-stationary continuous time Bayesian network model, the graph of the CTBN
is replaced by a graphs sequence G = (G1 , G2 , . . . , GE ), where a graph Ge represents the
causal dependency structure of the model for the epoch e  {1, 2, . . . , E}3 . This model is
structurally non-stationary because of the introduction of the graphs sequence and it can
handle transition times that are common to the whole network and/or node-specific.
Following the notations and definitions used for non-stationary dynamic Bayesian networks, we let T = (t1 , . . . , tE1 ) be the transition times sequence, i.e. the times at which
the causal dependency structure Ge , active at epoch e, is replaced by the causal dependency
structure Ge+1 , which becomes active at epoch e + 1. An epoch is defined to be the period
of time between two consecutive transitions, i.e. the epoch e is active during the period of
time starting at te1 and ending at te . The graph Ge+1 , which is active during the epoch
e + 1, differs from the graph Ge , which is active during the epoch e, in a set of edges that
we call the set of edge changes Ge .
Figure 1 shows a graphs sequence G = (G1 , G2 , G3 , G4 ) consisting of four epochs (E = 4)
with transition times T = (t1 , t2 , t3 ). Each epoch is associated with a set of edge changes.
Specifically, the graph G2 differs from the graph G1 by the following set of edge changes
G1 = {X3  X2 , X2 6 X3 , X1 6 X2 }, the graph G3 differs from the graph G2 by the
following set of edge changes G2 = {X2  X1 } and the graph G4 differs from the graph
G3 by the following set of edge changes G3 = {X3  X4 , X4  X1 , X1 6 X4 , X4 6 X3 }.
3. It is worthwhile to mention that the first epoch, i.e. the epoch starting at time 0 and ending at time t1
is associated with the graph G1 , while the last epoch, i.e. the epoch starting at time tE1 and ending at
time T (the supremum of the considered time interval, i.e. [0,T]) is associated with the graph GE .

6

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

X1

X2

X1

X2

X1

X2

X1

X2

X4

X3

X4

X3

X4

X3

X4

X3

0

3

2

1

t2

t1

4

t3

T

Figure 1: Graphs sequence G = (G1 , G2 , G3 , G4 ) of a nsCTBN with four epochs, E = 4, and
three transition times, T = (t1 , t2 , t3 ), where the edges are gained and lost over time.
Non-stationary continuous time Bayesian networks allow each node to have its own
sequence of parents sets, each parents set being active at a given epoch. Therefore, we
introduce the concept of homogeneous interval H(X) = (h1 , . . . , hM ) associated with node
X, which is defined as the union of consecutive epochs during which the same parents set
P a(X) is active for the node X. Note that if each epoch is associated with a different
parents set, then M is equal to E.
A non-stationary continuous time Bayesian network is defined as follows.
Definition 2. (Structurally) non-stationary continuous time Bayesian network. Let X be
a set of random variables X1 , . . . , XN . Each X has a finite domain of values V al(X) =
{x1 , . . . , xI }. A (structurally) non-stationary continuous time Bayesian network Nns =
(B, Mns ) over X consists of two components:
0 , specified as a Bayesian network B over X,
 an initial distribution PX

 a non-stationary continuous time transition model Mns specified as:
 a sequence of directed (possibly cyclic) graphs G = (Ge )E
e=1 whose nodes are
X1 , . . . , XN , where E represents the number of epochs;
P a (X)

G
 a conditional intensity matrix, QX,H(X)
, X  X, where P aG (X) denotes the
parents sets of X in G, and H(X) denotes the intervals associated with X.

P a (X)

G
The conditional intensity matrix QX,H(X)
consists of a set of intensity matrices
u
qxpa1 ,h
m
pa
 q u
x2 x1 ,hm
=

.
pau
qxI x1 ,hm



u
Qpa
X,hm

.
.
.
.


qxpa1 xuI ,hm
qxpa2 xuI ,hm 
,

.
pau
qxI ,hm

one for each configuration pau of each parents set P a(X)  P aG (X) which is active during
the interval hm  H(X).4
u
4. Note that the following equation qxpai ,h
=
m

P

xj 6=xi

7

qxpai xuj ,hm still holds.

fiVilla & Stella

3.2 Learning Framework
Learning a nsCTBN from a fully observed dataset D can be done using the Bayesian learning
framework taking into account the entire graphs sequence G. In the nsCTBNs case, we must
specify the prior probability over the graphs sequence G and, for each possible sequence, the
density measure over possible values of the parameters q G and  G . Once they prior P (G)
and the likelihood P (q G ,  G |G) are given, the marginal likelihood P (D|G) can be computed
and the Bayesian score can be evaluated. It is important to note that we are focused on
recovering the graphs sequence G and not on detecting possible changes of the parameters.
In fact, we identify non-stationarity in the parameters of the model, i.e. the entries of the
conditional intensity matrices, that are significant enough to result in structural changes of
the graph. Others changes are assumed to be small enough not to alter the graph structure.
3.2.1 Prior Probability over Graphs
Given the transition times T , and thus the number of epochs E, we assume that the prior
over the nsCTBNs structure G can be written as follows:
P (G|T ) = P (G1 , ..., GE |T ) = P (G1 , G1 , ..., GE1 |T ) = P (G1 )P (G1 , ..., GE1 |T ).
(15)
Equation (15) is justified because we assume that the probability distribution over edge
changes only is a function of the number of changes performed, which can also be defined
independently of the initial graph G1 . If some knowledge about particular edges or the
overall topology is available for the initial network, then we can use an informative prior
P (G1 ) otherwise we can resort to a uniform distribution. As in CTBNs, P (G1 ) must satisfy the structure modularity assumption (2), while the prior over the set of edge changes
P (G1 , . . . , GE1 |T ) defines the way in which edges change through adjacent epochs.
3.2.2 Prior Probability over Parameters
The prior over parameters P (q G ,  G |G, T ) is selected to satisfy the following assumptions:
independence between the sets of parameters characterizing the exponential and the multinomial distributions (6), parameter modularity (5) and parameter independence. The latter
assumption is divided into three components for nsCTBNs: global parameter independence,
interval parameter independence and local parameter independence.
Global parameter independence asserts that the parameters associated with each node
in a nsCTBNs graphs sequence are independent, so the prior over parameters decomposes
by variable X as follows:
Y
P aG (X)
P aG (X)
P (q G ,  G |G, T ) =
P (q X,H(X)
,  X,H(X)
|G, T ).
(16)
XX

Interval parameter independence states that the parameters associated with each interval
of the active parents for each node are independent, so the parameters associated with each
X and its parents sets P aG (X) are decomposable by interval hm  H(X) as follows:
P a (X)

P a (X)

G
G
P (q X,H(X)
,  X,H(X)
|G, T ) =

Y
hm

8

P a (X)

P a (X)

P (q X,hGm ,  X,hGm |G, T ).

(17)

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Local parameter independence states that the parameters associated with each state of
a variable in a given interval are independent, thus the parameters associated with each X
in the interval hm  H(X) are decomposable by parent configuration pau as follows:
YY
P a (X) P a (X)
u
u
(18)
,  pa
P (qxpai ,h
P (q X,hGm ,  X,hGm |G, T ) =
xi ,hm |G, T ).
m
pau xi

As in the CTBNs case, a Dirichlet distribution is used as prior for the parameters of the
multinomial distribution and a gamma distribution is used as prior for the parameters of
u
is the
the exponential distribution. The sufficient statistics are modified as follows: Txpa
i ,hm
amount of time spent in state X = xi while P a(X) = pau in the interval H(X) = hm , while
Mxpai xuj ,hm is the number of transitions from state X = xi to state X = xj while P a(X) = pau
P
in the interval H(X) = hm . We let Mxpai ,hu m = xj 6=xi Mxpai xuj ,hm to be the number of times
X leaves state xi while its parents P a(X) are in state pau during the interval H(X) = hm .
3.2.3 Marginal Likelihood
Given the graphs sequence G, and the transition times T , the marginal likelihood P (D|G, T )
of the dataset D can be computed in closed form using the priors and the sufficient statistics
previously defined. To derive the Bayesian-Dirichlet equivalent metric for nsCTBNs, we
make the same assumptions as those for CTBNs. In this case, the parameter independence
assumption is divided into global (16), interval (17) and local (18) parameter independence.
Therefore, the marginal likelihood becomes:
Y
P aG (X)
P aG (X)
P (D|G, T ) =
M L(q X,H(X)
|D) M L( X,H(X)
|D).
(19)
XX

The marginal likelihood of q in equation (19) can be calculated as follows:
(pau +1)


xi ,hm
pau
pau
u
+
1

+
M
 xpai ,h
Y
Y
Y
xi ,hm
xi ,hm
m
P aG (X)
M L(q X,H(X) |D) =
(pau +M pau +1) ,


xi ,hm
xi ,hm
pau
pau
pa
hm pau xi   u + 1
xi ,hm + Txi ,hm
xi ,hm

(20)

while the marginal likelihood of  in equation (19) can be calculated as follows:




pau
pau
u
 xpai ,h


+
M
Y
Y
Y
Y
xi xj ,hm
xi xj ,hm
m
P aG (X)




M L( X,H(X)
|D) =
.
pau
pau
pau
 xi xj ,hm
hm pau xi =xj  xi ,hm + Mxi ,hm
xi 6=xj
(21)
It is important to note that for nsCTBNs, the pseudocounts  as well as the imaginary
amount of time  are associated with each interval. This aspect requires a careful choice in
order not to be too biased towards these values when small intervals are analyzed.
A possible correction is to weight the CTBNs hyperparameters by a quantity proportional to the time interval width (hm  hm1 ), where hM denotes the total time. Thus, the
nsCTBNs hyperparameters could be defined as follows:
xpai xuj ,hm
xpai ,hu m

(hm  hm1 )
,
hM
(hm  hm1 )
= xpai u
.
hM
= xpai xuj

9

(22)
(23)

fiVilla & Stella

If you want to control the parameter priors using only two hyperparameters  and  ,
then you can use the uniform BDe for nsCTBNs (BDeu). In this case, the hyperparameters
defined in (22) and (23) are divided by the number U of possible configurations of the
parents P a(X) of node X times the cardinality I of the domain of X, as follows:
xpai xuj ,hm

=

xpai ,hu m

=

 (hm  hm1 )
,
UI
hM
 (hm  hm1 )
.
UI
hM

(24)
(25)

Equations (22) and (23) rescale the hyperparameters in such a way not to be biased with
respect to the epochs length, while equations (24) and (25) are based on the uniform
distribution and they have been used for performing all numerical experiments.
3.3 Bayesian Score Decomposition
The Bayesian score can be decomposed by variable based on the information available
about the transition times. In this regard, three knowledge settings are used to derive the
Bayesian score, namely: known transition times (KTT), known number of epochs (KNE)
and unknown number of epochs (UNE).
3.3.1 Known Transition Times
In this setting, the transition times T are known. Thus, the prior probability over the
graphs sequence P (G|T ) decomposes as in equation (15), while the marginal likelihood
decomposes by variable X according to equation (19).
Therefore, the Bayesian score BS(G : D, T ) can be written as follows:
BS(G : D, T ) = ln P (G1 ) + ln P (G1 , . . . , GE1 |T )
P a (X)

P a (X)

G
G
+ ln M L(q X,H(X)
|D) + ln M L( X,H(X)
|D).

(26)

In such a setting the structural learning problem of a non-stationary continuous time
Bayesian network consists of finding the graph G1 active during the first epoch (e = 1) and
the E  1 sets of edge changes G1 , . . . , GE1 together with the corresponding parameters
values, which maximize the Bayesian score defined in equation (26).
The graphs G2 , . . . , GE are selected by making assumptions on the ways by which the
edges change over continuous time. A common approach (Robinson & Hartemink, 2010)
consists of assuming that the graphs sequence G = (G1 , . . . , GE ) depends on a parameter
which controls the number of edge changes over continuous time. This approach uses a
truncated geometric distribution, with parameter p = 1  exp(c ), to model the number
of parents changes occurring at transition time te+1 :
X
ce =
|Ge (X)|.
(27)
XX

The variable ce counts the number of edge changes between two consecutive graphs Ge and
Ge+1 , while the parameter c controls the impact of the number of edge changes ce on the
score function (26).
10

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

If the edge changes Ge are assumed to be mutually independent, then the probability
for the edge changes through subsequent epochs can be written as follows:
P (G1 , . . . , GE1 |T ) =

E1
Y
e=1

E1
Y
(1  exp(c ))(exp(c ))ce

(exp(c ))ce ,
1  (exp(c ))cmax +1

(28)

e=1

where cmax is the truncation term. Therefore, if we assume a truncated geometric distribution on the number of parents changes occurring at each transition times and equation
(28) holds, then the Bayesian score (26) decomposes by variable X as follows:
BS(G : D, T ) =

X

ln P (P a(X) = P aG1 (X))  c

XX
P a (X)

G
+ ln M L(q X,H(X)
|D) +

E1
X

ce
e=1
P aG (X)
ln M L( X,H(X)
|D).

(29)

It is worthwhile to notice that the number of parents changes ce for each epoch e
penalizes the Bayesian score, and thus it discourages sudden variations in the parents set
between consecutive epochs, while the parameter c controls the impact of such changes on
the score function (26).
3.3.2 Known Number of Epochs
If the transition times T are unknown, then the Bayesian score can be written as follows:
BS(G, T : D) = ln P (G, T ) + ln P (D|G, T ).

(30)

Assuming that P (G, T ) = P (G)P (T ) the Bayesian score (30) becomes:
BS(G, T : D) = ln P (G) + ln P (T ) + ln P (D|G, T ).

(31)

If the number of epochs E is known, then the prior probability P (G) over the graphs
sequence G decomposes as in equation (15), while a truncated geometric distribution can
be used on the number of parents changes occurring at each transition time, as in the
known transition times setting.
Any choice for P (T ) can be made to include prior knowledge about the set of transition
times. However, if no information is available, a uniform prior on P (T ) is used, implying
that all possible values of transition times are equally likely for a given number of epochs
E. Thus, the Bayesian score (31) can be decomposed by variable X as follows:
BS(G, T : D) = ln P (T ) +
+

X

ln P (P a(X) = P aG1 (X))  c

XX
P aG (X)
ln M L(q X,H(X) |D)

E1
X

ce

e=1
P a (X)

G
+ ln M L( X,H(X)
|D),

(32)

where ce counts the number of edge changes between two consecutive parents sets, while c
controls the impacts on BS(G, T : D) of such edge changes, as it happens under the KTT
setting.
11

fiVilla & Stella

3.3.3 Unknown Number of Epochs
If the number of epochs E is unknown, then transition times T are unknown as well.
Under this setting, we learn a nsCTBN by exploiting what introduced under the KTT
and KNE settings. We assume that the structure of the non-stationary continuous time
Bayesian network can evolve at different speeds over continuous time. Such an assumption
is incorporated by using a truncated geometric distribution with parameter p = 1exp(e )
on the number of epochs. In general, large values of e encode the strong prior belief that
the structure of the nsCTBN changes slowly (i.e. few epochs exist).
Following what we presented under the KTT setting, the Bayesian score can be obtained
by subtracting the parameter e times the number of epochs E. Therefore, the Bayesian
score BS(G, T : D) decomposes by variable X as follows:
BS(G, T : D) = ln P (T )  e E +

X

ln P (P a(X) = P aG1 (X))  c

ce

e=1

XX
P a (X)

E1
X

P a (X)

G
G
+ ln M L(q X,H(X)
|D) + ln M L( X,H(X)
|D).

(33)

Note that the Bayesian score (33) contains two parameters, namely c and e , which
encode our prior belief about the structure of the nsCTBN. Specifically, the parameter c
regulates our prior belief about the smoothness of the edge changes (e.g. encouraging or
discouraging the edge changes per epoch), while the parameter e regulates our prior belief
about the number of epochs (e.g. encouraging or discouraging the creation of epochs).

4. Structural Learning
The optimal structure of nsCTBNs can be found by separately maximizing the components
of the Bayesian score associated with each node. This can be achieved by using an exact optimization algorithm based on dynamic programming when the transition times are
given. By contrast, when only the number of epochs is known or no information about the
transition times is available, we have to resort to approximate techniques based on Monte
Carlo or on simulated annealing. We present the exact algorithm for solving the structural
learning problem under the KTT setting. Then, we briefly outline the stochastic algorithms
to solve the structural learning problem under the KNE setting and under the UNE setting.
4.1 Known Transition Times
Under this setting the Bayesian score decomposes according to equation (29). Thus, the
optimal graphs sequence G  can be found by separately searching the optimal parents sequence G X for each node X. To solve the problem of finding the optimal parents sequence
G X for node X we consider a sequence consisting of M intervals H(X) = (h1 , . . . , hM ) and
S possible parents, so to have Z = 2S possible parents sets. To find the optimal parents
sequence G X we must compute M  Z marginal likelihood terms associated with q and ,
one marginal likelihood term for each possible parents set P az (X) and each interval hm .
Then, an optimization algorithm can be used to find the maximum of the component of the
Bayesian score associated with the node X.
12

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

An exhaustive search would be prohibitive, as it would require evaluating Z M scores,
one for each possible parents sequence G X . Unfortunately, also a greedy search strategy
that computes the parents set which maximizes the Bayesian score for each interval is not
viable. In fact, the function that counts the parents changes ce in (29) binds the choice of
the subsequent parents set, i.e. it binds Ge to Ge+1 .
However, the relation between the score of the variable X associated with the parents set
P a(X)
P a(X) in the interval hm , denoted as BSX,hm , and the score associated with the parents set
P a(X)

P a(X) in the interval hm1 , denoted as BSX,hm1 , can be defined by recursion as follows:
n
o
P a(X)
P a (X)
P a(X) P a(X)
BSX,hm = max BSX,hzm1  c cX,e + ln M L(q X,hm ,  X,hm |D) ,
(34)
P az

where cX,e = |Ge (X)|, while the marginal likelihoods of q and  are grouped together.
P a(X)
The score BSX,hm , associated with the parents set P a(X) for node X in the interval hm , is
introduced to clarify the recursion used in the Algorithm 1. Note that this score depends on
all the components of the score up to hm . In particular, not only the marginal likelihoods
component is involved, but also the term cX,e , which counts the parents changes, is included
as it binds the choice of subsequent parents sets. Equation (34) is exploited by dynamic
programming to select the optimal parents sequence G X for each node X.
Algorithm 1 takes as input the marginal likelihoods of q and  for each interval and
parents set, the prior probability about the initial parents set, the number of parents
changes, and the parameter c . Algorithm 1 ensures the optimal parents sequence G X for
the node X and its corresponding optimal Bayesian score. Its core is the computation of
the M  Z score matrix, denoted by SC, through the dynamic programming recursion. The
dynamic programming recursion for the interval h1 (m = 1) is defined as follows:
P a (X)

SC1z = ln M L(q X,hz1

P a (X)

,  X,hz1

|D) + ln P (P az (X) = P aGh1 (X)),

(35)

for 1  z  Z, while, for the intervals hm (m = 2, . . . , M ), the recursion is:
n
o
P a (X) P a (X)
z
u
SCm
= max SCm1
+ ln M L(q X,hzm ,  X,hzm |D)  c cX,e .
1uZ

After filling the score matrix SC, the value maxz {SC[M, z]} is the optimal Bayesian score,
while the optimal parents sequence is reconstructed backwards from M to 1 by using the
index matrix IN . The cost of computing the dynamic programming recursion is O(M Z 2 ),
which is polynomial for a fixed maximum number of parents S.
The problem of selecting the optimal parents sequence has an interesting graph representation. Indeed, it is possible to create a graph whose nodes are associated with marginal
likelihoods of q and  for the interval hm and for the parents set P az (X), while each node associated with the interval hm is linked with all the nodes associated with the interval hm+1 .
Each arc is associated with a weight computed as the difference between the marginal likelihoods in the interval hm for the parents set P az (X) and the cost of switching from the
parents set of the interval hm1 to the parents set of the interval hm . Two special nodes
are added to represent the start and the end of the optimal parents sequence. Such a graph
does not have cycles, thus the selection of the optimal parents sequence for each node can
be reduced to the longest path problem from the start node to the end node of a directed
acyclic graph, and thus it can be solved using either dynamic or linear programming.
13

fiVilla & Stella

Algorithm 1 LearnKTTX
Require: matrix containing the marginal likelihoods of q and  M LX[M, Z], vector containing the prior probability about the initial parents set P R[Z], matrix containing the
number of parents changes C[Z, Z] and the parameter for the parents changes c .
Ensure: score matrix SC[M, Z] and index matrix IN [M, Z].
1: Initialize SC[m, z]  , IN [m, z]  0.
2: for m  1, . . . , M do
3:
for z  1, . . . , Z do
4:
if (m = 1) then
5:
SC[m, z]  ln M LX[m, z] + ln P R[z]
6:
else
7:
for w  1, . . . , Z do
8:
score  SC[m  1, w] + ln M LX[m, z]  c C[w, z]
9:
if (score > SC[m, z]) then
10:
SC[m, z]  score
11:
IN [m, z]  w
12:
end if
13:
end for
14:
end if
15:
end for
16: end for
Learning a nsCTBN can be done following the following four steps procedure: i) use
u
the dataset D to compute for each variable X the sufficient statistics Txpa
and Mxpai xuj ,hm
i ,hm
according to the given transition times T ; ii) compute the marginal likelihoods (20) and (21),
and then fill the M LX matrix; iii) run Algorithm 1 for each node X to get the corresponding
optimal parents sequence; iv) collect the optimal parents sequence for each node X and
compute the corresponding CIMs using the sufficient statistics already computed in step i).
If we allow the intervals to differ from the transition times, i.e. they can be obtained
as one of all the possible unions of transition times; then we have to repeat the learning
procedure for all the E  (E  1)/2 cases. It is possible to speed up the computation
because the sufficient statistics can be aggregated through intervals. In such a way, we read
the dataset once, while the precomputed marginal likelihoods can be stored and reused for
the same intervals. Moreover, the computations can be performed in parallel for each node.
4.2 Known Number of Epochs
In this setting, we know the number of epochs, but the transition times are not given, so we
cannot directly apply Algorithm 1. However, once a tentative allocation T of the transition
times is given, we can apply Algorithm 1 to obtain the optimal nsCTBNs structure, under
the assumption that T is not too different from the true transition times T . To find an
optimal tentative allocation T  , i.e. an allocation that is as close as possible to T , we apply
the simulated annealing (SA) algorithm (Kirkpatrick, Gelatt, & Vecchi, 1983).
14

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Simulated annealing is an iterative algorithm that attempts to find the global optimum
x of a given function f (x) through a stochastic search over the feasible region. At iteration
k, when the SA algorithm is assumed to be in state xk , it samples a proposal state x0
according to some proposal distribution x0  P 0 (|xk ). Then, the SA algorithm computes the
quantity  = exp ((f (x)  f (x0 ))/CT ), where CT is the computational temperature. The
SA algorithm accepts the proposal state x0 with probability equal to min{1, }. Concisely,
SA always accepts any proposal state x0 where f (x0 ) > f (x) by setting xk+1 = x0 , while it
accepts the proposal state x0 when f (x0 ) < f (x) with probability  by setting xk+1 = x0
with probability  and xk+1 = xk with probability (1  ), i.e. in this case the state of
the SA algorithm does not change. The computational temperature reduces over iterations
according to a cooling schedule. It has been shown that if one cools sufficiently slowly, then
the algorithm will probably find the global optimum (Kirkpatrick et al., 1983). The design
of the cooling schedule is an important part of the SA algorithm (Bertsimas & Tsitsiklis,
1993). A possible approach is to use an exponential cooling schedule defined as follows:
CTk = CT0   k , where CT0 represents the initial temperature, typically set to 1.0,  is the
cooling rate, usually set to be close to 0.8, while k is the current iteration (Murphy, 2012).
In the nsCTBNs case, the state of the SA algorithm x is associated with the tentative
allocation T , while the function f (x) is the Bayesian score (32). Algorithm 2 takes as input
the sufficient statistics, the parameters used to run Algorithm 1 and the parameters of the
SA algorithm. It solves the structural learning problem under the KNE setting for a given
variable X by ensuring the optimal tentative allocation T  and its corresponding score.
Algorithm 2 LearnKNEX
Require: sufficient statistics SuffStatsX, prior probability P R[], number of parents
changes C[, ], parameter c , tentative allocation T , initial temperature CT0 , cooling
rate , number of iterations Iters, truncation parameter z and standard deviation .
Ensure: optimal tentative allocation T  and best Bayesian score bestSC.
1: Initialize k  0, T   T .
2: M LX  GetMLX(SuffStatsX , T )
3: bestSC  LearnKTTX(M LX, P R[], C[, ], c )
4: while (k < Iters) do
5:
T  TentativeAllocation(T  , z, )
6:
M LX  GetMLX(SuffStatsX , T )
7:
tentSC  LearnKTTX(M LX, P R[], C[, ], c )
8:
CT  CT0   kn

o
9:
accP rob  min 1, exp  (bestSCtentSC)
CT
10:
ur  UniRand()
11:
if (ur  accP rob) then
12:
T   T
13:
currSC  tentSC
14:
end if
15:
k k+1
16: end while
17: bestSC  currSC
15

fiVilla & Stella

The simulated annealing parameters we used include the tentative allocation T , the
initial temperature CT0 , the cooling rate  and the number of iterations Iters for the
exponential cooling schedule. Moreover, the truncation parameter z and standard deviation
 are used for the selection of the new tentative allocation T 0 according to the random
procedure shown in Algorithm 3. This procedure selects a transition time through a discrete
uniform distribution, UniRandDiscr(T ), and perturbs it according to a truncated normal
distribution, StdNormRand(), having a standard deviation equal to , with the addition of
point masses at z and z, where z represents the truncation parameter.
Algorithm 3 TentativeAllocation
Require: tentative allocation T , truncation parameter z and standard deviation .
Ensure: new tentative allocation T 0 .
1: t  UniRandDiscr(T )
2: T 0  T \ t
3: nr  StdNormRand()
4: if (nr < z) then
5:
nr  z
6: end if
7: if (nr > z) then
8:
nr  z
9: end if
10: t  t + nr  
11: T 0  T  t

4.3 Unknown Number of Epochs
In this setting the number of epochs is unknown; thus the structural learning algorithm
must be able to move across a different number of epochs, as well as the corresponding
transition times. Also in this case, we used a simulated annealing algorithm where the state
x is the tentative allocation T and the function to be optimized f (x) is the Bayesian score
shown in equation (33). The cooling schedule has been set the same as the one used under
the KNE setting. The proposal distribution differs from the one used under the KNE setting
as it uses two additional operators, namely the split and the merge operators. The split
operator allows to split a given interval [tm ; tm+1 ) into two subintervals [tm ; t) and [t; tm+1 )
where tm , tm+1  T . The merge operator allows to merge contiguous intervals [tm1 ; tm )
and [tm ; tm+1 ) to form the wider interval [tm1 ; tm+1 ) where tm1 , tm , tm+1  T .
The new state is obtained by sampling the number of epochs changes ec from a multinoulli distribution with parameters (p1 , p2 , p3 ), where p1 represents the probability that the
number of epochs of the next iteration |T | is decreased by one; p3 represents the probability
that the number of epochs of the next iteration |T | is increased by one, and p2 represents
the probability that number of epochs of the next iteration |T | does not change with respect
to the current one. If ec is equal to 2, then Algorithm 2 is invoked, if ec is equal to 1, then
the merge operator is applied before invoking Algorithm 2, while if ec is equal to 3, then
the split operator is applied before invoking Algorithm 2.
16

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Algorithm 4 solves the structural learning problem of nsCTBN under the UNE setting
for a given node X by ensuring the optimal tentative allocation T  and its corresponding
Bayesian score. This algorithm is similar to the one used under the KNE settings, but it
uses Algorithm 5 to apply the split and merge operators. The left(t) function in Algorithm
5 returns the transition time in T which comes immediately before transition time t.
Algorithm 4 LearnUNEX
Require: sufficient statistics SuffStatsX, prior probability P R[], number of parents
changes C[, ], parameter c , parameter e , tentative allocation T , initial temperature
CT0 , cooling rate , number of iterations Iters, truncation parameter z, standard deviation , split probability sp and merge probability mp.
Ensure: optimal tentative allocation T  and best Bayesian score bestSC.
1: Initialize k  0, T   T .
2: bestSC LearnKTTX(GetMLX(SuffStatsX, T ), P R[], C[, ], c ) e |T |
3: while (k < Iters) do
4:
T  SplitMerge(T  , sp, mp)
5:
T  TentativeAllocation(T , z, )
6:
tentSC  LearnKTTX(GetMLX(SuffStatsX, T ), P R[], C[, ], c ) e |T |
7:
CT  CT0   kn

o
8:
accP rob  min 1, exp  (bestSCtentSC)
CT
9:
ur  UniRand()
10:
if (ur  accP rob) then
11:
T   T
12:
currSC  tentSC
13:
end if
14:
k k+1
15: end while
16: bestSC  currSC
Algorithm 5 SplitMerge
Require: tentative allocation T , split probability sp and merge probability mp.
Ensure: new tentative allocation T 0 .
1: T 0  T
2: p  UniRand()
3: if (p < mp) then
4:
t  UniRandDiscr(T )
5:
T 0  T \ t
6: else
7:
if (p < (mp + sp)) then
8:
t  UniRandDiscr(T  T )
9:
nt  left(t) + tleft(t)
2
10:
T 0  T  nt
11:
end if
12: end if
17

fiVilla & Stella

5. Numerical Experiments
Numerical experiments are performed on both synthetic and real-world datasets. Synthetic
datasets are used to compare nsCTBNs to nsDBNs under the KTT, KNE and UNE knowledge settings in terms of accuracy, precision, recall and F1 measure. The following real-world
datasets: drosophila, saccharomyces cerevisiae and songbird, are used to compare nsCTBNs
to state-of-the-art algorithms, i.e. TSNI (a method based on ordinary differential equations),
nsDBN (Robinson & Hartemink, 2010) and non-homogeneous dynamic Bayesian networks
with Bayesian regularization (TVDBN) (Dondelinger, Lebre, & Husmeier, 2013), under the
UNE knowledge setting. Drosophila, saccharomyces cerevisiae and songbird datasets are
collected at fixed time intervals, thus we analyzed an additional real-world dataset, consisting of financial/economic variables evolving at different time granularities, to exploit the
expressiveness of nsCTBNs when events occur asynchronously. Note that while the performance comparison using synthetic datasets benefits from the knowledge of the ground
truth, the same does not apply to the performance comparison using real-world datasets
because the ground truth is not available. In such cases, the comparison exploits partial
and meta-knowledge available in the specialized literature.
5.1 Synthetic Datasets
Artificially generated datasets include data sampled from a rich set of nsDBN models,
i.e. nsDBN generated datasets, and a rich set of nsCTBN models, i.e. nsCTBN generated
datasets. Such nsDBN and nsCTBN models consist of five nodes associated with binary
and ternary variables. Numerical experiments concern learning the parents sets, transition
times and the number of epochs for a single node. This choice is motivated by the fact that
structural learning for nsCTBN can be performed for each single node independently from
the remaining ones. However, when transition times are unknown, having multiple parents
sets changes could make it easier to correctly identify the times of change.
5.1.1 nsDBN Generated Datasets
nsDBN generated datasets were sampled from nsDBN models5 associated with the following
number of epochs E  {2, 3, 4, 5}. In particular, for each number of epochs E, 10 different
nsDBN instances were sampled to obtain a number of datasets equal to 10, each one consisting of a single trajectory. Thus, 40 synthetic datasets were used to learn the structure
of nsDBN and nsCTBN (number of models =2) under the KTT, KNE and UNE settings.
Structural learning experiments were performed with c = {1, 2, 4} and e = {5, 10, 15}
for nsCTBN and s = {1, 2, 4} and with m = {10, 50, 100} for nsDBN6 . An overall number
of 1,200 experiments have been performed. In particular, we performed number of epochs 
number of datasets  number of c or s  number of models = 41032 = 240 experiments
under the KTT setting, 240 under the KNE setting, while number of epochs  number of
datasets  number of c or s  number of e or m  number of models = 410332 = 720
experiments have been performed under the UNE setting.
5. Inter-slice arcs are allowed, while intra-slice arcs are not allowed. This holds true for all nsDBN models
sampled to obtain the nsDBN generated datasets.
6. It is worthwhile to mention that the s and m parameters are the nsDBN counterparts of the c and
e parameters for the nsCTBN.

18

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

The nsdbn jar executable7 (Robinson & Hartemink, 2010) was used for structural learning of nsDBN, where we set the maximum number of proposed networks to 500,000 and
the burn-in period to 50,000 for nsDBN. nsCTBN were learned by using the following parameters setting: Iters = 1,000, CT0 = 1,000,  = 0.8, z = 3,  = 1, sp = 0.3, mp = 0.3,
 = 1 and  = 0.1 using the BDeu metric. Furthermore, for nsDBN and nsCTBN we set
the maximum number of parents to 4. Only arcs that occurred in more than 90 percent
of the samples8 belong to the inferred nsDBN and nsCTBN models. Accuracy (Acc), precision (P rc), recall (Rec) and F1 measure (F1 ) achieved by nsDBN and nsCTBN learned
under the KTT, KNE and UNE settings are reported in Table 1, 2 and 3 respectively. It
is worthwhile to mention that under the KNE and UNE settings, nsDBNs and nsCTBNs
almost always identified the correct number of epochs and the location of their associated
transition times. Accuracy, precision, recall and F1 measure have been computed in two
different ways. Firstly, we included all arcs of the true network for each epoch. Secondly, we
excluded the self-reference arcs, i.e. those arcs connecting the same node in two consecutive
time-slices of the true network for each epoch. In fact, while each node of a nsCTBN has the
self-reference arc by default, the same does not happen for nsDBNs. This means that in the
first case a nsDBN is required to learn arcs that a nsCTBN is not required to do. Therefore,
to ensure a fair comparison of nsCTBN to nsDBN we adopted the second case. Tables 1,
2 and 3 report the performance measure values computed by excluding self-reference arcs
from the set of arcs of the true networks for each epoch.
Table 1: nsCTBN compared to nsDBN under the KTT setting for nsDBN generated data.
Average, min (subscript) and max (superscript) performance values over 10 networks and
c for nsCTBN and s for nsDBN.
Number of epochs E
3
4

2
Acc
P rec
Rec
F1

5

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

0.961.00
0.93
0.901.00
0.67
0.771.00
0.40
0.801.00
0.57

0.921.00
0.75
1.001.00
1.00
0.851.00
0.50
0.911.00
0.67

0.950.98
0.93
0.791.00
0.50
0.670.83
0.43
0.710.91
0.47

1.00
0.920.75
1.00
1.001.00
1.00
0.860.63
1.00
0.920.77

0.950.99
0.89
0.871.00
0.40
0.650.90
0.25
0.730.95
0.31

1.00
0.820.63
1.00
0.960.75
1.00
0.700.38
1.00
0.800.50

0.940.97
0.89
0.851.00
0.50
0.580.80
0.25
0.690.86
0.35

0.820.95
0.55
0.991.00
0.80
0.710.91
0.33
0.820.95
0.47

According to Tables 1, 2 and 3, nsDBNs consistently achieve greater accuracy values
than those achieved by nsCTBNs under the three settings. Furthermore, for nsDBNs the
accuracy is stable with respect to the number of epochs E while the same does not happen
for nsCTBNs. Indeed, when the number of epochs E is greater than 3, nsCTBNs achieve
accuracy values which are significantly smaller than those achieved when the number of
epochs E is equal to 2 or 3. The same does not happen to nsDBNs where the accuracy is
robust with respect to the number of epochs E.
7. We acknowledge the precious help of Alex Hartemink who let us use the nsdbn jar executable program
for learning nsDBN models. Furthermore, he also provided the drosophila and songbird datasets.
8. Samples are obtained under the same parameters values.

19

fiVilla & Stella

Table 2: nsCTBN compared to nsDBN under the KNE setting for nsDBN generated data.
Average, min (subscript) and max (superscript) performance values over 10 networks and
c for nsCTBN and s for nsDBN.
Number of epochs E
3
4

2
Acc
P rec
Rec
F1

5

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

0.941.00
0.88
0.911.00
0.69
0.761.00
0.39
0.801.00
0.58

0.921.00
0.75
1.001.00
1.00
0.851.00
0.50
0.911.00
0.67

0.950.98
0.92
0.791.00
0.50
0.680.84
0.35
0.720.91
0.51

1.00
0.920.75
1.00
1.000.95
1.00
0.860.63
1.00
0.920.77

0.940.99
0.89
0.860.97
0.55
0.650.91
0.25
0.740.95
0.38

1.00
0.810.63
1.00
0.950.75
1.00
0.710.38
1.00
0.810.50

0.930.96
0.87
0.850.96
0.55
0.590.78
0.24
0.700.74
0.35

0.820.95
0.55
0.981.00
0.80
0.700.91
0.33
0.810.95
0.47

Table 3: nsCTBN compared to nsDBN under the UNE setting for nsDBN generated data.
Average, min (subscript) and max (superscript) performance values over 10 networks and
c , e for nsCTBN and s , m for nsDBN.
Number of epochs E
3
4

2
Acc
P rec
Rec
F1

5

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

0.951.00
0.88
0.910.98
0.70
0.750.98
0.38
0.790.96
0.55

0.921.00
0.75
1.001.00
1.00
0.851.00
0.50
0.911.00
0.67

0.950.98
0.93
0.801.00
0.52
0.660.81
0.41
0.700.89
0.48

1.00
0.920.75
1.00
1.001.00
1.00
0.860.63
1.00
0.920.77

0.930.98
0.89
0.870.95
0.67
0.650.87
0.26
0.740.87
0.41

0.99
0.810.61
1.00
0.950.73
0.99
0.700.36
0.99
0.800.48

0.920.96
0.89
0.840.90
0.71
0.570.78
0.23
0.680.85
0.34

0.810.93
0.55
0.981.00
0.80
0.690.87
0.33
0.810.93
0.47

A different picture emerges when focusing on the task to discover positive arcs. Indeed,
in such a case nsCTBNs achieve values of precision, recall and F1 measure, which are always
greater than those achieved by nsDBNs. nsCTBNs achieve precision values which are robust
with respect to the knowledge settings and the number of epochs E. The same does not
hold true for the recall performance measure. Indeed, nsCTBNs achieve a robust recall
with respect to the knowledge settings (KTT, KNE and UNE), while the recall achieved
by nsCTBNs significantly degrades when moving from 2 to 3 epochs under all knowledge
settings. The same happens to the F1 measure achieved by nsCTBNs. The results of
numerical experiments suggest that nsCTBNs are more effective than nsDBNs to discover
positive arcs, even if the datasets have been generated using nsDBNs. A possible explanation
for this behavior is that learning nsDBNs is more difficult than learning nsCTBNs. In
particular, nsDBNs must learn self-reference arcs while nsCTBNs do not. Furthermore, for
each node, nsCTBNs learn locally the sequence of parents sets while the same does not
happen for nsDBNs. In fact, nsDBNs learn globally the sequence of parents sets for all
nodes, i.e. they globally learn the sequence of networks, and thus they solve a learning
problem which is more difficult than the one solved by nsCTBNs.
20

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

5.1.2 nsCTBN Generated Datasets
We generated 40 synthetic datasets with E  {2, 3, 4, 5}, these datasets are then used to
learn the structure of nsCTBN under the three knowledge settings. The same parameters
setting is used as the one used for nsCTBN learning from nsDBN generated datasets (for
nsCTBN, we used  = 1,  = 0.1 and the BDeu metric), while, in this case, we did not
perform structural learning experiments for nsDBN models9 . The graphical structures of
the nsCTBN models sampled to obtain the datasets are the same as those sampled to
obtain the nsDBN datasets. The goal of these experiments is to analyze the performance
of nsCTBN structural learning algorithms under the three knowledge settings.
The analysis of data reported in Tables 4, 5 and 6 brings us to conclude that the
nsCTBN structural learning algorithms work very well under the three settings according
to the considered performance measures. Accuracy, recall and F1 measure decrease slightly
when the number of epochs increases from 2 to 5. In particular, the recall measure suffers
the greatest decrease from 1 to 0.95 when the number of epochs increases from 2 to 5.
Accuracy and F1 measure are very robust with respect to the number of epochs, while
precision is the most robust performance measure with respect to different datasets and
different values of the number of epochs under all knowledge settings.
Table 4: nsCTBN under the KTT setting for nsCTBN generated data. Average, min
(subscript) and max (superscript) performance values over 10 networks and c .

Acc
P rec
Rec
F1

2
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00

Number of
3
0.991.00
0.92
1.001.00
1.00
0.991.00
0.86
0.991.00
0.92

epochs E
4
0.991.00
0.94
1.001.00
1.00
0.991.00
0.89
0.991.00
0.94

5
0.981.00
0.90
1.001.00
1.00
0.961.00
0.86
0.981.00
0.92

Table 5: nsCTBN under the KNE setting for nsCTBN generated data. Average, min
(subscript) and max (superscript) performance values over 10 networks and c .

Acc
P rec
Rec
F1

2
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00

Number of
3
0.981.00
0.92
0.991.00
0.90
0.971.00
0.86
0.981.00
0.88

epochs E
4
0.991.00
0.94
1.001.00
0.99
0.981.00
0.89
0.991.00
0.94

5
0.971.00
0.90
1.001.00
0.97
0.961.00
0.85
0.981.00
0.90

9. nsCTBN generated data are asynchronous involving different time granularities, thus nsDBN cannot be
directly applied. An option is to preprocess these datasets to adapt them to nsDBNs. Given that this
would be strongly arbitrary and be penalizing for nsDBNs, we decided to learn only the nsCTBN models.

21

fiVilla & Stella

Table 6: nsCTBN under the UNE setting for nsCTBN generated data. Average, min
(subscript) and max (superscript) performance values over 10 networks and c and e .

2
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00

Acc
P rec
Rec
F1

Number of
3
0.991.00
0.92
1.001.00
1.00
0.991.00
0.86
0.991.00
0.92

epochs E
4
0.991.00
0.94
1.001.00
1.00
0.981.00
0.89
0.991.00
0.94

5
0.971.00
0.90
1.001.00
0.98
0.951.00
0.81
0.971.00
0.89

The best and worst values of accuracy for E = 5 reported in Table 6 belong to the
experiments performed on synthetic dataset number 3 and number 9 respectively. Their
results are illustrated hereafter. Figure 2(a) shows the graphs sequence of the true nsCTBN
for the synthetic datasets number 3, while Figure 2(b) displays the posterior distribution
over epochs (right), together with the distribution of the corresponding transition times
(left)10 of the learned nsCTBN in the UNE case. Figure 3 shows the same information as
those depicted in Figure 2, but for the synthetic dataset number 9. In the latter case, the
distribution over epochs is slightly in favor of the correct number of epochs.

(a) True nsCTBN model.
Distribution of the transition times

Distribution of the number of epochs

1

1
True
Retrieved
0.8
Posterior probability

Probability of transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0

5

10

15

20

25

30

35 40
Time

45

50

55

60

65

70

0

5
Number of epochs

(b) Learned nsCTBN model results.

Figure 2: nsCTBN generated dataset number 3: (a) true graphs sequence over E=5 epochs
and (b) distribution of the transition times (left) and posterior over epochs (right) associated
with the nsCTBN inferred under the UNE setting.

10. Transition times whose distance is less than 0.1 have been aggregated.

22

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

(a) True nsCTBN model.
Distribution of the transition times

Distribution of the number of epochs

1

1
True
Retrieved
0.8
Posterior probability

Probability of transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0

5

10

15

20

25

30

35

40 45
Time

50

55

60

65

70

75

80

0

4
5
Number of epochs

(b) Learned nsCTBN model results.

Figure 3: nsCTBN generated dataset number 9: (a) true graphs sequence over E=5 epochs
and (b) distribution of the transition times (left) and posterior over epochs (right) associated
with the nsCTBN inferred under the UNE setting.

5.2 Real-world Datasets
It is very difficult to find real-world datasets where the corresponding ground truth model
is completely known and/or a uniform consensus from domain experts has been reached.
Therefore, we decided to use the following three well-known datasets: drosophila, saccharomyces cerevisiae and songbird to compare the performance of nsCTBNs to that of nsDBNs
and other state-of-the-art algorithms, i.e. TSNI and TVDBN. Such datasets are publicly
available, clearly described and a rich and detailed discussion about their likely ground truth
models is given in the specialized literature. Furthermore, a macroeconomics dataset is introduced and analyzed. This dataset consists of 17 financial/economic variables collected
at different time granularity spanning from 1st January 1986 to 31st March 2015.
5.2.1 Drosophila
The drosophila dataset includes the mRNA expression levels of 4,028 genes at 67 successive time-points spanning the four stages of the Drosophila melanogaster life cycle (Lebre
et al., 2010): the embryonic (31 time-points), larval (10 time-points) and pupal stage (18
time-points) and the first 30 days of adulthood (8 time-points). For comparative purposes
(Dondelinger et al., 2013), we have analyzed the reduced drosophila dataset consisting of
gene expression time-series of 11 genes involved in wing muscle development. Given that
nsCTBNs are based on discrete variables, we binarized the expression level of the 11 genes
for the reduced drosophila dataset as done in the literature (Zhao, Serpedin, & Dougherty,
2006; Guo, Hanneke, Fu, & Xing, 2007; Robinson & Hartemink, 2010).
23

fiVilla & Stella

Firstly, the network inference task of the embryonic, larval, pupal and adulthood morphogenic stages was performed under the KTT setting (Robinson & Hartemink, 2010; Dondelinger et al., 2013). The nsCTBN structural learning was performed using the following
parameter values c = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} and by setting the maximum number of parents to 4. The nsCTBN learned with different c values are combined,
and only arcs that occurred in more than 20 percent of samples are included into the
inferred non-stationary continuous time Bayesian network. No other techniques predict
non-stationary directed networks (Robinson & Hartemink, 2010), so precision, recall and
F1 measure, computed with respect to networks inferred by Zhao et al. (2006) and Guo et
al. (2007), are reported in Table 7 for nsDBN, nsCTBN and TVDBN (Dondelinger et al.,
2013). The networks associated with the four epochs, as inferred with the nsCTBN on the
reduced drosophila dataset under the KTT setting, are depicted in Figure 4.
Table 7: Precision (Prec), recall (Rec) and F1 measure (F1 ) achieved by nsCTBN, nsDBN,
and TVDBN on the drosophila dataset are computed with respect to networks inferred by
Zhao et al. (2006) and Guo et al. (2007). Average values (Average) of precision, recall and
F1 measure achieved by Zhao et al. (2006) and Guo et al. (2007) are also reported.

nsDBN
nsCTBN
TVDBN

Zhao
Prec
0.58
0.33
0.17

et al. (2006)
Rec
F1
0.38 0.46
0.37 0.35
0.27 0.21

Guo et al. (2007)
Prec Rec
F1
0.47 0.34 0.39
0.41 0.43 0.42
0.36 0.61 0.45

Prec
0.52
0.37
0.27

Average
Rec
F1
0.36 0.42
0.40 0.39
0.44 0.33

According to Table 7, no optimal algorithm exists for the reduced drosophila dataset.
If the network retrieved by Zhao et al. (2006) is used as ground truth, then nsDBN is the
best model, while if the network retrieved by Guo et al. (2007) network is used as ground
truth, then TVDBN is the optimal one as far as the F1 measure is concerned. If the average
performance is computed, then nsDBN is the best model and TVDBN is the worst; while
nsCTBN achieves an F1 value that is close to the one achieved by nsDBN.
Secondly, we investigated whether the transition times inferred by structural learning of
nsCTBN under the UNE setting correspond to the known transitions between stages (Lebre
et al., 2010; Dondelinger et al., 2013). The network inference task was performed by learning
nsCTBN under the UNE setting with the following parameter values c = {0.2, 0.4, 1, 2}
and e = {0.5, 1, 2, 5}. Furthermore, we set the maximum number of parents to 2, the
number of iterations to 1,000 and the number of runs to 100.
Figure 5 shows the distribution of the transition times11 (left) and the posterior over
the number of epochs (right). The number of epochs is correctly detected to be 4 even if a
probability close to 0.1 is associated with 5 epochs. However, the transition times are not
all correctly identified. The embryonic stage is not correctly identified, the larval stage is
correctly discovered to start at time-point 31, while it is inferred to end at time-point 38
11. Each stem represents the posterior probability that the corresponding time-point starts a new epoch.
Therefore, a stem at time-point t means that an epoch ends at time-point t  1, while the next epoch
starts at time-point t.

24

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

instead of 40. nsCTBN did not identify the pupal and the adulthood stages, but it identified
two additional transition times (17 and 51). The same behavior is observed for nsDBNs,
while TVDBNs are capable to correctly identify the pupal and the adulthood stages. However, the TVDBN-0, TVDBN-Exp and TVDBN-Bino inferred networks (Dondelinger et al.,
2013) consist of a number of epochs ranging from 6 to 7.

mhc

mhc
gfl

gfl

mlc1

mlc1

eve

eve

msp300

msp300

actn

actn

myo61f

myo61f

up

up

prm

prm
twi

twi

sls

sls

(a) Embryonic (epoch from 0 to 30).

(b) Larval (epoch from 31 to 41).

mhc

mhc
gfl

gfl

mlc1

mlc1

eve

eve

msp300

msp300

actn

actn

myo61f

myo61f

up

up

prm

prm
twi

twi

sls

sls

(c) Pupal (epoch from 42 to 59).

(d) Adulthood (epoch from 60 to 66).

Figure 4: Networks inferred with nsCTBN under the KKT setting on the reduced drosophila
dataset. Only arcs that occurred in more than 20 percent of the networks associated with
different c values are included in the inferred nsCTBN model.

25

fiVilla & Stella

Distribution of the transition times

Distribution of the number of epochs

1

1
Retrieved
0.8
Posterior probability

Probability of transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60 63 66
Time

0

4
5
Number of epochs

Figure 5: Transition time graph (left) and posterior probability histogram over the number
of epochs E (right) associated with the nsCTBN model learned from the drosophila reduced
dataset under the UNE setting when c = 0.2 and e = 2.

5.2.2 Saccharomyces Cerevisiae
The saccharomyces cerevisiae dataset is obtained from a synthetic regulatory network with
5 genes in saccharomyces cerevisiae (Cantone, Marucci, Iorio, Ricci, Belcastro, Bansal,
Santini, di Bernardo, di Bernardo, & Cosma, 2009). It is obtained by measuring gene
expression time-series with RT-PCR (reverse transcription polymerase chain reaction) for
16 and 21 time-points under two conditions related to the carbon source: galactose (switch
on experimental condition) and glucose (switch off experimental condition). We merged the
time-series from the two experimental conditions under exclusion of the boundary point as
done in the literature (Dondelinger et al., 2013). The obtained time-series was binarized in
such a way that a 1 indicates that the gene expression level is greater than or equal to its
sample mean, while a 0 indicates the gene expression level is smaller than its sample mean.
The obtained dataset was used to infer the saccharomyces cerevisiae networks associated
with the switch on and switch off experimental conditions.
The network inference task was performed by learning a nsCTBN under the UNE setting
with the following parameter values c = {0.2, 0.4, 1, 2} and e = {0.2, 0.4, 1, 2}. Furthermore, we set the maximum number of parents to 4, the number of iterations to 1,000 and
the number of runs to 100. Only arcs that occurred in more than 50 percent of the runs are
included in the inferred nsCTBN model. Precision, recall and F1 measure values achieved
by nsCTBN are compared to those achieved by the state-of-the-art algorithms (i.e. TSNI,
nsDBN and TVDBN) in Table 8.
The result of the performed numerical experiment shows that nsCTBN is competitive with respect to state-of-the-art algorithms, while it achieves non-optimal results only
for precision associated with the switch on experimental condition. Under this condition,
5
nsCTBN achieves a precision equal to 0.5 ( 10
), while the optimal value achieved by TSNI
4
and TVDBN is 0.8 ( 5 ). On the contrary, nsCTBN achieves the best recall value, which is
equal to 0.63 ( 85 ). Under the switch off experimental condition, nsCTBN achieves the best
value for both precision, which is equal to 0.67 ( 69 ), and recall, which is equal to 0.75 ( 68 ).
26

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

We also computed the overall performance of the structural learning algorithms. In this
case, focusing the attention on the F1 measure, we can conclude that nsCTBN (0.63) is
comparable to TVDBN (0.60), which is considered to be the state-of-the-art algorithm for
the structural learning task applied to the saccharomyces cerevisiae dataset. The networks
inferred by the nsCTBN model under the switch on and switch off experimental conditions,
using c = 0.2 and c = 2, are depicted in Figure 6.
Table 8: nsCTBN compared to TSNI, nsDBN, and TVDBN when learning from the saccharomyces cerevisiae dataset. nsCTBN is learned under the UNE setting (c = 0.2, e = 2);
time-point 17 is used as the transition time between the switch on and the switch off experimental conditions. TSNI, nsDBN and TVDBN networks are described in the specialized
literature. Precision, recall and F1 measure are reported for the switch on and switch off
experimental conditions. The number of true positive arcs (superscript) and the sum of
true and false positive arcs (subscript) are reported for precision, while the number of true
positive arcs (superscript) and the sum of true positive and false negative arcs (subscript)
are reported for recall. Performance values achieved by aggregating the inferred networks
over the two epochs are also reported.

TSNI
nsDBN
TVDBN
nsCTBN

Switch on
P rec
Rec
0.8045 0.5048
0.3326 0.2528
0.8045 0.5048
0.50510 0.6358

F1
0.62
0.29
0.62
0.56

Switch off
P rec Rec
F1
0.6035 0.3838 0.46
0.6035 0.3838 0.46
0.5659 0.6358 0.59
0.6769 0.7568 0.71

GAL4

F1
0.54
0.37
0.60
0.63

GAL4

GAL80

CBF1

SWI5

Aggregated
P rec
Rec
0.70710 0.44716
0.45511 0.31516
0.64914 0.56916
0.5811
0.6911
19
16

GAL80

ASH1

SWI5

(a) Switch on network.

CBF1

ASH1

(b) Switch off network.

Figure 6: Switch on (a) and switch off (b) networks inferred with nsCTBN from the saccharomyces cerevisiae dataset under the UNE setting when c = 0.2, e = 2. The two pictures
report the positive arcs (black continuous), the false negative arcs (red dashed) and the
false positive arcs (green dotted) of the inferred networks.
27

fiVilla & Stella

Figure 7 shows the posterior distribution of the number of epochs (left) together with the
distribution of the transition times (right) for the nsCTBN learned with c = 0.2 and e = 2.
The transition between the switch on and switch off experimental conditions is known to
occur at time-point 17 (i.e. the switch off epoch starts at time-point 18). It is worthwhile
to notice that the small number of arcs, associated with the synthetic regulatory network
of saccharomyces cerevisiae, suggests that one should be very careful when evaluating the
result of the performed numerical experiment. In particular, we think that overstatements
on the effectiveness and/or superiority of different structural learning algorithms for the
learning task on the saccharomyces cerevisiae dataset should be avoided.
Distribution of the transition times

Distribution of the number of epochs

1

1
Retrieved
0.8
Posterior probability

Probability of transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0

2

4

6

8

10 12 14 16 18 20 22 24 26 28 30 32 34 36
Time

0

2
Number of epochs

Figure 7: Transition time graph (left) and posterior probability over the number of epochs
(right) associated with the nsCTBN inferred from the saccharomyces cerevisiae dataset
under the UNE setting when c = 0.2 and e = 2. The maximum aposteriori estimate over
the number of epochs is associated with E = 2 epochs: epoch 1 starts at time-point 1 and
ends at time-point 17, while epoch 2 starts at time-point 18 and ends at time-point 36.

5.2.3 Songbird
The songbird dataset was collected with eight electrodes placed into the vocal nuclei of six
female zebra finches (Smith et al., 2006). Voltage changes were recorded from populations
of neurons while the birds were provided with four different two-second auditory stimuli,
each presented from 18 to 20 times. Voltages were post-processed with a root mean square
transformation and binned to 5 ms (Robinson & Hartemink, 2010).
The songbird dataset is used to learn neural information flow networks, i.e. the networks
that represent the transmission of information between different regions of the songbird
brain. A neural information flow network represents the dynamic utilization of the potential
pathways along which information can travel. The identification of the neural information
flow networks in songbirds during auditory stimuli allows you to understand how sounds
are stored and processed in the songbirds brain. The songbird dataset consists of data
of 8 variables recorded from electrodes for two seconds pre-stimulus, two seconds during
stimulus and two seconds post-stimulus for six birds. The stimuli are hear-song, i.e. the
bird hears another bird singing, and white-noise, i.e. the bird hears a white noise stimulus.
28

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

We show the results of the nsCTBN learned on two out of the six birds of the songbird
dataset, namely bird 648 and bird 841. The results obtained for the other four birds are
similar. Given that nsCTBNs are based on discrete variables, the values of the 8 variables
were discretized into three bins using uniform quantiles (0, 13 , 23 , 1) according to the literature
(Robinson & Hartemink, 2010). The inference task of the neural information flow networks
was performed by learning a nsCTBN under the UNE setting with the following parameter
values c = {0.25, 0.5, 1, 2, 5, 10} and e = {0.25, 0.5, 1, 2, 5, 10}. We set the maximum
number of parents to 3, the number of iterations to 500 and the number of runs to 10.
Figure 8 (a) and (b) show the probability of transition (left) and the posterior probability
over the number of epochs (right) for bird 648 and bird 841 under the white-noise stimulus.
Figure 9 (a) and (b) show the probability of transition (left) and the posterior probability
over the number of epochs (right) for bird 648 and bird 841 under the hear-song stimulus.
Distribution of the transition times

Distribution of the number of epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability of transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

0

6

3
4
Number of epochs

(a) white-noise stimulus for bird 648: learned model results.
Distribution of the transition times

Distribution of the number of epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability of transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

6

0

3
4
Number of epochs

(b) white-noise stimulus for bird 841: learned model results.

Figure 8: Distribution of the transition times and posterior distribution over epochs for
nsCTBN under the UNE setting on the songbird dataset for the white-noise stimulus.
29

fiVilla & Stella

The location of the transition time-points under the white-noise stimulus and the hearsong stimulus are accurately inferred for bird 648 and bird 841. The posterior distribution
over the number of epochs for birds 648 and 841 under the white-noise stimulus is nearly
equally split between 3 and 4 epochs, while under the hear-song stimulus it is peaked over 3
epochs. Therefore, both the number of epochs and the location of the transition time-points
are reliably recovered by the nsCTBN learned under the UNE setting. Unfortunately, we
were not able to find any additional information to validate the learned nsCTBNs for this
dataset. Moreover, a comparison across different birds to eventually develop a consensus
network is not possible due to the songbird data collection settings. Indeed, each of the six
birds is characterized by its own electrodes, which make difficult to obtain a correspondence
map across different birds.

Distribution of the transition times

Distribution of the number of epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability of transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

0

6

3
4
Number of epochs

(a) hear-song stimulus for bird 648: learned model results.
Distribution of the transition times

Distribution of the number of epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability of transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

6

0

3
4
Number of epochs

(b) hear-song stimulus for bird 841: learned model results.

Figure 9: Distribution of the transition times and posterior distribution over epochs for
nsCTBN under the UNE setting on the songbird dataset for the hear-song stimulus.

30

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

5.2.4 Macroeconomics
The macroeconomics dataset consists of 17 financial/economic time-series pertaining to the
economy of the United States. Time-series have different time granularity and span from
1st January 1986 to 31st March 2015. More specifically, five time-series have daily granularity, namely Crude oil (OIL), USD to EUR spot exchange rate (USDEUR), Gold (GOLD),
S&P500 equity index (S&P500) and the 10-years treasury bond yield rate (US10yrsNote).
Eleven time-series have monthly granularity, namely production of total industry (PTI),
real manufacturing and trade industries sales (RMTIS), personal income (PI), unemployment (UN), consumer price index (CPI), federal funds rate (RATE), producer price index
(PPI), non-farm payrolls (NFP), new one-family houses sold (NHSold), new houses for sale
(NHSale) and new private house permits (NHPermit). Finally, the gross domestic product
(GDP) time-series has quarterly granularity.
The goal of this study is to discover how the financial and economic environment evolves
over time. In particular, we focused the attention to detect business cycles12 and the
associated change of relationships among financial and economic variables. Given that the
duration of a business cycle is highly variable, the ability to identify the turning point of a
cycle (i.e. when a recession starts) is of considerable importance to policymakers, financial
companies as well as to individuals. A substantial literature is available about the business
cycle turning points detection generally relying on Markov-switching models (Hamilton &
Raj, 2005). However, these models are not able to represent some important features such
as the dependence structure among variables in each business cycle.
In order to use the nsCTBN model in such a context, we applied a binary discretization
to the variable associated with each time-series. Discretization was performed using a lookback period of 1 year, i.e. if the current value is greater than the past one, then the binary
variable is set to 1 otherwise, it is set to 0. The approach of looking back into the past
is widely used in finance (Moskowitz, Ooi, & Pedersen, 2012). nsCTBNs learning was
performed under the UNE setting using the following parameter values: c = {0.5, 1, 2},
e = {0.1, 1, 10}, 2 maximum parents per node, 300 iterations and 10 runs.
Figure 10 shows the probability of transition (left side, left axis) versus the S&P500
equity index used as a reference (left side, right axis) and the posterior probability over the
number of epochs (right side). The nsCTBN consists of three epochs with transition times
close to the end of July 2000 and the end of November 2007. If we compare these dates to
the turning points of the US business cycle reported by the National Bureau of Economic
Research13 , then we see that we are not far from the turning point of March 2001 and very
close to the one of December 2007, while we missed the turning point which occurred in
July 1990, probably because of the limited length of the dataset.
Figure 11 shows the structure of the nsCTBN model corresponding to the most probable
number of epochs, i.e. E = 3. An arc is included in the nsCTBN model when it occurs in
more than 75% of the performed runs in each epoch. The retrieved networks correspond to
the following time periods: from January 1986 to July 2000 (epoch 1), from August 2000
to November 2007 (epoch 2) and from December 2007 to March 2015 (epoch 3).
12. Business cycles are fluctuations in aggregate economic activity, they are recurrent (i.e. it is possible to
identify expansion-recession cycles), persistent and not periodic (i.e. they differ in length and severity).
13. The official business cycle turning points and dates are available at http://www.nber.org/cycles.html

31

fiVilla & Stella

Distribution of the transition times vs S&P500

Distribution of the number of epochs
2500

1

0.8

2000

0.8

0.6

1500

0.4

1000

0.2

500

1

Posterior probability

Value

Probability of transition

Retrieved (left)
S&P500 (right)

0
1985

1990

1995

2000
2005
Time

2010

0.4

0.2

0
2020

2015

0.6

0

2

3
4
5
Number of epochs

Figure 10: Distribution of the transition times and S&P500 behavior over time (left). Posterior probability over epochs (right) for the learned nsCTBN under the UNE setting.
USDEUR

OIL
GOLD

USDEUR
OIL

UN

GOLD

US10YRS

US10YRS

PPI

NHPer

CPI

PPI

NHPer

PTI

SP500

PTI

SP500

PI
RMTIS

NHSale

RATE

PI

CPI

RMTIS

RATE
NFP

GDP
NHSold
NFP

GDP
UN

NHSale

(a) Epoch 1 (Jan 1986 - Jul 2000).

NHSold

(b) Epoch 2 (Aug 2000 - Nov 2007).
USDEUR
OIL

PPI

US10YRS

GOLD
NHSale

SP500
PTI

CPI

RATE

NHPer
PI
RMTIS
NFP
UN
GDP

NHSold

(c) Epoch 3 (Dec 2007 - Mar 2015).

Figure 11: nsCTBN learned on the macroeconomics dataset under the UNE setting.
nsCTBN corresponds to the most probable number of epochs (E = 3). An arc is included
in the nsCTBN model when it occurs in more than 75% of the runs in each epoch.

32

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

The novelty of this approach to the economic analysis opens the door to many considerations and new speculations about the economic variables during business cycles. In
this paper, we highlight two patterns emerging from the learned nsCTBN model: the well
known relevant role of the personal income (PI) and its relation to the unemployment (UN)
(Mankiw, 2014) and the less known relation of the non-farm payrolls (NFP) to the S&P500
equity index (S&P500) (Miao, Ramchander, & Zumwalt, 2014).

6. Conclusions
We introduced non-stationary continuous time Bayesian networks and developed three
structural learning algorithms to be used under different knowledge settings (i.e. KTT,
KNE and UNE) for the problem to be analyzed. The structural learning algorithm in
the known transition times case is exact and it exploits graph theory to infer the optimal
nsCTBNs structure. It has a polynomial time complexity under the assumption that the
maximum number of parents for each node is fixed. All the nsCTBNs structural learning algorithms are competitive to state-of-the-art algorithms when synthetic and real-world
datasets are considered. This statement is proved by a rich set of numerical experiments.
nsCTBNs can be adapted to use different score metrics, as far as the considered score
metrics integrates over the non-structural parameters. nsCTBNs exploit an interesting
property of CTBNs and offer the possibility to learn the optimal nsCTBNs structure for
each single variable. This could be extremely useful in the case when the non-stationary
behavior of the analyzed system is not synchronous, and thus it may be the case that each
node changes its parents independently from how other nodes change their parents set.
However, two main limitations exist with nsCTBNs: i) the variables are assumed to be
discrete; specifically each variable of the dataset must take value over a countable number of
states and ii) finding the optimal value for the c and e hyperparameters can be extremely
difficult (the same is true for nsDBNs). Concerning i), the problem of discretizing continuous
variables has been studied for a long time and robust solutions have been described in the
specialized literature. Discretizing continuous variables whose value is measured over time
has not been studied intensively and many issues still remain. The problem ii) of selecting
the optimal value of hyperparameters is known in the specialized literature and much can
be done when experts provide their valuable apriori knowledge. However, when such apriori
knowledge is poor or not available at all, selecting optimal hyperparameter values can be
extremely difficult. It is important to note that one of the strong limitations to studying
and comparing non-stationary models is the lack of ground truth models.
Possible directions for further research include the application of nsCTBNs structural
learning algorithms to other datasets, such as the arabidopsis thaliana dataset (Grzegorczyk,
Aderhold, & Husmeier, 2015) as well as other financial datasets supported by in-depth
economic analyses. Another interesting perspective is the study and development of a
modeling approach, going towards the direction of allowing each node to change its parents
set asynchronously. Furthermore, we think that to increase the applicability to real-world
time-series data of the proposed nsCTBNs structural learning algorithms the issue of timeseries discretization must be addressed. In particular, we think that this issue must be
addressed in an integrated manner with the nsCTBNs structural learning algorithm.
33

fiVilla & Stella

Finally, it could be interesting to apply the framework of nsCTBNs to address the
task of classification of objects in a streaming context when using a probabilistic graphical model based approach (Borchani, Martinez, Masegosa, Langseth, Nielsen, Salmeron,
Fernandez, Madsen, & Saez, 2015a; Borchani, Martnez, Masegosa, Langseth, Nielsen,
Salmeron, Fernandez, Madsen, & Saez, 2015b).

Acknowledgments
The authors wish to thank Alexander Hartemink for having kindly provided the nsDBN
jar executable and the associated datasets. A special thank goes to Marco Grzegorczyk for
providing the arabidopsis thaliana dataset together with fundamental information to analyze
it. The authors are greatly indebted to anonymous referees for their constructive comments
and their extremely helpful suggestions, which contributed to significantly improve the
quality of the paper. A special thank goes to the Associate Editor Manfred Jaeger.
Fabio Stella is the corresponding author of this article.

References
Acerbi, E., & Stella, F. (2014). Continuous time bayesian networks for gene network reconstruction: a comparative study on time course data. In The 10th International
Symposium on Bioinformatics Research and Applications, Zhangjiajie, China, 2014,
10.
Acerbi, E., Vigano, E., Poidinger, M., Mortellaro, A., Zelante, T., & Stella, F. (2016).
Continuous time bayesian networks identify prdm1 as a negative regulator of th17 cell
differentiation in humans. Scientific Reports, 6, 23128.
Acerbi, E., Zelante, T., Narang, V., & Stella, F. (2014). Gene network inference using
continuous time bayesian networks: a comparative study and application to th17 cell
differentiation. BMC Bioinformatics, 15 (1).
Ahmed, A., & Xing, E. P. (2009). Recovering time-varying networks of dependencies in social
and biological studies. Proceedings of the National Academy of Sciences, 106 (29),
1187811883.
Bertsimas, D., & Tsitsiklis, J. (1993). Simulated annealing. Statistical Science, 8 (1), 1015.
Borchani, H., Martinez, A. M., Masegosa, A., Langseth, H., Nielsen, T. D., Salmeron, A.,
Fernandez, A., Madsen, A. L., & Saez, R. (2015a). Dynamic Bayesian modeling for
risk prediction in credit operations. In The 13th Scandinavian Conference on Artificial
Intelligence (SCAI 2015), Halmstad, Sweden.
Borchani, H., Martnez, A. M., Masegosa, A. R., Langseth, H., Nielsen, T. D., Salmeron,
A., Fernandez, A., Madsen, A. L., & Saez, R. (2015b). Modeling concept drift: A
probabilistic graphical model based approach. In The 14th International Symposium
on Intelligent Data Analysis (IDA 2015), Saint-Etienne, France.
Boudali, H., & Dugan, J. B. (2006). A continuous-time bayesian network reliability modeling, and analysis framework. IEEE Transactions on Reliability, 55 (1), 8697.
34

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Burge, J., Lane, T., Link, H., Qiu, S., & Clark, V. P. (2009). Discrete dynamic bayesian
network analysis of fmri data. Human brain mapping, 30 (1), 122137.
Cantone, I., Marucci, L., Iorio, F., Ricci, M. A., Belcastro, V., Bansal, M., Santini, S.,
di Bernardo, M., di Bernardo, D., & Cosma, M. P. (2009). A yeast synthetic network
for in vivo assessment of reverse-engineering and modeling approaches. Cell, 137 (1),
172  181.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Comput. Intell., 5 (3), 142150.
Dondelinger, F., Lebre, S., & Husmeier, D. (2013). Non-homogeneous dynamic bayesian
networks with bayesian regularization for inferring gene regulatory networks with
gradually time-varying structure. Machine Learning, 90 (2), 191230.
Durante, D., & Dunson, D. B. (2014). Bayesian dynamic financial networks with timevarying predictors. Statistics & Probability Letters, 93, 1926.
Fan, Y., & Shelton, C. R. (2009). Learning continuous-time social network dynamics. In
The 25th Conference on Uncertainty in Artificial Intelligence (UAI 2009), Montreal,
Canada.
Friedman, N., & Koller, D. (2000). Being bayesian about bayesian network structure: A
bayesian approach to structure discovery in bayesian networks. Machine Learning,
50, 95125.
Gatti, E., Luciani, D., & Stella, F. (2011). A continuous time bayesian network model
for cardiogenic heart failure. Flexible Services and Manufacturing Journal, 24 (2),
496515.
Geiger, D., & Heckerman, D. (1997). A characterization of dirchlet distributions through
local and global independence. Annals of Statistics, 25, 13441368.
Grzegorczyk, M., Aderhold, A., & Husmeier, D. (2015). Inferring bi-directional interactions between circadian clock genes and metabolism with model ensembles. Statistical
Applications in Genetics and Molecular Biology, 14 (2), 143167.
Guo, F., Hanneke, S., Fu, W., & Xing, E. P. (2007). Recovering temporally rewiring networks: a model-based approach. In Machine Learning, Proceedings of the 24th International Conference (ICML 2007), Corvallis, USA, June 20-24, 2007, pp. 321328.
Hamilton, J. D., & Raj, B. (Eds.). (2005). Advances in Markov-Switching Models: Applications in Business Cycle Research and Finance. Studies in Empirical Economics.
Springer-Verlag.
Herbrich, R., Graepel, T., & Murphy, B. (2007). Structure from failure. In The 2nd USENIX
workshop on Tackling computer systems problems with machine learning techniques
(SYSML 07), Cambridge, USA, pp. 16.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated annealing.
Science, 220 (4598), 671680.
Lebre, S., Becq, J., Devaux, F., Stumpf, M., & Lelandais, G. (2010). Statistical inference of
the time-varying structure of gene regulation networks. BMC Systems Biology, 4 (1),
130+.
35

fiVilla & Stella

Liu, M., Hommersom, A., van der Heijden, M., & Lucas, P. J. (2016). Hybrid time bayesian
networks. International Journal of Approximate Reasoning, .
Mankiw, N. G. (2014). Principles of Macroeconomics (7th edition). South-Western College
Pub.
Marini, S., Trifoglio, E., Barbarini, N., Sambo, F., Camillo, B. D., Malovini, A., Manfrini,
M., Cobelli, C., & Bellazzi, R. (2015). A dynamic bayesian network model for longterm simulation of clinical complications in type 1 diabetes. Journal of Biomedical
Informatics, 57, 369  376.
Miao, H., Ramchander, S., & Zumwalt, J. K. (2014). S&p 500 index-futures price jumps
and macroeconomic news. Journal of Futures Markets, 34 (10), 9801001.
Moskowitz, T. J., Ooi, Y. H., & Pedersen, L. H. (2012). Time series momentum. Journal
of Financial Economics, 104 (2), 228250.
Mumford, J. A., & Ramsey, J. D. (2014). Bayesian networks for fmri: A primer. Neuroimage,
86, 573582.
Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. The MIT Press.
Nodelman, U. (2007). Continuous Time Bayesian Networks. Ph.D. thesis, Stanford University.
Nodelman, U., & Horvitz, E. (2003). Continuous time bayesian networks for inferring users
presence and activities with extensions for modeling and evaluation. Tech. rep. MSRTR-2003-97, Microsoft Research.
Nodelman, U., Shelton, C. R., & Koller, D. (2002). Continuous time bayesian networks. In
The 18th Conference on Uncertainty in Artificial Intelligence (UAI 2002), Edmonton,
Canada, pp. 378387.
Nodelman, U., Shelton, C., & Koller, D. (2003). Learning continuous time bayesian networks. In The 19th Conference on Uncertainty in Artificial Intelligence (UAI 2003),
Acapulco, Mexico, pp. 451458.
Pearl, J. (1989). Probabilistic reasoning in intelligent systems - networks of plausible inference. Morgan Kaufmann series in representation and reasoning. Morgan Kaufmann.
Robinson, J. W., & Hartemink, A. J. (2010). Learning non-stationary dynamic bayesian
networks. Journal of Machine Learning Research, 11, 36473680.
Scutari, M., & Denis, J.-B. (2014). Bayesian Networks with Examples in R. Chapman and
Hall, Boca Raton. ISBN 978-1482225587.
Segal, E., Peer, D., Regev, A., Koller, D., & Friedman, N. (2005). Learning module networks. Journal of Machine Learning Research, 6, 557588.
Smith, A. V., Yu, J., Smulders, T. V., Hartemink, A. J., & Jarvis, E. D. (2006). Computational Inference of Neural Information Flow Networks. PLoS Computational Biology,
2 (11), e161+.
Spiegelhalter, D. J., & Lauritzen, S. L. (1990). Sequential updating of conditional probabilities on directed graphical structures. Networks, 20 (5), 579605.
36

fiLearning Continuous Time Bayesian Networks in Non-stationary Domains

Sturlaugson, L., & Sheppard, J. W. (2014). Inference complexity in continuous time bayesian
networks. In The 30th Conference on Uncertainty in Artificial Intelligence (UAI
2014), Quebec City, Canada, pp. 772779.
Vinh, N. X., Chetty, M., Coppel, R., & Wangikar, P. P. (2012). Gene regulatory network
modeling via global optimization of high-order dynamic bayesian network. BMC
Bioinformatics, 13, 131.
Xu, J., & Shelton, C. R. (2008). Continuous time bayesian networks for host level network
intrusion detection. In The European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases (ECML PKDD 2008), Antwerp,
Belgium, pp. 613627.
Zhao, W., Serpedin, E., & Dougherty, E. R. (2006). Inferring gene regulatory networks
from time series data using the minimum description length principle. Bioinformatics,
22 (17), 21292135.
Zou, M., & Conzen, S. D. (2005). A new dynamic bayesian network (dbn) approach for identifying gene regulatory networks from time course microarray data. Bioinformatics,
21 (1), 7179.

37

fi