Journal of Artificial Intelligence Research 12 (2000) 149198

Submitted 11/99; published 3/00

A Model of Inductive Bias Learning
Jonathan Baxter

J ONATHAN .BAXTER @ ANU . EDU . AU

Research School of Information Sciences and Engineering
Australian National University, Canberra 0200, Australia

Abstract
A major problem in machine learning is that of inductive bias: how to choose a learners hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small
enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is
supplied by hand through the skill and insights of experts. In this paper a model for automatically
learning bias is investigated. The central assumption of the model is that the learner is embedded
within an environment of related learning tasks. Within such an environment the learner can sample
from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to
many of the problems in the environment. Under certain restrictions on the set of all hypothesis
spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently
large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an
environment of related tasks can potentially give much better generalization than learning a single
task.

1. Introduction
Often the hardest problem in any machine learning task is the initial choice of hypothesis space;
it has to be large enough to contain a solution to the problem at hand, yet small enough to ensure
good generalization from a small number of examples (Mitchell, 1991). Once a suitable bias has
been found, the actual learning task is often straightforward. Existing methods of bias generally
require the input of a human expert in the form of heuristics and domain knowledge (for example,
through the selection of an appropriate set of features). Despite their successes, such methods are
clearly limited by the accuracy and reliability of the experts knowledge and also by the extent to
which that knowledge can be transferred to the learner. Thus it is natural to search for methods for
automatically learning the bias.
In this paper we introduce and analyze a formal model of bias learning that builds upon
the PAC model of machine learning and its variants (Vapnik, 1982; Valiant, 1984; Blumer,
Ehrenfeucht, Haussler, & Warmuth, 1989; Haussler, 1992). These models typically take the
and training data
following general form: the learner is supplied with a hypothesis space
drawn independently according to some underlying distribution
on
. Based on the information contained in , the learners goal is to select a hypothesis
from minimizing some measure
of expected loss with respect to (for example, in the case of squared loss
). In such models the learners
bias is represented by the choice of ; if does not contain a good solution to the problem, then,
regardless of how much data the learner receives, it cannot learn.
Of course, the best way to bias the learner is to supply it with an containing just a single optimal hypothesis. But finding such a hypothesis is precisely the original learning problem, so in the

	ff
fi
fifi	fi
!"
#%$ '
&("

@



# 

)

*
+
#
$
 ,/.1032 46587 +  # 	 9;:<=?>
) *+   -

c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.





fiBAXTER

PAC model there is no distinction between bias learning and ordinary learning. Or put differently,
the PAC model does not model the process of inductive bias, it simply takes the hypothesis space
as given and proceeds from there. To overcome this problem, in this paper we assume that instead
of being faced with just a single learning task, the learner is embedded within an environment of
related learning tasks. The learner is supplied with a family of hypothesis spaces
, and its
) that is appropriate for the entire environment.
goal is to find a bias (i.e. hypothesis space
A simple example is the problem of handwritten character recognition. A preprocessing stage that
identifies and removes any (small) rotations, dilations and translations of an image of a character
will be advantageous for recognizing all characters. If the set of all individual character recognition
problems is viewed as an environment of learning problems (that is, the set of all problems of the
form distinguish A from all other characters, distinguish B from all other characters, and
so on), this preprocessor represents a bias that is appropriate for all problems in the environment.
It is likely that there are many other currently unknown biases that are also appropriate for this
environment. We would like to be able to learn these automatically.

DC A

A B 

There are many other examples of learning problems that can be viewed as belonging to environments of related problems. For example, each individual face recognition problem belongs to an
(essentially infinite) set of related learning problems (all the other individual face recognition problems); the set of all individual spoken word recognition problems forms another large environment,
as does the set of all fingerprint recognition problems, printed Chinese and Japanese character recognition problems, stock price prediction problems and so on. Even medical diagnostic and prognostic
problems, where a multitude of diseases are predicted from the same pathology tests, constitute an
environment of related learning problems.
In many cases these environments are not normally modeled as such; instead they are treated
as single, multiple category learning problems. For example, recognizing a group of faces would
normally be viewed as a single learning problem with multiple class labels (one for each face in
the group), not as multiple individual learning problems. However, if a reliable classifier for each
individual face in the group can be constructed then they can easily be combined to produce a
classifier for the whole group. Furthermore, by viewing the faces as an environment of related
learning problems, the results presented here show that bias can be learnt that will be good for
learning novel faces, a claim that cannot be made for the traditional approach.
This point goes to the heart of our model: we are not not concerned with adjusting a learners
bias so it performs better on some fixed set of learning problems. Such a process is in fact just
ordinary learning but with a richer hypothesis space in which some components labelled bias are
also able to be varied. Instead, we suppose the learner is faced with a (potentially infinite) stream of
tasks, and that by adjusting its bias on some subset of the tasks it improves its learning performance
on future, as yet unseen tasks.
Bias that is appropriate for all problems in an environment must be learnt by sampling from
many tasks. If only a single task is learnt then the bias extracted is likely to be specific to that
task. In the rest of this paper, a general theory of bias learning is developed based upon the idea of
learning multiple related tasks. Loosely speaking (formal results are stated in Section 2), there are
two main conclusions of the theory presented here:

E

Learning multiple related tasks reduces the sampling burden required for good generalization,
at least on a number-of-examples-required-per-task basis.
150

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

E

Bias that is learnt on sufficiently many training tasks is likely to be good for learning novel
tasks drawn from the same environment.

The second point shows that a form of meta-generalization is possible in bias learning. Ordinarily, we say a learner generalizes well if, after seeing sufficiently many training examples, it
produces a hypothesis that with high probability will perform well on future examples of the same
task. However, a bias learner generalizes well if, after seeing sufficiently many training tasks it produces a hypothesis space that with high probability contains good solutions to novel tasks. Another
term that has been used for this process is Learning to Learn (Thrun & Pratt, 1997).
Our main theorems are stated in an agnostic setting (that is,
does not necessarily contain a
hypothesis space with solutions to all the problems in the environment), but we also give improved
bounds in the realizable case. The sample complexity bounds appearing in these results are stated
in terms of combinatorial parameters related to the complexity of the set of all hypothesis spaces
available to the bias learner. For Boolean learning problems (pattern classification) these parameters
are the bias learning analogue of the Vapnik-Chervonenkis dimension (Vapnik, 1982; Blumer et al.,
1989).
As an application of the general theory, the problem of learning an appropriate set of neuralnetwork features for an environment of related tasks is formulated as a bias learning problem. In
the case of continuous neural-network features we are able to prove upper bounds on the number
of training tasks and number of examples of each training task required to ensure a set of features
that works well for the training tasks will, with high probability, work well on novel tasks drawn
where is
from the same environment. The upper bound on the number of tasks scales as
a measure of the complexity of the possible feature sets available to the learner, while the upper
where
is the number
bound on the number of examples of each task scales as
of examples required to learn a task if the true set of features (that is, the correct bias) is already
known, and is the number of tasks. Thus, in this case we see that as the number of related tasks
learnt increases, the number of examples required of each task for good generalization decays to
the minimum possible. For Boolean neural-network feature maps we are able to show a matching
lower bound on the number of examples required per task of the same form.

A

A

F JILKMGONPQ

P

F HG
F JIR

G

1.1 Related Work
There is a large body of previous algorithmic and experimental work in the machine learning and
statistics literature addressing the problems of inductive bias learning and improving generalization
through multiple task learning. Some of these approaches can be seen as special cases of, or at least
closely aligned with, the model described here, while others are more orthogonal. Without being
completely exhaustive, in this section we present an overview of the main contributions. See Thrun
and Pratt (1997, chapter 1) for a more comprehensive treatment.

E

Hierarchical Bayes. The earliest approaches to bias learning come from Hierarchical Bayesian
methods in statistics (Berger, 1985; Good, 1980; Gelman, Carlin, Stern, & Rubim, 1995).
In contrast to the Bayesian methodology, the present paper takes an essentially empirical
process approach to modeling the problem of bias learning. However, a model using a mixture
of hierarchical Bayesian and information-theoretic ideas was presented in Baxter (1997a),
with similar conclusions to those found here. An empirical study showing the utility of the
hierarchical Bayes approach in a domain containing a large number of related tasks was given
in Heskes (1998).
151

fiE

BAXTER

E

Early machine learning work. In Rendell, Seshu, and Tcheng (1987) VBMS or Variable Bias
Management System was introduced as a mechanism for selecting amongst different learning
algorithms when tackling a new learning problem. STABB or Shift To a Better Bias (Utgoff, 1986) was another early scheme for adjusting bias, but unlike VBMS, STABB was not
primarily focussed on searching for bias applicable to large problem domains. Our use of an
environment of related tasks in this paper may also be interpreted as an environment of
analogous tasks in the sense that conclusions about one task can be arrived at by analogy
with (sufficiently many of) the other tasks. For an early discussion of analogy in this context, see Russell (1989, S4.3), in particular the observation that for analogous problems the
sampling burden per task can be reduced.
Metric-based approaches. The metric used in nearest-neighbour classification, and in vector
quantization to determine the nearest code-book vector, represents a form of inductive bias.
Using the model of the present paper, and under some extra assumptions on the tasks in
the environment (specifically, that their marginal input-space distributions are identical and
they only differ in the conditional probabilities they assign to class labels), it can be shown
that there is an optimal metric or distance measure to use for vector quantization and onenearest-neighbour classification (Baxter, 1995a, 1997b; Baxter & Bartlett, 1998). This metric
can be learnt by sampling from a subset of tasks from the environment, and then used as a
distance measure when learning novel tasks drawn from the same environment. Bounds on
the number of tasks and examples of each task required to ensure good performance on novel
tasks were given in Baxter and Bartlett (1998), along with an experiment in which a metric
was successfully trained on examples of a subset of 400 Japanese characters and then used as
a fixed distance measure when learning 2600 as yet unseen characters.
A similar approach is described in Thrun and Mitchell (1995), Thrun (1996), in which a
neural networks output was trained to match labels on a novel task, while simultaneously
being forced to match its gradient to derivative information generated from a distance metric
trained on previous, related tasks. Performance on the novel tasks improved substantially
with the use of the derivative information.

E

E

Note that there are many other adaptive metric techniques used in machine learning, but these
all focus exclusively on adjusting the metric for a fixed set of problems rather than learning a
metric suitable for learning novel, related tasks (bias learning).
Feature learning or learning internal representations. As with adaptive metric techniques,
there are many approaches to feature learning that focus on adapting features for a fixed task
rather than learning features to be used in novel tasks. One of the few cases where features
have been learnt on a subset of tasks with the explicit aim of using them on novel tasks was
Intrator and Edelman (1996) in which a low-dimensional representation was learnt for a set
of multiple related image-recognition tasks and then used to successfully learn novel tasks of
the same kind. The experiments reported in Baxter (1995a, chapter 4) and Baxter (1995b),
Baxter and Bartlett (1998) are also of this nature.
Bias learning in Inductive Logic Programming (ILP). Predicate invention refers to the process in ILP whereby new predicates thought to be useful for the classification task at hand
are added to the learners domain knowledge. By using the new predicates as background domain knowledge when learning novel tasks, predicate invention may be viewed as a form of
152

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

inductive bias learning. Preliminary results with this approach on a chess domain are reported
in Khan, Muggleton, and Parson (1998).

E

E

Improving performance on a fixed reference task. Multi-task learning (Caruana, 1997)
trains extra neural network outputs to match related tasks in order to improve generalization
performance on a fixed reference task. Although this approach does not explicitly identify the
extra bias generated by the related tasks in a way that can be used to learn novel tasks, it is
an example of exploiting the bias provided by a set of related tasks to improve generalization
performance. Other similar approaches include Suddarth and Kergosien (1990), Suddarth and
Holden (1991), Abu-Mostafa (1993).

E

Bias as computational complexity. In this paper we consider inductive bias from a samplecomplexity perspective: how does the learnt bias decrease the number of examples required of
novel tasks for good generalization? A natural alternative line of enquiry is how the runningtime or computational complexity of a learning algorithm may be improved by training on
related tasks. Some early algorithms for neural networks in this vein are contained in Sharkey
and Sharkey (1993), Pratt (1992).
Reinforcement Learning. Many control tasks can appropriately be viewed as elements of sets
of related tasks, such as learning to navigate to different goal states, or learning a set of
complex motor control tasks. A number of papers in the reinforcement learning literature
have proposed algorithms for both sharing the information in related tasks to improve average
generalization performance across those tasks Singh (1992), Ring (1995), or learning bias
from a set of tasks to improve performance on future tasks Sutton (1992), Thrun and Schwartz
(1995).

1.2 Overview of the Paper
In Section 2 the bias learning model is formally defined, and the main sample complexity results
are given showing the utility of learning multiple related tasks and the feasibility of bias learning.
These results show that the sample complexity is controlled by the size of certain covering numbers
associated with the set of all hypothesis spaces available to the bias learner, in much the same way
as the sample complexity in learning Boolean functions is controlled by the Vapnik-Chervonenkis
dimension (Vapnik, 1982; Blumer et al., 1989). The results of Section 2 are upper bounds on
the sample complexity required for good generalization when learning multiple tasks and learning
inductive bias.
The general results of Section 2 are specialized to the case of feature learning with neural networks in Section 3, where an algorithm for training features by gradient descent is also presented.
For this special case we are able to show matching lower bounds for the sample complexity of
multiple task learning. In Section 4 we present some concluding remarks and directions for future
research. Many of the proofs are quite lengthy and have been moved to the appendices so as not to
interrupt the flow of the main text.
The following tables contain a glossary of the mathematical symbols used in the paper.
153

fiBAXTER

Symbol



U

"

Description
Input Space
Output Space
Distribution on
(learning task)
Loss function
Hypothesis Space
Hypothesis
Error of hypothesis on distribution
Training set
Learning Algorithm
Empirical error of on training set
Set of all learning tasks
Distribution over learning tasks
Family of hypothesis spaces
Loss of hypothesis space on environment
-sample
Empirical loss of on
Bias learning algorithm
Function induced by and
Set of
Average of
Same as
Set of
Set of
Function on probability distributions
Set of
Pseudo-metric on
Pseudo-metric on
Covering number of
Capacity of
Covering number of
Capacity of
Sequence of hypotheses
Sequence of distributions
Average loss of on
Average loss of on
Set of feature maps
Output class composed with feature maps
Hypothesis space associated with
Loss function class associated with
Covering number of
Capacity of
Pseudo-metric on feature maps
Covering number of

ST"

#
#
) * +  # 
V
#
)Y W *X  # 

Z
A
)\ *[  
	P]fi^_
W)*a`  
\
V
#b
#
b
#b
# 
 2 b fifi # c 2 b
 # 
 fifi #Rc  b
# 
 fifi #Rc  b
dQb c
bc
 # c 
 fifi #=c  b
b
b
TeA
e
e
cb
fA g
f[
e
A
h Jijfi A e fi f [ 
A e
k Jijfi A e c 
eA
cb
h Jijfi A c b fi flg 
A
c
m Jijfi b 
AP b
d A
n
P
g)*  d 
d n
d \
W)* `  d 
o
p
psr q
pb
h Jijfi p b fi f + 
pb
k Hijfi p b 
pb
ft + 2 uvxw  q fi qzy 
h Jijfi o fi f=t + 2 uv{w 
o





U

 # 
 fifi #=c 
  
 fifi  c 

154

qp

q fi qzy

q

Z

First Referenced
155
155
155
155
155
155
156
156
156
156
157
157
157
158
158
158
159
159
159
159
159
159
159
160
160
160
160
160
160
160
160
163
163
164
164
166
166
166
166
166
166
166
166

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

hSymbol
o
Jijfi o fi f t + 2 uv{w8 Description
Covering number of
k uv Jijfi o 
o
Capacity of
}|
Neural network hypothesis space
~ 0
restricted to vector 
 	^_
Growth function of
  
Vapnik-Chervonenkis dimension of
~ 
restricted to matrix 
A ~  	P]fi^_
A restricted to matrix 
Growth function of A
f  	PQ
Dimension function of A
f A 
Upper dimension function of A
f A  c
Lower dimension function cof A
n
= g  A 
Optimal performance of A
on
f
on 
# 
/ #=c Metric
#
#Rc
Average of 
 ,  ,
c
#
=
#
c

3
Set of 
/
 . >  2c 5
Permutations on integer pairs
\j
Permuted \
f ` d fi d y
U
d
Empirical 
 metric on functions
W)* g  
n
Optimal average error of on

First Referenced
166
166
167
172
172
172
173
173
173
173
173
173
175
179
179
180
182
182
182
185

2. The Bias Learning Model
In this section the bias learning model is formally introduced. To motivate the definitions, we first
describe the main features of ordinary (single-task) supervised learning models.
2.1 Single-Task Learning
Computational learning theory models of supervised learning usually include the following ingredients:

E

An input space

E



 on Ss"
E a loss function U$ "s"&D , and
a probability distribution

E

a hypothesis space

"

and an output space

,

,

which is a set of hypotheses or functions

#$ &"

.

As an example, if the problem is to learn to recognize images of Marys face using a neural network,
then would be the set of all images (typically represented as a subset of
where each component
is a pixel intensity), would be the set
, and the distribution would be peaked over images
of different faces and the correct class labels. The learners hypothesis space would be a class of
neural networks mapping the input space
to
. The loss in this case would be discrete loss:



"

fi

   fi
 
U 	zfi y  $   if L
 y



if

155

y

]

(1)

fiBAXTER

fi U
U 	fi y ]	L: y  >

" 

"  

Using the loss function allows us to present a unified treatment of both pattern recognition (
, as above), and real-valued function learning (e.g. regression) in which
and usually
.
The goal of the learner is to select a hypothesis
with minimum expected loss:

# CT

(2)
)* +  #  $ B U  # 	fi= f  	QfiR
#
for an minimizing
Of course, the learner does not know  and so it cannot search through
)* +  #  . In practice, the learner samples repeatedly from " according to the distribution  to
generate a training set
 $ 	 
 fi 
 fifi	  fi  j
(3)
# CT . Hence, in general
Based on the information contained in  the learner produces a hypothesis
V
a learner is simply a map from the set of all training samples to the hypothesis space :
V $  DT"   &

V
(stochastic learners can be treated by assuming a distribution-valued .)
#
Many algorithms seek to minimize the empirical loss of on  , where this is defined by:
 

W)* X  #  $  ^    U  # 	 fi 
(4)


Of course, there are more intelligent things to do with the data than simply minimizing empirical
errorfor example one can add regularisation terms to avoid over-fitting.
However the learner chooses its hypothesis , if we have a uniform bound (over all
) on
the probability of large deviation between
and
, then we can bound the learners genas a function of its empirical loss on the training set
. Whether such
eralization error
a bound holds depends upon the richness of . The conditions ensuring convergence between
and
are by now well understood; for Boolean function learning (
, discrete
loss), convergence is controlled by the VC-dimension1 of :

) W * X  # 

)*+  # 
) * +  # 

#
W) *X  # 

# C

) *+  # 

) W *X  # 
" Bfi

fi and suppose  
probability distribution on 
 	ff
fi
6fifi	 fibe isanygenerated
^
times from  fi according to  . Let
f$  =   . Then with probabilityby atsampling
least : (over the choice of the training set  ), all
# C will satisfy

 >
^
#
#

f


W
(5)
)* +   )* X  ffKO^     f K 

Theorem 1. Let

Proofs of this result may be found in Vapnik (1982), Blumer et al. (1989), and will not be
reproduced here.

aJ3

 











1. The VC dimension of a class of Boolean functions
is the largest integer such that there exists a subset
such that the restriction of to contains all Boolean functions on .

156

 

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

)*+  # 

) *+  # 

) W *X  # 

Theorem 1 only provides conditions under which the deviation between
and
is
will actually be small. This is
likely to be small, it does not guarantee that the true error
governed by the choice of . If contains a solution with small error and the learner minimizes
error on the training set, then with high probability
will be small. However, a bad choice of
will mean there is no hope of achieving small error. Thus, the bias of the learner in this model2
is represented by the choice of hypothesis space .

)*+  # 

2.2 The Bias Learning Model
The main extra assumption of the bias learning model introduced here is that the learner is embedded in an environment of related tasks, and can sample from the environment to generate multiple
training sets belonging to multiple different tasks. In the above model of ordinary (single-task)
on
. So in the bias learning
learning, a learning task is represented by a distribution
model, an environment of learning problems is represented by a pair
where is the set of
(i.e., is the set of all possible learning problems), and is a
all probability distributions on
distribution on . controls which learning problems the learner is likely to see3 . For example, if
the learner is in a face recognition environment, will be highly peaked over face-recognition-type
problems, whereas if the learner is in a character recognition environment will be peaked over
character-recognition-type problems (here, as in the introduction, we view these environments as
sets of individual classification problems, rather than single, multiple class classification problems).
Recall from the last paragraph of the previous section that the learners bias is represented by its
choice of hypothesis space . So to enable the learner to learn the bias, we supply it with a family
.
or set of hypothesis spaces
Putting all this together, formally a learning to learn or bias learning problem consists of:

Y

"

Y Z



" Y fi Z 

Z

Y

Z

Z

A $  

E

an input space

E

a loss function



and an output space

U$ "s"&D ,
E an environment  Y fi Z  where Y
Y
a distribution on ,
E

a hypothesis space family

"

(both of which are separable metric spaces),

is the set of all probability distributions on

A  

where each

U

U

C A

From now on we will assume the loss function has range
we assume that is bounded.



D"

is a set of functions

and

#%$ &"

Z

is

.

 fi6 , or equivalently, with rescaling,

2. The bias is also governed by how the learner uses the hypothesis space. For example, under some circumstances the
learner may choose not to use the full power of (a neural network example is early-stopping). For simplicity in
this paper we abstract away from such features of the algorithm and assume that it uses the entire hypothesis space
.
3. s domain is a -algebra of subsets of . A suitable one for our purposes is the Borel -algebra
generated
by the topology of weak convergence on . If we assume that and are separable metric spaces, then is also
a separable metric space in the Prohorov metric (which metrizes the topology of weak convergence) (Parthasarathy,
1967), so there is no problem with the existence of measures on
. See Appendix D for further discussion,
particularly the proof of part 5 in Lemma 32.













157








Q



fiBAXTER

We define the goal of a bias learner to be to find a hypothesis space
following loss:

C A

minimizing the

)*6[   $     =   )*+  #  f Z   
(6)
   =    U  # 	fi= f  	fi= f Z   
Z
#
The only way )*[   can be small is if, with high -probability, contains a good solution to
Z
any problem  drawn at random according to . In this sense )*[   measures how appropriate
Y Z
the bias embodied by is for the environment  fi  .
Z
In general the learner will not know , so it will not be able to find an minimizing )*[  

Z
Y
times from according to to yield:
c

 
 fifi  .





E Sample ^ times from S" according to each  to yield:
 B	 
 fi 
 fi	  fi   .
E The resulting P training setshenceforth called an 	P]fi^_ -sample if they are generated by the
above processare supplied to the learner. In the sequel, an 	P]fi^_ -sample will be denoted
by \ and written as a matrix:
	ff

fi

  	ff
fi
 j

..
..
..
..
\ $
(7)
.
.
.
	 c 
 fi c 
   	 c  fi c  !. c
c
An 	P]fi^_ -sample is simply P training sets  
 fifia sampled from P different learning tasks
c
 
fifi  , where each task is selected according to the environmental probability distribution Z .
The size of each training set is kept the same primarily to facilitate the analysis.
'C A .
Based on the information contained in \ , the learner must choose a hypothesis space
\
directly. However, the learner can sample from the environment in the following way:

E

Sample

P

One way to do this would be for the learner to find an
this is defined by:

minimizing the empirical loss on , where

) W *a `   
 
[

)
*
c
 
 fifi 

c





) W * `   $  P 
 3 R  )W * X?  # 

(8)

)*6[  

Note that
is simply the average of the best possible empirical error achievable on each
training set , using a function from . It is a biased estimate of
. An unbiased estimate of
would require choosing an with minimal average error over the distributions
, where this is defined by
.
As with ordinary learning, it is likely there are more intelligent things to do with the training data
than minimizing (8). Denoting the set of all
-samples by
, a general bias
learner is a map that takes
-samples as input and produces hypothesis spaces
as
output:

\

V

	P]fi^_


c  c  
  R 3    # 
) *+
	P]fi^_

. c 2 5 
V $  D

 s"
& A
'
c  
158

 Ss"  . c 2  5

P

SC A

(9)

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

V

(as stated, is a deterministic bias learner, however it is trivial to extend our results to stochastic
learners).
Note that in this paper we are concerned only with the sample complexity properties of a bias
learner ; we do not discuss issues of the computability of .
Since is searching for entire hypothesis spaces within a family of such hypothesis spaces
, there is an extra representational question in our model of bias learning that is not present in
is represented and searched by . We defer this
ordinary learning, and that is how the family
discussion until Section 2.5, after the main sample complexity results for this model of bias learning
have been introduced. For the specific case of learning a set of features suitable for an environment
of related learning problems, see Section 3.
Regardless of how the learner chooses its hypothesis space , if we have a uniform bound (over
all
) on the probability of large deviation between
and
, and we can compute
an upper bound on
, then we can bound the bias learners generalization error
.
With this view, the question of generalization within our bias learning model becomes: how many
tasks ( ) and how many examples of each task ( ) are required to ensure that
and
are close with high probability, uniformly over all
? Or, informally, how many tasks and how
many examples of each task are required to ensure that a hypothesis space with good solutions to
all the training tasks will contain good solutions to novel tasks drawn from the same environment?
It turns out that this kind of uniform convergence for bias learning is controlled by the size
of certain function classes derived from the hypothesis space family , in much the same way as
the VC-dimension of a hypothesis space
controls uniform convergence in the case of Boolean
function learning (Theorem 1). These size measures and other auxiliary definitions needed to
state the main theorem are introduced in the following subsection.

V

A

V

V

V

A

C A
P

) W *a`  

) W *`  

^

)*6[  

) W *`  

C A

)*[  
)*[  

A

#%$ &D" , define #=b$ "&( fi6 by
#b 	fi= $  U  # 	9fiR
(10)
For any hypothesis space in the hypothesis space family A , define
bff$ B #=b$# CT j
(11)
c
#
# c  , define  # 
fifi # c  b$  DT"  &( fi6 by
For any sequence of P hypotheses  
fifi
c   

 # 
 fifi #Rc  b 	 
 fi 
 fifi c fi c  $  P   U  # 	 fi 
(12)


db
#
#=c  b . For any in the hypothesis space family A , define
We will also use to denote  
 fifi
cb $ B # 
fifi # c  b$# 
fifi # c CT j
(13)
2.3 Covering Numbers

Definition 1. For any hypothesis

Define

cb $   cb 
A
 
159

(14)

fiBAXTER

#$  & "
b
b
P

"&( fi6

#b

In the first part of the definition above, hypotheses
are turned into functions
mapping
by composition with the loss function.
is then just the collection of all
such functions where the original hypotheses come from .
is often called a loss-function class.
In our case we are interested in the average loss across tasks, where each of the hypotheses
is chosen from a fixed hypothesis space . This motivates the definition of
and
. Finally,
is the collection of all
, with the restriction that all
belong to a single
hypothesis space
.

c
A b

C A

Definition 2. For each

 # 
fifi # c  b

C A

, define

For the hypothesis space family

A

Te $ Y (
&  fi6 by
e    $  3 R  )* +  # 

, define

db
# 
fifi # c

P c
b

(15)

e $B
  e $ C A j
A
(16)
cb
e that controls how large the 	P]fi^_ -sample \ must be to ensure
It is the size of A
and A
)W *a`   and )*6[   are close uniformly over all C A . Their size will be defined in terms of
certain covering
cb numbers, and for this we neede to define how to measure the distance between
elements of A
and also between elements of A .
n   
 fifi  c  be any sequence of P probability distributions on DT" . For
Definition 3. Let
c
db d b C A b , define
any fi y
flg  dQb fi d yb  $ M .  5 db 	 
 fi 
 fifi c fi c ;: d yb 	 
 fi 
 fifi c fi c  
(17)
f  
 	 
 fi 
  f  c 	 c fi c 
Z Y
e e C A e , define
Similarly, for any distribution on and any 
 fi >
f [  e
 fi e  $     e
   ;: e     f Z   
(18)
>
>
fg and f [ are pseudo-metrics on A cb and A e respectively.
It is easily verified that
e f
Te Te
eDC A e ,


Definition 4. An i -cover of  A fi [  is a set  
 fifi   such that for all
f [  Te fi e Ti for some  ; . Note that we do not require the Te to be contained in
h
Y
e f
A e , just that they be measurable functions
on . Let Jijfi A fi [  denote the size of the smallest
e
such cover. Define the capacity of A
by
k Jilfi A e  $ 
   h Jilfi A e fi f [ 
(19)
[
Y . h Jijfi A cb fi f g  is defined in a similar
where the supremum is over all probability measures on
c
fg in place of f [ . Define the capacity of A b by:
way, using
k Jijfi A cb  $ g  h Jilfi A cb fi fg 
(20)
where now the supremum is over all sequences of P probability measures on S" .
  R	 ff
    .
4. A pseudo-metric  is a metric without the condition that 
4

160

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

2.4 Uniform Convergence for Bias Learners
Now we have enough machinery to state the main theorem. In the theorem the hypothesis space
family is required to be permissible. Permissibility is discussed in detail in Appendix D, but note
that it is a weak measure-theoretic condition satisfied by almost all real-world hypothesis space
families. All logarithms are to base .

Y



"



Z

\
	P]fi^_

"




Z
c
Y
P
^
 
 fi 3fififiP




	



fi




fi







fi
	



fi














A
P 

k  e 






Pfi
i >     > fi A fi i >  fi
and the number of examples ^ of each task satisfies
k    fi A cb 






^!fi
Pffi  >   >  fi"i $> # fi
C A will satisfy
then with probability at least : (over the 	P]fi^_ -sample \ ), all
)*[  % )W *a`  9K i

Theorem 2. Suppose
and are separable metric spaces and let be any probability distribution on , the set of all distributions on
. Suppose is an
-sample generated by
sampling times from according to to give
, and then sampling times from each
to generate
,
. Let
be any permissible
hypothesis space family. If the number of tasks satisfies





(21)

(22)

(23)

Proof. See Appendix A.

k Jijfi A cb 
	P]fi^_ A

There are several important points to note about Theorem 2:

k J ijfi A e 

)* [  
# Cs
)*O[  

1. Provided the capacities
and
are finite, the theorem shows that any bias
can bound its generalisation error
in
learner that selects hypothesis spaces from
terms of
for sufficiently large
-samples . Most bias learners will not find the
exact value of
because it involves finding the smallest error of any hypothesis
on each of the training sets in . But any upper bound on
(found, for example
by gradient descent on some error function) will still give an upper bound on
. See
Section 3.3.1 for a brief discussion on how this can be achieved in a feature learning setting.

) W *a`  W  
)*aP `

) W *`  

\

 

)

*
[
P

C A

\

) W *a`  

^

and
are close uniformly over all
2. In order to learn bias (in the sense that
), both the number of tasks and the number of examples of each task
must
be sufficiently large. This is intuitively reasonable because the bias learner must see both
sufficiently many tasks to be confident of the nature of the environment, and sufficiently
many examples of each task to be confident of the nature of each task.

C A
Z
^

) W * `  

3. Once the learner has found an
with a small value of
, it can then use
to
learn novel tasks drawn according to . One then has the following theorem bounding the
sample complexity required for good generalisation when learning with (the proof is very
similar to the proof of the bound on in Theorem 2).



161

fiBAXTER

	 
 fi 
 fifi	  fi  


ilfi "  ijfi M
^

k ' b 
^ fi   i >    
) ( fi fi i > 
!
(24)
# CT will satisfy
then with probability at least %:  , all
)*+  # % )W *X  # K ij
k
The capacity Jilfi  appearing in equation (24) is defined in an analogous
 #b 	fito=the:
f #b # b $  * fashion
capacities in Definition 4 (we just use the pseudo-metric +  fi y  +

# yb 	QfiR  f  	fi= ). The important thing to note about Theorem 3 is that the number
of ex-

Theorem 3. Let
be a training set generated by sampling from
according to some distribution . Let be a permissible hypothesis space. For all
&%
with %
, if the number of training examples satisfies

amples required for good generalisation when learning novel tasks is proportional to the logarithm of the capacity of the learnt hypothesis space . In contrast, if the learner does not
do any bias learning, it will have no reason to select one hypothesis space
over any
other and consequently it would have to view as a candidate solution any hypothesis in any
of the hypothesis spaces
. Thus, its sample complexity will be proportional to the
capacity of ,
, which in general will be considerably larger than the capacity
. So by learning the learner has learnt to learn in the environment
of any individual
in the sense that it needs far smaller training sets to learn novel tasks.

Y fi Z 

C A

C
    b  A b
 A
C A

 :_
W) *a`  ff K i
)*[   
0
*   $ 3 R   ) * +  # 1fi

W *a`  
)

=    )*+  # 

4. Having learnt a hypothesis space
with a small value of
, Theorem 2 tells us that
with probability at least
, the expected value of
on a novel task will be
less than
. Of course, this does not rule out really bad performance on some tasks
. However, the probability of generating such bad tasks can be bounded. In particular,
note that
is just the expected value of the function
over , and so by Markovs
inequality, for -/. ,



e

- #

 0 * 
 2[
3

)
6
*
[
 
W
 )*` 

e

$ e  

-


9 Ki

fi
-



Y



(with probability

%: ).

ijfi

5. Keeping the accuracy and confidence parameters
fixed, note that the number of examples
required of each task for good generalisation obeys

^  F  P    k  ijfi A cb   
(25)
c
 k Jilfi A b  increases sublinearly with P , the upper bound on the number of
So provided  

examples required of each task will decrease as the number of tasks increases. This shows
that for suitably constructed hypothesis space families it is possible to share information
between tasks. This is discussed further after Theorem 4 below.
162

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

2.5 Choosing the Hypothesis Space Family

) *[  
	P]fi^_
\

A

.

) W * `  

)* [  
A C A
P A ^

Theorem 2 only provides conditions under which
and
are close, it does not guarantee that
is actually small. This is governed by the choice of . If contains a hypothesis
space with a small value of
and the learner is able to find an
minimizing error on
sample (i.e., minimizing
), then, for sufficiently large and , Theorem 2 enthe
sures that with high probability
will be small. However, a bad choice of will mean there
is no hope of finding an with small error. In this sense the choice of represents the hyper-bias
of the learner.
Note that from a sample complexity point of view, the optimal hypothesis space family to choose
that contains good solutions to all of the
is one containing a single, minimal hypothesis space
problems in the environment (or at least a set of problems with high -probability), and no more.
For then there is no bias learning to do (because there is no choice to be made between hypothesis
spaces), the output of the bias learning algorithm is guaranteed to be a good hypothesis space for
the environment, and since the hypothesis space is minimal, learning any problem within the environment using will require the smallest possible number of examples. However, this scenario
is analagous to the trivial scenario in ordinary learning in which the learning algorithm contains a
single, optimal hypothesis for the problem being learnt. In that case there is no learning to be done,
just as there is no bias learning to be done if the correct hypothesis space is already known.
At the other extreme, if contains a single hypothesis space consisting of all possible functions from
then bias learning is impossible because the bias learner cannot produce a
restricted hypothesis space as output, and hence cannot produce a hypothesis space with improved
sample complexity requirements on as yet unseen tasks.
Focussing on these two extremes highlights the minimal requirements on for successful bias
must be strictly smaller than the space of all
learning to occur: the hypothesis spaces
functions
, but not so small or so skewed that none of them contain good solutions to a
large majority of the problems in the environment.
It may seem that we have simply replaced the problem of selecting the right bias (i.e., selecting
the right hypothesis space ) with the equally difficult problem of selecting the right hyper-bias (i.e.,
the right hypothesis space family ). However, in many cases selecting the right hyper-bias is far
easier than selecting the right bias. For example, in Section 3 we will see how the feature selection
problem may be viewed as a bias selection problem. Selecting the right features can be extremely
difficult if one knows little about the environment, with intelligent trial-and-error typically the best
one can do. However, in a bias learning scenario, one only has to specify that a set of features should
exist, find a loosely parameterised set of features (for example neural networks), and then learn the
features by sampling from multiple related tasks.

)*[ 
)*[



W *a`  
)

 

A

A

Z

 & "

A

A

C A

'&"

A

2.6 Learning Multiple Tasks

P

Y fi Z 
A

It may be that the learner is not interested in learning to learn, but just wants to learn a fixed set
. As in the previous section, we assume the learner starts
of tasks from the environment
out with a hypothesis space family , and also that it receives an
-sample generated from
the distributions
. This time, however, the learner is simply looking for hypotheses
, all contained in the same hypothesis space , such that the average generalization
error of the hypotheses is minimal. Denoting
by and writing
,

P
# 
 fifi #Rc 

P

 
 fifi  c

	P]fi^_

 # 
 fifi #=c 

163

d

\

P

n    
 fifi  c 

fiBAXTER

c






) * g  d  $  P 
 ) *+  # 
(26)
c


 P     U  # 	fi= f  	 fi=fi


d
and the empirical loss of on \ is
c


W)*a`  d  $  P   )W *X?  # 
(27)

c    
 P    ^    U  # 	 4 fi 4 

 4 

#
#=c  , if we can prove a uniform bound on
As before, regardless of how the learner chooses  
 fifi
g
d
d
#
# c  that perform
W
the probability of large deviation between )* `   and )*   then any  
fifi
well on the training sets \ will with high probability perform well on future examples of the same

this error is given by:

tasks.

n 
   
fifi  c  P
^

" 


\

A   

	P]fi^_

Theorem 4. Let
be probability distributions on
and let be an
according to each . Let
be any
sample generated by sampling times from
permissible hypothesis space family. If the number of examples of each task satisfies

"

^

k  
) ( fi A cb  






 ffP  i  > 
5
^!fi
(28)
 fi i > #
d C A c will satisfy
then with probability at least : (over the choice of \ ), any
(29)
)* g  d % )W *a`  d ffK i
c
k b
(recall Definition 4 for the meaning of Jilfi A  ).
Proof. Omitted (follow the proof of the bound on ^ in Theorem 2).
The bound on ^ in Theorem 4 is virtually identical to the bound on ^ in Theorem 2, and note
again that it depends inversely on the number of tasks P (assuming that the first part of the max
k  cb
expression is the dominate one). Whether this helps depends on the rate of growth of  )
 ( fi A  as
a function of P . The following Lemma shows that this growth is always small enough to ensure that
we never do worse by learning multiple tasks (at least in terms of the upper bound on the number of
examples required per task).

A ,


k	 li fi A b   k H ilfi A cb  	k  li fi A b
  c 

Lemma 5. For any hypothesis space family

164

(30)

fiA M ODEL OF I NDUCTIVE B IAS L EARNING


#
#
R
#
c
b
Proof. Let 6 denote the set of all functions  
 fifi c  where each can be
k Hijfi A cb  a member
k Jijfi 6 of . any
!C A (recall Definition 1). Then
87
b
c
A
6
and
so
By
hypothesis space


k
k
	
 ilfi

b
H

j
i
fi



Lemma 29 in Appendix B,
6
A and so the right hand inequality follows.
n be the meaFor the first inequality,
let  be any probability measure on  " and let
c
sure on  !<"  obtained by using  on the first copy of  <"
cb flg in the product,#and
b C ignoring
b
 and
all other elements of the product. Let
be an i -cover for  A fi
. Pick any
A
c b C be such that fg  # fi # fifi #  b fi:9 
 fifi;9 c  b  i . But by construction,
let :9 
 fifi;9 
flg  # fi # fi3fi #  b fi: 9 
 fi;fi 9 c  b  f +  # fi: 9 
  b  , which establishes the first inequality.
  k  ji fi A b
     k Jijfi A cb P   k  ijfi A b
  
(31)
So keeping the accuracy parameters i and  fixed, and plugging (31) into (28), we see that the upper
bound on the number of examples required of each task never increases with the number of tasks,
and at best decreases as F NPQ . Although only an upper bound, this provides a strong hint that
By Lemma 5

learning multiple related tasks should be advantageous on a number of examples required per task
basis. In Section 3 it will be shown that for feature learning all types of behavior are possible, from
decrease.
no advantage at all to
2.7 Dependence on

i

F NPQ

Ni >

Ni

In Theorems 2, 3 and 4 the bounds on sample complexity all scale as
. This behavior can be
improved to
if the empirical loss is always guaranteed to be zero (i.e., we are in the realizable
case). The same behavior results if we are interested in relative deviation between empirical and
true loss, rather than absolute deviation. Formal theorems along these lines are stated in Appendix
A.3.

3. Feature Learning
The use of restricted feature sets is nearly ubiquitous as a method of encoding bias in many areas of
machine learning and statistics, including classification, regression and density estimation.
In this section we show how the problem of choosing a set of features for an environment of
related tasks can be recast as a bias learning problem. Explicit bounds on
and
are calculated for general feature classes in Section 3.2. These bounds are applied to the problem of
learning a neural network feature set in Section 3.3.

k  A e fiai3

k  A cb fiai3

3.1 The Feature Learning Model
Consider the following quote from Vapnik (1996):
The classical approach to estimating multidimensional functional dependencies is
based on the following belief:
Real-life problems are such that there exists a small number of strong features, simple
functions of which (say linear combinations) approximate well the unknown function.
Therefore, it is necessary to carefully choose a low-dimensional feature space and then
to use regular statistical techniques to construct an approximation.
165

fiBAXTER

q$  &
o

q


fi






fi




q
q
q

q
p
"
q C_o
pTr q $ B r q $ C p 
A
s
p
r
$
$

C
o
(32)
A  q q j
Now the problem of carefully choosing the right features q is equivalent to the bias learning
C A . Hence, provided the
problem find the right hypothesis space
k e
k cb learner is embedded within
an environment of related tasks, and the capacities  A fiai and  A fiai are finite, Theorem 2 tells
us that the feature set q can be learnt rather than carefully chosen. This represents an important
simplification, as choosing a set of features is often the most difficult part of any machine learning
problem.
k e
k cb
In Section 3.2 we give a theorem bounding  A fiai and  A fiai3 for general feature classes.
The theorem is specialized to neural network classes in Section 3.3.
p
Note that we have forced the function class to be the same for all feature maps q , although
p
this is not necessary. Indeed variants of the results to follow can be obtained if is allowed to vary
with q .

In general a set of strong features may be viewed as a function
+< mapping the input
into some (typically lower) dimensional space < . Let
be a set of such feature
space
=
maps (each may be viewed as a set of features
). It is the that must be
>= if <
carefully chosen in the above quote. In general, the simple functions of the features may be
represented as a class of functions mapping < to . If for each
we define the hypothesis
?9
9
, then we have the hypothesis space family
space

3.2 Capacity Bounds for General Feature Classes

	fi= &
q


s"

s"
p
U
C p
 q 	fi=
U
 fi6  fiR &   fi= U
b "
pb
b
p b r o $  r q $ C p b fi q Co 
pb
k Jilfi p b  $ D
 E  h Jilfi p b fi f + 
+
f
$   9zCBfiR:
where the supremum is over all probability measures on <" , and + :9fi;9 y   *GF
9 CBfi= f
y   CBfi= . To
 define the capacity p ofb o we first define a pseudo-metric f t + 2 uvxw on o by
pulling back the H metric on  through as follows:
ft + 2 uv w  q fi q y  $    u v 9 r q 	fi=;/: 9 r q y 	QfiR f  	fi=
(33)


I 
f=t
ft
It is easily verified that + 2 uvxw is a pseudo-metric. Note that for + 2 uv w to be well defined the suprep
b
mum over in the integrand must be measurable. This is guaranteed if theh hypothesis space family
o ft
A   p ib r q $ q C o  is permissible (Lemma
32, part 4). Now define Jilfi fi + 2 uv{w8 to be the
t
pb
f
o
o
smallest -cover of the pseudo-metric space  fi + 2 u v w  and the i -capacity of (with respect to )
as
k uv Jilfi o  $ D
   h Jilfi o fi f t + 2 uv{w 
+
where the supremum is over all probability measures on -" . Now we can state the main theorem
A@
to <
by
Notationally it is easier to view the feature maps as mapping from
, and also to absorb the loss function into the definition of by viewing each 9
as a
A@
:9 CB
into
via CB
. Previously this latter function would have been
map from <
denoted 9 but in what follows we will drop the subscript where this does not cause confusion. The
class to which 9 belongs will still be denoted by .
?9
9
With the above definitions let
. Define the capacity of in
the usual way,

of this section.

166

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

ii 
 K i >

Theorem 6. Let
,

A

be a hypothesis space family as in equation (32). Then for all

k J ilfi A cb 
k Jijfi A e  

k H i 
 fi p b  c k u v Ji > fi o 
k uv Hilfi o 

ijfiai 
 fiai > . 

with
(34)
(35)

Proof. See Appendix B.
3.3 Learning Neural Network Features

f

In general, a set of features may be viewed as a map from the (typically high-dimensional) input
=
to a much smaller dimensional space
( JLK
). In this section we consider approximatspace
ing such a feature map by a one-hidden-layer neural network with input nodes and J output nodes
QP R
ON
N
=
(Figure 1). We denote the set of all such feature maps by M
where
R
is a bounded subset of TS ( U is the number of weights (parameters) in the first two layers).
This set is the of the previous section.
Each feature N
,
J is defined by



o







f
|
  | 2 
 fifi | 2  $ C 

| 2 $   &  fi6  3fifi
b 


|N 2 	9 $ VWX   B 4 # 4 	 KYB b 
[Z
(36)
 
4 


#
b 
  are the output
#
where 4 	 is the output
of the \^] node in the first hidden layer, CB 
 fifi_B

$
node parameters for the  th feature and V is a sigmoid squashing function V &  fi6 . Each
# $   &S ,  3fifi U , computes
first layer hidden node






# 	9 $ V WX

 Z[
4  4 K
(37)





a
`
`
4



  are the hidden nodes parameters. We assume V is Lipschitz. The weight
where  
fifi
`
` 
vector for the entire feature map is thus
P  

fifi 


 fifi b 
 fifi b  
 _fi B

fi_fi B 
 b  
 fi_fi B = 
 fi_fi B = b  
 



`
`
`
`
Uf
U
and the total number of feature parameters U   KffK J  K  .
p
For arguments sake, assume the simple functions of the features (the class of the previous
5

section) are squashed affine maps using the same sigmoid function V above (in keeping with the
P
neural network flavor of the features). Thus, each setting of the feature weights generates a
hypothesis space:

| $ 



  =

 
N | 2 K


 f $  
fifi

  	C R y  fi
=
=

ed
d 
d
d 


R
=
where y is a bounded subset of   . The set of all such hypothesis spaces,
A $ B | Q$ P C R 
  i > k ; h?lmgnh  i > k h for all Hkporq .
5.  is Lipschitz if there exists a constant g such that h  j
Vcb

167

(38)

(39)

fiBAXTER

Multiple Output Classes
n

k

l

Feature
Map

d
Input

P

P

	P]fi^_

Figure 1: Neural network for feature learning. The feature map is implemented by the first two
hidden layers. The output nodes correspond to the different tasks in the
sample . Each node in the network computes a squashed linear function of the nodes in
the previous layer.

\

 
 fifi 
 


=
and feature
is a hypothesis space family. The restrictions on the output layer weights
P
d
d
weights , and the restriction to a Lipschitz squashing function are needed to obtain finite upper
bounds on the covering numbers in Theorem 2.
Finding a good set of features for the environment
is equivalent to finding a good hyP
, which in turn means finding a good set of feature map parameters .
pothesis space
As in Theorem 2, the correct set of features may be learnt by finding a hypothesis space with
small error on a sufficiently large
-sample . Specializing to squared loss, in the present
framework the empirical loss of
on (equation (8)) is given by

Y fi Z 

| C A

	P]fi^_
\
\
c

 >

=









b
b
(40)
)W *`  |  P 
 .tsu62 s  2w vwRvwv 2 s^x65 >y k ^ 4 
{z VYb b 
 d N | 2 	 4 9K d  f :< 4}|
Since our sigmoid function V only has range  fi6 , we also restrict the outputs " to this range.
|

3.3.1 A LGORITHMS

FOR

F INDING

A

G OOD S ET

OF

F EATURES

Provided the squashing function V is differentiable, gradient descent (with a small variation on
P
backpropagation to compute the derivatives) can be used to find feature weights minimizing (40)
(or at least a local minimum of (40)). The only extra difficulty over and above ordinary gradient
descent is the appearance of   in the definition of
. The solution is to perform gradient
P
= for each node and the feature weights . For
descent over both the output parameters
d
d
more details see Baxter (1995b) and Baxter (1995a, chapter 4), where empirical results supporting
the theoretical results presented here are also given.

R

W * `  | 
)

  fifi 
168

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

3.3.2 S AMPLE C OMPLEXITY B OUNDS

\
k J ijfi A cb 

FOR

N EURAL -N ETWORK F EATURE L EARNING

The size of ensuring that the resulting features will be good for learning novel tasks from the same
environment is given by Theorem 2. All we have to do is compute the logarithm of the covering
numbers
and
.

k J ijfi A e 
| $aP C ff S5 be a hypothesis space family where each }| is of the form
Theorem 7. Let A 3~
 

=




| $  V b
N | 2   K
f $  
fifi =  C  =  fi

d

d
d
d
|  O N | 2 
 fifi N | 2 =  is a neural network with U weights mapping from   to  = . If the
where M
P
feature weights and the output weights 3fi 
fifi = are bounded, the squashing function V is
U
d
d
d
Lipschitz, is squared loss, and the output space "   fi6 (any bounded subset of  will do), then
there exist constants  fi =y (independent of ilfi U and J ) such that for all i .  ,
  k Jijfi A cb     J KP}K U    
(41)
i
  k Jijfi A e    U   Ri y
(42)
(recall that we have specialized to squared loss here).
Proof. See Appendix B.
Noting that our neural network hypothesis space family
into Theorem 2 gives the following theorem.

A

is permissible, plugging (41) and (42)

A  | 

|

Theorem 8. Let
be a hypothesis space family where each hypothesis space
is a
set of squashed linear maps composed with a neural network feature map, as above. Suppose the
number of features is J , and the total number of feature weights is W. Assume all feature weights and
-sample
output weights are bounded, and the squashing function V is Lipschitz. Let be an
generated from the environment
. If

\

Y fi Z 

Pfi F  i  >  U   i K   = R fi

	P]fi^_

(43)

and

^!fi F  i  >   J K K U P    i K P      R 
| C A will satisfy
then with probability at least : any
)* [  }|  )W * `  }| 9 K il
169

(44)

(45)

fiBAXTER

3.3.3 D ISCUSSION

i

F  K



NPQ

1. Keeping the accuracy and confidence parameters and fixed, the upper bound on the number
of examples required of each task behaves like J
U
. If the learner is simply learning
fixed tasks (rather than learning to learn), then the same upper bound also applies (recall
Theorem 4).

P
^

F  

P




and the upper bound on
2. Note that if we do away with the feature map altogether then U
becomes J , independent of (apart from the less important term). So in terms of the
upper bound, learning tasks becomes just as hard as learning one task. At the other extreme,
if we fix the output weights then effectively J
and the number of examples required of
each task decreases as U
. Thus a range of behavior in the number of examples required
decrease as the number of
of each task is possible: from no improvement at all to an
tasks increases (recall the discussion at the end of Section 2.6).

P



F  NPQ

P

F NPQ

3. Once the feature map is learnt (which can be achieved using the techniques outlined in Baxter,
1995b; Baxter & Bartlett, 1998; Baxter, 1995a, chapter 4), only the output weights have to be
estimated to learn a novel task. Again keeping the accuracy parameters fixed, this requires no
more that J examples. Thus, as the number of tasks learnt increases, the upper bound on
the number of examples required of each task decays to the minimum possible, J .

F  

F  

4. If the small number of strong features assumption is correct, then J will be small. However,
typically we will have very little idea of what the features are, so to be confident that the neural
network is capable of implementing a good feature set it will need to be very large, implying
UJ .
J
U
decreases most rapidly with increasing when UJ , so at least in
terms of the upper bound on the number of examples required per task, learning small feature
sets is an ideal application for bias learning. However, the upper bound on the number of
tasks does not fare so well as it scales as U .

F  K

NPQ

P

F  


A special case of this multi-task framework is one in which the marginal distribution on the input
~ is the same for each task  3fifiP , and all that varies between tasks is the conditional
space 
distribution over the output space " . An example would be a multi-class problem such as face
l3fifiP; where P is the number of faces to be recognized and the
recognition, in which " S

marginal distribution on  is simply the natural distribution over images of those faces. In that
case, if for every example  4 we havein addition to the sample  4 from the  th tasks conditional
distribution on " samples from the remaining P:  conditional distributions on " , then we can
view the P training sets containing ^ examples each as one large training set for the multi-class
problem with ^TP examples altogether. The bound on ^ in Theorem 8 states that ^TP should be
F 	P J K U  , or proportional to the total number of parameters in the network, a result we would
3.3.4 C OMPARISON

WITH

T RADITIONAL M ULTIPLE -C LASS C LASSIFICATION

expect from6 (Haussler, 1992).
So when specialized to the traditional multiple-class, single task framework, Theorem 8 is consistent with the bounds already known. However, as we have already argued, problems such as face
recognition are not really single-task, multiple-class problems. They are more appropriately viewed
6. If each example can be classified with a large margin then naive parameter counting can be improved upon (Bartlett,
1998).

170

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

P

P

as a (potentially infinite) collection of distinct binary classification problems. In that case, the goal
of bias learning is not to find a single -output network that can classify some subset of faces
well. It is to learn a set of features that can reliably be used as a fixed preprocessing for distinguishing any single face from other faces. This is the new thing provided by Theorem 8: it tells us that
provided we have trained our -output neural network on sufficiently many examples of sufficiently
many tasks, we can be confident that the common feature map learnt for those tasks will be good
for learning any new, as yet unseen task, provided the new task is drawn from the same distribution
that generated the training tasks. In addition, learning the new task only requires estimating the J
output node parameters for that task, a vastly easier problem than estimating the parameters of the
entire network, from both a sample and computational complexity perspective. Also, since we have
high confidence that the learnt features will be good for learning novel tasks drawn from the same
environment, those features are themselves a candidate for further study to learn more about the
nature of the environment. The same claim could not be made if the features had been learnt on too
small a set of tasks to guarantee generalization to novel tasks, for then it is likely that the features
would implement idiosyncrasies specific to those tasks, rather than invariances that apply across
all tasks.

P

P

^

P

When viewed from a bias (or feature) learning perspective, rather than a traditional -class
classification perspective, the bound on the number of examples required of each task takes on
a somewhat different meaning. It tells us that provided is large (i.e., we are collecting examples
of a large number tasks), then we really only need to collect a few more examples than we would
examples vs. J examples).
otherwise have to collect if the feature map was already known ( J U
So it tells us that the burden imposed by feature learning can be made negligibly small, at least when
viewed from the perspective of the sampling burden required of each task.

P

K NP

3.4 Learning Multiple Tasks with Boolean Feature Maps

i

P



Ignoring the accuracy and confidence parameters and , Theorem 8 shows that the number of
examples required of each task when learning tasks with a common neural-network feature map
J
U
is bounded above by
, where J is the number of features and U is the number of
adjustable parameters in the feature map. Since
J examples are required to learn a single task
once the true features are known, this shows that the upper bound on the number of examples
required of each task decays (in order) to the minimum possible as the number of tasks increases.
This suggests that learning multiple tasks is advantageous, but to be truly convincing we need to
)
prove a lower bound of the same form. Proving lower bounds in a real-valued setting (
is complicated by the fact that a single example can convey an infinite amount of information, so
one typically has to make extra assumptions, such as that the targets
are corrupted by a
noise process. Rather than concern ourselves with such complications, in this section we restrict
our attention to Boolean hypothesis space families (meaning each hypothesis
maps to

and we measure error by discrete loss
if
and
otherwise).

F  K

NPQ

F  

P

 C "

"  

# C A 

U  # 	9fiR # 	}  U  # 	 9fiR
"   
We show that the sample complexity for learning P tasks with a Boolean hypothesis space family
f= 	PQ (that is, we give nearly matching upper
type parameter
A is controlled by a VC dimension
f  	PQ ). We then derive bounds on f  	PQ for the hypothesis space
and lower bounds involving

family considered in the previous section with the Lipschitz sigmoid function V replaced by a hard
threshold (linear threshold networks).
171

fiBAXTER

F  

As well as the bound on the number of examples required per task for good generalization across
those tasks, Theorem 8 also shows that features performing well on U
tasks will generalize well
to novel tasks, where U is the number of parameters in the feature map. Given that for many feature
learning problems U is likely to be quite large (recall Note 4 in Section 3.3.3), it would be useful
to know that
U
tasks are in fact necessary without further restrictions on the environmental
distributions generating the tasks. Unfortunately, we have not yet been able to show such a lower
bound.
There is some empirical evidence suggesting that in practice the upper bound on the number of
tasks may be very weak. For example, in Baxter and Bartlett (1998) we reported experiments in
which a set of neural network features learnt on a subset of only 400 Japanese characters turned out
to be good enough for classifying some 2600 unseen characters, even though the features contained
several hundred thousand parameters. Similar results may be found in Intrator and Edelman (1996)
and in the experiments reported in Thrun (1996) and Thrun and Pratt (1997, chapter 8). While
this gap between experiment and theory may be just another example of the looseness inherent in
general bounds, it may also be that the analysis can be tightened. In particular, the bound on the
number of tasks is insensitive to the size of the class of output functions (the class in Section 3.1),
which may be where the looseness has arisen.

ZF

 

p

3.4.1 U PPER AND L OWER B OUNDS
S PACE FAMILIES

FOR

L EARNING  TASKS

WITH

B OOLEAN H YPOTHESIS

T	 
 fifi   C   ~ 0

~ 0 $ B # 	 
 fifi # 	   $# CT j
~ 
~ 
Clearly  0    . If  0    we say shatters  . The growth function of is defined by
  	^_ $  0 L    / ~ 0  
   is the size of the largest set shattered by :
The Vapnik-Chervonenkis dimension
=   $    ^ $  	^_   j

First we recall some concepts from the theory of Boolean function learning. Let
be a class of
.
is the set of all binary vectors obtainable
Boolean functions on and
by applying functions in to :



An important result in the theory of learning Boolean functions is Sauers Lemma (Sauer, 1972), of
which we will also make use.
Lemma 9 (Sauers Lemma). For a Boolean function class

for all positive integers

^

with

   f ,

^ fi
  	^_     ^    f

 
.

We now generalize these concepts to learning

P

172

tasks with a Boolean hypothesis space family.

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

Definition 5. Let
input space by
matrices,



^ matrices over the
Denote the P
A . bec 2  a5 Boolean hypothesis
. c 2  space
5 family.
C
C
~



A , define to be the set of (binary)
. For each 
and
 # 
 	 

 
 # 
 	 
 

$z# 
fifi # c CT  
_~  $  
..
..
..

.
.
.
#=c 	 c 
   #Rc 	 c   t




A ~ }$      ~  
  	P]fi^_ by
Now for each P . fi^ .  , define
 	P]fi^_ $   L     A ~   

c   ~   c 


	

]
P

fi
_
^
%


Note that
matrix 
 . If  A   we say A shatters the
c

f  	PQ $    ^ $   	P]fi^_  j
Define

Define

Lemma 10.

. For each

P
.



let

f  A  $  =   A 
  and
f  A  $     =   
f A  fi f A 
f 	PQ fi    f  P A   fi f  A 
#

f A  K f  
  P  A 
Proof. The first inequality is trivial from the definitions. To get the second term in the maximum
C A with =   f  A  and construct a matrix
in the second
inequality, choose an
c
.
2
5

f
 C 
whose rows are of length  A  and are shattered by . Then clearly A 
 shatters  . For

the first term in the maximum take a sequence T	 
 fifi . 5  shattered by A (the hypothesis

space consisting of the union over all hypothesis spaces from A ), and distribute its elements equally
among the rows of  (throw away any leftovers). The set of matrices
 # 	ff

O
 # 	ff



   

$
#
C
..
..

..
A

.
.
.
#
c
#
c   t

	



	











c

f
~  and has size  .
where ^   A NP is a subset of A
c .c 5
Lemma 11.
^
 	P]fi^_  f=  	PQ  ?
173

fi





fiBAXTER

P  	P]fi^_    	 P9^_
 
 fifi c 
#

#>
^
f  	 PQ =   P f  	QP 

P

# 
 fifi #Rc

Proof. Observe that for each ,
where is the collection of all Boolean
obtained by first choosing functions
from some
functions on sequences
, and then applying
to the first examples,
to the second examples and so on. By
the definition of
,
, hence the result follows from Lemma 9 applied to
.

C A

^

 k  A cb fiai3

 	 P]fi  ^_
fi

If one follows the proof of Theorem 4 (in particular the proof of Theorem 18 in Appendix
A) then it is clear that for all .
,
may be replaced by
in the Boolean
E
case. Making this replacement in Theorem 18, and using the choices of
from the discussion
d
following Theorem 26, we obtain the following bound on the probability of large deviation between
empirical and true performance in this Boolean setting.

n    
 fifi  c  P
   and let \ be an


	P]fi^_
^
     

 . Let A B 
0
* \ $=d C A c $ )* g  d  fi )W a* `  d ffK ij    	P]fi  ^_ )   :  > P9^_N   

(46)
Corollary 13. Under the conditions of Theorem 12, if the number of examples ^ of each task


be probability distributions on
Theorem 12. Let

-sample generated by sampling times from
according to each
be any permissible Boolean hypothesis space family. For all % 
,

satisfies

^ fi i >   f  	PQ   i  K P   
!
d
then with probability at least : (over the choice of \ ), any
)* g  d % )W *a`  d ffK i



C A c

(47)
will satisfy
(48)

Proof. Applying Theorem 12, we require

   	 P]fi  ^_ )   :  > P9^_N    fi

which is satisfied if

^!fi  >  f  	 PQ  f   	^ PQ K P      fi

I fiM , if
where we have used Lemma 11. Now, for all m
^    K   I    K   Ifi



f  	PQNi > , (49) is satisfied if
then ^!fiI  ^ . So setting IL  
^!fi i >   f  	PQ   i  K P      
174

(49)

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

Corollary 13 shows that any algorithm learning
requires no more than

P

tasks using the hypothesis space family

^  F  i  >  =f  	 PQ   i K P      R

c

i

A

(50)

\

P

examples of each task to ensure that with high probability the average true error of any hypotheses
it selects from
is within of their average empirical error on the sample . We now give a
theorem showing that if the learning algorithm is required to produce hypotheses whose average
true error is within of the best possible error (achievable using
) for an arbitrary sequence of
distributions
, then within a
 factor the number of examples in equation (50) is also
necessary.

For any sequence
of probability distributions on
, define
by

A

i
 
fifi  c

= g  A c 

A
  

n    
 fifi  c  P
 g  A c  $   R   )* g  d 

c P



 


 contains at least two
A Pbe3afi Boolean
hypothesis space family such that A
fifi let V c be any learning algorithm taking as input 	P]fi^_c  -samples
c
.
2
5


C
\  (   and producing as output P hypotheses d S # 
 fifi #=c  C A . For all
%i%MN   and  %  %MN   , if
^ % i  >  f   	PQ KM:i >  P       : 3
!

 

c
n
   
fifi   such that with probability at least  (over the
then there exist distributions
\
random choice of ),
g)*  V c J\ . = g  A c ffK i

Theorem 14. Let
functions. For each

Proof. See Appendix C

   Ni3
f  	PQ A

3.4.2 L INEAR T HRESHOLD N ETWORKS

P
f  	PQ

factor, the sample complexity of
Theorems 13 and 14 show that within constants and a
learning tasks using the Boolean hypothesis space family is controlled by the complexity parameter
. In this section we derive bounds on
for hypothesis space families constructed
as thresholded linear combinations of Boolean feature maps. Specifically, we assume is of the
form given by (39), (38), (37) and (36), where now the squashing function V is replaced with a hard
threshold:

if fi
V
otherwise

	 $  :  

Rfi

A

fi

fi Ry

and we dont restrict the range of the feature and output layer weights. Note that in this case the
proof of Theorem 8 does not carry through because the constants   in Theorem 7 depend on the
Lipschitz bound on V .

A

f U

Theorem 15. Let be a hypothesis space family of the form given in (39), (38), (37) and (36), with
a hard threshold sigmoid function V . Recall that the parameters , and J are the input dimension,
number of hidden nodes in the feature map and number of features (output nodes in the feature map)
175

fiBAXTER

$  U  f K]K J  U K (the number of adjustable parameters in the feature
f  	PQ    U K J K       J K U Kz
P
>
Proof. Recall that
. c 2  for5 eachM | P ~  C TS , M | $   &( = denotes the feature map with parameters P .
C

For each 
, let
denote the matrix
 M | 	

6
 M | 	

.
.

.
|M 	.. c 
  . . M | 	 .. c  t 
~  is the set of all binary P ^ matrices obtainable by composing thresholded linear
Note that A
M | ~

respectively. Let U
map). Then,

functions with the elements of
, with the restriction that the same function must be applied to
each element in a row (but the functions may differ between rows). With a slight abuse of notation,
define

 	P]fi^_ $       


~ M


| ~  $aP C 

S



 



. c 2 5
C
Fix 
. By Sauers
 Lemma, each node in the first hidden layer of the feature map computes

f 5   functions on the P9^ input vectors in  . Thus, there can be at most
at most   ^TPQNb  . K
  ^TPQN f K  
 distinct functions from the input to the output of the first hidden layer on
the P9^ points in  . Fixing the first hidden layer
U b 
 parameters, each node in the second layer of. b the5
feature map computes at most   ^TPQN K  functions on the image of  produced at the output
U = 

of the first hidden layer. Thus the second hidden layer computes no more than   ^TPQN K 
functions on the output of the first hidden layer on the P9^ points in  . So, in total,
b . 
 5 ^TP = . b 
 5
T
^
P
  	P]fi^_   f  KR   U  K  

| ~  , the number of functions computable on each row of M | ~  by a
Now, for each possible matrix M


=
thresholded linear combination of the output of the feature map is at most   ^_N J K  . Hence,
c . = 
 5 obtainable by applying linear threshold functions to all the
the number of binary sign assignments
 . Thus,
rows is at most   ^_N J K 
b. 
5
c . 
5
.b 
 5
  	P]fi^_  f  ^TKP     U ^TKP   =   P  ^TKP   =  
J
$

q 	    is a convex function, hence for all IfiGfi  .  ,
IK U GY
K 

J
q  J K U K   J K U K  JRq JIffK U q HGffK q  
b
b 
U
=
=}
K

K








J

fi
 J IK U G]Kc 
I G  
U K , G f K and  P J K shows that
Substituting I 
c .= 
 5
U
T
^

P

K

K




 
S
 	P]fi^_   K J P K 
(51)
U

J

176

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

Hence, if

^TP J K U K 





K
K

(52)
.  P
J
 >  U K P J K 
 	P]fi^_%  c  and so by definition f  	PQ^ . For all I .  , observe that  . I   > 
then

U KN U K P J K and I    J K U K shows that
if T  I  >  I . Setting T  ^TP J K

U K .
(52) is satisfied if ^    U NP}K J K  >    J K
f
U
Theorem 16. Let A be as in Theorem 15 with the following extra restrictions: fi  , fi J and
f
 . Then
J
f  	PQ fi    U P  K J K 
 


f
f
Proof. We bound  A  and  A  and then apply Lemma 10. In the present setting A contains all
f
U
three-layer linear-threshold networks with input nodes, hidden nodes in the first hidden layer, J
^

U

hidden nodes in the second hidden layer and one output node. From Theorem 13 in Bartlett (1993),
we have

=   A 
 fi lf U K U  J : K3fi

f
which under the restrictions stated above is greater than U N  . Hence  A fi
 f Ufi

U

N.

f :

As J
and
J we can choose a feature weight assignment so that the feature map is the
J
identity on J components of the input vector and insensitive to the setting of the reminaing
components. Hence we can generate J
points in
whose image under the feature map is
J
shattered by the linear threshold output node, and so
.

K

f  A  ]  K

Combining Theorem 15 with Corrolary 13 shows that

^!fi F  i  >   U P K J K    i K P     
examples of each task suffice when learning P tasks using a linear threshold hypothesis space family,
while combining Theorem 16 with Theorem 14 shows that if

^   i  >   U P K J K   K P      
then any learning algorithm will fail on some set of P tasks.
4. Conclusion
The problem of inductive bias is one that has broad significance in machine learning. In this paper
we have introduced a formal model of inductive bias learning that applies when the learner is able
to sample from multiple related tasks. We proved that provided certain covering numbers computed
from the set of all hypothesis spaces available to the bias learner are finite, any hypothesis space
that contains good solutions to sufficiently many training tasks is likely to contain good solutions to
novel tasks drawn from the same environment.
In the specific case of learning a set of features, we showed that the number of examples
J
U
required of each task in an -task training set obeys
, where J is the number of

P

^ F  K

177

NPQ

^

fiBAXTER

features and U is a measure of the complexity of the feature class. We showed that this bound is
essentially tight for Boolean feature maps constructed from linear threshold networks. In addition,
we proved that the number of tasks required to ensure good performance from the features on novel
tasks is no more than U . We also showed how a good set of features may be found by gradient
descent.
The model of this paper represents a first step towards a formal model of hierarchical approaches
to learning. By modelling a learners uncertainty concerning its environment in probabilistic terms,
we have shown how learning can occur simultaneously at both the base levellearn the tasks at
handand at the meta-levellearn bias that can be transferred to novel tasks. From a technical
perspective, it is the assumption that tasks are distributed probabilstically that allows the performance guarantees to be proved. From a practical perspective, there are many problem domains that
can be viewed as probabilistically distributed sets of related tasks. For example, speech recognition
may be decomposed along many different axes: words, speakers, accents, etc. Face recognition
represents a potentially infinite domain of related tasks. Medical diagnosis and prognosis problems
using the same pathology tests are yet another example. All of these domains should benefit from
being tackled with a bias learning approach.
Natural avenues for further enquiry include:

E

F  

A

Alternative constructions for . Although widely applicable, the specific example on feature
learning via gradient descent represents just one possible way of generating and searching
the hypothesis space family . It would be interesting to investigate alternative methods,
including decision tree approaches, approaches from Inductive Logic Programming (Khan
et al., 1998), and whether more general learning techniques such as boosting can be applied
in a bias learning setting.

E

A

A

Algorithms for automatically determining the hypothesis space family . In our model the
structure of
is fixed apriori and represents the hyper-bias of the bias learner. It would
be interesting to see to what extent this structure can also be learnt.

E

E

A

Algorithms for automatically determining task relatedness. In ordinary learning there is usually little doubt whether an individual example belongs to the same learning task or not.
The analogous question in bias learning is whether an individual learning task belongs to a
given set of related tasks, which in contrast to ordinary learning, does not always have such
a clear-cut answer. For most of the examples we have discussed here, such as speech and
face recognition, the task-relatedness is not in question, but in other cases such as medical
problems it is not so clear. Grouping too large a subset of tasks together as related tasks could
clearly have a detrimental impact on bias-learning or multi-task learning, and there is emprical evidence to support this (Caruana, 1997). Thus, algorithms for automatically determining
task-relatedness are a potentially useful avenue for further research. In this context, see Silver
and Mercer (1996), Thrun and OSullivan (1996). Note that the question of task relatedness
is clearly only meaningful relative to a particular hypothesis space family (for example, all
possible collections of tasks are related if contains every possible hypothesis space).

A

A

Extended hierarchies. For an extension of our two-level approach to arbitrarily deep hierarchies,
see Langford (1999). An interesting further question is to what extent the hierarchy can
be inferred from data. This is somewhat related to the question of automatic induction of
structure in graphical models.
178

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

Acknowledgements
This work was supported at various times by an Australian Postgraduate Award, a Shell Australia Postgraduate Fellowship, U.K Engineering and Physical Sciences Research Council grants
K70366 and K70373, and an Australian Postdoctoral Fellowship. Along the way, many people
have contributed helpful comments and suggestions for improvement including Martin Anthony,
Peter Bartlett, Rich Caruana, John Langford, Stuart Russell, John Shawe-Taylor, Sebastian Thrun
and several anonymous referees.

Appendix A. Uniform Convergence Results
Theorem 2 provides a bound (uniform over all  ) on the probability of large deviation between
?1
?p
	 and 
	 . To obtain a more general result, we follow Haussler (1992) and introduce the
following parameterized class of metrics on A :
m

 
 8c


"  _"e


. Our main theorem will be a uniform bound on the probability of large values
"1 ?1
 ?p

 ?E"
?1
	

  . Theorem 2 will then follow as a corollary,
	
	 , rather than 
D
?E"
will better bounds for the realizable case  
(Appendix A.3).
p

where

Lemma 17. The following three properties of

3. For

are easily established:

E

1. For all 
2. For all

of
as

n

L


p E 
,

 "1 E?"1 _


,



E

,

"  E ?

 }



and

" ^_p  _ 


.


 

For ease of exposition we have up until now been dealing explicitly with hypothesis spaces 
Q!
	3 j}
containing functions 
, and then constructing loss functions Q mapping
 _
    _
A3 j}
by Q
. However, in general we can view



 for some loss function
+
 j}
Q just as a function from an abstract set  (
) to
and ignore its particular construction

in terms of the loss function . So for the remainder of this section, unless otherwise stated, all
 j}
. It will also be considerably more
hypothesis spaces  will be sets of functions mapping  to
_



convenient to transpose our notation for C
 -samples, writing the
training sets as columns
instead of rows:


 ...  . .. ...
 	    
 (Equation 9 and prior discussion),
where each 
fiff
. Recalling the definition
of





with this transposition lives in
. The following definition now generalizes quantities


like
,
and so on to this new setting.

Definition 6. Let  
of functions mapping
into
. For any 
    , let  be  orsetssimply
the map
 denote

    
   
 





 





?  

	





?











































179







 j}





fiBAXTER

      . Let !"#   denote the set of all such functions. Given
 $ %     &  and elements
of
,      (or equivalently an element of
by writing the  
 as rows), define
 



'   

(recall equation
 , define(8)). Similarly, for any product probability measure ( *)   +)  on
  -,#.0/   ( 

(recall equation (26)). For any  21
(not necessarily of the form 345  ),
define
   1 %, . /    1  ( 
 to , define
(recall equation (17)). For any class of functions mapping
6 57 98;:=?< > @7 
 and 7  is the
where the supremum is over all product probability measures on
>
size of the smallest 7 -cover of under  (recall Definition 4).


for all

5 
	























	
















?"













	



?







 









	





























 j}















 



	



 









 j}













	













The following theorem is the main result from which the rest of the uniform convergence results
in this paper are derived.

BA   C    






EDF @G
JI G I
( 9)   H) 
   GQP
+R 6 @G ST VU < G W T  (53)


be a permissible class of functions mapping 
Theorem 18. Let  
 into

 j}
 	


	




. Let 
be generated by
  independent trials from



 

according
to
some
product
probability
measure
.
For
all
,
,


K ML








     N8;:#<
O




p ? 
 ?

'



'











	

 





  



The following immediate corollary will also be of use later.

YX[Z U]\ G T H^ _&` R 6bac gd fe G D ih

Corollary 19. Under the same conditions as Theorem 18, if


!



then

K L





 	


 

     j8;:#<
O












  

p ?"
 ?

'



'

'



(54)

GP g


(55)

A.1 Proof of Theorem 18
The proof is via a double symmetrization argument of the kind given in chapter 2 of Pollard (1984).
I have also borrowed some ideas from the proof of Theorem 3 in Haussler (1992).
180

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

A.1.1 F IRST S YMMETRIZATION





     , let


 

 

    
..
..
..
.
.
 k  . 




 


An extra piece of notation: for all
bottom half, viz:
 






D

D

and   be the

be the top half of

  l	   
..
..
..
.
.
 l   .  















The following lemma is the first symmetrization trick. We relate the probability of large deviation
between an empirical estimate of the loss and the true loss to the probability of large deviation
between two independent empirical estimates of the loss.



mI G I



	

Lemma 20. Let  be a permissible set of functions from 


	j

!
 . For all
probability measure on 
and

K L











     N8;:#<
O


po

GP
     N8;:#< rq   
O

cFn

into

 ,

p ? 
 ? 1
'



 


YD K L










p

? 






 ? 


 j}

and let

)

be a

  ts G D P 








(56)

q
rq      ts G D    uo s G
q      s G zD y
     0w
q    uo s G and
q   uo s I G D y 

Proof. Note first that permissibility of  guarantees the measurability of suprema over 
p
" ? 
 ? 1




 

(Lemma 32 part 5). By the triangle inequality for
, if
and
" ? 

"





? 
? 

?

, then
. Thus,




  


]q  
K &v

Vo ts I G  D
     xw
K v







 



 





 



Q"



 	

? 







 

 ? 












""

? 




"

? 









 ? 1
 ? 1





(57)



By Chebyshevs
inequality, for any fixed  ,


K v






	


   

Q   ?  
 ?



o {I G D y
K L













-DF @G

o





? 1
as
  and
 

gives the result.





D


?

o





	

 r



G

   
|R o




?




?"
  


o

? 1


 

I GD P

_

. Substituting this last expression into the right hand side of (57)

181

fiBAXTER

A.1.2 S ECOND S YMMETRIZATION
The second symmetrization trick bounds the probability of large deviation between two empirical
estimates of the loss (i.e. the right hand side of (56)) by computing the probability of large deviation when elements are randomly permuted between the first and second sample. The following
definition introduces the appropriate permutation group for this purpose.

}       D  D
~





 



~     ~     ~  
  ~  

     and any ~  H}      , let 
For any
F  l      
 .. . . .  ..
 . l    .    
 into
"  be a permissible set of  functions
Lemma 21. Let
mapping
   and let    3W be
(as in the statement of Theorem 18). Fix 
ST -cover for , where   1 + 
'     
  1  
 where the  
 are the rows
an G
of . Then,
K L ~ H}      j8:#O < ]q ;     ;    ts G D P
  K v ~ H}      q      
     
 ts GR y (58)

' 
where each ~ H}      is chosen uniformly at random.
q ;     ;    ts G D (if there is
Proof. Fix ~ }      and let 
be such that
G ST . Without loss
no such  for any ~ we are already done). Choose 
such that





 3  . Now,
of generality we can assume  is of the form 
D

'    ffu   ff 
fi ff  ff 
fiff  



 
 
'     ffu   ff  
  ff    ff  
  ff   
  
'    ffp    q ff  
  ff   ff  
  ff  s 
  
'   ffu  q ff  
  ff   ff  
  ff  s
  
'     ffp    q ff  
  ff   ff  
  ff  ts 
  
'     ffu  q ff  
  ff   ff  
  ff  ts
q       s q         s 







Definition 7. For all integers
, let
denote the set of all permutations of the
  
>
  >
    
   
   
 
&
sequence of pairs of integers



 such that for all ,



























 and

 or

 and

, either 

 
 . 
 



 











 j}
















 



 p







"

? 


















 











 ? 








Q 



p



  





>










 
j





"

a

? 




 ?
  

  







>




















182








cp




















?
  









'

 ? 


















j





 



? 
















j







 



 









? 




8







 

	



'





















? 










 ?
  













fiA M ODEL OF I NDUCTIVE B IAS L EARNING

D

p

Hence, by the triangle inequality for

,

q        s
q         ts
q ;     ;    s 
]q     
But
and
  q G  | R   by construction
     s G |R . Thus,
(59) implies
rq          ts
v ~ H}      xw 
A v ~ H}      xw 


  p
 




>
"

 

  



>



? 





  ? 
 ?
? 
  ? 



> 


  

c  ?
 ?
c  ? 
  


  ?
 

'  


  


p ? 


 
?


'  


" ? 
 ?


 


  



  ? 

> 
>

"





q       s
q          ts
(59)
    ts G D by assumption, so
GD y
q          s GRy

  

? 




 ? 












'







"

 

? 












 ? 










 





which gives (58).



     ,be any function that can be written in the form 
 {3  
K Mv ~ b}      ]q          ts GR y YD VU < G T 
(60)
where each ~ H}      is chosen uniformly at random.


Proof. For any ~ f}      ,
   q ff  
  ff   ff   
  ff  ts 





p
ff



q          ts 
 

(61)


ff
fi


ff
 
    ffu 

  ,  , let
To simplify the notation denote ff 
fiff by  
fiff . For each pair  ,
D and

lff be an independent random variable
fi


ff


ff


such that 
fiff
with probability



lff   
  ff  
fiff with probability D . From (61),
K Mv ~ H}      q          s GR y




l



     
fiffYl
K  ~ H}         q ff  
  ff   ff   
  ff  ts  GR
 
'  ffp 


'  ffp  


     
fiff + 
K        
fiff  GR
 
'  ffu  

   ffu  


0 with bounded ranges  
 
  
 , HoFor zero-mean independent random variables  
effdings inequality (Devroye, Gyorfi, & Lugosi, 1996) is


K \  
   h YD VU <  D3 


 '  
 
   
  
 #
Now we bound the probability of each term in the right hand side of (58).

 



Lemma 22. Let
. For any














>







j









  ? 
 

? 












j





























Y







 ? 


>









 

? 






 j}

	

Q"



 









Y

D













"







? 












 ? 








j

























a



























183



'









 







fiBAXTER


fiff is  fi
 ff  
   ff  
fiff  
   ff , we have
     
fiff+  YD VU <   G D     
     ffu    
fiffV  
K l      
fiff  GR
  
    ffp   
fiff  
   ff
 
'  ffp  

   ffu  
  
Let 
 
    ffp   
fiff . As  
fiff ,  
    ffu    
fiff   
fiff  . Hence,

D VU <   G D  
' ffu  
'  
lff  ffu 
   
lff ff    YD VU <  G  D   

    
R . Hence
giving a value of
   is minimized by setting 
K v ~ f}      q          ts GR y YD VU <  G T 


Noting that the range of each












j











? 












 







 





 j





e 

 



Dj

 













j











 





e 












  























 j

j



"j



 ?
  



















a





as required.


   
21 and 22 give:
K L ~ H}      j8:#O < ]q ;     ;    ts G D P
YD > @G ST
VU <W G T  
)   ;)  and each ) 
 is the empirical distribution that
Note that
is simply  where (




 D (recall Definition 3). Hence,
puts point mass
on each ff

     N8;:=< q          s G D P
K L ~ H}     
O
YD 6 @G ST VU <W G T  
Now, for a random choice of , each 
fiff in is independently (but not identically) distributed and ~
)
only ever swaps 
fiff and 
   ff (so that ~ swaps a 
fiff drawn according to ff with another component
drawn according to the same distribution). Thus we can integrate out with respect to the choice of
~ and write
     j8;:#< q        s G D P
K L
O
YD 6 @G ST VU <W G T  
A.1.3 P UTTING


For fixed




IT T OGETHER
	
 
, Lemmas





"

? 






 ? 










'



 





 G

 

>













 





e



e





e



_



j





 



















 



"

? 




 ? 


























	

j















 



p

? 






 ? 








Applying Lemma 20 to this expression gives Theorem 18.
184







	

j



fiA M ODEL OF I NDUCTIVE B IAS L EARNING

A.2 Proof of Theorem 2

)   ;) 

(

Another piece of notation is required for the proof. For any hypothesis space 
 


measures
 on  , let



?




	

  & O o& 

' ]'=





?








)   ;) 
K  (
(



and any probability



? 

 is another empirical
Note that we have used ?   	 rather than ?   to indicate that

?1
estimate of
	 .
_
With the C
there is also generated a se -sampling process, in addition to the
 sample
 


 although these are not supplied to the learner.
quence of probability measures,






means
This notion is used
in the following Lemma, where    

 n
the probability of generating a sequence of measures from the environment
 and then an
C _
 -sample according to
such that A holds.

(

(

Lemma 23. If



K L (


and







1

	


  ! f  N8:#<





K L (   8;:# < 
  ! 8;:#<
K L







"  ?




  !   x 
 p

"1 ?"
 ?

	 

	

 ?}1

	




G D P Dg







then







	





"1 ?p
 ?1


	
	

Proof. Follows directly from the triangle inequality for

 

	

G D P Dg







(62)



(63)

G P g


.

We treat the two inequalities in Lemma 23 separately.

A.2.1 I NEQUALITY (62)





In the following Lemma we replace the supremum over 
over 8 .
Lemma 24.


K ML (






 

 


in inequality (62) with a supremum

   f  j8;:#<
QG P


    f  N8;:#/ <
K \ (
  

















p ?"
 ?

 

 





185









p  ?Ep
 ?

'



'



Gh

(64)

fiBAXTER


(
-&




8;:#< 

p 





G
jI 7

n

7

?E"
? 
Proof. Suppose that   are such that
. Let  satisfy this in
 

 ? 
 
?"
?"

 . By the definition of 
 , for all
there
equality. Suppose first that  


"
?"
?E"
exists 5 
. Hence by property (3) of the

 such that 



 
"  ?"
 ?E"


 
	
metric, for all
, there exists Y
such that
. Pick an arbitrary
 ? 
 ? 
 ? 
? 
?E"
satisfying this inequality. By definition, 
	
 , and so 



' .
   ?  
 ? 

 
(by assumption), by the compatibility of
with the ordering on the
As

 

p1 ?"
 ? 
'


p


'
reals,
, say. By the triangle inequality for ,





7

u



G

 

7
  I

  

G G g

 



  G g
  

Thus
can be
 7  g G g 7 and for any 7 an  satisfying this inequality
G


found. Choosing
shows that there exists 
such that
.
I
If instead, 
, then an identical argument can be run with the role of and (
interchanged. Thus in both cases,
8:# <
G w  

   G
p1 ?p
 ?



p1 ?"
 ?

'
?








'



'

Qc"  ?p
 ?E"
p1 ?"
 ?

 
	











"1 ?p
 ?
 

	 



	





'





"1 ?"
 ?



	

?"

	





8

Qp1 ?"
 ?

'





'





'







which completes the proof of the Lemma.
_
By the nature of the C
 sampling process,


     ;8 :#/ <


    j8;:=/ <
, / K \

 3
 AY   where  

K \ (
 





 







 

p ?"
 ?

'





'

Gh

'

 p  G h   (  (65)
H  and  is permissible by the assumed
Now
permissibility of (Lemma 32, Appendix D). Hence
satisfies the conditions of Corollary 19
D for G and g D for g in Corollary
and so combining Lemma 24, Equation (65) and substituting G






 









GQ

   ?  
 ?



































19 gives the following Lemma on the sample size required to ensure (62) holds.

+X[Z U L G  D f^ _&` T 6 @G S g   G T P
   f  j8;:#<



Lemma 25. If

then

K L (


) 





 













  







p ?"
 ?
 

 

   '
    )  








 o  )
g
D
G
G
4



+X[Z U L G  D ^ _&` T 6 @G S g  4 G T P

A.2.2 I NEQUALITY (63)




 









!



G D P Dg 


f )



?  


Note that ?  	

 and
	

 , i.e the expectation of 

where is distributed according to . So to bound the left-hand-side of (63) we can apply Corollary
  
19 with 
, replaced by  ,  replaced by  , and replaced by
and
respectively,
replaced by and  replaced by . Note that 
is permissible whenever  is (Lemma 32).
Thus, if

)





















? 



186



g D

(66)

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

then inequality (63) is satisfied.
Now, putting together Lemma 23, Lemma 25 and Equation 66, we have proved the following

more general version of Theorem 2.

[I G g I
YX[Z U L G  D ^ _&` T 6 @G S g   G T P
 D T 6 @G 0   T P
Y
[
X
Z
U
L
g
and
G ^ _&`
G
then
  ! j8;:#<
K L
GP g

7m
To get Theorem 2, observe that

D

D
7
7 |R
Setting G
and maximizing G
gives
. Substituting G
 p

_
 -sample genTheorem 26. Let  be a permissible hypothesis space family and let be an C
n










erated from the environment 
 . For all
and
, if



 	

?  

 







  







p1 ?"
 ?1
'


















 



 











!





















   ?  
 ?  
1

	
	



?  








and



7  ED D


 {5



.
into

Theorem 26 gives Theorem 2.

 7
G 

A.3 The Realizable Case

 7


7
S

G G
G 
G
G 0 G 7
G
JI G g I , if
and
Corollary 27. Under the same conditions as Theorem 26, for all 7
X[Z U L G  D G 7 ^ _&` T 6 G g 7    G T G 7 P
T 6 G 7    T P
D

[
X
Z
U
L
g
and
G G 7 ^ _&`
G G 7
then
G
  ! N8;:#<
K L
7P g
G

D
These bounds are particularly useful if we know that
, for then we can set G
G ).
(which maximizes G




7

In Theorem 2 the sample complexity for both
and  scales as
 . This can be improved to

 ?"



?
?Ep
	

	


if instead of requiring
, we require only that ?  	
 











?  



for some
. To see this, observe that ?   
 

	



"1 ?"
 ?}1

  


	
	

, so setting
in Theorem 26 and treating as a constant
gives:





_ 







$  





$  



 	



_ 





?1





 

  











    3  z       b and   
 as a composition of two function classes note that if for each 
   by
             

To write  
r 
	



of the form given in (32),   can be written



r























Recalling Definition 6, for 






$ 


?p

	

 

Appendix B. Proof of Theorem 6



 






?p

	

  





$



	

















	





_



_

_







187





_









_







we define

fiBAXTER

   bt  i     z   . Thus, setting    b  and     0

  
(67)
6 .
The following two Lemmas will enable us to bound 57

 




Lemma 28. Let
be of the form
where
. For all 7  7
,
6 57  7
6   57   6 57  

=

)
7
o



Proof. Fix a measure
on
and let  be a minimum size -cover for 
. By

6

$
)


$
)

5
7



definition
each b let
be the measure on
defined by

)     for any set   in. For



 is6 measurable).
the ~ -algebra on
( is measurable so


7
F
o

57  . Let

Let 
for 
. By definition again, 
6
6
 9 andsize -cover
  . Note

 be a minimum
57   57  so the Lemma
that
be

 will

7
7
o



can be shown to be an
-cover for
. So, given any  
choose
proved if
 1 H such that  o     1 7  and 1  V such that o    1 7 . Now,
o     1   1 o        1 o    1  1   1
 o     1 o     1
7 7 
    line follows from
where the first line follows from the triangle inequality for o and the second





o
F
o

o
o





the facts:
1
 1 and   1
 1 . Thus is an
7  7 -cover for 1 1 o and
so the result follows.
 (Definition 6), we have the following Lemma.
Recalling the definition of {
Lemma 29.
6 57 M3    6 57 


 
 . Let      be
)   )  on
Proof. Fix a product probability measure
(
/
7 -covers of  o   o . and let    %   . Given  % 
{&  , choose  93   such that o& 
  
 7 for each   . Now,





/
.
,
    93  
 
'  
 
 
'   
 
  (   

 

'  o& 
  

7

 and as  - 
'   
 the result follows.
Thus is an 7 -cover for 93


then
,









 j}

	

 j}





























	







 



 1





L

	





















 



c








 



'







Y











m











O









C

























C



E

	















:













	









 









m









_































 1



















j

 1

:

















 

































 	







 







!









'

























E























188

































>









 





fiA M ODEL OF I NDUCTIVE B IAS L EARNING

B.1 Bounding

6 @7 






j

6 a 7  7     e 6 @7    6   / a 7  e
and from Lemma 29,
6 @7    6 57    
 / 57
6

Using similar techniques to those used to prove Lemmas 28 and 29, 
satisfy
6   / 57  6   7  
From Lemma 28,


































:















(68)







(69)


can be shown to

m



(70)

Equations (67), (68), (69) and (70) together imply inequality (34).

S 6 

6
@7  when is a hypothesis space
We wish to prove that 57 


x

 family of the form

|
f4 . Note that each f  corresponds
to some   , and that
 )  '#   o    
 
i on , defined by
Any probability measure on induces a probability measure
zz  -,  )   )
for any  in the ~ -algebra on
. Note also that if
1 are bounded, positive functions on an

arbitrary set , then
  3   3 1   +8 :#3<   1   
(71)






#




#






. Let f 
Let be any probability measure on the space of probability measures on





 . Then,
be two elements of  with corresponding hypothesis spaces 


  -,    '=  o      '#  o      )
,  8; :#<  o     o      ) (by (71) above)


)  )
,  ,  8:#<     



         
8;:#<     is guaranteed
   by the permissibility of (Lemma 32 part 4, ApThe measurability of

M

|


   we have,  
pendix D). From
 f
(72)
> 57 
> a 7  = M|   e
^

B.2 Bounding
























m
/












? 1

























j







a





















a





a





 1

















? 1





? 1





 1



















? 1



 













 _











 

 _









 











which gives inequality (35).
189



 







 



? 1

 _



 







	

















fiBAXTER

B.3 Proof of Theorem 7
In order to prove the bounds in Theorem 7 we have to apply Theorem 6 to the neural network
hypothesis space family of equation (39). In this case the structure is

  


  G 
 
 G  @G G   G 
0







    ~  
' 
 for some bounded
where 
subset  of
and some Lipschitz squashing function ~ . The feature class 

is the set of all one hidden layer neural networks with inputs, hidden nodes, 	 outputs, ~ as
the squashing function and weights 
 fiff where ff is a bounded subset of  . The Lipschitz
restriction on ~ and the bounded restrictions on the weights ensure that  and  are Lipschitz
b
I 

for all  and
classes. Hence there exists
1  ,      1  I
  1  and for all  W and 1 such that
Y
I

 1  1  where   is the  norm
, 
in each case. The loss function is squared loss.
, hence for all  1  and all probability measures

) onNow,  (recall that we assumed
the output space was
),
o   1 -,     l    
 1 
 ) 
YD,       1   )   
(73)
  
) 
)
  1 - and
where
is the marginal distribution on
derived from . Similarly, for all
)
probability measures on 
,


 o     1 YD&,     1  )   
(74)
Define
6 a 7    e 98;:#o < > a 7    ) e

where the  supremum is over all probability measures on (the Borel subsets of)
, and
a
)
)
7
7
e




is
the
size
of
the
smallest
-cover
of
under
the
metric.
Similarly
set,
>
6 a 7    e 98;:#o < > a 7    ) e
where now the supremum is over all probability measures on  . Equations (73) and (74) imply
6 57  6  7D    
(75)
6 @7 6  7  
D&  
(76)
 
Applying Theorem 11 from Haussler (1992), we find

D





7

6 D     7!
6  D&7       D 7  " 
w











_





  j}







 











	







m/

_

m







 _
 


 j}






>

_



























_





 














 













 















_


























































 j}









m/





 j}

 



 

























	





























m























 









Substituting these two expressions into (75) and (76) and applying Theorem 6 yields Theorem
7.
190

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

Appendix C. Proof of Theorem 14
This proof follows a similar argument to the one presented in Anthony and Bartlett (1999) for
ordinary Boolean function learning.
First we need a technical Lemma.

D  D D  D  , with

G for all
#    # 
   D D D D K # 


   
 
K #   #  0 #    #  %$ G  'R & '( ) +/ *-. , n 1n 0 
,
 # denote the number of occurences of in the random sequence # #   #  .
Proof. Let


The function can be viewed as a decision rule, i.e. based on the observations # , tries to guess



D


D



D


D
whether the probability of
the Bayes

Dis  D if  # or WD , and. The
 #  optimal
D rule isD otherwise.
 #  decision
estimator: #   # 
Hence,
K  # 2$ G D K   # D  G D  D 


D K  # I D  G D  D
KD   # D  G  D  D 
 D



D


which is half the probability that a binomial
 random variable is at least WD . By
Sluds inequality (Slud, 1977),
K  # 3$ G D K 

 
G

I  I







Lemma 30. Let be a random variable uniformly distributed on




>
. Let
be i.i.d.
-valued random variables with G
>{

 
 
. For any function mapping
,






















1

&



























^

















"





























$





&







"



^







 







^





























"





&

"







n





















j

 . Tates inequality (Tate, 1953) states that for all
where  is normal 
,

K






'4

D





 

65 n 

)

1





  ! be shattered by , with  . For each row  in 7 let  
 be the set of
Let 7
D
)
) )
all  distributions on
such that
if is not contained in the
9

8


)

D

 and ) 
fiff
fi


ff





 th;: row  ofD 7 , and for each    f  ,
8

. Let
.
)
;
)

<<  is achieved by any sequence







Note that for (
, the optimal error _ 
     such that 
 
fiff if and only if ) 
 
fiff
  D  , and
always contains such a sequence because shatters 7 . The optimal error is then

<_ 6<        ) 
  
 3$     =   D   D 

' 

'  ffu 
Combining the last two inequalities completes the proof.


 










+


 >

C





_



>

3














C

  






1







C













 
 
3


 
 >
'















C

  



_

 >



C



_













?











G





5

191







1

>

C





1







fiBAXTER

and for any






    ,
p  _ 6< <  








 

(77)
     
 
fiff 3$ 
 
fiff  
For any
-sample , let each element 
fiff in the array
   ?=   
..
..
>
..
.
.
	  .=   
equal the number of occurrences of 
fiff in .
)   ;)  uniformly at random from  , and generate an Now, if we select (
A
@  (the output of the learning algorithm) we have:
sample using ( , then for 
    
 
fiff 2$ 
 
lff  C B ) />     
 
fiff 2$ 
 
lff  >
 B ) />     =    ) 
fiff 2$  
fiff 
lff

'  ffu 
) is the probability of generating a configuration > of the 
fiff under the -sampling
where />
process and the sum is over all possible configurations. From Lemma 30,
) 
lff 2$  
fiff 
fiff ER DF 'G ) * /. H , nn JI
,
?

C _





' 









 



 









 















C _







 















































 

   
 
fiff 3$ 
 fi
 ff 
 

K  


 

C





 























where



 











 j}

 j}

1



B )






















 

C

R'&


/>




(

 

) 

1





 

 









 












 ?

 @




_



  ? =    R

'  ffp 
M/ L /n . L
n


K



















Y


H
)  * /. , n n IJ
,





(78)


, (78)

 G

  







192

DF

G

1

=K *-K, , 0

_ <6<    G M





(79)


. Plugging this into (77) shows
that

K (



C _

-valued random variable  , G  


a









   
 
lff 3$ 
 fi
 ff   G 
 O/ L /n . L
(
)

G NR &
= K *-K, , n 0






by Jensens inequality. Since for any
implies:











C









and 







hence



































5



 











C



  

 G


fiA M ODEL OF I NDUCTIVE B IAS L EARNING

(

(



@

(

Since the inequality holds over the random choice of , it must also hold for some specific choice

algorithm
there is some sequence of distributions such that
of . Hence for any learning

K 

 ?



 @




Setting

Assuming equality in (80), we get











G 7




 G

  

e



(80)

_ <6<   7  g 




_

g



and



 @

 ?





e



K 



 G g

  

ensures

_ <<    G M



_









(81)

7g  



Solving (79) for , and substituting the above expressions for G and  shows that (81) is satisfied
provided
 &  g7      0 ^ _&` T g   D g
(82)
g
R ( R since G I |R and G g   ), and assuming
Setting 
g7  	 for  somefor	 some
YD ,  (82) becomes
  D T  D 
(83)

	 ^'_&` 
R the right hand side of (83) is approximately maximized at 
Subject to the constraint 
T QPSR & , at which point the value, exceeds
CDF 	  D&D 7 . Thus, for all 	 , if 7 g

 R 	 and
 D&D a  e
(84)
7
G







 





!









C





 

1



  
 

 





e 






 



j

!



C

1





















C









!

 

C





Q

 

>

>





^













<6<   7  g 
_
 contains at least two
g
To obtain the -dependence in Theorem 14 observe that by assumption
)UT be two distributions
functions 
, hence there exists an
such that  3$
. Let
)


D
)T
T
7

8
concentrated on
and
such that
and
;: 7 D . Let ( 9)
T  b) and ( -)  H) be the product distributions on

V
)
98   generated by ,T and       @   . Note that   and 
are both in
. If ( is one of (
and the learning algorithm
chooses the wrong hypothesis  ,
then
  _ 6< <   7 
K 

then







 










 ?

 @






 






 {









?





_











'


























 

  

_






 

 








193









 

















_





fiBAXTER

(

(

( (  and generate an
/ n
_ <<   7  NR & ( ) /*X. W W n 0


Now, if we choose uniformly at random from 
cording to , Lemma 30 shows that

K (
g
which is at least if









 ?



 @






_

























c





1





-sample

ac-





 r





_





JI g I  |R . Combining the two constraints on
X[Z U     finishes the proof.


C _



I 7 7 f^ _&` T g D g


provided





(85)


: (84) (with



	

P ) and (85), and using


Appendix D. Measurability
In order for Theorems 2 and 18 to hold in full generality we had to impose a constraint called
permissibility on the hypothesis space family  . Permissibility was introduced by Pollard (1984)
for ordinary hypothesis classes  . His definition is very similar to Dudleys image admissible
Suslin (Dudley, 1984). We will be extending this definition to cover hypothesis space families.
Throughout this section we assume all functions  map from (the complete separable metric
 j}
. Let   denote the Borel -algebra of any topological space . As in Section
space)  into
2.2, we view , the set of all probability measures on  , as a topological space by equipping it
with the topology of weak convergence.   is then the -algebra generated by this topology. The
following two definitions are taken (with minor modifications) from Pollard (1984).





Y ff

~



Y

 j}

ff

~

-valued functions on  is indexed by the set
Definition 8. A set  of
r

 j}
such that


ff

Definition 9. The set 
1.

ff



 





_



Zff

Q







is permissible if it can be indexed by a set

ff

ff

ff

if there exists a function

such that

is an analytic subset of a Polish7 space , and


~


2. the function
-algebra  

Y

ff





 j}

indexing 

by

\[]Y ff  .
An analytic subset ff of a Polish space ff


ff

is measurable with respect to the product


is simply the continuous image of a Borel subset
of another Polish space . The analytic subsets of a Polish space include the Borel sets. They
are important because projections of analytic sets are analytic, and can be measured in a complete
measure space whereas projections of Borel sets are not necessarily Borel, and hence cannot be
measured with a Borel measure. For more details see Dudley (1989), section 13.2.
Lemma 31. 

2| 


  






 j}

is permissible if 

  






are all permissible.

Proof. Omitted.
We now define permissibility of hypothesis space families.
7. A topological space is called Polish if it is metrizable such that it is a complete separable metric space.

194

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

 W







Definition 10. A hypothesis space family 
is permissible if there exist sets





are analytic subsets of Polish spaces and respectively, and a function


measurable with respect to

 , such that

^_[`Y ff \[`Y
ba  



fe hg



 ff

_}E




ff

Zff





ff

dc 



 and ff

that
,



 j}

@

 

Let 
be an analytic subset of a Polish space. Let  
 be a measure space and

denote the analytic subsets of . The following three facts about analytic sets are taken from
Pollard (1984), appendix C.

fe hg

 
(a) If 

@
(b)







ff

is complete then

@



e

A


.

e ]
[ Y  ff  .
ff  , the projection i   of  onto 

~

contains the product -algebra

(c) For any set



in

@

 

4

@

is in

)




.

 Y 



Y


Recall Definition 2 for the definition of  . In the following Lemma we assume that  
_
 n 
_ is complete
has been completed with respect to any probability measure , and also that
with respect to the environmental measure .





Lemma 32. For any permissible hypothesis space family  ,
1.   is permissible.


f is permissible.
3.
is permissible for all
.

8
#
:
<
O and '# O are measurable for all .
4.
5.  is measurable for all
.
6.  is permissible.
 is simply the set of all
Proof. As we have absorbed the loss function into the hypotheses ,
" such that
-fold products
. Thus (1) follows from Lemma 31. (2) and (3)
2. G







/

/



/





















are immediate from the definitions. As  is permissible for all   , (4) can be proved by an
identical argument to that used in the Measurable Suprema section of Pollard (1984), appendix
C.


 j}


 j}
, the function 
defined
For (5), note that for any Borel-measurable  








 is Borel measurable Kechris (1995, chapter 17). Now, permissibility of
by   



, and 
so 
is measurable
 automatically implies permissibility of 

/	
by (4).
r



 j}
in the appropriate way. To prove (6),
Now let  be indexed by

 


 j}
1
_

}

E





 _E 



 . By Fubinis theorem is a
define
by 



 j}
E






 -measurable function. Let
be defined by 

 1_}E
indexes 
in the appropriate way for 
to be permissible, provided it can
 .

 -measurable. This is where analyticity becomes important. Let
be shown that is  




 1_}E   _E
. By property (b) of analytic sets, 
.


 contains

1
E



1
E








The set
is the projection of
onto
, which by property (c) is
n 

also analytic. As 

 is assumed complete,
is measurable, by property (a). Thus is
a measurable function and the permissibility of 
follows.

)

kj .

0

)




Y ;[m Y ff ;[fiff Y 
qp  )

'#on )   Y )  r[fiY  
G
 c  c )
)
     p
Y

)

G

ff



W

 lj . 
)





4

195

c

c

f '# O f
@

 



ff




 )



c

fiBAXTER

References
Abu-Mostafa, Y. (1993). A method for learning from hints. In Hanson, S. J., Cowan, J. D., & Giles,
C. L. (Eds.), Advances in Neural Information Processing Systems 5, pp. 7380 San Mateo,
CA. Morgan Kaufmann.
Anthony, M., & Bartlett, P. L. (1999). Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, UK.
Bartlett, P. L. (1993). Lower bounds on the VC-dimension of multi-layer threshold networks. In
Proccedings of the Sixth ACM Conference on Computational Learning Theory, pp. 44150
New York. ACM Press. Summary appeared in Neural Computation, 5, no. 3.
Bartlett, P. L. (1998). The sample complexity of pattern classification with neural networks: the
size of the weights is more important than the size of the network. IEEE Transactions on
Information Theory, 44(2), 525536.
Baxter, J. (1995a). Learning Internal Representations. Ph.D. thesis, Department of Mathematics and Statistics, The Flinders University of South Australia. Copy available from
http://wwwsyseng.anu.edu.au/ jon/papers/thesis.ps.gz.

s

Baxter, J. (1995b). Learning internal representations. In Proceedings of the Eighth International
Conference on Computational Learning Theory, pp. 311320. ACM Press. Copy available
from http://wwwsyseng.anu.edu.au/ jon/papers/colt95.ps.gz.

s

Baxter, J. (1997a). A Bayesian/information theoretic model of learning to learn via multiple task
sampling. Machine Learning, 28, 740.
Baxter, J. (1997b). The canonical distortion measure for vector quantization and function approximation. In Proceedings of the Fourteenth International Conference on Machine Learning,
pp. 3947. Morgan Kaufmann.
Baxter, J., & Bartlett, P. L. (1998). The canonical distortion measure in feature space and 1-NN
classification. In Advances in Neural Information Processing Systems 10, pp. 245251. MIT
Press.
Berger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis. Springer-Verlag, New
York.
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1989). Learnability and the vapnikchervonenkis dimension. Journal of the ACM, 36, 929965.
Caruana, R. (1997). Multitask learning. Machine Learning, 28, 4170.
Devroye, L., Gyorfi, L., & Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition.
Springer, New York.
Dudley, R. M. (1984). A Course on Empirical Processes, Vol. 1097 of Lecture Notes in Mathematics, pp. 2142. Springer-Verlag.
Dudley, R. M. (1989). Real Analysis and Probability. Wadsworth & Brooks/Cole, California.
196

fiA M ODEL OF I NDUCTIVE B IAS L EARNING

Gelman, A., Carlin, J. B., Stern, H. S., & Rubim, D. B. (Eds.). (1995). Bayesian Data Analysis.
Chapman and Hall.
Good, I. J. (1980). Some history of the hierarchical Bayesian methodology. In Bernardo, J. M.,
Groot, M. H. D., Lindley, D. V., & Smith, A. F. M. (Eds.), Bayesian Statistics II. University
Press, Valencia.
Haussler, D. (1992). Decision theoretic generalizations of the pac model for neural net and other
learning applications. Information and Computation, 100, 78150.
Heskes, T. (1998). Solving a huge number of similar tasks: a combination of multi-task learning and
a hierarchical Bayesian approach. In Shavlik, J. (Ed.), Proceedings of the 15th International
Conference on Machine Learning (ICML 98), pp. 233241. Morgan Kaufmann.
Intrator, N., & Edelman, S. (1996). How to make a low-dimensional representation suitable for
diverse tasks. Connection Science, 8.
Kechris, A. S. (1995). Classical Descriptive Set Theory. Springer-Verlag, New York.
Khan, K., Muggleton, S., & Parson, R. (1998). Repeat learning using predicate invention. In Page,
C. D. (Ed.), Proceedings of the 8th International Workshop on Inductive Logic Programming
(ILP-98), LNAI 1446, pp. 65174. Springer-Verlag.
Langford, J. C. (1999). Staged learning. Tech. rep., CMU, School of Computer Science.
http://www.cs.cmu.edu/ jcl/research/ltol/staged latest.ps.

s

Mitchell, T. M. (1991). The need for biases in learning generalisations. In Dietterich, T. G., &
Shavlik, J. (Eds.), Readings in Machine Learning. Morgan Kaufmann.
Parthasarathy, K. R. (1967). Probabiliity Measures on Metric Spaces. Academic Press, London.
Pollard, D. (1984). Convergence of Stochastic Processes. Springer-Verlag, New York.
Pratt, L. Y. (1992). Discriminability-based transfer between neural networks. In Hanson, S. J.,
Cowan, J. D., & Giles, C. L. (Eds.), Advances in Neural Information Processing Systems 5,
pp. 204211. Morgan Kaufmann.
Rendell, L., Seshu, R., & Tcheng, D. (1987). Layered concept learning and dynamically-variable
bias management. In Proceedings of the Tenth International Joint Conference on Artificial
Intelligence (IJCAI 87), pp. 308314. IJCAI , Inc.
Ring, M. B. (1995). Continual Learning in Reinforcement Environments. R. Oldenbourg Verlag.
Russell, S. (1989). The Use of Knowledge in Analogy and Induction. Morgan Kaufmann.
Sauer, N. (1972). On the density of families of sets. Journal of Combinatorial Theory A, 13,
145168.
Sharkey, N. E., & Sharkey, A. J. C. (1993). Adaptive generalisation and the transfer of knowledge.
Artificial Intelligence Review, 7, 313328.
197

fiBAXTER

Silver, D. L., & Mercer, R. E. (1996). The parallel transfer of task knowledge using dynamic
learning rates based on a measure of relatedness. Connection Science, 8, 277294.
Singh, S. (1992). Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8, 323339.
Slud, E. (1977). Distribution inequalities for the binomial law. Annals of Probability, 4, 404412.
Suddarth, S. C., & Holden, A. D. C. (1991). Symolic-neural systems and the use of hints in developing complex systems. International Journal of Man-Machine Studies, 35, 291311.
Suddarth, S. C., & Kergosien, Y. L. (1990). Rule-injection hints as a means of improving network performance and learning time. In Proceedings of the EURASIP Workshop on Neural
Networks Portugal. EURASIP.
Sutton, R. (1992). Adapting bias by gradient descent: An incremental version of delta-bar-delta. In
Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 171176. MIT
Press.
Tate, R. F. (1953). On a double inequality of the normal distribution. Annals of Mathematical
Statistics, 24, 132134.
Thrun, S. (1996). Is learning the n-th thing any easier than learning the first?. In Advances in Neural
Information Processing Systems 8, pp. 640646. MIT Press.
Thrun, S., & Mitchell, T. M. (1995). Learning one more thing. In Proceedings of the International
Joint Conference on Artificial Intelligence, pp. 12171223. Morgan Kaufmann.
Thrun, S., & OSullivan, J. (1996). Discovering structure in multiple learning tasks: The TC algorithm. In Saitta, L. (Ed.), Proceedings of the 13th International Conference on Machine
Learning (ICML 96), pp. 489497. Morgen Kaufmann.
Thrun, S., & Pratt, L. (Eds.). (1997). Learning to Learn. Kluwer Academic.
Thrun, S., & Schwartz, A. (1995). Finding structure in reinforcement learning. In Tesauro, G.,
Touretzky, D., & Leen, T. (Eds.), Advances in Neural Information Processing Systems, Vol. 7,
pp. 385392. MIT Press.
Utgoff, P. E. (1986). Shift of bias for inductive concept learning. In Machine Learning: An Artificial
Intelligence Approach, pp. 107147. Morgan Kaufmann.
Valiant, L. G. (1984). A theory of the learnable. Comm. ACM, 27, 11341142.
Vapnik, V. N. (1982). Estimation of Dependences Based on Empirical Data. Springer-Verlag, New
York.
Vapnik, V. N. (1996). The Nature of Statistical Learning Theory. Springer Verlag, New York.

198

fi