Journal of Artificial Intelligence Research 41 (2011) 407-444

Submitted 02/11; published 07/11

Efficient Multi-Start Strategies for Local Search Algorithms
Andras Gyorgy

gya@szit.bme.hu

Machine Learning Research Group
Computer and Automation Research Institute
of the Hungarian Academy of Sciences
1111 Budapest, Hungary

Levente Kocsis

kocsis@sztaki.hu

Data Mining and Web Search Research Group, Informatics Laboratory
Computer and Automation Research Institute
of the Hungarian Academy of Sciences
1111 Budapest, Hungary

Abstract
Local search algorithms applied to optimization problems often suffer from getting
trapped in a local optimum. The common solution for this deficiency is to restart the
algorithm when no progress is observed. Alternatively, one can start multiple instances of
a local search algorithm, and allocate computational resources (in particular, processing
time) to the instances depending on their behavior. Hence, a multi-start strategy has to
decide (dynamically) when to allocate additional resources to a particular instance and
when to start new instances. In this paper we propose multi-start strategies motivated
by works on multi-armed bandit problems and Lipschitz optimization with an unknown
constant. The strategies continuously estimate the potential performance of each algorithm
instance by supposing a convergence rate of the local search algorithm up to an unknown
constant, and in every phase allocate resources to those instances that could converge to
the optimum for a particular range of the constant. Asymptotic bounds are given on the
performance of the strategies. In particular, we prove that at most a quadratic increase in
the number of times the target function is evaluated is needed to achieve the performance
of a local search algorithm started from the attraction region of the optimum. Experiments
are provided using SPSA (Simultaneous Perturbation Stochastic Approximation) and kmeans as local search algorithms, and the results indicate that the proposed strategies work
well in practice, and, in all cases studied, need only logarithmically more evaluations of the
target function as opposed to the theoretically suggested quadratic increase.

1. Introduction
Local search algorithms applied to optimization problems often suffer from getting trapped
in a local optimum. Moreover, local search algorithms that are guaranteed to converge
to a global optimum under some conditions (such as Simulated Annealing or Simultaneous
Perturbation Stochastic Approximation, SPSA, see, e.g., Spall, Hill, & Stark, 2006), usually
converge at a very slow pace when the conditions are satisfied. On the other hand, if the
algorithms are employed with more aggressive settings, much faster convergence to local
optima is achievable, but with no guarantee to find the global optimum. The common soluc
2011
AI Access Foundation. All rights reserved.

fiGyorgy & Kocsis

tion to escape from a local optimum is to restart the algorithm when no progress is observed
(see e.g., Mart, Moreno-Vega, & Duarte, 2010; Zabinsky, Bulger, & Khompatraporn, 2010,
and the references therein).
Alternatively, one can start multiple instances of the local search algorithm, and allocate
computational resources, in particular, processing time, to the instances depending on their
behavior. Instances can be started at any time, and so the number of instances may grow
over time depending on the allocation strategy. (see, e.g., Chapter 10 of Battiti, Brunato, &
Mascia, 2008 and the references therein). In this type of problems the computational cost
is usually measured as the total number of steps made by all search algorithm instances:
this often reflects the situation that the evaluation of the target function to be optimized
is expensive, and the costs related to determine which algorithms to use next are negligible
compared to the former (e.g., this is clearly the case if the task is to tune the parameters of
a system whose performance can only be tested via lengthy experiments, see, e.g., BartzBeielstein, 2006; Hutter, Hoos, Leyton-Brown, & Stutzle, 2009). In this paper we address
the above problem of dynamically starting several instances of local search algorithms and
allocating resources to the instances based on their (potential) performance.
To our knowledge, solutions to the above problem have either been based on heuristics
or on the assumption that the local optima the search algorithms converge to have an
extreme value distribution (see Section 2 below). In this paper, we propose new multi-start
strategies under very mild conditions on the target function, with attractive theoretical
and practical properties: Supposing a convergence rate of the local search algorithms up
to an unknown constant, our strategies continuously estimate the potential performance of
each algorithm instance and in every phase allocate resources to those instances that could
converge to the optimum for a particular range of the constant. The selection mechanism
is analogous to the DIRECT algorithm (Jones, Perttunen, & Stuckman, 1993; Finkel &
Kelley, 2004; Horn, 2006) for optimizing Lipschitz-functions with an unknown constant,
where preference is given to rectangles that may contain the global optimum. The optimum
within each rectangle is estimated in an optimistic way, and the estimate depends on the
size of the rectangle. In our strategies we use the function describing the convergence rate
of the local search algorithms in a similar way as the size of the rectangles are used in the
DIRECT algorithm.
Since in the proposed multi-start strategies the potential performance of each local search
algorithm is continuously estimated from the currently best value of the target function
returned by that algorithm, our method is restricted to work with local search algorithms
that return the best known value of the target function after each step. This is the case,
for example, in certain meta-learning problems, where the goal is to find a good parameter
setting of a learning algorithm. Here the search space is the parameter space of the learning
algorithm, and one step of the local search methods means running the learning algorithm
completely on a possibly very large data set. On the other hand, if the local search algorithm
is some sort of a gradient search optimizing an error function over some training data, then
the value of the target function is usually available only in the case of batch learning
(potentially after some very cheap computations), but not when the gradient is estimated
only from a few samples.
The rest of the paper is organized as follows. Section 2 summarizes related research.
The problem is defined formally in Section 3. The new multi-start local search strategies of
408

fiEfficient Multi-Start Strategies for Local Search Algorithms

this paper are described and analyzed in Section 4: in Section 4.1 we deal with a selection
mechanism among a fixed number of instances of the local search algorithm, while, in
addition, simple schedules for starting new instances are also considered in Section 4.2,
which are natural extensions of the case of finitely many local search algorithm instances.
This section concludes with a discussion of the results in Section 4.3. Simulation results
on real and synthetic data are provided in Section 5. Conclusions and future work are
described in Section 6.

2. Related Work
The problem of allocating resources among several instances of search algorithms can be
comfortably handled in a generalized version of the maximum K-armed bandit problem.
The original version of this problem consists of several rounds, where in each round one
chooses one of K arms, receives some reward depending on the choice, with the goal of
maximizing the highest reward received over several rounds. This model can easily be used
for our problem by considering each local search algorithm instance as an arm: pulling an
arm means taking one additional step of the corresponding algorithm, that is, evaluating
the target function at a point suggested by that algorithm, and the reward received is the
value of the target function at the sampled point. A generic algorithm for the standard
maximum K-armed bandit problem, where each reward is assumed to have independent and
identical distribution, is provided by Adam (2001), where the so-called reservation price of
an instance is introduced, which gives the maximum amount of resources worth to spend
on an instance: if an instance achieves its reservation price, it is useless to select it again.
The computation of the reservation price depends on a model of the algorithm that can be
learnt under some specific constraints.
Now consider a scenario where several instances of some, possibly randomized local
search algorithms are run after each other with the goal of maximizing the expected performance. Each instance is run until it terminates. In this scenario it is natural to assume
that the values returned by the instances (usually some local optima of the target function) are independent. Furthermore, since good search algorithms follow (usually heuristic)
procedures that yield substantially better results than pure random guessing, Cicirello and
Smith (2004, 2005) suggested that the rewards (evaluated target function values) of the
search instances may be viewed as the maximum of many random variables (if the instances are run for sufficiently long time), and hence may be modeled by extreme value
distributions. Several algorithms are based on this assumption, and are hence developed
for the maximum K-armed bandit problem with returns following generalized extreme value
distributions: Cicirello and Smith apply (somewhat heuristic) methods that use the above
extreme-value-distribution assumption at each decision point of a meta-learning algorithm,
while Streeter and Smith (2006a) use this model to obtain upper confidence bounds on the
performance estimate of each type of algorithms used and then try only the algorithms
with the best expected result. The latter is a theoretically justified example of the natural
strategy to probe the algorithm instances for a while, estimate their future performance
based on the results of this trial phase, and then use the most promising algorithm for the
time remaining. Streeter and Smith (2006b) proposed a distribution free approach that
409

fiGyorgy & Kocsis

combines a multi-armed bandit exploration strategy with a heuristic selection among the
available arms.
While in the standard maximum K-armed bandit problem the rewards in each round
are assumed to be independent, this is clearly not the case in our situation where the
algorithm instances are run parallel and the reward for evaluating the target function at a
point is the improvement upon the current maximum, since the samples chosen by a local
search algorithm usually depend on previous samples. Nevertheless, the ideas and lessons
learnt from the maximum K-armed bandit problems can be used in our case, as well: for
example, the algorithm Threshold Ascent of Streeter and Smith (2006b) gives reasonably
good solutions in our case, or the principle of probing instances for a while and then using
the most promising in the time remaining also carries over to this situation easily: such
algorithms, having first an exploration then an exploitation phase, will be referred to in
the sequel as explore-and-exploit algorithms. In this class of algorithms, simple rules were
suggested by Beck and Freuder (2004) to predict the future performance of each algorithm,
while Carchrae and Beck (2004) employ Bayesian prediction.
Another related problem is to find fast algorithms among several ones that solve the
same problem. More precisely, several algorithm instances are available that all produce
the correct answer to a certain question if run for a sufficiently long time. The time needed
for an algorithm instance to find the answer is assumed to be a random quantity with
independent and identical distributions for all the instances, and the goal is to combine the
given algorithms to minimize the expected running time until the answer is found. When the
distribution of the running time is known, an optimal non-adaptive time-allocation strategy1
is to perform a sequence of runs with a certain cut-off time that depends on the distribution
(Luby, Sinclair, & Zuckerman, 1993). If the distribution is unknown, a particular running
time sequence can be chosen that results in an expected total running time that is only a
logarithmic factor larger than the optimum achievable if the distribution is known. We note
that this strategy is among the few that provide a schedule that increases the number of
algorithm instances. The above set-up can be specialized to our problem: the goal is to find
an -optimal approximation of the optimum and the running time is the number of steps
needed by the given search algorithm to achieve such an approximation. Note that in this
case the running time of any algorithm instance providing an -suboptimal solution has to
be defined to be infinity, but the results of Luby et al. remain valid if an -optimal solution
can be found with positive probability. For the same problem, Kautz, Horvitz, Ruan,
Gomes, and Selman (2002) proposed an allocation strategy based on updating dynamically
the belief over the run-time distribution. Concerning the latter, Hoos and Stutzle (1999)
found empirically that run-time distributions are approximately exponential in certain (NPhard) problems, while Ribeiro, Rosseti, and Vallejos (2009) dealt with the comparison of
different run-time distributions.
Finally, when a set of time allocation strategies are available and the optimization problem is to be solved several times, one can use the standard multi-armed bandit framework
as done by Gagliolo and Schmidhuber (2006, 2007, 2010).
Running several instances of an algorithm or several algorithms in parallel and selecting
among the algorithms have been intensively studied, for example, in the area of meta1. In a non-adaptive time-allocation strategy the running time of an algorithm instance is fixed in advance,
that is, the measured performance of the algorithm instances has no effect on the schedule.

410

fiEfficient Multi-Start Strategies for Local Search Algorithms

learning (Vilalta & Drissi, 2002) or automatic algorithm configuration (Hutter et al., 2009).
The underlying problem is very similar in both cases: automatic algorithm configuration
usually refers to tuning search algorithms, while meta-learning is used for a subset of these
problems, tuning machine learning algorithms (the latter often allows more specific use of
the data). The main problem here is to allocate time slices to particular algorithms with the
aim of maximizing the best result returned. This allocation may depend on the intermediate
performance of the algorithms. Most of the automatic algorithm configuration and metalearning systems use various heuristics to explore the space of algorithms and parameters
(see, e.g., Hutter et al., 2009).
Finally, it is important to note that, although multi-start local search strategies solve
global optimization problems, we concentrate on maximizing the performance given the
underlying family of local optimization methods. Since the choice of the latter has a major
effect on the achievable performance, we do not compare our results to the vast literature
on global optimization.

3. Preliminaries
Assume we wish to maximize a real valued function f on the d-dimensional unit hypercube
[0, 1]d , that is, the goal is to find a maximizer x  [0, 1]d such that f (x ) = f  where
f  = max f (x)
x[0,1]d

denotes the maximum of f in [0, 1]d . For simplicity, we assume that f is continuous on
[0, 1]d .2 The continuity of f implies the existence of x , and, in particular, that f is bounded.
Therefore, without loss of generality, we assume that f is non-negative.
If the form of f is not known explicitly, search algorithms usually evaluate f at several
locations and return an estimate of x and f  based on these observations. There is an
obvious trade-off between the number of samples used (i.e., the number of points where
the target function f is evaluated) and the quality of the estimate, and the performance of
any search strategy may be measured by the accuracy it achieves in estimating f  under a
constraint on the number of samples used.
Given a local search algorithm A, a general strategy for finding a good approximation
of the optimum x is to run several instances of A initialized at different starting points
and approximate f  with the maximum f value observed. We concentrate on local search
algorithms A defined formally by a sequence of possibly randomized sampling functions
sn : [0, 1]dn  [0, 1]d , n = 1, 2, . . .: A evaluates f at locations X1 , X2 , . . . where Xi+1 =
si (X1 , . . . , Xi ) for i  1, and the starting point X1 = s0 is chosen uniformly at random from
[0, 1]d ; after n observations A returns the estimate of x and the maximum f  , respectively,
by
bn = argmax f (Xk )
bn ).
X
and
f (X
1kn1

where ties in the argmax function may be broken arbitrarily, that is, if more samples Xk
bn can be chosen to be any of them. To avoid ambiguity and
achieve the maximum, X
2. The results can easily be extended to (arbitrary valued) bounded piecewise continuous functions with
finitely many continuous components.

411

fiGyorgy & Kocsis

simplify notation, here and in the following, unless stated explicitly otherwise, we adopt the
convention to use argmax to denote the maximizing sample with the smallest index, but
the results remain valid under any other choice to break ties.
For simplicity, we consider only starting a single local search algorithm A at different
random points, although the results of this work can be extended to allow varying the
parameters of A (including the situation of running different local search algorithms, where
a parameter would choose the actually employed search algorithm); as well as to allow
dependence among the initializations of A (that is, the starting point and parameters of
a local search instance may depend on information previously obtained about the target
function).
It is clear that if the starting points are sampled uniformly from [0, 1]d and each algorithm
bn ) converges
is evaluated at its starting point then this strategy is consistent, that is, f (X
to the maximum of f with probability 1 as the number of instances tends to infinity (in the
worst case we perform a random search that is known to converge to the maximum almost
surely). On the other hand, if algorithm A has some favorable properties then it is possible
to design multi-start strategies that still keep the random search based consistency, but
provide much faster convergence to the optimum in terms of the number of evaluations of
f.
bn ) is bounded and non-decreasing, it converges (no matter what
Since the sequence f (X
random effects occur during the search). The next lemma, proved in Appendix A, shows
that, with high probability, the convergence cannot be arbitrarily slow.
bn ) = f .3 If P (E) > 0,
Lemma 1 For any f  [0, 1]d , let E denote the event limn f (X
bn )  f
then, for any 0 <  < 1 there is an event E  E with 1  P (E ) <  such that f (X
uniformly almost everywhere on E . In other words, there exists a non-negative, nonincreasing function g (n) with limn g (n) = 0 such that


fi
bt ) = f  1   .
b t )  f (X
bn )  g (n) for all nfi lim f (X
P lim f (X
(1)
t

t

In certain cases, g (n) = O(en ), as shown by Nesterov (2004) for (gradient-based)
optimization for convex functions, by Gerencser and Vago (2001) for noise-free SPSA for
convex functions, or by Kieffer (1982) for k-means clustering (or Lloyds algorithm) in one
dimension for log-concave densities. While these results pertain to the simple situation
where there is only one local optimum which is the global one, many of these results can
be extended to more general situations, and we observed exponential rate of convergence in
our own experiments with functions with many local maxima.
The convergence property of local search algorithms guaranteed by Lemma 1 will be
exploited in the next section to derive efficient multi-start search strategies.

4. Multi-Start Search Strategies
Standard multi-start search strategies run an instance of A until it seems to converge to
a location where there is no hope to beat the currently best approximation of f  . An
3. In practice we can usually assume that local search algorithms converge to local optima, and so f may
be assumed to be a local optimum.

412

fiEfficient Multi-Start Strategies for Local Search Algorithms

alternative way of using multiple instances of local search algorithms is to run all algorithms
in parallel, and in each round decide which algorithms can take an extra step. This approach
may be based on estimating the potential performance of a local search algorithm A based
on Lemma 1. Note that if g were known, an obvious way would be to run each instance
until their possible performances become separated with high probability in the sense that
the margin between the performance of the actually best and the second best algorithm
is so large that the actually best algorithm is guaranteed to be the best, in the long run,
with high probability. Then we could just pick the best instance and run it until the given
computational budget is exhausted (this would be a simple adaptation of the explore-andexploit idea of choosing the best algorithm based on a trial phase as in Beck & Freuder,
2004; Carchrae & Beck, 2004).
In practice, however, g is usually not known, but for certain problem classes and local
search algorithms it may be known to belong to some function class, for example, g may
be known up to a (multiplicative) constant factor (here, for example, the constant may
depend on certain characteristics of f , such as its maximum local steepness). Even in the
latter case, the best instance still cannot be selected with high probability no matter how
large the margin is (as g may be arbitrarily large). However, using ideas from the general
methodology for Lipschitz optimization with an unknown constant (Jones et al., 1993),
we can get around this problem and estimate, in a certain optimistic way, the potential
performance of each algorithm instance, and in each round we can step the most promising
ones.
The main idea of the resulting strategy can be summarized as follows. Assume we have
K instances of an algorithm A, denoted by A1 , . . . , AK . Let Xi,n , i = 1, . . . , K denote the
location at which f is evaluated by Ai at the nth time it can take a step, where Xi,1 is the
starting point of Ai . The estimate of the location of the maximum by algorithm Ai after n
samples (steps) is
bi,n = argmax f (Xi,t )
X
1tn

bi,n ).
and the maximum value of the function is estimated by fi,n = f (X
For any i, let fi = limn fi,n denote the limiting estimate of the maximum of f
provided by Ai . Let g be defined as in Lemma 1 for the largest of these values,
f = max fi .
i=1,...,K

Since f is the best achievable estimate of the maximum of f given the actual algorithms
A1 , . . . , AK , g gives a high probability convergence rate for those algorithms that provide
the best estimate of the maximum in the long run (note that the assumption deals with
each limiting estimate  usually a local maximum  separately, that is, here no assumption
is made on algorithms whose limiting estimates are less than f ). Then, if Ai evaluates f
at ni,r points by the end of the rth round and Ai converges to the best achievable estimate
f , by Lemma 1 we have, with probability at least 1  ,
fi  fi,ni,r  g (ni,r ),
and so

fi,ni,r + g (ni,r )
413

(2)

fiGyorgy & Kocsis

is an optimistic estimate of f . If Ai is suboptimal in the sense that limn fi,n < f then
the above estimate is still optimistic if the rate of convergence is not slower than g , and
pessimistic if the rate of convergence is slower than g . The latter is desirable in the sense
that we have a negatively biased estimate on the expected performance of an algorithm
that we do not want to use (we should not waste samples on suboptimal choices).
As in practice g is usually not known exactly, the estimate (2) often cannot be constructed. On the other hand, if g is known up to a constant factor then we can construct
a family of estimates for all scales: Let g denote a normalized version of g such that
g (0) = 1 and g (n)/g (n) is a constant for all n, and construct the family of estimates
fi,ni,r + cg (ni,r )

(3)

where c ranges over all positive reals. Then it is reasonable to choose, in each round,
those algorithms to take another step that provide the largest estimate for some values of c
(typically, if an algorithm gives the largest estimate for some c = c then there is an interval
I containing c such that the algorithm provides the largest estimate for any c  I). In
this way we can get around the fact that we do not know the real scaling factor of g , as
we certainly use the algorithms that provide the largest value of (3) for c = g (1)/g (1),
and, as it will be discussed later, we do not waste too many samples for algorithms that
maximize (3) for other values of c. Using the optimistic estimate (3) is very similar, in spirit,
to the optimistic estimates in the standard upper confidence bound-type solution to the
multi-armed bandit problem (Auer, Cesa-Bianchi, & Fischer, 2002) or in the well-known
A search algorithm (Hart, Nilsson, & Raphael, 1968).
However, the exact (local) convergence rate is not known, even up to a constant factor,
for many local search algorithms, and even if it is, the corresponding bounds are usually
meaningful only in the asymptotic regime, which is often not of practical interest. Therefore,
to give more freedom in the design of the algorithm, we are going to use an estimate of the
form
fi,ni,r + ch(ni,r )
(4)
where, similarly to the requirements on g , h is a positive, monotone decreasing function
with limn h(n) = 0. We will also assume, without loss of generality, that h(0) = 1.
The actual form of h will be based on the theoretical analysis of the resulting algorithms
and some heuristic considerations. Essentially we will use h functions that converge to zero
exponentially fast, which is in agreement with the exponentially fast local convergence rates
in the examples given after Lemma 1. The optimal choice of h, given, for example, g , is
not known, and is left for future work.
4.1 Constant Number of Instances
The above idea can be translated to the algorithm MetaMax(K) shown in Figure 1. Here
we consider the case when we have a fixed number of instances, and our goal is to perform
(almost) as well as the best of them (in hindsight), while using the minimum number of
br
evaluations of f . Note the slight abuse of notation that in the MetaMax(K) algorithm X
and fr denote the estimates of the algorithm after r rounds (and not r steps/samples).
In the first part of step (a) of MetaMax we sweep over all positive c and select local
search algorithms that maximize the estimate (4). It is easy to see, that if Ai maximizes
414

fiEfficient Multi-Start Strategies for Local Search Algorithms

MetaMax(K): A multi-start strategy with K algorithm
instances.
Parameters: K > 0 and a positive, monotone decreasing function h with
limn h(n) = 0.
Initialization: For each i = 1, . . . , K, take a step with each algorithm Ai
once, and let ni,0 = 1 and fi,0 = f (Xi,1 ).
For each round r = 1, 2, . . .
(a) For i = 1, . . . , K select algorithm Ai if there exists a c > 0 such that
fi,ni,r1 + ch(ni,r1 ) > fj,nj,r1 + ch(nj,r1 )

(5)

for all j = 1, . . . , K such that (ni,r1 , fi,ni,r1 ) 6= (nj,r1 , fj,nj,r1 ). If
there are several values of i selected that have the same step number
ni,r1 then keep only one of these selected uniformly at random.
(b) Step each selected Ai , and update variables. That is, set ni,r =
ni,r1 + 1 if Ai is selected, and ni,r = ni,r1 otherwise. For each
bi,n
selected Ai evaluate f (Xi,ni,r ) and compute the new estimates X
i,r
and fi,ni,r .
(c) Let Ir = argmaxi=1,...,K fi,ni,r denote the index of the algorithm with
the currently largest estimate of f  , and estimate the location of the
br = X
bIr ,n
maximum with X
and its value with fr = fIr ,nIr ,r .
Ir ,r
Figure 1: The MetaMax(K) algorithm.
(4) for a particular c = u then there is a closed interval I containing u such that Ai also
maximizes (4) for any c  I. Therefore, in each round, the strategy MetaMax(K) selects
the local search algorithms Ai for which the corresponding point (h(ni,r1 ), fi,ni,r1 ) is a
corner of the upper convex hull of the set
Pr = {(h(nj,r1 ), fj,nj,r1 ) : j = 1, . . . , K}  {(0, max fj,nj,r1 )}.
1jK

(6)

The selection mechanism is illustrated in Figure 2.
To avoid confusion, note that the random selection in step (a) of MetaMax(K) implies
that if all algorithms are in exactly the same state, that is, (ni,r1 , fi,ni,r1 ) = (nj,r1 , fj,nj,r1 )
for all i, j, then one algorithm is selected uniformly at random (this pathological situation
may arise, e.g., at the beginning of the algorithm or if all the local search algorithms give
the same estimate of f  for some range of step numbers). Apart from the case when one of
the least used algorithms provides the currently best estimate, which happens surely in the
first round but usually does not happen later (and includes the previous pathological case),
it is guaranteed that in each round we use at least two algorithms, one with the largest
415

fiGyorgy & Kocsis

0.7

0.6

f(x)

0.5

0.4

0.3

0.2

0.1

0
0.2

0.3

0.4

0.5

h(n)

0.6

0.7

0.8

0.9

Figure 2: Selecting algorithm instances in MetaMax: the points represent the algorithm
instances, and the algorithms that lie on the corners of the upper convex hull
(drawn with blue lines) are selected.

estimate fi,ni,r1 = fr1 (for very small values of c), and one with the smallest step number
nj,r1 (for very large values of c). Thus, usually at most half of the total number of function
calls to f can be used by any optimal local search algorithm. This observation gives a practical lower bound (which is valid apart from the pathological situation mentioned above) on
the proportion of function calls to f made by optimal local search algorithms; surprisingly,
Theorem 6 below shows that this lower bound is achieved by the algorithm asymptotically.
The randomization in step (a) that precludes using multiple instances with the same
step number is introduced to speed up the algorithm in certain pathological cases. For
example, if A1 converges to the correct estimate, while all the other algorithms A2 , . . . , AK
produce the same estimate in each round, independently of their samples, that is inferior
to the estimates of A1 , then if we use the randomization, half of the calls to compute f will
be made by A1 , but without the randomization this would drop down to 1/K as in each
round we would use each algorithm. Furthermore, we could take a step with all algorithms
that lie on the convex hull, but similar pathological examples can be constructed when it is
more beneficial to use only algorithms on the corners. On the other hand, it almost never
happens in practice that three algorithms lie on the same line, and so algorithms typically
never fall at non-corner points of the convex hull.
In the remainder of this section we analyze the performance of the MetaMax(K)
algorithm. Proposition 2 shows that the algorithm is consistent in the sense that its performance asymptotically achieves that of the best algorithm instance as the number of rounds
increases. To understand the algorithm better, Lemma 3 provides a general sufficient condition that an algorithm instance is not advanced in a given round, while, based on this
result, Lemma 4 provides conditions that ensure that suboptimal algorithm instances are
not used in a round if they have been stepped too many times (i.e., they have evaluated f
at too many points) before. Lemma 5 gives an upper bound on the number of algorithm
416

fiEfficient Multi-Start Strategies for Local Search Algorithms

instances used in a round. The results of the lemmas are then used to show in Theorems 6
and 8 and Remark 9 that optimal algorithm instances are used (asymptotically) at least
at a minimum frequency that, in turn, yields the asymptotic rate of convergence of the
MetaMax(K) algorithm.
The following proposition shows that the MetaMax(K) algorithm is consistent in a
sense:
Proposition 2 The MetaMax(K) algorithm is consistent in the sense that fr  f  for
all r, and
o
n
f   lim fi,n .
f   lim fr = min
r

i=1,...,K

n

Proof The proof follows trivially from the fact that each algorithm is selected infinitely
often, that is, limr ni,r = . To see the latter, we show that in every K rounds the
number of steps taken by the least used algorithm, that is, mini=1,...,K ni,r , is guaranteed
to increase by one. That is, for all k  0,
min ni,kK  k.

i=1,...,K

(7)

As described above, in each round we select exactly one of the algorithms that have made
the least number of steps. Thus, if there are K such algorithms, the minimum step number
per algorithm will increase in K rounds, which completes the proof.
2
The MetaMax(K) algorithm is more efficient if suboptimal algorithms do not step too
often. The next lemma provides sufficient conditions that an algorithm is not used in a
given round.
Lemma 3 An algorithm instance Aj is not used in any round r + 1 of the MetaMax(K)
algorithm, if there are algorithms Ai and Ak such that fi,ni,r > fj,nj,r > fk,nk,r and either
ni,r  nj,r or


h(nj,r )
h(nj,r )


fj,nj,r  fi,ni,r 1 
(8)
+ fk,nk,r
h(nk,r )
h(nk,r )
Proof As in each round the algorithms at the corners of the convex hull Pr+1 are used, it
is easy to see that an algorithm Aj is not used in a round r if there are algorithms Ai and
Ak such that fi,ni,r > fj,nj,r > fk,nk,r and either ni,r  nj,r or
fi,ni,r  fk,nk,r
fi,ni,r  fj,nj,r

.
h(nj,r )  h(ni,r )
h(nk,r )  h(ni,r )

(9)

To finish the proof we show that (8) implies the latter. Indeed, (9) is equivalent to
h(nk,r )(fi,ni,r  fj,nj,r )  h(nj,r )(fi,ni,r  fk,nk,r ) + h(ni,r )(fk,nk,r  fj,nj,r ).
As the last term in the right hand side of the above inequality is negative by our assumptions,
the inequality is satisfied if
h(nk,r )(fi,ni,r  fj,nj,r )  h(nj,r )(fi,ni,r  fk,nk,r )
417

fiGyorgy & Kocsis

which is equivalent to (8).

2

The above lemma provides conditions on not using some algorithm instances in a certain
round that depend on the actual performance of the instances. The next result gives similar
conditions, however, based on the best estimates (usually local optima) achievable with the
algorithms. Let fi = limr fi,ni,r be the asymptotic estimate of algorithm Ai for f  , and
let f = max1iK fi denote the best estimate achievable using algorithms A1 , . . . , AK .
Let O  {1, . . . , K} be the set of optimal algorithms that converge to the best estimate
f (for these algorithms), and let |O| denote the cardinality of O (i.e., the number of
optimal algorithm instances). Note that O is a random variable that depends on the actual
realizations of the possibly randomized search sequences of the algorithms. The next lemma
shows that if j 6 O, then Aj is not used at a round r if it has been used too often so far.
Lemma 4 Let

 = f  max fj
j6O

denote the margin between the estimates of the best and the second best algorithms. Then
for any 0 < u <  there is a random index R(u) > 0 such that for any j 6 O, Aj is not
used by MetaMax(K) at a round r + 1 > R(u) if
!!



f
j
min ni,r
1
nj,r  h1 h
.
(10)
i=1,...,K
f  u
Furthermore, let 0 <  < 1, for any i  O, let g,i denote the convergence rate of algorithm
Ai guaranteed by Lemma 1, and let g (n) = maxiO g,i (n) for all n. Then P (R(u) 
 )  1   (where g 1 is the generalized inverse of g ), and no suboptimal
Kg1 (u)|f1 , . . . , fK


algorithm Aj , j 6 O, is used for any r > Kg1 (u) with probability at least 1   |O| given the
.
limiting estimates f1 , . . . , fK
Proof Let i  O. Since limn fi,n = fi = f by assumption and (7) implies that
(u)
limr ni,r = , there is an almost surely finite random index Ri > 0 such that for all
(u)

r > Ri we have f  fi,n
 u and so
i,r
f  fr  u.

(11)

Using Lemma 1 we can easily derive a high probability upper bound on R(u) . Since for any
r > Kg1 (u) = R , (7) implies ni,r  g1 (u), Lemma 1 yields


fi
P fi  fi,ni,r  u for all r > R fi fi = f  1  .

It follows that with probability at least 1   |O| there is an i  O such that fi  fi,ni,r  u,
 )   |O| . Thus, to
which implies that R(u) can be chosen such that P (R(u) > R |f1 , . . . , fK
prove the lemma, it is enough to show (10).
Clearly, by (11), the algorithm will pick the estimate of one of the best algorithms after
R(u) rounds. Let Ak be an algorithm with the least number of steps taken up to the end
418

fiEfficient Multi-Start Strategies for Local Search Algorithms

of round r, that is, nk,r = mini ni,r . If fk,nk,r  fj,nj,r then Aj is not used in round r + 1.
Moreover, since Aj 6 O, fj,nj,r  fj < fr and in this case Aj is not used in round r + 1
if nj,r  nIr ,r (recall that nIr ,r is the number of evaluations of f initiated by the actually
best algorithm Ir ). Therefore, Lemma 3 implies that Aj is not used for r > R() if
!
fj,nj,r  fk,nk,r
h(nj,r )  h(nk,r ) 1 
.
fr  fk,n
k,r

This is clearly satisfied if
h(nj,r )  h(min ni,r ) 1 
i

fj
f  u

!

(12)

for any 0 < u < , since by (11) we have
1

fj
fj,nj,r  fk,nk,r
fj,nj,r
1
.
1
fr  fk,nk,r
fr
f  u

Applying the inverse of h to both sides of (12) proves (10), and, hence, the lemma.

2

The above result provides an individual condition for each suboptimal algorithm for
not being used in a round. On the other hand, if one of the optimal algorithms has been
stepped sufficiently many times, we can give a cumulative upper bound on the number of
suboptimal algorithms used in each round.
Lemma 5 Assume that h decreases asymptotically at least exponentially fast, that is, there
exist 0 <  < 1 and n > 0 such that h(n+1)
h(n) <  for all n > n . Assume that r is large
enough so that ni,r > n for all i, and let r = 1  maxi:fi,n

6=fr
i,r

fi,ni,r
fr

> 0. Then at most

r
 ln
ln   + 1 algorithms are stepped in round r + 1, where x denotes the smallest integer at
least as large as x.

Proof Let i0 , i1 , . . . , im denote the indices of the algorithms chosen in round r + 1 with
fi0 ,r < fi1 ,r <    < fim ,r = fr . Then Lemma 3 implies that ni0 ,r < ni1 ,r <    < nim ,r and
fr  fik ,r < (fr  fik1 ,r )
for all k = 1, . . . , m  1. Repeated application of the above inequality implies
fr r  fr  fim1 ,r < (fr  fim2 ,r ) <    < m1 (fr  fi0 ,r )  fr m1
which yields

ln r
ln 
As we assumed that m + 1 algorithms were chosen in round r + 1, this fact finishes the
proof.
2
m1<

419

fiGyorgy & Kocsis

Based on Lemmas 4 and 5, the next theorem shows that if the local search algorithm
converges fast enough (exponentially with a problem dependent rate, or faster than exponential) then half of the function calls to evaluate f correspond to optimal algorithm
instances.
Theorem 6 Assume that the performance of the algorithms Ai , i = 1, . . . , K are not all
the same, that is, |O| < K, and suppose that
(
)
fj
h(n + 1)
lim sup
.
(13)
< min 1 
j6O
h(n)
n
f
Then asymptotically at least half of the function
respond to an optimal algorithm. That is,
P
ni,r
1
P lim inf PiO

K
r
2
i=1 ni,r

calls to evaluate f in MetaMax(K) cor!
fi
fi 

fi f1 , . . . , fK
= 1.
fi

Furthermore, for any 0 <  < 1 and  > 0 there is a constant R(,) > 0 such that
$P
%!
K
n
i=1 i,r
f  fr  g
(2 + )|O|

(14)

(15)

 simultaneously for all r > R(,) , where
with probability at least 1  |O| given f1 , . . . , fK
(,)
g is defined in Lemma 4, and the threshold R
> 0 depends on , , h, g1 and , where
1
g and  are also defined in Lemma 4.

Proof We show that a suboptimal Aj is not chosen for large enough r if nj,r > mink nk,r .
By Lemma 4, it is sufficient to prove that, for large enough r,
!!
fj
1
(16)
min nk,r + 1  h
h(min nk,r ) 1 
k
k
f  u
for some 0 < u <  (recall that here r should be larger than R(u) , the almost surely finite
random index of Lemma 4).
As the minimum in (13) is taken over a finite set, it follows that there exists a small
enough positive u <  such that
(
)
fj
h(n + 1)
lim sup
,
(17)
 min 1 
j6O
h(n)
n
f  u
which clearly implies (16) as limr mink nk,r =  by (7). This fact finishes the proof of
(14), the first part of the theorem.
Next we prove (15). Let Nu > 0 be a threshold such that (17) holds for all n  Nu .
Furthermore, by Lemma 1 and the union bound, (1) holds for each local search algorithm
Ai with g,i in place of g simultaneously for all i  O with probability at least 1  |O|.
420

fiEfficient Multi-Start Strategies for Local Search Algorithms

Then (7) and a slight modification of Lemma 4 imply that (16) holds simultaneously for
.
all r > K max{g1 (u), Nu } = R with probability at least 1  |O| given f1 , . . . , fK
Since
in each such round at most two algorithms are used, for any r > R + c we have
P
n
i,r
c+R
PiO
> 2c+KR
 with high probability. Since the latter is bounded from below by 1/(2+)
K
n
i,r
i=1
PK
P
ni,r
for c  R (K2)/, we have iO ni,r  i=1
for any r > R(,) = R +R (K2)/
2+
with
l P highmprobability. Then there is an algorithm Ai , i  O such that it is used in at least
K
i=1

ni,r
|O|(2+)

rounds, implying the statement of the theorem via Lemma 1.

2

Remark 7 The above proof of Theorem 6 is based on Lemma 4. A proof based on Lemma 5
is also possible, since setting  = minj6O (1  fj /f ) in the lemma, (13) implies that, for
large enough r, r  , and so each round is approximately of length 2 by the lemma.
It may happen that although the decay rate of h is exponential, it is not quite fast
enough to satisfy (13), so the optimal scenarios in the above theorem do not hold. In this
case it turns out that the number of algorithms converging to the same local maximum
plays the key role in determining the usage frequency of the optimal algorithms.
Theorem 8 Assume that the estimates provided by the algorithms A1 , . . . , AK converge to
N > 1 distinct limit points, such that k0 = |O| algorithms converge to f , and k1 , k2 , . . . , kN 1
algorithms converge to each suboptimal limit points, respectively. Suppose furthermore
that h decreases asymptotically at least exponentially fast, that is, for some 0 <  < 1,
lim supn h(n+1)
h(n) < . Then
P lim inf
r

P

iO ni,r
PK
i=1 ni,r

!
fi
fi 
kmax

fi f , . . . , fK = 1.

K  k0 + kmax fi 1

where kmax = max1iN 1 ki .
Furthermore, using the definitions of Lemma 4 for any 0 <  < 1 and  > 0 there is a
constant threshold R(,)
%!
$
PK
k
n
max
i=1 i,r
f  fr  cg
(K  k0 + kmax + )|O|
 simultaneously for all r > R(,) , where
with probability at least 1  |O| given f1 , . . . , fK
R(,) depends on , .,f1 , . . . , sfK and the convergence rate of all the algorithms4 .
 are given, and fix the random trajectories of all the algorithms. If
Proof Suppose f1 , . . . , fK
there is a single suboptimal algorithm then the statement is trivial as kmax /(K k0 +kmax ) =
1/2 and in each round at least two algorithms are used, and at most one of them can be
the suboptimal one. From now on assume there are at least two suboptimal algorithms.
Assume that Aj and Ak converge to suboptimal local maxima (strictly less than f ). For
any r large enough, an optimal algorithm Ai is better than any of the suboptimal ones, that

4. Instead of the convergence rate of all algorithms, R(,) may be defined to be dependent on g and .

421

fiGyorgy & Kocsis

is, if Ai converges to f then fi,ni,r > fj,nj,r , fk,nk,r . Assume, without loss of generality, that
fj,nj,r  fk,nk,r . If nj,r  nk,r then clearly only Aj can be chosen in round r + 1. Assume
nj,r > nk,r . Since fj,nj,r and fk,nk,r are convergent sequences (as r  ), for large enough
r we have, for some j,k < 1,
1

fj,nj,r  fk,nk,r
> j,k  tj,k


fi,n  fk,n
i,r

(18)

k,r

where tj,k = ln j,k / ln  is a positive integer. Note that if Aj and Ak converge to the
same point, that is, limr (fj,nj,r  fk,nk,r ) = 0, the second term on the left hand side of
(18) converges to 0, and so j,k can be chosen to be , implying tj,k = 1. Rearranging the
above inequality one obtains
(1  tj,k )fi,ni,r + tj,k fk,nk,r > fj,nj,r .

(19)

If nj  nk  tj,k , then the conditions on h and the fact that both nj,r and nk,r tend to
infinity as r   (recall (7)) imply that, for large enough r, h(nj,r )/h(nk,r ) < tj,k . Since
fi,ni,r  fk,nk,r for large enough r, from (19) we obtain


h(nj,r ) 
h(nj,r ) 
tj,k 
tj,k 

fk,nk,r .
fi,ni,r +
fj,nj,r < (1   )fi,ni,r +  fk,nk,r  1 
h(nk,r )
h(nk,r )
Thus, by Lemma 3, if r is large enough, Aj cannot be used in round r + 1 if nj,r  nk,r  tj,k .
Since both nj,r and nk,r tend to infinity, it follows that, for large enough r,
|nj,r  nk,r |  tj,k

(20)

for any two suboptimal algorithms Aj and Ak . Note that this fact also implies that from
any set of suboptimal algorithms converging to the same point, eventually at most one can
be used in any round (since the corresponding thresholds are tj,k = 1).
Clearly, (7) implies that both nj,r and nk,r grow linearly with r, and since their differences is bounded by (20), limr nj,r /nk,r = 1. Therefore, for any suboptimal algorithm
Aj , we have limn nj,r /r  1/kmax (this is the maximal rate of using elements from the
largest group of suboptimal algorithms converging to the same local optimum). Finally, as
an optimal algorithm is used in each round r, for large enough r, we have
P
P
iO ni,r
iO ni,r
P
lim inf PK
= lim inf P
r
r
n
+
i,r
n
i6O ni,r
iO
i,r
i=1
kmax
r
 lim
=
,
r
r r + (K  k0 )
K  k0 + kmax
kmax
where we used the fact that a/(a + b) is an increasing function of a for a, b > 0. Since the
,
above inequality holds for all realizations of the trajectories of A1 , . . . , AK , given f1 , . . . , fK
the first statement of the theorem follows.
The second statement follows similarly to (15) in Theorem 6. Since the exact value of
R(,) is not of particular interest, the derivation is omitted.
2

422

fiEfficient Multi-Start Strategies for Local Search Algorithms

Remark 9 The main message of the above theorem is the somewhat surprising observation
that suboptimal algorithms are slowed down if there is a large group of suboptimal algorithms
converging to the same local optimum; the rate suboptimal algorithms are used at is bounded
by the the size of the largest such group.
4.2 Unbounded Number of Instances
It is clear that if the local search algorithms are not consistent (i.e., they do not achieve
the global optimum f  ), then, despite its favorable properties, the MetaMax(K) strategy
is inconsistent, too. However, if we increase the number of algorithms to infinity then we
get the consistency from random search, while still keeping the reasonably fast convergence
rate from MetaMax(K).
Clearly, one needs to balance between exploration and exploitation, that is, we have
to control how often we introduce a new algorithm. One solution is to let the MetaMax
algorithm solve this problem: the MetaMax() algorithm, given in Figure 3, is the extension of MetaMax(K) that is able to run with infinitely many local search algorithm
instances. Here a major issue is that new local search algorithms have to be started from
time to time (this ensures that the algorithm will converge to the global maximum of f
since it also performs a random search): this is implemented by modifying step (a) of the
MetaMax(K) algorithm so that a new, randomly initialized local search algorithm is introduced in each round (randomly selecting one algorithm uniformly from the infinitely
many possible algorithms not used so far). Obviously, we have to skip the initialization
step of MetaMax(K) and start each algorithm with 0 samples. To better control the
length of each round (i.e., the exploration), in each round r we allow the use of a different
function h, denoted by hr1 that may depend on any value measured before round r (this
is suppressed in the notation). As before, we assume that hr (0) = 1, hr (n) is monotone decreasing in n, and limn hr (n) = 0 for all r. Typically we will make hr1 be dependent on
PKr1
the total number of steps (i.e., the function calls to evaluate f ) tr1 = i=1
ni,r1 made
by all the algorithms before round r, where Kr1 is the number of algorithm instances used
before round r; note that Kr1 = r  1 for all r, as we start exactly one new algorithm in
each round.
It is desired that, although the number of local search algorithms grows to infinity,
the number of times the best local search algorithm is advanced by the MetaMax()
algorithm approaches infinity reasonably fast. Somewhat relaxing the random initialization
condition, we may imagine a situation where the local search algorithms are initialized in
some clever, deterministic way, and so in the first few steps they do not find any better
value than their initial guesses. If all algorithms are optimal (this may be viewed as a result
of the clever initialization), then they may provide, for example, the identical estimates
0.5, 0.5, 1 for the first three steps. Then it is easy to see that each algorithm is stepped
exactly twice, thus no convergence to the optimum (which would be found after the third
step) is achieved. Although the random initialization of the search algorithms guarantees
the consistency of MetaMax() (see Proposition 10 below), robust behavior in even such
pathological cases is preferred.
This can be achieved by a slight modification of the algorithm: if in a round a local search
algorithm overtakes the currently best algorithm, that is, Ir 6= Ir1 , then algorithm AIr
423

fiGyorgy & Kocsis

MetaMax(): A multi-start strategy with infinitely many
algorithm instances.
Parameters: {hr }, a set of positive, monotone decreasing functions with
limn hr (n) = 0.
For each round r = 1, 2, . . .
(a) Initialize algorithm Ar by setting nr,r1 = 0, fr,0 = 0.
(b) For i = 1, . . . , r select algorithm Ai if there exists a c > 0 such that
fi,ni,r1 + chr1 (ni,r1 ) > fj,nj,r1 + chr1 (nj,r1 )
for all j = 1, . . . , r such that (ni,r1 , fi,ni,r1 ) 6= (nj,r1 , fj,nj,r1 ). If
there are several values of i selected that have the same step number
ni,r1 then keep only one of these selected uniformly at random.
(c) Step each selected Ai , and update variables. That is, set ni,r =
ni,r1 + 1 if Ai is selected, and ni,r = ni,r1 otherwise. For each
bi,n
selected Ai evaluate f (Xi,ni,r ) and compute the new estimates X
i,r
and fi,ni,r .

(d) Let Ir = argmaxi=1,...,r fi,ni,r denote the index of the algorithm with
the currently largest estimate of f  , and estimate the location of the
br = X
bIr ,n
maximum with X
and its value with fr = fIr ,nIr ,r .
Ir ,r
Figure 3: The MetaMax() algorithm.

is stepped several times until it is used more times than AIr1 .5 The resulting algorithm,
called MetaMax, is given in Figure 4. Note that the algorithms MetaMax() and
MetaMax conceptually differ only at one place: step (c) is extended with step (c) in
the new algorithm. As a result, a technical modification also appears in step (d), and, to
simplify the presentation of the MetaMax algorithm, a slight, insignificant modification is
also introduced in step (b), see the discussion below.
The modification in MetaMax is not really significant in the practical examples we
studied (see Section 5), as the number of steps taken by an algorithm that overtakes the
currently best algorithm grows very quickly also in the MetaMax() algorithm, since in
MetaMax an overtake usually introduces very short rounds (close to the minimum length
two in many cases) until the leading algorithm becomes also the most used one. The goal
of the modification in step (b) is only to synchronize the choice of the optimal algorithms
in steps (b) and (c). An equally good solution would be to choose, in case of a tie in step
5. In this way we achieve that the actually best algorithm dominates all others in terms of both accuracy
and the number of calls made by the algorithms to compute the target function. This is the same type
of dominance as used by Hutter et al. (2009) in a slightly different context.

424

fiEfficient Multi-Start Strategies for Local Search Algorithms

MetaMax: A multi-start strategy with infinitely many
algorithm instances.
Parameters: {hr }, a set of positive, monotone decreasing functions with
limn hr (n) = 0.
For each round r = 1, 2, . . .
(a) Initialize algorithm Ar by setting nr,r1 = 0, fr,0 = 0.
(b) For i = 1, . . . , r select algorithm Ai if there exists a c > 0 such that
fi,ni,r1 + chr1 (ni,r1 ) > fj,nj,r1 + chr1 (nj,r1 )
for all j = 1, . . . , r such that (ni,r1 , fi,ni,r1 ) 6= (nj,r1 , fj,nj,r1 ). If
there are several values of i selected that have the same step number
ni,r1 then keep only one of these that has the smallest index.
(c) Step each selected Ai , and update variables. That is, set ni,r =
ni,r1 + 1 if Ai is selected, and ni,r = ni,r1 otherwise. For each
bi,n
selected Ai evaluate f (Xi,ni,r ) and compute the new estimates X
i,r

and fi,ni,r .

(c) Let Ir = argmaxi=1,...,r fi,ni,r denote the index of the algorithm with
the currently largest estimate of f  (in case Ir is not unique, choose
the one with the smallest number of steps ni,r ). If Ir 6= Ir1 , step
algorithm AIr (nIr1 ,r  nIr ,r + 1) times and set nIr ,r = nIr1 ,r + 1.
br = X
bIr ,n
(d) Estimate the location of the maximum with X
and its
Ir ,r
value with fr = fIr ,nIr ,r .
Figure 4: The MetaMax algorithm.

(c), an algorithm that was used in the current round. Also note that, as a result of the
modifications, the currently best algorithm (with index Ir ) has taken the most steps, and so
the extra number of steps taken in step (c) is indeed positive. An important consequence
of the modifications is that, in any round r, the number of steps taken by the local search
algorithm AIr , which is the best at the end of the round, is between r and 2r (see Theorem 15
below).
The rest of the section is devoted to the theoretical analysis of MetaMax() and
MetaMax, following the lines of the analysis provided for MetaMax(K). First, in Proposition 10, it is shown that the algorithm is consistent, that is, the solution found by the
algorithm actually converges to f  . Lemma 12 (a counterpart of Lemma 4) shows that
suboptimal algorithms can make only finitely many steps, while Lemma 14 gives an upper
bound on the length of each round. The main theoretical results of this section apply only
425

fiGyorgy & Kocsis

to the MetaMax algorithm: Theorem 15 gives a lower bound on the number of steps taken
by the actually best algorithm by the end of a given round, while, as a consequence, Theorem 16 shows the rate of convergence of the algorithm as a function of the total number of
steps (i.e., function calls to evaluate f ) used by all algorithm instances: it turns out that at
most quadratically more steps are needed than for a generic local search algorithm instance
that converges to the optimum.
Since the MetaMax() and the MetaMax strategies perform a random search (the
number of algorithms tends to infinity as the length of each round is finite), the algorithms
are consistent:
Proposition 10 The strategies MetaMax() and the MetaMax are consistent. That
is,
lim fr = f 
r

almost surely.
Proof Clearly, the event that fr does not converge to f  can be written as
n

 \
 n
 [
o
o
[
lim fr 6= f  =
fr < f   1/n

r

(21)

n=1 R=1 r=R

Now the continuity of f implies that, for any n, if X is chosen uniformly from [0, 1]d then
qn P
= P(f (X) > f   1/n) > 0. Thus, for any round r, P(fr < f   1/n)  (1  qn )r , and


so 
r=1 P(fr < f  1/n) is finite. Therefore, the Borel-Cantelli lemma (see, e.g., Ash &
Doleans-Dade, 2000) implies that
!
 n
 [
o
\

=0
P
fr < f  1/n
R=1 r=R

for all n. This, together with (21) finishes the proof, as
P



lim fr 6= f

r








X

n=1

P

 n
 [
\

R=1 r=R

fr < f  1/n


o

!

= 0.
2

In the reminder of this section we will assume that local search algorithms that achieve
an almost optimal value eventually converge to the optimum.
Assumption 11 Let F   R denote the set of local maxima of f , and let  = f  
supfF  ,f<f  f. We assume that  > 0 and if an algorithm Ai is such that fi,n > f   
for some n, then limn fi,n = f  .
If the local search algorithms converge to local optima (which is a reasonable assumption
in practice), the above assumption is usually satisfied: the only situation when it does not
hold is the pathological case when f has infinitely many local maxima and the set of these
maxima is dense at the global maximum.
426

fiEfficient Multi-Start Strategies for Local Search Algorithms

Under Assumption 11 we can prove, similarly to Lemma 4, that any suboptimal algorithm is selected only a limited number of times that increases with h1
r . In particular, if
hr = h for all r large enough, then any suboptimal algorithm is chosen only finitely many
times.
Lemma 12 Suppose Assumption 11, and let q = P(f (X) > f   /2) for an X uniformly
distributed in [0, 1]d . Then, for both the MetaMax() and the MetaMax algorithms, a
suboptimal algorithm Aj started before round r +1 is not used at round r +1, with probability
at least 1  (1  q)r , if



1
nj,r  hr
.
2f   
In addition, if hr (n) is a non-decreasing function of r for all n, then
lim sup
r

h1
r1

n
 j,r



2f  

 <

almost surely.

(22)

In particular, if hr is a constant function, that is, hr = h0 for all r, then limr nj,r < 
almost surely.
Remark 13 Note that in the second
 part of the lemma we coulddrop the
 monotonicity
1
1


assumption on hr and replace hr1 2f   with max0r r1 hr 2f   in (22).
Proof Consider if algorithm Aj is used at a round r + 1. First note that with probability
at least 1  (1  q)r , fr > f   /2. Furthermore, the newly introduced algorithm, Ar+1 is
not used yet, and we have nr+1,r = 0 and fr+1,0 = 0. Thus, by Lemma 3, Aj is not used if


h(nj,r )


= fr (1  hr (nj,r )) .
fj,nj,r  fr 1 
hr (0)
Since this is equivalent to
nj,r 
and
h1
r

fj,nj,r
1
fr

h1
r

fj,nj,r
1
fr

!



by

h1
r



!

,


2f   



fr  fj,nj,r

fr  (f   )
(f   /2)  (f   )
= 
,
(23)

>

f  /2
2f  
fr
fr
the first statement of the proof follows.
b denote the first round in which there is an optimal
To prove the second part, let R


algorithm Ai with fi,ni,Rb > f  /2. Then for any suboptimal algorithm Aj , the first part
b
of the lemma implies that, for any r > R,










1
1
b
b
+ 1 = max R, hr1
+1
nj,r  max R, max
h
0r  r1 r
2f   
2f   
427

fiGyorgy & Kocsis

where the equality holds since h1
r (n) is non-decreasing in r. Thus




b
nj,r
R
1
  lim sup max
, 1 +




lim sup



 h1

r h1
r
h1
r1 2f  
r1 2f  
r1 2f  




b
1
R
, 1 +



 max



 h1
h1
0

0

2f  

(24)

2f  

b is finite, (24) is also finite with
where we used that h1
is non-decreasing in r. Since R
r
probability 1.
2
A simple modification of Lemma 5 implies that once a /2-optimal sample point is
found then only a limited number of suboptimal algorithms is chosen in each round.
Lemma 14 Consider algorithms MetaMax() and MetaMax. Suppose Assumption 11
holds, and assume that f   fR < /2 for some R > 0. In anyround r > R, if hr (n) = rn


for some 0 < r < 1 and for all n  0, then at most

ln 2f 
ln(1/r )

algorithms are chosen that

have estimates fj  f   .

Proof The proof follows from Lemma 5 taking into account that any suboptimal algorithm
Aj satisfies fj  f    and that at least one optimal algorithm is chosen in each round
r > R: Similarly to (23), r defined in Lemma 5 can be bounded as r > /(2f  ),
and so
l
m  2f   
ln
ln(1/r )

the number of suboptimal algorithms used in round r is bounded by ln(1/
.
 ln(1/
r)
r)
2
Finally we can derive the convergence rate of the algorithm MetaMax. First we bound
the number of steps taken by the currently best algorithm, both in terms of the number of
rounds and the total number of steps taken by all the local search algorithms.
Theorem 15 Consider the MetaMax algorithm. At the end of any round r the number
of steps taken by the currently best algorithm is between r and 2r. That is,
r  nIr ,r < 2r.

(25)

Furthermore, the number of calls nIr ,r to evaluate f by the currently
best algorithm AIr can
Pr
be bounded by a function of the total number of times tr = i=1 ni,r the target function f
is evaluated by all local search instances as

2tr + 7  1
nIr ,r 
.
(26)
2
Proof The first statement of the lemma is very simple, since in any round the actually
best algorithm takes one step if there is no overtaking, and one or two steps if there is
428

fiEfficient Multi-Start Strategies for Local Search Algorithms

overtaking. Indeed, in any round r  2, if there is no overtaking, that is, Ir = Ir1 ,
then nIr ,r = nIr ,r1 + 1. Otherwise, if Ir 6= Ir1 , then nIr ,r = nIr1 ,r + 1, and since
0  nIr1 ,r  nIr1 ,r1  1, we have
1  nIr ,r  nIr1 ,r1  2
in all situations. Since in the first round clearly the only algorithm used takes 1 step, that
is, nI1 ,1 = 1, (25) follows.
To prove the second part, notice that in any round r, at most nIr1 ,r1 + 1 algorithms
can be stepped in step (c) as no algorithm can be used that has taken more steps than the
currently best one. Also, in step (c) no extra samples are used if there is no overtaking.
In case of overtaking, AIr has to be advanced in step (c), as well as AIr1 , and so at most
nIr1 ,r1 + 1 extra steps has to be taken by AIr . Therefore,
tr  tr1 + 2nIr1 ,r1 + 2.
Thus, since no overtaking happens in round 1, we obtain
tr  1 +

r
X

2(nIs1 ,s1 + 1).

s=2

Then, by (25) we have
tr  1 + 4

r
X
s=2

s = 1 + 2(r + 2)(r  1)  1 + 2(nIr ,r + 2)(nIr ,r  1)

which yields (26).

2

Note that in the proof we used a crude estimate on the length of a usual round (without
overtaking) relative to, for example, Lemma 14. This, however, affects the result only by a
constant factor as long as we are not able to bound the number of rounds or the number
of extra steps taken when overtaking happens, since the effect of the overtakings itself
introduces the quadratic dependence in the proof of (26). Experimental results in Section 5
show (see Figure 10) that the number of algorithm instances (which is in turn the number r
of rounds) has a usual growth rate of (tr / ln tr ), which, if taken into account, may sharpen
the bound on how often the best algorithm is chosen.
Under Assumption 11, the random search component of MetaMax implies that eventually we will have an optimal algorithm that is the best. From that point the convergence
rate of the optimal local search algorithms determine the performance of the search, and
the number of steps taken by the best local search algorithm is bounded by Theorem 15.
Theorem 16 Suppose Assumption 11 holds. Then there is an almost surely finite random
index R such that for all rounds r > R, the estimate fr of the MetaMax algorithm and
the total number of steps tr taken by all local search algorithms up to the end of round r
satisfies


2t
+
7

1
r

f  fr  g
2
with probability at least 1  , where g is defined by Lemma 1 for the global maximum f  .
429

fiGyorgy & Kocsis

Remark 17 (i) The value of R can be bounded with high probability using the properties
of uniform random search for the actual problem; this would yield similar bounds as in
Theorems 6 and 8 for the MetaMax(K) algorithm. (ii) Note the exploration-exploitation
trade-off in the MetaMax algorithm: the value of R is potentially decreased if we introduce
new algorithms more often, while nIr ,r is reduced at the same time. (iii) Theorems 15 and 16
imply that, asymptotically, the MetaMax algorithm needs only quadratically more function
evaluations than a local search algorithm
Psthat is ensured to converge to the optimum. In
particular, if f is of the form f (x) =
i=1 fi (x)ISi (x) where the Si form a partition of
[0, 1]d , ISi denotes the indicator function of Si , and the fi belong to some nicely behaving
function class such that a local search algorithm started in Si converges to the maximum
of fi on Si (e.g., f is a piecewise concave function with exponential convergence rate for
the SPSA algorithm, which is used with a sufficiently small step size), then we can preserve
the performance of a local search algorithm for the original function class at the price of
an asymptotically quadratic increase in the number of function calls to evaluate f (i.e., the
total number of steps taken by all local search algorithm instances).
4.3 Discussion on the Results
In some sense the theoretical results presented in the previous sections are weak. The
consistency result for the MetaMax(K) algorithm follows easily from the fact that each
local search algorithm is used infinitely many times, and the consistency of MetaMax()
and MetaMax follows from the consistency of a random search. The performance bounds
provided have a disadvantage that they are asymptotic in the sense that they hold only after
a possibly large number of rounds (a weakness of the bounds is that the minimum number
of rounds is obtained from properties of uniform random search/sampling for the particular
problem, neglecting more attractive properties of the algorithms). In fact, it is quite easy
to construct scheduling strategies that are consistent and asymptotically an arbitrarily
large fraction of the function evaluations (even almost all) is used by optimal local search
algorithms: the explore-and-exploit algorithms achieve both of these goals if the number of
function evaluations to be used is known ahead and they use an arbitrarily small fraction
of the evaluations of the target function f for exploration. We compare the performance of
our algorithms to such explore-and-exploit algorithms in Section 5. In particular, to match
the performance guarantees for the MetaMax family, we use algorithms that spend half
of their time with exploration and half with exploitation, where in the exploration part a
uniform allocation strategy is used when there is a finite number of local search algorithms,
and the schedule of Luby et al. (1993) is used for infinitely many local search algorithms.
Although the theoretical guarantees proved in this paper for the MetaMax family also hold
for these explore-and-exploit algorithms, in all experiments the MetaMax family seems to
behave superior compared to these algorithms, as expected.
The theoretical results also do not give sufficient guidance on how to chose the parameter
h or hr (the time-varying version of h is not considered for the MetaMax(K) algorithm
for simplicity and ease of presentation). Most of our results require a sufficiently fast
exponential decay in h, which is problem dependent and cannot be determined in advance. A
sufficiently fast decay rate would ensure, for example, that for the MetaMax(K) algorithm
we could always use the stronger results of Theorem 6 and would never have to deal with
430

fiEfficient Multi-Start Strategies for Local Search Algorithms

the case when only the bound of Theorem 8 holds. One may easily choose h to be any
function that decreases super-exponentially: this would make the asymptotic bounds work,
however, would slow down exploration (in the extreme case of hr (n)  0, which is excluded
by our conditions, no exploration would be performed, and the algorithms would use only
the actually best local search algorithm). In practice we have always found that it is
appropriate to chose hr decay exponentially. Furthermore, we have found it even more
effective to gradually decrease the decay rate to enhance exploration as time elapses (the
rationale behind this approach is the assumption that good algorithms should have more
or less converged after a while, and so there may be a greater potential in exploration
to improve our estimates). Finally, the connection between g and h should be further
investigated.
Keeping the above limitations of the theoretical results in mind, we still believe that
the theoretical analyses given provide important insight to the algorithms and may guide
a potential user in practical applications, especially since the properties of the MetaMax
family that can be proved in the asymptotic regime (e.g., that the rounds are quite short)
can usually be observed in practice, as well. Furthermore, we think that it is possible to
improve the analysis to bound the thresholds from which the results become valid with
reasonable values, but this would require a different approach and, therefore, is left for
future work.

5. Experiments
The variants of the MetaMax algorithm are tested in synthetic and real examples. Since
there was only negligible difference in the performance of MetaMax() and MetaMax,6
in the following we present results only for MetaMax(K) and MetaMax. First we demonstrate the performance of our algorithm in optimizing a synthetic function (using SPSA as
the local search algorithm). Next the behavior of our algorithm is tested on standard data
sets. We show how MetaMax can be applied for tuning the parameters of machine learning
algorithms: a classification task is solved by a neural network, and the parameters of the
training algorithm (back-propagation) are fine-tuned by MetaMax combined with SPSA.
MetaMax is also applied to boost the performance of k-means clustering. At the end of
the section, we compare the results of the experiments to the theoretical bounds obtained
in Section 4.2.
In all the experiments, in accordance with our simplifying assumptions introduced in
Section 3, the main difference between the individual runs of the particular local search
algorithm is their starting point. Obviously, more general diversification techniques exist:
for example, the parameters of the local search algorithm could also vary from instance to
instance (including running instances of different local search algorithms, where a parameter would select the actually employed search algorithm), and the initialization (starting
point and parametrization) of a new instance could also depend on the results delivered by
6. For example, the relative difference between the average error eMetaMax() of MetaMax() and
eMetaMax of MetaMax in optimizing the parameters of a multi-layer perceptron for learning the letter
data set (see Section 5.1 and especially Figure 6, right for more details) was 0.033 with a standard
deviation of 0.06 (averaged
over 1000 experiments), where the relative difference is defined as
fi
fi
fieMetaMax()  eMetaMax fi / max(eMetaMax() , eMetaMax ).

431

fiGyorgy & Kocsis

existing instances. Although the MetaMax strategies could also be applied to these more
general scenarios, their behavior can be better studied in the simpler scenario; hence, our
experiments correspond only to this setup.
5.1 Optimizing Parameters with SPSA
In this section we compare the two versions of the MetaMax algorithm with six multi-start
strategies, including three with a constant and three with a variable number of algorithm
instances. The strategies are run for a fixed T time steps, that is, the target function can
be evaluated T times, together by all local search instances (note that several reference
strategies use T as a parameter).
We used SPSA (Simultaneous Perturbation Stochastic Approximation; Spall, 1992) as
the base local search algorithm in all cases. SPSA is a local search algorithm with a sampling
function that uses gradient descent with a stochastic approximation of the derivative: at
the actual location Xt = (Xt,1 , . . . , Xt,d ), SPSA estimates the lth partial derivative of f by
f (Xt + t Bt )  f (Xt  t Bt )
,
ft,l (Xt,l ) =
2t Bt,l
where the Bt,l are i.i.d. Bernoulli random variables that are the components of the vector
Bt , and then uses the sampling function st (Xt ) = Xt + at ft (Xt ) to choose the next point
to be sampled, that is,
Xt+1,l = Xt,l + at ft,l (Xt,l )
for l = 1, . . . , d (t and at are scalar parameters).
In the implementation of the algorithm we have followed the guidelines provided in
(Spall, 1998), with the gain sequence at = a/(A + t + 1) , and perturbation size t =
/(t + 1) , where A = 60,  = 0.602 and  = 0.101. The values of a and  vary in the
different experiments; they are chosen heuristically based on our experience with similar
problems (this should not cause any problem here, as the goal of the experiments is not to
provide fast solutions of the global optimization problems at hand but to demonstrate the
behavior of the multi-start algorithms to be compared). In addition to the two evaluations
required at the perturbed points, we also evaluate the function at the current point Xt .
The starting point is chosen randomly, and the function is evaluated first at this point.
The six reference algorithms the MetaMax(K) and MetaMax algorithms are compared to the following:
Unif: This algorithm selects from a constant number of instances of SPSA uniformly.
In our implementation the instance It = t mod K is selected at time t, where K denotes
the number of instances.
ThrAsc: The Threshold Ascent algorithm of Streeter and Smith (2006b). The algorithm begins with selecting each of a fixed number of instances once. After this phase at
each time step t ThrAsc selects the best s estimates produced so far by all algorithm
instances Ai , i = 1, . . . , K in all the previous time steps, and for each Ai it counts how
many of these estimates were produced by Ai . Denoting the latter value by Si,t , at time
t the algorithm selects the instance with index It = argmaxi U (Si,t /ni,t , ni,t ), where ni,t is
432

fiEfficient Multi-Start Strategies for Local Search Algorithms

the number of times the ith instance has been selected up to time t,
U (, n) =  +

+

p
2n + 2
n

and  = ln(2T K/). s and  are the parameters of the algorithm, and in the experiments
the best value for s appeared to be 100, while  was set to 0.01. We note that Threshold
Ascent has been developed for the maximum K-armed bandit problem; nevertheless, it
provides sufficiently good performance in our setup to test it in our experiments.
Rand: The random search algorithm. It can be seen as running a sequence of SPSA
algorithms such that each instance is used for exactly one step, which is the evaluation of
the random starting point of the SPSA algorithm.
Luby: The algorithm based on the work of Luby et al. (1993). This method runs several
instances of SPSA sequentially after each other, where the ith instance is run for ti steps,
with ti defined by

2k1 ,
if i = 2k  1
ti =
ti2k1 +1 ,
if 2k1  i < 2k  1
The above definition produces a schedule such that from the first 2k  1 algorithm instances
one is run for 2k1 steps, two for 2k2 steps, four for 2k3 steps, and so on.
EE-Unif: This algorithm is an instance of explore-and-exploit algorithms. For the first
T /2 steps the Unif algorithm is used for exploration, and, subsequently, in the exploration
phase, the SPSA instance that has achieved the highest value in the exploration phase is
selected.
EE-Luby: This algorithm is similar to EE-Unif, except that Luby is used for exploration.
Both versions of the MetaMax algorithm were tested. Motivated by the fact that SPSA
is known to converge to a global optimum exponentially fast if f satisfies some restrictive
conditions (Gerencser & Vago, 2001), we chose a hr (n) that decays exponentially fast. To
control the exploration of the so far suboptimal algorithm instances, we allowed hr (n) to
be a time-varying function, that is, it changes with tr , the total number of function calls to
evaluate f (or equally, the total number of steps taken) by all algorithms so far. Thus, at
round r + 1 we used


hr (n) = en/

tr

(27)

(note that we used the time-varying version of hr also in the case of MetaMax(K)  the
latter can easily be extended to this situation, but this has been omitted to simplify the
presentation).
For the algorithms with a fixed number of local search instances (MetaMax(K), Unif,
EE-Unif, and ThrAsc), the number of instances K was set to 100 in the simulations, as
this choice provided reasonably good performance in all problems analyzed.
The multi-start algorithms were tested using two versions of a synthetic function, and
by tuning the parameters of a learning algorithm on two standard data sets.
433

fiGyorgy & Kocsis

The synthetic function was a slightly modified7 version of the Griewank function (Griewank,
1981):
d
d
Y
2xl X 4 2 x2l
cos  
f (x) =
100
l
l=1
l=1
where x = (x1 , . . . , xd ) and the xl were constrained to the interval [1, 1]. We show the
results for the 2-dimensional and the 10-dimensional cases.
The parameters of SPSA were a = 0.05 and  = 0.1 for the 2-dimensional case, and
a = 0.5 and  = 0.1 for the 10-dimensional case. The performance of the search algorithms
were measured by the error defined as the difference between the maximum value of the
function (in this case 1) and the best result obtained by the search algorithm in a given
number of steps. The results for the above multi-start strategies for the two- and the 10dimensional test functions are shown in Figure 5. Each error curve is averaged over 10,000
runs, and each strategy was run for 100,000 steps (or iterations). One may observe that in
both cases the two versions of the MetaMax algorithm converge the fastest. ThrAsc is
better than Unif, while Luby seems fairly competitive with these two. The two exploreand-exploit-type algorithms (EE-Unif and EE-Luby) have similar performance on the 2dimensional function, and clearly better than their non-exploiting base algorithms, but on
the 10-dimensional function their behavior is somewhat pathological in the sense that for low
values of T their performances are the best among all algorithms, but with increasing T , the
error actually increases such that their respective base algorithms achieve smaller errors for
some values of T . The random search seems an option only for the 2-dimensional function.
Similar results were obtained for dimensions between 2 and 10. The pathological behavior of
the explore-and-exploit algorithms start to appear gradually starting from the 5-dimensional
function, and it is pronounced from 8 dimensions onwards. Limited experimental data
obtained for higher dimensions up to 100 (averaged over only a few hundred runs) shows
that the superiority of MetaMax is preserved for high-dimensional problems as well.
The reason for the pathological behavior of the explore-and-exploit strategies (i.e., why
the error curves are not monotone decreasing in the number of iterations) can be illustrated
as follows. Assume we have two SPSA instances, one converging to the global optimum
and another one converging to a suboptimal local optimum. Assume that in the first few
steps the optimal algorithm gives better result, then the suboptimal algorithm takes over
and reaches its local maximum, while if the algorithms are run even further, the optimal
algorithm beats the suboptimal one. If the exploration is stopped in the first or the last
regime, an explore-and-exploit algorithm will choose the first, optimal local search instance,
whose performance may get to quite close to the global optimum in the exploitation phase
(even if it is stopped in the first regime). If the exploration is stopped in the middle regime,
the suboptimal search instance will be selected for exploitation, whose performance may
not even get close to the global optimum. In this scenario, the error after the exploitation
phase (i.e. at the end) is lower if T is small, and increases with higher values of T . Decrease
in the error with increasing T is only assured when the optimal instance converges in the
exploration phase past all suboptimal local optima, which results in selecting the optimal
local search instance for exploitation. In the above scenario the error will decrease fast
7. The modification was made in order to have more significant differences between the values of the function
at the global maximum and at other local maxima.

434

fi10

10

1

1

0.1

0.1

average error

average error

Efficient Multi-Start Strategies for Local Search Algorithms

0.01

0.001

0.0001

0.001

0.0001
Rand
Luby
Unif
ThrAsc
EE-Luby
EE-Unif
MetaMax100
MetaMax

1e-05

1e-06

0.01

1

Rand
Luby
Unif
ThrAsc
EE-Luby
EE-Unif
MetaMax100
MetaMax

1e-05

10

100

1000

10000

1e-06

100000

1

10

iteration

100

1000

10000

100000

iteration

Figure 5: The average error for the multi-start strategies on the 2-dimensional (left) and
10-dimensional (right) modified Griewank function. 99% confidence intervals are
shown with the same color as the corresponding curves. Note that most of the
intervals are very small.
1

0.1

0.01

0.01

average error

average error

0.1

0.001

0.0001
Rand
Luby
Unif
ThrAsc
EE-Luby
EE-Unif
MetaMax100
MetaMax

1e-05

1e-06

1

0.001

0.0001

Rand
Luby
Unif
ThrAsc
EE-Luby
EE-Unif
MetaMax100
MetaMax

1e-05

10

100

1000

10000

100000

iteration

1e-06

1

10

100

1000

10000

100000

iteration

Figure 6: The average error for the multi-start strategies on tuning the parameters of Multilayer Perceptron for the vehicle data set (left) and the letter data set (right).
99% confidence intervals are also shown with the same color as the corresponding
curves.

initially, then increase for a while and then it may decrease again till it converges to 0,
which is quite similar to what we observe in Figure 5, right. This pathological behavior
becomes more transparent if there are many local search algorithms, as the length of the
exploitation phase scales with the number of local search instances if the length of the
exploration for each instance is kept fixed. Analyzing the experimental data shows that
more complex versions of the scenario outlined above have occurred in the simulations and
are the main cause of the observed pathological behavior (the non-monotonicity of the error
curves).
For tuning the parameters of a learning algorithm, we have used two standard data
sets from the UCI Machine Learning Repository (Asuncion & Newman, 2007): vehicle and
435

fiGyorgy & Kocsis

letter, and the Multilayer Perceptron learning algorithm of Weka (Witten & Frank, 2005)
(here the back-propagation algorithm is used in the training phase). Two parameters were
tuned: the learning rate and the momentum, both in the range of [0, 1]. The size of the
hidden layer for the Multilayer Perceptron was set to 8, while the number of epochs to
100. The parameters of the SPSA algorithm were a = 0.5 and  = 0.1, the same as for
the 10-dimensional Griewank function (as in the previous experiment, the parameters were
chosen based on our experience). The rate of correctly classified items on the test set for
vehicle using Multilayer Perceptron with varying values of the two parameters is shown in
Figure 7, with the highest rate being 0.910112. Similarly, the classification rate for letter
is shown in Figure 8, with the highest rate being 0.7505.
The error rates of the optimized Multilayer Perceptron on the data sets vehicle and
letter are shown in Figure 6, when the parameters of the learning algorithm were tuned
by the multi-start strategies above. The error in these cases is the difference between the
best classification rate that can be obtained (0.910112 and 0.7505, respectively) and the
best classification rate obtained by the multi-start strategies in a given number of steps.
The results shown are averaged over 1,000 runs. We observe that the MetaMax algorithm
(with an increasing number of algorithm instances) converged fastest in average, the three
strategies with a fixed number of algorithm instances had nearly identical results, Luby
(and its explore-and-exploit variant) was slightly worse than these, and the random search
was the slowest, although it performed not nearly as badly as for the synthetic functions.
The reason why the random search had relatively better performance (relative to those that
used SPSA) could be twofold: (i) large parts of the error surface offer fairly small error,
and (ii) the error surface is less smooth, and therefore SPSA is less successful in using
gradient information. The explore-and-exploit variants performed well on the vehicle data
set initially, but their performance worsened for larger values of T (compared to MetaMax,
and to other algorithms to some extent). This, coupled with the observation for Figure 5,
right would suggest that explore-and-exploit variants are more competitive for small values
of T , despite their asymptotic guarantees.
In summary, the MetaMax algorithm (with an increasing number of algorithm instances) provided by far the best performance in all tests, usually requiring significantly
fewer steps to find the optimum than the other algorithms. E.g., for the letter data set
only the MetaMax algorithm found the global optimum in all runs in 100,000 time steps.
We can conclude that MetaMax converged faster than the other multi-start strategies investigated in all four test cases, with a notable advantage on the difficult surfaces (at least
from a gradient-based optimization viewpoint) induced by the classification tasks.
5.2 k-Means Clustering
In this section we consider the problem of partitioning a set of M d-dimensional real vectors
xj  Rd , j = 1, . . . , M into N clusters, where each cluster Si is represented by a center (or
reconstruction) point ci  Rd , i = 1, . . . , N . The cost function to be minimized is the sum
of distances
PN(x,Pci ) from the data points to the corresponding centers, that is, we want to
minimize i=1 xSi (x, ci ). There are two necessary conditions for optimality (see, e.g.,
Linde, Buzo, & Gray, 1980; Gersho & Gray, 1992): for all i = 1, . . . , N ,
Si = {x : (x, ci )  (x, cj ) for all j = 1, . . . , N }
436

(28)

fiEfficient Multi-Start Strategies for Local Search Algorithms

Figure 7: Classification rate on the vehicle data set. The rates are plotted by subtracting
them from 0.911112 and thus global optima are at the scattered black spots
corresponding to a value equal to 0.001.

Figure 8: Classification rate on the letter data set. The rates are plotted by subtracting them from 0.7515 and thus global optima are at the scattered black spots
corresponding to a value equal to 0.001.

(with ties broken arbitrarily) and
ci = argmin
cRd

X

xSi

437

(x, c).

(29)

fiGyorgy & Kocsis

P

x

8
i
A usual choice for  is the squared Euclidean distance, in which case ci = xS
|Si | . According to the above necessary conditions, the k-means algorithm (or Generalized-Lloyd
algorithm, see, e.g., Linde et al., 1980; Gersho & Gray, 1992) alternates between partitioning the data set according to (28) with the centers being fixed, and recomputing the
centers by (29) while the partitioning is kept fixed. It can easily be seen that the cost
(or error) cannot increase in any of the above steps, hence the algorithm converges to a
local minimum of the cost function. In practice, the algorithm stops when there is no (or
insufficient) decrease in the cost function. However, the k-means algorithm is often trapped
in a local optimum, whose value is influenced by the initial set of centers. As with SPSA,
restarting k-means with a different initialization may result in finding the global optimum.
We consider two initialization techniques: the first, termed k-means, chooses the centers
uniformly at random from the data points; the second, k-means++ (Arthur & Vassilvitskii,
2007) chooses an initial center uniformly at random from the data set, and then chooses the
further centers from the data points with a probability that is proportional to the distance
between the data point and the closest center already selected.

The k-means algorithm usually terminates after a relatively small number of steps, and
thus multi-start strategies with bounded number of instances would run out of active local
search algorithms, and therefore do not appear particularly attractive. However, this is a
natural domain to consider the strategy that starts a new instance, when the previous has
finished. This strategy will be referred subsequently as Serial. For the above mentioned
considerations, we test only MetaMax, the variant of our algorithms applicable for an
unbounded number of instances.9 As in the experiments with SPSA, we used hr as in (27).
Note that some theoretical results indicate that k-means may converge at an exponential
rate (in particular, Kieffer, 1982 showed that the rate of convergence is exponential for
random variables with log-concave densities in 1-dimension provided that the logarithm of
the density is not piecewise affine).
Two multi-start strategies, Serial and MetaMax were tested for the data set cloud
from the UCI Machine Learning Repository (Asuncion & Newman, 2007). The data set
was employed by Arthur and Vassilvitskii (2007) as well. The number of clusters was set to
ten. The performance of the multi-start strategies is defined as the difference between the
smallest cost function obtained by the strategy in a given number of steps and the smallest
cost seen in any of the experiments (5626.6357). The results averaged over 1,000 runs are
plotted in Figure 9. With both initialization methods the MetaMax strategy converges
faster then the Serial strategy. We note that for this data set, k-means++ with its
more clever initialization procedure yields faster convergence than the standard k-means
with its uniform initialization, which is consistent with the results presented by Arthur and
Vassilvitskii (2007).

8. An extension to clustering random variables is well-known and straightforward, but is omitted here
because in this paper we only consider clustering finite data sets.
9. Note that in the MetaMax algorithm we have to do the practical modification that if a local search
algorithm has terminated then it will not be chosen anymore. This clearly improves the performance as
an algorithm is not chosen anymore when no improvement can be observed.

438

fiEfficient Multi-Start Strategies for Local Search Algorithms

100000

10000
1000
100

average error

average error

10000

1000

100

10
1
0.1
0.01
0.001

10

0.0001
1

Serial kmeans
MetaMax kmeans

1

10

100

1000

10000

100000

iteration

1e-05

Serial kmeans++
MetaMax kmeans++

1

10

100

1000

10000

100000

iteration

Figure 9: The average error for the multi-start strategies with k-means (left) and kmeans++ (right). 99% confidence intervals are shown with the same color as
the corresponding curves.

5.3 Practical Considerations
In all experiments with the MetaMax algorithm presented above, we observed that the
number of algorithm instances r (shown in Figure 10) grows at a rate of (tr / ln tr ) (recall
that tr is the total number of function calls to evaluate f , or the total number of steps,
by all algorithm instances by the end of round r). On the other hand, in the derivation of

the theoretical bounds (see Theorem 15 and Theorem 16) we used a bound r  ( tr ).
In contrast to the quadratic penalty suggested by Theorem 16, plugging the (tr / ln tr )
estimate of r into the theorem we would find that only a logarithmic factor more calls
to evaluate f (total number of steps) are needed to achieve the performance of a search
algorithm started from the attraction region of the optimum.
Finally, perhaps the main practical question concerning the MetaMax family of multistart algorithms is to decide when to use them. As a rule of thumb, we have to say that
there should be a sufficiently large performance difference between an average run of
the local search algorithm and the best one. Clearly, if a single local search produces an
acceptable result then it is not worth the effort to run several instances of the local search,
especially not with a complicated schedule. In many real problems it is often the case
that it is relatively easy to get close to the optimum, which may be acceptable for some
applications, but approaching the optimum with greater precision is hard; if the latter is of
importance, the MetaMax algorithm and its variants may be very useful. Last, one may
wonder about the computational costs of our algorithms. As it was discussed before, we
consider the case when the evaluation of the target function is very expensive: this is clearly
not the case for the Griewank function, which is only used to demonstrate basic properties
of the algorithm, but holds for many of the optimization problems in practice, including
all other experiments considered in this paper. In these problems the function evaluation
is indeed expensive (and depends on the available data), while the overhead introduced
by the MetaMax algorithms depends on the number of rounds. For the MetaMax(K)
algorithm we have to find the upper convex hull of a set of K points in each round; in
the worst case this can take as long as O(K 2 ) calculations, but in practice this is usually
439

fiGyorgy & Kocsis

number of algorithm instances * ln(t)/t

1.8

Griewank 2D
Griewank 10D
vehicle
letter
K-MEANS
K-MEANS++
min
max

1.6
1.4
1.2
1
0.8
0.6
0.4
0.2

1

10

100

1000

10000

100000

iteration

Figure 10: Number of algorithm instances (r) for MetaMax. The average number of
instances are shown on the six benchmarks: Griewank function (2- and 10dimensional), parameter tuning of Multilayer Perceptron (on the vehicle and
on the letter data set), and clustering with k-means and k-means++. The
maximum and minimum number of instances over all runs of all benchmarks
are also shown. One can notice that for larger values of tr , 0.45tr / ln tr  r 
1.65tr / ln tr .

much cheaper, as the upper convex hull is determined by the point that corresponds to
the actually best estimate and by the point that corresponds to the least used algorithm,
which requires only O(K) computations, or even less, if some special ordering tricks are
introduced. Since the target function f is evaluated at least twice in each round, on average
at most O(K 2 ) computational overhead is needed for each evaluation of f in the worst
case, which is practically reduced to O(K), or even less. Similar considerations hold for the
MetaMax() and the MetaMax algorithms, resulting in an average O(r2 ) worst-case
overhead for each call to f (in r rounds), which is closer to O(r) or even less in practice.
In all examples we considered (apart of the case for the Griewank function), this amount
of overhead has been negligible relative to the computational resources needed to evaluate
f at a single point.

6. Conclusions
In this paper we provided multi-start strategies for local search algorithms. The strategies
continuously estimate the potential performance of each algorithm instance in an optimistic
way, by supposing a convergence rate of the local search algorithms up to an unknown
constant, and in every phase resources are allocated to those instances that could converge
to the optimum for a particular range of the constant. Three versions of the algorithm
were presented, one that is able to follow the performance of the best of a fixed number of
local search algorithm instances, and two that, with gradually increasing the number of the
local search algorithms, achieve global consistency. A theoretical analysis of the asymptotic
440

fiEfficient Multi-Start Strategies for Local Search Algorithms

behavior of the algorithms was also given. Specifically, under some mild conditions on the
function to be maximized (e.g., the set of the values of the local maxima is not dense at
the global maximum), our best algorithm, MetaMax, preserves the performance of a local
search algorithm for the original function class with at most a quadratic increase in the
number of times the target function needs to be evaluated (asymptotically). Simulations
demonstrate that the algorithms work quite well in practice.
While the theoretical bound suggests that the target function has to be evaluated a
quadratic factor more times to achieve the performance of a search algorithm that is started
from the attraction region of the optimum, in the experiments we found only a logarithmic
penalty. It is not clear whether this difference is the result of our slightly conservative
(asymptotic) analysis or the choice of the experimental settings. Also, a finite sample
analysis of the algorithm is of interest, as the experiments indicate that the MetaMax
algorithm provides good performance even for a relatively small number of steps taken by
the local search algorithms, in the sense that it provides a speed-up compared to other
approaches even if the number of times the target function can be evaluated (i.e., the total
number of steps that can be taken by all algorithms together) is relatively small. Finally,
future work is needed to clarify the connection between the convergence rate of the optimal
algorithms (g ) and the function hr used in the exploration.

Acknowledgments
The authors would like to thank the anonymous referees for their numerous insightful
and constructive comments. This research was supported in part by the Mobile Innovation
Center of Hungary, by the National Development Agency of Hungary from the Research and
Technological Innovation Fund (KTIA-OTKA CNK 77782), and by the PASCAL2 Network
of Excellence (EC grant no. 216886). Parts of this paper were presented at ECML 2009
(Kocsis & Gyorgy, 2009).

Appendix A. Proof of Lemma 1
bn ). Since Un  0 almost everywhere on E, Egoroffs
Fix   (0, 1) and let Un = f  f (X
theorem (see, e.g. Ash & Doleans-Dade, 2000) implies that there is an event E  E with
1  P (E ) <  such that Un  0 uniformly almost everywhere on E . The second part of
the lemma follows from the definition of uniform convergence.
2

References
Adam, K. (2001). Learning while searching for the best alternative. Journal of Economic
Theory, 101, 252280.
Arthur, D., & Vassilvitskii, S. (2007). k-means++: The advantages of careful seeding. In
Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms, pp.
10271035.
Ash, R. B., & Doleans-Dade, C. A. (2000). Probability & Measure Theory. Academic Press.
441

fiGyorgy & Kocsis

Asuncion, A., & Newman, D. J. (2007). UCI machine learning repository.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite time analysis of the multiarmed
bandit problem. Machine Learning, 47 (2-3), 235256.
Bartz-Beielstein, T. (2006). Experimental Research in Evolutionary Computation  The
New Experimentalism. Natural Computing Series. Springer, New York.
Battiti, R., Brunato, M., & Mascia, F. (2008). Reactive Search and Intelligent Optimization,
Vol. 45 of Operations research/Computer Science Interfaces. Springer Verlag.
Beck, C. J., & Freuder, E. C. (2004). Simple rules for low-knowledge algorithm selection.
In Regin, J. C., & Rueher, M. (Eds.), CPAIOR, Lecture Notes in Computer Science
3011, pp. 5064. Springer.
Carchrae, T., & Beck, J. C. (2004). Low-knowledge algorithm control. In Proceedings of
the Nineteenth National Conference on Artificial Intelligence (AAAI), pp. 4954.
Cicirello, V. A., & Smith, S. F. (2004). Heuristic selection for stochastic search optimization:
Modeling solution quality by extreme value theory. In In Proceedings of the 10th
International Conference on Principles and Practice of Constraint Programming, pp.
197211. Springer.
Cicirello, V. A., & Smith, S. F. (2005). The max k-armed bandit: A new model of exploration
applied to search heuristic selection. In In Proceedings of the Twentieth National
Conference on Artificial Intelligence, pp. 13551361.
Finkel, D. E., & Kelley, C. T. (2004). Convergence analysis of the direct algorithm. Tech.
rep. CRSC-TR04-28, NCSU Mathematics Department.
Gagliolo, M., & Schmidhuber, J. (2006). Learning dynamic algorithm portfolios. Annals of
Mathematics and Artificial Intelligence, 47 (34), 295328. AI&MATH 2006 Special
Issue.
Gagliolo, M., & Schmidhuber, J. (2007). Learning restart strategies. In Veloso, M. M. (Ed.),
IJCAI 2007  Twentieth International Joint Conference on Artificial Intelligence,
vol. 1, pp. 792797. AAAI Press.
Gagliolo, M., & Schmidhuber, J. (2010). Algorithm selection as a bandit problem with
unbounded losses. In Blum, C., & Battiti, R. (Eds.), Learning and Intelligent Optimization, Vol. 6073 of Lecture Notes in Computer Science, pp. 8296. Springer
Berlin/Heidelberg.
Gerencser, L., & Vago, Z. (2001). The mathematics of noise-free SPSA. In Proceedings of
the IEEE Conference on Decision and Control, pp. 44004405.
Gersho, A., & Gray, R. M. (1992). Vector Quantization and Signal Compression. Kluwer,
Boston.
Griewank, A. O. (1981). Generalized descent for global optimization. Journal of Optimization Theory and Applications, 34, 1139.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination
of minimum cost paths. Systems Science and Cybernetics, IEEE Transactions on,
4 (2), 100 107.
442

fiEfficient Multi-Start Strategies for Local Search Algorithms

Hoos, H. H., & Stutzle, T. (1999). Towards a characterisation of the behaviour of stochastic
local search algorithms for SAT. Artificial Intelligence, 112, 213232.
Horn, M. (2006). Optimal algorithms for global optimization in case of unknown lipschitz
constant. Journal of Complexity, 22 (1), 5070.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: an automatic
algorithm configuration framework. Journal of Artificial Intelligence Research, 36 (1),
267306.
Jones, D. R., Perttunen, C. D., & Stuckman, B. E. (1993). Lipschitzian optimization without
the lipschitz constant. Journal of Optimization Theory and Applications, 79 (1), 157
181.
Kautz, H., Horvitz, E., Ruan, Y., Gomes, C., & Selman, B. (2002). Dynamic restart policies. In Proceedings of the Eighteenth National Conference on Artificial Intelligence
(AAAI), pp. 674681.
Kieffer, J. C. (1982). Exponential rate of convergence for Lloyds method I. IEEE Trans.
Inform. Theory, IT-28, 205210.
Kocsis, L., & Gyorgy, A. (2009). Efficient multi-start strategies for local search algorithms.
In Buntine, W., Grobelnik, M., Mladenic, D., & Shawe-Taylor, J. (Eds.), Machine
Learning and Knowledge Discovery in Databases, Vol. 5781 of Lecture Notes in Computer Science, pp. 705720. Springer Berlin/Heidelberg.
Linde, Y., Buzo, A., & Gray, R. M. (1980). An algorithm for vector quantizer design. IEEE
Transactions on Communications, COM-28, 8495.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup of Las Vegas algorithms.
Information Processing Letters, 47, 173180.
Mart, R., Moreno-Vega, J., & Duarte, A. (2010). Advanced multi-start methods. In Gendreau, M., & Potvin, J.-Y. (Eds.), Handbook of Metaheuristics, 2nd edition (2 edition).
Springer.
Nesterov, Y. (2004). Introductory Lectures on Convex Optimization: A Basic Course.
Kluwer Academic Publishers.
Ribeiro, C., Rosseti, I., & Vallejos, R. (2009). On the use of run time distributions to evaluate
and compare stochastic local search algorithms. In Stutzle, T., Birattari, M., & Hoos,
H. (Eds.), Engineering Stochastic Local Search Algorithms. Designing, Implementing
and Analyzing Effective Heuristic s, Vol. 5752 of Lecture Notes in Computer Science,
pp. 1630. Springer Berlin/Heidelberg.
Spall, J., Hill, S., & Stark, D. (2006). Theoretical framework for comparing several stochastic
optimization approaches. In Calafiore, G., & Dabbene, F. (Eds.), Probabilistic and
Randomized Methods for Design under Uncertainty, chap. 3, pp. 99117. SpringerVerlag, London.
Spall, J. C. (1992). Multivariate stochastic approximation using a simultaneous perturbation
gradient approximation. IEEE Transactions on Automatic Control, 37, 332341.
Spall, J. C. (1998). Implementation of the simultaneous perturbation algorithm for stochastic optimization. IEEE Transactions on Aerospace Electronic Systems, 34, 817823.
443

fiGyorgy & Kocsis

Streeter, M. J., & Smith, S. F. (2006a). An asymptotically optimal algorithm for the max
k-armed bandit problem. In Proceedings, The Twenty-First National Conference on
Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, pp. 135142.
Streeter, M. J., & Smith, S. F. (2006b). A simple distribution-free approach to the max
k-armed bandit problem. In Principles and Practice of Constraint Programming CP 2006, 12th International Conference, CP 2006, Nantes, France, September 25-29,
2006, Proceedings, pp. 560574.
Vilalta, R., & Drissi, Y. (2002). A perspective view and survey of meta-learning. Artificial
Intelligence Review, 18 (2), 7795.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools and
Techniques (2nd edition). Morgan Kaufmann, San Francisco.
Zabinsky, Z. B., Bulger, D., & Khompatraporn, C. (2010). Stopping and restarting strategy
for stochastic sequential search in global optimization. J. of Global Optimization,
46 (2), 273286.

444

fi