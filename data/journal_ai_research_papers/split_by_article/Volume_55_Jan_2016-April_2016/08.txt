Journal of Articial Intelligence Research 55 (2016) 1091-1133

Submitted 09/2015; published 04/2016

Semantic Visualization with
Neighborhood Graph Regularization
Tuan M. V. Le
Hady W. Lauw

vmtle.2012@phdis.smu.edu.sg
hadywlauw@smu.edu.sg

School of Information Systems
Singapore Management University
80 Stamford Road, Singapore 178902

Abstract
Visualization of high-dimensional data, such as text documents, is useful to map out
the similarities among various data points. In the high-dimensional space, documents
are commonly represented as bags of words, with dimensionality equal to the vocabulary
size. Classical approaches to document visualization directly reduce this into visualizable
two or three dimensions. Recent approaches consider an intermediate representation in
topic space, between word space and visualization space, which preserves the semantics
by topic modeling. While aiming for a good t between the model parameters and the
observed data, previous approaches have not considered the local consistency among data
instances. We consider the problem of semantic visualization by jointly modeling topics
and visualization on the intrinsic document manifold, modeled using a neighborhood graph.
Each document has both a topic distribution and visualization coordinate. Specically, we
propose an unsupervised probabilistic model, called Semafore, which aims to preserve the
manifold in the lower-dimensional spaces through a neighborhood regularization framework
designed for the semantic visualization task. To validate the ecacy of Semafore, our
comprehensive experiments on a number of real-life text datasets of news articles and
Web pages show that the proposed methods outperform the state-of-the-art baselines on
objective evaluation metrics.

1. Introduction
Text documents come in various avors, such as Web pages, news articles, blog posts, emails,
or messages on social media such as Twitter. While much is in English, there are increasing
amounts of content in various languages as well. With the backdrop of the growth in volume, diversity, and complexity of various corpora, we need more useful tools to analyze the
wealth of text content. One form of analysis which we will look into in this paper is visualization. There are dierent types of visualizations, be it of the temporal or longitudinal,
networked, or other natures. What we are interested in is a form of visualization where we
can represent a collection of documents as coordinates on the same low-dimensional space,
so as to learn of the similarities and dierences among documents based on their distances
on the visualization space.
Visualization of high-dimensional data is an important exploratory data analysis task,
which is actively studied by various academic communities. While the HCI community is
interested in the presentation of information, as well as other interface aspects (Chi, 2000),
the machine learning community is interested in the quality of dimensionality reduction
c
2016
AI Access Foundation. All rights reserved.

fiLe & Lauw

(Van der Maaten & Hinton, 2008), i.e., how to transform the high-dimensional representation into a lower-dimensional representation that can be shown on a scatterplot. This
visualization form is simple, and widely applicable across various domains.
Consider therefore the problem of visualizing documents on a scatterplot. Commonly,
a document is represented as a bag of words, i.e., a vector of word counts. This highdimensional representation would be reduced into coordinates on a visualizable 2D (or 3D)
space. One pioneering technique is Multidimensional Scaling (MDS) (Kruskal, 1964). The
goal is to preserve the distances in the high-dimensional space in the low-dimensional embedding. When applied to documents, a visualization technique for generic high-dimensional
data, e.g., MDS, may not necessarily preserve the topical semantics. Words are often ambiguous, with issues such as polysemy, when the same word carries multiple senses, and
synonymy, when dierent words carry the same sense. Because the dimensions in the original representation (which are words) may not accurately capture this ambiguity, this aects
the quality of the reduced representation (which is the visualization space) as well.
To model semantics in documents in a way that can resolve some of this ambiguity, the
current popular approach is by topic modeling, such as PLSA (Hofmann, 1999) or LDA
(Blei, Ng, & Jordan, 2003). Each document is associated with a probability distribution
over a set of topics. Each topic is a probability distribution over words in the vocabulary. In
this way, polysemous words can be separated into dierent topics, and synonymous words
can be grouped into the same topic.
Topic modeling itself is another form of dimensionality reduction: from word space to
topic space. The word space refers to a documents original representation, which is usually
a bag of words. The topic space refers to the simplex of topic distributions. A documents
probability distribution over topics is eectively the representation of this document in
this topic space. However, a topic model by itself is not designed for visualization. While
one possible visualization is to plot documents topic distributions on a simplex, a 2D
visualization space could express only three topics, which is very limiting.
Given its success in modeling semantics in documents, we therefore ask the question
of whether and how best to do both forms of dimensionality reductions (visualization and
topic modeling) for documents. The end goal is to arrive at a visualization of documents
that is consistent with both the semantic representation (topics), as well as the original
representation (words). This coupling is a distinct task from topic modeling or visualization
respectively, as it enables novel capabilities. For one thing, topic modeling helps to create
a richer visualization, as we can now associate each coordinate on the visualization space
with both topic and word distributions, providing semantics to the visualization space.
For another, the tight integration potentially allows the visualization to serve as a way
to explore and tune topic models, allowing users to introduce feedback (Hu, Boyd-Graber,
Satino, & Smith, 2014) to the model through a visual interface (Choo, Lee, Reddy, & Park,
2013). These capabilities support several use case scenarios. One potential use case is a
document organizer system. The visualization could potentially help in assigning categories
to documents, by showing how closely related documents have been labeled. Another is
an augmented retrieval system. Given a query, the results may include not just relevant
documents, but also other similar documents (neighbors in the visualization).
1092

fiSemantic Visualization with Neighborhood Graph Regularization

1.1 Problem Statement
We refer to the task of jointly modeling topics and visualization as semantic visualization.
The input is a set of documents D. For a specied number of topics Z and visualization
dimensionality (assumed to be 2D, without losing any generality), the goal is to derive,
for every document in D, a latent coordinate on the visualization space, and a probability
distribution over the Z topics. While we focus on documents in our description, the same
approach would apply to visualization of other data types for which latent factor modeling,
i.e., topic model, makes sense.
A straightforward way is to undergo two-step reductions. In the rst reduction, the
original representation for documents are reduced into topic distributions using topic modeling. In the second reduction, documents topic distributions are further reduced into
visualization coordinates. This approach may have some value compared to direct reduction from word space to visualization space. However, it is not ideal, because the disjoint
reductions could mean that errors may propagate from the rst to the second reduction,
and the resulting visualization may not faithfully capture the original representation.
A better way to solve this problem is to join up the two reductions into a single, joint
process that produces both topic distributions and visualization coordinates. This approach
was rst pioneered by PLSV (Iwata, Yamada, & Ueda, 2008), which also showed that the
joint approach outperformed the disjoint approach. PLSV derives the latent parameters
by maximizing the likelihood of observing the documents. This goal is concerned with the
error between the model and the observation.
In the literature, it is found that algorithms that ensure smoothness tend to perform
better at learning tasks (Zhou, Bousquet, Lal, Weston, & Scholkopf, 2004). Smoothness
concerns preserving the observed proximity between documents. This objective arises naturally from the assumption that the intrinsic geometry of the data is a low-rank, non-linear
subspace within the high-dimensional space. Therefore, preserving neighborhood structure
is important for learning tasks. This assumption is well-accepted in the machine learning
community (Laerty & Wasserman, 2007), and nds application in both supervised and
unsupervised learning (Belkin & Niyogi, 2003; Zhou et al., 2004; Zhu, Ghahramani, Lafferty, et al., 2003). Recently, there is a preponderance of evidence that this assumption also
applies to text data in particular (Cai, Mei, Han, & Zhai, 2008; Cai, Wang, & He, 2009;
Huh & Fienberg, 2012). We therefore propose to incorporate this assumption into a new
unsupervised, semantic visualization model.
1.2 Overview
We propose an unsupervised probabilistic model that jointly derives topic distributions and
visualization coordinates on the intrinsic geometry of the data. Our proposed model is called
Semafore, which stands for SEmantic visualization with MAniFOld REgularization. We
build a neighborhood regularization framework into a semantic visualization model. The
framework involves new issues to resolve, including the regularization function, and the
space in which regularization should take place.
The model is evaluated on a series of real-life, publicly available datasets, which are
also benchmark datasets used in document classication task. An advantage of a statistical
method, such as ours, is that it is not dependent on a specic language. Two of the datasets
1093

fiLe & Lauw

are in English, and one is in Brazilian Portuguese. While our model is unsupervised (class
label is neither required nor used in learning), to objectively quantify the visualization quality, we leverage on the class label information. It is a common assumption that documents
of the same class are expected to be neighbors on the original space (Belkin, Niyogi, &
Sindhwani, 2006; Zhou et al., 2004; Zhu et al., 2003), which suggests that they should also
be close on the visualization space. We investigate the eectiveness of Semafore in placing
documents of the same class nearby on the visualization space, and systematically compare
it to existing baselines without one or more of our properties, namely: joint modeling of
topic and visualization, or neighborhood regularization.
1.3 Contributions
While visualization and topic modeling are, separately, well-studied problems, the interface
between the two, semantic visualization, is a relatively new problem, with very few previous
work. In this work, we make the following contributions.
 We propose incorporating neighborhood structure in semantic visualization. In this
respect, we propose a probabilistic model Semafore, with two integrated components. One is a kernelized semantic visualization model, enabling the substitution of
the kernel functions that relate visualization coordinates to topic distributions (see
Section 3.3). The other is a neighborhood graph regularization framework for semantic
visualization as described in Section 4.1.
 Realizing the neighborhood graph regularization involves an exploration of how to
incorporate the appropriate forms of the neighborhood structure. In this respect,
we investigate the eects of neighborhood graph construction techniques such as knearest neighbors (k-NN), -ball, and disjoint minimum spanning trees (DMST), as
well as dierent edge weight estimations such as heat-kernel (see Section 4.2) in the
context of semantic visualization.
 In Section 5, we describe the requisite learning algorithms based on maximum a
posteriori (MAP) estimation using expectation-maximization (EM), in order to t
the parameters for the various regularization functions and kernels that we propose.
 Our nal contribution is the evaluation of Semafores eectiveness on a series of reallife, public datasets described in Section 6, which shows that Semafore outperforms
existing baselines on a well-established and objective visualization metric.
In our prior work (Le & Lauw, 2014b), we proposed the problem and described the preliminary model. In this extended article, there are signicant technical changes that provide
a signicantly more comprehensive discussion of the model. For instance, we now discuss
the Student-t kernel, in addition to the previously introduced Gaussian kernel. Furthermore, we investigate the ecacies of dierent neighborhood graph constructions, including
the -ball and DMST graphs, in addition to the previously introduced kNN graph. The
graph weights are also enhanced through investigation of heat kernel, in addition to the
simple-minded binary scheme previously. As discussed in Section 6.3, these enhancements
collectively result in statistically signicant improvements over the previous model. Beyond
1094

fiSemantic Visualization with Neighborhood Graph Regularization

the technical enhancements, we also provide more comprehensive model analysis and empirical validation, including richer quantitative and qualitative discussions of the visualizations
and the resulting topic models, as well as a metric to measure topic interpretability based
on pairwise mutual information.

2. Related Work
In this section, we discuss the dierent aspects of our work, identify the related papers in
the literature, and point out the key conceptual dierences.
2.1 Visualization and Dimensionality Reduction
One way to perform visualization is by using a generic dimensionality reduction technique.
Such techniques come in several avors, depending on the objective. Principal component
analysis (PCA) (Jollie, 2005) identies the components that explain most of the variance
in the data. Related to PCA is singular value decomposition (SVD) (Golub & Van Loan,
2012). Comparatively, independent component analysis (ICA) (Comon, 1994) identies
the components that are independent of one another, whereas linear discriminant analysis
(Fishers LDA) (Fisher, 1936) identies the components that most discriminate between
known class labels. Being generic, these techniques are more frequently applied to feature
extraction, as they are not optimized for visualization. They focus more on the properties of
the components (e.g., orthogonality, independence) rather than on the intrinsic relationship
among data instances. Furthermore, as they are based on linear projections, they may not
capture non-linearities in the data well.
Another category of techniques, which is more directly related to visualization, is the
embedding approach. It aims to preserve the high-dimensional similarities or dierences
in the low-dimensional embedding. One pioneering such work is multidimensional scaling
(MDS) (Kruskal, 1964). Given a set of pairwise distances ij between data points i and j,
MDS determines coordinates xi and xj respectively, such that the embedded visualization
distance ||xi  xj || approximates ij as much as possible. For MDS, the distance to be
preserved ij is frequently the linear distance, measuring the distance along a straight line
between two points in the input space. Instead of this linear distance, Isomap (Tenenbaum,
De Silva, & Langford, 2000) seeks to preserve the geodesic distance, by nding shortest paths
in a graph with edges connecting neighboring data points. LLE (Roweis & Saul, 2000) seeks
to preserve linear distances, but only among the neighboring points and avoiding the need to
estimate pairwise distances between widely separated data points. Recently there are also
works applying a similar concept to embedding but using probabilistic modeling, such as
PE (Iwata, Saito, Ueda, Stromsten, Griths, & Tenenbaum, 2007), SNE (Hinton & Roweis,
2002), t-SNE (Van der Maaten & Hinton, 2008), and GTM (Bishop, Svensen, & Williams,
1998). Yet others are based on semi-denite programming (Shaw & Jebara, 2007, 2009).
Alternatively, several embedding techniques do not aim to preserve relationship among
data instances, but rather other properties such as local minima (Kim & Torre, 2010).
Importantly, all these techniques are not optimized for semantic visualization, as they do
not model topics at all. The coordinates do not reect any semantic meaning, other than
reecting the optimization objective.
1095

fiLe & Lauw

There are only a few related works so far that seek to address the semantic visualization
task directly. The closest previous work that does both topic modeling and visualization
in a single generative process is Probabilistic Latent Semantic Visualization (PLSV) (Iwata
et al., 2008), which also shows that a joint approach outperforms a separate approach. Just
as PLSV builds upon the foundation of the topic modeling technique Probabilistic Latent
Semantic Analysis (PLSA) (Hofmann, 1999) by incorporating visualization coordinates, so
do we build upon the foundation of PLSV by incorporating RBF kernels (Section 3.3) and
neighborhood structure (Section 4).
There are also related works that share a similar objective, but do not share the same
paradigm of visualization or topic modeling. For instance, LDA-SOM (Millar, Peterson, &
Mendenhall, 2009) rst conducts topic modeling using Latent Dirichlet Allocation (LDA)
(Blei et al., 2003), and then separately embeds the documents topic distributions on a
Self-Organizing Map (SOM) (Kohonen, 1990). However, this is not a joint model, and
SOM uses a dierent visualization space than the Euclidean space that we are interested
in. For another instance, SSE (Le & Lauw, 2014a) builds on the Spherical Admixture
Model (SAM) (Reisinger, Waters, Silverthorn, & Mooney, 2010) belonging to the class of
spherical topic models targeted at spherical (unit vector) reprepresentations of topics and
documents, which are not directly comparable or equivalent with the simplex representation
and multinomial modeling (probability distribution over words) adopted in this work as well
as PLSV.
By semantic visualization, we refer to the task of joining visualization and topic modeling. A related, but dierent, task is topic visualization, where the objective is to visualize
the topics, in terms of which keywords are dominant for each topic (Chaney & Blei, 2012;
Chuang, Manning, & Heer, 2012), which topics are dominant in a corpus (Wei, Liu, Song,
Pan, Zhou, Qian, Shi, Tan, & Zhang, 2010), and how topics are related to one another
(Gretarsson, Odonovan, Bostandjiev, Hollerer, Asuncion, Newman, & Smyth, 2012).
2.2 Topic Modeling
Topic model involves statistical modeling of text (documents and words) in order to discover
some abstract concepts or topics that occur in a corpus. Beginning with latent semantic
indexing (Dumais, Furnas, Landauer, Deerwester, Deerwester, et al., 1995), topic model
evolves into the modern probabilistic treatments, such as Probabilistic Latent Semantic
Analysis (PLSA) (Hofmann, 1999) and Latent Dirichlet Allocation (LDA) (Blei et al.,
2003). Intuitively, a topic captures a collection of words that tend to co-occur because
they describe the same concept. This has the appeal of producing highly interpretable
statistical models that let users make semantic sense of the corpus. Other than text-only
document corpora, topic models have also been applied to cases where links are observed
in addition to text (McCallum, Wang, & Corrada-Emmanuel, 2007).
Meanwhile, the assumption that the intrinsic geometry of the data is a non-linear low
dimensional subspace within the high-dimensional space nds application in both supervised
and unsupervised (Belkin & Niyogi, 2003) learning algorithms. It is especially prevalent in
semi-supervised learning (Zhou et al., 2004; Zhu et al., 2003) as a way to bridge labeled
and unlabeled data. Regularization as a technique to realize this assumption has a long
history (Belkin et al., 2006). The specic form of the regularization function varies among
1096

fiSemantic Visualization with Neighborhood Graph Regularization

applications. The study of this assumption for unsupervised topic models begins with
LapPLSI (Cai et al., 2008), which introduces regularization to PLSA (Hofmann, 1999),
by minimizing the Euclidean distance between neighboring documents topic distributions.
Follow-up work introduce other distance functions (Cai et al., 2009; Wu, Bu, Chen, Zhu,
Zhang, Liu, Wang, & Cai, 2012). While these previous work focus on maintaining proximity
of similar documents, DTM (Huh & Fienberg, 2012) adds a new criterion to also maintain
the distance among dierent documents. Our work is dierent in that we also need to
contend with the visualization aspects, and not just topic modeling.
2.3 Semantic Similarity
Other than topic models, there are alternative mechanisms to learn the semantic relationship
between documents. One way is by measuring the semantic similarity among documents
or words. For instance, in vector space model, documents may be represented as a term
vector, and their similarity may be expressed in terms of cosine similarity (Turney, Pantel,
et al., 2010). Other than word occurrences alone, there could also be additional signals
of semantic similarity. For instance, working with Wikipedia corpus, the categories and
links are also took into account to determine the similarity among articles (Gabrilovich
& Markovitch, 2009; Ponzetto & Strube, 2007). Our work diers from these in several
important respects. First, our objective is not in the similarity value per se, but rather in
determining lower-dimensional embedding coordinates, which would allow visualization as
one application. Second, our method is based on probabilistic modeling of latent variables,
akin to topic modeling, instead of operating on the vector space model representation of
documents.

3. Semantic Visualization
We introduce the problem formulation for semantic visualization in Section 3.1. Our focus in
this paper is on the eects of the neighborhood graph structure on the semantic visualization
task. We gure that the clearest way to showcase these eects is to design a neighborhood
preservation framework over and above an existing generative process, such as PLSV (Iwata
et al., 2008), which we will review in Section 3.2. In Section 3.3, we describe an innovation
over the semantic visualization model, which is an abstraction of the mapping between the
topic space and the visualization space using radial basis function (RBF) kernels. This
allows the exploration of various kernels, of which we identify two for further exploration.
For ease of following the discussion, we include a table of notations in Table 1.
3.1 Problem
For the task of semantic visualization, the input is a corpus of documents D = {d1 , . . . , dN }.
Every dn is a bag of words, and wnm denotes the mth word in dn . The total number of
words in dn is Mn . The objective is to learn, for each dn , a latent distribution over Z
topics {P(z|dn )}Z
z=1 . Each topic z is associated with a parameter z , which is a probability
distribution {P(w|z )}wW over words in the vocabulary W . The words with the highest
probabilities for a given topic capture the semantic of that topic.
1097

fiLe & Lauw

Notation
dn
xn
Mn
z
z
z
W
N
Z





Description
a specic document
latent coordinate of dn in the visualization space
number of words in document dn
a specic topic
coordinate of topic z in the visualization space
word distribution of topic z
the vocabulary (the set of words in the lexicon)
total number of documents in the corpus
total number of topics (user-dened)
the collection of xn s for all documents
the collection of z s for all topics
the collection of z s for all topics
the collective set of parameters {, , }

Table 1: Notations.
In semantic visualization, there is an additional objective for semantic visualization,
which is to learn, for each document dn , its latent coordinate xn on a low-dimensionality
visualization space. Similarly, each topic z is associated with a latent coordinate z on
the visualization space. A document dn s topic distribution is then expressed in terms
of the Euclidean distance between its coordinate xn and the dierent topic coordinates
 = {z }Z
z=1 . Intuitively, the closer is xn to a topics z , the higher is P(z|dn ) or the
probability of topic z for document dn .
In the following sections, we systematically describe the various components of our
solution. The generative process that links the latent variables (coordinates) and the words
in the documents is described in Section 3.2. The specic relationship between documents
and topics coordinates constitutes a specic mapping function, which we model as an RBF
kernel in Section 3.3. In the following Section 4, we discuss how to incorporate neighborhood
structure into semantic visualization.
3.2 Generative Process
We now describe the generative process of documents based on both topics and visualization
coordinates. Below we review PLSV whose graphical model is shown in Figure 1. Our
eventual complete model is a generalization of this model, involving enhancements through
kernelization (Section 3.3) and neighborhood structure preservation (Section 4).
The generative process is as follows:
1. For each topic z = 1, . . . , Z:
(a) Draw zs word distribution: z  Dirichlet()
(b) Draw zs coordinate: z  Normal(0,  1 I)
2. For each document dn , where n = 1, . . . , N :
(a) Draw dn s coordinate: xn  Normal(0,  1 I)
1098

fiSemantic Visualization with Neighborhood Graph Regularization

N



Mn w

x

Z

z









Figure 1: Graphical model of PLSV.
(b) For each word wnm  dn :
i. Draw a topic: z  Multi({P(z|xn , )}Z
z=1 )
ii. Draw a word: wnm  Multi(z )
Here,  is a Dirichlet prior, I is the identity matrix,  and  control the variance of the
Z
Z
Normal distributions. The parameters  = {xn }N
n=1 ,  = {z }z=1 ,  = {z }z=1 , collectively
denoted as  = , , , are learned from documents D based on maximum a posteriori
estimation. The log likelihood function is shown in Equation 1.
L(|D) =

Mn
N 

n=1 m=1

log

Z


P(z|xn , )P(wnm |z )

(1)

z=1

We reiterate that our focus here is on incorporating neighborhood graph structure into
semantic visualization. By building a neighborhood graph regularization framework into
an existing generative process, i.e., PLSV, we can clearly observe that any improvement
over PLSV arises from the neighborhood graph regularization. In this sense, our work
is in the tradition of introducing neighborhood graph regularization to probabilistic topic
modeling (Huh & Fienberg, 2012; Cai et al., 2008, 2009), where the contributions relate
to the neighborhood graph regularization, rather than the generative process. That said,
there is one signicant dierence to PLSV, which is our exibility in allowing various kernel
functions, which we will discuss next.
3.3 RBF Kernels
In the Step 2(b)i of the above generative process, the topic z of a word is drawn from
the distribution {P(z|xn , )}Z
z=1 . This distribution relates the coordinates of topics in
the visualization space  = {z }Z
z=1 and the coordinate xn of a document dn with the
documents topic distribution {P(z|dn )}Z
z=1 .
This relationship can be formulated as a mapping problem where we want to nd a
function G which maps a point in visualization space to a point in the topic space. However,
the form of G cannot be known exactly because both visualization space and topic space
are latent spaces and G may be dierent across dierent domains. Therefore, to compute
the topic distributions, we need a way to approximate G.
To build a function approximation of the unknown function G, we use the abstraction
of Radial Basis Function (RBF) neural networks (Bishop, 1995) because feedforward multilayered RBF neural networks with one hidden layer can serve as a universal approximator
1099

fiLe & Lauw

K nz



t
Z      
Z










/xn

Figure 2: Topic distribution is expressed as a function of visualization coordinates using
Radial Basis Function (RBF) network.

to arbitrary continuous functions (Park & Sandberg, 1991). This property provides the
condence that the model would have the ability to approximate any existing relationship
between visualization space and topic space with arbitrary precision. Unlike PLSV (Iwata
et al., 2008) that dened a specic mapping function, our approach generalizes the semantic visualization model by dening the mapping problem in terms of kernelization, which
admits several mapping functions within the family of RBF kernels.
In our context, Radial Basis Function (Buhmann, 2000) will relate coordinate variables
based on distances which denes a kernel function (||xn  z ||) in terms of how far a data
point (e.g., xn ) is from a center (e.g., z ). The kernel function  may take on various forms,
e.g., Gaussian, multi-quadric, inverse quadratic, polyharmonic spline. To express P(z|dn )
as a function of xn , we consider the normalized architecture of RBF network, with three
layers as shown in Figure 2. The input layer consists of one input node (xn ). The hidden
layer consists of Z number of normalized RBF activation functions. Each is centered at
z and computes Z (||xn z ||) . The linear output layer consists of Z output nodes.
z  =1

(||xn z ||)

Each output node yz (xn ) corresponds to P(z|dn ), which is a linear combination of the RBF
functions, as shown in Equation 2. Here, wz,z  is the weight of inuence of the RBF function

of z  on the P(z|dn ), with the constraint Z
z  =1 wz,z  = 1.
Z
P(z|dn ) = yz (xn ) =

z  =1 wz,z   (||xn  z  ||)
Z
z  =1 (||xn  z  ||)

(2)

While Equation 2 is the general form, to instantiate a specic mapping function, we
need to determine both the assignment of wz,z  and the form of the function . For wz,z  ,
we will experiment with a special case wz,z  = 1 when z = z  and 0 otherwise.
For the kernel function , one variation we consider is Gaussian, which yields the function in Equation 3, where  refers to the collective set of z s. Note that here we set variance
of Gaussian to 1. However, its true value is not really important because a dierent variance
value just produces a re-scaled visualization with the scaling factor equal to that variance.
1100

fiSemantic Visualization with Neighborhood Graph Regularization

exp( 12 ||xn  z ||2 )
P(z|dn )Gaussian = P(z|xn , )Gaussian = Z
1
2
z  =1 exp( 2 ||xn  z  || )

(3)

Another variation of  being considered is Student-t. This distribution is also used by
t-SNE (Van der Maaten & Hinton, 2008) in the context of non-semantic, direct embedding to mitigate the eects of crowding. Due to mismatched dimensionalities, the points
are crunched together in the center of the visualization, which prevents gaps from forming
between the clusters. Therefore, we hypothesize that using Student-t as radial basis function, which yields the function in Equation 4, can help to improve the performance of our
model if crowding becomes an issue. Note that the Student-t distribution with one degree
of freedom yields a radial basis function having the form similar to the inverse quadratic.
(1 + ||xn  z ||2 )1
P(z|dn )Studentt = P(z|xn , )Studentt = Z
2 1
z  =1 (1 + ||xn  z  || )

(4)

The Gaussian function (Equation 3) was also used previously in the baseline PLSV
(Iwata et al., 2008) that we will compare to. Its inclusion helps to establish parity for
comparative purposes, both to investigate the eectiveness of the alternative Student-t
kernel (described above), as well as that of the neighborhood regularization (described in
the next section).

4. Neighborhood Graph Regularization Framework
There are recent works (Cai et al., 2008, 2009; Huh & Fienberg, 2012) trying to preserve the
local neighborhood structure when learning low-dimensional topic representations of documents. These works assume that documents are sampled from a nonlinear low-dimensional
subspace that are embedded in a high-dimensional space. Therefore, the local neighborhood
structure is important for revealing the hidden topics of documents and should be preserved
when learning topic representations of documents (Bai, Guo, Lan, & Cheng, 2014). In the
generative process for semantic visualization described in Section 3, the document parameters are sampled independently, and may not necessarily reect the underlying local neighborhood structure. We therefore seek to realize this assumption for semantic visualization.
In particular, we assume that when two documents di and dj are close in the original space,
then their parameters i and j of the low-rank representation are similar as well. Coupled
with the kernelized semantic visualization model described in Section 3, the neighborhood
preservation approach described in this section constitutes our proposed model, Semafore,
which stands for SEmantic visualization with MAniFOld REgularization.
4.1 Neighborhood Regularization
The neighborhood structure can be represented by a neighborhood graph. Given a set of
data points in the Euclidean space, a neighborhood graph is constructed with the input
data points as vertices. By denition, edges are symmetric, i.e., ij = ji , and weighted.
The collection of edge weights are collectively denoted as  = {ij }.
For the moment, we will assume that we have the neighborhood graph, and address
the issue of how this neighborhood graph may be incorporated into our semantic visualiza1101

fiLe & Lauw

tion framework. In actuality, the neighborhood graph construction itself is an important
component, whose construction is described in detail in Section 4.2.
One eective means to incorporate a neighborhood structure into a learning model is
through a regularization framework (Belkin et al., 2006). This leads to a re-design of the
log-likelihood function in Equation 1 into a new regularized function L (Equation 5), where
 consists of the parameters (visualization coordinates and topic distributions), and D and
 are the documents and neighborhood structure.
L(|D, ) = L(|D) +   R(|)

(5)

The rst component L is the log-likelihood function in Equation 1, which reects the
t between the latent parameters  and the observation D. The second component R is a
regularization function, which reects the consistency between the latent parameters  of
neighboring documents in the neighborhood structure .  is the regularization parameter,
commonly found in neighborhood based algorithms (Belkin et al., 2006; Cai et al., 2008,
2009), which controls the extent of regularization (we will experiment with dierent s in
experiments).
4.1.1 Proposed Regularization Function
We now turn to the denition of the R function. The intuition is that the data points that
are close in the high-dimensional space, should also be close in their low-rank representations, i.e., local consistency, also known as smoothness. One function that satises this is
R+ in Equation 6. Here, F is a distance function that operates on the low-rank space.
Minimizing R+ leads to minimizing the distance F(i , j ) between neighbors (ij = 1).
R+ (|) =

N


ij  F(i , j )

(6)

i,j=1;i=j

The above level of local consistency is still insucient, because it does not regulate how
non-neighbors (i.e., ij = 0) behave. For instance, it does not prevent non-neighbors from
having similar low-rank representations. Another valid objective in visualization is to keep
non-neighbors apart, which is satised by another objective function R in Equation 7. R
is minimized when two non-neighbors di and dj (i.e., ij = 0) are distant in their low-rank
representations. The addition of 1 to F is to prevent division-by-zero error.
R (|) =

N

i,j=1;i=j;ij =0

1  ij
F(i , j ) + 1

(7)

We hypothesize that neither objective is eective on its own. A more complete objective
would capture the spirits of both keeping neighbors close, and keeping non-neighbors apart.
Therefore, we put Equation 6 and Equation 7 together using summation and maximize the
objective function as shown in Equation 8. Note that the coecient 12 in Equation 8 is for
simplifying the formula of the derivative of R (|).
1
R (|) =  (R+ (|) + R (|))
2
1102

(8)

fiSemantic Visualization with Neighborhood Graph Regularization

2

d1

1

d2

I1

I2

0
-2

-1

0
-1

1

2

3

4

d3

-2

Figure 3: Example of how the same topic distribution may have dierent visualization coordinates. Any points on the red line have same topic distributions.

Summation preserves the absolute magnitude of the distance, and helps to improve the
visualization task by keeping non-neighbors separated on a visualizable Euclidean space.
Taking the product is unsuitable, because it constrains the ratio of distances between neighbors to distances between non-neighbors. This may result in the crowding eect, where
many documents are clustered together, because the relative ratio may be maintained, but
the absolute distances on the visualization space could be too small.
Other than the proposed regularization function above, it is also possible to consider
other regularization functions. For instance, we have also experimented with modifying the
regularization function adapted from Discriminative Topic Model (DTM) (Huh & Fienberg,
2012), which addressed topic modeling but not semantic visualization. Note that while in
the original DTM formulation, the distance function F(i , j ) operates in the topic space,
we adapt it for semantic visualization by redening the distance function F(i , j ) so that
it will operate in the visualization space instead. This modied DTM formulation is shown
to underperform the proposed regularization function above (Le & Lauw, 2014b).
4.1.2 Enforcing Neighborhood Structure: Visualization vs. Topic Space
We now turn to the denition of F(1 , 2 ). In neighborhood-based models (Belkin et al.,
2006; Cai et al., 2008, 2009), there is only one low-rank representative space. For semantic
visualization, there are two: topic and visualization spaces. We look into where and how
to enforce the neighborhood graph structure.
At rst glance, they seem equivalent. After all, they are representations of the same
documents. However, this is not necessarily the case. Consider a simple example of two
topics z1 and z2 with visualization coordinates 1 = (0, 0) and 2 = (2, 0) respectively.
Meanwhile, there are three documents {d1 , d2 , d3 } with coordinates x1 = (1, 1), x2 = (1, 1),
and x3 = (1, 1). If two documents have the same coordinates, they will also have the
same topic distributions. In this example, x1 and x2 are both equidistant from 1 and 2 ,
and therefore according to Equation 3, they have the same topic distribution P(z1 |d1 ) =
P(z1 |d2 ) = 0.5, and P(z2 |d1 ) = P(z2 |d2 ) = 0.5. If two documents have the same topic
distributions, they may not necessarily have the same coordinates. d3 also has the same
1103

fiLe & Lauw

topic distribution as d1 and d2 , but a dierent coordinate. In fact, any coordinate of the
form (1, ?) will have the same topic distribution. This example is illustrated in Figure 3.
This suggests that enforcing neighborhood structure on the topic space may not necessarily lead to having data points closer on the visualization space. We postulate that
regularizing the visualization space is more eective. There are also advantages in computational eciency to doing so, which we will describe further shortly. Therefore, we
dene F(i , j ) as the squared Euclidean distance ||xi  xj ||2 between the corresponding
visualization coordinates.
4.2 Neighborhood Graph
We discuss how the neighborhood graph may be approximated, which concerns the two
issues of how the graph edges are dened, as well as how they are weighted. The neighborhood graph is constructed in the original data space where we represent each document as a
tf-idf vector (Manning, Raghavan, Schutze, et al., 2008). We also experiment with dierent
vector representations, including word counts and term frequencies, and nd tf-idf to give
the best results. The distance between two document vectors is measured using Euclidean
distance.
4.2.1 Graph Construction
There have been research studies on the properties and methods for construction of neighborhood graphs (Zemel & Carreira-Perpinan, 2004; Carey & Mahadevan, 2014). Since the
construction of neighborhood graph is a critical step that may aect the performance of
various graph-based algorithms, this problem itself is a research issue of independent interest. Our scope is in exploring how some well-established graph construction techniques
may apply to the case of semantic visualization. We will investigate these various graph
construction methods empirically in Section 6.
In the following, we briey review two categories of graph construction methods.
1. Neighborhood-based Graphs. In this formulation, edges are formed between data points
that are deemed to be suciently close to each other. This admits dierent denitions
of sucient closeness. The most common denitions found in the literature include
the two below.
(a) -ball: The neighborhood graph contains an edge connecting two documents di
and dj , if di and dj have a distance less than a threshold .
(b) k-nearest neighbors (k-NN) graph: The neighborhood graph contains an edge
connecting two documents di and dj , if di is in the set Nk (dj ) of the knearest
neighbors of dj , or dj is in the set Nk (di ).
-ball and k-NN both have strongly data-dependent parameters (i.e.,  and k) and it is
not straightforward to choose the best value for these parameters. Neither guarantees
that the graph would be connected. They also need to be carefully selected or tuned,
as to some extent they also aect the balance between the contribution of neighbors
R+ and non-neighbors R to the neighborhood regularization R in Equation 8. In
1104

fiSemantic Visualization with Neighborhood Graph Regularization

Appendix A, we explore empirically how these graph parameters can help to maintain
this balance within the neighborhood regularization function.
-ball suers from another issue that it tends to produce many edges for the points
located at high-density regions, and thus has little restriction on the maximum degree
of a vertex. k-NN does not suer from that problem and is one of the most commonly
used types of graphs.
In our subsequent development and experiments, we will experiment with both -ball
and k-NN graph as there may be some variance in the performance of dierent graph
construction techniques for dierent datasets (Hein, Audibert, & Luxburg, 2007; Ting,
Huang, & Jordan, 2010; Coifman & Lafon, 2006).
2. Minimum Spanning Tree-based Graphs. While -ball and k-NN are quite sensitive
to noise and sparsity, graph construction based on combining multiple minimum
spanning trees can help to reduce sensitivity to noise of the output graph (Zemel
& Carreira-Perpinan, 2004). There are two variations based on this approach.
(a) Perturbed Minimum Spanning Trees (PMST): PMST builds a neighborhood
graph by generating T > 1 perturbed copies of the whole dataset according
to the local noise model and tting an MST to each perturbed copy. A weight
eij  [0, 1] will be assigned to the edge between points xi and xj equal to the
average number of times that edge appears on the trees.
(b) Disjoint Minimum Spanning Trees (DMST): DMST produces a neighborhood
graph by nding a deterministic collection of r minimum spanning trees that
satises the property that no tree in the collection uses any edge of other trees.
The neighborhood graph is the union of all edges of trees and contains r(N  1)
edges.
As the representative of this category, we use DMST, which is deterministic and easier
to construct than PMST while showing similar ecacies.
4.2.2 Graph Weighting
The next issue is how to assign weights to the edges in the neighborhood graph. In this
respect, we consider two variations of edge weights.
1. Simple Minded :

ij =

1,
0,

if only if di and dj are connected,
otherwise.

(9)

This is the simplest approach where we use binary weighting to assign the weights
to the edges. However, this approach to assign uniform weights to edges can be
sensitive to errors, because of the cli eect from 1 immediately to 0. Moreover,
since the weights are not smoothed, it could result in some loss of information. We
hypothesize that among the connected nodes, there may still be some dierences in
terms of degrees of similarity, which are expressed by their mutual distances. This
motivates the second approach below.
1105

fiLe & Lauw

2. Heat Kernel :

ij =

exp(
0,

||di dj ||2
),


if only if di and dj are connected,
otherwise.

(10)

An alternative approach is using the Heat Kernel function (Belkin & Niyogi, 2001;
Jebara, Wang, & Chang, 2009). Heat Kernel has the advantage over Simple Minded by
allowing smoother weights for the edges, which helps address the issues of sensitivity
and loss of information. However, while Simple Minded is not parameterized, Heat
Kernel has one parameter that needs to be determined (i.e.,  ). Note that for  = ,
Heat Kernel degenerates into Simple Minded, i.e., the former is the more general
formulation. The exact value of  is not important in our model because it would
eectively be absorbed by the regularization parameter. For simplicity, we set  = 2.

5. Model Fitting
We now discuss how the parameters of the model described in Sections 3 and 4 can be
learned. One well-accepted framework to learn model parameters using maximum a posteriori (MAP) estimation is the Expectation-Maximization or EM algorithm (Dempster,
Laird, & Rubin, 1977).
For our model, the regularized conditional expectation of the complete-data log likelihood in MAP estimation with priors is:
Q(|) =
+

Mn 
N 
Z




P(z|n, m, ) log P(z|xn , )P(wnm |z )

n=1 m=1 z=1
N


Z


n=1

z=1

log(P(xn )) +

log(P(z )) +

Z


log(P(z ))

z=1

+   R(|),
where  is the current estimate. P(z|n, m, ) is the class posterior probability of the nth
document and the mth word in the current estimate. P(z ) is a symmetric Dirichlet prior
with parameter  for word probability z . P(xn ) and P(z ) are Gaussian priors with a zero
mean and a spherical covariance for the document coordinates xn and topic coordinates z .
We set the hyper-parameters to  = 0.01,  = 0.1N and  = 0.1Z following PLSV (Iwata
et al., 2008).
In the E-step, P(z|n, m, ) is updated as follows:
P(z|n, m, ) = Z

P(z|xn , )P(wnm |z )

z  =1 P(z

 |x

n , )P(wnm |z  )

.

In the M-step, by maximizing Q(|) w.r.t zw , the next estimate of word probability
zw is as follows:
 N  Mn
m=1 I(wnm = w)P(z|n, m, ) + 
n=1
zw = W 
,
N  Mn

m=1 I(wnm = w )P(z|n, m, ) + W
w =1
n=1
1106

fiSemantic Visualization with Neighborhood Graph Regularization

where I(.) is the indicator function. z and xn cannot be solved in a closed form, and are
estimated by maximizing Q(|) using quasi-Newton (Liu & Nocedal, 1989).
The computation fo the gradients of Q(|) w.r.t z and xn depend on the specic
kernel used (see Section 3.3).
 For the Gaussian kernel, we have the following gradients:
n


Q(|)  
=
P(z|xn , )  P(z|n, m, ) (z  xn )  z ,
z

N

Q(|)
=
xn

M

n=1 m=1
Mn 
Z




m=1 z=1


R(|)
.
P(z|xn , )  P(z|n, m, ) (xn  z )  xn +  
xn

 For the Student-t kernel, we have the following gradients:

N Mn 
2 P(z|xn , )  P(z|n, m, ) (z  xn )
Q(|)  
=
 z ,
z
1 + ||xn  z ||2
n=1 m=1


Mn 
Z

2 P(z|xn , )  P(z|n, m, ) (xn  z )
Q(|)
R(|)
=
 xn +  
.
2
xn
1 + ||xn  z ||
xn
m=1 z=1

The gradient of R(|) w.r.t. xn is computed depending on the form of the regularization function R(|). When we use the proposed regularization function R (|)
described in Section 4.1.1, we have the following gradient:
R (|)
R(|)
=
xn
xn
 


(xn  xj )
1  
=
4nj (xn  xj ) 
4(1  nj )
.
2
2
(F(n , j ) + 1)
j=1;j=n

j=1;j=n

As mentioned earlier, there is an eciency advantage to regularizing on the visualization space. R(|) does not contain the variable z if we do regularization on visualization
is O(N 2 ). In contrast, if we do regularizaspace. The complexity of computing all R(|)
xn
tion on topic space, we have to take the gradient of R(|) w.r.t to z . That contributes
. Therefore, regularizatowards a greater complexity of O(Z 2  N 2 ) to compute all R(|)
z
tion on topic space would run much slower than on visualization space.

6. Experiments
The main objective of our experiments is to evaluate the eectiveness of neighborhood regularization for semantic visualization model. After describing the experimental setup, we
rst examine the dierent design choices of the model relating to kernel, graph construction, and regularization function. Thereafter, we compare Semafore against the baseline
methods that also aim to address both visualization and topic modeling, quantitatively and
qualitatively, rst in terms of visualization and then in terms of topic modeling.
1107

fiLe & Lauw

6.1 Experimental Setup
In this section, we give a description of benchmark datasets as well as suitable metrics that
are used for evaluation.
6.1.1 Datasets
We use three real-life, publicly available datasets (Cardoso-Cachopo, 2007) for evaluation.
 20N ews contains newsgroup articles (in English) from 20 classes.
 Reuters8 contains newswire articles (in English) from 8 classes.
 Cade12 contains web pages (in Brazilian Portuguese) classied into 12 classes.
These are benchmark datasets used for document classication. While our task is fully
unsupervised, the ground-truth class labels are useful for an objective evaluation. We
create balanced classes by sampling fty documents from each class, following the practice
in PLSV (Iwata et al., 2008). This results in, for one sample, 1000 documents for 20N ews,
400 for Reuters8, and 600 for Cade12. The vocabulary sizes are 5.4K for 20N ews, 1.9K for
Reuters8, 7.6K for Cade12. As the algorithms are probabilistic, we generate ve samples
for each dataset. For each sample, we conduct ve independent runs. Therefore, the result
reported for each setting is the average over a total of 25 runs.
6.1.2 Metrics
For a suitable metric, we return to the fundamental principle that a good visualization
should preserve the relationship between documents (in high-dimensional space) in the
lower-dimensional visualization space. User studies, even when well-designed, could be
overly subjective and may not be repeatable across dierent users reliably. Therefore, for a
more objective evaluation, we rely on two types of quantitative analysis:
 Classication: This evaluation relies on the ground-truth class labels found in the
datasets. This is a well-established practice in many clustering and visualization
works in machine learning. The basis for this evaluation is the reasonable assumption
that documents of the same class are more related than documents of dierent classes.
Therefore a good visualization would place documents of the same class as neighbors
on the visualization.
For each document dn , we hide its true class cn , and generate a prediction for
its class Ct (n) by taking the majority class among its t-nearest neighbors, as determined by Euclidean distance on the visualization space. Classication accuracy
Classif ication Acc(t) is dened as the fraction of documents whose predicted class
Ct (n) matches the true class cn . More specically, we have:
N
1  
(Ct (n) = cn ),
Classif ication Acc(t) =
N
n=1

1108

fiSemantic Visualization with Neighborhood Graph Regularization

where  is the delta function that equals 1 if the prediction matches and 0 otherwise.
The same metric is used in PLSV (Iwata et al., 2008). While accuracy is computed
based on documents coordinates, the same trends will be produced if computed based
on topic distributions (due to their coupling through the kernels described in Section 3.3).
 Neighborhood Preservation: This evaluation does not rely on the ground-truth class
labels but on the local neighborhood structure in the input data. The assumption is
that a good visualization would be able to preserve the local structure in the input
data as much as possible. If two documents are neighbors in the input data, they
should still be neighbors in the visualization space.
For every document dn , we compute sets of t-nearest neighbors Yt (n) and Xt (n) of
document dn in the input data and the visualization respectively. The neighborhood
preservation accuracy P reservation Acc(t) is then dened as the average fraction of
the overlap size of Yt (n) and Xt (n) over the size of Yt (n) (i.e. t), where n = 1, . . . , N .
More specically, we have:

P reservation Acc(t) =

N
1  |Yt (n)  Xt (n)|
,
N
t
n=1

where |Yt (n)  Xt (n)| is the size of the overlap set Yt (n)  Xt (n).
A similar measure can be found in the literature (Akkucuk & Carroll, 2006), where
it is called the rate of agreement in local structure or agreement rate and is used
to measure how well the local structure is preserved between the input data and the
low dimensional embedding. It is also used for tuning the parameters of a non-linear
dimensionality reduction method (Chen & Buja, 2009).
In the subsequent experiments, we let t vary in the range [5, 50] with the step size
5 and report the accuracies. Since dierent methods may behave dierently at dierent
ts, choosing a specic t for comparison may be unfair for some methods. Moreover, a
method that consistently does well for dierent ts would also have a smoother local
structure. Therefore, when comparing various methods, we present the preservation or
classication accuracies averaged across t  [5, 50], denoted P reservation Acc(Avg) and
Classif ication Acc(Avg) respectively.
6.2 Parameter Study
In this section, we study the eects of graph parameters on our model. Specically, the
parameters concern the graph construction, including the number of neighbors k in k-NN
graph, the distance threshold  in -ball graph, and the number of minimum spanning trees
r in DMST. For each type of graph, we use the Simple Minded weight. For the following
gures, the regularization function is R with  = 10 and the number of topics Z = 20.
We use neighborhood preservation accuracy P reservation Acc(t) to show the eects of
graph parameters because this metric does not need ground-truth class labels, which are
not always available for tuning these graph parameters.
1109

fiLe & Lauw




















































W 

W 



W 



E















Z















Figure 4: Preservation accuracy of Semafore when using k-NN graph with dierent neighborhood size k for (a) 20N ews, (b) Reuters8, and (c) Cade12.











































W 

W 

W 













E















Z















Figure 5: Preservation accuracy of Semafore when using DMST graph with dierent number of minimum spanning trees r for (a) 20N ews, (b) Reuters8, and (c) Cade12.





































E











W 

W 



W 



















Z





















Figure 6: Preservation accuracy of Semafore when using -ball graph with dierent values
of distance threshold  for (a) 20N ews, (b) Reuters8, and (c) Cade12.

1110

fiSemantic Visualization with Neighborhood Graph Regularization

In Figure 4, we show the performance of our model with dierent neighborhood size k in
k-NN graph for dierent datasets. For every k, we vary t and plot the P reservation Acc(t).
Figure 4 shows that the optimum k for 20N ews, Reuters8, and Cade12 is 10, 10, and 5
respectively. We compute the average accuracy P reservation Acc(Avg) and it conrms
that the optima are indeed at those k values. From now on, we will use k=10 for 20N ews
and Reuters8, and k=5 for Cade12 when k-NN graph is used.
For DMST graph, we plot the P reservation Acc(t) for dierent number of minimum
spanning trees r with dierent datasets in Figure 5. It is dicult to see which r is the best in
the gure because the dierences between them are not much. The P reservation Acc(Avg)
is computed and it shows that for all three datasets, the optimum is about at r=5,6,7.
Subsequently, we will use r=6 for DMST graphs for all three datasets.
For -ball graph, in Figure 6 we plot the P reservation Acc(t) for dierent values of 
in the range [1.32, 1.40]. We choose that range because =1.32 and =1.40 roughly give an
average number of neighbors of 5 and 100 respectively. The P reservation Acc(Avg) shows
that the optimum  for 20N ews, Reuters8, and Cade12 is 1.34, 1.35, and 1.33 respectively.
6.3 Model Analysis
In this section, we study the various design choices involved in designing the Semafore
model, before nally concluding on the eventual synthesis of design choices to be used for
comparison against the baselines. To keep the discussion focused and organized, in each of
the following sub-section, we vary a single design choice, in order to isolate its eects. When
unvaried, the model has the following setup by default: the number of topics is Z = 20,
the graph construction method is k-NN, the graph weighting method is simple minded, the
RBF kernel is Gaussian, and the regularization function is R with  = 10.
6.3.1 Neighborhood Graph Construction
We investigate three graph construction methods: k-NN, -ball and DMST, which are representatives of neighborhood-based and minimum spanning tree-based methods respectively.
For each graph, its parameter is tuned as shown in Section 6.2. For the regularization
parameter , we try dierent settings of  on each dataset. It so happens that  = 10
performs the best for all the graph construction methods across the three datasets.
In Figure 7, we run Semafore with dierent types of graph on the three datasets and
report the P reservation Acc(Avg) at dierent number of topics Z. The results show that
dierent types of graph behave dierently with dierent datasets. In 20N ews, -ball and
DMST give our model highest performance. Since the dierence between the two are not
statistically signicant, we choose to use DMST for subsequent experiments on 20N ews.
For Reuters8, since -ball outperforms the others (signicant at 0.05 level), it is going to
be the default choice for subsequent experiments. For Cade12, the choice is DMST, which
is slightly better than k-NN (statistically signicant for Z = 10, 40, 50).
6.3.2 Neighborhood Graph Weighting
We now compare two variations of graph weighting methods, namely: Simple Minded and
Heat Kernel methods. In this experiment, we use k-NN graph with specic ks for dierent
1111

fiLe & Lauw







D^d





W 



W 

W 

EE











Ed
E











Ed



Z





Ed


Figure 7: The eects of dierent graph construction methods on our models performance.







,<t



W 



W 

W 

^Dt











Ed
E












Ed
Z





Ed


Figure 8: The eects of dierent graph weighting schemes on our models performance.
The graph used in this experiment is k-NN graph with specic ks for dierent
datasets as studied in Section 6.2.











Ed
E

^<



W 



W 

W 

'<











Ed
Z












Ed


Figure 9: The eects of Gaussian and Student-t RBF kernels on our models performance.
1112

fiSemantic Visualization with Neighborhood Graph Regularization

Regularization function
Graph construction
Graph weighting
RBF kernel

20N ews
R
DMST
Heat Kernel
Student-t

Reuters8
R
-ball
Heat Kernel
Student-t

Cade12
R
DMST
Simple Minded
Student-t

Table 2: Synthesized Model for Each Dataset.
datasets as studied in Section 6.2. The regularization parameter  is set to 10 after trying
various settings and picking the best one.
In Figure 8, we compare Simple Minded method and Heat Kernel method to see their
inuences on our model at dierent number of topics Z. We observe that Heat Kernel
is signicantly and consistently better than Simple Minded method across all the cases
in 20N ews and Reuters8. The dierence is statistically signicant at 0.01 level. One
explanation is that Heat Kernel assigns smoother weights to the graph edges, and thus is
more robust than Simple Minded. For Cade12, Simple Minded is slightly better, though
the dierences are statistically signicant at 0.05 level only for Z = 40. Subsequently, we
will use Heat Kernel for 20N ews and Reuters8, and Simple Minded for Cade12 as part of
the nal synthesis.
6.3.3 RBF Kernel
As described in Section 3.3, we express topic distributions as a function of visualization
coordinates using RBF network as an abstraction. In this section, we show how dierent
RBF kernels aect our models performance. The two kernels we are exploring are Gaussian
(Equation 3) and Student-t (Equation 4). We tune the regularization term  for each kernel
and see that the best one for the two kernels are  = 10.
Figure 9 shows the results for dierent number of topics Z. Student-t kernel has a slight
edge over Gaussian kernel consistently across dierent number of topics. The dierence is
small, but is statistically signicant (at 0.05 level) in a majority of the cases (for 20N ews
at Z = 10, 20, 30, 50, for Reuters8 at Z = 30, and for Cade12 at Z = 10, 30, 50). The slight
improvement could be a sign that crowding problem does exist in the model. Student-t
kernel would be even more useful when there is more extreme crowding issues, such as
when the number of documents to be visualized is even larger. Subsequently, due to its
slight edge, we will use Student-t as part of the nal synthesis. As we will see shortly, using
Student-t within the synthesized model results in a signicant improvement overall.
6.3.4 Synthesised Semafore Model
Based on the model analysis in the preceding paragraphs, we combine the design choices
into a nal synthesis model called Semafore. The synthesized model is slightly dierent
for dierent datasets, as listed in Table 2.
We now conduct another set of experiments to verify that those synthesized models
would produce a noticeable improvement over the earlier version (kNN + Simple Minded
+ Gaussian Kernel) that appeared in our earlier work (Le & Lauw, 2014b), underlining
1113

fiLe & Lauw

EE^D'<
,<^<













Ed










E





Ed
Z

EE^D'<
D^d^D^<

W


W


W


EE^D'<
D^d,<^<













Ed


Figure 10: Our synthesized models with dierent properties compared to the earlier version
(kNN + Simple Minded + Gaussian Kernel) that appeared in our earlier work
(Le & Lauw, 2014b).

the utility of the subsequent enhancements. Figure 10 shows that this is indeed the case.
Based on the standard deviations shown in the gures, the improvements are very clear
in 20N ews and Reuters8 but not so clear in Cade12. Paired samples t-test indicate that
the improvement is signicant at 0.05 level or lower in all cases, except for the cases where
Z = 10, 20 in Cade12. We will use these synthesized models in the comparisons against the
baseline methods in the following section.
6.4 Comparison of Visualizations
We now compare our proposed model with several baselines. First, we outline the set of
comparative methods. Thereafter, we discuss quantitative evaluation (in terms of accuracy),
as well as qualitative evaluation (in terms of example visualizations). Finally, we will show
that the gains in visualization quality does not come at the expense of topic modeling.
As semantic visualization seeks to ensure consistency between topic model and visualization, the comparison focuses on methods producing both topics and visualization coordinates
which are listed in Table 3.
 Semafore is our proposed method that incorporates neighborhood structure into
semantic visualization.
 PLSV (Iwata et al., 2008) is the state-of-the-art, representing the joint approach
without neighborhood structure preservation.
 PE (LDA) represents the pipeline approach involving topic modeling with LDA (Blei
et al., 2003), followed by visualizing documents topic distributions with PE (Iwata
et al., 2007). This pipeline is better than the LDA/MDS that appeared in our earlier
work (Le & Lauw, 2014b). There are other pipeline methods, shown inferior to PLSV
(Iwata et al., 2008), which are not reproduced here to avoid duplication.
1114

fiSemantic Visualization with Neighborhood Graph Regularization

Visualization

Topic model

Joint model

Neighborhood

Semafore
PLSV
PE (LDA)
t-SNE (LDA)
Table 3: Comparative Methods.
 t-SNE (LDA) is another pipeline approach that rst uses LDA (Blei et al., 2003) to
learn topic model and then use t-SNE (Van der Maaten & Hinton, 2008) to visualize
documents topic distributions.
For completeness, we also conduct experiments for comparing our method with t-SNE
and Laplacian EigenMaps (LE) (Belkin & Niyogi, 2003) (direct visualization, without topic
modeling). To keep the discussion focused, we show them in Appendix B, as we do not
consider t-SNE and LE as comparative baselines because these two methods only model
visualization, but not topics.
6.4.1 Accuracy
In this section, we compare our model with several baselines in terms of classication
accuracy (Figure 11) and neighborhood preservation accuracy (Figure 12). In the two
gures, only the standard deviations for Semafore are shown.
Classcation Accuracy. Figure 11(a), 11(c) and 11(e) show the Classf ication Acc(t)
at dierent ts for Z = 20 for 20N ews, Reuters8, and Cade12 respectively. At any t, the
comparison shows outperformance by Semafore over the baselines consistently. All four
methods show the same behavior that their performances decrease when t increases. As t
increases, they may lose accuracy in predicting labels for documents near to the border of
each cluster.
Now, we vary the number of topics Z. In Figure 11(b), we show the performance in
Classf ication Acc(Avg) on 20N ews. Figure 11(d) and 11(f) show the same for Reuters8
and Cade12 respectively. From these gures, we draw the following observations about the
comparative methods:
 Semafore performs the best on all datasets across various numbers of topics (Z).
Semafore beats PLSV by 25% to 51% on 20N ews, by 613% on Reuters8, and by
2232% on Cade12. These margins of performance with respect to PLSV are statistically signicant at 0.01 signicant level or lower in all cases. This eectively showcases
the utility of neighborhood regularization in enhancing the quality of visualization.
By preserving local consistency, Semafore achieves a good accuracy even at small
number of topics (e.g., 10).
 PLSV performs better than PE (LDA) and t-SNE (LDA), which shows that there
is utility to having a joint, instead of separate, modeling of topics and visualization.
PE (LDA) and t-SNE (LDA) are worse than PLSV because it embeds documents by
using two-step reductions that optimize separately two dierent objective functions.
1115

fiLe & Lauw

Therefore, the errors from the previous step may propagate to the next, without an
opportunity for correction. This may cause distortions in the visualization.
 In some cases, PLSV, PE (LDA) and t-SNE (LDA) tend to have decreasing accuracies when the number of topics increases. This may be because when number of topic
increases, the topic distributions and the word probabilities may overt the data and
thus the accuracy is reduced. In contrast, Semafore shows a quite stable performance across dierent numbers of topics. This may be explained by the utility of
neighborhood regularization, which helps to prevent overtting when the number of
topics increases.
Neighborhood Preservation Accuracy. While having better classication accuracy,
Semafore also preserves well the local structure of the input data in the visualization space.
The P reservation Acc(t) results in Figure 12(a), 12(c) and 12(e) show that Semafore is
consistently better than the other baselines in terms of neighborhood preservation across
dierent ts and dierent datasets. In Figure 12(b), 12(d) and 12(f), we vary the number
of topics Z and report the P reservation Acc(Avg) results. Semafore beats PLSV by
41% to 76% on 20N ews, by 2436% on Reuters8, and by 2945% on Cade12 in terms
of neighborhood preservation accuracy. The improvements of Semafore over PLSV are
statistically signicant at 0.01 signicant level or lower in all cases.
The above accuracy results are based on visualization coordinates. We have also computed accuracies based on topic distributions, which have similar trends.
6.4.2 Visualizations
To provide an intuitive appreciation, we briey describe a qualitative comparison of visualizations. For each method on each dataset, a visualization is shown as a scatterplot (best
seen in color). Each document has a coordinate, and is assigned a shape and color based
on its class. Each topic also has a coordinate, drawn as a black, hollow circle. A legend is
provided, mapping each symbol to the corresponding class label.
Note that this is an illustrative, rather than a comparative discussion, as an objective
evaluation should not rely on eyeballing alone. However, as we have shown the quantitative
results in the preceding section, in this section, we focus on the qualitative study of the
output visualizations.
20News. Figure 13 shows a visualization of 20N ews dataset. Semafores Figure 13(a)
shows that the dierent classes are well separated. There are distinct clusters of blue squares
and purple diamonds at the top for hockey and baseball classes respectively, clusters of
orange triangles and pink asterisks at the bottom for cryptography and medicine, etc.
Beyond individual classes, the visualization also places related classes nearby. Computerrelated classes are found on the lower left. Politics and religion are on the lower right.
Comparatively, Figure 13(b) by PLSV shows crowding at the center. For instance,
motorcycle (green dashes) and autos (red dashes) are mixed at the center without a good
separation. Figure 13(c) by PE (LDA) is worse. PE (LDA) does not give good separation
for not similar classes. It mixes autos (red dashes) and space (green circles) together at
the center. Medicine (pink asterisks) is also mixed with other classes in PE (LDA) while
Semafore and PLSV give a good separation for it. Figure 13(d) is visualization by tSNE (LDA). Although t-SNE (LDA) can separate well hockey (blue squares) and baseball
1116

fiSemantic Visualization with Neighborhood Graph Regularization

W>^s


















W>




^









Es





W>











Zs

Zs







^





W>^s




















^E>




Ed



 

W>^s






Ed
Es



W>




^

^E>


s 

^E>











Ed
s

Figure 11: Classication Accuracy Comparison.

1117





fiLe & Lauw

W>^s


















W>
W

W

^









Es

W

W>








Zs

Zs







^





W>^s



















^E>




Ed



W

W>^s











Ed
Es



W>
W

W

^

^E>


s 

^E>










Ed
s

Figure 12: Preservation Accuracy Comparison.

1118





fiSemantic Visualization with Neighborhood Graph Regularization

(purple diamonds) classes, it is not able to detect their semantic similarities (as baseball
and hockey are both about sports). In addition, it still mixes documents of dierent classes
together at the center and on the upper right.
Reuters8. Figure 14 shows the visualization outputs for Reuters8 dataset. Semafore
in Figure 14(a) is better at separating the eight classes into distinct clusters. In an anticlockwise direction from the top, we have navy blue diamonds (money-fx ), red dashes
(interest), red squares (crude), light blue pluses (earn), green triangles (acq), purple crosses
(ship), blue asterisks (grain), and nally orange circles (trade).
In comparison, PLSV in Figure 14(b) shows that several classes are intermixed at the
center, including red dashes (interest), orange circles (trade), and navy blue diamonds
(money-fx ). PE (LDA) in Figure 14(c) is also worse when it mixes dierentiated classes
such as red dashes (interest) and navy blue diamonds (money-fx ) together. t-SNE (LDA)
in Figure 14(d) seems have better cluster separation but still mix documents with dierent
classes together such as red squares (crude) and green triangles (acq) on the upper right.
Green triangles (acq) also mix with light blue pluses (earn) on the left in the visualization
by t-SNE (LDA).
Cade12. Figure 15 shows the visualization outputs for Cade12. This is the most
challenging dataset. Even so, Semafore in Figure 15(a) still achieves a better separation
between the classes, as compared to PLSV in Figure 15(b). Particularly, Semafore gives
better separation for esportes (green triangles) as well as compras-on-line (orange circles)
than PLSV and PE (LDA). t-SNE (LDA) shows quite good clusters for esportes (green
triangles) as well as compras-on-line (orange circles) but it also merges many dierent
classes together as in the clusters on the right and on the upper right.
6.5 Comparison of Topic Models
One question is whether Semafores gain in visualization quality over the closest baseline
PLSV is at the expense of the quality of its topic model. To investigate this, we will
compare the topic models of Semafore and PLSV, which share a core generative process.
For parity, in this comparison, we only include the joint models, whereby the visualization
coordinates aect the topic models as well.
The metric we use to measure the quality of topic models is pairwise mutual information
or PMI. It measures topic interpretability, based on coocurrence frequencies of the top words
in each topic in a large external corpus. Although other metrics such as perplexity or heldout likelihood can show the generalization ability of a learned topic model on unseen test
data, these traditional metrics do not capture whether topics are coherent (Chang, Gerrish,
Wang, Boyd-Graber, & Blei, 2009). Therefore, in this comparison, we rely on PMI, which
can measure the quality of topic words in terms of their interpretability to a human. To
human subjects, interpretability is closely related to coherence (Newman, Lau, Grieser, &
Baldwin, 2010), i.e., how much the top keywords in each topic are associated with each
other. After an extensive study of evaluation methods for coherence, Newman et al. (2010)
identify Pointwise Mutual Information (PMI) as the best measure, in terms of having the
greatest correlation with human judgments.
PMI is based on term cooccurrences. For a pair of words wi and wj , PMI is dened
p(wi ,wj )
as log p(wi )p(w
. For a topic, we average the pairwise PMIs among the top 10 words of
j)
1119

fiLe & Lauw

^

W>^s

W>

^E>
>











































Figure 13: Visualization of documents in 20N ews for number of topics Z = 20. Each point
represents a document and the shape and color represent document class. Each
topic is drawn as a black, hollow circle.
1120

fiSemantic Visualization with Neighborhood Graph Regularization

^

W>^s

W>

^E>
>



















Figure 14: Visualization of documents in Reuters8 for number of topics Z = 20. Each
point represents a document and the shape and color represent document class.
Each topic is drawn as a black, hollow circle.

1121

fiLe & Lauw

^

W>^s

W>

^E>
>



























Figure 15: Visualization of documents in Cade12 for number of topics Z = 20. Each point
represents a document and the shape and color represent document class. Each
topic is drawn as a black, hollow circle.

1122

fiSemantic Visualization with Neighborhood Graph Regularization

W>^s

^






WD/^

WD/^

^






W>^s









Ed








Ed



Z

E

Figure 16: Topic Interpretability of Semafore and PLSV in terms of PMI Score (higher
is better).

that topic. For a topic model, we average PMI across the topics. Intuitively, PMI is higher
(better), if each topic features words that are highly correlated with one another.
Key to PMI is the use of an external corpus to estimate p(wi , wj ) and p(wi ). Following
Newman et al. (2009), we use Google Web 1T 5-gram Version 1 (Brants & Franz, 2006),
a huge corpus of n-grams generated from 1 trillion word tokens. p(wi ) is estimated from
the frequencies of 1-grams. As recommended by Newman et al., p(wi , wj ) is estimated from
the frequencies of 5-grams. We obtain PMI for the English-based 20N ews and Reuters8,
but not for Cade12 because we do not possess a large-scale n-gram corpus specically for
Brazilian Portuguese.
In Figure 16, we plot the PMI score for various number of topics Z. Semafore performs
better than PLSV across most of the topics settings. In Figure 16(a) for 20News, except for
the case at Z = 10, all cases of Semafores outperformance are signicant at 0.05 level or
lower. In Figure 16(b) for Reuters8, all cases of Semafores outperformance are signicant
at 0.05 level or lower except for the Z = 30. These results show that Semafore improves
visualization while not sacricing the topic interpretability of learned topics.
For a greater appreciation of the quality of the output topic models, in Appendix C, we
show several examples of topic models for Z = 20, for both Semafore and PLSV, in terms
of the top keywords with the highest probabilities for each topic.

7. Conclusion
In this paper, we address the semantic visualization problem, which jointly conducts topic
modeling and visualization of documents. We propose a new framework to incorporate
neighborhood structure within a probabilistic semantic visualization model called Semafore.
The model is carefully designed to reect the context of semantic visualization, leading to
a number of design choices related to the RBF kernel for mapping topic and visualiza1123

fiLe & Lauw

tion spaces, the approximation of neighborhood graph through construction and weighting,
as well as the appropriate regularization functions and spaces. Experiments on real-life
datasets show that Semafore signicantly outperforms the baselines in terms of visualization quality and accuracy, while having a similar, if not slightly better topic model. This
provides evidence that neighborhood structure, together with joint modeling of topics and
visualization, is important for semantic visualization.

Appendix A. Balancing Contributions of Neighbors and Non-neighbors
to Regularization
As mentioned in Section 4.2, the balance between the contribution of neighbors R+ and
non-neighbors R to the neighborhood regularization R in Equation 8 may require careful
tuning of the graph parameters (i.e.,  or k). For example, in the case of using k-NN graph
and N total number of documents, we would have kN terms in the neighbor regularization
R+ , and (N k)N terms in the non-neighbor regularization R . Supposing that N increases
signicantly, there might be imbalance if k were to remain unchanged. Therefore, as N
changes, k should also be tuned accordingly to maintain this balance. For a simplistic
point, the ratio between kN and (N  k)N would remain roughly the same if both N and
k grow by similar factors. In practice, we recommend tuning k carefully.
We run additional experiments to validate the above argument on the 20N ews dataset.
Our basic point is that as N changes, k can be tuned to still show signicant improvement
due to the neighborhood graph regularization. The closest baseline is PLSV, both empirically in terms of classication accuracy, as well as conceptually as PLSV shares a similar
generative process but with a dierent kernel and without neighborhood regularization.
Hence, we compare the performance of our method Semafore (with k-NN graph, heat
kernel weighting, and Student-t kernel) to PLSV on various data sizes at Z = 20 topics.
 Figure 17(a) is for dataset of size N = 500, and Semafore runs with k = 10.
 Figure 17(b) is for dataset of size N = 1000, and Semafore runs with k = 10.
 Figure 17(c) is for dataset of size N = 5000, and Semafore runs with k = 50.
We note that there is a 10X dierence between the smallest and the largest datasets.
Yet the relative outperformance of Semafore over PLSV by around 15% to 20% is evident
across the three datasets. This supports the case that k can be tuned to produce a positive
eect using neighborhood graph regularization.

Appendix B. Additional Comparisons
As mentioned in Section 6.4, for completeness, we include here additional comparisons to
visualization methods that do not also aim at topic modeling. In particular, we include two
methods. First, we include t-SNE (Van der Maaten & Hinton, 2008), which is also used
in the composite t-SNE (LDA). Second, we include Laplacian EigenMaps (LE) (Belkin &
Niyogi, 2003), which takes as input the neighborhood graph. Figure 18 and Figure 19
show the classication accuracy and preservation accuracy of Semafore , t-SNE and LE
1124

fiSemantic Visualization with Neighborhood Graph Regularization










  



^



W>^s

^


W>^s




^















  












E

E

W>^s





  





E

Figure 17: Classication accuracy comparison on 20N ews with various data sizes (Z = 20).

^

^E







 


 





>













































E













Z



Figure 18: Classication accuracy comparison.

^

^E







W


W

W



>
















E
























Z

Figure 19: Preservation accuracy comparison.

1125










fiLe & Lauw

when varying t. Semafore outperforms LE in all cases. For t-SNE, Semafore outperforms t-SNE for Reuters8. However, for 20N ews and Cade12, it is more dicult to tell
whether Semafore or t-SNE is better. t-SNE tends to have decreasing accuracy as t increases. This is expected because t-SNE is known to focus on preserving the local structure
(Van der Maaten & Hinton, 2008). When t is small, we basically consider only the local
structure of the visualization. When t increases, we consider the more global structure of the
visualization and Semafore outperforms t-SNE signicantly. Overall, Semafore is more
stable than t-SNE as t changes, which indicates that Semafore tries to balance preserving
the local and global structure better than t-SNE. We emphasize that this comparison is for
information purpose only, as we do not regard t-SNE and LE as comparative baselines.

Appendix C. Topic Model Examples
We showcase the topic models derived by Semafore and PLSV. For 20N ews, Table 4 shows
the topics of Semafore, and Table 5 shows the topics of PLSV. For Reuters8, Table 6
shows the topics of Semafore, and Table 7 shows the topics of PLSV. For Cade12, Table 8
shows the topics of Semafore, and Table 9 shows the topics of PLSV.
For each method, we show the list of twenty topics. For each topic, we produce the
top ten words with the highest probabilities. As shown by the top words, the topics do
correspond strongly to some of the classes. For example, topic s19 in Table 4 for 20N ews is
about Christianity, which corresponds to the soc.religion.christian class. Topic s4 is about
cars and motorcycles, corresponding to rec.autos and rec.motorcycles. Topic s12 is probably
concerning the categories of rec.sport.baseball and/or rec.sport.hockey.
Overall, we observe that the quality of topic words are comparable across the comparative methods. Note that there is no direct correspondence between the topics of dierent
methods (e.g., the rst topic of Semafore may not correspond to the rst topic of PLSV).
Through manual inspection, we can see that there are some related topics, e.g., s4 and p6 ,
or s12 and p7 . However, the sets of topics and the set of keywords for each topic are not
identical. This is borne out in the slight dierence in terms of PMI scores.
This qualitative study helps to show that Semafore improves the visualization quality,
while still maintaining at least the same quality of topic words, if not better. This supports
the conclusion reached by the quantitative comparisons in the main manuscript.

1126

fiSemantic Visualization with Neighborhood Graph Regularization

Table 4: Semafores Topic Model for 20N ews (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
space, system, -rcb-, book, computer, university, list, post, price, science
article, year, good, write, guy, well, time, head, question, leave
gun, law, kuwait, people, death, fbus, article, control, weapon, child
window, le, program, widget, application, type, will, resource, call, function
car, bike, speed, engine, drive, lock, turn, mile, front, change
will, power, place, work, rate, write, sound, lead, good, interested
write, article, thing, time, people, better, start, problem, will, good
write, time, people, friend, pay, public, article, tax, opinion, money
people, claim, write, system, person, moral, evidence, objective, read, state
image, datum, graphic, send, le, format, package, software, mail, include
armenian, re, jew, child, kill, start, people, turkish, door, israel
system, board, will, datum, time, work, tape, test, copy, command
game, team, year, player, win, play, will, hit, season, hockey
will, post, space, good, time, include, cost, option, launch, people
drive, card, window, appear, disk, ram, driver, memory, work, color
mr., president, stephanopoulo, state, group, consider, party, question, issue, press
write, article, well, will, thing, work, point, include, time, help
key, article, chip, food, write, people, government, encryption, thing, algorithm
price, buy, apple, computer, dealer, t, model, problem, sell, monitor
god, jesus, will, christian, religion, faith, truth, bible, belief, church

Table 5: PLSVs Topic Model for 20N ews (for 20 topics)
Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
write, people, christian, belief, time, faith, god, religion, life, will
god, will, jesus, kuwait, atheist, church, christian, man, religion, sin
armenian, appear, art, turkish, tartar, 1st, village, armenia, 1.40, genocide
will, key, write, time, article, government, system, thing, chip, hit
mr., stephanopoulo, president, will, party, state, door, time, meeting, open
write, re, article, gun, system, -rcb-, start, people, fbus, claim
car, will, bike, engine, drive, well, dealer, battery, change, front
game, win, year, will, team, play, season, good, goal, playo
player, team, write, hockey, game, fan, article, year, will, guy
space, system, datum, will, april, nasa, security, university, computer, list
graphic, image, le, ftp, send, format, package, system, datum, object
image, datum, program, window, version, le, software, tool, support, user
drive, jumper, master, ndet loop, slave, rate, gun, function, crime, set
window, le, card, will, program, color, driver, support, disk, bit
people, write, state, article, law, government, country, rights, jew, will
write, article, thing, people, good, will, time, lot, year, day
work, drive, tape, scsus, problem, simm, controller, write, memory, article
widget, -rcb-, window, -lcb-, application, resource, set, visual, type, le
price, will, write, system, computer, article, apple, chip, monitor, board
will, vote, comp, newsgroup, suit, problem, os2, sco, post, mail

1127

fiLe & Lauw

Table 6: Semafores Topic Model for Reuters8 (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
company, pipeline, raise, crude, march, spokesman, renery, capacity, corp, post
pct, bank, day, stg, today, reuter, money, market, mln, bill
oer, share, company, board, group, acquire, stock, dlr, acquisition, receive
exchange, currency, dollar, west, nance, baker, monetary, germany, continue, interest
share, reuter, dlr, mln, buy, company, corp, pay, stock, group
price, opec, market, bpd, ocial, february, month, output, saudus, january
rate, bank, pct, cut, fund, prime, point, reserve, issue, lower
billion, foreign, import, increase, dlr, trade, economic, export, will, country
bank, billion, market, government, fall, stock, economy, rise, surplus, decit
will, company, sell, pct, vessel, operation, week, billion, shipping, unit
strike, port, union, spokesman, cargo, employer, worker, sector, redundancy, court
oil, export, dlr, industry, year, pct, future, company, report, price
reuter, pct, report, national, week, brazil, today, increase, pay, april
trade, japan, japanese, reagan, state, tari, unite, market, washington, ocial
grain, mln, soviet, crop, tonne, year, usda, production, fall, analyst
trade, talk, gulf, gatt, bill, yeutter, round, reuter, call, negotiation
certicate, reuter, cost, government, program, agreement, agriculture, will, study, loan
year, ocial, import, will, state, price, government, china, land, rise
mln, ct, loss, net, shr, dlr, prot, qtr, reuter, year
oil, mln, will, barrel, dlr, crude, source, level, petroleum, day

Table 7: PLSVs Topic Model for Reuters8 (for 20 topics)
Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
will, oil, company, reuter, industry, canada, price, shell, raise, sell
rate, currency, dollar, exchange, baker, west, will, bank, reuter, treasury
bank, pct, day, import, year, rate, export, february, expect, reuter
share, company, corp, oer, stock, board, will, reuter, dlr, buy
rate, bank, pct, prime, cut, point, interest, market, lower, savings
market, bank, stock, price, japan, ministry, rise, ocial, gulf, bond
reuter, pct, week, report, year, march, mark, american, commission, gure
mln, ct, loss, net, dlr, shr, year, prot, qtr, reuter
mln, pct, billion, stg, dlr, reuter, market, january, revise, rise
billion, dlr, rate, market, surplus, currency, reserve, trading, dollar, foreign
oil, opec, price, bpd, pipeline, mln, crude, ocial, dlr, output
crude, dlr, barrel, corp, capacity, renery, oil, company, oer, group
reuter, ocial, state, cut, gulf, government, today, action, force, tell
oil, government, indonesium, price, foreign, bank, billion, reserve, company, industry
certicate, company, mln, year, grain, cooperative, program, dlr, government, cost
year, trade, agriculture, reuter, grain, agreement, gatt, yeutter, nancial, agricultural
strike, port, union, spokesman, employer, brazil, cargo, worker, redundancy, sector
trade, japan, japanese, reagan, tari, unite, washington, state, nakasone, semiconductor
grain, mln, crop, tonne, soviet, year, ocial, china, pct, oer
trade, country, minister, talk, state, meeting, economic, exchange, issue, baldrige

1128

fiSemantic Visualization with Neighborhood Graph Regularization

Table 8: Semafores Topic Model for Cade12 (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
sp, aulas, tecnologia, rj, sao, area, janeiro, particulares, areas, sica
terra, jun, gif, busca, virtual, brasil, forum, tempo, noticias, revistas
trabalho, seguranca, saude, medicina, ocupacional, prevencao, ppra, pcmso, imagem, imagens
peixes, cade, lazer, pesca, agua, rio, praia, hotel, sao, doce
agar, vida, personal, sica, base, tratamento, tem, pode, sistema, trainer
sao, br, rio, sul, criancas, www, escola, mail, http, atendimento
links, page, home, fotos, pagina, dicas, download, tenis, informacoes, jogos
internet, informatica, acesso, mg, br, servicos, provedor, mail, revista, horizonte
servicos, sao, paulo, entregas, entrega, sp, cesta, express, empresa, servico
pesca, sp, grupo, brasil, eventos, video, mg, informacoes, turismo, danca
astronomia, pagina, jose, foi, bem, espaco, tem, veja, losoa, correio
mp, banda, musicas, rock, musica, page, letras, bandas, pagina, site
historia, cultura, mundo, site, page, brasil, informacoes, rs, livro, arte
noticias, jornal, cidade, sp, sao, regiao, demolay, ordem, rio, capitulo
empresas, informacoes, informacao, dados, atraves, textos, mail, equipe, unicamp, centro
engenharia, servicos, projetos, empresa, consultoria, quimica, instituto, pesquisa, rio, manutencao
site, informacoes, brasil, associacao, educacao, pagina, organizacao, centro, brasileira, direitos
software, web, empresa, sistemas, sistema, br, marketing, desenvolvimento, windows, dados
virtual, online, venda, produtos, cade, shopping, internet, loja, compras, cursos
futebol, informacoes, fotos, clube, historia, paulo, sao, quake, pagina, cade

Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
engenharia, projetos, servicos, trabalho, empresa, consultoria, seguranca, sp, medicina, sao
sao, ong, rio, instituto, personal, educacao, organizacao, sp, paulo, ns
sao, br, desenvolvimento, sistema, tratamento, mail, sistemas, clientes, informacoes, empresa
aulas, formula, quimica, particulares, informacoes, matematica, pilotos, fotos, sica, site
jornal, tenis, noticias, esportes, sp, informacoes, sao, esporte, fotos, links
musica, page, rock, bandas, links, home, pagina, musicas, music, fotos
pesca, demolay, sp, peixes, sao, fotos, ordem, capitulo, paulo, jitsu
mp, musicas, nacionais, agar, internacionais, rock, formato, site, page, pagina
pesquisa, tecnologia, informacoes, cade, ciencia, geograa, pesquisas, area, instituto, pagina
site, pagina, internet, mail, clique, veja, br, pode, foi, links
astronomia, informacoes, cultura, site, pagina, brasil, home, page, fotos, historia
banda, fotos, rock, letras, page, musicas, pagina, site, home, mp
internet, provedor, acesso, mg, informatica, software, servicos, belo, horizonte, manutencao
futebol, clube, sao, paulo, campeonato, historia, informacoes, pagina, turismo, tricolor
noticias, terra, internet, brasil, informatica, online, jornal, virtual, servicos, busca
links, page, quake, home, pagina, fotos, dicas, mp, download, informacoes
grupo, banda, karate, pagina, page, informacoes, fotos, home, rio, historia
produtos, virtual, shopping, cade, venda, online, sao, rio, loja, compras
br, sao, informacoes, marketing, mail, empresa, internet, www, fax, site
vida, dia, sao, foi, terra, panico, jose, tem, planetas, grande

Table 9: PLSVs Topic Model for Cade12 (for 20 topics)

1129

fiLe & Lauw

References
Akkucuk, U., & Carroll, J. D. (2006). PARAMAP vs. Isomap: a comparison of two nonlinear
mapping algorithms. Journal of Classication, 23 (2), 221254.
Bai, L., Guo, J., Lan, Y., & Cheng, X. (2014). Local Linear Matrix Factorization for
Document Modeling. In Advances in Information Retrieval, pp. 398411. Springer.
Belkin, M., & Niyogi, P. (2001). Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems (NIPS),
Vol. 14, pp. 585591.
Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data
representation. Neural Computation, 15 (6), 13731396.
Belkin, M., Niyogi, P., & Sindhwani, V. (2006). Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning
Research (JMLR), 7, 23992434.
Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Oxford University Press.
Bishop, C. M., Svensen, M., & Williams, C. K. (1998). GTM: The generative topographic
mapping. Neural Computation, 10 (1), 215234.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of
Machine Learning Research (JMLR), 3, 9931022.
Brants, T., & Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium,
Philadelphia.
Buhmann, M. D. (2000). Radial basis functions. Acta Numerica 2000, 9.
Cai, D., Mei, Q., Han, J., & Zhai, C. (2008). Modeling hidden topics on document manifold.
In Proceedings of the ACM Conference on Information and Knowledge Management
(CIKM).
Cai, D., Wang, X., & He, X. (2009). Probabilistic dyadic data analysis with local and global
consistency. In Proceedings of the International Conference on Machine Learning
(ICML).
Cardoso-Cachopo, A. (2007). Improving Methods for Single-label Text Categorization. PhD
Thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa.
Carey, C., & Mahadevan, S. (2014). Manifold Spanning Graphs. In Twenty-Eighth AAAI
Conference on Articial Intelligence.
Chaney, A. J.-B., & Blei, D. M. (2012). Visualizing Topic Models. In Proceedings of the
International AAAI Conference on Web and Social Media (ICWSM).
Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J. L., & Blei, D. M. (2009). Reading
tea leaves: How humans interpret topic models. In Advances in Neural Information
Processing Systems, pp. 288296.
Chen, L., & Buja, A. (2009). Local multidimensional scaling for nonlinear dimension reduction, graph drawing, and proximity analysis. Journal of the American Statistical
Association, 104 (485), 209219.
1130

fiSemantic Visualization with Neighborhood Graph Regularization

Chi, E. H.-h. (2000). A taxonomy of visualization techniques using the data state reference model. In Proceedings of the IEEE Symposium on Information Visualization
(InfoVis), pp. 6975.
Choo, J., Lee, C., Reddy, C. K., & Park, H. (2013). UTOPIAN: User-driven topic modeling based on interactive nonnegative matrix factorization. IEEE Transactions on
Visualization and Computer Graphics, 19 (12), 19922001.
Chuang, J., Manning, C. D., & Heer, J. (2012). Termite: visualization techniques for assessing textual topic models. In Proceedings of the International Working Conference
on Advanced Visual Interfaces (AVI), pp. 7477.
Coifman, R. R., & Lafon, S. (2006). Diusion maps. Applied and Computational Harmonic
Analysis, 21 (1), 5  30.
Comon, P. (1994). Independent component analysis, a new concept?. Signal Processing,
36 (3), 287314.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39 (1),
138.
Dumais, S., Furnas, G., Landauer, T., Deerwester, S., Deerwester, S., et al. (1995). Latent
semantic indexing. In Proceedings of the Text Retrieval Conference.
Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of
Eugenics, 7 (2), 179188.
Gabrilovich, E., & Markovitch, S. (2009). Wikipedia-based semantic interpretation for
natural language processing. Journal of Articial Intelligence Research (JAIR), 34 (2),
443.
Golub, G. H., & Van Loan, C. F. (2012). Matrix Computations, Vol. 3. JHU Press.
Gretarsson, B., Odonovan, J., Bostandjiev, S., Hollerer, T., Asuncion, A., Newman, D., &
Smyth, P. (2012). TopicNets: Visual analysis of large text corpora with topic modeling.
ACM Transactions on Intelligent Systems and Technology (TIST), 3 (2), 23.
Hein, M., Audibert, J.-y., & Luxburg, U. V. (2007). Graph Laplacians and their Convergence
on Random Neighborhood Graphs. In Journal of Machine Learning Research, pp.
13251368.
Hinton, G. E., & Roweis, S. T. (2002). Stochastic neighbor embedding. In Advances in
Neural Information Processing Systems (NIPS), pp. 833840.
Hofmann, T. (1999). Probabilistic latent semantic indexing. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR), pp. 5057.
Hu, Y., Boyd-Graber, J., Satino, B., & Smith, A. (2014). Interactive topic modeling.
Machine Learning, 95 (3), 423469.
Huh, S., & Fienberg, S. E. (2012). Discriminative topic modeling based on manifold learning.
ACM Transactions on Knowledge Discovery from Data (TKDD), 5 (4), 20.
1131

fiLe & Lauw

Iwata, T., Saito, K., Ueda, N., Stromsten, S., Griths, T. L., & Tenenbaum, J. B. (2007).
Parametric embedding for class visualization. Neural Computation, 19 (9), 25362556.
Iwata, T., Yamada, T., & Ueda, N. (2008). Probabilistic latent semantic visualization: topic
model for visualizing documents. In Proceedings of the ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD), pp. 363371.
Jebara, T., Wang, J., & Chang, S.-F. (2009). Graph construction and b-matching for semisupervised learning. In Proceedings of the 26th Annual International Conference on
Machine Learning, pp. 441448. ACM.
Jollie, I. (2005). Principal Component Analysis. Wiley Online Library.
Kim, M., & Torre, F. (2010). Local minima embedding. In Proceedings of the International
Conference on Machine Learning (ICML), pp. 527534.
Kohonen, T. (1990). The self-organizing map. Proceedings of the IEEE, 78 (9), 14641480.
Kruskal, J. B. (1964). Multidimensional scaling by optimizing goodness of t to a nonmetric
hypothesis. Psychometrika, 29 (1), 127.
Laerty, J. D., & Wasserman, L. (2007). Statistical Analysis of Semi-Supervised Regression.
In Advances in Neural Information Processing Systems (NIPS), pp. 801808.
Le, T., & Lauw, H. W. (2014a). Semantic visualization for spherical representation. In
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pp. 10071016. ACM.
Le, T. M., & Lauw, H. W. (2014b). Manifold learning for jointly modeling topic and
visualization. In Proceedings of the AAAI Conference on Articial Intelligence.
Liu, D. C., & Nocedal, J. (1989). On the limited memory BFGS method for large scale
optimization. Mathematical Programming, 45, 503528.
Manning, C. D., Raghavan, P., Schutze, H., et al. (2008). Introduction to Information
Retrieval, Vol. 1. Cambridge University Press Cambridge.
McCallum, A., Wang, X., & Corrada-Emmanuel, A. (2007). Topic and role discovery in
social networks with experiments on enron and academic email.. Journal of Articial
Intelligence Research (JAIR), 30, 249272.
Millar, J. R., Peterson, G. L., & Mendenhall, M. J. (2009). Document Clustering and
Visualization with Latent Dirichlet Allocation and Self-Organizing Maps. In FLAIRS
Conference, Vol. 21, pp. 6974.
Newman, D., Karimi, S., & Cavedon, L. (2009). External evaluation of topic models. In
Australasian Document Computing Symposium (ADCS).
Newman, D., Lau, J. H., Grieser, K., & Baldwin, T. (2010). Automatic evaluation of topic
coherence. In Human Language Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Computational Linguistics, pp. 100
108.
Park, J., & Sandberg, I. W. (1991). Universal approximation using radial-basis-function
networks. Neural Computation, 3 (2), 246257.
1132

fiSemantic Visualization with Neighborhood Graph Regularization

Ponzetto, S. P., & Strube, M. (2007). Knowledge derived from Wikipedia for computing
semantic relatedness.. Journal of Articial Intelligence Research (JAIR), 30, 181212.
Reisinger, J., Waters, A., Silverthorn, B., & Mooney, R. J. (2010). Spherical topic models.
In Proceedings of the International Conference on Machine Learning (ICML), pp.
903910.
Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear
embedding. Science, 290 (5500), 23232326.
Shaw, B., & Jebara, T. (2007). Minimum volume embedding. In Proceedings of the International Conference on Articial Intelligence and Statistics (AISTATS), pp. 460467.
Shaw, B., & Jebara, T. (2009). Structure preserving embedding. In Proceedings of the
International Conference on Machine Learning (ICML), pp. 937944. ACM.
Tenenbaum, J. B., De Silva, V., & Langford, J. C. (2000). A global geometric framework
for nonlinear dimensionality reduction. Science, 290 (5500), 23192323.
Ting, D., Huang, L., & Jordan, M. I. (2010). An Analysis of the Convergence of Graph Laplacians. In Proceedings of the International Conference on Machine Learning (ICML).
Turney, P. D., Pantel, P., et al. (2010). From frequency to meaning: Vector space models
of semantics. Journal of Articial Intelligence Research (JAIR), 37 (1), 141188.
Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine
Learning Research (JMLR), 9 (2579-2605), 85.
Wei, F., Liu, S., Song, Y., Pan, S., Zhou, M. X., Qian, W., Shi, L., Tan, L., & Zhang,
Q. (2010). Tiara: a visual exploratory text analytic system. In Proceedings of the
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(KDD), pp. 153162.
Wu, H., Bu, J., Chen, C., Zhu, J., Zhang, L., Liu, H., Wang, C., & Cai, D. (2012). Locally
discriminative topic modeling. Pattern Recognition, 45 (1), 617625.
Zemel, R. S., & Carreira-Perpinan, M. A. (2004). Proximity graphs for clustering and
manifold learning. In Advances in Neural Information Processing Systems (NIPS),
pp. 225232.
Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Scholkopf, B. (2004). Learning with local
and global consistency. Advances in Neural Information Processing Systems (NIPS),
16 (16).
Zhu, X., Ghahramani, Z., Laerty, J., et al. (2003). Semi-supervised learning using Gaussian
elds and harmonic functions. In Proceedings of the International Conference on
Machine Learning (ICML), Vol. 3, pp. 912919.

1133

fi