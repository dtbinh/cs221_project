Journal of Artificial Intelligence Research 55 (2016) 889-952

Submitted 9/15; published 4/16

Searching for the M Best Solutions in Graphical Models
Natalia Flerova

NFLEROVA @ UCI . EDU

University of California, Irvine
Irvine, CA 92697, USA

Radu Marinescu

RADU . MARINESCU @ IE . IBM . COM

IBM Research  Ireland

Rina Dechter

DECHTER @ UCI . EDU

University of California, Irvine
Irvine, CA 92697, USA

Abstract
The paper focuses on finding the m best solutions to combinatorial optimization problems
using best-first or depth-first branch and bound search. Specifically, we present a new algorithm mA*, extending the well-known A* to the m-best task, and for the first time prove that all its desirable
properties, including soundness, completeness and optimal efficiency, are maintained. Since bestfirst algorithms require extensive memory, we also extend the memory-efficient depth-first branch
and bound to the m-best task.
We adapt both algorithms to optimization tasks over graphical models (e.g., Weighted CSP and
MPE in Bayesian networks), provide complexity analysis and an empirical evaluation. Our experiments confirm theory that the best-first approach is largely superior when memory is available, but
depth-first branch and bound is more robust. We also show that our algorithms are competitive with
related schemes recently developed for the m-best task.

1. Introduction
The usual aim of combinatorial optimization is to find an optimal solution, minimum or maximum,
of an objective function. However, in many applications it is desirable to obtain not just a single
optimal solution, but a set of the first m best solutions for some integer m. We are motivated by
many real-life domains, in which such task arises. For instance, a problem of finding the most likely
haplotype in a pedigree can be presented as finding the most probable assignment in a Bayesian
network that encodes the genetic information (Fishelson, Dovgolevsky, & Geiger, 2005). In practice the data is often corrupted or missing, which makes the single optimal solution unreliable. It
is possible to increase the confidence in the answer by finding a set of the m best solutions and
then choosing the final solution with an expert help or by obtaining additional genetic data. More
examples of the m-best tasks arise in procurement auction problems and in probabilistic expert systems, where certain constraints often cannot be directly incorporated into the model, either because
they make the problem infeasibly complex or they are too vague to formalize (e.g. idiosyncratic
preferences of a human user). Thus in such domains it may be more practical to first find several
good solutions to a relaxed problem and then pick the one that satisfies all additional constraints in a
post-processing manner. Additionally, sometimes a set of diverse assignments with approximately
the same cost is required, as in reliable communication network design. Finally, in the context of
summation problem over graphical models, such as probability of evidence or the partition function,
an approximation can be derived by summing over the m most likely tuples.
c
2016
AI Access Foundation. All rights reserved.

fiF LEROVA , M ARINESCU , & D ECHTER

The problem of finding the m best solutions has been well studied. One of the earliest and
most influential works belongs to Lawler (1972). He provided a general scheme that extends any
optimization algorithm to the m-best task. The idea is to compute the next best solution successively
by finding a single optimal solution for a slightly different reformulation of the original problem that
excludes the solutions generated so far. This approach has been extended and improved over the
years and is still one of the primary strategies for finding the m best solutions. Other approaches
are more direct, trying to avoid the repeated computation inherent to Lawlers scheme. Two earlier
works that are most relevant and provide the highest challenge to our work are by Nilsson (1998)
and Aljazzar and Leue (2011).
 Nilsson proposed a junction-tree based message-passing scheme that iteratively finds the m
best solutions. He claimed that it has the best runtime complexity among m-best schemes for
graphical models. Our analysis (Section 6) shows that indeed Nilssons scheme has the second
best worst case time complexity after our algorithm BE+m-BF (Section 5.3). However, in
practice this scheme is not feasible for problems having a large induced width.
 In their recent work Aljazzar and Leue proposed an algorithm called K*, an A* search-style
scheme for finding the k shortest paths that is interleaved with breadth-first search. They used
a specialized data structure and it is unclear if this approach can be straightforwardly extended
to graphical models, a point that we will leave to future work.
One of the popular approximate approaches to solving optimization problems is based on the
LP-relaxation of the problem (Wainwright & Jordan, 2003). The m-best extension of this approach
(Fromer & Globerson, 2009) does not guarantee exact solutions, but is quite efficient in practice.
We will discuss these and other previous works further in Section 6.
Our main focus lies in optimization in the context of graphical models, such as Bayesian networks, Markov networks and constraint networks. However, some of the algorithms developed
can be used for more general purpose tasks, such as finding m shortest paths in a graph. Various
graph-exploiting algorithms for solving optimization tasks over graphical models were developed
in the past few decades. Such algorithms are often characterized as being either of inference type
(e.g., message-passing schemes, variable elimination) or of search type (e.g., AND/OR search or
recursive-conditioning). In our earlier works, (e.g., Flerova, Dechter, & Rollon, 2011), we extended
inference schemes as represented by the bucket elimination algorithm (BE) (Dechter, 1999, 2013) to
the task of finding the m best solutions. However, due to their large memory requirements, variable
elimination algorithms, including bucket elimination, cannot be used in practice for finding exact
solutions to combinatorial optimization tasks when the problems graph is dense. Depth-first branch
and bound (DFBnB) and best-first search (BFS) are more flexible and can trade space for time. Our
work explores the question of solving the m best solutions task using the heuristic search schemes.
Our contribution lies in extending the heuristic algorithms to the m best solutions task. We describe general purpose m-best variants of both depth-first branch and bound and best-first search,
more specifically A*, yielding algorithms m-BB and m-A* respectively, and analyze their properties. We show that m-A* inherits all A*s desirable properties (Dechter & Pearl, 1985), most
significantly it is optimally efficient compared to any alternative exact search-based scheme. We
also discuss the size of the search space explored by m-BB. We then extend our new m-best algorithms to graphical models by exploring the AND/OR search space.
We evaluate the resulting algorithms on 6 benchmarks having more than 300 instances in total,
and examine the impact of the number of solutions m on the algorithms behaviour. In particular,
890

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

we observe that the runtime of the most of the schemes (except for the depth-first branch and bound
exploring an AND/OR tree) scales much better with m than what worst case theoretical analysis
suggests.
We also show that a m-A* search using the exact bucket elimination heuristic (a scheme we
call BE+m-BF) is highly efficient on easier problems but suffers severely from memory issues
over denser graphs, far more than the A*-based schemes using approximate mini-bucket heuristics. Finally, we compare our schemes with some of the most efficient algorithms based on the
LP-relaxation (Fromer & Globerson, 2009; Batra, 2012), showing competitiveness and even superiority for large values of m (m  10), while providing optimality guarantees.
The paper is organized as follows. In Section 2 we provide relevant background. Section 3
presents the extension of best-first search to the m-best task. In particular, we define m-A*, the
extension of A* algorithm to finding the m best solutions (3.1), and prove its main properties (3.2).
Section 4 describes algorithm m-BB, an extension of depth-first branch and bound algorithm to
solving the m-best task. In Section 5 we discuss the adaptation of the two newly proposed m-best
search algorithms for AND/OR search spaces over graphical models, including a hybrid method
BE+m-BF that incorporates both variable elimination and heuristic search. Section 6 elaborates on
the related work and contrasts it with our methods. Section 7 presents the empirical evaluation of
our m-best schemes and Section 8 concludes.

2. Background
We begin by formally defining the graphical models framework and providing background on
heuristic search.
2.1 Graphical Models
We denote variables by upper-case letters (e.g., X, Y, Z) and their values of variables by lower-case
letters (e.g., x, y, z). Sets of variables are denoted by upper-case letters in bold (e.g. X, Y, Z). The
assignment (X1 = x1 , . . . , Xn = xn ) can be abbreviated as x = (x1 , . . . , xn ).
We denote functions by letters f, g, h etc., and a set of functions byPF. A function f over a
scope S1 = {X1 , . . . , Xr } is denoted by fS1 . P
The summation
P operator xX defines a sum over
all possible values of variables in X, namely x1 X1 , . . . , xn Xn . Minimization minxX and
maximization maxxX operators are defined in a similar manner. Note that we use terms elimination
P
and marginalization interchangeably.
For convenience we sometimes use minx (maxx , x ) to
P
denote minxX (maxxX , xX ).
A graphical model is a collection of local functions over subsets of variables that conveys probabilistic, deterministic, or preferential information, and whose structure is described by a graph. The
graph captures independencies or irrelevance information inherent in the model, that can be useful
for interpreting the modeled data and, most significantly, can be exploited by reasoning algorithms.
The set of local functions can be combined in a variety of ways to generate a global function, whose
scope is the set of all variables.
N
D EFINITION 1 (Graphical model). A graphical model M is a 4-tuple M = hX, D, F, i:
1. X = {X1 , . . . , Xn } is a finite set of variables;

2. D = {D1 , . . . , Dn } is the set of their respective finite domains of values;
891

fiF LEROVA , M ARINESCU , & D ECHTER

3. F = {f1 , . . . , fr } is a set of non-negative real-valued discrete functions, defined over scopes
of variables Si  X. They are called local functions.
N
N
Q P
4.
is a combination operator, e.g.,
 { , } (product, sum)
The graphical model represents
a global function, whose scope is X and which is the combination
N
of all the local functions: rj=1 fj .
N
P
When N
= Qand fi : DSi  N we have Weighted Constraint Satisfaction Problems (WCSPs). When
=
and fi = Pi (Xi | pai ) we have a Bayesian network. The probabilities P are
defined relative to a directed acyclic graph G over X, where the set Xi1 , . . . , Xik are the parents
pai of Xi , i.e. for each Xij there is an edge pointing from Xij to Xi . For illustration, consider the
Bayesian network with 5 variables whose directed acyclic graph (DAG) is given in Figure 1(a).
The most common optimization task for Bayesian network is the most probable explanation
(MPE) also known as maximum a posteriori hypothesis (MAP),1 where the goal is to compute the
optimal value
r
Y

C = max
fj (xSj )
x

j=1

and its optimizing configuration


x = argmax
x

r
Y

fj (xSj )

j=1

The related task, typical for WCSP,
is the min-sum, namely computing a minimal cost
P
P assignment (min-sum): C  = minx j fj (x) and the optimizing configuration x = argminx j fj (x).
Historically this task is also sometimes referred to as energy Q
minimization. It is equivalent to

an MPE/MAP task in the following sense: if Cmax
= maxx j fj (x) is a solution to an MPE




problem,
P then Cmax = exp (Cmin ), where Cmin is a solution to a min-sum problem Cmin =
minx j gj (x) and j, gj (x) =  log (fj (x)).
A graphical model defines the primal graph that captures dependencies between the problems
variables. It has variables as its vertices. An edge connects any two vertices whose variables appear
in the scope of the same function. An important property of a graphical model, characterizing the
complexity of its reasoning tasks is the induced width. An ordered graph is a pair (G, o) where
G is an undirected graph, and o = (X1 , . . . , Xn ) is an ordering of nodes. The width of a node
is the number of the nodes neighbors that precede it in the ordering. The width of a graph along
an ordering o is the maximum width over all nodes. An induced ordered graph is obtained from
an ordered graph as follows: nodes are processed from last to first based on o; when node Xj is
processed, all its preceding neighbors are connected. The width of an ordered induced graph along
the ordering o is called induced width along o and is denoted by w (o). The induced width of a
graph, denoted by w , is the minimal induced width over all its orderings. Abusing notation we
sometimes use w to denote the induced width along a particular ordering, when the meaning is
clear from the context.
Figure 1(b) depicts the primal graph of the Bayesian network from Figure 1(a). Figures 1(c) and
1(d) show the induced graphs of the primal graph from Figure 1(a) respectively along the orderings
1. In some communities MAP also refers to the task of optimizing a partial assignment to the variables. However, in
this paper we use MAP and MPE as interchangeable, both referring to an optimal full variable assignment.

892

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

P (A)

A

A

B

E

C

D

D

C

E

B

A

A

(c)

(d)

P (B|A)

B

C

B

C

P (C|A)

E

E

P (E|B, C)

D

D

P (D|A, B)

(a)

(b)

Figure 1: (a) A DAG of a Bayesian network, (b) its primal graph (also called moral graph),
(c) its induced graph along o = (A, E, D, C, B), and (d) its induced graph along
o = (A, B, C, D, E), example by Gogate (2009).

o = (A, E, D, C, B) and o0 = (A, B, C, D, E). The dashed lines in Figure 1(c) represent the
induced edges, namely edges that are absent from the moral graph, but were introduced in the
induced graph. We can see that the induced width along ordering o is w (o) = 4 and the induced
width along ordering o0 is w (o0 ) = 2, respectively.
2.2 Heuristic Search
Our analysis focuses on best-first search (BFS), whose behaviour for the task of finding a single
optimal solution is well understood. Assuming a minimization task, best-first search always expands
the node with the best (i.e., smallest) value of the heuristic evaluation function. It maintains a graph
of explored paths, a list CLOSED of expanded nodes and a frontier of OPEN nodes. BFS chooses
from OPEN a node n with the smallest value of a heuristic evaluation function f (n), expands it by
generating its successors succ(n), places it on CLOSED, and places succ(n) in OPEN. The most
popular variant of best-first search, A*, uses the heuristic evaluation function f (n) = g(n) + h(n),
where g(n) is the cost of the path from the root to n, and h(n) is a heuristic function that estimates
the optimal cost to go h (n) from n to a goal node. A heuristic function is called admissible if
it never overestimates (for minimization) the true minimal cost to reach the goal h (n). Namely,
n h(n)  h (n). A heuristic is called consistent or monotonic, if for every node n and for every
successor n0 of n the following inequality holds: h(n)  c(n, n0 ) + h(n0 ). If h(n) is consistent,
then the values of evaluation function f (n) along any path are non-decreasing. It is known that
regardless of the tie-breaking rule A* expands any node n reachable by a strictly C  -bounded path
from the root, and such a node is referred to as surely expanded by A* (Dechter & Pearl, 1985).
A path  is C  -bounded relative to f , if n   : f (n) < C  , where C  is the cost of optimal
solution.
A* search has a number of attractive properties (Nillson, 1980; Pearl, 1984; Dechter & Pearl,
1985):
893

fiF LEROVA , M ARINESCU , & D ECHTER

 Soundness and completeness: A* terminates with the optimal solution.
 When h is consistent, A* explores only the set of nodes S = {n|f (n)  C  } and it surely
expands all the nodes having S = {n|f (n) < C  }.
 Optimal efficiency under consistent heuristic: When h is consistent, any node surely expanded by A* must be expanded by any other sound and complete search algorithm using the
same heuristic information.
 Optimal efficiency for node expansions: When the heuristic function is consistent, A*,
when searching a graph, expands each node at most once, and at the time of nodes expansion
A* has found the shortest path to it.
 Dominance: Given two heuristic functions h1 and h2 , s.t. n h1 (n) < h2 (n), A1 will expand
every node surely expanded by A2 , where Ai uses heuristic hi .
Although best-first search is known to be the best algorithm in terms of number of nodes expanded (Dechter & Pearl, 1985), it requires exponential memory in the worst-case.
A popular alternative is the depth-first branch and bound (DFBnB), whose most attractive feature, compared to best-first search, is that it can be executed with linear memory. Yet, when the
search space is a graph, it can exploit memory to improve its performance by flexibly trading space
and time. Depth-first branch and bound expands nodes in a depth-first manner, maintaining the cost
of the best solution found so far which is an upper bound U B on the cost of the optimal solution.
If the heuristic evaluation function of the current node n is greater or equal to the upper bound, the
node is pruned and the subtree below it is never explored. In the worst case depth-first branch and
bound explores the entire search space. In the best case the first solution found is optimal, in which
case its performance can be as good as BFS. However, if the solution depth is unbound, depth-first
search might follow an infinite branch and never terminate. Also, if the search space is a graph,
DFBnB may expand nodes numerous time, unless it uses caching and checks for duplicates.
2.3 Search in Graphical Models
Search algorithms provide a way to systematically enumerate all possible assignments of a given
graphical model. Optimization problems over graphical models can be naturally presented as the
task of finding an optimal cost path in an appropriate search space.
The simplest variant of a search space is the so-called OR search tree. Each level corresponds to
a variable from the original problem. The nodes correspond to partial variable assignments and the
arc weights are derived from problems input functions. The size of such a search tree is bounded
by O(k n ), where n is the number of variables and k is the maximum domain size.
Throughout this section we are going to illustrate the concepts using an example problem with
six variables {A, B, C, D, E, F } and six pairwise functions. Its primal graph is shown in Figure
2(a). Figure 2(b) displays the OR search tree corresponding to the lexicographical ordering.
2.3.1 AND/OR S EARCH S PACES
OR search trees are blind to the problem decomposition encoded in graphical models and can therefore be inefficient. They do not exploit the independencies in the model. AND/OR search spaces
894

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

(a) Primal
graph

(b) OR search tree along ordering A, B, C, D, E, F

Figure 2: An example problem with 6 variables {A, B, C, D, E, F } and 5 pairwise functions.
for graphical models have been introduced to better capture the problem structure (Dechter & Mateescu, 2007). The AND/OR search space is defined relative to a pseudo tree of the primal graph
that captures problem decomposition. Figure 3(a) shows the pseudo tree of the example problem.
D EFINITION 2. A pseudo tree of an undirected graph G = (V, E) is a directed rooted tree T =
(V, E 0 ), such that every arc of G not included in E 0 is a back-arc in T , namely it connects a node
in T to an ancestor in T . The arcs in E 0 may not all be included in E.
Given a graphical model M = hX, D, Fi with primal graph G and a pseudo tree T of G, the
AND/OR search tree ST contains alternating levels of OR and AND nodes. Its structure is based
on the underlying pseudo tree T . The root node of ST is an OR node labelled by the variable at the
root of T . The children of an OR node Xi are AND nodes labelled with value assignments hXi , xi i
(or simply hxi i). The children of an AND node hXi , xi i are OR nodes labelled with the children
of Xi in T , representing conditionally independent subproblems. An AND/OR tree corresponding
to the pseudo tree in Figure 3(a) is shown in Figure 3(b). The arcs from nodes Xi to hXi , xi i in an
AND/OR search tree are annotated by the weights derived from the cost functions in F:
D EFINITION 3 (arc weight). The weight w(Xi , xi ) of the arc (Xi , hXi , xi i) is the combination (i.e.
sum for WCSP and product for MPE) of all the functions, whose scope includes Xi and is fully
assigned along the path from root to the node corresponding to hXi , xi i, evaluated at all values
along the path.
Some identical subproblems can be identified by their context (namely, a partial instantiation of
their ancestors that separates the subproblem from the rest of the problem graph), can be merged,
yielding an AND/OR search graph (Dechter & Mateescu, 2007). Merging all context-mergeable
nodes yields the context-minimal AND/OR search graph, denoted by CT . An example can be seen in
Figure 3(c). The size of the context-minimal AND/OR search graph can be shown to be exponential
in the induced width of G along the pseudo tree T (Dechter & Mateescu, 2007).
A solution tree T of CT is a subtree such that: (1) it contains the root node of CT ; (2) if an
internal AND node n is in T , then all its children are in T ; (3) if an internal OR node n is in T ,
then exactly one of its children is in T ; (4) every tip node in T (i.e., nodes with no children) is a
terminal node. The cost of a solution tree is the product, for MPE or sum for WCSP, of the weights
associated with its arcs.
Each node n in CT is associated with a value v(n) capturing the optimal solution cost of the
conditioned subproblem rooted at n. Assuming an MPE/MAP problem, it was shown that v(n) can
895

fiF LEROVA , M ARINESCU , & D ECHTER

A

OR
AND

0

1

OR

B

B

AND

0

1

0

1

A

C

OR

B
C

E

D

F

AND

0

OR

C

E
1

0

1

D D

F

F

0

AND 0 1 0 1 0 1 0 1

E

C

1

0

1

D D

F

F

0

0 1 0 1 0 1 0 1

(a) Pseudo tree

1

0

1

D D

F

F

0

E
1

0

1

D D

F

F

0 1 0 1 0 1 0 1

0 1 0 1 0 1 0 1

(b) AND/OR search tree

A

OR
AND

0

1

OR

B

B

AND

0

1

C

C

OR
AND

C

E

0

1

0

0

C

C
1

0

1

0

1

E
1

0

E
1

0

E

E
1

0

1

0

1

OR

D

D

D

D

F

F

F

F

AND

0 1

0 1

0 1

0 1

0 1

0 1

0 1

0 1

(c) Context-minimal AND/OR search graph

Figure 3: AND/OR search spaces for graphical models.

be computed recursively based on the values of ns successors: OR nodes by maximization, AND
nodes by multiplication. For WCSPs, v(n) for OR and AND nodes is updated by minimization and
by summation, respectively (Dechter & Mateescu, 2007).
We next provide an overview of a depth-first branch and bound and best-first search algorithms,
that explore AND/OR search spaces (Marinescu & Dechter, 2009b, 2009a; Otten & Dechter, 2011).
These schemes use heuristics generated either by the mini-bucket elimination scheme (2.3.4) or
through soft arc-consistency schemes (Marinescu & Dechter, 2009a, 2009b; Schiex, 2000; Darwiche, Dechter, Choi, Gogate, & Otten, 2008) or their composite (Ihler, Flerova, Dechter, & Otten,
2012). As it is customary in the heuristic search literature, when defining search algorithms we
assume without loss of generality a minimization task (i.e., min-sum optimization problem).
896

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 1: AOBF exploring the AND/OR search tree (Marinescu & Dechter, 2009b)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

24
25
26
27
28
29
30
31
32

Input: A graphical model M = hX, D, Fi, pseudo tree T rooted at X1 , heuristic function h()
Output: Optimal solution to M
Create root OR node s labelled by X1 and let G (explored search space) = {s};
Initialize v(s) = h(s) and best partial solution tree T  to G;
while s is not SOLVED do
Select non-terminal tip node n in T  . If there is no such node then break;
if n is an OR node labeled Xi then
forall the xi  D(Xi ) do
Create AND child n0 = hXi , xi i;
if n is TERMINAL then
Mark n0 SOLVED;
succ(n)  succ(n)  n0 ;
else if n is an AND node labeled hXi , xi i then
forall the successor Xj of Xi in T do
Create OR child n0 = Xj ;
succ(n)  succ(n)  n0 ;
Initialize v(n0 ) = h(n0 ) for all new nodes;
Add new nodes to the explores search space G  G  {succ(n)};
Let S  {n};
while S 6=  do
Let p be a node in S that has no descendants in G still in S;
S  S  {p};
if p is OR node then
v(p) = minksucc(p) (w(p, k) + v(k));
Mark best successor k of OR ancestors p, such that k = arg minksucc(p) (w(p, k) + v(k))
(maintaining previously marked successor if still best);
Mark p as SOLVED if its best marked successor is solved;
else if p is AND
P node then
v(p) = ksucc(p) v(k);
Mark all arcs to the successors;
Mark p as SOLVED if all its children are SOLVED;
if p changes its value or p is marked SOLVED then
Add to S all those parents of p such that p is one of their successors through a marked arc;
Recompute T  by following marked arcs from the root s;
return hv(s), T  i;

2.3.2 B EST-F IRST AND/OR S EARCH
The state-of-the-art version of best-first search for AND/OR search spaces for graphical models
is the Best-First AND/OR search algorithm (AOBF) (Marinescu & Dechter, 2009b). AOBF is a
variant of AO* (Nillson, 1980) that explores the context-minimal AND/OR search graph.
AOBF is described by Algorithm 1. For simplicity, we present the algorithm for traversing
an AND/OR search tree. AOBF maintains the explicated part of the search space G and also keeps
track of the current best partial solution tree T  . It interleaves iteratively a top-down node expansion
step (lines 4-16), which selects a non-terminal tip node of T  and generates its children in G, with
a bottom-up cost revision step (lines 17-30), which updates the values of the internal nodes based
on their childrens values. If a newly generated child node is terminal it is marked solved (line 9).
897

fiF LEROVA , M ARINESCU , & D ECHTER

During the bottom-up phase, OR nodes that have at least one solved child and AND nodes who have
all children solved are also marked as solved. The algorithm also marks the arc to the best AND
child of an OR node through which the minimum is achieved (line 23). Following the backward
step, a new best partial solution tree T  is recomputed (line 31). AOBF terminates when the root
node is marked solved. If the heuristic used is admissible, at the point of termination T  is the
optimal solution with cost v(s), where s is the root node of the search space.
Extending the algorithm to explore the context-minimal AND/OR search graph is straightforward and can be done as follows. When expanding a non-terminal AND node in lines 11-14, AOBF
does not generate the corresponding OR children that are already present in the explicated search
space G but rather links to them. All these identical OR nodes in G are easily recognized based on
their contexts (Marinescu & Dechter, 2009b).
T HEOREM 1 (complexity, Marinescu & Dechter, 2009b). Algorithm AOBF traversing the context
minimal AND/OR graph has time and space complexity of O(n  k w ), where n is the number of
variable in the problem, w is the induced width of the pseudo tree and k bounds the domain size.
2.3.3 D EPTH -F IRST AND/OR B RANCH AND B OUND
The depth-first AND/OR Branch and Bound (AOBB) (Marinescu & Dechter, 2009a) algorithm
traverses the AND/OR search space in a depth-first rather than a best-first manner, while keeping
track of the current upper bound on the minimal solution cost.
As before and for simplicity, we present the variant of the algorithm that explores an AND/OR
search tree. AOBB described by Algorithm 2 interleaves forward node expansion (lines 4-17) with
a backward cost revision (or propagation) step (lines 19-29) that updates node values (capturing
the current best solution to the subproblem rooted at each node), until search terminates and the
optimal solution has been found. A node n will be pruned (lines 12-13) if the current upper bound
is higher than the nodes heuristic lower bound, computed recursively using the procedure described
in Algorithm 3.

In the worst case, AOBB explores the entire search space, namely O(n  k w ) nodes (assuming
a context-minimal AND/OR search graph). In practice, however, AOBB is likely to expand more
nodes than AOBF using the same heuristic, but the empirical performance of AOBB depends heavily
on the order in which the solutions are encountered, namely on how quickly the algorithm finds a
close to optimal solution that it will use as an upper bound for pruning.
2.3.4 M INI -B UCKET H EURISTICS
The AND/OR search algorithms presented (AOBF and AOBB) most often use the mini-bucket
(also known as MBE) heuristic h(n). Mini-Bucket Elimination or MBE (Dechter & Rish, 2003)
is an approximate version of an exact variable elimination algorithm called bucket elimination (BE)
(Dechter, 1999). MBE (Algorithm 4) bounds the space and time complexity of full bucket elimination (which is exponential in the induced width w ). Given a variable ordering, the algorithm
associates each variable Xi with a bucket which contains all functions defined on this variable,
but not on higher index variables. Large buckets are partitioned into smaller subsets, called minibuckets, each containing at most i distinct variables. The parameter i is called the i-bound. The
algorithm processes buckets them from last to first (lines 2-10 in Algorithm 4). The mini-buckets of
the same variable are processed separately. Assuming a min-sum problem, MBE calculates the sum
898

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 2: AOBB exploring the AND/OR search tree (Marinescu & Dechter, 2009b)

1
2
3
4
5
6
7
8
9
10
11

12
13

Input: A graphical model M = hX, D, Fi, pseudo tree T rooted at X1 , heuristic function h();
Output: Optimal solution to M
Create root OR node s labelled by X1 and let the stack of created but not expanded nodes OP EN  {s};
Initialize v(s)   and best partial solution tree rooted in s T  (s)  ; U B  ;
while OP EN 6=  do
Select top node n in OPEN;
if n is OR node labeled Xi then
foreach xi  D(Xi ) do
Add AND child n0 labeled hXi , xi i to list of successors of n;
Initialize v(n0 ) = 0, best partial solution tree rooted in n T  (n0 ) = ;
if n is AND node labelled hXi , xi i then
foreach OR ancestor k of n do
Recursively evaluate the cost of the partial solution tree rooted in k, based on heuristic function
h(), assign its cost to f (k); // see Algorithm 3
if evaluated partial solution is not better than current upper bound at k (e.g. f (k)  v(k) then
Prune the subtree below the current tip node n;
else

14

foreach successor Xj of Xi  T do
Add OR child n0 labeled Xj to list of successors of n;
Initialize v(n0 )  , best partial solution tree rooted in n, T  (n0 )  ;

15
16
17

18
19
20
21
22
23
24
25
26
27

28
29

Add successors of n on top of OPEN;
while list of successors of node n is empty do
if node n is the root node then
return solution: v(n), T  (n) ;
else
if p is AND node then
v(p)  v(p) + v(n), T  (p)  T  (p)  T  (n);
else if p is OR node then
if the new value of better than the old one, e.g. v(p) > (c(p, n) + v(n)) for minimization then
v(p)  w(p, n) + v(n), T  (p)  T  (p)  hxi , Xi i;
Remove n from the list of successors of p;
Move one level up: n  p;

of the functions in each mini-bucket and eliminates its variable using the min operator (line 9). The
new function is placed in the appropriate lower bucket (line 10). MBE generates a bound (lower for
minimization and upper for maximization) on the optimal value. Higher i values take more computational resources, but yield more accurate bounds. When i is large enough (i.e., i  w ), MBE
coincides with full Bucket Elimination.
T HEOREM 2 (complexity, Dechter & Rish, 2003). Given a graphical model with variable ordering
o having induced width w (o) and an i-bound parameter i, the time of the mini-bucket algorithm


MBE(i) is O(nk min(i,w (o))+1 ) and space complexity is O(nk min(i,w (o)) ), where n is the number
of problem variables and k is the maximum domains size.
Mini-bucket elimination can be viewed as message passing from leaves to root along a minibucket tree. A mini-bucket tree of a graphical model M has the mini-buckets as its nodes. BucketX
899

fiF LEROVA , M ARINESCU , & D ECHTER

Algorithm 3: Recursive computation of the heuristic evaluation function
1

2
3
4

function evalPartialSolutionTree(T (n), h(n))
Input: Partial solution subtree T (n) rooted at node n, heuristic function h(n);
Output: Heuristic evaluation function f (T (n));
if succ(n) ==  then
if n is an AND node then
return 0;
else

5

return h(n);

6
7
8
9
10
11
12
13

else
if n is an AND node then
let k1 , . . . , kl be the OR children of n;
P
return li=1 evalPartialSolutionTree(T (ki ), h(ki ));
else if n is an OR node then
let k be the AND child of n;
return w(n, k) + evalPartialSolutionTree(T (k), h(k));

Algorithm 4: Mini-Bucket Elimination

1

2
3
4
5
6
7
8
9
10

11

12

Input: A model M = hX, D, Fi, ordering o, parameter i
Output: Approximate solution to M, and the ordered augmented buckets
Initialize: Partition the functions in F into Bucket1 , . . . , Bucketn , where Bucketi contains all functions
whose highest variable is Xi .
// Backward pass
for p  n downto 1 do
Let h1 , . . . , hj be the functions (original and intermediate) in Bucketp ; let S1 , . . . , Sj be their scopes;
if Xp is instantiated (Xp = xp ) then
Assign Xp = xp to each hi and put each resulting function into its appropriate bucket;
else
Generate an i-partitioning;
foreach Qk  Q0 do
P
Generate the message function hkb : hkb = minxp Xp ji=1 hi ;
Add hkb to the bucket of Xb , the largest-index variable in scope(hkb );
// Forward pass
Assign a value to each variable in the ordering o so that the combination of the functions in each bucket is
minimal;
return The function computed in the bucket of the first variable and the corresponding assignment;

is a child of BucketY is the function hXY generated in BucketX when variable X is eliminated,
is placed in BucketY . Therefore, every vertex other than the root has one parent and possibly
several child vertices. Note that a mini-bucket tree corresponds to a pseudo tree, where the minibuckets of the same variables are combined to form what we call augmented buckets, corresponding
to variable nodes (Dechter & Mateescu, 2007).
Mini-bucket elimination is often used to generate heuristics for search algorithms over graphical
models, formulated for OR search spaces by Kask and Dechter (1999a, 1999b) and extended to
AND/OR search by Marinescu and Dechter (2005).
900

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

D EFINITION 4 (MBE heuristic for the AND/OR search space, Marinescu & Dechter, 2005). Given
an ordered set of augmented buckets {B(X1 ), . . . , B(Xn )} generated by the Mini-Bucket algorithm
M BE(i) along the bucket tree T , and given a node n in the AND/OR search tree, the static minibucket heuristic function h(n) is computed as follows:
1. If n is an AND node, labeled by hXp , xp i, then:
X
h(n) =
hkj
hkj {B(Xp )B(Xp1 ...Xpq )}

Namely, it is the sum of the intermediate functions hkj that satisfy the following two properties:
 they are generated in buckets B(Xk ), where Xk is any descendant of Xp in the bucket tree T
 they reside in bucket B(Xp ) or the bucket B(Xp1 . . . Xpq ) = {B(Xp1 ), . . . , B(Xpq )} that correspond to the ancestors {Xp1 ), . . . , Xpq } of Xp in T
2. If n is an OR node, labeled by Xp , then:
h(n) =

min

msucc(p)

(w(n, m) + h(m))

where m are the AND children of n labeled with values xp of Xp .
Having established the necessary background, we will now turn to the main part of the paper,
presenting our contributions, beginning with the extension of best-first search to the m-best task.
As it is customary in the heuristic search literature and without loss of generality, we will assume in
the remaining of the paper a min-sum optimization problem.

3. Best-First Search for Finding the M Best Solutions
Extending best-first search (Section 2.2) and in particular its most popular version, A*, to the mbest task is fairly straightforward and was suggested, for example, by Charniak and Shimony (1994).
Instead of stopping after finding the optimal solution, the algorithm continues exploring the search
space, reporting the next discovered solutions up until m of them are obtained. We will show that
these solutions are indeed the m best and that they are found in a decreasing order of their optimality.
In particular, the second solution reported is the second best solution and, in general, the ith solution
discovered is the ith best.
3.1 m-A*: Definition
The m-best tree-search variant of A* denoted m-A* (Algorithm 5, assumes a consistent heuristic)
solves an m-best optimization problem over any general search graph. We will show later how it
can be extended to general admissible heuristics.
The scheme expands the nodes in the order of increasing value of f in the usual A* manner.
It keeps the lists of created nodes OPEN and expanded nodes CLOSED, as usual, maintaining a
search tree, denoted by T r. Beginning with the start node s, m-A* picks the node with the smallest
evaluation function f (n) in OPEN and puts it in CLOSED (line 7). If the node is a goal, a new
solution is reported (lines 8-13). Otherwise, the node is expanded and its children are created (lines
15-23). The algorithm may encounter each node multiple times and will maintain up to m of its
901

fiF LEROVA , M ARINESCU , & D ECHTER

Algorithm 5: m-A* exploring a graph, assuming consistent heuristic

1
2
3
4
5
6
7

8
9

10
11

Input: An implicit directed search graph G = (N, E), with a start node s and a set of goal nodes Goals, a
consistent heuristic evaluation function h(n), parameter m
Output: the m best solutions
Initialize: OPEN=, CLOSED=, a tree T r = , i = 1 (i counts the current solution being searched for)
OPEN  {s}; f (s) = h(s);
Make s the root of T r;
while i  m do
if OPEN is empty then
return the solutions found so far;
Remove a node, denoted n, in OPEN having a minimum f (break ties arbitrarily, but in favour of goal nodes
and deeper nodes) and put it in CLOSED;
if n is a goal node then
Output the current solution obtained by tracing back pointers from n to s (pointers are assigned in step
22); denote this solution as Soli ;
if i = m then
return;
else

12

i  i + 1;

13
14
15
16
17
18
19
20
21
22
23

24

else
Expand node n, generating all its children Ch ;
foreach n0  Ch do
if n0 already appears in OPEN or CLOSED m times then
Discard node n0 ;
else
Compute current path cost g(n0 ) = g(n) + c(n, n0 );
Compute evaluation function f (n0 ) = g(n0 ) + h(n0 ) ;
Attach a pointer from n0 back to n in T r;
Insert n0 into the right place in OPEN based on f (n0 );
return The set of the m best solutions found

copies in the OPEN and CLOSED lists combined (line 17), with separate paths to each copy in the
explored search tree (lines 22-23). Nodes encountered beyond m times are discarded (line 18). We
denote by Ci the ith best solution cost, by fi (n) the cost of the ith best solution going through node
n, by fi (n) the heuristic evaluation function estimating fi (n) and by gi (n) and hi (n) the estimates
of the ith best costs from s to n and from n to a goal, respectively.
If the heuristic is not consistent, whenever the algorithm reaches a node it has seen before (if
the search space is a graph and not a tree), there exists a possibility of the new path improving on
the previously discovered ones. Therefore, lines 17-18 should be revised in the following way to
account for the possibility that a better path to n0 is discovered:
17
If n0 appears already more than m times in the union of OPEN or CLOSED then
18
If g(n0 ) is strictly smaller than gm (n0 ), the current m-best path to n0 then
19
Keep n0 with a pointer to n and put n back in OPEN
20
Discard the earlier subtree rooted at n
Figure 4 shows an example of m-A* for finding the m = 3 shortest paths on a toy problem. The
left hand side of Figure 4 shows the problem graph with 7 variables and 8 edges, together with the
902

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

A
3

h(A) = 5

2

h(B) = 4

A

order in which
nodes are expanded

#1

h(C) = 2

B

#5

C

B f =6

h(D) = 3

C2
C3

1

2

#8

D

D2

4

#2

m=3
C1 = 8

C f =4

= 10
= 10

#3
D1 f = 6

f =8

2
h(F ) = 1

h(E) = 1

E

F
3

#10 f = 9 #9
E2
F2 f = 8

3
G

h(G) = 0

G4
f = 12

#12
G3

#7 f = 6 #4
F1

f = 8 E1

#11
G2

f = 10

f = 10

#6
G1
f =8

nodes on CLOSED

(a) Problem graph

(b) Trace of m-A*

Figure 4: Example problem. On the left: problem graph and heuristic values h(n) for each node.
On the right: trace of m-A* for finding m = 3 best solutions and evaluation function f (n)
for each node. White nodes are in CLOSED, the grey one was created, but discarded.

admissible heuristic functions for each node. Note that the heuristic is not consistent. For example,
h(A) > h(C) + c(A, C). A is the start node, G is the goal node. On the right side of the Figure
we present the trace of m-A*, with the evaluation function for each copy of the nodes created by
the time the 3rd solution is found. The white nodes are in CLOSED, the grey one (node G4 ) was
created, but never put in OPEN. The algorithm expands the nodes in OPEN in increasing order of
the evaluation functions. We assume that the ties are broken in favour of deeper nodes. First, m-A*
discovers the solution A  C  D  F  G with cost C1 = 8, next the solution A  C  D  E  G
with cost C1 = 10 is found. The third solutions is A  B  D  F  G with cost C1 = 10. Note
that two copies of each node D, E and F and four copies of G were created. The goal node G4 was
discarded, because we bound the total number of copies of a particular node by m = 3.
N
T HEOREM 3. Given a graphical model M = hX, D, F, i with n variables whose domain size is
bounded by k, the worst case time and space complexity of m-A* exploring an OR search tree of M
is O(k n ).
Proof. In worst case m-A* would explore the entire OR search tree, whose size is O(k n ) (Section 2.3). Since the underlying search space is a tree, the algorithm will never encounter any of the
nodes more than once, thus no nodes will be duplicated.
903

fiF LEROVA , M ARINESCU , & D ECHTER

3.2 Properties of m-A*
In this section we extend the desirable properties of A*, listed in Section 2.2, to the m-best case. For
simplicity and without loss of generality, we assume throughout that the search graph accommodates
at least m distinct solutions.
T HEOREM 4. Given an optimization task, its implicit directed search graph G and some integer
parameter m  1, m-A* guided by an admissible heuristic has the following properties:
1. Soundness and completeness: m-A* terminates with the m best solutions generated in order
of their costs.
2. Optimal efficiency under consistent heuristic: Any node that is surely expanded2 by m-A*
must be expanded by any other search algorithm traversing G that is guaranteed to find the
m best solutions having the same heuristic information.
3. Optimal efficiency for node expansions: m-A* expands each node at most m times when the
heuristic is consistent. The ith path found to a node is the ith best path.
4. Dominance: Given two heuristic functions h1 and h2 , such that for every n h1 (n) < h2 (n),
m-A*1 will expand every node surely expanded by m-A*2 , when m-A*i is using heuristic hi .
We prove the properties of m-A* in Sections 3.2.1-3.2.2.
3.2.1 S OUNDNESS AND C OMPLETENESS
Algorithm m-A* maintains up to m copies of each node and discards the rest. We will next show
that this restriction does not compromise completeness.
P ROPOSITION 1. Any node discarded by m-A* does not lead to any of the m-best solutions.
Proof. Consider a consistent heuristic first (as described in Algorithm 5). At the moment when
m-A* discovered a node n for the (m + 1)th time, m copies of n reside on OPEN or CLOSED
and the algorithm maintains m distinct paths to each. Let m be the (m + 1)th path. As we will
prove in Theorem 10, when node n is discovered for the (m + 1)th time, the cost Cnew of the newly
discovered path new is the (m + 1)th best, namely it is no better than the costs already discovered:
Cnew  Cm . Therefore, the eliminated (m + 1)th path to node n is guaranteed to be worse than
the remaining m ones and thus can not be a part of any of the potential m-best optimal solutions
that might be passing through node n.
If the heuristic is not consistent, m-A* can be modified to replace the worst of the previously
discovered paths m with the newly found new , if the cost of the latter is better and place the new
copy in OPEN. Thus, again, it is safe to bound the number of copies by m.
It is clear that along any particular solution path  the evaluation function over all the nodes on
 is bounded by the paths cost C(), when the heuristic is admissible.
P ROPOSITION 2. The following is true regarding m-A*:
1. For any solution path , forall n  , f (n)  C().
2. To be precisely defined in Section 3.2.3

904

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

2. Unless  was already discovered by m-A*, there is always a node n on  which resides in
OPEN.
3. Therefore, as long as m-A* did not discover  there must be a node in OPEN having f (n) 
C().
Proof. 1. f (n) = g (n) + h(n) and since h(n)  c (n, t) due to admissibility, where c (n, t) is
the actual cost from n to the goal node t along , we conclude that f (n)  g (n) + h(n) = C().
2. Any reachable path from the root always has a leaf in OPEN unless all the nodes along the path
are expanded and are in CLOSED.
3. Follows easily from 1 and 2.
It follows immediately from Proposition 2 (stated similarly by Nilsson, 1982) that:
P ROPOSITION 3. [Necessary condition for node expansion] Any node n expanded during m-A*
when searching for the ith best solution (1  i  m) satisfies f (n)  Ci .
and it is also clear that
P ROPOSITION 4. [Sufficient condition for node expansion] Every node n in OPEN, such that
f (n) < Ci , must be expanded by m-A* before the ith best solution is found.
Soundness and completeness of m-A* follows quite immediately.
T HEOREM 5 (soundness and completeness). Algorithm m-A* generates the m-best solutions in
order, namely, the ith solution generated is the ith best solution.
Proof. Let us assume by contradiction that this is not the case. Let the ith generated solution path
i be the first one that is not generated according to the best-first order. Namely the ith solution
generated has a cost C such that C > Ci . However, when the algorithm selected the goal ti along
i , its evaluation function was f (ti ) = gi (ti ) = C, while, based on Proposition 2, there was a
node n0 in OPEN whose evaluation function was at most Ci . Thus n0 should have been selected for
expansion instead of ti . We have a contradiction and therefore the result follows.
3.2.2 T HE I MPACT OF THE H EURISTIC S TRENGTH
Like for A*, the performance of m-A* improves with more accurate heuristic.
P ROPOSITION 5. Consider two heuristic functions h1 and h2 . Let us denote by m-A*1 the algorithm that uses heuristic h1 and by m-A*2 the one using heuristic h2 . If the heuristic h1 is
more informed than h2 , namely for every node n, h2 (n) < h1 (n), algorithm m-A*2 will expand
every node that will be expanded by the algorithm m-A*1 before finding the j th solution for any
j  {1, 2, . . . , m}, assuming the same tie-breaking rule.
Proof. Since h1 is more informed than h2 , h1 (n) > h2 (n) for every non-goal node n. Let us
assume that m-A*1 expands some non-terminal node n before finding the j th best solution with
cost Cj . If node n is expanded, it means that (a) at some point it is on OPEN and (b) its evaluation
function satisfies f1 (n) = g(n) + h1 (n)  Cj (Proposition 3). Consider the current path  from
start node to n. Each node n0   on the path was selected at some point for expansion and thus
905

fiF LEROVA , M ARINESCU , & D ECHTER

m-A search space

search space of
C1 Cm
any other algorithm
Ci


|Cm
 C1 |

search space
explored by m-A
compared to A

Figure 5: The schematic representation of the search spaces explored by the m-A* algorithm, de
pending on m and cost Cm

the evaluation functions of all these nodes are also bounded by the cost of the j th best solution:
f1 (n0 )  Cj . Since h1 (n0 ) > h2 (n0 ) for every node n0 along the path , their evaluation functions
according to heuristic h2 (n) obey:
f2 (n0 ) = g(n0 ) + h2 (n0 ) < g(n0 ) + h1 (n0 ) < Cj

(1)

and thus each node n0 must also be expanded by m-A*2 .
Consider the case of the exact heuristic. It is easy to show that:
T HEOREM 6. If h = h is the exact heuristic, then m-A* generates solutions only on j-optimal
paths 1  j  m.
Proof. Since h is exact, the f values on OPEN are expanded in sequence of values C1  C2 
 . All the generated nodes having evaluation function f = C  are by definition
. . .  Ci . . .  Cm
1
on optimal paths (since h = h ), all those who have f = C2 must be on paths that can be second
best and so on. Notice that some solutions can have the same costs.
When h = h , m-A*s complexity is clearly linear in the number of nodes having evaluation
 . However, when the cost function has only a small range of values, there may be
function f   Cm
 . To avoid this exponential frontier we
an exponential number of solution paths having the cost Cm
chose the tie-breaking rule of expanding deeper nodes first, yielding a number of node expansions
bounded by m  n, when n bounds the solution length. Clearly:

T HEOREM 7. When m-A* has access
favour of deeper
P to h = h , then, using a tie-breaking rule in
th
nodes, it expands at most #N = i #Ni nodes, where #Ni is the length of the i optimal solution
path. Clearly, #N  m  n.

906

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Figure 6: The graph G0 represents a new problem instance constructed by appending a branch leading to a new goal node t to node n.

3.2.3 M -A* WITH C ONSISTENT H EURISTIC
When m-A* uses a consistent heuristic, it has several useful properties.
Optimal efficiency under consistent heuristic. Algorithm A* is known to be optimally efficient
for consistent heuristic (Dechter & Pearl, 1985). Namely, any other algorithm that extends search
paths from the root and uses the same heuristic information as A* will expand every node that is
surely expanded by A*, i.e., it will expand every n, such that f (n) < C  . We extend the notion of
nodes surely expanded by A* to the m-best case:
 -bounded
P ROPOSITION 6. Algorithm m-A* will expand any node n reachable by a strictly Cm
path from the root, regardless of the tie-breaking rule. The set of such nodes is referred to as surely
expanded by m-A*.
 -bounded path  = {s, n , n , . . . n}. The start node s is
Proof. Let us consider the strictly Cm
1 2
clearly expanded at the beginning of the search and its children, including node n1 , are placed
 , node n must be expanded by m-A* before finding the mth best
on OPEN. Since f (n1 ) < Cm
1
solution (Proposition 4), its children, including n2 , in turn are placed in OPEN. The same is true for
all nodes of , including n.

T HEOREM 8 (m-optimal efficiency). Any search algorithm, that is guaranteed to find the m-best
solutions and that explores the same search space as m-A* and has the same consistent heuristic,
will have to expand any node that is surely expanded by m-A*. Namely it will expand every node
 , i.e. f (n0 ) < C  , n0  .
that lies on any path  that is dominated by Cm
m
The proof idea is similar to the work by Dechter and Pearl (1985). Namely we can show that
any algorithm that does not expand a node n, surely expanded by m-A*, can miss one of the m-best
solutions, when applied to a slightly modified problem:
Proof. Let us consider a problem having the search graph G and a consistent heuristic h. Assume
that node n is surely expanded by m-A* before finding the j th best solution. Let B be an algorithm
that uses the same heuristic h and is guaranteed to find the m best solutions. Let also assume that
node n is not expanded by B. A consistency of the heuristic also allows us to better characterize the
nodes expanded by m-A*.
We can create a new problem graph G0 (see Figure 6) by adding a new goal node t with
h(t) = 0, connecting it to n by an edge having cost c = h(n) + , where  = 0.5(Cj  D)
907

fiF LEROVA , M ARINESCU , & D ECHTER

and D = maxn0 Sj f (n0 ), where Sj is the set of nodes surely expanded by m-A* before finding the
j th solution. It is possible to show that the heuristic h is admissible for the graph G0 (Dechter &
Pearl, 1985). Since  = 0.5(Cj  D), C  = D  2. By construction, the evaluation function of
the new goal node is:

f (t) = g(t) + h(t) = g(n) + c = g(n) + h(n) +  = f (n) +   D +  = Cj   < Cj

(2)

which means that t is reachable from s by a path whose cost is strictly bounded by Cj . That
guarantees that m-A* will expand t (Proposition 6), discovering a solution with cost Cj  . On the
other hand, algorithm B, that does not expand node n in the original problem, will still not expand it
and thus will not reach node t and will only discover the solution with cost Cj , not returning the true
set of m best solutions to the modified problem. From the contradiction the theorem follows.
P ROPOSITION 7. If the heuristic function employed by m-A* is consistent, the values of the evaluation function f of the sequence of expanded nodes are non-decreasing.
The proof is a straightforward extension of a result by Nilsson (1980).
Proof. Let node n2 be expanded immediately after n1 . If n2 was already in OPEN at the time when
n1 was expanded, then from the node selection rule it follows that f (n1 )  f (n2 ). If n2 was not
in OPEN, then it must have been added to it as a result of expansion of n1 , i.e., be a child of n1 .
In this case the cost of getting to n2 from the start node is g(n2 ) = g(n1 ) + c(n1 , n2 ) and the
evaluation function of node n2 is f (n2 ) = g(n2 ) + h(n2 ) = g(n1 ) + c(n1 , n2 ) + h(n2 ). Since h(n)
is consistent, h(n1 )  c(n1 , n2 )+h(n2 ) and f (n2 )  g(n1 )+h(n1 ). Namely, f (n2 )  f (n1 ).
If the heuristic function is consistent, we have a stronger condition of Proposition 4:
T HEOREM 9. Algorithm m-A* using a consistent heuristic function:
;
1. expands all nodes n such that f (n) < Cm
;
2. never expands any nodes with evaluation function f (n) > Cm

3. expands some nodes such that f (n) = Cm , subject to a tie-breaking rule.
 and node n is never expanded by
Proof. 1. Assume that there exists a node n such that f (n) < Cm
m-A*. Such a situation can only arise if node n has never been in the OPEN list, otherwise it would
have been expanded, according to Proposition 4. That implies that the parent of node n in the search
space (let us denote it by node p) has never been expanded. However, similarly how it is done in the
 . Thus
proof of Proposition 7, it is easy to show that f (p)  f (n) and, consequently f (p) < Cm
node p must also have never been in OPEN, otherwise it would be expanded. Clearly, this is true
for all the ancestors of n, up to the start node s. Since node s is clearly in OPEN at the beginning of
the search, the initial assumption is incorrect and the property follows.
2. and 3. Follow directly from Proposition 3.

Figure 7 provides a schematic summary of the search space explored by m-A* having a consistent heuristic.
908

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS


{n|f(n) < Cm
}

search space
explored by m-A

*

*

{n|f (n) > Cm
}


{n|f (n) = Cm
}

Figure 7: The nodes explored by the m-A* algorithm with a consistent heuristic.
Optimal efficiency for node expansions. Whenever a node n is selected for expansion for the
first time by m-A*, the algorithm has already found the shortest path to that node. We can extend
this property as follows:
T HEOREM 10. Given a consistent heuristic h, when m-A* selects a node n for expansion for the
ith time, then g(n) = gi (n), namely it has found the ith best path from start node s to n.
Proof. By induction. For i = 1 (basic step) the theorem holds (Nillson, 1980). Assume that it also
holds for (i  1)th expansion of node n. Let us consider the ith case, i > 1 (inductive step). We
have already expanded the node n (i  1) times and due to the inductive hypothesis we have already
found the (i  1) distinct best paths to the node n. Let us assume that the cost of the newly found
solution path is greater than the ith optimal one, i.e. gi (n) > gi (n). Then, there exists a different,
undiscovered path  from s to n with cost g (n) = gi (n) < gi (n). From Proposition 2 there exists
in OPEN a node n0  . Obviously, node n0 must be located between the start node s and node
n. Denoting by C (n0 , n) = c(n0 , n1 ) +    + c(nk , n), from the heuristic consistency it easily
follows that h(n0 ) < C (n0 , n) + h(n) and that the evaluation function of node n0 along path 
is f (n0 ) = g (n0 ) + h(n0 ) < g (n0 ) + C (n0 , n) + h(n). Seeing that the cost of path  from
s to n is g (n) = g (n0 ) + C , we conclude that f (n0 ) < f (n). However, that contradicts our
assumption that node n was expanded for the ith time before node n0 . The theorem follows.
3.2.4 T HE I MPACT OF THE R EQUIRED B EST S OLUTIONS m
The sequence of the sizes of search spaces explored by m-A* as a function of m is obviously monotonically increasing with m. Denoting by j-A* and i-A* the versions of the m-A* algorithm that
search respectively for j and i best solutions, we can make the following straightforward characterization:
P ROPOSITION 8. Given a search graph and consistent heuristic,
1. Any node expanded by i-A* is expanded by j-A* if i < j and if both use the same tie-breaking
rule.
909

fiF LEROVA , M ARINESCU , & D ECHTER

2. The set S(i, j) of nodes defined by S(i, j) = {n|Ci < f (n) < Cj } will surely be expanded
by j-A* and surely not expanded by i-A*.
3. If Cj = Ci , the difference in the number of nodes expanded by i-A* and j-A* is determined
by the tie-breaking rule.
The proof follows trivially from Theorem 9. As a result, larger discrepancy between the respective costs Cj  Ci yields larger difference in the search spaces explored by j-A* and i-A*.
This difference, however, also depends on the granularity with which the values of a sequence of
observed evaluation functions increase, which is related to the arc costs (or weights) of the search
graph. If Ci = Cj = C, then the search space explored by i-A* and j-A* will differ only in the
frontier of nodes satisfying f (n) = C. Figure 5 represents schematically the search spaces explored
by the i-A* algorithm.

4. Depth-First Branch and Bound for Finding the M Best Solutions
Along with its valuable properties, m-A* inherits also the disadvantages of A*: its exponential space
complexity, which makes the algorithm infeasible for many applications. An alternative approach
is searching using depth-first branch and bound (DFBnB), which can be implemented in linear
space if necessary and is therefore often more practical. DFBnB finds the optimal solution by
exploring the search space in a depth-first manner. The algorithm maintains a cost U of the best
solution encountered so far and prunes search nodes whose lower-bounding evaluation function
f (n) = g(n) + h(n) is larger than U . Extending DFBnB to the m-best task is straightforward, as
we describe next.
4.1 The m-BB Algorithm
Algorithm m-BB, the depth-first branch and bound extension to the m-best task, that explores a
search tree is presented in Algorithm 6. As usual, the algorithm maintains lists of OPEN and
CLOSED nodes. It also maintains a sorted list of CANDIDATE nodes that contains the best m
solutions found so far. Nodes on OPEN are organized in a last in - first out manner in order to
facilitate a depth-first exploration of the search space (i.e., OPEN is a stack). At each step, m-BB
expands the next node n in OPEN (line 5). If it is a goal node, a new complete solution is found
(line 6) and it is stored in the CANDIDATE list (line 7-9), which is then re-sorted (line 10). Only
up to m best solutions are maintained (lines 11-13).
The main modification to the depth-first branch and bound, when extended to the m-best task,
is in its pruning condition. Let U1  U2  . . .  Um denote the costs of the m best solutions
encountered thus far. Then Um is the upper bound used for pruning. Before m solutions are discovered, no pruning takes place. Algorithm m-BB expands the current node n, generates its children
(lines 15-17) and computes their evaluation function (line 18-19). It prunes a subproblem below n
iff f (n)  Um (lines 20-23). It is easy to see that when the algorithm terminates, it outputs the
m-best solutions to the problem.
T HEOREM 11. Algorithm m-BB is sound and complete for the m-best solutions task.
Proof. Algorithm m-BB explores the search space systematically. The only solutions that are
 , where C  is the m
skipped are the ones satisfying f (n)  Um (see step 22). Since Um  Cm
m
910

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 6: m-BB exploring a graph, assuming a consistent heuristic

1

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

24

Input: An implicit directed search graph G = (N, E) with a start node n0 and a set of goal nodes Goals. A
heuristic function h(n). Parameter m (the number of desired solutions).
Output: the m best solutions
Initialize: OPEN=, CLOSED=, a tree T r = , sorted list CANDIDATE = , UpperBound = , i = 1 (i
counts the current solution being searched for);
Put the start node n0 in OPEN, g(n0 ) = 0, f (n0 ) = h(n0 );
Assign n0 to be the root of T r;
while OPEN is not empty do
Remove the top node from OPEN, denoted n, and put it on CLOSED;
if n is a goal node then
soli  solution obtained by tracing back pointers from n to n0 (pointers assigned at step 17);
Ci  cost of soli ;
Place solution soli on CANDIDATE;
Sort CANDIDATE in increasing order of solution costs;
if size of CANDIDATE list  m then
Um  cost of the mth element in CANDIDATE;
Keep first m elements of CANDIDATE, discard the rest;
else
Expand node n, generating its children succ(n);
forall the n0  succ(n) do
Attach a pointer from n0 back to n in T r;
g(n0 ) = g(n) + c(n, n0 );
f (n0 ) = g(n0 ) + h(n0 );
if f (n0 ) < Um then
Place n0 in OPEN;
else
Discard n0 ;
return the solutions on CANDIDATE list

 and therefore that path cannot lead to a newly discovered
best solution cost, it implies f (n)  Cm
m-best cost.

N
T HEOREM 12. Given a graphical model M = hX, D, F, i, the worst case time complexity of
m-BB that explores an OR search tree of M is O(k n + log m), where n is the number of variables,
k is the domain size and m is the number of required solutions. Space complexity is O(n).
Proof. In worst case m-BB would explore the entire OR search tree of size O(k n ). The maintaining
of CANDIDATE list introduces additional time overhead of O(log m). Since the OR search tree
yields no caching, m-BB uses space linear in the number of variables.
4.2 Characterization of the Search Space Explored by m-BB
We have already shown that m-A* is superior to any exact search algorithm for finding the mbest solutions when the heuristic is consistent (Theorem 8). In particular, m-BB must expand all
 }. From
the nodes that are surely expanded by m-A*, namely the set of nodes {n|f (n) < Cm
Theorem 8 and the pruning condition it is clear that:
911

fiF LEROVA , M ARINESCU , & D ECHTER

P ROPOSITION 9. Given a consistent heuristic m-BB must expand any node in the set {n|f (n) <
 }. Also, there are instances for which m-BB will expands nodes satisfying f (n) > C  .
Cm
m
Several sources of overhead of m-BB are discussed next.
4.2.1 M -BB VS . BB
Pruning in m-BB does not occur until the upper bound on the current mth best solution is assigned
a valid value, i.e., until m solutions are found. In the absence of determinism, when all solutions are
consistent, the time it takes to find m arbitrary solutions in depth-first manner is O(m  n), where
n is the length of solution (for graphical models n coincides with the number of variables). If the
problem contains determinism it may be difficult to find even a single solution. This means that for
m-BB the search may be exhaustive for quite some time.
4.2.2 T HE I MPACT OF S OLUTION O RDER
The difference in the number of nodes expanded by BB and m-BB depends greatly on the variance
between the solution costs. If all the solutions have the same cost, then U1 = Um . However, such
a situation is unlikely and therefore the conditions for m-BBs node expansions are impacted by
1 , . . . , U j } be the non-increasing sequence of
the order in which solutions are discovered. Let {Um
m
th
the upper bounds on the m best solution, up to a point when m-BB uncovered the j th solution.
j
Initially Um
= , for j  {1, . . . , m  1}.
P ROPOSITION 10. Between the discovery of the (j  1)th and the j th solutions the set of nodes
j1
  U j  U j1  .
expanded by m-BB are included in Sj = {n | f (n)  Um
}, where Cm
m
m
Proof. Between discovering the (j  1)th and j th solutions m-BB expands only nodes satisfying
j1
j1
{n | f (n)  Um
}, hence j : Cj  Um
. Once the j th solution is found, it either replaces the
j
th
previous bound on m solution Um = Cj or some k th upper bound, k  {1, . . . , m  1}, yielding
j1
j
  U j  U j1 .
. Either way, Cm
= Um1
Um
m
m
4.2.3 O RDERING OVERHEAD
The need to keep a list of m sorted solutions (the CANDIDATE list) implies O(log m) overhead
for each new solution discovered. The total number of solutions encountered before termination is
hard to characterize.
4.2.4 C ACHING OVERHEAD
The overhead related to caching arises only when m-BB explores a search graph and uses caching.
This version of the algorithm (not explicitly presented) stores the m best partial solutions to any
fully explored subproblems (and a subset of m when only a partial set is discovered) and re-uses
these results whenever the subproblem is encountered again. In order to implement caching, mBB requires to store a list of length m for each node that is cached. Moreover, the cached partial
solutions need to be sorted, which yields an O(m log m) time overhead per cached node.
912

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

5. Adapting m-A* and m-BB for Graphical Models
Our main task is to find the m best solutions to optimization tasks over graphical models. Therefore,
we adapt the m-best search algorithms m-A* and m-BB to explore the AND/OR search space over
graphical models, yielding algorithms m-AOBF and m-AOBB, respectively. We will also describe
a hybrid algorithm BE+m-BF, combining Bucket Elimination and m-A*.
5.1 m-AOBF: Best-First AND/OR Search for M Best Solutions in Graphical Models
The extension of algorithm AOBF (Section 2.3.2) to the m-best task seems fairly straightforward,
in principle. m-AOBF is AOBF that continues searching after discovering the first solution, until
the required number of m best solutions is obtained. The actual implementation requires several
modifications as we discuss next.
It is not easy to extend AOBFs bottom-up node values updates and corresponding arc marking
mechanism to the m-best task. Therefore, in order to keep track of the current best partial solution
tree while searching for the ith best solution we adopt a naive approach that maintains explicitly a
list OPEN containing entire partial solution trees (not just nodes), sorted in ascending order of their
heuristic evaluation costs. Algorithm 7 presents the pseudo-code of our simple scheme that explores
the AND/OR search tree and generates solutions one by one in order of their costs. At each step,
the algorithm removes the next partial solution tree T 0 from OPEN (line 4). If T 0 is a complete
solution, it is added to the list of solutions along with its cost (lines 5-8), otherwise the algorithm
expands a tip node n of T 0 , generating its successors (line 10-17). Each such newly generated node
n0 is added to T 0 separately, yielding a new partial solution tree T 00 (lines 19-23), whose cost is
recursively evaluated using Algorithm 3, as in AOBB (line 28). These new partial trees are then
placed in OPEN (line 29). Search stops when all m solutions have been found.
We note that the maintenance of the OPEN list containing explicit partial solution subtrees is a
source of significant additional overhead which will become apparent in the empirical evaluation in
Section 7. Thus, the question whether the performance of m-AOBF can be improved further is open
and is therefore a rich topic of future work.
All m-A* properties (Section 3.2) can be extended to m-AOBF. In particular, algorithm mAOBF with an admissible heuristic is sound and complete, terminating with the m best solutions
generated in order of their costs. m-AOBF is also optimal in terms of the number of nodes expanded
compared with any other algorithm that explores the same AND/OR search space with the same
consistent heuristic function.
T HEOREM 13 (m-AOBF complexity). The complexity of algorithm m-AOBF traversing either the
h1
AND/OR search tree or the context minimal AND/OR search graph is time and space O(k deg ),
where h is the depth of the underlying pseudo tree, k is the maximum domain size, and deg bounds
the degree of the nodes in the pseudo tree. If the pseudo tree is balanced (each internal node has
exactly deg child nodes), then the time and space complexity is O(k n ), where n is the number of
variables.
The real complexity bound of m-AOBF comes from the cost function. It appears however that
maintaining an OPEN list in a brute force manner does not lend itself easily to an effective way
of enumerating all partial solution subtrees and therefore the search space of all partial solution
subtrees is actually exponential in n. The detailed proof of Theorem 13 is given in the Appendix.
913

fiF LEROVA , M ARINESCU , & D ECHTER

Algorithm 7: m-AOBF exploring an AND/OR search tree

1
2
3
4
5
6
7
8
9

10
11
12
13
14
15
16
17
18

19
20
21
22
23
24
25
26
27
28

29
30

Input: A graphical model M = hX, D, Fi, pseudo tree T rooted at X1 , heuristic function h(), parameter m;
Output: The m best solutions to M
Create root OR node s labelled by X1 , let G = {s} (explored search space) and T = {s} (partial solution tree);
Initialize S  ; OP EN  {T }; i = 1; (i counts the current solution being searched for);
while i  m and OP EN 6=  do
Select the top partial solution tree T 0 and remove it from OPEN;
if T 0 is a complete solution then
S  S  {hf (T 0 ), T 0 i};
i  i + 1;
continue;
Select a non-terminal tip node n in T 0 ;
// Expand node n
if n is OR node labeled Xi then
forall the xi  D(Xi ) do
Create AND child n0 labeled hXi , xi i;
succ(n)  succ(n)  {n0 };
else if n is AND node labeled hXi , xi i then
forall the successor Xj of Xi in T do
Create an OR child n0 labeled Xj ;
succ(n)  succ(n)  {n0 };
G  G  {succ(n)};
// Generate new partial solution trees
L  ;
forall the n0  succ(n) do Initialize v(n0 ) = h(n0 );
if n is OR node then
forall the n0  succ(n) do
Create a new partial solution tree T 00  T 0  {n0 };
L  L  {T 00 };
else if n is AND node then
Create a new partial solution tree T 00  T 0  {succ(n)};
forall the T 00  L do
Recursively evaluate and assign to f (T 00 ) the cost of the partial solution tree T 00 , based on heuristic
function h(); // see Algorithm 3
Place T 00 in OPEN, keeping it sorted in the ascending order of costs f (T 00 );
return The m best solutions found S;

5.2 m-AOBB: AND/OR Branch and Bound for M Best Solutions in Graphical Models
Algorithm m-AOBB extends the AND/OR Branch and Bound search (AOBB, Section 2.3.3) to the
m-best task. The main difference between AOBB and m-AOBB is in the value function computed
for each node. m-AOBB tracks the costs of the m best partial solutions of each solved subproblem. Thus it extends the node value v(n) and solution tree T  (n) rooted by n in AOBB to ordered
sets of length m, denoted by v(n) and T  (n), respectively, where v(n) = {v1 (n), . . . , vm (n)} is
an ordered set of the costs of the m best solutions to the subproblem rooted by n, and T  (n) =
 (n)} is a set of corresponding solution trees. This extension arises due to the
{T1 (n), . . . , Tm
depth-first manner of search space exploration of m-AOBB in conjunction with the AND/OR decomposition. Therefore, due to the AND/OR decomposition m-AOBB needs to completely solve
914

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 8: m-AOBB exploring an AND/OR search tree

1
2

3
4
5
6
7
8
9
10
11
12
13
14

Input: A graphical model M = hX, D, Fi, pseudo tree T rooted at X1 , heuristic function h(), parameter m;
Output: The m best solutions to M
// INITIALIZE
Create root OR node s labeled by X1 and let the stack of created but not expanded nodes OP EN = {s};
Initialize v(s) =  (a set of bounds on m best solutions under s) and a set of best partial solution trees rooted in s
T  (s) = ; U B = , sorted list CAN DIDAT E = ;
while OP EN 6=  do
Select top node n in OPEN;
// EXPAND
if n is OR node labeled Xi then
foreach xi  D(Xi ) do
Add AND child n0 labeled hXi , xi i to list succ(n) containing the successors of n;
Initialize v(n0 ) = 0, a set of best partial solution trees rooted in n T  (n0 ) = ;
if n is AND node labeled hXi , xi i then
Let p be ancestor of n;
Recursively evaluate and assign to f (p) the cost of the partial solution tree rooted in p, based on the heuristic h(); //
see Algorithm 3
if vm (p) <  and f (p)  vm (p) then
Prune the subtree below the current tip node n;
else
foreach successor Xj of Xi  T do
Add OR child n0 labeled Xj to list succ(n) containing the successors of n;
Initialize v(n0 ) = , a set of best partial solution trees rooted in n T  (n0 ) = ;

15
16
17

18
19
20
21
22
23
24
25

26

27
28
29

30

31
32
33
34

Remove n from OPEN and add succ(n) on top of OPEN;
// PROPAGATE
while list of successors of node n is empty do
if n is the root node then
return a set of solutions rooted at n and their costs: T  (n), v(n) ;
else
Update ancestors of n, AND and OR nodes p, bottom up:
if p is AND node then
Combine the set of the partial solution trees to the subproblem rooted in p T  (p) and the set of partial
solution trees rooted in n T  (n) and their costs v(p) and v(n); // see Algorithm 9
Assign the resulting set of the costs and the set of the best partial solution trees respectively to v(p) and
T  (p);
else if p is OR node then
foreach solution cost vi (n) in the set v(n) do
Update the cost with the weight of the arc, creating a new set of costs v 0 (n):
vi0 (n) = c(p, n) + vi (n);
Merge the sets of partial solutions v(n) and v(p) and the sets of partial solution trees rooted in p and n:
T  (p) and T  (n), keeping m best elements; //Algorithm 10
Assign results of merging respectively to v(p) and T  (p);
Remove n from the list of successors of p;
Move one level up: n  p;
return v(s) and T  (s)

the subproblems rooted in all the children n0 of an AND node n, before even a single solution to a
subproblem above n is acquired (unlike the m-BB case). Consequently, during the bottom-up phase
sets of m costs have to be propagated and updated. m-AOBF on the other hand, only maintains a
set of partial solution trees.
915

fiF LEROVA , M ARINESCU , & D ECHTER

Algorithm 9: Combining the sets of costs and partial solution trees
1

2
3
4

5
6
7
8
9
10
11

function Combine(v(n), v(p), T  (n),T  (n))
Input: Input sorted sets of costs v(n), v(p), corresponding partial solution trees T  (n), T  (p), number of
required solutions m
Output: A set of costs m best combined solutions v 0 (p), corresponding partial solution trees T 0 (p)
// INITIALIZE
Sorted list OPEN, initially empty; //contains potential cost combinations
v 0 (p)  ; T 0 (p)  ;
k = 1; //number of partial solutions already assembled, up to m in total
// Search over possible combinations
OPEN v1 (n) + v1 (p);
while k < m and OP EN is not empty do
Remove the top node V on OPEN, where V = Svi (n) + vj (p);
vk0 (p)  V ;
0
T  (p)  Ti (n)  Tj (p);
if vi+1 (n) + vj (p) not in OP EN then
Put vi+1 (n) + vj (p) in OP EN ;

13

if vi (n) + vj+1 (p) not on OP EN then
Put vi (n) + vj+1 (p) in OP EN ;

14

k  k + 1;

15

return v 0 (p), T  (p);

12

Unlike m-AOBF that discovers solutions one by one in order of their costs, m-AOBB (pseudocode in Algorithm 8) reports the entire set of m solutions at once, at termination. m-AOBB interleaves forward node expansion (lines 5-18) with a backward propagation (or cost revision) step
(lines 19-33) that updates node values until search terminates. A node n will be pruned (lines 12-13)
if the current upper bound on the mth solution under n, vm (n), is lower than the nodes evaluation
functions f (n), which is computed recursively as in AOBB (Algorithm 3). During the bottom-up
propagation phase at each AND node the partial solutions to the subproblems rooted in the nodes
children are combined (line 24-26, Algorithm 9). At each parent OR node p v(p) and T  (p) are
updated to incorporate the new and possibly better partial solutions rooted in a child node n (lines
27-31, Algorithm 10).
5.2.1 C HARACTERIZING N ODE P ROCESSING OVERHEAD
In addition to the increase in the explored search space that m-BB experiences compared with BB
due to the reduced pruning (Section 4.2), AND/OR search introduces additional overhead for mAOBB. The propagation of a set of m costs and of m partial solution trees leads to an increase in
memory by a factor of m per node. Processing the partial solutions at both OR and AND nodes
introduces an additional overhead.
T HEOREM 14. Algorithm m-AOBB exploring the AND/OR search tree has a time overhead of
O(m  deg  log m) per AND node and O(m  k) per OR node, where deg bounds the degree of the
pseudo tree and k is the largest domain size. Assuming k < deg  log(m), the total worst case time
complexity is O(n  k h deg  m log(m)) and the space complexity is O(mn). The time complexity

of m-AOBB exploring the AND/OR search graph is O(n  k w deg  m log(m)), space complexity

O(mn  k w ).
916

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 10: Merging the sets of costs and partial solution trees
1

2
3
4
5

6
7
8
9
10
11
12
13
14
15
16

17

function Merge(v(n),v(p),T  (n),T  (p))
Input: Input sorted cost sets v(n) and v(p), sets of corresponding partial solution trees T  (n) and T  (p),
number of required solutions m
Output: v 0 (p), a merged set of m best solution costs, T 0 (p) a set of corresponding partial solution trees
// INITIALIZE
v 0 (p)  ;
T 0 (p)  ;
i, j  1; //indices in the cost sets
k  1; //index in the resulting array
// Merge two sorted sets
while k  m do
if vi (p)  vj (n) then
vk0 (p)  vi (p);
0
Tk (p)  Ti (p);
i  i + 1;
k  k + 1;
else
vk0 (p)  vj (n);
0
Tk (p)  Tj (n);
j  j + 1;
k  k + 1;
return v 0 (p) and T 0 (p);

Proof. Combining the sets of current m-best partial solutions (Algorithm 9) introduces an overheard
of O(m log(m)). The resulting time overhead per AND node is O(deg  m log(m)). Merging two
sorted sets of costs (Algorithm 10) can be done in O(m) steps. If an OR node has O(k) children,
the resulting overhead is O(m  k). Assuming k < deg  log(m), the complexity is dominated by
processing of the AND nodes. In the worst case, the tree version of m-AOBB, called m-AOBB-tree,
would explore the complete search space of size O(n  k h ), where h bounds the depth of the pseudo

tree, while the graph version, called m-AOBB-graph, would visit a space of of size O(n  k w ),
where w is the induced width of the pseudo tree. The space complexity of m-AOBB-tree follows
from the need to propagate the sets of O(m) partial solutions of length O(n). The time overhead for
m-AOBB is the same for AND/OR trees and AND/OR graphs. The space complexity of m-AOBBgraph is explained by the need to store m partial solutions for each cached node.

5.3 Algorithm BE+m-BF
It is known that exact heuristics for graphical models can be generated by the Bucket Elimination
(BE) algorithm described in Section 2.3.4. We can therefore first compile the exact heuristics along
an ordering using BE and then apply m-A* (or m-AOBF, both will work the same at this point),
using these exact heuristics. The resulting algorithm is called BE+m-BF. Worst-case analysis of
this algorithm will show that it yields the best worst-case complexity compared with any known
m-best algorithm for graphical models.
917

fiF LEROVA , M ARINESCU , & D ECHTER

T HEOREM 15. The time complexity of BE+m-BF is O(nk w+1 + nm) when n is the number of
variables, k is the largest domain size, w is the induced width of the problem and m is the desired
number of solutions. The space complexity is O(nk w + nm).
Proof. BEs time complexity is O(nk w+1 ) and space complexity of O(nk w ) (Dechter, 1999).
Since BE compiles an exact heuristic function, m-A* with this exact heuristic expands nodes for
which f (n) = Cj only while searching for ith solution. If the algorithm breaks ties in favour of
deeper nodes, it will only expand nodes on solution paths. Each path has length n, yielding the total
time and space complexity of this step of the algorithm equal to O(n  m).

6. Related Work
We can distinguish several primary approaches employed by earlier m-best exact algorithms, some
mentioned already in the Introduction. Note that some of the original works do not include space
complexity analysis and the bounds provided are often our own.
The first and most influential approach was introduced by Lawler (1972). It aimed to use of-theshelf optimization schemes for best solutions. Lawler showed how to extend any given optimization
algorithm to the m-best task. At each step, the algorithm seeks the best solution to a re-formulation
of the original problem that excludes the solutions already discovered. The scheme has been improved over the years and is still one of the primary strategies for finding the m-best solutions. The
time and space complexity of Lawlers scheme are O(nmT (n)) and O(S(n)) respectively, where
T (n) and S(n) are the time and space complexity of finding a single best solution. For example,
if we use AOBF as the underlying optimization algorithm, the use of Lawlers method yields time


complexity of O(n2 mk w log n ) and space complexity of O(nk w log n ).
Hamacher and Queyranne (1985) built upon Lawlers work but used as building blocks algorithms that find both the first and second best solutions. Once two best solutions are generated, a
new problem is formulated so that the second best solution is the best solution to the new problem.
Then, the second best solution for the new problem becomes the overall third best solution and the
procedure is repeated. The algorithm has time complexity of O(m  T2 (n)) and space complexity of
O(S2 (n)), where T2 (n) and S2 (n) are respectively the time and space for finding the second best
solution. The complexity of this method is always bounded from above by that of Lawler, seeing
that Lawlers scheme can be used as an algorithm for finding the second best task. Using m-AOBF

to find the two best solutions, we obtain time complexity of O(2mnk w log n ) and space complexity

O(2nk w log n ).
Nilsson (1998) applied Lawlers method using a join-tree algorithm. On top of that his algorithm reuses computations from previous iterations. His scheme, called max-flow algorithm, uses
message-passing on a junction tree to calculate the initial max-marginal functions for each cluster
(e.g. probability values of the most probable assignments task) yielding the best solution. Note that
this step is equivalent to running the bucket-elimination algorithm. Subsequent solutions are recovered by conditioning search which consult the generated function. The time complexity analysis by
Nilsson (1998) is O(2p|C| + 2mp|R| + pm log (pm)), where p is the number of cliques in the joint
tree, |C| is the size of the largest clique and |R| is the size of the largest residual (i.e. the number
of variables in a cluster but not in neighbouring clusters). The space complexity can be bounded by
O(p|C| + p(|S|)), where |S| is the size of a separator between the clusters. If applied to a buckettree, Nilssons scheme has time and space complexity of O(2nk w+1 + mn(2k + log(mn)) and


O(nk w +1 + nk w ) respectively, since the the bucket tree has p = n cliques, whose size is bounded
918

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS



by |C| = k w +1 and the residual in each cluster is |R| = k (the domain of a single variable).
Thus the algorithm has better time complexity than all other schemes mentioned so far, except for
BE+m-BF.
Two works by de Campos et al. build upon Nilssons approach, extending it to solving the mbest MAP task using genetic algorithms (de Campos, Gamez, & Moral, 1999) and probability trees
(de Campos, Gamez, & Moral, 2004), respectively. These schemes are approximate and the authors
provide no theoretical analysis of their complexity.
Fairly recently, Yanover and Weiss (2004) developed an iterative scheme based on belief propagation, called BMMF. At each iteration BMMF uses Loopy Belief Propagation to solve two new
problems obtained by restricting the values of certain variables. When applied to a junction tree

having induced width w (whose largest cluster size is bounded by k w +1 ) it is an exact algorithm
having time complexity O(2mnk w+1 ) and space complexity O(nk w + mnk). When applied on
a loopy graph, BMMF is not guaranteed to find exact solutions.
Another approach based on Lawlers idea uses optimization via the LP-relaxation (Wainwright
& Jordan, 2003), formulated by Fromer and Globerson (2009). Their method, called Spanning TRee
Inequalities and Partitioning for Enumerating Solutions, or STRIPES, also partitions the search
space, while systematically excluding all previously determined assignments. At each step new
constraints are added to an LP optimization problem, which is solved via an off-the-shelf LP-solver.
In general, the algorithm is approximate. However, on trees or junction-trees it is exact if the
underlying LP solver reports solutions within the time limit. PESTEELARS is an extension of the
above scheme by Batra (2012) that solves the LP relaxation using message-passing approach that,
unlike conventional LP solvers, exploits the structure of the problems graph. The complexity of
these LP-based algorithm is hard to characterize using the usual graph parameters.
Another approach extends variable elimination (or dynamic programming) schemes to directly
obtain the m best solutions. In our recent paper (Flerova et al., 2011) we extended bucket elimination and mini-bucket elimination to the m-best solutions task, yielding an exact scheme called
elim-m-opt and its approximate version called mbe-m-opt, respectively. This work also embeds
the m-best optimization task within the semi-ring framework. The time and space complexities of


algorithm elim-m-opt are bounded by O(m log mnk w +1 ) and O(mnk w ), respectively.
Two related dynamic programming based ideas are by Seroussi and Golmard (1994) and Elliot
(2007). Seroussi and Golmard extract the m solutions directly, by propagating the m best partial
solutions along a junction tree. Given a junction tree with p cliques, largest cluster size |C|, separator
size bounded by |S| and branching degree deg, the time complexity of the algorithm is O(m2  p 
|C|  deg) and the space complexity is O(m  p  |S|). Adapted to a bucket tree, this algorithm has


time complexity O(m2 nk w +1 deg) and space complexity of O(mnk w ). Elliot propagates the m
best partial solutions along a representation called Valued And-Or Acyclic Graph, also known as
a smooth deterministic decomposable negation normal form (sd-DNNF) (Darwiche, 2001). The

time complexity of Elliots algorithm is O(nk w +1 m log (m  deg)) and the space complexity is

O(mnk w +1 ).
Several methods focus on search schemes obtaining multiple optimal solution for the k shortest
paths task (KSP). For a survey see the paper by Eppstein (1994). The majority of these algorithms
assume that the entire search graph is available in memory and thus are not directly applicable. A
recent exception is by Aljazzar and Leue (2011), whose K  algorithm finds the k shortest paths
during search on-the-fly and thus can be potentially useful for graphical models. The algorithm
interleaves A* search on the problems implicit graph G and Dijkstras algorithm (1959) on a spe919

fiF LEROVA , M ARINESCU , & D ECHTER

BE+m-BF


O(nk w

m-AOBF


O(nk w

log n

+1

+ mn)

O(2nk w

)



Nilsson 1998

+1

+ mn(2 log(mn) + 2k))

elim-m-opt


Gosh et al. 2012

Elliot
2007


Yanover and Weiss 2004

m-AOBB


Hamacher and Queyranne

O(mnk w

O(mnk w

+1

+1



O(mnk w )

log m)

log (m  deg))

O(mnk w

O(mnk w deg log m)



O(2mnk w

+1

log n

Aljazzar and Leue 2011


O(nk w w log (nk) + m)

)

)

Seroussi and Golmard 1994
O(m2 nk w+1 deg)

Lawler 1972

O(n2 mk w )

m-A*-tree
O(k n )

m-BB-tree

O(k n + log m)

Figure 8: Time complexity comparison of the exact m-best algorithms specified for a bucket tree.
A parent node in the graph has a better complexity than its children. Problem parameters:
n - number of variables, k - largest domain size, w - induced width, deg - the degree of
the join (bucket) tree. Our algorithms are highlighted.

cific path graph structure denoted P (G). P (G) is a directed graph, the vertices of which correspond
to edges in the problem graph G. Given a consistent heuristic, K  , when applied to an AND/OR

search graph is time and space O(nk w w log(n  k) + m).
More recently, Gosh, et al., (2012) introduced a best-first search algorithm for generating ordered solutions for explicit AND/OR trees or graphs. The time complexity of their algorithm can
be bounded by O(mnk w ), when applied to a context-minimal AND/OR search graph. The space
complexity is bounded by O(s  nk w+1 ), where s is the number of candidate solutions generated
and stored by the algorithm, hard to quantify using usual graph parameters. However, this approach,
which explores the space of complete solutions, does not seem to be practical for graphical models
because it requires the entire AND/OR search space to be fully explicated in memory before attempting to generate even the second best solution. In contrast, our algorithms generate the m best
solutions while traversing the space of partial solutions.
920

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Figure 8 provides a visual comparison between the worst-case time complexity bounds of the
discussed schemes in a form of a directed graph, where each node corresponds to an algorithm
and a parent in the graph has a better complexity than its child. We assume in our analysis that
n >> m > k > 2.
We see that the best emerging scheme, as far as worst-case performance goes is BE+m-BF.
However, since it requires compiling of the exact heuristics, it is often infeasible. We see also that
algorithm elim-m-opt appears to have relatively good time complexity superior, for example, to mAOBB search. However, as we showed in our previous work (Flerova et al., 2011), it is quite limited
empirically. Note that the worst-case analysis often fails to capture the practical performance, either because the algorithms that have good worst-case performance require too much memory, or
because it ignores the power of the cost function in bounding the performance.

7. Experimental Evaluation
Our experiments consist of two parts: evaluation of the m-best search algorithms on the benchmarks
from recent UAI and Pascal2 competitions and comparison of our schemes with some of the previously developed algorithms on randomly generated networks, whose parameters and structure had
to be restricted due to the limitations of the available implementations of the competing schemes.
We defer the discussion of the second part of experiments until Section 7.5, concentrating now on
the evaluation our m-best search schemes only.
7.1 Overview and Methodology
We used 6 benchmarks, all, except for binary grids, came from real-world domains:
1. Pedigrees
2. Binary grids
3. WCSP
4. Promedas
5. Proteins
6. Segmentation
The pedigrees benchmark (pedigree*) was used in the UAI 2008 competition.3 They arise
from the domain of genetic linkage analysis and are associated with the task of haplotyping. A
haplotype is a sequence of alleles at different loci inherited by an individual from one parent, and
the two haplotypes (maternal and paternal) of an individual constitute this individuals genotype.
When genotypes are measured by standard procedures, the result is a list of unordered pairs of
alleles, one pair for each locus. The maximum likelihood haplotype problem consists of finding
a joint haplotype configuration for all members of the pedigree which maximizes the probability
of data. It can be shown that, given the pedigree data, the haplotyping problem is equivalent to
computing the most probable explanation of a Bayesian network that represents the pedigree (see
the paper by Fishelson and Geiger (2002) for more details).
3. http://graphmod.ics.uci.edu/group/Repository

921

fiF LEROVA , M ARINESCU , & D ECHTER

Benchmark
Pedigrees
Grids
WCSP
Promedas
Proteins
Segmentation

# inst
13
32
61
86
72
47

n
581-1006
144-2500
25-1057
197-2113
15-242
222-234

k
3-7
2
2-100
2
18-81
2-21

w
16-39
15-90
5-287
5-120
5-16
15-18

hT
52-104
48-283
11-337
34-187
7-44
47-67

Table 1: Benchmark parameters: # inst - number of instances, n - number of variables, k - domain
size, w - induced width, hT - pseudo tree height.

In each of the binary grid networks (50-*, 75-* and 90-*)4 the nodes corresponding
to binary variables are arranged in an N by N square and the functions are defined over pairs of
variables and are generated uniformly randomly.
The WCSP (*.wcsp) benchmark includes random binary WCSPs, scheduling problems from
the SPOT5 benchmark, and radio link frequency assignment problems, providing a large variety of
problem parameters.
Protein side-chain prediction (pdb*) networks correspond to side-chain conformation prediction tasks in the protein folding problem (Yanover, Schueler-Furman, & Weiss, 2008). The
resulting instances have relatively few nodes, but very large variable domains, generally rendering
most instances very complex.
Promedas (or chain *) and segmentation (* s.binary) are probabilistic networks that
come from the set of problems used in the 2011 Probabilistic Inference Challenge.5 Promedas instances are based on a Bayesian network model developed for expert systems for medical diagnosis
(Wemmenhove, Mooij, Wiegerinck, Leisink, Kappen, & Neijt, 2007). Segmentation is a common
benchmark used in computer vision, modeling the task of image segmentation as an MPE problem,
namely assigning a label to every pixel in an image, such that pixels with the same label share
certain characteristics.
Table 1 describes the benchmark parameters: # inst - number of instances, n - number of variables, k - maximum domain size, w - induced width of the ordering used, hT - pseudo-tree height.
The induced width is not only one of the crucial parameters indicating the difficulty of the problem,
but the difference between the induced width and the mini-bucket i-bound signifies the strength of
the heuristic. When the i-bound is considerably smaller than the induced width, the heuristic is
weak, while the i-bound equal or greater than the induced width yields an exact heuristic, which in
turn yields much faster search. Clearly, a large number of variables, a high domain size or a large
pseudo tree height suggest harder problems.
7.1.1 A LGORITHMS
We can distinguish 6 algorithms: BE+m-BF, m-A*-tree and m-BB-tree exploring a regular OR
search tree and their modifications that explore an AND/OR search tree, denoted m-AOBF-tree and
m-AOBB-tree. We also consider a variant of m-AOBF that explores the AND/OR search graph
m-AOBF-graph. We did not implement the m-AOBB over AND/OR search graph, because the
4. http://graphmod.ics.uci.edu/repos/mpe/grids/
5. http://www.cs.huji.ac.il/project/PASCAL/archive/mpe.tgz

922

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

overhead due to book-keeping looked prohibitive. We used m-AOBF as representative for AND/OR
graph search, and as we will see it proved indeed to be not cost-effective. All algorithms were
guided by pre-compiled mini-bucket heuristics, described in Section 2.3.4. We used 10 i-bounds,
ranging from 2 to 22. However, for some hard problems computing the mini-bucket heuristic with
the larger i-bounds proved infeasible, so the actual range of i-bounds varies among the benchmarks
and among instances within a benchmark. All algorithms were restricted to a static variable ordering
computed using a min-fill heuristic (Kjrulff, 1990). Both AND/OR schemes used the same pseudo
tree. In our implementation algorithms m-BB, m-BF and m-AOBF break ties lexicographically,
while algorithm m-AOBB solves the independent subproblems rooted at an AND node in increasing
order of their lower bound heuristic estimates.
The algorithms were implemented in C++ (32-bit) and the experiments were run on a 2.6GHz
quad-core processor. The memory limit was set for 4 GB per problem, the time limit to 3 hours. We
report the CPU time (in seconds) and the number of nodes expanded during search. For uniformity
we consider the task throughout to be the maximization-product problem, also known as Most
Probable Explanation task (MPE or MAP). We focus on complete and exact solutions only and thus
do not report the results if the algorithm found less than m solutions (for best-first schemes) or if
the optimality of the solutions was not proved (for branch and bound schemes).
7.1.2 G OALS OF THE E MPIRICAL E VALUATION
We will address the following aspects:
1. Comparing best-first and depth-first branch and bound approaches
2. The impact of AND/OR decomposition on the search performance
3. Scalability of the algorithms with the number of required solutions m
4. Comparison with earlier proposed algorithms
7.2 The Main Trends in the Behavior of the Algorithms
Tables 2, 4, 6, 8, 10, and 12 present for each of our algorithms the raw results in the form of runtime
in seconds and number of expanded nodes for select instances from each benchmark, selected to
best illustrate the prevailing trends. For each benchmark we show the results for two values of the
i-bound, corresponding, in most cases, to relatively weak and strong heuristics. Note that the ibound has no impact on the BE+m-BF, since it always calculates the exact heuristic. We show three
values of number of solutions m, equal to 1 (ordinary optimization problem), 10 and 100.
In order to see the bigger picture, in Figures 9-14 we show bar charts representing for each
benchmark a median runtime and a number of instances solved by each algorithm for a particular
strength of the heuristic (i-bound) for m  {1, 2, 5, 10, 100}. The y-axis is on logarithmic scale.
The numbers above the bars indicate the actual values of median time in seconds and number of
solved instances, respectively. It is important to note that in these figures we only account for harder
instances, for which the i-bound did not yield exact heuristic. We acknowledge that the median
times are not strictly comparable since they are calculated over a varied number of instances solved
by each algorithm. However, this metric is robust to outliers and gives us an intuition about the
algorithms relative success. In addition, Tables 3, 5, 7, 9, 11 and 13 show for each benchmark the
number of instances, for which a given algorithm is the best in terms of runtime and in terms of
number of expanded nodes. If several algorithms show the same best result, it counts towards the
score of all of them.
923

fiF LEROVA , M ARINESCU , & D ECHTER

instance
(n,k,w ,h)

i-bound

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
Timeout
0.05
6647
OOM
OOM
OOM
1269.0
348648825
22.99
2320223
0.05
6647

number of solutions
m=10
nodes
OOM
OOM
OOM
Timeout
Timeout
0.06
6671
OOM
OOM
OOM
1275.65
348648869
164.72
12110559
0.06
6671

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
OOT
1461.76
46419482
OOM
OOM
OOM
OOM
151.49
33563300
107.03
4274313
OOM

OOM
OOM
OOM
OOT
2389.32
74629839
OOM
OOM
OOM
OOM
152.27
33609110
185.66
7245553
OOM

OOM
OOM
OOM
OOT
3321.47
83802828
OOM
OOM
OOM
OOM
148.08
36255491
251.98
8319419
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
5760.25
14260410
OOM
OOM
OOM
793.56
2579416
Timeout
484.69
1530768
OOM

OOM
OOM
OOM
Timeout
OOT
OOM
OOM
OOM
OOM
Timeout
551.67
1995114
OOM

OOM
OOM
OOM
Timeout
OOT
OOM
OOM
OOM
OOM
Timeout
858.29
3507104
OOM

10.22

10.29

algorithm

m=1
time

503.wcsp

4

(144, 4, 9, 44)

8

myciel5g 3.wcsp

4

(47,2, 19, 46)

8

satellite01ac.wcsp

4

(79, 8, 19, 56)

8

29.wcsp

4

(83, 4, 18, 58)

8

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

10.23
11.59
12.96
1.81
0.0
0.05
0.09
0.02
0.17
0.02
0.0

nodes

464134
OOM
938812
2243619
87717
111
2347
1401
2098
37629
1577
111

time

11.58
12.89
2.63
0.0
0.05
0.09
0.02
0.17
0.33
0.0

464182
OOM
938869
2245137
147851
168
2395
1447
2155
38463
24239
168

m=100
time

nodes
OOM
OOM
OOM
Timeout
Timeout
0.06
6984
OOM
OOM
OOM
1255.46
348651775
8010.42
568148386
0.06
6984

11.57
12.77
115.3
0.01
0.08
0.13
0.02
0.25
79.38
0.01

464698
OOM
939508
2279587
9189667
739
2899
1482
2724
55125
6731546
739

Table 2: WCSP: CPU time (in seconds) and number of nodes expanded. A Timeout stands for
exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold we
highlight the best time and number of nodes for each m. Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

We next provide some elaboration and interpretation of the results.
7.2.1 WCSP
Table 2 shows the results for two values of the i-bound for select instances chosen to best illustrate
the common trends seen across the WCSP benchmark. Figure 9 presents the median time and
number of solved instances for each algorithm for i=16. Table 3 shows for the same i-bound (i=16)
the number of instances for which each of the schemes had the best runtime and best number of
expanded nodes. For many problem instances of this benchmark the mini-bucket elimination with
the large i-bounds is infeasible, thus we present the results for a small and a medium i-bounds.
924

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

BE+m-BF
m-AOBF tree

m-AOBF graph
m-A* tree

1.94
0.01
0.04
0.08
0.05

1.71

10

100

6
3
3
2

3
3
2

3
3
2

3
3
2

2

Solved instances

WCSPs, i=16

3
3

4

5

6

6

m-BB tree
m-AOBB tree

5
5

5
5

5

6

m-AOBF graph
m-A* tree

6

BE+m-BF
m-AOBF tree
6

0.0
0.01
0.03
0.05

1.7
0.0
0.01
0.02
0.05

5
m

2

5
5

1

10-1

2.8

2.79

2.71
1.71
0.0
0.01
0.02
0.04

1.71
0.1

0.0
0.01
0.02
0.05

Median time
WCSPs, i=16

100

m-BB tree
m-AOBB tree
6.33

101

100

1

2

5

m

10

100

Figure 9: Median time and number of solved instances (out of 49) for select values of m for WCSPs,
i-bound=16. Numbers above bars - actual values of time (sec) and # instances. Total
instances in benchmark: 61, discarded instances due to exact heuristic: 12.

BE+m-BF. As suggested by theory, whenever BE+m-BF does not run out of memory, it is the
most efficient scheme. See for example Table 2, 503.wcsp and 29.wcsp. However, calculation of
the exact heuristic is only feasible for easier instances and, as Figure 9 shows, it can only solve
925

fiF LEROVA , M ARINESCU , & D ECHTER

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

WCSPs: # inst=61, n=14-1058
k=2-100, w =6-287, hT =8-585, i-bound=16
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
43
43
43
43
43
1/2
1/1
0/1
1/1
0/0
1/2
0/2
0/2
0/2
0/3
5/1
4/3
5/3
4/3
5/2
1/0
2/0
1/0
1/0
0/0
1/2
2/0
0/0
0/0
0/0
2/1
2/1
2/1
2/1
2/1

Table 3: Number of instances, for which each algorithm has the best runtime (#BT) and best number
of expanded nodes (#BN), WCSPs. Out of 61 instances 12 have exact heuristics. The table
accounts for remaining 49, i-bound= 16 .

2 WCSP instances. As seen in Table 3, on these two instances BE+m-BF demonstrated the best
runtime among all the schemes.
m-AOBB-tree. For a number of problems for small values of m, m-AOBB-tree is superior to mBB-tree both in terms of the runtime and in number of expanded nodes. For example, for 29.wcsp,
i=4, m=10 m-AOBB-tree requires 2.63 seconds to solve the problem and expands 147851 nodes
while the runtime of m-BB-tree is 12.89 seconds and it expands 2245137 nodes. However, on the
majority of instances m-AOBB-tree is slower than all other schemes, as seen in Figure 9. Moreover,
m-AOBB-tree scales poorly with the number of solutions. For m = 100 it very often has both the
worst runtime and the largest explored search space among all the schemes, e.g. i=8, 503.wcsp.
Such striking decrease in performance as m grows is consistent across various benchmarks and
can be explained by the need to combine sets of partial solutions at AND nodes as we described
earlier. The overhead connected to AND/OR decomposition also accounts for the larger time per
node ratio of m-AOBB-tree, compared to other schemes. For example, in Table 2 for instance
myciel5g 3.wcsp, i=8, for m=10 and m=100 m-AOBB-tree expands less nodes than m-BB-tree, but
its runtime is larger. Nevertheless, m-AOBB-tree has its benefits. Since it is more space efficient
than the other algorithms, it is often the only scheme able for find solutions for the harder instances,
especially when the heuristic is weak, as we see for myciel5g 3.wcsp for i=4 and satellite01ac.wcsp
for both i=4 and i=8, respectively.
m-BB-tree. In Figure 9 we see that m-BB-tree solves almost the same number of problems as
m-AOBB-tree while having considerably better median time.
m-AOBF-tree and m-AOBF-graph. Unsurprisingly, best-first search algorithms often run out of
space on problems feasible for branch and bound, such as 503.wcsp and myciel5g 3.wcsp for i=8.
m-AOBF-based schemes are overall inferior to other algorithms, solving, as Figure 9 shows, the
least number of problems. Both schemes run out of memory much more often than m-A*-tree. We
believe this is due to overhead of maintaining an OPEN list of partial solution trees, as opposed of
an OPEN list of individual nodes as m-A*-tree does. Whenever the m-AOBF schemes do manage
to find solutions, as for example for instance 29.wcsp, i=8, m-AOBF-graph explores the smallest
926

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

search space among the schemes, except for BE+m-BF. At the same time m-AOBF-tree sometimes
expands more nodes compared not only to m-AOBF-graph, but also to m-A*-tree, which is not what
we would normally expect, since m-A*-tree traverses an OR search space, which is inherently larger
than the AND/OR one. However, it is important to remember that though better space efficiency
of AND/OR schemes is often observed, it is not guaranteed. Many factors, such as tie breaking
between the nodes with the same value of evaluation function, can impact the performance of mAOBF-tree. m-AOBF-tree and m-AOBF-graph have almost the same median time and number of
solved problems, as seen in Figure 9.
m-A*-tree. Out of the three best first algorithms m-A*-tree is overall the best. In Figure 9 we see
that it solves more instances than all other schemes for all values of m and its median runtime is
close to that of BE+m-BF. Table 3 proves that for i-bound=16 this scheme is the fastest among all
the schemes on largest number of instances, showing best runtime on 4-5 instances, depending on
m. This is explained in part by the relatively reduced overhead for maintaining the search space and
OPEN list in memory, compared for example with the m-AOBF schemes.
7.2.2 P EDIGREES
Table 4 displays the results for select instances from the Pedigree benchmark for two i-bounds each.
Overall, the difference between the results for the algorithms greatly diminishes as the heuristic
strength increases. Figure 10 shows the median time and number of solved instances for select
values of m for i=16. The number of instances for which each of the schemes had the best runtime
and best number of expanded nodes for the same i-bound is presented in Table 5.
BE+m-BF. Here too BE+m-BF is often superior to other algorithms, especially when the other
schemes use lower values of the i-bound, e.g. pedigree23, i=12, all ms. For large i-bounds and thus
more accurate heuristics the difference is much smaller. Moreover, sometimes BE+m-BF can be
slower than other schemes, due to the time required to calculate the exact heuristic, e.g. pedigree23,
i=16. Table 5 shows that BE+m-BF is overall the fastest. We see that on the Pedigree benchmark
this algorithm is quite successful, as is evident from the many instances it solved (see Figure 10).
m-AOBB-tree. For low values of m m-AOBB-tree is slightly superior to all other algorithms,
solving the most number of instances, (see Figure 10). On the other hand, its median time is the
largest. It fails to solve any instances for m=100. From Table 4 we see that m-AOBB-tree is the
slowest, (e.g., pedigree23, i=16, all ms). Yet, for instance pedigree33, i=12, m=1, this scheme is
the only one to find any solution.
m-BB-tree. As expected, m-BB-tree is inferior to the best-first search schemes unless the latter
run out of memory. As was the case for WCSP, this scheme is often faster than m-AOBB-tree, for
example, on pedigree30, i=16, all values of m. The bar charts show that m-BB-tree has the second
worst median time for all values of m, but solves the most number of problems for m=100.
m-AOBF schemes. Both m-AOBF algorithms are unsuccessful on the Pedigree benchmark. They
often run out of memory even for m = 1 (e.g. pedigree33, i=22). For most instances where they
do report solution m-AOBF-tree is faster than m-AOBF-graph, though the difference is usually not
very large.
m-A*-tree. As we saw for WCSPs, on some pedigree instances m-A*-tree is faster than the two
m-AOBF schemes, as seen in Figure 10, all values of m. Moreover, it is superior on harder instances
927

fiF LEROVA , M ARINESCU , & D ECHTER

instance
(n,k,w ,h)

i-bound

pedigree33

12

algorithm

m=1
time

(798, 4, 24, 132)
22

pedigree30

12

(1290, 5, 20, 105)

16

pedigree23

12

(403, 5, 21, 64)

16

pedigree20

12

(438, 5, 20, 65)
16

nodes

number of solutions
m=10
time
nodes
OOM
OOM
OOM
Timeout
Timeout
OOM
OOM
OOM
1.55
77138
4.15
177397
Timeout
OOM

m=100
nodes
OOM
OOM
OOM
Timeout
Timeout
OOM
OOM
OOM
3.76
112422
21.48
655141
Timeout
OOM
time

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
7814.77
145203641
OOM
OOM
OOM
1.32
73625
2.98
145717
2.88
70644
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
2510.59
33453995
7.90
6423
OOM
OOM
65.43
4866388
84.28
12243789
594.36
6907399
7.90
6423

OOM
OOM
OOM
Timeout
OOT
7.99
7611
OOM
OOM
65.86
4867551
85.72
12298570
Timeout
7.99
7611

OOM
OOM
OOM
Timeout
OOT
9.22
23028
OOM
OOM
67.01
4882985
127.25
13027245
Timeout
9.22
23028

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
8.44
713664
23.0
4446224
32.11
831096
7.11
630
OOM
OOM
0.52
53862
4.5
837288
13.39
346145
7.11
630

OOM
OOM
715729
4676953
75355901
2482
OOM
OOM
55927
959931
50252107
2482

OOM
OOM
11.15
904802
35.46
6179124
Timeout
7.68
19297
OOM
OOM
1.17
85300
14.14
1641751
OOM
7.68
19297

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

27.0
34.23
88.85
24.95

24.94
27.32
54.46
24.95

OOM
OOM
2321986
7239379
4365855
491
OOM
OOM
2279192
5857301
2531322
491

8.46
24.56
1077.9
7.24

0.58
5.61
713.73
7.24

26.66
37.63
1019.76
24.99

24.93
29.14
970.61
24.99

OOM
OOM
2324701
7434961
63940515
3482
OOM
OOM
2281907
5946423
59333828
3482

OOM
OOM
2353927
9155747
Timeout
26.16
32643
OOM
OOM
25.97
2311133
40.2
6617200
Timeout
26.16
32643
27.47
66.81

Table 4: Pedigrees: CPU time (in seconds) and number of nodes expanded. A Timeout stands
for exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold
we highlight the best time and number of nodes for each m.Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

infeasible for both m-AOBF schemes and BE+m-BF, e.g. pedigree23, i=16. As shown in Figure 10,
it solves 5 instances for i=16, for all ms, which is the best or second best results, depending on the
number of solutions. However, the median time of m-A*-tree is considerably larger than that of
BE+m-BF, while for this i-bound the latter solves only a single instance less.
7.2.3 B INARY G RIDS
Table 6 shows the results for select instances from the grid networks domain. Figure 11 shows the
median runtime and number of solved instances for i=18, while Table 7 presents the number of
instances, for which an algorithm is the best, for the same i-bound. Most trends in the algorithms
928

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

842.17

BE+m-BF
m-AOBF tree

6.66

5.62

5.65

28.13

29.14

40.2

98.29

m-BB tree
m-AOBB tree
5
5
4

5
5
2

4

5
5
5

6
4

5
5

6

0.0

0.0
0.0

100

m-AOBF graph
m-A* tree

5
5
4

10

m

BE+m-BF
m-AOBF tree

10

100

0.0

5

m

0.0
0.0

2

0.0
0.0

1

0.0
0.0

10-1

0.0
0.0

100

0.0
0.0

Solved instances
Pedigrees, i=16

0.32
0.0
0.0

5

2

4

1

0.27
0.0
0.0

0.23
0.0
0.0

100

0.23
0.0
0.0

1.39

5.6

101

5.58

27.32

102

27.41

62.91

Median time
Pedigrees, i=16

280.27

103

Figure 10: Median time and number of solved instances (out of 12) for select values of m for Pedigrees, i-bound=16. Numbers above bars - actual values of time (sec) and # instances.
Total instances in benchmark: 13 , discarded instances due to exact heuristic: 1.

behavior observed on WCSP and Pedigree benchmarks can be also noticed on the Grid benchmark
as well. In particular, m-AOBB-tree is very successful when m is small, even solving the most
instances, as seen in Figure 11. But it shows worse results for m=100 and for any number of
solutions has the largest median time. m-BB-tree has smaller median time for all ms, but is still
929

fiF LEROVA , M ARINESCU , & D ECHTER

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Pedigrees: # inst=13, n=335-1290
k=3-7, w =15-47, hT =52-204, i-bound=16
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
6
6
6
6
6
0/0
0/0
0/0
0/0
0/0
0/0
0/0
0/0
0/0
0/0
1/1
1/1
1/1
1/1
1/1
0/0
0/0
1/1
1/1
1/1
1/1
1/1
0/0
0/0
0/0
4/4
4/4
4/4
4/4
4/4

Table 5: Number of instances, for which each algorithm has the best runtime (#BT) and best number
of expanded nodes (#BN), Pedigrees. Out of 13 instances 1 have exact heuristics. The table
accounts for remaining 12, i-bound= 16 .

considerably slower than any of the best-first schemes. m-A*-tree presents the best compromise
between a small medium running time and a relatively large number of solved instances. Table 7
shows that for majority of grid instances it is the fastest algorithm. The two m-AOBF schemes have
results quite similar to each other, solving almost the same number of instances for all ms with little
difference in median runtimes, as is shown in Figure 11. They both are consistently inferior to all
other schemes except for BE+m-BF, which often runs out of memory. The main difference of the
Grid benchmark compared with the previously discussed domains lies in the behaviour of BE+mBF when the i-bound is high. Even though it expands less nodes, for many problems BE+m-BF is
slower than the other schemes due to the large time required to compute the exact heuristic. For
example, on grid 75-19-5, i=18, for m=10 the runtime of BE+m-BF is 143.11 seconds, while even
m-AOBB-tree, known to be slow, terminates in just 94.0 seconds. At the same time, for this instance
BE+m-BF explores the smallest search space for all values of m.
7.2.4 P ROMEDAS
Table 8 shows the results for the Promedas benchmark. Figure 12 presents the median time and number of solved instances for the benchmark for i=16. Table 9 shows for the same i-bound the number
of instances for which each of the schemes had the best runtime and best number of expanded nodes.
A significant fraction of the instances is not solved by any of the algorithms, especially for low and
medium i-bounds. Unlike the other benchmarks, m-AOBB-tree not only solves the most instances
for small ms, but also is quite successful for m=100, solving only one instance less than the best
scheme for this value of m, m-BB-tree. Moreover, sometimes m-AOBB-tree is the only scheme to
report any solutions, especially for weak heuristic, e.g. or chain 50.fg and or chain 212.fg, i=12.
BE+m-BF runs out of memory on most instances, as seen in Table 8. Overall, the variance of the
algorithms performance is more significant for Promedas than for the previously discussed benchmarks. For example, as we see in Figure 12, for i=16 m-A*-tree, m-BB-tree and m-AOBB-tree solve
between 25 and 33 instances for m  [1, 10], while BE+m-BF and both m-AOBF-based schemes
solve only between 4 and 8 instances. Table 9 demonstrates that m-A*-tree most often is the fastest
of the algorithms.
930

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

instance
(n,k,w ,h)

50-15-5

i-bound

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
8.83
866865
8.75
1967152
3.29
251502
OOM
OOM
OOM
OOM
OOM
Timeout
347.24
17332742
OOM
OOM

number of solutions
m=10
time
nodes
OOM
OOM
11.97
1177549
11.91
2647393
34.28
2485393
OOM
OOM
OOM
OOM
OOM
Timeout
692.59
28676212
OOM
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
3290939
289
OOM
OOM
51205
355700
85289
289

OOM
OOM
OOM
Timeout
368.18
16431707
18.47
1220
OOM
OOM
0.39
55783
1.83
421798
116.77
7505310
18.47
1220

OOM
OOM
OOM
Timeout
Timeout
18.67
9534
OOM
OOM
0.89
104621
4.92
892065
Timeout
18.67
9534

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
347.24
17332742
OOM
OOM
OOM
OOM
1.3
153399
74.83
14968683
46.53
2450725
OOM
OOM

OOM
OOM
OOM
Timeout
692.59
28676212
OOM
OOM
OOM
OOM
1.88
211547
76.93
15403354
118.26
4940247
OOM
OOM

OOM
OOM
OOM
Timeout
2277.92
75442102
OOM
OOM
OOM
OOM
3.53
362344
85.42
16631321
563.22
18306275
OOM
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
3591.1
119431966
143.11
361
OOM
OOM
14.3
1609506
16.27
4005082
39.66
1367955
143.11
361

OOM
OOM
OOM
Timeout
Timeout
143.11
2330
OOM
OOM
18.76
2029844
22.28
5320573
94.0
3480629
143.11
2330

OOM
OOM
OOM
Timeout
Timeout
144.11
16897
OOM
OOM
28.04
2995437
37.26
8191215
Timeout
144.11
16897

algorithm

m=1
time

10

(400, 2, 27, 99)
18

50-17-5
10

(289, 2, 22, 84)
18

90-20-5

10

(400, 2, 27, 99)

18

75-19-5

10

(361, 2, 25, 89)
18

82.95
18.45

0.35
1.39
1.79
18.45

nodes

m=100
time

nodes
OOM
OOM
20.22
1931039
22.06
4708311
Timeout
OOM
OOM
OOM
OOM
OOM
Timeout
2277.92
75442102
OOM
OOM

Table 6: Grids: CPU time (in seconds) and number of nodes expanded. A Timeout stands for
exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold we
highlight the best time and number of nodes for each m. Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

7.2.5 P ROTEIN
Table 10 shows select Protein instances for i=4 and i=8, respectively. Figure 13 and Table 11 show
the summary of the results for i=4. This benchmark is fairly difficult due to very large domain size
(up to 81). The heuristic calculation is not feasible for higher i-bounds. In particular, BE+m-BF has
considerable problems in calculating the exact heuristic. Even for low i-bounds only relatively easy
instances are solved. Note that for instances pdb1ctk and pdb1dlw i-bound=8 yields exact heuristic.
Both m-AOBF-tree and m-AOBF-graph fail to find any solutions within the memory limit on the
majority of instances, e.g., pdb1b2v and pdb1cxy, i=4. There is not much difference between the
runtimes of all algorithms, with an exception of m-AOBB-tree. For example, for pdb1b2v, i=8,
931

fiF LEROVA , M ARINESCU , & D ECHTER

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree
440.64

BE+m-BF
m-AOBF tree

37.26
0.16

10

m

100

m-BB tree
m-AOBB tree
13
15

13
15
18

13
15

13
15

19

m-AOBF graph
m-A* tree

19

BE+m-BF
m-AOBF tree

0.39

1.04
1.23
0.02

0.38

0.9
1.3
0.01

5

2

2.14
3.23
1.0

22.28

20.16
45.35

17.49
28.04
0.36

1

13
15
18

10-1

0.0

0.35

0.01

100

0.83
0.78

101

1.43
0.88

16.27
20.68

Median time
Grids, i=18

102

91.79

103

6
5
3
3

4

5
5

5
5
4

5

6
4

5

5

Solved instances

Grids, i=18

6

101

100

1

2

5

m

10

100

Figure 11: Median time and number of solved instances (out of 31) for select values of m for
Grids, i-bound=18. Numbers above bars - actual values of time (sec) and # instances.
Total instances in benchmark: 32, discarded instances due to exact heuristic: 1.

m-AOBB-tree requires 6.46 seconds to find m=10 solutions, while the runtimes of other algorithms
range from 0.03 to 0.09 seconds (except for BE+m-BF which runs our of memory). However,
the slow performance of m-AOBB-tree on easier problems that are feasible for all algorithms is
compensated by the fact that for many instances it is the only scheme to report any solution, solving
932

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Grids: # inst=32, n=144-2500
k=2-2, w =15-74, hT =48-312, i-bound=18
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
12
12
13
13
14
0/0
0/0
0/0
0/0
0/0
0/0
0/1
0/2
0/1
0/3
8/2
8/2
8/6
9/6
7/6
1/0
1/0
0/0
1/0
2/2
7 / 10
7 / 10
5/5
5/5
2/2
5/7
5/6
6/5
6/6
6/4

Table 7: Number of instances, for which each algorithm has the best runtime (#BT) and best number
of expanded nodes (#BN), Grids. Out of 32 instances 1 have exact heuristics. The table
accounts for remaining 31, i-bound= 18 .

most instances by considerable amount for m  [1, 10] (Figure 13). Table 11 shows that m-AOBBtree is the best both in terms of time and space for the overwhelming majority of problems for all
values of m except for m = 100.
7.2.6 S EGMENTATION
Table 12 shows the results for select instances from the Segmentation benchmark for two i-bounds,
namely i=4 and i=12, while Figure 14 and Table 13 present the summary of the results for i=12.
Unlike WCSP, for this benchmark we chose to display relatively low i-bounds not because calculating heuristic with larger is is infeasible, but because the problems have low induced width and
we wished to avoid displaying results obtained with exact heuristics. The main peculiarity of this
benchmark is the striking success of BE+m-BF. Overall it solves as many instances as the usually
superior m-A*-tree and m-BB-tree, as is seen in Figure 14. Moreover, its runtime is superior to
the other schemes, as is true for all instances in Table 12 and is also illustrated by the results in the
Table 13. When the heuristic is very weak, m-AOBB-tree is fairly successful, for example, finding
solutions for all values of m for 12 4 s.binary, i=4, which is infeasible for any other scheme except
for BE+m-BF. However, as usual, m-AOBB-tree is the overall slowest of the schemes.
7.3 Best-first vs Depth-First Branch and Bound for the M Best Solutions
Let us again consider the data presented in Tables 2-13 and Figures 9-14 in order to summarize our
observations and contrast the performance of best-first and depth-first branch and bound schemes.
Among the best-first search schemes m-A*-tree is the most successful. It is often very effective,
when armed with a good heuristic, and requires less space than the other best-first schemes. As we
already noted, BE+m-BF shows good results on the Segmentation benchmark, where it is the best
algorithm in terms of the median runtime, while solving at least the same number of problems as
the other schemes. However, on the other benchmarks the calculation of the exact heuristic is often
infeasible.
933

fiF LEROVA , M ARINESCU , & D ECHTER

instance
(n,k,w ,h)

i-bound

16

(620, 2, 30, 64)

22

(676, 2, 30, 70)

or chain 212.fg

12

16

12

(773, 2, 33, 79)
22

or chain 50.fg

m=1
time

or chain 107.fg

or chain 141.fg

algorithm

12

661, 2, 36, 76)
22

nodes

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
7.89
919865
14.58
3139711
67.95
1398364
OOM
OOM
OOM
9.2
1093564
17.0
3861414
122.01
3214924
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
9553.91
1276222668
272.0
9878480
OOM
OOM
OOM
14.16
1261489
279.61
56821714
140.9
6490042
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

number of solutions
m=10
time
nodes
OOM
OOM
18.89
2108122
35.15
7051974
229.49
5134280
OOM
OOM
OOM
20.83
2465364
42.36
9205755
418.46
11123810
OOM

m=100
time

44.61
102.6
627.94

56.59
100.45
855.84

nodes
OOM
OOM
4641627
18494630
13594667
OOM
OOM
OOM
6356871
19217427
21388619
OOM

OOM
OOM
OOM
OOT
25481595
OOM
OOM
OOM
3379926
87947802
14103095
OOM

OOM
OOM
OOM
OOT
2091.26
64400241
OOM
OOM
OOM
OOM
885.72
160581726
909.48
33842266
OOM

OOM
OOM
OOM
Timeout
49808550
OOM
OOM
OOM
1118792
15922806
11336657
OOM

OOM
OOM
OOM
Timeout
4206.07
111853485
OOM
OOM
OOM
33.87
3669711
141.66
27615033
1239.88
24717964
OOM

OOM
OOM
OOM
Timeout
Timeout
OOM
OOM
OOM
78.37
8186757
342.51
58246101
5032.11
86444575
OOM

OOM
OOM
OOM
Timeout
1404.27
33495406
OOM
OOM
OOM
53.87
5673948
91.15
18515503
Timeout
OOM

OOM
OOM
OOM
Timeout
3748.85
93992107
OOM
OOM
OOM
OOM
176.14
34915510
Timeout
OOM

OOM
OOM
OOM
Timeout
10070.0
245628104
OOM
OOM
OOM
OOM
447.46
85945673
Timeout
OOM

1772.8

9.91
78.08
584.83

721.67

38.48
460.15
315.2

Table 8: Promedas: CPU time (in seconds) and number of nodes expanded. An Timeout stands
for exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold
we highlight the best time and number of nodes for each m. Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

The two m-AOBF-based schemes are overall inferior due to prohibitively large memory, solving
fewer instances than the other algorithms. We believe that a non-trivial extension of AOBF from
a single solution to the m-best task is not straightforward, because it is hard to represent multiple
partial solution trees in an efficient manner. In order to have an efficient m-AOBF implementation,
one needs to quickly identify which partial solution subtree to select and extend next, when searching for the (k + 1)th solution after finding the k th best solution. While AOBF (for 1 solution) uses
an arc-marking mechanism to efficiently represent the current best partial solution subtree during
search, this is not easy to extend for the case when searching for the m best solutions. Therefore,
as was shown in Section 5.1, our m-AOBF implements a naive mechanism where each of the par934

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

BE+m-BF
m-AOBF tree

105

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

10

m

100

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree
27
26
8

8

8

8

2

3

4
4

5

6

6

7

7
7

Solved instances

Promedas, i=16

101

8

17

26
27
30

28
27
32

28
27
33

BE+m-BF
m-AOBF tree

1.36
2.97
4.45
3.62

60.1
7.47
0.15
1.55
2.0

3.43
1.95
7.08
0.08

5

2

185.53

325.85

267.69
51.79

146.85
0.03

1

25
27
31

10-1

2.03
1.37

5.19
0.01
0.48
0.62

101

100

7.64

33.68

102

45.83

109.68

Median time
Promedas, i=16

103

1268.88

104

100

1

2

5

m

10

100

Figure 12: Median time and number of solved instances (out of 86) summary for select values of
m for Promedas, i-bound=16. Numbers above bars - actual values of time (sec) and #
instances. Total instances in benchmark: 75, discarded instances due to exact heuristic:
11.

tial solution trees is represented explicitly in memory. This simple representation, however, incurs
a considerable computational overhead when searching for the m best solutions, which is indeed
revealed by our experiments. A more efficient implementation of m-AOBF is left for future work.
935

fiF LEROVA , M ARINESCU , & D ECHTER

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Promedas: # inst=86, n=197-2113
k=2-2, w =5-120, hT =34-187, i-bound=16
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
42
42
45
44
46
0/0
0/0
0/0
0/0
0/0
0/1
0/2
0/2
0/3
0/2
22 / 17
21 / 17
18 / 17
18 / 15
9/9
1/0
1/0
1/0
2/0
10 / 5
4/8
4/8
3/5
4/8
2/7
8/7
8/6
8/6
8/5
8/6

Table 9: Number of instances, for which each algorithm has the best runtime (#BT) and best number
of expanded nodes (#BN), Promedas. Out of 86 instances 11 have exact heuristics. The
table accounts for remaining 75, i-bound= 16 .

Unsurprisingly, the branch and bound algorithms are more robust in terms of memory and also
dominate m-A*-tree and other best-first schemes on many benchmarks in terms of the number of
instances solved. However, they tend to have considerably larger median time and expand more
nodes. In particular, m-AOBB-tree does not scale well with the number of solutions and for large
values of m the runtime increases drastically. Unlike m-AOBF, whose inferior performance can be
attributed to specifics of implementation, the depth-first m-AOBB suffers from issues inherent to
solving the m-best problem in a depth-first manner. As Algorithm 10 describes, m-AOBB needs to
merge the m best partial solution at each internal node, which hurts the performance significantly,
but cannot be avoided, unless the algorithmic approach itself is fundamentally changed. We did not
see a way to overcome this limitation.
Overall, whenever the calculation of the exact heuristic is feasible, BE+m-BF should be the
algorithm of choice. Otherwise, m-A*-tree is superior for the relatively easy problems, while mAOBB-tree is the best scheme for hard memory intensive instances. This superiority of a best-first
approach, whenever memory is available, is expected, based, on the one hand, on intuition derived
from our knowledge of the task of finding a single solution, and on the other hand, on the theoretical
results in Section 3.2.
7.4 Scalability of the Algorithms with the Number of Required Solutions
Figures 15-17 present the plots showing the runtime in seconds and the number of expanded nodes
as a function of number of solutions m (on a log scale) for two instances from each benchmark.
Figure 15 displays results for WCSP and Pedigree benchmarks, Figure 16 - for Grids and Promedas,
Figure 17 - for Proteins and Segmentation. Lower values (on the y-axis) are preferable. Each row
contains two instances from each benchmarks for a specific value of the i-bound, the runtime plots
being shown above the ones containing the expanded nodes. The examples are chosen to best
illustrate the prevailing tendencies.
Note that the theoretical analysis suggests that the runtime of BE+m-BF, the best among the

algorithms, should scale with m since its worst case complexity is O(nk w + mn). The theoretical
complexity of the best-first search schemes m-AOBF-tree and m-A*-tree is linear in the number of
936

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

instance
(n,k,w ,h)

pdb1b2v

(133, 36, 13, 33)

pdb1cxy

(70, 81, 9, 19)

i-bound

m=1
time

4

8

4

8

pdb1ctj

4

(62, 81, 8, 21)

8

pdb1dlw

4

(84, 81, 8, 29)

algorithm

8

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

nodes
OOM
OOM

0.12
10.23
6.51
0.02
0.03
0.0
0.01
0.29

2508
948337
100584
OOM
95
95
139
2401
6563
OOM

number of solutions
m=10
nodes
OOM
OOM
0.17
3186
11.09
1034645
35.14
827365
OOM
0.06
294
0.09
108
0.03
597
0.07
8861
6.46
256588
OOM
time

OOM
OOM
0.38
0.4

3708
51020
OOM
OOM
OOM
0.01
121
0.03
5791
0.66
7029
OOM

10.43
844.08
5.64
0.01
0.0
0.02
0.01
0.01
0.22
0.01

46.17
47.27
187.38
0.01

0.14
1.06
18.53
0.01

OOM
OOM
35400
76609260
74833
62
49
45
62
1118
3098
62
OOM
OOM
579108
6380302
1451906
294
OOM
OOM
6900
162913
154850
294

m=100
time

nodes
OOM
OOM

0.34
14.4
4462.0
0.42
0.64
0.15
0.5

6249
1404370
230849005
OOM
1956
135
3051
67330
Timeout
OOM

OOM
OOM
0.48
0.6

OOM
OOM
4434
73849

OOM

10854
191203
OOM
OOM
OOM
0.1
2870
0.27
53702
44.28
1335157
OOM

OOM
OOM

OOM
OOM

OOM
OOM
OOM
0.04
0.07
2.04

13.23
1039.96
18.29
0.02
0.07
0.11
0.03
0.03
2.32
0.02

46.26
47.33
544.55
0.05

0.18
1.09
157.01
0.05

480
11429
34567

43538
94422614
306054
265
302
74
265
5385
54324
265
OOM
OOM
579405
6391107
12759004
635
OOM
OOM
7240
167037
8632114
635

0.94
1.45

19.74
1325.84
157.43
0.07
0.42
0.75
0.08
0.15
71.86
0.07

46.49
50.72
0.39

0.52
1.86
0.39

65340
120786833
5307198
1050
1825
95
1057
31066
3273324
1050
OOM
OOM
582375
6762911
OOT
4265
OOM
OOM
10855
280189
OOT
4265

Table 10: Protein: CPU time (in seconds) and number of nodes expanded. An Timeout stands
for exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold
we highlight the best time and number of nodes for each m. Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

solutions, while for m-BB-tree the overhead due to the m-best task is a factor of (m  log m) and
for m-AOBB-tree it is (m log m  deg), where deg is the degree of the pseudo tree. We observed
that compared to other schemes the runtime of BE+m-BF indeed rises quite slowly as the number
of solutions increases, even as m reaches 100. The runtime m-A*-tree also scales well with m. The
behaviour of m-BB-tree depends a lot on the benchmarks. On Pedigrees and Protein its runtime
changes very little on most instances as the number of solutions grows, but on the other benchmarks,
the runtime for m=100 tends to be significantly larger then for m=1. m-AOBF-tree and m-AOBFgraph often do not provide any solutions even for m=1 or, alternatively, run out of memory as
m slightly increases (m  [2, 10]). These algorithms are clearly not successful in practice. As
937

fiF LEROVA , M ARINESCU , & D ECHTER

BE+m-BF
m-AOBF tree

105

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

1457.61

45.28

41.91

190.28
39.45

177.3
37.46

102

220.64

103

50.72

597.38

Median time
Protein, i=4

104

5

2

10

m

100

10
10

13
13

13
13

13
13

13
13

Solved instances
Protein, i=4

20

25

35

42
35
25

35

42

m-BB tree
m-AOBB tree

25

35
25

25

35

44

m-AOBF graph
m-A* tree

44

BE+m-BF
m-AOBF tree

0.14
0.38
0.67

2.95

2.89

0.03
0.19
0.39

1

0.01
0.17
0.32

2.87
0.01
0.14
0.27

2.84
0.01
0.13
0.25

100

3.39

101

100

1

2

5

m

6

6

6

6

6

101

10

100

Figure 13: Median time and number of solved instances (out of 72) for select values of m for
Protein, i-bound=4. Numbers above bars - actual values of time (sec) and # instances.
Total instances in benchmark: 72, discarded instances due to exact heuristic: 0.

we discussed before, both the runtime and number of expanded nodes of m-AOBB-tree increase
drastically as m gets larger.
938

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

m=1
#BT / #BN
26
8/3
1 / 10
11 / 9
8/1
21 / 22
6/4

Protein: # inst=72, n=15-242
k=18-81, w =5-16, hT =7-44, i-bound=4
m=2
m=5
m=10
#BT / #BN #BT / #BN #BT / #BN
26
27
27
7/1
7/1
7/0
2 / 12
2 / 12
1 / 13
9/9
12 / 9
14 / 10
9/2
8/3
6/3
21 / 21
18 / 19
17 / 17
6/2
5/2
5/2

m=100
#BT / #BN
34
5/0
1 / 10
17 / 13
11 / 10
3/3
6/2

Table 11: Number of instances, for which each algorithm has the best runtime (#BT) and best
number of expanded nodes (#BN), Protein. Out of 72 instances 0 have exact heuristics.
The table accounts for remaining 72, i-bound= 4 .

7.5 Comparison with Competing Algorithms
We compare our methods with a number of previously developed schemes described in more details
in Section 6: STRIPES, PESTEELARS and Nilssons algorithm. The implementations of these
schemes were provided by Dhruv Batra. The first two approaches are based on ideas of LP relaxations and are approximate, but are known to often find exact solutions, though they provide
no guarantees of optimality. Nilssons algorithm is an exact message-passing scheme operating on
a junction tree. For the first set of experiments (on a tree benchmark) we also show results for
STILARS algorithm, an older version of the PESTEELARS algorithm. However, this scheme is
consistently inferior to the other two LP-based schemes and is not considered for the other two
benchmarks. In the following, we collectively refer to these 4 algorithms as competing schemes.
7.5.1 R ANDOMLY G ENERATED B ENCHMARKS
The available to us code of the LP-based and Nilssons approaches was developed to run on restricted inputs only, and so it could not be applied to the benchmarks used in the bulk of our evaluation described above. We concluded that re-implementing the competing codes to work on general
input would be too time consuming and would not provide any additional insights. Thus we chose
to compare our algorithms with the competitors using benchmarks that can be acceptable to the
competing schemes.
Specifically, the comparison was performed on the following three benchmarks: random trees,
random binary grids and random graphs with submodular potentials, that we call submodular
graphs in the remainder of the section. Table 14 shows the parameters of the benchmarks. The
instances were generated in the following manner. First, a vector of 12 logarithmically spaced integers between 10 and 103.5 was generated, serving as the number of variables for the instances.
For binary grids benchmarks each value was used to generate two problems with the same number
of variables. The edges between the variables were generated uniformly randomly, while making
sure that the end graph is a tree, grid or a loopy graph, depending on the benchmark. For each edge
we define a binary potential and for each vertex a unary potential in an exponential form: f = e ,
939

fiF LEROVA , M ARINESCU , & D ECHTER

instance
(n,k,w ,h)

i-bound

4

(225, 2, 16, 48)

12

(227, 2, 16, 57)

7 9 s.binary

(234, 2, 16, 53)

OOM
OOM
OOM
Timeout
164.91
5653312
0.0
225
7.31
103327
10.47
1843
0.03
3754
0.04
8251
0.08
4158
0.0
225

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
71.71
2733703
0.01
227
0.23
3338
0.33
799
0.01
585
0.05
10687
0.21
11076
0.01
227

OOM
OOM
OOM
Timeout
360.14
14906212
0.02
1365
3.75
46121
5.72
1827
0.09
9103
0.19
30119
14.28
1054628
0.02
1365

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
127.17
3337949
0.01
234
8.85
122663
OOM
0.02
1978
0.03
4415
0.05
2750
0.01
234

OOM
OOM
OOM
Timeout
505.08
17976200
0.03
1337
OOM
OOM
0.06
4170
0.11
13357
10.54
806490
0.03
1337

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
110.19
4227437
0.01
231
OOM
OOM
1.02
102671
2.07
428791
0.75
39170
0.01
231

OOM
OOM
OOM
Timeout
555.6
23302165
0.03
1615
OOM
OOM
1.17
115407
2.9
527967
11.99
809403
0.03
1615

m=1
time

12 4 s.binary

16 16 s.binary

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

number of solutions
m=10
nodes
OOM
OOM
OOM
Timeout
505.82
18888321
0.02
1619
10.36
143333
OOM
0.06
5692
0.21
24349
1.62
118074
0.02
1619

algorithm

4

12

4

12

11 4 s.binary

4

(231, 2, 16, 57)

12

nodes

time

m=100
time

nodes
OOM
OOM
OOM
Timeout
4371.05
189179726
0.21
11194
OOM
OOM
0.3
18616
1.32
131571
489.57
40961080
0.21
11194

0.19

0.38
1.2
0.19

0.21

0.28
0.95
0.21

OOM
OOM
OOM
Timeout
OOT
11157
OOM
OOM
30542
141591
OOT
11157
OOM
OOM
OOM
Timeout
OOT
10212
OOM
OOM
15807
89675
OOT
10212

OOM
OOM
OOM
Timeout
OOT
0.28
14241
OOM
OOM
1.86
167983
7.1
1010155
8497.93
617227854
0.28
14241

Table 12: Segmentation: CPU time (in seconds) and number of nodes expanded. An Timeout
stands for exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In
bold we highlight the best time and number of nodes for each m. Parameters: n - number
of variables, k - domain size, w - induced width, h-pseudo tree height.

where  is a real number sampled from a uniform distribution. For the third benchmark the potentials are further modified to be submodular. On the random trees the m-best optimization LP
problem is guaranteed to be tight, on the graphs with submodular potentials the LP optimization
problem is tight, but its m-best extension is not, and on the arbitrary loopy graphs, including grids,
the algorithms provide no guarantees.
7.5.2 C OMPETING A LGORITHMS  P ERFORMANCE
Table 15 shows the runtimes for select instances from the random tree benchmark for our 5 mbest search schemes and the competing LP schemes STILARS, PESTEELARS and STRIPES. We
940

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

BE+m-BF
m-AOBF tree

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

105

1949.3

103

5

100

13

14
14

18

19

20

24

24
24
24

m-BB tree
m-AOBB tree

24

24
24
24

24
20

24

24
24
24

m-AOBF graph
m-A* tree

20
20

21
20

Solved instances
Segmentation, i=12

24

24
24
24

BE+m-BF
m-AOBF tree

101

10

m

24
24

2

0.02
0.31
0.4
0.03
0.08

1

10-1

0.01
0.22
0.26
0.02
0.04
0.7

0.01
0.15
0.2
0.01
0.03
0.04

100

3.94

101

0.17
1.1
1.55
0.17
0.65

102

0.0
0.08
0.1
0.01
0.01
0.03

Median time
Segmentation, i=12

104

1

2

5

m

10

100

Figure 14: Median time and number of solved instances (out of 47) for select values of m for
Segmentation, i-bound=12. Numbers above bars - actual values of time (sec) and #
instances. Total instances in benchmark: 47, discarded due to exact heuristic: 0.

observed on this benchmarks that STILARS was always inferior to the other two schemes and
therefore it was excluded from the remainder of evaluation. Instead, in Tables 16 and 17, where
we show results for the random binary grids and submodular graphs benchmarks, we added for
comparison Nilssons max-flow algorithm. In the Table 15-17 the time limit was set to 1 hour,
memory limit to 3 GB. The schemes behavior is quite consistent across the instances.
941

fiF LEROVA , M ARINESCU , & D ECHTER

Time vs m. WCSPs: bwt3ac.wcsp

Time vs m. WCSPs: queen5_5_3.wcsp

(45, 11, 16, 27), i=4

(25, 3, 18, 21), i=16

7

15

6

Time, sec

Time, sec

5
10

5

4
3
2
1

0

0

1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

2.0

1e6

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

m-AOBF-tree
m-AOBF-graph

Nodes vs m. WCSPs: bwt3ac.wcsp
(45, 11, 16, 27), i=4
1.4

1e6

100.0

Number of solutions m
m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Nodes vs m. WCSPs: queen5_5_3.wcsp
(25, 3, 18, 21), i=16

1.2
1.5

1.0

Nodes

Nodes

0.8

1.0

0.6

0.5

0.4

0.0

0.0

0.2

1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Time vs m. Pedigrees: pedigree9

Time vs m. Pedigrees: pedigree30

(1290, 5, 20, 105), i=16

(1119, 7, 25, 123), i=22

1400

2000

1200

1500

Time, sec

1000

Time, sec

10.0

Number of solutions m

800

1000

600
400
200
0

500
0

1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Pedigrees: pedigree30
1e7

10.0

Number of solutions m
m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Nodes vs m. Pedigrees: pedigree9

(1290, 5, 20, 105), i=16

5

3

(1119, 7, 25, 123), i=22

Nodes

4

1.5

Nodes

2.0

1e7

1.0

2

0.5

1

0.0

0
1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Figure 15: CPU time in seconds and number of expanded nodes as a function of number of solutions. WCSP and Pedigrees, 4 GB, 3 hours.

942

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Time vs m. Grids: 50-19-5

Time vs m. Grids: 75-18-5

(361, 2, 25, 93), i=18

(324, 2, 24, 85), i=18

40

3500
3000
2500
2000
1500
1000
500
0

Time, sec

Time, sec

30
20
10
0
1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Grids: 50-19-5
6

7
6
5
4
3
2
1
0

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

(324, 2, 24, 85), i=18

1e5

5

Nodes

Nodes

4
3
2
1
0

1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

Time vs m. Promedas: or_chain_17.fg

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Time vs m. Promedas: or_chain_212.fg
(773, 2, 33, 79), i=22

(531, 2, 19, 51), i=16

5000

10

4000

8

Time, sec

Time, sec

100.0

Nodes vs m. Grids: 75-18-5

(361, 2, 25, 93), i=18

8 1e8

10.0

Number of solutions m

3000

6

2000

4

1000

2
0

0
1.0

10.0

m-AOBF-tree
m-AOBF-graph

1e5

100.0

Number of solutions m
m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Promedas: or_chain_17.fg
(531, 2, 19, 51), i=16

1e8

0.6

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Nodes vs m. Promedas: or_chain_212.fg
(773, 2, 33, 79), i=22

Nodes

0.8

3

Nodes

4

100.0

Number of solutions m

0.4

2
1

0.2

0

0.0
1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Figure 16: CPU time in seconds and a number of expanded nodes as a function of number of solutions. Grids and Promedas, 4 GB, 3 hours.

943

fiF LEROVA , M ARINESCU , & D ECHTER

Time vs m. Protein: pdb1at0

Time vs m. Protein: pdb1b2v

(122, 81, 8, 25), i=4

(133, 36, 13, 33), i=8

800
700
600
500
400
300
200
100
0

7
6

Time, sec

Time, sec

5
4
3
2
1
0
1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Protein: pdb1at0

m-AOBB-tree
BE+m-BF

2.5
2.0

0.6

Nodes

Nodes

1.5

0.4

1.0

0.2

0.5

0.0

0.0
1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Time vs m. Segmentation: 10_16_s.binary

Time vs m. Segmentation: 7_29_s.binary

(230, 2, 15, 52), i=4

(234, 2, 15, 63), i=12

10000

8

8000

6

Time, sec

Time, sec

m-A*-tree
m-BB-tree

(133, 36, 13, 33), i=8

1e5

0.8

6000

4

4000

2

2000

0

0

1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Time vs m. Segmentation: 10_16_s.binary

Time vs m. Segmentation: 7_29_s.binary

(230, 2, 15, 52), i=4

(234, 2, 15, 63), i=12

10000

8

8000

6

Time, sec

Time, sec

100.0

Nodes vs m. Protein: pdb1b2v

(122, 81, 8, 25), i=4

1e7

10.0

Number of solutions m

6000

4

4000
2000

2

0

0
1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Figure 17: CPU time in seconds and number of expanded nodes as a function of number of solutions. Protein and Segmentation, 4 GB, 3 hours.

944

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Segmentation: # inst=47, n=222-234
k=2-21, w =15-18, hT =47-67, i-bound=12
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
23
23
23
23
23
0/5
0/0
0/0
0/0
0/0
0/5
0/7
0 / 11
0 / 15
0 / 14
15 / 0
11 / 0
12 / 0
11 / 0
3/0
7/0
5/0
3/0
2/0
0/0
0/0
3/0
0/0
0/0
0/0
21 / 19
20 / 17
22 / 13
24 / 9
24 / 10

Table 13: Number of instances, for which each algorithm has the best runtime (#BT) and best number of expanded nodes (#BN), Segmentation. Out of 47 instances 0 have exact heuristics.
The table accounts for remaining 47, i-bound= 12 .

Benchmark
Random trees
Random Binary Grids
Random submodular graphs

# inst
12
24
12

n
10-5994
16-3192
16-3192

k
2-4
2
2

w
1
6-79
4-74

hT
5-132
9-221
9-208

Table 14: Benchmark parameters: # inst - number of instances, n - number of variables, k - domain
size, w - induced width, hT - pseudo tree height.

STILARS and Nilssons schemes are always dominated by the other two competing schemes
in terms of runtime. STRIPES and PESTEELARS are sometimes faster than all our schemes for
m=1, e.g. tree nnodes880 ps1 k4, however, on all three benchmark they scale rather poorly with
m. For m  5 they are almost always inferior to our algorithms, provided that the latter report
any results, with occasional exception of m-AOBB-tree, which also tends to be slow for large m.
The only problems on which PESTEELARS and STRIPES are superior to our search schemes
are the largest networks having over a 1000 variables, such as grid nnodes3192 ps2 k2, which
are infeasible for our algorithms. Overall, our five m-best algorithms proved superiority over the
considered competing schemes on the majority of instances, often having better runtime, especially
when m > 2, while guaranteeing solution optimality.

8. Conclusion
Most of the work on finding m best solutions over graphical models was focused on either iterative
schemes based on Lawlers idea or on dynamic programming (e.g., variable-elimination or treeclustering). We showed for the first time that for combinatorial optimization defined over graphical
models the traditional heuristic search paradigms are not only directly applicable, but often superior.
Specifically, we extended best-first and depth-first branch and bound search algorithms to solve
the m-best optimization task, presenting m-A* and m-BB, respectively. We showed that the properties of A* extend to the m-A* algorithm and, in particular, proved that m-A* is superior to any
945

fiF LEROVA , M ARINESCU , & D ECHTER

instance

tree nnodes245 ps1 k2

(245, 2, 2, 32)

tree nnodes880 ps1 k4

(880, 4, 2, 52)

tree nnodes5994 ps1 k4

(5994, 4, 2, 189)

algorithm

i-bound=4. k=2
m=5
m=10
time
time
0.05
0.09
0.08
0.13
0.01
0.02
0.02
0.03
0.06
14.14
7.93
33.3
0.4
0.88
0.51
1.32

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
STILARS
STRIPES
PESTEELARS

m=1
time
0.02
0.02
0.0
0.0
0.02
0.0
0.09
0.0

m=2
time
0.02
0.03
0.0
0.0
0.02
0.04
0.17
0.13

m=100
time
0.61
0.95
0.12
0.37
3045.25
1757.41
13.88
47.32

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
STILARS
STRIPES
PESTEELARS

0.3
0.48
0.08
0.1
1.17
0.0
5.67
0.0

0.46
0.76
0.17
0.3
1.37
0.11
11.26
0.87

1.06
1.8
0.24
0.54
52.36
28.19
28.09
6.13

1.9
3.28
0.48
1.23
927.12
81.21
56.41
9.26

OOM
OOM
3.67
14.38
Timeout
2440.22
607.01
79.0

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
STILARS
STRIPES
PESTEELARS

OOM
OOM
5.44
5.77
851.48
0.05
248.53
0.05

OOM
OOM
10.68
29.04
922.19
2.72
506.25
18.28

OOM
OOM
18.31
36.49
Timeout
64.48
1279.87
91.17

OOM
OOM
37.21
97.73
Timeout
250.36
2576.87
169.39

OOM
OOM
206.26
1112.2
Timeout
7325.4
Timeout
Timeout

Table 15: Random trees, i-bound=4. Timeout - out of time, OOM - out of memory. 3 GB, 1 hour.
other search scheme for the m-best task. We also analyzed the overhead of both algorithms caused
by the need to find multiple solutions. We introduced BE+m-BF, a hybrid of variable elimination
and best-first search scheme and showed that it has the best worst-case time complexity among all
m-best algorithms over graphical models known to us.
We evaluated our schemes empirically. We observed that the AND/OR decomposition of the
search space, which significantly boosts the performance of traditional heuristic search schemes,
was not cost-effective for m-best search algorithms, at least with our current implementation. As
expected, the best-first schemes dominate the branch and bound algorithms whenever sufficient
space is available, but fail on the memory-intensive problems. We compared our schemes with 4
previously developed algorithms: three approximate schemes based on an LP-relaxation of the problem and an algorithm performing message passing on a junction tree. We showed that our schemes
often dominate the competing schemes, known to be efficient, in terms of runtime, especially when
the required number of solutions is large. Moreover, our scheme guarantee solution optimality.

Acknowledgement
This work was sponsored in part by NSF grants IIS-1065618 and IIS-1254071, and by the United
States Air Force under Contract No. FA8750-14-C-0011 under the DARPA PPAML program.

946

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

instance

grid nnodes380 ps1 k2

(380, 2, 25, 379)

grid nnodes380 ps2 k2

(380, 2, 25, 61)

grid nnodes3192 ps2 k2

(3192, 2, 75, 217)

algorithm

m=2
time
Random binary grid

i-bound=20
m=5
m=10
time
time

m=25
time

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
0.49
0.56
55.93
112.5
5.06
4.63

OOM
OOM
0.53
0.64
106.34
772.49
46.57
13.4

OOM
OOM
0.58
0.71
202.65
1860.46
172.95
28.95

OOM
OOM
0.67
0.91
2027.66
5026.68
361.04
75.28

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
0.2
0.32
7.62
110.4
2.23
3.98

OOM
OOM
0.23
0.36
12.82
757.14
19.41
11.5

OOM
OOM
0.26
0.58
67.59
1820.45
38.54
24.34

OOM
OOM
0.36
0.95
1964.18
4985.0
Timeout
Timeout

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
OOM
Timeout
Timeout
OOM
123.45
26.86

OOM
OOM
OOM
Timeout
Timeout
OOM
658.05
81.27

OOM
OOM
OOM
Timeout
Timeout
OOM
3035.29
172.35

OOM
OOM
OOM
Timeout
Timeout
OOM
Timeout
Timeout

Table 16: Random binary grids, i-bound=20. Timeout - out of time, OOM - out of memory. 3 GB,
1 hour.

947

fiF LEROVA , M ARINESCU , & D ECHTER

instance

gen nnodes132 ps1 k2

(132, 2, 13, 34)

gen nnodes380 ps1 k2

(380, 2, 25, 61)

gen nnodes1122 ps1 k2

(1122, 2, 43, 112)

algorithm

i-bound=20
m=5
m=10
time
time
0.02
0.03
0.03
0.06
0.01
0.02
0.0
0.03
0.09
5.44
60.81
144.93
1.32
3.13
8.52
18.56

m=25
time
0.05
0.09
0.02
0.05
120.67
394.26
13.24
48.76

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

m=2
time
0.01
0.01
0.0
0.0
0.03
9.34
0.5
2.9

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
0.47
0.54
51.77
105.58
2.07
4.38

OOM
OOM
0.51
0.61
110.96
728.0
6.2
14.09

OOM
OOM
0.57
0.73
141.68
1753.98
13.21
29.96

OOM
OOM
0.72
1.03
2027.05
4817.09
76.0
75.04

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
OOM
Timeout
Timeout
OOM
16.46
9.69

OOM
OOM
OOM
Timeout
Timeout
OOM
57.96
28.84

OOM
OOM
OOM
Timeout
Timeout
OOM
107.73
61.04

OOM
OOM
OOM
Timeout
Timeout
OOM
282.4
158.7

Table 17: Random loopy graphs with submodular potentials, i-bound=20. Timeout - out of time,
OOM - out of memory. 3 GB, 1 hour.

948

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Figure 18: Example AND/OR search tree with 3 layers of OR and AND nodes.

Appendix A. Proof of Theorem 13
Let ST be the AND/OR search tree relative to pseudo tree T with depth h, n be the number of
variables, k be the maximum domain size, and deg be the maximum degree of the nodes in T .
Define a partial solution subtree T 0 to be a subtree of ST such that: (1) T 0 contains the root s
of ST ; (2) if a non-terminal OR node n is in T 0 , then T 0 contains exactly one AND child node n0 of
n; (3) if a non-terminal AND node n is in T 0 then T 0 contains all OR child nodes n01 , . . . , n0j of n;
(4) a leaf or tip node of T 0 doesnt have any successors in T 0 .
The nodes in ST are grouped into layers. There are h layers such that the ith layer, denoted by
Li , where 1  i  h, contains all the OR nodes whose variables have depth i in T , together with
their AND children. We assume that the root of T has depth 1. For illustration, Figure 18 depicts
an AND/OR search tree with 3 layers, where for example L1 = {A, hA, 0i, hA, 1i}.
We denote by TiOR the set of partial solution subtrees whose leaf nodes are OR nodes in Li .
Similarly, TiAN D is the set of partial solution subtrees whose leaf nodes are AND nodes in Li . A
partial solution subtree T 0  T2OR whose leaf nodes are OR nodes belonging to the 2nd layer is
highlighted in Figure 18, namely T 0 = {A, hA, 0i, B, C}.
L EMMA 1. Given T 0  TiOR and T 00  TiAN D such that T 00 is an extension of T 0 , then T 0 and T 00
have the same number of leaf nodes.
Proof. Let m be the number of OR leaf nodes in T 0 . By definition, each of the m nodes can be
extended by exactly one AND child node in T 00 . It follows that T 00 has also m AND leaf nodes.
L EMMA 2. Given T 0  TiOR , the number of leaf nodes T 0 , denoted by mi , is at most deg i1 .
Proof. We show by induction that mi = deg i1 . If i = 1 then m1 = 1. Assume that for i = p  1,
OR . We first extend T 0 to T 00  T AN D . By Lemma 1, T 00 and T 0
mp1 = deg p2 , and let T 0  Tp1
p1
have the same number of leaf nodes, namely mp1 . Next, we extend T 00 to T 000  TpOR . Since each
of the mp1 AND leaf nodes in T 00 can have at most deg OR child nodes in T 000 , it follows that mp ,
the number of leaf nodes in T 000 is mp = mp1  deg = deg p2  deg = deg p1 .
Proof of Theorem 13 Consider the number of partial solution subtrees N that are contained by ST :
949

fiF LEROVA , M ARINESCU , & D ECHTER

N=

h
X

(NiOR + NiAN D )

(3)

i=1

NiOR

where
= |TiOR | and NiAN D = |TiAN D |, respectively.
0
AN D , it is easy to see that T 0 can be extended to a single partial solution subtree
Given T  Ti1
00
OR
T  Ti such that each of the leaf nodes in T 0 has at most deg OR child nodes in T 00 . Therefore:
AN D
NiOR = Ni1

(4)

Given T 0  TiOR , T 0 can be extended to at most k m partial solution subtrees T 00  TiAN D
because each of the m OR leaf nodes in T 0 can have exactly one AND child node in T 00 and k
bounds the domain size. By Lemmas 1 and 2, we then have that:
NiAN D = NiOR  k deg

i1

(5)

Using Equations 4 and 5, as well as N1OR = 1, we rewrite Equation 3 as follows:
N = (1 + k)
+ (k + k deg+1 )
+ (k deg+1 + k deg

2 +deg+1

)
(6)

+ ...
+ (k deg
 O(k

h2 +deg h3 +...+1

deg h 1
deg1

+ k deg

h1 +deg h2 +...+1

)

)

Thus, the worst-case number of partial solution subtrees that need to be stored in OPEN is
h1
h1
N  O(k deg ). Therefore, the time and space complexity of m-AOBF follows as O(k deg ).
When the pseudo tree T is balanced, namely each internal node has exactly deg child nodes, the
time and space complexity bound is to O(k n ), since n  O(deg h1 ).

References
Aljazzar, H., & Leue, S. (2011). K : A heuristic search algorithm for finding the k shortest paths.
Artificial Intelligence, 175(18), 21292154.
Batra, D. (2012). An efficient message-passing algorithm for the M-best MAP problem. Uncertainty
in Artificial Intelligence.
Charniak, E., & Shimony, S. (1994). Cost-based abduction and MAP explanation. Artificial Intelligence, 66(2), 345374.
Darwiche, A. (2001). Decomposable negation normal form. Journal of the ACM (JACM), 48(4),
608647.
Darwiche, A., Dechter, R., Choi, A., Gogate, V., & Otten, L. (2008).
Results from the probablistic inference evaluation of UAI08, a web-report in
http://graphmod.ics.uci.edu/uai08/Evaluation/Report.
In: Uncertainty in Artificial Intelligence applications workshop.
950

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

de Campos, L. M., Gamez, J. A., & Moral, S. (1999). Partial abductive inference in bayesian belief
networks using a genetic algorithm. Pattern Recognition Letters, 20(11), 12111217.
de Campos, L. M., Gamez, J. A., & Moral, S. (2004). Partial abductive inference in bayesian networks by using probability trees. In Enterprise Information Systems V, pp. 146154. Springer.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial Intelligence,
113(1), 4185.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces for graphical models. Artificial Intelligence, 171(2-3), 73106.
Dechter, R., & Rish, I. (2003). Mini-buckets: A general scheme for bounded inference. Journal of
the ACM, 50(2), 107153.
Dechter, R. (2013). Reasoning with probabilistic and deterministic graphical models: Exact algorithms. Synthesis Lectures on Artificial Intelligence and Machine Learning, 7(3), 1191.
Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies and the optimality of A*.
Journal of the ACM (JACM), 32(3), 505536.
Dijkstra, E. W. (1959). A note on two problems in connexion with graphs. Numerische mathematik,
1(1), 269271.
Elliott, P. (2007). Extracting the K Best Solutions from a Valued And-Or Acyclic Graph. Masters
thesis, Massachusetts Institute of Technology.
Eppstein, D. (1994). Finding the k shortest paths. In Proceedings 35th Symposium on the Foundations of Computer Science, pp. 154165. IEEE Comput. Soc. Press.
Fishelson, M., & Geiger, D. (2002). Exact genetic linkage computations for general pedigrees. In
International Conference on Intelligent Systems for Molecular Biology (ISMB), pp. 189198.
Fishelson, M. a., Dovgolevsky, N., & Geiger, D. (2005). Maximum likelihood haplotyping for
general pedigrees. Human Heredity, 59(1), 4160.
Flerova, N., Dechter, R., & Rollon, E. (2011). Bucket and mini-bucket schemes for m best solutions
over graphical models. In Graph structures for knowledge representation and reasoning
workshop.
Fromer, M., & Globerson, A. (2009). An lp view of the m-best map problem. Advances in Neural
Information Processing Systems, 22, 567575.
Ghosh, P., Sharma, A., Chakrabarti, P., & Dasgupta, P. (2012). Algorithms for generating ordered
solutions for explicit AND/OR structures. Journal of Artificial Intelligence (JAIR), 44(1),
275333.
Gogate, V. G. (2009). Sampling Algorithms for Probabilistic Graphical Models with Determinism
DISSERTATION. Ph.D. thesis, University of California, Irvine.
Hamacher, H., & Queyranne, M. (1985). K best solutions to combinatorial optimization problems.
Annals of Operations Research, 4(1), 123143.
Ihler, A. T., Flerova, N., Dechter, R., & Otten, L. (2012). Join-graph based cost-shifting schemes.
arXiv preprint arXiv:1210.4878.
Kask, K., & Dechter, R. (1999a). Branch and bound with mini-bucket heuristics. In IJCAI, Vol. 99,
pp. 426433.
951

fiF LEROVA , M ARINESCU , & D ECHTER

Kask, K., & Dechter, R. (1999b). Mini-bucket heuristics for improved search. In Proceedings
of the Fifteenth conference on Uncertainty in artificial intelligence, pp. 314323. Morgan
Kaufmann Publishers Inc.
Kjrulff, U. (1990). Triangulation of graphsalgorithms giving small total state space. Tech. Report
R-90-09.
Lawler, E. (1972). A procedure for computing the k best solutions to discrete optimization problems
and its application to the shortest path problem. Management Science, 18(7), 401405.
Marinescu, R., & Dechter, R. (2009a). AND/OR Branch-and-Bound search for combinatorial optimization in graphical models. Artificial Intelligence, 173(16-17), 14571491.
Marinescu, R., & Dechter, R. (2009b). Memory intensive AND/OR search for combinatorial optimization in graphical models. Artificial Intelligence, 173(16-17), 14921524.
Marinescu, R., & Dechter, R. (2005). AND/OR branch-and-bound for graphical models. In International Joint Conference on Artificial Intelligence, Vol. 19, p. 224. Lawrence Erlbaum
Associates Ltd.
Nillson, N. J. (1980). Principles of Artificial Intelligence. Tioga, Palo Alto, CA.
Nilsson, D. (1998). An efficient algorithm for finding the M most probable configurations in probabilistic expert systems. Statistics and Computing, 8(2), 159173.
Nilsson, N. (1982). Principles of artificial intelligence. Springer Verlag.
Otten, L., & Dechter, R. (2011). Anytime AND/OR depth first search for combinatorial optimization. In SOCS.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies. Addison-Wesley.
Schiex, T. (2000). Arc consistency for soft constraints. International Conference on Principles and
Practice of Constraint Programming (CP), 411424.
Seroussi, B., & Golmard, J. (1994). An algorithm directly finding the K most probable configurations in Bayesian networks. International Journal of Approximate Reasoning, 11(3), 205
233.
Wainwright, M. J., & Jordan, M. I. (2003). Variational inference in graphical models: The view from
the marginal polytope. In Proceedings of the Annual Allerton congerence on communication
control and computing, Vol. 41, pp. 961971. Citeseer.
Wemmenhove, B., Mooij, J. M., Wiegerinck, W., Leisink, M., Kappen, H. J., & Neijt, J. P. (2007).
Inference in the promedas medical expert system. In Artificial intelligence in medicine, pp.
456460. Springer.
Yanover, C., & Weiss, Y. (2004). Finding the M Most Probable Configurations Using Loopy Belief
Propagation. In Advances in Neural Information Processing Systems 16. The MIT Press.
Yanover, C., Schueler-Furman, O., & Weiss, Y. (2008). Minimizing and learning energy functions
for side-chain prediction. Journal of Computational Biology, 15(7), 899911.

952

fi