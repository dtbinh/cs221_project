Journal of Articial Intelligence Research 40 (2011) 571-598

Submitted 11/10; published 03/11

Multiagent Learning in Large Anonymous Games
Ian A. Kash

kash@seas.harvard.edu

Center for Research on Computation and Society
Harvard University

Eric J. Friedman

ejf27@cornell.edu

Department of Operations Research
and Information Engineering
Cornell University

Joseph Y. Halpern

halpern@cs.cornell.edu

Department of Computer Science
Cornell University

Abstract
In large systems, it is important for agents to learn to act eectively, but sophisticated
multi-agent learning algorithms generally do not scale. An alternative approach is to nd
restricted classes of games where simple, ecient algorithms converge. It is shown that
stage learning eciently converges to Nash equilibria in large anonymous games if bestreply dynamics converge. Two features are identied that improve convergence. First,
rather than making learning more dicult, more agents are actually benecial in many
settings. Second, providing agents with statistical information about the behavior of others
can signicantly reduce the number of observations needed.

1. Introduction
Designers of distributed systems are frequently unable to determine how an agent in the
system should behave, because optimal behavior depends on the users preferences and
the actions of others. A natural approach is to have agents use a learning algorithm.
Many multiagent learning algorithms have been proposed including simple strategy update
procedures such as ctitious play (Fudenberg & Levine, 1998), multiagent versions of Qlearning (Watkins & Dayan, 1992), and no-regret algorithms (Cesa-Bianchi & Lugosi, 2006).
Our goal in this work is to help the designers of distributed systems understand when
learning is practical. As we discuss in Section 2, existing algorithms are generally unsuitable
for large distributed systems. In a distributed system, each agent has a limited view of the
actions of other agents. Algorithms that require knowing, for example, the strategy chosen
by every agent cannot be implemented. Furthermore, the size of distributed systems requires
fast convergence. Users may use the system for short periods of time and conditions in the
system change over time, so a practical algorithm for a system with thousands or millions
of users needs to have a convergence rate that is sublinear in the number of agents. Existing
algorithms tend to provide performance guarantees that are polynomial or even exponential.
Finally, the large number of agents in the system guarantees that there will be noise. Agents
will make mistakes and will behave in unexpectedly. Even if no agent changes his strategy,
there can still be noise in agent payos. For example, a gossip protocol will match dierent

c
2011
AI Access Foundation. All rights reserved.

fiKash, Friedman, & Halpern

agents from round to round; congestion in the underlying network may eect message delays
between agents. A learning algorithm needs to be robust to this noise.
While nding an algorithm that satises these requirements for arbitrary games may
be dicult, distributed systems have characteristics that make the problem easier. First,
they involve a large number of agents. Having more agents may seem to make learning
harderafter all, there are more possible interactions. However, it has the advantage that
the outcome of an action typically depends only weakly on what other agents do. This
makes outcomes robust to noise. Having a large number of agents also make it less useful
for an agent to try to inuence others; it becomes a better policy to try to learn an optimal
response. In contrast, with a small number of agents, an agent can attempt to guide learning
agents into an outcome that is benecial for him.
Second, distributed systems are often anonymous; it does not matter who does something, but rather how many agents do it. For example, when there is congestion on a link,
the experience of a single agent does not depend on who is sending the packets, but on how
many are being sent. Anonymous games have a long history in the economics literature
(e.g., Blonski, 2001) and have been a subject of recent interest in the computer science
literature (Daskalakis & Papadimitriou, 2007; Gradwohl & Reingold, 2008).
Finally, and perhaps most importantly, in a distributed system the system designer
controls the game agents are playing. This gives us a somewhat dierent perspective than
most work, which takes the game as given. We do not need to solve the hard problem
of nding an ecient algorithm for all games. Instead, we can nd algorithms that work
eciently for interesting classes of games, where for us interesting means the type of
games a system designer might wish agents to play. Such games should be well behaved,
since it would be strange to design a system where an agents decisions can inuence other
agents in pathological ways.
In Section 3, we show that stage learning (Friedman & Shenker, 1998) is robust, implementable with minimal information, and converges eciently for an interesting class of
games. In this algorithm, agents divide the rounds of the game into a series of stages. In
each stage, the agent uses a xed strategy except that he occasionally explores. At the end
of a stage, the agent chooses as his strategy for the next stage whatever strategy had the
highest average reward in the current stage. We prove that, under appropriate conditions, a
large system of stage learners will follow (approximate) best-reply dynamics1 despite errors
and exploration.
For games where best-reply dynamics converge, our theorem guarantees that learners
will play an approximate Nash equilibrium. In contrast to previous results, where the convergence guarantee scales poorly with the number of agents, our theorem guarantees convergence in a nite amount of time with an innite number of agents. While the assumption
that best-reply dynamics converge is a strong one, many interesting games converge under best-reply dynamics, including dominance-solvable games, games with monotone best
replies, and max-solvable games (Nisan, Schapira, & Zohar, 2008). The class of max-solvable
games in particular includes many important games such as Transmission Control Protocol
(TCP) congestion control, interdomain routing with the Border Gateway Protocol (BGP),
cost-sharing games, and stable-roommates games (Nisan, Schapira, Valiant, & Zohar, 2011).
1. In this paper, we consider best-reply dynamics where all agents update their strategy at the same time.
Some other results about best-reply dynamics assume agents update their strategy one at a time.

572

fiMultiagent Learning in Large Anonymous Games

Marden, Arslan, and Shamma (2007a) have observed that convergence of best-reply dynamics is often a property of games that humans design (although their observation was for a
slightly dierent notion of best-reply dynamics). Moreover, convergence of best-reply dynamics is a weaker assumption than a common assumption made in the mechanism design
literature, that the games of interest have dominant strategies (each agent has a strategy
that is optimal no matter what other agents do).
Simulation results, presented in Section 4, show that convergence is fast in practice: a
system with thousands of agents can converge in a few thousand rounds. Furthermore, we
identify two factors that determine the rate and quality of convergence. One is the number
of agents: having more agents makes the noise in the system more consistent so agents can
learn using fewer observations. The other is giving agents statistical information about the
behavior of other agents; this can speed convergence by an order of magnitude. Indeed,
even noisy statistical information about agent behavior, which should be relatively easy to
obtain and disseminate, can signicantly improve performance.
While our theoretical results are limited to stage learning, they provide intuition about
why other well behaved learning algorithms should also converge. Our simulations, which
include two other learning algorithms, bear this out. Furthermore, to demonstrate the
applicability of stage learning in more realistic settings, we simulate the results of learning
in a scrip system (Kash, Friedman, & Halpern, 2007). Our results demonstrate that stage
learning is robust to factors such as churn (agents joining and leaving the system) and
asynchrony (agents using stages of dierent lengths). However, stage learning is not robust
to all changes. We include simulations of games with a small number of agents, games
that are not anonymous, and games that are not continuous. These games violate the
assumptions of our theoretical results; our simulations show that, in these games, stage
learning converges very slowly or not at all.
Finally, not all participants in a system will necessarily behave as expected. For learning
to be useful in a real system, it needs to be robust to such behavior. In Section 5, we show
that the continuity of utility functions is a key property that makes stage learning robust
to Byzantine behavior by a small fraction of agents.

2. Related Work
One approach to learning to play games is to generalize reinforcement learning algorithms
such as Q-learning (Watkins & Dayan, 1992). One nice feature of this approach is that it
can handle games with state, which is important in distributed systems. In Q-learning, an
agent associates a value with each state-action pair. When he chooses action  in state ,
he updates the value (, ) based on the reward he received and the best value he can
achieve in the resulting state  (max ( ,  )). When generalizing to multiple agents, 
and  become vectors of the state and action of every agent and the max is replaced by a
prediction of the behavior of other agents. Dierent algorithms use dierent predictions;
for example, Nash-Q uses a Nash equilibrium calculation (Hu & Wellman, 2003). See the
work of Shoham, Powers, and Grenager (2003) for a survey.
Unfortunately, these algorithms converge too slowly for a large distributed system. The
algorithm needs to experience each possible action prole many times to guarantee convergence. So, with  agents and  strategies, the naive convergence time is ( ). Even with
573

fiKash, Friedman, & Halpern

a better representation for anonymous games, the convergence time is still ( ) (typically
  ). There is also a more fundamental problem with this approach: it assumes information that an agent is unlikely to have. In order to know which value to update, the
agent must learn the action chosen by every other agent. In practice, an agent will learn
something about the actions of the agents with whom he directly interacts, but is unlikely
to gain much information about the actions of other agents.
Another approach is no-regret learning, where agents choose a strategy for each round
that guarantees that the regret of their choices will be low. Hart and Mas-Colell (2000)
present such a learning procedure that converges to a correlated equilibrium 2 given knowledge of what the payos of every action would have been in each round. They also provide a variant of their algorithm that requires only information about the agents actual
payos (Hart & Mas-Colell, 2001). However, to guarantee convergence to within  of a
correlated equilibrium requires (/2 log ), still too slow for large systems. Furthermore, the convergence guarantee is that the distribution of play converges to equilibrium;
the strategies of individual learners will not converge. Many other no-regret algorithms
exist (Blum & Mansour, 2007). In Section 4, we use the Exp3 algorithm (Auer, CesaBianchi, Freund, & Schapire, 2002). They can achieve even better convergence in restricted
settings. For example, Blum, Even-Dar, and Ligett (2006) showed that in routing games
a continuum of no-regret learners will approximate Nash equilibrium in a nite amount of
time. Jafari, Greenwald, Gondek, and Ercal (2001) showed that no-regret learners converge
to Nash equilibrium in dominance solvable, constant sum, and general sum 2  2 games.
Foster and Young (2006) use a stage-learning procedure that converges to Nash equilibrium for two-player games. Germano and Lugosi (2007) showed that it converges for generic
-player games (games where best replies are unique). Young (2009) uses a similar algorithm without explicit stages that also converges for generic -player games. Rather than
selecting best replies, in these algorithms agents choose new actions randomly when not in
equilibrium. Unfortunately, these algorithms involve searching the whole strategy space,
so their convergence time is exponential. Another algorithm that uses stages to provide a
stable learning environment is the ESRL algorithm for coordinated exploration (Verbeeck,
Nowe, Parent, & Tuyls, 2007).
Marden, Arslan, and Shamma (2007b) and Marden, Young, Arslan, and Shamma (2009)
use an algorithm with experimentation and best replies but without explicit stages that
converges for weakly acyclic games, where best-reply dynamics converge when agents move
one at a time, rather than moving all at once, as we assume here. Convergence is based
on the existence of a sequence of exploration moves that lead to equilibrium. With 
agents who explore with probability , this analysis gives a convergence time of (1/ ).
Furthermore, the guarantee requires  to be suciently small that agents essentially explore
one at a time, so  needs to be (1/).
Adlakha, Johari, Weintraub, and Goldsmith (2010) have independently given conditions
for the existence of an oblivious equilibrium, or mean eld equilibrium, in stochastic
games. Just as in our model they require that the game be large, anonymous, and continuous. In an oblivious equilibrium, each player reacts only to the average states and
2. Correlated equilibrium is a more general solution concept than Nash equilibrium (see Osborne & Rubenstein, 1994); every Nash equilibrium is a correlated equilibrium, but there may be correlated equilibria
that are not Nash equilibria.

574

fiMultiagent Learning in Large Anonymous Games

strategies of other players rather than their exact values. However, this model assumes that
a players payo depends only on the state of other players and not their actions. Adlakha
and Johari (2010) consider stochastic games with strategic complementarities and show that
mean eld equilibria exist, best-reply dynamics converge, and myopic learning dynamics
(which require only knowledge of the aggregate states of other players) can nd them.
There is a long history of work examining simple learning procedures such as ctitious
play (Fudenberg & Levine, 1998), where each agent makes a best response assuming that
each other players strategy is characterized by the empirical frequency of his observed
moves. In contrast to algorithms with convergence guarantees for general games, these algorithms fail to converge in many games. But for classes of games where they do converge,
they tend to do so rapidly. However, most work in this area assumes that the actions of
agents are observed by all agents, agents know the payo matrix, and payos are deterministic. A recent approach in this tradition is based on the Win or Learn Fast principle,
which has limited convergence guarantees but often performs well in practice (Bowling &
Veloso, 2001). Hopkins (1999) showed that many such procedures converge in symmetric
games with an innite number of learners, although his results provide no guarantees about
the rate of convergence.
There is also a body of empirical work on the convergence of learning algorithms in
multiagent settings. Q-learning has had empirical success in pricing games (Tesauro &
Kephart, 2002), -player cooperative games (Claus & Boutilier, 1998), and grid world
games (Bowling, 2000). Greenwald at al. (2001) showed that a number of algorithms,
including stage learning, converge in a variety of simple games. Marden et al. (2009) found
that their algorithm converged must faster in a congestion game than the theoretical analysis
would suggest. Our theorem suggests an explanation for these empirical observations: bestreply dynamics converge in all these games. While our theorem applies directly only to
stage learning, it provides intuition as to why algorithms that learn quickly enough and
change their behavior slowly enough rapidly converge to Nash equilibrium in practice.

3. Theoretical Results
In this section we present the theoretical analysis of our model. We then provide support
from simulations in the following section.
3.1 Large Anonymous Games
We are interested in anonymous games with countably many agents. Assuming that there
are countably many agents simplies the proofs; it is straightforward to extend our results
to games with a large nite number of agents. Our model is adapted from that of Blonski (2001). Formally, a large anonymous game is characterized by a tuple  = (, , , Pr).
  is the countably innite set of agents.
  is a nite set of actions from which each agent can choose (for simplicity, we assume
that each agent can choose from the same set of actions).
 (), the set of probability distributions over , has two useful interpretations. The
rst is as the set of mixed actions. For    we will abuse notation and denote the
575

fiKash, Friedman, & Halpern

mixed action that is  with probability 1 as . In each round each agent chooses one
of these mixed actions. The second interpretation of   () is as the fraction of
agents choosing each action   . This is important for our notion of anonymity,
which says an agents utility should depend only on how many agents choose each
action rather than who chooses it.
  = { :   ()} is the set of (mixed) action proles (i.e. which action each
agent chooses). Given the mixed action of every agent, we want to know the fraction
of agents that end up choosing action . For   , let ()() denote the probability
with which agent  plays  according to ()  ().We can then express the fraction
of agents in  that choose action  as lim (1/) =0 ()(), if this limit exists. If
the limit exists for all actions   , let   () give the value of the limit for each
. The proles  that we use are all determined by a simple random process. For such
proles , the strong law of large numbers (SLLN) guarantees that with probability
1  is well dened. Thus it will typically be well dened (using similar limits) for us
to talk about the fraction of agents who do something.
    is the nite set of payos that agents can receive.
 Pr :   ()  ( ) denotes the distribution over payos that results when the
agent performs action  and other agents follow action prole . We use a probability
distribution over payos rather than a payo to model the fact that agent payos may
change even if no agent changes his strategy. The expected utility of an agent who
performs

 mixed action  when other agents follow action distribution  is (, ) =

 () Pr, (). Our denition of Pr in terms of () rather than 
ensures the game is anonymous. We further require that Pr (and thus ) be Lipschitz
continuous.3 For deniteness, we use the L1 norm as our notion of distance when
specifying continuity (the L1 distance between two vectors is the sum of the absolute
values of the dierences in each component). Note that this formulation assumes all
agents share a common utility function. This assumption can be relaxed to allow
agents to have a nite number of types, which we show in Appendix A.
An example of a large anonymous game is one where, in each round, each agent plays a
two-player game against an opponent chosen at random. Such random matching games are
common in the literature (e.g., Hopkins, 1999), and the meaning of an opponent chosen
at random can be made formal (Boylan, 1992). In such a game,  is the set of actions of
the two-player game and  is the set of payos of the game. Once every agent chooses an
action, the distribution over opponent actions is characterized by some   (). Let ,
denote the payo for the agent if he plays  and the other agent plays  . Then the utility
of mixed action  given distribution  is

(, ) =
()( ), .
, 2

3. Lipschitz continuity imposes the additional constraint that there is some constant  such that  Pr(, )
Pr(,  )/   1   for all  and  . Intuitively, this ensures that the distribution of outcomes does
not change too fast. This is a standard assumption that is easily seen to hold in the games that have
typically been considered in the literature.

576

fiMultiagent Learning in Large Anonymous Games

3.2 Best-Reply Dynamics
Given a game  and an action distribution , a natural goal for an agent is to play the
action that maximizes his expected utility with respect to : argmax (, ). We call
such an action a best reply to . In a practical amount of time, an agent may have diculty
determining which of two actions with close expected utilities is better, so we will allow
agents to choose actions that are close to best replies. If  is a best reply to , then  is
an -best reply to  if ( , ) +   (, ). There may be more than one -best reply; we
denote the set of -best replies ABR  ().
We do not have a single agent looking for a best reply; every agent is trying to nd a one
at the same time. If agents start o with some action distribution 0 , after they all nd a
best reply there will be a new action distribution 1 . We assume that 0 () = 1/ (agents
choose their initial strategy uniformly at random), but our results apply to any distribution
used to determine the initial strategy. We say that a sequence (0 , 1 , . . .) is an -bestreply sequence if the support of +1 is a subset of ABR ( ); that is +1 gives positive
probability only to approximate best replies to  . A  best-reply sequence converges if
there exists some  such that for all  > ,  =  . Note that this is a particularly
strong notion of convergence because we require the  to converge in nite time and not
merely in the limit. A game may have innitely many best-reply sequences, so we say that
approximate best-reply dynamics converge if there exists some  > 0 such that every -bestreply sequence converges. The limit distribution  determines a mixed strategy that is an
-Nash equilibrium (i.e. the support of  is a subset of  ( )).
Our main result shows that learners can successfully learn in large anonymous games
where approximate best-reply dynamics converge. The number of stages needed to converge
is determined by the number of best replies needed before the sequence converges. It is possible to design games that have long best-reply sequences, but in practice most games have
short sequences. One condition that guarantees this is if 0 and all the degenerate action
distributions    (i.e., distributions that assign probability 1 to some   ) have unique
best replies. In this case, there can be at most  best replies before equilibrium is reached,
because we have assumed that all agents have the same utility function. Furthermore, in
such games the distinction between -best replies and best replies is irrelevant; for suciently small , a -best reply is a best reply. It is not hard to show that the property that
degenerate strategies have unique best replies is generic; it holds for almost every game.
3.3 Stage Learners
An agent who wants to nd a best reply may not know the set of payos  , the mapping
from actions to distributions over payos Pr, or the action distribution  (and, indeed, 
may be changing over time), so he will have to use some type of learning algorithm to learn
it. Our approach is to divide the play of the game into a sequence of stages. In each stage,
the agent almost always plays some xed action , but also explores other actions. At the
end of the stage, he chooses a new  for the next stage based on what he has learned. An
important feature of this approach is that agents maintain their actions for the entire stage,
so each stage provides a stable environment in which agents can learn. To simplify our
results, we specify a way of exploring and learning within a stage (originally described in
Friedman & Shenker, 1998), but our results should generalize to any reasonable learning
577

fiKash, Friedman, & Halpern

algorithm used to learn within a stage. (We discuss what is reasonable in Section 6.) In
this section, we show that, given a suitable parameter, at each stage most agents will have
learned a best reply to the environment of that stage.
Given a game , in each round  agent  needs to select a mixed action , . Our
agents use strategies that we denote  , for   , where  () = 1   and  ( = ) =
/(  1). Thus, with  , an agent almost always plays , but with probability  explores
other strategies uniformly at random. Thus far we have not specied what information an
agent can use to choose ,. Dierent games may provide dierent information. All that
we require is that an agent know all of his previous actions and his previous payos. More
precisely, for all  < , he knows his action  () (which is determined by , ) and his
payos  () (which is determined by Pr(, ,  ), where  is the action distribution for
round  ; note that we do not assume that the agent knows  .) Using this information, we
can express the average value of an action over the previous  = 1/2  rounds (the length
of a stage).4 Let (, , ) = {     <    () = } be 
the set of recent rounds in
which  was played by . Then the average value is  (, , ) =  (,,)  ()/(, , )
if (, , ) > 0 and 0 otherwise. While we need the value of  only at times that are
multiples of  , for convenience we dene it for arbitrary times .
We say that an agent is an -stage learner if he chooses his actions as follows. If  = 0,
 is chosen at random from {    }. If  is a nonzero multiple of  , , = (, ) where
(, ) = argmax  (, , ). Otherwise, , = ,1 . Thus, within a stage, his mixed
action is xed; at the end of a stage he updates it to use the action with the highest average
value during the previous stage.
The evolution of a game played by stage learners is not deterministic; each agent chooses
a random ,0 and the sequence of  () and  () he observes is also random. However, with
a countably innite set of agents, we can use the SLLN to make statements about the overall
behavior of the game. Let  () = ,. A run of the game consists of a sequence of triples
( ,  ,  ). The SLLN guarantees that with probability 1 the fraction of agents who choose
a strategy  in  is  (). Similarly, the fraction of agents who chose  in  that receive
payo  will be Pr(,  )() with probability 1.
To make our notion of a stage precise, we refer to the sequence of tuples
( ,  ,  ) . . . ((+1) 1 , (+1) 1 , (+1) 1 ) as stage  of the run. During stage  there
is a stationary action distribution that we denote  . If ,(+1) =  and   ABR  ( ),
then we say that agent  has learned an -best reply during stage  of the run. As the following lemma shows, for suciently small , most agents will learn an -best reply.
Lemma 3.1. For all large anonymous games , action proles, approximations  > 0, and
probabilities of error  > 0, there exists an  > 0 such that for  <  and all , if all
agents are -stage learners, then at least a 1   fraction of agents will learn an -best reply
during stage .
Proof. (Sketch) On average, an agent using strategy  plays action  (1  ) times during
a stage and plays all other actions  /(  1) times each. For  large, the realized number
of times played will be close to the expectation value with high probability. Thus, if  is
suciently large, then the average payo from each action will be exponentially close to the
4. The use of the exponent 2 is arbitrary. We require only that the expected number of times a strategy is
explored increases as  decreases.

578

fiMultiagent Learning in Large Anonymous Games

true expected value (via a standard Hoeding bound on sums of i.i.d. random variables), and
thus each learner will correctly identify an action with approximately the highest expected
payo with probability at least 1  . By the SLLN, at least a 1   fraction of agents will
learn an -best reply. A detailed version of this proof in a more general setting can be found
in the work by Friedman and Shenker (1998).
3.4 Convergence Theorem
Thus far we have dened large anonymous games where approximate best-reply dynamics
converge. If all agents in the game are -stage learners, then the sequence 0 , 1 , . . . of
action distributions in a run of the game is not a best-reply sequence, but it is close. The
action used by most agents most of the time in each  is the action used in  for some
approximate best reply sequence.
In order to prove this, we need to dene close. Our denition is based on the error rate
 and exploration rate  that introduces noise into  . Intuitively, distribution  is close to
 if, by changing the strategies of an  fraction of agents and having all agents explore an 
fraction of the time, we can go from an action prole with corresponding action distribution
 to one with corresponding distribution . Note that this denition will not be symmetric.
In this denition,  identies what (pure) action each agent is using that leads to , 
allows an  fraction of agents to use some other action, and  incorporates the fact that
each agent is exploring, so each strategy is an  (the agent usually plays  but explores
with probability ).
Denition 3.2. Action distribution  is (, )-close to  if there exist ,  , and    such
that:
  =  and  =  ;
 ()   for all   ;
    1  2 (this allows an  fraction of agents in  to play a dierent strategy
from );
 for some   , if  () =  then () =  .
The use of  in the nal requirement ensures that if two distributions are (, )-close
then they are also ( ,  )-close for all    and   . As an example of the asymmetry of
this denition,  is (0, ) close to , but the reverse is not true. While (, )-closeness is a
useful distance measure for our analysis, it is an unnatural notion of distance for specifying
the continuity of , where we used the L1 norm. The following simple lemma shows that
this distinction is unimportant; if  is suciently (, )-close to  then it is close according
to the L1 measure as well.
Lemma 3.3. If  is (, )-close to , then 
  1  2( + ).
Proof. Since  is (, )-close to , there exist ,  , and  as in Denition 3.2. Consider the
distributions  = ,  , and  = . We can view these three distributions as vectors, and
calculate their L1 distances. By Denition 3.2,    1  2.    1  2 because
an  fraction of agents explore. Thus by the triangle inequality, the L1 distance between 
and  is at most 2( + ).
579

fiKash, Friedman, & Halpern

We have assumed that approximate best reply sequences of  converge, but during
a run of the game agents will actually be learning approximate best replies to  . The
following lemma shows that this distinction does not matter if  and  are suciently close.
Lemma 3.4. For all  there exists a  such that if  is (, )-close to ,  > 0,  > 0, and
 +  <  then ABR (/2) (
)  ABR  ().
Proof. Let  be the maximum of the Lipschitz constants for all (, ) (one constant for each
) and  = /(8). Then for all  that are (, )-close to  and all , (, )  (, ) 

  1   2/(8) = /4 by Lemma 3.3.
Let  
/ ABR  () and   argmax ( , ). Then (, ) +  < ( , ). Combining
this with the above gives (, ) + /2 < ( , ). Thus  
/ ABR/2 (
).
Lemmas 3.1 and 3.4 give requirements on (, ). In the statement of the theorem, we call
(, ) -acceptable if they satisfy the requirements of both lemmas for /2 and all -best-reply
sequences converge in .
Theorem 3.5. Let  be a large anonymous game where approximate best-reply dynamics
converge and let (, ) be -acceptable for . If all agents are -stage learners then, for all
runs, there exists an -best-reply sequence 0 , 1 , . . . such that in stage  at least a 1  
fraction will learn a best reply to  with probability 1.
Proof. 0 = 0 (both are the uniform distribution), so 0 is (, )-close to . Assume  is
(, )-close to . By Lemma 3.1 at least a 1   fraction will learn a /2-best reply to  .
By Lemma 3.4, this is a -best reply to  . Thus +1 will be (, )-close to +1 .
Theorem 3.5 guarantees that after a nite number of stages, agents will be close to
an approximate Nash equilibrium prole. Specically,  will be (, )-close to an -Nash
equilibrium prole  . Note that this means that  is actually an   -Nash equilibrium for
a larger   that depends on ,,, and the Lipschitz constant .
Our three requirements for a practical learning algorithm were that it require minimal
information, converge quickly in a large system, and be robust to noise. Stage learning
requires only that an agent know his own payos, so the rst condition is satised. Theorem 3.5 shows that it satises the other two requirements. Convergence is guaranteed in a
nite number of stages. While the number of stages depends on the game, in Section 3.2 we
argued that in many cases it will be quite small. Finally, robustness comes from tolerating
an  fraction of errors. While in our proofs we assumed these errors were due to learning,
the analysis is the same if some of this noise is from other sources such as churn or agents
making errors. We discuss this issue more in Section 6.

4. Simulation Results
In this section, we discuss experimental results that demonstrate the practicality of learning
in large anonymous games. Theorem 3.5 guarantees convergence for a suciently small
exploration probability , but decreasing  also increases  , the length of a stage. Our
rst set of experiments shows that the necessary values of  and  are quite reasonable in
practice. While our theorem applies only to stage learning, the analysis provides intuition as
580

fiMultiagent Learning in Large Anonymous Games

to why a reasonable algorithm that changes slowly enough that other learners have a chance
to learn best replies should converge as well. To demonstrate this, we also implemented two
other learning algorithms, which also quickly converged.
Our theoretical results make two signicant predictions about factors that inuence
the rate of convergence. Lemma 3.1 tells us that the length of a stage is determined by
the number of times each strategy needs to be explored to get an accurate estimate of its
value. Thus, the amount of information provided by each observation has a large eect
on the rate of convergence. For example, in a random matching game, an agents payo
provides information about the strategy of one other agent. On the other hand, if he
receives his expected payo for being matched, a single observation provides information
about the entire distribution of strategies. In the latter case the agent can learn with many
fewer observations. A related prediction is that having more agents will lead to faster
convergence, particularly in games where payos are determined by the average behavior of
other agents, because variance in payos due to exploration and mistakes decreases as the
number of agents increases. Our experimental results illustrate both of these phenomena.
The game used in our rst set of experiments, like many simple games used to test
learning algorithms, is symmetric. Hopkins (1999) showed that many learning algorithms
are well behaved in symmetric games with large populations. To demonstrate that our
main results are due to something other than symmetry, we also tested stage learning on
an asymmetric game, and observed convergence even with a small population.
To explore the applicability of stage learning in a more practical setting that violates a
number of the assumptions of our theorem, we implemented a variant of stage learning for
a game based on a scrip system (Kash et al., 2007). To demonstrate the applicability of
this approach to real systems, we included experiments where there is churn (agents leaving
and being replaced by new agents) and agents learning at dierent rates.
Finally, we give examples of games that are not large, not anonymous, and not continuous, and provide simulations showing that stage learners learn far more slowly in these
games than in those that satisfy the hypotheses of Theorem 3.5, or do not learn to play
equilibrium at all. These examples demonstrate that these assumptions are essential for
our results.
4.1 A Contribution Game
In our rst set of experiments, agents play a contribution game (also called a Diamondtype search model in the work by Milgrom & Roberts, 1990). In the contribution game,
two agents choose strategies from 0 to 19, indicating how much eort they contribute to a
collective enterprise. The value to an agent depends on how much he contributes, as well
as how much the other agent contributes. If he contributes  and the contribution of the
other agent is , then his utility is 4  (  5)3 . In each round of our game, each agent is
paired with a random agent and they play the contribution game. In this game, best-reply
dynamics converge within 4 stages from any starting distribution.
We implemented three learning algorithms to run on this game. Our implementation of
stage learners is as described in Section 3.3, with  = 0.05. Rather than taking the length of
stage  to be 1/2 , we set  = 2500 to have suciently long stages for this value of , rather
than decreasing  until stages are long enough. Our second algorithm is based on that of

581

fiKash, Friedman, & Halpern

6
2 Agents
10 Agents
100 Agents

Distance from Equilibrium

5

4

3

2

1

0

0

1

2

3

4

Time

5
4

x 10

Figure 1: Stage learners with random matching.
3.5
2 Agents
10 Agents
100 Agents

Distance from Equilibrium

3

2.5

2

1.5

1

0.5

0

1

2

3
Time

4

5
4

x 10

Figure 2: Hart and Mas-Colell with random matching.
Hart and Mas-Colell (2001), with improvements suggested by Greenwald, Friedman, and
Shenker (2001). This algorithm takes parameters  and  (the exploration probability).
We used  = 16 and  = 0.05. Our nal learning algorithm is Exp3 (Auer et al., 2002).
We set , the exploration probability, to 0.05. This algorithm requires that payos be
normalized to lie in [0, 1]. Since a few choices of strategies lead to very large negative
payos, a naive normalization leads to almost every payo being close to 1. For better
performance, we normalized payos such that most payos fell into the range [0, 1] and any
that were outside were set to 0 or 1 as appropriate.
The results of these three algorithms are shown in Figures 1, 2, and 3. Each curve
shows the distance from equilibrium as a function of the number of rounds of a population
of agents of a given size using a given learning algorithm. The results were averaged over
ten runs. Since the payos for nearby strategies are close, we want our notion of distance
to take into account that agents playing 7 are closer to equilibrium (8) thanthose playing
zero. Therefore, we consider the expected distance of  from equilibrium:
 ()  8.
To determine , we counted the number of times each action was taken over the length of
582

fiMultiagent Learning in Large Anonymous Games

5
2 Agents
10 Agents
100 Agents

4.5

Distance from Equilibrium

4
3.5
3
2.5
2
1.5
1
0.5

0

1

2

3

4

Time

5
4

x 10

Figure 3: Exp3 with random matching.
6
2 Agents
10 Agents
100 Agents

Distance from Equilibrium

5

4

3

2

1

0

0

1

2

3
Time

4

5
4

x 10

Figure 4: Stage learning with average-based payos.
a stage, so in practice the distance will never be zero due to mistakes and exploration. For
ease of presentation, the graph shows only populations of size up to 100; similar results
were obtained for populations up to 5000 agents.
For stage learning, increasing the population size has a dramatic impact. With two
agents, mistakes and best replies to the results of these mistakes cause behavior to be quite
chaotic. With ten agents, agents successfully learn, although mistakes and suboptimal
strategies are quite frequent. With one hundred agents, all the agents converge quickly to
near equilibrium strategies and signicant mistakes are rare.
Despite a lack of theoretical guarantees, our other two algorithms also converge, although
somewhat more slowly. The long-run performance of Exp3 is similar to stage learning.
Hart and Mas-Colells algorithm only has asymptotic convergence guarantees, and tends
to converge slowly in practice if tuned for tight convergence. So to get it to converge in a
reasonable amount of time we tuned the parameters to accept somewhat weaker convergence
(although for the particular game shown here the dierence in convergence is not dramatic).

583

fiKash, Friedman, & Halpern

1.8
2 Agents
10 Agents
100 Agents

1.6

Distance from Equilibrium

1.4
1.2
1
0.8
0.6
0.4
0.2
0

0

0.5

1

1.5
Time

2

2.5
4

x 10

Figure 5: Stage learners in a congestion game.
Convergence of stage learning in the random-matching game takes approximately 10,000
rounds, which is too slow for many applications. If a system design requires this type of
matching, this makes learning problematic. However, the results of Figure 4 suggest that
the learning could be done much faster if the system designer could supply agents with
more information. This suggests that collecting statistical information about the behavior
of agents may be a critical feature for ensuring fast convergence. To model such a scenario,
consider a related game where, rather than being matched against a random opponent, all
agents contribute to the same project and their reward is based on the average contribution
of the other agents. The results of stage learning in this game are shown in Figure 4. With
so much more information available to agents from each observation, we were able to cut
the length of a stage by a factor of 10. The number of stages needed to reach equilibrium
remained essentially the same. Convergence was tighter as well; mistakes were rare and
almost all of the distance from equilibrium is due to exploration.
4.2 A Congestion Game
For a dierent game, we tested the performance of stage learners in a congestion game.
This game models a situation where two agents share a network link. They gain utility
proportional to their transmission rate over the link, but are penalized based on the resulting
congestion they experience. The game is asymmetric because the two dierent types of
agents place dierent values on transmission rate. The game is described in detail by
Greenwald, Friedman, and Shenker (2001), who showed that no-regret learners are able to
nd the equilibrium of this game. An extension of our theoretical results to games with
multiple types is presenting in Appendix A.
Figure 5 shows that stage learners were able to learn very quickly in this game, using
stages of length 250 even though they were being randomly matched against a player of
the other type. Because the dierent types of agents had dierent equilibrium strategies,
the distance measure we use is to treat the observed distribution of strategies and the
equilibrium distribution as vectors and compute their L1 distance.

584

fiMultiagent Learning in Large Anonymous Games

1

Fraction Playing Equilibrium Strategy

0.9
0.8
0.7
0.6
0.5
0.4
0.3

Capacity 2
Capacity 4
Capacity 5

0.2
0.1
0

0

5

10
Number of Stages

15

20

Figure 6: Stage learners in a TCP-like game.
4.3 A TCP-like Game
In the previous example, we considered a random-matching game where two agents of
dierent types share a link. We now consider a game where a large number of agents share
several links (this game is a variant of the congestion control game studied in Nisan et al.,
2011).
There are three types of agents using a network. Each agent chooses an integer rate at
which to transmit between 0 and 10. Links in the network have a maximum average rate at
which agents can transmit; if this is exceeded they share the capacity evenly among agents.
An agents utility is his overall transmission rate through the network minus a penalty for
trac that was dropped due to congestion. If an agent attempts to transmit at a rate of 
and has an actual rate of  the penalty is 0.5(  ).5
All agents share a link with an average capacity of 5. One third of agents are further
constrained by sharing a link with an average capacity of 2 and another third share a link
with average capacity of 4. This game has the unique equilibrium where agents in the rst
third choose a rate of 2, agents in the second third choose a rate of 4, and agents in the
nal third choose a rate of 9 (so that the overall average rate is 5). This results in a game
where best-reply dynamics converge in ve stages from a uniform starting distribution.
Figure 6 shows the results for 90 learners (30 of each type) with  = 50000 and  = 0.01,
averaged over ten runs. Agents constrained by an average capacity of two quickly learn their
equilibrium strategy, followed by those with an average capacity of four. Agents constrained
by an average capacity of ve learn their equilibrium strategy, but have a sawtooth pattern
where a small fraction alternately plays 10 rather than 9. This is because, with exploration,
it is actually optimal for a small number of agents to play 10. Once a noticeable fraction
does so, 9 is uniquely optimal. This demonstrates that, strictly speaking, this game does
not satisfy our continuity requirement. In equilibrium, the demand for bandwidth is exactly
equal to the supply. Thus, small changes in the demand of other agents due to exploration
can have a large eect on the amount that can actually be demanded and thus on the payos
5. This penalty is not used in the work by Nisan et al. (2011); using it avoids the tie-breaking issues they
consider.

585

fiKash, Friedman, & Halpern

1

Fraction Playing Equilibrium Strategy

0.9
0.8
0.7
0.6
0.5
0.4
0.3
Type 1
Type 2
Type 3

0.2
0.1
0

0

5

10
Number of Stages

15

20

Figure 7: Stage learners in random TCP-like games.
of various strategies. However, the structure of the game is such that play still tends to
remain close to the equilibrium in terms of the rates agents choose.
In addition to the specic parameters mentioned above, we also ran 100 simulations
where each of the three capacities was a randomly chosen integer between 0 and 10. Figure 7
shows that, on average, the results were similar. All three types of agents share a common
constraint; type 1 and type 2 each have an additional constraint. Unsurprisingly, since these
two types are symmetric their results are almost identical. All three types demonstrate the
sawtooth behavior, with type 3 doing so in more runs due to examples like Figure 6 where
having fewer constraints gives agents more exibility. This primarily comes from runs where
type 1 and type 2 have constraints that are larger than the overall constraint (i.e. only the
overall constraint matters). Thus all three types have the ability to benet from resources
not demanded when other agents explore.
4.4 A Scrip System Game
Our motivation for this work is to help the designers of distributed systems understand
when learning is practical. In order to demonstrate how stage learning could be applied in
such a setting, we tested a variant of stage learners in the model of a scrip system used by
Kash et al. (2007). In the model, agents pay other agents to provide them service and in
turn provide service themselves to earn money to pay for future service. Agents may place
dierent values on receiving service (), incur dierent costs to provide service (), discount
future utility at dierent rates (), and have dierent availabilities to provide service ().
We used a single type of agent with parameters  = 1.0,  = 0.05,  = 0.9,  = 1, average
amount of money per agent  = 1, and stages of 200 rounds per agent (only one agent
makes a request each round).
This model is not a large anonymous game because whether an agent should provide
service depends on how much money he currently has. Thus, stage learning as specied does
not work, because it does not take into account the current state of the (stochastic) game.
Despite this, we can still implement a variant of stage learning: x a strategy during each
stage and then at the end of the stage use an algorithm designed for this game to determine
586

fiMultiagent Learning in Large Anonymous Games

1.6
10 Agents
100 Agents

1.4

Distance from Equilibrium

1.2
1
0.8
0.6
0.4
0.2
0
0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

Time

2
4

x 10

Figure 8: Stage learners in a scrip system.
1.6
10 Agents
100 Agents

1.4

Distance from Equilibrium

1.2
1
0.8
0.6
0.4
0.2
0
0.2

0.4

0.6

0.8

1

1.2
Time

1.4

1.6

1.8

2
4

x 10

Figure 9: A scrip system with churn.
a new strategy that is a best reply to what the agent observed. Our algorithm works by
estimating the agents probabilities of making a request and being chosen as a volunteer
in each round, and then uses these probabilities to compute an optimal policy. Figure 8
shows that this is quite eective. The distance measure used is based on directly measuring
the distance of the agents chosen (threshold) strategy from the equilibrium strategy, since
unlike the previous games it is impossible to directly infer the agents strategy in each round
solely from his decision whether or not to volunteer. Note that the number of rounds has
been normalized based on the number of agents in Figure 8 and later gures; stages actually
lasted ten times as long with 100 agents.
Real systems do not have a static population of learning agents. To demonstrate the
robustness of stage learning to churn, we replaced ten percent of the agents with new agents
with randomly chosen initial strategies at the end of each period. As Figure 9 shows, this
has essentially no eect on convergence.

587

fiKash, Friedman, & Halpern

1.6
10 Agents
100 Agents

1.4

Distance from Equilibrium

1.2
1
0.8
0.6
0.4
0.2
0
0.2

0.4

0.6

0.8

1

1.2
Time

1.4

1.6

1.8

2
4

x 10

Figure 10: A scrip system with dierent stage lengths.
Finally, in a real system it is often unreasonable to expect all agents to be able to update
their strategies at the same time. Figure 10 shows that having half the agents use stages of
222 rounds per agent rather than 200 did not have a signicant eect on convergence.6
4.5 Learning Counterexamples
At rst glance, Theorem 3.5 may seem trivial. In a game where best-reply dynamics are
guaranteed to converge, it seems obvious that agents who attempt to nd best replies
should successfully nd them and reach equilibrium. However, as we show in this section,
this fact alone is not sucient. In particular, all three of the key features of the games we
studythat they are large, anonymous, and continuousare required for the theorem to
hold.
First, if the game has only a small number of agents, a mistake made by a single
agent could be quite important, to the point where learning essentially has to start over.
So, while our results can be converted into results about the probability that none of a
nite number of agents will make a mistake in a given stage, the expected time to reach
equilibrium following this algorithm can be signicantly longer than the best-reply dynamics
would suggest. The following is an example of a game where the number of best replies
needed to reach equilibrium is approximately the number of strategies, but our experimental
results show that the number of stages needed by stage learners to nd the equilibrium is
signicantly longer. (We conjecture that in fact the learning time is exponentially longer.)
In contrast, Theorem 3.5 guarantees that, for games satisfying our requirements, the number
of stages needed is equal to the number of best replies.
Consider a game with three agents, where , the set of actions, is {0, 1, . . . , }. The
utility functions of the agents are symmetric; the rst agents utility function is given by
the following table:
6. In general we expect that small variations in stage lengths will not aect convergence; however large
enough dierences can result in non-Nash convergence. See the work by Greenwald et al. (2001) for some
simulations and analysis.

588

fiMultiagent Learning in Large Anonymous Games

actions
(0, , )
(, , )
(0, 1, 0)
(0, 0, 1)
(1, 1, 0)
(1, 0, 1)
(, , )
(, , )
(, , )
(, , )

payo
1
0
0
0
1
1
1
0
1
0

conditions
if  =  and either  > 1 or  > 1
if  =  and  > 0 and either  > 1 or  > 1

if  =  + 1
if  =  + 1 and  < 
if  = 

Agents learning best replies can be viewed as climbing a ladder. The best reply to
(, , ) is ( + 1,  + 1,  + 1) until agents reach (, , ), which is a Nash equilibrium.
However, when a mistake is made, agents essentially start over. To see how this works,
suppose that agents are at (3, 3, 3) and for the next stage one makes a mistake and they
select (5, 4, 4). This leads to the best reply sequence (5, 0, 0), (1, 0, 0), (1, 1, 1), at which
point agents can begin climbing again. The somewhat complicated structure of payos
near 0 ensures that agents begin climbing again from arbitrary patterns of mistakes. In a
typical run,  + 2 stages of best replies are needed to reach equilibrium: one stage with the
initial randomly-chosen strategies, one stage where all three agents switch to strategy 0,
and  stages of climbing. The exact number of stages can vary if two or more agents choose
the same initial strategy, but can never be greater than  + 3.
The following table gives the number of rounds (averaged over ten runs) for stage learners
in this game to rst reach equilibrium. As the number of strategies varies, the length of a
stage is  = 100( + 1), with exploration probability  = 0.05.
 rounds to reach 
4
7.0
9
19.3
14
25.8
19
39.5
24
37.3
29
102.7
34
169.4
39
246.6
With  = 4, stage learners typically require  + 2 stages, with an occasional error raising
the average slightly. With  between 9 and 24, a majority of runs feature at least one agent
making a mistake, so the number of stages required is closer to 2. With  = 29 and up,
there are many opportunities for agents to make a mistake, so the number of stages required
on average is in the range of 3 to 6. Thus learning is slower than best-reply dynamics,
and the disparity grows as the number of strategies increases.
A small modication of this example shows the problems that arise in games that are
not anonymous. In a non-anonymous game with a large number of agents, payos can
depend entirely on the actions of a small number of agents. For example, we can split the
set  of agents into three disjoint sets, 0 , 1 , and 2 , and choose agents 0  0 , 1  1 ,
589

fiKash, Friedman, & Halpern

1
0.9
0.8

Fraction Playing 0

0.7
0.6
0.5
10 agents
100 agents
1000 agents

0.4
0.3
0.2
0.1
0

0

5

10
Number of Stages

15

20

Figure 11: Stage learners in a discontinuous game.
and 2  2 . Again, each agent chooses an action in {0, . . . , }. The payos of agents 0, 1,
and 2 are determined as above; everyone in 0 gets the same payo as 0, everyone in 1
gets the same payo as 1, and everyone in 2 gets the same payo as 2. Again, convergence
to equilibrium will be signicantly slower than with best-reply dynamics.
Finally, consider the following game, which is large and anonymous, but does not satisfy
the continuity requirement. The set of actions is  = {0, 1}, and each agent always receives
a payo in  = {0, 1, 10}. If an agent chooses action 0, his payo is always 1 (Pr0, (1) = 1).
If he chooses action 1, his payo is 10 if every other agent chooses action 1, 10 if every
other agent chooses action 0, and 0 otherwise (Pr1,(1,0) (10) = 1, Pr1,(0,1) (10) = 1, and
Pr1, (0) = 1) for   {(1, 0), (0, 1)}).
In this game, suppose approximate best-reply dynamics start at (0.5, 0.5) (each action
is chosen by half of the agents). As they have not coordinated, the unique approximate best
reply for all agents is action 0, so after one best reply, the action distribution will be (1, 0).
Since agents have now coordinated, another round of approximate best replies leads to the
equilibrium (0, 1). If the agents are stage learners, after the rst stage they will learn an
approximate best reply to (0.5, 0.5) (exploration does not change the action prole in this
case), so most will adopt the mixed action 0 : playing 0 with probability 1   and 1 with
probability . Thus, even if no agents make a mistake, the action distribution for the next
stage will have at least an  fraction playing action 1. Thus the unique approximate best
reply will be action 0; stage learners will be stuck at 0, and never reach the equilibrium
of 1.
Figure 11 shows the fraction of times strategy 0 was played during each stage (averaged
over ten runs) for 10, 100, and 1000 agents ( = 100 and  = 0.05). With ten agents,
some initial mistakes are made, but after stage 10 strategy 0 was played about 2.5% of
the time in all runs, which corresponds to the fraction of time we expect to see it simply
from exploration. With 100 agents we see another sawtooth pattern where most agents
are stuck playing 0, but in alternating rounds a small fraction plays 1. This happens
because, in rounds where all are playing 0, a small fraction are lucky and explore 1 when
no other agents explore. As a result, they adopt strategy 1 for the next stage. However,
most do not, so in the following stage agents return to all playing 0. Such oscillating
590

fiMultiagent Learning in Large Anonymous Games

behavior has been observed in other learning contexts, for example among competing myopic
pricebots (Kephart, Hanson, & Greenwald, 2000). With 1000 agents, such lucky agents are
quite rare, so essentially all agents are constantly stuck playing 0.

5. Learning with Byzantine Agents
In practice, learning algorithms need to be robust to the presence of other agents not
following the algorithm. We have seen that stage learning in large anonymous games is
robust to agents who do not learn and instead follow some xed strategy in each stage.
In the analysis, these agents can simply be treated as agents who made a mistake in the
previous stage. However, an agent need not follow some xed strategy; an agent attempting
to interfere with the learning of other for malicious reasons or personal gain will likely adapt
his strategy over time. However, as we show in this section, stage learning can also handle
such manipulation in large anonymous games.
Gradwohl and Reingold (2008) examined several classes of games and introduced the
notion of a stable equilibrium as one in which a change of strategy by a small fraction
of agents only has a small eect on the payo of other agents. Their denition is for
games with a nite number of agents, but it can easily be adapted to our notion of a large
anonymous game. We take this notion a step further and characterize the game, rather
than an equilibrium, as stable if every strategy is stable.
Denition 5.1. A large anonymous game  is (, )-stable if for all ,   () such that
   1   and all   , (, )  (,  )  .
One class of games they consider is -continuous games. -continuity is essentially a
version of Lipschitz continuity for nite games, so it easy to show that large anonymous
games are stable, where the amount of manipulation that can be tolerated depends on the
Lipschitz constants for agents utility functions.
Lemma 5.2. For all large anonymous games , there exists a constant  such that for
all ,  is (, / )-stable
Proof. For all , (, ) is Lipschitz continuous with a constant  such that (, ) 
(,  )/   1   . Take  = max  . Then for all  and  such that    1 
/ ,
(, )  (,  )      1
     1
(
)

 ( )

 .

Gradwohl and Reingold (2008) show that stable equilibria have several nice properties.
If a small fraction of agents deviate, payos for the other agent will not decrease very much
relative to equilibrium. Additionally, following those strategies will still be an approximate
591

fiKash, Friedman, & Halpern

equilibrium despite the deviation. Finally, this means that the strategies still constitute
an approximate equilibrium even if asynchronous play causes the strategies of a fraction of
agents to be revealed to others.
We show that, if the game is stable, then learning is also robust to the actions of a small
fraction of Byzantine agents. The following lemma adapts Lemma 3.1 to show that, in each
stage, agents can learn approximate best replies despite the actions of Byzantine agents.
Thus agents can successfully reach equilibrium, as shown by Theorem 3.5.
To state the lemma, we need to dene the actions of a Byzantine agent. If there were
 and
no Byzantine agents, then in stage  there would be some stationary strategy 

corresponding fraction of agents choosing each action  . A  fraction of Byzantine agents
can change their actions arbitrarily each round, but doing so will have no eect on the
actions of the other agents. Thus, the Byzantine agents can cause the observed fraction of
agents choosing each strategy in round  to be any  such that    1 < 2. We refer
to a sequence  , . . . , ( +1)1 such that this condition holds for each  as a consistent
sequence. When we say that agents learn an -best reply during stage , we mean that the
 , the actions that the players
strategy that they learn is an approximate best reply to 
would have used had there been no Byzantine players, not the actual action prole, which
includes the strategies used by the Byzantine players.
Lemma 5.3. For all large anonymous games , action distributions   , approximations
 > 0, probabilities of error  > 0, and fractions of agents  < /6 , there exists an  > 0
such that for  <  , all , and all consistent sequences  , . . . , ( +1)1 , if all agents are
-stage learners, then at least a 1   fraction of agents will learn an -best reply during
stage  despite a  fraction of Byzantine agents.
Proof. Consider an agent  and round  in stage . If all agents were stage learners then
the action distribution would be  =   . However, the Byzantine agents have changed it
such that      2. Fix an action . By Lemma 5.2,
(,  )  (,  )   2 <

2

 =
6
3

This means that Byzantine agents can adjust an agents expected estimate of the value
of an action by at most /3. Let  be a best reply to   (the action used by stage learners
during stage ). In each round  of stage ,
( ,  )  ( ,  ) <


.
3

For any action  that is not an -best reply,
( ,  )  (,  ) = (( ,  )  (,  ) + ((,  )  (,  )) >  


2
=
.
3
3

Thus, regardless of the actions of the  fraction of Byzantine agents, agent s expected
estimate of the value of  exceeds his expected estimate of the value of  by at least /3.
Using Hoeding bounds as before, for suciently large , s estimates will be exponentially
close to these expectations, so with probability at least 1  , he will not select as best any
action that is not an -best reply. By the SLLN, this means that at least a 1   fraction
of agents will learn an -best reply.
592

fiMultiagent Learning in Large Anonymous Games

Thus, as Lemma 5.3 shows, not only can stage learners learn despite some agents learning incorrect values, they can also tolerate a suciently small number of agents behaving
arbitrarily.

6. Discussion
While our results show that a natural learning algorithm can learn eciently in an interesting class of games, there are many further issues that merit exploration.
6.1 Other Learning Algorithms
Our theorem assumes that agents use a simple rule for learning within each stage: they
average the value of payos received. However, there are certainly other rules for estimating the value of an action; any of these can be used as long as the rule guarantees that
errors can be made arbitrarily rare given sucient time. It is also not necessary to restrict
agents to stage learning. Stage learning guarantees a stationary environment for a period
of time, but such strict behavior may not be needed or practical. Other approaches, such
as exponentially discounting the weight of observations (Greenwald et al., 2001; Marden
et al., 2009) or Win or Learn Fast (Bowling & Veloso, 2001) allow an algorithm to focus
its learning on recent observations and provide a stable environment in which other agents
can learn.
6.2 Other Update Rules
In addition to using dierent algorithms to estimate the values of actions, a learner could
also change the way he uses those values to update his behavior. For example, rather than
basing his new strategy on only the last stage, he could base it on the entire history of
stages and use a rule in the spirit of ctitious play. Since there are games where ctitious
play converges but best-reply dynamics do not, this could extend our results to another
interesting class of games, as long as the errors in each period do not accumulate over time.
Another possibility is to update probabilistically or use a tolerance to determine whether
to update (see, e.g., Foster & Young, 2006; Hart & Mas-Colell, 2001). This could allow
convergence in games where best-reply dynamics oscillate or decrease the fraction of agents
who make mistakes once the system reaches equilibrium.
6.3 Model Assumptions
Our model makes several unrealistic assumptions, most notably that there are countably
many agents who all share the same utility function. Essentially the same results holds
with a large, nite number of agents, adding a few more error terms. In particular, since
there is always a small probability that every agent makes a mistake at the same time, we
can prove only that no more than a 1   fraction of the agents make errors in most rounds,
and that agents spend most of their time playing equilibrium strategies.
We have also implicitly assumed that the set of agents is xed. As Figure 9 shows,
we could easily allow for churn. A natural strategy for newly arriving agents is to pick
a random  to use in the next stage. If all agents do this, it follows that convergence is
unaected: we can treat the new agents as part of the  fraction that made a mistake in the
593

fiKash, Friedman, & Halpern

last stage. Furthermore, this tells us that newly arriving agents catch up very quickly.
After a single stage, new agents are guaranteed to have learned a best reply with probability
at least 1  .
Finally, we have assumed that all agents have the same utility function. Our results can
easily be extended to include a nite number of dierent types of agents, each with their
own utility function, since the SLLN can be applied to each type of agent. This extension
is discussed in Appendix A. We believe that our results hold even if the set of possible
types is innite. This can happen, for example, if an agents utility depends on a valuation
drawn from some interval. However, some care is needed to dene best-reply sequences in
this case.
6.4 State
One common feature of distributed systems not addressed in the theoretical portion of this
work is state. As we saw with the scrip system in Section 4.4, an agents current state is
often an important factor in choosing an optimal action.
In principle, we could extend our framework to games with state: in each stage each
agent chooses a policy to usually follow and explores other actions with probability . Each
agent could then use some o-policy algorithm (one where the agent can learn without
controlling the sequence of observations; see Kaelbling, Littman, & Moore, 1996 for examples) to learn an optimal policy to use in the next stage. One major problem with
this approach is that standard algorithms learn too slowly for our purposes. For example, Q-learning (Watkins & Dayan, 1992) typically needs to observe each state-action pair
hundreds of times in practice. The low exploration probability means that the expected
/ rounds needed to explore each pair even once is large. Ecient learning requires
more specialized algorithms that can make better use of the structure of a problem. However, the use of specialized algorithms makes providing a general guarantee of convergence
more dicult. Another problem is that, even if an agent explores each action for each of
his possible local states, the payo he receives will depend on the states of the other agents,
and thus the actions they chose. We need some property of the game to guarantee that this
distribution of states is in some sense well behaved. Adlakha and Joharis (2010) work on
mean eld equilibria gives one such condition. In this setting, the use of publicly available
statistics might provide a solution to these problems.
6.5 Mixed Equilibria
Another restriction of our results is that our agents only learn pure strategies. One way
to address this is to discretize the mixed strategy space (see, e.g., Foster & Young, 2006).
If one of the resulting strategies is suciently close to an equilibrium strategy and bestreply dynamics converge with the discretized strategies, then we expect agents to converge
to a near-equilibrium distribution of strategies. We have had empirical success using this
approach to learn to play rock-paper-scissors.

594

fiMultiagent Learning in Large Anonymous Games

7. Conclusion
Learning in distributed systems requires algorithms that are scalable to thousands of agents
and can be implemented with minimal information about the actions of other agents. Most
general-purpose multiagent learning algorithms fail one or both of these requirements. We
have shown here that stage learning can be an ecient solution in large anonymous games
where approximate best-reply dynamics lead to approximate pure strategy Nash equilibria.
Many interesting classes of games have this property, and it is frequently found in designed
games. In contrast to previous work, the time to convergence guaranteed by the theorem
does not increase with the number of agents. If system designers can nd an appropriate
game satisfying these properties on which to base their systems, they can be condent that
nodes can eciently learn appropriate behavior.
Our results also highlight two factors that aid convergence. First, having more learners often improves performance. With more learners, the noise introduced into payos by
exploration and mistakes becomes more consistent. Second, having more information typically improves performance. Publicly available statistics about the observed behavior of
agents can allow an agent to learn eectively while making fewer local observations. Our
simulations demonstrate the eects of these two factors, as well how our results generalize
to situations with other learning algorithms, churn, asynchrony, and Byzantine behavior.
7.1 Acknowledgments
Most of the work was done while IK was at Cornell University. EF, IK, and JH are supported
in part by NSF grant ITR-0325453. JH is also supported in part by NSF grant IIS-0812045
and by AFOSR grants FA9550-08-1-0438 and FA9550-05-1-0055. EF is also supported in
part by NSF grant CDI-0835706.

Appendix A. Multiple Types
In this section, we extend our denition of a large anonymous game to settings where
agents may have dierent utility functions. To do so, we introduce the notion of a type.
Agents utilities may depend on their type and the fraction of each type taking each action.
As our results rely on the strong law of large numbers, we restrict the set of types to
be nite. Formally, a large anonymous game with types is characterized by a tuple  =
(, , , , , Pr). We dene , , , and  as before. For the remaining terms:
  is a nite set of agent types.
  :    is a function mapping each agent to his type.
 As before, () is the set of probability distributions over , and can be viewed as
the set of mixed actions available to an agent. But now, to describe the fraction of
agents of each type choosing each action, we must use element of () .
 Pr :  ()  ( ) determines the distribution over payos that results when
an agent of type  performs action  and other agents follow action prole . The
expected utility of an agent of type  who performs mixed action  when other agents

595

fiKash, Friedman, & Halpern



follow action distribution  is (, , ) =   () Pr,, (). As before, we
further require that Pr (and thus ) be Lipschitz continuous.
The revised denitions of an -best reply, an -Nash equilibrium, an -best-reply sequence, convergence of approximate best-reply dynamics, and (, )-close follow naturally
from the revised denitions of  and . Lemma 3.1 now applies to each type of agent separately, and shows that all but a small fraction of each type will learn an approximate best
reply in each stage. Lemma 3.3 and Lemma 3.4 hold given the revised denitions of  and
. Thus Theorem 3.5, which combines these, also still holds.

References
Adlakha, S., & Johari, R. (2010). Mean eld equilibrium in dynamic games with complementarities. In IEEE Conference on Decision and Control (CDC).
Adlakha, S., Johari, R., Weintraub, G. Y., & Goldsmith, A. (2010). Mean eld analysis
for large population stochastic games. In IEEE Conference on Decision and Control
(CDC).
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (2002). The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32 (1), 4877.
Blonski, M. (2001). Equilibrium characterization in large anonymous games. Tech. rep.,
U. Mannheim.
Blum, A., Even-Dar, E., & Ligett, K. (2006). Routing without regret: on convergence
to Nash equilibria of regret-minimizing algorithms in routing games. In 25th ACM
Symp. on Principles of Distributed Computing (PODC), pp. 4552.
Blum, A., & Mansour, Y. (2007). Learning, regret minimization, and equilibria. In Nisan,
N., Roughgarden, T., Tardos, E., & Vazirani, V. (Eds.), Algorithmic Game Theory,
pp. 79102. Cambridge University Press.
Bowling, M. H. (2000). Convergence problems of general-sum multiagent reinforcement
learning. In 17th Int. Conf. on Machine Learning (ICML 2000), pp. 8994.
Bowling, M. H., & Veloso, M. M. (2001). Rational and convergent learning in stochastic
games. In 17th Int. Joint Conference on Articial Intelligence (IJCAI 2001), pp.
10211026.
Boylan, R. T. (1992). Laws of large numbers for dynamical systems with randomly matched
indviduals. Journal of Economic Theory, 57, 473504.
Cesa-Bianchi, N., & Lugosi, G. (2006). Prediction, Learning and Games. Cambridge University Press.
Claus, C., & Boutilier, C. (1998). The dynamics of reinforcement learning in cooperative
multiagent systems. In AAAI-97 Workshop on Multiagent Learning, pp. 746752.
Daskalakis, C., & Papadimitriou, C. H. (2007). Computing equilibria in anonymous games.
In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2007),
pp. 8393.

596

fiMultiagent Learning in Large Anonymous Games

Foster, D. P., & Young, P. (2006). Regret testing: Learning to play Nash equilibrium without
knowing you have an opponent. Theoretical Economics, 1, 341367.
Friedman, E. J., & Shenker, S. (1998). Learning and implementation on the internet. Tech.
rep., Cornell University.
Fudenberg, D., & Levine, D. (1998). Theory of Learning in Games. MIT Press.
Germano, F., & Lugosi, G. (2007). Global Nash convergence of Foster and Youngs regret
testing. Games and Economic Behavior, 60 (1), 135154.
Gradwohl, R., & Reingold, O. (2008). Fault tolerance in large games. In Proc. 9th ACM
Conference on Electronic Commerce (EC 2008), pp. 274283.
Greenwald, A., Friedman, E. J., & Shenker, S. (2001). Learning in networks contexts:
Experimental results from simulations. Games and Economic Behavior, 35 (1-2), 80
123.
Hart, S., & Mas-Colell, A. (2000). A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68 (5), 11271150.
Hart, S., & Mas-Colell, A. (2001). A reinforecement learning procedure leading to correlated
equilibrium. In Debreu, G., Neuefeind, W., & Trockel, W. (Eds.), Economic Essays,
pp. 181200. Springer.
Hopkins, E. (1999). Learning, matching, and aggregation. Games and Economic Behavior,
26, 79110.
Hu, J., & Wellman, M. P. (2003). Nash Q-learning for general-sum stochastic games. Journal
of Machine Learning Research, 4, 10391069.
Jafari, A., Greenwald, A. R., Gondek, D., & Ercal, G. (2001). On no-regret learning,
ctitious play, and nash equilibrium. In Proc. Eighteenth International Conference on
Machine Learning (ICML), pp. 226233.
Kaelbling, L. P., Littman, M. L., & Moore, A. P. (1996). Reinforcement learning: A survey.
J. Artif. Intell. Res. (JAIR), 4, 237285.
Kash, I. A., Friedman, E. J., & Halpern, J. Y. (2007). Optimizing scrip systems: Eciency,
crashes, hoarders and altruists. In Eighth ACM Conference on Electronic Commerce
(EC 2007), pp. 305315.
Kephart, J. O., Hanson, J. E., & Greenwald, A. R. (2000). Dynamic pricing by software
agents. Computer Networks, 32 (6), 731752.
Marden, J. R., Arslan, G., & Shamma, J. S. (2007a). Connections between cooperative
control and potential games. In Proc. 2007 European Control Conference (ECC).
Marden, J. R., Arslan, G., & Shamma, J. S. (2007b). Regret based dynamics: convergence in
weakly acyclic games. In 6th Int. Joint Conf. on Autonomous Agents and Multiagent
Systems (AAMAS), pp. 4249.
Marden, J. R., Young, H. P., Arslan, G., & Shamma, J. S. (2009). Payo-based dynamics
for multi-player weakly acyclic games. SIAM Journal on Control and Optimization,
48 (1), 373396.

597

fiKash, Friedman, & Halpern

Milgrom, P., & Roberts, J. (1990). Rationalizability, learning, and equilibrium in games
with strategic complement- arities. Econometrica, 58 (6), 12551277.
Nisan, N., Schapira, M., Valiant, G., & Zohar, A. (2011). Best-response mechanisms. In
Proc. Second Syposium on Innovations in Computer Science (ICS). To Appear.
Nisan, N., Schapira, M., & Zohar, A. (2008). Asynchronous best-reply dynamics. In
Proc. 4th International Workshop on Internet and Netwrok Economics (WINE), pp.
531538.
Osborne, M., & Rubenstein, A. (1994). A Course in Game Theory. MIT Press.
Shoham, Y., Powers, R., & Grenager, T. (2003). Multi-agent reinforcement learning: a
critical survey. Tech. rep., Stanford.
Tesauro, G., & Kephart, J. O. (2002). Pricing in agent economies using multi-agent Qlearning. Autonomous Agents and Multi-Agent Systems, 5 (3), 289304.
Verbeeck, K., Nowe, A., Parent, J., & Tuyls, K. (2007). Exploring selsh reinforcement
learning in repeated games with stochastic rewards. Journal of Autonomous Agents
and Multi-agent Systems, 14, 239269.
Watkins, C. J., & Dayan, P. (1992). Technical note Q-learning. Machine Learning, 8,
279292.
Young, H. P. (2009). Learning by trial and error. Games and Economic Behavior, 65 (2),
626643.

598

fi