<cite>Gert  De Cooman, Jasper  De Bock and M&#225;rcio  Alves Diniz (2015) "Coherent Predictive Inference under Exchangeability with Imprecise Probabilities", Volume 52, pages 1-95</cite>
<p class="media"><a href="/media/4490/live-4490-8400-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4490/live-4490-8402-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4490'>doi:10.1613/jair.4490</a></p>
<p>Coherent reasoning under uncertainty can be represented in a very general manner by coherent sets of desirable gambles. In a context that does not allow for indecision, this leads to an approach that is mathematically equivalent to working with coherent conditional probabilities. If we do allow for indecision, this leads to a more general foundation for coherent (imprecise-)probabilistic inference. In this framework, and for a given finite category set, coherent predictive inference under exchangeability can be represented using Bernstein coherent cones of multivariate polynomials on the simplex generated by this category set. This is a powerful generalisation of de Finetti's Representation Theorem allowing for both imprecision and indecision.<br />
<br />
We define an inference system as a map that associates a Bernstein coherent cone of polynomials with every finite category set. Many inference principles encountered in the literature can then be interpreted, and represented mathematically, as restrictions on such maps. We discuss, as particular examples, two important inference principles: representation insensitivity—a strengthened version of Walley's representation invariance—and specificity. We show that there is an infinity of inference systems that satisfy these two principles, amongst which we discuss in particular the skeptically cautious inference system, the inference systems corresponding to (a modified version of) Walley and Bernard's Imprecise Dirichlet Multinomial Models (IDMM), the skeptical IDMM inference systems, and the Haldane inference system. We also prove that the latter produces the same posterior inferences as would be obtained using Haldane's improper prior, implying that there is an infinity of proper priors that produce the same coherent posterior inferences as Haldane's improper one. Finally, we impose an additional inference principle that allows us to characterise uniquely the immediate predictions for the IDMM inference systems.</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Coherent Predictive Inference under Exchangeability with Imprecise Probabilities">
<meta name="citation_author" content="De Cooman, Gert">
<meta name="citation_author" content="De Bock, Jasper">
<meta name="citation_author" content="Diniz, M&#225;rcio Alves">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="95">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4490/live-4490-8400-jair.pdf">

<cite>Carmel  Domshlak and Vitaly  Mirkis (2015) "Deterministic Oversubscription Planning as Heuristic Search: Abstractions and Reformulations", Volume 52, pages 97-169</cite>
<p class="media"><a href="/media/4443/live-4443-8417-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4443/live-4443-8416-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4443'>doi:10.1613/jair.4443</a></p>
<p>While in classical planning the objective is to achieve one of the equally attractive goal states at as low total action cost as possible, the objective in deterministic oversubscription planning (OSP) is to achieve an as valuable as possible subset of goals within a fixed allowance of the total action cost. Although numerous applications in various fields share the latter objective, no substantial algorithmic advances have been made in deterministic OSP. Tracing the key sources of progress in classical planning, we identify a severe lack of effective domain-independent approximations for OSP. <br />
<br />
With our focus here on optimal planning, our goal is to bridge this gap. Two classes of approximation techniques have been found especially useful in the context of optimal classical planning: those based on state-space  abstractions and those based on logical landmarks for goal reachability. The question we  study here is whether some similar-in-spirit, yet possibly mathematically  different, approximation techniques can be developed for OSP. In the context of abstractions, we define the notion of additive abstractions for OSP, study the complexity of deriving effective abstractions from a rich space of hypotheses, and reveal some  substantial, empirically relevant islands of tractability. In the context of  landmarks, we show how standard goal-reachability landmarks of certain classical planning tasks  can be compiled into the OSP task of interest, resulting in an equivalent OSP task with a lower cost allowance, and thus with a smaller search space.  Our empirical evaluation confirms the effectiveness of the proposed techniques, and opens a wide gate for further developments in oversubscription planning.  </p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Deterministic Oversubscription Planning as Heuristic Search: Abstractions and Reformulations">
<meta name="citation_author" content="Domshlak, Carmel">
<meta name="citation_author" content="Mirkis, Vitaly">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="97">
<meta name="citation_lastpage" content="169">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4443/live-4443-8417-jair.pdf">

<cite>Yair  Wiener and Ran  El-Yaniv (2015) "Agnostic Pointwise-Competitive Selective Classification", Volume 52, pages 171-201</cite>
<p class="media"><a href="/media/4439/live-4439-8435-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4439/live-4439-8434-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4439'>doi:10.1613/jair.4439</a></p>
<p>Pointwise-competitive classifier from class F is required to classify identically to the best classifier in hindsight from F. For noisy, agnostic settings we present a strategy for learning pointwise-competitive classifiers from a finite training sample provided that the classifier can abstain from prediction at a certain region of its choice. For some interesting hypothesis classes and families of distributions, the measure of this rejected region is shown to be diminishing at a fast rate, with high probability. Exact implementation of the proposed learning strategy is dependent on an ERM oracle that can be hard to compute in the agnostic case. We thus consider a heuristic approximation procedure that is based on SVMs, and show empirically that this algorithm consistently outperforms a traditional rejection mechanism based on distance from decision boundary.</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Agnostic Pointwise-Competitive Selective Classification">
<meta name="citation_author" content="Wiener, Yair">
<meta name="citation_author" content="El-Yaniv, Ran">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="171">
<meta name="citation_lastpage" content="201">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4439/live-4439-8435-jair.pdf">

<cite>Ronald  de Haan, Iyad  Kanj and Stefan  Szeider (2015) "On the Subexponential-Time Complexity of CSP", Volume 52, pages 203-234</cite>
<p class="media"><a href="/media/4540/live-4540-8441-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4540'>doi:10.1613/jair.4540</a></p>
<p>Not all NP-complete problems share the same practical hardness with respect to exact computation.  Whereas some NP-complete problems are amenable to efficient computational methods, others are yet to show any such sign. It becomes a major challenge to develop a theoretical framework that is more fine-grained than the theory of NP-completeness, and that can explain the distinction between the exact complexities of various NP-complete problems. This distinction is highly relevant for constraint satisfaction problems under natural restrictions, where various shades of hardness can be observed in practice.<br />
<br />
Acknowledging the NP-hardness of such problems, one has to look beyond polynomial time computation. The theory of subexponential-time complexity provides such a framework, and has been enjoying increasing popularity in complexity theory. An instance of the constraint satisfaction problem with n variables over a domain of d values can be solved by brute-force in d<sup>n</sup> steps (omitting a polynomial factor). In this paper we study the existence of subexponential-time algorithms, that is, algorithms running in d<sup>o(n)</sup> steps, for various natural restrictions of the constraint satisfaction problem. We consider both the constraint satisfaction problem in which all the constraints are given extensionally as tables, and that in which all the constraints are given intensionally in the form of global constraints.  We provide tight characterizations of the subexponential-time complexity of the aforementioned problems with respect to several natural structural parameters, which allows us to draw a detailed landscape of the subexponential-time complexity of the constraint satisfaction problem. Our analysis provides fundamental results indicating whether and when one can significantly improve on the brute-force search approach for solving the constraint satisfaction problem.<br />
</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="On the Subexponential-Time Complexity of CSP">
<meta name="citation_author" content="de Haan, Ronald">
<meta name="citation_author" content="Kanj, Iyad">
<meta name="citation_author" content="Szeider, Stefan">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="203">
<meta name="citation_lastpage" content="234">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4540/live-4540-8441-jair.pdf">

<cite>Broes  De Cat, Marc  Denecker, Maurice  Bruynooghe and Peter  Stuckey (2015) "Lazy Model Expansion: Interleaving Grounding with Search", Volume 52, pages 235-286</cite>
<p class="media"><a href="/media/4591/live-4591-8481-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4591/live-4591-8482-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4591'>doi:10.1613/jair.4591</a></p>
<p>Finding satisfying assignments for the variables involved in a set of constraints can be cast as a (bounded) model generation problem: search for (bounded) models of a theory in some logic. The state-of-the-art approach for bounded model generation for rich knowledge representation languages like ASP and FO(.) and a CSP modeling language such as Zinc, is ground-and-solve: reduce the theory to a ground or propositional one and apply a search algorithm to the resulting theory.<br />
<br />
An important bottleneck is the blow-up of the size of the theory caused by the grounding phase. Lazily grounding the theory during search is a way to overcome this bottleneck. We present a theoretical framework and an implementation in the context of the FO(.) knowledge representation language. Instead of grounding all parts of a theory, justifications are derived for some parts of it. Given a partial assignment for the grounded part of the theory and valid justifications for the formulas of the non-grounded part, the justifications provide a recipe to construct a complete assignment that satisfies the non-grounded part. When a justification for a particular formula becomes invalid during search, a new one is derived; if that fails, the formula is split in a part to be grounded and a part that can be justified. Experimental results illustrate the power and generality of this approach.</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Lazy Model Expansion: Interleaving Grounding with Search">
<meta name="citation_author" content="De Cat, Broes">
<meta name="citation_author" content="Denecker, Marc">
<meta name="citation_author" content="Bruynooghe, Maurice">
<meta name="citation_author" content="Stuckey, Peter">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="235">
<meta name="citation_lastpage" content="286">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4591/live-4591-8481-jair.pdf">

<cite>Paolo  Liberatore (2015) "Revision by History", Volume 52, pages 287-329</cite>
<p class="media"><a href="/media/4608/live-4608-8484-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4608/live-4608-8486-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4608'>doi:10.1613/jair.4608</a></p>
<p>This article proposes a solution to the problem of obtaining plausibility information, which is necessary to perform belief revision: given a sequence of revisions, together with their results, derive a possible initial order that has generated them; this is different from the usual assumption of starting from an all-equal initial order and modifying it by a sequence of revisions. Four semantics for iterated revision are considered: natural, restrained, lexicographic and reinforcement. For each, a necessary and sufficient condition to the existence of an order generating a given history of revisions and results is proved. Complexity is proved coNP complete in all cases but one (reinforcement revision with unbounded sequence length). <br />
</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Revision by History">
<meta name="citation_author" content="Liberatore, Paolo">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="287">
<meta name="citation_lastpage" content="329">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4608/live-4608-8484-jair.pdf">

<cite>Shan  Xue, Alan  Fern and Daniel  Sheldon (2015) "Scheduling Conservation Designs for Maximum Flexibility via Network Cascade Optimization", Volume 52, pages 331-360</cite>
<p class="media"><a href="/media/4679/live-4679-8496-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4679/live-4679-8497-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4679'>doi:10.1613/jair.4679</a></p>
<p>One approach to conserving endangered species is to purchase and protect a set of land parcels in a way that maximizes the expected future population spread. Unfortunately, an ideal set of parcels may have a cost that is beyond the immediate budget constraints and must thus be purchased incrementally. This raises the challenge of deciding how to schedule the parcel purchases in a way that maximizes the flexibility of budget usage while keeping population spread loss in control. In this paper, we introduce a formulation of this scheduling problem that does not rely on knowing the future budgets of an organization. In particular, we consider scheduling purchases in a way that achieves a population spread no less than desired but delays purchases as long as possible. Such schedules offer conservation planners maximum flexibility and use available budgets in the most efficient way. We develop the problem formally as a stochastic optimization problem over a network cascade model describing a commonly used model of population spread. Our solution approach is based on reducing the stochastic problem to a novel variant of the directed Steiner tree problem, which we call the set-weighted directed Steiner graph problem. We show that this problem is computationally hard, motivating the development of a primal-dual algorithm for the problem that computes both a feasible solution and a bound on the quality of an optimal solution. We evaluate the approach on both real and synthetic conservation data with a standard population spread model. The algorithm is shown to produce near optimal results and is much more scalable than more generic off-the-shelf optimizers. Finally, we evaluate a variant of the algorithm to explore the trade-offs between budget savings and population growth.</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Scheduling Conservation Designs for Maximum Flexibility via Network Cascade Optimization">
<meta name="citation_author" content="Xue, Shan">
<meta name="citation_author" content="Fern, Alan">
<meta name="citation_author" content="Sheldon, Daniel">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="331">
<meta name="citation_lastpage" content="360">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4679/live-4679-8496-jair.pdf">

<cite>Been  Kim, Caleb  M. Chacha and Julie  A. Shah (2015) "Inferring Team Task Plans from Human Meetings: A Generative Modeling Approach with Logic-Based Prior", Volume 52, pages 361-398</cite>
<p class="media"><a href="/media/4496/live-4496-8574-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4496/live-4496-8575-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4496'>doi:10.1613/jair.4496</a></p>
<p>We aim to reduce the burden of programming and deploying autonomous systems to work in concert with people in time-critical domains such as military field operations and disaster response. Deployment plans for these operations are frequently negotiated on-the-fly by teams of human planners. A human operator then translates the agreed-upon plan into machine instructions for the robots. We present an algorithm that reduces this translation burden by inferring the final plan from a processed form of the human team's planning conversation. Our hybrid approach combines probabilistic generative modeling with logical plan validation used to compute a highly structured prior over possible plans, enabling us to overcome the challenge of performing inference over a large solution space with only a small amount of noisy data from the team planning session. We validate the algorithm through human subject experimentations and show that it is able to infer a human team's final plan with 86% accuracy on average. We also describe a robot demonstration in which two people plan and execute a first-response collaborative task with a PR2 robot. To the best of our knowledge, this is the first work to integrate a logical planning technique within a generative model to perform plan inference. </p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Inferring Team Task Plans from Human Meetings: A Generative Modeling Approach with Logic-Based Prior">
<meta name="citation_author" content="Kim, Been">
<meta name="citation_author" content="Chacha, Caleb M.">
<meta name="citation_author" content="Shah, Julie A.">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="361">
<meta name="citation_lastpage" content="398">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4496/live-4496-8574-jair.pdf">

<cite>Diederik  Marijn Roijers, Shimon  Whiteson and Frans  A. Oliehoek (2015) "Computing Convex Coverage Sets for Faster Multi-objective Coordination", Volume 52, pages 399-443</cite>
<p class="media"><a href="/media/4550/live-4550-8595-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4550/live-4550-8596-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4550'>doi:10.1613/jair.4550</a></p>
<p>In this article, we propose new algorithms for multi-objective coordination graphs (MO-CoGs). Key to the efficiency of these algorithms is that they compute a convex coverage set (CCS) instead of a Pareto coverage set (PCS). Not only is a CCS a sufficient solution set for a large class of problems, it also has important characteristics that facilitate more efficient solutions. We propose two main algorithms for computing a CCS in MO-CoGs. Convex multi-objective variable elimination (CMOVE) computes a CCS by performing a series of agent eliminations, which can be seen as solving a series of local multi-objective subproblems. Variable elimination linear support (VELS) iteratively identifies the single weight vector, w, that can lead to the maximal possible improvement on a partial CCS and calls variable elimination to solve a scalarized instance of the problem for w. VELS is faster than CMOVE for small and medium numbers of objectives and can compute an &#949;-approximate CCS in a fraction of the runtime. In addition, we propose variants of these methods that employ AND/OR tree search instead of variable elimination to achieve memory efficiency. We analyze the runtime and space complexities of these methods, prove their correctness, and compare them empirically against a naive baseline and an existing PCS method, both in terms of memory-usage and runtime.  Our results show that, by focusing on the CCS, these methods achieve much better scalability in the number of agents than the current state of the art.</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Computing Convex Coverage Sets for Faster Multi-objective Coordination">
<meta name="citation_author" content="Roijers, Diederik Marijn">
<meta name="citation_author" content="Whiteson, Shimon">
<meta name="citation_author" content="Oliehoek, Frans A.">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="399">
<meta name="citation_lastpage" content="443">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4550/live-4550-8595-jair.pdf">

<cite>Marie-Catherine  de Marneffe, Marta  Recasens and Christopher  Potts (2015) "Modeling the Lifespan of Discourse Entities with Application to Coreference Resolution", Volume 52, pages 445-475</cite>
<p class="media"><a href="/media/4565/live-4565-8580-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4565/live-4565-8582-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4565'>doi:10.1613/jair.4565</a></p>
<p>A discourse typically involves numerous entities, but few are mentioned more than once. Distinguishing those that die out after just one mention (singleton) from those that lead longer lives (coreferent) would dramatically simplify the hypothesis space for coreference resolution models, leading to increased performance. To realize these gains, we build a classifier for predicting the singleton/coreferent distinction. The model’s feature representations synthesize linguistic insights about the factors affecting discourse entity lifespans (especially negation, modality, and attitude predication) with existing results about the benefits of “surface” (part-of-speech and n-gram-based) features for coreference resolution. The model is effective in its own right, and the feature representations help to identify the anchor phrases in bridging anaphora as well. Furthermore, incorporating the model into two very different state-of-the-art coreference resolution systems, one rule-based and the other learning-based, yields significant performance improvements.</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Modeling the Lifespan of Discourse Entities with Application to Coreference Resolution">
<meta name="citation_author" content="de Marneffe, Marie-Catherine">
<meta name="citation_author" content="Recasens, Marta">
<meta name="citation_author" content="Potts, Christopher">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="445">
<meta name="citation_lastpage" content="475">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4565/live-4565-8580-jair.pdf">

<cite>Athirai  A. Irissappane and Jie   Zhang (2015) "A Case-Based Reasoning Framework to Choose Trust Models for Different E-Marketplace Environments", Volume 52, pages 477-505</cite>
<p class="media"><a href="/media/4595/live-4595-8608-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4595'>doi:10.1613/jair.4595</a></p>
<p>The performance of trust models highly depend on the characteristics of the environments where they are applied. Thus, it becomes challenging to choose a suitable trust model for a given e-marketplace environment, especially when ground truth about the agent (buyer and seller) behavior is unknown (called unknown environment). We propose a case-based reasoning framework to choose suitable trust models for unknown environments, based on the intuition that if a trust model performs well in one environment, it will do so in another similar environment. Firstly, we build a case base with a number of simulated environments (with known ground truth) along with the trust models most suitable for each of them. Given an unknown environment, case-based retrieval algorithms retrieve the most similar case(s), and the trust model of the most similar case(s) is chosen as the most suitable model for the unknown environment. Evaluation results confirm the effectiveness of our framework in choosing suitable trust models for different e-marketplace environments.</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="A Case-Based Reasoning Framework to Choose Trust Models for Different E-Marketplace Environments">
<meta name="citation_author" content="A. Irissappane, Athirai">
<meta name="citation_author" content="Zhang, Jie ">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="477">
<meta name="citation_lastpage" content="505">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4595/live-4595-8608-jair.pdf">

<cite>Piotr  Faliszewski, Edith  Hemaspaandra and Lane  A. Hemaspaandra (2015) "Weighted Electoral Control", Volume 52, pages 507-542</cite>
<cite>AAMAS 2013 Best Paper Award Finalist</cite><p class="media"><a href="/media/4621/live-4621-8614-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4621/live-4621-8615-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4621'>doi:10.1613/jair.4621</a></p>
<p>Although manipulation and bribery have been extensively studied under weighted voting, there has been almost no work done on election control under weighted voting. This is unfortunate, since weighted voting appears in many important natural settings. In this paper, we study the complexity of controlling the outcome of weighted elections through adding and deleting voters. We obtain polynomial-time algorithms, NP-completeness results, and for many NP-complete cases, approximation algorithms. In particular, for scoring rules we completely characterize the complexity of weighted voter control. Our work shows that for quite a few important cases, either polynomial-time exact algorithms or polynomial-time approximation algorithms exist.</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Weighted Electoral Control">
<meta name="citation_author" content="Faliszewski, Piotr">
<meta name="citation_author" content="Hemaspaandra, Edith">
<meta name="citation_author" content="Hemaspaandra, Lane A.">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="507">
<meta name="citation_lastpage" content="542">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4621/live-4621-8614-jair.pdf">

<cite>Minh  Dao-Tran, Thomas  Eiter, Michael  Fink and Thomas  Krennwallner (2015) "Distributed Evaluation of Nonmonotonic Multi-context Systems", Volume 52, pages 543-600</cite>
<p class="media"><a href="/media/4574/live-4574-8627-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4574'>doi:10.1613/jair.4574</a></p>
<p>Multi-context Systems (MCSs) are a formalism for systems consisting of knowledge bases (possibly heterogeneous and non-monotonic) that are interlinked via bridge rules, where the global system semantics emerges from the local semantics of the knowledge bases (also called “contexts”) in an equilibrium. While MCSs and related formalisms are inherently targeted for distributed set- tings, no truly distributed algorithms for their evaluation were available. We address this short- coming and present a suite of such algorithms which includes a basic algorithm DMCS, an ad- vanced version DMCSOPT that exploits topology-based optimizations, and a streaming algorithm DMCS-STREAMING that computes equilibria in packages of bounded size. The algorithms be- have quite differently in several respects, as experienced in thorough experimental evaluation of a system prototype. From the experimental results, we derive a guideline for choosing the appropriate algorithm and running mode in particular situations, determined by the parameter settings.</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="Distributed Evaluation of Nonmonotonic Multi-context Systems">
<meta name="citation_author" content="Dao-Tran, Minh">
<meta name="citation_author" content="Eiter, Thomas">
<meta name="citation_author" content="Fink, Michael">
<meta name="citation_author" content="Krennwallner, Thomas">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="543">
<meta name="citation_lastpage" content="600">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4574/live-4574-8627-jair.pdf">

<cite>Haonan  Yu, N.  Siddharth, Andrei  Barbu and Jeffrey  Mark Siskind (2015) "A Compositional Framework for Grounding Language Inference, Generation, and Acquisition in Video", Volume 52, pages 601-713</cite>
<cite>ACL 2013 Best Paper Award</cite><p class="media"><a href="/media/4556/live-4556-8631-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4556'>doi:10.1613/jair.4556</a></p>
<p>We present an approach to simultaneously reasoning about a video clip and an entire natural-language sentence. The compositional nature of language is exploited to construct models which represent the meanings of entire sentences composed out of the meanings of the words in those sentences mediated by a grammar that encodes the predicate-argument relations. We demonstrate that these models faithfully represent the meanings of sentences and are sensitive to how the roles played by participants (nouns), their characteristics (adjectives), the actions performed (verbs), the manner of such actions (adverbs), and changing spatial relations between participants (prepositions) affect the meaning of a sentence and how it is grounded in video. We exploit this methodology in three ways. In the first, a video clip along with a sentence are taken as input and the participants in the event described by the sentence are highlighted, even when the clip depicts multiple similar simultaneous events. In the second, a video clip is taken as input without a sentence and a sentence is generated that describes an event in that clip. In the third, a corpus of video clips is paired with sentences which describe some of the events in those clips and the meanings of the words in those sentences are learned. We learn these meanings without needing to specify which attribute of the video clips each word in a given sentence refers to. The learned meaning representations are shown to be intelligible to humans.<br />
</p>
<a href="/vol/vol52.html">Click here to return to Volume 52 contents list</a>
<meta name="citation_title" content="A Compositional Framework for Grounding Language Inference, Generation, and Acquisition in Video">
<meta name="citation_author" content="Yu, Haonan">
<meta name="citation_author" content="Siddharth, N.">
<meta name="citation_author" content="Barbu, Andrei">
<meta name="citation_author" content="Siskind, Jeffrey Mark">
<meta name="citation_publication_date" content="2015">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="601">
<meta name="citation_lastpage" content="713">
<meta name="citation_volume" content="52">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4556/live-4556-8631-jair.pdf">
