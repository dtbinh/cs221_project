<cite>M.  J. Carman and C.  A. Knoblock (2007) "Learning Semantic Definitions of Online Information Sources", Volume 30, pages 1-50</cite>
<p class="media"><a href="/media/2205/live-2205-3434-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2205/live-2205-3435-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2205'>doi:10.1613/jair.2205</a></p>
<p>The Internet contains a very large number of information sources providing many types of data from weather forecasts to travel deals and financial information. These sources can be accessed via Web-forms, Web Services, RSS feeds and so on. In order to make automated use of these sources, we need to model them semantically, but writing semantic descriptions for Web Services is both tedious and error prone. In this paper we investigate the problem of automatically generating such models. We introduce a framework for learning Datalog definitions of Web sources. In order to learn these definitions, our system actively invokes the sources and compares the data they produce with that of known sources of information. It then performs an inductive logic search through the space of plausible source definitions in order to learn the best possible semantic model for each new source. In this paper we perform an empirical evaluation of the system using real-world Web sources. The evaluation demonstrates the effectiveness of the approach, showing that we can automatically learn complex models for real sources in reasonable time. We also compare our system with a complex schema matching system, showing that our approach can handle the kinds of problems tackled by the latter.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Learning Semantic Definitions of Online Information Sources">
<meta name="citation_author" content="Carman, M. J.">
<meta name="citation_author" content="Knoblock, C. A.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="50">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2205/live-2205-3434-jair.pdf">

<cite>V.  Bulitko, N.  Sturtevant, J.  Lu and T.  Yau (2007) "Graph Abstraction in Real-time Heuristic Search", Volume 30, pages 51-100</cite>
<p class="media"><a href="/media/2293/live-2293-3453-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2293/live-2293-3454-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2293'>doi:10.1613/jair.2293</a></p>
<p>Real-time heuristic search methods are used by situated agents in applications that require the amount of planning per move to be independent of the problem size. Such agents plan only a few actions at a time in a local search space and avoid getting trapped in local minima by improving their heuristic function over time. We extend a wide class of real-time search algorithms with automatically-built state abstraction and prove completeness and convergence of the resulting family of algorithms. We then analyze the impact of abstraction in an extensive empirical study in real-time pathfinding. Abstraction is found to improve efficiency by providing better trading offs between planning time, learning speed and other negatively correlated performance measures.<br />
</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Graph Abstraction in Real-time Heuristic Search">
<meta name="citation_author" content="Bulitko, V.">
<meta name="citation_author" content="Sturtevant, N.">
<meta name="citation_author" content="Lu, J.">
<meta name="citation_author" content="Yau, T.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="51">
<meta name="citation_lastpage" content="100">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2293/live-2293-3453-jair.pdf">

<cite>M.  Pistore and M.  Y. Vardi (2007) "The Planning Spectrum - One, Two, Three, Infinity", Volume 30, pages 101-132</cite>
<p class="media"><a href="/media/1909/live-1909-3467-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1909/live-1909-3468-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1909'>doi:10.1613/jair.1909</a></p>
<p>Linear Temporal Logic (LTL) is widely used for defining conditions on the execution paths of dynamic systems.  In the case of dynamic systems that allow for nondeterministic evolutions, one has to specify, along with an LTL formula f, which are the paths that are required to satisfy the formula.  Two extreme cases are the universal interpretation A.f, which requires that the formula be satisfied for all execution paths, and the existential interpretation E.f, which requires that the formula be satisfied for some execution path.<br />
  <br />
When LTL is applied to the definition of goals in planning problems on nondeterministic domains, these two extreme cases are too restrictive. It is often impossible to develop plans that achieve the goal in all the nondeterministic evolutions of a system, and it is too weak to require that the goal is satisfied by some execution.<br />
  <br />
In this paper we explore alternative interpretations of an LTL formula that are between these extreme cases.  We define a new language that permits an arbitrary combination of the A and E quantifiers, thus allowing, for instance, to require that each finite execution can be extended to an execution satisfying an LTL formula (AE.f), or that there is some finite execution whose extensions all satisfy an LTL formula (EA.f).  We show that only eight of these combinations of path quantifiers are relevant, corresponding to an alternation of the quantifiers of length one (A and E), two (AE and EA), three (AEA and EAE), and infinity ((AE)* and (EA)*).  We also present a planning algorithm for the new language that is based on an automata-theoretic approach, and study its complexity.<br />
</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="The Planning Spectrum - One, Two, Three, Infinity">
<meta name="citation_author" content="Pistore, M.">
<meta name="citation_author" content="Vardi, M. Y.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="101">
<meta name="citation_lastpage" content="132">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1909/live-1909-3467-jair.pdf">

<cite>J.  L. Bredin, D.  C. Parkes and Q.  Duong (2007) "Chain: A Dynamic Double Auction Framework for Matching Patient Agents ", Volume 30, pages 133-179</cite>
<p class="media"><a href="/media/2303/live-2303-3506-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2303/live-2303-3507-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2303'>doi:10.1613/jair.2303</a></p>
<p>In this paper we present and evaluate a general framework for the design of truthful auctions for matching agents in a dynamic, two-sided market. A single commodity, such as a resource or a task, is bought and sold by multiple buyers and sellers that arrive and depart over time. Our algorithm, Chain, provides the first framework that allows a truthful dynamic double auction (DA) to be constructed from a truthful, single-period  (i.e. static) double-auction rule. The pricing and matching method of the Chain construction is unique amongst dynamic-auction rules that adopt the same building block.  We examine experimentally the allocative efficiency of Chain when instantiated on various single-period rules, including the canonical McAfee double-auction rule. For a baseline we also consider non-truthful double auctions populated with ``zero-intelligence plus"-style learning agents. Chain-based auctions perform well in comparison with other schemes, especially as arrival intensity falls and agent valuations become more volatile. </p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Chain: A Dynamic Double Auction Framework for Matching Patient Agents ">
<meta name="citation_author" content="Bredin, J. L.">
<meta name="citation_author" content="Parkes, D. C.">
<meta name="citation_author" content="Duong, Q.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="133">
<meta name="citation_lastpage" content="179">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2303/live-2303-3506-jair.pdf">

<cite>S.  P. Ponzetto and M.  Strube (2007) "Knowledge Derived From Wikipedia For Computing Semantic Relatedness", Volume 30, pages 181-212</cite>
<cite>Honorable Mention for the 2010 IJCAI-JAIR Best Paper Prize</cite><p class="media"><a href="/media/2308/live-2308-3485-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2308/live-2308-3484-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2308'>doi:10.1613/jair.2308</a></p>
<p>Wikipedia provides a semantic network for computing semantic relatedness in a more structured fashion than a search engine and with more coverage than WordNet. We present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures perform better using Wikipedia than a baseline given by Google counts, and we show that Wikipedia outperforms WordNet on some datasets. We also address the question whether and how Wikipedia can be integrated into NLP applications as a knowledge base. Including Wikipedia improves the performance of a machine learning based coreference resolution system, indicating that it represents a valuable resource for NLP applications. Finally, we show that our method can be easily used for languages other than English by computing semantic relatedness for a German dataset.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Knowledge Derived From Wikipedia For Computing Semantic Relatedness">
<meta name="citation_author" content="Ponzetto, S. P.">
<meta name="citation_author" content="Strube, M.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="181">
<meta name="citation_lastpage" content="212">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2308/live-2308-3485-jair.pdf">

<cite>A.  Felner, R.  E. Korf, R.  Meshulam and R.  C. Holte (2007) "Compressed Pattern Databases", Volume 30, pages 213-247</cite>
<p class="media"><a href="/media/2241/live-2241-3491-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2241/live-2241-3490-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2241'>doi:10.1613/jair.2241</a></p>
<p>A pattern database (PDB) is a heuristic function implemented as a lookup table that stores the lengths of optimal solutions for subproblem instances. Standard PDBs have a distinct entry in the table for each subproblem instance. In this paper we investigate compressing PDBs by merging several entries into one, thereby allowing the use of PDBs that exceed available memory in their uncompressed form. We introduce a number of methods for determining which entries to merge and discuss their relative merits. These vary from domain-independent approaches that allow any set of entries in the PDB to be merged, to more intelligent methods that take into account the structure of the problem. The choice of the best compression method is based on domain-dependent attributes. We present experimental results on a number of combinatorial problems, including the four-peg Towers of Hanoi problem, the sliding-tile puzzles, and the Top-Spin puzzle. For the Towers of Hanoi, we show that the search time can be reduced by up to three orders of magnitude by using compressed PDBs compared to uncompressed PDBs of the same size. More modest improvements were observed for the other domains.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Compressed Pattern Databases">
<meta name="citation_author" content="Felner, A.">
<meta name="citation_author" content="Korf, R. E.">
<meta name="citation_author" content="Meshulam, R.">
<meta name="citation_author" content="Holte, R. C.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="213">
<meta name="citation_lastpage" content="247">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2241/live-2241-3491-jair.pdf">

<cite>A.  McCallum, X.  Wang and A.  Corrada-Emmanuel (2007) "Topic and Role Discovery in Social Networks with Experiments on Enron and Academic Email", Volume 30, pages 249-272</cite>
<p class="media"><a href="/media/2229/live-2229-3495-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2229/live-2229-3498-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2229'>doi:10.1613/jair.2229</a></p>
<p>Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the attributes such as language content or topics on those links. We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the direction-sensitive messages sent between entities. The model builds on Latent Dirichlet Allocation (LDA) and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient---steering the discovery of topics according to the relationships between people. We give results on both the Enron email corpus and a researcher's email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people's roles and gives lower perplexity on previously unseen messages. We also present the Role-Author-Recipient-Topic (RART) model, an extension to ART that explicitly represents people's roles.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Topic and Role Discovery in Social Networks with Experiments on Enron and Academic Email">
<meta name="citation_author" content="McCallum, A.">
<meta name="citation_author" content="Wang, X.">
<meta name="citation_author" content="Corrada-Emmanuel, A.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="249">
<meta name="citation_lastpage" content="272">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2229/live-2229-3495-jair.pdf">

<cite>G.  Stoilos, G.  Stamou, J.  Z. Pan, V.  Tzouvaras and I.  Horrocks (2007) "Reasoning with Very Expressive Fuzzy Description Logics", Volume 30, pages 273-320</cite>
<p class="media"><a href="/media/2279/live-2279-3505-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2279/live-2279-3502-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2279'>doi:10.1613/jair.2279</a></p>
<p>It is widely recognized today that the management of imprecision and vagueness will yield more intelligent and realistic knowledge-based applications. Description Logics (DLs) are a family of knowledge representation languages that have gained considerable attention the last decade, mainly due to their decidability and the existence of empirically high performance of reasoning algorithms. In this paper, we extend the well known fuzzy ALC DL to the fuzzy SHIN DL, which extends the fuzzy ALC DL with transitive role axioms (S), inverse roles (I), role hierarchies (H) and number restrictions (N). We illustrate why transitive role axioms are difficult to handle in the presence of fuzzy interpretations and how to handle them properly. Then we extend these results by adding role hierarchies and finally number restrictions. The main contributions of the paper are the decidability proof of the fuzzy DL languages fuzzy-SI and fuzzy-SHIN, as well as decision procedures for the knowledge base satisfiability problem of the fuzzy-SI and fuzzy-SHIN.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Reasoning with Very Expressive Fuzzy Description Logics">
<meta name="citation_author" content="Stoilos, G.">
<meta name="citation_author" content="Stamou, G.">
<meta name="citation_author" content="Pan, J. Z.">
<meta name="citation_author" content="Tzouvaras, V.">
<meta name="citation_author" content="Horrocks, I.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="273">
<meta name="citation_lastpage" content="320">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2279/live-2279-3505-jair.pdf">

<cite>C.  M. Li, F.  Manya and J.  Planes (2007) "New Inference Rules for Max-SAT", Volume 30, pages 321-359</cite>
<p class="media"><a href="/media/2215/live-2215-3516-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2215/live-2215-3515-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2215'>doi:10.1613/jair.2215</a></p>
<p>Exact Max-SAT solvers, compared with SAT solvers, apply little inference at each node of the proof tree. Commonly used SAT inference rules like unit propagation produce a simplified formula that preserves satisfiability but, unfortunately, solving the Max-SAT problem for the simplified formula is not equivalent to solving it for the original formula. In this paper, we define a number of original inference rules that, besides being applied efficiently, transform Max-SAT instances into equivalent Max-SAT instances which are easier to solve. The soundness of the rules, that can be seen as refinements of unit resolution adapted to Max-SAT, are proved in a novel and simple way via an integer programming transformation. With the aim of finding out how powerful the inference rules are in practice, we have developed a new Max-SAT solver, called MaxSatz, which incorporates those rules, and performed  an experimental investigation. The results provide empirical evidence that MaxSatz is very competitive, at least, on random Max-2SAT, random Max-3SAT, Max-Cut, and Graph 3-coloring instances, as well as on the benchmarks from the Max-SAT Evaluation 2006.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="New Inference Rules for Max-SAT">
<meta name="citation_author" content="Li, C. M.">
<meta name="citation_author" content="Manya, F.">
<meta name="citation_author" content="Planes, J.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="321">
<meta name="citation_lastpage" content="359">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2215/live-2215-3516-jair.pdf">

<cite>J.  Bell (2007) "Natural Events", Volume 30, pages 361-412</cite>
<p class="media"><a href="/media/2383/live-2383-3533-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2383/live-2383-3532-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2383'>doi:10.1613/jair.2383</a></p>
<p>This paper develops an inductive theory of predictive common sense reasoning. The theory  provides the basis for an integrated solution to the three traditional problems of reasoning about change;  the frame, qualification, and ramification problems. The theory is also capable of representing non-deterministic events, and it provides a means for stating defeasible preferences over the outcomes of conflicting simultaneous events. </p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Natural Events">
<meta name="citation_author" content="Bell, J.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="361">
<meta name="citation_lastpage" content="412">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2383/live-2383-3533-jair.pdf">

<cite>M.  A. Walker, A.  Stent, F.  Mairesse and R.  Prasad (2007) "Individual and Domain Adaptation in Sentence Planning for Dialogue", Volume 30, pages 413-456</cite>
<p class="media"><a href="/media/2329/live-2329-3557-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2329/live-2329-3556-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2329'>doi:10.1613/jair.2329</a></p>
<p>One of the biggest challenges in the development and deployment of spoken dialogue systems is the design of the spoken language generation module. This challenge arises from the need for the generator to adapt to many features of the dialogue domain, user population, and dialogue context.  A promising approach is trainable generation, which uses general-purpose linguistic knowledge that is automatically adapted to the features of interest, such as the application domain, individual user, or user group.  In this paper we present and evaluate a trainable sentence planner for providing restaurant information in the MATCH dialogue system.  We show that trainable sentence planning can produce complex information presentations whose quality is comparable to the output of a template-based generator tuned to this domain.  We also show that our method easily supports adapting the sentence planner to individuals, and that the individualized sentence planners generally perform better than models trained and tested on a population of individuals. Previous work has documented and utilized individual preferences for content selection, but to our knowledge, these results provide the first demonstration of individual preferences for sentence planning operations, affecting the content order, discourse structure and sentence structure of system responses. Finally, we evaluate the contribution of different feature sets, and show that, in our application, n-gram features often do as well as features based on higher-level linguistic representations.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Individual and Domain Adaptation in Sentence Planning for Dialogue">
<meta name="citation_author" content="Walker, M. A.">
<meta name="citation_author" content="Stent, A.">
<meta name="citation_author" content="Mairesse, F.">
<meta name="citation_author" content="Prasad, R.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="413">
<meta name="citation_lastpage" content="456">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2329/live-2329-3557-jair.pdf">

<cite>F.  Mairesse, M.  A. Walker, M.  R. Mehl and R.  K. Moore (2007) "Using Linguistic Cues for the Automatic Recognition of Personality in Conversation and Text", Volume 30, pages 457-500</cite>
<p class="media"><a href="/media/2349/live-2349-3562-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2349/live-2349-3561-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2349'>doi:10.1613/jair.2349</a></p>
<p>It is well known that utterances convey a great deal of information about the speaker in addition to their semantic content.  One such type of information consists of cues to the speaker's personality traits, the most fundamental dimension of variation between humans.  Recent work explores the automatic detection of other types of pragmatic variation in text and conversation, such as emotion, deception, speaker charisma, dominance, point of view, subjectivity, opinion and sentiment. Personality affects these other aspects of linguistic production, and thus personality recognition may be useful for these tasks, in addition to many other potential applications.  However, to date, there is little work on the automatic recognition of personality traits.  This article reports experimental results for recognition of all Big Five personality traits, in both conversation and text, utilising both self and observer ratings of personality.  While other work reports classification results, we experiment with classification, regression and ranking models. For each model, we analyse the effect of different feature sets on accuracy. Results show that for some traits, any type of statistical model performs significantly better than the baseline, but ranking models perform best overall. We also present an experiment suggesting that ranking models are more accurate than multi-class classifiers for modelling personality. In addition, recognition models trained on observed personality perform better than models trained using self-reports, and the optimal feature set depends on the personality trait. A qualitative analysis of the learned models confirms previous findings linking language and personality, while revealing many new linguistic markers.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Using Linguistic Cues for the Automatic Recognition of Personality in Conversation and Text">
<meta name="citation_author" content="Mairesse, F.">
<meta name="citation_author" content="Walker, M. A.">
<meta name="citation_author" content="Mehl, M. R.">
<meta name="citation_author" content="Moore, R. K.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="457">
<meta name="citation_lastpage" content="500">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2349/live-2349-3562-jair.pdf">

<cite>S.  Greco, I.  Trubitsyna and E.  Zumpano (2007) "On the Semantics of Logic Programs with Preferences", Volume 30, pages 501-523</cite>
<p class="media"><a href="/media/2371/live-2371-3586-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2371/live-2371-3585-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2371'>doi:10.1613/jair.2371</a></p>
<p>This work is a contribution to prioritized reasoning in logic programming in the presence of preference relations involving atoms. The  technique, providing a new interpretation for prioritized logic programs, is inspired by the semantics of Prioritized Logic Programming and enriched with the use of structural information of preference of Answer Set Optimization Programming. Specifically, the analysis  of the logic program is carried out together with the analysis of preferences in order to determine the choice order and the sets of comparable models. The new semantics is compared with other approaches known in the literature and complexity analysis is also performed, showing that, with respect to other similar approaches previously proposed, the complexity of computing preferred stable models does not increase.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="On the Semantics of Logic Programs with Preferences">
<meta name="citation_author" content="Greco, S.">
<meta name="citation_author" content="Trubitsyna, I.">
<meta name="citation_author" content="Zumpano, E.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="501">
<meta name="citation_lastpage" content="523">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2371/live-2371-3586-jair.pdf">

<cite>S.  I. Hill and A.  Doucet (2007) "A Framework for Kernel-Based Multi-Category Classification", Volume 30, pages 525-564</cite>
<p class="media"><a href="/media/2251/live-2251-3590-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2251/live-2251-3589-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2251'>doi:10.1613/jair.2251</a></p>
<p>A geometric framework for understanding multi-category classification is introduced, through which many existing 'all-together' algorithms can be understood.  The structure enables parsimonious optimisation, through a direct extension of the binary methodology.  The focus is on Support Vector Classification, with parallels drawn to related methods.<br />
<br />
The ability of the framework to compare algorithms is illustrated by a brief discussion of Fisher consistency.  Its utility in improving understanding of multi-category analysis is demonstrated through a derivation of improved generalisation bounds.<br />
<br />
It is also described how this architecture provides insights regarding how to further improve on the speed of existing multi-category classification algorithms.  An initial example of how this might be achieved is developed in the formulation of a straightforward multi-category Sequential Minimal Optimisation algorithm.  Proof-of-concept experimental results have shown that this, combined with the mapping of pairwise results, is comparable with benchmark optimisation speeds.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="A Framework for Kernel-Based Multi-Category Classification">
<meta name="citation_author" content="Hill, S. I.">
<meta name="citation_author" content="Doucet, A.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="525">
<meta name="citation_lastpage" content="564">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2251/live-2251-3590-jair.pdf">

<cite>C.  Domshlak and J.  Hoffmann (2007) "Probabilistic Planning via Heuristic Forward Search and Weighted Model Counting", Volume 30, pages 565-620</cite>
<p class="media"><a href="/media/2289/live-2289-3600-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2289/live-2289-3601-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2289'>doi:10.1613/jair.2289</a></p>
<p>We present a new algorithm for probabilistic planning with no observability.  Our algorithm, called  Probabilistic-FF, extends the heuristic forward-search machinery of Conformant-FF to problems with probabilistic uncertainty about both the initial state and action effects. Specifically,  Probabilistic-FF combines Conformant-FF's techniques with a powerful machinery for weighted model counting in (weighted) CNFs, serving to elegantly define both the search space and the heuristic function. Our evaluation of  Probabilistic-FF shows its fine scalability in a range of probabilistic domains, constituting a several orders of magnitude improvement over previous results in this area. We use a problematic case to point out the main open issue to be addressed by further research.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Probabilistic Planning via Heuristic Forward Search and Weighted Model Counting">
<meta name="citation_author" content="Domshlak, C.">
<meta name="citation_author" content="Hoffmann, J.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="565">
<meta name="citation_lastpage" content="620">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2289/live-2289-3600-jair.pdf">

<cite>I.  Bhattacharya and L.  Getoor (2007) "Query-time Entity Resolution", Volume 30, pages 621-657</cite>
<p class="media"><a href="/media/2290/live-2290-3617-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2290/live-2290-3618-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2290'>doi:10.1613/jair.2290</a></p>
<p>Entity resolution is the problem of reconciling database references corresponding to the same real-world entities. Given the abundance of publicly available databases that have unresolved entities, we motivate the problem of query-time entity resolution quick and accurate resolution for answering queries over such `unclean' databases at query-time.  Since collective entity resolution approaches --- where related references are resolved jointly --- have been shown to be more accurate than independent attribute-based resolution for off-line entity resolution, we focus on developing new algorithms for collective resolution for answering entity resolution queries at query-time.  For this purpose, we first formally show that, for collective resolution, precision and recall for individual entities follow a geometric progression as neighbors at increasing distances are considered. Unfolding this progression leads naturally to a two stage `expand and resolve' query processing strategy. In this strategy, we first extract the related records for a query using two novel expansion operators, and then resolve the extracted records collectively. We then show how the same strategy can be adapted for query-time entity resolution by identifying and resolving only those database references that are the most helpful for processing the query. We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing. We then show how the same queries can be answered in real-time using our adaptive approach while preserving the gains of collective resolution. In addition to experiments on real datasets, we use synthetically generated data to empirically demonstrate the validity of the performance trends predicted by our analysis of collective entity resolution over a wide range of structural characteristics in the data.</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Query-time Entity Resolution">
<meta name="citation_author" content="Bhattacharya, I.">
<meta name="citation_author" content="Getoor, L.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="621">
<meta name="citation_lastpage" content="657">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2290/live-2290-3617-jair.pdf">

<cite>I.  Szita and A.  Lorincz (2007) "Learning to Play Using Low-Complexity Rule-Based Policies: Illustrations through Ms. Pac-Man", Volume 30, pages 659-684</cite>
<p class="media"><a href="/media/2368/live-2368-3623-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2368/live-2368-3622-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2368'>doi:10.1613/jair.2368</a></p>
<p>In this article we propose a method that can deal with certain combinatorial reinforcement learning tasks. We demonstrate the approach in the popular Ms. Pac-Man game. We define a set of high-level observation and action modules, from which rule-based policies are constructed automatically. In these policies, actions are temporally extended, and may work concurrently. The policy of the agent is encoded by a compact decision list. The components of the list are selected from a large pool of rules,  which can be either hand-crafted or generated automatically. A suitable selection of rules is learnt  by the cross-entropy method, a recent global optimization algorithm that fits our framework smoothly.  Cross-entropy-optimized policies perform better than our hand-crafted policy, and reach the score of average human players. We argue that learning is successful mainly because (i) policies may apply concurrent actions and thus the policy space is sufficiently rich, (ii) the search is biased towards low-complexity policies and therefore,  solutions with a compact description can be found quickly if they exist.<br />
</p>
<a href="/vol/vol30.html">Click here to return to Volume 30 contents list</a>
<meta name="citation_title" content="Learning to Play Using Low-Complexity Rule-Based Policies: Illustrations through Ms. Pac-Man">
<meta name="citation_author" content="Szita, I.">
<meta name="citation_author" content="Lorincz, A.">
<meta name="citation_publication_date" content="2007">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="659">
<meta name="citation_lastpage" content="684">
<meta name="citation_volume" content="30">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2368/live-2368-3623-jair.pdf">
