<cite>M.  Cadoli,  F.  M. Donini,  P.  Liberatore and  M.  Schaerf (2000) "Space Efficiency of Propositional Knowledge Representation Formalisms", Volume 13, pages 1-31</cite>
<p class="media"><a href="/media/664/live-664-1854-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/664/live-664-1852-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume13/cadoli00a-html/cadoli00a.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.664'>doi:10.1613/jair.664</a></p>
<p>We investigate the space efficiency of a Propositional    Knowledge Representation (PKR) formalism. Intuitively, the space    efficiency of a formalism F in representing a certain piece of    knowledge A, is the size of the shortest formula of F that    represents A. In this paper we assume that knowledge is    either a set of propositional interpretations (models) or a set of    propositional formulae (theorems). We provide a formal way of    talking about the relative ability of PKR formalisms to compactly    represent a set of models or a set of theorems. We introduce two new    compactness measures, the corresponding classes, and show that the    relative space efficiency of a PKR formalism in representing    models/theorems is directly related to such classes. In particular,    we consider formalisms for nonmonotonic reasoning, such as    circumscription and default logic, as well as belief revision    operators and the stable model semantics for logic programs with    negation. One interesting result is that formalisms with the same    time complexity do not necessarily belong to the same space    efficiency class.</p>
<a href="/vol/vol13.html">Click here to return to Volume 13 contents list</a>
<meta name="citation_title" content="Space Efficiency of Propositional Knowledge Representation Formalisms">
<meta name="citation_author" content="Cadoli,  M.">
<meta name="citation_author" content="Donini,  F. M.">
<meta name="citation_author" content="Liberatore,  P.">
<meta name="citation_author" content="Schaerf,  M.">
<meta name="citation_publication_date" content="2000">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="31">
<meta name="citation_volume" content="13">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/664/live-664-1854-jair.pdf">

<cite>M.  Hauskrecht (2000) "Value-Function Approximations for Partially Observable Markov Decision Processes", Volume 13, pages 33-94</cite>
<p class="media"><a href="/media/678/live-678-1858-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/678/live-678-1856-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.678'>doi:10.1613/jair.678</a></p>
<p>Partially observable Markov decision processes (POMDPs)    provide an elegant mathematical framework for modeling complex    decision and planning problems in stochastic domains in which states    of the system are observable only indirectly, via a set of imperfect    or noisy observations. The modeling advantage of POMDPs, however,    comes at a price -- exact methods for solving them are computationally    very expensive and thus applicable in practice only to very simple    problems. We focus on efficient approximation (heuristic) methods that    attempt to alleviate the computational problem and trade off accuracy    for speed. We have two objectives here. First, we survey various    approximation methods, analyze their properties and relations and    provide some new insights into their differences. Second, we present a    number of new approximation methods and novel refinements of existing    techniques. The theoretical results are supported by experiments on a    problem from the agent navigation domain.</p>
<a href="/vol/vol13.html">Click here to return to Volume 13 contents list</a>
<meta name="citation_title" content="Value-Function Approximations for Partially Observable Markov Decision Processes">
<meta name="citation_author" content="Hauskrecht,  M.">
<meta name="citation_publication_date" content="2000">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="33">
<meta name="citation_lastpage" content="94">
<meta name="citation_volume" content="13">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/678/live-678-1858-jair.pdf">

<cite>D.  F. Gordon (2000) "Asimovian Adaptive Agents", Volume 13, pages 95-153</cite>
<p class="media"><a href="/media/720/live-720-1895-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/720/live-720-1893-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.720'>doi:10.1613/jair.720</a></p>
<p>The goal of this research is to develop agents that     are adaptive and predictable and timely. At first blush,     these three requirements seem contradictory. For example,      adaptation risks introducing undesirable side effects,     thereby making agents' behavior less predictable. Furthermore,    although formal verification can assist in ensuring    behavioral predictability, it is known to be time-consuming. <p> Our solution to the challenge of satisfying all three    requirements is the following. Agents have finite-state    automaton plans, which are adapted online via evolutionary    learning (perturbation) operators. To ensure that critical    behavioral constraints are always satisfied, agents' plans    are first formally verified. They are then reverified after    every adaptation. If reverification concludes that constraints    are violated, the plans are repaired. The main objective of     this paper is to improve the efficiency of reverification     after learning, so that agents have a sufficiently rapid     response time. We present two solutions: positive results     that certain learning operators are a priori guaranteed to    preserve useful classes of behavioral assurance constraints    (which implies that no reverification is needed for these     operators), and efficient incremental reverification algorithms     for those learning operators that have negative a priori results.</p>
<a href="/vol/vol13.html">Click here to return to Volume 13 contents list</a>
<meta name="citation_title" content="Asimovian Adaptive Agents">
<meta name="citation_author" content="Gordon,  D. F.">
<meta name="citation_publication_date" content="2000">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="95">
<meta name="citation_lastpage" content="153">
<meta name="citation_volume" content="13">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/720/live-720-1895-jair.pdf">

<cite>J.  Cheng and  M.  J. Druzdzel (2000) "AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential Reasoning in Large Bayesian Networks", Volume 13, pages 155-188</cite>
<cite>Honorable Mention for the 2005 IJCAI-JAIR Best Paper Prize</cite><p class="media"><a href="/media/764/live-764-1924-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/764/live-764-1922-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume13/cheng00a-html/cheng00a-html.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.764'>doi:10.1613/jair.764</a></p>
<p>Stochastic sampling algorithms, while an attractive alternative to    exact algorithms in very large Bayesian network models, have been    observed to perform poorly in evidential reasoning with extremely    unlikely evidence. To address this problem, we propose an adaptive    importance sampling algorithm, AIS-BN, that shows promising    convergence rates even under extreme conditions and seems to    outperform the existing sampling algorithms consistently. Three    sources of this performance improvement are (1) two heuristics for    initialization of the importance function that are based on the    theoretical properties of importance sampling in finite-dimensional    integrals and the structural advantages of Bayesian networks, (2) a    smooth learning method for the importance function, and (3) a dynamic    weighting function for combining samples from different stages of the    algorithm.    <p>   We tested the performance of the AIS-BN algorithm along with two state    of the art general purpose sampling algorithms, likelihood weighting    (Fung & Chang, 1989; Shachter & Peot, 1989) and self-importance    sampling (Shachter & Peot, 1989). We used in our tests three large    real Bayesian network models available to the scientific community:    the CPCS network (Pradhan et al., 1994), the PathFinder network    (Heckerman, Horvitz, & Nathwani, 1990), and the ANDES network (Conati,    Gertner, VanLehn, & Druzdzel, 1997), with evidence as unlikely as    10^-41. While the AIS-BN algorithm always performed better than the    other two algorithms, in the majority of the test cases it achieved    orders of magnitude improvement in precision of the results.    Improvement in speed given a desired precision is even more dramatic,    although we are unable to report numerical results here, as the other    algorithms almost never achieved the precision reached even by the    first few iterations of the AIS-BN algorithm.</p>
<a href="/vol/vol13.html">Click here to return to Volume 13 contents list</a>
<meta name="citation_title" content="AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential Reasoning in Large Bayesian Networks">
<meta name="citation_author" content="Cheng,  J.">
<meta name="citation_author" content="Druzdzel,  M. J.">
<meta name="citation_publication_date" content="2000">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="155">
<meta name="citation_lastpage" content="188">
<meta name="citation_volume" content="13">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/764/live-764-1924-jair.pdf">

<cite>R.  M. Jensen and  M.  M. Veloso (2000) "OBDD-based Universal Planning for Synchronized Agents in Non-Deterministic Domains", Volume 13, pages 189-226</cite>
<p class="media"><a href="/media/649/live-649-1847-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/649/live-649-1845-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.649'>doi:10.1613/jair.649</a>
<br/><a href="/media/649/live-649-1848-jair.tar.gz">Appendix </a> - UMOP Planner</p>
<p>Recently model checking representation and search techniques    were shown to be efficiently applicable to planning, in particular to    non-deterministic planning. Such planning approaches use Ordered    Binary Decision Diagrams (OBDDs) to encode a planning domain as a    non-deterministic finite automaton and then apply fast algorithms from    model checking to search for a solution. OBDDs can effectively scale    and can provide universal plans for complex planning domains. We are    particularly interested in addressing the complexities arising in    non-deterministic, multi-agent domains.  In this article, we present    UMOP, a new universal OBDD-based planning framework for    non-deterministic, multi-agent domains. We introduce a new planning    domain description language, NADL, to specify non-deterministic,    multi-agent domains.  The language contributes the explicit definition    of controllable agents and uncontrollable environment agents. We    describe the syntax and semantics of NADL and show how to build an    efficient OBDD-based representation of an NADL description.  The UMOP    planning system uses NADL and different OBDD-based universal planning    algorithms. It includes the previously developed strong and strong    cyclic planning algorithms. In addition, we introduce our new    optimistic planning algorithm that relaxes optimality guarantees and    generates plausible universal plans in some domains where no strong    nor strong cyclic solution exists. We present empirical results    applying UMOP to domains ranging from deterministic and single-agent    with no environment actions to non-deterministic and multi-agent with    complex environment actions. UMOP is shown to be a rich and efficient    planning system.</p>
<a href="/vol/vol13.html">Click here to return to Volume 13 contents list</a>
<meta name="citation_title" content="OBDD-based Universal Planning for Synchronized Agents in Non-Deterministic Domains">
<meta name="citation_author" content="Jensen,  R. M.">
<meta name="citation_author" content="Veloso,  M. M.">
<meta name="citation_publication_date" content="2000">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="189">
<meta name="citation_lastpage" content="226">
<meta name="citation_volume" content="13">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/649/live-649-1847-jair.pdf">

<cite>T.  G. Dietterich (2000) "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition", Volume 13, pages 227-303</cite>
<cite>2003 IJCAI-JAIR Best Paper Prize</cite><p class="media"><a href="/media/639/live-639-1834-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/639/live-639-1832-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.639'>doi:10.1613/jair.639</a></p>
<p>This paper presents a new approach to hierarchical    reinforcement learning based on decomposing the target Markov decision    process (MDP) into a hierarchy of smaller MDPs and decomposing the    value function of the target MDP into an additive combination of the    value functions of the smaller MDPs.  The decomposition, known as the    MAXQ decomposition, has both a procedural semantics---as a subroutine    hierarchy---and a declarative semantics---as a representation of the    value function of a hierarchical policy.  MAXQ unifies and extends    previous work on hierarchical reinforcement learning by Singh,    Kaelbling, and Dayan and Hinton.  It is based on the assumption that    the programmer can identify useful subgoals and define subtasks that    achieve these subgoals.  By defining such subgoals, the programmer    constrains the set of policies that need to be considered during    reinforcement learning.  The MAXQ value function decomposition can    represent the value function of any policy that is consistent with the    given hierarchy.  The decomposition also creates opportunities to    exploit state abstractions, so that individual MDPs within the    hierarchy can ignore large parts of the state space.  This is    important for the practical application of the method.  This paper    defines the MAXQ hierarchy, proves formal results on its    representational power, and establishes five conditions for the safe    use of state abstractions.  The paper presents an online model-free    learning algorithm, MAXQ-Q, and proves that it converges with    probability 1 to a kind of locally-optimal policy known as a    recursively optimal policy, even in the presence of the five kinds of    state abstraction.  The paper evaluates the MAXQ representation and    MAXQ-Q through a series of experiments in three domains and shows    experimentally that MAXQ-Q (with state abstractions) converges to a    recursively optimal policy much faster than flat Q learning.  The fact    that MAXQ learns a representation of the value function has an    important benefit: it makes it possible to compute and execute an    improved, non-hierarchical policy via a procedure similar to the    policy improvement step of policy iteration.  The paper demonstrates    the effectiveness of this non-hierarchical execution experimentally.    Finally, the paper concludes with a comparison to related work and a    discussion of the design tradeoffs in hierarchical reinforcement learning.</p>
<a href="/vol/vol13.html">Click here to return to Volume 13 contents list</a>
<meta name="citation_title" content="Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition">
<meta name="citation_author" content="Dietterich,  T. G.">
<meta name="citation_publication_date" content="2000">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="227">
<meta name="citation_lastpage" content="303">
<meta name="citation_volume" content="13">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/639/live-639-1834-jair.pdf">

<cite>A.  Cimatti and  M.  Roveri (2000) "Conformant Planning via Symbolic Model Checking", Volume 13, pages 305-338</cite>
<p class="media"><a href="/media/774/live-774-1928-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/774/live-774-1926-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://sra.itc.it/research/conformant_planning/index.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.774'>doi:10.1613/jair.774</a>
<br/><a href="/media/774/live-774-1929-jair.tar.gz">Appendix </a> - CMBP Planner</p>
<p>We tackle the problem of planning in nondeterministic    domains, by presenting a new approach to conformant planning.    Conformant planning is the problem of finding a sequence of actions    that is guaranteed to achieve the goal despite the nondeterminism of    the domain. Our approach is based on the representation of the    planning domain as a finite state automaton. We use Symbolic Model    Checking techniques, in particular Binary Decision Diagrams, to    compactly represent and efficiently search the automaton. In this    paper we make the following contributions. First, we present a general    planning algorithm for conformant planning, which applies to fully    nondeterministic domains, with uncertainty in the initial condition    and in action effects. The algorithm is based on a breadth-first,    backward search, and returns conformant plans of minimal length, if a    solution to the planning problem exists, otherwise it terminates    concluding that the problem admits no conformant solution. Second, we    provide a symbolic representation of the search space based on Binary    Decision Diagrams (BDDs), which is the basis for search techniques    derived from symbolic model checking. The symbolic representation    makes it possible to analyze potentially large sets of states and    transitions in a single computation step, thus providing for an    efficient implementation.  Third, we present CMBP (Conformant Model    Based Planner), an efficient implementation of the data structures and    algorithm described above, directly based on BDD manipulations, which    allows for a compact representation of the search layers and an    efficient implementation of the search steps. Finally, we present an    experimental comparison of our approach with the state-of-the-art    conformant planners CGP, QBFPLAN and GPT. Our analysis includes all    the planning problems from the distribution packages of these systems,    plus other problems defined to stress a number of specific factors.    Our approach appears to be the most effective: CMBP is strictly more    expressive than QBFPLAN and CGP and, in all the problems where a    comparison is possible, CMBP outperforms its competitors, sometimes by    orders of magnitude.</p>
<a href="/vol/vol13.html">Click here to return to Volume 13 contents list</a>
<meta name="citation_title" content="Conformant Planning via Symbolic Model Checking">
<meta name="citation_author" content="Cimatti,  A.">
<meta name="citation_author" content="Roveri,  M.">
<meta name="citation_publication_date" content="2000">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="305">
<meta name="citation_lastpage" content="338">
<meta name="citation_volume" content="13">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/774/live-774-1928-jair.pdf">
