<cite>S.  K. Murthy,  S.  Kasif and  S.  Salzberg (1994) "A System for Induction of Oblique Decision Trees", Volume 2, pages 1-32</cite>
<p class="media"><a href="/media/63/live-63-1401-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/63/live-63-1400-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.63'>doi:10.1613/jair.63</a>
<br/><a href="/media/63/live-63-1402-jair.tar.Z">Appendix 1</a> - Source code for OC1&nbsp;|&nbsp;<a href="/media/63/live-63-2586-jair.htm">Appendix 2</a> - Papers referencing OC1</p>
<p>This article describes a new system for induction ofoblique   decision trees.  This system, OC1, combines deterministic   hill-climbing with two forms of randomization to find a goodoblique   split (in the form of a hyperplane) at each node of a decisiontree.   Oblique decision tree methods are tuned especially for domains in   which the attributes are numeric, although they can be adapted to   symbolic or mixed symbolic/numeric attributes.  We presentextensive   empirical studies, using both real and artificial data, thatanalyze   OC1's ability to construct oblique trees that are smaller and more   accurate than their axis-parallel counterparts.  We also examinethe   benefits of randomization for the construction of oblique decisiontrees.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="A System for Induction of Oblique Decision Trees">
<meta name="citation_author" content="Murthy,  S. K.">
<meta name="citation_author" content="Kasif,  S.">
<meta name="citation_author" content="Salzberg,  S.">
<meta name="citation_publication_date" content="1994">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="32">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/63/live-63-1401-jair.pdf">

<cite>A.  J. Grove,  J.  Y. Halpern and  D.  Koller (1994) "Random Worlds and Maximum Entropy", Volume 2, pages 33-88</cite>
<p class="media"><a href="/media/61/live-61-1396-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/61/live-61-1394-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.61'>doi:10.1613/jair.61</a></p>
<p>Given a knowledge base <i>KB</i> containing first-order and statistical facts, we consider a principled method, called the <i>random-worlds method</i>, for computing a degree of belief that some formula <i>Phi</i> holds given <i>KB</i>.  If  we are reasoning about a world or system consisting of <i>N</i> individuals, then we can consider all possible worlds, or first-order models, withdomain <i>{1,...,N}</i> that satisfy <i>KB</i>, and compute thefraction of them in which <i>Phi</i> is true. We define the degree of belief to be the asymptotic value of this fraction as <i>N</i> grows large.  We show that when the vocabulary underlying <i>Phi</i> and<i>KB</i> uses constants and unary predicates only, we can naturally associate an <i>entropy</i> with each world. As <i>N</i> grows larger,there are many more worlds with higher entropy.  Therefore, we can usea <i> maximum-entropy</i> computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general.  Of equal interest to the result itself are the limitations on its scope.  Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case.  These observations suggest unexpected limitations to the applicability of maximum-entropy methods.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Random Worlds and Maximum Entropy">
<meta name="citation_author" content="Grove,  A. J.">
<meta name="citation_author" content="Halpern,  J. Y.">
<meta name="citation_author" content="Koller,  D.">
<meta name="citation_publication_date" content="1994">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="33">
<meta name="citation_lastpage" content="88">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/61/live-61-1396-jair.pdf">

<cite>T.  Kitani,  Y.  Eriguchi and  M.  Hara (1994) "Pattern Matching and Discourse Processing in Information Extraction   from Japanese Text", Volume 2, pages 89-110</cite>
<p class="media"><a href="/media/53/live-53-1391-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/53/live-53-1390-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.53'>doi:10.1613/jair.53</a></p>
<p>Information extraction is the task of automaticallypicking   up information of interest from an unconstrained text.  Informationof   interest is usually extracted in two steps.  First, sentence level   processing locates relevant pieces of information scatteredthroughout   the text; second, discourse processing merges coreferential   information to generate the output.  In the first step, pieces of   information are locally identified without recognizing any   relationships among them.  A key word search or simple patternsearch   can achieve this purpose.  The second step requires deeperknowledge   in order to understand relationships among separately identified   pieces of information.  Previous information extraction systems   focused on the first step, partly because they were not required to   link up each piece of information with other pieces.  To link the   extracted pieces of information and map them onto a structuredoutput   format, complex discourse processing is essential.  This paperreports   on a Japanese information extraction system that merges information   using a pattern matcher and discourse processor.  Evaluationresults   show a high level of system performance which approaches human   performance.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Pattern Matching and Discourse Processing in Information Extraction   from Japanese Text">
<meta name="citation_author" content="Kitani,  T.">
<meta name="citation_author" content="Eriguchi,  Y.">
<meta name="citation_author" content="Hara,  M.">
<meta name="citation_publication_date" content="1994">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="89">
<meta name="citation_lastpage" content="110">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/53/live-53-1391-jair.pdf">

<cite>S.  Safra and  M.  Tennenholtz (1994) "On Planning while Learning", Volume 2, pages 111-129</cite>
<p class="media"><a href="/media/51/live-51-1389-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/51/live-51-1388-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.51'>doi:10.1613/jair.51</a></p>
<p>This paper introduces a framework for Planning while   Learning where an agent is given a goal to achieve in anenvironment   whose behavior is only partially known to the agent.      We discuss the tractability of various plan-design processes. We   show that for a large natural class of Planning while Learning   systems, a plan can be presented and verified in a reasonable time.   However, coming up algorithmically with a plan, even for simple   classes of systems is apparently intractable.      We emphasize the role of off-line plan-design processes, andshow   that, in most natural cases, the verification (projection) part canbe   carried out in an efficient algorithmic manner.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="On Planning while Learning">
<meta name="citation_author" content="Safra,  S.">
<meta name="citation_author" content="Tennenholtz,  M.">
<meta name="citation_publication_date" content="1994">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="111">
<meta name="citation_lastpage" content="129">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/51/live-51-1389-jair.pdf">

<cite>S.  Soderland and  W.  Lehnert (1994) "Wrap-Up: a Trainable Discourse Module for Information Extraction", Volume 2, pages 131-158</cite>
<p class="media"><a href="/media/68/live-68-1409-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/68/live-68-1408-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.68'>doi:10.1613/jair.68</a></p>
<p>The vast amounts of on-line text now available have ledto   renewed interest in information extraction (IE) systems thatanalyze   unrestricted text, producing a structured representation ofselected   information from the text. This paper presents a novel approachthat   uses machine learning to acquire knowledge for some of the higher   level IE processing.  Wrap-Up is a trainable IE discourse component   that makes intersentential inferences and identifies logicalrelations   among information extracted from the text.  Previous corpus-based   approaches were limited to lower level processing such as   part-of-speech tagging, lexical disambiguation, and dictionary   construction.  Wrap-Up is fully trainable, and not onlyautomatically   decides what classifiers are needed, but even derives the featureset   for each classifier automatically. Performance equals that of a   partially trainable discourse module requiring manual customization   for each domain.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Wrap-Up: a Trainable Discourse Module for Information Extraction">
<meta name="citation_author" content="Soderland,  S.">
<meta name="citation_author" content="Lehnert,  W.">
<meta name="citation_publication_date" content="1994">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="131">
<meta name="citation_lastpage" content="158">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/68/live-68-1409-jair.pdf">

<cite>W.  L. Buntine (1994) "Operations for Learning with Graphical Models", Volume 2, pages 159-225</cite>
<p class="media"><a href="/media/62/live-62-1399-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/62/live-62-1397-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.62'>doi:10.1613/jair.62</a></p>
<p>This paper is a multidisciplinary review of empirical,  statistical learning from a graphical model perspective.  Well-known  examples of graphical models include Bayesian networks, directed  graphs representing a Markov chain, and undirected networks  representing a Markov field.  These graphical models are extended to  model data analysis and empirical learning using the notation of  plates.  Graphical operations for simplifying and manipulating a  problem are provided including decomposition, differentiation, andthe  manipulation of probability models from the exponential family.  Two  standard algorithm schemas for learning are reviewed in a graphical  framework: Gibbs sampling and the expectation maximizationalgorithm.  Using these operations and schemas, some popular algorithms can be  synthesized from their graphical specification.  This includes  versions of linear regression, techniques for feed-forward networks,  and learning Gaussian and discrete Bayesian networks from data.  The  paper concludes by sketching some implications for data analysis and  summarizing how some popular algorithms fall within the framework  presented.  The main original contributions here are the decompositiontechniques  and the demonstration that graphical models provide a framework for  understanding and developing complex learning algorithms.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Operations for Learning with Graphical Models">
<meta name="citation_author" content="Buntine,  W. L.">
<meta name="citation_publication_date" content="1994">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="159">
<meta name="citation_lastpage" content="225">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/62/live-62-1399-jair.pdf">

<cite>S.  Minton,  J.  Bresina and  M.  Drummond (1994) "Total-Order and Partial-Order Planning: A Comparative Analysis", Volume 2, pages 227-262</cite>
<p class="media"><a href="/media/64/live-64-1405-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/64/live-64-1404-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.washington.edu/research/jair/volume2/minton94a-html/paper.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.64'>doi:10.1613/jair.64</a>
<br/><a href="/media/64/live-64-1407-jair.tar.Z">Appendix </a> - containing source code and data</p>
<p>For many years, the intuitions underlying partial-order   planning were largely taken for granted. Only in the past few years   has there been renewed interest in the fundamental principles   underlying this paradigm.  In this paper, we present a rigorous   comparative analysis of partial-order and total-order planning by   focusing on two specific planners that can be directly compared. We   show that there are some subtle assumptions that underly the   wide-spread intuitions regarding the supposed efficiency of   partial-order planning. For instance, the superiority ofpartial-order   planning can depend critically upon the search strategy and the   structure of the search space.  Understanding the underlying   assumptions is crucial for constructing efficient planners.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Total-Order and Partial-Order Planning: A Comparative Analysis">
<meta name="citation_author" content="Minton,  S.">
<meta name="citation_author" content="Bresina,  J.">
<meta name="citation_author" content="Drummond,  M.">
<meta name="citation_publication_date" content="1994">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="227">
<meta name="citation_lastpage" content="262">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/64/live-64-1405-jair.pdf">

<cite>T.  G. Dietterich and  G.  Bakiri (1995) "Solving Multiclass Learning Problems via Error-Correcting Output Codes", Volume 2, pages 263-286</cite>
<p class="media"><a href="/media/105/live-105-1426-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/105/live-105-1425-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.105'>doi:10.1613/jair.105</a></p>
<p>Multiclass learning problems involve finding a definitionfor an unknown function <i>f</i>(x) whose range is a discrete setcontaining <i>k</i> > 2 values (i.e., <i>k</i> ``classes'').  Thedefinition is acquired by studying collections of training examples ofthe form [x_i, <i> f </i>(x_i)].  Existing approaches tomulticlass learning problems include direct application of multiclassalgorithms such as the decision-tree algorithms C4.5 and CART,application of binary concept learning algorithms to learn individualbinary functions for each of the <i> k </i> classes, and application ofbinary concept learning algorithms with distributed outputrepresentations.  This paper compares these three approaches to a newtechnique in which error-correcting codes are employed as adistributed output representation.  We show that these outputrepresentations improve the generalization performance of both C4.5and backpropagation on a wide range of multiclass learning tasks.  Wealso demonstrate that this approach is robust with respect to changesin the size of the training sample, the assignment of distributedrepresentations to particular classes, and the application ofoverfitting avoidance techniques such as decision-tree pruning.Finally, we show that---like the other methods---the error-correctingcode technique can provide reliable class probability estimates.Taken together, these results demonstrate that error-correcting outputcodes provide a general-purpose method for improving the performanceof inductive learning programs on multiclass problems.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Solving Multiclass Learning Problems via Error-Correcting Output Codes">
<meta name="citation_author" content="Dietterich,  T. G.">
<meta name="citation_author" content="Bakiri,  G.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="263">
<meta name="citation_lastpage" content="286">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/105/live-105-1426-jair.pdf">

<cite>P.  Cichosz (1995) "Truncating Temporal Differences: On the Efficient Implementation of    TD(lambda) for Reinforcement Learning", Volume 2, pages 287-318</cite>
<p class="media"><a href="/media/135/live-135-1449-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/135/live-135-1448-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.135'>doi:10.1613/jair.135</a></p>
<p>Temporal difference (TD) methods constitute a class of   methods for learning predictions in multi-step prediction problems,   parameterized by a recency factor lambda. Currently the most important   application of these methods is to temporal credit assignment in   reinforcement learning. Well known reinforcement learning algorithms,   such as AHC or Q-learning, may be viewed as instances of TD learning.   This paper examines the issues of the efficient and general   implementation of TD(lambda) for arbitrary lambda, for use with   reinforcement learning algorithms optimizing the discounted sum of   rewards. The traditional approach, based on eligibility traces, is   argued to suffer from both inefficiency and lack of generality. The   TTD (Truncated Temporal Differences) procedure is proposed as an   alternative, that indeed only approximates TD(lambda), but requires   very little computation per action and can be used with arbitrary   function representation methods.  The idea from which it is derived is   fairly simple and not new, but probably unexplored so far. Encouraging   experimental results are presented, suggesting that using lambda > 0   with the TTD procedure allows one to obtain a significant learning   speedup at essentially the same cost as usual TD(0) learning.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Truncating Temporal Differences: On the Efficient Implementation of    TD(lambda) for Reinforcement Learning">
<meta name="citation_author" content="Cichosz,  P.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="287">
<meta name="citation_lastpage" content="318">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/135/live-135-1449-jair.pdf">

<cite>S.  Hanks and  D.  S. Weld (1995) "A Domain-Independent Algorithm for Plan Adaptation", Volume 2, pages 319-360</cite>
<p class="media"><a href="/media/79/live-79-1414-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/79/live-79-1413-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.79'>doi:10.1613/jair.79</a></p>
<p>The paradigms of transformational planning, case-based   planning, and plan debugging all involve a process known as <i> plan   adaptation </i> - modifying or repairing an old plan so it solves a new   problem.  In this paper we provide a domain-independent algorithm for   plan adaptation, demonstrate that it is sound, complete, and   systematic, and compare it to other adaptation algorithms in the   literature. <p>   Our approach is based on a view of planning as searching a graph of   partial plans.  Generative planning starts at the graph's root and   moves from node to node using plan-refinement operators.  In planning   by adaptation, a library plan - an arbitrary node in the plan   graph - is the starting point for the search, and the plan-adaptation   algorithm can apply both the same refinement operators available to a   generative planner and can also retract constraints and steps from the   plan.  Our algorithm's completeness ensures that the adaptation   algorithm will eventually search the entire graph and its systematicity    ensures that it will do so without redundantly searching any parts of   the graph.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="A Domain-Independent Algorithm for Plan Adaptation">
<meta name="citation_author" content="Hanks,  S.">
<meta name="citation_author" content="Weld,  D. S.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="319">
<meta name="citation_lastpage" content="360">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/79/live-79-1414-jair.pdf">

<cite>J.  Ortega (1995) "On the Informativeness of the DNA Promoter Sequences Domain Theory", Volume 2, pages 361-367</cite>
<p class="media"><a href="/media/161/live-161-1464-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/161/live-161-1463-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.161'>doi:10.1613/jair.161</a></p>
<p>The DNA promoter sequences domain theory and database havebecome popular for testing systems that integrate empirical andanalytical learning.  This note reports a simple change andreinterpretation of the domain theory in terms of M-of-N concepts,involving no learning, that results in an accuracy of 93.4% on the 106items of the database.  Moreover, an exhaustive search of the space ofM-of-N domain theory interpretations indicates that the expectedaccuracy of a randomly chosen interpretation is 76.5%, and that amaximum accuracy of 97.2% is achieved in 12 cases.  This demonstratesthe informativeness of the domain theory, without the complications ofunderstanding the interactions between various learning algorithms andthe theory.  In addition, our results help characterize the difficultyof learning using the DNA promoters theory.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="On the Informativeness of the DNA Promoter Sequences Domain Theory">
<meta name="citation_author" content="Ortega,  J.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="361">
<meta name="citation_lastpage" content="367">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/161/live-161-1464-jair.pdf">

<cite>P.  D. Turney (1995) "Cost-Sensitive Classification: Empirical Evaluation of a Hybrid    Genetic Decision Tree Induction Algorithm", Volume 2, pages 369-409</cite>
<p class="media"><a href="/media/120/live-120-1428-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/120/live-120-1427-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.washington.edu/research/jair/volume2/turney95a-html/title.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.120'>doi:10.1613/jair.120</a></p>
<p>This paper introduces ICET, a new algorithm for   cost-sensitive classification. ICET uses a genetic algorithm to evolve   a population of biases for a decision tree induction algorithm. The   fitness function of the genetic algorithm is the average cost of   classification when using the decision tree, including both the costs   of tests (features, measurements) and the costs of classification   errors. ICET is compared here with three other algorithms for   cost-sensitive classification - EG2, CS-ID3, and IDX - and also with   C4.5, which classifies without regard to cost. The five algorithms are   evaluated empirically on five real-world medical datasets. Three sets   of experiments are performed.  The first set examines the baseline   performance of the five algorithms on the five datasets and   establishes that ICET performs significantly better than its   competitors. The second set tests the robustness of ICET under a   variety of conditions and shows that ICET maintains its advantage. The   third set looks at ICET's search in bias space and discovers a way to   improve the search.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Cost-Sensitive Classification: Empirical Evaluation of a Hybrid    Genetic Decision Tree Induction Algorithm">
<meta name="citation_author" content="Turney,  P. D.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="369">
<meta name="citation_lastpage" content="409">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/120/live-120-1428-jair.pdf">

<cite>S.  K. Donoho and  L.  A. Rendell (1995) "Rerepresenting and Restructuring Domain Theories:  A Constructive    Induction Approach", Volume 2, pages 411-446</cite>
<p class="media"><a href="/media/129/live-129-1435-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/129/live-129-1434-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.129'>doi:10.1613/jair.129</a>
<br/><a href="/media/129/live-129-1437-jair.tar.Z">Appendix </a> - </p>
<p>Theory revision integrates inductive learning and background   knowledge by combining training examples with a coarse domain theory   to produce a more accurate theory.  There are two challenges that   theory revision and other theory-guided systems face.  First, a   representation language appropriate for the initial theory may be   inappropriate for an improved theory.  While the original   representation may concisely express the initial theory, a more   accurate theory forced to use that same representation may be bulky,   cumbersome, and difficult to reach.  Second, a theory structure   suitable for a coarse domain theory may be insufficient for a   fine-tuned theory.  Systems that produce only small, local changes to   a theory have limited value for accomplishing complex structural   alterations that may be required.<p>      Consequently, advanced theory-guided learning systems require flexible    representation and flexible structure.  An analysis of various theory    revision systems and theory-guided learning systems reveals specific    strengths and weaknesses in terms of these two desired properties.     Designed to capture the underlying qualities of each system, a new    system uses theory-guided constructive induction.  Experiments in    three domains show improvement over previous theory-guided systems.     This leads to a study of the behavior, limitations, and potential of    theory-guided constructive induction.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Rerepresenting and Restructuring Domain Theories:  A Constructive    Induction Approach">
<meta name="citation_author" content="Donoho,  S. K.">
<meta name="citation_author" content="Rendell,  L. A.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="411">
<meta name="citation_lastpage" content="446">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/129/live-129-1435-jair.pdf">

<cite>P.  David (1995) "Using Pivot Consistency to Decompose and Solve Functional CSPs", Volume 2, pages 447-474</cite>
<p class="media"><a href="/media/167/live-167-1467-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/167/live-167-1465-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.167'>doi:10.1613/jair.167</a></p>
<p>Many studies have been carried out in order to increase thesearch efficiency of constraint satisfaction problems; among them,some make use of <i> structural </i> properties of the constraintnetwork; others take into account <i> semantic </i> properties of theconstraints, generally assuming that <i> all </i> the constraints possessthe given property.  In this paper, we propose a new decompositionmethod benefiting from both semantic properties of <i> functional </i>constraints (not <i> bijective </i> constraints) and structuralproperties of the network; furthermore, not all the constraints needto be functional.  We show that under some conditions, the existenceof solutions can be guaranteed.  We first characterize a particularsubset of the variables, which we name a <i> root set.</i>   We thenintroduce <i> pivot consistency, </i> a new local consistency which is aweak form of path consistency and can be achieved in O(n^2d^2)complexity (instead of O(n^3d^3) for path consistency), and wepresent associated properties; in particular, we show that anyconsistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional CSPs.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Using Pivot Consistency to Decompose and Solve Functional CSPs">
<meta name="citation_author" content="David,  P.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="447">
<meta name="citation_lastpage" content="474">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/167/live-167-1467-jair.pdf">

<cite>A.  Schaerf,  Y.  Shoham and  M.  Tennenholtz (1995) "Adaptive Load Balancing: A Study in Multi-Agent Learning", Volume 2, pages 475-500</cite>
<p class="media"><a href="/media/121/live-121-1432-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/121/live-121-1431-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.121'>doi:10.1613/jair.121</a></p>
<p>We study the process of multi-agent reinforcement learning in the context ofload balancing in a distributed system, without use of either centralcoordination or explicit communication.  We first define a precise frameworkin which to study adaptive load balancing, important features of which are itsstochastic nature and the purely local information available to individualagents.  Given this framework, we show illuminating results on the interplaybetween basic adaptive behavior parameters and their effect on systemefficiency.  We then investigate the properties of adaptive load balancing inheterogeneous populations, and address the issue of exploration vs.exploitation in that context.  Finally, we show that naive use ofcommunication may not improve, and might even harm system efficiency.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Adaptive Load Balancing: A Study in Multi-Agent Learning">
<meta name="citation_author" content="Schaerf,  A.">
<meta name="citation_author" content="Shoham,  Y.">
<meta name="citation_author" content="Tennenholtz,  M.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="475">
<meta name="citation_lastpage" content="500">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/121/live-121-1432-jair.pdf">

<cite>W.  W. Cohen (1995) "Pac-Learning Recursive Logic Programs: Efficient Algorithms", Volume 2, pages 501-539</cite>
<p class="media"><a href="/media/97/live-97-1420-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/97/live-97-1419-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.97'>doi:10.1613/jair.97</a></p>
<p>We present algorithms that learn certain classes of   function-free recursive logic programs in polynomial time from   equivalence queries.  In particular, we show that a single k-ary   recursive constant-depth determinate clause is learnable. Two-clause   programs consisting of one learnable recursive clause and one   constant-depth determinate non-recursive clause are also learnable, if   an additional ``basecase'' oracle is assumed.  These results   immediately imply the pac-learnability of these classes.  Although   these classes of learnable recursive programs are very constrained, it   is shown in a companion paper that they are maximally general, in that   generalizing either class in any natural way leads to a   computationally difficult learning problem.  Thus, taken together with   its companion paper, this paper establishes a boundary of efficient   learnability for recursive logic programs.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Pac-Learning Recursive Logic Programs: Efficient Algorithms">
<meta name="citation_author" content="Cohen,  W. W.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="501">
<meta name="citation_lastpage" content="539">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/97/live-97-1420-jair.pdf">

<cite>W.  W. Cohen (1995) "Pac-learning Recursive Logic Programs: Negative Results", Volume 2, pages 541-573</cite>
<p class="media"><a href="/media/1917/live-1917-2520-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1917/live-1917-2521-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1917'>doi:10.1613/jair.1917</a></p>
<p>In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiant's model of pac-learnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses. </p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Pac-learning Recursive Logic Programs: Negative Results">
<meta name="citation_author" content="Cohen, W. W.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="541">
<meta name="citation_lastpage" content="573">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1917/live-1917-2520-jair.pdf">

<cite>S.  J. Russell and  D.  Subramanian (1995) "Provably Bounded-Optimal Agents", Volume 2, pages 575-609</cite>
<p class="media"><a href="/media/133/live-133-1446-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/133/live-133-1445-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.133'>doi:10.1613/jair.133</a></p>
<p>Since its inception, artificial intelligence has relied  upon a theoretical foundation centered around <i> perfect rationality </i> as   the desired property of intelligent systems. We argue, as others have   done, that this foundation is inadequate because it imposes   fundamentally unsatisfiable requirements. As a result, there has   arisen a wide gap between theory and practice in AI, hindering   progress in the field. We propose instead a property called <i> bounded   optimality</i>. Roughly speaking, an agent is bounded-optimal if its   program is a solution to the constrained optimization problem   presented by its architecture and the task environment. We show how to   construct agents with this property for a simple class of machine   architectures in a broad class of real-time environments. We   illustrate these results using a simple model of an automated mail   sorting facility.  We also define a weaker property, <i> asymptotic   bounded optimality</i> (ABO), that generalizes the notion of optimality in   classical complexity theory.  We then construct <i> universal </i> ABO   programs, i.e., programs that are ABO no matter what real-time   constraints are applied.  Universal ABO programs can be used as   building blocks for more complex systems. We conclude with a   discussion of the prospects for bounded optimality as a theoretical   basis for AI, and relate it to similar trends in philosophy,   economics, and game theory.</p>
<a href="/vol/vol2.html">Click here to return to Volume 2 contents list</a>
<meta name="citation_title" content="Provably Bounded-Optimal Agents">
<meta name="citation_author" content="Russell,  S. J.">
<meta name="citation_author" content="Subramanian,  D.">
<meta name="citation_publication_date" content="1995">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="575">
<meta name="citation_lastpage" content="609">
<meta name="citation_volume" content="2">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/133/live-133-1446-jair.pdf">
