<cite>D.  Cohen,  M.  Cooper,  P.  Jeavons and  A.  Krokhin (2004) "A Maximal Tractable Class of Soft Constraints", Volume 22, pages 1-22</cite>
<p class="media"><a href="/media/1400/live-1400-2301-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1400/live-1400-2300-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1400'>doi:10.1613/jair.1400</a></p>
<p>Many researchers in artificial intelligence are beginning to explore the use of soft constraints to express a set of (possibly conflicting) problem  requirements. A soft constraint is a function defined on a collection of  variables which associates some measure of desirability with each possible  combination of values for those variables. However, the crucial question of  the computational complexity of finding the optimal solution to a collection of soft constraints has so far received very little attention. In this paper we identify a class of soft binary constraints for which the problem of  finding the optimal solution is tractable. In other words, we show that for  any given set of such constraints, there exists a polynomial time algorithm  to determine the assignment having the best overall combined measure of  desirability. This tractable class includes many commonly-occurring soft constraints, such as 'as near as possible' or 'as soon as possible after', as well as crisp constraints such as 'greater than'. Finally, we show that  this tractable class is maximal, in the sense that adding any other form of  soft binary constraint which is not in the class gives rise to a class of  problems which is NP-hard.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="A Maximal Tractable Class of Soft Constraints">
<meta name="citation_author" content="Cohen,  D.">
<meta name="citation_author" content="Cooper,  M.">
<meta name="citation_author" content="Jeavons,  P.">
<meta name="citation_author" content="Krokhin,  A.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="22">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1400/live-1400-2301-jair.pdf">

<cite>D.  Dubois,  H.  Fargier and  H.  Prade (2004) "Ordinal and Probabilistic Representations of Acceptance", Volume 22, pages 23-56</cite>
<p class="media"><a href="/media/1265/live-1265-2232-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1265/live-1265-2230-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1265'>doi:10.1613/jair.1265</a></p>
<p>An accepted belief is a proposition considered likely enough by an agent, to be inferred from as if it were true. This paper bridges the gap between probabilistic and logical representations of accepted beliefs. To this end, natural properties of relations on propositions, describing relative strength of belief are augmented with some conditions ensuring that accepted beliefs form a deductively closed set. This requirement turns out to be very restrictive. In particular, it is shown that the sets of accepted belief of an agent can always be derived from a family of possibility rankings of states. An agent accepts a proposition in a given context if this proposition is considered more possible than its negation in this context, for all possibility rankings in the family. These results are closely connected to the non-monotonic 'preferential' inference system of Kraus, Lehmann and Magidor and the so-called plausibility functions of Friedman and Halpern. The extent to which probability theory is compatible with acceptance relations is laid bare. A solution to the lottery paradox, which is considered as a major impediment to the use of non-monotonic inference is proposed using a special kind of probabilities (called lexicographic, or big-stepped). The setting of acceptance relations also proposes another way of approaching the theory of belief change after the works of GÃƒÂ¤rdenfors and colleagues. Our view considers the acceptance relation as a primitive object from which belief sets are derived in various contexts.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Ordinal and Probabilistic Representations of Acceptance">
<meta name="citation_author" content="Dubois,  D.">
<meta name="citation_author" content="Fargier,  H.">
<meta name="citation_author" content="Prade,  H.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="23">
<meta name="citation_lastpage" content="56">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1265/live-1265-2232-jair.pdf">

<cite>C.  J. Meek and  W.  P. Birmingham (2004) "A Comprehensive Trainable Error Model for Sung Music Queries", Volume 22, pages 57-91</cite>
<p class="media"><a href="/media/1334/live-1334-2268-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1334/live-1334-2267-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1334'>doi:10.1613/jair.1334</a></p>
<p>We propose a model for errors in sung queries, a variant of the hidden Markov model (HMM). This is a solution to the problem of identifying the degree of similarity between a (typically error-laden) sung query and a potential target in a database of musical works, an important problem in the field of music information retrieval. Similarity metrics are a critical component of `query-by-humming' (QBH) applications which search audio and multimedia databases for strong matches to oral queries. Our model comprehensively expresses the types of {m error} or variation between target and query: cumulative and non-cumulative local errors, transposition, tempo and tempo changes, insertions, deletions and modulation. The model is not only expressive, but automatically trainable, or able to learn and generalize from query examples. We present results of simulations, designed to assess the discriminatory potential of the model, and tests with real sung queries, to demonstrate relevance to real-world applications.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="A Comprehensive Trainable Error Model for Sung Music Queries">
<meta name="citation_author" content="Meek,  C. J.">
<meta name="citation_author" content="Birmingham,  W. P.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="57">
<meta name="citation_lastpage" content="91">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1334/live-1334-2268-jair.pdf">

<cite>H.  Chockler and  J.  Y. Halpern (2004) "Responsibility and Blame: A Structural-Model Approach", Volume 22, pages 93-115</cite>
<p class="media"><a href="/media/1391/live-1391-2298-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1391/live-1391-2297-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1391'>doi:10.1613/jair.1391</a></p>
<p>Causality is typically treated an all-or-nothing concept; either A is a cause of B or it is not. We extend the definition of causality introduced by Halpern and Pearl [2004a] to take into account the degree of responsibility of A for B.  For example, if someone wins an election 11-0, then each person who votes for him is less responsible for the victory than if he had won 6-5.  We then define a notion of degree of blame, which takes into account an agent's epistemic state. Roughly speaking, the degree of blame of A for B is the expected degree of responsibility of A for B, taken over the epistemic state of an agent.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Responsibility and Blame: A Structural-Model Approach">
<meta name="citation_author" content="Chockler,  H.">
<meta name="citation_author" content="Halpern,  J. Y.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="93">
<meta name="citation_lastpage" content="115">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1391/live-1391-2298-jair.pdf">

<cite>P.  Derbeko, R.  El-Yaniv and R.  Meir (2004) "Explicit Learning Curves for Transduction and Application to Clustering and Compression Algorithms", Volume 22, pages 117-142</cite>
<p class="media"><a href="/media/1417/live-1417-2312-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1417/live-1417-2311-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1417'>doi:10.1613/jair.1417</a></p>
<p>Inductive learning is based on inferring a general rule from a finite data set and using it to label new data. In transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points, which are given to the learner prior to learning. Although transduction seems at the outset to be an easier task than induction, there have not been many provably useful algorithms for transduction. Moreover, the precise relation between induction and transduction has not yet been determined. The main theoretical developments related to transduction were presented by Vapnik more than twenty years ago. One of Vapnik's basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail. While tight, this bound is given implicitly via a computational routine. Our first contribution is a somewhat looser but explicit characterization of a slightly extended PAC-Bayesian version of Vapnik's transductive bound. This characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement. We then derive error bounds for compression schemes such as (transductive) support vector machines and for transduction algorithms based on clustering. The main observation used for deriving these new error bounds and algorithms is that the unlabeled test points, which in the transductive setting are known in advance, can be used in order to construct useful data dependent prior distributions over the hypothesis space.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Explicit Learning Curves for Transduction and Application to Clustering and Compression Algorithms">
<meta name="citation_author" content="Derbeko, P.">
<meta name="citation_author" content="El-Yaniv, R.">
<meta name="citation_author" content="Meir, R.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="117">
<meta name="citation_lastpage" content="142">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1417/live-1417-2312-jair.pdf">

<cite>C.  V. Goldman and  S.  Zilberstein (2004) "Decentralized Control of Cooperative Systems: Categorization and Complexity Analysis", Volume 22, pages 143-174</cite>
<p class="media"><a href="/media/1427/live-1427-2315-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1427/live-1427-2314-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1427'>doi:10.1613/jair.1427</a></p>
<p>Decentralized control of cooperative systems captures the operation of a group of decision makers that share a single global objective.  The difficulty in solving optimally such problems arises when the agents lack full observability of the global state of the system when they operate. The general problem has been shown to be NEXP-complete. In this paper, we identify classes of decentralized control problems whose complexity ranges between NEXP and P. In particular, we study problems characterized by independent transitions, independent observations, and goal-oriented objective functions.  Two algorithms are shown to solve optimally useful classes of goal-oriented decentralized processes in polynomial time.  This paper also studies information sharing among the decision-makers, which can improve their performance. We distinguish between three ways in which agents can exchange information: indirect communication, direct communication and sharing state features that are not controlled by the agents.  Our analysis shows that for every class of problems we consider, introducing direct or indirect communication does not change the worst-case complexity.  The results provide a better understanding of the complexity of decentralized control problems that arise in practice and facilitate the development of planning algorithms for these problems.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Decentralized Control of Cooperative Systems: Categorization and Complexity Analysis">
<meta name="citation_author" content="Goldman,  C. V.">
<meta name="citation_author" content="Zilberstein,  S.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="143">
<meta name="citation_lastpage" content="174">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1427/live-1427-2315-jair.pdf">

<cite>S.  Park,  E.  H. Durfee and  W.  P. Birmingham (2004) "Use of Markov Chains to Design an Agent Bidding Strategy for Continuous Double Auctions", Volume 22, pages 175-214</cite>
<p class="media"><a href="/media/1466/live-1466-2322-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1466/live-1466-2321-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1466'>doi:10.1613/jair.1466</a></p>
<p>As computational agents are developed for increasingly complicated e-commerce applications, the complexity of the decisions they face demands advances in artificial intelligence techniques. For example, an agent representing a seller in an auction should try to maximize the seller?s profit by reasoning about a variety of possibly uncertain pieces of information, such as the maximum prices various buyers might be willing to pay, the possible prices being offered by competing sellers, the rules by which the auction operates, the dynamic arrival and matching of offers to buy and sell, and so on. A naive application of multiagent reasoning techniques would require the seller?s agent to explicitly model all of the other agents through an extended time horizon, rendering the problem intractable for many realistically-sized problems. We have instead devised a new strategy that an agent can use to determine its bid price based on a more tractable Markov chain model of the auction process.  We have experimentally identified the conditions under which our new strategy works well, as well as how well it works in comparison to the optimal performance the agent could have achieved had it known the future. Our results show that our new strategy in general performs well, outperforming other tractable heuristic strategies in a majority of experiments, and is particularly effective in a 'seller?s market', where many buy offers are available.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Use of Markov Chains to Design an Agent Bidding Strategy for Continuous Double Auctions">
<meta name="citation_author" content="Park,  S.">
<meta name="citation_author" content="Durfee,  E. H.">
<meta name="citation_author" content="Birmingham,  W. P.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="175">
<meta name="citation_lastpage" content="214">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1466/live-1466-2322-jair.pdf">

<cite>J.  Hoffmann,  J.  Porteous and  L.  Sebastia (2004) "Ordered Landmarks in Planning", Volume 22, pages 215-278</cite>
<p class="media"><a href="/media/1492/live-1492-2338-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1492/live-1492-2337-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1492'>doi:10.1613/jair.1492</a></p>
<p>Many known planning tasks have inherent constraints concerning the best order in which to achieve the goals. A number of research efforts have been made to detect such constraints and to use them for guiding search, in the hope of speeding up the planning process.   </p><p>  We go beyond the previous approaches by considering ordering constraints not only over the (top-level) goals, but also over the sub-goals that will necessarily arise during planning. Landmarks are facts that must be true at some point in every valid solution plan.  We extend Koehler and Hoffmann's definition of reasonable orders between top level goals to the more general case of landmarks. We show how landmarks can be found, how their reasonable orders can be approximated, and how this information can be used to decompose a given planning task into several smaller sub-tasks. Our methodology is completely domain- and planner-independent. The implementation demonstrates that the approach can yield significant runtime performance improvements when used as a control loop around state-of-the-art sub-optimal planning systems, as exemplified by FF and LPG.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Ordered Landmarks in Planning">
<meta name="citation_author" content="Hoffmann,  J.">
<meta name="citation_author" content="Porteous,  J.">
<meta name="citation_author" content="Sebastia,  L.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="215">
<meta name="citation_lastpage" content="278">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1492/live-1492-2338-jair.pdf">

<cite>A.  Felner,  R.  E. Korf and  S.  Hanan (2004) "Additive Pattern Database Heuristics", Volume 22, pages 279-318</cite>
<cite>Honorable Mention for the 2007 IJCAI-JAIR Best Paper Prize</cite><p class="media"><a href="/media/1480/live-1480-2332-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1480/live-1480-2331-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1480'>doi:10.1613/jair.1480</a></p>
<p>We explore a method for computing admissible heuristic evaluation functions for search problems. It utilizes pattern databases, which are precomputed tables of the exact cost of solving various subproblems of an existing problem. Unlike standard pattern database heuristics, however, we partition our problems into disjoint subproblems, so that the costs of solving the different subproblems can be added together without overestimating the cost of solving the original problem. Previously, we showed how to statically partition the sliding-tile puzzles into disjoint groups of tiles to compute an admissible heuristic, using the same partition for each state and problem instance. Here we extend the method and show that it applies to other domains as well. We also present another method for additive heuristics which we call dynamically partitioned pattern databases. Here we partition the problem into disjoint subproblems for each state of the search dynamically. We discuss the pros and cons of each of these methods and apply both methods to three different problem domains: the sliding-tile puzzles, the 4-peg Towers of Hanoi problem, and finding an optimal vertex cover of a graph. We find that in some problem domains, static partitioning is most effective, while in others dynamic partitioning is a better choice. In each of these problem domains, either statically partitioned or dynamically partitioned pattern database heuristics are the best known heuristics for the problem.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Additive Pattern Database Heuristics">
<meta name="citation_author" content="Felner,  A.">
<meta name="citation_author" content="Korf,  R. E.">
<meta name="citation_author" content="Hanan,  S.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="279">
<meta name="citation_lastpage" content="318">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1480/live-1480-2332-jair.pdf">

<cite>P.  Beame,  H.  Kautz and  A.  Sabharwal (2004) "Towards Understanding and Harnessing the Potential of Clause Learning", Volume 22, pages 319-351</cite>
<cite>Honorable Mention for the 2008 IJCAI-JAIR Best Paper Prize</cite><p class="media"><a href="/media/1410/live-1410-2304-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1410/live-1410-2303-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/beame04a-html/beame04a-html.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1410'>doi:10.1613/jair.1410</a></p>
<p>Efficient implementations of DPLL with the addition of clause learning are the fastest complete Boolean satisfiability solvers and can handle many significant real-world problems, such as verification, planning and design. Despite its importance, little is known of the ultimate strengths and limitations of the technique. This paper presents the first precise characterization of clause learning as a proof system (CL), and begins the task of understanding its power by relating it to the well-studied resolution proof system. In particular, we show that with a new learning scheme, CL can provide exponentially shorter proofs than many proper refinements of general resolution (RES) satisfying a natural property. These include regular and Davis-Putnam resolution, which are already known to be much stronger than ordinary DPLL. We also show that a slight variant of CL with unlimited restarts is as powerful as RES itself. Translating these analytical results to practice, however, presents a challenge because of the nondeterministic nature of clause learning algorithms. We propose a novel way of exploiting the underlying problem structure, in the form of a high level problem description such as a graph or PDDL specification, to guide clause learning algorithms toward faster solutions. We show that this leads to exponential speed-ups on grid and randomized pebbling problems, as well as substantial improvements on certain ordering formulas.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Towards Understanding and Harnessing the Potential of Clause Learning">
<meta name="citation_author" content="Beame,  P.">
<meta name="citation_author" content="Kautz,  H.">
<meta name="citation_author" content="Sabharwal,  A.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="319">
<meta name="citation_lastpage" content="351">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1410/live-1410-2304-jair.pdf">

<cite>M.  Bowling and  M.  Veloso (2004) "Existence of Multiagent Equilibria with Limited Agents", Volume 22, pages 353-384</cite>
<p class="media"><a href="/media/1332/live-1332-2262-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1332/live-1332-2261-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1332'>doi:10.1613/jair.1332</a></p>
<p>Multiagent learning is a necessary yet challenging problem as multiagent systems become more prevalent and environments become more dynamic.  Much of the groundbreaking work in this area draws on notable results from game theory, in particular, the concept of Nash equilibria.  Learners that directly learn an equilibrium obviously rely on their existence.  Learners that instead seek to play optimally with respect to the other players also depend upon equilibria since equilibria are fixed points for learning.  From another perspective, agents with limitations are real and common.  These may be undesired physical limitations as well as self-imposed rational limitations, such as abstraction and approximation techniques, used to make learning tractable.  This article explores the interactions of these two important concepts: equilibria and limitations in learning.  We introduce the question of whether equilibria continue to exist when agents have limitations.  We look at the general effects limitations can have on agent behavior, and define a natural extension of equilibria that accounts for these limitations.  Using this formalization, we make three major contributions: (i) a counterexample for the general existence of equilibria with limitations, (ii) sufficient conditions on limitations that preserve their existence, (iii) three general classes of games and limitations that satisfy these conditions.  We then present empirical results from a specific multiagent learning algorithm applied to a specific instance of limited agents.  These results demonstrate that learning with limitations is feasible, when the conditions outlined by our theoretical analysis hold.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Existence of Multiagent Equilibria with Limited Agents">
<meta name="citation_author" content="Bowling,  M.">
<meta name="citation_author" content="Veloso,  M.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="353">
<meta name="citation_lastpage" content="384">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1332/live-1332-2262-jair.pdf">

<cite>R.  Begleiter,  R.  El-Yaniv and  G.  Yona (2004) "On Prediction Using Variable Order Markov Models", Volume 22, pages 385-421</cite>
<p class="media"><a href="/media/1491/live-1491-2335-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1491/live-1491-2334-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1491'>doi:10.1613/jair.1491</a></p>
<p>This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a ``decomposed'' CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="On Prediction Using Variable Order Markov Models">
<meta name="citation_author" content="Begleiter,  R.">
<meta name="citation_author" content="El-Yaniv,  R.">
<meta name="citation_author" content="Yona,  G.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="385">
<meta name="citation_lastpage" content="421">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1491/live-1491-2335-jair.pdf">

<cite>R.  Becker,  S.  Zilberstein,  V.  Lesser and  C.  V. Goldman (2004) "Solving Transition Independent Decentralized Markov Decision Processes", Volume 22, pages 423-455</cite>
<p class="media"><a href="/media/1497/live-1497-2345-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1497/live-1497-2344-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1497'>doi:10.1613/jair.1497</a></p>
<p>Formal treatment of collaborative multi-agent systems has been lagging behind the rapid progress in sequential decision making by individual agents.  Recent work in the area of decentralized Markov Decision Processes (MDPs) has contributed to closing this gap, but the computational complexity of these models remains a serious obstacle. To overcome this complexity barrier, we identify a specific class of decentralized MDPs in which the agents' transitions are independent. The class consists of independent collaborating agents that are tied together through a structured global reward function that depends on all of their histories of states and actions.  We present a novel algorithm for solving this class of problems and examine its properties, both as an optimal algorithm and as an anytime algorithm. To our best knowledge, this is the first algorithm to optimally solve a non-trivial subclass of decentralized MDPs.  It lays the foundation for further work in this area on both exact and approximate algorithms.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Solving Transition Independent Decentralized Markov Decision Processes">
<meta name="citation_author" content="Becker,  R.">
<meta name="citation_author" content="Zilberstein,  S.">
<meta name="citation_author" content="Lesser,  V.">
<meta name="citation_author" content="Goldman,  C. V.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="423">
<meta name="citation_lastpage" content="455">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1497/live-1497-2345-jair.pdf">

<cite>G.  Erkan and  D.  R. Radev (2004) "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", Volume 22, pages 457-479</cite>
<p class="media"><a href="/media/1523/live-1523-2354-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1523/live-1523-2353-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1523'>doi:10.1613/jair.1523</a></p>
<p>We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="LexRank: Graph-based Lexical Centrality as Salience in Text Summarization">
<meta name="citation_author" content="Erkan,  G.">
<meta name="citation_author" content="Radev,  D. R.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="457">
<meta name="citation_lastpage" content="479">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1523/live-1523-2354-jair.pdf">

<cite>H.  E. Dixon,  M.  L. Ginsberg,  E.  M. Luks and  A.  J. Parkes (2004) "Generalizing Boolean Satisfiability II: Theory", Volume 22, pages 481-534</cite>
<p class="media"><a href="/media/1555/live-1555-2385-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1555/live-1555-2384-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1555'>doi:10.1613/jair.1555</a></p>
<p>This is the second of three planned papers describing ZAP, a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers.  The fundamental idea underlying ZAP is that many problems passed to such engines contain rich internal structure that is obscured by the Boolean representation used; our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance.  This paper presents the theoretical basis for the ideas underlying ZAP, arguing that existing ideas in this area exploit a single, recurring structure in that multiple database axioms can be obtained by operating on a single axiom using a subgroup of the group of permutations on the literals in the problem.  We argue that the group structure precisely captures the general structure at which earlier approaches hinted, and give numerous examples of its use.  We go on to extend the Davis-Putnam-Logemann-Loveland inference procedure to this broader setting, and show that earlier computational improvements are either subsumed or left intact by the new method.  The third paper in this series discusses ZAP's implementation and presents experimental performance results.</p>
<a href="/vol/vol22.html">Click here to return to Volume 22 contents list</a>
<meta name="citation_title" content="Generalizing Boolean Satisfiability II: Theory">
<meta name="citation_author" content="Dixon,  H. E.">
<meta name="citation_author" content="Ginsberg,  M. L.">
<meta name="citation_author" content="Luks,  E. M.">
<meta name="citation_author" content="Parkes,  A. J.">
<meta name="citation_publication_date" content="2004">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="481">
<meta name="citation_lastpage" content="534">
<meta name="citation_volume" content="22">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1555/live-1555-2385-jair.pdf">
