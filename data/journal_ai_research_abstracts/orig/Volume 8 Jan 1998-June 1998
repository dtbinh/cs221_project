<cite>J.  Engelfriet (1998) "Monotonicity and Persistence in Preferential Logics", Volume 8, pages 1-21</cite>
<p class="media"><a href="/media/461/live-461-1683-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/461/live-461-1682-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.461'>doi:10.1613/jair.461</a></p>
<p>An important characteristic of many logics for Artificial    Intelligence is their nonmonotonicity. This means that adding a    formula to the premises can invalidate some of the consequences. There    may, however, exist formulae that can always be safely added to the    premises without destroying any of the consequences: we say they    respect monotonicity. Also, there may be formulae that, when they are    a consequence, can not be invalidated when adding any formula to the    premises: we call them conservative. We study these two classes of    formulae for preferential logics, and show that they are closely    linked to the formulae whose truth-value is preserved along the    (preferential) ordering. We will consider some preferential logics for    illustration, and prove syntactic characterization results for them.    The results in this paper may improve the efficiency of theorem    provers for preferential logics.</p>
<a href="/vol/vol8.html">Click here to return to Volume 8 contents list</a>
<meta name="citation_title" content="Monotonicity and Persistence in Preferential Logics">
<meta name="citation_author" content="Engelfriet,  J.">
<meta name="citation_publication_date" content="1998">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="21">
<meta name="citation_volume" content="8">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/461/live-461-1683-jair.pdf">

<cite>G.  Gogic,  C.  H. Papadimitriou and  M.  Sideri (1998) "Incremental Recompilation of Knowledge", Volume 8, pages 23-37</cite>
<p class="media"><a href="/media/380/live-380-1640-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/380/live-380-1638-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.380'>doi:10.1613/jair.380</a></p>
<p>Approximating a general formula from above and below by Horn    formulas (its Horn envelope and Horn core, respectively) was proposed    by Selman and Kautz (1991, 1996) as a form of ``knowledge    compilation,'' supporting rapid approximate reasoning; on the negative    side, this scheme is static in that it supports no updates, and has    certain complexity drawbacks pointed out by Kavvadias, Papadimitriou    and Sideri (1993).  On the other hand, the many frameworks and schemes    proposed in the literature for theory update and revision are plagued    by serious complexity-theoretic impediments, even in the Horn case, as    was pointed out by Eiter and Gottlob (1992), and is further    demonstrated in the present paper.  More fundamentally, these schemes    are not inductive, in that they may lose in a single update any    positive properties of the represented sets of formulas (small size,    Horn structure, etc.).  In this paper we propose a new scheme,    incremental recompilation, which combines Horn approximation and    model-based updates; this scheme is inductive and very efficient, free    of the problems facing its constituents.  A set of formulas is    represented by an upper and lower Horn approximation.  To update, we    replace the upper Horn formula by the Horn envelope of its    minimum-change update, and similarly the lower one by the Horn core of    its update; the key fact which enables this scheme is that Horn    envelopes and cores are easy to compute when the underlying formula is    the result of a minimum-change update of a Horn formula by a clause.    We conjecture that efficient algorithms are possible for more complex    updates.</p>
<a href="/vol/vol8.html">Click here to return to Volume 8 contents list</a>
<meta name="citation_title" content="Incremental Recompilation of Knowledge">
<meta name="citation_author" content="Gogic,  G.">
<meta name="citation_author" content="Papadimitriou,  C. H.">
<meta name="citation_author" content="Sideri,  M.">
<meta name="citation_publication_date" content="1998">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="23">
<meta name="citation_lastpage" content="37">
<meta name="citation_volume" content="8">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/380/live-380-1640-jair.pdf">

<cite>S.  Argamon-Engelson and  M.  Koppel (1998) "Tractability of Theory Patching", Volume 8, pages 39-65</cite>
<p class="media"><a href="/media/438/live-438-1669-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/438/live-438-1667-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.438'>doi:10.1613/jair.438</a></p>
<p>In this paper we consider the problem of `theory patching',    in which we are given a domain theory, some of whose components are    indicated to be possibly flawed, and a set of labeled training    examples for the domain concept.  The theory patching problem is to    revise only the indicated components of the theory, such that the    resulting theory correctly classifies all the training examples.    Theory patching is thus a type of theory revision in which revisions    are made to individual components of the theory.  Our concern in this    paper is to determine for which classes of logical domain theories the    theory patching problem is tractable.  We consider both propositional    and first-order domain theories, and show that the theory patching    problem is equivalent to that of determining what information    contained in a theory is `stable' regardless of what revisions might    be performed to the theory.  We show that determining stability is    tractable if the input theory satisfies two conditions: that revisions    to each theory component have monotonic effects on the classification    of examples, and that theory components act independently in the    classification of examples in the theory.  We also show how the    concepts introduced can be used to determine the soundness and    completeness of particular theory patching algorithms.</p>
<a href="/vol/vol8.html">Click here to return to Volume 8 contents list</a>
<meta name="citation_title" content="Tractability of Theory Patching">
<meta name="citation_author" content="Argamon-Engelson,  S.">
<meta name="citation_author" content="Koppel,  M.">
<meta name="citation_publication_date" content="1998">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="39">
<meta name="citation_lastpage" content="65">
<meta name="citation_volume" content="8">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/438/live-438-1669-jair.pdf">

<cite>A.  Moore and  M.  S. Lee (1998) "Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets", Volume 8, pages 67-91</cite>
<p class="media"><a href="/media/453/live-453-1678-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/453/live-453-1676-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.453'>doi:10.1613/jair.453</a></p>
<p>This paper introduces new algorithms and data structures for    quick counting for machine learning datasets.  We focus on the    counting task of constructing contingency tables, but our approach is    also applicable to counting the number of records in a dataset that    match conjunctive queries.  Subject to certain assumptions, the costs    of these operations can be shown to be independent of the number of    records in the dataset and loglinear in the number of non-zero entries    in the contingency table.<p>      We provide a very sparse data structure, the ADtree, to minimize    memory use. We provide analytical worst-case bounds for this structure    for several models of data distribution.  We empirically demonstrate    that tractably-sized data structures can be produced for large    real-world datasets by (a) using a sparse tree structure that never    allocates memory for counts of zero, (b) never allocating memory for    counts that can be deduced from other counts, and (c) not bothering to    expand the tree fully near its leaves.<p>       We show how the ADtree can be used to accelerate Bayes net structure    finding algorithms, rule learning algorithms, and feature selection    algorithms, and we provide a number of empirical results comparing    ADtree methods against traditional direct counting approaches.  We    also discuss the possible uses of ADtrees in other machine learning    methods, and discuss the merits of ADtrees in comparison with    alternative representations such as kd-trees, R-trees and Frequent Sets.</p>
<a href="/vol/vol8.html">Click here to return to Volume 8 contents list</a>
<meta name="citation_title" content="Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets">
<meta name="citation_author" content="Moore,  A.">
<meta name="citation_author" content="Lee,  M. S.">
<meta name="citation_publication_date" content="1998">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="67">
<meta name="citation_lastpage" content="91">
<meta name="citation_volume" content="8">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/453/live-453-1678-jair.pdf">

<cite>B.  Srivastava and  S.  Kambhampati (1998) "Synthesizing Customized Planners from Specifications", Volume 8, pages 93-128</cite>
<p class="media"><a href="/media/428/live-428-1662-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/428/live-428-1660-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.428'>doi:10.1613/jair.428</a></p>
<p>Existing plan synthesis approaches in artificial    intelligence fall into two categories -- domain independent and domain    dependent.  The domain independent approaches are applicable across a    variety of domains, but may not be very efficient in any one given    domain.  The domain dependent approaches need to be (re)designed for    each domain separately, but can be very efficient in the domain for    which they are designed.  One enticing alternative to these approaches    is to automatically synthesize domain independent planners given the    knowledge about the domain and the theory of planning. In this paper,    we investigate the feasibility of using existing automated software    synthesis tools to support such synthesis. Specifically, we describe    an architecture called CLAY in which the Kestrel Interactive    Development System (KIDS) is used to derive a domain-customized    planner through a semi-automatic combination of a declarative theory    of planning, and the declarative control knowledge specific to a given    domain, to semi-automatically combine them to derive domain-customized    planners.  We discuss what it means to write a declarative theory of    planning and control knowledge for KIDS, and illustrate our approach    by generating a class of domain-specific planners using state space    refinements.  Our experiments show that the synthesized planners can    outperform classical refinement planners (implemented as    instantiations of UCP, Kambhampati & Srivastava, 1995), using the same    control knowledge.  We will contrast the costs and benefits of the    synthesis approach with conventional methods for customizing domain    independent planners.</p>
<a href="/vol/vol8.html">Click here to return to Volume 8 contents list</a>
<meta name="citation_title" content="Synthesizing Customized Planners from Specifications">
<meta name="citation_author" content="Srivastava,  B.">
<meta name="citation_author" content="Kambhampati,  S.">
<meta name="citation_publication_date" content="1998">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="93">
<meta name="citation_lastpage" content="128">
<meta name="citation_volume" content="8">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/428/live-428-1662-jair.pdf">

<cite>J.  Fuernkranz (1998) "Integrative Windowing", Volume 8, pages 129-164</cite>
<p class="media"><a href="/media/487/live-487-1704-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/487/live-487-1702-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume8/fuernkranz98a-html/fuernkranz98a.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.487'>doi:10.1613/jair.487</a></p>
<p>In this paper we re-investigate windowing for rule learning    algorithms.  We show that, contrary to previous results for decision    tree learning, windowing can in fact achieve significant run-time    gains in noise-free domains and explain the different behavior of rule    learning algorithms by the fact that they learn each rule    independently. The main contribution of this paper is integrative    windowing, a new type of algorithm that further exploits this property    by integrating good rules into the final theory right after they have    been discovered. Thus it avoids re-learning these rules in subsequent    iterations of the windowing process. Experimental evidence in a    variety of noise-free domains shows that integrative windowing can in    fact achieve substantial run-time gains. Furthermore, we discuss the    problem of noise in windowing and present an algorithm that is able to    achieve run-time gains in a set of experiments in a simple domain with    artificial noise.</p>
<a href="/vol/vol8.html">Click here to return to Volume 8 contents list</a>
<meta name="citation_title" content="Integrative Windowing">
<meta name="citation_author" content="Fuernkranz,  J.">
<meta name="citation_publication_date" content="1998">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="129">
<meta name="citation_lastpage" content="164">
<meta name="citation_volume" content="8">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/487/live-487-1704-jair.pdf">

<cite>A.  Darwiche (1998) "Model-Based Diagnosis using Structured System Descriptions", Volume 8, pages 165-222</cite>
<p class="media"><a href="/media/462/live-462-1687-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/462/live-462-1685-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.462'>doi:10.1613/jair.462</a></p>
<p>This paper presents a comprehensive approach for model-based    diagnosis which includes proposals for characterizing and computing    preferred diagnoses, assuming that the system description is augmented    with a system structure (a directed graph explicating the    interconnections between system components).  Specifically, we first    introduce the notion of a consequence, which is a syntactically    unconstrained propositional sentence that characterizes all    consistency-based diagnoses and show that standard characterizations    of diagnoses, such as minimal conflicts, correspond to syntactic    variations on a consequence. Second, we propose a new syntactic    variation on the consequence known as negation normal form (NNF) and    discuss its merits compared to standard variations.  Third, we    introduce a basic algorithm for computing consequences in NNF given a    structured system description. We show that if the system structure    does not contain cycles, then there is always a linear-size    consequence in NNF which can be computed in linear time. For arbitrary    system structures, we show a precise connection between the complexity    of computing consequences and the topology of the underlying system    structure.  Finally, we present an algorithm that enumerates the    preferred diagnoses characterized by a consequence. The algorithm is    shown to take linear time in the size of the consequence if the    preference criterion satisfies some general conditions.</p>
<a href="/vol/vol8.html">Click here to return to Volume 8 contents list</a>
<meta name="citation_title" content="Model-Based Diagnosis using Structured System Descriptions">
<meta name="citation_author" content="Darwiche,  A.">
<meta name="citation_publication_date" content="1998">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="165">
<meta name="citation_lastpage" content="222">
<meta name="citation_volume" content="8">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/462/live-462-1687-jair.pdf">

<cite>L.  Finkelstein and  S.  Markovitch (1998) "A Selective Macro-learning Algorithm and its Application to the NxN Sliding-Tile Puzzle", Volume 8, pages 223-263</cite>
<p class="media"><a href="/media/484/live-484-1699-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/484/live-484-1697-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume8/finkelstein98a-html/hillary.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.484'>doi:10.1613/jair.484</a>
<br/><a href="/media/484/live-484-1700-jair.html">Appendix </a> - Source code</p>
<p>One of the most common mechanisms used for speeding up    problem solvers is macro-learning.  Macros are sequences of basic    operators acquired during problem solving.  Macros are used by the    problem solver as if they were basic operators.  The major problem    that macro-learning presents is the vast number of macros that are    available for acquisition.  Macros increase the branching factor of    the search space and can severely degrade problem-solving efficiency.    To make macro learning useful, a program must be selective in    acquiring and utilizing macros.  This paper describes a general method    for selective acquisition of macros. Solvable training problems are    generated in increasing order of difficulty.  The only macros acquired    are those that take the problem solver out of a local minimum to a    better state.  The utility of the method is demonstrated in several    domains, including the domain of NxN sliding-tile puzzles. After    learning on small puzzles, the system is able to efficiently solve    puzzles of any size.</p>
<a href="/vol/vol8.html">Click here to return to Volume 8 contents list</a>
<meta name="citation_title" content="A Selective Macro-learning Algorithm and its Application to the NxN Sliding-Tile Puzzle">
<meta name="citation_author" content="Finkelstein,  L.">
<meta name="citation_author" content="Markovitch,  S.">
<meta name="citation_publication_date" content="1998">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="223">
<meta name="citation_lastpage" content="263">
<meta name="citation_volume" content="8">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/484/live-484-1699-jair.pdf">
