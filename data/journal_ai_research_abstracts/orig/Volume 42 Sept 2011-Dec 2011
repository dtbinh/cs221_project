<cite>T.  Walsh (2011) "Where Are the Hard Manipulation Problems?", Volume 42, pages 1-29</cite>
<p class="media"><a href="/media/3223/live-3223-5837-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3223/live-3223-5836-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3223'>doi:10.1613/jair.3223</a></p>
<p>Voting is a simple mechanism to combine together the preferences of multiple agents. Unfortunately, agents may try to manipulate the result by mis-reporting their preferences. One barrier that might exist to such manipulation is computational complexity. In particular, it has been shown that it is NP-hard to compute how to manipulate a number of different voting rules. How- ever, NP-hardness only bounds the worst-case complexity. Recent theoretical results suggest that manipulation may often be easy in practice. In this paper, we show that empirical studies are useful in improving our understanding of this issue. We consider two settings which represent the two types of complexity results that have been identified in this area: manipulation with un-weighted votes by a single agent, and manipulation with weighted votes by a coalition of agents. In the first case, we consider Single Transferable Voting (STV), and in the second case, we consider veto voting. STV is one of the few voting rules used in practice where it is NP-hard to compute how a single agent can manipulate the result when votes are unweighted. It also appears one of the harder voting rules to manipulate since it involves multiple rounds. On the other hand, veto voting is one of the simplest representatives of voting rules where it is NP-hard to compute how a coalition of weighted agents can manipulate the result. In our experiments, we sample a number of distributions of votes including uniform, correlated and real world elections. In many of the elections in our experiments, it was easy to compute how to manipulate the result or to prove that manipulation was impossible. Even when we were able to identify a situation in which manipulation was hard to compute (e.g. when votes are highly correlated and the election is “hung”), we found that the computational difficulty of computing manipulations was somewhat precarious (e.g. with such “hung” elections, even a single uncorrelated voter was enough to make manipulation easy to compute).</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Where Are the Hard Manipulation Problems?">
<meta name="citation_author" content="Walsh, T.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="29">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3223/live-3223-5837-jair.pdf">

<cite>R.  Booth, T.  Meyer, I.  Varzinczak and R.  Wassermann (2011) "On the Link between Partial Meet, Kernel, and Infra Contraction and its Application to Horn Logic", Volume 42, pages 31-53</cite>
<p class="media"><a href="/media/3364/live-3364-5846-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3364/live-3364-5849-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3364'>doi:10.1613/jair.3364</a></p>
<p>Standard belief change assumes an underlying logic containing full classical propositional logic. However, there are good reasons for considering belief change in less expressive logics as well. In this paper we build on recent investigations by Delgrande on contraction for Horn logic. We show that the standard basic form of contraction, partial meet, is too strong in the Horn case. This result stands in contrast to Delgrande’s conjecture that orderly maxichoice is the appropriate form of contraction for Horn logic. We then define a more appropriate notion of basic contraction for the Horn case, influenced by the convexity property holding for full propositional logic and which we refer to as infra contraction. The main contribution of this work is a result which shows that the construction method for Horn contraction for belief sets based on our infra remainder sets corresponds exactly to Hansson’s classical kernel contraction for belief sets, when restricted to Horn logic. This result is obtained via a detour through contraction for belief bases. We prove that kernel contraction for belief bases produces precisely the same results as the belief base version of infra contraction. The use of belief bases to obtain this result provides evidence for the conjecture that Horn belief change is best viewed as a 'hybrid' version of belief set change and belief base change. One of the consequences of the link with base contraction is the provision of a representation result for Horn contraction for belief sets in which a version of the Core-retainment postulate features.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="On the Link between Partial Meet, Kernel, and Infra Contraction and its Application to Horn Logic">
<meta name="citation_author" content="Booth, R.">
<meta name="citation_author" content="Meyer, T.">
<meta name="citation_author" content="Varzinczak, I.">
<meta name="citation_author" content="Wassermann, R.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="31">
<meta name="citation_lastpage" content="53">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3364/live-3364-5846-jair.pdf">

<cite>K.  C. Wang and A.  Botea (2011) "MAPP: a Scalable Multi-Agent Path Planning Algorithm with Tractability and Completeness Guarantees", Volume 42, pages 55-90</cite>
<p class="media"><a href="/media/3370/live-3370-5850-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3370/live-3370-5851-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3370'>doi:10.1613/jair.3370</a></p>
<p>Multi-agent path planning is a challenging problem with numerous real-life applications.  Running a centralized search such as A* in the combined state space of all units is complete and cost-optimal, but scales poorly, as the state space size is exponential in the number of mobile units.  Traditional decentralized approaches, such as FAR and  WHCA*, are faster and more scalable, being based on problem decomposition.  However, such methods are incomplete and provide no guarantees with respect to the running time or the solution quality.  They are not necessarily able to tell in a reasonable time whether they would succeed in finding a solution to a given instance. <br />
<br />
We introduce MAPP, a tractable algorithm for multi-agent path planning on undirected graphs.  We present a basic version and several extensions.<br />
They have low-polynomial worst-case upper bounds for the running time, the memory requirements, and the length of solutions.  Even though all algorithmic versions are incomplete in the general case, each provides formal guarantees on problems it can solve.  For each version, we discuss the algorithm's completeness with respect to clearly defined subclasses of instances.<br />
<br />
Experiments were run on realistic game grid maps.  MAPP solved 99.86% of all mobile units, which is 18--22% better than the percentage of FAR and WHCA*.  MAPP marked 98.82% of all units as provably solvable during the first stage of plan computation.  Parts of MAPP's computation can be re-used across instances on the same map.  Speed-wise, MAPP is competitive or significantly faster than WHCA*, depending on whether MAPP performs all computations from scratch.  When data that MAPP can re-use are preprocessed offline and readily available, MAPP is slower than the very fast FAR algorithm by a factor of 2.18 on average.  MAPP's solutions are on average 20% longer than FAR's solutions and 7--31% longer than WHCA*'s solutions. </p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="MAPP: a Scalable Multi-Agent Path Planning Algorithm with Tractability and Completeness Guarantees">
<meta name="citation_author" content="Wang, K. C.">
<meta name="citation_author" content="Botea, A.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="55">
<meta name="citation_lastpage" content="90">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3370/live-3370-5850-jair.pdf">

<cite>R.  Hoshino and K.  Kawarabayashi (2011) "Scheduling Bipartite Tournaments to Minimize Total Travel Distance", Volume 42, pages 91-124</cite>
<p class="media"><a href="/media/3388/live-3388-5879-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3388/live-3388-5878-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3388'>doi:10.1613/jair.3388</a></p>
<p>In many professional sports leagues, teams from opposing leagues/conferences compete against one another, playing inter-league games.  This is an example of a bipartite tournament.  In this paper, we consider the problem of reducing the total travel distance of bipartite tournaments, by analyzing inter-league scheduling from the perspective of discrete optimization.  This research has natural applications to sports scheduling, especially for leagues such as the National Basketball Association (NBA) where teams must travel long distances across North America to play all their games, thus consuming much time, money, and greenhouse gas emissions.<br />
<br />
We introduce the Bipartite Traveling Tournament Problem (BTTP), the inter-league variant of the well-studied Traveling Tournament Problem. We prove that the 2n-team BTTP is NP-complete, but for small values of n, a distance-optimal inter-league schedule can be generated from an algorithm based on minimum-weight 4-cycle-covers.  We apply our theoretical results to the 12-team Nippon Professional Baseball (NPB) league in Japan, producing a provably-optimal schedule requiring 42950 kilometres of total team travel, a 16% reduction compared to the actual distance traveled by these teams during the 2010 NPB season.  We also develop a nearly-optimal inter-league tournament for the 30-team NBA league, just 3.8% higher than the trivial theoretical lower bound.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Scheduling Bipartite Tournaments to Minimize Total Travel Distance">
<meta name="citation_author" content="Hoshino, R.">
<meta name="citation_author" content="Kawarabayashi, K.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="91">
<meta name="citation_lastpage" content="124">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3388/live-3388-5879-jair.pdf">

<cite>J.  Lee and Y.  Meng (2011) "First-Order Stable Model Semantics and First-Order Loop Formulas", Volume 42, pages 125-180</cite>
<p class="media"><a href="/media/3337/live-3337-5883-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3337/live-3337-5882-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3337'>doi:10.1613/jair.3337</a></p>
<p>Lin and Zhao's theorem on loop formulas states that in the propositional case the stable model semantics of a logic program can be completely characterized by propositional loop formulas, but this result does not fully carry over to the first-order case. We investigate the precise relationship between the first-order stable model semantics and first-order loop formulas, and study conditions under which the former can be represented by the latter. In order to facilitate the comparison, we extend the definition of a first-order loop formula which was limited to a nondisjunctive program, to a disjunctive program and to an arbitrary first-order theory. Based on the studied relationship we extend the syntax of a logic program with explicit quantifiers, which allows us to do reasoning involving non-Herbrand stable models using first-order reasoners. Such programs can be viewed as a special class of first-order theories under the stable model semantics, which yields more succinct loop formulas than the general language due to their restricted syntax.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="First-Order Stable Model Semantics and First-Order Loop Formulas">
<meta name="citation_author" content="Lee, J.">
<meta name="citation_author" content="Meng, Y.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="125">
<meta name="citation_lastpage" content="180">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3337/live-3337-5883-jair.pdf">

<cite>P.  Dai, Mausam  , D.  S. Weld and J.  Goldsmith (2011) "Topological Value Iteration Algorithms", Volume 42, pages 181-209</cite>
<p class="media"><a href="/media/3390/live-3390-5896-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3390/live-3390-5897-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3390'>doi:10.1613/jair.3390</a></p>
<p>Value iteration is a powerful yet inefficient algorithm for Markov decision processes (MDPs) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. In order to overcome this problem, many approaches have been proposed. Among them, ILAO* and variants of RTDP are state-of-the-art ones. These methods use reachability analysis and heuristic search to avoid some unnecessary backups. However, none of these approaches build the graphical structure of the state transitions in a pre-processing step or use the structural information to systematically decompose a problem, whereby generating an intelligent backup sequence of the state space. In this paper, we present two optimal MDP algorithms. The first algorithm,  topological value iteration (TVI), detects the structure of MDPs and backs up states based on topological sequences. It (1) divides an MDP into strongly-connected components (SCCs), and (2) solves these components sequentially. TVI outperforms VI and other state-of-the-art algorithms vastly when an MDP has multiple, close-to-equal-sized SCCs. The second algorithm,  focused  topological value iteration (FTVI), is an extension of TVI. FTVI restricts its attention to connected components that are relevant for solving the MDP. Specifically, it uses a small amount of heuristic search to eliminate provably sub-optimal actions; this pruning allows FTVI to find smaller connected components, thus running faster.  We demonstrate that FTVI outperforms TVI by an order of magnitude, averaged across several domains. Surprisingly, FTVI also significantly outperforms popular `heuristically-informed' MDP algorithms such as ILAO*, LRTDP, BRTDP and Bayesian-RTDP in many domains, sometimes by as much as two orders of magnitude. Finally, we characterize the type of domains where FTVI excels --- suggesting a way to an informed choice of solver.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Topological Value Iteration Algorithms">
<meta name="citation_author" content="Dai, P.">
<meta name="citation_author" content=", Mausam">
<meta name="citation_author" content="Weld, D. S.">
<meta name="citation_author" content="Goldsmith, J.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="181">
<meta name="citation_lastpage" content="209">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3390/live-3390-5896-jair.pdf">

<cite>G. R.  Santhanam, S.  Basu and V.  Honavar (2011) "Representing and Reasoning with Qualitative Preferences for Compositional Systems", Volume 42, pages 211-274</cite>
<p class="media"><a href="/media/3339/live-3339-5899-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3339/live-3339-5910-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3339'>doi:10.1613/jair.3339</a></p>
<p>Many applications, e.g., Web service composition, complex system design, team formation, etc., rely on methods for identifying collections of objects or entities satisfying some functional requirement. Among the collections that satisfy the functional requirement, it is often necessary to identify one or more collections that are optimal with respect to user preferences over a set of attributes that describe the non-functional properties of the collection.<br />
<br />
We develop a formalism that lets users express the relative importance among attributes and qualitative preferences over the valuations of each attribute. We define a dominance relation that allows us to compare collections of objects in terms of preferences over attributes of the objects that make up the collection. We establish some key properties of the dominance relation. In particular, we show that the dominance relation is a strict partial order when the intra-attribute preference relations are strict partial orders and the relative importance preference relation is an interval order.<br />
<br />
We provide algorithms that use this dominance relation to identify the set of most preferred collections. We show that under certain conditions, the algorithms are guaranteed to return only (sound), all (complete), or at least one (weakly complete) of the most preferred collections. We present results of simulation experiments comparing the proposed algorithms with respect to (a) the quality of solutions (number of most preferred solutions) produced by the algorithms, and (b) their performance and efficiency. We also explore some interesting conjectures suggested by the results of our experiments that relate the properties of the user preferences, the dominance relation, and the algorithms.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Representing and Reasoning with Qualitative Preferences for Compositional Systems">
<meta name="citation_author" content="Santhanam, G. R.">
<meta name="citation_author" content="Basu, S.">
<meta name="citation_author" content="Honavar, V.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="211">
<meta name="citation_lastpage" content="274">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3339/live-3339-5899-jair.pdf">

<cite>R.  Ribeiro and D.  Martins de Matos (2011) "Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity", Volume 42, pages 275-308</cite>
<p class="media"><a href="/media/3387/live-3387-5920-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3387/live-3387-5919-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3387'>doi:10.1613/jair.3387</a></p>
<p>In automatic summarization, centrality-as-relevance means that the most important content of an information source, or a collection of information sources, corresponds to the most central passages, considering a representation where such notion makes sense (graph, spatial, etc.). We assess the main paradigms, and introduce a new centrality-based relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content. Geometric proximity is used to compute semantic relatedness. Centrality (relevance) is determined by considering the whole input source (and not only local information), and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized. The method consists in creating, for each passage of the input source, a support set consisting only of the most semantically related passages. Then, the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets. This model produces extractive summaries that are generic, and language- and domain-independent. Thorough automatic evaluation shows that the method achieves state-of-the-art performance, both in written text, and automatically transcribed speech summarization, including when compared to considerably more complex approaches.<br />
</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity">
<meta name="citation_author" content="Ribeiro, R.">
<meta name="citation_author" content="Martins de Matos, D.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="275">
<meta name="citation_lastpage" content="308">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3387/live-3387-5920-jair.pdf">

<cite>C.  Yuan, H.  Lim and T.  Lu (2011) "Most Relevant Explanation in Bayesian Networks", Volume 42, pages 309-352</cite>
<p class="media"><a href="/media/3301/live-3301-5931-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3301/live-3301-5930-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3301'>doi:10.1613/jair.3301</a></p>
<p>A major inference task in Bayesian networks is explaining why some variables are observed in their particular states using a set of target variables. Existing methods for solving this problem often generate explanations that are either too simple (underspecified) or too complex (overspecified). In this paper, we introduce a method called Most Relevant Explanation (MRE) which finds a partial instantiation of the target variables that maximizes the generalized Bayes factor (GBF) as the best explanation for the given evidence. Our study shows that GBF has several theoretical properties that enable MRE to automatically identify the most relevant target variables in forming its explanation. In particular, conditional Bayes factor (CBF), defined as the GBF of a new explanation conditioned on an existing explanation, provides a soft measure on the degree of relevance of the variables in the new explanation in explaining the evidence given the existing explanation. As a result, MRE is able to automatically prune less relevant variables from its explanation. We also show that CBF is able to capture well the explaining-away phenomenon that is often represented in Bayesian networks. Moreover, we define two dominance relations between the candidate solutions and use the relations to generalize MRE to find a set of top explanations that is both diverse and representative. Case studies on several benchmark diagnostic Bayesian networks show that MRE is often able to find explanatory hypotheses that are not only precise but also concise.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Most Relevant Explanation in Bayesian Networks">
<meta name="citation_author" content="Yuan, C.">
<meta name="citation_author" content="Lim, H.">
<meta name="citation_author" content="Lu, T.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="309">
<meta name="citation_lastpage" content="352">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3301/live-3301-5931-jair.pdf">

<cite>E.  Talvitie and S.  Singh (2011) "Learning to Make Predictions In Partially Observable Environments Without a Generative Model", Volume 42, pages 353-392</cite>
<p class="media"><a href="/media/3396/live-3396-5942-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3396/live-3396-5941-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3396'>doi:10.1613/jair.3396</a></p>
<p>When faced with the problem of learning a model of a high-dimensional environment, a common approach is to limit the model to make only a restricted set of predictions, thereby simplifying the learning problem. These partial models may be directly useful for making decisions or may be combined together to form a more complete, structured model. However, in partially observable (non-Markov) environments, standard model-learning methods learn generative models, i.e. models that provide a probability distribution over all possible futures (such as POMDPs). It is not straightforward to restrict such models to make only certain predictions, and doing so does not always simplify the learning problem. In this paper we present prediction profile models: non-generative partial models for partially observable systems that make only a given set of predictions, and are therefore far simpler than generative models in some cases. We formalize the problem of learning a prediction profile model as a transformation of the original model-learning problem, and show empirically that one can learn prediction profile models that make a small set of important predictions even in systems that are too complex for standard generative models.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Learning to Make Predictions In Partially Observable Environments Without a Generative Model">
<meta name="citation_author" content="Talvitie, E.">
<meta name="citation_author" content="Singh, S.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="353">
<meta name="citation_lastpage" content="392">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3396/live-3396-5942-jair.pdf">

<cite>P.  D. Grunwald and J.  Y. Halpern (2011) "Making Decisions Using Sets of Probabilities: Updating, Time Consistency, and Calibration", Volume 42, pages 393-426</cite>
<p class="media"><a href="/media/3374/live-3374-5957-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3374/live-3374-5958-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3374'>doi:10.1613/jair.3374</a></p>
<p>We consider how an agent should update her beliefs when her beliefs are represented by a set P of probability distributions, given that the agent makes decisions using the minimax criterion, perhaps the best-studied and most commonly-used criterion in the literature. We adopt a game-theoretic framework, where the agent plays against a bookie, who chooses some distribution from P. We consider two reasonable games that differ in what the bookie knows when he makes his choice. Anomalies that have been observed before, like time inconsistency, can be understood as arising because different games are being played, against bookies with different information. We characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information. Finally, we consider the relationship between updating and calibration when uncertainty is described by sets of probabilities. Our results emphasize the key role of the rectangularity condition of Epstein and Schneider.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Making Decisions Using Sets of Probabilities: Updating, Time Consistency, and Calibration">
<meta name="citation_author" content="Grunwald, P. D.">
<meta name="citation_author" content="Halpern, J. Y.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="393">
<meta name="citation_lastpage" content="426">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3374/live-3374-5957-jair.pdf">

<cite>D.  Golovin and A.  Krause (2011) "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization", Volume 42, pages 427-486</cite>
<cite>2013 IJCAI-JAIR Best Paper</cite><p class="media"><a href="/media/3278/live-3278-5964-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3278/live-3278-5965-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3278'>doi:10.1613/jair.3278</a></p>
<p>Many problems in artificial intelligence require adaptively making a sequence of decisions with uncertain outcomes under partial observability. Solving such stochastic optimization problems is a fundamental but notoriously difficult challenge.  In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies.  We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing  performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations. We illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse AI applications including management of sensing resources, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations. </p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization">
<meta name="citation_author" content="Golovin, D.">
<meta name="citation_author" content="Krause, A.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="427">
<meta name="citation_lastpage" content="486">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3278/live-3278-5964-jair.pdf">

<cite>M.  Alviano, F.  Calimeri, W.  Faber, N.  Leone and S.  Perri (2011) "Unfounded Sets and Well-Founded Semantics of Answer Set Programs with Aggregates ", Volume 42, pages 487-527</cite>
<p class="media"><a href="/media/3432/live-3432-5974-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3432/live-3432-5973-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3432'>doi:10.1613/jair.3432</a></p>
<p>Logic programs with aggregates (LPA) are one of the major linguistic extensions to Logic Programming (LP). In this work, we propose a generalization of the notions of unfounded set and well-founded semantics for programs with monotone and antimonotone aggregates (LPAma programs). In particular, we present a new notion of unfounded set for LPAma programs, which is a sound generalization of the original definition for standard (aggregate-free) LP. On this basis, we define a well-founded operator for LPAma programs, the fixpoint of which is called well-founded model (or well-founded semantics) for LPAma programs. The most important properties of unfounded sets and the well-founded semantics for standard LP are retained by this generalization, notably existence and uniqueness of the well-founded model, together with a strong relationship to the answer set semantics for LPAma programs. We show that one of the D-well-founded semantics, defined by Pelov, Denecker, and Bruynooghe for a broader class of aggregates using approximating operators, coincides with the well-founded model as defined in this work on LPAma programs. We also discuss some complexity issues, most importantly we give a formal proof of tractable computation of the well-founded model for LPA programs. Moreover, we prove that for general LPA programs, which may contain aggregates that are neither monotone nor antimonotone, deciding satisfaction of aggregate expressions with respect to partial interpretations is coNP-complete. As a consequence, a well-founded semantics for general LPA programs that allows for tractable computation is unlikely to exist, which justifies the restriction on LPAma programs. Finally, we present a prototype system extending DLV, which supports the well-founded semantics for LPAma programs, at the time of writing the only implemented system that does so. Experiments with this prototype show significant computational advantages of aggregate constructs over equivalent aggregate-free encodings.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Unfounded Sets and Well-Founded Semantics of Answer Set Programs with Aggregates ">
<meta name="citation_author" content="Alviano, M.">
<meta name="citation_author" content="Calimeri, F.">
<meta name="citation_author" content="Faber, W.">
<meta name="citation_author" content="Leone, N.">
<meta name="citation_author" content="Perri, S.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="487">
<meta name="citation_lastpage" content="527">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3432/live-3432-5974-jair.pdf">

<cite>E.  Elkind, P.  Faliszewski and A.  Slinko (2011) "Cloning in Elections: Finding the Possible Winners", Volume 42, pages 529-573</cite>
<p class="media"><a href="/media/3468/live-3468-5978-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3468/live-3468-5977-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3468'>doi:10.1613/jair.3468</a></p>
<p>We consider the problem of manipulating elections by cloning candidates. In our model, a manipulator can replace each candidate c by several clones, i.e., new candidates that are so similar to c that each voter simply replaces c in his vote with a block of these new candidates, ranked consecutively. The outcome of the resulting election may then depend on the number of clones as well as on how each voter orders the clones within the block. We formalize what it means for a cloning manipulation to be successful (which turns out to be a surprisingly delicate issue), and, for a number of common voting rules, characterize the preference profiles for which a successful cloning manipulation exists. We also consider the model where there is a cost associated with producing each clone, and study the complexity of finding a minimum-cost cloning manipulation. Finally, we compare cloning with two related problems: the problem of control by adding candidates and the problem of possible (co)winners when new alternatives can join.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Cloning in Elections: Finding the Possible Winners">
<meta name="citation_author" content="Elkind, E.">
<meta name="citation_author" content="Faliszewski, P.">
<meta name="citation_author" content="Slinko, A.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="529">
<meta name="citation_lastpage" content="573">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3468/live-3468-5978-jair.pdf">

<cite>M.  Ponsen, S.  de Jong and M.  Lanctot (2011) "Computing Approximate Nash Equilibria and Robust Best-Responses Using Sampling", Volume 42, pages 575-605</cite>
<p class="media"><a href="/media/3402/live-3402-5992-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3402/live-3402-5991-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3402'>doi:10.1613/jair.3402</a></p>
<p>This article discusses two contributions to decision-making in complex partially observable stochastic games. First, we apply two state-of-the-art search techniques that use Monte-Carlo sampling to the task of approximating a Nash-Equilibrium (NE) in such games, namely Monte-Carlo Tree Search (MCTS) and Monte-Carlo Counterfactual Regret Minimization (MCCFR). MCTS has been proven to approximate a NE in perfect-information games. We show that the algorithm quickly finds a reasonably strong strategy (but not a NE) in a complex imperfect information game, i.e. Poker. MCCFR on the other hand has theoretical NE convergence guarantees in such a game. We apply MCCFR for the first time in Poker. Based on our experiments, we may conclude that MCTS is a valid approach if one wants to learn reasonably strong strategies fast, whereas MCCFR is the better choice if the quality of the strategy is most important. <br />
<br />
Our second contribution relates to the observation that a NE is not a best response against players that are not playing a NE. We present Monte-Carlo Restricted Nash Response (MCRNR), a sample-based algorithm for the computation of restricted Nash strategies. These are robust best-response strategies that (1) exploit non-NE opponents more than playing a NE and (2) are not (overly) exploitable by other strategies. We combine the advantages of two state-of-the-art algorithms, i.e. MCCFR and Restricted Nash Response (RNR). MCRNR samples only relevant parts of the game tree. We show that MCRNR learns quicker than standard RNR in smaller games. Also we show in Poker that MCRNR learns robust best-response strategies fast, and that these strategies exploit opponents more than playing a NE does.<br />
</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Computing Approximate Nash Equilibria and Robust Best-Responses Using Sampling">
<meta name="citation_author" content="Ponsen, M.">
<meta name="citation_author" content="de Jong, S.">
<meta name="citation_author" content="Lanctot, M.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="575">
<meta name="citation_lastpage" content="605">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3402/live-3402-5992-jair.pdf">

<cite>P.  R. Conrad and B.  C. Williams (2011) "Drake: An Efficient Executive for Temporal Plans with Choice", Volume 42, pages 607-659</cite>
<p class="media"><a href="/media/3478/live-3478-5996-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3478/live-3478-5995-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3478'>doi:10.1613/jair.3478</a></p>
<p>This work presents Drake, a dynamic executive for temporal plans with choice. Dynamic plan execution strategies allow an autonomous agent to react quickly to unfolding events, improving the robustness of the agent. Prior work developed methods for dynamically dispatching Simple Temporal Networks, and further research enriched the expressiveness of the plans executives could handle, including discrete choices, which are the focus of this work. However, in some approaches to date, these additional choices induce significant storage or latency requirements to make flexible execution possible. <br />
<br />
Drake is designed to leverage the low latency made possible by a preprocessing step called compilation, while avoiding high memory costs through a compact representation. We leverage the concepts of labels and environments, taken from prior work in Assumption-based Truth Maintenance Systems (ATMS), to concisely record the implications of the discrete choices, exploiting the structure of the plan to avoid redundant reasoning or storage. Our labeling and maintenance scheme, called the Labeled Value Set Maintenance System, is distinguished by its focus on properties fundamental to temporal problems, and, more generally, weighted graph algorithms. In particular, the maintenance system focuses on maintaining a minimal representation of non-dominated constraints. We benchmark Drake's performance on random structured problems, and find that Drake reduces the size of the compiled representation by a factor of over 500 for large problems, while incurring only a modest increase in run-time latency, compared to prior work in compiled executives for temporal plans with discrete choices.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Drake: An Efficient Executive for Temporal Plans with Choice">
<meta name="citation_author" content="Conrad, P. R.">
<meta name="citation_author" content="Williams, B. C.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="607">
<meta name="citation_lastpage" content="659">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3478/live-3478-5996-jair.pdf">

<cite>J.  M. Pe&#241;a (2011) "Finding Consensus Bayesian Network Structures", Volume 42, pages 661-687</cite>
<p class="media"><a href="/media/3427/live-3427-6003-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3427/live-3427-6002-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3427'>doi:10.1613/jair.3427</a></p>
<p>Suppose that multiple experts (or learning algorithms) provide us with alternative Bayesian network (BN) structures over a domain, and that we are interested in combining them into a single consensus BN structure. Specifically, we are interested in that the consensus BN structure only represents independences all the given BN structures agree upon and that it has as few parameters associated as possible. In this paper, we prove that there may exist several non-equivalent consensus BN structures and that finding one of them is NP-hard. Thus, we decide to resort to heuristics to find an approximated consensus BN structure. In this paper, we consider the heuristic proposed by Matzkevich and Abramson, which builds upon two algorithms, called Methods A and B, for efficiently deriving the minimal directed independence map of a BN structure relative to a given node ordering. Methods A and B are claimed to be correct although no proof is provided (a proof is just sketched). In this paper, we show that Methods A and B are not correct and propose a correction of them.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Finding Consensus Bayesian Network Structures">
<meta name="citation_author" content="Pe&#241;a, J. M.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="661">
<meta name="citation_lastpage" content="687">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3427/live-3427-6003-jair.pdf">

<cite>E.  Amig&#243;, J.  Gonzalo, J.  Artiles and F.  Verdejo (2011) "Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks", Volume 42, pages 689-718</cite>
<p class="media"><a href="/media/3401/live-3401-6013-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3401/live-3401-6016-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3401'>doi:10.1613/jair.3401</a></p>
<p>Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. A problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria (such as van Rijsbergen’s F-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. UIR is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted.<br />
<br />
Besides discussing the theoretical foundations of UIR, this paper presents empirical results that confirm the validity and usefulness of the metric for the Text Clustering problem, where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them. Remarkably, our experiments show that UIR can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks">
<meta name="citation_author" content="Amig&#243;, E.">
<meta name="citation_author" content="Gonzalo, J.">
<meta name="citation_author" content="Artiles, J.">
<meta name="citation_author" content="Verdejo, F.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="689">
<meta name="citation_lastpage" content="718">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3401/live-3401-6013-jair.pdf">

<cite>P.  A. Bonatti, M.  Faella and L.  Sauro (2011) "Defeasible Inclusions in Low-Complexity DLs", Volume 42, pages 719-764</cite>
<p class="media"><a href="/media/3360/live-3360-6018-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3360/live-3360-6020-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3360'>doi:10.1613/jair.3360</a></p>
<p>Some of the applications of OWL and RDF (e.g. biomedical knowledge representation and semantic policy formulation) call for extensions of these languages with nonmonotonic constructs such as inheritance with overriding.  Nonmonotonic description logics have been studied for many years, however no practical such knowledge representation languages exist, due to a combination of semantic difficulties and high computational complexity. Independently, low-complexity description logics such as DL-lite and EL have been introduced and incorporated in the OWL standard.  Therefore, it is interesting to see whether the syntactic restrictions characterizing DL-lite and EL bring computational benefits to their nonmonotonic versions, too. In this paper we extensively investigate the computational complexity of Circumscription when knowledge bases are formulated in DL-lite_R, EL, and fragments thereof.  We identify fragments whose complexity ranges from P to the second level of the polynomial hierarchy, as well as fragments whose complexity raises to PSPACE and beyond.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Defeasible Inclusions in Low-Complexity DLs">
<meta name="citation_author" content="Bonatti, P. A.">
<meta name="citation_author" content="Faella, M.">
<meta name="citation_author" content="Sauro, L.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="719">
<meta name="citation_lastpage" content="764">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3360/live-3360-6018-jair.pdf">

<cite>P.  Vytelingum, T.  D. Voice, S.  D. Ramchurn, A.  Rogers and N.  R. Jennings (2011) "Theoretical and Practical Foundations of Large-Scale Agent-Based Micro-Storage in the Smart Grid", Volume 42, pages 765-813</cite>
<cite>AAMAS 2010 iRobot Best Paper Award</cite><p class="media"><a href="/media/3446/live-3446-6022-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3446/live-3446-6021-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3446'>doi:10.1613/jair.3446</a></p>
<p>In this paper, we present a novel decentralised management technique that allows electricity micro-storage devices, deployed within individual homes as part of a smart electricity grid, to converge to profitable and efficient behaviours. Specifically, we propose the use of software agents, residing on the users' smart meters, to automate and optimise the charging cycle of micro-storage devices in the home to minimise its costs, and we present a study of both the theoretical underpinnings and the implications of a practical solution, of using software agents for such micro-storage management. First, by formalising the strategic choice each agent makes in deciding when to charge its battery, we develop a game-theoretic framework within which we can analyse the competitive equilibria of an electricity grid populated by such agents and hence predict the best consumption profile for that population given their battery properties and individual load profiles. Our framework also allows us to compute theoretical bounds on the amount of storage that will be adopted by the population. Second, to analyse the practical implications of micro-storage deployments in the grid, we present a novel algorithm that each agent can use to optimise its battery storage profile in order to minimise its owner's costs. This algorithm uses a learning strategy that allows it to adapt as the price of electricity changes in real-time, and we show that the adoption of these strategies results in the system converging to the theoretical equilibria. Finally, we empirically evaluate the adoption of our micro-storage management technique within a complex setting, based on the UK electricity market, where agents may have widely varying load profiles, battery types, and learning rates. In this case, our approach yields savings of up to 14% in energy cost for an average consumer using a storage device with a capacity of less than 4.5 kWh and up to a 7% reduction in carbon emissions resulting from electricity generation (with only domestic consumers adopting micro-storage and, commercial and industrial consumers not changing their demand). Moreover, corroborating our theoretical bound, an equilibrium is shown to exist where no more than 48% of households would wish to own storage devices and where social welfare would also be improved (yielding overall annual savings of nearly &#163;1.5B).</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Theoretical and Practical Foundations of Large-Scale Agent-Based Micro-Storage in the Smart Grid">
<meta name="citation_author" content="Vytelingum, P.">
<meta name="citation_author" content="Voice, T. D.">
<meta name="citation_author" content="Ramchurn, S. D.">
<meta name="citation_author" content="Rogers, A.">
<meta name="citation_author" content="Jennings, N. R.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="765">
<meta name="citation_lastpage" content="813">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3446/live-3446-6022-jair.pdf">

<cite>J.  Wu, R.  Kalyanam and R.  Givan (2011) "Stochastic Enforced Hill-Climbing", Volume 42, pages 815-850</cite>
<p class="media"><a href="/media/3420/live-3420-6032-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3420/live-3420-6031-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3420'>doi:10.1613/jair.3420</a>
<br/><a href="/media/3420/live-3420-6035-jair.pdf">Appendix 1</a> - Per-problem results&nbsp;|&nbsp;<a href="/media/3420/live-3420-6036-jair.zip">Appendix 2</a> - Additionally generated problems used in the evaluation&nbsp;|&nbsp;<a href="/media/3420/live-3420-6037-jair.pdf">Appendix 3</a> - Details of omitted problems
</p>
<p>Enforced hill-climbing is an effective deterministic hill-climbing technique that deals with local optima using breadth-first search (a process called ``basin flooding'').  We propose and evaluate a stochastic generalization<br />
of enforced hill-climbing for online use in goal-oriented probabilistic planning problems. We assume a provided heuristic function estimating expected cost to the goal with flaws such as local optima and plateaus that thwart straightforward greedy action choice.  While breadth-first search is effective in exploring basins around local optima in deterministic problems, for stochastic problems we dynamically build and solve a heuristic-based Markov decision process (MDP) model of the basin in order to find a good escape policy exiting the local optimum.  We note that building this model involves integrating the heuristic into the MDP problem because the local goal is to improve the heuristic.<br />
<br />
We evaluate our proposal in twenty-four recent probabilistic planning-competition benchmark domains and twelve probabilistically interesting problems from recent literature.  For evaluation, we show that stochastic enforced hill-climbing (SEH) produces better policies than greedy heuristic following for value/cost functions derived in two very different ways: one type derived by using deterministic heuristics on a deterministic relaxation and a second type derived by automatic learning of Bellman-error features from domain-specific experience. Using the first type of heuristic, SEH is shown to generally outperform all planners from the first three international probabilistic planning competitions.<br />
</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Stochastic Enforced Hill-Climbing">
<meta name="citation_author" content="Wu, J.">
<meta name="citation_author" content="Kalyanam, R.">
<meta name="citation_author" content="Givan, R.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="815">
<meta name="citation_lastpage" content="850">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3420/live-3420-6037-jair.pdf">

<cite>M.  L. Ginsberg (2011) "Dr.Fill: Crosswords and an Implemented Solver for Singly Weighted CSPs", Volume 42, pages 851-886</cite>
<p class="media"><a href="/media/3437/live-3437-6039-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3437/live-3437-6040-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3437'>doi:10.1613/jair.3437</a></p>
<p>We describe Dr.Fill, a program that solves American-style crossword puzzles.  From a technical perspective, Dr.Fill works by converting crosswords to weighted CSPs, and then using a variety of novel techniques to find a solution.  These techniques include generally applicable heuristics for variable and value selection, a variant of limited discrepancy search, and postprocessing and partitioning ideas.  Branch and bound is not used, as it was incompatible with postprocessing and was determined experimentally to be of little practical value.  Dr.Filll's performance on crosswords from the American Crossword Puzzle Tournament suggests that it ranks among the top fifty or so crossword solvers in the world.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Dr.Fill: Crosswords and an Implemented Solver for Singly Weighted CSPs">
<meta name="citation_author" content="Ginsberg, M. L.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="851">
<meta name="citation_lastpage" content="886">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3437/live-3437-6039-jair.pdf">

<cite>N.  Agmon, G.  A. Kaminka and S.  Kraus (2011) "Multi-Robot Adversarial Patrolling: Facing a Full-Knowledge Opponent", Volume 42, pages 887-916</cite>
<p class="media"><a href="/media/3365/live-3365-6046-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3365/live-3365-6047-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3365'>doi:10.1613/jair.3365</a></p>
<p>The problem of adversarial multi-robot patrol has gained interest in recent years, mainly due to its immediate relevance to various security applications. In this problem, robots are required to repeatedly visit a target area in a way that maximizes their chances of detecting an adversary trying to penetrate through the patrol path. When facing a strong adversary that knows the patrol strategy of the robots, if the robots use a deterministic patrol algorithm, then in many cases it is easy for the adversary to penetrate undetected (in fact, in some of those cases the adversary can guarantee penetration). Therefore this paper presents a non-deterministic patrol framework for the robots. Assuming that the strong adversary will take advantage of its knowledge and try to penetrate through the patrol's weakest spot, hence an optimal algorithm is one that maximizes the chances of detection in that point. We therefore present a polynomial-time algorithm for determining an optimal patrol under the Markovian strategy assumption for the robots, such that the probability of detecting the adversary in the patrol's weakest spot is maximized. We build upon this framework and describe an optimal patrol strategy for several robotic models based on their movement abilities (directed or undirected) and sensing abilities (perfect or imperfect), and in different environment models - either patrol around a perimeter (closed polygon) or an open fence (open polyline).<br />
</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Multi-Robot Adversarial Patrolling: Facing a Full-Knowledge Opponent">
<meta name="citation_author" content="Agmon, N.">
<meta name="citation_author" content="Kaminka, G. A.">
<meta name="citation_author" content="Kraus, S.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="887">
<meta name="citation_lastpage" content="916">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3365/live-3365-6046-jair.pdf">

<cite>D.  Gabbay, D.  Pearce and A.  Valverde (2011) "Interpolable Formulas in Equilibrium Logic and Answer Set Programming", Volume 42, pages 917-943</cite>
<p class="media"><a href="/media/3329/live-3329-6054-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3329/live-3329-6053-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3329'>doi:10.1613/jair.3329</a></p>
<p>Interpolation is an important property of classical and many non-classical logics that has been shown to have interesting applications in computer science and AI. Here we study the Interpolation Property for the the non-monotonic system of equilibrium logic, establishing weaker or stronger forms of interpolation depending on the precise interpretation of the inference relation. These results also yield a form of interpolation for ground logic programs under the answer sets semantics. For disjunctive logic programs we also study the property of uniform interpolation that is closely related to the concept of variable forgetting. The first-order version of equilibrium logic has analogous Interpolation properties whenever the collection of equilibrium models is (first-order) definable. Since this is the case for so-called safe programs and theories, it applies to the usual situations that arise in practical answer set programming.</p>
<a href="/vol/vol42.html">Click here to return to Volume 42 contents list</a>
<meta name="citation_title" content="Interpolable Formulas in Equilibrium Logic and Answer Set Programming">
<meta name="citation_author" content="Gabbay, D.">
<meta name="citation_author" content="Pearce, D.">
<meta name="citation_author" content="Valverde, A.">
<meta name="citation_publication_date" content="2011">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="917">
<meta name="citation_lastpage" content="943">
<meta name="citation_volume" content="42">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3329/live-3329-6054-jair.pdf">
