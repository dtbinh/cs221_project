<cite>P.  Nightingale, I.  P. Gent, C.  Jefferson and I.  Miguel (2013) "Short and Long Supports for Constraint Propagation", Volume 46, pages 1-45</cite>
<p class="media"><a href="/media/3749/live-3749-6780-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3749/live-3749-6781-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3749'>doi:10.1613/jair.3749</a></p>
<p>Special-purpose constraint propagation algorithms frequently make implicit use of short supports -- by examining a subset of the variables, they can infer support (a justification that a variable-value pair may still form part of an assignment that satisfies the constraint) for all other variables and values and save substantial work -- but short supports have not been studied in their own right. The two main contributions of this paper are the identification of short supports as important for constraint propagation, and the introduction of HaggisGAC, an efficient and effective general purpose propagation algorithm for exploiting short supports. Given the complexity of HaggisGAC, we present it as an optimised version of a simpler algorithm ShortGAC. Although experiments demonstrate the efficiency of ShortGAC compared with other general-purpose propagation algorithms where a compact set of short supports is available, we show theoretically and experimentally that HaggisGAC is even better. We also find that HaggisGAC performs better than GAC-Schema on full-length supports. We also introduce a variant algorithm HaggisGAC-Stable, which is adapted to avoid work on backtracking and in some cases can be faster and have significant reductions in memory use. All the proposed algorithms are excellent for propagating disjunctions of constraints. In all experiments with disjunctions we found our algorithms to be faster than Constructive Or and GAC-Schema by at least an order of magnitude, and up to three orders of magnitude.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Short and Long Supports for Constraint Propagation">
<meta name="citation_author" content="Nightingale, P.">
<meta name="citation_author" content="Gent, I. P.">
<meta name="citation_author" content="Jefferson, C.">
<meta name="citation_author" content="Miguel, I.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="45">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3749/live-3749-6780-jair.pdf">

<cite>E.  Huang and R.  E. Korf (2013) "Optimal Rectangle Packing: An Absolute Placement Approach", Volume 46, pages 47-87</cite>
<p class="media"><a href="/media/3735/live-3735-6794-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3735/live-3735-6793-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3735'>doi:10.1613/jair.3735</a></p>
<p>We consider the problem of finding all enclosing rectangles of minimum area that can contain a given set of rectangles without overlap.  Our rectangle packer chooses the x-coordinates of all the rectangles before any of the y-coordinates. We then transform the problem into a perfect-packing problem with no empty space by adding additional rectangles. To determine the y-coordinates, we branch on the different rectangles that can be placed in each empty position. Our packer allows us to extend the known solutions for a consecutive-square benchmark from 27 to 32 squares. We also introduce three new benchmarks, avoiding properties that make a benchmark easy, such as rectangles with shared dimensions. Our third benchmark consists of rectangles of increasingly high precision. To pack them efficiently, we limit the rectangles' coordinates and the bounding box dimensions to the set of subset sums of the rectangles' dimensions. Overall, our algorithms represent the current state-of-the-art for this problem, outperforming other algorithms by orders of magnitude, depending on the benchmark.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Optimal Rectangle Packing: An Absolute Placement Approach">
<meta name="citation_author" content="Huang, E.">
<meta name="citation_author" content="Korf, R. E.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="47">
<meta name="citation_lastpage" content="87">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3735/live-3735-6794-jair.pdf">

<cite>C.  Sauper and R.  Barzilay (2013) "Automatic Aggregation by Joint Modeling of Aspects and Values", Volume 46, pages 89-127</cite>
<p class="media"><a href="/media/3647/live-3647-6805-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3647/live-3647-6803-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3647'>doi:10.1613/jair.3647</a></p>
<p>We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis.  Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect.  This approach directly enables discovery of highly-rated or inconsistent aspects of a product.  Our generative model admits an efficient variational mean-field inference algorithm.  It is also easily extensible, and we describe several modifications and their effects on model structure and inference.  We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries.  We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy.  We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Automatic Aggregation by Joint Modeling of Aspects and Values">
<meta name="citation_author" content="Sauper, C.">
<meta name="citation_author" content="Barzilay, R.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="89">
<meta name="citation_lastpage" content="127">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3647/live-3647-6805-jair.pdf">

<cite>M.  Guo, E.  Markakis, K.  R. Apt and V.  Conitzer (2013) "Undominated Groves Mechanisms", Volume 46, pages 129-163</cite>
<p class="media"><a href="/media/3810/live-3810-6808-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3810/live-3810-6807-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3810'>doi:10.1613/jair.3810</a></p>
<p>The family of Groves mechanisms, which includes the well-known VCG mechanism (also known as the Clarke mechanism), is a family of efficient and strategy-proof mechanisms. Unfortunately, the Groves mechanisms are generally not budget balanced. That is, under such mechanisms, payments may flow into or out of the system of the agents, resulting in deficits or reduced utilities for the agents. We consider the following problem: within the family of Groves mechanisms, we want to identify mechanisms that give the agents the highest utilities, under the constraint that these mechanisms must never incur deficits.<br />
<br />
We adopt a prior-free approach. We introduce two general measures for comparing mechanisms in prior-free settings. We say that a non-deficit Groves mechanism M individually dominates another non-deficit Groves mechanism M' if for every type profile, every agent's utility under M is no less than that under M', and this holds with strict inequality for at least one type profile and one agent. We say that a non-deficit Groves mechanism M collectively dominates another non-deficit Groves mechanism M' if for every type profile, the agents' total utility under M is no less than that under M', and this holds with strict inequality for at least one type profile. The above definitions induce two partial orders on non-deficit Groves mechanisms. We study the maximal elements corresponding to these two partial orders, which we call the individually undominated mechanisms and the collectively undominated mechanisms, respectively.<br />
</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Undominated Groves Mechanisms">
<meta name="citation_author" content="Guo, M.">
<meta name="citation_author" content="Markakis, E.">
<meta name="citation_author" content="Apt, K. R.">
<meta name="citation_author" content="Conitzer, V.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="129">
<meta name="citation_lastpage" content="163">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3810/live-3810-6808-jair.pdf">

<cite>V.  Qazvinian, D.  R. Radev, S.  M. Mohammad, B.  Dorr, D.  Zajic, M.  Whidby and T.  Moon (2013) "Generating Extractive Summaries of Scientific Paradigms", Volume 46, pages 165-201</cite>
<p class="media"><a href="/media/3732/live-3732-6832-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3732/live-3732-6833-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3732'>doi:10.1613/jair.3732</a></p>
<p>Researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. Our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature.  We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose C-LexRank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. Next, we further extend our experiments  to summarize a set of papers, which cover the same scientific topic. We generate extractive summaries of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Generating Extractive Summaries of Scientific Paradigms">
<meta name="citation_author" content="Qazvinian, V.">
<meta name="citation_author" content="Radev, D. R.">
<meta name="citation_author" content="Mohammad, S. M.">
<meta name="citation_author" content="Dorr, B.">
<meta name="citation_author" content="Zajic, D.">
<meta name="citation_author" content="Whidby, M.">
<meta name="citation_author" content="Moon, T.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="165">
<meta name="citation_lastpage" content="201">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3732/live-3732-6832-jair.pdf">

<cite>H.  Zhao, X.  Zhang and C.  Kit (2013) "Integrative Semantic Dependency Parsing via Efficient Large-scale Feature Selection", Volume 46, pages 203-233</cite>
<p class="media"><a href="/media/3717/live-3717-6837-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3717/live-3717-6836-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3717'>doi:10.1613/jair.3717</a>
<br/><a href="http://bcmi.sjtu.edu.cn/~zhaohai/TSRLENAllT.txt">Appendix </a> - Feature templates data set </p>
<p>Semantic parsing, i.e., the automatic derivation of meaning representation such as an instantiated predicate-argument structure for a sentence, plays a critical role in deep processing of natural language. Unlike all other top systems of semantic dependency parsing that have to rely on a pipeline framework to chain up a series of submodels each specialized for a specific subtask, the one presented in this article integrates everything into one model, in hopes of achieving desirable integrity and practicality for real applications while maintaining a competitive performance. This integrative approach tackles semantic parsing as a word pair classification problem using a maximum entropy classifier. We leverage adaptive pruning of argument candidates and large-scale feature selection engineering to allow the largest feature space ever in use so far in this field, it achieves a state-of-the-art performance on the evaluation data set for CoNLL-2008 shared task, on top of all but one top pipeline system, confirming its feasibility and effectiveness.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Integrative Semantic Dependency Parsing via Efficient Large-scale Feature Selection">
<meta name="citation_author" content="Zhao, H.">
<meta name="citation_author" content="Zhang, X.">
<meta name="citation_author" content="Kit, C.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="203">
<meta name="citation_lastpage" content="233">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3717/live-3717-6837-jair.pdf">

<cite>N.  Goernitz, M.  Kloft, K.  Rieck and U.  Brefeld (2013) "Toward Supervised Anomaly Detection", Volume 46, pages 235-262</cite>
<p class="media"><a href="/media/3623/live-3623-6845-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3623/live-3623-6844-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3623'>doi:10.1613/jair.3623</a></p>
<p>Anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions. However, the predictive performance of purely unsupervised anomaly detection  often fails to match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation. Our first contribution shows that  classical semi-supervised approaches, originating from a supervised classifier, are inappropriate and hardly detect  new and unknown anomalies. We argue that semi-supervised anomaly detection  needs to ground on the unsupervised learning paradigm and devise a novel algorithm that meets this requirement. Although being intrinsically non-convex, we further show that the optimization problem has a convex equivalent under relatively mild assumptions. Additionally, we propose an active learning strategy to automatically filter candidates for  labeling. In an empirical study on network intrusion detection data, we observe that the proposed learning methodology requires much less labeled data than the state-of-the-art, while achieving higher detection accuracies.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Toward Supervised Anomaly Detection">
<meta name="citation_author" content="Goernitz, N.">
<meta name="citation_author" content="Kloft, M.">
<meta name="citation_author" content="Rieck, K.">
<meta name="citation_author" content="Brefeld, U.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="235">
<meta name="citation_lastpage" content="262">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3623/live-3623-6845-jair.pdf">

<cite>S.  Ordyniak and S.  Szeider (2013) "Parameterized Complexity Results for Exact Bayesian Network Structure Learning", Volume 46, pages 263-302</cite>
<p class="media"><a href="/media/3744/live-3744-6870-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3744/live-3744-6871-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3744'>doi:10.1613/jair.3744</a></p>
<p>Bayesian network structure learning is the notoriously difficult problem of discovering a Bayesian network that optimally represents a given set of training data.  In this paper we study the computational worst-case complexity of exact Bayesian network structure learning under graph theoretic restrictions on the (directed) super-structure.  The super-structure is an undirected graph that contains as subgraphs the skeletons of solution networks. We introduce the directed super-structure as a natural generalization of its undirected counterpart. Our results apply to several variants of score-based Bayesian network structure learning where the score of a network decomposes into local scores of its nodes.<br />
<br />
Results: We show that exact Bayesian network structure learning can be carried out in non-uniform polynomial time if the super-structure has bounded treewidth, and in linear time if in addition the super-structure has bounded maximum degree. Furthermore, we show that if the directed super-structure is acyclic, then exact Bayesian network structure learning can be carried out in quadratic time. We complement these positive results with a number of hardness results. We show that both restrictions (treewidth and degree) are essential and cannot be dropped without loosing uniform polynomial time tractability (subject to a complexity-theoretic assumption). Similarly, exact Bayesian network structure learning remains NP-hard for "almost acyclic" directed super-structures.  Furthermore, we show that the restrictions remain essential if we do not search for a globally optimal network but aim to improve a given network by means of at most k arc additions, arc deletions, or arc reversals (k-neighborhood local search).  </p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Parameterized Complexity Results for Exact Bayesian Network Structure Learning">
<meta name="citation_author" content="Ordyniak, S.">
<meta name="citation_author" content="Szeider, S.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="263">
<meta name="citation_lastpage" content="302">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3744/live-3744-6870-jair.pdf">

<cite>A.  Metodi, M.  Codish and P.  J. Stuckey (2013) "Boolean Equi-propagation for Concise and Efficient SAT Encodings of  Combinatorial Problems", Volume 46, pages 303-341</cite>
<p class="media"><a href="/media/3809/live-3809-6877-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3809/live-3809-6878-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3809'>doi:10.1613/jair.3809</a></p>
<p>We present an approach to propagation-based SAT encoding of combinatorial problems, Boolean equi-propagation, where constraints are modeled as Boolean functions which propagate information about equalities between Boolean literals.  This information is then applied to simplify the CNF encoding of the constraints.  A key factor is that considering only a small fragment of a constraint model at one time enables us to apply stronger, and even complete, reasoning to detect equivalent literals in that fragment. Once detected, equivalences apply to simplify the entire constraint model and facilitate further reasoning on other fragments. Equi-propagation in combination with partial evaluation and constraint simplification provide the foundation for a powerful approach to SAT-based finite domain constraint solving.  We introduce a tool called BEE (Ben-Gurion Equi-propagation Encoder) based on these ideas and demonstrate for a variety of benchmarks that our approach leads to a considerable reduction in the size of CNF encodings and subsequent speed-ups in SAT solving times.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Boolean Equi-propagation for Concise and Efficient SAT Encodings of  Combinatorial Problems">
<meta name="citation_author" content="Metodi, A.">
<meta name="citation_author" content="Codish, M.">
<meta name="citation_author" content="Stuckey, P. J.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="303">
<meta name="citation_lastpage" content="341">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3809/live-3809-6877-jair.pdf">

<cite>A.  Coles, A.  Coles, M.  Fox and D.  Long (2013) "A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in Planning", Volume 46, pages 343-412</cite>
<p class="media"><a href="/media/3788/live-3788-6903-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3788/live-3788-6907-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3788'>doi:10.1613/jair.3788</a>
<br/><a href="/media/3788/live-3788-6906-jair.zip">Appendix </a> - The complete set of domain and problem files used for data generation. </p>
<p>Although the use of metric fluents is fundamental to many practical planning problems, the study of heuristics to support fully automated planners working with these fluents remains relatively unexplored. The most widely used heuristic is the relaxation of metric fluents into interval-valued variables --- an idea first proposed a decade ago. Other heuristics depend on domain encodings that supply additional information about fluents, such as capacity constraints or other resource-related annotations. <br />
<br />
A particular challenge to these approaches is in handling interactions between metric fluents that represent exchange, such as the transformation of quantities of raw materials into quantities of processed goods, or trading of money for materials. The usual relaxation of metric fluents is often very poor in these situations, since it does not recognise that resources, once spent, are no longer available to be spent again.<br />
<br />
We present a heuristic for numeric planning problems building on the propositional relaxed planning graph, but using a mathematical program for numeric reasoning.  We define a class of producer--consumer planning problems and demonstrate how the numeric constraints in these can be modelled in a mixed integer program (MIP).  This MIP is then combined with a metric Relaxed Planning Graph (RPG) heuristic to produce an integrated hybrid heuristic. The MIP tracks resource use more accurately than the usual relaxation, but relaxes the ordering of actions, while the RPG captures the causal propositional aspects of the problem.  We discuss how these two components interact to produce a single unified heuristic and go on to explore how further numeric features of planning problems can be integrated into the MIP. We show that encoding a limited subset of the propositional problem to augment the MIP can yield more accurate guidance, partly by exploiting structure such as propositional landmarks and propositional resources.  Our results show that the use of this heuristic enhances scalability on problems where numeric resource interaction is key in finding a solution.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in Planning">
<meta name="citation_author" content="Coles, A.">
<meta name="citation_author" content="Coles, A.">
<meta name="citation_author" content="Fox, M.">
<meta name="citation_author" content="Long, D.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="343">
<meta name="citation_lastpage" content="412">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3788/live-3788-6903-jair.pdf">

<cite>N.  A. Snooke and M.  H. Lee (2013) "Qualitative Order of Magnitude Energy-Flow-Based Failure Modes and Effects Analysis", Volume 46, pages 413-447</cite>
<p class="media"><a href="/media/3898/live-3898-6913-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3898/live-3898-6912-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3898'>doi:10.1613/jair.3898</a></p>
<p>This paper presents a structured power and energy-flow-based qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow. The modelling is split into two parts.<br />
Power flow is a global phenomenon and is therefore naturally represented and analysed by a network comprised of the relevant structural elements from the components of a system. The power flow analysis is a platform for higher-level behaviour prediction of energy related aspects using local component behaviour models to capture a state-based representation with a global time. The primary application is Failure Modes and Effects Analysis (FMEA) and a form of exaggeration reasoning is used, combined with an order of magnitude representation to derive the worst case failure modes.  <br />
<br />
The novel aspects of the work are an order of magnitude(OM) qualitative network analyser to represent any power domain and topology, including multiple power sources, a feature that was not required for earlier specialised electrical versions of the approach. Secondly, the representation of generalised energy related behaviour as state-based local models is presented as a modelling strategy that can be more vivid and intuitive for a range of topologically complex applications than qualitative equation-based representations.The two-level modelling strategy allows the broad system behaviour coverage of qualitative simulation to be exploited for the FMEA task, while limiting the difficulties of qualitative ambiguity explanation that can arise from abstracted numerical models. We have used the method to support an automated FMEA system with examples of an aircraft fuel system and domestic a heating system discussed in this paper.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Qualitative Order of Magnitude Energy-Flow-Based Failure Modes and Effects Analysis">
<meta name="citation_author" content="Snooke, N. A.">
<meta name="citation_author" content="Lee, M. H.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="413">
<meta name="citation_lastpage" content="447">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3898/live-3898-6913-jair.pdf">

<cite>F.  A. Oliehoek, M.  T. J. Spaan, C.  Amato and S.  Whiteson (2013) "Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs", Volume 46, pages 449-509</cite>
<p class="media"><a href="/media/3804/live-3804-6917-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3804/live-3804-6918-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3804'>doi:10.1613/jair.3804</a></p>
<p>This article presents the state-of-the-art in optimal solution methods for decentralized partially observable Markov decision processes (Dec-POMDPs), which are general models for collaborative multiagent planning under uncertainty. Building off the generalized multiagent A* (GMAA*) algorithm, which reduces the problem to a tree of one-shot collaborative Bayesian games (CBGs), we describe several advances that greatly expand the range of Dec-POMDPs that can be solved optimally.  First, we introduce lossless incremental clustering of the CBGs solved by GMAA*, which achieves exponential speedups without sacrificing optimality.  Second, we introduce incremental expansion of nodes in the GMAA* search tree, which avoids the need to expand all children, the number of which is in the worst case doubly exponential in the node's depth.  This is particularly beneficial when little clustering is possible.  In addition, we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger Dec-POMDPs.  We provide theoretical guarantees that, when a suitable heuristic is used, both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent.  Finally, we present extensive empirical results demonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, can optimally solve Dec-POMDPs of unprecedented size.<br />
</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs">
<meta name="citation_author" content="Oliehoek, F. A.">
<meta name="citation_author" content="Spaan, M. T. J.">
<meta name="citation_author" content="Amato, C.">
<meta name="citation_author" content="Whiteson, S.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="449">
<meta name="citation_lastpage" content="509">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3804/live-3804-6917-jair.pdf">

<cite>M.  Ono, B.  C. Williams and Lars  Blackmore (2013) "Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk", Volume 46, pages 511-577</cite>
<p class="media"><a href="/media/3893/live-3893-7324-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3893/live-3893-6920-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3893'>doi:10.1613/jair.3893</a>
<br/><a href="/media/3893/live-3893-7325-jair.html">Appendix </a> - Errata</p>
<p>This paper presents a model-based planner called the Probabilistic Sulu Planner  or the p-Sulu Planner, which controls stochastic systems in a goal directed manner within user-specified risk bounds. The objective of the p-Sulu Planner is to allow users to command continuous, stochastic systems, such as unmanned aerial and space vehicles, in a manner that is both intuitive and safe. To this end, we first develop a new plan representation called a chance-constrained qualitative state plan (CCQSP), through which users can specify the desired evolution of the plant state as well as the acceptable level of risk. An example of a CCQSP statement is ``go to A through B within 30 minutes, with less than 0.001% probability of failure."  We then develop the p-Sulu Planner, which can tractably solve a CCQSP planning problem. In order to enable CCQSP planning, we develop the following two capabilities in this paper: 1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a continuous domain with temporal constraints. The first capability is to ensures that the probability of failure is bounded. The second capability is essential for the planner to solve problems with a continuous state space such as vehicle path planning. We demonstrate the capabilities of the p-Sulu Planner by simulations on two real-world scenarios: the path planning and scheduling of a personal aerial vehicle as well as the space rendezvous of an autonomous cargo spacecraft. </p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk">
<meta name="citation_author" content="Ono, M.">
<meta name="citation_author" content="Williams, B. C.">
<meta name="citation_author" content="Blackmore, Lars">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="511">
<meta name="citation_lastpage" content="577">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3893/live-3893-7324-jair.pdf">

<cite>D.  H. Wolpert and J.  W. Bono (2013) "Predicting Behavior in Unstructured Bargaining with a Probability Distribution", Volume 46, pages 579-605</cite>
<p class="media"><a href="/media/3662/live-3662-6943-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3662/live-3662-7184-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3662'>doi:10.1613/jair.3662</a></p>
<p>In experimental tests of human behavior in unstructured bargaining games, typically many joint utility outcomes are found to occur, not just one. This suggests we predict the outcome of such a game as a probability distribution. This is in contrast to what is conventionally done (e.g, in the Nash bargaining solution), which is predict a single outcome. We show how to translate Nash's bargaining axioms to provide a distribution over outcomes rather than a single outcome. We then prove that a subset of those axioms forces the distribution over utility outcomes to be a power-law distribution. Unlike Nash's original result, our result holds even if the feasible set is finite. When the feasible set is convex and comprehensive, the mode of the power law distribution is the Harsanyi bargaining solution, and if we require symmetry it is the Nash bargaining solution. However, in general these modes of the joint utility distribution are not the experimentalist's Bayes-optimal predictions for the joint utility. Nor are the bargains corresponding to the modes of those joint utility distributions the modes of the distribution over bargains in general, since more than one bargain may result in the same joint utility. After introducing distributional bargaining solution concepts, we show how an external regulator can use them to optimally design an unstructured bargaining scenario. Throughout we demonstrate our analysis in computational experiments involving flight rerouting negotiations in the National Airspace System. We emphasize that while our results are formulated for unstructured bargaining, they can also be used to make predictions for noncooperative games where the modeler knows the utility functions of the players over possible outcomes of the game, but does not know the move spaces the players use to determine those outcomes.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Predicting Behavior in Unstructured Bargaining with a Probability Distribution">
<meta name="citation_author" content="Wolpert, D. H.">
<meta name="citation_author" content="Bono, J. W.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="579">
<meta name="citation_lastpage" content="605">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3662/live-3662-6943-jair.pdf">

<cite>T.  P. Michalak, K.  V. Aadithya, P.  L. Szczepanski, B.  Ravindran and N.  R. Jennings (2013) "Efficient Computation of the Shapley Value for Game-Theoretic Network Centrality", Volume 46, pages 607-650</cite>
<p class="media"><a href="/media/3806/live-3806-6969-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3806/live-3806-6973-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3806'>doi:10.1613/jair.3806</a>
<br/><a href="http://tomaszmichalak.home.pl/page19.php">Appendix </a> - Source code.</p>
<p>The Shapley value---probably the most important normative payoff division scheme in coalitional games---has recently been advocated as a useful measure of centrality in networks. However, although this approach has a variety of real-world applications (including social and organisational networks, biological networks and communication networks), its computational properties have not been widely studied. To date, the only practicable approach to compute Shapley value-based centrality has been via Monte Carlo simulations which are computationally expensive and not guaranteed to give an exact answer. Against this background, this paper presents the first study of the computational aspects of the Shapley value for network centralities. Specifically, we develop exact analytical formulae for Shapley value-based centrality in both weighted and unweighted networks and develop efficient (polynomial time) and exact algorithms based on them. We empirically evaluate these algorithms on two real-life examples (an infrastructure network representing the topology of the Western States Power Grid and a collaboration network from the field of astrophysics) and demonstrate that they deliver significant speedups over the Monte Carlo approach. For instance, in the case of unweighted networks our algorithms are able to return the exact solution about 1600 times faster than the Monte Carlo approximation, even if we allow for a generous 10% error margin for the latter method.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Efficient Computation of the Shapley Value for Game-Theoretic Network Centrality">
<meta name="citation_author" content="Michalak, T. P.">
<meta name="citation_author" content="Aadithya, K. V.">
<meta name="citation_author" content="Szczepanski, P. L.">
<meta name="citation_author" content="Ravindran, B.">
<meta name="citation_author" content="Jennings, N. R.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="607">
<meta name="citation_lastpage" content="650">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3806/live-3806-6969-jair.pdf">

<cite>B.  Bagheri Hariri, D.  Calvanese, M.  Montali, G.  De Giacomo, R.  De Masellis and P.  Felli (2013) "Description Logic Knowledge and Action Bases", Volume 46, pages 651-686</cite>
<p class="media"><a href="/media/3826/live-3826-6984-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3826/live-3826-6983-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3826'>doi:10.1613/jair.3826</a></p>
<p>Description logic Knowledge and Action Bases (KAB) are a mechanism for providing both a semantically rich representation of the information on the domain of interest in terms of a description logic knowledge base and actions to change such information over time, possibly introducing new objects. We resort to a variant of DL-Lite where the unique name assumption is not enforced and where equality between objects may be asserted and inferred. Actions are specified as sets of conditional effects, where conditions are based on epistemic queries over the knowledge base (TBox and ABox), and effects are expressed in terms of new ABoxes. In this setting, we address verification of temporal properties expressed in a variant of first-order mu-calculus with quantification across states. Notably, we show decidability of verification, under a suitable restriction inspired by the notion of weak acyclicity in data exchange.<br />
</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="Description Logic Knowledge and Action Bases">
<meta name="citation_author" content="Bagheri Hariri, B.">
<meta name="citation_author" content="Calvanese, D.">
<meta name="citation_author" content="Montali, M.">
<meta name="citation_author" content="De Giacomo, G.">
<meta name="citation_author" content="De Masellis, R.">
<meta name="citation_author" content="Felli, P.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="651">
<meta name="citation_lastpage" content="686">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3826/live-3826-6984-jair.pdf">

<cite>S.  Cai, K.  Su, C.  Luo and A.  Sattar (2013) "NuMVC: An Efficient Local Search Algorithm for Minimum Vertex Cover", Volume 46, pages 687-716</cite>
<p class="media"><a href="/media/3907/live-3907-6992-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/3907/live-3907-6991-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.3907'>doi:10.1613/jair.3907</a></p>
<p>The Minimum Vertex Cover (MVC) problem is a prominent NP-hard combinatorial optimization problem of great importance in both theory and application. Local search has proved successful for this problem. However, there are two main drawbacks in state-of-the-art MVC local search algorithms. First, they select a pair of vertices to exchange simultaneously, which is time-consuming. Secondly, although using edge weighting techniques to diversify the search, these algorithms lack mechanisms for decreasing the weights. To address these issues, we propose two new strategies: two-stage exchange and edge weighting with forgetting. The two-stage exchange strategy selects two vertices to exchange separately and performs the exchange in two stages. The strategy of edge weighting with forgetting not only increases weights of uncovered edges, but also decreases some weights for each edge periodically. These two strategies are used in designing a new MVC local search algorithm, which is referred to as NuMVC.<br />
<br />
We conduct extensive experimental studies on the standard benchmarks, namely DIMACS and BHOSLIB. The experiment comparing NuMVC with state-of-the-art heuristic algorithms show that NuMVC is at least competitive with the nearest competitor namely PLS on the DIMACS benchmark, and clearly dominates all competitors on the BHOSLIB benchmark. Also, experimental results indicate that NuMVC finds an optimal solution much faster than the current best exact algorithm for Maximum Clique on random instances as well as some structured ones. Moreover, we study the effectiveness of the two strategies and the run-time behaviour through experimental analysis.</p>
<a href="/vol/vol46.html">Click here to return to Volume 46 contents list</a>
<meta name="citation_title" content="NuMVC: An Efficient Local Search Algorithm for Minimum Vertex Cover">
<meta name="citation_author" content="Cai, S.">
<meta name="citation_author" content="Su, K.">
<meta name="citation_author" content="Luo, C.">
<meta name="citation_author" content="Sattar, A.">
<meta name="citation_publication_date" content="2013">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="687">
<meta name="citation_lastpage" content="716">
<meta name="citation_volume" content="46">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/3907/live-3907-6992-jair.pdf">
