<cite>D.  Ortiz-Boyer,  C.  Herv&#225;s-Mart&#237;nez and  N.  Garc&#237;a-Pedrajas (2005) "CIXL2: A Crossover Operator for Evolutionary Algorithms Based on Population Features", Volume 24, pages 1-48</cite>
<p class="media"><a href="/media/1660/live-1660-2416-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1660/live-1660-2415-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume24/ortizboyer05a-html/Ortiz-Boyer.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1660'>doi:10.1613/jair.1660</a></p>
<p>In this paper we propose a crossover operator for evolutionary algorithms with real values that is based on the statistical theory of population distributions. The operator is based on the theoretical distribution of the values of the genes of the best individuals in the population.  The proposed operator takes into account the localization and dispersion features of the best individuals of the population with the objective that these features would be inherited by the offspring. Our aim is the optimization of the balance between exploration and exploitation in the search process.   </p><p> In order to test the efficiency and robustness of this crossover, we have used a set of functions to be optimized with regard to different criteria, such as, multimodality, separability, regularity and epistasis. With this set of functions we can extract conclusions in function of the problem at hand. We analyze the results using ANOVA and multiple comparison statistical tests. </p><p> As an example of how our crossover can be used to solve artificial intelligence problems, we have applied the proposed model to the problem of obtaining the weight of each network in a ensemble of neural networks. The results obtained are above the performance of standard methods.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="CIXL2: A Crossover Operator for Evolutionary Algorithms Based on Population Features">
<meta name="citation_author" content="Ortiz-Boyer,  D.">
<meta name="citation_author" content="Herv&#225;s-Mart&#237;nez,  C.">
<meta name="citation_author" content="Garc&#237;a-Pedrajas,  N.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="48">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1660/live-1660-2416-jair.pdf">

<cite>P.   J. Gmytrasiewicz and  P.  Doshi (2005) "A Framework for Sequential Planning in Multi-Agent Settings", Volume 24, pages 49-79</cite>
<p class="media"><a href="/media/1579/live-1579-2391-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1579/live-1579-2390-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1579'>doi:10.1613/jair.1579</a></p>
<p>This paper extends the framework of partially observable Markov decision processes (POMDPs) to multi-agent settings by incorporating the notion of agent models into the state space.  Agents maintain beliefs over physical states of the environment and over models of other agents, and they use Bayesian updates to maintain their beliefs over time. The solutions map belief states to actions. Models of other agents may include their belief states and are related to agent types considered in games of incomplete information.  We express the agents' autonomy by postulating that their models are not directly manipulable or observable by other agents.  We show that important properties of POMDPs, such as convergence of value iteration, the rate of convergence, and piece-wise linearity and convexity of the value functions carry over to our framework.  Our approach complements a more traditional approach to interactive settings which uses Nash equilibria as a solution paradigm.  We seek to avoid some of the drawbacks of equilibria which may be non-unique and do not capture off-equilibrium behaviors.  We do so at the cost of having to represent, process and continuously revise models of other agents. Since the agent's beliefs may be arbitrarily nested, the optimal solutions to decision making problems are only asymptotically computable.  However, approximate belief updates and approximately optimal plans are computable. We illustrate our framework using a simple application domain, and we show examples of belief updates and value functions.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="A Framework for Sequential Planning in Multi-Agent Settings">
<meta name="citation_author" content="Gmytrasiewicz,  P.  J.">
<meta name="citation_author" content="Doshi,  P.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="49">
<meta name="citation_lastpage" content="79">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1579/live-1579-2391-jair.pdf">

<cite>P.  Geibel and  F.  Wysotzki (2005) "Risk-Sensitive Reinforcement Learning Applied to Control under Constraints", Volume 24, pages 81-108</cite>
<p class="media"><a href="/media/1666/live-1666-2420-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1666/live-1666-2419-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1666'>doi:10.1613/jair.1666</a></p>
<p>In this paper, we consider Markov Decision Processes (MDPs) with error states.  Error states are those states entering which is undesirable or dangerous. We define the risk with respect to a policy as the probability of entering such a state when the policy is pursued. We consider the problem of finding good policies whose risk is smaller than some user-specified threshold, and formalize it as a constrained MDP with two criteria. The first criterion corresponds to the value function originally given. We will show that the risk can be formulated as a second criterion function based on a cumulative return, whose definition is independent of the original value function.  We present a model free, heuristic reinforcement learning algorithm that aims at finding good deterministic policies.  It is based on weighting the original value function and the risk. The weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function. The algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column. This control task was originally formulated as an optimal control problem with chance constraints, and it was solved under certain assumptions on the model to obtain an optimal solution. The power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Risk-Sensitive Reinforcement Learning Applied to Control under Constraints">
<meta name="citation_author" content="Geibel,  P.">
<meta name="citation_author" content="Wysotzki,  F.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="81">
<meta name="citation_lastpage" content="108">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1666/live-1666-2420-jair.pdf">

<cite>P.  J. Hawkins,  V.  Lagoon and  P.  J. Stuckey (2005) "Solving Set Constraint Satisfaction Problems using ROBDDs", Volume 24, pages 109-156</cite>
<p class="media"><a href="/media/1638/live-1638-2400-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1638/live-1638-2399-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1638'>doi:10.1613/jair.1638</a></p>
<p>In this paper we present a new approach to modeling finite set domain constraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We show that it is possible to construct an efficient set domain propagator which compactly represents many set domains and set constraints using ROBDDs.  We demonstrate that the ROBDD-based approach provides unprecedented flexibility in modeling constraint satisfaction problems, leading to performance improvements. We also show that the ROBDD-based modeling approach can be extended to the modeling of integer and multiset constraint problems in a straightforward manner. Since domain propagation is not always practical, we also show how to incorporate less strict consistency notions into the ROBDD framework, such as set bounds, cardinality bounds and lexicographic bounds consistency. Finally, we present experimental results that demonstrate the ROBDD-based solver performs better than various more conventional constraint solvers on several standard set constraint problems.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Solving Set Constraint Satisfaction Problems using ROBDDs">
<meta name="citation_author" content="Hawkins,  P. J.">
<meta name="citation_author" content="Lagoon,  V.">
<meta name="citation_author" content="Stuckey,  P. J.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="109">
<meta name="citation_lastpage" content="156">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1638/live-1638-2400-jair.pdf">

<cite>P.  W. Jordan and  M.  A. Walker (2005) "Learning Content Selection Rules for Generating Object Descriptions in Dialogue", Volume 24, pages 157-194</cite>
<p class="media"><a href="/media/1591/live-1591-2394-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1591/live-1591-2393-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1591'>doi:10.1613/jair.1591</a></p>
<p>A fundamental requirement of any task-oriented dialogue system is the ability to generate object descriptions that refer to objects in the task domain. The subproblem of content selection for object descriptions in task-oriented dialogue has been the focus of much previous work and a large number of models have been proposed. In this paper, we use the annotated COCONUT corpus of task-oriented design dialogues to develop feature sets based on Dale and Reiter's (1995) incremental model, Brennan and Clark's (1996) conceptual pact model, and Jordan's (2000b) intentional influences model, and use these feature sets in a machine learning experiment to automatically learn a model of content selection for object descriptions.  Since Dale and Reiter's model requires a representation of discourse structure, the corpus annotations are used to derive a representation based on Grosz and Sidner's (1986) theory of the intentional structure of discourse, as well as two very simple representations of discourse structure based purely on recency. We then apply the rule-induction program RIPPER to train and test the content selection component of an object description generator on a set of 393 object descriptions from the corpus. To our knowledge, this is the first reported experiment of a trainable content selection component for object description generation in dialogue. Three separate content selection models that are based on the three theoretical models, all independently achieve accuracies significantly above the majority class baseline (17%) on unseen test data, with the intentional influences model (42.4%) performing significantly better than either the incremental model (30.4%) or the conceptual pact model (28.9%). But the best performing models combine all the feature sets, achieving accuracies near 60%. Surprisingly, a simple recency-based representation of discourse structure does as well as one based on intentional structure.  To our knowledge, this is also the first empirical comparison of a representation of Grosz and Sidner's model of discourse structure with a simpler model for any generation task.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Learning Content Selection Rules for Generating Object Descriptions in Dialogue">
<meta name="citation_author" content="Jordan,  P. W.">
<meta name="citation_author" content="Walker,  M. A.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="157">
<meta name="citation_lastpage" content="194">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1591/live-1591-2394-jair.pdf">

<cite>M.  T.J. Spaan and  N.  Vlassis (2005) "Perseus: Randomized Point-based Value Iteration for POMDPs", Volume 24, pages 195-220</cite>
<p class="media"><a href="/media/1659/live-1659-2413-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1659/live-1659-2412-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1659'>doi:10.1613/jair.1659</a></p>
<p>Partially observable Markov decision processes (POMDPs) form an attractive and principled framework for agent planning under uncertainty.  Point-based approximate techniques for POMDPs compute a policy based on a finite set of points collected in advance from the agent's belief space.  We present a randomized point-based value iteration algorithm called Perseus.  The algorithm performs approximate value backup stages, ensuring that in each backup stage the value of each point in the belief set is improved; the key observation is that a single backup may improve the value of many belief points.  Contrary to other point-based methods, Perseus backs up only a (randomly selected) subset of points in the belief set, sufficient for improving the value of each belief point in the set. We show how the same idea can be extended to dealing with continuous action spaces.  Experimental results show the potential of Perseus in large scale POMDP problems.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Perseus: Randomized Point-based Value Iteration for POMDPs">
<meta name="citation_author" content="Spaan,  M. T.J.">
<meta name="citation_author" content="Vlassis,  N.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="195">
<meta name="citation_lastpage" content="220">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1659/live-1659-2413-jair.pdf">

<cite>J.  P. Watson,  L.  D. Whitley and  A.  E. Howe (2005) "Linking Search Space Structure, Run-Time Dynamics, and Problem Difficulty: A Step Toward Demystifying Tabu Search", Volume 24, pages 221-261</cite>
<p class="media"><a href="/media/1576/live-1576-2388-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1576/live-1576-2387-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1576'>doi:10.1613/jair.1576</a></p>
<p>Tabu search is one of the most effective heuristics for locating high-quality solutions to a diverse array of NP-hard combinatorial optimization problems. Despite the widespread success of tabu search, researchers have a poor understanding of many key theoretical aspects of this algorithm, including models of the high-level run-time dynamics and identification of those search space features that influence problem  difficulty. We consider these questions in the context of the job-shop  scheduling problem (JSP), a domain where tabu search algorithms have  been shown to be remarkably effective. Previously, we demonstrated  that the mean distance between random local optima and the nearest  optimal solution is highly correlated with problem difficulty for a  well-known tabu search algorithm for the JSP introduced by Taillard. In this paper, we discuss various shortcomings of this measure and  develop a new model of problem difficulty that corrects these deficiencies. We show that Taillard's algorithm can be modeled  with high fidelity as a simple variant of a straightforward random  walk. The random walk model accounts for nearly all of the variability in the cost required to locate both optimal and sub-optimal solutions to random JSPs, and provides an explanation for differences in the difficulty of random versus structured JSPs. Finally, we discuss and  empirically substantiate two novel predictions regarding tabu search  algorithm behavior. First, the method for constructing the initial solution is  highly unlikely to impact the performance of tabu search. Second, tabu  tenure should be selected to be as small as possible while simultaneously  avoiding search stagnation; values larger than necessary lead to  significant degradations in performance.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Linking Search Space Structure, Run-Time Dynamics, and Problem Difficulty: A Step Toward Demystifying Tabu Search">
<meta name="citation_author" content="Watson,  J. P.">
<meta name="citation_author" content="Whitley,  L. D.">
<meta name="citation_author" content="Howe,  A. E.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="221">
<meta name="citation_lastpage" content="261">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1576/live-1576-2388-jair.pdf">

<cite>V.  Bayer-Zubek and  T.  G. Dietterich (2005) "Integrating Learning from Examples into the Search for Diagnostic Policies", Volume 24, pages 263-303</cite>
<p class="media"><a href="/media/1512/live-1512-2351-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1512/live-1512-2350-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1512'>doi:10.1613/jair.1512</a></p>
<p>This paper studies the problem of learning diagnostic policies from training examples. A diagnostic policy is a complete description of the decision-making actions of a diagnostician (i.e., tests followed by a diagnostic decision) for all possible combinations of test results.  An optimal diagnostic policy is one that minimizes the expected total cost, which is the sum of measurement costs and misdiagnosis costs.  In most diagnostic settings, there is a tradeoff between these two kinds of costs. </p><p> This paper formalizes diagnostic decision making as a Markov Decision Process (MDP). The paper introduces a new family of systematic search algorithms based on the AO* algorithm to solve this MDP.  To make AO* efficient, the paper describes an admissible heuristic that enables AO* to prune large parts of the search space.  The paper also introduces several greedy algorithms including some improvements over previously-published methods. The paper then addresses the question of learning diagnostic policies from examples.  When the probabilities of diseases and test results are computed from training data, there is a great danger of overfitting. To reduce overfitting, regularizers are integrated into the search algorithms.  Finally, the paper compares the proposed methods on five benchmark diagnostic data sets.  The studies show that in most cases the systematic search methods produce better diagnostic policies than the greedy methods. In addition, the studies show that for training sets of realistic size, the systematic search algorithms are practical on today's desktop computers.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Integrating Learning from Examples into the Search for Diagnostic Policies">
<meta name="citation_author" content="Bayer-Zubek,  V.">
<meta name="citation_author" content="Dietterich,  T. G.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="263">
<meta name="citation_lastpage" content="303">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1512/live-1512-2351-jair.pdf">

<cite>P.  Cimiano,  A.  Hotho and  S.  Staab (2005) "Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis", Volume 24, pages 305-339</cite>
<p class="media"><a href="/media/1648/live-1648-2403-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1648/live-1648-2402-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume24/cimiano05a-html/cimiano.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1648'>doi:10.1613/jair.1648</a></p>
<p>We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus. The approach is based on Formal Concept Analysis (FCA), a method mainly used for the analysis of data, i.e. for investigating and processing explicitly given information.  We follow Harris' distributional hypothesis and model the context of a certain term as a vector representing syntactic dependencies which are automatically acquired from the text corpus with a linguistic parser.  On the basis of this context information, FCA produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy.  The approach is evaluated by comparing the resulting concept hierarchies with hand-crafted taxonomies for two domains: tourism and finance.  We also directly compare our approach with hierarchical agglomerative clustering as well as with Bi-Section-KMeans as an instance of a divisive clustering algorithm. Furthermore, we investigate the impact of using different measures weighting the contribution of each attribute as well as of applying a particular smoothing technique to cope with data sparseness.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis">
<meta name="citation_author" content="Cimiano,  P.">
<meta name="citation_author" content="Hotho,  A.">
<meta name="citation_author" content="Staab,  S.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="305">
<meta name="citation_lastpage" content="339">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1648/live-1648-2403-jair.pdf">

<cite>R.  Khardon,  D.  Roth and  R.  A. Servedio (2005) "Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms", Volume 24, pages 341-356</cite>
<p class="media"><a href="/media/1655/live-1655-2407-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1655/live-1655-2406-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1655'>doi:10.1613/jair.1655</a></p>
<p>The paper studies machine learning problems where each example is described using a set of Boolean features and where hypotheses are represented by linear threshold elements.  One method of increasing the expressiveness of learned hypotheses in this context is to expand the feature set to include conjunctions of basic features. This can be done explicitly or where possible by using a kernel function. Focusing on the well known Perceptron and Winnow algorithms, the paper demonstrates a tradeoff between the computational efficiency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm. </p><p> We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions.  We show that these kernels can be used to efficiently run the Perceptron algorithm over a feature space of exponentially many conjunctions; however we also show that using such kernels, the Perceptron algorithm can provably make an exponential number of mistakes even when learning simple functions. </p><p> We then consider the question of whether kernel functions can analogously be used to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. Known upper bounds imply that the Winnow algorithm can learn Disjunctive Normal Form (DNF) formulae with a polynomial mistake bound in this setting.  However, we prove that it is computationally hard to simulate Winnow's behavior for learning DNF over such a feature set. This implies that the kernel functions which correspond to running Winnow for this problem are not efficiently computable, and that there is no general construction that can run Winnow with kernels.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms">
<meta name="citation_author" content="Khardon,  R.">
<meta name="citation_author" content="Roth,  D.">
<meta name="citation_author" content="Servedio,  R. A.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="341">
<meta name="citation_lastpage" content="356">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1655/live-1655-2407-jair.pdf">

<cite>G.  Gottlob,  G.  Greco and  F.  Scarcello (2005) "Pure Nash Equilibria: Hard and Easy Games", Volume 24, pages 357-406</cite>
<cite>2008 IJCAI-JAIR Best Paper Prize</cite><p class="media"><a href="/media/1683/live-1683-2423-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1683/live-1683-2422-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1683'>doi:10.1613/jair.1683</a></p>
<p>We investigate complexity issues related to pure Nash equilibria of strategic games. We show that, even in very restrictive settings, determining whether a game has a pure Nash Equilibrium is NP-hard, while deciding whether a game has a strong Nash equilibrium is SigmaP2-complete.  We then study practically relevant restrictions that lower the complexity. In particular, we are interested in quantitative and qualitative restrictions of the way each player's payoff depends on moves of other players.  We say that a game has small neighborhood if the utility function for each player depends only on (the actions of) a logarithmically small number of other players. The dependency structure of a game G can be expressed by a graph DG(G) or by a hypergraph H(G). By relating Nash equilibrium problems to constraint satisfaction problems (CSPs), we show that if G has small neighborhood and if H(G) has bounded hypertree width (or if DG(G) has bounded treewidth), then finding pure Nash and Pareto equilibria is feasible in polynomial time. If the game is graphical, then these problems are LOGCFL-complete and thus in the class NC2 of highly parallelizable problems.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Pure Nash Equilibria: Hard and Easy Games">
<meta name="citation_author" content="Gottlob,  G.">
<meta name="citation_author" content="Greco,  G.">
<meta name="citation_author" content="Scarcello,  F.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="357">
<meta name="citation_lastpage" content="406">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1683/live-1683-2423-jair.pdf">

<cite>P.  S. Dutta,  N.  R. Jennings and  L.  Moreau (2005) "Cooperative Information Sharing to Improve Distributed Learning in Multi-Agent Systems", Volume 24, pages 407-463</cite>
<p class="media"><a href="/media/1735/live-1735-2426-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1735/live-1735-2425-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1735'>doi:10.1613/jair.1735</a></p>
<p>Effective coordination of agents' actions in partially-observable domains is a major challenge of multi-agent systems research.  To address this, many researchers have developed techniques that allow the agents to make decisions based on estimates of the states and actions of other agents that are typically learnt using some form of machine learning algorithm.  Nevertheless, many of these approaches fail to provide an actual means by which the necessary information is made available so that the estimates can be learnt.  To this end, we argue that cooperative communication of state information between agents is one such mechanism.  However, in a dynamically changing environment, the accuracy and timeliness of this communicated information determine the fidelity of the learned estimates and the usefulness of the actions taken based on these.  Given this, we propose a novel information-sharing protocol,  post-task-completion sharing, for the distribution of state information.  We then show, through a formal analysis, the improvement in the quality of estimates produced using our strategy over the widely used protocol of sharing information between nearest neighbours.  Moreover, communication heuristics designed around our information-sharing principle are subjected to empirical evaluation along with other benchmark strategies (including Littman's Q-routing and Stone's TPOT-RL) in a simulated call-routing application.  These studies, conducted across a range of environmental settings, show that, compared to the different benchmarks used, our strategy generates an improvement of up to 60% in the call connection rate; of more than 1000% in the ability to connect long-distance calls; and incurs as low as 0.25 of the message overhead.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Cooperative Information Sharing to Improve Distributed Learning in Multi-Agent Systems">
<meta name="citation_author" content="Dutta,  P. S.">
<meta name="citation_author" content="Jennings,  N. R.">
<meta name="citation_author" content="Moreau,  L.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="407">
<meta name="citation_lastpage" content="463">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1735/live-1735-2426-jair.pdf">

<cite>Q.  B. Vo and  N.  Y. Foo (2005) "Reasoning about Action: An Argumentation - Theoretic Approach", Volume 24, pages 465-518</cite>
<p class="media"><a href="/media/1602/live-1602-2397-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1602/live-1602-2396-jair.ps.Z">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1602'>doi:10.1613/jair.1602</a></p>
<p>We present a uniform non-monotonic solution to the problems of reasoning about action on the basis of an argumentation-theoretic approach. Our theory is provably correct relative to a sensible minimisation policy introduced on top of a temporal propositional logic. Sophisticated problem domains can be formalised in our framework.  As much attention of researchers in the field has been paid to the traditional and basic problems in reasoning about actions such as the frame, the qualification and the ramification problems, approaches to these problems within our formalisation lie at heart of the expositions presented in this paper.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Reasoning about Action: An Argumentation - Theoretic Approach">
<meta name="citation_author" content="Vo,  Q. B.">
<meta name="citation_author" content="Foo,  N. Y.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="465">
<meta name="citation_lastpage" content="518">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1602/live-1602-2397-jair.pdf">

<cite>J.  Hoffmann and S.   Edelkamp (2005) "The Deterministic Part of IPC-4: An Overview", Volume 24, pages 519-579</cite>
<p class="media"><a href="/media/1677/live-1677-2522-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1677/live-1677-2500-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1677'>doi:10.1613/jair.1677</a>
<br/><a href="/media/1677/live-1677-2502-jair.tgz">Appendix 1</a> - IPC-4 results, graphed &nbsp;|&nbsp;<a href="/media/1677/live-1677-2503-jair.tgz">Appendix 2</a> - The full (raw) IPC-4 results data </p>
<p>We provide an overview of the organization and results of the deterministic part of the 4th International Planning Competition, i.e., of the part concerned with evaluating systems doing deterministic planning. IPC-4 attracted even more competing systems than its already large predecessors, and the competition event was revised in several important respects. After giving an introduction to the IPC, we briefly explain the main differences between the deterministic part of IPC-4 and its predecessors. We then introduce formally the language used, called PDDL2.2 that extends PDDL2.1 by derived predicates and timed initial literals. We list the competing systems and overview the results of the competition. The entire set of data is far too large to be presented in full. We provide a detailed summary; the complete data is available in an online appendix. We explain how we awarded the competition prizes.  </p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="The Deterministic Part of IPC-4: An Overview">
<meta name="citation_author" content="Hoffmann, J.">
<meta name="citation_author" content="Edelkamp, S. ">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="519">
<meta name="citation_lastpage" content="579">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1677/live-1677-2522-jair.pdf">

<cite>A.  Botea, M.  Enzenberger, M.  Mueller and J.  Schaeffer (2005) "Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators", Volume 24, pages 581-621</cite>
<p class="media"><a href="/media/1696/live-1696-2537-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1696/live-1696-2538-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume24/botea05a-html/index.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1696'>doi:10.1613/jair.1696</a></p>
<p>Despite recent progress in AI planning, many benchmarks remain challenging for current planners. In many domains, the performance of a planner can greatly be improved by discovering and exploiting information about the domain structure that is not explicitly encoded in the initial PDDL formulation. In this paper we present and compare two automated methods that learn relevant information from previous experience in a domain and use it to solve new problem instances. Our methods share a common four-step strategy. First, a domain is analyzed and structural information is extracted, then macro-operators are generated based on the previously discovered structure. A filtering and ranking procedure selects the most useful macro-operators. Finally, the selected macros are used to speed up future searches.  We have successfully used such an approach in the fourth international planning competition IPC-4. Our system, Macro-FF, extends Hoffmann's state-of-the-art planner FF 2.3 with support for two kinds of macro-operators, and with engineering enhancements. We demonstrate the effectiveness of our ideas on benchmarks from international planning competitions. Our results indicate a large reduction in search effort in those complex domains where structural information can be inferred.   </p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators">
<meta name="citation_author" content="Botea, A.">
<meta name="citation_author" content="Enzenberger, M.">
<meta name="citation_author" content="Mueller, M.">
<meta name="citation_author" content="Schaeffer, J.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="581">
<meta name="citation_lastpage" content="621">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1696/live-1696-2537-jair.pdf">

<cite>D.  Achlioptas, H.  Jia and C.  Moore (2005) "Hiding Satisfying Assignments: Two are Better than One", Volume 24, pages 623-639</cite>
<p class="media"><a href="/media/1681/live-1681-2540-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1681/live-1681-2541-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1681'>doi:10.1613/jair.1681</a></p>
<p>The evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances. A plausible source of such instances consists of random k-SAT formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment A. Unfortunately, instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics. Roughly speaking, for a number of different algorithms, A acts as a stronger and stronger attractor as the formula's density increases. Motivated by recent results on the geometry of the space of satisfying truth assignments of random k-SAT and NAE-k-SAT formulas, we introduce a simple twist on this basic model, which appears to dramatically increase its hardness. Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also forbid the clauses violated by its complement, so that both A and compliment of A are satisfying. It appears that under this "symmetrization" the effects of the two attractors largely cancel out, making it much harder for algorithms to find any truth assignment. We give theoretical and experimental evidence supporting this assertion. </p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Hiding Satisfying Assignments: Two are Better than One">
<meta name="citation_author" content="Achlioptas, D.">
<meta name="citation_author" content="Jia, H.">
<meta name="citation_author" content="Moore, C.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="623">
<meta name="citation_lastpage" content="639">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1681/live-1681-2540-jair.pdf">

<cite>N.  Samaras and K.  Stergiou (2005) "Binary Encodings of Non-binary Constraint Satisfaction Problems: Algorithms and Experimental Results", Volume 24, pages 641-684</cite>
<p class="media"><a href="/media/1776/live-1776-2526-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1776/live-1776-2527-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume24/samaras05a-html/samaras.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1776'>doi:10.1613/jair.1776</a></p>
<p>A non-binary Constraint Satisfaction Problem (CSP) can be solved directly using extended versions of binary techniques. Alternatively, the non-binary problem can be translated into an equivalent binary one. In this case, it is generally accepted that the translated problem can be solved by applying well-established techniques for binary CSPs. In this paper we evaluate the applicability of the latter approach. We demonstrate that the use of standard techniques for binary CSPs in the encodings of non-binary problems is problematic and results in models that are very rarely competitive with the non-binary representation. To overcome this, we propose specialized arc consistency and search algorithms for binary encodings, and we evaluate them theoretically and empirically. We consider three binary representations; the hidden variable encoding, the dual encoding, and the double encoding. Theoretical and empirical results show that, for certain classes of non-binary constraints, binary encodings are a competitive option, and in many cases, a better one than the non-binary representation. </p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Binary Encodings of Non-binary Constraint Satisfaction Problems: Algorithms and Experimental Results">
<meta name="citation_author" content="Samaras, N.">
<meta name="citation_author" content="Stergiou, K.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="641">
<meta name="citation_lastpage" content="684">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1776/live-1776-2526-jair.pdf">

<cite>J.  Hoffmann (2005) "Where 'Ignoring Delete Lists' Works: Local Search Topology in Planning Benchmarks", Volume 24, pages 685-758</cite>
<p class="media"><a href="/media/1747/live-1747-2543-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1747/live-1747-2544-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1747'>doi:10.1613/jair.1747</a>
<br/><a href="/media/1747/live-1747-2545-jair.ps">Appendix </a> - Technical details</p>
<p>Between 1998 and 2004, the planning community has seen vast progress in terms of the sizes of benchmark examples that domain-independent planners can tackle successfully. The key technique behind this progress is the use of heuristic functions based on relaxing the planning task at hand, where the relaxation is to assume that all delete lists are empty. The unprecedented success of such methods, in many commonly used benchmark examples, calls for an understanding of what classes of domains these methods are well suited for.   In the investigation at hand, we derive a formal background to such an understanding. We perform a case study covering a range of 30 commonly used STRIPS and ADL benchmark domains, including all examples used in the first four international planning competitions. We *prove* connections between domain structure and local search topology -- heuristic cost surface properties -- under an idealized version of the heuristic functions used in modern planners. The idealized heuristic function is called h^+, and differs from the practically used functions in that it returns the length of an *optimal* relaxed plan, which is NP-hard to compute. We identify several key characteristics of the topology under h^+, concerning the existence/non-existence of unrecognized dead ends, as well as the existence/non-existence of constant upper bounds on the difficulty of escaping local minima and benches. These distinctions divide the (set of all) planning domains into a taxonomy of classes of varying h^+ topology. As it turns out, many of the 30 investigated domains lie in classes with a relatively easy topology. Most particularly, 12 of the domains lie in classes where FF's search algorithm, provided with h^+, is a polynomial solving mechanism.   We also present results relating h^+ to its approximation as implemented in FF. The behavior regarding dead ends is provably the same. We summarize the results of an empirical investigation showing that, in many domains, the topological qualities of h^+ are largely inherited by the approximation. The overall investigation gives a rare example of a successful analysis of the connections between typical-case problem structure, and search performance. The theoretical investigation also gives hints on how the topological phenomena might be automatically recognizable by domain analysis techniques. We outline some preliminary steps we made into that direction. </p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Where 'Ignoring Delete Lists' Works: Local Search Topology in Planning Benchmarks">
<meta name="citation_author" content="Hoffmann, J.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="685">
<meta name="citation_lastpage" content="758">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1747/live-1747-2543-jair.pdf">

<cite>S.  Sanghai, P.  Domingos and D.  Weld (2005) "Relational Dynamic Bayesian Networks", Volume 24, pages 759-797</cite>
<p class="media"><a href="/media/1625/live-1625-3226-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1625/live-1625-3227-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1625'>doi:10.1613/jair.1625</a>
<br/><a href="/media/1625/live-1625-3225-jair.pdf">Appendix </a> - Retraction</p>
<p>Stochastic processes that involve the creation of objects and relations over time are widespread, but relatively poorly studied. For example, accurate fault diagnosis in factory assembly processes requires inferring the probabilities of erroneous assembly operations, but doing this efficiently and accurately is difficult. Modeled as dynamic Bayesian networks, these processes have discrete variables with very large domains and extremely high dimensionality. In this paper, we introduce relational dynamic Bayesian networks (RDBNs), which are an extension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a generalization of dynamic probabilistic relational models (DPRMs), which we had proposed in our previous work to model dynamic uncertain domains. We first extend the Rao-Blackwellised particle filtering described in our earlier work to RDBNs. Next, we lift the assumptions associated with Rao-Blackwellization in RDBNs and propose two new forms of particle filtering. The first one uses abstraction hierarchies over the predicates to smooth the particle filter's estimates. The second employs kernel density estimation with a kernel function specifically designed for relational domains. Experiments show these two methods greatly outperform standard particle filtering on the task of assembly plan execution monitoring. <br />
</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Relational Dynamic Bayesian Networks">
<meta name="citation_author" content="Sanghai, S.">
<meta name="citation_author" content="Domingos, P.">
<meta name="citation_author" content="Weld, D.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="759">
<meta name="citation_lastpage" content="797">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1625/live-1625-3226-jair.pdf">

<cite>M.  Beetz and H.  Grosskreutz (2005) "Probabilistic Hybrid Action Models for Predicting Concurrent Percept-driven Robot Behavior", Volume 24, pages 799-849</cite>
<p class="media"><a href="/media/1565/live-1565-2552-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1565/live-1565-2553-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1565'>doi:10.1613/jair.1565</a></p>
<p>This article develops Probabilistic Hybrid Action Models (PHAMs), a realistic causal model for predicting the behavior generated by modern percept-driven robot plans. PHAMs represent aspects of robot behavior that cannot be represented by most action models used in AI planning: the temporal structure of continuous control processes, their non-deterministic effects, several modes of their interferences, and the achievement of triggering conditions in closed-loop robot plans. <br />
<br />
The main contributions of this article are: (1) PHAMs, a model of concurrent percept-driven behavior, its formalization, and proofs that the model generates probably, qualitatively accurate predictions; and (2) a resource-efficient inference method for PHAMs based on sampling projections from probabilistic action models and state descriptions. We show how PHAMs can be applied to planning the course of action of an autonomous robot office courier based on analytical and experimental results. <br />
</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Probabilistic Hybrid Action Models for Predicting Concurrent Percept-driven Robot Behavior">
<meta name="citation_author" content="Beetz, M.">
<meta name="citation_author" content="Grosskreutz, H.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="799">
<meta name="citation_lastpage" content="849">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1565/live-1565-2552-jair.pdf">

<cite>H.L.S.  Younes, M.  L. Littman, D.  Weissman and J.  Asmuth (2005) "The First Probabilistic Track of the International Planning Competition", Volume 24, pages 851-887</cite>
<p class="media"><a href="/media/1880/live-1880-2554-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1880/live-1880-2555-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume24/younes05a-html/index.html" onclick="window.open(this.href);return false;">HTML</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1880'>doi:10.1613/jair.1880</a>
<br/><a href="/media/1880/live-1880-2556-jair.tgz">Appendix </a> - Competition logs</p>
<p>The 2004 International Planning Competition, IPC-4, included a probabilistic planning track for the first time. We describe the new domain specification language we created for the track, our evaluation methodology, the competition domains we developed, and the results of the participating teams.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="The First Probabilistic Track of the International Planning Competition">
<meta name="citation_author" content="Younes, H.L.S.">
<meta name="citation_author" content="Littman, M. L.">
<meta name="citation_author" content="Weissman, D.">
<meta name="citation_author" content="Asmuth, J.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="851">
<meta name="citation_lastpage" content="887">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1880/live-1880-2554-jair.pdf">

<cite>M.  Jaeger (2005) "Ignorability in Statistical and Probabilistic Inference", Volume 24, pages 889-917</cite>
<p class="media"><a href="/media/1657/live-1657-2558-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1657/live-1657-2559-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1657'>doi:10.1613/jair.1657</a></p>
<p>When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a ''static'' analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grunwald and Halpern. We then turn to a ''procedural'' analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property. <br />
</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Ignorability in Statistical and Probabilistic Inference">
<meta name="citation_author" content="Jaeger, M.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="889">
<meta name="citation_lastpage" content="917">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1657/live-1657-2558-jair.pdf">

<cite>M.H.L.  van den Briel and S.  Kambhampati (2005) "Optiplan: Unifying IP-based and Graph-based Planning", Volume 24, pages 919-931</cite>
<p class="media"><a href="/media/1698/live-1698-2560-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1698/live-1698-2561-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1698'>doi:10.1613/jair.1698</a></p>
<p>The Optiplan planning system is the first integer programming-based planner that successfully participated in the international planning competition. This engineering note describes the architecture of Optiplan and provides the integer programming formulation that enabled it to perform reasonably well in the competition. We also touch upon some recent developments that make integer programming encodings significantly more competitive.</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="Optiplan: Unifying IP-based and Graph-based Planning">
<meta name="citation_author" content="van den Briel, M.H.L.">
<meta name="citation_author" content="Kambhampati, S.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="919">
<meta name="citation_lastpage" content="931">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1698/live-1698-2560-jair.pdf">

<cite>B.  Bonet and H.  Geffner (2005) "mGPT: A Probabilistic Planner Based on Heuristic Search", Volume 24, pages 933-944</cite>
<p class="media"><a href="/media/1688/live-1688-2562-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/1688/live-1688-2563-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.1688'>doi:10.1613/jair.1688</a></p>
<p>We describe the version of the GPT planner used in the probabilistic track of the 4th International Planning Competition (IPC-4). This version, called mGPT, solves Markov Decision Processes specified in the PPDDL language by extracting and using different classes of lower bounds along with various heuristic-search algorithms. The lower bounds are extracted from deterministic relaxations where the alternative probabilistic effects of an action are mapped into different, independent, deterministic actions. The heuristic-search algorithms use these lower bounds for focusing the updates and delivering a consistent value function over all states reachable from the initial state and the greedy policy. <br />
</p>
<a href="/vol/vol24.html">Click here to return to Volume 24 contents list</a>
<meta name="citation_title" content="mGPT: A Probabilistic Planner Based on Heuristic Search">
<meta name="citation_author" content="Bonet, B.">
<meta name="citation_author" content="Geffner, H.">
<meta name="citation_publication_date" content="2005">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="933">
<meta name="citation_lastpage" content="944">
<meta name="citation_volume" content="24">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/1688/live-1688-2562-jair.pdf">
