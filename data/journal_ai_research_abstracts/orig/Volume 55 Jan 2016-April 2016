<cite>Marta  R. Costa-juss&#224;, Srinivas  Bangalore, Patrik  Lambert, Llu&#237;s   M&#224;rquez and Elena  Montiel-Ponsoda (2016) "Introduction to the Special Issue on Cross-Language Algorithms and Applications", Volume 55, pages 1-15</cite>
<p class="media"><a href="/media/5022/live-5022-9074-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/5022/live-5022-9075-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5022'>doi:10.1613/jair.5022</a>
<br/><a href="/media/5022/live-5022-9079-jair.html">Appendix </a> - Erratum</p>
<p>With the increasingly global nature of our everyday interactions, the need for multilin- gual technologies to support efficient and effective information access and communication cannot be overemphasized. Computational modeling of language has been the focus of Natural Language Processing, a subdiscipline of Artificial Intelligence. One of the current challenges for this discipline is to design methodologies and algorithms that are cross- language in order to create multilingual technologies rapidly. The goal of this JAIR special issue on Cross-Language Algorithms and Applications (CLAA) is to present leading re- search in this area, with emphasis on developing unifying themes that could lead to the development of the science of multi- and cross-lingualism. In this introduction, we provide the reader with the motivation for this special issue and summarize the contributions of the papers that have been included. The selected papers cover a broad range of cross-lingual technologies including machine translation, domain and language adaptation for sentiment analysis, cross-language lexical resources, dependency parsing, information retrieval and knowledge representation. We anticipate that this special issue will serve as an invaluable resource for researchers interested in topics of cross-lingual natural language processing.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Introduction to the Special Issue on Cross-Language Algorithms and Applications">
<meta name="citation_author" content="Costa-juss&#224;, Marta R.">
<meta name="citation_author" content="Bangalore, Srinivas">
<meta name="citation_author" content="Lambert, Patrik">
<meta name="citation_author" content="M&#224;rquez, Llu&#237;s ">
<meta name="citation_author" content="Montiel-Ponsoda, Elena">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="15">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5022/live-5022-9074-jair.pdf">

<cite>V&#237;ctor M.  S&#225;nchez-Cartagena, Juan Antonio  P&#233;rez-Ortiz and Felipe  S&#225;nchez-Mart&#237;nez (2016) "Integrating Rules and Dictionaries from Shallow-Transfer Machine Translation into Phrase-Based Statistical Machine Translation", Volume 55, pages 17-61</cite>
<p class="media"><a href="/media/4761/live-4761-9056-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4761'>doi:10.1613/jair.4761</a></p>
<p>We describe a hybridisation strategy whose objective is to integrate linguistic resources from shallow-transfer rule-based machine translation (RBMT) into phrase-based statistical machine translation (PBSMT). It basically consists of enriching the phrase table of a PBSMT system with bilingual phrase pairs matching transfer rules and dictionary entries from a shallow-transfer RBMT system. This new strategy takes advantage of how the linguistic resources are used by the RBMT system to segment the source-language sentences to be translated, and overcomes the limitations of existing hybrid approaches that treat the RBMT systems as a black box. Experimental results confirm that our approach delivers translations of  higher quality than existing ones, and that it is specially useful when the parallel corpus available for training the SMT system is small or when translating out-of-domain texts that are well covered by the RBMT dictionaries. A combination of this approach with a recently proposed unsupervised shallow-transfer rule inference algorithm results in a significantly greater translation quality than that of a baseline PBSMT; in this case, the only hand-crafted resource used are the dictionaries commonly used in RBMT. Moreover, the translation quality achieved by the hybrid system built with automatically inferred rules is similar to that obtained by those built with hand-crafted rules.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Integrating Rules and Dictionaries from Shallow-Transfer Machine Translation into Phrase-Based Statistical Machine Translation">
<meta name="citation_author" content="S&#225;nchez-Cartagena, V&#237;ctor M.">
<meta name="citation_author" content="P&#233;rez-Ortiz, Juan Antonio">
<meta name="citation_author" content="S&#225;nchez-Mart&#237;nez, Felipe">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="17">
<meta name="citation_lastpage" content="61">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4761/live-4761-9056-jair.pdf">

<cite>Yulia  Tsvetkov and Chris  Dyer (2016) "Cross-Lingual Bridges with Models of Lexical Borrowing", Volume 55, pages 63-93</cite>
<p class="media"><a href="/media/4786/live-4786-9058-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4786'>doi:10.1613/jair.4786</a></p>
<p>Linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical, phonological, morphological, and syntactic) from a “donor” language to a “recipient” language as a result of contacts between communities speaking different languages. Borrowed words are found in all languages, and—in contrast to cognate relationships—borrowing relationships may exist across unrelated languages (for example, about 40% of Swahili’s vocabulary is borrowed from the unrelated language Arabic). In this work, we develop a model of morpho-phonological transformations across languages. Its features are based on universal constraints from Optimality Theory (OT), and we show that compared to several standard—but linguistically more na&#239;ve—baselines, our OT-inspired model obtains good performance at predicting donor forms from borrowed forms with only a few dozen training examples, making this a cost-effective strategy for sharing lexical information across languages. We demonstrate applications of the lexical borrowing model in machine translation, using resource-rich donor language to obtain translations of out-of-vocabulary loanwords in a lower resource language. Our framework obtains substantial improvements (up to 1.6 BLEU) over standard baselines.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Cross-Lingual Bridges with Models of Lexical Borrowing">
<meta name="citation_author" content="Tsvetkov, Yulia">
<meta name="citation_author" content="Dyer, Chris">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="63">
<meta name="citation_lastpage" content="93">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4786/live-4786-9058-jair.pdf">

<cite>Saif  M. Mohammad, Mohammad  Salameh and Svetlana  Kiritchenko (2016) "How Translation Alters Sentiment", Volume 55, pages 95-130</cite>
<p class="media"><a href="/media/4787/live-4787-9067-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4787'>doi:10.1613/jair.4787</a></p>
<p>Sentiment analysis research has predominantly been on English texts. Thus there exist many sentiment resources for English, but less so for other languages. Approaches to improve sentiment analysis in a resource-poor focus language include: (a) translate the focus language text into a resource-rich language such as English, and apply a powerful English sentiment analysis system on the text, and (b) translate resources such as sentiment labeled corpora and sentiment lexicons from English into the focus language, and use them as additional resources in the focus-language sentiment analysis system. In this paper we systematically examine both options. We use Arabic social media posts as stand-in for the focus language text. We show that sentiment analysis of English translations of Arabic texts produces competitive results, w.r.t. Arabic sentiment analysis. We show that Arabic sentiment analysis systems benefit from the use of automatically translated English sentiment lexicons. We also conduct manual annotation studies to examine why the sentiment of a translation is different from the sentiment of the source word or text. This is especially relevant for building better automatic translation systems. In the process, we create a state-of-the-art Arabic sentiment analysis system, a new dialectal Arabic sentiment lexicon, and the first Arabic-English parallel corpus that is independently annotated for sentiment by Arabic and English speakers.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="How Translation Alters Sentiment">
<meta name="citation_author" content="Mohammad, Saif M.">
<meta name="citation_author" content="Salameh, Mohammad">
<meta name="citation_author" content="Kiritchenko, Svetlana">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="95">
<meta name="citation_lastpage" content="130">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4787/live-4787-9067-jair.pdf">

<cite>Alejandro  Moreo Fern&#225;ndez, Andrea  Esuli and Fabrizio  Sebastiani (2016) "Distributional Correspondence Indexing for Cross-Lingual and Cross-Domain Sentiment Classification.", Volume 55, pages 131-163</cite>
<p class="media"><a href="/media/4762/live-4762-9070-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4762'>doi:10.1613/jair.4762</a></p>
<p>Domain Adaptation (DA) techniques aim at enabling machine learning methods learn effective classifiers for a "target'' domain when the only available training data belongs to a different "source'' domain. In this paper we present the Distributional Correspondence Indexing (DCI) method for domain adaptation in sentiment classification. DCI derives term representations in a vector space common to both domains where each dimension reflects its distributional correspondence to a pivot, i.e., to a highly predictive term that behaves similarly across domains. Term correspondence is quantified by means of a distributional correspondence function (DCF). We propose a number of efficient DCFs that are motivated by the distributional hypothesis, i.e., the hypothesis according to which terms with similar meaning tend to have similar distributions in text. Experiments show that DCI obtains better performance than current state-of-the-art techniques for cross-lingual and cross-domain sentiment classification. DCI also brings about a significantly reduced computational cost, and requires a smaller amount of human intervention. As a final contribution, we discuss a more challenging formulation of the domain adaptation problem, in which both the cross-domain and cross-lingual dimensions are tackled simultaneously.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Distributional Correspondence Indexing for Cross-Lingual and Cross-Domain Sentiment Classification.">
<meta name="citation_author" content="Moreo Fern&#225;ndez, Alejandro">
<meta name="citation_author" content="Esuli, Andrea">
<meta name="citation_author" content="Sebastiani, Fabrizio">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="131">
<meta name="citation_lastpage" content="163">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4762/live-4762-9070-jair.pdf">

<cite>Mamoun  Abu Helou, Matteo  Palmonari and Mustafa  Jarrar (2016) "Effectiveness of Automatic Translations for Cross-Lingual Ontology Mapping", Volume 55, pages 165-208</cite>
<p class="media"><a href="/media/4789/live-4789-9080-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4789'>doi:10.1613/jair.4789</a></p>
<p>Accessing or integrating data lexicalized in different languages is a challenge. Multilingual lexical resources play a fundamental role in reducing the language barriers to map concepts lexicalized in different languages. In this paper we present a large-scale study on the effectiveness of automatic translations to support two key cross-lingual ontology mapping tasks: the retrieval of candidate matches and the selection of the correct matches for inclusion in the final alignment.  We conduct our experiments using four different large gold standards, each one consisting of a pair of mapped wordnets, to cover four different families of languages. We categorize concepts based on their lexicalization (type of words, synonym richness, position in a subconcept graph) and analyze their distributions in the gold standards. Leveraging this categorization, we measure several aspects of translation effectiveness, such as word-translation correctness, word sense coverage, synset and synonym coverage. Finally, we thoroughly discuss several findings of our study, which we believe are helpful for the design of more sophisticated cross-lingual mapping algorithms.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Effectiveness of Automatic Translations for Cross-Lingual Ontology Mapping">
<meta name="citation_author" content="Abu Helou, Mamoun">
<meta name="citation_author" content="Palmonari, Matteo">
<meta name="citation_author" content="Jarrar, Mustafa">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="165">
<meta name="citation_lastpage" content="208">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4789/live-4789-9080-jair.pdf">

<cite>J&#246;rg  Tiedemann and Zeljko  Agi&#263; (2016) "Synthetic Treebanking for Cross-Lingual Dependency Parsing", Volume 55, pages 209-248</cite>
<p class="media"><a href="/media/4785/live-4785-9084-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4785'>doi:10.1613/jair.4785</a></p>
<p>How do we parse the languages for which no treebanks are available? This contribution addresses the cross-lingual viewpoint on statistical dependency parsing, in which we attempt to make use of resource-rich source language treebanks to build and adapt models for the under-resourced target languages. We outline the benefits, and indicate the drawbacks of the current major approaches. We emphasize synthetic treebanking: the automatic creation of target language treebanks by means of annotation projection and machine translation. We present competitive results in cross-lingual dependency parsing using a combination of various techniques that contribute to the overall success of the method. We further include a detailed discussion about the impact of part-of-speech label accuracy on parsing results that provide guidance in practical applications of cross-lingual methods for truly under-resourced languages.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Synthetic Treebanking for Cross-Lingual Dependency Parsing">
<meta name="citation_author" content="Tiedemann, J&#246;rg">
<meta name="citation_author" content="Agi&#263;, Zeljko">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="209">
<meta name="citation_lastpage" content="248">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4785/live-4785-9084-jair.pdf">

<cite>Ahmad  Khwileh, Debasis   Ganguly  and Gareth   J. F. Jones  (2016) "Utilisation of Metadata Fields and Query Expansion in Cross-Lingual Search of User-Generated Internet Video", Volume 55, pages 249-281</cite>
<p class="media"><a href="/media/4775/live-4775-9087-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4775'>doi:10.1613/jair.4775</a></p>
<p>Recent years have seen significant efforts in the area of Cross Language Information Retrieval (CLIR) for text retrieval. This work initially focused on formally published content, but more recently research has begun to concentrate on CLIR for informal social media content. However, despite the current expansion in online multimedia archives, there has been little work on CLIR for this content. While there has been some limited work on Cross-Language Video Retrieval (CLVR) for professional videos, such as documentaries or TV news broadcasts, there has to date, been no significant investigation of CLVR for the rapidly growing archives of informal user generated (UGC) content. Key differences between such UGC and professionally produced content are the nature and structure of the textual UGC metadata associated with it, as well as the form and quality of the content itself. In this setting, retrieval effectiveness may not only suffer from translation errors common to all CLIR tasks, but also recognition errors associated with the automatic speech recognition (ASR) systems used to transcribe the spoken content of the video and with the informality and inconsistency of the associated user-created metadata for each video. This work proposes and evaluates techniques to improve CLIR effectiveness of such noisy UGC content. Our experimental investigation shows that different sources of evidence, e.g. the content from different fields of the structured metadata, significantly affect CLIR effectiveness. Results from our experiments also show that each metadata field has a varying robustness to query expansion (QE) and hence can have a negative impact on the CLIR effectiveness. Our work proposes a novel adaptive QE technique that predicts the most reliable source for expansion and shows how this technique can be effective for improving the CLIR effectiveness for UGC content.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Utilisation of Metadata Fields and Query Expansion in Cross-Lingual Search of User-Generated Internet Video">
<meta name="citation_author" content="Khwileh, Ahmad">
<meta name="citation_author" content="Ganguly , Debasis ">
<meta name="citation_author" content=" J. F. Jones , Gareth">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="249">
<meta name="citation_lastpage" content="281">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4775/live-4775-9087-jair.pdf">

<cite>Jan  Rupnik, Andrej  Muhic, Gregor  Leban, Primoz  Skraba, Blaz  Fortuna and Marko  Grobelnik (2016) "News Across Languages - Cross-Lingual Document Similarity and Event Tracking", Volume 55, pages 283-316</cite>
<p class="media"><a href="/media/4780/live-4780-9097-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4780'>doi:10.1613/jair.4780</a></p>
<p>In today's world, we follow news which is distributed globally. Significant events are reported by different sources and in different languages. In this work, we address the problem of tracking of events in a large multilingual stream. Within a recently developed system Event Registry we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event.  Taking a multilingual stream and clusters of articles from each language, we compare different cross-lingual document similarity measures based on Wikipedia. This allows us to compute the similarity of any two articles regardless of language. Building on previous work, we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data. Using this capability, we then propose an approach to link clusters of articles across languages which represent the same event. We provide an extensive evaluation of the system as a whole, as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="News Across Languages - Cross-Lingual Document Similarity and Event Tracking">
<meta name="citation_author" content="Rupnik, Jan">
<meta name="citation_author" content="Muhic, Andrej">
<meta name="citation_author" content="Leban, Gregor">
<meta name="citation_author" content="Skraba, Primoz">
<meta name="citation_author" content="Fortuna, Blaz">
<meta name="citation_author" content="Grobelnik, Marko">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="283">
<meta name="citation_lastpage" content="316">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4780/live-4780-9097-jair.pdf">

<cite>Chien-Ju  Ho, Aleksandrs  Slivkins and Jennifer  Wortman Vaughan (2016) "Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems", Volume 55, pages 317-359</cite>
<p class="media"><a href="/media/4940/live-4940-9105-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4940'>doi:10.1613/jair.4940</a></p>
<p>Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of "bonus" payments. In this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each "arm" representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). Our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems">
<meta name="citation_author" content="Ho, Chien-Ju">
<meta name="citation_author" content="Slivkins, Aleksandrs">
<meta name="citation_author" content="Vaughan, Jennifer Wortman">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="317">
<meta name="citation_lastpage" content="359">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4940/live-4940-9105-jair.pdf">

<cite>Ziyu  Wang, Frank  Hutter, Masrour  Zoghi, David  Matheson and Nando  de Feitas (2016) "Bayesian Optimization in a Billion Dimensions via Random Embeddings", Volume 55, pages 361-387</cite>
<p class="media"><a href="/media/4806/live-4806-9131-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4806'>doi:10.1613/jair.4806</a></p>
<p>Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Bayesian Optimization in a Billion Dimensions via Random Embeddings">
<meta name="citation_author" content="Wang, Ziyu">
<meta name="citation_author" content="Hutter, Frank">
<meta name="citation_author" content="Zoghi, Masrour">
<meta name="citation_author" content="Matheson, David">
<meta name="citation_author" content="de Feitas, Nando">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="361">
<meta name="citation_lastpage" content="387">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4806/live-4806-9131-jair.pdf">

<cite>Aron  Culotta, Nirmal  Kumar Ravi and Jennifer  Cutler (2016) "Predicting Twitter User Demographics using Distant Supervision from Website Traffic Data", Volume 55, pages 389-408</cite>
<cite>AAAI 2015 Outstanding Paper Honorable Mention</cite><p class="media"><a href="/media/4935/live-4935-9136-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4935'>doi:10.1613/jair.4935</a></p>
<p>Understanding the demographics of users of online social networks has important applications for health, marketing, and public messaging. Whereas most prior approaches rely on a supervised learning approach, in which individual users are labeled with demographics for training, we instead create a distantly labeled dataset by collecting audience measurement data for 1,500 websites (e.g., 50% of visitors to gizmodo.com are estimated to have a bachelor's degree). We then fit a regression model to predict these demographics from information about the followers of each website on Twitter. Using patterns derived both from textual content and the social network of each user, our final model produces an average held-out correlation of .77 across seven different variables (age, gender, education, ethnicity, income, parental status, and political preference). We then apply this model to classify individual Twitter users by ethnicity, gender, and political preference, finding performance that is surprisingly competitive with a fully supervised approach.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Predicting Twitter User Demographics using Distant Supervision from Website Traffic Data">
<meta name="citation_author" content="Culotta, Aron">
<meta name="citation_author" content="Ravi, Nirmal Kumar">
<meta name="citation_author" content="Cutler, Jennifer">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="389">
<meta name="citation_lastpage" content="408">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4935/live-4935-9136-jair.pdf">

<cite>Raffaella  Bernardi, Ruket  Cakici, Desmond  Elliott, Aykut  Erdem, Erkut  Erdem, Nazli  Ikizler-Cinbis, Frank  Keller, Adrian  Muscat and Barbara  Plank (2016) "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", Volume 55, pages 409-442</cite>
<p class="media"><a href="/media/4900/live-4900-9139-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4900/live-4900-9140-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4900'>doi:10.1613/jair.4900</a></p>
<p>Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures">
<meta name="citation_author" content="Bernardi, Raffaella">
<meta name="citation_author" content="Cakici, Ruket">
<meta name="citation_author" content="Elliott, Desmond">
<meta name="citation_author" content="Erdem, Aykut">
<meta name="citation_author" content="Erdem, Erkut">
<meta name="citation_author" content="Ikizler-Cinbis, Nazli">
<meta name="citation_author" content="Keller, Frank">
<meta name="citation_author" content="Muscat, Adrian">
<meta name="citation_author" content="Plank, Barbara">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="409">
<meta name="citation_lastpage" content="442">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4900/live-4900-9139-jair.pdf">

<cite>Jilles  Steeve Dibangoye, Christopher  Amato, Olivier  Buffet and Fran&#231;ois  Charpillet (2016) "Optimally Solving Dec-POMDPs as Continuous-State MDPs", Volume 55, pages 443-497</cite>
<p class="media"><a href="/media/4623/live-4623-9142-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4623/live-4623-9143-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4623'>doi:10.1613/jair.4623</a></p>
<p>Decentralized partially observable Markov decision processes (Dec-POMDPs) provide a general model for decision-making under uncertainty in decentralized settings, but are difficult to solve optimally (NEXP-Complete). As a new way of solving these problems, we introduce the idea of transforming a Dec-POMDP into a continuous-state deterministic MDP with a piecewise-linear and convex value function. This approach makes use of the fact that planning can be accomplished in a centralized offline manner, while execution can still be decentralized. This new Dec-POMDP formulation, which we call an occupancy MDP, allows powerful POMDP and continuous-state MDP methods to be used for the first time. To provide scalability, we refine this approach by combining heuristic search and compact representations that exploit the structure present in multi-agent domains, without losing the ability to converge to an optimal solution.  In particular, we introduce a feature-based heuristic search value iteration (FB-HSVI) algorithm that relies on feature-based compact representations, point-based updates and efficient action selection.  A theoretical analysis demonstrates that FB-HSVI terminates in finite time with an optimal solution. We include an extensive empirical analysis using well-known benchmarks, thereby demonstrating that our approach provides significant scalability improvements compared to the state of the art.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Optimally Solving Dec-POMDPs as Continuous-State MDPs">
<meta name="citation_author" content="Dibangoye, Jilles Steeve">
<meta name="citation_author" content="Amato, Christopher">
<meta name="citation_author" content="Buffet, Olivier">
<meta name="citation_author" content="Charpillet, Fran&#231;ois">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="443">
<meta name="citation_lastpage" content="497">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4623/live-4623-9142-jair.pdf">

<cite>Ana  Armas Romero, Mark  Kaminski, Bernardo  Cuenca Grau and Ian  Horrocks (2016) "Module Extraction in Expressive Ontology Languages via Datalog Reasoning", Volume 55, pages 499-564</cite>
<p class="media"><a href="/media/4898/live-4898-9149-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4898'>doi:10.1613/jair.4898</a></p>
<p>Module extraction is the task of computing a (preferably small) fragment M of an ontology T that preserves a class of entailments over a signature of interest S. Extracting modules of minimal size is well-known to be computationally hard, and often algorithmically infeasible, especially for highly expressive ontology languages. Thus, practical techniques typically rely on approximations, where M provably captures the relevant entailments, but is not guaranteed to be minimal. Existing approximations ensure that M preserves all second-order entailments of T w.r.t. S, which is a stronger condition than is required in many applications, and may lead to unnecessarily large modules in practice. In this paper we propose a novel approach in which module extraction is reduced to a reasoning problem in datalog. Our approach generalises existing approximations in an elegant way. More importantly, it allows extraction of modules that are tailored to preserve only specific kinds of entailments, and thus are often significantly smaller. Our evaluation on a wide range of ontologies confirms the feasibility and benefits of our approach in practice.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Module Extraction in Expressive Ontology Languages via Datalog Reasoning">
<meta name="citation_author" content="Armas Romero, Ana">
<meta name="citation_author" content="Kaminski, Mark">
<meta name="citation_author" content="Cuenca Grau, Bernardo">
<meta name="citation_author" content="Horrocks, Ian">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="499">
<meta name="citation_lastpage" content="564">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4898/live-4898-9149-jair.pdf">

<cite>Felix  Brandt and Christian  Geist (2016) "Finding Strategyproof Social Choice Functions via SAT Solving", Volume 55, pages 565-602</cite>
<p class="media"><a href="/media/4959/live-4959-9411-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4959'>doi:10.1613/jair.4959</a>
<br/><a href="/media/4959/live-4959-9413-jair.txt">Appendix </a> - Erratum</p>
<p>A promising direction in computational social choice is to address research problems using computer-aided proving techniques. In particular with SAT solvers, this approach has been shown to be viable not only for proving classic impossibility theorems such as Arrow's Theorem but also for finding new impossibilities in the context of preference extensions. In this paper, we demonstrate that these computer-aided techniques can also be applied to improve our understanding of strategyproof irresolute social choice functions. These functions, however, requires a more evolved encoding as otherwise the search space rapidly becomes much too large. Our contribution is two-fold: We present an efficient encoding for translating such problems to SAT and leverage this encoding to prove new results about strategyproofness with respect to Kelly's and Fishburn's preference extensions. For example, we show that no Pareto-optimal majoritarian social choice function satisfies Fishburn-strategyproofness. Furthermore, we explain how human-readable proofs of such results can be extracted from minimal unsatisfiable cores of the corresponding SAT formulas. </p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Finding Strategyproof Social Choice Functions via SAT Solving">
<meta name="citation_author" content="Brandt, Felix">
<meta name="citation_author" content="Geist, Christian">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="565">
<meta name="citation_lastpage" content="602">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4959/live-4959-9411-jair.pdf">

<cite>Robert  Bredereck, Piotr  Faliszewski, Rolf  Niedermeier and Nimrod  Talmon (2016) "Large-Scale Election Campaigns: Combinatorial Shift Bribery", Volume 55, pages 603-652</cite>
<p class="media"><a href="/media/4927/live-4927-9180-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4927'>doi:10.1613/jair.4927</a></p>
<p>We study the complexity of a combinatorial variant of the Shift Bribery problem in elections. In the standard Shift Bribery problem, we are given an election where each voter has a preference order over the set of candidates and where an outside agent, the briber, can pay each voter to rank the briber's favorite candidate a given number of positions higher. The goal is to ensure the victory of the briber's preferred candidate. The combinatorial variant of the problem, introduced in this paper, models settings where it is possible to affect the position of the preferred candidate in multiple votes, either positively or negatively, with a single bribery action. This variant of the problem is particularly interesting in the context of large-scale campaign management problems (which, from the technical side, are modeled as bribery problems).  We show that, in general, the combinatorial variant of the problem is highly intractable; specifically, NP-hard, hard in the parameterized sense, and hard to approximate. Nevertheless, we provide parameterized algorithms and approximation algorithms for natural restricted cases.<br />
</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Large-Scale Election Campaigns: Combinatorial Shift Bribery">
<meta name="citation_author" content="Bredereck, Robert">
<meta name="citation_author" content="Faliszewski, Piotr">
<meta name="citation_author" content="Niedermeier, Rolf">
<meta name="citation_author" content="Talmon, Nimrod">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="603">
<meta name="citation_lastpage" content="652">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4927/live-4927-9180-jair.pdf">

<cite>Xiaoyuan  Zhu and Changhe  Yuan (2016) "Exact Algorithms for MRE Inference", Volume 55, pages 653-683</cite>
<p class="media"><a href="/media/4867/live-4867-9187-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4867/live-4867-9188-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4867'>doi:10.1613/jair.4867</a></p>
<p>Most Relevant Explanation (MRE) is an inference task in Bayesian networks that finds the most relevant partial instantiation of target variables as an explanation for given evidence by maximizing the Generalized Bayes Factor (GBF). No exact MRE algorithm has been developed previously except exhaustive search. This paper fills the void by introducing two Breadth-First Branch-and-Bound (BFBnB) algorithms for solving MRE based on novel upper bounds of GBF. One upper bound is created by decomposing the computation of GBF using a target blanket decomposition of evidence variables. The other upper bound improves the first bound in two ways. One is to split the target blankets that are too large by converting auxiliary nodes into pseudo-targets so as to scale to large problems. The other is to perform summations instead of maximizations on some of the target variables in each target blanket. Our empirical evaluations show that the proposed BFBnB algorithms make exact MRE inference tractable in Bayesian networks that could not be solved previously.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Exact Algorithms for MRE Inference">
<meta name="citation_author" content="Zhu, Xiaoyuan">
<meta name="citation_author" content="Yuan, Changhe">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="653">
<meta name="citation_lastpage" content="683">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4867/live-4867-9187-jair.pdf">

<cite>Roderick  Sebastiaan  de Nijs, Christian   Landsiedel, Dirk  Wollherr and Martin  Buss (2016) "Quadratization and Roof Duality of Markov Logic Networks", Volume 55, pages 685-714</cite>
<p class="media"><a href="/media/5023/live-5023-9198-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5023'>doi:10.1613/jair.5023</a></p>
<p>This article discusses the quadratization of Markov Logic Networks, which enables efficient approximate MAP computation by means of maximum flows. The procedure relies on a pseudo-Boolean representation of the model, and allows handling models of any order. The employed pseudo-Boolean representation can be used to identify problems that are guaranteed to be solvable in low polynomial-time. Results on common benchmark problems show that the proposed approach finds optimal assignments for most variables in excellent computational time and approximate solutions that match the quality of ILP-based solvers.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Quadratization and Roof Duality of Markov Logic Networks">
<meta name="citation_author" content="de Nijs, Roderick Sebastiaan ">
<meta name="citation_author" content="Landsiedel, Christian ">
<meta name="citation_author" content="Wollherr, Dirk">
<meta name="citation_author" content="Buss, Martin">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="685">
<meta name="citation_lastpage" content="714">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5023/live-5023-9198-jair.pdf">

<cite>Alberto  Garcia-Duran, Antoine  Bordes, Nicolas  Usunier and Yves  Grandvalet (2016) "Combining Two and Three-Way Embedding Models for Link Prediction in Knowledge Bases", Volume 55, pages 715-742</cite>
<p class="media"><a href="/media/5013/live-5013-9208-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/5013/live-5013-9207-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5013'>doi:10.1613/jair.5013</a></p>
<p>This paper tackles the problem of endogenous link prediction for knowledge base completion. Knowledge bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. Previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose Tatec, a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Combining Two and Three-Way Embedding Models for Link Prediction in Knowledge Bases">
<meta name="citation_author" content="Garcia-Duran, Alberto">
<meta name="citation_author" content="Bordes, Antoine">
<meta name="citation_author" content="Usunier, Nicolas">
<meta name="citation_author" content="Grandvalet, Yves">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="715">
<meta name="citation_lastpage" content="742">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5013/live-5013-9208-jair.pdf">

<cite>Francesco  Parisi and John  Grant (2016) "Knowledge Representation in Probabilistic Spatio-Temporal Knowledge Bases", Volume 55, pages 743-798</cite>
<p class="media"><a href="/media/4883/live-4883-9212-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4883'>doi:10.1613/jair.4883</a></p>
<p>We represent knowledge as integrity constraints in a formalization of probabilistic spatio-temporal knowledge bases. We start by defining the syntax and semantics of a formalization called PST knowledge bases.  This definition generalizes an earlier version, called SPOT, which is a declarative framework for the representation and processing of probabilistic spatio-temporal data where probability is represented as an interval because the exact value is unknown. We augment the previous definition by adding a type of non-atomic formula that expresses integrity constraints. The result is a highly expressive formalism for knowledge representation dealing with probabilistic spatio-temporal data.  We obtain complexity results both for checking the consistency of PST knowledge bases and for answering queries in PST knowledge bases, and also specify tractable cases.  All the domains in the PST framework are finite, but we extend our results also to arbitrarily large finite domains.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Knowledge Representation in Probabilistic Spatio-Temporal Knowledge Bases">
<meta name="citation_author" content="Parisi, Francesco">
<meta name="citation_author" content="Grant, John">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="743">
<meta name="citation_lastpage" content="798">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4883/live-4883-9212-jair.pdf">

<cite>Zhiwen  Fang, Chu-Min  Li and Ke  Xu (2016) "An Exact Algorithm Based on MaxSAT Reasoning for the Maximum Weight Clique Problem", Volume 55, pages 799-833</cite>
<p class="media"><a href="/media/4953/live-4953-9215-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4953/live-4953-9216-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4953'>doi:10.1613/jair.4953</a></p>
<p>Recently, MaxSAT reasoning is shown very effective in computing a tight upper bound for a Maximum Clique (MC) of a (unweighted) graph. In this paper, we apply MaxSAT reasoning to compute a tight upper bound for a Maximum Weight Clique (MWC) of a wighted graph. We first study three usual encodings of MWC into weighted partial MaxSAT dealing with hard clauses, which must be satisfied in all solutions, and soft clauses, which are weighted and can be falsified. The drawbacks of these encodings motivate us to propose an encoding of MWC into a special weighted partial MaxSAT formalism, called LW (Literal-Weighted) encoding and dedicated for upper bounding an MWC, in which both soft clauses and literals in soft clauses are weighted. An optimal solution of the LW MaxSAT instance gives an upper bound for an MWC, instead of an optimal solution for MWC. We then introduce two notions called the Top-k literal failed clause and the Top-k empty clause to extend classical MaxSAT reasoning techniques, as well as two sound transformation rules to transform an LW MaxSAT instance. Successive transformations of an LW MaxSAT instance driven by MaxSAT reasoning give a tight upper bound for the encoded MWC. The approach is implemented in a branch-and-bound algorithm called MWCLQ. Experimental evaluations on the broadly used DIMACS benchmark, BHOSLIB benchmark, random graphs and the benchmark from the winner determination problem show that our approach allows MWCLQ to reduce the search space significantly and to solve MWC instances effectively. Consequently, MWCLQ outperforms state-of-the-art exact algorithms on the vast majority of instances. Moreover, it is surprisingly effective in solving hard and dense instances.<br />
</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="An Exact Algorithm Based on MaxSAT Reasoning for the Maximum Weight Clique Problem">
<meta name="citation_author" content="Fang, Zhiwen">
<meta name="citation_author" content="Li, Chu-Min">
<meta name="citation_author" content="Xu, Ke">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="799">
<meta name="citation_lastpage" content="833">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4953/live-4953-9215-jair.pdf">

<cite>Dietmar  Jannach, Thomas  Schmitz and Kostyantyn  Shchekotykhin (2016) "Parallel Model-Based Diagnosis on Multi-Core Computers", Volume 55, pages 835-887</cite>
<p class="media"><a href="/media/5001/live-5001-9237-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5001'>doi:10.1613/jair.5001</a></p>
<p>Model-Based Diagnosis (MBD) is a principled and domain-independent way of analyzing why a system under examination is not behaving as expected. Given an abstract description (model) of the system's components and their behavior when functioning normally, MBD techniques rely on observations about the actual system behavior to reason about possible causes when there are discrepancies between the expected and observed behavior. Due to its generality, MBD has been successfully applied in a variety of application domains over the last decades.<br />
<br />
In many application domains of MBD, testing different hypotheses about the reasons for a failure can be computationally costly, e.g., because complex simulations of the system behavior have to be performed. In this work, we therefore propose different schemes of parallelizing the diagnostic reasoning process in order to better exploit the capabilities of modern multi-core computers. We propose and systematically evaluate parallelization schemes for Reiter's hitting set algorithm for finding all or a few leading minimal diagnoses using two different conflict detection techniques. Furthermore, we perform initial experiments for a basic depth-first search strategy to assess the potential of parallelization when searching for one single diagnosis. Finally, we test the effects of parallelizing "direct encodings" of the diagnosis problem in a constraint solver.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Parallel Model-Based Diagnosis on Multi-Core Computers">
<meta name="citation_author" content="Jannach, Dietmar">
<meta name="citation_author" content="Schmitz, Thomas">
<meta name="citation_author" content="Shchekotykhin, Kostyantyn">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="835">
<meta name="citation_lastpage" content="887">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5001/live-5001-9237-jair.pdf">

<cite>Natalia  Flerova, Radu  Marinescu and Rina  Dechter (2016) "Searching for the M Best Solutions in Graphical Models", Volume 55, pages 889-952</cite>
<p class="media"><a href="/media/4985/live-4985-9241-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4985/live-4985-9240-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4985'>doi:10.1613/jair.4985</a></p>
<p>  The paper focuses on finding the m best solutions to combinatorial optimization problems using best-first or depth-first branch and bound search.  Specifically, we present a new algorithm m-A*, extending the well-known A* to the m-best task, and for the first time prove that all its desirable properties, including soundness, completeness and optimal efficiency, are maintained. Since best-first algorithms require extensive memory, we also extend the memory-efficient depth-first branch and bound to the m-best task.<br />
<br />
  We adapt both algorithms to optimization tasks over graphical models (e.g., Weighted CSP and MPE in Bayesian networks), provide complexity analysis and an empirical evaluation. Our experiments confirm theory that the best-first approach is largely superior when memory is available, but depth-first branch and bound is more robust.  We also show that our algorithms are competitive with related schemes recently developed for the m-best task.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Searching for the M Best Solutions in Graphical Models">
<meta name="citation_author" content="Flerova, Natalia">
<meta name="citation_author" content="Marinescu, Radu">
<meta name="citation_author" content="Dechter, Rina">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="889">
<meta name="citation_lastpage" content="952">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4985/live-4985-9241-jair.pdf">

<cite>Ivan  Vuli&#263; and Marie-Francine  Moens (2016) "Bilingual Distributed Word Representations from Document-Aligned Comparable Data", Volume 55, pages 953-994</cite>
<p class="media"><a href="/media/4986/live-4986-9243-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4986/live-4986-9242-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4986'>doi:10.1613/jair.4986</a></p>
<p>We propose a new model for learning bilingual word representations from non-parallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Bilingual Distributed Word Representations from Document-Aligned Comparable Data">
<meta name="citation_author" content="Vuli&#263;, Ivan">
<meta name="citation_author" content="Moens, Marie-Francine">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="953">
<meta name="citation_lastpage" content="994">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4986/live-4986-9243-jair.pdf">

<cite>Jiang  Guo, Wanxiang  Che, David  Yarowsky, Haifeng  Wang and Ting  Liu (2016) "A Distributed Representation-Based Framework for Cross-Lingual Transfer Parsing", Volume 55, pages 995-1023</cite>
<p class="media"><a href="/media/4955/live-4955-9262-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4955'>doi:10.1613/jair.4955</a></p>
<p>This paper investigates the problem of cross-lingual transfer parsing, aiming at inducing dependency parsers for low-resource languages while using only training data from a resource-rich language (e.g., English). Existing model transfer approaches typically don't include lexical features, which are not transferable across languages. In this paper, we bridge the lexical feature gap by using distributed feature representations and their composition. We provide two algorithms for inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space. Consequently, both lexical features and non-lexical features can be used in our model for cross-lingual transfer. Furthermore, our framework is flexible enough to incorporate additional useful features such as cross-lingual word clusters. Our combined contributions achieve an average relative error reduction of 10.9% in labeled attachment score as compared with the delexicalized parser, trained on English universal treebank and transferred to three other languages. It also significantly outperforms state-of-the-art delexicalized models augmented with projected cluster features on identical data. Finally, we demonstrate that our models can be further boosted with minimal supervision (e.g., 100 annotated sentences) from target languages, which is of great significance for practical usage.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="A Distributed Representation-Based Framework for Cross-Lingual Transfer Parsing">
<meta name="citation_author" content="Guo, Jiang">
<meta name="citation_author" content="Che, Wanxiang">
<meta name="citation_author" content="Yarowsky, David">
<meta name="citation_author" content="Wang, Haifeng">
<meta name="citation_author" content="Liu, Ting">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="995">
<meta name="citation_lastpage" content="1023">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4955/live-4955-9262-jair.pdf">

<cite>Osman  Ba&#351;kaya and David  Jurgens (2016) "Semi-supervised Learning with Induced Word Senses for State of the Art Word Sense Disambiguation", Volume 55, pages 1025-1058</cite>
<p class="media"><a href="/media/4917/live-4917-9264-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4917/live-4917-9267-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4917'>doi:10.1613/jair.4917</a></p>
<p>Word Sense Disambiguation (WSD) aims to determine the meaning of a word in context, and successful approaches are known to benefit many applications in Natural Language Processing.  Although supervised learning has been shown to provide superior WSD performance, current sense-annotated corpora do not contain a sufficient number of instances per word type to train supervised systems for all words.  While unsupervised techniques have been proposed to overcome this data sparsity problem, such techniques have not outperformed supervised methods. In this paper, we propose a new approach to building semi-supervised WSD systems that combines a small amount of sense-annotated data with information from Word Sense Induction, a fully-unsupervised technique that automatically learns the different senses of a word based on how it is used. In three experiments, we show how sense induction models may be effectively combined to ultimately produce high-performance semi-supervised WSD systems that exceed the performance of state-of-the-art supervised WSD techniques trained on the same sense-annotated data. We anticipate that our results and released software will also benefit evaluation practices for sense induction systems and those working in low-resource languages by demonstrating how to quickly produce accurate WSD systems with minimal annotation effort.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Semi-supervised Learning with Induced Word Senses for State of the Art Word Sense Disambiguation">
<meta name="citation_author" content="Ba&#351;kaya, Osman">
<meta name="citation_author" content="Jurgens, David">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1025">
<meta name="citation_lastpage" content="1058">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4917/live-4917-9264-jair.pdf">

<cite>Hanxiao  Liu, Wanli  Ma, Yiming  Yang and Jaime  Carbonell (2016) "Learning Concept Graphs from Online Educational Data", Volume 55, pages 1059-1090</cite>
<p class="media"><a href="/media/5002/live-5002-9537-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5002'>doi:10.1613/jair.5002</a>
<br/><a href="/media/5002/live-5002-9539-jair.txt">Appendix 1</a> - Erratum&nbsp;|&nbsp;<a href="/media/5002/live-5002-9545-jair.zip">Appendix 2</a> - Source code and data</p>
<p>This paper addresses an open challenge in educational data mining, i.e., the problem of automatically mapping online courses from different providers (universities, MOOCs, etc.) onto a universal space of concepts, and predicting latent prerequisite dependencies (directed links) among both concepts and courses. We propose a novel approach for inference within and across course-level and concept-level directed graphs. In the training phase, our system projects partially observed course-level prerequisite links onto directed concept-level links; in the testing phase, the induced concept-level links are used to infer the unknown course-level prerequisite links. Whereas courses may be specific to one institution, concepts are shared across different providers. The bi-directional mappings enable our system to perform interlingua-style transfer learning, e.g. treating the concept graph as the interlingua and transferring the prerequisite relations across universities via the interlingua. Experiments on our newly collected datasets of courses from MIT, Caltech, Princeton and CMU show promising results.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Learning Concept Graphs from Online Educational Data">
<meta name="citation_author" content="Liu, Hanxiao">
<meta name="citation_author" content="Ma, Wanli">
<meta name="citation_author" content="Yang, Yiming">
<meta name="citation_author" content="Carbonell, Jaime">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1059">
<meta name="citation_lastpage" content="1090">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5002/live-5002-9537-jair.pdf">

<cite>Tuan  M. V. Le and Hady  W. Lauw (2016) "Semantic Visualization with Neighborhood Graph Regularization", Volume 55, pages 1091-1133</cite>
<cite>AAAI 2014 Honorable Mention for Outstanding Paper</cite><p class="media"><a href="/media/4983/live-4983-9285-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4983'>doi:10.1613/jair.4983</a></p>
<p>Visualization of high-dimensional data, such as text documents, is useful to map out the similarities among various data points. In the high-dimensional space, documents are commonly represented as bags of words, with dimensionality equal to the vocabulary size. Classical approaches to document visualization directly reduce this into visualizable two or three dimensions. Recent approaches consider an intermediate representation in topic space, between word space and visualization space, which preserves the semantics by topic modeling. While aiming for a good fit between the model parameters and the observed data, previous approaches have not considered the local consistency among data instances. We consider the problem of semantic visualization by jointly modeling topics and visualization on the intrinsic document manifold, modeled using a neighborhood graph. Each document has both a topic distribution and visualization coordinate. Specifically, we propose an unsupervised probabilistic model, called SEMAFORE, which aims to preserve the manifold in the lower-dimensional spaces through a neighborhood regularization framework designed for the semantic visualization task. To validate the efficacy of SEMAFORE, our comprehensive experiments on a number of real-life text datasets of news articles and Web pages show that the proposed methods outperform the state-of-the-art baselines on objective evaluation metrics.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Semantic Visualization with Neighborhood Graph Regularization">
<meta name="citation_author" content="Le, Tuan M. V.">
<meta name="citation_author" content="Lauw, Hady W.">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1091">
<meta name="citation_lastpage" content="1133">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4983/live-4983-9285-jair.pdf">

<cite>Stefano  V. Albrecht and Subramanian  Ramamoorthy (2016) "Exploiting Causality for Selective Belief Filtering in Dynamic Bayesian Networks", Volume 55, pages 1135-1178</cite>
<p class="media"><a href="/media/5044/live-5044-9280-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5044'>doi:10.1613/jair.5044</a></p>
<p>Dynamic Bayesian networks (DBNs) are a general model for stochastic processes with partially observed states. Belief filtering in DBNs is the task of inferring the belief state (i.e. the probability distribution over process states) based on incomplete and noisy observations. This can be a hard problem in complex processes with large state spaces. In this article, we explore the idea of accelerating the filtering task by automatically exploiting causality in the process. We consider a specific type of causal relation, called passivity, which pertains to how state variables cause changes in other variables. We present the Passivity-based Selective Belief Filtering (PSBF) method, which maintains a factored belief representation and exploits passivity to perform selective updates over the belief factors. PSBF produces exact belief states under certain assumptions and approximate belief states otherwise, where the approximation error is bounded by the degree of uncertainty in the process. We show empirically, in synthetic processes with varying sizes and degrees of passivity, that PSBF is faster than several alternative methods while achieving competitive accuracy. Furthermore, we demonstrate how passivity occurs naturally in a complex system such as a multi-robot warehouse, and how PSBF can exploit this to accelerate the filtering task.</p>
<a href="/vol/vol55.html">Click here to return to Volume 55 contents list</a>
<meta name="citation_title" content="Exploiting Causality for Selective Belief Filtering in Dynamic Bayesian Networks">
<meta name="citation_author" content="Albrecht, Stefano V.">
<meta name="citation_author" content="Ramamoorthy, Subramanian">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1135">
<meta name="citation_lastpage" content="1178">
<meta name="citation_volume" content="55">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5044/live-5044-9280-jair.pdf">
