e  davis 1999 order of magnitude comparisons of distance volume 10 pages 138

order of magnitude reasoning  reasoning by rough    comparisons of the sizes of quantities  is often called back of    the envelope calculation with the implication that the calculations    are quick though approximate  this paper exhibits an interesting    class of constraint sets in which order of magnitude reasoning is    demonstrably fast  specifically we present a polynomialtime    algorithm that can solve a set of constraints of the form points a    and b are much closer together than points c and d  we prove that    this algorithm can be applied if much closer together is    interpreted either as referring to an infinite difference in scale or    as referring to a finite difference in scale as long as the    difference in scale is greater than the number of variables in the    constraint set  we also prove that the firstorder theory over such    constraints is decidable



t  hogg 1999 solving highly constrained search problems with quantum computers volume 10 pages 3966

a previously developed quantum search algorithm for solving    1sat problems in a single step is generalized to apply to a range    of highly constrained ksat problems we identify a bound on the    number of clauses in satisfiability problems for which the    generalized algorithm can find a solution in a constant number of    steps as the number of variables increases this performance    contrasts with the linear growth in the number of steps required by    the best classical algorithms and the exponential number required    by classical and quantum methods that ignore the problem    structure in some cases the algorithm can also guarantee that    insoluble problems in fact have no solutions unlike previously    proposed quantum search algorithms



j  y halpern 1999 a counter example to theorems of cox and fine volume 10 pages 6785

coxs wellknown theorem justifying the use of probability is shown not to hold in finite domains the counterexample also suggests that coxs assumptions are insufficient to prove the result even in infinite domains the same counterexample is used to disprove a result of fine on comparative conditional probability



d  long and  m  fox 1999 efficient implementation of the plan graph in stan volume 10 pages 87115

stan is a graphplanbased planner socalled because it uses    a variety of state analysis techniques to enhance its performance    stan competed in the aips98 planning competition where it compared    well with the other competitors in terms of speed finding solutions    fastest to many of the problems posed although the domain analysis    techniques stan exploits are an important factor in its overall    performance we believe that the speed at which stan solved the    competition problems is largely due to the implementation of its plan    graph the implementation is based on two insights that many of the    graph construction operations can be implemented as bitlevel logical    operations on bit vectors and that the graph should not be explicitly    constructed beyond the fix point this paper describes the    implementation of stans plan graph and provides experimental results    which demonstrate the circumstances under which advantages can be    obtained from using this implementation



n  friedman and  j  y halpern 1999 modeling  belief  in  dynamic  systems part  ii  revision  and  update volume 10 pages 117167

the study of belief change has been an active area in    philosophy and ai  in recent years two special cases of belief    change belief revision and belief update have been studied in    detail  in a companion paper friedman  halpern 1997 we introduce    a new framework to model belief change this framework combines    temporal and epistemic modalities with a notion of plausibility    allowing us to examine the change of beliefs over time in this paper    we show how belief revision and belief update can be captured in our    framework  this allows us to compare the assumptions made by each    method and to better understand the principles underlying them  in    particular it shows that katsuno and mendelzons notion of belief    update katsuno  mendelzon 1991a depends on several strong    assumptions that may limit its applicability in artificial    intelligence  finally our analysis allow us to identify a notion of    minimal change that underlies a broad range of belief change    operations including revision and update



d  fuchs and  m  fuchs 1999 cooperation between topdown and bottomup theorem provers volume 10 pages 169198

topdown and bottomup theorem proving approaches each    have specific advantages and disadvantages  bottomup provers profit    from strong redundancy control but suffer from the lack of    goalorientation whereas topdown provers are goaloriented but often    have weak calculi when their proof lengths are considered  in order    to integrate both approaches we try to achieve cooperation between a    topdown and a bottomup prover in two different ways the first    technique aims at supporting a bottomup with a topdown prover a    topdown prover generates subgoal clauses they are then processed by    a bottomup prover  the second technique deals with the use of    bottomup generated lemmas in a topdown prover we apply our concept    to the areas of model elimination and superposition  we discuss the    ability of our techniques to shorten proofs as well as to reorder the    search space in an appropriate manner furthermore in order to    identify subgoal clauses and lemmas which are actually relevant for    the proof task we develop methods for a relevancybased filtering    experiments with the provers setheo and spass performed in the problem    library tptp reveal the high potential of our cooperation approaches



t  lukasiewicz 1999 probabilistic deduction with conditional constraints over basic events volume 10 pages 199241

we study the problem of probabilistic deduction with    conditional constraints over basic events we show that globally    complete probabilistic deduction with conditional constraints over    basic events is nphard we then concentrate on the special case of    probabilistic deduction in conditional constraint trees we elaborate    very efficient techniques for globally complete probabilistic    deduction in detail for conditional constraint trees with point    probabilities we present a local approach to globally complete    probabilistic deduction which runs in linear time in the size of the    conditional constraint trees for conditional constraint trees with    interval probabilities we show that globally complete probabilistic    deduction can be done in a global approach by solving nonlinear    programs we show how these nonlinear programs can be transformed into    equivalent linear programs which are solvable in polynomial time in    the size of the conditional constraint trees



w  w cohen  r  e schapire and  y  singer 1999 learning to order things volume 10 pages 243270

there are many applications in which it is desirable to    order rather than classify instances here we consider the problem of    learning how to order instances given feedback in the form of    preference judgments ie statements to the effect that one instance    should be ranked ahead of another  we outline a twostage approach in    which one first learns by conventional means a binary preference    function indicating whether it is advisable to rank one instance    before another here we consider an online algorithm for learning    preference functions that is based on freund and schapires hedge    algorithm  in the second stage new instances are ordered so as to    maximize agreement with the learned preference function  we show that    the problem of finding the ordering that agrees best with a learned    preference function is npcomplete  nevertheless we describe simple    greedy algorithms that are guaranteed to find a good approximation    finally we show how metasearch can be formulated as an ordering    problem and present experimental results on learning a combination of    search experts each of which is a domainspecific query expansion    strategy for a web search engine



k  m ting and  i  h witten 1999 issues in stacked generalization volume 10 pages 271289

stacked generalization is a general method of using a    highlevel model to combine lowerlevel models to achieve greater    predictive accuracy  in this paper we address two crucial issues    which have been considered to be a black art in classification tasks    ever since the introduction of stacked generalization in 1992 by    wolpert the type of generalizer that is suitable to derive the    higherlevel model and the kind of attributes that should be used as    its input  we find that best results are obtained when the    higherlevel model combines the confidence and not just the    predictions of the lowerlevel ones   we demonstrate the effectiveness of stacked generalization for combining     three different types of learning algorithms for classification tasks    we also compare the performance of stacked generalization with    majority vote and published results of arcing and bagging



t  s jaakkola and  m  i jordan 1999 variational probabilistic inference and the qmrdt network volume 10 pages 291322

we describe a variational approximation method for efficient    inference in largescale probabilistic models  variational methods    are deterministic procedures that provide approximations to marginal    and conditional probabilities of interest  they provide alternatives    to approximate inference methods based on stochastic sampling or    search  we describe a variational approach to the problem of    diagnostic inference in the quick medical reference qmr network    the qmr network is a largescale probabilistic graphical model built    on statistical and expert knowledge  exact probabilistic inference is    infeasible in this model for all but a small set of cases  we    evaluate our variational inference algorithm on a large set of    diagnostic test cases comparing the algorithm to a stateoftheart    stochastic sampling method



j  rintanen 1999 constructing conditional plans by a theoremprover volume 10 pages 323352

the research on conditional planning rejects the assumptions    that there is no uncertainty or incompleteness of knowledge with    respect to the state and changes of the system the plans operate on    without these assumptions the sequences of operations that achieve the    goals depend on the initial state and the outcomes of nondeterministic    changes in the system  this setting raises the questions of how to    represent the plans and how to perform plan search the answers are    quite different from those in the simpler classical framework  in    this paper we approach conditional planning from a new viewpoint that    is motivated by the use of satisfiability algorithms in classical    planning  translating conditional planning to formulae in the    propositional logic is not feasible because of inherent computational    limitations  instead we translate conditional planning to quantified    boolean formulae  we discuss three formalizations of conditional    planning as quantified boolean formulae and present experimental    results obtained with a theoremprover



d  e joslin and  d  p clements 1999 squeaky wheel optimization volume 10 pages 353373

we describe a general approach to optimization which we term    squeaky wheel optimization swo  in swo a greedy algorithm is    used to construct a solution which is then analyzed to find the    trouble spots ie those elements that if improved are likely to    improve the objective function score  the results of the analysis are    used to generate new priorities that determine the order in which the    greedy algorithm constructs the next solution  this    constructanalyzeprioritize cycle continues until some limit is    reached or an acceptable solution is found         swo can be viewed as operating on two search spaces solutions and    prioritizations  successive solutions are only indirectly related    via the reprioritization that results from analyzing the prior    solution  similarly successive prioritizations are generated by    constructing and analyzing solutions  this coupled search has some    interesting properties which we discuss         we report encouraging experimental results on two domains scheduling    problems that arise in fiberoptic cable manufacturing and graph    coloring problems  the fact that these domains are very different    supports our claim that swo is a general technique for optimization



s  chien  a  stechert and  d  mutz 1999 efficient heuristic hypothesis ranking volume 10 pages 375397

this paper considers the problem of learning the ranking of    a set of stochastic alternatives based upon incomplete information    ie a limited number of samples  we describe a system that at    each decision cycle outputs either a complete ordering on the    hypotheses or decides to gather additional information ie    observations at some cost  the ranking problem is a generalization    of the previously studied hypothesis selection problem  in selection    an algorithm must select the single best hypothesis while in ranking    an algorithm must order all the hypotheses      the central problem we address is achieving the desired ranking    quality while minimizing the cost of acquiring additional samples  we    describe two algorithms for hypothesis ranking and their application    for the probably approximately correct pac and expected loss el    learning criteria  empirical results are provided to demonstrate the    effectiveness of these ranking procedures on both synthetic and    realworld datasets



a  borgida 1999 extensible knowledge representation the case of description reasoners volume 10 pages 399434

this paper offers an approach to extensible knowledge    representation and reasoning for a family of formalisms known as    description logics the approach is based on the notion of adding new    concept constructors and includes a heuristic methodology for    specifying the desired extensions as well as a modularized software    architecture that supports implementing extensions  the architecture    detailed here falls in the normalizecompared paradigm and supports    both intentional reasoning subsumption involving concepts and    extensional reasoning involving individuals after incremental updates    to the knowledge base      the resulting approach can be used to extend the reasoner with    specialized notions that are motivated by specific problems or    application areas such as reasoning about dates plans etc in    addition it provides an opportunity to implement constructors that    are not currently yet sufficiently well understood theoretically but    are needed in practice also for constructors that are provably hard    to reason with eg ones whose presence would lead to    undecidability it allows the implementation of incomplete reasoners    where the incompleteness is tailored to be acceptable for the    application at hand



d  barber and p  de van laar 1999 variational cumulant expansions for intractable distributions volume 10 pages 435455

intractable distributions present a common difficulty in    inference within the probabilistic knowledge representation framework    and variational methods have recently been popular in providing an    approximate solution in this article we describe a perturbational    approach in the form of a cumulant expansion which to lowest order    recovers the standard kullbackleibler variational bound    higherorder terms describe corrections on the variational approach    without incurring much further computational cost  the relationship    to other perturbational approaches such as tap is also elucidated  we    demonstrate the method on a particular class of undirected graphical    models boltzmann machines for which our simulation results confirm    improved accuracy and enhanced stability during learning



e  birnbaum and  e  l lozinskii 1999 the good old davisputnam procedure helps counting models volume 10 pages 457477

as was shown recently many important ai problems require    counting the number of models of propositional formulas the problem    of counting models of such formulas is according to present    knowledge computationally intractable in a worst case based on the    davisputnam procedure we present an algorithm cdp that computes    the exact number of models of a propositional cnf or dnf formula    f let m and n be the number of clauses and variables of f    respectively and let p denote the probability that a literal l of f    occurs in a clause c of f then the average running time of cdp is    shown to be onmd where d1log1p  the practical    performance of cdp has been estimated in a series of experiments on a    wide variety of cnf formulas
