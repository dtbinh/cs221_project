m  cadoli  f  m donini  p  liberatore and  m  schaerf 2000 space efficiency of propositional knowledge representation formalisms volume 13 pages 131

we investigate the space efficiency of a propositional    knowledge representation pkr formalism intuitively the space    efficiency of a formalism f in representing a certain piece of    knowledge a is the size of the shortest formula of f that    represents a in this paper we assume that knowledge is    either a set of propositional interpretations models or a set of    propositional formulae theorems we provide a formal way of    talking about the relative ability of pkr formalisms to compactly    represent a set of models or a set of theorems we introduce two new    compactness measures the corresponding classes and show that the    relative space efficiency of a pkr formalism in representing    modelstheorems is directly related to such classes in particular    we consider formalisms for nonmonotonic reasoning such as    circumscription and default logic as well as belief revision    operators and the stable model semantics for logic programs with    negation one interesting result is that formalisms with the same    time complexity do not necessarily belong to the same space    efficiency class



m  hauskrecht 2000 valuefunction approximations for partially observable markov decision processes volume 13 pages 3394

partially observable markov decision processes pomdps    provide an elegant mathematical framework for modeling complex    decision and planning problems in stochastic domains in which states    of the system are observable only indirectly via a set of imperfect    or noisy observations the modeling advantage of pomdps however    comes at a price  exact methods for solving them are computationally    very expensive and thus applicable in practice only to very simple    problems we focus on efficient approximation heuristic methods that    attempt to alleviate the computational problem and trade off accuracy    for speed we have two objectives here first we survey various    approximation methods analyze their properties and relations and    provide some new insights into their differences second we present a    number of new approximation methods and novel refinements of existing    techniques the theoretical results are supported by experiments on a    problem from the agent navigation domain



d  f gordon 2000 asimovian adaptive agents volume 13 pages 95153

the goal of this research is to develop agents that     are adaptive and predictable and timely at first blush     these three requirements seem contradictory for example      adaptation risks introducing undesirable side effects     thereby making agents behavior less predictable furthermore    although formal verification can assist in ensuring    behavioral predictability it is known to be timeconsuming  our solution to the challenge of satisfying all three    requirements is the following agents have finitestate    automaton plans which are adapted online via evolutionary    learning perturbation operators to ensure that critical    behavioral constraints are always satisfied agents plans    are first formally verified they are then reverified after    every adaptation if reverification concludes that constraints    are violated the plans are repaired the main objective of     this paper is to improve the efficiency of reverification     after learning so that agents have a sufficiently rapid     response time we present two solutions positive results     that certain learning operators are a priori guaranteed to    preserve useful classes of behavioral assurance constraints    which implies that no reverification is needed for these     operators and efficient incremental reverification algorithms     for those learning operators that have negative a priori results



j  cheng and  m  j druzdzel 2000 aisbn an adaptive importance sampling algorithm for evidential reasoning in large bayesian networks volume 13 pages 155188




honorable mention for the 2005 ijcaijair best paper prize

stochastic sampling algorithms while an attractive alternative to    exact algorithms in very large bayesian network models have been    observed to perform poorly in evidential reasoning with extremely    unlikely evidence to address this problem we propose an adaptive    importance sampling algorithm aisbn that shows promising    convergence rates even under extreme conditions and seems to    outperform the existing sampling algorithms consistently three    sources of this performance improvement are 1 two heuristics for    initialization of the importance function that are based on the    theoretical properties of importance sampling in finitedimensional    integrals and the structural advantages of bayesian networks 2 a    smooth learning method for the importance function and 3 a dynamic    weighting function for combining samples from different stages of the    algorithm       we tested the performance of the aisbn algorithm along with two state    of the art general purpose sampling algorithms likelihood weighting    fung  chang 1989 shachter  peot 1989 and selfimportance    sampling shachter  peot 1989 we used in our tests three large    real bayesian network models available to the scientific community    the cpcs network pradhan et al 1994 the pathfinder network    heckerman horvitz  nathwani 1990 and the andes network conati    gertner vanlehn  druzdzel 1997 with evidence as unlikely as    1041 while the aisbn algorithm always performed better than the    other two algorithms in the majority of the test cases it achieved    orders of magnitude improvement in precision of the results    improvement in speed given a desired precision is even more dramatic    although we are unable to report numerical results here as the other    algorithms almost never achieved the precision reached even by the    first few iterations of the aisbn algorithm



r  m jensen and  m  m veloso 2000 obddbased universal planning for synchronized agents in nondeterministic domains volume 13 pages 189226

recently model checking representation and search techniques    were shown to be efficiently applicable to planning in particular to    nondeterministic planning such planning approaches use ordered    binary decision diagrams obdds to encode a planning domain as a    nondeterministic finite automaton and then apply fast algorithms from    model checking to search for a solution obdds can effectively scale    and can provide universal plans for complex planning domains we are    particularly interested in addressing the complexities arising in    nondeterministic multiagent domains  in this article we present    umop a new universal obddbased planning framework for    nondeterministic multiagent domains we introduce a new planning    domain description language nadl to specify nondeterministic    multiagent domains  the language contributes the explicit definition    of controllable agents and uncontrollable environment agents we    describe the syntax and semantics of nadl and show how to build an    efficient obddbased representation of an nadl description  the umop    planning system uses nadl and different obddbased universal planning    algorithms it includes the previously developed strong and strong    cyclic planning algorithms in addition we introduce our new    optimistic planning algorithm that relaxes optimality guarantees and    generates plausible universal plans in some domains where no strong    nor strong cyclic solution exists we present empirical results    applying umop to domains ranging from deterministic and singleagent    with no environment actions to nondeterministic and multiagent with    complex environment actions umop is shown to be a rich and efficient    planning system



t  g dietterich 2000 hierarchical reinforcement learning with the maxq value function decomposition volume 13 pages 227303




2003 ijcaijair best paper prize

this paper presents a new approach to hierarchical    reinforcement learning based on decomposing the target markov decision    process mdp into a hierarchy of smaller mdps and decomposing the    value function of the target mdp into an additive combination of the    value functions of the smaller mdps  the decomposition known as the    maxq decomposition has both a procedural semanticsas a subroutine    hierarchyand a declarative semanticsas a representation of the    value function of a hierarchical policy  maxq unifies and extends    previous work on hierarchical reinforcement learning by singh    kaelbling and dayan and hinton  it is based on the assumption that    the programmer can identify useful subgoals and define subtasks that    achieve these subgoals  by defining such subgoals the programmer    constrains the set of policies that need to be considered during    reinforcement learning  the maxq value function decomposition can    represent the value function of any policy that is consistent with the    given hierarchy  the decomposition also creates opportunities to    exploit state abstractions so that individual mdps within the    hierarchy can ignore large parts of the state space  this is    important for the practical application of the method  this paper    defines the maxq hierarchy proves formal results on its    representational power and establishes five conditions for the safe    use of state abstractions  the paper presents an online modelfree    learning algorithm maxqq and proves that it converges with    probability 1 to a kind of locallyoptimal policy known as a    recursively optimal policy even in the presence of the five kinds of    state abstraction  the paper evaluates the maxq representation and    maxqq through a series of experiments in three domains and shows    experimentally that maxqq with state abstractions converges to a    recursively optimal policy much faster than flat q learning  the fact    that maxq learns a representation of the value function has an    important benefit it makes it possible to compute and execute an    improved nonhierarchical policy via a procedure similar to the    policy improvement step of policy iteration  the paper demonstrates    the effectiveness of this nonhierarchical execution experimentally    finally the paper concludes with a comparison to related work and a    discussion of the design tradeoffs in hierarchical reinforcement learning



a  cimatti and  m  roveri 2000 conformant planning via symbolic model checking volume 13 pages 305338

we tackle the problem of planning in nondeterministic    domains by presenting a new approach to conformant planning    conformant planning is the problem of finding a sequence of actions    that is guaranteed to achieve the goal despite the nondeterminism of    the domain our approach is based on the representation of the    planning domain as a finite state automaton we use symbolic model    checking techniques in particular binary decision diagrams to    compactly represent and efficiently search the automaton in this    paper we make the following contributions first we present a general    planning algorithm for conformant planning which applies to fully    nondeterministic domains with uncertainty in the initial condition    and in action effects the algorithm is based on a breadthfirst    backward search and returns conformant plans of minimal length if a    solution to the planning problem exists otherwise it terminates    concluding that the problem admits no conformant solution second we    provide a symbolic representation of the search space based on binary    decision diagrams bdds which is the basis for search techniques    derived from symbolic model checking the symbolic representation    makes it possible to analyze potentially large sets of states and    transitions in a single computation step thus providing for an    efficient implementation  third we present cmbp conformant model    based planner an efficient implementation of the data structures and    algorithm described above directly based on bdd manipulations which    allows for a compact representation of the search layers and an    efficient implementation of the search steps finally we present an    experimental comparison of our approach with the stateoftheart    conformant planners cgp qbfplan and gpt our analysis includes all    the planning problems from the distribution packages of these systems    plus other problems defined to stress a number of specific factors    our approach appears to be the most effective cmbp is strictly more    expressive than qbfplan and cgp and in all the problems where a    comparison is possible cmbp outperforms its competitors sometimes by    orders of magnitude
