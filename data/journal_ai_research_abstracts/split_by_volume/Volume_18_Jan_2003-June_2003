c  thompson 2003 acquiring wordmeaning mappings for natural language interfaces volume 18 pages 144

this paper focuses on a system wolfie word learning from    interpreted examples that acquires a semantic lexicon from a corpus    of sentences paired with semantic representations  the lexicon    learned consists of phrases paired with meaning representations    wolfie is part of an integrated system that learns to transform    sentences into representations such as logical database queries        experimental results are presented demonstrating wolfies ability to    learn useful lexicons for a database interface in four different    natural languages  the usefulness of the lexicons learned by wolfie    are compared to those acquired by a similar system with results    favorable to wolfie  a second set of experiments demonstrates    wolfies ability to scale to larger and more difficult albeit    artificially generated corpora           in natural language acquisition it is difficult to gather the    annotated data needed for supervised learning however unannotated    data is fairly plentiful  active learning methods attempt to select    for annotation and training only the most informative examples and    therefore are potentially very useful in natural language    applications  however most results to date for active learning have    only considered standard classification tasks  to reduce annotation    effort while maintaining accuracy we apply active learning to    semantic lexicons  we show that active learning can significantly    reduce the number of annotated examples required to achieve a given    level of performance



a  t cemgil and  b  kappen 2003 monte carlo methods for tempo tracking and rhythm quantization volume 18 pages 4581

we present a probabilistic generative model for timing    deviations in expressive music performance the structure of the    proposed model is equivalent to a switching state space model the    switch variables correspond to discrete note locations as in a musical    score the continuous hidden variables denote the tempo  we formulate    two well known music recognition problems namely tempo tracking and    automatic transcription rhythm quantization as filtering and maximum    a posteriori map state estimation tasks exact computation of    posterior features such as the map state is intractable in this model    class so we introduce monte carlo methods for integration and    optimization we compare markov chain monte carlo mcmc methods such    as gibbs sampling simulated annealing and iterative improvement and    sequential monte carlo methods particle filters our simulation    results suggest better results with sequential methods  the methods    can be applied in both online and batch scenarios such as tempo    tracking and transcription and are thus potentially useful in a number    of music applications such as adaptive automatic accompaniment score    typesetting and music information retrieval



o  grumberg  s  livne and  s  markovitch 2003 learning to order bdd variables in verification volume 18 pages 83116

the size and complexity of software and hardware systems    have significantly increased in the past years  as a result it is    harder to guarantee their correct behavior one of the most successful    methods for automated verification of finitestate systems is model    checking most of the current modelchecking systems use binary    decision diagrams bdds for the representation of the tested model    and in the verification process of its properties generally bdds    allow a canonical compact representation of a boolean function given    an order of its variables the more compact the bdd is the better    performance one gets from the verifier however finding an optimal    order for a bdd is an npcomplete problem therefore several    heuristic methods based on expert knowledge have been developed for    variable ordering       we propose an alternative approach in which the variable ordering     algorithm gains ordering experience from training models and     uses the learned knowledge for finding good orders our     methodology is based on offline learning of pair precedence     classifiers from training models that is learning which variable     pair permutation is more likely to lead to a good order for each     training model a number of training sequences are evaluated every     training model variable pair permutation is then tagged based on     its performance on the evaluated orders the tagged permutations     are then passed through a feature extractor and are given as     examples to a classifier creation algorithm given a model for     which an order is requested the ordering algorithm consults each     precedence classifier and constructs a pair precedence table     which is used to create the order      our algorithm was integrated with smv which is one of the most     widely used verification systems preliminary empirical evaluation of our     methodology using real benchmark models shows performance that     is better than random ordering and is competitive with existing     algorithms that use expert knowledge we believe that in     subdomains of models alu caches etc our system will prove     even more valuable this is because it features the ability to     learn subdomain knowledge something that no other ordering     algorithm does



j  peral and  a  ferrandez 2003 translation of pronominal anaphora between english and spanish discrepancies and evaluation volume 18 pages 117147

this paper evaluates the different tasks carried out in the    translation of pronominal anaphora in a machine translation mt    system the mt interlingua approach named agir anaphora generation    with an interlingua representation improves upon other proposals    presented to date because it is able to translate intersentential    anaphors detect coreference chains and translate spanish zero    pronouns into englishissues hardly considered by other systems the    paper presents the resolution and evaluation of these anaphora    problems in agir with the use of different kinds of knowledge    lexical morphological syntactic and semantic the translation of    english and spanish anaphoric thirdperson personal pronouns    including spanish zero pronouns into the target language has been    evaluated on unrestricted corpora we have obtained a precision of    804 and 848 in the translation of spanish and english pronouns    respectively although we have only studied the spanish and english    languages our approach can be easily extended to other languages such    as portuguese italian or japanese



k  lerman  s  n minton and  c  a knoblock 2003 wrapper maintenance a machine learning approach volume 18 pages 149181

the proliferation of online information sources has led to an    increased use of wrappers for extracting data from web sources while    most of the previous research has focused on quick and efficient    generation of wrappers the development of tools for wrapper    maintenance has received less attention this is an important research    problem because web sources often change in ways that prevent the    wrappers from extracting data correctly  we present an efficient    algorithm that learns structural information about data from positive    examples alone we describe how this information can be used for two    wrapper maintenance applications wrapper verification and    reinduction the wrapper verification system detects when a wrapper is    not extracting correct data usually because the web source has    changed its format  the reinduction algorithm automatically recovers    from changes in the web source by identifying data on web pages so    that a new wrapper may be generated for this source  to validate our    approach we monitored 27 wrappers over a period of a year the    verification algorithm correctly discovered 35 of the 37 wrapper    changes and made 16 mistakes resulting in precision of 073 and    recall of 095 we validated the reinduction algorithm on ten web    sources we were able to successfully reinduce the wrappers obtaining    precision and recall values of 090 and 080 on the data    extraction task



k  c tan  e  f khor  t  h lee and  r  sathikannan 2003 an evolutionary algorithm with advanced goal and priority specification for multiobjective optimization volume 18 pages 183215

this paper presents an evolutionary algorithm with a new    goalsequence domination scheme for better decision support in    multiobjective optimization the approach allows the inclusion of    advanced hardsoft priority and constraint information on each    objective component and is capable of incorporating multiple    specifications with overlapping or nonoverlapping objective functions    via logical or and and connectives to drive the search    towards multiple regions of tradeoff in addition we propose a    dynamic sharing scheme that is simple and adaptively estimated    according to the online population distribution without needing any a    priori parameter setting each feature in the proposed algorithm is    examined to show its respective contribution and the performance of    the algorithm is compared with other evolutionary optimization    methods it is shown that the proposed algorithm has performed well in    the diversity of evolutionary search and uniform distribution of    nondominated individuals along the final tradeoffs without    significant computational effort the algorithm is also applied to the    design optimization of a practical servo control system for hard disk    drives with a single voicecoilmotor actuator results of the    evolutionary designed servo control system show a superior closedloop    performance compared to classical pid or rpt approaches



d  e wilkins  t  j lee and  p  berry 2003 interactive execution monitoring of agent teams volume 18 pages 217261

there is an increasing need for automated support for humans    monitoring the activity of distributed teams of cooperating agents    both human and machine we characterize the domainindependent    challenges posed by this problem and describe how properties of    domains influence the challenges and their solutions we will    concentrate on dynamic datarich domains where humans are ultimately    responsible for team behavior thus the automated aid should    interactively support effective and timely decision making by the    human we present a domainindependent categorization of the types of    alerts a planbased monitoring system might issue to a user where    each type generally requires different monitoring techniques we    describe a monitoring framework for integrating many domainspecific    and taskspecific monitoring techniques and then using the concept of    value of an alert to avoid operator overload       we use this framework to describe an execution monitoring approach we    have used to implement execution assistants eas in two different    dynamic datarich realworld domains to assist a human in    monitoring team behavior one domain army small unit operations has    hundreds of mobile geographically distributed agents a combination    of humans robots and vehicles the other domain teams of unmanned    ground and air vehicles has a handful of cooperating robots both    domains involve unpredictable adversaries in the vicinity our    approach customizes monitoring behavior for each specific task plan    and situation as well as for user preferences our eas alert the    human controller when reported events threaten plan execution or    physically threaten team members alerts were generated in a timely    manner without inundating the user with too many alerts less than    10 percent of alerts are unwanted as judged by domain experts



d  poole and  n  l zhang 2003 exploiting contextual independence in probabilistic inference volume 18 pages 263313

bayesian belief networks have grown to prominence because    they provide compact representations for many problems for which    probabilistic inference is appropriate and there are algorithms to    exploit this compactness the next step is to allow compact    representations of the conditional probabilities of a variable given    its parents in this paper we present such a representation that    exploits contextual independence in terms of parent contexts which    variables act as parents may depend on the value of other    variables the internal representation is in terms of contextual    factors confactors that is simply a pair of a context and a table    the algorithm contextual variable elimination is based on the    standard variable elimination algorithm that eliminates the nonquery    variables in turn but when eliminating a variable the tables that    need to be multiplied can depend on the context this algorithm    reduces to standard variable elimination when there is no contextual    independence structure to exploit we show how this can be much more    efficient than variable elimination when there is structure to    exploit we explain why this new method can exploit more structure    than previous methods for structured belief network inference and an    analogous algorithm that uses trees



r  i brafman and  c  domshlak 2003 structure and complexity in planning with unary operators volume 18 pages 315349

unary operator domains  ie domains in which operators    have a single effect  arise naturally in many control problems  in    its most general form the problem of strips planning in unary    operator domains is known to be as hard as the general strips planning    problem  both are pspacecomplete however unary operator domains    induce a natural structure called the domains causal graph this    graph relates between the preconditions and effect of each domain    operator causal graphs were exploited by williams and nayak in order    to analyze plan generation for one of the controllers in nasas    deepspace one spacecraft there they utilized the fact that when    this graph is acyclic a serialization ordering over any subgoal can    be obtained quickly in this paper we conduct a comprehensive study of    the relationship between the structure of a domains causal graph and    the complexity of planning in this domain  on the positive side we    show that a nontrivial polynomial time plan generation algorithm    exists for domains whose causal graph induces a polytree with a    constant bound on its node indegree on the negative side we show    that even plan existence is hard when the graph is a directedpath    singly connected dag  more generally we show that the number of    paths in the causal graph is closely related to the complexity of    planning in the associated domain  finally we relate our results to    the question of complexity of planning with serializable subgoals



p  f patelschneider and  r  sebastiani 2003 a new general method to generate random modal formulae for testing decision procedures volume 18 pages 351389

the recent emergence of heavilyoptimized modal decision    procedures has highlighted the key role of empirical testing in this    domain  unfortunately the introduction of extensive empirical tests    for modal logics is recent and so far none of the proposed test    generators is very satisfactory  to cope with this fact we present a    new random generation method that provides benefits over previous    methods for generating empirical tests  it fixes and much generalizes    one of the bestknown methods the random cnfm test allowing for    generating a much wider variety of problems covering in principle the    whole input space  our new method produces much more suitable test    sets for the current generation of modal decision procedures  we    analyze the features of the new method by means of an extensive    collection of empirical tests
independence  the study of what is relevant to a given    problem of reasoning  has received an increasing attention from the    ai community in this paper we consider two basic forms of    independence namely a syntactic one and a semantic one we show    features and drawbacks of them  in particular while the syntactic    form of independence is computationally easy to check there are cases    in which things that intuitively are not relevant are not recognized    as such  we also consider the problem of forgetting ie distilling    from a knowledge base only the part that is relevant to the set of    queries constructed from a subset of the alphabet while such process    is computationally hard it allows for a simplification of subsequent    reasoning and can thus be viewed as a form of compilation once the    relevant part of a knowledge base has been extracted all reasoning    tasks to be performed can be simplified



s  acid and l  m de campos 2003 searching for bayesian network structures in the space of restricted acyclic partially directed graphs volume 18 pages 445490

although many algorithms have been designed to construct    bayesian network structures using different approaches and principles    they all employ only two methods those based on independence    criteria and those based on a scoring function and a search procedure    although some methods combine the two within the scoresearch    paradigm the dominant approach uses local search methods in the space    of directed acyclic graphs dags where the usual choices for    defining the elementary modifications local changes that can be    applied are arc addition arc deletion and arc reversal in this    paper we propose a new local search method that uses a different    search space and which takes account of the concept of equivalence    between network structures restricted acyclic partially directed    graphs rpdags in this way the number of different configurations    of the search space is reduced thus improving efficiency moreover    although the final result must necessarily be a local optimum given    the nature of the search method the topology of the new search space    which avoids making early decisions about the directions of the arcs    may help to find better local optima than those obtained by searching    in the dag space detailed results of the evaluation of the proposed    search method on several test problems including the wellknown alarm    monitoring system are also presented



e  reiter  s  g sripada and  r  robertson 2003 acquiring correct knowledge for natural language generation volume 18 pages 491516

natural language generation nlg systems are computer software    systems that produce texts in english and other human languages often    from nonlinguistic input data nlg systems like most ai systems need    substantial amounts of knowledge  however our experience in two     nlg projects suggests that it is difficult to acquire correct knowledge     for nlg systems indeed every knowledge acquisition ka technique     we tried had significant problems in general terms these problems were     due to the complexity novelty and poorly understood nature of the     tasks our systems attempted and were worsened by the fact that people    write so differently this meant in particular that corpusbased ka     approaches suffered because it was impossible to assemble a sizable     corpus of highquality consistent manually written texts in our domains    and structured expertoriented ka techniques suffered because experts     disagreed and because we could not get enough information about special     and unusual cases to build robust systems we believe that such problems     are likely to affect many other nlg systems as well  in the long term     we hope that new ka techniques may emerge to help nlg system builders      in the shorter term we believe that understanding how individual ka     techniques can fail and using a mixture of different ka techniques     with different strengths and weaknesses can help developers acquire     nlg knowledge that is mostly correct
