r  micalizio and p  torasso 2014 cooperative monitoring to diagnose multiagent plans volume 51 pages 170

diagnosing the execution of a multiagent plan map means identifying and explaining action failures ie actions that did not reach their expected effects current approaches to map diagnosis are substantially centralized and assume that action failures are independent of each other
an experimental analysis demonstrates that the cwcm methodology together with the proposed diagnostic inferences are effective in identifying and explaining action failures even in scenarios where the system observability is significantly reduced



m  winikoff and s  cranefield 2014 on the testability of bdi agent systems volume 51 pages 71131

before deploying a software system we need to assure ourselves and stakeholders that the system will behave correctly this assurance is usually done by testing the system however it is intuitively obvious that adaptive systems including agentbased systems  can exhibit complex behaviour and are thus harder to test in this paper we examine this obvious intuition in the case of beliefdesireintention bdi agents  we analyse the size of the behaviour space of bdi agents  and show that although the intuition is correct the factors that influence the size are not what we expected them to be specifically we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected we also discuss the implications of these findings on the testability of bdi agents




k  woodsend and m  lapata 2014 text rewriting improves semantic role labeling volume 51 pages 133164

largescale annotated corpora are a prerequisite to developing highperformance nlp systems such corpora are expensive to produce limited in size often demanding linguistic expertise in this paper we use text rewriting as a means of increasing the amount of labeled data available for model training our method uses automatically extracted rewrite rules from comparable corpora and bitexts to generate multiple versions of sentences annotated with gold standard labels we apply this idea to semantic role labeling and show that a model trained on rewritten data outperforms the state of the art on the conll2009 benchmark dataset




z  feldman and c  domshlak 2014 simple regret optimization in online planning for markov decision processes volume 51 pages 165205

we consider online planning in markov decision processes mdps in online planning the agent focuses on its current state only deliberates about the set of possible policies from that state onwards and when interrupted uses the outcome of that exploratory deliberation to choose what action to perform next formally the performance of algorithms for online planning is assessed in terms of simple regret the agents expected performance loss when the chosen action rather than an optimal one is followed
to date stateoftheart algorithms for online planning in general mdps are either best effort or guarantee only polynomialrate reduction of simple regret over time here we introduce a new montecarlo tree search algorithm brue that guarantees exponentialrate and smooth reduction of simple regret  at a high level brue is based on a simple yet nonstandard statespace sampling scheme mcts2e in which different parts of each sample are dedicated to different exploratory objectives we further extend brue with a variant of learning by forgetting the resulting parametrized algorithm bruealpha exhibits even more attractive formal guarantees than brue our  empirical evaluation shows that both brue and its generalization bruealpha are also very effective in practice and  compare favorably to the stateoftheart



a  adiga c  j kuhlman h  s mortveit and a  k s vullikanti 2014 sensitivity of diffusion dynamics to network uncertainty volume 51 pages 207226




aaai 2013 honorable mention for best paper

simple diffusion processes on networks have been used to model analyze and predict diverse phenomena such as spread of diseases information and memes more often than not the underlying network data is noisy and sampled this prompts the following natural question how sensitive are the diffusion dynamics and subsequent conclusions to uncertainty in the network structure 
in this paper we consider two popular diffusion models independent cascade ic model and linear threshold lt model we study how the expected number of vertices that are influencedinfected for particular initial conditions are affected by network perturbations through rigorous analysis under the assumption of a reasonable perturbation model we establish the following main results 1 for the ic model we characterize the sensitivity to network perturbation in terms of the critical probability for phase transition of the network we find that the expected number of infections is quite stable unless the transmission probability is close to the critical probability 2 we show that the standard lt model with uniform edge weights is relatively stable under network perturbations 3 we study these sensitivity questions using extensive simulations on diverse real world networks and find that our theoretical predictions for both models match the observations quite closely 4 experimentally the transient behavior ie the time series of the number of infections in both models appears to be more sensitive to network perturbations



z  zhuang and m  pagnucco 2014 entrenchmentbased horn contraction volume 51 pages 227254

the agm framework is the benchmark approach in belief change since the framework assumes an underlying logic containing classical propositional logic it can not be applied to systems with a logic weaker than propositional logic  to remedy this limitation several researchers have studied agmstyle contraction and revision under the horn fragment of propositional logic ie horn logic  in this paper we contribute to this line of research by investigating the horn version of the agm entrenchmentbased contraction  the study is challenging as the construction of entrenchmentbased contraction refers to arbitrary disjunctions which are not expressible under horn logic  in order to adapt the construction to horn logic we make use of a horn approximation technique called horn strengthening we provide a representation theorem for the newly constructed contraction which we refer to as entrenchmentbased horn contraction  ideally contractions defined under horn logic ie horn contractions should be as rational as agm contraction  we propose the notion of horn equivalence which intuitively captures the equivalence between horn contraction and agm contraction  we show that under this notion entrenchmentbased horn contraction is equivalent to a restricted form of entrenchmentbased contraction



c  b228ckstr246m a  jonsson and p  jonsson 2014 automaton plans volume 51 pages 255291

macros have long been used in planning to represent subsequences of operators macros can be used in place of individual operators during search sometimes reducing the effort required to find a plan to the goal another use of macros is to compactly represent long plans in this paper we introduce a novel solution concept called automaton plans in which plans are represented using hierarchies of automata automaton plans can be viewed as an extension of macros that enables parameterization and branching we provide several examples that illustrate how automaton plans can be useful both as a compact representation of exponentially long plans and as an alternative to sequential solutions in benchmark domains such as logistics and grid we also compare automaton plans to other compact plan representations from the literature and find that automaton plans are strictly more expressive than macros but strictly less expressive than htns and certain representations allowing efficient sequential access to the operators of the plan



r  nissim and r  brafman 2014 distributed heuristic forward search for multiagent planning volume 51 pages 293332

this paper deals with the problem of classical planning for multiple cooperative agents who have private information about their local state and capabilities they do not want to reveal two main approaches have recently been proposed to solve this type of problem  one is based on reduction to distributed constraint satisfaction and the other on partialorder planning techniques in classical singleagent planning constraintbased and partialorder planning techniques are currently dominated by heuristic forward search the question arises whether it is possible to formulate a distributed heuristic forward search algorithm for privacypreserving classical multiagent planning our work provides a positive answer to this question in the form of a general approach to distributed statespace search in which each agent performs only the part of the state expansion relevant to it the resulting algorithms are simple and efficient  outperforming previous algorithms by orders of magnitude  while offering similar flexibility to that of forwardsearch algorithms for singleagent planning furthermore one particular variant of our general approach yields a distributed version of the a algorithm that is the first costoptimal distributed algorithm for privacypreserving planning



f  belardinelli a  lomuscio and f  patrizi 2014 verification of agentbased artifact systems volume 51 pages 333376

artifact systems are a novel paradigm for specifying and implementing business processes described in terms of interacting modules called artifacts artifacts consist of data and lifecycles accounting respectively for the relational structure of the artifacts states and their possible evolutions over time in this paper we put forward artifactcentric multiagent systems a novel formalisation of artifact systems in the context of multiagent systems operating on them differently from the usual processbased models of services we give a semantics that explicitly accounts for the data structures on which artifact systems are defined




a  metodi r  stern m  kalech and m  codish 2014 a novel satbased approach to model based diagnosis volume 51 pages 377411

this paper introduces a novel encoding of model based diagnosis mbd to boolean satisfaction sat focusing on minimal cardinality diagnosis the encoding is based on a combination of sophisticated mbd preprocessing algorithms and the application of a sat compiler which optimizes the encoding to provide more succinct cnf representations than obtained with previous works experimental evidence indicates that our approach is superior to all published algorithms for minimal cardinality mbd in particular we can determine for the first time minimal cardinality diagnoses for the entire standard iscas85 and 74xxx benchmarks our results open the way to improve the stateoftheart on a range of similar mbd problems



s  cai c  luo and k  su 2014 scoring functions based on second level score for ksat with long clauses volume 51 pages 413441

it is widely acknowledged that stochastic local search sls algorithms can efficiently find models for satisfiable instances of the satisfiability sat problem especially for random ksat instances however compared to random 3sat instances where sls algorithms have shown great success random ksat instances with long clauses remain very difficult recently the notion of second level score denoted as score2 was proposed for improving sls algorithms on longclause sat instances and was first used in the powerful ccasat solver as a tie breaker




b  de wilde a  w ter mors and c  witteveen 2014 push and rotate a complete multiagent pathfinding algorithm volume 51 pages 443492

multiagent pathfinding is a relevant problem in a wide range of domains for example in robotics and video games research formally the problem considers a graph consisting of vertices and edges and a set of agents occupying vertices an agent can only move to an unoccupied neighbouring vertex and the problem of finding the minimal sequence of moves to transfer each agent from its start location to its destination is an nphard problem
in our experiments we compare our approach with the push and swap mapp and bibox algorithms the latter algorithm is restricted to a smaller class of instances as it requires biconnected graphs but can nevertheless be considered state of the art due to its strong performance our experiments show that push and swap suffers from incompleteness mapp is generally not competitive with push and rotate and bibox is better than push and rotate on randomly generated biconnected instances while push and rotate performs better on grids



a  g cohn s  li w  liu and j  renz 2014 reasoning about topological and cardinal direction relations between 2dimensional spatial objects volume 51 pages 493532

increasing the expressiveness of qualitative spatial calculi is an essential step towards meeting the requirements of applications this can be achieved by combining existing calculi in a way that we can express spatial information using relations from multiple calculi the great challenge is to develop reasoning algorithms that are correct and complete when reasoning over the combined information previous work has mainly studied cases where the interaction between the combined calculi was small or where one of the two calculi was very simple in this paper we tackle the important combination of topological and directional information for extended spatial objects we combine some of the best known calculi in qualitative spatial reasoning the rcc8 algebra for representing topological information and the rectangle algebra ra and the cardinal direction calculus cdc for directional information we consider two different interpretations of the rcc8 algebra one uses a weak connectedness relation the other uses a strong connectedness relation in both interpretations we show that reasoning with topological and directional information is decidable and remains in np our computational complexity results unveil the significant differences between ra and cdc and that between weak and strong rcc8 models take the combination of basic rcc8 and basic cdc constraints as an example we show that the consistency problem is in p only when we use the strong rcc8 algebra and explicitly know the corresponding basic ra constraints




a  lopezortiz s  angelopoulos and a  m hamel 2014 optimal scheduling of contract algorithms for anytime problemsolving volume 51 pages 533554

a contract algorithm is an algorithm which is given as part of the input a specified amount of allowable computation time the algorithm must then complete its execution within the allotted time an interruptible algorithm in contrast can be interrupted at an arbitrary point in time at which point it must report its currently best solution it is known that contract algorithms can simulate interruptible algorithms using  iterative deepening techniques this simulation is done at a penalty  in the performance of the solution as measured by the socalled acceleration ratio
lastly we show how to evaluate the average acceleration ratio of the class of exponential strategies in the setting of n problem instances and m parallel processors this is a broad class of schedules that tend to be either optimal or nearoptimal for several variants of the basic problem



d  cohen j  crampton a  gagarin g  gutin and m  jones 2014 iterative plan construction for the workflow satisfiability problem volume 51 pages 555577

the workflow satisfiability problem wsp is a problem of practical interest that arises whenever tasks need to be performed by authorized users subject to constraints defined by business rules  we are required to decide whether there exists a plan  an assignment of tasks to authorized users  such that all constraints are satisfied  it is natural to see the wsp as a subclass of the constraint satisfaction problem csp in which the variables are tasks and the domain is the set of users  what makes the wsp distinctive is that the number of tasks is usually very small compared to the number of users so it is appropriate to ask for which constraint languages the wsp is fixedparameter tractable fpt parameterized by the number of tasks
this novel approach to the wsp using techniques from csp has enabled us to  design a generic algorithm which is fpt for several families of workflow constraints considered in the literature furthermore we prove that the union of fpt languages remains fpt if they satisfy a simple compatibility condition  lastly we identify a new fpt constraint language userindependent constraints that includes many of the constraints of interest in business processing systems   we demonstrate that our generic algorithm has provably optimal running time o2klog k for this language where k is the number of tasks



i  kash a  d procaccia and n  shah 2014 no agent left behind dynamic fair division of multiple resources volume 51 pages 579603

recently fair division theory has emerged as a promising approach for allocation of multiple computational resources among agents while in reality agents are not all present in the system simultaneously previous work has studied static settings where all relevant information is known upfront our goal is to better understand the dynamic setting on the conceptual level we develop a dynamic model of fair division and propose desirable axiomatic properties for dynamic resource allocation mechanisms on the technical level we construct two novel mechanisms that provably satisfy some of these properties and analyze their performance using real data we believe that our work informs the design of superior multiagent systems and at the same time expands the scope of fair division theory by initiating the study of dynamic and fair resource allocation mechanisms



p  nguyen m  hilario and a  kalousis 2014 using metamining to support data mining workflow planning and optimization volume 51 pages 605644

knowledge discovery in databases is a complex process that involves many different data processing and learning operators todays knowledge discovery support systems can contain several hundred operators  a major challenge is to assist the user in designing workflows which are not only valid but also  ideally  optimize some performance measure associated with the user goal  in this paper we present such a system the system relies on a metamining module which analyses past data mining experiments and extracts metamining models which associate dataset characteristics with workflow descriptors in view of workflow performance optimization the metamining model is used within a data mining workflow planner to guide the planner during the workflow planning we learn the metamining models  using a similarity learning approach and extract the workflow descriptors by mining the workflows for generalized relational patterns accounting also for domain knowledge provided by a data mining ontology  we evaluate the quality of the data mining workflows that the system produces on a collection of real world datasets coming from biology and show that it produces workflows that are significantly better than alternative methods that can only do workflow selection and not planning



g  stefanoni b  motik m  kroetzsch and s  rudolph 2014 the complexity of answering conjunctive and navigational queries over owl 2 el knowledge bases volume 51 pages 645705

owl 2 el is a popular ontology language that supports role inclusionsthat is axioms that capture compositional properties of roles role inclusions closely correspond to contextfree grammars which was used to show that answering conjunctive queries cqs over owl 2 el knowledge bases with unrestricted role inclusions is undecidable however owl 2 el inherits from owl 2 dl the syntactic regularity restriction on role inclusions which ensures that role chains implying a particular role can be described using a finite automaton fa this is sufficient to ensure decidability of cq answering however the fas can be worstcase exponential in size so the known approaches do not provide a tight upper complexity bound
in this paper we solve this open problem and show that answering cqs over owl 2 el knowledge bases is pspacecomplete in combined complexity ie the complexity measured in the total size of the input to this end we use a novel encoding of regular role inclusions using boundedstack pushdown automatathat is fas extended with a stack of bounded size apart from theoretical interest our encoding can be used in practical tableau algorithms to avoid the exponential blowup due to role inclusions in addition we sharpen the lower complexity bound and show that the problem is pspacehard even if we consider only role inclusions as part of the input ie the query and all other parts of the knowledge base are fixed finally we turn our attention to navigational queries over owl 2 el knowledge bases and we show that answering positive conversefree conjunctive graph xpath queries is pspacecomplete as well this is interesting since allowing the converse operator in queries is known to make the problem exptimehard thus in this paper we present several important contributions to the landscape of the complexity of answering expressive queries over description logic knowledge bases



o  cepek s  gursky and p  kucera 2014 on minimum representations of matched formulas volume 51 pages 707723

a boolean formula in conjunctive normal form cnf is called matched if the system of sets of variables which appear in individual clauses has a system of distinct representatives each matched cnf is trivially satisfiable each clause can be satisfied by its representative variable another property which is easy to see is that the class of matched cnfs is not closed under partial assignment of truth values to variables this latter property leads to a fact proved here that given two matched cnfs it is conp complete to decide whether they are logically equivalent the construction in this proof leads to another result a much shorter and simpler proof of the fact that the boolean minimization problem for matched cnfs is a complete problem for the second level of the polynomial hierarchy the main result of this paper deals with the structure of clause minimum cnfs we prove here that if a boolean function f admits a representation by a matched cnf then every clause minimum cnf representation of f is matched 




c  r shelton and g  ciardo 2014 tutorial on structured continuoustime markov processes volume 51 pages 725778

a continuoustime markov process ctmp is a collection of variables indexed by a continuous quantity time it obeys the markov property that the distribution over a future variable is independent of past variables given the state at the present time we introduce continuoustime markov process representations and algorithms for filtering smoothing expected sufficient statistics calculations and model estimation assuming no prior knowledge of continuoustime processes but some basic knowledge of probability and statistics we begin by describing flat or unstructured markov processes and then move to structured markov processes those arising from state spaces consisting of assignments to variables including kronecker decisiondiagram and continuoustime bayesian network representations we provide the first connection between decisiondiagrams and continuoustime bayesian networks




p  kissmann and j  hoffmann 2014 bdd ordering heuristics for classical planning volume 51 pages 779804

symbolic search using binary decision diagrams bdds can often save large amounts of memory due to its concise representation of state sets a decisive factor for this methods success is the chosen variable ordering generally speaking it is plausible that dependent variables should be brought close together in order to reduce bdd sizes in planning variable dependencies are typically captured by means of causal graphs and in preceding work these were taken as the basis for finding bdd variable orderings starting from the observation that the two concepts of dependency are actually quite different we introduce a framework for assessing the strength of variable ordering heuristics in subclasses of planning it turns out that even for extremely simple planning tasks causal graph based variable orders may be exponentially worse than optimal




d  zilli o  parson g  v merrett and a  rogers 2014 a hidden markov modelbased acoustic cicada detector for crowdsourced smartphone biodiversity monitoring volume 51 pages 805827

in recent years the field of computational sustainability has striven to apply artificial intelligence techniques to solve ecological and environmental problems in ecology a key issue for the safeguarding of our planet is the monitoring of biodiversity automated acoustic recognition of species aims to provide a costeffective method for biodiversity monitoring this is particularly appealing for detecting endangered animals with a distinctive call such as the new forest cicada to this end we pursue a crowdsourcing approach whereby the millions of visitors to the new forest where this insect was historically found will help to monitor its presence by means of a smartphone app that can detect its mating call existing research in the field of acoustic insect detection has typically focused upon the classification of recordings collected from fixed field microphones such approaches segment a lengthy audio recording into individual segments of insect activity which are independently classified using cepstral coefficients extracted from the recording as features this paper reports on a contrasting approach whereby we use crowdsourcing to collect recordings via a smartphone app and present an immediate feedback to the users as to whether an insect has been found our classification approach does not remove silent parts of the recording via segmentation but instead uses the temporal patterns throughout each recording to classify the insects present we show that our approach can successfully discriminate between the call of the new forest cicada and similar insects found in the new forest and is robust to common types of environment noise a large scale trial deployment of our smartphone app collected over 6000 reports of insect activity from over 1000 users despite the cicada not having been rediscovered in the new forest the effectiveness of this approach was confirmed for both the detection algorithm which successfully identified the same cicada through the app in countries where the same species is still present and of the crowdsourcing methodology which collected a vast number of recordings and involved thousands of contributors



b  bosansky c  kiekintveld v  lisy and m  pechoucek 2014 an exact doubleoracle algorithm for zerosum extensiveform games with imperfect information volume 51 pages 829866

developing scalable solution algorithms is one of the central problems in computational game theory we present an iterative algorithm for computing an exact nash equilibrium for twoplayer zerosum extensiveform games with imperfect information our approach combines two key elements 1 the compact sequenceform representation of extensiveform games and 2 the algorithmic framework of doubleoracle methods the main idea of our algorithm is to restrict the game by allowing the players to play only selected sequences of available actions after solving the restricted game new sequences are added by finding best responses to the current solution using fast algorithms 
our main contributions include 1 a generic sequenceform doubleoracle algorithm for solving zerosum extensiveform games 2 fast methods for maintaining a valid restricted game model when adding new sequences 3 a search algorithm and pruning methods for computing bestresponse sequences 4 theoretical guarantees about the convergence of the algorithm to a nash equilibrium 5 experimental analysis of our algorithm on several games including an approximate version of the algorithm
