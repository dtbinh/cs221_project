roy  barhaim ido  dagan and jonathan  berant 2015 knowledgebased textual inference via parsetree transformations volume 54 pages 157

textual inference is an important component in many applications for understanding natural language classical approaches to textual inference rely on logical representations for meaning which may be regarded as external to the natural language itself however practical applications usually adopt shallower lexical or lexicalsyntactic representations which correspond closely to language structure in many cases such approaches lack a principled meaning representation and inference framework we describe an inference formalism that operates directly on languagebased structures particularly syntactic parse trees new trees are generated by applying inference rules which provide a unified representation for varying types of inferences we use manual and automatic methods to generate these rules which cover generic linguistic structures as well as specific lexicalbased inferences  we also present a novel packed datastructure and a corresponding inference algorithm that allows efficient implementation of this formalism we proved the correctness of the new algorithm and established its efficiency analytically and empirically the utility of our approach was illustrated on two tasks unsupervised relation extraction from a large corpus and the recognizing textual entailment rte benchmarks 



sigve  hortemo s230ther jan arne  telle and martin   vatshelle 2015 solving sat and maxsat by dynamic programming  volume 54 pages 5982

we look at dynamic programming algorithms for propositional model counting also called sat and maxsat tools from graph structure theory in particular treewidth have been used to successfully identify tractable cases in many subfields of ai including sat constraint satisfaction problems csp bayesian reasoning and planning in this paper we attack sat and maxsat using similar but more modern graph structure tools the tractable cases will include formulas whose class of incidence graphs have not only unbounded treewidth but also unbounded cliquewidth we show that our algorithms extend all previous results for maxsat and sat achieved by dynamic programming along structural decompositions of the incidence graph of the input formula we present some limited experimental results comparing implementations of our algorithms to stateoftheart sat and maxsat solvers as a proof of concept that warrants further research




ruben  izquierdo armando  suarez and german  rigau 2015 word vs classbased word sense disambiguation volume 54 pages 83122

as empirically demonstrated by the word sense disambiguation wsd tasks of the last sensevalsemeval exercises assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed many authors argue that one possible reason could be the use of inappropriate sets of word meanings in particular wordnet has been used as a defacto standard repository of word meanings in most of these tasks thus instead of using the word senses defined in wordnet some approaches have derived semantic classes representing groups of word senses however the meanings represented by wordnet have been only used for wsd at a very finegrained sense level or at a very coarsegrained semantic class level also called supersenses we suspect that an appropriate level of abstraction could be on between both levels the contributions of this paper are manifold first we propose a simple method to automatically derive semantic classes at intermediate levels of abstraction covering all nominal and verbal wordnet meanings second we empirically demonstrate that our automatically derived semantic classes outperform classical approaches based on word senses and more coarsegrained sense groupings third we also demonstrate that our supervised wsd system benefits from using these new semantic classes as additional semantic features while reducing the amount of training examples finally we also demonstrate the robustness of our supervised semantic classbased wsd system when tested on out of domain corpus



scott  kiesel ethan  burns and wheeler  ruml 2015 achieving goals quickly using realtime search experimental results in video games volume 54 pages 123158

in realtime domains such as video games planning happens concurrently with execution and the planning algorithm has a strictly bounded amount of time before it must return the next action for the agent to execute we explore the use of realtime heuristic search in two benchmark domains inspired by video games unlike classic benchmarks such as grid pathfinding and the sliding tile puzzle these new domains feature exogenous change and directed state space graphs we consider the setting in which planning and acting are concurrent and we use the natural objective of minimizing goal achievement time using both the classic benchmarks and the new domains we investigate several enhancements to a leading realtime search algorithm lsslrta we show experimentally that 1 it is better to plan after each action or to use a dynamically sized lookahead 2 abased lookahead can cause undesirable actions to be selected and 3 online debiasing of the heuristic can lead to improved performance we hope this work encourages future research on applying realtime search in dynamic domains



llu237s  formiga alberto  barr243ncede241o llu237s  m224rquez carlos a  henr237quez and jos233 b  mari241o 2015 leveraging online user feedback to improve statistical machine translation volume 54 pages 159192

in this article we present a threestep methodology for dynamically improving a statistical machine translation smt system by incorporating human feedback in the form of free edits on the system translations we target at feedback provided by casual users which is typically errorprone thus we first propose a filtering step to automatically identify the better useredited translations and discard the useless ones a second step produces a pivotbased alignment between source and useredited sentences focusing on the errors made by the system finally a third step produces a new translation model and combines it linearly with the one from the original system we perform a thorough evaluation on a realworld dataset collected from the reversonet translation service and show that every step in our methodology contributes significantly to improve a general purpose smt system interestingly the quality improvement is not only due to the increase of lexical coverage but to a better lexical selection reordering and morphology finally we show the robustness of the methodology by applying it to a different scenario in which the new examples come from an automatically webcrawled parallel corpus using exactly the same architecture and models provides again a significant improvement of the translation quality of a general purpose baseline smt system



hannes  strass 2015 expressiveness of twovalued semantics for abstract dialectical frameworks volume 54 pages 193231

we analyse the expressiveness of brewka and woltrans abstract dialectical frameworks for twovalued semantics by expressiveness we mean the ability to encode a desired set of twovalued interpretations over a given propositional vocabulary a using only atoms from a we also compare adfs expressiveness with that of the twovalued semantics of abstract argumentation frameworks normal logic programs and propositional logic while the computational complexity of the twovalued model existence problem for all these languages is almost the same we show that the languages form a neat hierarchy with respect to their expressiveness we then demonstrate that this hierarchy collapses once we allow to introduce a linear number of new vocabulary elements we finally also analyse and compare the representational succinctness of adfs for twovalued model semantics that is their capability to represent twovalued interpretation sets in a spaceefficient manner



meir  kalech and shulamit  reches 2015 decision making with dynamic uncertain events volume 54 pages 233275

when to make a decision is a key question in decision making problems characterized by uncertainty in this paper we deal with decision making in environments where information arrives dynamically we address the tradeoff between waiting and stopping strategies on the one hand waiting to obtain more information reduces uncertainty but it comes with a cost stopping and making a decision based on an expected utility reduces the cost of waiting but the decision is based on uncertain information we propose an optimal algorithm and two approximation algorithms we prove that one approximation is optimistic  waits at least as long as the optimal algorithm while the other is pessimistic  stops not later than the optimal algorithm we evaluate our algorithms theoretically and empirically and show that the quality of the decision in both approximations is nearoptimal and much faster than the optimal algorithm also we can conclude from the experiments that the cost function is a key factor to chose the most effective algorithm



till  mossakowski and reinhard  moratz 2015 relations between spatial calculi about directions and orientations volume 54 pages 277308

qualitative spatial descriptions characterize essential properties of spatial objects or configurations by relying on relative comparisons rather than measuring typically in qualitative approaches only relatively coarse distinctions between configurations are made qualitative spatial knowledge can be used to represent incomplete and underdetermined knowledge in a systematic way this is especially useful if the task is to describe features of classes of configurations rather than individual configurations




yujiao   zhou bernardo  cuenca grau yavor  nenov mark  kaminski and ian  horrocks 2015 pagoda payasyougo ontology query answering using a datalog reasoner volume 54 pages 309367

answering conjunctive queries over ontologyenriched datasets is a core reasoning task for many applications query answering is however computationally very expensive which has led to the development of query answering procedures that sacrifice either expressive power of the ontology language or the completeness of query answers in order to improve scalability in this paper we describe a hybrid approach to query answering over owl 2 ontologies that combines a datalog reasoner with a fullyfledged owl 2 reasoner in order to provide scalable payasyougo performance the key feature of our approach is that it delegates the bulk of the computation to the datalog reasoner and resorts to expensive owl 2 reasoning only as necessary to fully answer the query furthermore although our main goal  is to efficiently answer queries over owl 2 ontologies and data our technical results are very general and our approach is applicable to firstorder knowledge representation languages that can be captured by rules allowing for existential quantification and disjunction in the head our only assumption is the availability of a datalog reasoner and a fullyfledged reasoner for the language of interest  both of which are used as black boxes we have implemented our techniques in the pagoda system which combines the datalog reasoner rdfox and the  owl 2 reasoner hermit our extensive evaluation shows that pagoda succeeds in providing scalable payasyougo query answering for a wide range of owl 2 ontologies datasets and queries



fazlul  hasan siddiqui and patrik  haslum 2015 continuing plan quality optimisation volume 54 pages 369435

finding high quality plans for large planning problems is hard although some current anytime planners are often able to improve plans quickly they tend to reach a limit at which the plans produced are still very far from the best possible but these planners fail to find any further improvement even when given several hours of runtime
even starting from the best plans found by other means bdpo2 is able to continue improving plan quality often producing better plans than other anytime planners when all are given enough runtime the best results however are achieved by a combination of different techniques working together



igor  rochlin and david  sarne 2015 constraining information sharing to improve cooperative information gathering volume 54 pages 437469

this paper considers the problem of cooperation between selfinterested agents in acquiring better information regarding the nature of the different options and opportunities available to them by sharing individual findings with others the agents can potentially achieve a substantial improvement in overall and individual expected benefits  unfortunately it is well known that with selfinterested agents equilibrium considerations often dictate solutions that are far from the fully cooperative ones hence the agents do not manage to fully exploit the potential benefits encapsulated in such cooperation  in this paper we introduce analyze and demonstrate the benefit of five methods aiming to improve cooperative information gathering  common to all five that they constrain and limit the information sharing process  nevertheless the decrease in benefit due to the limited sharing is outweighed by the resulting substantial improvement in the equilibrium individual information gathering strategies the equilibrium analysis given in the paper which in itself is an important contribution to the study of cooperation between selfinterested agents enables demonstrating that for a wide range of settings an improved individual expected benefit is achieved for all agents when applying each of the five methods



joseph  y halpern 2015 weighted regretbased likelihood a new approach to describing uncertainty volume 54 pages 471492

recently halpern and leung suggested representing uncertainty by a set of weighted  probability measures and suggested a way of making decisions based on this representation of uncertainty maximizing weighted regret  their paper does not answer an apparently simpler question what it means according to this representation of uncertainty for an event e to be more likely than an event e    in this paper a notion of comparative likelihood when uncertainty is represented by a set of weighted probability measures is defined  it generalizes the ordering defined by probability and by lower probability in a natural way a generalization of upper probability can also be defined  a complete axiomatic characterization of this notion of regretbased likelihood is given 




haris  aziz markus   brill felix   fischer paul   harrenstein jerome  lang and hans georg  seedig 2015 possible and necessary winners of partial tournaments volume 54 pages 493534

we study the problem of computing possible and necessary winners for partially specified weighted and unweighted tournaments this problem arises naturally in elections with incompletely specified votes partially completed sports competitions and more generally in any scenario where the outcome of some pairwise comparisons is not yet fully known we specifically consider a number of wellknown solution conceptsincluding the uncovered set borda ranked pairs and maximinand show that for most of them possible and necessary winners can be identified in polynomial time these positive algorithmic results stand in sharp contrast to earlier results concerning possible and necessary winners given partially specified preference profiles



andreas  steigmiller and birte  glimm 2015 payasyougo description logic reasoning by coupling tableau and saturation procedures volume 54 pages 535592

nowadays saturationbased reasoners for the owl el profile of the web ontology language are able to handle large ontologies such as snomed very efficiently however it is currently unclear how saturationbased reasoning procedures can be extended to very expressive description logics such as sroiqthe logical underpinning of the current and second iteration of the web ontology language tableaubased procedures on the other hand are not limited to specific description logic languages or owl profiles but even highly optimised tableaubased reasoners might not be efficient enough to handle large ontologies such as snomed in this paper we present an approach for tightly coupling tableau and saturationbased procedures that we implement in the owl dl reasoner konclude our detailed evaluation shows that this combination significantly improves the reasoning performance for a wide range of ontologies



ben  strasser adi  botea and daniel  harabor 2015 compressing optimal paths with run length encoding volume 54 pages 593629

we introduce a novel approach to compressed path databases space efficient oracles used to very quickly identify the first edge on a shortest path our algorithm achieves query running times on the 100 nanosecond scale being significantly faster than stateoftheart firstmove oracles from the literature space consumption is competitive due to a compression approach that rearranges rows and columns in a firstmove matrix and then performs run length encoding rle on the contents of the matrix one variant of our implemented system was by a convincing margin the fastest entry in the 2014 gridbased path planning competition
we give a first tractability analysis for the compression scheme used by our algorithm we study the complexity of computing a database of minimum size for general directed and undirected graphs we find that in both cases the problem is npcomplete we also show that for graphs which can be decomposed along articulation points the problem can be decomposed into independent parts with a corresponding reduction in its level of difficulty in particular this leads to simple and tractable algorithms with linear running time which yield optimal compression results for trees



tatsuya  imai and alex  fukunaga 2015 on a practical  integerlinear programming model for deletefree tasks and its use as a heuristic for costoptimal planning volume 54 pages 631677

we propose a new integerlinear programming model for the delete relaxation in costoptimal planning while a straightforward ip for the delete relaxation is impractical our enhanced model incorporates variable reduction techniques based on  landmarks relevancebased constraints dominated action elimination immediate action application and inverse action constraints resulting in an ip that can be used to directly solve deletefree planning problems we show that our ip model is competitive with previous stateoftheart solvers for deletefree problems the lprelaxation of the ip model is often a very good approximation to the ip providing an approach to approximating the optimal value of the deletefree task that is complementary to the wellknown lmcut heuristic we also show that constraints that partially consider delete effects can be added to our iplp models we embed the new iplp models into a forwardsearch based planner and show that the performance of the resulting planner on standard ipc benchmarks is comparable with the stateoftheart for costoptimal planning
