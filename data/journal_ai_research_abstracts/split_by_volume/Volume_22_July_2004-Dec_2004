d  cohen  m  cooper  p  jeavons and  a  krokhin 2004 a maximal tractable class of soft constraints volume 22 pages 122

many researchers in artificial intelligence are beginning to explore the use of soft constraints to express a set of possibly conflicting problem  requirements a soft constraint is a function defined on a collection of  variables which associates some measure of desirability with each possible  combination of values for those variables however the crucial question of  the computational complexity of finding the optimal solution to a collection of soft constraints has so far received very little attention in this paper we identify a class of soft binary constraints for which the problem of  finding the optimal solution is tractable in other words we show that for  any given set of such constraints there exists a polynomial time algorithm  to determine the assignment having the best overall combined measure of  desirability this tractable class includes many commonlyoccurring soft constraints such as as near as possible or as soon as possible after as well as crisp constraints such as greater than finally we show that  this tractable class is maximal in the sense that adding any other form of  soft binary constraint which is not in the class gives rise to a class of  problems which is nphard



d  dubois  h  fargier and  h  prade 2004 ordinal and probabilistic representations of acceptance volume 22 pages 2356

an accepted belief is a proposition considered likely enough by an agent to be inferred from as if it were true this paper bridges the gap between probabilistic and logical representations of accepted beliefs to this end natural properties of relations on propositions describing relative strength of belief are augmented with some conditions ensuring that accepted beliefs form a deductively closed set this requirement turns out to be very restrictive in particular it is shown that the sets of accepted belief of an agent can always be derived from a family of possibility rankings of states an agent accepts a proposition in a given context if this proposition is considered more possible than its negation in this context for all possibility rankings in the family these results are closely connected to the nonmonotonic preferential inference system of kraus lehmann and magidor and the socalled plausibility functions of friedman and halpern the extent to which probability theory is compatible with acceptance relations is laid bare a solution to the lottery paradox which is considered as a major impediment to the use of nonmonotonic inference is proposed using a special kind of probabilities called lexicographic or bigstepped the setting of acceptance relations also proposes another way of approaching the theory of belief change after the works of grdenfors and colleagues our view considers the acceptance relation as a primitive object from which belief sets are derived in various contexts



c  j meek and  w  p birmingham 2004 a comprehensive trainable error model for sung music queries volume 22 pages 5791

we propose a model for errors in sung queries a variant of the hidden markov model hmm this is a solution to the problem of identifying the degree of similarity between a typically errorladen sung query and a potential target in a database of musical works an important problem in the field of music information retrieval similarity metrics are a critical component of querybyhumming qbh applications which search audio and multimedia databases for strong matches to oral queries our model comprehensively expresses the types of m error or variation between target and query cumulative and noncumulative local errors transposition tempo and tempo changes insertions deletions and modulation the model is not only expressive but automatically trainable or able to learn and generalize from query examples we present results of simulations designed to assess the discriminatory potential of the model and tests with real sung queries to demonstrate relevance to realworld applications



h  chockler and  j  y halpern 2004 responsibility and blame a structuralmodel approach volume 22 pages 93115

causality is typically treated an allornothing concept either a is a cause of b or it is not we extend the definition of causality introduced by halpern and pearl 2004a to take into account the degree of responsibility of a for b  for example if someone wins an election 110 then each person who votes for him is less responsible for the victory than if he had won 65  we then define a notion of degree of blame which takes into account an agents epistemic state roughly speaking the degree of blame of a for b is the expected degree of responsibility of a for b taken over the epistemic state of an agent



p  derbeko r  elyaniv and r  meir 2004 explicit learning curves for transduction and application to clustering and compression algorithms volume 22 pages 117142

inductive learning is based on inferring a general rule from a finite data set and using it to label new data in transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points which are given to the learner prior to learning although transduction seems at the outset to be an easier task than induction there have not been many provably useful algorithms for transduction moreover the precise relation between induction and transduction has not yet been determined the main theoretical developments related to transduction were presented by vapnik more than twenty years ago one of vapniks basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail while tight this bound is given implicitly via a computational routine our first contribution is a somewhat looser but explicit characterization of a slightly extended pacbayesian version of vapniks transductive bound this characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement we then derive error bounds for compression schemes such as transductive support vector machines and for transduction algorithms based on clustering the main observation used for deriving these new error bounds and algorithms is that the unlabeled test points which in the transductive setting are known in advance can be used in order to construct useful data dependent prior distributions over the hypothesis space



c  v goldman and  s  zilberstein 2004 decentralized control of cooperative systems categorization and complexity analysis volume 22 pages 143174

decentralized control of cooperative systems captures the operation of a group of decision makers that share a single global objective  the difficulty in solving optimally such problems arises when the agents lack full observability of the global state of the system when they operate the general problem has been shown to be nexpcomplete in this paper we identify classes of decentralized control problems whose complexity ranges between nexp and p in particular we study problems characterized by independent transitions independent observations and goaloriented objective functions  two algorithms are shown to solve optimally useful classes of goaloriented decentralized processes in polynomial time  this paper also studies information sharing among the decisionmakers which can improve their performance we distinguish between three ways in which agents can exchange information indirect communication direct communication and sharing state features that are not controlled by the agents  our analysis shows that for every class of problems we consider introducing direct or indirect communication does not change the worstcase complexity  the results provide a better understanding of the complexity of decentralized control problems that arise in practice and facilitate the development of planning algorithms for these problems



s  park  e  h durfee and  w  p birmingham 2004 use of markov chains to design an agent bidding strategy for continuous double auctions volume 22 pages 175214

as computational agents are developed for increasingly complicated ecommerce applications the complexity of the decisions they face demands advances in artificial intelligence techniques for example an agent representing a seller in an auction should try to maximize the sellers profit by reasoning about a variety of possibly uncertain pieces of information such as the maximum prices various buyers might be willing to pay the possible prices being offered by competing sellers the rules by which the auction operates the dynamic arrival and matching of offers to buy and sell and so on a naive application of multiagent reasoning techniques would require the sellers agent to explicitly model all of the other agents through an extended time horizon rendering the problem intractable for many realisticallysized problems we have instead devised a new strategy that an agent can use to determine its bid price based on a more tractable markov chain model of the auction process  we have experimentally identified the conditions under which our new strategy works well as well as how well it works in comparison to the optimal performance the agent could have achieved had it known the future our results show that our new strategy in general performs well outperforming other tractable heuristic strategies in a majority of experiments and is particularly effective in a sellers market where many buy offers are available



j  hoffmann  j  porteous and  l  sebastia 2004 ordered landmarks in planning volume 22 pages 215278

many known planning tasks have inherent constraints concerning the best order in which to achieve the goals a number of research efforts have been made to detect such constraints and to use them for guiding search in the hope of speeding up the planning process     we go beyond the previous approaches by considering ordering constraints not only over the toplevel goals but also over the subgoals that will necessarily arise during planning landmarks are facts that must be true at some point in every valid solution plan  we extend koehler and hoffmanns definition of reasonable orders between top level goals to the more general case of landmarks we show how landmarks can be found how their reasonable orders can be approximated and how this information can be used to decompose a given planning task into several smaller subtasks our methodology is completely domain and plannerindependent the implementation demonstrates that the approach can yield significant runtime performance improvements when used as a control loop around stateoftheart suboptimal planning systems as exemplified by ff and lpg



a  felner  r  e korf and  s  hanan 2004 additive pattern database heuristics volume 22 pages 279318




honorable mention for the 2007 ijcaijair best paper prize

we explore a method for computing admissible heuristic evaluation functions for search problems it utilizes pattern databases which are precomputed tables of the exact cost of solving various subproblems of an existing problem unlike standard pattern database heuristics however we partition our problems into disjoint subproblems so that the costs of solving the different subproblems can be added together without overestimating the cost of solving the original problem previously we showed how to statically partition the slidingtile puzzles into disjoint groups of tiles to compute an admissible heuristic using the same partition for each state and problem instance here we extend the method and show that it applies to other domains as well we also present another method for additive heuristics which we call dynamically partitioned pattern databases here we partition the problem into disjoint subproblems for each state of the search dynamically we discuss the pros and cons of each of these methods and apply both methods to three different problem domains the slidingtile puzzles the 4peg towers of hanoi problem and finding an optimal vertex cover of a graph we find that in some problem domains static partitioning is most effective while in others dynamic partitioning is a better choice in each of these problem domains either statically partitioned or dynamically partitioned pattern database heuristics are the best known heuristics for the problem



p  beame  h  kautz and  a  sabharwal 2004 towards understanding and harnessing the potential of clause learning volume 22 pages 319351




honorable mention for the 2008 ijcaijair best paper prize

efficient implementations of dpll with the addition of clause learning are the fastest complete boolean satisfiability solvers and can handle many significant realworld problems such as verification planning and design despite its importance little is known of the ultimate strengths and limitations of the technique this paper presents the first precise characterization of clause learning as a proof system cl and begins the task of understanding its power by relating it to the wellstudied resolution proof system in particular we show that with a new learning scheme cl can provide exponentially shorter proofs than many proper refinements of general resolution res satisfying a natural property these include regular and davisputnam resolution which are already known to be much stronger than ordinary dpll we also show that a slight variant of cl with unlimited restarts is as powerful as res itself translating these analytical results to practice however presents a challenge because of the nondeterministic nature of clause learning algorithms we propose a novel way of exploiting the underlying problem structure in the form of a high level problem description such as a graph or pddl specification to guide clause learning algorithms toward faster solutions we show that this leads to exponential speedups on grid and randomized pebbling problems as well as substantial improvements on certain ordering formulas



m  bowling and  m  veloso 2004 existence of multiagent equilibria with limited agents volume 22 pages 353384

multiagent learning is a necessary yet challenging problem as multiagent systems become more prevalent and environments become more dynamic  much of the groundbreaking work in this area draws on notable results from game theory in particular the concept of nash equilibria  learners that directly learn an equilibrium obviously rely on their existence  learners that instead seek to play optimally with respect to the other players also depend upon equilibria since equilibria are fixed points for learning  from another perspective agents with limitations are real and common  these may be undesired physical limitations as well as selfimposed rational limitations such as abstraction and approximation techniques used to make learning tractable  this article explores the interactions of these two important concepts equilibria and limitations in learning  we introduce the question of whether equilibria continue to exist when agents have limitations  we look at the general effects limitations can have on agent behavior and define a natural extension of equilibria that accounts for these limitations  using this formalization we make three major contributions i a counterexample for the general existence of equilibria with limitations ii sufficient conditions on limitations that preserve their existence iii three general classes of games and limitations that satisfy these conditions  we then present empirical results from a specific multiagent learning algorithm applied to a specific instance of limited agents  these results demonstrate that learning with limitations is feasible when the conditions outlined by our theoretical analysis hold



r  begleiter  r  elyaniv and  g  yona 2004 on prediction using variable order markov models volume 22 pages 385421

this paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet using variable order markov models the class of such algorithms is large and in principle includes any lossless compression algorithm we focus on six prominent prediction algorithms including context tree weighting ctw prediction by partial match ppm and probabilistic suffix trees psts we discuss the properties of these algorithms and compare their performance using real life sequences from three domains proteins english text and music pieces the comparison is made with respect to prediction quality as measured by the average logloss we also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks our results indicate that a decomposed ctw a variant of the ctw algorithm and ppm outperform all other algorithms in sequence prediction tasks somewhat surprisingly a different algorithm which is a modification of the lempelziv compression algorithm significantly outperforms all algorithms on the protein classification problems



r  becker  s  zilberstein  v  lesser and  c  v goldman 2004 solving transition independent decentralized markov decision processes volume 22 pages 423455

formal treatment of collaborative multiagent systems has been lagging behind the rapid progress in sequential decision making by individual agents  recent work in the area of decentralized markov decision processes mdps has contributed to closing this gap but the computational complexity of these models remains a serious obstacle to overcome this complexity barrier we identify a specific class of decentralized mdps in which the agents transitions are independent the class consists of independent collaborating agents that are tied together through a structured global reward function that depends on all of their histories of states and actions  we present a novel algorithm for solving this class of problems and examine its properties both as an optimal algorithm and as an anytime algorithm to our best knowledge this is the first algorithm to optimally solve a nontrivial subclass of decentralized mdps  it lays the foundation for further work in this area on both exact and approximate algorithms



g  erkan and  d  r radev 2004 lexrank graphbased lexical centrality as salience in text summarization volume 22 pages 457479

we introduce a stochastic graphbased method for computing relative importance of textual units for natural language processing we test the technique on the problem of text summarization ts extractive ts relies on the concept of sentence salience to identify the most important sentences in a document or set of documents salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudosentence we consider a new approach lexrank for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences in this model a connectivity matrix based on intrasentence cosine similarity is used as the adjacency matrix of the graph representation of sentences our system based on lexrank ranked in first place in more than one task in the recent duc 2004 evaluation in this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier duc evaluations we discuss several methods to compute centrality using the similarity graph the results show that degreebased methods including lexrank outperform both centroidbased methods and other systems participating in duc in most of the cases furthermore the lexrank with threshold method outperforms the other degreebased techniques including continuous lexrank we also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents



h  e dixon  m  l ginsberg  e  m luks and  a  j parkes 2004 generalizing boolean satisfiability ii theory volume 22 pages 481534

this is the second of three planned papers describing zap a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers  the fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the boolean representation used our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance  this paper presents the theoretical basis for the ideas underlying zap arguing that existing ideas in this area exploit a single recurring structure in that multiple database axioms can be obtained by operating on a single axiom using a subgroup of the group of permutations on the literals in the problem  we argue that the group structure precisely captures the general structure at which earlier approaches hinted and give numerous examples of its use  we go on to extend the davisputnamlogemannloveland inference procedure to this broader setting and show that earlier computational improvements are either subsumed or left intact by the new method  the third paper in this series discusses zaps implementation and presents experimental performance results
