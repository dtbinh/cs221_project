n  l zhang and  t  kocka 2004 effective dimensions of hierarchical latent class models volume 21 pages 117

hierarchical latent class hlc models are treestructured bayesian networks where leaf nodes are observed while internal nodes are latent  there are no theoretically well justified model selection criteria for hlc models in particular and bayesian networks with latent nodes in general nonetheless empirical studies suggest that the bic score is a reasonable criterion to use in practice for learning hlc models  empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced  with effective model dimension in the penalty term of the bic score  effective dimensions are difficult to compute in this paper we prove a theorem that relates the effective dimension of an hlc model to the effective dimensions of a number of latent class models  the theorem makes it computationally feasible to compute the effective dimensions of large hlc models  the theorem can also be used to compute the effective dimensions of general tree models



m  p wellman  d  m reeves  k  m lochner and  y  vorobeychik 2004 price prediction in a trading agent competition volume 21 pages 1936

the 2002 trading agent competition tac presented a challenging market game in the domain of travel shopping  one of the pivotal issues in this domain is uncertainty about hotel prices which have a significant influence on the relative cost of alternative trip schedules  thus virtually all participants employ some method for predicting hotel prices  we survey approaches employed in the tournament finding that agents apply an interesting diversity of techniques taking into account differing sources of evidence bearing on prices  based on data provided by entrants on their agents actual predictions in the tac02 finals and semifinals we analyze the relative efficacy of these approaches  the results show that taking into account gamespecific information about flight prices is a major distinguishing factor  machine learning methods effectively induce the relationship between flight and hotel prices from game data and a purely analytical approach based on competitive equilibrium analysis achieves equal accuracy with no historical data  employing a new measure of prediction quality we relate absolute accuracy to bottomline performance in the game



d  monderer and  m  tennenholtz 2004 kimplementation volume 21 pages 3762

this paper discusses an interested party who wishes to influence the behavior of agents in a game multiagent interaction which is not under his control the interested party cannot design a new game cannot enforce agents behavior cannot enforce payments by the agents and cannot prohibit strategies available to the agents however he can influence the outcome of the game by committing to nonnegative monetary transfers for the different strategy profiles that may be selected by the agents  the interested party assumes that agents are rational in the commonly agreed sense that they do not use dominated strategies hence a certain subset of outcomes is implemented in a given game if by adding nonnegative payments rational players will necessarily produce an outcome in this subset obviously by making sufficiently big payments one can implement any desirable outcome the question is what is the cost of implementation in this paper we introduce the notion of kimplementation of a desired set of strategy profiles where k stands for the amount of payment that need to be actually made in order to implement desirable outcomes a major point in kimplementation is that monetary offers need not necessarily materialize when following desired behaviors  we define and study kimplementation in the contexts of games with complete and incomplete information in the latter case we mainly focus on the vcg games our setting is later extended to deal with mixed strategies using correlation devices together the paper introduces and studies the implementation of desirable outcomes by a reliable party who cannot modify game rules ie provide protocols complementing previous work in mechanism design while making it more applicable to many realistic cs settings



k  o stanley and  r  miikkulainen 2004 competitive coevolution through evolutionary complexification volume 21 pages 63100

two major goals in machine learning are the discovery and improvement of solutions to complex problems  in this paper we argue that complexification ie the incremental elaboration of solutions through adding new structure achieves both these goals  we demonstrate the power of complexification through the neuroevolution of augmenting topologies neat method which evolves increasingly complex neural network architectures  neat is applied to an openended coevolutionary robot duel domain where robot controllers compete head to head  because the robot duel domain supports a wide range of strategies and because coevolution benefits from an escalating arms race it serves as a suitable testbed for studying complexification  when compared to the evolution of networks with fixed structure complexifying evolution discovers significantly more sophisticated strategies  the results suggest that in order to discover and improve complex solutions evolution and search in general should be allowed to complexify as well as optimize



j  d park and a  darwiche 2006 complexity results and approximation strategies for map explanations volume 21 pages 101133

map is the problem of finding a most probable instantiation of a set of variables given evidence map has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation pr or the problem of computing the most probable explanation mpe this paper investigates the complexity of map in bayesian networks specifically we show that map is complete for nppp and provide further negative complexity results for algorithms based on variable elimination we also show that map remains hard even when mpe and pr become easy for example we show that map is npcomplete when the networks are restricted to polytrees and even then can not be effectively approximated  given the difficulty of computing map exactly and the difficulty of approximating map while providing useful guarantees on the resulting approximation we investigate best effort approximations we introduce a generic map approximation framework we provide two instantiations of the framework one for networks which are amenable to exact inference pr and one for networks for which even exact inference is too hard this allows map approximation on networks that are too complex to even exactly solve the easier problems pr and mpe experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques and provide accurate map estimates in many cases 



c  boutilier  r  i brafman  c  domshlak  h  h hoos and  d  poole 2004 cpnets a tool for representing and reasoning withconditional ceteris paribus preference statements volume 21 pages 135191




2009 ijcaijair best paper prize

information about user preferences plays a key role in automated decision making in many domains it is desirable to assess such preferences in a qualitative rather than quantitative way in this paper we propose a qualitative graphical representation of preferences that reflects conditional dependence and independence of preference statements under a ceteris paribus all else being equal interpretation such a representation is often compact and arguably quite natural in many circumstances we provide a formal semantics for this model and describe how the structure of the network can be exploited in several inference tasks such as determining whether one outcome dominates is preferred to another ordering a set outcomes according to the preference relation and constructing the best outcome subject to available evidence



h  e dixon  m  l ginsberg and  a  j parkes 2004 generalizing boolean satisfiability i background and survey of existing work volume 21 pages 193243

this is the first of three planned papers describing zap a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern highperformance solvers  the fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the boolean representation used our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance  this paper is a survey of the work underlying zap and discusses previous attempts to improve the performance of the davisputnamlogemannloveland algorithm by exploiting the structure of the problem being solved  we examine existing ideas including extensions of the boolean language to allow cardinality constraints pseudoboolean representations symmetry and a limited form of quantification  while this paper is intended as a survey our research results are contained in the two subsequent articles with the theoretical structure of zap described in the second paper in this series and zaps implementation described in the third



o  arieli  m  denecker  b  van nuffelen and  m  bruynooghe 2004 coherent integration of databases by abductive logic programming volume 21 pages 245286

abstract we introduce an abductive method for a coherent integration of independent datasources the idea is to compute a list of datafacts that should be inserted to the amalgamated database or retracted from it in order to restore its consistency this method is implemented by an abductive solver called asystem that applies sldnfaresolution on a metatheory that relates different possibly contradicting input databases we also give a pure modeltheoretic analysis of the possible ways to recover consistent data from an inconsistent database in terms of those models of the database that exhibit as minimal inconsistent information as reasonably possible this allows us to characterize the recovered databases in terms of the preferred ie most consistent models of the theory the outcome is an abductivebased application that is sound and complete with respect to a corresponding modelbased preferential semantics and  to the best of our knowledge  is more expressive thus more general than any other implementation of coherent integration of databases



m  j nederhof and  g  satta 2004 idlexpressions a formalism for representing and parsing finite languages in natural language processing volume 21 pages 287317

we  propose  a  formalism  for  representation  of  finite  languages referred to  as the class of idlexpressions  which combines concepts that were  only considered in  isolation in existing  formalisms  the suggested  applications  are  in  natural  language  processing  more specifically  in surface  natural language  generation and  in machine translation where a sentence is  obtained by first generating a large set of candidate sentences represented  in a compact way and then by filtering  such a  set  through  a parser   we  study several  formal properties of idlexpressions and compare this new formalism with more standard  ones   we  also  present  a  novel  parsing  algorithm  for idlexpressions  and  prove a  nontrivial  upper  bound  on its  time complexity



j  y halpern and  d  koller 2004 representation dependence in probabilistic inference volume 21 pages 319356

nondeductive reasoning systems are often representation dependent representing the same situation in two different ways may cause such a system to return two different answers  some have viewed this as a significant problem  for example the principle of maximum entropyhas been subjected to much criticism due to its representation dependence there has however been almost no work investigating representation dependence  in this paper we formalize this notion and show that it is not a problem specific to maximum entropy  in fact we show that any representationindependent probabilistic inference procedure that ignores irrelevant information is essentially entailment in a precise sense  moreover we show that representation independence is incompatible with even a weak default assumption of independence  we then show that invariance under a restricted class of representation changes can form a reasonable compromise between representation independence and other desiderata and provide a construction of a family of inference procedures that provides such restricted representation independence using relative entropy



b  hnich  b  m smith and  t  walsh 2004 dual modelling of permutation and injection problems volume 21 pages 357391

when writing a constraint program we have to choose which variables  should be the decision variables and how to represent the constraints  on these variables in many cases there is considerable choice for  the decision variables  consider for example permutation problems in which we have as many values as variables and each variable takes  an unique value in such problems we can choose between a primal and a dual viewpoint in the dual viewpoint each dual variable  represents one of the primal values whilst each dual value represents one of the primal variables alternatively by means of channelling  constraints to link the primal and dual variables we can have a  combined model with both sets of variables in this paper we perform  an extensive theoretical and empirical study of such primal dual and  combined models for two classes of problems permutation problems and  injection problems our results show that it often be advantageous to  use multiple viewpoints and to have constraints which channel between them to maintain consistency they also illustrate a general  methodology for comparing different constraint models



c  a thompson  m  h goker and  p  langley 2004 a personalized system for conversational recommendations volume 21 pages 393428

searching for and making decisions about information is becoming increasingly difficult as the amount of information and number of choices increases  recommendation systems help users find items of interest of a particular type such as movies or restaurants but are still somewhat awkward to use  our solution is to take advantage of the complementary strengths of personalized recommendation systems and dialogue systems creating personalized aides  we present a  system  the adaptive place advisor  that treats item selection as an interactive conversational process with the program inquiring about item attributes and the user responding  individual longterm user preferences are unobtrusively obtained in the course of normal recommendation dialogues and used to direct future conversations with the same user  we present a novel user model that influences both item search and the questions asked during a conversation  we demonstrate the effectiveness of our system in significantly reducing the time and number of interactions required to find a satisfactory item as compared to a control group of users interacting with a nonadaptive version of the system



p  gorniak and  d  roy 2004 grounded semantic composition for visual scenes volume 21 pages 429470

we present a visuallygrounded language understanding model based on a study of how people verbally describe objects in scenes the emphasis of the model is on the combination of individual word meanings to produce meanings for complex referring expressions the model has been implemented and it is able to understand a broad range of spatial referring expressions we describe our implementation of word level visuallygrounded semantics and their embedding in a compositional parsing framework the implemented system selects the correct referents in response to natural language expressions for a large percentage of test cases in an analysis of the systems successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account



w  zhang 2004 phase transitions and backbones of the asymmetric traveling salesman problem volume 21 pages 471497

in recent years there has been much interest in phase transitions of combinatorial problems  phase transitions have been successfully used to analyze combinatorial optimization problems characterize their typicalcase features and locate the hardest problem instances  in this paper we study phase transitions of the asymmetric traveling salesman problem atsp an nphard combinatorial optimization problem that has many realworld applications  using random instances of up to 1500 cities in which intercity distances are uniformly distributed we empirically show that many properties of the problem including the optimal tour cost and backbone size experience sharp transitions as the precision of intercity distances increases across a critical value  our experimental results on the costs of the atsp tours and assignment problem agree with the theoretical result that the asymptotic cost of assignment problem is pi 2 6 the number of cities goes to infinity  in addition we show that the average computational cost of the wellknown branchandbound subtour elimination algorithm for the problem also exhibits a thrashing behavior transitioning from easy to difficult as the distance precision increases  these results answer positively an open question regarding the existence of phase transitions in the atsp and provide guidance on how difficult atsp problem instances should be generated



j  keppens and  q  shen 2004 compositional model repositories via dynamic constraint satisfaction with orderofmagnitude preferences volume 21 pages 499550

the predominant knowledgebased approach to automated model construction compositional modelling employs a set of models of particular functional components  its inference mechanism takes a scenario describing the constituent interacting components of a system and translates it into a useful mathematical model  this paper presents a novel compositional modelling approach aimed at building model repositories  it furthers the field in two respects  firstly it expands the application domain of compositional modelling to systems that can not be easily described in terms of interacting functional components such as ecological systems  secondly it enables the incorporation of user preferences into the model selection process  these features are achieved by casting the compositional modelling problem as an activitybased dynamic preference constraint satisfaction problem where the dynamic constraints describe the restrictions imposed over the composition of partial models and the preferences correspond to those of the user of the automated modeller in addition the preference levels are represented through the use of symbolic values that differ in orders of magnitude



p  liberatore 2004 on polynomial sized mdp succinct policies volume 21 pages 551577

policies of markov decision processes mdps determine the next action to execute from the current state and possibly the history the past states when the number of states is large succinct representations are often used to compactly represent both the mdps and the policies in a reduced amount of space in this paper some problems related to the size of succinctly represented policies are analyzed namely it is shown that some mdps have policies that can only be represented in space superpolynomial in the size of the mdp unless the polynomial hierarchy collapses this fact motivates the study of the problem of deciding whether a given mdp has a policy of a given size and reward since some algorithms for mdps work by finding a succinct representation of the value function the problem of deciding the existence of a succinct representation of a value function of a given size and reward is also considered



a  borodin  r  elyaniv and  v  gogan 2004 can we learn to beat the best stock volume 21 pages 579594

a novel algorithm for actively trading stocks is presented while traditional expert advice and universal algorithms as well as standard technical trading heuristics attempt to predict winners or trends our approach relies on predictable statistical relations between all pairs of stocks in the market our empirical results on historical markets provide strong evidence that this type of technical trading can beat the market and moreover can beat the best stock in the market in doing so we utilize a new idea for smoothing critical parameters in the context of expert learning



m  babaioff and  n  nisan 2004 concurrent auctions across the supply chain volume 21 pages 595629

with the recent technological feasibility of electronic commerce over the internet much attention has been given to the design of electronic markets for various types of electronicallytradable goods such markets however will normally need to function in some relationship with markets for other related goods usually those downstream or upstream in the supply chain  thus for example an electronic market for rubber tires for trucks will likely need to be strongly influenced by the rubber market as well as by the truck market    in this paper we design protocols for exchange of information between a sequence of markets along a single supply chain  these protocols allow each of these markets to function separately while the information exchanged ensures efficient global behavior across the supply chain  each market that forms a link in the supply chain operates as a double auction where the bids on one side of the double auction come from bidders in the corresponding segment of the industry and the bids on the other side are synthetically generated by the protocol to express the combined information from all other links in the chain  the double auctions in each of the markets can be of several types and we study several variants of incentive compatible double auctions comparing them in terms of their efficiency and of the market revenue



a  felner  r  stern  a  benyair  s  kraus and  n  netanyahu 2004 pha finding the shortest path with  a in an unknown physical environment volume 21 pages 631670

we address the problem of finding the shortest path between two points in an unknown real physical environment where a traveling agent must move around in the environment to explore unknown territory  we introduce the physicala algorithm pha for solving this problem pha expands all the mandatory nodes that a would expand and returns the shortest path between the two points  however due to the physical nature of the problem the complexity of the algorithm is measured by the traveling effort of the moving agent and not by the number of generated nodes as in standard a  pha is presented as a twolevel algorithm such that its high level a chooses the next node to be expanded and its low level directs the agent to that node in order to explore it  we present a number of variations for both the highlevel and lowlevel procedures and evaluate their performance theoretically and experimentally  we show that the travel cost of our best variation is fairly close to the optimal travel cost assuming that the mandatory nodes of a are known in advance  we then generalize our algorithm to the multiagent case where a number of cooperative agents are designed to solve the problem  specifically we provide an experimental implementation for such a system  it should be noted that the problem addressed here is not a navigation problem but rather a problem of finding the shortest path between two points for future usage
