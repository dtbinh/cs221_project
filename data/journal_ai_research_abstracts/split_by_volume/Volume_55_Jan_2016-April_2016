marta  r costajuss224 srinivas  bangalore patrik  lambert llu237s   m224rquez and elena  montielponsoda 2016 introduction to the special issue on crosslanguage algorithms and applications volume 55 pages 115

with the increasingly global nature of our everyday interactions the need for multilin gual technologies to support efficient and effective information access and communication cannot be overemphasized computational modeling of language has been the focus of natural language processing a subdiscipline of artificial intelligence one of the current challenges for this discipline is to design methodologies and algorithms that are cross language in order to create multilingual technologies rapidly the goal of this jair special issue on crosslanguage algorithms and applications claa is to present leading re search in this area with emphasis on developing unifying themes that could lead to the development of the science of multi and crosslingualism in this introduction we provide the reader with the motivation for this special issue and summarize the contributions of the papers that have been included the selected papers cover a broad range of crosslingual technologies including machine translation domain and language adaptation for sentiment analysis crosslanguage lexical resources dependency parsing information retrieval and knowledge representation we anticipate that this special issue will serve as an invaluable resource for researchers interested in topics of crosslingual natural language processing



v237ctor m  s225nchezcartagena juan antonio  p233rezortiz and felipe  s225nchezmart237nez 2016 integrating rules and dictionaries from shallowtransfer machine translation into phrasebased statistical machine translation volume 55 pages 1761

we describe a hybridisation strategy whose objective is to integrate linguistic resources from shallowtransfer rulebased machine translation rbmt into phrasebased statistical machine translation pbsmt it basically consists of enriching the phrase table of a pbsmt system with bilingual phrase pairs matching transfer rules and dictionary entries from a shallowtransfer rbmt system this new strategy takes advantage of how the linguistic resources are used by the rbmt system to segment the sourcelanguage sentences to be translated and overcomes the limitations of existing hybrid approaches that treat the rbmt systems as a black box experimental results confirm that our approach delivers translations of  higher quality than existing ones and that it is specially useful when the parallel corpus available for training the smt system is small or when translating outofdomain texts that are well covered by the rbmt dictionaries a combination of this approach with a recently proposed unsupervised shallowtransfer rule inference algorithm results in a significantly greater translation quality than that of a baseline pbsmt in this case the only handcrafted resource used are the dictionaries commonly used in rbmt moreover the translation quality achieved by the hybrid system built with automatically inferred rules is similar to that obtained by those built with handcrafted rules



yulia  tsvetkov and chris  dyer 2016 crosslingual bridges with models of lexical borrowing volume 55 pages 6393

linguistic borrowing is the phenomenon of transferring linguistic constructions lexical phonological morphological and syntactic from a donor language to a recipient language as a result of contacts between communities speaking different languages borrowed words are found in all languages andin contrast to cognate relationshipsborrowing relationships may exist across unrelated languages for example about 40 of swahilis vocabulary is borrowed from the unrelated language arabic in this work we develop a model of morphophonological transformations across languages its features are based on universal constraints from optimality theory ot and we show that compared to several standardbut linguistically more na239vebaselines our otinspired model obtains good performance at predicting donor forms from borrowed forms with only a few dozen training examples making this a costeffective strategy for sharing lexical information across languages we demonstrate applications of the lexical borrowing model in machine translation using resourcerich donor language to obtain translations of outofvocabulary loanwords in a lower resource language our framework obtains substantial improvements up to 16 bleu over standard baselines



saif  m mohammad mohammad  salameh and svetlana  kiritchenko 2016 how translation alters sentiment volume 55 pages 95130

sentiment analysis research has predominantly been on english texts thus there exist many sentiment resources for english but less so for other languages approaches to improve sentiment analysis in a resourcepoor focus language include a translate the focus language text into a resourcerich language such as english and apply a powerful english sentiment analysis system on the text and b translate resources such as sentiment labeled corpora and sentiment lexicons from english into the focus language and use them as additional resources in the focuslanguage sentiment analysis system in this paper we systematically examine both options we use arabic social media posts as standin for the focus language text we show that sentiment analysis of english translations of arabic texts produces competitive results wrt arabic sentiment analysis we show that arabic sentiment analysis systems benefit from the use of automatically translated english sentiment lexicons we also conduct manual annotation studies to examine why the sentiment of a translation is different from the sentiment of the source word or text this is especially relevant for building better automatic translation systems in the process we create a stateoftheart arabic sentiment analysis system a new dialectal arabic sentiment lexicon and the first arabicenglish parallel corpus that is independently annotated for sentiment by arabic and english speakers



alejandro  moreo fern225ndez andrea  esuli and fabrizio  sebastiani 2016 distributional correspondence indexing for crosslingual and crossdomain sentiment classification volume 55 pages 131163

domain adaptation da techniques aim at enabling machine learning methods learn effective classifiers for a target domain when the only available training data belongs to a different source domain in this paper we present the distributional correspondence indexing dci method for domain adaptation in sentiment classification dci derives term representations in a vector space common to both domains where each dimension reflects its distributional correspondence to a pivot ie to a highly predictive term that behaves similarly across domains term correspondence is quantified by means of a distributional correspondence function dcf we propose a number of efficient dcfs that are motivated by the distributional hypothesis ie the hypothesis according to which terms with similar meaning tend to have similar distributions in text experiments show that dci obtains better performance than current stateoftheart techniques for crosslingual and crossdomain sentiment classification dci also brings about a significantly reduced computational cost and requires a smaller amount of human intervention as a final contribution we discuss a more challenging formulation of the domain adaptation problem in which both the crossdomain and crosslingual dimensions are tackled simultaneously



mamoun  abu helou matteo  palmonari and mustafa  jarrar 2016 effectiveness of automatic translations for crosslingual ontology mapping volume 55 pages 165208

accessing or integrating data lexicalized in different languages is a challenge multilingual lexical resources play a fundamental role in reducing the language barriers to map concepts lexicalized in different languages in this paper we present a largescale study on the effectiveness of automatic translations to support two key crosslingual ontology mapping tasks the retrieval of candidate matches and the selection of the correct matches for inclusion in the final alignment  we conduct our experiments using four different large gold standards each one consisting of a pair of mapped wordnets to cover four different families of languages we categorize concepts based on their lexicalization type of words synonym richness position in a subconcept graph and analyze their distributions in the gold standards leveraging this categorization we measure several aspects of translation effectiveness such as wordtranslation correctness word sense coverage synset and synonym coverage finally we thoroughly discuss several findings of our study which we believe are helpful for the design of more sophisticated crosslingual mapping algorithms



j246rg  tiedemann and zeljko  agi263 2016 synthetic treebanking for crosslingual dependency parsing volume 55 pages 209248

how do we parse the languages for which no treebanks are available this contribution addresses the crosslingual viewpoint on statistical dependency parsing in which we attempt to make use of resourcerich source language treebanks to build and adapt models for the underresourced target languages we outline the benefits and indicate the drawbacks of the current major approaches we emphasize synthetic treebanking the automatic creation of target language treebanks by means of annotation projection and machine translation we present competitive results in crosslingual dependency parsing using a combination of various techniques that contribute to the overall success of the method we further include a detailed discussion about the impact of partofspeech label accuracy on parsing results that provide guidance in practical applications of crosslingual methods for truly underresourced languages



ahmad  khwileh debasis   ganguly  and gareth   j f jones  2016 utilisation of metadata fields and query expansion in crosslingual search of usergenerated internet video volume 55 pages 249281

recent years have seen significant efforts in the area of cross language information retrieval clir for text retrieval this work initially focused on formally published content but more recently research has begun to concentrate on clir for informal social media content however despite the current expansion in online multimedia archives there has been little work on clir for this content while there has been some limited work on crosslanguage video retrieval clvr for professional videos such as documentaries or tv news broadcasts there has to date been no significant investigation of clvr for the rapidly growing archives of informal user generated ugc content key differences between such ugc and professionally produced content are the nature and structure of the textual ugc metadata associated with it as well as the form and quality of the content itself in this setting retrieval effectiveness may not only suffer from translation errors common to all clir tasks but also recognition errors associated with the automatic speech recognition asr systems used to transcribe the spoken content of the video and with the informality and inconsistency of the associated usercreated metadata for each video this work proposes and evaluates techniques to improve clir effectiveness of such noisy ugc content our experimental investigation shows that different sources of evidence eg the content from different fields of the structured metadata significantly affect clir effectiveness results from our experiments also show that each metadata field has a varying robustness to query expansion qe and hence can have a negative impact on the clir effectiveness our work proposes a novel adaptive qe technique that predicts the most reliable source for expansion and shows how this technique can be effective for improving the clir effectiveness for ugc content



jan  rupnik andrej  muhic gregor  leban primoz  skraba blaz  fortuna and marko  grobelnik 2016 news across languages  crosslingual document similarity and event tracking volume 55 pages 283316

in todays world we follow news which is distributed globally significant events are reported by different sources and in different languages in this work we address the problem of tracking of events in a large multilingual stream within a recently developed system event registry we examine two aspects of this problem how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event  taking a multilingual stream and clusters of articles from each language we compare different crosslingual document similarity measures based on wikipedia this allows us to compute the similarity of any two articles regardless of language building on previous work we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data using this capability we then propose an approach to link clusters of articles across languages which represent the same event we provide an extensive evaluation of the system as a whole as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm



chienju  ho aleksandrs  slivkins and jennifer  wortman vaughan 2016 adaptive contract design for crowdsourcing markets bandit algorithms for repeated principalagent problems volume 55 pages 317359

crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete the payment for a particular task is typically set by the tasks requester and may be adjusted based on the quality of the completed work for example through the use of bonus payments in this paper we study the requesters problem of dynamically adjusting qualitycontingent payments for tasks we consider a multiround version of the wellknown principalagent model whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester in particular our formulation significantly generalizes the budgetfree online task pricing problems studied in prior work we treat this problem as a multiarmed bandit problem with each arm representing a potential contract to cope with the large and in fact infinite number of arms we propose a new algorithm agnosticzooming which discretizes the contract space into a finite number of regions effectively treating each region as a single arm this discretization is adaptively refined so that more promising regions of the contract space are eventually discretized more finely we analyze this algorithm showing that it achieves regret sublinear in the time horizon and substantially improves over nonadaptive discretization which is the only competing approach in the literature our results advance the state of art on several different topics the theory of crowdsourcing markets principalagent problems multiarmed bandits and dynamic pricing



ziyu  wang frank  hutter masrour  zoghi david  matheson and nando  de feitas 2016 bayesian optimization in a billion dimensions via random embeddings volume 55 pages 361387

bayesian optimization techniques have been successfully applied to robotics planning sensor placement recommendation advertising intelligent user interfaces and automatic algorithm configuration despite these successes the approach is restricted to problems of moderate dimension and several workshops on bayesian optimization have identified its scaling to highdimensions as one of the holy grails of the field in this paper we introduce a novel random embedding idea to attack this problem the resulting random embedding bayesian optimization rembo algorithm is very simple has important invariance properties and applies to domains with both categorical and continuous variables we present a thorough theoretical analysis of rembo empirical results confirm that rembo can effectively solve problems with billions of dimensions provided the intrinsic dimensionality is low they also show that rembo achieves stateoftheart performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver



aron  culotta nirmal  kumar ravi and jennifer  cutler 2016 predicting twitter user demographics using distant supervision from website traffic data volume 55 pages 389408




aaai 2015 outstanding paper honorable mention

understanding the demographics of users of online social networks has important applications for health marketing and public messaging whereas most prior approaches rely on a supervised learning approach in which individual users are labeled with demographics for training we instead create a distantly labeled dataset by collecting audience measurement data for 1500 websites eg 50 of visitors to gizmodocom are estimated to have a bachelors degree we then fit a regression model to predict these demographics from information about the followers of each website on twitter using patterns derived both from textual content and the social network of each user our final model produces an average heldout correlation of 77 across seven different variables age gender education ethnicity income parental status and political preference we then apply this model to classify individual twitter users by ethnicity gender and political preference finding performance that is surprisingly competitive with a fully supervised approach



raffaella  bernardi ruket  cakici desmond  elliott aykut  erdem erkut  erdem nazli  ikizlercinbis frank  keller adrian  muscat and barbara  plank 2016 automatic description generation from images a survey of models datasets and evaluation measures volume 55 pages 409442

automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities in this survey we classify the existing approaches based on how they conceptualize this problem viz models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space we provide a detailed review of existing models highlighting their advantages and disadvantages moreover we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machinegenerated image descriptions finally we extrapolate future directions in the area of automatic image description generation



jilles  steeve dibangoye christopher  amato olivier  buffet and fran231ois  charpillet 2016 optimally solving decpomdps as continuousstate mdps volume 55 pages 443497

decentralized partially observable markov decision processes decpomdps provide a general model for decisionmaking under uncertainty in decentralized settings but are difficult to solve optimally nexpcomplete as a new way of solving these problems we introduce the idea of transforming a decpomdp into a continuousstate deterministic mdp with a piecewiselinear and convex value function this approach makes use of the fact that planning can be accomplished in a centralized offline manner while execution can still be decentralized this new decpomdp formulation which we call an occupancy mdp allows powerful pomdp and continuousstate mdp methods to be used for the first time to provide scalability we refine this approach by combining heuristic search and compact representations that exploit the structure present in multiagent domains without losing the ability to converge to an optimal solution  in particular we introduce a featurebased heuristic search value iteration fbhsvi algorithm that relies on featurebased compact representations pointbased updates and efficient action selection  a theoretical analysis demonstrates that fbhsvi terminates in finite time with an optimal solution we include an extensive empirical analysis using wellknown benchmarks thereby demonstrating that our approach provides significant scalability improvements compared to the state of the art



ana  armas romero mark  kaminski bernardo  cuenca grau and ian  horrocks 2016 module extraction in expressive ontology languages via datalog reasoning volume 55 pages 499564

module extraction is the task of computing a preferably small fragment m of an ontology t that preserves a class of entailments over a signature of interest s extracting modules of minimal size is wellknown to be computationally hard and often algorithmically infeasible especially for highly expressive ontology languages thus practical techniques typically rely on approximations where m provably captures the relevant entailments but is not guaranteed to be minimal existing approximations ensure that m preserves all secondorder entailments of t wrt s which is a stronger condition than is required in many applications and may lead to unnecessarily large modules in practice in this paper we propose a novel approach in which module extraction is reduced to a reasoning problem in datalog our approach generalises existing approximations in an elegant way more importantly it allows extraction of modules that are tailored to preserve only specific kinds of entailments and thus are often significantly smaller our evaluation on a wide range of ontologies confirms the feasibility and benefits of our approach in practice



felix  brandt and christian  geist 2016 finding strategyproof social choice functions via sat solving volume 55 pages 565602

a promising direction in computational social choice is to address research problems using computeraided proving techniques in particular with sat solvers this approach has been shown to be viable not only for proving classic impossibility theorems such as arrows theorem but also for finding new impossibilities in the context of preference extensions in this paper we demonstrate that these computeraided techniques can also be applied to improve our understanding of strategyproof irresolute social choice functions these functions however requires a more evolved encoding as otherwise the search space rapidly becomes much too large our contribution is twofold we present an efficient encoding for translating such problems to sat and leverage this encoding to prove new results about strategyproofness with respect to kellys and fishburns preference extensions for example we show that no paretooptimal majoritarian social choice function satisfies fishburnstrategyproofness furthermore we explain how humanreadable proofs of such results can be extracted from minimal unsatisfiable cores of the corresponding sat formulas 



robert  bredereck piotr  faliszewski rolf  niedermeier and nimrod  talmon 2016 largescale election campaigns combinatorial shift bribery volume 55 pages 603652

we study the complexity of a combinatorial variant of the shift bribery problem in elections in the standard shift bribery problem we are given an election where each voter has a preference order over the set of candidates and where an outside agent the briber can pay each voter to rank the bribers favorite candidate a given number of positions higher the goal is to ensure the victory of the bribers preferred candidate the combinatorial variant of the problem introduced in this paper models settings where it is possible to affect the position of the preferred candidate in multiple votes either positively or negatively with a single bribery action this variant of the problem is particularly interesting in the context of largescale campaign management problems which from the technical side are modeled as bribery problems  we show that in general the combinatorial variant of the problem is highly intractable specifically nphard hard in the parameterized sense and hard to approximate nevertheless we provide parameterized algorithms and approximation algorithms for natural restricted cases




xiaoyuan  zhu and changhe  yuan 2016 exact algorithms for mre inference volume 55 pages 653683

most relevant explanation mre is an inference task in bayesian networks that finds the most relevant partial instantiation of target variables as an explanation for given evidence by maximizing the generalized bayes factor gbf no exact mre algorithm has been developed previously except exhaustive search this paper fills the void by introducing two breadthfirst branchandbound bfbnb algorithms for solving mre based on novel upper bounds of gbf one upper bound is created by decomposing the computation of gbf using a target blanket decomposition of evidence variables the other upper bound improves the first bound in two ways one is to split the target blankets that are too large by converting auxiliary nodes into pseudotargets so as to scale to large problems the other is to perform summations instead of maximizations on some of the target variables in each target blanket our empirical evaluations show that the proposed bfbnb algorithms make exact mre inference tractable in bayesian networks that could not be solved previously



roderick  sebastiaan  de nijs christian   landsiedel dirk  wollherr and martin  buss 2016 quadratization and roof duality of markov logic networks volume 55 pages 685714

this article discusses the quadratization of markov logic networks which enables efficient approximate map computation by means of maximum flows the procedure relies on a pseudoboolean representation of the model and allows handling models of any order the employed pseudoboolean representation can be used to identify problems that are guaranteed to be solvable in low polynomialtime results on common benchmark problems show that the proposed approach finds optimal assignments for most variables in excellent computational time and approximate solutions that match the quality of ilpbased solvers



alberto  garciaduran antoine  bordes nicolas  usunier and yves  grandvalet 2016 combining two and threeway embedding models for link prediction in knowledge bases volume 55 pages 715742

this paper tackles the problem of endogenous link prediction for knowledge base completion knowledge bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns which unfortunately usually end up overfitting on rare relationships or in approaches that trade capacity for simplicity in order to fairly model all relationships frequent or not in this paper we propose tatec a happy medium obtained by complementing a highcapacity model with a simpler one both pretrained separately and then combined we present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving stateoftheart results on four benchmarks of the literature



francesco  parisi and john  grant 2016 knowledge representation in probabilistic spatiotemporal knowledge bases volume 55 pages 743798

we represent knowledge as integrity constraints in a formalization of probabilistic spatiotemporal knowledge bases we start by defining the syntax and semantics of a formalization called pst knowledge bases  this definition generalizes an earlier version called spot which is a declarative framework for the representation and processing of probabilistic spatiotemporal data where probability is represented as an interval because the exact value is unknown we augment the previous definition by adding a type of nonatomic formula that expresses integrity constraints the result is a highly expressive formalism for knowledge representation dealing with probabilistic spatiotemporal data  we obtain complexity results both for checking the consistency of pst knowledge bases and for answering queries in pst knowledge bases and also specify tractable cases  all the domains in the pst framework are finite but we extend our results also to arbitrarily large finite domains



zhiwen  fang chumin  li and ke  xu 2016 an exact algorithm based on maxsat reasoning for the maximum weight clique problem volume 55 pages 799833

recently maxsat reasoning is shown very effective in computing a tight upper bound for a maximum clique mc of a unweighted graph in this paper we apply maxsat reasoning to compute a tight upper bound for a maximum weight clique mwc of a wighted graph we first study three usual encodings of mwc into weighted partial maxsat dealing with hard clauses which must be satisfied in all solutions and soft clauses which are weighted and can be falsified the drawbacks of these encodings motivate us to propose an encoding of mwc into a special weighted partial maxsat formalism called lw literalweighted encoding and dedicated for upper bounding an mwc in which both soft clauses and literals in soft clauses are weighted an optimal solution of the lw maxsat instance gives an upper bound for an mwc instead of an optimal solution for mwc we then introduce two notions called the topk literal failed clause and the topk empty clause to extend classical maxsat reasoning techniques as well as two sound transformation rules to transform an lw maxsat instance successive transformations of an lw maxsat instance driven by maxsat reasoning give a tight upper bound for the encoded mwc the approach is implemented in a branchandbound algorithm called mwclq experimental evaluations on the broadly used dimacs benchmark bhoslib benchmark random graphs and the benchmark from the winner determination problem show that our approach allows mwclq to reduce the search space significantly and to solve mwc instances effectively consequently mwclq outperforms stateoftheart exact algorithms on the vast majority of instances moreover it is surprisingly effective in solving hard and dense instances




dietmar  jannach thomas  schmitz and kostyantyn  shchekotykhin 2016 parallel modelbased diagnosis on multicore computers volume 55 pages 835887

modelbased diagnosis mbd is a principled and domainindependent way of analyzing why a system under examination is not behaving as expected given an abstract description model of the systems components and their behavior when functioning normally mbd techniques rely on observations about the actual system behavior to reason about possible causes when there are discrepancies between the expected and observed behavior due to its generality mbd has been successfully applied in a variety of application domains over the last decades
in many application domains of mbd testing different hypotheses about the reasons for a failure can be computationally costly eg because complex simulations of the system behavior have to be performed in this work we therefore propose different schemes of parallelizing the diagnostic reasoning process in order to better exploit the capabilities of modern multicore computers we propose and systematically evaluate parallelization schemes for reiters hitting set algorithm for finding all or a few leading minimal diagnoses using two different conflict detection techniques furthermore we perform initial experiments for a basic depthfirst search strategy to assess the potential of parallelization when searching for one single diagnosis finally we test the effects of parallelizing direct encodings of the diagnosis problem in a constraint solver



natalia  flerova radu  marinescu and rina  dechter 2016 searching for the m best solutions in graphical models volume 55 pages 889952

  the paper focuses on finding the m best solutions to combinatorial optimization problems using bestfirst or depthfirst branch and bound search  specifically we present a new algorithm ma extending the wellknown a to the mbest task and for the first time prove that all its desirable properties including soundness completeness and optimal efficiency are maintained since bestfirst algorithms require extensive memory we also extend the memoryefficient depthfirst branch and bound to the mbest task
we adapt both algorithms to optimization tasks over graphical models eg weighted csp and mpe in bayesian networks provide complexity analysis and an empirical evaluation our experiments confirm theory that the bestfirst approach is largely superior when memory is available but depthfirst branch and bound is more robust  we also show that our algorithms are competitive with related schemes recently developed for the mbest task



ivan  vuli263 and mariefrancine  moens 2016 bilingual distributed word representations from documentaligned comparable data volume 55 pages 953994

we propose a new model for learning bilingual word representations from nonparallel documentaligned data following the recent advances in word representation learning our model learns dense realvalued word vectors that is bilingual word embeddings bwes unlike prior work on inducing bwes which heavily relied on parallel sentencealigned corpora andor readily available translation resources such as dictionaries the article reveals that bwes may be learned solely on the basis of documentaligned comparable data without any additional lexical resources nor syntactic information we present a comparison of our approach with previous stateoftheart models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling muptm as well as with distributional local contextcounting models we demonstrate the utility of the induced bwes in two semantic tasks 1 bilingual lexicon extraction 2 suggesting word translations in context for polysemous words our simple yet effective bwebased models significantly outperform the muptmbased and contextcounting representation models from comparable data as well as prior bwebased models and acquire the best reported results on both tasks for all three tested language pairs



jiang  guo wanxiang  che david  yarowsky haifeng  wang and ting  liu 2016 a distributed representationbased framework for crosslingual transfer parsing volume 55 pages 9951023

this paper investigates the problem of crosslingual transfer parsing aiming at inducing dependency parsers for lowresource languages while using only training data from a resourcerich language eg english existing model transfer approaches typically dont include lexical features which are not transferable across languages in this paper we bridge the lexical feature gap by using distributed feature representations and their composition we provide two algorithms for inducing crosslingual distributed representations of words which map vocabularies from two different languages into a common vector space consequently both lexical features and nonlexical features can be used in our model for crosslingual transfer furthermore our framework is flexible enough to incorporate additional useful features such as crosslingual word clusters our combined contributions achieve an average relative error reduction of 109 in labeled attachment score as compared with the delexicalized parser trained on english universal treebank and transferred to three other languages it also significantly outperforms stateoftheart delexicalized models augmented with projected cluster features on identical data finally we demonstrate that our models can be further boosted with minimal supervision eg 100 annotated sentences from target languages which is of great significance for practical usage



osman  ba351kaya and david  jurgens 2016 semisupervised learning with induced word senses for state of the art word sense disambiguation volume 55 pages 10251058

word sense disambiguation wsd aims to determine the meaning of a word in context and successful approaches are known to benefit many applications in natural language processing  although supervised learning has been shown to provide superior wsd performance current senseannotated corpora do not contain a sufficient number of instances per word type to train supervised systems for all words  while unsupervised techniques have been proposed to overcome this data sparsity problem such techniques have not outperformed supervised methods in this paper we propose a new approach to building semisupervised wsd systems that combines a small amount of senseannotated data with information from word sense induction a fullyunsupervised technique that automatically learns the different senses of a word based on how it is used in three experiments we show how sense induction models may be effectively combined to ultimately produce highperformance semisupervised wsd systems that exceed the performance of stateoftheart supervised wsd techniques trained on the same senseannotated data we anticipate that our results and released software will also benefit evaluation practices for sense induction systems and those working in lowresource languages by demonstrating how to quickly produce accurate wsd systems with minimal annotation effort



hanxiao  liu wanli  ma yiming  yang and jaime  carbonell 2016 learning concept graphs from online educational data volume 55 pages 10591090

this paper addresses an open challenge in educational data mining ie the problem of automatically mapping online courses from different providers universities moocs etc onto a universal space of concepts and predicting latent prerequisite dependencies directed links among both concepts and courses we propose a novel approach for inference within and across courselevel and conceptlevel directed graphs in the training phase our system projects partially observed courselevel prerequisite links onto directed conceptlevel links in the testing phase the induced conceptlevel links are used to infer the unknown courselevel prerequisite links whereas courses may be specific to one institution concepts are shared across different providers the bidirectional mappings enable our system to perform interlinguastyle transfer learning eg treating the concept graph as the interlingua and transferring the prerequisite relations across universities via the interlingua experiments on our newly collected datasets of courses from mit caltech princeton and cmu show promising results



tuan  m v le and hady  w lauw 2016 semantic visualization with neighborhood graph regularization volume 55 pages 10911133




aaai 2014 honorable mention for outstanding paper

visualization of highdimensional data such as text documents is useful to map out the similarities among various data points in the highdimensional space documents are commonly represented as bags of words with dimensionality equal to the vocabulary size classical approaches to document visualization directly reduce this into visualizable two or three dimensions recent approaches consider an intermediate representation in topic space between word space and visualization space which preserves the semantics by topic modeling while aiming for a good fit between the model parameters and the observed data previous approaches have not considered the local consistency among data instances we consider the problem of semantic visualization by jointly modeling topics and visualization on the intrinsic document manifold modeled using a neighborhood graph each document has both a topic distribution and visualization coordinate specifically we propose an unsupervised probabilistic model called semafore which aims to preserve the manifold in the lowerdimensional spaces through a neighborhood regularization framework designed for the semantic visualization task to validate the efficacy of semafore our comprehensive experiments on a number of reallife text datasets of news articles and web pages show that the proposed methods outperform the stateoftheart baselines on objective evaluation metrics



stefano  v albrecht and subramanian  ramamoorthy 2016 exploiting causality for selective belief filtering in dynamic bayesian networks volume 55 pages 11351178

dynamic bayesian networks dbns are a general model for stochastic processes with partially observed states belief filtering in dbns is the task of inferring the belief state ie the probability distribution over process states based on incomplete and noisy observations this can be a hard problem in complex processes with large state spaces in this article we explore the idea of accelerating the filtering task by automatically exploiting causality in the process we consider a specific type of causal relation called passivity which pertains to how state variables cause changes in other variables we present the passivitybased selective belief filtering psbf method which maintains a factored belief representation and exploits passivity to perform selective updates over the belief factors psbf produces exact belief states under certain assumptions and approximate belief states otherwise where the approximation error is bounded by the degree of uncertainty in the process we show empirically in synthetic processes with varying sizes and degrees of passivity that psbf is faster than several alternative methods while achieving competitive accuracy furthermore we demonstrate how passivity occurs naturally in a complex system such as a multirobot warehouse and how psbf can exploit this to accelerate the filtering task
