p  van beek and  d  w manchak 1996 the design and experimental analysis of algorithms for temporal reasoning volume 4 pages 118

many applications  from planning and scheduling to    problems in molecular biology  rely heavily on a temporal reasoning    component  in this paper we discuss the design and empirical    analysis of algorithms for a temporal reasoning system based on    allens influential intervalbased framework for representing temporal    information  at the core of the system are algorithms for determining    whether the temporal information is consistent and if so finding    one or more scenarios that are consistent with the temporal    information  two important algorithms for these tasks are a path    consistency algorithm and a backtracking algorithm  for the path    consistency algorithm we develop techniques that can result in up to    a tenfold speedup over an already highly optimized implementation    for the backtracking algorithm we develop variable and value ordering    heuristics that are shown empirically to dramatically improve the    performance of the algorithm  as well we show that a previously    suggested reformulation of the backtracking search problem can reduce    the time and space requirements of the backtracking search  taken    together the techniques we develop allow a temporal reasoning    component to solve problems that are of practical size



g  brewka 1996 wellfounded semantics for extended logic programs with dynamic preferences volume 4 pages 1936

the paper describes an extension of wellfounded    semantics for logic programs with two types of negation in this    extension information about preferences between rules can be expressed    in the logical language and derived dynamically this is achieved by    using a reserved predicate symbol and a naming technique conflicts    among rules are resolved whenever possible on the basis of derived    preference information the wellfounded conclusions of prioritized    logic programs can be computed in polynomial time a legal reasoning    example illustrates the usefulness of the approach



a  l delcher  a  j grove  s  kasif and  j  pearl 1996 logarithmictime updates and queries in probabilistic networks volume 4 pages 3759

traditional databases commonly support efficient query and    update procedures that operate in time which is sublinear in the size    of the database  our goal in this paper is to take a first step    toward dynamic reasoning in probabilistic databases with comparable    efficiency  we propose a dynamic data structure that supports    efficient algorithms for updating and querying singly connected    bayesian networks  in the conventional algorithm new evidence is    absorbed in o1 time and queries are processed in time on where n    is the size of the network  we propose an algorithm which after a    preprocessing phase allows us to answer queries in time olog n at    the expense of olog n time per evidence absorption  the usefulness    of sublinear processing time manifests itself in applications    requiring near realtime response over large probabilistic    databases we briefly discuss a potential application of dynamic    probabilistic reasoning in computational biology



l  k saul  t  jaakkola and  m  i jordan 1996 mean field theory for sigmoid belief networks volume 4 pages 6176

we develop a mean field theory for sigmoid belief networks    based on ideas from statistical mechanics  our mean field theory    provides a tractable approximation to the true probability    distribution in these networks it also yields a lower bound on the    likelihood of evidence  we demonstrate the utility of this framework    on a benchmark problem in statistical pattern recognitionthe    classification of handwritten digits



j  r quinlan 1996 improved use of continuous attributes in c45 volume 4 pages 7790

a reported weakness of c45 in domains with continuous    attributes is addressed by modifying the formation and evaluation of    tests on continuous attributes  an mdlinspired penalty is applied to    such tests eliminating some of them from consideration and altering    the relative desirability of all tests  empirical trials show that    the modifications lead to smaller decision trees with higher    predictive accuracies  results also confirm that a new version of    c45 incorporating these changes is superior to recent approaches that    use global discretization and that construct small trees with    multiinterval splits



t  hogg 1996 quantum computing and phase transitions in combinatorial search volume 4 pages 91128

we introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some np search problems on average this is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices this quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism furthermore empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior and at the same location as seen in many previously studied classical search methods specifically difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems 



d  a cohn  z  ghahramani and  m  i jordan 1996 active learning with statistical models volume 4 pages 129145

for many types of machine learning algorithms one can    compute the statistically optimal way to select training data  in    this paper we review how optimal data selection techniques have been    used with feedforward neural networks  we then show how the same    principles may be used to select data for two alternative    statisticallybased learning architectures mixtures of gaussians and    locally weighted regression  while the techniques for neural networks    are computationally expensive and approximate the techniques for    mixtures of gaussians and locally weighted regression are both    efficient and accurate  empirically we observe that the optimality    criterion sharply decreases the number of training examples the    learner needs in order to achieve good performance



d  fisher 1996 iterative optimization and simplification of hierarchical clusterings volume 4 pages 147178

clustering is often used for discovering structure in data    clustering systems differ in the objective function used to evaluate    clustering quality and the control strategy used to search the space    of clusterings ideally the search strategy should consistently    construct clusterings of high quality but be computationally    inexpensive as well in general we cannot have it both ways but we    can partition the search so that a system inexpensively constructs a    tentative clustering for initial examination followed by iterative    optimization which continues to search in background for improved    clusterings given this motivation we evaluate an inexpensive    strategy for creating initial clusterings coupled with several    control strategies for iterative optimization each of which    repeatedly modifies an initial clustering in search of a better    one one of these methods appears novel as an iterative optimization    strategy in clustering contexts once a clustering has been    constructed it is judged by analysts  often according to    taskspecific criteria several authors have abstracted these criteria    and posited a generic performance task akin to pattern completion    where the error rate over completed patterns is used to externally    judge clustering utility given this performance task we adapt    resamplingbased pruning strategies used by supervised learning    systems to the task of simplifying hierarchical clusterings thus    promising to ease postclustering analysis finally we propose a    number of objective functions based on attributeselection measures    for decisiontree induction that might perform well on the error rate    and simplicity dimensions



e  marchiori 1996 practical methods for proving termination of general logic programs volume 4 pages 179208

termination of logic programs with negated body atoms here    called general logic programs is an important topic one reason is    that many computational mechanisms used to process negated atoms like    clarks negation as failure and chans constructive negation are    based on termination conditions  this paper introduces a methodology    for proving termination of general logic programs wrt the prolog    selection rule  the idea is to distinguish parts of the program    depending on whether or not their termination depends on the selection    rule to this end the notions of low weakly up and upacceptable    program are introduced  we use these notions to develop a methodology    for proving termination of general logic programs and show how    interesting problems in nonmonotonic reasoning can be formalized and    implemented by means of terminating general logic programs



t  walsh 1996 a divergence critic for inductive proof volume 4 pages 209235

inductive theorem provers often diverge this paper    describes a simple critic a computer program which monitors the    construction of inductive proofs attempting to identify diverging    proof attempts divergence is recognized by means of a difference    matching procedure the critic then proposes lemmas and    generalizations which ripple these differences away so that the    proof can go through without divergence the critic enables the    theorem prover spike to prove many theorems completely automatically    from the definitions alone



l  p kaelbling  m  l littman and  a  w moore 1996 reinforcement learning  a survey volume 4 pages 237285

this paper surveys the field of reinforcement learning from    a computerscience perspective it is written to be accessible to    researchers familiar with machine learning  both the historical basis    of the field and a broad selection of current work are summarized    reinforcement learning is the problem faced by an agent that learns    behavior through trialanderror interactions with a dynamic    environment  the work described here has a resemblance to work in    psychology but differs considerably in the details and in the use of    the word reinforcement  the paper discusses central issues of    reinforcement learning including trading off exploration and    exploitation establishing the foundations of the field via markov    decision theory learning from delayed reinforcement constructing    empirical models to accelerate learning making use of generalization    and hierarchy and coping with hidden state  it concludes with a    survey of some implemented systems and an assessment of the practical    utility of current methods for reinforcement learning



l  pryor and  g  collins 1996 planning for contingencies a decisionbased approach volume 4 pages 287339

a fundamental assumption made by classical ai planners is    that there is no uncertainty in the world the planner has full    knowledge of the conditions under which the plan will be executed and    the outcome of every action is fully predictable  these planners    cannot therefore construct contingency plans ie plans in which    different actions are performed in different circumstances  in this    paper we discuss some issues that arise in the representation and    construction of contingency plans and describe cassandra a    partialorder contingency planner  cassandra uses explicit    decisionsteps that enable the agent executing the plan to decide    which plan branch to follow  the decisionsteps in a plan result in    subgoals to acquire knowledge which are planned for in the same way    as any other subgoals  cassandra thus distinguishes the process of    gathering information from the process of making decisions  the    explicit representation of decisions in cassandra allows a coherent    approach to the problems of contingent planning and provides a solid    base for extensions such as the use of different decisionmaking    procedures



s  h nienhuyscheng 1996 least generalizations and greatest specializations of sets of clauses volume 4 pages 341363

the main operations in inductive logic programming ilp are    generalization and specialization which only make sense in a    generality order  in ilp the three most important generality orders    are subsumption implication and implication relative to background    knowledge  the two languages used most often are languages of clauses    and languages of only horn clauses this gives a total of six    different ordered languages  in this paper we give a systematic    treatment of the existence or nonexistence of least generalizations    and greatest specializations of finite sets of clauses in each of    these six ordered sets  we survey results already obtained by others    and also contribute some answers of our own        our main new results are firstly the existence of a computable least    generalization under implication of every finite set of clauses    containing at least one nontautologous functionfree clause among    other not necessarily functionfree clauses  secondly we show that    such a least generalization need not exist under relative implication    not even if both the set that is to be generalized and the background    knowledge are functionfree  thirdly we give a complete discussion    of existence and nonexistence of greatest specializations in each of    the six ordered languages



j  gratch and  s  chien 1996 adaptive problemsolving for largescale scheduling problems a case study volume 4 pages 365396

although most scheduling problems are nphard domain    specific techniques perform well in practice but are quite expensive    to construct  in adaptive problemsolving solving domain specific    knowledge is acquired automatically for a general problem solver with    a flexible control architecture  in this approach a learning system    explores a space of possible heuristic methods for one wellsuited to    the eccentricities of the given domain and problem distribution  in    this article we discuss an application of the approach to scheduling    satellite communications  using problem distributions based on actual    mission requirements our approach identifies strategies that not only    decrease the amount of cpu time required to produce schedules but    also increase the percentage of problems that are solvable within    computational resource limitations



g  i webb 1996 further experimental evidence against the utility of occams razor volume 4 pages 397417

this paper presents new experimental evidence against the    utility of occams razor  asystematic procedure is presented for    postprocessing decision trees produced by c45  this procedure was    derived by rejecting occams razor and instead attending to the    assumption that similar objects are likely to belong to the same    class  it increases a decision trees complexity without altering the    performance of that tree on the training data from which it is    inferred  the resulting more complex decision trees are demonstrated    to have on average for a variety of common learning tasks higher    predictive accuracy than the less complex original decision trees    this result raises considerable doubt about the utility of occams    razor as it is commonly applied in modern machine learning



s  bhansali  g  a kramer and  t  j hoar 1996 a principled approach towards symbolic geometric constraint satisfaction volume 4 pages 419443

an important problem in geometric reasoning is to find the    configuration of a collection of geometric bodies so as to satisfy a    set of given constraints recently it has been suggested that this    problem can be solved efficiently by symbolically reasoning about    geometry this approach called degrees of freedom analysis employs a    set of specialized routines called plan fragments that specify how to    change the configuration of a set of bodies to satisfy a new    constraint while preserving existing constraints a potential    drawback which limits the scalability of this approach is concerned    with the difficulty of writing plan fragments  in this paper we    address this limitation by showing how these plan fragments can be    automatically synthesized using first principles about geometric    bodies actions and topology



p  tadepalli and  b  k natarajan 1996 a formal framework for speedup learning from problems and solutions volume 4 pages 445475

speedup learning seeks to improve the computational    efficiency of problem solving with experience in this paper we    develop a formal framework for learning efficient problem solving from    random problems and their solutions we apply this framework to two    different representations of learned knowledge namely control rules    and macrooperators and prove theorems that identify sufficient    conditions for learning in each representation our proofs are    constructive in that they are accompanied with learning algorithms     our framework captures both empirical and explanationbased     speedup learning in a unified fashion  we illustrate our framework    with implementations in two domains symbolic integration and eight    puzzle this work integrates many strands of experimental and    theoretical work in machine learning including empirical learning of    control rules macrooperator learning explanationbased learning    ebl and probably approximately correct pac learning



r  i brafman and  m  tennenholtz 1996 on partially controlled multiagent systems volume 4 pages 477507

motivated by the control theoretic distinction between    controllable and uncontrollable events we distinguish between two    types of agents within a multiagent system controllable agents    which are directly controlled by the systems designer and    uncontrollable agents which are not under the designers direct    control we refer to such systems as partially controlled multiagent    systems and we investigate how one might influence the behavior of    the uncontrolled agents through appropriate design of the controlled    agents in particular we wish to understand which problems are    naturally described in these terms what methods can be applied to    influence the uncontrollable agents the effectiveness of such    methods and whether similar methods work across different    domains using a gametheoretic framework this paper studies the    design of partially controlled multiagent systems in two contexts in    one context the uncontrollable agents are expected utility    maximizers while in the other they are reinforcement learners we    suggest different techniques for controlling agents behavior in each    domain assess their success and examine their relationship
