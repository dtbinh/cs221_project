a  e howe and  e  dahlman 2002 a critical assessment of benchmark comparison in planning volume 17 pages 133

recent trends in planning research have led to empirical     comparison becoming commonplace the field has started to settle into    a methodology for such comparisons which for obvious practical    reasons requires running a subset of planners on a subset of    problems  in this paper we characterize the methodology and    examine eight implicit assumptions about the problems planners and    metrics used in many of these comparisons the problem assumptions    are pr1 the performance of a general purpose planner should not be    penalizedbiased if executed on a sampling of problems and domains    pr2 minor syntactic differences in representation do not affect    performance and pr3 problems should be solvable by strips capable    planners unless they require adl the planner assumptions are pl1    the latest version of a planner is the best one to use pl2 default    parameter settings approximate good performance and pl3 time    cutoffs do not unduly bias outcome the metrics assumptions are    m1 performance degrades similarly for each planner when run on    degraded runtime environments eg machine platform and m2 the    number of plan steps distinguishes performance we find that most of    these assumptions are not supported empirically in particular that    planners are affected differently by these assumptions we conclude    with a call to the community to devote research resources to    improving the state of the practice and especially to enhancing the    available benchmark problems



r  barzilay and  n  elhadad 2002 inferring strategies for sentence ordering in multidocument news summarization volume 17 pages 3555

the problem of organizing information for multidocument    summarization so that the generated summary is coherent has received    relatively little attention  while sentence ordering for single    document summarization can be determined from the ordering of    sentences in the input article this is not the case for multidocument    summarization where summary sentences may be drawn from different    input articles in this paper we propose a methodology for studying    the properties of ordering information in the news genre and describe    experiments done on a corpus of multiple acceptable orderings we    developed for the task based on these experiments we implemented a    strategy for ordering information that combines constraints from    chronological order of events and topical relatedness  evaluation of    our augmented algorithm shows a significant improvement of the    ordering over two baseline strategies



j  y halpern and  r  pucella 2002 a logic for reasoning about upper probabilities volume 17 pages 5781

we present a propositional logic to reason about the uncertainty of events where the uncertainty is modeled by a set of probability measures assigning an interval of probability to each event we give a sound and complete axiomatization for the logic and show that the satisfiability problem is npcomplete no harder than satisfiability for propositional logic



g  a kaminka  d  v pynadath and  m  tambe 2002 monitoring teams by overhearing a multiagent planrecognition approach volume 17 pages 83135

recent years are seeing an increasing need for online    monitoring of teams of cooperating agents eg for visualization or    performance tracking however in monitoring deployed teams we often    cannot rely on the agents to always communicate their state to the    monitoring system this paper presents a nonintrusive approach to    monitoring by overhearing where the monitored teams state is    inferred via planrecognition from teammembers routine    communications exchanged as part of their coordinated task execution    and observed overheard by the monitoring system key challenges in    this approach include the demanding runtime requirements of    monitoring the scarceness of observations increasing monitoring    uncertainty and the need to scaleup monitoring to address    potentially large teams to address these we present a set of    complementary novel techniques exploiting knowledge of the social    structures and procedures in the monitored team i an efficient    probabilistic planrecognition algorithm wellsuited for processing    communications as observations ii an approach to exploiting    knowledge of the teams social behavior to predict future observations    during execution reducing monitoring uncertainty and iii    monitoring algorithms that trade expressivity for scalability    representing only certain useful monitoring hypotheses but allowing    for any number of agents and their different activities to be    represented in a single coherent entity we present an empirical    evaluation of these techniques in combination and apart in    monitoring a deployed team of agents running on machines physically    distributed across the country and engaged in complex dynamic task    execution we also compare the performance of these techniques to    human expert and novice monitors and show that the techniques    presented are capable of monitoring at humanexpert levels despite    the difficulty of the task



r  nock 2002 inducing interpretable voting classifiers without trading accuracy for simplicity theoretical results approximation algorithms volume 17 pages 137170

recent advances in the study of voting classification    algorithms have brought empirical and theoretical results clearly    showing the discrimination power of ensemble classifiers it has been    previously argued that the search of this classification power in the    design of the algorithms has marginalized the need to obtain    interpretable classifiers therefore the question of whether one    might have to dispense with interpretability in order to keep    classification strength is being raised in a growing number of machine    learning or data mining papers the purpose of this paper is to study    both theoretically and empirically the problem first we provide    numerous results giving insight into the hardness of the    simplicityaccuracy tradeoff for voting classifiers then we provide    an efficient topdown and prune induction heuristic widc mainly    derived from recent results on the weak learning and boosting    frameworks  it is to our knowledge the first attempt to build a    voting classifier as a base formula using the weak learning framework    the one which was previously highly successful for decision tree    induction and not the strong learning framework as usual for such    classifiers with boostinglike approaches while it uses a wellknown    induction scheme previously successful in other classes of concept    representations thus making it easy to implement and compare widc    also relies on recent or new results we give about particular cases of    boosting known as partition boosting and ranking loss    boosting experimental results on thirtyone domains most of which    readily available tend to display the ability of widc to produce    small accurate and interpretable decision committees



p  scerri  d  v pynadath and  m  tambe 2002 towards adjustable autonomy for the real world volume 17 pages 171228

adjustable autonomy refers to entities dynamically varying    their own autonomy transferring decisionmaking control to other    entities typically agents transferring control to human users in key    situations  determining whether and when such transfersofcontrol    should occur is arguably the fundamental research problem in    adjustable autonomy previous work has investigated various approaches    to addressing this problem but has often focused on individual    agenthuman interactions  unfortunately domains requiring    collaboration between teams of agents and humans reveal two key    shortcomings of these previous approaches first these approaches use    rigid oneshot transfers of control that can result in unacceptable    coordination failures in multiagent settings  second they ignore    costs eg in terms of time delays or effects on actions to an    agents team due to such transfersofcontrol     to remedy these problems this article presents a novel approach to    adjustable autonomy based on the notion of a transferofcontrol    strategy  a transferofcontrol strategy consists of a conditional    sequence of two types of actions i actions to transfer    decisionmaking control eg from an agent to a user or vice versa    and ii actions to change an agents prespecified coordination    constraints with team members aimed at minimizing miscoordination    costs the goal is for highquality individual decisions to be made    with minimal disruption to the coordination of the team  we present a    mathematical model of transferofcontrol strategies the model guides    and informs the operationalization of the strategies using markov    decision processes which select an optimal strategy given an    uncertain environment and costs to the individuals and teams the    approach has been carefully evaluated including via its use in a    realworld deployed multiagent system that assists a research group    in its daily activities



a  darwiche and  p  marquis 2002 a knowledge compilation map volume 17 pages 229264




2006 ijcaijair best paper prize

we propose a perspective on knowledge compilation which    calls for analyzing different compilation approaches according to two    key dimensions the succinctness of the target compilation language    and the class of queries and transformations that the language    supports in polytime we then provide a knowledge compilation map    which analyzes a large number of existing target compilation languages    according to their succinctness and their polytime transformations and    queries we argue that such analysis is necessary for placing new    compilation approaches within the context of existing ones we also go    beyond classical flat target compilation languages based on cnf and    dnf and consider a richer nested class based on directed acyclic    graphs such as obdds which we show to include a relatively large    number of target compilation languages



h  chan and  a  darwiche 2002 when do numbers really matter volume 17 pages 265287

common wisdom has it that small distinctions in the    probabilities parameters quantifying a belief network do not matter    much for the results of probabilistic queries yet one can develop    realistic scenarios under which small variations in network parameters    can lead to significant changes in computed queries a pending    theoretical question is then to analytically characterize parameter    changes that do or do not matter in this paper we study the    sensitivity of probabilistic queries to changes in network parameters    and prove some tight bounds on the impact that such parameters can    have on queries our analytic results pinpoint some interesting    situations under which parameter changes do or do not matter these    results are important for knowledge engineers as they help them    identify influential network parameters they also help explain some    of the previous experimental results and observations with regards to    network robustness against parameter changes



r  bod 2002 a unified model of structural organization in language and music volume 17 pages 289308

is there a general model that can predict the perceived    phrase structure in language and music while it is usually assumed    that humans have separate faculties for language and music this work    focuses on the commonalities rather than on the differences between    these modalities aiming at finding a deeper faculty our key idea    is that the perceptual system strives for the simplest structure the    simplicity principle but in doing so it is biased by the    likelihood of previous structures the likelihood principle we    present a series of dataoriented parsing dop models that combine    these two principles and that are tested on the penn treebank and the    essen folksong collection our experiments show that 1 a combination    of the two principles outperforms the use of either of them and 2    exactly the same model with the same parameter setting achieves    maximum accuracy for both language and music we argue that our    results suggest an interesting parallel between linguistic and musical    structuring



y  gao and  j  culberson 2002 an analysis of phase transition in nk landscapes volume 17 pages 309332

in this paper we analyze the decision version of the nk    landscape model from the perspective of threshold phenomena and phase    transitions under two random distributions the uniform probability    model and the fixed ratio model for the uniform probability model we    prove that the phase transition is easy in the sense that there is a    polynomial algorithm that can solve a random instance of the problem    with the probability asymptotic to 1 as the problem size tends to    infinity for the fixed ratio model we establish several upper bounds    for the solubility threshold and prove that random instances with    parameters above these upper bounds can be solved polynomially this    together with our empirical study for random instances generated below    and in the phase transition region suggests that the phase transition    of the fixed ratio model is also easy



a  alani and  m  deriche 2002 a new technique for combining multiple classifiers using the dempstershafer theory of evidence volume 17 pages 333361

this paper presents a new classifier combination    technique based on the dempstershafer theory of evidence the    dempstershafer theory of evidence is a powerful method for    combining measures of evidence from different classifiers however    since each of the available methods that estimates the evidence of    classifiers has its own limitations we propose here a new    implementation which adapts to training data so that the overall    mean square error is minimized the proposed technique is shown to    outperform most available classifier combination methods when    tested on three different classification problems
much work in ai deals with the selection of proper actions    in a given known or unknown environment however the way to select    a proper action when facing other agents is quite unclear most work    in ai adopts classical gametheoretic equilibrium analysis to predict    agent behavior in such settings this approach however does not    provide us with any guarantee for the agent in this paper we    introduce competitive safety analysis this approach bridges the gap    between the desired normative ai approach where a strategy should be    selected in order to guarantee a desired payoff and equilibrium    analysis  we show that a safety level strategy is able to guarantee    the value obtained in a nash equilibrium in several classical    computer science settings  then we discuss the concept of    competitive safety strategies and illustrate its use in a    decentralized load balancing setting typical to network problems in    particular we show that when we have many agents it is possible to    guarantee an expected payoff which is a factor of 89 of the payoff    obtained in a nash equilibrium  our discussion of competitive safety    analysis for decentralized load balancing is further developed to deal    with many communication links and arbitrary speeds  finally we    discuss the extension of the above concepts to bayesian games and    illustrate their use in a basic auctions setup



a  fern  r  givan and  j  m siskind 2002 specifictogeneral learning for temporal events with application to learning event definitions from video volume 17 pages 379449

we develop analyze and evaluate a novel supervised    specifictogeneral learner for a simple temporal logic and use the    resulting algorithm to learn visual event definitions from video    sequences  first we introduce a simple propositional temporal    eventdescription language called ama that is sufficiently expressive    to represent many events yet sufficiently restrictive to support    learning  we then give algorithms along with lower and upper    complexity bounds for the subsumption and generalization problems for    ama formulas we present a positiveexamplesonly specifictogeneral    learning method based on these algorithms we also present a    polynomialtimecomputable syntactic subsumption test that    implies semantic subsumption without being equivalent to it a    generalization algorithm based on syntactic subsumption can be used in    place of semantic generalization to improve the asymptotic complexity    of the resulting learning algorithm finally we apply this algorithm    to the task of learning relational event definitions from video and    show that it yields definitions that are competitive with handcoded    ones



h  h bui  s  venkatesh and  g  west 2002 policy recognition in the abstract hidden markov model volume 17 pages 451499

in this paper we present a method for recognising an    agents behaviour in dynamic noisy uncertain domains and across    multiple levels of abstraction  we term this problem online plan    recognition under uncertainty and view it generally as probabilistic    inference on the stochastic process representing the execution of the    agents plan our contributions in this paper are twofold  in terms    of probabilistic inference we introduce the abstract hidden markov    model ahmm a novel type of stochastic processes provide its    dynamic bayesian network dbn structure and analyse the properties of    this network  we then describe an application of the    raoblackwellised particle filter to the ahmm which allows us to    construct an efficient hybrid inference method for this model  in    terms of plan recognition we propose a novel plan recognition    framework based on the ahmm as the plan execution model  the    raoblackwellised hybrid inference for ahmm can take advantage of the    independence properties inherent in a model of plan execution leading    to an algorithm for online probabilistic plan recognition that scales    well with the number of levels in the plan hierarchy  this    illustrates that while stochastic models for plan execution can be    complex they exhibit special structures which if exploited can lead    to efficient plan recognition algorithms  we demonstrate the    usefulness of the ahmm framework via a behaviour recognition system in    a complex spatial environment using distributed video surveillance    data



d  gamberger and  n  lavrac 2002 expertguided subgroup discovery methodology and application volume 17 pages 501527

this paper presents an approach to expertguided subgroup    discovery  the main step of the subgroup discovery process the    induction of subgroup descriptions is performed by a heuristic beam    search algorithm using a novel parametrized definition of rule    quality which is analyzed in detail  the other important steps of the    proposed subgroup discovery process are the detection of statistically    significant properties of selected subgroups and subgroup    visualization statistically significant properties are used to enrich    the descriptions of induced subgroups while the visualization shows    subgroup properties in the form of distributions of the numbers of    examples in the subgroups the approach is illustrated by the results    obtained for a medical problem of early detection of patient risk    groups
