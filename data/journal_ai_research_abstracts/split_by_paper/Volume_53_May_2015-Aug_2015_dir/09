timothy  a mann shie  mannor and doina  precup 2015 approximate value iteration with temporally extended actions volume 53 pages 375438

temporally extended actions have proven useful for reinforcement learning but their duration also makes them valuable for efficient planning the options framework provides a concrete way to implement and reason about temporally extended actions existing literature has demonstrated the value of planning with options empirically but there is a lack of theoretical analysis formalizing when planning with options is more efficient than planning with primitive actions we provide a general analysis of the convergence rate of a popular approximate value iteration avi algorithm called fitted value iteration fvi with options our analysis reveals that longer duration options and a pessimistic estimate of the value function both lead to faster convergence furthermore options can improve convergence even when they are suboptimal and sparsely distributed throughout the statespace next we consider the problem of generating useful options for planning based on a subset of landmark states this suggests a new algorithm landmarkbased avi lavi that represents the value function only at the landmark states we analyze both fvi and lavi using the proposed landmarkbased options and compare the two algorithms our experimental results in three different domains demonstrate the key properties from the analysis our theoretical and experimental results demonstrate that options can play an important role in avi by decreasing approximation error and inducing fast convergence

