g  erkan and  d  r radev 2004 lexrank graphbased lexical centrality as salience in text summarization volume 22 pages 457479

we introduce a stochastic graphbased method for computing relative importance of textual units for natural language processing we test the technique on the problem of text summarization ts extractive ts relies on the concept of sentence salience to identify the most important sentences in a document or set of documents salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudosentence we consider a new approach lexrank for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences in this model a connectivity matrix based on intrasentence cosine similarity is used as the adjacency matrix of the graph representation of sentences our system based on lexrank ranked in first place in more than one task in the recent duc 2004 evaluation in this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier duc evaluations we discuss several methods to compute centrality using the similarity graph the results show that degreebased methods including lexrank outperform both centroidbased methods and other systems participating in duc in most of the cases furthermore the lexrank with threshold method outperforms the other degreebased techniques including continuous lexrank we also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents

