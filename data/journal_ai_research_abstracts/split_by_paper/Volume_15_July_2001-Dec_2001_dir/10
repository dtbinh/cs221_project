j  baxter and  p  l bartlett 2001 infinitehorizon policygradient estimation volume 15 pages 319350

gradientbased approaches to direct policy search in    reinforcement learning have received much recent attention as a means    to solve problems of partial observability and to avoid some of the    problems associated with policy degradation in valuefunction methods    in this paper we introduce gpomdp a simulationbased algorithm for    generating a biased estimate of the gradient of the average reward in    partially observable markov decision processes pomdps controlled by    parameterized stochastic policies a similar algorithm was proposed by    kimura et al 1995 the algorithms chief advantages are that it    requires storage of only twice the number of policy parameters uses    one free beta which has a natural interpretation in terms of    biasvariance tradeoff and requires no knowledge of the underlying    state we prove convergence of gpomdp and show how the correct choice    of the parameter beta is related to the mixing time of the    controlled pomdp we briefly describe extensions of gpomdp to    controlled markov chains continuous state observation and control    spaces multipleagents higherorder derivatives and a version for    training stochastic policies with internal states  in a companion    paper baxter et al this volume we show how the gradient estimates    generated by gpomdp can be used in both a traditional stochastic    gradient algorithm and a conjugategradient procedure to find local    optima of the average reward

