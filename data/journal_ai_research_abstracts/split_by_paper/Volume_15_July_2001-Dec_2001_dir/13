t  sato and  y  kameya 2001 parameter learning of logic programs for symbolicstatistical modeling volume 15 pages 391454

we propose a logicalmathematical framework for statistical    parameter learning of parameterized logic programs ie  definite    clause programs containing probabilistic facts with a parameterized    distribution  it extends the traditional least herbrand model    semantics in logic programming to distribution semantics possible    world semantics with a probability distribution which is    unconditionally applicable to arbitrary logic programs including ones    for hmms pcfgs and bayesian networks        we also propose a new em algorithm the graphical em algorithm that    runs for a class of parameterized logic programs representing    sequential decision processes where each decision is exclusive and    independent  it runs on a new data structure called support graphs    describing the logical relationship between observations and their    explanations and learns parameters by computing inside and outside    probability generalized for logic programs           the complexity analysis shows that when combined with oldt search for    all explanations for observations the graphical em algorithm despite    its generality has the same time complexity as existing em    algorithms ie the baumwelch algorithm for hmms the insideoutside    algorithm for pcfgs and the one for singly connected bayesian    networks that have been developed independently in each research    field  learning experiments with pcfgs using two corpora of moderate    size indicate that the graphical em algorithm can significantly    outperform the insideoutside algorithm

