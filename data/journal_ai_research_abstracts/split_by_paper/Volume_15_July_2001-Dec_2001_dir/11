j  baxter  p  l bartlett and  l  weaver 2001 experiments with infinitehorizon policygradient estimation volume 15 pages 351381

in this paper we present algorithms that perform gradient    ascent of the average reward in a partially observable markov decision    process pomdp  these algorithms are based on gpomdp an algorithm    introduced in a companion paper baxter  bartlett this volume    which computes biased estimates of the performance gradient in pomdps    the algorithms chief advantages are that it uses only one free    parameter beta which has a natural interpretation in terms of    biasvariance tradeoff it requires no knowledge of the underlying    state and it can be applied to infinite state control and    observation spaces  we show how the gradient estimates produced by    gpomdp can be used to perform gradient ascent both with a traditional    stochasticgradient algorithm and with an algorithm based on    conjugategradients that utilizes gradient information to bracket    maxima in line searches experimental results are presented    illustrating both the theoretical results of baxter  bartlett this    volume on a toy problem and practical aspects of the algorithms on a    number of more realistic problems

