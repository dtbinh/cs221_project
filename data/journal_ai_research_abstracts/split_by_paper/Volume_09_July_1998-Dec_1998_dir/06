a  ruiz  p  e lopezdeteruel and  m  c garrido 1998 probabilistic inference from arbitrary uncertainty using mixtures of factorized generalized gaussians volume 9 pages 167217

this paper presents a general and efficient framework for    probabilistic inference and learning from arbitrary uncertain    information it exploits the calculation properties of finite mixture    models conjugate families and factorization both the joint    probability density of the variables and the likelihood function of    the objective or subjective observation are approximated by a    special mixture model in such a way that any desired conditional    distribution can be directly obtained without numerical    integration we have developed an extended version of the expectation    maximization em algorithm to estimate the parameters of mixture    models from uncertain training examples indirect observations as a    consequence any piece of exact or uncertain information about both    input and output values is consistently handled in the inference and    learning stages this ability extremely useful in certain situations    is not found in most alternative methods the proposed framework is    formally justified from standard probabilistic principles and    illustrative examples are provided in the fields of nonparametric    pattern classification nonlinear regression and pattern    completion finally experiments on a real application and comparative    results over standard databases provide empirical evidence of the    utility of the method in a wide range of applications

