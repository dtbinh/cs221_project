



p  stone  r  e schapire  m  l littman  j  a csirik and d  mcallester 2003 decisiontheoretic bidding based on learned density models in simultaneous interacting auctions volume 19 pages 209242



auctions are becoming an increasingly popular method for transacting business especially over the internet  this article presents a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for interacting goods  a core component of our approach learns a model of the empirical price dynamics based on past data and uses the model to analytically calculate to the greatest extent possible optimal bids  we introduce a new and general boostingbased algorithm for conditional density estimation problems of this kind ie supervised learning problems in which the goal is to estimate the entire conditional distribution of the realvalued label  this approach is fully implemented as attac2001 a topscoring agent in the second trading agent competition tac01 we present experiments demonstrating the effectiveness of our boostingbased price predictor relative to several reasonable alternatives





m  e pollack  d  joslin and  m  paolucci 1997 flaw selection strategies for partialorder planning volume 6 pages 223262



several recent studies have compared the relative efficiency    of alternative flaw selection strategies for partialorder causal link    pocl planning  we review this literature and present new    experimental results that generalize the earlier work and explain some    of the discrepancies in it  in particular we describe the leastcost    flaw repair lcfr strategy developed and analyzed by joslin and    pollack 1994 and compare it with other strategies including    gerevini and schuberts 1996 zlifo strategy  lcfr and zlifo make    very different and apparently conflicting claims about the most    effective way to reduce searchspace size in pocl planning  we    resolve this conflict arguing that much of the benefit that gerevini    and schubert ascribe to the lifo component of their zlifo strategy is    better attributed to other causes  we show that for many problems a    strategy that combines leastcost flaw selection with the delay of    separable threats will be effective in reducing searchspace size and    will do so without excessive computational overhead  although such a    strategy thus provides a good default we also show that certain    domain characteristics may reduce its effectiveness



a pattern database pdb is a heuristic function implemented as a lookup table that stores the lengths of optimal solutions for subproblem instances standard pdbs have a distinct entry in the table for each subproblem instance in this paper we investigate compressing pdbs by merging several entries into one thereby allowing the use of pdbs that exceed available memory in their uncompressed form we introduce a number of methods for determining which entries to merge and discuss their relative merits these vary from domainindependent approaches that allow any set of entries in the pdb to be merged to more intelligent methods that take into account the structure of the problem the choice of the best compression method is based on domaindependent attributes we present experimental results on a number of combinatorial problems including the fourpeg towers of hanoi problem the slidingtile puzzles and the topspin puzzle for the towers of hanoi we show that the search time can be reduced by up to three orders of magnitude by using compressed pdbs compared to uncompressed pdbs of the same size more modest improvements were observed for the other domains









r  mateescu k  kask v  gogate and r  dechter 2010 joingraph propagation algorithms volume 37 pages 279328



the paper investigates parameterized approximate messagepassing schemes that are based on bounded inference and are inspired by pearls belief propagation algorithm bp we start with the bounded inference miniclustering algorithm and then move to the iterative scheme called iterative joingraph propagation ijgp that combines both iteration and bounded inference algorithm ijgp belongs to the class of generalized belief propagation algorithms a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of miniclustering and belief propagation as well as a number of other stateoftheart algorithms on several classes of networks we also provide insight into the accuracy of iterative bp and ijgp by relating these algorithms to well known classes of constraint propagation schemes





n  v chawla and   karakoulas 2005 learning from labeled and unlabeled data an empirical study across techniques and domains volume 23 pages 331366



there has been increased interest in devising learning techniques that combine unlabeled data with labeled data  ie semisupervised learning however to the best of our knowledge no study has been performed across various techniques and different types and amounts of labeled and unlabeled data moreover most of the published work on semisupervised learning techniques assumes that the labeled and unlabeled data come from the same distribution it is possible for the labeling process to be associated with a selection bias such that the distributions of data points in the labeled and unlabeled sets are different not correcting for such bias can result in biased function approximation with potentially poor performance in this paper we present an empirical study of various semisupervised learning techniques on a variety of datasets we attempt to answer various questions such as the effect of independence or relevance amongst features the effect of the size of the labeled and unlabeled sets and the effect of noise we also investigate the impact of sampleselection bias on the semisupervised learning techniques under study and implement a bivariate probit technique particularly designed to correct for such bias





j  hoffmann  j  porteous and  l  sebastia 2004 ordered landmarks in planning volume 22 pages 215278



many known planning tasks have inherent constraints concerning the best order in which to achieve the goals a number of research efforts have been made to detect such constraints and to use them for guiding search in the hope of speeding up the planning process     we go beyond the previous approaches by considering ordering constraints not only over the toplevel goals but also over the subgoals that will necessarily arise during planning landmarks are facts that must be true at some point in every valid solution plan  we extend koehler and hoffmanns definition of reasonable orders between top level goals to the more general case of landmarks we show how landmarks can be found how their reasonable orders can be approximated and how this information can be used to decompose a given planning task into several smaller subtasks our methodology is completely domain and plannerindependent the implementation demonstrates that the approach can yield significant runtime performance improvements when used as a control loop around stateoftheart suboptimal planning systems as exemplified by ff and lpg



active learners alleviate the burden of labeling large amounts of data by detecting and asking the user to label only the most informative examples in the domain  we focus here on active learning for multiview domains in which there are several disjoint subsets of features views each of which is sufficient to learn the target concept  in this paper we make several contributions first we introduce cotesting which is the first approach to multiview active learning  second we extend the multiview learning framework by also exploiting weak views which are adequate only for learning a concept that is more generalspecific than the target concept  finally we empirically show that cotesting outperforms existing active learners on a variety of real world domains such as wrapper induction web page classification advertisement removal and discourse tree parsing









v  bayerzubek and  t  g dietterich 2005 integrating learning from examples into the search for diagnostic policies volume 24 pages 263303



this paper studies the problem of learning diagnostic policies from training examples a diagnostic policy is a complete description of the decisionmaking actions of a diagnostician ie tests followed by a diagnostic decision for all possible combinations of test results  an optimal diagnostic policy is one that minimizes the expected total cost which is the sum of measurement costs and misdiagnosis costs  in most diagnostic settings there is a tradeoff between these two kinds of costs  this paper formalizes diagnostic decision making as a markov decision process mdp the paper introduces a new family of systematic search algorithms based on the ao algorithm to solve this mdp  to make ao efficient the paper describes an admissible heuristic that enables ao to prune large parts of the search space  the paper also introduces several greedy algorithms including some improvements over previouslypublished methods the paper then addresses the question of learning diagnostic policies from examples  when the probabilities of diseases and test results are computed from training data there is a great danger of overfitting to reduce overfitting regularizers are integrated into the search algorithms  finally the paper compares the proposed methods on five benchmark diagnostic data sets  the studies show that in most cases the systematic search methods produce better diagnostic policies than the greedy methods in addition the studies show that for training sets of realistic size the systematic search algorithms are practical on todays desktop computers





d  poole and  n  l zhang 2003 exploiting contextual independence in probabilistic inference volume 18 pages 263313



bayesian belief networks have grown to prominence because    they provide compact representations for many problems for which    probabilistic inference is appropriate and there are algorithms to    exploit this compactness the next step is to allow compact    representations of the conditional probabilities of a variable given    its parents in this paper we present such a representation that    exploits contextual independence in terms of parent contexts which    variables act as parents may depend on the value of other    variables the internal representation is in terms of contextual    factors confactors that is simply a pair of a context and a table    the algorithm contextual variable elimination is based on the    standard variable elimination algorithm that eliminates the nonquery    variables in turn but when eliminating a variable the tables that    need to be multiplied can depend on the context this algorithm    reduces to standard variable elimination when there is no contextual    independence structure to exploit we show how this can be much more    efficient than variable elimination when there is structure to    exploit we explain why this new method can exploit more structure    than previous methods for structured belief network inference and an    analogous algorithm that uses trees





w  w cohen  r  e schapire and  y  singer 1999 learning to order things volume 10 pages 243270



there are many applications in which it is desirable to    order rather than classify instances here we consider the problem of    learning how to order instances given feedback in the form of    preference judgments ie statements to the effect that one instance    should be ranked ahead of another  we outline a twostage approach in    which one first learns by conventional means a binary preference    function indicating whether it is advisable to rank one instance    before another here we consider an online algorithm for learning    preference functions that is based on freund and schapires hedge    algorithm  in the second stage new instances are ordered so as to    maximize agreement with the learned preference function  we show that    the problem of finding the ordering that agrees best with a learned    preference function is npcomplete  nevertheless we describe simple    greedy algorithms that are guaranteed to find a good approximation    finally we show how metasearch can be formulated as an ordering    problem and present experimental results on learning a combination of    search experts each of which is a domainspecific query expansion    strategy for a web search engine







maximilian  fickert joerg  hoffmann and marcel  steinmetz 2016 combining the delete relaxation with criticalpath heuristics a direct characterization volume 56 pages 269327





r  ribeiro and d  martins de matos 2011 centralityasrelevance support sets and similarity as geometric proximity volume 42 pages 275308



in automatic summarization centralityasrelevance means that the most important content of an information source or a collection of information sources corresponds to the most central passages considering a representation where such notion makes sense graph spatial etc we assess the main paradigms and introduce a new centralitybased relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content geometric proximity is used to compute semantic relatedness centrality relevance is determined by considering the whole input source and not only local information and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized the method consists in creating for each passage of the input source a support set consisting only of the most semantically related passages then the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets this model produces extractive summaries that are generic and language and domainindependent thorough automatic evaluation shows that the method achieves stateoftheart performance both in written text and automatically transcribed speech summarization including when compared to considerably more complex approaches







i  rezek d  s leslie s  reece s  j roberts a  rogers r  k dash and n  r jennings 2008 on similarities between inference in game theory and  machine learning volume 33 pages 259283





j  m wiebe t  p ohara thorsten  ohrstromsandgren and k  j mckeever 1998 an empirical approach to temporal reference resolution volume 9 pages 247293



scheduling dialogs during which people negotiate the    times of appointments are common in everyday life  this paper    reports the results of an indepth empirical investigation of    resolving explicit temporal references in scheduling dialogs  there    are four phases of this work data annotation and evaluation model    development system implementation and evaluation and model    evaluation and analysis  the system and model were developed    primarily on one set of data and then applied later to a much more    complex data set to assess the generalizability of the model for the    task being performed  many different types of empirical methods are    applied to pinpoint the strengths and weaknesses of the approach    detailed annotation instructions were developed and an intercoder    reliability study was performed showing that naive annotators can    reliably perform the targeted annotations  a fully automatic system    has been developed and evaluated on unseen test data with good    results on both data sets  we adopt a pure realization of a    recencybased focus model to identify precisely when it is and is not    adequate for the task being addressed  in addition to system results    an indepth evaluation of the model itself is presented based on    detailed manual annotations  the results are that few errors occur    specifically due to the model of focus being used and the set of    anaphoric relations defined in the model are low in ambiguity for both    data sets





m  palomar and  p  martinezbarco 2001 computational approach to anaphora resolution in spanish dialogues volume 15 pages 263287



this paper presents an algorithm for identifying nounphrase    antecedents of pronouns and adjectival anaphors in spanish    dialogues we believe that anaphora resolution requires numerous    sources of information in order to find the correct antecedent of the    anaphor these sources can be of different kinds eg linguistic    information discoursedialogue structure information or topic    information for this reason our algorithm uses various different    kinds of information hybrid information the algorithm is based on    linguistic constraints and preferences and uses an anaphoric    accessibility space within which the algorithm finds the noun    phrase we present some experiments related to this algorithm and this    space using a corpus of 204 dialogues the algorithm is implemented in    prolog according to this study 959 of antecedents were located in    the proposed space a precision of 813 was obtained for pronominal    anaphora resolution and 815 for adjectival anaphora











a  becker  r  baryehuda and  d  geiger 2000 randomized algorithms for the loop cutset problem volume 12 pages 219234



we show how to find a minimum weight loop cutset in a    bayesian network with high probability finding such a loop cutset is    the first step in the method of conditioning for inference  our    randomized algorithm for finding a loop cutset outputs a minimum loop    cutset after oc 6k kn steps with probability at least     1  1  16kc6k where c  1 is a constant specified by the    user k is the minimal size of a minimum weight loop cutset and n is    the number of vertices  we also show empirically that a variant of    this algorithm often finds a loop cutset that is closer to the minimum    weight loop cutset than the ones found by the best deterministic    algorithms known











ahmad  khwileh debasis   ganguly  and gareth   j f jones  2016 utilisation of metadata fields and query expansion in crosslingual search of usergenerated internet video volume 55 pages 249281



recent years have seen significant efforts in the area of cross language information retrieval clir for text retrieval this work initially focused on formally published content but more recently research has begun to concentrate on clir for informal social media content however despite the current expansion in online multimedia archives there has been little work on clir for this content while there has been some limited work on crosslanguage video retrieval clvr for professional videos such as documentaries or tv news broadcasts there has to date been no significant investigation of clvr for the rapidly growing archives of informal user generated ugc content key differences between such ugc and professionally produced content are the nature and structure of the textual ugc metadata associated with it as well as the form and quality of the content itself in this setting retrieval effectiveness may not only suffer from translation errors common to all clir tasks but also recognition errors associated with the automatic speech recognition asr systems used to transcribe the spoken content of the video and with the informality and inconsistency of the associated usercreated metadata for each video this work proposes and evaluates techniques to improve clir effectiveness of such noisy ugc content our experimental investigation shows that different sources of evidence eg the content from different fields of the structured metadata significantly affect clir effectiveness results from our experiments also show that each metadata field has a varying robustness to query expansion qe and hence can have a negative impact on the clir effectiveness our work proposes a novel adaptive qe technique that predicts the most reliable source for expansion and shows how this technique can be effective for improving the clir effectiveness for ugc content



one approach to conserving endangered species is to purchase and protect a set of land parcels in a way that maximizes the expected future population spread unfortunately an ideal set of parcels may have a cost that is beyond the immediate budget constraints and must thus be purchased incrementally this raises the challenge of deciding how to schedule the parcel purchases in a way that maximizes the flexibility of budget usage while keeping population spread loss in control in this paper we introduce a formulation of this scheduling problem that does not rely on knowing the future budgets of an organization in particular we consider scheduling purchases in a way that achieves a population spread no less than desired but delays purchases as long as possible such schedules offer conservation planners maximum flexibility and use available budgets in the most efficient way we develop the problem formally as a stochastic optimization problem over a network cascade model describing a commonly used model of population spread our solution approach is based on reducing the stochastic problem to a novel variant of the directed steiner tree problem which we call the setweighted directed steiner graph problem we show that this problem is computationally hard motivating the development of a primaldual algorithm for the problem that computes both a feasible solution and a bound on the quality of an optimal solution we evaluate the approach on both real and synthetic conservation data with a standard population spread model the algorithm is shown to produce near optimal results and is much more scalable than more generic offtheshelf optimizers finally we evaluate a variant of the algorithm to explore the tradeoffs between budget savings and population growth









e  a hansen and r  zhou 2007 anytime heuristic search volume 28 pages 267297



we describe how to convert the heuristic search algorithm a into an anytime algorithm that finds a sequence of improved solutions and eventually converges to an optimal solution the approach we adopt uses weighted heuristic search to find an approximate solution quickly and then continues the weighted search to find improved solutions as well as to improve a bound on the suboptimality of the current solution when the time available to solve a search problem is limited or uncertain this creates an anytime heuristic search algorithm that allows a flexible tradeoff between search time and solution quality we analyze the properties of the resulting anytime a algorithm and consider its performance in three domains slidingtile puzzles strips planning and multiple sequence alignment to illustrate the generality of this approach we also describe how to transform the memoryefficient search algorithm recursive bestfirst search rbfs into an anytime algorithm







a  yates and o  etzioni 2009 unsupervised methods for determining object and relation synonyms on the web volume 34 pages 255296



r  hoshino and k  kawarabayashi 2012 generating approximate solutions to the ttp using a linear distance relaxation volume 45 pages 257286



in some domestic professional sports leagues the home stadiums are located in cities connected by a common train line running in one direction for these instances we can incorporate this geographical information to determine optimal or nearlyoptimal solutions to the nteam traveling tournament problem ttp an nphard sports scheduling problem whose solution is a double roundrobin tournament schedule that minimizes the sum total of distances traveled by all n teams

we conclude the paper by applying this linear distance relaxation to general nonlinear nteam ttp instances where we develop fast approximate solutions by simply assuming the n teams lie on a straight line and solving the modified problem we show that this technique surprisingly generates the distanceoptimal tournament on all benchmark sets on 6 teams as well as closetooptimal schedules for larger n even when the teams are located around a circle or positioned in threedimensional space









i  konstas and m  lapata 2013 a global model for concepttotext generation volume 48 pages 305346



we represent planning as a set of loosely coupled network flow problems where each network corresponds to one of the state variables in the planning domain the network nodes correspond to the state variable values and the network arcs correspond to the value transitions the planning problem is to find a path a sequence of actions in each network such that when merged they constitute a feasible plan in this paper we present a number of integer programming formulations that model these loosely coupled networks with varying degrees of flexibility since merging may introduce exponentially many ordering constraints we implement a socalled branchandcut algorithm in which these constraints are dynamically generated and added to the formulation when needed our results are very promising they improve upon previous planning as integer programming approaches and lay the foundation for integer programming approaches for cost optimal planning











meghyn  bienvenu magdalena  ortiz and mantas  simkus 2015 regular path queries in lightweight description logics complexity and algorithms volume 53 pages 315374





l  h ihrig and  s  kambhampati 1997 storing and indexing plan derivations through explanationbased analysis of retrieval failures volume 7 pages 161198



casebased planning cbp provides a way of scaling up    domainindependent planning to solve large problems in complex    domains  it replaces the detailed and lengthy search for a solution    with the retrieval and adaptation of previous planning experiences    in general cbp has been demonstrated to improve performance over    generative fromscratch planning  however the performance    improvements it provides are dependent on adequate judgements as to    problem similarity  in particular although cbp may substantially    reduce planning effort overall it is subject to a misretrieval    problem the success of cbp depends on these retrieval errors being    relatively rare this paper describes the design and implementation of    a replay framework for the casebased planner dersnlpebl dersnlpebl    extends current cbp methodology by incorporating explanationbased    learning techniques that allow it to explain and learn from the    retrieval failures it encounters  these techniques are used to refine    judgements about case similarity in response to feedback when a wrong    decision has been made  the same failure analysis is used in building    the case library through the addition of repairing cases large    problems are split and stored as single goal subproblems  multigoal    problems are stored only when these smaller cases fail to be merged    into a full solution  an empirical evaluation of this approach    demonstrates the advantage of learning from experienced retrieval    failure







r  huang y  chen and w  zhang 2012 sas planning as satisfiability volume 43 pages 293328







b  bidyuk r  dechter and e  rollon 2010 active tuplesbased scheme for bounding posterior beliefs volume 39 pages 335371



we introduce a new measure of the discrepancy in strategic games between the social welfare in a nash equilibrium and in a social optimum that we call selfishness level it is the smallest fraction of the social welfare that needs to be offered to each player to achieve that a social optimum is realized in a pure nash equilibrium the selfishness level is unrelated to the price of stability and the price of anarchy and is invariant under positive linear transformations of the payoff functions also it naturally applies to other solution concepts and other forms of games

in particular the selfishness level of finite ordinal potential games is finite while that of weakly acyclic games can be infinite  we derive explicit bounds on the selfishness level of fair cost sharing games and linear congestion games which depend on specific parameters of the underlying game but are independent of the number of players  further we show that the selfishness level of the nplayers prisoners dilemma is cbn1c where b and c are the benefit and cost for cooperation respectively  that of the nplayers public goods game is 1cnc1 where c is the public good multiplier and that of the travelers dilemma game is b12 where b is the bonus finally the selfishness level of cournot competition an example of an infinite ordinal potential game tragedy of the commons and bertrand competition is infinite









m  do and  s  kambhampati 2003 sapa a multiobjective metric temporal planner volume 20 pages 155194



sapa is a domainindependent heuristic forward chaining planner  that can handle durative actions metric resource constraints  and deadline goals it is designed to be capable of handling the  multiobjective nature of metric temporal planning  our technical  contributions include i planninggraph based methods for deriving  heuristics that are sensitive to both cost and makespan ii  techniques for adjusting the heuristic estimates to take action  interactions and metric resource limitations into account and  iii a linear time greedy postprocessing technique to improve  execution flexibility of the solution plans an implementation of sapa using many of the techniques presented in this paper was one of the best domain independent planners for domains with metric and temporal constraints in the third international planning competition held at aips02 we describe the technical details of extracting the heuristics and present an empirical evaluation of the current implementation of sapa





h  e dixon  m  l ginsberg and  a  j parkes 2004 generalizing boolean satisfiability i background and survey of existing work volume 21 pages 193243



this is the first of three planned papers describing zap a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern highperformance solvers  the fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the boolean representation used our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance  this paper is a survey of the work underlying zap and discusses previous attempts to improve the performance of the davisputnamlogemannloveland algorithm by exploiting the structure of the problem being solved  we examine existing ideas including extensions of the boolean language to allow cardinality constraints pseudoboolean representations symmetry and a limited form of quantification  while this paper is intended as a survey our research results are contained in the two subsequent articles with the theoretical structure of zap described in the second paper in this series and zaps implementation described in the third





g  gange p  j stuckey and v  lagoon 2010 fast set bounds propagation using a bddsat hybrid volume 38 pages 307338



binary decision diagram bdd based set bounds propagation is a powerful approach to solving setconstraint satisfaction problems however prior bdd based techniques in cur the significant overhead of constructing and manipulating graphs during search we present a setconstraint solver which combines bddbased setbounds propagators with the learning abilities of a modern sat solver together with a number of improvements beyond the basic algorithm this solver is highly competitive with existing propagation based set constraint solvers





2003 ijcaijair best paper prize



this paper presents a new approach to hierarchical    reinforcement learning based on decomposing the target markov decision    process mdp into a hierarchy of smaller mdps and decomposing the    value function of the target mdp into an additive combination of the    value functions of the smaller mdps  the decomposition known as the    maxq decomposition has both a procedural semanticsas a subroutine    hierarchyand a declarative semanticsas a representation of the    value function of a hierarchical policy  maxq unifies and extends    previous work on hierarchical reinforcement learning by singh    kaelbling and dayan and hinton  it is based on the assumption that    the programmer can identify useful subgoals and define subtasks that    achieve these subgoals  by defining such subgoals the programmer    constrains the set of policies that need to be considered during    reinforcement learning  the maxq value function decomposition can    represent the value function of any policy that is consistent with the    given hierarchy  the decomposition also creates opportunities to    exploit state abstractions so that individual mdps within the    hierarchy can ignore large parts of the state space  this is    important for the practical application of the method  this paper    defines the maxq hierarchy proves formal results on its    representational power and establishes five conditions for the safe    use of state abstractions  the paper presents an online modelfree    learning algorithm maxqq and proves that it converges with    probability 1 to a kind of locallyoptimal policy known as a    recursively optimal policy even in the presence of the five kinds of    state abstraction  the paper evaluates the maxq representation and    maxqq through a series of experiments in three domains and shows    experimentally that maxqq with state abstractions converges to a    recursively optimal policy much faster than flat q learning  the fact    that maxq learns a representation of the value function has an    important benefit it makes it possible to compute and execute an    improved nonhierarchical policy via a procedure similar to the    policy improvement step of policy iteration  the paper demonstrates    the effectiveness of this nonhierarchical execution experimentally    finally the paper concludes with a comparison to related work and a    discussion of the design tradeoffs in hierarchical reinforcement learning



we consider the problem of computing a lightest derivation of a global structure using a set of weighted rules  a large variety of inference problems in ai can be formulated in this framework  we generalize a search and heuristics derived from abstractions to a broad class of lightest derivation problems  we also describe a new algorithm that searches for lightest derivations using a hierarchy of abstractions  our generalization of a gives a new algorithm for searching andor graphs in a bottomup fashion











s  ordyniak and s  szeider 2013 parameterized complexity results for exact bayesian network structure learning volume 46 pages 263302



control and bribery are settings in which an external agent seeks to influence the outcome of an election  constructive control of elections refers to attempts by an agent to via such actions as additiondeletionpartition of candidates or voters ensure that a given candidate wins  destructive control refers to attempts by an agent to via the same actions preclude a given candidates victory an election system in which an agent can sometimes affect the result and it can be determined in polynomial time on which inputs the agent can succeed is said to be vulnerable to the given type of control  an election system in which an agent can sometimes affect the result yet in which it is nphard to recognize the inputs on which the agent can succeed is said to be resistant to the given type of control











till  mossakowski and reinhard  moratz 2015 relations between spatial calculi about directions and orientations volume 54 pages 277308



the treatment of exogenous events in planning is practically important in many realworld domains where the preconditions of certain plan actions are affected by such events in this paper we focus on planning in temporal domains with exogenous events that happen at known times imposing the constraint that certain actions in the plan must be executed during some predefined time windows when actions have durations handling such temporal constraints adds an extra difficulty to planning we propose an approach to planning in these domains which integrates constraintbased temporal reasoning into a graphbased planning framework using local search our techniques are implemented in a planner that took part in the 4th international planning competition ipc4 a statistical analysis of the results of ipc4 demonstrates the effectiveness of our approach in terms of both cputime and plan quality additional experiments show the good performance of the temporal reasoning techniques integrated into our planner 









m  koppel  r  feldman and  a  m segre 1994 biasdriven revision of logical domain theories volume 1 pages 159208



the theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies  in this paper we present our approach to the theory revision problem for propositional domain theories  the approach described here called ptr uses probabilities associated with domain theory elements to numerically track the flow of proof through the theory  this allows us to measure the precise role of a clause or literal in allowing or preventing a desired or undesired derivation for a given example  this information is used to efficiently locate and repair flawed elements of the theory  ptr is proved to converge to a theory which correctly classifies all examples and shown experimentally to be fast and accurate even for deep theories





f  ygge and  h  akkermans 1999 decentralized markets versus central control a comparative study volume 11 pages 301333







t  g dietterich and  g  bakiri 1995 solving multiclass learning problems via errorcorrecting output codes volume 2 pages 263286



multiclass learning problems involve finding a definitionfor an unknown function ifix whose range is a discrete setcontaining iki  2 values ie iki classes  thedefinition is acquired by studying collections of training examples ofthe form xi i f ixi  existing approaches tomulticlass learning problems include direct application of multiclassalgorithms such as the decisiontree algorithms c45 and cartapplication of binary concept learning algorithms to learn individualbinary functions for each of the i k i classes and application ofbinary concept learning algorithms with distributed outputrepresentations  this paper compares these three approaches to a newtechnique in which errorcorrecting codes are employed as adistributed output representation  we show that these outputrepresentations improve the generalization performance of both c45and backpropagation on a wide range of multiclass learning tasks  wealso demonstrate that this approach is robust with respect to changesin the size of the training sample the assignment of distributedrepresentations to particular classes and the application ofoverfitting avoidance techniques such as decisiontree pruningfinally we show thatlike the other methodsthe errorcorrectingcode technique can provide reliable class probability estimatestaken together these results demonstrate that errorcorrecting outputcodes provide a generalpurpose method for improving the performanceof inductive learning programs on multiclass problems





n  v chawla  k  w bowyer  l  o hall and  w  p kegelmeyer 2002 smote synthetic minority oversampling technique volume 16 pages 321357



an approach to the construction of classifiers from    imbalanced datasets is described a dataset is imbalanced if the    classification categories are not approximately equally    represented often realworld data sets are predominately composed of    normal examples with only a small percentage of abnormal or    interesting examples it is also the case that the cost of    misclassifying an abnormal interesting example as a normal example    is often much higher than the cost of the reverse    error undersampling of the majority normal class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class this paper shows that a combination of our method of    oversampling the minority abnormal class and undersampling the    majority normal class can achieve better classifier performance in    roc space than only undersampling the majority class  this paper    also shows that a combination of our method of oversampling the    minority class and undersampling the majority class can achieve    better classifier performance in roc space than varying the loss    ratios in ripper or class priors in naive bayes our method of    oversampling the minority class involves creating synthetic minority    class examples  experiments are performed using c45 ripper and a    naive bayes classifier the method is evaluated using the area under    the receiver operating characteristic curve auc and the roc convex    hull strategy





2006 ijcaijair best paper prize



we propose a perspective on knowledge compilation which    calls for analyzing different compilation approaches according to two    key dimensions the succinctness of the target compilation language    and the class of queries and transformations that the language    supports in polytime we then provide a knowledge compilation map    which analyzes a large number of existing target compilation languages    according to their succinctness and their polytime transformations and    queries we argue that such analysis is necessary for placing new    compilation approaches within the context of existing ones we also go    beyond classical flat target compilation languages based on cnf and    dnf and consider a richer nested class based on directed acyclic    graphs such as obdds which we show to include a relatively large    number of target compilation languages





y  wang c  hang and m  p singh 2011 a probabilistic approach for maintaining trust based on evidence volume 40 pages 221267



leading agentbased trust models address two important needs  first they show how an agent may estimate the trustworthiness of another agent based on prior interactions  second they show how agents may share their knowledge in order to cooperatively assess the trustworthiness of others  however in reallife settings information relevant to trust is usually obtained piecemeal not all at once  unfortunately the problem of maintaining trust has drawn little attention  existing approaches handle trust updates in a heuristic not a principled manner

this paper builds on a formal model that considers probability and certainty as two dimensions of trust  it proposes a mechanism using which an agent can update the amount of trust it places in other agents on an ongoing basis  this paper shows via simulation that the proposed approach a provides accurate estimates of the trustworthiness of agents that change behavior frequently and b captures the dynamic behavior of the agents  this paper includes an evaluation based on a real dataset drawn from amazon marketplace a leading ecommerce site





y  bengio and  p  frasconi 1995 diffusion of context and credit information in markovian models volume 3 pages 249270



this paper studies the problem of ergodicity of transition    probability matrices in markovian models such as hidden markov models    hmms and how it makes very difficult the task of learning to    represent longterm context for sequential data  this phenomenon    hurts the forward propagation of longterm context information as    well as learning a hidden state representation to represent longterm    context which depends on propagating credit information backwards in    time  using results from markov chain theory we show that this    problem of diffusion of context and credit is reduced when the    transition probabilities approach 0 or 1 ie the transition    probability matrices are sparse and the model essentially    deterministic  the results found in this paper apply to learning    approaches based on continuous optimization such as gradient descent    and the baumwelch algorithm



the agm framework is the benchmark approach in belief change since the framework assumes an underlying logic containing classical propositional logic it can not be applied to systems with a logic weaker than propositional logic  to remedy this limitation several researchers have studied agmstyle contraction and revision under the horn fragment of propositional logic ie horn logic  in this paper we contribute to this line of research by investigating the horn version of the agm entrenchmentbased contraction  the study is challenging as the construction of entrenchmentbased contraction refers to arbitrary disjunctions which are not expressible under horn logic  in order to adapt the construction to horn logic we make use of a horn approximation technique called horn strengthening we provide a representation theorem for the newly constructed contraction which we refer to as entrenchmentbased horn contraction  ideally contractions defined under horn logic ie horn contractions should be as rational as agm contraction  we propose the notion of horn equivalence which intuitively captures the equivalence between horn contraction and agm contraction  we show that under this notion entrenchmentbased horn contraction is equivalent to a restricted form of entrenchmentbased contraction









l  m de campos 1996 characterizations of decomposable dependency models volume 5 pages 289300



decomposable dependency models possess a number of    interesting and useful properties this paper presents new    characterizations of decomposable models in terms of independence    relationships which are obtained by adding a single axiom to the    wellknown set characterizing dependency models that are isomorphic to    undirected graphs we also briefly discuss a potential application of    our results to the problem of learning graphical models from data





m  suda 2014 property directed reachability for automated planning volume 50 pages 265319



property directed reachability pdr is a very promising recent method for deciding reachability in symbolically represented transition systems while originally conceived as a model checking algorithm for hardware circuits it has already been successfully applied in several other areas this paper is the first investigation of pdr from the perspective of automated planning

similarly to the planning as satisfiability paradigm pdr draws its strength from internally employing an efficient satsolver we show that most standard encoding schemes of planning into sat can be directly used to turn pdr into a planning algorithm as a nonobvious alternative we propose to replace the satsolver inside pdr by a planningspecific procedure implementing the same interface this satsolver free variant is not only more efficient but offers additional insights and opportunities for further improvements an experimental comparison to the state of the art planners finds it highly competitive solving most problems on several domains







f  t liu k  m ting y  yu and z  h zhou 2008 spectrum of variablerandom trees volume 32 pages 355384





l  finkelstein and  s  markovitch 1998 a selective macrolearning algorithm and its application to the nxn slidingtile puzzle volume 8 pages 223263



one of the most common mechanisms used for speeding up    problem solvers is macrolearning  macros are sequences of basic    operators acquired during problem solving  macros are used by the    problem solver as if they were basic operators  the major problem    that macrolearning presents is the vast number of macros that are    available for acquisition  macros increase the branching factor of    the search space and can severely degrade problemsolving efficiency    to make macro learning useful a program must be selective in    acquiring and utilizing macros  this paper describes a general method    for selective acquisition of macros solvable training problems are    generated in increasing order of difficulty  the only macros acquired    are those that take the problem solver out of a local minimum to a    better state  the utility of the method is demonstrated in several    domains including the domain of nxn slidingtile puzzles after    learning on small puzzles the system is able to efficiently solve    puzzles of any size





d  fisher 1996 iterative optimization and simplification of hierarchical clusterings volume 4 pages 147178



clustering is often used for discovering structure in data    clustering systems differ in the objective function used to evaluate    clustering quality and the control strategy used to search the space    of clusterings ideally the search strategy should consistently    construct clusterings of high quality but be computationally    inexpensive as well in general we cannot have it both ways but we    can partition the search so that a system inexpensively constructs a    tentative clustering for initial examination followed by iterative    optimization which continues to search in background for improved    clusterings given this motivation we evaluate an inexpensive    strategy for creating initial clusterings coupled with several    control strategies for iterative optimization each of which    repeatedly modifies an initial clustering in search of a better    one one of these methods appears novel as an iterative optimization    strategy in clustering contexts once a clustering has been    constructed it is judged by analysts  often according to    taskspecific criteria several authors have abstracted these criteria    and posited a generic performance task akin to pattern completion    where the error rate over completed patterns is used to externally    judge clustering utility given this performance task we adapt    resamplingbased pruning strategies used by supervised learning    systems to the task of simplifying hierarchical clusterings thus    promising to ease postclustering analysis finally we propose a    number of objective functions based on attributeselection measures    for decisiontree induction that might perform well on the error rate    and simplicity dimensions







a   khudyak kozorovitsky and o  kurland 2011 from identical to similar fusing retrieved lists based on interdocument similarities volume 41 pages 267296





r  debruyne and  c  bessiere 2001 domain filtering consistencies volume 14 pages 205230



enforcing local consistencies is one of the main features    of constraint reasoning which level of local consistency should be    used when searching for solutions in a constraint network is a basic    question arc consistency and partial forms of arc consistency have    been widely studied and have been known for sometime through the    forward checking or the mac search algorithms until recently    stronger forms of local consistency remained limited to those that    change the structure of the constraint graph and thus could not be    used in practice especially on large networks this paper focuses on    the local consistencies that are stronger than arc consistency    without changing the structure of the network ie only removing    inconsistent values from the domains in the last five years several    such local consistencies have been proposed by us or by others we    make an overview of all of them and highlight some relations between    them we compare them both theoretically and experimentally    considering their pruning efficiency and the time required to enforce    them

















p  stone  r  e schapire  m  l littman  j  a csirik and d  mcallester 2003 decisiontheoretic bidding based on learned density models in simultaneous interacting auctions volume 19 pages 209242



auctions are becoming an increasingly popular method for transacting business especially over the internet  this article presents a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for interacting goods  a core component of our approach learns a model of the empirical price dynamics based on past data and uses the model to analytically calculate to the greatest extent possible optimal bids  we introduce a new and general boostingbased algorithm for conditional density estimation problems of this kind ie supervised learning problems in which the goal is to estimate the entire conditional distribution of the realvalued label  this approach is fully implemented as attac2001 a topscoring agent in the second trading agent competition tac01 we present experiments demonstrating the effectiveness of our boostingbased price predictor relative to several reasonable alternatives





m  e pollack  d  joslin and  m  paolucci 1997 flaw selection strategies for partialorder planning volume 6 pages 223262



several recent studies have compared the relative efficiency    of alternative flaw selection strategies for partialorder causal link    pocl planning  we review this literature and present new    experimental results that generalize the earlier work and explain some    of the discrepancies in it  in particular we describe the leastcost    flaw repair lcfr strategy developed and analyzed by joslin and    pollack 1994 and compare it with other strategies including    gerevini and schuberts 1996 zlifo strategy  lcfr and zlifo make    very different and apparently conflicting claims about the most    effective way to reduce searchspace size in pocl planning  we    resolve this conflict arguing that much of the benefit that gerevini    and schubert ascribe to the lifo component of their zlifo strategy is    better attributed to other causes  we show that for many problems a    strategy that combines leastcost flaw selection with the delay of    separable threats will be effective in reducing searchspace size and    will do so without excessive computational overhead  although such a    strategy thus provides a good default we also show that certain    domain characteristics may reduce its effectiveness



a pattern database pdb is a heuristic function implemented as a lookup table that stores the lengths of optimal solutions for subproblem instances standard pdbs have a distinct entry in the table for each subproblem instance in this paper we investigate compressing pdbs by merging several entries into one thereby allowing the use of pdbs that exceed available memory in their uncompressed form we introduce a number of methods for determining which entries to merge and discuss their relative merits these vary from domainindependent approaches that allow any set of entries in the pdb to be merged to more intelligent methods that take into account the structure of the problem the choice of the best compression method is based on domaindependent attributes we present experimental results on a number of combinatorial problems including the fourpeg towers of hanoi problem the slidingtile puzzles and the topspin puzzle for the towers of hanoi we show that the search time can be reduced by up to three orders of magnitude by using compressed pdbs compared to uncompressed pdbs of the same size more modest improvements were observed for the other domains









r  mateescu k  kask v  gogate and r  dechter 2010 joingraph propagation algorithms volume 37 pages 279328



the paper investigates parameterized approximate messagepassing schemes that are based on bounded inference and are inspired by pearls belief propagation algorithm bp we start with the bounded inference miniclustering algorithm and then move to the iterative scheme called iterative joingraph propagation ijgp that combines both iteration and bounded inference algorithm ijgp belongs to the class of generalized belief propagation algorithms a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of miniclustering and belief propagation as well as a number of other stateoftheart algorithms on several classes of networks we also provide insight into the accuracy of iterative bp and ijgp by relating these algorithms to well known classes of constraint propagation schemes





n  v chawla and   karakoulas 2005 learning from labeled and unlabeled data an empirical study across techniques and domains volume 23 pages 331366



there has been increased interest in devising learning techniques that combine unlabeled data with labeled data  ie semisupervised learning however to the best of our knowledge no study has been performed across various techniques and different types and amounts of labeled and unlabeled data moreover most of the published work on semisupervised learning techniques assumes that the labeled and unlabeled data come from the same distribution it is possible for the labeling process to be associated with a selection bias such that the distributions of data points in the labeled and unlabeled sets are different not correcting for such bias can result in biased function approximation with potentially poor performance in this paper we present an empirical study of various semisupervised learning techniques on a variety of datasets we attempt to answer various questions such as the effect of independence or relevance amongst features the effect of the size of the labeled and unlabeled sets and the effect of noise we also investigate the impact of sampleselection bias on the semisupervised learning techniques under study and implement a bivariate probit technique particularly designed to correct for such bias





j  hoffmann  j  porteous and  l  sebastia 2004 ordered landmarks in planning volume 22 pages 215278



many known planning tasks have inherent constraints concerning the best order in which to achieve the goals a number of research efforts have been made to detect such constraints and to use them for guiding search in the hope of speeding up the planning process     we go beyond the previous approaches by considering ordering constraints not only over the toplevel goals but also over the subgoals that will necessarily arise during planning landmarks are facts that must be true at some point in every valid solution plan  we extend koehler and hoffmanns definition of reasonable orders between top level goals to the more general case of landmarks we show how landmarks can be found how their reasonable orders can be approximated and how this information can be used to decompose a given planning task into several smaller subtasks our methodology is completely domain and plannerindependent the implementation demonstrates that the approach can yield significant runtime performance improvements when used as a control loop around stateoftheart suboptimal planning systems as exemplified by ff and lpg



active learners alleviate the burden of labeling large amounts of data by detecting and asking the user to label only the most informative examples in the domain  we focus here on active learning for multiview domains in which there are several disjoint subsets of features views each of which is sufficient to learn the target concept  in this paper we make several contributions first we introduce cotesting which is the first approach to multiview active learning  second we extend the multiview learning framework by also exploiting weak views which are adequate only for learning a concept that is more generalspecific than the target concept  finally we empirically show that cotesting outperforms existing active learners on a variety of real world domains such as wrapper induction web page classification advertisement removal and discourse tree parsing









v  bayerzubek and  t  g dietterich 2005 integrating learning from examples into the search for diagnostic policies volume 24 pages 263303



this paper studies the problem of learning diagnostic policies from training examples a diagnostic policy is a complete description of the decisionmaking actions of a diagnostician ie tests followed by a diagnostic decision for all possible combinations of test results  an optimal diagnostic policy is one that minimizes the expected total cost which is the sum of measurement costs and misdiagnosis costs  in most diagnostic settings there is a tradeoff between these two kinds of costs  this paper formalizes diagnostic decision making as a markov decision process mdp the paper introduces a new family of systematic search algorithms based on the ao algorithm to solve this mdp  to make ao efficient the paper describes an admissible heuristic that enables ao to prune large parts of the search space  the paper also introduces several greedy algorithms including some improvements over previouslypublished methods the paper then addresses the question of learning diagnostic policies from examples  when the probabilities of diseases and test results are computed from training data there is a great danger of overfitting to reduce overfitting regularizers are integrated into the search algorithms  finally the paper compares the proposed methods on five benchmark diagnostic data sets  the studies show that in most cases the systematic search methods produce better diagnostic policies than the greedy methods in addition the studies show that for training sets of realistic size the systematic search algorithms are practical on todays desktop computers





d  poole and  n  l zhang 2003 exploiting contextual independence in probabilistic inference volume 18 pages 263313



bayesian belief networks have grown to prominence because    they provide compact representations for many problems for which    probabilistic inference is appropriate and there are algorithms to    exploit this compactness the next step is to allow compact    representations of the conditional probabilities of a variable given    its parents in this paper we present such a representation that    exploits contextual independence in terms of parent contexts which    variables act as parents may depend on the value of other    variables the internal representation is in terms of contextual    factors confactors that is simply a pair of a context and a table    the algorithm contextual variable elimination is based on the    standard variable elimination algorithm that eliminates the nonquery    variables in turn but when eliminating a variable the tables that    need to be multiplied can depend on the context this algorithm    reduces to standard variable elimination when there is no contextual    independence structure to exploit we show how this can be much more    efficient than variable elimination when there is structure to    exploit we explain why this new method can exploit more structure    than previous methods for structured belief network inference and an    analogous algorithm that uses trees





w  w cohen  r  e schapire and  y  singer 1999 learning to order things volume 10 pages 243270



there are many applications in which it is desirable to    order rather than classify instances here we consider the problem of    learning how to order instances given feedback in the form of    preference judgments ie statements to the effect that one instance    should be ranked ahead of another  we outline a twostage approach in    which one first learns by conventional means a binary preference    function indicating whether it is advisable to rank one instance    before another here we consider an online algorithm for learning    preference functions that is based on freund and schapires hedge    algorithm  in the second stage new instances are ordered so as to    maximize agreement with the learned preference function  we show that    the problem of finding the ordering that agrees best with a learned    preference function is npcomplete  nevertheless we describe simple    greedy algorithms that are guaranteed to find a good approximation    finally we show how metasearch can be formulated as an ordering    problem and present experimental results on learning a combination of    search experts each of which is a domainspecific query expansion    strategy for a web search engine







maximilian  fickert joerg  hoffmann and marcel  steinmetz 2016 combining the delete relaxation with criticalpath heuristics a direct characterization volume 56 pages 269327





r  ribeiro and d  martins de matos 2011 centralityasrelevance support sets and similarity as geometric proximity volume 42 pages 275308



in automatic summarization centralityasrelevance means that the most important content of an information source or a collection of information sources corresponds to the most central passages considering a representation where such notion makes sense graph spatial etc we assess the main paradigms and introduce a new centralitybased relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content geometric proximity is used to compute semantic relatedness centrality relevance is determined by considering the whole input source and not only local information and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized the method consists in creating for each passage of the input source a support set consisting only of the most semantically related passages then the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets this model produces extractive summaries that are generic and language and domainindependent thorough automatic evaluation shows that the method achieves stateoftheart performance both in written text and automatically transcribed speech summarization including when compared to considerably more complex approaches







i  rezek d  s leslie s  reece s  j roberts a  rogers r  k dash and n  r jennings 2008 on similarities between inference in game theory and  machine learning volume 33 pages 259283





j  m wiebe t  p ohara thorsten  ohrstromsandgren and k  j mckeever 1998 an empirical approach to temporal reference resolution volume 9 pages 247293



scheduling dialogs during which people negotiate the    times of appointments are common in everyday life  this paper    reports the results of an indepth empirical investigation of    resolving explicit temporal references in scheduling dialogs  there    are four phases of this work data annotation and evaluation model    development system implementation and evaluation and model    evaluation and analysis  the system and model were developed    primarily on one set of data and then applied later to a much more    complex data set to assess the generalizability of the model for the    task being performed  many different types of empirical methods are    applied to pinpoint the strengths and weaknesses of the approach    detailed annotation instructions were developed and an intercoder    reliability study was performed showing that naive annotators can    reliably perform the targeted annotations  a fully automatic system    has been developed and evaluated on unseen test data with good    results on both data sets  we adopt a pure realization of a    recencybased focus model to identify precisely when it is and is not    adequate for the task being addressed  in addition to system results    an indepth evaluation of the model itself is presented based on    detailed manual annotations  the results are that few errors occur    specifically due to the model of focus being used and the set of    anaphoric relations defined in the model are low in ambiguity for both    data sets





m  palomar and  p  martinezbarco 2001 computational approach to anaphora resolution in spanish dialogues volume 15 pages 263287



this paper presents an algorithm for identifying nounphrase    antecedents of pronouns and adjectival anaphors in spanish    dialogues we believe that anaphora resolution requires numerous    sources of information in order to find the correct antecedent of the    anaphor these sources can be of different kinds eg linguistic    information discoursedialogue structure information or topic    information for this reason our algorithm uses various different    kinds of information hybrid information the algorithm is based on    linguistic constraints and preferences and uses an anaphoric    accessibility space within which the algorithm finds the noun    phrase we present some experiments related to this algorithm and this    space using a corpus of 204 dialogues the algorithm is implemented in    prolog according to this study 959 of antecedents were located in    the proposed space a precision of 813 was obtained for pronominal    anaphora resolution and 815 for adjectival anaphora











a  becker  r  baryehuda and  d  geiger 2000 randomized algorithms for the loop cutset problem volume 12 pages 219234



we show how to find a minimum weight loop cutset in a    bayesian network with high probability finding such a loop cutset is    the first step in the method of conditioning for inference  our    randomized algorithm for finding a loop cutset outputs a minimum loop    cutset after oc 6k kn steps with probability at least     1  1  16kc6k where c  1 is a constant specified by the    user k is the minimal size of a minimum weight loop cutset and n is    the number of vertices  we also show empirically that a variant of    this algorithm often finds a loop cutset that is closer to the minimum    weight loop cutset than the ones found by the best deterministic    algorithms known











ahmad  khwileh debasis   ganguly  and gareth   j f jones  2016 utilisation of metadata fields and query expansion in crosslingual search of usergenerated internet video volume 55 pages 249281



recent years have seen significant efforts in the area of cross language information retrieval clir for text retrieval this work initially focused on formally published content but more recently research has begun to concentrate on clir for informal social media content however despite the current expansion in online multimedia archives there has been little work on clir for this content while there has been some limited work on crosslanguage video retrieval clvr for professional videos such as documentaries or tv news broadcasts there has to date been no significant investigation of clvr for the rapidly growing archives of informal user generated ugc content key differences between such ugc and professionally produced content are the nature and structure of the textual ugc metadata associated with it as well as the form and quality of the content itself in this setting retrieval effectiveness may not only suffer from translation errors common to all clir tasks but also recognition errors associated with the automatic speech recognition asr systems used to transcribe the spoken content of the video and with the informality and inconsistency of the associated usercreated metadata for each video this work proposes and evaluates techniques to improve clir effectiveness of such noisy ugc content our experimental investigation shows that different sources of evidence eg the content from different fields of the structured metadata significantly affect clir effectiveness results from our experiments also show that each metadata field has a varying robustness to query expansion qe and hence can have a negative impact on the clir effectiveness our work proposes a novel adaptive qe technique that predicts the most reliable source for expansion and shows how this technique can be effective for improving the clir effectiveness for ugc content



one approach to conserving endangered species is to purchase and protect a set of land parcels in a way that maximizes the expected future population spread unfortunately an ideal set of parcels may have a cost that is beyond the immediate budget constraints and must thus be purchased incrementally this raises the challenge of deciding how to schedule the parcel purchases in a way that maximizes the flexibility of budget usage while keeping population spread loss in control in this paper we introduce a formulation of this scheduling problem that does not rely on knowing the future budgets of an organization in particular we consider scheduling purchases in a way that achieves a population spread no less than desired but delays purchases as long as possible such schedules offer conservation planners maximum flexibility and use available budgets in the most efficient way we develop the problem formally as a stochastic optimization problem over a network cascade model describing a commonly used model of population spread our solution approach is based on reducing the stochastic problem to a novel variant of the directed steiner tree problem which we call the setweighted directed steiner graph problem we show that this problem is computationally hard motivating the development of a primaldual algorithm for the problem that computes both a feasible solution and a bound on the quality of an optimal solution we evaluate the approach on both real and synthetic conservation data with a standard population spread model the algorithm is shown to produce near optimal results and is much more scalable than more generic offtheshelf optimizers finally we evaluate a variant of the algorithm to explore the tradeoffs between budget savings and population growth









e  a hansen and r  zhou 2007 anytime heuristic search volume 28 pages 267297



we describe how to convert the heuristic search algorithm a into an anytime algorithm that finds a sequence of improved solutions and eventually converges to an optimal solution the approach we adopt uses weighted heuristic search to find an approximate solution quickly and then continues the weighted search to find improved solutions as well as to improve a bound on the suboptimality of the current solution when the time available to solve a search problem is limited or uncertain this creates an anytime heuristic search algorithm that allows a flexible tradeoff between search time and solution quality we analyze the properties of the resulting anytime a algorithm and consider its performance in three domains slidingtile puzzles strips planning and multiple sequence alignment to illustrate the generality of this approach we also describe how to transform the memoryefficient search algorithm recursive bestfirst search rbfs into an anytime algorithm







a  yates and o  etzioni 2009 unsupervised methods for determining object and relation synonyms on the web volume 34 pages 255296



r  hoshino and k  kawarabayashi 2012 generating approximate solutions to the ttp using a linear distance relaxation volume 45 pages 257286



in some domestic professional sports leagues the home stadiums are located in cities connected by a common train line running in one direction for these instances we can incorporate this geographical information to determine optimal or nearlyoptimal solutions to the nteam traveling tournament problem ttp an nphard sports scheduling problem whose solution is a double roundrobin tournament schedule that minimizes the sum total of distances traveled by all n teams

we conclude the paper by applying this linear distance relaxation to general nonlinear nteam ttp instances where we develop fast approximate solutions by simply assuming the n teams lie on a straight line and solving the modified problem we show that this technique surprisingly generates the distanceoptimal tournament on all benchmark sets on 6 teams as well as closetooptimal schedules for larger n even when the teams are located around a circle or positioned in threedimensional space









i  konstas and m  lapata 2013 a global model for concepttotext generation volume 48 pages 305346



we represent planning as a set of loosely coupled network flow problems where each network corresponds to one of the state variables in the planning domain the network nodes correspond to the state variable values and the network arcs correspond to the value transitions the planning problem is to find a path a sequence of actions in each network such that when merged they constitute a feasible plan in this paper we present a number of integer programming formulations that model these loosely coupled networks with varying degrees of flexibility since merging may introduce exponentially many ordering constraints we implement a socalled branchandcut algorithm in which these constraints are dynamically generated and added to the formulation when needed our results are very promising they improve upon previous planning as integer programming approaches and lay the foundation for integer programming approaches for cost optimal planning











meghyn  bienvenu magdalena  ortiz and mantas  simkus 2015 regular path queries in lightweight description logics complexity and algorithms volume 53 pages 315374





l  h ihrig and  s  kambhampati 1997 storing and indexing plan derivations through explanationbased analysis of retrieval failures volume 7 pages 161198



casebased planning cbp provides a way of scaling up    domainindependent planning to solve large problems in complex    domains  it replaces the detailed and lengthy search for a solution    with the retrieval and adaptation of previous planning experiences    in general cbp has been demonstrated to improve performance over    generative fromscratch planning  however the performance    improvements it provides are dependent on adequate judgements as to    problem similarity  in particular although cbp may substantially    reduce planning effort overall it is subject to a misretrieval    problem the success of cbp depends on these retrieval errors being    relatively rare this paper describes the design and implementation of    a replay framework for the casebased planner dersnlpebl dersnlpebl    extends current cbp methodology by incorporating explanationbased    learning techniques that allow it to explain and learn from the    retrieval failures it encounters  these techniques are used to refine    judgements about case similarity in response to feedback when a wrong    decision has been made  the same failure analysis is used in building    the case library through the addition of repairing cases large    problems are split and stored as single goal subproblems  multigoal    problems are stored only when these smaller cases fail to be merged    into a full solution  an empirical evaluation of this approach    demonstrates the advantage of learning from experienced retrieval    failure







r  huang y  chen and w  zhang 2012 sas planning as satisfiability volume 43 pages 293328







b  bidyuk r  dechter and e  rollon 2010 active tuplesbased scheme for bounding posterior beliefs volume 39 pages 335371



we introduce a new measure of the discrepancy in strategic games between the social welfare in a nash equilibrium and in a social optimum that we call selfishness level it is the smallest fraction of the social welfare that needs to be offered to each player to achieve that a social optimum is realized in a pure nash equilibrium the selfishness level is unrelated to the price of stability and the price of anarchy and is invariant under positive linear transformations of the payoff functions also it naturally applies to other solution concepts and other forms of games

in particular the selfishness level of finite ordinal potential games is finite while that of weakly acyclic games can be infinite  we derive explicit bounds on the selfishness level of fair cost sharing games and linear congestion games which depend on specific parameters of the underlying game but are independent of the number of players  further we show that the selfishness level of the nplayers prisoners dilemma is cbn1c where b and c are the benefit and cost for cooperation respectively  that of the nplayers public goods game is 1cnc1 where c is the public good multiplier and that of the travelers dilemma game is b12 where b is the bonus finally the selfishness level of cournot competition an example of an infinite ordinal potential game tragedy of the commons and bertrand competition is infinite









m  do and  s  kambhampati 2003 sapa a multiobjective metric temporal planner volume 20 pages 155194



sapa is a domainindependent heuristic forward chaining planner  that can handle durative actions metric resource constraints  and deadline goals it is designed to be capable of handling the  multiobjective nature of metric temporal planning  our technical  contributions include i planninggraph based methods for deriving  heuristics that are sensitive to both cost and makespan ii  techniques for adjusting the heuristic estimates to take action  interactions and metric resource limitations into account and  iii a linear time greedy postprocessing technique to improve  execution flexibility of the solution plans an implementation of sapa using many of the techniques presented in this paper was one of the best domain independent planners for domains with metric and temporal constraints in the third international planning competition held at aips02 we describe the technical details of extracting the heuristics and present an empirical evaluation of the current implementation of sapa





h  e dixon  m  l ginsberg and  a  j parkes 2004 generalizing boolean satisfiability i background and survey of existing work volume 21 pages 193243



this is the first of three planned papers describing zap a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern highperformance solvers  the fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the boolean representation used our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance  this paper is a survey of the work underlying zap and discusses previous attempts to improve the performance of the davisputnamlogemannloveland algorithm by exploiting the structure of the problem being solved  we examine existing ideas including extensions of the boolean language to allow cardinality constraints pseudoboolean representations symmetry and a limited form of quantification  while this paper is intended as a survey our research results are contained in the two subsequent articles with the theoretical structure of zap described in the second paper in this series and zaps implementation described in the third





g  gange p  j stuckey and v  lagoon 2010 fast set bounds propagation using a bddsat hybrid volume 38 pages 307338



binary decision diagram bdd based set bounds propagation is a powerful approach to solving setconstraint satisfaction problems however prior bdd based techniques in cur the significant overhead of constructing and manipulating graphs during search we present a setconstraint solver which combines bddbased setbounds propagators with the learning abilities of a modern sat solver together with a number of improvements beyond the basic algorithm this solver is highly competitive with existing propagation based set constraint solvers





2003 ijcaijair best paper prize



this paper presents a new approach to hierarchical    reinforcement learning based on decomposing the target markov decision    process mdp into a hierarchy of smaller mdps and decomposing the    value function of the target mdp into an additive combination of the    value functions of the smaller mdps  the decomposition known as the    maxq decomposition has both a procedural semanticsas a subroutine    hierarchyand a declarative semanticsas a representation of the    value function of a hierarchical policy  maxq unifies and extends    previous work on hierarchical reinforcement learning by singh    kaelbling and dayan and hinton  it is based on the assumption that    the programmer can identify useful subgoals and define subtasks that    achieve these subgoals  by defining such subgoals the programmer    constrains the set of policies that need to be considered during    reinforcement learning  the maxq value function decomposition can    represent the value function of any policy that is consistent with the    given hierarchy  the decomposition also creates opportunities to    exploit state abstractions so that individual mdps within the    hierarchy can ignore large parts of the state space  this is    important for the practical application of the method  this paper    defines the maxq hierarchy proves formal results on its    representational power and establishes five conditions for the safe    use of state abstractions  the paper presents an online modelfree    learning algorithm maxqq and proves that it converges with    probability 1 to a kind of locallyoptimal policy known as a    recursively optimal policy even in the presence of the five kinds of    state abstraction  the paper evaluates the maxq representation and    maxqq through a series of experiments in three domains and shows    experimentally that maxqq with state abstractions converges to a    recursively optimal policy much faster than flat q learning  the fact    that maxq learns a representation of the value function has an    important benefit it makes it possible to compute and execute an    improved nonhierarchical policy via a procedure similar to the    policy improvement step of policy iteration  the paper demonstrates    the effectiveness of this nonhierarchical execution experimentally    finally the paper concludes with a comparison to related work and a    discussion of the design tradeoffs in hierarchical reinforcement learning



we consider the problem of computing a lightest derivation of a global structure using a set of weighted rules  a large variety of inference problems in ai can be formulated in this framework  we generalize a search and heuristics derived from abstractions to a broad class of lightest derivation problems  we also describe a new algorithm that searches for lightest derivations using a hierarchy of abstractions  our generalization of a gives a new algorithm for searching andor graphs in a bottomup fashion











s  ordyniak and s  szeider 2013 parameterized complexity results for exact bayesian network structure learning volume 46 pages 263302



control and bribery are settings in which an external agent seeks to influence the outcome of an election  constructive control of elections refers to attempts by an agent to via such actions as additiondeletionpartition of candidates or voters ensure that a given candidate wins  destructive control refers to attempts by an agent to via the same actions preclude a given candidates victory an election system in which an agent can sometimes affect the result and it can be determined in polynomial time on which inputs the agent can succeed is said to be vulnerable to the given type of control  an election system in which an agent can sometimes affect the result yet in which it is nphard to recognize the inputs on which the agent can succeed is said to be resistant to the given type of control











till  mossakowski and reinhard  moratz 2015 relations between spatial calculi about directions and orientations volume 54 pages 277308



the treatment of exogenous events in planning is practically important in many realworld domains where the preconditions of certain plan actions are affected by such events in this paper we focus on planning in temporal domains with exogenous events that happen at known times imposing the constraint that certain actions in the plan must be executed during some predefined time windows when actions have durations handling such temporal constraints adds an extra difficulty to planning we propose an approach to planning in these domains which integrates constraintbased temporal reasoning into a graphbased planning framework using local search our techniques are implemented in a planner that took part in the 4th international planning competition ipc4 a statistical analysis of the results of ipc4 demonstrates the effectiveness of our approach in terms of both cputime and plan quality additional experiments show the good performance of the temporal reasoning techniques integrated into our planner 









m  koppel  r  feldman and  a  m segre 1994 biasdriven revision of logical domain theories volume 1 pages 159208



the theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies  in this paper we present our approach to the theory revision problem for propositional domain theories  the approach described here called ptr uses probabilities associated with domain theory elements to numerically track the flow of proof through the theory  this allows us to measure the precise role of a clause or literal in allowing or preventing a desired or undesired derivation for a given example  this information is used to efficiently locate and repair flawed elements of the theory  ptr is proved to converge to a theory which correctly classifies all examples and shown experimentally to be fast and accurate even for deep theories





f  ygge and  h  akkermans 1999 decentralized markets versus central control a comparative study volume 11 pages 301333







t  g dietterich and  g  bakiri 1995 solving multiclass learning problems via errorcorrecting output codes volume 2 pages 263286



multiclass learning problems involve finding a definitionfor an unknown function ifix whose range is a discrete setcontaining iki  2 values ie iki classes  thedefinition is acquired by studying collections of training examples ofthe form xi i f ixi  existing approaches tomulticlass learning problems include direct application of multiclassalgorithms such as the decisiontree algorithms c45 and cartapplication of binary concept learning algorithms to learn individualbinary functions for each of the i k i classes and application ofbinary concept learning algorithms with distributed outputrepresentations  this paper compares these three approaches to a newtechnique in which errorcorrecting codes are employed as adistributed output representation  we show that these outputrepresentations improve the generalization performance of both c45and backpropagation on a wide range of multiclass learning tasks  wealso demonstrate that this approach is robust with respect to changesin the size of the training sample the assignment of distributedrepresentations to particular classes and the application ofoverfitting avoidance techniques such as decisiontree pruningfinally we show thatlike the other methodsthe errorcorrectingcode technique can provide reliable class probability estimatestaken together these results demonstrate that errorcorrecting outputcodes provide a generalpurpose method for improving the performanceof inductive learning programs on multiclass problems





n  v chawla  k  w bowyer  l  o hall and  w  p kegelmeyer 2002 smote synthetic minority oversampling technique volume 16 pages 321357



an approach to the construction of classifiers from    imbalanced datasets is described a dataset is imbalanced if the    classification categories are not approximately equally    represented often realworld data sets are predominately composed of    normal examples with only a small percentage of abnormal or    interesting examples it is also the case that the cost of    misclassifying an abnormal interesting example as a normal example    is often much higher than the cost of the reverse    error undersampling of the majority normal class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class this paper shows that a combination of our method of    oversampling the minority abnormal class and undersampling the    majority normal class can achieve better classifier performance in    roc space than only undersampling the majority class  this paper    also shows that a combination of our method of oversampling the    minority class and undersampling the majority class can achieve    better classifier performance in roc space than varying the loss    ratios in ripper or class priors in naive bayes our method of    oversampling the minority class involves creating synthetic minority    class examples  experiments are performed using c45 ripper and a    naive bayes classifier the method is evaluated using the area under    the receiver operating characteristic curve auc and the roc convex    hull strategy





2006 ijcaijair best paper prize



we propose a perspective on knowledge compilation which    calls for analyzing different compilation approaches according to two    key dimensions the succinctness of the target compilation language    and the class of queries and transformations that the language    supports in polytime we then provide a knowledge compilation map    which analyzes a large number of existing target compilation languages    according to their succinctness and their polytime transformations and    queries we argue that such analysis is necessary for placing new    compilation approaches within the context of existing ones we also go    beyond classical flat target compilation languages based on cnf and    dnf and consider a richer nested class based on directed acyclic    graphs such as obdds which we show to include a relatively large    number of target compilation languages





y  wang c  hang and m  p singh 2011 a probabilistic approach for maintaining trust based on evidence volume 40 pages 221267



leading agentbased trust models address two important needs  first they show how an agent may estimate the trustworthiness of another agent based on prior interactions  second they show how agents may share their knowledge in order to cooperatively assess the trustworthiness of others  however in reallife settings information relevant to trust is usually obtained piecemeal not all at once  unfortunately the problem of maintaining trust has drawn little attention  existing approaches handle trust updates in a heuristic not a principled manner

this paper builds on a formal model that considers probability and certainty as two dimensions of trust  it proposes a mechanism using which an agent can update the amount of trust it places in other agents on an ongoing basis  this paper shows via simulation that the proposed approach a provides accurate estimates of the trustworthiness of agents that change behavior frequently and b captures the dynamic behavior of the agents  this paper includes an evaluation based on a real dataset drawn from amazon marketplace a leading ecommerce site





y  bengio and  p  frasconi 1995 diffusion of context and credit information in markovian models volume 3 pages 249270



this paper studies the problem of ergodicity of transition    probability matrices in markovian models such as hidden markov models    hmms and how it makes very difficult the task of learning to    represent longterm context for sequential data  this phenomenon    hurts the forward propagation of longterm context information as    well as learning a hidden state representation to represent longterm    context which depends on propagating credit information backwards in    time  using results from markov chain theory we show that this    problem of diffusion of context and credit is reduced when the    transition probabilities approach 0 or 1 ie the transition    probability matrices are sparse and the model essentially    deterministic  the results found in this paper apply to learning    approaches based on continuous optimization such as gradient descent    and the baumwelch algorithm



the agm framework is the benchmark approach in belief change since the framework assumes an underlying logic containing classical propositional logic it can not be applied to systems with a logic weaker than propositional logic  to remedy this limitation several researchers have studied agmstyle contraction and revision under the horn fragment of propositional logic ie horn logic  in this paper we contribute to this line of research by investigating the horn version of the agm entrenchmentbased contraction  the study is challenging as the construction of entrenchmentbased contraction refers to arbitrary disjunctions which are not expressible under horn logic  in order to adapt the construction to horn logic we make use of a horn approximation technique called horn strengthening we provide a representation theorem for the newly constructed contraction which we refer to as entrenchmentbased horn contraction  ideally contractions defined under horn logic ie horn contractions should be as rational as agm contraction  we propose the notion of horn equivalence which intuitively captures the equivalence between horn contraction and agm contraction  we show that under this notion entrenchmentbased horn contraction is equivalent to a restricted form of entrenchmentbased contraction









l  m de campos 1996 characterizations of decomposable dependency models volume 5 pages 289300



decomposable dependency models possess a number of    interesting and useful properties this paper presents new    characterizations of decomposable models in terms of independence    relationships which are obtained by adding a single axiom to the    wellknown set characterizing dependency models that are isomorphic to    undirected graphs we also briefly discuss a potential application of    our results to the problem of learning graphical models from data





m  suda 2014 property directed reachability for automated planning volume 50 pages 265319



property directed reachability pdr is a very promising recent method for deciding reachability in symbolically represented transition systems while originally conceived as a model checking algorithm for hardware circuits it has already been successfully applied in several other areas this paper is the first investigation of pdr from the perspective of automated planning

similarly to the planning as satisfiability paradigm pdr draws its strength from internally employing an efficient satsolver we show that most standard encoding schemes of planning into sat can be directly used to turn pdr into a planning algorithm as a nonobvious alternative we propose to replace the satsolver inside pdr by a planningspecific procedure implementing the same interface this satsolver free variant is not only more efficient but offers additional insights and opportunities for further improvements an experimental comparison to the state of the art planners finds it highly competitive solving most problems on several domains







f  t liu k  m ting y  yu and z  h zhou 2008 spectrum of variablerandom trees volume 32 pages 355384





l  finkelstein and  s  markovitch 1998 a selective macrolearning algorithm and its application to the nxn slidingtile puzzle volume 8 pages 223263



one of the most common mechanisms used for speeding up    problem solvers is macrolearning  macros are sequences of basic    operators acquired during problem solving  macros are used by the    problem solver as if they were basic operators  the major problem    that macrolearning presents is the vast number of macros that are    available for acquisition  macros increase the branching factor of    the search space and can severely degrade problemsolving efficiency    to make macro learning useful a program must be selective in    acquiring and utilizing macros  this paper describes a general method    for selective acquisition of macros solvable training problems are    generated in increasing order of difficulty  the only macros acquired    are those that take the problem solver out of a local minimum to a    better state  the utility of the method is demonstrated in several    domains including the domain of nxn slidingtile puzzles after    learning on small puzzles the system is able to efficiently solve    puzzles of any size





d  fisher 1996 iterative optimization and simplification of hierarchical clusterings volume 4 pages 147178



clustering is often used for discovering structure in data    clustering systems differ in the objective function used to evaluate    clustering quality and the control strategy used to search the space    of clusterings ideally the search strategy should consistently    construct clusterings of high quality but be computationally    inexpensive as well in general we cannot have it both ways but we    can partition the search so that a system inexpensively constructs a    tentative clustering for initial examination followed by iterative    optimization which continues to search in background for improved    clusterings given this motivation we evaluate an inexpensive    strategy for creating initial clusterings coupled with several    control strategies for iterative optimization each of which    repeatedly modifies an initial clustering in search of a better    one one of these methods appears novel as an iterative optimization    strategy in clustering contexts once a clustering has been    constructed it is judged by analysts  often according to    taskspecific criteria several authors have abstracted these criteria    and posited a generic performance task akin to pattern completion    where the error rate over completed patterns is used to externally    judge clustering utility given this performance task we adapt    resamplingbased pruning strategies used by supervised learning    systems to the task of simplifying hierarchical clusterings thus    promising to ease postclustering analysis finally we propose a    number of objective functions based on attributeselection measures    for decisiontree induction that might perform well on the error rate    and simplicity dimensions







a   khudyak kozorovitsky and o  kurland 2011 from identical to similar fusing retrieved lists based on interdocument similarities volume 41 pages 267296





r  debruyne and  c  bessiere 2001 domain filtering consistencies volume 14 pages 205230



enforcing local consistencies is one of the main features    of constraint reasoning which level of local consistency should be    used when searching for solutions in a constraint network is a basic    question arc consistency and partial forms of arc consistency have    been widely studied and have been known for sometime through the    forward checking or the mac search algorithms until recently    stronger forms of local consistency remained limited to those that    change the structure of the constraint graph and thus could not be    used in practice especially on large networks this paper focuses on    the local consistencies that are stronger than arc consistency    without changing the structure of the network ie only removing    inconsistent values from the domains in the last five years several    such local consistencies have been proposed by us or by others we    make an overview of all of them and highlight some relations between    them we compare them both theoretically and experimentally    considering their pruning efficiency and the time required to enforce    them













