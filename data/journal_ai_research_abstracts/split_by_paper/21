j  hoffmann i  weber and f  m kraft 2012 sap speaks pddl exploiting a softwareengineering model for planning in business process management volume 44 pages 587632



w  e walsh and  m  p wellman 2003 decentralized supply chain formation a market protocol and competitive equilibrium analysis volume 19 pages 513567



supply chain formation is the process of determining the structure and terms of exchange relationships to enable a multilevel multiagent production activity  we present a simple model of supply chains highlighting two characteristic features hierarchical subtask decomposition and resource contention  to decentralize the formation process we introduce a market price system over the resources produced along the chain  in a competitive equilibrium for this system agents choose locally optimal allocations with respect to prices and outcomes are optimal overall  to determine prices we define a market protocol based on distributed progressive auctions and myopic nonstrategic agent bidding policies  in the presence of resource contention this protocol produces better solutions than the greedy protocols common in the artificial intelligence and multiagent systems literature  the protocol often converges to highvalue supply chains and when competitive equilibria exist typically to approximate competitive equilibria  however complementarities in agent production technologies can cause the protocol to wastefully allocate inputs to agents that do not produce their outputs  a subsequent decommitment phase recovers a significant fraction of the lost surplus









c  domshlak and j  hoffmann 2007 probabilistic planning via heuristic forward search and weighted model counting volume 30 pages 565620



g  erkan and  d  r radev 2004 lexrank graphbased lexical centrality as salience in text summarization volume 22 pages 457479



we introduce a stochastic graphbased method for computing relative importance of textual units for natural language processing we test the technique on the problem of text summarization ts extractive ts relies on the concept of sentence salience to identify the most important sentences in a document or set of documents salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudosentence we consider a new approach lexrank for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences in this model a connectivity matrix based on intrasentence cosine similarity is used as the adjacency matrix of the graph representation of sentences our system based on lexrank ranked in first place in more than one task in the recent duc 2004 evaluation in this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier duc evaluations we discuss several methods to compute centrality using the similarity graph the results show that degreebased methods including lexrank outperform both centroidbased methods and other systems participating in duc in most of the cases furthermore the lexrank with threshold method outperforms the other degreebased techniques including continuous lexrank we also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents









d  a dolgov and e  h durfee 2006 resource allocation among agents with mdpinduced preferences volume 27 pages 505549



a  botea m  enzenberger m  mueller and j  schaeffer 2005 macroff improving ai planning with automatically learned macrooperators volume 24 pages 581621



despite recent progress in ai planning many benchmarks remain challenging for current planners in many domains the performance of a planner can greatly be improved by discovering and exploiting information about the domain structure that is not explicitly encoded in the initial pddl formulation in this paper we present and compare two automated methods that learn relevant information from previous experience in a domain and use it to solve new problem instances our methods share a common fourstep strategy first a domain is analyzed and structural information is extracted then macrooperators are generated based on the previously discovered structure a filtering and ranking procedure selects the most useful macrooperators finally the selected macros are used to speed up future searches  we have successfully used such an approach in the fourth international planning competition ipc4 our system macroff extends hoffmanns stateoftheart planner ff 23 with support for two kinds of macrooperators and with engineering enhancements we demonstrate the effectiveness of our ideas on benchmarks from international planning competitions our results indicate a large reduction in search effort in those complex domains where structural information can be inferred   





e  birnbaum and  e  l lozinskii 1999 the good old davisputnam procedure helps counting models volume 10 pages 457477



as was shown recently many important ai problems require    counting the number of models of propositional formulas the problem    of counting models of such formulas is according to present    knowledge computationally intractable in a worst case based on the    davisputnam procedure we present an algorithm cdp that computes    the exact number of models of a propositional cnf or dnf formula    f let m and n be the number of clauses and variables of f    respectively and let p denote the probability that a literal l of f    occurs in a clause c of f then the average running time of cdp is    shown to be onmd where d1log1p  the practical    performance of cdp has been estimated in a series of experiments on a    wide variety of cnf formulas



timebounded a is a realtime singleagent deterministic search algorithm that expands states of a graph in the same order as a does but that unlike a interleaves search and action execution  known to outperform stateoftheart realtime search algorithms based on korfs learning realtime a lrta in some benchmarks it has not been studied in detail and is sometimes not considered as a true realtime search algorithm since it fails in nonreversible problems even it the goal is still reachable from the current state  in this paper we propose and study timebounded bestfirst search tbbfs a straightforward generalization of the timebounded approach to any bestfirst search algorithm furthermore we propose restarting timebounded weighted a tbrwa an algorithm that deals more adequately with nonreversible search graphs eliminating  backtracking moves and incorporating search restarts and heuristic learning in nonreversible problems we prove that tbbfs terminates and we deduce cost bounds for the solutions returned by timebounded weighted a tbwa an instance of tbbfs furthermore we prove tbrwa under reasonable conditions terminates  we evaluate tbwa in both grid pathfinding and the 15puzzle in addition we evaluate tbrwa on the racetrack problem we compare our algorithms to lsslrtwa a variant of lrta that can exploit lookahead search and a weighted heuristic a general observation is that the performance of both tbwa and tbrwa improves as the weight parameter is increased in addition our timebounded algorithms almost always outperform lsslrtwa by a significant margin









m  ponsen s  de jong and m  lanctot 2011 computing approximate nash equilibria and robust bestresponses using sampling volume 42 pages 575605



this article discusses two contributions to decisionmaking in complex partially observable stochastic games first we apply two stateoftheart search techniques that use montecarlo sampling to the task of approximating a nashequilibrium ne in such games namely montecarlo tree search mcts and montecarlo counterfactual regret minimization mccfr mcts has been proven to approximate a ne in perfectinformation games we show that the algorithm quickly finds a reasonably strong strategy but not a ne in a complex imperfect information game ie poker mccfr on the other hand has theoretical ne convergence guarantees in such a game we apply mccfr for the first time in poker based on our experiments we may conclude that mcts is a valid approach if one wants to learn reasonably strong strategies fast whereas mccfr is the better choice if the quality of the strategy is most important 



s  de jong s  uyttendaele and k  tuyls 2008 learning to reach agreement in a continuous ultimatum game volume 33 pages 551574



it is wellknown that acting in an individually rational manner according to the principles of classical game theory may lead to suboptimal solutions in a class of problems named social dilemmas in contrast humans generally do not have much difficulty with social dilemmas as they are able to balance personal benefit and group benefit as agents in multiagent systems are regularly confronted with social dilemmas for instance in tasks such as resource allocation these agents may benefit from the inclusion of mechanisms thought to facilitate human fairness although many of such mechanisms have already been implemented in a multiagent systems context their application is usually limited to rather abstract social dilemmas with a discrete set of available strategies usually two given that many realworld examples of social dilemmas are actually continuous in nature we extend this previous work to more general dilemmas in which agents operate in a continuous strategy space the social dilemma under study here is the wellknown ultimatum game in which an optimal solution is achieved if agents agree on a common strategy we investigate whether a scalefree interaction network facilitates agents to reach agreement especially in the presence of fixedstrategy agents that represent a desired eg human outcome moreover we study the influence of rewiring in the interaction network the agents are equipped with continuousaction learning automata and play a large number of random pairwise games in order to establish a common strategy from our experiments we may conclude that results obtained in discretestrategy games can be generalized to continuousstrategy games to a certain extent a scalefree interaction network structure allows agents to achieve agreement on a common strategy and rewiring in the interaction network greatly enhances the agents ability to reach agreement however it also becomes clear that some alternative mechanisms such as reputation and volunteering have many subtleties involved and do not have convincing beneficial effects in the continuous case





the causal graph of a planning instance is an important tool for planning both in practice and in theory the theoretical studies of causal graphs have largely analysed the computational complexity of planning for instances where the causal graph has a certain structure often in combination with other parameters like the domain size of the variables chen and gim233nez ignored even the structure and considered only the size of the weakly connected components they proved that planning is tractable if the components are bounded by a constant and otherwise intractable their intractability result was however conditioned by an assumption from parameterised complexity theory that has no known useful relationship with the standard complexity classes we approach the same problem from the perspective of standard complexity classes and prove that planning is nphard for classes with unbounded components under an additional restriction we refer to as spclosed we then argue that most nphardness theorems for causal graphs are difficult to apply and thus prove a more general result even if the component sizes grow slowly and the class is not densely populated with graphs planning still cannot be tractable unless the polynomial hierachy collapses both these results still hold when restricted to the class of acyclic causal graphs we finally give a partial characterization of the borderline between nphard and npintermediate classes giving further insight into the problem







ana  armas romero mark  kaminski bernardo  cuenca grau and ian  horrocks 2016 module extraction in expressive ontology languages via datalog reasoning volume 55 pages 499564



module extraction is the task of computing a preferably small fragment m of an ontology t that preserves a class of entailments over a signature of interest s extracting modules of minimal size is wellknown to be computationally hard and often algorithmically infeasible especially for highly expressive ontology languages thus practical techniques typically rely on approximations where m provably captures the relevant entailments but is not guaranteed to be minimal existing approximations ensure that m preserves all secondorder entailments of t wrt s which is a stronger condition than is required in many applications and may lead to unnecessarily large modules in practice in this paper we propose a novel approach in which module extraction is reduced to a reasoning problem in datalog our approach generalises existing approximations in an elegant way more importantly it allows extraction of modules that are tailored to preserve only specific kinds of entailments and thus are often significantly smaller our evaluation on a wide range of ontologies confirms the feasibility and benefits of our approach in practice









acl 2013 best paper award





s  r k branavan h  chen j  eisenstein and r  barzilay 2009 learning documentlevel semantic properties from freetext annotations volume 34 pages 569603



this paper presents a new method for inferring the semantic properties of documents by leveraging freetext keyphrase annotations  such annotations are becoming increasingly abundant due to the recent dramatic growth in semistructured usergenerated online content one especially relevant domain is product reviews which are often annotated by their authors with proscons keyphrases such as a real bargain or good value these annotations are representative of the underlying semantic properties however unlike expert annotations they are noisy lay authors may use different labels to denote the same property and some labels may be missing  to learn using such noisy annotations we find a hidden paraphrase structure which clusters the keyphrases  the paraphrase structure is linked with a latent topic model of the review texts enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews  our approach is implemented as a hierarchical bayesian model with joint inference  we find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties  multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases



replanning via determinization is a recent popular approach for online planning in mdps in this paper we adapt this idea to classical nonstochastic domains with partial information and sensing actions presenting a new planner sdr sample determinize replan at each step we generate a  solution plan to a classical planning problem induced by the original problem we execute this plan as long as it is safe to do so when this is no longer the case we replan the classical planning problem we generate is based on the translationbased approach for conformant planning introduced by palacios and geffner the state of the classical planning problem generated in this approach captures the belief state of the agent in the original problem unfortunately when this method is applied to planning problems with sensing it yields a nondeterministic planning problem that is typically very large our main contribution is the introduction of state sampling techniques for overcoming these two problems in addition we introduce a novel lazy regressionbased method for querying the agents belief state during runtime  we provide a comprehensive experimental evaluation of the planner showing that it scales better than the stateoftheart clg planner on existing benchmark problems but also highlighting its weaknesses with new domains  we also discuss its theoretical guarantees











jl  p233rez de la cruz l  mandow and e  machuca 2013 a case of pathology in multiobjective heuristic search volume 48 pages 717732



multirobot path planning is difficult due to the combinatorial explosion of the search space with every new robot added complete search of the combined statespace soon becomes intractable in this paper we present a novel form of abstraction that allows us to plan much more efficiently the key to this abstraction is the partitioning of the map into subgraphs of known structure with entry and exit restrictions which we can represent compactly planning then becomes a search in the much smaller space of subgraph configurations once an abstract plan is found it can be quickly resolved into a correct but possibly suboptimal concrete plan without the need for further search we prove that this technique is sound and complete and demonstrate its practical effectiveness on a real map











piotr  krysta orestis  telelis and carmine  ventre 2015 mechanisms for multiunit combinatorial auctions with a few distinct goods volume 53 pages 721744



to achieve scalability of query answering the developers of semantic web applications are often forced to use incomplete owl 2 reasoners which fail to derive all answers for at least one query ontology and data set the lack of completeness guarantees however may be unacceptable for applications in areas such as health care and defence where missing answers can adversely affect the applications functionality furthermore even if an application can tolerate some level of incompleteness it is often advantageous to estimate how many and what kind of answers are being lost

our results thus provide a theoretical and practical foundation for the design of future ontologybased information systems that maximise scalability while minimising or even eliminating incompleteness of query answers











e  burns s  lemons w  ruml and r  zhou 2010 bestfirst heuristic search for multicore machines  volume 39 pages 689743







y  wu p  austrin t  pitassi and d  liu 2014 inapproximability of treewidth and related problems volume 49 pages 569600



j  keppens and  q  shen 2004 compositional model repositories via dynamic constraint satisfaction with orderofmagnitude preferences volume 21 pages 499550



the predominant knowledgebased approach to automated model construction compositional modelling employs a set of models of particular functional components  its inference mechanism takes a scenario describing the constituent interacting components of a system and translates it into a useful mathematical model  this paper presents a novel compositional modelling approach aimed at building model repositories  it furthers the field in two respects  firstly it expands the application domain of compositional modelling to systems that can not be easily described in terms of interacting functional components such as ecological systems  secondly it enables the incorporation of user preferences into the model selection process  these features are achieved by casting the compositional modelling problem as an activitybased dynamic preference constraint satisfaction problem where the dynamic constraints describe the restrictions imposed over the composition of partial models and the preferences correspond to those of the user of the automated modeller in addition the preference levels are represented through the use of symbolic values that differ in orders of magnitude







g  greco e  malizia l  palopoli and f  scarcello 2010 nontransferable utility coalitional games via mixedinteger linear constraints volume 38 pages 633685



coalitional games serve the purpose of modeling payoff distribution problems in scenarios where agents can collaborate by forming coalitions in order to obtain higher worths than by acting in isolation in the classical transferable utility tu setting coalition worths can be freely distributed amongst agents however in several application scenarios this is not the case and the nontransferable utility setting ntu must be considered where additional applicationoriented constraints are imposed on the possible worth distributions

in this paper an approach to define ntu games is proposed which is based on describing allowed distributions via a set of mixedinteger linear constraints applied to an underlying tu game it is shown that such games allow nontransferable conditions on worth distributions to be specified in a natural and succinct way the properties and the relationships among the most prominent solution concepts for ntu games that hold when they are applied on mixedinteger constrained games are investigated finally a thorough analysis is carried out to assess the impact of issuing constraints on the computational complexity of some of these solution concepts







b  bagheri hariri d  calvanese m  montali g  de giacomo r  de masellis and p  felli 2013 description logic knowledge and action bases volume 46 pages 651686



m  zytnicki c  gaspin s  de givry and t  schiex 2009 bounds arc consistency for weighted csps volume 35 pages 593621



  the weighted constraint satisfaction problem wcsp framework allows representing and solving problems involving both hard constraints and cost functions it has been applied to various problems including resource allocation bioinformatics scheduling etc to solve such problems solvers usually rely on branchandbound algorithms equipped with local consistency filtering mostly soft arc consistency  however these techniques are not well suited to solve problems with very large domains motivated by the resolution of an rna gene localization problem inside large genomic sequences and in the spirit of bounds consistency for large domains in crisp csps we introduce soft bounds arc consistency a new weighted local consistency specifically designed for wcsp with very large domains compared to  soft arc consistency bac provides significantly improved time and space asymptotic complexity in this paper we show how the semantics of cost functions can be exploited to further improve the time complexity of bac we also compare both in theory and in practice the efficiency of bac on a wcsp  with bounds consistency enforced on a crisp csp using cost variables on two different real problems modeled as wcsp including our rna gene localization problem we observe that maintaining bounds arc consistency outperforms arc consistency and also improves over bounds consistency enforced on a constraint model with cost variables





we introduce a novel approach to compressed path databases space efficient oracles used to very quickly identify the first edge on a shortest path our algorithm achieves query running times on the 100 nanosecond scale being significantly faster than stateoftheart firstmove oracles from the literature space consumption is competitive due to a compression approach that rearranges rows and columns in a firstmove matrix and then performs run length encoding rle on the contents of the matrix one variant of our implemented system was by a convincing margin the fastest entry in the 2014 gridbased path planning competition

we give a first tractability analysis for the compression scheme used by our algorithm we study the complexity of computing a database of minimum size for general directed and undirected graphs we find that in both cases the problem is npcomplete we also show that for graphs which can be decomposed along articulation points the problem can be decomposed into independent parts with a corresponding reduction in its level of difficulty in particular this leads to simple and tractable algorithms with linear running time which yield optimal compression results for trees







a  roy 2006 fault tolerant boolean satisfiability volume 25 pages 503527



a deltamodel is a satisfying assignment of a boolean formula for which any small alteration such as a single bit flip can be repaired by flips to some small number of other bits yielding a new satisfying assignment  these satisfying assignments represent robust solutions to optimization problems eg scheduling where it is possible to recover from unforeseen events eg a resource becoming unavailable  the concept of deltamodels was introduced by ginsberg parkes and roy aaai 1998  where it was proved that finding deltamodels for general boolean formulas is npcomplete  in this paper we extend that result by studying the complexity of finding deltamodels for classes of boolean formulas which are known to have polynomial time satisfiability solvers  in particular we examine 2sat hornsat affinesat dualhornsat 0valid and 1valid sat  we see a wide variation in the complexity of finding deltamodels eg while 2sat and affinesat have polynomial time tests for deltamodels testing whether a hornsat formula has one is npcomplete





w  w cohen 1995 paclearning recursive logic programs efficient algorithms volume 2 pages 501539



we present algorithms that learn certain classes of   functionfree recursive logic programs in polynomial time from   equivalence queries  in particular we show that a single kary   recursive constantdepth determinate clause is learnable twoclause   programs consisting of one learnable recursive clause and one   constantdepth determinate nonrecursive clause are also learnable if   an additional basecase oracle is assumed  these results   immediately imply the paclearnability of these classes  although   these classes of learnable recursive programs are very constrained it   is shown in a companion paper that they are maximally general in that   generalizing either class in any natural way leads to a   computationally difficult learning problem  thus taken together with   its companion paper this paper establishes a boundary of efficient   learnability for recursive logic programs









i  a kash e  j friedman and j  y halpern 2011 multiagent learning in large anonymous games volume 40 pages 571598



p  idestamalmquist 1995 generalization of clauses under implication volume 3 pages 467489



in the area of inductive learning generalization is a main    operation and the usual definition of induction is based on logical    implication  recently there has been a rising interest in clausal    representation of knowledge in machine learning almost all inductive    learning systems that perform generalization of clauses use the    relation thetasubsumption instead of implication the main reason is    that there is a wellknown and simple technique to compute least    general generalizations under thetasubsumption but not under    implication  however generalization under thetasubsumption is    inappropriate for learning recursive clauses which is a crucial    problem since recursion is the basic program structure of logic    programs     we note that implication between clauses is undecidable and we    therefore introduce a stronger form of implication called    timplication which is decidable between clauses we show that for    every finite set of clauses there exists a least general    generalization under timplication  we describe a technique to reduce    generalizations under implication of a clause to generalizations under    thetasubsumption of what we call an expansion of the original    clause moreover we show that for every nontautological clause there    exists a tcomplete expansion which means that every generalization    under timplication of the clause is reduced to a generalization under    thetasubsumption of the expansion





a  lopezortiz s  angelopoulos and a  m hamel 2014 optimal scheduling of contract algorithms for anytime problemsolving volume 51 pages 533554



a contract algorithm is an algorithm which is given as part of the input a specified amount of allowable computation time the algorithm must then complete its execution within the allotted time an interruptible algorithm in contrast can be interrupted at an arbitrary point in time at which point it must report its currently best solution it is known that contract algorithms can simulate interruptible algorithms using  iterative deepening techniques this simulation is done at a penalty  in the performance of the solution as measured by the socalled acceleration ratio

lastly we show how to evaluate the average acceleration ratio of the class of exponential strategies in the setting of n problem instances and m parallel processors this is a broad class of schedules that tend to be either optimal or nearoptimal for several variants of the basic problem



icaps 2012 best paper award



heuristic functions based on the delete relaxation compute upper and lower bounds on the optimal deleterelaxation heuristic h and are of paramount importance in both optimal and satisficing planning here we introduce a principled and flexible technique for improving h by augmenting deleterelaxed planning tasks with a limited amount of delete information this is done by introducing special fluents that explicitly represent conjunctions of fluents in the original planning task rendering h the perfect heuristic h in the limit previous work has introduced a method in which the growth of the task is potentially exponential in the number of conjunctions introduced we formulate an alternative technique relying on conditional effects limiting the growth of the task to be linear in this number we show that this method still renders h the perfect heuristic h in the limit we propose techniques to find an informative set of conjunctions to be introduced in different settings and analyze and extend existing methods for lowerbounding and upperbounding h in the presence of conditional effects we evaluate the resulting heuristic functions empirically on a set of ipc benchmarks and show that they are sometimes much more informative than standard deleterelaxation heuristics









l  bordeaux m  cadoli and t  mancini 2008 a unifying framework for structural properties of csps definitions complexity tractability volume 32 pages 607629



s  bhansali  g  a kramer and  t  j hoar 1996 a principled approach towards symbolic geometric constraint satisfaction volume 4 pages 419443



an important problem in geometric reasoning is to find the    configuration of a collection of geometric bodies so as to satisfy a    set of given constraints recently it has been suggested that this    problem can be solved efficiently by symbolically reasoning about    geometry this approach called degrees of freedom analysis employs a    set of specialized routines called plan fragments that specify how to    change the configuration of a set of bodies to satisfy a new    constraint while preserving existing constraints a potential    drawback which limits the scalability of this approach is concerned    with the difficulty of writing plan fragments  in this paper we    address this limitation by showing how these plan fragments can be    automatically synthesized using first principles about geometric    bodies actions and topology















j  hoffmann i  weber and f  m kraft 2012 sap speaks pddl exploiting a softwareengineering model for planning in business process management volume 44 pages 587632

