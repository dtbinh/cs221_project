

l  finkelstein  s  markovitch and  e  rivlin 2003 optimal schedules for parallelizing anytime algorithms the case of shared resources volume 19 pages 73138



the performance of anytime algorithms can be improved by simultaneously solving several instances of algorithmproblem pairs these pairs may include different instances of a problem such as starting from a different initial state different algorithms if several alternatives exist or several runs of the same algorithm for nondeterministic algorithms in this paper we present a methodology for designing an optimal scheduling policy based on the statistical characteristics of the algorithms involved we formally analyze the case where the processes share resources a singleprocessor model and provide an algorithm for optimal scheduling  we analyze theoretically and empirically the behavior of our scheduling algorithm for various distribution types  finally we present empirical results of applying our scheduling algorithm to the latin square problem





p  agre and  i  horswill 1997 lifeworld analysis volume 6 pages 111145



we argue that the analysis of agentenvironment interactions should be extended to include the conventions and invariants maintained by agents throughout their activity  we refer to this thicker notion of environment as a lifeworld and present a partial set of formal tools for describing structures of lifeworlds and the ways in which they computationally simplify activity  as one specific example we apply the tools to the analysis of the toast system and show how versions of the system with very different control structures in fact implement a common control structure together with different conventions for encoding task state in the positions or states of objects in the environment









j  l bredin d  c parkes and q  duong 2007 chain a dynamic double auction framework for matching patient agents  volume 30 pages 133179



h  r andersen t  hadzic and d  pisinger 2010 interactive cost configuration over decision diagrams volume 37 pages 99139



in many ai domains such as product configuration a user should interactively specify a solution that must satisfy  a set of constraints in such scenarios offline compilation of feasible solutions into a tractable representation is an important approach to delivering efficient backtrackfree  user interaction online in particularbinary decision diagrams bdds have been successfully used as a compilation target for product and service configuration in this paper we discuss how to extend bddbased configuration to scenarios involving cost functions which express user preferences

we first show that an efficient robust and easy to implement extension is possible if the cost function is additive and feasible solutions are represented using multivalued decision diagrams mdds we also discuss the effect on mdd size if the cost function is nonadditive or if it is encoded explicitly into mdd we then discuss interactive configuration in the presence of multiple cost functions we prove that even in its simplest form multiplecost configuration is nphard in the input mdd however for solving twocost configuration we develop a pseudopolynomial scheme and a fully polynomial approximation scheme the applicability of our approach is demonstrated through experiments over realworld configuration models and productcatalogue datasets response times are generally within a fraction of a second even for very large instances 





w  zhang and  n  l zhang 2005 restricted value iteration theory and algorithms volume 23 pages 123165



value iteration is a popular algorithm for finding near optimal policies for pomdps  it is inefficient due to the need to account for the entire belief space which necessitates the solution of large numbers of linear programs  in this paper we study value iteration restricted to belief subsets we show that together with properly chosen belief subsets restricted value iteration yields nearoptimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time we also apply restricted value iteration to two interesting classes of pomdps namely informative pomdps and neardiscernible pomdps





alejandro  moreo fern225ndez andrea  esuli and fabrizio  sebastiani 2016 lightweight random indexing for polylingual text classification volume 57 pages 151185



multilingual text classification mltc is a text classification task in which documents are written each in one among a set l of natural languages and in which all documents must be classified under the same classification scheme irrespective of language there are two main variants of mltc namely crosslingual text classification cltc and polylingual text classification pltc in pltc which is the focus of this paper we assume differently from cltc that for each language in l there is a representative set of training documents pltc consists of improving the accuracy of each of the l monolingual classifiers by also leveraging the training documents written in the other l 8722 1 languages the obvious solution consisting of generating a single polylingual classifier from the juxtaposed monolingual vector spaces is usually infeasible since the dimensionality of the resulting vector space is roughly l times that of a monolingual one and is thus often unmanageable as a response the use of machine translation tools or multilingual dictionaries has been proposed however these resources are not always available or are not always free to use

one machinetranslationfree and dictionaryfree method that to the best of our knowledge has never been applied to pltc before is random indexing ri we analyse ri in terms of space and time efficiency and propose a particular configuration of it that we dub lightweight random indexing  lri by running experiments on two well known public benchmarks reuters rcv1rcv2 a comparable corpus and jrcacquis a parallel one we show lri to outperform both in terms of effectiveness and efficiency a number of previously proposed machinetranslationfree and dictionaryfree pltc methods that we use as baselines





h  chockler and  j  y halpern 2004 responsibility and blame a structuralmodel approach volume 22 pages 93115



causality is typically treated an allornothing concept either a is a cause of b or it is not we extend the definition of causality introduced by halpern and pearl 2004a to take into account the degree of responsibility of a for b  for example if someone wins an election 110 then each person who votes for him is less responsible for the victory than if he had won 65  we then define a notion of degree of blame which takes into account an agents epistemic state roughly speaking the degree of blame of a for b is the expected degree of responsibility of a for b taken over the epistemic state of an agent







m  lapata and a  lascarides 2006 learning sentenceinternal temporal relations volume 27 pages 85117



in this paper we propose a data intensive approach for inferring sentenceinternal temporal relations temporal inference is relevant for practical nlp applications which either extract or synthesize temporal information eg summarisation question answering  our method bypasses the need for manual coding by exploiting the presence of markers like after which overtly signal a temporal relation we first show that models trained on main and subordinate clauses connected with a temporal marker achieve good performance on a pseudodisambiguation task simulating temporal inference during testing the temporal marker is treated as unseen and the models must select the right marker from a set of possible candidates  secondly we assess whether the proposed approach holds promise for the semiautomatic creation of temporal annotations  specifically we use a model trained on noisy and approximate data ie main and subordinate clauses to predict intrasentential relations present in timebank a corpus annotated rich temporal information  our experiments compare and contrast several probabilistic models differing in their feature space linguistic assumptions and data requirements  we evaluate performance against gold standard corpora and also against human subjects 



p  j hawkins  v  lagoon and  p  j stuckey 2005 solving set constraint satisfaction problems using robdds volume 24 pages 109156



in this paper we present a new approach to modeling finite set domain constraint problems using reduced ordered binary decision diagrams robdds we show that it is possible to construct an efficient set domain propagator which compactly represents many set domains and set constraints using robdds  we demonstrate that the robddbased approach provides unprecedented flexibility in modeling constraint satisfaction problems leading to performance improvements we also show that the robddbased modeling approach can be extended to the modeling of integer and multiset constraint problems in a straightforward manner since domain propagation is not always practical we also show how to incorporate less strict consistency notions into the robdd framework such as set bounds cardinality bounds and lexicographic bounds consistency finally we present experimental results that demonstrate the robddbased solver performs better than various more conventional constraint solvers on several standard set constraint problems





j  peral and  a  ferrandez 2003 translation of pronominal anaphora between english and spanish discrepancies and evaluation volume 18 pages 117147



this paper evaluates the different tasks carried out in the    translation of pronominal anaphora in a machine translation mt    system the mt interlingua approach named agir anaphora generation    with an interlingua representation improves upon other proposals    presented to date because it is able to translate intersentential    anaphors detect coreference chains and translate spanish zero    pronouns into englishissues hardly considered by other systems the    paper presents the resolution and evaluation of these anaphora    problems in agir with the use of different kinds of knowledge    lexical morphological syntactic and semantic the translation of    english and spanish anaphoric thirdperson personal pronouns    including spanish zero pronouns into the target language has been    evaluated on unrestricted corpora we have obtained a precision of    804 and 848 in the translation of spanish and english pronouns    respectively although we have only studied the spanish and english    languages our approach can be easily extended to other languages such    as portuguese italian or japanese





d  long and  m  fox 1999 efficient implementation of the plan graph in stan volume 10 pages 87115



stan is a graphplanbased planner socalled because it uses    a variety of state analysis techniques to enhance its performance    stan competed in the aips98 planning competition where it compared    well with the other competitors in terms of speed finding solutions    fastest to many of the problems posed although the domain analysis    techniques stan exploits are an important factor in its overall    performance we believe that the speed at which stan solved the    competition problems is largely due to the implementation of its plan    graph the implementation is based on two insights that many of the    graph construction operations can be implemented as bitlevel logical    operations on bit vectors and that the graph should not be explicitly    constructed beyond the fix point this paper describes the    implementation of stans plan graph and provides experimental results    which demonstrate the circumstances under which advantages can be    obtained from using this implementation







javad  azimi xiaoli  fern and alan  fern 2016 budgeted optimization with constrained experiments volume 56 pages 119152



motivated by a realworld problem we study a novel budgeted optimization problem where the goal is to optimize an unknown function f given a budget by requesting a sequence of samples from the function in our setting however evaluating the function at precisely specified points is not practically possible due to prohibitive costs instead we can only request constrained experiments a constrained experiment denoted by q specifies a subset of the input space for the experimenter to sample the function from the outcome of q includes a sampled experiment x and its function output fx importantly as the constraints of q become looser the cost of fulfilling the request decreases but the uncertainty about the location x increases our goal is to manage this tradeoff by selecting a set of constrained experiments that best optimize f within the budget  we study this problem in two different settings the nonsequential or batch setting where a set of constrained experiments is selected at once and the sequential setting where experiments are selected one at a time we evaluate our proposed methods for both settings using synthetic and real functions the experimental results demonstrate the efficacy of the proposed methods



r  hoshino and k  kawarabayashi 2011 scheduling bipartite tournaments to minimize total travel distance volume 42 pages 91124



in many professional sports leagues teams from opposing leaguesconferences compete against one another playing interleague games  this is an example of a bipartite tournament  in this paper we consider the problem of reducing the total travel distance of bipartite tournaments by analyzing interleague scheduling from the perspective of discrete optimization  this research has natural applications to sports scheduling especially for leagues such as the national basketball association nba where teams must travel long distances across north america to play all their games thus consuming much time money and greenhouse gas emissions

we introduce the bipartite traveling tournament problem bttp the interleague variant of the wellstudied traveling tournament problem we prove that the 2nteam bttp is npcomplete but for small values of n a distanceoptimal interleague schedule can be generated from an algorithm based on minimumweight 4cyclecovers  we apply our theoretical results to the 12team nippon professional baseball npb league in japan producing a provablyoptimal schedule requiring 42950 kilometres of total team travel a 16 reduction compared to the actual distance traveled by these teams during the 2010 npb season  we also develop a nearlyoptimal interleague tournament for the 30team nba league just 38 higher than the trivial theoretical lower bound





y  gal and a  pfeffer 2008 networks of influence diagrams a formalism for representing  agents  beliefs and  decisionmaking processes volume 33 pages 109147



this paper presents networks of influence diagrams nid a compact natural and highly expressive language for reasoning about agents beliefs and decisionmaking processes  nids are graphical structures in which agents mental models are represented as nodes in a network a mental model for an agent may itself use descriptions of the mental models of other agents nids are demonstrated by examples showing how they can be used to describe conflicting and cyclic belief structures and certain forms of bounded rationality  in an opponent modeling domain nids were able to outperform other computational agents whose strategies were not known in advance  nids are equivalent in representation to bayesian games  but they are more compact and structured than this formalism in particular the equilibrium definition for nids makes an explicit distinction between agents optimal strategies and how they actually behave in reality





c  backstrom 1998 computational aspects of reordering plans volume 9 pages 99137



this article studies the problem of modifying the action    ordering of a plan in order to optimise the plan according to various    criteria  one of these criteria is to make a plan less constrained    and the other is to minimize its parallel execution time  three    candidate definitions are proposed for the first of these criteria    constituting a sequence of increasing optimality guarantees  two of    these are based on deordering plans which means that ordering    relations may only be removed not added while the third one uses    reordering where arbitrary modifications to the ordering are allowed    it is shown that only the weakest one of the three criteria is    tractable to achieve the other two being nphard and even difficult    to approximate  similarly optimising the parallel execution time of    a plan is studied both for deordering and reordering of plans  in the    general case both of these computations are nphard  however it is    shown that optimal deorderings can be computed in polynomial time for    a class of planning languages based on the notions of producers    consumers and threats which includes most of the commonly used    planning languages  computing optimal reorderings can potentially    lead to even faster parallel executions but this problem remains    nphard and difficult to approximate even under quite severe restrictions





i  refanidis and  i  vlahavas 2001 the grt planning system backward heuristic construction in forward statespace planning volume 15 pages 115161



this paper presents grt a domainindependent heuristic    planning system for strips worlds grt solves problems in two    phases in the preprocessing phase it estimates the distance between    each fact and the goals of the problem in a backward direction then    in the search phase these estimates are used in order to further    estimate the distance between each intermediate state and the goals    guiding so the search process in a forward direction and on a    bestfirst basis the paper presents the benefits from the adoption of    opposite directions between the preprocessing and the search phases    discusses some difficulties that arise in the preprocessing phase and    introduces techniques to cope with them moreover it presents several    methods of improving the efficiency of the heuristic by enriching the    representation and by reducing the size of the problem finally a    method of overcoming local optimal states based on domain axioms is    proposed according to it difficult problems are decomposed into    easier subproblems that have to be solved sequentially the    performance results from various domains including those of the    recent planning competitions show that grt is among the fastest    planners





b  motik r  shearer and i  horrocks 2009 hypertableau reasoning for description logics volume 36 pages 165228



we present a novel reasoning calculus for the description logic shoiqa knowledge representation formalism with applications in areas such as the semantic web unnecessary nondeterminism and the construction of large models are two primary sources of inefficiency in the tableaubased reasoning calculi used in stateoftheart reasoners in order to reduce nondeterminism we base our calculus on hypertableau and hyperresolution calculi which we extend with a blocking condition to ensure termination in order to reduce the size of the constructed models we introduce anywhere pairwise blocking we also present an improved nominal introduction rule that ensures termination in the presence of nominals inverse roles and number restrictionsa combination of dl constructs that has proven notoriously difficult to handle our implementation shows significant performance improvements over stateoftheart reasoners on several wellknown ontologies





k  xu and  w  li 2000 exact phase transitions in random constraint satisfaction problems volume 12 pages 93103



in this paper we propose a new type of random csp model    called model rb which is a revision to the standard model b it is    proved that phase transitions from a region where almost all problems    are satisfiable to a region where almost all problems are    unsatisfiable do exist for model rb as the number of variables    approaches infinity  moreover the critical values at which the phase    transitions occur are also known exactly by relating the hardness of    model rb to model b it is shown that there exist a lot of hard    instances in model rb





j  c boerkoel jr and e  h durfee 2013 distributed reasoning for multiagent simple temporal problems volume 47 pages 95156



this research focuses on building foundational algorithms for scheduling agents that assist people in managing their activities in environments where tempo and complex activity interdependencies outstrip peoples cognitive capacity  we address the critical challenge of reasoning over individuals interacting schedules to efficiently answer queries about how to meet scheduling goals while respecting individual privacy and autonomy to the extent possible  we formally define the multiagent simple temporal problem for naturally capturing and reasoning over the distributed but interconnected scheduling problems of multiple individuals  our hypothesis is that combining bottomup and topdown approaches will lead to effective solution techniques  in our bottomup phase an agent externalizes constraints that compactly summarize how its local subproblem affects other agents subproblems whereas in our topdown phase an agent proactively constructs and internalizes new local constraints that decouple its subproblem from others  we confirm this hypothesis by devising distributed algorithms that calculate summaries of the joint solution space for multiagent scheduling problems without centralizing or otherwise redistributing the problems  the distributed algorithms permit concurrent execution to achieve significant speedup over the current art and also increase the level of privacy and independence in individual agent reasoning  these algorithms are most advantageous for problems where interactions between the agents are sparse compared to the complexity of agents individual problems





saif  m mohammad mohammad  salameh and svetlana  kiritchenko 2016 how translation alters sentiment volume 55 pages 95130



sentiment analysis research has predominantly been on english texts thus there exist many sentiment resources for english but less so for other languages approaches to improve sentiment analysis in a resourcepoor focus language include a translate the focus language text into a resourcerich language such as english and apply a powerful english sentiment analysis system on the text and b translate resources such as sentiment labeled corpora and sentiment lexicons from english into the focus language and use them as additional resources in the focuslanguage sentiment analysis system in this paper we systematically examine both options we use arabic social media posts as standin for the focus language text we show that sentiment analysis of english translations of arabic texts produces competitive results wrt arabic sentiment analysis we show that arabic sentiment analysis systems benefit from the use of automatically translated english sentiment lexicons we also conduct manual annotation studies to examine why the sentiment of a translation is different from the sentiment of the source word or text this is especially relevant for building better automatic translation systems in the process we create a stateoftheart arabic sentiment analysis system a new dialectal arabic sentiment lexicon and the first arabicenglish parallel corpus that is independently annotated for sentiment by arabic and english speakers





ronald  de haan iyad  kanj and stefan  szeider 2015 on the subexponentialtime complexity of csp volume 52 pages 203234



not all npcomplete problems share the same practical hardness with respect to exact computation  whereas some npcomplete problems are amenable to efficient computational methods others are yet to show any such sign it becomes a major challenge to develop a theoretical framework that is more finegrained than the theory of npcompleteness and that can explain the distinction between the exact complexities of various npcomplete problems this distinction is highly relevant for constraint satisfaction problems under natural restrictions where various shades of hardness can be observed in practice





a  i coles and a  j smith 2007 marvin a heuristic search planner with online macroaction learning volume 28 pages 119156



this paper describes marvin a planner that competed in the fourth international planning competition ipc 4 marvin uses actionsequencememoisation techniques to generate macroactions which are then used during search for a solution plan we provide an overview of its architecture and search behaviour detailing the algorithms used we also empirically demonstrate the effectiveness of its features in various planning domains in particular the effects on performance due to the use of macroactions the novel features of its search behaviour and the native support of adl and derived predicates







d  s bernstein c  amato e  a hansen and s  zilberstein 2009 policy iteration for decentralized control of markov decision processes volume 34 pages 89132



coordination of distributed agents is required for problems arising in many areas including multirobot systems networking and ecommerce  as a formal framework for such problems we use the decentralized partially observable markov decision process decpomdp  though much work has been done on optimal dynamic programming algorithms for the singleagent version of the problem optimal algorithms for the multiagent case have been elusive  the main contribution of this paper is an optimal policy iteration algorithm for solving decpomdps  the algorithm uses stochastic finitestate controllers to represent policies  the solution can include a correlation device which allows agents to correlate their actions without communicating  this approach alternates between expanding the controller and performing valuepreserving transformations which modify the controller without sacrificing value  we present two efficient valuepreserving transformations one can reduce the size of the controller and the other can improve its value while keeping the size fixed  empirical results demonstrate the usefulness of valuepreserving transformations in increasing value while keeping controller size to a minimum to broaden the applicability of the approach we also present a heuristic version of the policy iteration algorithm which sacrifices convergence to optimality  this algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents actions are known while this assumption may not hold in general it helps produce higher quality solutions in our test problems







sa  mirroshandel and g  ghassemsani 2012 towards unsupervised learning of temporal relations between events volume 45 pages 125163



a  cal236 g  gottlob and m  kifer 2013 taming the infinite chase query answering under expressive relational constraints volume 48 pages 115174



the chase algorithm is a fundamental tool for query evaluation and for testing query containment under tuplegenerating dependencies tgds and equalitygenerating dependencies egds  so far most of the research on this topic has focused on cases where the chase procedure terminates  this paper introduces expressive classes of tgds defined via syntactic restrictions guarded tgds gtgds and weakly guarded sets of tgds wgtgds  for these classes the chase procedure is not guaranteed to terminate and thus may have an infinite outcome nevertheless we prove that the problems of conjunctivequery answering and query containment under such tgds are decidable  we provide decision procedures and tight complexity bounds for these problems  then we show how egds can be incorporated into our results by providing conditions under which egds do not harmfully interact with tgds and do not affect the decidability and complexity of query answering  we show applications of the aforesaid classes of constraints to the problem of answering conjunctive queries in flogic lite an objectoriented ontology language and in some tractable description logics







y  zhang and y  ding 2008 ctl model update for system modifications volume 31 pages 113155



model checking is a promising technology which has been applied for verification of many hardware and software systems in this paper we introduce the concept of model update towards the development of an automatic system modification tool that extends model checking functions we define primitive update operations on the models of computation tree logic ctl and formalize the principle of minimal change for ctl model update these primitive update operations together with the underlying minimal change principle serve as the foundation for ctl model update essential semantic and computational characterizations are provided for our ctl model update approach we then describe a formal algorithm that implements this approach we also illustrate two case studies of ctl model updates for the wellknown microwave oven example and the andrew file system 1 from which we further propose a method to optimize the update results in complex system modifications



marijn  heule matti  j228rvisalo florian  lonsing martina  seidl and armin  biere 2015 clause elimination for sat and qsat volume 53 pages 127168



the famous archetypical npcomplete problem of boolean satisfiability sat and its pspacecomplete generalization of quantified boolean satisfiability qsat have become central declarative programming paradigms through which realworld instances of various computationally hard problems can be efficiently solved this success has been achieved through several breakthroughs in practical implementations of decision procedures for sat and qsat that is in sat and qsat solvers here simplification techniques for conjunctive normal form cnf for sat and for prenex conjunctive normal form pcnf for qsatthe standard input formats of sat and qsat solvershave recently proven very effective in increasing solver efficiency when applied before ie in preprocessing or during ie in inprocessing satisfiability search

in this article we develop and analyze clause elimination procedures for pre and inprocessing clause elimination procedures form a family of pcnf formula simplification techniques which remove clauses that have specific in practice polynomialtime redundancy properties while maintaining the satisfiability status of the formulas extending known procedures such as tautology subsumption and blocked clause elimination we introduce novel elimination procedures based on asymmetric variants of these techniques and also develop a novel family of socalled covered clause elimination procedures as well as natural liftings of the cnflevel procedures to pcnf we analyze the considered clause elimination procedures from various perspectives furthermore for the variants not preserving logical equivalence under clause elimination we show how to reconstruct solutions to original cnfs from satisfying assignments to simplified cnfs which is important for practical applications for the procedures complementing the more theoretical analysis we present results on an empirical evaluation on the practical importance of the clause elimination procedures in terms of the effect on solver runtimes on standard realworld application benchmarks it turns out that the importance of applying the clause elimination procedures developed in this work is empirically emphasized in the context of stateoftheart qsat solving





c  g nevillmanning and  i  h witten 1997 identifying hierarchical structure in sequences a lineartime algorithm volume 7 pages 6782



sequitur is an algorithm that infers a hierarchical    structure from a sequence of discrete symbols by replacing repeated    phrases with a grammatical rule that generates the phrase and    continuing this process recursively the result is a hierarchical    representation of the original sequence which offers insights into    its lexical structure the algorithm is driven by two constraints that    reduce the size of the grammar and produce structure as a byproduct     sequitur breaks new ground by operating incrementally moreover the    methods simple structure permits a proof that it operates in space    and time that is linear in the size of the input our implementation    can process 50000 symbols per second and has been applied to an    extensive range of real world sequences







t  flati and r  navigli 2012 the cqc algorithm cycling in graphs to semantically enrich and enhance a bilingual dictionary volume 43 pages 135171



bilingual machinereadable dictionaries are knowledge resources useful in many automatic tasks however compared to monolingual computational lexicons like wordnet bilingual dictionaries typically provide a lower amount of structured information such as lexical and semantic relations and often do not cover the entire range of possible translations for a word of interest in this paper we present cycles and quasicycles cqc a novel algorithm for the automated disambiguation of ambiguous translations in the lexical entries of a bilingual machinereadable dictionary the dictionary is represented as a graph and cyclic patterns are sought in the graph to assign an appropriate sense tag to each translation in a lexical entry further we use the algorithms output to improve the quality of the dictionary itself by suggesting accurate solutions to structural problems such as misalignments partial alignments 

and missing entries finally we successfully apply cqc to the task of synonym extraction



g  chalkiadakis e  elkind e  markakis m  polukarov and n  r jennings 2010 cooperative games with overlapping coalitions volume 39 pages 179216



in the usual models of cooperative game theory the outcome of a coalition formation process is either the grand coalition or a coalition structure that consists of disjoint coalitions however in many domains where coalitions are associated with tasks an agent may be involved in executing more than one task and thus may distribute his resources among several coalitions to tackle such scenarios we introduce a model for cooperative games with overlapping coalitionsor overlapping coalition formation ocf games we then explore the issue of stability in this setting in particular we introduce a notion of the core which generalizes the corresponding notion in the traditional nonoverlapping scenario then under some quite general conditions we characterize the elements of the core and show that any element of the core maximizes the social welfare we also introduce a concept of balancedness for overlapping coalitional games and use it to characterize coalition structures that can be extended to elements of the core finally we generalize the notion of convexity to our setting and show that under some natural assumptions convex games have a nonempty core moreover we introduce two alternative notions of stability in ocf that allow a wider range of deviations and explore the relationships among the corresponding definitions of the core as well as the classic nonoverlapping core and the aubin core we illustrate the general properties of the three cores and also study them from a computational perspective thus obtaining additional insights into their fundamental structure





j  w crandall 2014 towards minimizing disappointment in repeated games volume 49 pages 111142



we consider the problem of learning in repeated games against arbitrary associates  specifically we study the ability of expert algorithms to quickly learn effective strategies in repeated games towards the ultimate goal of learning nearoptimal behavior against any arbitrary associate within only a handful of interactions  our contribution is threefold  first we advocate a new metric called disappointment for evaluating expert algorithms in repeated games  unlike minimizing traditional notions of regret minimizing disappointment in repeated games is equivalent to maximizing payoffs  unfortunately eliminating disappointment is impossible to guarantee in general  however it is possible for an expert algorithm to quickly achieve low disappointment against many known classes of algorithms in many games  second we show that popular existing expert algorithms often fail to achieve low disappointment against a variety of associates particularly in early rounds of the game  finally we describe a new metaalgorithm that can be applied to existing expert algorithms to substantially reduce disappointment in many twoplayer repeated games when associates follow various static reinforcement learning and expert algorithms





m  s boddy 2003 imperfect match  pddl 21 and real applications volume 20 pages 133137



pddl was originally conceived and constructed as a lingua franca for the international planning competition  pddl21 embodies a set of extensions intended to support the expression of something closer to real planning problems  this objective has only been partially achieved due in large part to a deliberate focus on not moving too far from classical planning models and solution methods





k  o stanley and  r  miikkulainen 2004 competitive coevolution through evolutionary complexification volume 21 pages 63100



two major goals in machine learning are the discovery and improvement of solutions to complex problems  in this paper we argue that complexification ie the incremental elaboration of solutions through adding new structure achieves both these goals  we demonstrate the power of complexification through the neuroevolution of augmenting topologies neat method which evolves increasingly complex neural network architectures  neat is applied to an openended coevolutionary robot duel domain where robot controllers compete head to head  because the robot duel domain supports a wide range of strategies and because coevolution benefits from an escalating arms race it serves as a suitable testbed for studying complexification  when compared to the evolution of networks with fixed structure complexifying evolution discovers significantly more sophisticated strategies  the results suggest that in order to discover and improve complex solutions evolution and search in general should be allowed to complexify as well as optimize





i  androutsopoulos and p  malakasiotis 2010 a survey of paraphrasing and textual entailment methods volume 38 pages 135187



paraphrasing methods recognize generate or extract phrases sentences or longer natural language expressions that convey almost the same information textual entailment methods on the other hand recognize generate or extract pairs of natural language expressions such that a human who reads and trusts the first element of a pair would most likely infer that the other element is also true paraphrasing can be seen as bidirectional textual entailment and methods from the two areas are often similar both kinds of methods are useful at least in principle in a wide range of natural language processing applications including question answering summarization text generation and machine translation we summarize key ideas from the two areas by considering in turn recognition generation and extraction methods also pointing to prominent articles and resources





j  cheng and  m  j druzdzel 2000 aisbn an adaptive importance sampling algorithm for evidential reasoning in large bayesian networks volume 13 pages 155188









j  c beck 2007 solutionguided multipoint constructive search for job shop scheduling volume 29 pages 4977



solutionguided multipoint constructive search sgmpcs is a novel constructive search technique that performs a series of resourcelimited tree searches where each search begins either from an empty solution as in randomized restart or from a solution that has been encountered during the search a small number of these elite solutions is maintained during the search we introduce the technique and perform three sets of experiments on the job shop scheduling problem first a systematic fully crossed study of sgmpcs is carried out to evaluate the performance impact of various parameter settings second we inquire into the diversity of the elite solution set showing contrary to expectations that a less diverse set leads to stronger performance finally we compare the best parameter setting of sgmpcs from the first two experiments to chronological backtracking limited discrepancy search randomized restart and a sophisticated tabu search algorithm on a set of wellknown benchmark problems results demonstrate that sgmpcs is significantly better than the other constructive techniques tested though lags behind the tabu search



m  guo e  markakis k  r apt and v  conitzer 2013 undominated groves mechanisms volume 46 pages 129163



the family of groves mechanisms which includes the wellknown vcg mechanism also known as the clarke mechanism is a family of efficient and strategyproof mechanisms unfortunately the groves mechanisms are generally not budget balanced that is under such mechanisms payments may flow into or out of the system of the agents resulting in deficits or reduced utilities for the agents we consider the following problem within the family of groves mechanisms we want to identify mechanisms that give the agents the highest utilities under the constraint that these mechanisms must never incur deficits









v  conitzer 2009 eliciting singlepeaked preferences using comparison queries volume 35 pages 161191





scott  kiesel ethan  burns and wheeler  ruml 2015 achieving goals quickly using realtime search experimental results in video games volume 54 pages 123158



in realtime domains such as video games planning happens concurrently with execution and the planning algorithm has a strictly bounded amount of time before it must return the next action for the agent to execute we explore the use of realtime heuristic search in two benchmark domains inspired by video games unlike classic benchmarks such as grid pathfinding and the sliding tile puzzle these new domains feature exogenous change and directed state space graphs we consider the setting in which planning and acting are concurrent and we use the natural objective of minimizing goal achievement time using both the classic benchmarks and the new domains we investigate several enhancements to a leading realtime search algorithm lsslrta we show experimentally that 1 it is better to plan after each action or to use a dynamically sized lookahead 2 abased lookahead can cause undesirable actions to be selected and 3 online debiasing of the heuristic can lead to improved performance we hope this work encourages future research on applying realtime search in dynamic domains





honorable mention for the 2011 ijcaijair best paper prize



we study an approach to policy selection for large relational markov decision processes mdps we consider a variant of approximate policy iteration api that replaces the usual valuefunction learning step with a learning step in policy space this is advantageous in domains where good policies are easier to represent and learn than the corresponding value functions which is often the case for the relational mdps we are interested in in order to apply api to such problems we introduce a relational policy language and corresponding learner in addition we introduce a new bootstrapping routine for goalbased planning domains based on random walks such bootstrapping is necessary for many large relational mdps where reward is extremely sparse as api is ineffective in such domains when initialized with an uninformed policy our experiments show that the resulting system is able to find good policies for a number of classical planning domains and their stochastic variants by solving them as extremely large relational mdps the experiments also point to some limitations of our approach suggesting future work



i  p gent and  t  walsh 1993 an empirical analysis of search in gsat volume 1 pages 4759



we describe an extensive study of search in gsat an approximation procedure for propositional satisfiability gsat performs greedy hillclimbing on the number of satisfied clauses in a truth assignment  our experiments provide a more complete picture of gsats search than previous accounts we describe in detail the two phases of search rapid hillclimbing followed by a long plateau search  we demonstrate that when applied to randomly generated 3sat problems there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate  our results allow us to make detailed numerical conjectures about the length of the hillclimbing phase the average gradient of this phase and to conjecture that both the average score and average branching rate decay exponentially during plateau search we end by showing how these results can be used to direct future theoretical analysis  this work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms





d  opitz and  r  maclin 1999 popular ensemble methods an empirical study volume 11 pages 169198



an ensemble consists of a set of individually trained    classifiers such as neural networks or decision trees whose    predictions are combined when classifying novel instances  previous    research has shown that an ensemble is often more accurate than any of    the single classifiers in the ensemble  bagging breiman 1996c and    boosting freund  shapire 1996 shapire 1990 are two relatively    new but popular methods for producing ensembles  in this paper we    evaluate these methods on 23 data sets using both neural networks and    decision trees as our classification algorithm  our results clearly    indicate a number of conclusions  first while bagging is almost    always more accurate than a single classifier it is sometimes much    less accurate than boosting  on the other hand boosting can create    ensembles that are less accurate than a single classifier     especially when using neural networks  analysis indicates that the    performance of the boosting methods is dependent on the    characteristics of the data set being examined  in fact further    results show that boosting ensembles may overfit noisy data sets thus    decreasing its performance  finally consistent with previous    studies our work suggests that most of the gain in an ensembles    performance comes in the first few classifiers combined however    relatively large gains can be seen up to 25 classifiers when boosting    decision trees





s  safra and  m  tennenholtz 1994 on planning while learning volume 2 pages 111129



this paper introduces a framework for planning while   learning where an agent is given a goal to achieve in anenvironment   whose behavior is only partially known to the agent      we discuss the tractability of various plandesign processes we   show that for a large natural class of planning while learning   systems a plan can be presented and verified in a reasonable time   however coming up algorithmically with a plan even for simple   classes of systems is apparently intractable      we emphasize the role of offline plandesign processes andshow   that in most natural cases the verification projection part canbe   carried out in an efficient algorithmic manner





h  shatkay and  l  p kaelbling 2002 learning geometricallyconstrained hidden markov models for robot navigation bridging the topologicalgeometrical gap volume 16 pages 167207



hidden markov models hmms and partially observable markov    decision processes pomdps provide useful tools for modeling    dynamical systems  they are particularly useful for representing the    topology of environments such as road networks and office buildings    which are typical for robot navigation and planning  the work    presented here describes a formal framework for incorporating readily    available odometric information and geometrical constraints into both    the models and the algorithm that learns them  by taking advantage of    such information learning hmmspomdps can be made to generate better    solutions and require fewer iterations while being robust in the face    of data reduction  experimental results obtained from both simulated    and real robot data demonstrate the effectiveness of the approach





g  a kaminka  d  v pynadath and  m  tambe 2002 monitoring teams by overhearing a multiagent planrecognition approach volume 17 pages 83135



recent years are seeing an increasing need for online    monitoring of teams of cooperating agents eg for visualization or    performance tracking however in monitoring deployed teams we often    cannot rely on the agents to always communicate their state to the    monitoring system this paper presents a nonintrusive approach to    monitoring by overhearing where the monitored teams state is    inferred via planrecognition from teammembers routine    communications exchanged as part of their coordinated task execution    and observed overheard by the monitoring system key challenges in    this approach include the demanding runtime requirements of    monitoring the scarceness of observations increasing monitoring    uncertainty and the need to scaleup monitoring to address    potentially large teams to address these we present a set of    complementary novel techniques exploiting knowledge of the social    structures and procedures in the monitored team i an efficient    probabilistic planrecognition algorithm wellsuited for processing    communications as observations ii an approach to exploiting    knowledge of the teams social behavior to predict future observations    during execution reducing monitoring uncertainty and iii    monitoring algorithms that trade expressivity for scalability    representing only certain useful monitoring hypotheses but allowing    for any number of agents and their different activities to be    represented in a single coherent entity we present an empirical    evaluation of these techniques in combination and apart in    monitoring a deployed team of agents running on machines physically    distributed across the country and engaged in complex dynamic task    execution we also compare the performance of these techniques to    human expert and novice monitors and show that the techniques    presented are capable of monitoring at humanexpert levels despite    the difficulty of the task





j  veness ks  ng m  hutter w  uther and d  silver 2011 a montecarlo aixi approximation volume 40 pages 95142







q  zhao and  t  nishida 1995 using qualitative hypotheses to identify inaccurate data volume 3 pages 119145



identifying inaccurate data has long been regarded as a    significant and difficult problem in ai in this paper we present a    new method for identifying inaccurate data on the basis of qualitative    correlations among related data first we introduce the definitions    of related data and qualitative correlations among related data  then    we put forward a new concept called support coefficient function    scf scf can be used to extract represent and calculate    qualitative correlations among related data within a dataset we    propose an approach to determining dynamic shift intervals of    inaccurate data and an approach to calculating possibility of    identifying inaccurate data respectively both of the approaches are    based on scf finally we present an algorithm for identifying    inaccurate data by using qualitative correlations among related data    as confirmatory or disconfirmatory evidence we have developed a    practical system for interpreting infrared spectra by applying the    method and have fully tested the system against several hundred real    spectra the experimental results show that the method is    significantly better than the conventional methods used in many    similar systems









z  feldman and c  domshlak 2014 simple regret optimization in online planning for markov decision processes volume 51 pages 165205



a  gerevini and  l  schubert 1996 accelerating partialorder planners some techniques for effective search control and pruning volume 5 pages 95137



we propose some domainindependent techniques for bringing wellfounded partialorder planners closer to practicality the first two techniques are aimed at improving search control while keeping overhead costs low  one is based on a simple adjustment to the default a heuristic used by ucpop to select plans for refinement the other is based on preferring zero commitment forced plan refinements whenever possible and using lifo prioritization otherwise a more radical technique is the use of operator parameter domains to prune search these domains are initially computed from the definitions of the operators and the initial and goal conditions using a polynomialtime algorithm that propagates sets of constants through the operator graph starting in the initial conditions during planning parameter domains can be used to prune nonviable operator instances and to remove spurious clobbering threats  in experiments based on modifications of ucpop our improved plan and goal selection strategies gave speedups by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version crucially the hardest problems gave the greatest improvements the pruning technique based on parameter domains often gave speedups by an order of magnitude or more for difficult problems both with the default ucpop search strategy and with our improved strategy the lisp code for our techniques and for the test problems is provided in online appendices





b  de keijzer t  b klos and y  zhang 2014 finding optimal solutions for voting game design problems volume 50 pages 105140



in many circumstances where multiple agents need to make a joint decision voting is used to aggregate the agents preferences each agents vote carries a weight and if the sum of the weights of the  agents in favor of some outcome is larger than or equal to a given quota then this outcome is decided upon the distribution of weights leads to a certain distribution of power several power indices have been proposed to measure such power in the socalled inverse problem we are given a target distribution of power and are asked to come up with a game in the form of a quota plus an assignment of weights to the players whose power distribution is as close as possible to the target distribution according to some specied distance measure

we first present a doubly exponential algorithm for enumerating the set of simple games we then improve on this algorithm for the class of weighted voting games and obtain a quadratic exponential ie 2on2 algorithm for enumerating them we show that this improved algorithm runs in outputpolynomial time making it the fastest possible enumeration algorithm up to a polynomial factor finally we propose an exact anytimealgorithm that runs in exponential time for the power index weighted voting game design problem the inverse problem we implement this algorithm to find a weighted voting game with a normalized banzhaf power distribution closest to a target power index and perform experiments to obtain some insights about the set of weighted voting games we remark that our algorithm is applicable to optimizing any exponentialtime computable function the distance of the normalized banzhaf index to a target power index is merely taken as an example





d  terekhov and j   c beck 2008 a constraint programming approach for solving a queueing control problem volume 32 pages 123167



in a facility with front room and back room operations it is useful to switch workers between the rooms in order to cope with changing customer demand assuming stochastic customer arrival and service times we seek a policy for switching workers such that the expected customer waiting time is minimized while the expected back room staffing is sufficient to perform all work  three novel constraint programming models and several shaving procedures for these models are presented experimental results show that a model based on closedform expressions together with a combination of shaving procedures is the most efficient this model is able to find and prove optimal solutions for many problem instances within a reasonable runtime previously the only available approach was a heuristic algorithm furthermore a hybrid method combining the heuristic and the best constraint programming method is shown to perform as well as the heuristic in terms of solution quality over time while achieving the same performance in terms of proving optimality as the pure constraint programming model this is the first work of which we are aware that solves such queueingbased problems with constraint programming





a  moore and  m  s lee 1998 cached sufficient statistics for efficient machine learning with large datasets volume 8 pages 6791



this paper introduces new algorithms and data structures for    quick counting for machine learning datasets  we focus on the    counting task of constructing contingency tables but our approach is    also applicable to counting the number of records in a dataset that    match conjunctive queries  subject to certain assumptions the costs    of these operations can be shown to be independent of the number of    records in the dataset and loglinear in the number of nonzero entries    in the contingency table      we provide a very sparse data structure the adtree to minimize    memory use we provide analytical worstcase bounds for this structure    for several models of data distribution  we empirically demonstrate    that tractablysized data structures can be produced for large    realworld datasets by a using a sparse tree structure that never    allocates memory for counts of zero b never allocating memory for    counts that can be deduced from other counts and c not bothering to    expand the tree fully near its leaves       we show how the adtree can be used to accelerate bayes net structure    finding algorithms rule learning algorithms and feature selection    algorithms and we provide a number of empirical results comparing    adtree methods against traditional direct counting approaches  we    also discuss the possible uses of adtrees in other machine learning    methods and discuss the merits of adtrees in comparison with    alternative representations such as kdtrees rtrees and frequent sets





l  k saul  t  jaakkola and  m  i jordan 1996 mean field theory for sigmoid belief networks volume 4 pages 6176



we develop a mean field theory for sigmoid belief networks    based on ideas from statistical mechanics  our mean field theory    provides a tractable approximation to the true probability    distribution in these networks it also yields a lower bound on the    likelihood of evidence  we demonstrate the utility of this framework    on a benchmark problem in statistical pattern recognitionthe    classification of handwritten digits





e  hebrard d  marx b  osullivan and i  razgon 2011 soft constraints of difference and equality volume 41 pages 97130



in many combinatorial problems one may need to model the diversity or similarity of assignments in a solution for example one may wish to maximise or minimise the number of distinct values in a solution to formulate problems of this type we can use soft variants of the well known alldifferent and allequal constraints  we present a taxonomy of six soft global constraints generated by combining the two latter ones and the two standard cost functions which are either maximised or minimised we characterise the complexity of achieving arc and bounds consistency on these constraints resolving those cases for which nphardness was neither proven nor disproven in particular we explore in depth the constraint ensuring that at least k pairs of variables have a common value we show that achieving arc consistency is nphard however achieving bounds consistency can be done in polynomial time through dynamic programming moreover we show that the maximum number of pairs of equal variables can be approximated by a factor 12 with a linear time greedy algorithm finally we provide a fixed parameter tractable algorithm with respect to the number of values appearing in more than two distinct domains interestingly this taxonomy shows that enforcing equality is harder than enforcing difference





c  lusena  j  goldsmith and  m  mundhenk 2001 nonapproximability results for partially observable markov decision processes volume 14 pages 83103



we show that for several variations of partially observable    markov decision processes polynomialtime algorithms for finding    control policies are unlikely to or simply dont have guarantees of    finding policies within a constant factor or a constant summand of    optimal  here unlikely means unless some complexity classes    collapse where the collapses considered are pnp ppspace or    pexp  until or unless these collapses are shown to hold any    controlpolicy designer must choose between such performance    guarantees and efficient computation









p  nakov and h t  ng 2012 improving statistical machine translation for a resourcepoor language using related resourcerich languages volume 44 pages 179222





r  booth and t  meyer 2006 admissible and restrained revision volume 26 pages 127151



as partial justification of their framework for iterated belief revision darwiche and pearl convincingly argued against boutiliers natural revision and provided a prototypical revision operator that fits into their scheme  we show that the darwichepearl arguments lead naturally to the acceptance of a smaller class of operators which we refer to as admissible admissible revision ensures that the penultimate input is not ignored completely thereby eliminating natural revision but includes the darwichepearl operator nayaks lexicographic revision operator and a newly introduced operator called restrained revision we demonstrate that restrained revision is the most conservative of admissible revision operators  effecting as few changes as possible while lexicographic revision is the least conservative and point out that restrained revision can also be viewed as a composite operator consisting of natural revision preceded by an application of a backwards revision operator previously studied by papini finally we propose the establishment of a principled approach for choosing an appropriate revision operator in different contexts and discuss future work 



l  finkelstein  s  markovitch and  e  rivlin 2003 optimal schedules for parallelizing anytime algorithms the case of shared resources volume 19 pages 73138



the performance of anytime algorithms can be improved by simultaneously solving several instances of algorithmproblem pairs these pairs may include different instances of a problem such as starting from a different initial state different algorithms if several alternatives exist or several runs of the same algorithm for nondeterministic algorithms in this paper we present a methodology for designing an optimal scheduling policy based on the statistical characteristics of the algorithms involved we formally analyze the case where the processes share resources a singleprocessor model and provide an algorithm for optimal scheduling  we analyze theoretically and empirically the behavior of our scheduling algorithm for various distribution types  finally we present empirical results of applying our scheduling algorithm to the latin square problem





p  agre and  i  horswill 1997 lifeworld analysis volume 6 pages 111145



we argue that the analysis of agentenvironment interactions should be extended to include the conventions and invariants maintained by agents throughout their activity  we refer to this thicker notion of environment as a lifeworld and present a partial set of formal tools for describing structures of lifeworlds and the ways in which they computationally simplify activity  as one specific example we apply the tools to the analysis of the toast system and show how versions of the system with very different control structures in fact implement a common control structure together with different conventions for encoding task state in the positions or states of objects in the environment









j  l bredin d  c parkes and q  duong 2007 chain a dynamic double auction framework for matching patient agents  volume 30 pages 133179



h  r andersen t  hadzic and d  pisinger 2010 interactive cost configuration over decision diagrams volume 37 pages 99139



in many ai domains such as product configuration a user should interactively specify a solution that must satisfy  a set of constraints in such scenarios offline compilation of feasible solutions into a tractable representation is an important approach to delivering efficient backtrackfree  user interaction online in particularbinary decision diagrams bdds have been successfully used as a compilation target for product and service configuration in this paper we discuss how to extend bddbased configuration to scenarios involving cost functions which express user preferences

we first show that an efficient robust and easy to implement extension is possible if the cost function is additive and feasible solutions are represented using multivalued decision diagrams mdds we also discuss the effect on mdd size if the cost function is nonadditive or if it is encoded explicitly into mdd we then discuss interactive configuration in the presence of multiple cost functions we prove that even in its simplest form multiplecost configuration is nphard in the input mdd however for solving twocost configuration we develop a pseudopolynomial scheme and a fully polynomial approximation scheme the applicability of our approach is demonstrated through experiments over realworld configuration models and productcatalogue datasets response times are generally within a fraction of a second even for very large instances 





w  zhang and  n  l zhang 2005 restricted value iteration theory and algorithms volume 23 pages 123165



value iteration is a popular algorithm for finding near optimal policies for pomdps  it is inefficient due to the need to account for the entire belief space which necessitates the solution of large numbers of linear programs  in this paper we study value iteration restricted to belief subsets we show that together with properly chosen belief subsets restricted value iteration yields nearoptimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time we also apply restricted value iteration to two interesting classes of pomdps namely informative pomdps and neardiscernible pomdps





alejandro  moreo fern225ndez andrea  esuli and fabrizio  sebastiani 2016 lightweight random indexing for polylingual text classification volume 57 pages 151185



multilingual text classification mltc is a text classification task in which documents are written each in one among a set l of natural languages and in which all documents must be classified under the same classification scheme irrespective of language there are two main variants of mltc namely crosslingual text classification cltc and polylingual text classification pltc in pltc which is the focus of this paper we assume differently from cltc that for each language in l there is a representative set of training documents pltc consists of improving the accuracy of each of the l monolingual classifiers by also leveraging the training documents written in the other l 8722 1 languages the obvious solution consisting of generating a single polylingual classifier from the juxtaposed monolingual vector spaces is usually infeasible since the dimensionality of the resulting vector space is roughly l times that of a monolingual one and is thus often unmanageable as a response the use of machine translation tools or multilingual dictionaries has been proposed however these resources are not always available or are not always free to use

one machinetranslationfree and dictionaryfree method that to the best of our knowledge has never been applied to pltc before is random indexing ri we analyse ri in terms of space and time efficiency and propose a particular configuration of it that we dub lightweight random indexing  lri by running experiments on two well known public benchmarks reuters rcv1rcv2 a comparable corpus and jrcacquis a parallel one we show lri to outperform both in terms of effectiveness and efficiency a number of previously proposed machinetranslationfree and dictionaryfree pltc methods that we use as baselines





h  chockler and  j  y halpern 2004 responsibility and blame a structuralmodel approach volume 22 pages 93115



causality is typically treated an allornothing concept either a is a cause of b or it is not we extend the definition of causality introduced by halpern and pearl 2004a to take into account the degree of responsibility of a for b  for example if someone wins an election 110 then each person who votes for him is less responsible for the victory than if he had won 65  we then define a notion of degree of blame which takes into account an agents epistemic state roughly speaking the degree of blame of a for b is the expected degree of responsibility of a for b taken over the epistemic state of an agent







m  lapata and a  lascarides 2006 learning sentenceinternal temporal relations volume 27 pages 85117



in this paper we propose a data intensive approach for inferring sentenceinternal temporal relations temporal inference is relevant for practical nlp applications which either extract or synthesize temporal information eg summarisation question answering  our method bypasses the need for manual coding by exploiting the presence of markers like after which overtly signal a temporal relation we first show that models trained on main and subordinate clauses connected with a temporal marker achieve good performance on a pseudodisambiguation task simulating temporal inference during testing the temporal marker is treated as unseen and the models must select the right marker from a set of possible candidates  secondly we assess whether the proposed approach holds promise for the semiautomatic creation of temporal annotations  specifically we use a model trained on noisy and approximate data ie main and subordinate clauses to predict intrasentential relations present in timebank a corpus annotated rich temporal information  our experiments compare and contrast several probabilistic models differing in their feature space linguistic assumptions and data requirements  we evaluate performance against gold standard corpora and also against human subjects 



p  j hawkins  v  lagoon and  p  j stuckey 2005 solving set constraint satisfaction problems using robdds volume 24 pages 109156



in this paper we present a new approach to modeling finite set domain constraint problems using reduced ordered binary decision diagrams robdds we show that it is possible to construct an efficient set domain propagator which compactly represents many set domains and set constraints using robdds  we demonstrate that the robddbased approach provides unprecedented flexibility in modeling constraint satisfaction problems leading to performance improvements we also show that the robddbased modeling approach can be extended to the modeling of integer and multiset constraint problems in a straightforward manner since domain propagation is not always practical we also show how to incorporate less strict consistency notions into the robdd framework such as set bounds cardinality bounds and lexicographic bounds consistency finally we present experimental results that demonstrate the robddbased solver performs better than various more conventional constraint solvers on several standard set constraint problems





j  peral and  a  ferrandez 2003 translation of pronominal anaphora between english and spanish discrepancies and evaluation volume 18 pages 117147



this paper evaluates the different tasks carried out in the    translation of pronominal anaphora in a machine translation mt    system the mt interlingua approach named agir anaphora generation    with an interlingua representation improves upon other proposals    presented to date because it is able to translate intersentential    anaphors detect coreference chains and translate spanish zero    pronouns into englishissues hardly considered by other systems the    paper presents the resolution and evaluation of these anaphora    problems in agir with the use of different kinds of knowledge    lexical morphological syntactic and semantic the translation of    english and spanish anaphoric thirdperson personal pronouns    including spanish zero pronouns into the target language has been    evaluated on unrestricted corpora we have obtained a precision of    804 and 848 in the translation of spanish and english pronouns    respectively although we have only studied the spanish and english    languages our approach can be easily extended to other languages such    as portuguese italian or japanese





d  long and  m  fox 1999 efficient implementation of the plan graph in stan volume 10 pages 87115



stan is a graphplanbased planner socalled because it uses    a variety of state analysis techniques to enhance its performance    stan competed in the aips98 planning competition where it compared    well with the other competitors in terms of speed finding solutions    fastest to many of the problems posed although the domain analysis    techniques stan exploits are an important factor in its overall    performance we believe that the speed at which stan solved the    competition problems is largely due to the implementation of its plan    graph the implementation is based on two insights that many of the    graph construction operations can be implemented as bitlevel logical    operations on bit vectors and that the graph should not be explicitly    constructed beyond the fix point this paper describes the    implementation of stans plan graph and provides experimental results    which demonstrate the circumstances under which advantages can be    obtained from using this implementation







javad  azimi xiaoli  fern and alan  fern 2016 budgeted optimization with constrained experiments volume 56 pages 119152



motivated by a realworld problem we study a novel budgeted optimization problem where the goal is to optimize an unknown function f given a budget by requesting a sequence of samples from the function in our setting however evaluating the function at precisely specified points is not practically possible due to prohibitive costs instead we can only request constrained experiments a constrained experiment denoted by q specifies a subset of the input space for the experimenter to sample the function from the outcome of q includes a sampled experiment x and its function output fx importantly as the constraints of q become looser the cost of fulfilling the request decreases but the uncertainty about the location x increases our goal is to manage this tradeoff by selecting a set of constrained experiments that best optimize f within the budget  we study this problem in two different settings the nonsequential or batch setting where a set of constrained experiments is selected at once and the sequential setting where experiments are selected one at a time we evaluate our proposed methods for both settings using synthetic and real functions the experimental results demonstrate the efficacy of the proposed methods



r  hoshino and k  kawarabayashi 2011 scheduling bipartite tournaments to minimize total travel distance volume 42 pages 91124



in many professional sports leagues teams from opposing leaguesconferences compete against one another playing interleague games  this is an example of a bipartite tournament  in this paper we consider the problem of reducing the total travel distance of bipartite tournaments by analyzing interleague scheduling from the perspective of discrete optimization  this research has natural applications to sports scheduling especially for leagues such as the national basketball association nba where teams must travel long distances across north america to play all their games thus consuming much time money and greenhouse gas emissions

we introduce the bipartite traveling tournament problem bttp the interleague variant of the wellstudied traveling tournament problem we prove that the 2nteam bttp is npcomplete but for small values of n a distanceoptimal interleague schedule can be generated from an algorithm based on minimumweight 4cyclecovers  we apply our theoretical results to the 12team nippon professional baseball npb league in japan producing a provablyoptimal schedule requiring 42950 kilometres of total team travel a 16 reduction compared to the actual distance traveled by these teams during the 2010 npb season  we also develop a nearlyoptimal interleague tournament for the 30team nba league just 38 higher than the trivial theoretical lower bound





y  gal and a  pfeffer 2008 networks of influence diagrams a formalism for representing  agents  beliefs and  decisionmaking processes volume 33 pages 109147



this paper presents networks of influence diagrams nid a compact natural and highly expressive language for reasoning about agents beliefs and decisionmaking processes  nids are graphical structures in which agents mental models are represented as nodes in a network a mental model for an agent may itself use descriptions of the mental models of other agents nids are demonstrated by examples showing how they can be used to describe conflicting and cyclic belief structures and certain forms of bounded rationality  in an opponent modeling domain nids were able to outperform other computational agents whose strategies were not known in advance  nids are equivalent in representation to bayesian games  but they are more compact and structured than this formalism in particular the equilibrium definition for nids makes an explicit distinction between agents optimal strategies and how they actually behave in reality





c  backstrom 1998 computational aspects of reordering plans volume 9 pages 99137



this article studies the problem of modifying the action    ordering of a plan in order to optimise the plan according to various    criteria  one of these criteria is to make a plan less constrained    and the other is to minimize its parallel execution time  three    candidate definitions are proposed for the first of these criteria    constituting a sequence of increasing optimality guarantees  two of    these are based on deordering plans which means that ordering    relations may only be removed not added while the third one uses    reordering where arbitrary modifications to the ordering are allowed    it is shown that only the weakest one of the three criteria is    tractable to achieve the other two being nphard and even difficult    to approximate  similarly optimising the parallel execution time of    a plan is studied both for deordering and reordering of plans  in the    general case both of these computations are nphard  however it is    shown that optimal deorderings can be computed in polynomial time for    a class of planning languages based on the notions of producers    consumers and threats which includes most of the commonly used    planning languages  computing optimal reorderings can potentially    lead to even faster parallel executions but this problem remains    nphard and difficult to approximate even under quite severe restrictions





i  refanidis and  i  vlahavas 2001 the grt planning system backward heuristic construction in forward statespace planning volume 15 pages 115161



this paper presents grt a domainindependent heuristic    planning system for strips worlds grt solves problems in two    phases in the preprocessing phase it estimates the distance between    each fact and the goals of the problem in a backward direction then    in the search phase these estimates are used in order to further    estimate the distance between each intermediate state and the goals    guiding so the search process in a forward direction and on a    bestfirst basis the paper presents the benefits from the adoption of    opposite directions between the preprocessing and the search phases    discusses some difficulties that arise in the preprocessing phase and    introduces techniques to cope with them moreover it presents several    methods of improving the efficiency of the heuristic by enriching the    representation and by reducing the size of the problem finally a    method of overcoming local optimal states based on domain axioms is    proposed according to it difficult problems are decomposed into    easier subproblems that have to be solved sequentially the    performance results from various domains including those of the    recent planning competitions show that grt is among the fastest    planners





b  motik r  shearer and i  horrocks 2009 hypertableau reasoning for description logics volume 36 pages 165228



we present a novel reasoning calculus for the description logic shoiqa knowledge representation formalism with applications in areas such as the semantic web unnecessary nondeterminism and the construction of large models are two primary sources of inefficiency in the tableaubased reasoning calculi used in stateoftheart reasoners in order to reduce nondeterminism we base our calculus on hypertableau and hyperresolution calculi which we extend with a blocking condition to ensure termination in order to reduce the size of the constructed models we introduce anywhere pairwise blocking we also present an improved nominal introduction rule that ensures termination in the presence of nominals inverse roles and number restrictionsa combination of dl constructs that has proven notoriously difficult to handle our implementation shows significant performance improvements over stateoftheart reasoners on several wellknown ontologies





k  xu and  w  li 2000 exact phase transitions in random constraint satisfaction problems volume 12 pages 93103



in this paper we propose a new type of random csp model    called model rb which is a revision to the standard model b it is    proved that phase transitions from a region where almost all problems    are satisfiable to a region where almost all problems are    unsatisfiable do exist for model rb as the number of variables    approaches infinity  moreover the critical values at which the phase    transitions occur are also known exactly by relating the hardness of    model rb to model b it is shown that there exist a lot of hard    instances in model rb





j  c boerkoel jr and e  h durfee 2013 distributed reasoning for multiagent simple temporal problems volume 47 pages 95156



this research focuses on building foundational algorithms for scheduling agents that assist people in managing their activities in environments where tempo and complex activity interdependencies outstrip peoples cognitive capacity  we address the critical challenge of reasoning over individuals interacting schedules to efficiently answer queries about how to meet scheduling goals while respecting individual privacy and autonomy to the extent possible  we formally define the multiagent simple temporal problem for naturally capturing and reasoning over the distributed but interconnected scheduling problems of multiple individuals  our hypothesis is that combining bottomup and topdown approaches will lead to effective solution techniques  in our bottomup phase an agent externalizes constraints that compactly summarize how its local subproblem affects other agents subproblems whereas in our topdown phase an agent proactively constructs and internalizes new local constraints that decouple its subproblem from others  we confirm this hypothesis by devising distributed algorithms that calculate summaries of the joint solution space for multiagent scheduling problems without centralizing or otherwise redistributing the problems  the distributed algorithms permit concurrent execution to achieve significant speedup over the current art and also increase the level of privacy and independence in individual agent reasoning  these algorithms are most advantageous for problems where interactions between the agents are sparse compared to the complexity of agents individual problems





saif  m mohammad mohammad  salameh and svetlana  kiritchenko 2016 how translation alters sentiment volume 55 pages 95130



sentiment analysis research has predominantly been on english texts thus there exist many sentiment resources for english but less so for other languages approaches to improve sentiment analysis in a resourcepoor focus language include a translate the focus language text into a resourcerich language such as english and apply a powerful english sentiment analysis system on the text and b translate resources such as sentiment labeled corpora and sentiment lexicons from english into the focus language and use them as additional resources in the focuslanguage sentiment analysis system in this paper we systematically examine both options we use arabic social media posts as standin for the focus language text we show that sentiment analysis of english translations of arabic texts produces competitive results wrt arabic sentiment analysis we show that arabic sentiment analysis systems benefit from the use of automatically translated english sentiment lexicons we also conduct manual annotation studies to examine why the sentiment of a translation is different from the sentiment of the source word or text this is especially relevant for building better automatic translation systems in the process we create a stateoftheart arabic sentiment analysis system a new dialectal arabic sentiment lexicon and the first arabicenglish parallel corpus that is independently annotated for sentiment by arabic and english speakers





ronald  de haan iyad  kanj and stefan  szeider 2015 on the subexponentialtime complexity of csp volume 52 pages 203234



not all npcomplete problems share the same practical hardness with respect to exact computation  whereas some npcomplete problems are amenable to efficient computational methods others are yet to show any such sign it becomes a major challenge to develop a theoretical framework that is more finegrained than the theory of npcompleteness and that can explain the distinction between the exact complexities of various npcomplete problems this distinction is highly relevant for constraint satisfaction problems under natural restrictions where various shades of hardness can be observed in practice





a  i coles and a  j smith 2007 marvin a heuristic search planner with online macroaction learning volume 28 pages 119156



this paper describes marvin a planner that competed in the fourth international planning competition ipc 4 marvin uses actionsequencememoisation techniques to generate macroactions which are then used during search for a solution plan we provide an overview of its architecture and search behaviour detailing the algorithms used we also empirically demonstrate the effectiveness of its features in various planning domains in particular the effects on performance due to the use of macroactions the novel features of its search behaviour and the native support of adl and derived predicates







d  s bernstein c  amato e  a hansen and s  zilberstein 2009 policy iteration for decentralized control of markov decision processes volume 34 pages 89132



coordination of distributed agents is required for problems arising in many areas including multirobot systems networking and ecommerce  as a formal framework for such problems we use the decentralized partially observable markov decision process decpomdp  though much work has been done on optimal dynamic programming algorithms for the singleagent version of the problem optimal algorithms for the multiagent case have been elusive  the main contribution of this paper is an optimal policy iteration algorithm for solving decpomdps  the algorithm uses stochastic finitestate controllers to represent policies  the solution can include a correlation device which allows agents to correlate their actions without communicating  this approach alternates between expanding the controller and performing valuepreserving transformations which modify the controller without sacrificing value  we present two efficient valuepreserving transformations one can reduce the size of the controller and the other can improve its value while keeping the size fixed  empirical results demonstrate the usefulness of valuepreserving transformations in increasing value while keeping controller size to a minimum to broaden the applicability of the approach we also present a heuristic version of the policy iteration algorithm which sacrifices convergence to optimality  this algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents actions are known while this assumption may not hold in general it helps produce higher quality solutions in our test problems







sa  mirroshandel and g  ghassemsani 2012 towards unsupervised learning of temporal relations between events volume 45 pages 125163



a  cal236 g  gottlob and m  kifer 2013 taming the infinite chase query answering under expressive relational constraints volume 48 pages 115174



the chase algorithm is a fundamental tool for query evaluation and for testing query containment under tuplegenerating dependencies tgds and equalitygenerating dependencies egds  so far most of the research on this topic has focused on cases where the chase procedure terminates  this paper introduces expressive classes of tgds defined via syntactic restrictions guarded tgds gtgds and weakly guarded sets of tgds wgtgds  for these classes the chase procedure is not guaranteed to terminate and thus may have an infinite outcome nevertheless we prove that the problems of conjunctivequery answering and query containment under such tgds are decidable  we provide decision procedures and tight complexity bounds for these problems  then we show how egds can be incorporated into our results by providing conditions under which egds do not harmfully interact with tgds and do not affect the decidability and complexity of query answering  we show applications of the aforesaid classes of constraints to the problem of answering conjunctive queries in flogic lite an objectoriented ontology language and in some tractable description logics







y  zhang and y  ding 2008 ctl model update for system modifications volume 31 pages 113155



model checking is a promising technology which has been applied for verification of many hardware and software systems in this paper we introduce the concept of model update towards the development of an automatic system modification tool that extends model checking functions we define primitive update operations on the models of computation tree logic ctl and formalize the principle of minimal change for ctl model update these primitive update operations together with the underlying minimal change principle serve as the foundation for ctl model update essential semantic and computational characterizations are provided for our ctl model update approach we then describe a formal algorithm that implements this approach we also illustrate two case studies of ctl model updates for the wellknown microwave oven example and the andrew file system 1 from which we further propose a method to optimize the update results in complex system modifications



marijn  heule matti  j228rvisalo florian  lonsing martina  seidl and armin  biere 2015 clause elimination for sat and qsat volume 53 pages 127168



the famous archetypical npcomplete problem of boolean satisfiability sat and its pspacecomplete generalization of quantified boolean satisfiability qsat have become central declarative programming paradigms through which realworld instances of various computationally hard problems can be efficiently solved this success has been achieved through several breakthroughs in practical implementations of decision procedures for sat and qsat that is in sat and qsat solvers here simplification techniques for conjunctive normal form cnf for sat and for prenex conjunctive normal form pcnf for qsatthe standard input formats of sat and qsat solvershave recently proven very effective in increasing solver efficiency when applied before ie in preprocessing or during ie in inprocessing satisfiability search

in this article we develop and analyze clause elimination procedures for pre and inprocessing clause elimination procedures form a family of pcnf formula simplification techniques which remove clauses that have specific in practice polynomialtime redundancy properties while maintaining the satisfiability status of the formulas extending known procedures such as tautology subsumption and blocked clause elimination we introduce novel elimination procedures based on asymmetric variants of these techniques and also develop a novel family of socalled covered clause elimination procedures as well as natural liftings of the cnflevel procedures to pcnf we analyze the considered clause elimination procedures from various perspectives furthermore for the variants not preserving logical equivalence under clause elimination we show how to reconstruct solutions to original cnfs from satisfying assignments to simplified cnfs which is important for practical applications for the procedures complementing the more theoretical analysis we present results on an empirical evaluation on the practical importance of the clause elimination procedures in terms of the effect on solver runtimes on standard realworld application benchmarks it turns out that the importance of applying the clause elimination procedures developed in this work is empirically emphasized in the context of stateoftheart qsat solving





c  g nevillmanning and  i  h witten 1997 identifying hierarchical structure in sequences a lineartime algorithm volume 7 pages 6782



sequitur is an algorithm that infers a hierarchical    structure from a sequence of discrete symbols by replacing repeated    phrases with a grammatical rule that generates the phrase and    continuing this process recursively the result is a hierarchical    representation of the original sequence which offers insights into    its lexical structure the algorithm is driven by two constraints that    reduce the size of the grammar and produce structure as a byproduct     sequitur breaks new ground by operating incrementally moreover the    methods simple structure permits a proof that it operates in space    and time that is linear in the size of the input our implementation    can process 50000 symbols per second and has been applied to an    extensive range of real world sequences







t  flati and r  navigli 2012 the cqc algorithm cycling in graphs to semantically enrich and enhance a bilingual dictionary volume 43 pages 135171



bilingual machinereadable dictionaries are knowledge resources useful in many automatic tasks however compared to monolingual computational lexicons like wordnet bilingual dictionaries typically provide a lower amount of structured information such as lexical and semantic relations and often do not cover the entire range of possible translations for a word of interest in this paper we present cycles and quasicycles cqc a novel algorithm for the automated disambiguation of ambiguous translations in the lexical entries of a bilingual machinereadable dictionary the dictionary is represented as a graph and cyclic patterns are sought in the graph to assign an appropriate sense tag to each translation in a lexical entry further we use the algorithms output to improve the quality of the dictionary itself by suggesting accurate solutions to structural problems such as misalignments partial alignments 

and missing entries finally we successfully apply cqc to the task of synonym extraction



g  chalkiadakis e  elkind e  markakis m  polukarov and n  r jennings 2010 cooperative games with overlapping coalitions volume 39 pages 179216



in the usual models of cooperative game theory the outcome of a coalition formation process is either the grand coalition or a coalition structure that consists of disjoint coalitions however in many domains where coalitions are associated with tasks an agent may be involved in executing more than one task and thus may distribute his resources among several coalitions to tackle such scenarios we introduce a model for cooperative games with overlapping coalitionsor overlapping coalition formation ocf games we then explore the issue of stability in this setting in particular we introduce a notion of the core which generalizes the corresponding notion in the traditional nonoverlapping scenario then under some quite general conditions we characterize the elements of the core and show that any element of the core maximizes the social welfare we also introduce a concept of balancedness for overlapping coalitional games and use it to characterize coalition structures that can be extended to elements of the core finally we generalize the notion of convexity to our setting and show that under some natural assumptions convex games have a nonempty core moreover we introduce two alternative notions of stability in ocf that allow a wider range of deviations and explore the relationships among the corresponding definitions of the core as well as the classic nonoverlapping core and the aubin core we illustrate the general properties of the three cores and also study them from a computational perspective thus obtaining additional insights into their fundamental structure





j  w crandall 2014 towards minimizing disappointment in repeated games volume 49 pages 111142



we consider the problem of learning in repeated games against arbitrary associates  specifically we study the ability of expert algorithms to quickly learn effective strategies in repeated games towards the ultimate goal of learning nearoptimal behavior against any arbitrary associate within only a handful of interactions  our contribution is threefold  first we advocate a new metric called disappointment for evaluating expert algorithms in repeated games  unlike minimizing traditional notions of regret minimizing disappointment in repeated games is equivalent to maximizing payoffs  unfortunately eliminating disappointment is impossible to guarantee in general  however it is possible for an expert algorithm to quickly achieve low disappointment against many known classes of algorithms in many games  second we show that popular existing expert algorithms often fail to achieve low disappointment against a variety of associates particularly in early rounds of the game  finally we describe a new metaalgorithm that can be applied to existing expert algorithms to substantially reduce disappointment in many twoplayer repeated games when associates follow various static reinforcement learning and expert algorithms





m  s boddy 2003 imperfect match  pddl 21 and real applications volume 20 pages 133137



pddl was originally conceived and constructed as a lingua franca for the international planning competition  pddl21 embodies a set of extensions intended to support the expression of something closer to real planning problems  this objective has only been partially achieved due in large part to a deliberate focus on not moving too far from classical planning models and solution methods





k  o stanley and  r  miikkulainen 2004 competitive coevolution through evolutionary complexification volume 21 pages 63100



two major goals in machine learning are the discovery and improvement of solutions to complex problems  in this paper we argue that complexification ie the incremental elaboration of solutions through adding new structure achieves both these goals  we demonstrate the power of complexification through the neuroevolution of augmenting topologies neat method which evolves increasingly complex neural network architectures  neat is applied to an openended coevolutionary robot duel domain where robot controllers compete head to head  because the robot duel domain supports a wide range of strategies and because coevolution benefits from an escalating arms race it serves as a suitable testbed for studying complexification  when compared to the evolution of networks with fixed structure complexifying evolution discovers significantly more sophisticated strategies  the results suggest that in order to discover and improve complex solutions evolution and search in general should be allowed to complexify as well as optimize





i  androutsopoulos and p  malakasiotis 2010 a survey of paraphrasing and textual entailment methods volume 38 pages 135187



paraphrasing methods recognize generate or extract phrases sentences or longer natural language expressions that convey almost the same information textual entailment methods on the other hand recognize generate or extract pairs of natural language expressions such that a human who reads and trusts the first element of a pair would most likely infer that the other element is also true paraphrasing can be seen as bidirectional textual entailment and methods from the two areas are often similar both kinds of methods are useful at least in principle in a wide range of natural language processing applications including question answering summarization text generation and machine translation we summarize key ideas from the two areas by considering in turn recognition generation and extraction methods also pointing to prominent articles and resources





j  cheng and  m  j druzdzel 2000 aisbn an adaptive importance sampling algorithm for evidential reasoning in large bayesian networks volume 13 pages 155188









j  c beck 2007 solutionguided multipoint constructive search for job shop scheduling volume 29 pages 4977



solutionguided multipoint constructive search sgmpcs is a novel constructive search technique that performs a series of resourcelimited tree searches where each search begins either from an empty solution as in randomized restart or from a solution that has been encountered during the search a small number of these elite solutions is maintained during the search we introduce the technique and perform three sets of experiments on the job shop scheduling problem first a systematic fully crossed study of sgmpcs is carried out to evaluate the performance impact of various parameter settings second we inquire into the diversity of the elite solution set showing contrary to expectations that a less diverse set leads to stronger performance finally we compare the best parameter setting of sgmpcs from the first two experiments to chronological backtracking limited discrepancy search randomized restart and a sophisticated tabu search algorithm on a set of wellknown benchmark problems results demonstrate that sgmpcs is significantly better than the other constructive techniques tested though lags behind the tabu search



m  guo e  markakis k  r apt and v  conitzer 2013 undominated groves mechanisms volume 46 pages 129163



the family of groves mechanisms which includes the wellknown vcg mechanism also known as the clarke mechanism is a family of efficient and strategyproof mechanisms unfortunately the groves mechanisms are generally not budget balanced that is under such mechanisms payments may flow into or out of the system of the agents resulting in deficits or reduced utilities for the agents we consider the following problem within the family of groves mechanisms we want to identify mechanisms that give the agents the highest utilities under the constraint that these mechanisms must never incur deficits









v  conitzer 2009 eliciting singlepeaked preferences using comparison queries volume 35 pages 161191





scott  kiesel ethan  burns and wheeler  ruml 2015 achieving goals quickly using realtime search experimental results in video games volume 54 pages 123158



in realtime domains such as video games planning happens concurrently with execution and the planning algorithm has a strictly bounded amount of time before it must return the next action for the agent to execute we explore the use of realtime heuristic search in two benchmark domains inspired by video games unlike classic benchmarks such as grid pathfinding and the sliding tile puzzle these new domains feature exogenous change and directed state space graphs we consider the setting in which planning and acting are concurrent and we use the natural objective of minimizing goal achievement time using both the classic benchmarks and the new domains we investigate several enhancements to a leading realtime search algorithm lsslrta we show experimentally that 1 it is better to plan after each action or to use a dynamically sized lookahead 2 abased lookahead can cause undesirable actions to be selected and 3 online debiasing of the heuristic can lead to improved performance we hope this work encourages future research on applying realtime search in dynamic domains





honorable mention for the 2011 ijcaijair best paper prize



we study an approach to policy selection for large relational markov decision processes mdps we consider a variant of approximate policy iteration api that replaces the usual valuefunction learning step with a learning step in policy space this is advantageous in domains where good policies are easier to represent and learn than the corresponding value functions which is often the case for the relational mdps we are interested in in order to apply api to such problems we introduce a relational policy language and corresponding learner in addition we introduce a new bootstrapping routine for goalbased planning domains based on random walks such bootstrapping is necessary for many large relational mdps where reward is extremely sparse as api is ineffective in such domains when initialized with an uninformed policy our experiments show that the resulting system is able to find good policies for a number of classical planning domains and their stochastic variants by solving them as extremely large relational mdps the experiments also point to some limitations of our approach suggesting future work



i  p gent and  t  walsh 1993 an empirical analysis of search in gsat volume 1 pages 4759



we describe an extensive study of search in gsat an approximation procedure for propositional satisfiability gsat performs greedy hillclimbing on the number of satisfied clauses in a truth assignment  our experiments provide a more complete picture of gsats search than previous accounts we describe in detail the two phases of search rapid hillclimbing followed by a long plateau search  we demonstrate that when applied to randomly generated 3sat problems there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate  our results allow us to make detailed numerical conjectures about the length of the hillclimbing phase the average gradient of this phase and to conjecture that both the average score and average branching rate decay exponentially during plateau search we end by showing how these results can be used to direct future theoretical analysis  this work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms





d  opitz and  r  maclin 1999 popular ensemble methods an empirical study volume 11 pages 169198



an ensemble consists of a set of individually trained    classifiers such as neural networks or decision trees whose    predictions are combined when classifying novel instances  previous    research has shown that an ensemble is often more accurate than any of    the single classifiers in the ensemble  bagging breiman 1996c and    boosting freund  shapire 1996 shapire 1990 are two relatively    new but popular methods for producing ensembles  in this paper we    evaluate these methods on 23 data sets using both neural networks and    decision trees as our classification algorithm  our results clearly    indicate a number of conclusions  first while bagging is almost    always more accurate than a single classifier it is sometimes much    less accurate than boosting  on the other hand boosting can create    ensembles that are less accurate than a single classifier     especially when using neural networks  analysis indicates that the    performance of the boosting methods is dependent on the    characteristics of the data set being examined  in fact further    results show that boosting ensembles may overfit noisy data sets thus    decreasing its performance  finally consistent with previous    studies our work suggests that most of the gain in an ensembles    performance comes in the first few classifiers combined however    relatively large gains can be seen up to 25 classifiers when boosting    decision trees





s  safra and  m  tennenholtz 1994 on planning while learning volume 2 pages 111129



this paper introduces a framework for planning while   learning where an agent is given a goal to achieve in anenvironment   whose behavior is only partially known to the agent      we discuss the tractability of various plandesign processes we   show that for a large natural class of planning while learning   systems a plan can be presented and verified in a reasonable time   however coming up algorithmically with a plan even for simple   classes of systems is apparently intractable      we emphasize the role of offline plandesign processes andshow   that in most natural cases the verification projection part canbe   carried out in an efficient algorithmic manner





h  shatkay and  l  p kaelbling 2002 learning geometricallyconstrained hidden markov models for robot navigation bridging the topologicalgeometrical gap volume 16 pages 167207



hidden markov models hmms and partially observable markov    decision processes pomdps provide useful tools for modeling    dynamical systems  they are particularly useful for representing the    topology of environments such as road networks and office buildings    which are typical for robot navigation and planning  the work    presented here describes a formal framework for incorporating readily    available odometric information and geometrical constraints into both    the models and the algorithm that learns them  by taking advantage of    such information learning hmmspomdps can be made to generate better    solutions and require fewer iterations while being robust in the face    of data reduction  experimental results obtained from both simulated    and real robot data demonstrate the effectiveness of the approach





g  a kaminka  d  v pynadath and  m  tambe 2002 monitoring teams by overhearing a multiagent planrecognition approach volume 17 pages 83135



recent years are seeing an increasing need for online    monitoring of teams of cooperating agents eg for visualization or    performance tracking however in monitoring deployed teams we often    cannot rely on the agents to always communicate their state to the    monitoring system this paper presents a nonintrusive approach to    monitoring by overhearing where the monitored teams state is    inferred via planrecognition from teammembers routine    communications exchanged as part of their coordinated task execution    and observed overheard by the monitoring system key challenges in    this approach include the demanding runtime requirements of    monitoring the scarceness of observations increasing monitoring    uncertainty and the need to scaleup monitoring to address    potentially large teams to address these we present a set of    complementary novel techniques exploiting knowledge of the social    structures and procedures in the monitored team i an efficient    probabilistic planrecognition algorithm wellsuited for processing    communications as observations ii an approach to exploiting    knowledge of the teams social behavior to predict future observations    during execution reducing monitoring uncertainty and iii    monitoring algorithms that trade expressivity for scalability    representing only certain useful monitoring hypotheses but allowing    for any number of agents and their different activities to be    represented in a single coherent entity we present an empirical    evaluation of these techniques in combination and apart in    monitoring a deployed team of agents running on machines physically    distributed across the country and engaged in complex dynamic task    execution we also compare the performance of these techniques to    human expert and novice monitors and show that the techniques    presented are capable of monitoring at humanexpert levels despite    the difficulty of the task





j  veness ks  ng m  hutter w  uther and d  silver 2011 a montecarlo aixi approximation volume 40 pages 95142







q  zhao and  t  nishida 1995 using qualitative hypotheses to identify inaccurate data volume 3 pages 119145



identifying inaccurate data has long been regarded as a    significant and difficult problem in ai in this paper we present a    new method for identifying inaccurate data on the basis of qualitative    correlations among related data first we introduce the definitions    of related data and qualitative correlations among related data  then    we put forward a new concept called support coefficient function    scf scf can be used to extract represent and calculate    qualitative correlations among related data within a dataset we    propose an approach to determining dynamic shift intervals of    inaccurate data and an approach to calculating possibility of    identifying inaccurate data respectively both of the approaches are    based on scf finally we present an algorithm for identifying    inaccurate data by using qualitative correlations among related data    as confirmatory or disconfirmatory evidence we have developed a    practical system for interpreting infrared spectra by applying the    method and have fully tested the system against several hundred real    spectra the experimental results show that the method is    significantly better than the conventional methods used in many    similar systems









z  feldman and c  domshlak 2014 simple regret optimization in online planning for markov decision processes volume 51 pages 165205



a  gerevini and  l  schubert 1996 accelerating partialorder planners some techniques for effective search control and pruning volume 5 pages 95137



we propose some domainindependent techniques for bringing wellfounded partialorder planners closer to practicality the first two techniques are aimed at improving search control while keeping overhead costs low  one is based on a simple adjustment to the default a heuristic used by ucpop to select plans for refinement the other is based on preferring zero commitment forced plan refinements whenever possible and using lifo prioritization otherwise a more radical technique is the use of operator parameter domains to prune search these domains are initially computed from the definitions of the operators and the initial and goal conditions using a polynomialtime algorithm that propagates sets of constants through the operator graph starting in the initial conditions during planning parameter domains can be used to prune nonviable operator instances and to remove spurious clobbering threats  in experiments based on modifications of ucpop our improved plan and goal selection strategies gave speedups by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version crucially the hardest problems gave the greatest improvements the pruning technique based on parameter domains often gave speedups by an order of magnitude or more for difficult problems both with the default ucpop search strategy and with our improved strategy the lisp code for our techniques and for the test problems is provided in online appendices





b  de keijzer t  b klos and y  zhang 2014 finding optimal solutions for voting game design problems volume 50 pages 105140



in many circumstances where multiple agents need to make a joint decision voting is used to aggregate the agents preferences each agents vote carries a weight and if the sum of the weights of the  agents in favor of some outcome is larger than or equal to a given quota then this outcome is decided upon the distribution of weights leads to a certain distribution of power several power indices have been proposed to measure such power in the socalled inverse problem we are given a target distribution of power and are asked to come up with a game in the form of a quota plus an assignment of weights to the players whose power distribution is as close as possible to the target distribution according to some specied distance measure

we first present a doubly exponential algorithm for enumerating the set of simple games we then improve on this algorithm for the class of weighted voting games and obtain a quadratic exponential ie 2on2 algorithm for enumerating them we show that this improved algorithm runs in outputpolynomial time making it the fastest possible enumeration algorithm up to a polynomial factor finally we propose an exact anytimealgorithm that runs in exponential time for the power index weighted voting game design problem the inverse problem we implement this algorithm to find a weighted voting game with a normalized banzhaf power distribution closest to a target power index and perform experiments to obtain some insights about the set of weighted voting games we remark that our algorithm is applicable to optimizing any exponentialtime computable function the distance of the normalized banzhaf index to a target power index is merely taken as an example





d  terekhov and j   c beck 2008 a constraint programming approach for solving a queueing control problem volume 32 pages 123167



in a facility with front room and back room operations it is useful to switch workers between the rooms in order to cope with changing customer demand assuming stochastic customer arrival and service times we seek a policy for switching workers such that the expected customer waiting time is minimized while the expected back room staffing is sufficient to perform all work  three novel constraint programming models and several shaving procedures for these models are presented experimental results show that a model based on closedform expressions together with a combination of shaving procedures is the most efficient this model is able to find and prove optimal solutions for many problem instances within a reasonable runtime previously the only available approach was a heuristic algorithm furthermore a hybrid method combining the heuristic and the best constraint programming method is shown to perform as well as the heuristic in terms of solution quality over time while achieving the same performance in terms of proving optimality as the pure constraint programming model this is the first work of which we are aware that solves such queueingbased problems with constraint programming





a  moore and  m  s lee 1998 cached sufficient statistics for efficient machine learning with large datasets volume 8 pages 6791



this paper introduces new algorithms and data structures for    quick counting for machine learning datasets  we focus on the    counting task of constructing contingency tables but our approach is    also applicable to counting the number of records in a dataset that    match conjunctive queries  subject to certain assumptions the costs    of these operations can be shown to be independent of the number of    records in the dataset and loglinear in the number of nonzero entries    in the contingency table      we provide a very sparse data structure the adtree to minimize    memory use we provide analytical worstcase bounds for this structure    for several models of data distribution  we empirically demonstrate    that tractablysized data structures can be produced for large    realworld datasets by a using a sparse tree structure that never    allocates memory for counts of zero b never allocating memory for    counts that can be deduced from other counts and c not bothering to    expand the tree fully near its leaves       we show how the adtree can be used to accelerate bayes net structure    finding algorithms rule learning algorithms and feature selection    algorithms and we provide a number of empirical results comparing    adtree methods against traditional direct counting approaches  we    also discuss the possible uses of adtrees in other machine learning    methods and discuss the merits of adtrees in comparison with    alternative representations such as kdtrees rtrees and frequent sets





l  k saul  t  jaakkola and  m  i jordan 1996 mean field theory for sigmoid belief networks volume 4 pages 6176



we develop a mean field theory for sigmoid belief networks    based on ideas from statistical mechanics  our mean field theory    provides a tractable approximation to the true probability    distribution in these networks it also yields a lower bound on the    likelihood of evidence  we demonstrate the utility of this framework    on a benchmark problem in statistical pattern recognitionthe    classification of handwritten digits





e  hebrard d  marx b  osullivan and i  razgon 2011 soft constraints of difference and equality volume 41 pages 97130



in many combinatorial problems one may need to model the diversity or similarity of assignments in a solution for example one may wish to maximise or minimise the number of distinct values in a solution to formulate problems of this type we can use soft variants of the well known alldifferent and allequal constraints  we present a taxonomy of six soft global constraints generated by combining the two latter ones and the two standard cost functions which are either maximised or minimised we characterise the complexity of achieving arc and bounds consistency on these constraints resolving those cases for which nphardness was neither proven nor disproven in particular we explore in depth the constraint ensuring that at least k pairs of variables have a common value we show that achieving arc consistency is nphard however achieving bounds consistency can be done in polynomial time through dynamic programming moreover we show that the maximum number of pairs of equal variables can be approximated by a factor 12 with a linear time greedy algorithm finally we provide a fixed parameter tractable algorithm with respect to the number of values appearing in more than two distinct domains interestingly this taxonomy shows that enforcing equality is harder than enforcing difference





c  lusena  j  goldsmith and  m  mundhenk 2001 nonapproximability results for partially observable markov decision processes volume 14 pages 83103



we show that for several variations of partially observable    markov decision processes polynomialtime algorithms for finding    control policies are unlikely to or simply dont have guarantees of    finding policies within a constant factor or a constant summand of    optimal  here unlikely means unless some complexity classes    collapse where the collapses considered are pnp ppspace or    pexp  until or unless these collapses are shown to hold any    controlpolicy designer must choose between such performance    guarantees and efficient computation









p  nakov and h t  ng 2012 improving statistical machine translation for a resourcepoor language using related resourcerich languages volume 44 pages 179222





r  booth and t  meyer 2006 admissible and restrained revision volume 26 pages 127151



as partial justification of their framework for iterated belief revision darwiche and pearl convincingly argued against boutiliers natural revision and provided a prototypical revision operator that fits into their scheme  we show that the darwichepearl arguments lead naturally to the acceptance of a smaller class of operators which we refer to as admissible admissible revision ensures that the penultimate input is not ignored completely thereby eliminating natural revision but includes the darwichepearl operator nayaks lexicographic revision operator and a newly introduced operator called restrained revision we demonstrate that restrained revision is the most conservative of admissible revision operators  effecting as few changes as possible while lexicographic revision is the least conservative and point out that restrained revision can also be viewed as a composite operator consisting of natural revision preceded by an application of a backwards revision operator previously studied by papini finally we propose the establishment of a principled approach for choosing an appropriate revision operator in different contexts and discuss future work 

