2003 ijcaijair best paper prize

this paper presents a new approach to hierarchical    reinforcement learning based on decomposing the target markov decision    process mdp into a hierarchy of smaller mdps and decomposing the    value function of the target mdp into an additive combination of the    value functions of the smaller mdps  the decomposition known as the    maxq decomposition has both a procedural semanticsas a subroutine    hierarchyand a declarative semanticsas a representation of the    value function of a hierarchical policy  maxq unifies and extends    previous work on hierarchical reinforcement learning by singh    kaelbling and dayan and hinton  it is based on the assumption that    the programmer can identify useful subgoals and define subtasks that    achieve these subgoals  by defining such subgoals the programmer    constrains the set of policies that need to be considered during    reinforcement learning  the maxq value function decomposition can    represent the value function of any policy that is consistent with the    given hierarchy  the decomposition also creates opportunities to    exploit state abstractions so that individual mdps within the    hierarchy can ignore large parts of the state space  this is    important for the practical application of the method  this paper    defines the maxq hierarchy proves formal results on its    representational power and establishes five conditions for the safe    use of state abstractions  the paper presents an online modelfree    learning algorithm maxqq and proves that it converges with    probability 1 to a kind of locallyoptimal policy known as a    recursively optimal policy even in the presence of the five kinds of    state abstraction  the paper evaluates the maxq representation and    maxqq through a series of experiments in three domains and shows    experimentally that maxqq with state abstractions converges to a    recursively optimal policy much faster than flat q learning  the fact    that maxq learns a representation of the value function has an    important benefit it makes it possible to compute and execute an    improved nonhierarchical policy via a procedure similar to the    policy improvement step of policy iteration  the paper demonstrates    the effectiveness of this nonhierarchical execution experimentally    finally the paper concludes with a comparison to related work and a    discussion of the design tradeoffs in hierarchical reinforcement learning

