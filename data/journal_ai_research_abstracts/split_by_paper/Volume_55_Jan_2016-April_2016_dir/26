ivan  vuli263 and mariefrancine  moens 2016 bilingual distributed word representations from documentaligned comparable data volume 55 pages 953994

we propose a new model for learning bilingual word representations from nonparallel documentaligned data following the recent advances in word representation learning our model learns dense realvalued word vectors that is bilingual word embeddings bwes unlike prior work on inducing bwes which heavily relied on parallel sentencealigned corpora andor readily available translation resources such as dictionaries the article reveals that bwes may be learned solely on the basis of documentaligned comparable data without any additional lexical resources nor syntactic information we present a comparison of our approach with previous stateoftheart models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling muptm as well as with distributional local contextcounting models we demonstrate the utility of the induced bwes in two semantic tasks 1 bilingual lexicon extraction 2 suggesting word translations in context for polysemous words our simple yet effective bwebased models significantly outperform the muptmbased and contextcounting representation models from comparable data as well as prior bwebased models and acquire the best reported results on both tasks for all three tested language pairs

