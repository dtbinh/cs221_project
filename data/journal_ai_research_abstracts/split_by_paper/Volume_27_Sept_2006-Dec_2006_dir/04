m  lapata and a  lascarides 2006 learning sentenceinternal temporal relations volume 27 pages 85117

in this paper we propose a data intensive approach for inferring sentenceinternal temporal relations temporal inference is relevant for practical nlp applications which either extract or synthesize temporal information eg summarisation question answering  our method bypasses the need for manual coding by exploiting the presence of markers like after which overtly signal a temporal relation we first show that models trained on main and subordinate clauses connected with a temporal marker achieve good performance on a pseudodisambiguation task simulating temporal inference during testing the temporal marker is treated as unseen and the models must select the right marker from a set of possible candidates  secondly we assess whether the proposed approach holds promise for the semiautomatic creation of temporal annotations  specifically we use a model trained on noisy and approximate data ie main and subordinate clauses to predict intrasentential relations present in timebank a corpus annotated rich temporal information  our experiments compare and contrast several probabilistic models differing in their feature space linguistic assumptions and data requirements  we evaluate performance against gold standard corpora and also against human subjects 

