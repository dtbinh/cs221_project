



in reallife temporal scenarios uncertainty and preferences are often  essential and coexisting aspects  we present a formalism where quantitative temporal constraints with both preferences and uncertainty can be defined  we show how three classical notions of controllability that is strong weak and dynamic which have been developed for uncertain temporal problems can be generalized to handle preferences as well after defining this general framework we focus on problems where preferences follow the fuzzy approach and with properties that assure tractability for such problems we propose algorithms to check the presence of the controllability properties in particular we show that in such a setting dealing simultaneously with preferences and uncertainty does not increase the complexity of controllability testing  we also develop a dynamic execution algorithm of polynomial complexity that produces temporal plans under uncertainty that are optimal with respect to fuzzy preferences





s  sanghai p  domingos and d  weld 2005 relational dynamic bayesian networks volume 24 pages 759797



stochastic processes that involve the creation of objects and relations over time are widespread but relatively poorly studied for example accurate fault diagnosis in factory assembly processes requires inferring the probabilities of erroneous assembly operations but doing this efficiently and accurately is difficult modeled as dynamic bayesian networks these processes have discrete variables with very large domains and extremely high dimensionality in this paper we introduce relational dynamic bayesian networks rdbns which are an extension of dynamic bayesian networks dbns to firstorder logic rdbns are a generalization of dynamic probabilistic relational models dprms which we had proposed in our previous work to model dynamic uncertain domains we first extend the raoblackwellised particle filtering described in our earlier work to rdbns next we lift the assumptions associated with raoblackwellization in rdbns and propose two new forms of particle filtering the first one uses abstraction hierarchies over the predicates to smooth the particle filters estimates the second employs kernel density estimation with a kernel function specifically designed for relational domains experiments show these two methods greatly outperform standard particle filtering on the task of assembly plan execution monitoring 





heshan  du and natasha  alechina 2016 qualitative spatial logics for buffered geometries volume 56 pages 693745



this paper describes a series of new qualitative spatial logics for checking consistency of sameas and partof matches between spatial objects from different geospatial datasets especially from crowdsourced datasets since geometries in crowdsourced data are usually not very accurate or precise we buffer geometries by a margin of error or a level of tolerance and define spatial relations for buffered geometries the spatial logics formalize the notions of buffered equal intuitively corresponding to possibly sameas buffered part of possibly partof near possibly connected and far definitely disconnected a sound and complete axiomatisation of each logic is provided with respect to models based on metric spaces for each of the logics the satisfiability problem is shown to be npcomplete finally we briefly describe how the logics are used in a system for generating and debugging matches between spatial objects and report positive experimental evaluation results for the system



many artificial intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings a problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings this paper introduces the unanimous improvement ratio uir a measure that complements standard metric combination criteria such as van rijsbergens fmeasure and indicates how robust the measured differences are to changes in the relative weights of the individual metrics uir is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted

besides discussing the theoretical foundations of uir this paper presents empirical results that confirm the validity and usefulness of the metric for the text clustering problem where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them remarkably our experiments show that uir can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed











b  cuenca grau i  horrocks m  kr246tzsch c  kupke d  magka b  motik and z  wang 2013 acyclicity notions for existential rules and their application to query answering in ontologies volume 47 pages 741808







roderick  sebastiaan  de nijs christian   landsiedel dirk  wollherr and martin  buss 2016 quadratization and roof duality of markov logic networks volume 55 pages 685714



recently considerable focus has been given to the problem of determining the boundary between tractable and intractable planning problems in this paper we study the complexity of planning in the class cn of planning problems characterized by unary operators and directed path causal graphs although this is one of the simplest forms of causal graphs a planning problem can have we show that planning is intractable for cn unless p  np even if the domains of state variables have bounded size in particular we show that plan existence for cnk is nphard for k5 by reduction from cnfsat here k denotes the upper bound on the size of the state variable domains our result reduces the complexity gap for the class cnk to cases k3 and k4 only since cn2 is known to be tractable









m  bodirsky and m  hils 2012 tractable set constraints volume 45 pages 731759



many fundamental problems in artificial intelligence knowledge representation and verification involve reasoning about sets and relations between sets and can be modeled as set constraint satisfaction problems set csps such problems are frequently intractable but there are several important set csps that are known to be polynomialtime tractable  we introduce a large class of set csps that can be solved in quadratic time  our class which we call ei contains all previously known tractable set csps but also some new ones that are of crucial importance for example in description logics  the class of ei set constraints has an elegant universalalgebraic characterization which we use to show that every set constraint language that properly contains all ei set constraints already has a finite sublanguage with an nphard constraint satisfaction problem



a  dhurandhar and j  wang 2013 single network relational transductive learning volume 48 pages 813839



relational classification on a single connected network has been of particular interest in the machine learning and data mining communities in the last decade or so this is mainly due to the explosion in popularity of social networking sites such as facebook linkedin and google amongst others in statistical relational learning many techniques have been developed to address this problem where we have a connected unweighted homogeneousheterogeneous graph that is partially labeled and the goal is to propagate the labels to the unlabeled nodes in this paper we provide a different perspective by enabling the effective use of graph transduction techniques for this problem we thus exploit the strengths of this class of methods for relational learning problems we accomplish this by providing a simple procedure for constructing a weight matrix that serves as input to a rich class of graph transduction techniques our procedure has multiple desirable properties for example the weights it assigns to edges between unlabeled nodes naturally relate to a measure of association commonly used in statistics namely the gamma test statistic we further portray the efficacy of our approach on synthetic as well as real data by comparing it with stateoftheart relational learning algorithms and graph transduction techniques with an adjacency matrix or a real valued weight matrix computed using available attributes as input in these experiments we see that our approach consistently outperforms other approaches when the graph is sparsely labeled and remains competitive with the best when the proportion of known labels increases









m  vasirani and s  ossowski 2012 a marketinspired approach for intersection management in urban road traffic networks volume 43 pages 621659



this paper studies the relationship between resolution and conflict driven clause learning cdcl without restarts and refutes some conjectured possible separations  we prove that the guarded xorified pebbling tautology clauses which urquhart proved are hard for regular resolution as well as the guarded graph tautology clauses of alekhnovich johannsen pitassi and urquhart have polynomial size pool resolution refutations that use only input lemmas as learned clauses  for the latter set of clauses we extend this to prove that a cdcl search without restarts can refute these clauses in polynomial time provided it makes the right choices for decision literals and clause learning  this holds even if the cdcl search is required to greedily process conflicts arising from unit propagation this refutes the conjecture that the guarded graph tautology clauses or the guarded xorified pebbling tautology clauses can be used to separate cdcl without restarts from general resolution  together with subsequent results by buss and kolodziejczyk this means we lack any good conjectures about how to establish the exact logical strength of conflictdriven clause learning without restarts









a  felner  r  stern  a  benyair  s  kraus and  n  netanyahu 2004 pha finding the shortest path with  a in an unknown physical environment volume 21 pages 631670



we address the problem of finding the shortest path between two points in an unknown real physical environment where a traveling agent must move around in the environment to explore unknown territory  we introduce the physicala algorithm pha for solving this problem pha expands all the mandatory nodes that a would expand and returns the shortest path between the two points  however due to the physical nature of the problem the complexity of the algorithm is measured by the traveling effort of the moving agent and not by the number of generated nodes as in standard a  pha is presented as a twolevel algorithm such that its high level a chooses the next node to be expanded and its low level directs the agent to that node in order to explore it  we present a number of variations for both the highlevel and lowlevel procedures and evaluate their performance theoretically and experimentally  we show that the travel cost of our best variation is fairly close to the optimal travel cost assuming that the mandatory nodes of a are known in advance  we then generalize our algorithm to the multiagent case where a number of cooperative agents are designed to solve the problem  specifically we provide an experimental implementation for such a system  it should be noted that the problem addressed here is not a navigation problem but rather a problem of finding the shortest path between two points for future usage







p  a bonatti c  lutz and f  wolter 2009 the complexity of circumscription in dls volume 35 pages 717773



many websearch queries serve as the beginning of an exploration of an unknown space of information rather than looking for a specific web page to answer such queries effec tively the search engine should attempt to organize the space of relevant information in a way that facilitates exploration

aspector combines two sources of information to compute aspects we discover candidate aspects by analyzing query logs and cluster them to eliminate redundancies we then use a masscollaboration knowledge base eg wikipedia to compute candidate aspects for queries that occur less frequently and to group together aspects that are likely to be semantically related we present a user study that indicates that the aspects we compute are rated favorably against three competing alternatives  related searches proposed by google cluster labels assigned by the clusty search engine and navigational searches proposed by bing











g  stefanoni b  motik m  kroetzsch and s  rudolph 2014 the complexity of answering conjunctive and navigational queries over owl 2 el knowledge bases volume 51 pages 645705







uai 2013 best student paper



in the efficient social choice problem the goal is to assign values subject to side constraints to a set of variables to maximize the total utility across a population of agents where each agent has private information about its utility function in this paper we model the social choice problem as a distributed constraint optimization problem dcop in which each agent can  communicate with other agents that share an interest in one or more variables whereas existing dcop algorithms can be easily manipulated by an agent either by misreporting private  information or deviating from the algorithm we introduce mdpop the first dcop algorithm that provides a faithful distributed implementation for efficient social choice this provides a concrete example of how the methods of mechanism design can be unified with those of distributed optimization faithfulness ensures that no agent can benefit by unilaterally deviating from any aspect of the protocol neither informationrevelation computation nor communication and whatever the private information of other agents we allow for payments by agents to a central bank which is the only central authoritythat we require to achieve faithfulness we carefully integrate the vickreyclarkegroves vcg mechanism with the dpop algorithm such that  each agent is only asked to perform computation report information and send messages that is in its own best interest determining agent is payment requires solving the social choice problem without agent i here we present a method to reuse computation performed in solving the main problem in a way that is robust against manipulation by the excluded agent experimental results on structured problems show that as much as 87 of the computation required for solving the marginal problems can be avoided by reuse providing very good scalability in the number of agents





