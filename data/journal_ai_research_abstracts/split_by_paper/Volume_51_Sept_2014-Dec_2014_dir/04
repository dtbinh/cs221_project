z  feldman and c  domshlak 2014 simple regret optimization in online planning for markov decision processes volume 51 pages 165205

we consider online planning in markov decision processes mdps in online planning the agent focuses on its current state only deliberates about the set of possible policies from that state onwards and when interrupted uses the outcome of that exploratory deliberation to choose what action to perform next formally the performance of algorithms for online planning is assessed in terms of simple regret the agents expected performance loss when the chosen action rather than an optimal one is followed

to date stateoftheart algorithms for online planning in general mdps are either best effort or guarantee only polynomialrate reduction of simple regret over time here we introduce a new montecarlo tree search algorithm brue that guarantees exponentialrate and smooth reduction of simple regret  at a high level brue is based on a simple yet nonstandard statespace sampling scheme mcts2e in which different parts of each sample are dedicated to different exploratory objectives we further extend brue with a variant of learning by forgetting the resulting parametrized algorithm bruealpha exhibits even more attractive formal guarantees than brue our  empirical evaluation shows that both brue and its generalization bruealpha are also very effective in practice and  compare favorably to the stateoftheart

