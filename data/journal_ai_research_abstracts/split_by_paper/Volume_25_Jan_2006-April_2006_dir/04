honorable mention for the 2011 ijcaijair best paper prize

we study an approach to policy selection for large relational markov decision processes mdps we consider a variant of approximate policy iteration api that replaces the usual valuefunction learning step with a learning step in policy space this is advantageous in domains where good policies are easier to represent and learn than the corresponding value functions which is often the case for the relational mdps we are interested in in order to apply api to such problems we introduce a relational policy language and corresponding learner in addition we introduce a new bootstrapping routine for goalbased planning domains based on random walks such bootstrapping is necessary for many large relational mdps where reward is extremely sparse as api is ineffective in such domains when initialized with an uninformed policy our experiments show that the resulting system is able to find good policies for a number of classical planning domains and their stochastic variants by solving them as extremely large relational mdps the experiments also point to some limitations of our approach suggesting future work

