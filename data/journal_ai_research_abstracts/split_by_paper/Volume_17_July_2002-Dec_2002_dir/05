r  nock 2002 inducing interpretable voting classifiers without trading accuracy for simplicity theoretical results approximation algorithms volume 17 pages 137170

recent advances in the study of voting classification    algorithms have brought empirical and theoretical results clearly    showing the discrimination power of ensemble classifiers it has been    previously argued that the search of this classification power in the    design of the algorithms has marginalized the need to obtain    interpretable classifiers therefore the question of whether one    might have to dispense with interpretability in order to keep    classification strength is being raised in a growing number of machine    learning or data mining papers the purpose of this paper is to study    both theoretically and empirically the problem first we provide    numerous results giving insight into the hardness of the    simplicityaccuracy tradeoff for voting classifiers then we provide    an efficient topdown and prune induction heuristic widc mainly    derived from recent results on the weak learning and boosting    frameworks  it is to our knowledge the first attempt to build a    voting classifier as a base formula using the weak learning framework    the one which was previously highly successful for decision tree    induction and not the strong learning framework as usual for such    classifiers with boostinglike approaches while it uses a wellknown    induction scheme previously successful in other classes of concept    representations thus making it easy to implement and compare widc    also relies on recent or new results we give about particular cases of    boosting known as partition boosting and ranking loss    boosting experimental results on thirtyone domains most of which    readily available tend to display the ability of widc to produce    small accurate and interpretable decision committees

