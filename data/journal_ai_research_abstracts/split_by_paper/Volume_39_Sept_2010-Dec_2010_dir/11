m  geist and o  pietquin 2010 kalman temporal differences volume 39 pages 483532

because reinforcement learning suffers from a lack of scalability online value and q function approximation has received increasing interest this last decade this contribution introduces a novel approximation scheme namely the kalman temporal differences ktd framework that exhibits the following features sampleefficiency nonlinear approximation nonstationarity handling and uncertainty management a first ktdbased algorithm is provided for deterministic markov decision processes mdp which produces biased estimates in the case of stochastic transitions than the extended ktd framework xktd solving stochastic mdp is described convergence is analyzed for special cases for both deterministic and stochastic transitions related algorithms are experimented on classical benchmarks they compare favorably to the state of the art while exhibiting the announced features

