g  m weiss and  f  provost 2003 learning when training data are costly the effect of class distribution on tree induction volume 19 pages 315354

for large realworld inductive learning problems the number of training examples often must be limited due to the costs associated with procuring preparing and storing the training examples andor the computational costs associated with learning from them in such circumstances one question of practical importance is if only n training examples can be selected in what proportion should the classes be represented  in this article we help to answer this question by analyzing for a fixed trainingset size the relationship between the class distribution of the training data and the performance of classification trees induced from these data we study twentysix data sets and for each determine the best class distribution for learning  the naturally occurring class distribution is shown to generally perform well when classifier performance is evaluated using undifferentiated error rate 01 loss  however when the area under the roc curve is used to evaluate classifier performance a balanced distribution is shown to perform well  since neither of these choices for class distribution always generates the bestperforming classifier we introduce a budgetsensitive progressive sampling algorithm for selecting training examples based on the class associated with each example  an empirical analysis of this algorithm shows that the class distribution of the resulting training set yields classifiers with good nearlyoptimal classification performance

