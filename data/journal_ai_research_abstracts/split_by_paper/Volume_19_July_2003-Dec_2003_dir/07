e  wiewiora 2003 potentialbased shaping and qvalue initialization are equivalent volume 19 pages 205208

shaping has proven to be a powerful but precarious means of improving reinforcement learning performance ng harada and russell 1999 proposed the potentialbased shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior   in this note we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms more specifically we prove that a reinforcement learner with initial qvalues based on the shaping algorithms potential function make the same updates throughout learning as a learner receiving potentialbased shaping rewards we further prove that under a broad category of policies the behavior of these two learners are indistinguishable the comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithms benefit in addition the equivalence raises previously unaddressed issues concerning the efficiency of learning with potentialbased shaping

