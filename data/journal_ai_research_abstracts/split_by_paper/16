



hierarchical task decomposition is a method used in many agent systems to organize agent knowledge  this work shows how the combination of a hierarchy and persistent assertions of knowledge can lead to difficulty in maintaining logical consistency in asserted knowledge we explore the problematic consequences of persistent assumptions in the reasoning process and introduce novel potential solutions  having implemented one of the possible solutions dynamic hierarchical justification its effectiveness is demonstrated with an empirical analysis









m  a walker a  stent f  mairesse and r  prasad 2007 individual and domain adaptation in sentence planning for dialogue volume 30 pages 413456



one of the biggest challenges in the development and deployment of spoken dialogue systems is the design of the spoken language generation module this challenge arises from the need for the generator to adapt to many features of the dialogue domain user population and dialogue context  a promising approach is trainable generation which uses generalpurpose linguistic knowledge that is automatically adapted to the features of interest such as the application domain individual user or user group  in this paper we present and evaluate a trainable sentence planner for providing restaurant information in the match dialogue system  we show that trainable sentence planning can produce complex information presentations whose quality is comparable to the output of a templatebased generator tuned to this domain  we also show that our method easily supports adapting the sentence planner to individuals and that the individualized sentence planners generally perform better than models trained and tested on a population of individuals previous work has documented and utilized individual preferences for content selection but to our knowledge these results provide the first demonstration of individual preferences for sentence planning operations affecting the content order discourse structure and sentence structure of system responses finally we evaluate the contribution of different feature sets and show that in our application ngram features often do as well as features based on higherlevel linguistic representations



we develop multiattribute auctions that accommodate generalized additive independent gai preferences we propose an iterative auction mechanism that maintains prices on potentially overlapping gai clusters of attributes thus decreases elicitation and computational burden and creates an open competition among suppliers over a multidimensional domain most significantly the auction is guaranteed to achieve surplus which approximates optimal welfare up to a small additive factor under reasonable equilibrium strategies of traders the main departure of gai auctions from previous literature is to accommodate nonadditive trader preferences hence allowing traders to condition their evaluation of specific attributes on the value of other attributes at the same time the gai structure supports a compact representation of prices enabling a tractable auction process we perform a simulation study demonstrating and quantifying the significant efficiency advantage of more expressive preference modeling we draw random gaistructured utility functions with various internal structures generate additive functions that approximate the gai utility and compare the performance of the auctions using the two representations we find that allowing traders to express existing dependencies among attributes improves the economic efficiency of multiattribute auctions





the graphplan algorithm for generating optimal makespan plans containing parallel sets of actions remains one of the most effective ways to generate such plans  however despite enhancements on a range of fronts the approach is currently dominated in terms of speed by state space planners that employ distancebased heuristics to quickly generate serial plans  we report on a family of strategies that employ available memory to construct a search trace so as to learn from various aspects of graphplans iterative search episodes in order to expedite search in subsequent episodes  the planning approaches can be partitioned into two classes according to the type and extent of search experience captured in the trace  the planners using the more aggressive tracing method are able to avoid much of graphplans redundant search effort while planners in the second class trade off this aspect in favor of a much higher degree of freedom than graphplan in traversing the space of states generated during regression search on the planning graph  the tactic favored by the second approach exploiting the search trace to transform the depthfirst ida nature of graphplans search into an iterative state space view is shown to be the more powerful  we demonstrate that distancebased state space heuristics can be adapted to informed traversal of the search trace used by the second class of planners and develop an augmentation targeted specifically at planning graph search  guided by such a heuristic the stepoptimal version of the planner in this class clearly dominates even a highly enhanced version of graphplan  by adopting beam search on the search trace we then show that virtually optimal parallel plans can be generated at speeds quite competitive with a modern heuristic state space planner







efficient implementations of dpll with the addition of clause learning are the fastest complete boolean satisfiability solvers and can handle many significant realworld problems such as verification planning and design despite its importance little is known of the ultimate strengths and limitations of the technique this paper presents the first precise characterization of clause learning as a proof system cl and begins the task of understanding its power by relating it to the wellstudied resolution proof system in particular we show that with a new learning scheme cl can provide exponentially shorter proofs than many proper refinements of general resolution res satisfying a natural property these include regular and davisputnam resolution which are already known to be much stronger than ordinary dpll we also show that a slight variant of cl with unlimited restarts is as powerful as res itself translating these analytical results to practice however presents a challenge because of the nondeterministic nature of clause learning algorithms we propose a novel way of exploiting the underlying problem structure in the form of a high level problem description such as a graph or pddl specification to guide clause learning algorithms toward faster solutions we show that this leads to exponential speedups on grid and randomized pebbling problems as well as substantial improvements on certain ordering formulas













we investigate complexity issues related to pure nash equilibria of strategic games we show that even in very restrictive settings determining whether a game has a pure nash equilibrium is nphard while deciding whether a game has a strong nash equilibrium is sigmap2complete  we then study practically relevant restrictions that lower the complexity in particular we are interested in quantitative and qualitative restrictions of the way each players payoff depends on moves of other players  we say that a game has small neighborhood if the utility function for each player depends only on the actions of a logarithmically small number of other players the dependency structure of a game g can be expressed by a graph dgg or by a hypergraph hg by relating nash equilibrium problems to constraint satisfaction problems csps we show that if g has small neighborhood and if hg has bounded hypertree width or if dgg has bounded treewidth then finding pure nash and pareto equilibria is feasible in polynomial time if the game is graphical then these problems are logcflcomplete and thus in the class nc2 of highly parallelizable problems







natural language generation nlg systems are computer software    systems that produce texts in english and other human languages often    from nonlinguistic input data nlg systems like most ai systems need    substantial amounts of knowledge  however our experience in two     nlg projects suggests that it is difficult to acquire correct knowledge     for nlg systems indeed every knowledge acquisition ka technique     we tried had significant problems in general terms these problems were     due to the complexity novelty and poorly understood nature of the     tasks our systems attempted and were worsened by the fact that people    write so differently this meant in particular that corpusbased ka     approaches suffered because it was impossible to assemble a sizable     corpus of highquality consistent manually written texts in our domains    and structured expertoriented ka techniques suffered because experts     disagreed and because we could not get enough information about special     and unusual cases to build robust systems we believe that such problems     are likely to affect many other nlg systems as well  in the long term     we hope that new ka techniques may emerge to help nlg system builders      in the shorter term we believe that understanding how individual ka     techniques can fail and using a mixture of different ka techniques     with different strengths and weaknesses can help developers acquire     nlg knowledge that is mostly correct



we describe a general approach to optimization which we term    squeaky wheel optimization swo  in swo a greedy algorithm is    used to construct a solution which is then analyzed to find the    trouble spots ie those elements that if improved are likely to    improve the objective function score  the results of the analysis are    used to generate new priorities that determine the order in which the    greedy algorithm constructs the next solution  this    constructanalyzeprioritize cycle continues until some limit is    reached or an acceptable solution is found         swo can be viewed as operating on two search spaces solutions and    prioritizations  successive solutions are only indirectly related    via the reprioritization that results from analyzing the prior    solution  similarly successive prioritizations are generated by    constructing and analyzing solutions  this coupled search has some    interesting properties which we discuss         we report encouraging experimental results on two domains scheduling    problems that arise in fiberoptic cable manufacturing and graph    coloring problems  the fact that these domains are very different    supports our claim that swo is a general technique for optimization









xujin  chen xiaodong  hu tieyan  liu weidong  ma tao  qin pingzhong  tang changjun  wang and bo  zheng 2016 efficient mechanism design for online scheduling volume 56 pages 429461



this paper concerns the mechanism design for online scheduling in a strategic setting in this setting each job is owned by a selfinterested agent who may misreport the release time deadline length and value of her job while we need to determine not only the schedule of the jobs but also the payment of each agent we focus on the design of incentive compatible ic mechanisms and study the maximization of social welfare ie the aggregated value of completed jobs by competitive analysis we first derive two lower bounds on the competitive ratio of any deterministic ic mechanism to characterize the landscape of our research we then propose a deterministic ic mechanism and show that such a simple mechanism works very well for both the preemptionrestart model and the preemptionresume model we show the mechanism can achieve the optimal competitive ratio of 5 for equallength jobs and a near optimal competitive ratio within a constant factor for unequallength jobs



d  golovin and a  krause 2011 adaptive submodularity theory and applications in active learning and stochastic optimization volume 42 pages 427486











d  zhang and y  zhang 2008 an ordinal bargaining solution with fixedpoint property volume 33 pages 433464



in default reasoning usually not all possible ways of    resolving conflicts between default rules are acceptable  criteria    expressing acceptable ways of resolving the conflicts may be hardwired    in the inference mechanism for example specificity in inheritance    reasoning can be handled this way or they may be given abstractly as    an ordering on the default rules  in this article we investigate    formalizations of the latter approach in reiters default logic  our    goal is to analyze and compare the computational properties of three    such formalizations in terms of their computational complexity the    prioritized default logics of baader and hollunder and brewka and a    prioritized default logic that is based on lexicographic comparison    the analysis locates the propositional variants of these logics on the    second and third levels of the polynomial hierarchy and identifies    the boundary between tractable and intractable inference for    restricted classes of prioritized default theories







i consider the problem of learning an optimal path graphical    model from data and show the problem to be nphard for the maximum    likelihood and minimum description length approaches and a bayesian    approach this hardness result holds despite the fact that the problem    is a restriction of the polynomially solvable problem of finding the    optimal tree graphical model











a  greenwald s  lee and v  naroditskiy 2009 roxybot06 stochastic prediction and optimization in tac travel volume 36 pages 513546



the paper addresses the problem of computing goal orderings    which is one of the longstanding issues in ai planning  it makes two    new contributions  first it formally defines and discusses two    different goal orderings which are called the reasonable and the    forced ordering both orderings are defined for simple strips    operators as well as for more complex adl operators supporting    negation and conditional effects the complexity of these orderings is    investigated and their practical relevance is discussed secondly two    different methods to compute reasonable goal orderings are developed    one of them is based on planning graphs while the other investigates    the set of actions directly finally it is shown how the ordering    relations which have been derived for a given set of goals g can be    used to compute a socalled goal agenda that divides g into an ordered    set of subgoals any planner can then in principle use the goal    agenda to plan for increasing sets of subgoals  this can lead to an    exponential complexity reduction as the solution to a complex    planning problem is found by solving easier subproblems since only a    polynomial overhead is caused by the goal agenda computation a    potential exists to dramatically speed up planning algorithms as we    demonstrate in the empirical evaluation where we use this method in    the	ipp planner









l  cigler and b  faltings 2013 decentralized anticoordination through multiagent learning volume 47 pages 441473



to achieve an optimal outcome in many situations agents need to choose distinct actions from one another this is the case notably in many resource allocation problems where a single resource can only be used by one agent at a time how shall a designer of a multiagent system program its identical agents to behave each in a different way

we present a multiagent learning algorithm that converges in polynomial number of steps to a correlated equilibrium of a channel allocation game a variant of the resource allocation game we show that the agents learn to play for each coordination signal value a randomly chosen purestrategy nash equilibrium of the game therefore the outcome is an efficient correlated equilibrium this ce becomes more fair as the number of the available coordination signal values increases











piotr  faliszewski edith  hemaspaandra and lane  a hemaspaandra 2015 weighted electoral control volume 52 pages 507542





in this paper we apply computeraided theorem discovery technique to discover theorems about strongly equivalent logic programs under the answer set semantics our discovered theorems capture new classes of strongly equivalent logic programs that can lead to new program simplification rules that preserve strong equivalence specifically with the help of computers we discovered exact conditions that capture the strong equivalence between a rule and the empty set between two rules between two rules and one of the two rules between two rules and another rule and between three rules and two of the three rules







e  gabrilovich and s  markovitch 2009 wikipediabased semantic interpretation for natural language processing volume 34 pages 443498







pseudoboolean constraints are omnipresent in practical applications and thus a significant effort has been devoted to the development of good sat encoding techniques for them some of these encodings first construct a binary decision diagram bdd for the constraint and then encode the bdd into a propositional formula  these bddbased approaches have some important advantages such as not being dependent on the size of the coefficients or being able to share the same bdd for representing many constraints







jd  fernandez and f  vico 2013 ai methods in algorithmic composition a comprehensive survey volume 48 pages 513582



algorithmic composition is the partial or total automation of the process of music composition by using computers since the 1950s different computational techniques related to artificial intelligence have been used for algorithmic composition including grammatical representations probabilistic methods neural networks symbolic rulebased systems constraint programming and evolutionary algorithms this survey aims to be a comprehensive account of research on algorithmic composition presenting a thorough view of the field for researchers in artificial intelligence







j  clarke and m  lapata 2008 global inference for sentence compression an integer linear programming approach volume 31 pages 399429



sentence compression holds promise for many applications ranging from summarization to subtitle generation  our work views sentence compression as an optimization problem and uses integer linear programming ilp to infer globally optimal compressions in the presence of linguistically motivated constraints we show how previous formulations of sentence compression can be recast as ilps and extend these models with novel global constraints experimental results on written and spoken texts demonstrate improvements over stateoftheart models





masood  feyzbakhsh rankooh and gholamreza  ghassemsani 2015 itsat an efficient satbased temporal planner volume 53 pages 541632



planning as satisfiability is known as an efficient approach to deal with many types of planning problems however this approach has not been competitive with the statespace based methods in temporal planning this paper describes itsat as an efficient satbased satisfiability based temporal planner capable of temporally expressive planning the novelty of itsat lies in the way it handles temporal constraints of given problems without getting involved in the difficulties of introducing continuous variables into the corresponding satisfiability problems we also show how as in satbased classical planning carefully devised preprocessing and encoding schemata can considerably improve the efficiency of satbased temporal planning we present two preprocessing methods for mutex relation extraction and action compression we also show that the separation of causal and temporal reasoning enables us to employ compact encodings that are based on the concept of parallel execution semantics although such encodings have been shown to be quite effective in classical planning itsat is the first temporal planner utilizing this type of encoding our empirical results show that not only does itsat outperform the stateoftheart temporally expressive planners it is also competitive with the fast temporal planners that cannot handle required concurrency



the assessment of bidirectional heuristic search has been    incorrect since it was first published more than a quarter of a    century ago  for quite a long time this search strategy did not    achieve the expected results and there was a major misunderstanding    about the reasons behind it  although there is still widespread    belief that bidirectional heuristic search is afflicted by the problem    of search frontiers passing each other we demonstrate that this    conjecture is wrong  based on this finding we present both a new    generic approach to bidirectional heuristic search and a new approach    to dynamically improving heuristic values that is feasible in    bidirectional search only  these approaches are put into perspective    with both the traditional and more recently proposed approaches in    order to facilitate a better overall understanding  empirical results    of experiments with our new approaches show that bidirectional    heuristic search can be performed very efficiently and also with    limited memory  these results suggest that bidirectional heuristic    search appears to be better for solving certain difficult problems    than corresponding unidirectional search  this provides some evidence    for the usefulness of a search strategy that was long neglected  in    summary we show that bidirectional heuristic search is viable and    consequently propose that it be reconsidered







icaps 2011 honorable mention for best student paper



k   daniel a  nash s  koenig and a  felner 2010 theta anyangle path planning on grids volume 39 pages 533579



  grids with blocked and unblocked cells are often used to represent terrain in robotics and video games however paths formed by grid edges can be longer than true shortest paths in the terrain since their headings are artificially constrained we present two new correct and complete anyangle pathplanning algorithms that avoid this shortcoming  basic theta and anglepropagation theta are both variants of a that propagate information along grid edges without constraining paths to grid edges basic theta is simple to understand and implement fast and finds short paths however it is not guaranteed to find true shortest paths anglepropagation theta achieves a better worstcase complexity per vertex expansion than basic theta by propagating angle ranges when it expands vertices but is more complex not as fast and finds slightly longer paths we refer to basic theta and anglepropagation theta collectively as theta theta has unique properties which we analyze in detail we show experimentally that it finds shorter paths than both a with postsmoothed paths and field d the only other version of a we know of that propagates information along grid edges without constraining paths to grid edges with a runtime comparable to that of a on grids finally we extend theta to grids that contain unblocked cells with nonuniform traversal costs and introduce variants of theta which provide different tradeoffs between path length and runtime







g  greco and f  scarcello 2014 mechanisms for fair allocation problems nopunishment payment rules in verifiable settings volume 49 pages 403449



mechanism design is considered in the context of fair allocations of indivisible goods with monetary compensation by focusing on problems where agents declarations on allocated goods can be verified before payments are performed a setting is considered where verification might be subject to errors so that payments have to be awarded under the presumption of innocence as incorrect declared values do not necessarily mean manipulation attempts by the agents within this setting a mechanism is designed that is shown to be truthful efficient and budgetbalanced moreover agents utilities are fairly determined by the shapley value of suitable coalitional games and enjoy highly desirable properties such as equal treatment of equals envyfreeness and a stronger one called individualoptimality in particular the latter property guarantees that for every agent herhis utility is the maximum possible one over any alternative optimal allocation 

the computational complexity of the proposed mechanism is also studied it turns out that it is pcomplete so that to deal with applications with many agents involved two polynomialtime randomized variants are also proposed one that is still truthful and efficient and which is approximately budgetbalanced with high probability and another one that is truthful in expectation while still budgetbalanced and efficient



talplanner is a forwardchaining planner that relies on domain knowledge in the shape of temporal logic formulas in order to prune irrelevant parts of the search space talplanner recently participated in the third international planning competition which had a clear emphasis on increasing the complexity of the problem domains being used as benchmark tests and the expressivity required to represent these domains in a planning system  like many other planners talplanner had support for some but not all aspects of this increase in expressivity and a number of changes to the planner were required after a short introduction to talplanner this article describes some of the changes that were made before and during the competition  we also describe the process of introducing suitable domain knowledge for several of the competition domains







when writing a constraint program we have to choose which variables  should be the decision variables and how to represent the constraints  on these variables in many cases there is considerable choice for  the decision variables  consider for example permutation problems in which we have as many values as variables and each variable takes  an unique value in such problems we can choose between a primal and a dual viewpoint in the dual viewpoint each dual variable  represents one of the primal values whilst each dual value represents one of the primal variables alternatively by means of channelling  constraints to link the primal and dual variables we can have a  combined model with both sets of variables in this paper we perform  an extensive theoretical and empirical study of such primal dual and  combined models for two classes of problems permutation problems and  injection problems our results show that it often be advantageous to  use multiple viewpoints and to have constraints which channel between them to maintain consistency they also illustrate a general  methodology for comparing different constraint models







this paper proposes a method to construct an adaptive agent that is universal with respect to a given class of experts where each expert is designed specifically for a particular environment this adaptive control problem is formalized as the problem of minimizing the relative entropy of the adaptive agent from the expert that is most suitable for the unknown environment if the agent is a passive observer then the optimal solution is the wellknown bayesian predictor however if the agent is active then its past actions need to be treated as causal interventions on the io stream rather than normal probability conditions here it is shown that the solution to this new variational problem is given by a stochastic controller called the bayesian control rule which implements adaptive behavior as a mixture of experts furthermore it is shown that under mild assumptions the bayesian control rule converges to the control law of the most suitable expert







in this article we work towards the goal of developing agents that can learn to act in complex worlds  we develop a probabilistic relational planning rule representation that compactly models noisy nondeterministic action effects and show how such rules can be effectively learned  through experiments in simple planning domains and a 3d simulated blocks world with realistic physics we demonstrate that this learning algorithm allows agents to effectively model world dynamics







f  a oliehoek m  t j spaan c  amato and s  whiteson 2013 incremental clustering and expansion for faster optimal planning in decpomdps volume 46 pages 449509



this article presents the stateoftheart in optimal solution methods for decentralized partially observable markov decision processes decpomdps which are general models for collaborative multiagent planning under uncertainty building off the generalized multiagent a gmaa algorithm which reduces the problem to a tree of oneshot collaborative bayesian games cbgs we describe several advances that greatly expand the range of decpomdps that can be solved optimally  first we introduce lossless incremental clustering of the cbgs solved by gmaa which achieves exponential speedups without sacrificing optimality  second we introduce incremental expansion of nodes in the gmaa search tree which avoids the need to expand all children the number of which is in the worst case doubly exponential in the nodes depth  this is particularly beneficial when little clustering is possible  in addition we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger decpomdps  we provide theoretical guarantees that when a suitable heuristic is used both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent  finally we present extensive empirical results demonstrating that gmaaice an algorithm that synthesizes these advances can optimally solve decpomdps of unprecedented size





we study the complexity of influencing elections through bribery how computationally complex is it for an external actor to determine whether by paying certain voters to change their preferences a specified candidate can be made the elections winner  we study this problem for election systems as varied as scoring protocols and dodgson voting and in a variety of settings regarding homogeneousvsnonhomogeneous electorate bribability boundedsizevsarbitrarysized candidate sets weightedvsunweighted voters and succinctvsnonsuccinct input specification  we obtain both polynomialtime bribery algorithms and proofs of the intractability of bribery and indeed our results show that the complexity of bribery is extremely sensitive to the setting  for example we find settings in which bribery is npcomplete but manipulation by voters is in p and we find settings in which bribing weighted voters is npcomplete but bribing voters with individual bribe thresholds is in p  for the broad class of elections including plurality borda kapproval and veto known as scoring protocols we prove a dichotomy result for bribery of weighted voters we find a simpletoevaluate condition that classifies every case as either npcomplete or in p









joseph  y halpern 2015 weighted regretbased likelihood a new approach to describing uncertainty volume 54 pages 471492



recently halpern and leung suggested representing uncertainty by a set of weighted  probability measures and suggested a way of making decisions based on this representation of uncertainty maximizing weighted regret  their paper does not answer an apparently simpler question what it means according to this representation of uncertainty for an event e to be more likely than an event e    in this paper a notion of comparative likelihood when uncertainty is represented by a set of weighted probability measures is defined  it generalizes the ordering defined by probability and by lower probability in a natural way a generalization of upper probability can also be defined  a complete axiomatic characterization of this notion of regretbased likelihood is given 









this paper analyzes the correctness of the subsumption algorithm used in classic a description logicbased knowledge representation system that is being used in practical applications  in order to deal efficiently with individuals in classic descriptions the developers have had to use an algorithm that is incomplete with respect to the standard modeltheoretic semantics for description logics we provide a variant semantics for descriptions with respect to which the current implementation is complete and which can be independently motivated  the soundness and completeness of the polynomialtime subsumption algorithm is established using description graphs which are an abstracted version of the implementation structures used in classic and are of independent interest













this paper introduces icet a new algorithm for   costsensitive classification icet uses a genetic algorithm to evolve   a population of biases for a decision tree induction algorithm the   fitness function of the genetic algorithm is the average cost of   classification when using the decision tree including both the costs   of tests features measurements and the costs of classification   errors icet is compared here with three other algorithms for   costsensitive classification  eg2 csid3 and idx  and also with   c45 which classifies without regard to cost the five algorithms are   evaluated empirically on five realworld medical datasets three sets   of experiments are performed  the first set examines the baseline   performance of the five algorithms on the five datasets and   establishes that icet performs significantly better than its   competitors the second set tests the robustness of icet under a   variety of conditions and shows that icet maintains its advantage the   third set looks at icets search in bias space and discovers a way to   improve the search







this paper presents a new classifier combination    technique based on the dempstershafer theory of evidence the    dempstershafer theory of evidence is a powerful method for    combining measures of evidence from different classifiers however    since each of the available methods that estimates the evidence of    classifiers has its own limitations we propose here a new    implementation which adapts to training data so that the overall    mean square error is minimized the proposed technique is shown to    outperform most available classifier combination methods when    tested on three different classification problems

much work in ai deals with the selection of proper actions    in a given known or unknown environment however the way to select    a proper action when facing other agents is quite unclear most work    in ai adopts classical gametheoretic equilibrium analysis to predict    agent behavior in such settings this approach however does not    provide us with any guarantee for the agent in this paper we    introduce competitive safety analysis this approach bridges the gap    between the desired normative ai approach where a strategy should be    selected in order to guarantee a desired payoff and equilibrium    analysis  we show that a safety level strategy is able to guarantee    the value obtained in a nash equilibrium in several classical    computer science settings  then we discuss the concept of    competitive safety strategies and illustrate its use in a    decentralized load balancing setting typical to network problems in    particular we show that when we have many agents it is possible to    guarantee an expected payoff which is a factor of 89 of the payoff    obtained in a nash equilibrium  our discussion of competitive safety    analysis for decentralized load balancing is further developed to deal    with many communication links and arbitrary speeds  finally we    discuss the extension of the above concepts to bayesian games and    illustrate their use in a basic auctions setup







x  tannier and p  muller 2011 evaluating temporal graphs built from texts via transitive reduction volume 40 pages 375413



temporal information has been the focus of recent attention in information extraction leading to some standardization effort in particular for the task of relating events in a text this task raises the problem of comparing two annotations of a given text because relations between events in a story are intrinsically interdependent and cannot be evaluated separately  a proper evaluation measure is also crucial in the context of a machine learning approach to the problem  finding a common comparison referent at the text level is not obvious and we argue here in favor of a shift from eventbased measures to measures on a unique textual object a minimal underlying temporal graph or more formally the transitive reduction of the graph of relations between event boundaries we support it by an  investigation of its properties on synthetic data and on a wellknow  temporal corpus





this article describes an application of three wellknown    statistical methods in the field of gametree search using a large    number of classified othello positions feature weights for evaluation    functions with a gamephaseindependent meaning are estimated by means    of logistic regression fishers linear discriminant and the    quadratic discriminant function for normally distributed    features thereafter the playing strengths are compared by means of    tournaments between the resulting versions of a worldclass othello    program in this application logistic regression  which is used here    for the first time in the context of game playing  leads to better    results than the other approaches











s  cai c  luo and k  su 2014 scoring functions based on second level score for ksat with long clauses volume 51 pages 413441



structured prediction is the problem of learning a function that maps structured inputs to structured outputs prototypical examples of structured prediction include partofspeech tagging and semantic segmentation of images inspired by the recent successes of searchbased structured prediction we introduce a new framework for structured prediction called hcsearch given a structured input the framework uses a search procedure guided by a learned heuristic h to uncover high quality candidate outputs and then employs a separate learned cost function c to select a final prediction among those outputs the overall loss of this prediction architecture decomposes into the loss due to h not leading to high quality outputs and the loss due to c not selecting the best among the generated outputs guided by this decomposition we minimize the overall loss in a greedy stagewise manner by first training h to quickly uncover high quality outputs via imitation learning and then training c to correctly rank the outputs generated via h according to their true losses importantly this training procedure is sensitive to the particular loss function of interest and the timebound allowed for predictions experiments on several benchmark domains show that our approach significantly outperforms several stateoftheart methods









f  stulp and m  beetz 2008 refining the execution of abstract actions with learned action models volume 32 pages 487523



robots reason about abstract actions such as go to position l in order to decide what to do or to generate plans for their intended course of action the use of abstract actions enables robots to employ small action libraries which reduces the search space for decision making when executing the actions however the robot must tailor the abstract actions to the specific task and situation context at hand

in this article we propose a novel robot action execution system that learns success and performance models for possible specializations of abstract actions at execution time the robot uses these models to optimize the execution of abstract actions to the respective task contexts the robot can so use abstract actions for efficient reasoning without compromising the performance of action execution we show the impact of our action execution model in three robotic domains and on two kinds of action execution problems 1 the instantiation of free action parameters to optimize the expected performance of action sequences 2 the automatic introduction of additional subgoals to make action sequences more reliable



a fundamental assumption made by classical ai planners is    that there is no uncertainty in the world the planner has full    knowledge of the conditions under which the plan will be executed and    the outcome of every action is fully predictable  these planners    cannot therefore construct contingency plans ie plans in which    different actions are performed in different circumstances  in this    paper we discuss some issues that arise in the representation and    construction of contingency plans and describe cassandra a    partialorder contingency planner  cassandra uses explicit    decisionsteps that enable the agent executing the plan to decide    which plan branch to follow  the decisionsteps in a plan result in    subgoals to acquire knowledge which are planned for in the same way    as any other subgoals  cassandra thus distinguishes the process of    gathering information from the process of making decisions  the    explicit representation of decisions in cassandra allows a coherent    approach to the problems of contingent planning and provides a solid    base for extensions such as the use of different decisionmaking    procedures









x  lu h  m schwartz and s  n givigi 2011 policy invariance under reward transformations for generalsum stochastic games volume 41 pages 397406



we extend the potentialbased shaping method from markov decision processes to multiplayer generalsum stochastic games we prove that the nash equilibria in a stochastic game remains unchanged after potentialbased shaping is applied to the environment the property of policy invariance provides a possible way of speeding convergence when learning to play a stochastic game











icaps 2011 best student paper



today mobile robots are expected to carry out increasingly complex tasks in multifarious realworld environments often the tasks require a certain semantic understanding of the workspace consider for example spoken instructions from a human collaborator referring to objects of interest the robot must be able to accurately detect these objects to correctly understand the instructions however existing object detection while competent is not perfect in particular the performance of detection algorithms is commonly sensitive to the position of the sensor relative to the objects in the scene

this paper presents an online planning algorithm which learns an explicit model of the spatial dependence of object detection and generates plans which maximize the expected performance of the detection and by extension the overall plan performance crucially the learned sensor model incorporates spatial correlations between measurements capturing the fact that successive measurements taken at the same or nearby locations are not independent we show how this sensor model can be incorporated into an efficient forward search algorithm in the information space of detected objects allowing the robot to generate motion plans efficiently  we investigate the performance of our approach by addressing the tasks of door and text detection in indoor environments and demonstrate significant improvement in detection performance during task execution over alternative methods in simulated and real robot experiments







