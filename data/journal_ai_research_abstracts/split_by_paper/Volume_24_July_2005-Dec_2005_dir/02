p   j gmytrasiewicz and  p  doshi 2005 a framework for sequential planning in multiagent settings volume 24 pages 4979

this paper extends the framework of partially observable markov decision processes pomdps to multiagent settings by incorporating the notion of agent models into the state space  agents maintain beliefs over physical states of the environment and over models of other agents and they use bayesian updates to maintain their beliefs over time the solutions map belief states to actions models of other agents may include their belief states and are related to agent types considered in games of incomplete information  we express the agents autonomy by postulating that their models are not directly manipulable or observable by other agents  we show that important properties of pomdps such as convergence of value iteration the rate of convergence and piecewise linearity and convexity of the value functions carry over to our framework  our approach complements a more traditional approach to interactive settings which uses nash equilibria as a solution paradigm  we seek to avoid some of the drawbacks of equilibria which may be nonunique and do not capture offequilibrium behaviors  we do so at the cost of having to represent process and continuously revise models of other agents since the agents beliefs may be arbitrarily nested the optimal solutions to decision making problems are only asymptotically computable  however approximate belief updates and approximately optimal plans are computable we illustrate our framework using a simple application domain and we show examples of belief updates and value functions

