simone  parisi matteo  pirotta and marcello  restelli 2016 multiobjective reinforcement learning through continuous pareto manifold approximation volume 57 pages 187227

many realworld control applications from economics to robotics are characterized by the presence of multiple conflicting objectives in these problems the standard concept of optimality is replaced by paretooptimality and the goal is to find the pareto frontier a set of solutions representing different compromises among the objectives despite recent advances in multiobjective optimization achieving an accurate representation of the pareto frontier is still an important challenge  in this paper we propose a reinforcement learning policy gradient approach to learn a continuous approximation of the pareto   frontier in multiobjective markov decision problems momdps differently from previous policy gradient algorithms where n optimization routines are executed to have n solutions our approach performs a single gradient ascent run generating at each step an improved continuous approximation of the pareto frontier the idea is   to optimize the parameters of a function defining a manifold in the policy parameters space so that the corresponding image in the objectives space gets as close as possible to the true pareto frontier besides deriving how to compute and estimate such gradient we will also discuss the nontrivial issue of defining a metric to assess the quality of the candidate pareto frontiers finally the properties of the proposed approach are empirically evaluated on two problems a linearquadratic gaussian regulator and a water reservoir control task

