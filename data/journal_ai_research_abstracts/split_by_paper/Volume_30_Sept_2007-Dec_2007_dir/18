i  szita and a  lorincz 2007 learning to play using lowcomplexity rulebased policies illustrations through ms pacman volume 30 pages 659684

in this article we propose a method that can deal with certain combinatorial reinforcement learning tasks we demonstrate the approach in the popular ms pacman game we define a set of highlevel observation and action modules from which rulebased policies are constructed automatically in these policies actions are temporally extended and may work concurrently the policy of the agent is encoded by a compact decision list the components of the list are selected from a large pool of rules  which can be either handcrafted or generated automatically a suitable selection of rules is learnt  by the crossentropy method a recent global optimization algorithm that fits our framework smoothly  crossentropyoptimized policies perform better than our handcrafted policy and reach the score of average human players we argue that learning is successful mainly because i policies may apply concurrent actions and thus the policy space is sufficiently rich ii the search is biased towards lowcomplexity policies and therefore  solutions with a compact description can be found quickly if they exist

