h  daume iii and d  marcu 2006 domain adaptation for statistical classifiers volume 26 pages 101126

the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution  unfortunately in many applications the indomain test data is drawn from a distribution that is related but not identical to the outofdomain distribution of the training data we consider the common case in which labeled outofdomain data is plentiful but labeled indomain data is scarce  we introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts  we present efficient inference algorithms for this special case based on the technique of conditional expectation maximization  our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain 

