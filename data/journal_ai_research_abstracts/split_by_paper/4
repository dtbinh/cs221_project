

recently planning based on answer set programming has been proposed as an approach towards realizing declarative planning systems in this paper we present the language kc which extends the declarative planning language k by action costs kc provides the notion of admissible and optimal plans which are plans whose overall action costs are within a given limit resp minimum over all plans ie cheapest plans as we demonstrate this novel language allows for expressing some nontrivial planning tasks in a declarative way furthermore it can be utilized for representing planning problems under other optimality criteria such as computing shortest plans with the least number of steps and refinement combinations of cheapest and fastest plans we study complexity aspects of the language kc and provide a transformation to logic programs such that planning problems are solved via answer set programming furthermore we report experimental results on selected problems our experience is encouraging that answer set planning may be a valuable approach to expressive planning systems in which intricate planning problems can be naturally specified and solved







most modern formalisms used in databases and artificial    intelligence for describing an application domain are based on the    notions of class or concept and relationship among classes  one    interesting feature of such formalisms is the possibility of defining    a class ie providing a set of properties that precisely    characterize the instances of the class  many recent articles point    out that there are several ways of assigning a meaning to a class    definition containing some sort of recursion  in this paper we argue    that instead of choosing a single style of semantics we achieve    better results by adopting a formalism that allows for different    semantics to coexist we demonstrate the feasibility of our argument    by presenting a knowledge representation formalism the description    logic mualcq with the above characteristics  in addition to the    constructs for conjunction disjunction negation quantifiers and    qualified number restrictions mualcq includes special fixpoint    constructs to express suitably interpreted recursive definitions    these constructs enable the usual framebased descriptions to be    combined with definitions of recursive data structures such as    directed acyclic graphs lists streams etc  we establish several    properties of mualcq including the decidability and the computational    complexity of reasoning by formulating a correspondence with a    particular modal logic of programs called the modal mucalculus







m  pistore and m  y vardi 2007 the planning spectrum  one two three infinity volume 30 pages 101132



linear temporal logic ltl is widely used for defining conditions on the execution paths of dynamic systems  in the case of dynamic systems that allow for nondeterministic evolutions one has to specify along with an ltl formula f which are the paths that are required to satisfy the formula  two extreme cases are the universal interpretation af which requires that the formula be satisfied for all execution paths and the existential interpretation ef which requires that the formula be satisfied for some execution path





we present an incentivecompatible polynomialtime approximation scheme for multiunit auctions with general kminded player valuations the mechanism fully optimizes over an appropriately chosen subrange of possible allocations and then uses vcg payments over this subrange we show that obtaining a fully polynomialtime incentivecompatible approximation scheme at least using vcg payments is nphard for the case of valuations given by black boxes we give a polynomialtime incentivecompatible 2approximation mechanism and show that no better is possible at least using vcg payments







in this paper we confront the problem of applying reinforcement learning to agents that perceive the environment through many sensors and that can perform  parallel actions using many actuators as is the case in complex autonomous robots we argue that reinforcement learning can only be successfully applied to this case  if strong assumptions are made on the characteristics of the environment in which  the learning is performed so that the relevant sensor readings and motor commands can be  readily identified the introduction of such assumptions leads to stronglybiased  learning systems that can eventually lose the generality of traditional  reinforcementlearning algorithms  in this line we observe that in realistic situations the reward received by the robot  depends only on a reduced subset of all the executed actions and that only a reduced  subset of the sensor inputs possibly different in each situation and for each action  are relevant to predict the reward we formalize this property in the so called  categorizability assumption and we present an algorithm that takes advantage of  the categorizability of the environment allowing a decrease in the learning time with  respect to existing reinforcementlearning algorithms results of the application of the  algorithm to a couple of simulated realisticrobotic problems landmarkbased navigation  and the sixlegged robot gait generation are reported to validate our approach and to  compare it to existing flat and generalizationbased reinforcementlearning approaches







partialorder plans pops are attractive because of their leastcommitment nature which provides enhanced plan flexibility at execution time relative to sequential plans current research on automated plan generation focuses on producing sequential plans despite the appeal of pops in this paper we examine pop generation by relaxing or modifying the action orderings of a sequential plan to optimize for plan criteria that promote flexibility our approach relies on a novel partial weighted maxsat encoding of a sequential plan that supports the minimization of deordering or reordering of actions using a similar technique we further demonstrate how to remove redundant actions from the plan and how to combine this criterion with the objective of maximizing a pops flexibility our partial weighted maxsat encoding allows us to compute a pop from a sequential plan effectively we compare the efficiency of our approach to previous methods for pop generation via sequentialplan relaxation our results show that while an existing heuristic approach consistently produces the optimal deordering of a sequential plan our approach has greater flexibility when we consider reordering the actions in the plan while also providing a guarantee of optimality we also investigate and confirm the accuracy of the standard flex metric typically used to predict the true flexibility of a pop as measured by the number of linearizations it represents







we propose a model for errors in sung queries a variant of the hidden markov model hmm this is a solution to the problem of identifying the degree of similarity between a typically errorladen sung query and a potential target in a database of musical works an important problem in the field of music information retrieval similarity metrics are a critical component of querybyhumming qbh applications which search audio and multimedia databases for strong matches to oral queries our model comprehensively expresses the types of m error or variation between target and query cumulative and noncumulative local errors transposition tempo and tempo changes insertions deletions and modulation the model is not only expressive but automatically trainable or able to learn and generalize from query examples we present results of simulations designed to assess the discriminatory potential of the model and tests with real sung queries to demonstrate relevance to realworld applications







j  y chai z  prasov and s  qu 2006 cognitive principles in robust multimodal interpretation volume 27 pages 5583



multimodal conversational interfaces provide a natural means for users to communicate with computer systems through multiple modalities such as speech and gesture to build effective multimodal interfaces automated interpretation of user multimodal inputs is important inspired by the previous investigation on cognitive status in multimodal human machine interaction we have developed a greedy algorithm for interpreting user referring expressions ie multimodal reference resolution this algorithm incorporates the cognitive principles of conversational implicature and givenness hierarchy and applies constraints from various sources eg temporal semantic and contextual to resolve references our empirical results have shown the advantage of this algorithm in efficiently resolving a variety of user references because of its simplicity and generality this approach has the potential to improve the robustness of multimodal input interpretation





in this paper we consider markov decision processes mdps with error states  error states are those states entering which is undesirable or dangerous we define the risk with respect to a policy as the probability of entering such a state when the policy is pursued we consider the problem of finding good policies whose risk is smaller than some userspecified threshold and formalize it as a constrained mdp with two criteria the first criterion corresponds to the value function originally given we will show that the risk can be formulated as a second criterion function based on a cumulative return whose definition is independent of the original value function  we present a model free heuristic reinforcement learning algorithm that aims at finding good deterministic policies  it is based on weighting the original value function and the risk the weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function the algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column this control task was originally formulated as an optimal control problem with chance constraints and it was solved under certain assumptions on the model to obtain an optimal solution the power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed







the size and complexity of software and hardware systems    have significantly increased in the past years  as a result it is    harder to guarantee their correct behavior one of the most successful    methods for automated verification of finitestate systems is model    checking most of the current modelchecking systems use binary    decision diagrams bdds for the representation of the tested model    and in the verification process of its properties generally bdds    allow a canonical compact representation of a boolean function given    an order of its variables the more compact the bdd is the better    performance one gets from the verifier however finding an optimal    order for a bdd is an npcomplete problem therefore several    heuristic methods based on expert knowledge have been developed for    variable ordering       we propose an alternative approach in which the variable ordering     algorithm gains ordering experience from training models and     uses the learned knowledge for finding good orders our     methodology is based on offline learning of pair precedence     classifiers from training models that is learning which variable     pair permutation is more likely to lead to a good order for each     training model a number of training sequences are evaluated every     training model variable pair permutation is then tagged based on     its performance on the evaluated orders the tagged permutations     are then passed through a feature extractor and are given as     examples to a classifier creation algorithm given a model for     which an order is requested the ordering algorithm consults each     precedence classifier and constructs a pair precedence table     which is used to create the order      our algorithm was integrated with smv which is one of the most     widely used verification systems preliminary empirical evaluation of our     methodology using real benchmark models shows performance that     is better than random ordering and is competitive with existing     algorithms that use expert knowledge we believe that in     subdomains of models alu caches etc our system will prove     even more valuable this is because it features the ability to     learn subdomain knowledge something that no other ordering     algorithm does







coxs wellknown theorem justifying the use of probability is shown not to hold in finite domains the counterexample also suggests that coxs assumptions are insufficient to prove the result even in infinite domains the same counterexample is used to disprove a result of fine on comparative conditional probability







anyangle pathfinding is a fundamental problem in robotics and computer games the goal is to find a shortest path between a pair of points on a grid map such that the path is not artificially constrained to the points of the grid prior research has focused on approximate online solutions a number of exact methods exist but they all require superlinear space and preprocessing time  in this study we describe anya a new and optimal anyangle pathfinding algorithm where other works find approximate anyangle paths by searching over individual points from the grid anya finds optimal paths by searching over sets of states represented as intervals each interval is identified onthefly from each interval anya selects a single representative point that it uses to compute an admissible cost estimate for the entire set  anya always returns an optimal path if one exists  moreover it does so without any offline preprocessing or the introduction of additional memory overheads  in a range of empirical comparisons we show that anya is competitive with several recent suboptimal online and preprocessing based techniques and is up to an order of magnitude faster than the most common benchmark algorithm a gridbased implementation of a







multiagent path planning is a challenging problem with numerous reallife applications  running a centralized search such as a in the combined state space of all units is complete and costoptimal but scales poorly as the state space size is exponential in the number of mobile units  traditional decentralized approaches such as far and  whca are faster and more scalable being based on problem decomposition  however such methods are incomplete and provide no guarantees with respect to the running time or the solution quality  they are not necessarily able to tell in a reasonable time whether they would succeed in finding a solution to a given instance 

experiments were run on realistic game grid maps  mapp solved 9986 of all mobile units which is 1822 better than the percentage of far and whca  mapp marked 9882 of all units as provably solvable during the first stage of plan computation  parts of mapps computation can be reused across instances on the same map  speedwise mapp is competitive or significantly faster than whca depending on whether mapp performs all computations from scratch  when data that mapp can reuse are preprocessed offline and readily available mapp is slower than the very fast far algorithm by a factor of 218 on average  mapps solutions are on average 20 longer than fars solutions and 731 longer than whcas solutions 







this article focuses on word sense disambiguation wsd which is a natural language processing task that is thought to be important for many language technology applications such as information retrieval information extraction or machine translation one of the main issues preventing the deployment of wsd technology is the lack of training examples for machine learning systems also known as the knowledge acquisition bottleneck a method which has been shown to work for small samples of words is the automatic acquisition of examples we have previously shown that one of the most promising example acquisition methods scales up and produces a freely available database of 150 million examples from web snippets for all polysemous nouns in wordnet this paper focuses on the issues that arise when using those examples all alone or in addition to manually tagged examples to train a supervised wsd system for all nouns the extensive evaluation on both lexicalsample and allwords senseval benchmarks shows that we are able to improve over commonly used baselines and to achieve toprank performance the good use of the prior distributions from the senses proved to be a crucial factor







it is common to view programs as a combination of logic and    control the logic part defines what the program must do the control    part  how to do it  the logic programming paradigm was developed    with the intention of separating the logic from the control    recently extensive research has been conducted on automatic    generation of control for logic programs  only a few of these works    considered the issue of automatic generation of control for improving    the efficiency of logic programs  in this paper we present a novel    algorithm for automatic finding of lowestcost subgoal orderings  the    algorithm works using the divideandconquer strategy  the given set    of subgoals is partitioned into smaller sets based on cooccurrence    of free variables the subsets are ordered recursively and merged    yielding a provably optimal order  we experimentally demonstrate the    utility of the algorithm by testing it in several domains and discuss    the possibilities of its cooperation with other existing methods







the chief aim of this paper is to propose meanfield    approximations for a broad class of belief networks of which sigmoid    and noisyor networks can be seen as special cases  the     approximations are based on a powerful meanfield theory suggested by    plefka  we show that saul jaakkola and jordan s approach is the    first order approximation in plefkas approach via a variational    derivation  the application of plefkas theory to belief networks is    not computationally tractable  to tackle this problem we propose new    approximations based on taylor series  small scale experiments show    that the proposed schemes are attractive







we present a novel bayesian topic model for learning discourselevel document structure our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics we propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents we show that this space of orderings can be effectively represented using a distribution over permutations called the generalized mallows model we apply our method to three complementary discourselevel tasks crossdocument alignment document segmentation and information ordering our experiments show that incorporating our permutationbased model in these applications yields substantial improvements in performance over previously proposed methods







pearl and dechter 1996 claimed that the dseparation    criterion for conditional independence in acyclic causal networks also    applies to networks of discrete variables that have feedback cycles    provided that the variables of the system are uniquely determined by    the random disturbances  i show by example that this is not true in    general  some condition stronger than uniqueness is needed such as    the existence of a causal dynamics guaranteed to lead to the unique    solution







dungs abstract argumentation theory can be seen as a general framework for nonmonotonic reasoning an important question is then what is the class of logics that can be subsumed as instantiations of this theory the goal of this paper is to identify and study the large class of logicbased instantiations of dungs theory which correspond to the maxiconsistent operator ie to the function which returns maximal consistent subsets of an inconsistent knowledge base in other words we study the class of instantiations where very extension of the argumentation system corresponds to exactly one maximal consistent subset of the knowledge base we show that an attack relation belonging to this class must be conflictdependent must not be valid must not be conflictcomplete must not be symmetric etc then we show that some attack relations serve as lower or upper bounds of the class eg if an attack relation contains canonical undercut then it is not a member of this class by using our results we show for all existing attack relations whether or not they belong to this class we also define new attack relations which are members of this class finally we interpret our results and discuss more general questions like what is the added value of argumentation in such a setting we believe that this work is a first step towards achieving our longterm goal which is to better understand the role of argumentation and particularly the expressivity of logicbased instantiations of dungstyle argumentation frameworks







linguistic borrowing is the phenomenon of transferring linguistic constructions lexical phonological morphological and syntactic from a donor language to a recipient language as a result of contacts between communities speaking different languages borrowed words are found in all languages andin contrast to cognate relationshipsborrowing relationships may exist across unrelated languages for example about 40 of swahilis vocabulary is borrowed from the unrelated language arabic in this work we develop a model of morphophonological transformations across languages its features are based on universal constraints from optimality theory ot and we show that compared to several standardbut linguistically more na239vebaselines our otinspired model obtains good performance at predicting donor forms from borrowed forms with only a few dozen training examples making this a costeffective strategy for sharing lexical information across languages we demonstrate applications of the lexical borrowing model in machine translation using resourcerich donor language to obtain translations of outofvocabulary loanwords in a lower resource language our framework obtains substantial improvements up to 16 bleu over standard baselines







pointwisecompetitive classifier from class f is required to classify identically to the best classifier in hindsight from f for noisy agnostic settings we present a strategy for learning pointwisecompetitive classifiers from a finite training sample provided that the classifier can abstain from prediction at a certain region of its choice for some interesting hypothesis classes and families of distributions the measure of this rejected region is shown to be diminishing at a fast rate with high probability exact implementation of the proposed learning strategy is dependent on an erm oracle that can be hard to compute in the agnostic case we thus consider a heuristic approximation procedure that is based on svms and show empirically that this algorithm consistently outperforms a traditional rejection mechanism based on distance from decision boundary







to test incomplete search algorithms for constraint satisfaction problems such as 3sat we need a source of hard but satisfiable benchmark instances  a simple way to do this is to choose a random truth assignment a and then choose clauses randomly from among those satisfied by a however this method tends to produce easy problems since the majority of literals point toward the hidden assignment a last year achlioptas jia and moore proposed a problem generator that cancels this effect by hiding both a and its complement while the resulting formulas appear to be just as hard for dpll algorithms as random 3sat formulas with no hidden assignment they can be solved by walksat in only polynomial time  

here we propose a new method to cancel the attraction to a by choosing a clause with t  0 literals satisfied by a with probability proportional to qt for some q  1  by varying q we can generate formulas whose variables have no bias ie which are equally likely to be true or false we can even cause the formula to deceptively point away from a  we present theoretical and experimental results suggesting that these formulas are exponentially hard both for dpll algorithms and for incomplete algorithms such as walksat







a  gershman a  meisels  and r  zivan 2009 asynchronous forward bounding for distributed cops volume 34 pages 6188



a new search algorithm for solving distributed constraint optimization problems discops is presented agents assign variables sequentially and compute bounds on partial assignments asynchronously the asynchronous bounds computation is based on the propagation of partial assignments the asynchronous forwardbounding algorithm afb is a distributed optimization search algorithm that keeps one consistent partial assignment at all times the algorithm is described in detail and its correctness proven experimental evaluation shows that afb outperforms synchronous branch and bound by many orders of magnitude and produces a phase transition as the tightness of the problem increases this is an analogous effect to the phase transition that has been observed when local consistency maintenance is applied to maxcsps the afb algorithm is further enhanced by the addition of a backjumping mechanism resulting in the afbbj algorithm  distributed backjumping is based on accumulated information on bounds of all values and on processing concurrently a queue of candidate goals for the next move back the afbbj algorithm is compared experimentally to other discop algorithms adopt dpop optapo and is shown to be a very efficient algorithm for discops







h  vlaeminck j  vennekens m  denecker and m  bruynooghe 2012 an approximative inference method for solving 87078704so satisfiability problems volume 45 pages 79124



this paper considers the fragment 87078704so of secondorder logic many interesting problems such as conformant planning can be naturally expressed as finite domain satisfiability problems of this logic such satisfiability problems are computationally hard 931p2 and many of these problems are often solved approximately in this paper we develop a general approximative method ie a sound but incomplete method for solving 87078704so satisfiability problems we use a syntactic representation of a constraint propagation method for firstorder logic to transform such an 87078704so satisfiability problem to an 8707soid satisfiability problem secondorder logic extended with inductive definitions the finite domain satisfiability problem for the latter language is in np and can be handled by several existing solvers inductive definitions are a powerful knowledge representation tool and this moti vates us to also approximate 87078704soid problems in order to do this we first show how to perform propagation on such inductive definitions next we use this to approximate 87078704soid satisfiability problems all this provides a general theoretical framework for a number of approximative methods in the literature moreover we also show how we can use this framework for solving practical useful problems such as conformant planning in an effective way



sequential decisionmaking problems with multiple objectives arise naturally in practice and pose unique challenges for research in decisiontheoretic planning and learning which has largely focused on singleobjective settings this article surveys algorithms designed for sequential decisionmaking problems with multiple objectives though there is a growing body of literature on this subject little of it makes explicit under what circumstances special methods are needed to solve multiobjective problems therefore we identify three distinct scenarios in which converting such a problem to a singleobjective one is impossible infeasible or undesirable furthermore we propose a taxonomy that classifies multiobjective methods according to the applicable scenario the nature of the scalarization function which projects multiobjective values to scalar ones and the type of policies considered we show how these factors determine the nature of an optimal solution which can be a single policy a convex hull or a pareto front using this taxonomy we survey the literature on multiobjective methods for planning and learning finally we discuss key applications of such methods and outline opportunities for future work







we introduce cui networks a compact graphical representation of utility functions over multiple attributes cui networks model multiattribute utility functions using the wellstudied and widely applicable utility independence concept we show how conditional utility independence leads to an effective functional decomposition that can be exhibited graphically and how local compact data at the graph nodes can be used to calculate joint utility we discuss aspects of elicitation network construction and optimization and contrast our new representation with previous graphical preference modeling







the article introduces a ceteris paribus modal logic called cp interpreted on the equivalence classes induced by finite sets of propositional atoms this logic is studied and then used to embed three logics of strategic interaction namely atemporal stit the coalition logic of propositional control cl8722pc and the starless fragment of the dynamic logic of propositional assignments dl8722pa the embeddings highlight a common ceteris paribus structure underpinning the key operators of all these apparently very different logics and show we argue remarkable similarities behind some of the most influential formalisms for reasoning about strategic interaction







the easyhardeasy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning we test the generality of this explanation by examining one of its predictions if the number of solutions is held fixed by the choice of problems then increased pruning should lead to a monotonic decrease in search cost instead we find the easyhardeasy pattern in median search cost even when the number of solutions is held constant for some search methods this generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost in these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems rather than changing numbers of solutions







recent research has shown that surprisingly rich models of human activity can be learned from gps positional data however most effort to date has concentrated on modeling single individuals or statistical properties of groups of people moreover prior work focused solely on modeling actual successful executions and not failed or attempted executions of the activities of interest we in contrast take on the task of understanding human interactions attempted interactions and intentions from noisy sensor data in a fully relational multiagent setting we use a realworld game of capture the flag to illustrate our approach in a welldefined domain that involves many distinct cooperative and competitive joint activities we model the domain using markov logic a statisticalrelational language and learn a theory that jointly denoises the data and infers occurrences of highlevel activities such as a player capturing an enemy our unified model combines constraints imposed by the geometry of the game area the motion model of the players and by the rules and dynamics of the game in a probabilistically and logically sound fashion we show that while it may be impossible to directly detect a multiagent activity due to sensor noise or malfunction the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it further we show that given a model of successfully performed multiagent activities along with a set of examples of failed attempts at the same activities our system automatically learns an augmented model that is capable of recognizing success and failure as well as goals of peoples actions with high accuracy we compare our approach with other alternatives and show that our unified model which takes into account not only relationships among individual players but also relationships among activities over the entire length of a game although more computationally costly is significantly more accurate finally we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks







lama is a classical planning system based on heuristic forward search its core feature is the use of a pseudoheuristic derived from landmarks propositional formulas that must be true in every solution of a planning task lama builds on the fast downward planning system using finitedomain rather than binary state variables and multiheuristic search the latter is employed to combine the landmark heuristic with a variant of the wellknown ff heuristic both heuristics are costsensitive focusing on highquality solutions in the case where actions have nonuniform cost a weighted a search is used with iteratively decreasing weights so that the planner continues to search for plans of better quality until the search is terminated

lama showed best performance among all planners in the sequential satisficing track of the international planning competition 2008 in this paper we present the system in detail and investigate which features of lama are crucial for its performance we present individual results for some of the domains used at the competition demonstrating good and bad cases for the techniques implemented in lama overall we find that using landmarks improves performance whereas the incorporation of action costs into the heuristic estimators proves not to be beneficial we show that in some domains a search that ignores cost solves far more problems raising the question of how to deal with action costs more effectively in the future the iterated weighted a search greatly improves results and shows synergy effects with the use of landmarks







properties like logical closure and consistency are  important properties in any logical reasoning system caminada and amgoud showed that not every logicbased argument system satisfies these relevant properties but under  conditions like closure under contraposition or transposition of the monotonic part of the underlying logic aspiclike systems satisfy these properties in contrast the logical closure and  consistency properties are not wellunderstood for other  wellknown and widely applied systems like logic programming or assumption based argumentation though conditions like closure under contraposition or transposition seem intuitive in aspiclike systems they  rule out many sensible aspiclike systems that satisfy both properties of closure and consistency  

we present a new condition referred to as the selfcontradiction axiom that guarantees the consistency property  in both aspiclike and assumptionbased systems and is implied by both properties of closure under  contraposition or transposition we develop a logicassociated abstract argumentation framework by associating abstract argumentation with abstract logics to represent  the conclusions of arguments we show that logicassociated abstract argumentation frameworks  capture  aspiclike systems without preferences and assumptionbased argumentation we present two simple and natural properties of  compactness and cohesion in  logicassociated abstract argumentation frameworks and show that they capture  the logical closure and consistency properties we demonstrate that  in both  assumptionbased argumentation and aspiclike systems  cohesion  follows naturally from the selfcontradiction axiom we further give a translation from aspiclike systems without preferences into equivalent assumptionbased systems that keeps the selfcontradiction axiom invariant  







in this commentary i argue that although pddl is a very useful standard for the planning competition its design does not properly consider the issue of domain modeling hence i would not advocate its use in specifying planning domains outside of the context of the planning competition rather the field needs to explore different approaches and grapple more directly with the problem of effectively modeling and utilizing all of the diverse pieces of knowledge we typically have about planning domains







this paper discusses an interested party who wishes to influence the behavior of agents in a game multiagent interaction which is not under his control the interested party cannot design a new game cannot enforce agents behavior cannot enforce payments by the agents and cannot prohibit strategies available to the agents however he can influence the outcome of the game by committing to nonnegative monetary transfers for the different strategy profiles that may be selected by the agents  the interested party assumes that agents are rational in the commonly agreed sense that they do not use dominated strategies hence a certain subset of outcomes is implemented in a given game if by adding nonnegative payments rational players will necessarily produce an outcome in this subset obviously by making sufficiently big payments one can implement any desirable outcome the question is what is the cost of implementation in this paper we introduce the notion of kimplementation of a desired set of strategy profiles where k stands for the amount of payment that need to be actually made in order to implement desirable outcomes a major point in kimplementation is that monetary offers need not necessarily materialize when following desired behaviors  we define and study kimplementation in the contexts of games with complete and incomplete information in the latter case we mainly focus on the vcg games our setting is later extended to deal with mixed strategies using correlation devices together the paper introduces and studies the implementation of desirable outcomes by a reliable party who cannot modify game rules ie provide protocols complementing previous work in mechanism design while making it more applicable to many realistic cs settings







distributed constraint optimization dcop problems are a popular way of formulating and solving agentcoordination problems a dcop problem is a problem where several agents coordinate their values such that the sum of the resulting constraint costs is minimal it is often desirable to solve dcop problems with memorybounded and asynchronous algorithms we introduce branchandbound adopt bnbadopt a memorybounded asynchronous dcop search algorithm that uses the messagepassing and communication framework of adopt modi shen tambe  yokoo 2005 a well known memorybounded asynchronous dcop search algorithm but changes the search strategy of adopt from bestfirst search to depthfirst branchandbound search our experimental results show that bnbadopt finds costminimal solutions up to one order of magnitude faster than adopt for a variety of large dcop problems and is as fast as ncbb a memorybounded synchronous dcop search algorithm for most of these dcop problems additionally it is often desirable to find boundederror solutions for dcop problems within a reasonable amount of time since finding costminimal solutions is nphard the existing boundederror approximation mechanism allows users only to specify an absolute error bound on the solution cost but a relative error bound is often more intuitive thus we present two new boundederror approximation mechanisms that allow for relative error bounds and implement them on top of bnbadopt







the goal of this research is to develop agents that     are adaptive and predictable and timely at first blush     these three requirements seem contradictory for example      adaptation risks introducing undesirable side effects     thereby making agents behavior less predictable furthermore    although formal verification can assist in ensuring    behavioral predictability it is known to be timeconsuming  our solution to the challenge of satisfying all three    requirements is the following agents have finitestate    automaton plans which are adapted online via evolutionary    learning perturbation operators to ensure that critical    behavioral constraints are always satisfied agents plans    are first formally verified they are then reverified after    every adaptation if reverification concludes that constraints    are violated the plans are repaired the main objective of     this paper is to improve the efficiency of reverification     after learning so that agents have a sufficiently rapid     response time we present two solutions positive results     that certain learning operators are a priori guaranteed to    preserve useful classes of behavioral assurance constraints    which implies that no reverification is needed for these     operators and efficient incremental reverification algorithms     for those learning operators that have negative a priori results







2011 ijcaijair best paper prize



a major achievement of mechanism design theory is a general method for the construction of truthful mechanisms called vcg vickrey  clarke  groves when applying this method to complex problems such as combinatorial auctions a difficulty arises vcg mechanisms are required to compute optimal outcomes and are therefore computationally infeasible however if the optimal outcome is replaced by the results of a suboptimal algorithm the resulting mechanism termed vcgbased is no longer necessarily truthful the first part of this paper studies this phenomenon in depth and shows that it is near universal  specifically we prove that essentially all reasonable approximations or heuristics for combinatorial auctions as well as a wide class of cost minimization problems yield nontruthful vcgbased mechanisms we generalize these results for affine maximizers

the second part of this paper proposes a general method for circumventing the above problem we introduce a modification of vcgbased mechanisms in which the agents are given a chance to improve the output of the underlying algorithm when the agents behave truthfully the welfare obtained by the mechanism is at least as good as the one obtained by the algorithms output we provide a strong rationale for truthtelling behavior our method satisfies individual rationality as well





we present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis  our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product eg sushi and miso for a japanese restaurant and determines the corresponding sentiment of each aspect  this approach directly enables discovery of highlyrated or inconsistent aspects of a product  our generative model admits an efficient variational meanfield inference algorithm  it is also easily extensible and we describe several modifications and their effects on model structure and inference  we test our model on two tasks joint aspect identification and sentiment analysis on a set of yelp reviews and aspect identification alone on a set of medical summaries  we evaluate the performance of the model on aspect identification sentiment analysis and perword labeling accuracy  we demonstrate that our model outperforms applicable baselines by a considerable margin yielding up to 32 relative error reduction on aspect identification and up to 20 relative error reduction on sentiment analysis







s  d ramchurn c  mezzetti a  giovannucci j  a rodriguezaguilar r  k dash and n  r jennings 2009 trustbased mechanisms for robust and efficient task allocation in the presence of execution uncertainty volume 35 pages 119159



vickreyclarkegroves vcg mechanisms are often used to allocate tasks to selfish and rational agents vcg mechanisms are  incentive compatible direct mechanisms that are efficient ie maximise social utility and individually rational ie agents prefer to join rather than opt out however an important assumption of these mechanisms is that the agents will always successfully complete their allocated tasks clearly this assumption is unrealistic in many realworld applications where agents can and  often do fail in their endeavours moreover whether an agent is deemed to have failed may be perceived differently by different agents such subjective perceptions about an agents probability of succeeding at a given task are often captured and reasoned about using the notion of trust given this background in this paper we investigate the design of novel mechanisms that take into account the  trust between agents when allocating tasks  





ruben  izquierdo armando  suarez and german  rigau 2015 word vs classbased word sense disambiguation volume 54 pages 83122



as empirically demonstrated by the word sense disambiguation wsd tasks of the last sensevalsemeval exercises assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed many authors argue that one possible reason could be the use of inappropriate sets of word meanings in particular wordnet has been used as a defacto standard repository of word meanings in most of these tasks thus instead of using the word senses defined in wordnet some approaches have derived semantic classes representing groups of word senses however the meanings represented by wordnet have been only used for wsd at a very finegrained sense level or at a very coarsegrained semantic class level also called supersenses we suspect that an appropriate level of abstraction could be on between both levels the contributions of this paper are manifold first we propose a simple method to automatically derive semantic classes at intermediate levels of abstraction covering all nominal and verbal wordnet meanings second we empirically demonstrate that our automatically derived semantic classes outperform classical approaches based on word senses and more coarsegrained sense groupings third we also demonstrate that our supervised wsd system benefits from using these new semantic classes as additional semantic features while reducing the amount of training examples finally we also demonstrate the robustness of our supervised semantic classbased wsd system when tested on out of domain corpus





a  fern s  yoon and r  givan 2006 approximate policy iteration with a policy language bias solving relational markov decision processes volume 25 pages 75118







because of their occasional need to return to shallow points in a search tree existing backtracking methods can sometimes erase meaningful progress toward solving a search problem in this paper we present a method by which backtrack points can be moved deeper in the search space thereby avoiding this difficulty the technique developed is a variant of dependencydirected backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches







this paper presents a new approach to identifying and    eliminating mislabeled training instances for supervised learning the    goal of this approach is to improve classification accuracies produced    by learning algorithms by improving the quality of the training data    our approach uses a set of learning algorithms to create classifiers    that serve as noise filters for the training data  we evaluate single    algorithm majority vote and consensus filters on five datasets that    are prone to labeling errors  our experiments illustrate that    filtering significantly improves classification accuracy for noise    levels up to 30 percent  an analytical and empirical evaluation of    the precision of our approach shows that consensus filters are    conservative at throwing away good data at the expense of retaining    bad data and that majority filters are better at detecting bad data at    the expense of throwing away good data  this suggests that for    situations in which there is a paucity of data consensus filters are    preferable whereas majority vote filters are preferable for    situations with an abundance of data







information extraction is the task of automaticallypicking   up information of interest from an unconstrained text  informationof   interest is usually extracted in two steps  first sentence level   processing locates relevant pieces of information scatteredthroughout   the text second discourse processing merges coreferential   information to generate the output  in the first step pieces of   information are locally identified without recognizing any   relationships among them  a key word search or simple patternsearch   can achieve this purpose  the second step requires deeperknowledge   in order to understand relationships among separately identified   pieces of information  previous information extraction systems   focused on the first step partly because they were not required to   link up each piece of information with other pieces  to link the   extracted pieces of information and map them onto a structuredoutput   format complex discourse processing is essential  this paperreports   on a japanese information extraction system that merges information   using a pattern matcher and discourse processor  evaluationresults   show a high level of system performance which approaches human   performance







designing the dialogue policy of a spoken dialogue system    involves many nontrivial choices  this paper presents a reinforcement    learning approach for automatically optimizing a dialogue policy    which addresses the technical challenges in applying reinforcement    learning to a working dialogue system with human users  we report on    the design construction and empirical evaluation of njfun an    experimental spoken dialogue system that provides users with access to    information about fun things to do in new jersey  our results show    that by optimizing its performance via reinforcement learning njfun    measurably improves system performance

inductive logic programming or relational learning is a    powerful paradigm for machine learning or data mining  however in    order for ilp to become practically useful the efficiency of ilp    systems must improve substantially to this end the notion of a query    pack is introduced it structures sets of similar    queries furthermore a mechanism is described for executing such    query packs  a complexity analysis shows that considerable efficiency    improvements can be achieved through the use of this query pack    execution mechanism this claim is supported by empirical results    obtained by incorporating support for query pack execution in two    existing learning systems







we present a propositional logic to reason about the uncertainty of events where the uncertainty is modeled by a set of probability measures assigning an interval of probability to each event we give a sound and complete axiomatization for the logic and show that the satisfiability problem is npcomplete no harder than satisfiability for propositional logic







weighted voting is a classic model of cooperation among agents in decisionmaking domains in such games each player has a weight and a coalition of players wins the game if its total weight meets or exceeds a given quota a players power in such games is usually not directly proportional to his weight and is measured by a power index the most prominent among which are the shapleyshubik index and the banzhaf indexin this paper we investigate by how much a player can change his power as measured by the shapleyshubik index or the banzhaf index by means of a falsename manipulation ie splitting his weight among two or more identities for both indices we provide upper and lower bounds on the effect of weightsplitting we then show that checking whether a beneficial split exists is nphard and discuss efficient algorithms for restricted cases of this problem as well as randomized algorithms for the general case we also provide an experimental evaluation of these algorithms finally we examine related forms of manipulative behavior such as annexation where a player subsumes other players or merging where several players unite into one we characterize the computational complexity of such manipulations and provide limits on their effects for the banzhaf index we describe a new paradox which we term the annexation nonmonotonicity paradox







abstraction is one of the most promising approaches to improve the    performance of problem solvers in several domains abstraction by    dropping sentences of a domain description  as used in most    hierarchical planners  has proven useful in this paper we present    examples which illustrate significant drawbacks of abstraction by    dropping sentences to overcome these drawbacks we propose a more    general view of abstraction involving the change of representation    language we have developed a new abstraction methodology and a    related sound and complete learning algorithm that allows the complete    change of representation language of planning cases from concrete to    abstract  however to achieve a powerful change of the representation    language the abstract language itself as well as rules which describe    admissible ways of abstracting states must be provided in the domain    model this new abstraction approach is the core of paris plan    abstraction and refinement in an integrated system a system in which    abstract planning cases are automatically learned from given concrete    cases an empirical study in the domain of process planning in    mechanical engineering shows significant advantages of the proposed    reasoning from abstract cases over classical hierarchical planning







k  woodsend and m  lapata 2014 text rewriting improves semantic role labeling volume 51 pages 133164



largescale annotated corpora are a prerequisite to developing highperformance nlp systems such corpora are expensive to produce limited in size often demanding linguistic expertise in this paper we use text rewriting as a means of increasing the amount of labeled data available for model training our method uses automatically extracted rewrite rules from comparable corpora and bitexts to generate multiple versions of sentences annotated with gold standard labels we apply this idea to semantic role labeling and show that a model trained on rewritten data outperforms the state of the art on the conll2009 benchmark dataset





cue phrases may be used in a discourse sense to explicitly    signal discourse structure but also in a sentential sense to convey    semantic rather than structural information  correctly classifying    cue phrases as discourse or sentential is critical in natural language    processing systems that exploit discourse structure eg for    performing tasks such as anaphora resolution and plan recognition    this paper explores the use of machine learning for classifying cue    phrases as discourse or sentential  two machine learning programs    cgrendel and c45 are used to induce classification models from sets    of preclassified cue phrases and their features in text and speech    machine learning is shown to be an effective technique for not only    automating the generation of classification models but also for    improving upon previous results  when compared to manually derived    classification models already in the literature the learned models    often perform with higher accuracy and contain new linguistic insights    into the data  in addition the ability to automatically construct    classification models makes it easier to comparatively analyze the    utility of alternative feature representations of the data  finally    the ease of retraining makes the learning approach more scalable and    flexible than manual methods







there is a growing interest in intelligent assistants for a variety of applications from sorting email to helping people with disabilities to do their daily chores in this paper we formulate the problem of intelligent assistance in a decisiontheoretic framework and present both theoretical and empirical results we first introduce a class of pomdps called hiddengoal mdps hgmdps which formalizes the problem of interactively assisting an agent whose goal is hidden and whose actions are observable in spite of its restricted nature we show that optimal action selection for hgmdps is pspacecomplete even for deterministic dynamics we then introduce a more restricted model called helper action mdps hamdps which are sufficient for modeling many realworld problems we show classes of hamdps for which efficient algorithms are possible more interestingly for general hamdps we show that a simple myopic policy achieves a near optimal regret compared to an oracle assistant that knows the agents goal we then introduce more sophisticated versions of this policy for the general case of hgmdps that we combine with a novel approach for quickly learning about the agent being assisted we evaluate our approach in two gamelike computer environments where human subjects perform tasks and in a realworld domain of providing assistance during folder navigation in a computer desktop environment the results show that in all three domains the framework results in an assistant that substantially reduces user effort with only modest computation







in this article we consider the issue of optimal control in collaborative multiagent systems with stochastic dynamics  the agents have a joint task in which they have to reach a number of target states  the dynamics of the agents contains additive control and additive noise and the autonomous part factorizes over the agents  full observation of the global state is assumed  the goal is to minimize the accumulated joint cost which consists of integrated instantaneous costs and a joint end cost  the joint end cost expresses the joint task of the agents the instantaneous costs are quadratic in the control and factorize over the agents  the optimal control is given as a weighted linear combination of singleagent to singletarget controls  the singleagent to singletarget controls are expressed in terms of diffusion processes  these controls when not closed form expressions are formulated in terms of path integrals which are calculated approximately by metropolishastings sampling  the weights in the control are interpreted as marginals of a joint distribution over agent to target assignments  the structure of the latter is represented by a graphical model and the marginals are obtained by graphical model inference  exact inference of the graphical model will break down in large systems and so approximate inference methods are needed  we use naive mean field approximation and belief propagation to approximate the optimal control in systems with linear dynamics  we compare the approximate inference methods with the exact solution and we show that they can accurately compute the optimal control  finally we demonstrate the control method in multiagent systems with nonlinear dynamics consisting of up to 80 agents that have to reach an equal number of target states







in this paper we consider the problem of theory patching    in which we are given a domain theory some of whose components are    indicated to be possibly flawed and a set of labeled training    examples for the domain concept  the theory patching problem is to    revise only the indicated components of the theory such that the    resulting theory correctly classifies all the training examples    theory patching is thus a type of theory revision in which revisions    are made to individual components of the theory  our concern in this    paper is to determine for which classes of logical domain theories the    theory patching problem is tractable  we consider both propositional    and firstorder domain theories and show that the theory patching    problem is equivalent to that of determining what information    contained in a theory is stable regardless of what revisions might    be performed to the theory  we show that determining stability is    tractable if the input theory satisfies two conditions that revisions    to each theory component have monotonic effects on the classification    of examples and that theory components act independently in the    classification of examples in the theory  we also show how the    concepts introduced can be used to determine the soundness and    completeness of particular theory patching algorithms







traditional databases commonly support efficient query and    update procedures that operate in time which is sublinear in the size    of the database  our goal in this paper is to take a first step    toward dynamic reasoning in probabilistic databases with comparable    efficiency  we propose a dynamic data structure that supports    efficient algorithms for updating and querying singly connected    bayesian networks  in the conventional algorithm new evidence is    absorbed in o1 time and queries are processed in time on where n    is the size of the network  we propose an algorithm which after a    preprocessing phase allows us to answer queries in time olog n at    the expense of olog n time per evidence absorption  the usefulness    of sublinear processing time manifests itself in applications    requiring near realtime response over large probabilistic    databases we briefly discuss a potential application of dynamic    probabilistic reasoning in computational biology







we address the costsensitive feature acquisition problem where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features because acquiring the features is costly as well the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized we describe the value of information lattice voila an optimal and efficient feature subset acquisition framework unlike the common practice which is to acquire features greedily voila can reason with subsets of features voila efficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process through empirical evaluation on five medical datasets we show that the greedy strategy is often reluctant to acquire features as it cannot forecast the benefit of acquiring multiple features in combination







in recent years many improvements to backtracking algorithms for    solving constraint satisfaction problems have been proposed    the techniques for improving backtracking algorithms can    be conveniently classified as lookahead schemes and    lookback schemes  unfortunately lookahead and lookback    schemes are not entirely orthogonal as it has been observed    empirically that the enhancement of lookahead techniques    is sometimes counterproductive to the effects of lookback    techniques in this paper we focus on the relationship between    the two most important lookahead techniquesusing a variable    ordering heuristic and maintaining a level of local consistency    during the backtracking searchand the lookback technique of    conflictdirected backjumping cbj we show that there exists    a perfect dynamic variable ordering such that cbj becomes    redundant we also show theoretically that as the level of local    consistency that is maintained in the backtracking search is    increased the less that backjumping will be an improvement    our theoretical results partially explain why a backtracking    algorithm doing more in the lookahead phase cannot benefit    more from the backjumping lookback scheme finally we show    empirically that adding cbj to a backtracking algorithm that    maintains generalized arc consistency gac an algorithm that    we refer to as gaccbj can still provide orders of magnitude    speedups our empirical results contrast with bessiere and    regins conclusion 1996 that cbj is useless to an algorithm    that maintains arc consistency









c  b228ckstr246m and p  jonsson 2012 algorithms and limits for compact plan representations volume 44 pages 141177



compact representations of objects is a common concept in  computer science  automated planning can be viewed as a case of this concept a planning instance is a compact implicit representation of a graph and the problem is to find a path a plan in this graph  while the graphs themselves are represented compactly as planning instances the paths are usually represented explicitly as sequences of actions  some cases are known where the plans always have compact representations for example using macros  we show that these results do not extend to the general case by proving a number of bounds for compact representations of plans under various criteria like efficient sequential or random access of actions  in addition to this we show that our results have consequences for what can be gained from reformulating planning into some other problem  as a contrast to this we also prove a number of positive results demonstrating restricted cases where plans do have useful compact representations as well as proving that macro plans have favourable access properties  our results are finally discussed in relation to other relevant contexts



h  daume iii and d  marcu 2006 domain adaptation for statistical classifiers volume 26 pages 101126



the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution  unfortunately in many applications the indomain test data is drawn from a distribution that is related but not identical to the outofdomain distribution of the training data we consider the common case in which labeled outofdomain data is plentiful but labeled indomain data is scarce  we introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts  we present efficient inference algorithms for this special case based on the technique of conditional expectation maximization  our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain 





recently planning based on answer set programming has been proposed as an approach towards realizing declarative planning systems in this paper we present the language kc which extends the declarative planning language k by action costs kc provides the notion of admissible and optimal plans which are plans whose overall action costs are within a given limit resp minimum over all plans ie cheapest plans as we demonstrate this novel language allows for expressing some nontrivial planning tasks in a declarative way furthermore it can be utilized for representing planning problems under other optimality criteria such as computing shortest plans with the least number of steps and refinement combinations of cheapest and fastest plans we study complexity aspects of the language kc and provide a transformation to logic programs such that planning problems are solved via answer set programming furthermore we report experimental results on selected problems our experience is encouraging that answer set planning may be a valuable approach to expressive planning systems in which intricate planning problems can be naturally specified and solved







most modern formalisms used in databases and artificial    intelligence for describing an application domain are based on the    notions of class or concept and relationship among classes  one    interesting feature of such formalisms is the possibility of defining    a class ie providing a set of properties that precisely    characterize the instances of the class  many recent articles point    out that there are several ways of assigning a meaning to a class    definition containing some sort of recursion  in this paper we argue    that instead of choosing a single style of semantics we achieve    better results by adopting a formalism that allows for different    semantics to coexist we demonstrate the feasibility of our argument    by presenting a knowledge representation formalism the description    logic mualcq with the above characteristics  in addition to the    constructs for conjunction disjunction negation quantifiers and    qualified number restrictions mualcq includes special fixpoint    constructs to express suitably interpreted recursive definitions    these constructs enable the usual framebased descriptions to be    combined with definitions of recursive data structures such as    directed acyclic graphs lists streams etc  we establish several    properties of mualcq including the decidability and the computational    complexity of reasoning by formulating a correspondence with a    particular modal logic of programs called the modal mucalculus







m  pistore and m  y vardi 2007 the planning spectrum  one two three infinity volume 30 pages 101132



linear temporal logic ltl is widely used for defining conditions on the execution paths of dynamic systems  in the case of dynamic systems that allow for nondeterministic evolutions one has to specify along with an ltl formula f which are the paths that are required to satisfy the formula  two extreme cases are the universal interpretation af which requires that the formula be satisfied for all execution paths and the existential interpretation ef which requires that the formula be satisfied for some execution path





we present an incentivecompatible polynomialtime approximation scheme for multiunit auctions with general kminded player valuations the mechanism fully optimizes over an appropriately chosen subrange of possible allocations and then uses vcg payments over this subrange we show that obtaining a fully polynomialtime incentivecompatible approximation scheme at least using vcg payments is nphard for the case of valuations given by black boxes we give a polynomialtime incentivecompatible 2approximation mechanism and show that no better is possible at least using vcg payments







in this paper we confront the problem of applying reinforcement learning to agents that perceive the environment through many sensors and that can perform  parallel actions using many actuators as is the case in complex autonomous robots we argue that reinforcement learning can only be successfully applied to this case  if strong assumptions are made on the characteristics of the environment in which  the learning is performed so that the relevant sensor readings and motor commands can be  readily identified the introduction of such assumptions leads to stronglybiased  learning systems that can eventually lose the generality of traditional  reinforcementlearning algorithms  in this line we observe that in realistic situations the reward received by the robot  depends only on a reduced subset of all the executed actions and that only a reduced  subset of the sensor inputs possibly different in each situation and for each action  are relevant to predict the reward we formalize this property in the so called  categorizability assumption and we present an algorithm that takes advantage of  the categorizability of the environment allowing a decrease in the learning time with  respect to existing reinforcementlearning algorithms results of the application of the  algorithm to a couple of simulated realisticrobotic problems landmarkbased navigation  and the sixlegged robot gait generation are reported to validate our approach and to  compare it to existing flat and generalizationbased reinforcementlearning approaches







partialorder plans pops are attractive because of their leastcommitment nature which provides enhanced plan flexibility at execution time relative to sequential plans current research on automated plan generation focuses on producing sequential plans despite the appeal of pops in this paper we examine pop generation by relaxing or modifying the action orderings of a sequential plan to optimize for plan criteria that promote flexibility our approach relies on a novel partial weighted maxsat encoding of a sequential plan that supports the minimization of deordering or reordering of actions using a similar technique we further demonstrate how to remove redundant actions from the plan and how to combine this criterion with the objective of maximizing a pops flexibility our partial weighted maxsat encoding allows us to compute a pop from a sequential plan effectively we compare the efficiency of our approach to previous methods for pop generation via sequentialplan relaxation our results show that while an existing heuristic approach consistently produces the optimal deordering of a sequential plan our approach has greater flexibility when we consider reordering the actions in the plan while also providing a guarantee of optimality we also investigate and confirm the accuracy of the standard flex metric typically used to predict the true flexibility of a pop as measured by the number of linearizations it represents







we propose a model for errors in sung queries a variant of the hidden markov model hmm this is a solution to the problem of identifying the degree of similarity between a typically errorladen sung query and a potential target in a database of musical works an important problem in the field of music information retrieval similarity metrics are a critical component of querybyhumming qbh applications which search audio and multimedia databases for strong matches to oral queries our model comprehensively expresses the types of m error or variation between target and query cumulative and noncumulative local errors transposition tempo and tempo changes insertions deletions and modulation the model is not only expressive but automatically trainable or able to learn and generalize from query examples we present results of simulations designed to assess the discriminatory potential of the model and tests with real sung queries to demonstrate relevance to realworld applications







j  y chai z  prasov and s  qu 2006 cognitive principles in robust multimodal interpretation volume 27 pages 5583



multimodal conversational interfaces provide a natural means for users to communicate with computer systems through multiple modalities such as speech and gesture to build effective multimodal interfaces automated interpretation of user multimodal inputs is important inspired by the previous investigation on cognitive status in multimodal human machine interaction we have developed a greedy algorithm for interpreting user referring expressions ie multimodal reference resolution this algorithm incorporates the cognitive principles of conversational implicature and givenness hierarchy and applies constraints from various sources eg temporal semantic and contextual to resolve references our empirical results have shown the advantage of this algorithm in efficiently resolving a variety of user references because of its simplicity and generality this approach has the potential to improve the robustness of multimodal input interpretation





in this paper we consider markov decision processes mdps with error states  error states are those states entering which is undesirable or dangerous we define the risk with respect to a policy as the probability of entering such a state when the policy is pursued we consider the problem of finding good policies whose risk is smaller than some userspecified threshold and formalize it as a constrained mdp with two criteria the first criterion corresponds to the value function originally given we will show that the risk can be formulated as a second criterion function based on a cumulative return whose definition is independent of the original value function  we present a model free heuristic reinforcement learning algorithm that aims at finding good deterministic policies  it is based on weighting the original value function and the risk the weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function the algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column this control task was originally formulated as an optimal control problem with chance constraints and it was solved under certain assumptions on the model to obtain an optimal solution the power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed







the size and complexity of software and hardware systems    have significantly increased in the past years  as a result it is    harder to guarantee their correct behavior one of the most successful    methods for automated verification of finitestate systems is model    checking most of the current modelchecking systems use binary    decision diagrams bdds for the representation of the tested model    and in the verification process of its properties generally bdds    allow a canonical compact representation of a boolean function given    an order of its variables the more compact the bdd is the better    performance one gets from the verifier however finding an optimal    order for a bdd is an npcomplete problem therefore several    heuristic methods based on expert knowledge have been developed for    variable ordering       we propose an alternative approach in which the variable ordering     algorithm gains ordering experience from training models and     uses the learned knowledge for finding good orders our     methodology is based on offline learning of pair precedence     classifiers from training models that is learning which variable     pair permutation is more likely to lead to a good order for each     training model a number of training sequences are evaluated every     training model variable pair permutation is then tagged based on     its performance on the evaluated orders the tagged permutations     are then passed through a feature extractor and are given as     examples to a classifier creation algorithm given a model for     which an order is requested the ordering algorithm consults each     precedence classifier and constructs a pair precedence table     which is used to create the order      our algorithm was integrated with smv which is one of the most     widely used verification systems preliminary empirical evaluation of our     methodology using real benchmark models shows performance that     is better than random ordering and is competitive with existing     algorithms that use expert knowledge we believe that in     subdomains of models alu caches etc our system will prove     even more valuable this is because it features the ability to     learn subdomain knowledge something that no other ordering     algorithm does







coxs wellknown theorem justifying the use of probability is shown not to hold in finite domains the counterexample also suggests that coxs assumptions are insufficient to prove the result even in infinite domains the same counterexample is used to disprove a result of fine on comparative conditional probability







anyangle pathfinding is a fundamental problem in robotics and computer games the goal is to find a shortest path between a pair of points on a grid map such that the path is not artificially constrained to the points of the grid prior research has focused on approximate online solutions a number of exact methods exist but they all require superlinear space and preprocessing time  in this study we describe anya a new and optimal anyangle pathfinding algorithm where other works find approximate anyangle paths by searching over individual points from the grid anya finds optimal paths by searching over sets of states represented as intervals each interval is identified onthefly from each interval anya selects a single representative point that it uses to compute an admissible cost estimate for the entire set  anya always returns an optimal path if one exists  moreover it does so without any offline preprocessing or the introduction of additional memory overheads  in a range of empirical comparisons we show that anya is competitive with several recent suboptimal online and preprocessing based techniques and is up to an order of magnitude faster than the most common benchmark algorithm a gridbased implementation of a







multiagent path planning is a challenging problem with numerous reallife applications  running a centralized search such as a in the combined state space of all units is complete and costoptimal but scales poorly as the state space size is exponential in the number of mobile units  traditional decentralized approaches such as far and  whca are faster and more scalable being based on problem decomposition  however such methods are incomplete and provide no guarantees with respect to the running time or the solution quality  they are not necessarily able to tell in a reasonable time whether they would succeed in finding a solution to a given instance 

experiments were run on realistic game grid maps  mapp solved 9986 of all mobile units which is 1822 better than the percentage of far and whca  mapp marked 9882 of all units as provably solvable during the first stage of plan computation  parts of mapps computation can be reused across instances on the same map  speedwise mapp is competitive or significantly faster than whca depending on whether mapp performs all computations from scratch  when data that mapp can reuse are preprocessed offline and readily available mapp is slower than the very fast far algorithm by a factor of 218 on average  mapps solutions are on average 20 longer than fars solutions and 731 longer than whcas solutions 







this article focuses on word sense disambiguation wsd which is a natural language processing task that is thought to be important for many language technology applications such as information retrieval information extraction or machine translation one of the main issues preventing the deployment of wsd technology is the lack of training examples for machine learning systems also known as the knowledge acquisition bottleneck a method which has been shown to work for small samples of words is the automatic acquisition of examples we have previously shown that one of the most promising example acquisition methods scales up and produces a freely available database of 150 million examples from web snippets for all polysemous nouns in wordnet this paper focuses on the issues that arise when using those examples all alone or in addition to manually tagged examples to train a supervised wsd system for all nouns the extensive evaluation on both lexicalsample and allwords senseval benchmarks shows that we are able to improve over commonly used baselines and to achieve toprank performance the good use of the prior distributions from the senses proved to be a crucial factor







it is common to view programs as a combination of logic and    control the logic part defines what the program must do the control    part  how to do it  the logic programming paradigm was developed    with the intention of separating the logic from the control    recently extensive research has been conducted on automatic    generation of control for logic programs  only a few of these works    considered the issue of automatic generation of control for improving    the efficiency of logic programs  in this paper we present a novel    algorithm for automatic finding of lowestcost subgoal orderings  the    algorithm works using the divideandconquer strategy  the given set    of subgoals is partitioned into smaller sets based on cooccurrence    of free variables the subsets are ordered recursively and merged    yielding a provably optimal order  we experimentally demonstrate the    utility of the algorithm by testing it in several domains and discuss    the possibilities of its cooperation with other existing methods







the chief aim of this paper is to propose meanfield    approximations for a broad class of belief networks of which sigmoid    and noisyor networks can be seen as special cases  the     approximations are based on a powerful meanfield theory suggested by    plefka  we show that saul jaakkola and jordan s approach is the    first order approximation in plefkas approach via a variational    derivation  the application of plefkas theory to belief networks is    not computationally tractable  to tackle this problem we propose new    approximations based on taylor series  small scale experiments show    that the proposed schemes are attractive







we present a novel bayesian topic model for learning discourselevel document structure our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics we propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents we show that this space of orderings can be effectively represented using a distribution over permutations called the generalized mallows model we apply our method to three complementary discourselevel tasks crossdocument alignment document segmentation and information ordering our experiments show that incorporating our permutationbased model in these applications yields substantial improvements in performance over previously proposed methods







pearl and dechter 1996 claimed that the dseparation    criterion for conditional independence in acyclic causal networks also    applies to networks of discrete variables that have feedback cycles    provided that the variables of the system are uniquely determined by    the random disturbances  i show by example that this is not true in    general  some condition stronger than uniqueness is needed such as    the existence of a causal dynamics guaranteed to lead to the unique    solution







dungs abstract argumentation theory can be seen as a general framework for nonmonotonic reasoning an important question is then what is the class of logics that can be subsumed as instantiations of this theory the goal of this paper is to identify and study the large class of logicbased instantiations of dungs theory which correspond to the maxiconsistent operator ie to the function which returns maximal consistent subsets of an inconsistent knowledge base in other words we study the class of instantiations where very extension of the argumentation system corresponds to exactly one maximal consistent subset of the knowledge base we show that an attack relation belonging to this class must be conflictdependent must not be valid must not be conflictcomplete must not be symmetric etc then we show that some attack relations serve as lower or upper bounds of the class eg if an attack relation contains canonical undercut then it is not a member of this class by using our results we show for all existing attack relations whether or not they belong to this class we also define new attack relations which are members of this class finally we interpret our results and discuss more general questions like what is the added value of argumentation in such a setting we believe that this work is a first step towards achieving our longterm goal which is to better understand the role of argumentation and particularly the expressivity of logicbased instantiations of dungstyle argumentation frameworks







linguistic borrowing is the phenomenon of transferring linguistic constructions lexical phonological morphological and syntactic from a donor language to a recipient language as a result of contacts between communities speaking different languages borrowed words are found in all languages andin contrast to cognate relationshipsborrowing relationships may exist across unrelated languages for example about 40 of swahilis vocabulary is borrowed from the unrelated language arabic in this work we develop a model of morphophonological transformations across languages its features are based on universal constraints from optimality theory ot and we show that compared to several standardbut linguistically more na239vebaselines our otinspired model obtains good performance at predicting donor forms from borrowed forms with only a few dozen training examples making this a costeffective strategy for sharing lexical information across languages we demonstrate applications of the lexical borrowing model in machine translation using resourcerich donor language to obtain translations of outofvocabulary loanwords in a lower resource language our framework obtains substantial improvements up to 16 bleu over standard baselines







pointwisecompetitive classifier from class f is required to classify identically to the best classifier in hindsight from f for noisy agnostic settings we present a strategy for learning pointwisecompetitive classifiers from a finite training sample provided that the classifier can abstain from prediction at a certain region of its choice for some interesting hypothesis classes and families of distributions the measure of this rejected region is shown to be diminishing at a fast rate with high probability exact implementation of the proposed learning strategy is dependent on an erm oracle that can be hard to compute in the agnostic case we thus consider a heuristic approximation procedure that is based on svms and show empirically that this algorithm consistently outperforms a traditional rejection mechanism based on distance from decision boundary







to test incomplete search algorithms for constraint satisfaction problems such as 3sat we need a source of hard but satisfiable benchmark instances  a simple way to do this is to choose a random truth assignment a and then choose clauses randomly from among those satisfied by a however this method tends to produce easy problems since the majority of literals point toward the hidden assignment a last year achlioptas jia and moore proposed a problem generator that cancels this effect by hiding both a and its complement while the resulting formulas appear to be just as hard for dpll algorithms as random 3sat formulas with no hidden assignment they can be solved by walksat in only polynomial time  

here we propose a new method to cancel the attraction to a by choosing a clause with t  0 literals satisfied by a with probability proportional to qt for some q  1  by varying q we can generate formulas whose variables have no bias ie which are equally likely to be true or false we can even cause the formula to deceptively point away from a  we present theoretical and experimental results suggesting that these formulas are exponentially hard both for dpll algorithms and for incomplete algorithms such as walksat







a  gershman a  meisels  and r  zivan 2009 asynchronous forward bounding for distributed cops volume 34 pages 6188



a new search algorithm for solving distributed constraint optimization problems discops is presented agents assign variables sequentially and compute bounds on partial assignments asynchronously the asynchronous bounds computation is based on the propagation of partial assignments the asynchronous forwardbounding algorithm afb is a distributed optimization search algorithm that keeps one consistent partial assignment at all times the algorithm is described in detail and its correctness proven experimental evaluation shows that afb outperforms synchronous branch and bound by many orders of magnitude and produces a phase transition as the tightness of the problem increases this is an analogous effect to the phase transition that has been observed when local consistency maintenance is applied to maxcsps the afb algorithm is further enhanced by the addition of a backjumping mechanism resulting in the afbbj algorithm  distributed backjumping is based on accumulated information on bounds of all values and on processing concurrently a queue of candidate goals for the next move back the afbbj algorithm is compared experimentally to other discop algorithms adopt dpop optapo and is shown to be a very efficient algorithm for discops







h  vlaeminck j  vennekens m  denecker and m  bruynooghe 2012 an approximative inference method for solving 87078704so satisfiability problems volume 45 pages 79124



this paper considers the fragment 87078704so of secondorder logic many interesting problems such as conformant planning can be naturally expressed as finite domain satisfiability problems of this logic such satisfiability problems are computationally hard 931p2 and many of these problems are often solved approximately in this paper we develop a general approximative method ie a sound but incomplete method for solving 87078704so satisfiability problems we use a syntactic representation of a constraint propagation method for firstorder logic to transform such an 87078704so satisfiability problem to an 8707soid satisfiability problem secondorder logic extended with inductive definitions the finite domain satisfiability problem for the latter language is in np and can be handled by several existing solvers inductive definitions are a powerful knowledge representation tool and this moti vates us to also approximate 87078704soid problems in order to do this we first show how to perform propagation on such inductive definitions next we use this to approximate 87078704soid satisfiability problems all this provides a general theoretical framework for a number of approximative methods in the literature moreover we also show how we can use this framework for solving practical useful problems such as conformant planning in an effective way



sequential decisionmaking problems with multiple objectives arise naturally in practice and pose unique challenges for research in decisiontheoretic planning and learning which has largely focused on singleobjective settings this article surveys algorithms designed for sequential decisionmaking problems with multiple objectives though there is a growing body of literature on this subject little of it makes explicit under what circumstances special methods are needed to solve multiobjective problems therefore we identify three distinct scenarios in which converting such a problem to a singleobjective one is impossible infeasible or undesirable furthermore we propose a taxonomy that classifies multiobjective methods according to the applicable scenario the nature of the scalarization function which projects multiobjective values to scalar ones and the type of policies considered we show how these factors determine the nature of an optimal solution which can be a single policy a convex hull or a pareto front using this taxonomy we survey the literature on multiobjective methods for planning and learning finally we discuss key applications of such methods and outline opportunities for future work







we introduce cui networks a compact graphical representation of utility functions over multiple attributes cui networks model multiattribute utility functions using the wellstudied and widely applicable utility independence concept we show how conditional utility independence leads to an effective functional decomposition that can be exhibited graphically and how local compact data at the graph nodes can be used to calculate joint utility we discuss aspects of elicitation network construction and optimization and contrast our new representation with previous graphical preference modeling







the article introduces a ceteris paribus modal logic called cp interpreted on the equivalence classes induced by finite sets of propositional atoms this logic is studied and then used to embed three logics of strategic interaction namely atemporal stit the coalition logic of propositional control cl8722pc and the starless fragment of the dynamic logic of propositional assignments dl8722pa the embeddings highlight a common ceteris paribus structure underpinning the key operators of all these apparently very different logics and show we argue remarkable similarities behind some of the most influential formalisms for reasoning about strategic interaction







the easyhardeasy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning we test the generality of this explanation by examining one of its predictions if the number of solutions is held fixed by the choice of problems then increased pruning should lead to a monotonic decrease in search cost instead we find the easyhardeasy pattern in median search cost even when the number of solutions is held constant for some search methods this generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost in these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems rather than changing numbers of solutions







recent research has shown that surprisingly rich models of human activity can be learned from gps positional data however most effort to date has concentrated on modeling single individuals or statistical properties of groups of people moreover prior work focused solely on modeling actual successful executions and not failed or attempted executions of the activities of interest we in contrast take on the task of understanding human interactions attempted interactions and intentions from noisy sensor data in a fully relational multiagent setting we use a realworld game of capture the flag to illustrate our approach in a welldefined domain that involves many distinct cooperative and competitive joint activities we model the domain using markov logic a statisticalrelational language and learn a theory that jointly denoises the data and infers occurrences of highlevel activities such as a player capturing an enemy our unified model combines constraints imposed by the geometry of the game area the motion model of the players and by the rules and dynamics of the game in a probabilistically and logically sound fashion we show that while it may be impossible to directly detect a multiagent activity due to sensor noise or malfunction the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it further we show that given a model of successfully performed multiagent activities along with a set of examples of failed attempts at the same activities our system automatically learns an augmented model that is capable of recognizing success and failure as well as goals of peoples actions with high accuracy we compare our approach with other alternatives and show that our unified model which takes into account not only relationships among individual players but also relationships among activities over the entire length of a game although more computationally costly is significantly more accurate finally we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks







lama is a classical planning system based on heuristic forward search its core feature is the use of a pseudoheuristic derived from landmarks propositional formulas that must be true in every solution of a planning task lama builds on the fast downward planning system using finitedomain rather than binary state variables and multiheuristic search the latter is employed to combine the landmark heuristic with a variant of the wellknown ff heuristic both heuristics are costsensitive focusing on highquality solutions in the case where actions have nonuniform cost a weighted a search is used with iteratively decreasing weights so that the planner continues to search for plans of better quality until the search is terminated

lama showed best performance among all planners in the sequential satisficing track of the international planning competition 2008 in this paper we present the system in detail and investigate which features of lama are crucial for its performance we present individual results for some of the domains used at the competition demonstrating good and bad cases for the techniques implemented in lama overall we find that using landmarks improves performance whereas the incorporation of action costs into the heuristic estimators proves not to be beneficial we show that in some domains a search that ignores cost solves far more problems raising the question of how to deal with action costs more effectively in the future the iterated weighted a search greatly improves results and shows synergy effects with the use of landmarks







properties like logical closure and consistency are  important properties in any logical reasoning system caminada and amgoud showed that not every logicbased argument system satisfies these relevant properties but under  conditions like closure under contraposition or transposition of the monotonic part of the underlying logic aspiclike systems satisfy these properties in contrast the logical closure and  consistency properties are not wellunderstood for other  wellknown and widely applied systems like logic programming or assumption based argumentation though conditions like closure under contraposition or transposition seem intuitive in aspiclike systems they  rule out many sensible aspiclike systems that satisfy both properties of closure and consistency  

we present a new condition referred to as the selfcontradiction axiom that guarantees the consistency property  in both aspiclike and assumptionbased systems and is implied by both properties of closure under  contraposition or transposition we develop a logicassociated abstract argumentation framework by associating abstract argumentation with abstract logics to represent  the conclusions of arguments we show that logicassociated abstract argumentation frameworks  capture  aspiclike systems without preferences and assumptionbased argumentation we present two simple and natural properties of  compactness and cohesion in  logicassociated abstract argumentation frameworks and show that they capture  the logical closure and consistency properties we demonstrate that  in both  assumptionbased argumentation and aspiclike systems  cohesion  follows naturally from the selfcontradiction axiom we further give a translation from aspiclike systems without preferences into equivalent assumptionbased systems that keeps the selfcontradiction axiom invariant  







in this commentary i argue that although pddl is a very useful standard for the planning competition its design does not properly consider the issue of domain modeling hence i would not advocate its use in specifying planning domains outside of the context of the planning competition rather the field needs to explore different approaches and grapple more directly with the problem of effectively modeling and utilizing all of the diverse pieces of knowledge we typically have about planning domains







this paper discusses an interested party who wishes to influence the behavior of agents in a game multiagent interaction which is not under his control the interested party cannot design a new game cannot enforce agents behavior cannot enforce payments by the agents and cannot prohibit strategies available to the agents however he can influence the outcome of the game by committing to nonnegative monetary transfers for the different strategy profiles that may be selected by the agents  the interested party assumes that agents are rational in the commonly agreed sense that they do not use dominated strategies hence a certain subset of outcomes is implemented in a given game if by adding nonnegative payments rational players will necessarily produce an outcome in this subset obviously by making sufficiently big payments one can implement any desirable outcome the question is what is the cost of implementation in this paper we introduce the notion of kimplementation of a desired set of strategy profiles where k stands for the amount of payment that need to be actually made in order to implement desirable outcomes a major point in kimplementation is that monetary offers need not necessarily materialize when following desired behaviors  we define and study kimplementation in the contexts of games with complete and incomplete information in the latter case we mainly focus on the vcg games our setting is later extended to deal with mixed strategies using correlation devices together the paper introduces and studies the implementation of desirable outcomes by a reliable party who cannot modify game rules ie provide protocols complementing previous work in mechanism design while making it more applicable to many realistic cs settings







distributed constraint optimization dcop problems are a popular way of formulating and solving agentcoordination problems a dcop problem is a problem where several agents coordinate their values such that the sum of the resulting constraint costs is minimal it is often desirable to solve dcop problems with memorybounded and asynchronous algorithms we introduce branchandbound adopt bnbadopt a memorybounded asynchronous dcop search algorithm that uses the messagepassing and communication framework of adopt modi shen tambe  yokoo 2005 a well known memorybounded asynchronous dcop search algorithm but changes the search strategy of adopt from bestfirst search to depthfirst branchandbound search our experimental results show that bnbadopt finds costminimal solutions up to one order of magnitude faster than adopt for a variety of large dcop problems and is as fast as ncbb a memorybounded synchronous dcop search algorithm for most of these dcop problems additionally it is often desirable to find boundederror solutions for dcop problems within a reasonable amount of time since finding costminimal solutions is nphard the existing boundederror approximation mechanism allows users only to specify an absolute error bound on the solution cost but a relative error bound is often more intuitive thus we present two new boundederror approximation mechanisms that allow for relative error bounds and implement them on top of bnbadopt







the goal of this research is to develop agents that     are adaptive and predictable and timely at first blush     these three requirements seem contradictory for example      adaptation risks introducing undesirable side effects     thereby making agents behavior less predictable furthermore    although formal verification can assist in ensuring    behavioral predictability it is known to be timeconsuming  our solution to the challenge of satisfying all three    requirements is the following agents have finitestate    automaton plans which are adapted online via evolutionary    learning perturbation operators to ensure that critical    behavioral constraints are always satisfied agents plans    are first formally verified they are then reverified after    every adaptation if reverification concludes that constraints    are violated the plans are repaired the main objective of     this paper is to improve the efficiency of reverification     after learning so that agents have a sufficiently rapid     response time we present two solutions positive results     that certain learning operators are a priori guaranteed to    preserve useful classes of behavioral assurance constraints    which implies that no reverification is needed for these     operators and efficient incremental reverification algorithms     for those learning operators that have negative a priori results







2011 ijcaijair best paper prize



a major achievement of mechanism design theory is a general method for the construction of truthful mechanisms called vcg vickrey  clarke  groves when applying this method to complex problems such as combinatorial auctions a difficulty arises vcg mechanisms are required to compute optimal outcomes and are therefore computationally infeasible however if the optimal outcome is replaced by the results of a suboptimal algorithm the resulting mechanism termed vcgbased is no longer necessarily truthful the first part of this paper studies this phenomenon in depth and shows that it is near universal  specifically we prove that essentially all reasonable approximations or heuristics for combinatorial auctions as well as a wide class of cost minimization problems yield nontruthful vcgbased mechanisms we generalize these results for affine maximizers

the second part of this paper proposes a general method for circumventing the above problem we introduce a modification of vcgbased mechanisms in which the agents are given a chance to improve the output of the underlying algorithm when the agents behave truthfully the welfare obtained by the mechanism is at least as good as the one obtained by the algorithms output we provide a strong rationale for truthtelling behavior our method satisfies individual rationality as well





we present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis  our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product eg sushi and miso for a japanese restaurant and determines the corresponding sentiment of each aspect  this approach directly enables discovery of highlyrated or inconsistent aspects of a product  our generative model admits an efficient variational meanfield inference algorithm  it is also easily extensible and we describe several modifications and their effects on model structure and inference  we test our model on two tasks joint aspect identification and sentiment analysis on a set of yelp reviews and aspect identification alone on a set of medical summaries  we evaluate the performance of the model on aspect identification sentiment analysis and perword labeling accuracy  we demonstrate that our model outperforms applicable baselines by a considerable margin yielding up to 32 relative error reduction on aspect identification and up to 20 relative error reduction on sentiment analysis







s  d ramchurn c  mezzetti a  giovannucci j  a rodriguezaguilar r  k dash and n  r jennings 2009 trustbased mechanisms for robust and efficient task allocation in the presence of execution uncertainty volume 35 pages 119159



vickreyclarkegroves vcg mechanisms are often used to allocate tasks to selfish and rational agents vcg mechanisms are  incentive compatible direct mechanisms that are efficient ie maximise social utility and individually rational ie agents prefer to join rather than opt out however an important assumption of these mechanisms is that the agents will always successfully complete their allocated tasks clearly this assumption is unrealistic in many realworld applications where agents can and  often do fail in their endeavours moreover whether an agent is deemed to have failed may be perceived differently by different agents such subjective perceptions about an agents probability of succeeding at a given task are often captured and reasoned about using the notion of trust given this background in this paper we investigate the design of novel mechanisms that take into account the  trust between agents when allocating tasks  





ruben  izquierdo armando  suarez and german  rigau 2015 word vs classbased word sense disambiguation volume 54 pages 83122



as empirically demonstrated by the word sense disambiguation wsd tasks of the last sensevalsemeval exercises assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed many authors argue that one possible reason could be the use of inappropriate sets of word meanings in particular wordnet has been used as a defacto standard repository of word meanings in most of these tasks thus instead of using the word senses defined in wordnet some approaches have derived semantic classes representing groups of word senses however the meanings represented by wordnet have been only used for wsd at a very finegrained sense level or at a very coarsegrained semantic class level also called supersenses we suspect that an appropriate level of abstraction could be on between both levels the contributions of this paper are manifold first we propose a simple method to automatically derive semantic classes at intermediate levels of abstraction covering all nominal and verbal wordnet meanings second we empirically demonstrate that our automatically derived semantic classes outperform classical approaches based on word senses and more coarsegrained sense groupings third we also demonstrate that our supervised wsd system benefits from using these new semantic classes as additional semantic features while reducing the amount of training examples finally we also demonstrate the robustness of our supervised semantic classbased wsd system when tested on out of domain corpus





a  fern s  yoon and r  givan 2006 approximate policy iteration with a policy language bias solving relational markov decision processes volume 25 pages 75118







because of their occasional need to return to shallow points in a search tree existing backtracking methods can sometimes erase meaningful progress toward solving a search problem in this paper we present a method by which backtrack points can be moved deeper in the search space thereby avoiding this difficulty the technique developed is a variant of dependencydirected backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches







this paper presents a new approach to identifying and    eliminating mislabeled training instances for supervised learning the    goal of this approach is to improve classification accuracies produced    by learning algorithms by improving the quality of the training data    our approach uses a set of learning algorithms to create classifiers    that serve as noise filters for the training data  we evaluate single    algorithm majority vote and consensus filters on five datasets that    are prone to labeling errors  our experiments illustrate that    filtering significantly improves classification accuracy for noise    levels up to 30 percent  an analytical and empirical evaluation of    the precision of our approach shows that consensus filters are    conservative at throwing away good data at the expense of retaining    bad data and that majority filters are better at detecting bad data at    the expense of throwing away good data  this suggests that for    situations in which there is a paucity of data consensus filters are    preferable whereas majority vote filters are preferable for    situations with an abundance of data







information extraction is the task of automaticallypicking   up information of interest from an unconstrained text  informationof   interest is usually extracted in two steps  first sentence level   processing locates relevant pieces of information scatteredthroughout   the text second discourse processing merges coreferential   information to generate the output  in the first step pieces of   information are locally identified without recognizing any   relationships among them  a key word search or simple patternsearch   can achieve this purpose  the second step requires deeperknowledge   in order to understand relationships among separately identified   pieces of information  previous information extraction systems   focused on the first step partly because they were not required to   link up each piece of information with other pieces  to link the   extracted pieces of information and map them onto a structuredoutput   format complex discourse processing is essential  this paperreports   on a japanese information extraction system that merges information   using a pattern matcher and discourse processor  evaluationresults   show a high level of system performance which approaches human   performance







designing the dialogue policy of a spoken dialogue system    involves many nontrivial choices  this paper presents a reinforcement    learning approach for automatically optimizing a dialogue policy    which addresses the technical challenges in applying reinforcement    learning to a working dialogue system with human users  we report on    the design construction and empirical evaluation of njfun an    experimental spoken dialogue system that provides users with access to    information about fun things to do in new jersey  our results show    that by optimizing its performance via reinforcement learning njfun    measurably improves system performance

inductive logic programming or relational learning is a    powerful paradigm for machine learning or data mining  however in    order for ilp to become practically useful the efficiency of ilp    systems must improve substantially to this end the notion of a query    pack is introduced it structures sets of similar    queries furthermore a mechanism is described for executing such    query packs  a complexity analysis shows that considerable efficiency    improvements can be achieved through the use of this query pack    execution mechanism this claim is supported by empirical results    obtained by incorporating support for query pack execution in two    existing learning systems







we present a propositional logic to reason about the uncertainty of events where the uncertainty is modeled by a set of probability measures assigning an interval of probability to each event we give a sound and complete axiomatization for the logic and show that the satisfiability problem is npcomplete no harder than satisfiability for propositional logic







weighted voting is a classic model of cooperation among agents in decisionmaking domains in such games each player has a weight and a coalition of players wins the game if its total weight meets or exceeds a given quota a players power in such games is usually not directly proportional to his weight and is measured by a power index the most prominent among which are the shapleyshubik index and the banzhaf indexin this paper we investigate by how much a player can change his power as measured by the shapleyshubik index or the banzhaf index by means of a falsename manipulation ie splitting his weight among two or more identities for both indices we provide upper and lower bounds on the effect of weightsplitting we then show that checking whether a beneficial split exists is nphard and discuss efficient algorithms for restricted cases of this problem as well as randomized algorithms for the general case we also provide an experimental evaluation of these algorithms finally we examine related forms of manipulative behavior such as annexation where a player subsumes other players or merging where several players unite into one we characterize the computational complexity of such manipulations and provide limits on their effects for the banzhaf index we describe a new paradox which we term the annexation nonmonotonicity paradox







abstraction is one of the most promising approaches to improve the    performance of problem solvers in several domains abstraction by    dropping sentences of a domain description  as used in most    hierarchical planners  has proven useful in this paper we present    examples which illustrate significant drawbacks of abstraction by    dropping sentences to overcome these drawbacks we propose a more    general view of abstraction involving the change of representation    language we have developed a new abstraction methodology and a    related sound and complete learning algorithm that allows the complete    change of representation language of planning cases from concrete to    abstract  however to achieve a powerful change of the representation    language the abstract language itself as well as rules which describe    admissible ways of abstracting states must be provided in the domain    model this new abstraction approach is the core of paris plan    abstraction and refinement in an integrated system a system in which    abstract planning cases are automatically learned from given concrete    cases an empirical study in the domain of process planning in    mechanical engineering shows significant advantages of the proposed    reasoning from abstract cases over classical hierarchical planning







k  woodsend and m  lapata 2014 text rewriting improves semantic role labeling volume 51 pages 133164



largescale annotated corpora are a prerequisite to developing highperformance nlp systems such corpora are expensive to produce limited in size often demanding linguistic expertise in this paper we use text rewriting as a means of increasing the amount of labeled data available for model training our method uses automatically extracted rewrite rules from comparable corpora and bitexts to generate multiple versions of sentences annotated with gold standard labels we apply this idea to semantic role labeling and show that a model trained on rewritten data outperforms the state of the art on the conll2009 benchmark dataset





cue phrases may be used in a discourse sense to explicitly    signal discourse structure but also in a sentential sense to convey    semantic rather than structural information  correctly classifying    cue phrases as discourse or sentential is critical in natural language    processing systems that exploit discourse structure eg for    performing tasks such as anaphora resolution and plan recognition    this paper explores the use of machine learning for classifying cue    phrases as discourse or sentential  two machine learning programs    cgrendel and c45 are used to induce classification models from sets    of preclassified cue phrases and their features in text and speech    machine learning is shown to be an effective technique for not only    automating the generation of classification models but also for    improving upon previous results  when compared to manually derived    classification models already in the literature the learned models    often perform with higher accuracy and contain new linguistic insights    into the data  in addition the ability to automatically construct    classification models makes it easier to comparatively analyze the    utility of alternative feature representations of the data  finally    the ease of retraining makes the learning approach more scalable and    flexible than manual methods







there is a growing interest in intelligent assistants for a variety of applications from sorting email to helping people with disabilities to do their daily chores in this paper we formulate the problem of intelligent assistance in a decisiontheoretic framework and present both theoretical and empirical results we first introduce a class of pomdps called hiddengoal mdps hgmdps which formalizes the problem of interactively assisting an agent whose goal is hidden and whose actions are observable in spite of its restricted nature we show that optimal action selection for hgmdps is pspacecomplete even for deterministic dynamics we then introduce a more restricted model called helper action mdps hamdps which are sufficient for modeling many realworld problems we show classes of hamdps for which efficient algorithms are possible more interestingly for general hamdps we show that a simple myopic policy achieves a near optimal regret compared to an oracle assistant that knows the agents goal we then introduce more sophisticated versions of this policy for the general case of hgmdps that we combine with a novel approach for quickly learning about the agent being assisted we evaluate our approach in two gamelike computer environments where human subjects perform tasks and in a realworld domain of providing assistance during folder navigation in a computer desktop environment the results show that in all three domains the framework results in an assistant that substantially reduces user effort with only modest computation







in this article we consider the issue of optimal control in collaborative multiagent systems with stochastic dynamics  the agents have a joint task in which they have to reach a number of target states  the dynamics of the agents contains additive control and additive noise and the autonomous part factorizes over the agents  full observation of the global state is assumed  the goal is to minimize the accumulated joint cost which consists of integrated instantaneous costs and a joint end cost  the joint end cost expresses the joint task of the agents the instantaneous costs are quadratic in the control and factorize over the agents  the optimal control is given as a weighted linear combination of singleagent to singletarget controls  the singleagent to singletarget controls are expressed in terms of diffusion processes  these controls when not closed form expressions are formulated in terms of path integrals which are calculated approximately by metropolishastings sampling  the weights in the control are interpreted as marginals of a joint distribution over agent to target assignments  the structure of the latter is represented by a graphical model and the marginals are obtained by graphical model inference  exact inference of the graphical model will break down in large systems and so approximate inference methods are needed  we use naive mean field approximation and belief propagation to approximate the optimal control in systems with linear dynamics  we compare the approximate inference methods with the exact solution and we show that they can accurately compute the optimal control  finally we demonstrate the control method in multiagent systems with nonlinear dynamics consisting of up to 80 agents that have to reach an equal number of target states







in this paper we consider the problem of theory patching    in which we are given a domain theory some of whose components are    indicated to be possibly flawed and a set of labeled training    examples for the domain concept  the theory patching problem is to    revise only the indicated components of the theory such that the    resulting theory correctly classifies all the training examples    theory patching is thus a type of theory revision in which revisions    are made to individual components of the theory  our concern in this    paper is to determine for which classes of logical domain theories the    theory patching problem is tractable  we consider both propositional    and firstorder domain theories and show that the theory patching    problem is equivalent to that of determining what information    contained in a theory is stable regardless of what revisions might    be performed to the theory  we show that determining stability is    tractable if the input theory satisfies two conditions that revisions    to each theory component have monotonic effects on the classification    of examples and that theory components act independently in the    classification of examples in the theory  we also show how the    concepts introduced can be used to determine the soundness and    completeness of particular theory patching algorithms







traditional databases commonly support efficient query and    update procedures that operate in time which is sublinear in the size    of the database  our goal in this paper is to take a first step    toward dynamic reasoning in probabilistic databases with comparable    efficiency  we propose a dynamic data structure that supports    efficient algorithms for updating and querying singly connected    bayesian networks  in the conventional algorithm new evidence is    absorbed in o1 time and queries are processed in time on where n    is the size of the network  we propose an algorithm which after a    preprocessing phase allows us to answer queries in time olog n at    the expense of olog n time per evidence absorption  the usefulness    of sublinear processing time manifests itself in applications    requiring near realtime response over large probabilistic    databases we briefly discuss a potential application of dynamic    probabilistic reasoning in computational biology







we address the costsensitive feature acquisition problem where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features because acquiring the features is costly as well the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized we describe the value of information lattice voila an optimal and efficient feature subset acquisition framework unlike the common practice which is to acquire features greedily voila can reason with subsets of features voila efficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process through empirical evaluation on five medical datasets we show that the greedy strategy is often reluctant to acquire features as it cannot forecast the benefit of acquiring multiple features in combination







in recent years many improvements to backtracking algorithms for    solving constraint satisfaction problems have been proposed    the techniques for improving backtracking algorithms can    be conveniently classified as lookahead schemes and    lookback schemes  unfortunately lookahead and lookback    schemes are not entirely orthogonal as it has been observed    empirically that the enhancement of lookahead techniques    is sometimes counterproductive to the effects of lookback    techniques in this paper we focus on the relationship between    the two most important lookahead techniquesusing a variable    ordering heuristic and maintaining a level of local consistency    during the backtracking searchand the lookback technique of    conflictdirected backjumping cbj we show that there exists    a perfect dynamic variable ordering such that cbj becomes    redundant we also show theoretically that as the level of local    consistency that is maintained in the backtracking search is    increased the less that backjumping will be an improvement    our theoretical results partially explain why a backtracking    algorithm doing more in the lookahead phase cannot benefit    more from the backjumping lookback scheme finally we show    empirically that adding cbj to a backtracking algorithm that    maintains generalized arc consistency gac an algorithm that    we refer to as gaccbj can still provide orders of magnitude    speedups our empirical results contrast with bessiere and    regins conclusion 1996 that cbj is useless to an algorithm    that maintains arc consistency









c  b228ckstr246m and p  jonsson 2012 algorithms and limits for compact plan representations volume 44 pages 141177



compact representations of objects is a common concept in  computer science  automated planning can be viewed as a case of this concept a planning instance is a compact implicit representation of a graph and the problem is to find a path a plan in this graph  while the graphs themselves are represented compactly as planning instances the paths are usually represented explicitly as sequences of actions  some cases are known where the plans always have compact representations for example using macros  we show that these results do not extend to the general case by proving a number of bounds for compact representations of plans under various criteria like efficient sequential or random access of actions  in addition to this we show that our results have consequences for what can be gained from reformulating planning into some other problem  as a contrast to this we also prove a number of positive results demonstrating restricted cases where plans do have useful compact representations as well as proving that macro plans have favourable access properties  our results are finally discussed in relation to other relevant contexts



h  daume iii and d  marcu 2006 domain adaptation for statistical classifiers volume 26 pages 101126



the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution  unfortunately in many applications the indomain test data is drawn from a distribution that is related but not identical to the outofdomain distribution of the training data we consider the common case in which labeled outofdomain data is plentiful but labeled indomain data is scarce  we introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts  we present efficient inference algorithms for this special case based on the technique of conditional expectation maximization  our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain 



