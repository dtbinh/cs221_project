s  w carden 2014 convergence of a qlearning variant for continuous states and actions volume 49 pages 705731

this paper presents a reinforcement learning algorithm for solving infinite horizon markov decision processes under the expected total discounted reward criterion when both the state and action spaces are continuous  this algorithm is based on watkins qlearning but uses nadarayawatson kernel smoothing to generalize knowledge to unvisited states  as expected continuity conditions must be imposed on the mean rewards and transition probabilities  using results from kernel regression theory this algorithm is proven capable of producing a qvalue function estimate that is uniformly within an arbitrary tolerance of the true qvalue function with probability one  the algorithm is then applied to an example problem to empirically show convergence as well 

