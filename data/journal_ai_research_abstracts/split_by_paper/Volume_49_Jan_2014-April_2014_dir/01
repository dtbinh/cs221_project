e  bruni n  k  tran and m  baroni 2014 multimodal distributional semantics volume 49 pages 147

distributional semantic models derive computational representations of word meaning from the patterns of cooccurrence of words in text such models have been a success story of computational linguistics being able to provide reliable estimates of semantic relatedness for the many semantic tasks requiring them however distributional models extract meaning information exclusively from text which is an extremely impoverished basis compared to the rich perceptual sources that ground human semantic knowledge we address the lack of perceptual grounding of distributional models by exploiting computer vision techniques that automatically identify discrete visual words in images so that the distributional representation of a word can be extended to also encompass its cooccurrence with the visual words of images it is associated with we propose a flexible architecture to integrate text and imagebased distributional information and we show in a set of empirical tests that our integrated model is superior to the purely textbased approach and it provides somewhat complementary semantic information with respect to the latter

