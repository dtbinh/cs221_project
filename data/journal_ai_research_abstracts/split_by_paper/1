b  zanuttini 2003 new polynomial classes for logicbased abduction volume 19 pages 110



we address the problem of propositional logicbased    abduction ie the problem of searching for a best explanation for a    given propositional observation according to a given propositional    knowledge base we give a general algorithm based on the notion of    projection then we study restrictions over the representations of the    knowledge base and of the query and find new polynomial classes of    abduction problems



d  r wilson and  t  r martinez 1997 improved heterogeneous distance functions volume 6 pages 134



instancebased learning techniques typically handle    continuous and linear input values well but often do not handle    nominal input attributes appropriately  the value difference metric    vdm was designed to find reasonable distance values between nominal    attribute values but it largely ignores continuous attributes    requiring discretization to map continuous values into nominal values    this paper proposes three new heterogeneous distance functions called    the heterogeneous value difference metric hvdm the interpolated    value difference metric ivdm and the windowed value difference    metric wvdm  these new distance functions are designed to handle    applications with nominal attributes continuous attributes or both    in experiments on 48 applications the new distance metrics achieve    higher classification accuracy on average than three previous distance    functions on those datasets that have both nominal and continuous    attributes



m  j carman and c  a knoblock 2007 learning semantic definitions of online information sources volume 30 pages 150



the internet contains a very large number of information sources providing many types of data from weather forecasts to travel deals and financial information these sources can be accessed via webforms web services rss feeds and so on in order to make automated use of these sources we need to model them semantically but writing semantic descriptions for web services is both tedious and error prone in this paper we investigate the problem of automatically generating such models we introduce a framework for learning datalog definitions of web sources in order to learn these definitions our system actively invokes the sources and compares the data they produce with that of known sources of information it then performs an inductive logic search through the space of plausible source definitions in order to learn the best possible semantic model for each new source in this paper we perform an empirical evaluation of the system using realworld web sources the evaluation demonstrates the effectiveness of the approach showing that we can automatically learn complex models for real sources in reasonable time we also compare our system with a complex schema matching system showing that our approach can handle the kinds of problems tackled by the latter



g  tsatsaronis i  varlamis and m  vazirgiannis 2010 text relatedness based on a word thesaurus volume 37 pages 139



the computation of relatedness between two fragments of text in an automated manner requires taking into account a wide range of factors pertaining to the meaning the two fragments convey and the pairwise relations between their words without doubt a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words such a measure that captures well both aspects of text relatedness may help in many tasks such as text retrieval classification and clustering in this paper we present a new approach for measuring the semantic relatedness between words based on their implicit semantic links the approach exploits only a word thesaurus in order to devise implicit semantic links between words based on this approach we introduce omiotis a new measure of semantic relatedness between texts which capitalizes on the wordtoword semantic relatedness measure sr and extends it to measure the relatedness between texts we gradually validate our method we first evaluate the performance of the semantic relatedness measure between individual words covering wordtoword similarity and relatedness synonym identification and word analogy then we proceed with evaluating the performance of our method in measuring texttotext semantic relatedness in two tasks namely sentencetosentence similarity and paraphrase recognition experimental evaluation shows that the proposed method outperforms every lexiconbased method of semantic relatedness in the selected tasks and the used data sets and competes well against corpusbased and hybrid approaches 



n  roy  g  gordon and  s  thrun 2005 finding approximate pomdp solutions through belief compression volume 23 pages 140



standard value function approaches to finding policies for partially observable markov decision processes pomdps are generally considered to be intractable for large models the intractability of these algorithms is to a large extent a consequence of computing an exact optimal policy over the entire belief space  however in realworld pomdp problems computing the optimal policy for the full belief space is often unnecessary for good control even for problems with complicated policy classes the beliefs experienced by the controller often lie near a structured lowdimensional subspace embedded in the highdimensional belief space finding a good approximation to the optimal value function for only this subspace can be much easier than computing the full value functionp p  we introduce a new method for solving largescale pomdps by reducing the dimensionality of the belief space we use exponential family principal components analysis collins dasgupta  schapire 2002 to represent sparse highdimensional belief spaces using small sets of learned features of the belief state we then plan only in terms of the lowdimensional belief features by planning in this lowdimensional space we can find policies for pomdp models that are orders of magnitude larger than models that can be handled by conventional techniquesp p we demonstrate the use of this algorithm on a synthetic problem and on mobile robot navigation tasks



simone  villa and fabio  stella 2016 learning continuous time bayesian networks in nonstationary domains volume 57 pages 137



nonstationary continuous time bayesian networks are introduced they allow the parents set of each node to change over continuous time three settings are developed for learning nonstationary continuous time bayesian networks from data known transition times known number of epochs and unknown number of epochs a score function for each setting is derived and the corresponding learning algorithm is developed a set of numerical experiments on synthetic data is used to compare the effectiveness of nonstationary continuous time bayesian networks to that of nonstationary dynamic bayesian networks furthermore the performance achieved by nonstationary continuous time bayesian networks is compared to that achieved by stateoftheart algorithms on four realworld datasets namely drosophila saccharomyces cerevisiae songbird and macroeconomics 



d  cohen  m  cooper  p  jeavons and  a  krokhin 2004 a maximal tractable class of soft constraints volume 22 pages 122



many researchers in artificial intelligence are beginning to explore the use of soft constraints to express a set of possibly conflicting problem  requirements a soft constraint is a function defined on a collection of  variables which associates some measure of desirability with each possible  combination of values for those variables however the crucial question of  the computational complexity of finding the optimal solution to a collection of soft constraints has so far received very little attention in this paper we identify a class of soft binary constraints for which the problem of  finding the optimal solution is tractable in other words we show that for  any given set of such constraints there exists a polynomial time algorithm  to determine the assignment having the best overall combined measure of  desirability this tractable class includes many commonlyoccurring soft constraints such as as near as possible or as soon as possible after as well as crisp constraints such as greater than finally we show that  this tractable class is maximal in the sense that adding any other form of  soft binary constraint which is not in the class gives rise to a class of  problems which is nphard



d  geiger c  meek and y  wexler 2006 a variational inference procedure allowing internal structure for overlapping clusters and deterministic constraints volume 27 pages 123



we develop a novel algorithm called vip for structured variational approximate inference this algorithm extends known algorithms to allow efficient multiple potential updates for overlapping clusters and overcomes the difficulties imposed by deterministic constraints the algorithms convergence is proven and its applicability demonstrated  for genetic linkage analysis



d  ortizboyer  c  herv225smart237nez and  n  garc237apedrajas 2005 cixl2 a crossover operator for evolutionary algorithms based on population features volume 24 pages 148



in this paper we propose a crossover operator for evolutionary algorithms with real values that is based on the statistical theory of population distributions the operator is based on the theoretical distribution of the values of the genes of the best individuals in the population  the proposed operator takes into account the localization and dispersion features of the best individuals of the population with the objective that these features would be inherited by the offspring our aim is the optimization of the balance between exploration and exploitation in the search process    in order to test the efficiency and robustness of this crossover we have used a set of functions to be optimized with regard to different criteria such as multimodality separability regularity and epistasis with this set of functions we can extract conclusions in function of the problem at hand we analyze the results using anova and multiple comparison statistical tests  as an example of how our crossover can be used to solve artificial intelligence problems we have applied the proposed model to the problem of obtaining the weight of each network in a ensemble of neural networks the results obtained are above the performance of standard methods



c  thompson 2003 acquiring wordmeaning mappings for natural language interfaces volume 18 pages 144



this paper focuses on a system wolfie word learning from    interpreted examples that acquires a semantic lexicon from a corpus    of sentences paired with semantic representations  the lexicon    learned consists of phrases paired with meaning representations    wolfie is part of an integrated system that learns to transform    sentences into representations such as logical database queries        experimental results are presented demonstrating wolfies ability to    learn useful lexicons for a database interface in four different    natural languages  the usefulness of the lexicons learned by wolfie    are compared to those acquired by a similar system with results    favorable to wolfie  a second set of experiments demonstrates    wolfies ability to scale to larger and more difficult albeit    artificially generated corpora           in natural language acquisition it is difficult to gather the    annotated data needed for supervised learning however unannotated    data is fairly plentiful  active learning methods attempt to select    for annotation and training only the most informative examples and    therefore are potentially very useful in natural language    applications  however most results to date for active learning have    only considered standard classification tasks  to reduce annotation    effort while maintaining accuracy we apply active learning to    semantic lexicons  we show that active learning can significantly    reduce the number of annotated examples required to achieve a given    level of performance



e  davis 1999 order of magnitude comparisons of distance volume 10 pages 138



order of magnitude reasoning  reasoning by rough    comparisons of the sizes of quantities  is often called back of    the envelope calculation with the implication that the calculations    are quick though approximate  this paper exhibits an interesting    class of constraint sets in which order of magnitude reasoning is    demonstrably fast  specifically we present a polynomialtime    algorithm that can solve a set of constraints of the form points a    and b are much closer together than points c and d  we prove that    this algorithm can be applied if much closer together is    interpreted either as referring to an infinite difference in scale or    as referring to a finite difference in scale as long as the    difference in scale is greater than the number of variables in the    constraint set  we also prove that the firstorder theory over such    constraints is decidable



franz  baader meghyn  bienvenu carsten  lutz and frank  wolter 2016 query and predicate emptiness in ontologybased data access volume 56 pages 159



in ontologybased data access obda database querying is enriched with an ontology that provides domain knowledge and additional vocabulary for query formulation we identify query emptiness and predicate emptiness as two central reasoning services in this context query emptiness asks whether a given query has an empty answer over all databases formulated in a given vocabulary predicate emptiness is defined analogously but quantifies universally over all queries that contain a given predicate in this paper we determine the computational complexity of query emptiness and predicate emptiness in the el dllite and alcfamilies of description logics investigate the connection to ontology modules and perform a practical case study to evaluate the new reasoning services



t  walsh 2011 where are the hard manipulation problems volume 42 pages 129



voting is a simple mechanism to combine together the preferences of multiple agents unfortunately agents may try to manipulate the result by misreporting their preferences one barrier that might exist to such manipulation is computational complexity in particular it has been shown that it is nphard to compute how to manipulate a number of different voting rules how ever nphardness only bounds the worstcase complexity recent theoretical results suggest that manipulation may often be easy in practice in this paper we show that empirical studies are useful in improving our understanding of this issue we consider two settings which represent the two types of complexity results that have been identified in this area manipulation with unweighted votes by a single agent and manipulation with weighted votes by a coalition of agents in the first case we consider single transferable voting stv and in the second case we consider veto voting stv is one of the few voting rules used in practice where it is nphard to compute how a single agent can manipulate the result when votes are unweighted it also appears one of the harder voting rules to manipulate since it involves multiple rounds on the other hand veto voting is one of the simplest representatives of voting rules where it is nphard to compute how a coalition of weighted agents can manipulate the result in our experiments we sample a number of distributions of votes including uniform correlated and real world elections in many of the elections in our experiments it was easy to compute how to manipulate the result or to prove that manipulation was impossible even when we were able to identify a situation in which manipulation was hard to compute eg when votes are highly correlated and the election is hung we found that the computational difficulty of computing manipulations was somewhat precarious eg with such hung elections even a single uncorrelated voter was enough to make manipulation easy to compute



s  esmeir and s  markovitch 2008 anytime induction of lowcost lowerror classifiers a samplingbased approach volume 33 pages 131



machine learning techniques are gaining prevalence in the production of a wide range of classifiers for complex realworld applications with nonuniform testing and misclassification costs the increasing complexity of these applications poses a real challenge to resource management during learning and classification in this work we introduce act anytime costsensitive tree learner a novel framework for operating in such complex environments act is an anytime algorithm that allows learning time to be increased in return for lower classification costs it builds a tree topdown and exploits additional time resources to obtain better estimations for the utility of the different candidate splits using sampling techniques act approximates the cost of the subtree under each candidate split and favors the one with a minimal cost as a stochastic algorithm act is expected to be able to escape local minima into which greedy methods may be trapped experiments with a variety of datasets were conducted to compare act to the stateoftheart costsensitive tree learners the results show that for the majority of domains act produces significantly less costly trees act also exhibits good anytime behavior with diminishing returns



m  l littman  j  goldsmith and   mundhenk m 1998 the computational complexity of probabilistic planning volume 9 pages 136





j  hong 2001 goal recognition through goal graph analysis volume 15 pages 130



we present a novel approach to goal recognition based on a    twostage paradigm of graph construction and analysis first a graph    structure called a goal graph is constructed to represent the observed    actions the state of the world and the achieved goals as well as    various connections between these nodes at consecutive time    steps then the goal graph is analysed at each time step to recognise    those partially or fully achieved goals that are consistent with the    actions observed so far the goal graph analysis also reveals valid    plans for the recognised goals or part of these goals        our approach to goal recognition does not need a plan library  it    does not suffer from the problems in the acquisition and handcoding    of large plan libraries neither does it have the problems in    searching the plan space of exponential size we describe two    algorithms for goal graph construction and analysis in this    paradigm these algorithms are both provably sound polynomialtime    and polynomialspace the number of goals recognised by our algorithms    is usually very small after a sequence of observed actions has been    processed thus the sequence of observed actions is well explained by    the recognised goals with little ambiguity we have evaluated these    algorithms in the unix domain in which excellent performance has been    achieved in terms of accuracy efficiency and scalability



a  artale d  calvanese r  kontchakov and m  zakharyaschev 2009 the dllite family and relations volume 36 pages 169



the recently introduced series of description logics under the common moniker dllite has attracted attention of the description logic and semantic web communities due to the low computational complexity of inference on the one hand and the ability to represent  conceptual modeling formalisms on the other  the main aim of this article is to carry out a thorough and systematic investigation of inference in extensions of the original dllite logics along five axes by i adding the boolean connectives and ii number restrictions to concept constructs iii allowing role hierarchies iv allowing role disjointness symmetry asymmetry reflexivity irreflexivity and transitivity constraints and v adopting or dropping  the unique same assumption  we analyze the combined complexity of satisfiability for the resulting logics as well as the data complexity of instance checking and answering positive existential queries  our approach is based on embedding dllite logics in suitable fragments of the onevariable firstorder logic which provides useful insights into their properties and in particular computational behavior



s  kambhampati 2000 planning graph as a dynamic csp exploiting ebl ddb and other csp search techniques in graphplan volume 12 pages 134



this paper reviews the connections between graphplans    planninggraph and the dynamic constraint satisfaction problem and    motivates the need for adapting csp search techniques to the graphplan    algorithm  it then describes how explanation based learning    dependency directed backtracking dynamic variable ordering forward    checking sticky values and randomrestart search strategies can be    adapted to graphplan empirical results are provided to demonstrate    that these augmentations improve graphplans performance significantly    up to 1000x speedups on several benchmark problems  special    attention is paid to the explanationbased learning and dependency    directed backtracking techniques as they are empirically found to be    most useful in improving the performance of graphplan



g  wang q  song h  sun x  zhang b  xu and y  zhou 2013 a feature subset selection algorithm automatic recommendation method volume 47 pages 134



many feature subset selection fss algorithms have been proposed but not all of them are appropriate for a given feature selection problem at the same time so far there is rarely a good way to choose appropriate fss algorithms for the problem at hand thus fss algorithm automatic recommendation is very important and practically useful in this paper a meta learning based fss algorithm automatic recommendation method is presented the proposed method first identifies the data sets that are most similar to the one at hand by the knearest neighbor classification algorithm and the distances among these data sets are calculated based on the commonlyused data set characteristics then it ranks all the candidate fss algorithms according to their performance on these similar data sets and chooses the algorithms with best performance as the appropriate ones the performance of the candidate fss algorithms is evaluated by a multicriteria metric that takes into account not only the classification accuracy over the selected features but also the runtime of feature selection and the number of selected features the proposed recommendation method is extensively tested on 115 real world data sets with 22 wellknown and frequentlyused different fss algorithms for five representative classifiers the results show the effectiveness of our proposed fss algorithm recommendation method



marta  r costajuss224 srinivas  bangalore patrik  lambert llu237s   m224rquez and elena  montielponsoda 2016 introduction to the special issue on crosslanguage algorithms and applications volume 55 pages 115



with the increasingly global nature of our everyday interactions the need for multilin gual technologies to support efficient and effective information access and communication cannot be overemphasized computational modeling of language has been the focus of natural language processing a subdiscipline of artificial intelligence one of the current challenges for this discipline is to design methodologies and algorithms that are cross language in order to create multilingual technologies rapidly the goal of this jair special issue on crosslanguage algorithms and applications claa is to present leading re search in this area with emphasis on developing unifying themes that could lead to the development of the science of multi and crosslingualism in this introduction we provide the reader with the motivation for this special issue and summarize the contributions of the papers that have been included the selected papers cover a broad range of crosslingual technologies including machine translation domain and language adaptation for sentiment analysis crosslanguage lexical resources dependency parsing information retrieval and knowledge representation we anticipate that this special issue will serve as an invaluable resource for researchers interested in topics of crosslingual natural language processing



gert  de cooman jasper  de bock and m225rcio  alves diniz 2015 coherent predictive inference under exchangeability with imprecise probabilities volume 52 pages 195



coherent reasoning under uncertainty can be represented in a very general manner by coherent sets of desirable gambles in a context that does not allow for indecision this leads to an approach that is mathematically equivalent to working with coherent conditional probabilities if we do allow for indecision this leads to a more general foundation for coherent impreciseprobabilistic inference in this framework and for a given finite category set coherent predictive inference under exchangeability can be represented using bernstein coherent cones of multivariate polynomials on the simplex generated by this category set this is a powerful generalisation of de finettis representation theorem allowing for both imprecision and indecision

we define an inference system as a map that associates a bernstein coherent cone of polynomials with every finite category set many inference principles encountered in the literature can then be interpreted and represented mathematically as restrictions on such maps we discuss as particular examples two important inference principles representation insensitivitya strengthened version of walleys representation invarianceand specificity we show that there is an infinity of inference systems that satisfy these two principles amongst which we discuss in particular the skeptically cautious inference system the inference systems corresponding to a modified version of walley and bernards imprecise dirichlet multinomial models idmm the skeptical idmm inference systems and the haldane inference system we also prove that the latter produces the same posterior inferences as would be obtained using haldanes improper prior implying that there is an infinity of proper priors that produce the same coherent posterior inferences as haldanes improper one finally we impose an additional inference principle that allows us to characterise uniquely the immediate predictions for the idmm inference systems



b  bidyuk and r  dechter 2007 cutset sampling for bayesian networks volume 28 pages 148



the paper presents a new sampling methodology for bayesian networks that samples only a subset of variables and applies exact inference to the rest  cutset sampling is a network structureexploiting application of the raoblackwellisation principle to sampling in bayesian networks  it improves convergence by exploiting memorybased inference algorithms  it can also be viewed as an anytime approximation of the exact cutsetconditioning algorithm developed by pearl  cutset sampling can be implemented efficiently when the sampled variables constitute a loopcutset of the bayesian network and more generally when the induced width of the networks graph conditioned on the observed sampled variables is bounded by a constant w  we demonstrate empirically the benefit of this scheme on a range of benchmarks



s  chernova and m  veloso 2009 interactive policy learning through confidencebased autonomy volume 34 pages 125



we present confidencebased autonomy cba an interactive algorithm for policy learning from demonstration  the cba algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents  the first component confident execution enables the agent to identify states in which demonstration is required to request a demonstration from the human teacher and to learn a policy based on the acquired data  the algorithm selects demonstrations based on a measure of action selection confidence and our results show that using confident execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher  the second algorithmic component corrective demonstration enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance  cba and its individual components are compared and evaluated in a complex simulated driving domain  the complete cba algorithm results in the best overall learning performance successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning



f  belardinelli and a  lomuscio 2012 interactions between knowledge and time in a firstorder logic for multiagent systems completeness results volume 45 pages 145



we investigate a class of firstorder temporalepistemic logics for reasoning about multiagent systems we encode typical properties of systems including perfect recall synchronicity no learning and having a unique initial state in terms of variants of quantified interpreted systems a firstorder extension of interpreted systems we identify several monodic fragments of firstorder temporalepistemic logic and show their completeness with respect to their corresponding classes of quantified interpreted systems



m  alabbas and a  ramsay 2013 natural language inference for arabic using extended tree edit distance with subtrees volume 48 pages 122



many natural language processing nlp applications require the computation of similarities between pairs of syntactic or semantic trees many researchers have used tree edit distance for this task but this technique suffers from the drawback that it deals with single node operations only we have extended the standard tree edit distance algorithm to deal with subtree transformation operations as well as single nodes the extended algorithm with subtree operations tedst is more effective and flexible than the standard algorithm especially for applications that pay attention to relations among nodes eg in linguistic trees deleting a modifier subtree should be cheaper than the sum of deleting its components individually we describe the use of tedst for checking entailment between two arabic text snippets the preliminary results of using tedst were encouraging when compared with two stringbased approaches and with the standard algorithm



f  heras j  larrosa and a  oliveras 2008 minimaxsat an efficient weighted maxsat solver volume 31 pages 132



in this paper we introduce minimaxsat a new maxsat solver that is built on top of minisat it incorporates the best current sat and maxsat techniques it can handle hard clausesclauses of mandatory satisfaction as in sat soft clauses clauses whose falsification is penalized by a cost as in maxsat as well as pseudoboolean objective functions and constraints  its main features are learning and  backjumping on hard clauses resolutionbased and substractionbased  lower bounding and lazy propagation with the twowatched literal scheme  our empirical evaluation comparing a wide set of solving alternatives on a broad set of optimization benchmarks indicates that the performance of minimaxsat is usually close to the best specialized alternative and in some cases even better



pannaga  shivaswamy and thorsten  joachims 2015 coactive learning volume 53 pages 140



we propose coactive learning as a model of interaction between a learning system and a human user where both have the common goal of providing results of maximum utility to the user interactions in the coactive learning model take the following form at each step the system eg search engine receives a context eg query and predicts an object eg ranking the user responds by correcting the system if necessary providing a slightly improved but not necessarily optimal object as feedback we argue that such preference feedback can be inferred in large quantity from observable user behavior eg clicks in web search unlike the optimal feedback required in the expert model or the cardinal valuations required for bandit learning despite the relaxed requirements for the feedback we show that it is possible to adapt many existing online learning algorithms to the coactive framework in particular we provide algorithms that achieve square root regret in terms of cardinal utility even though the learning algorithm never observes cardinal utility values directly we also provide an algorithm with logarithmic regret in the case of strongly convex loss functions an extensive empirical study demonstrates the applicability of our model and algorithms on a movie recommendation task as well as ranking for web search



j  y halpern 1997 defining relative likelihood in partiallyordered preferential structures volume 7 pages 124



starting with a likelihood or preference order on worlds we    extend it to a likelihood ordering on sets of worlds in a natural way    and examine the resulting logic  lewis earlier considered such a    notion of relative likelihood in the context of studying    counterfactuals but he assumed a total preference order on worlds    complications arise when examining partial orders that are not present    for total orders  there are subtleties involving the exact approach    to lifting the order on worlds to an order on sets of worlds  in    addition the axiomatization of the logic of relative likelihood in    the case of partial orders gives insight into the connection between    relative likelihood and default reasoning



f  stulp a  fedrizzi l  m246senlechner and m  beetz 2012 learning and reasoning with actionrelated places for robust mobile manipulation volume 43 pages 142



we propose the concept of actionrelated place arplace as a powerful and flexible representation of taskrelated place in the context of mobile manipulation arplace represents robot base locations not as a single position but rather as a collection of positions each with an associated probability that the manipulation action will succeed when located there arplaces are generated using a predictive model that is acquired through experiencebased learning and take into account the uncertainty the robot has about its own location and the location of the object to be manipulated

when executing the task rather than choosing one specific goal position based only on the initial knowledge about the task context the robot instantiates an arplace and bases its decisions on this arplace which is updated as new information about the task becomes available to show the advantages of this leastcommitment approach we present a transformational planner that reasons about arplaces in order to optimize symbolic plans  our empirical evaluation demonstrates that using arplaces leads to more robust and efficient mobile manipulation in the face of state estimation uncertainty on our simulated robot



t  lang and m  toussaint 2010 planning with noisy probabilistic relational rules volume 39 pages 149



noisy probabilistic relational rules are a promising world model representation for several reasons they are compact and generalize over world instantiations they are usually interpretable and they can be learned effectively from the action experiences in complex worlds we investigate reasoning with such rules in grounded relational domains our algorithms exploit the compactness of rules for efficient and flexible decisiontheoretic planning as a first approach we combine these rules with the upper confidence bounds applied to trees uct algorithm based on lookahead trees our second approach converts these rules into a structured dynamic bayesian network representation and predicts the effects of action sequences using approximate inference and beliefs over world states we evaluate the effectiveness of our approaches for planning in a simulated complex 3d robot manipulation scenario with an articulated manipulator and realistic physics and in domains of the probabilistic planning competition empirical results show that our methods can solve problems where existing methods fail



e  bruni n  k  tran and m  baroni 2014 multimodal distributional semantics volume 49 pages 147



distributional semantic models derive computational representations of word meaning from the patterns of cooccurrence of words in text such models have been a success story of computational linguistics being able to provide reliable estimates of semantic relatedness for the many semantic tasks requiring them however distributional models extract meaning information exclusively from text which is an extremely impoverished basis compared to the rich perceptual sources that ground human semantic knowledge we address the lack of perceptual grounding of distributional models by exploiting computer vision techniques that automatically identify discrete visual words in images so that the distributional representation of a word can be extended to also encompass its cooccurrence with the visual words of images it is associated with we propose a flexible architecture to integrate text and imagebased distributional information and we show in a set of empirical tests that our integrated model is superior to the purely textbased approach and it provides somewhat complementary semantic information with respect to the latter



d  long and  m  fox 2003 the 3rd international planning competition results and analysis volume 20 pages 159



this paper reports the outcome of the third in the series of biennial international planning competitions held in association with the international conference on ai planning and scheduling aips in 2002 in addition to describing the domains the planners and the objectives of the competition the paper includes analysis of the results the results are analysed from several perspectives in order to address the questions of comparative performance between planners comparative difficulty of domains the degree of agreement between planners about the relative difficulty of individual problem instances and the question of how well planners scale relative to one another over increasingly difficult problems the paper addresses these questions through statistical analysis of the raw results of the competition in order to determine which results can be considered to be adequately supported by the data the paper concludes with a discussion of some challenges for the future of the competition series



n  l zhang and  t  kocka 2004 effective dimensions of hierarchical latent class models volume 21 pages 117



hierarchical latent class hlc models are treestructured bayesian networks where leaf nodes are observed while internal nodes are latent  there are no theoretically well justified model selection criteria for hlc models in particular and bayesian networks with latent nodes in general nonetheless empirical studies suggest that the bic score is a reasonable criterion to use in practice for learning hlc models  empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced  with effective model dimension in the penalty term of the bic score  effective dimensions are difficult to compute in this paper we prove a theorem that relates the effective dimension of an hlc model to the effective dimensions of a number of latent class models  the theorem makes it computationally feasible to compute the effective dimensions of large hlc models  the theorem can also be used to compute the effective dimensions of general tree models



s  katrenko p  w adriaans and m  van someren 2010 using local alignments for relation recognition volume 38 pages 148



this paper discusses the problem of marrying structural similarity with semantic relatedness for information extraction from text aiming at accurate recognition of relations we introduce local alignment kernels and explore various possibilities of using them for this task we give a definition of a local alignment la kernel based on the smithwaterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences we show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge our experiments suggest that the la kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin additional series of experiments have been conducted on the data sets of seven general relation types where the performance of the la kernel is comparable to the current stateoftheart results



m  cadoli  f  m donini  p  liberatore and  m  schaerf 2000 space efficiency of propositional knowledge representation formalisms volume 13 pages 131



we investigate the space efficiency of a propositional    knowledge representation pkr formalism intuitively the space    efficiency of a formalism f in representing a certain piece of    knowledge a is the size of the shortest formula of f that    represents a in this paper we assume that knowledge is    either a set of propositional interpretations models or a set of    propositional formulae theorems we provide a formal way of    talking about the relative ability of pkr formalisms to compactly    represent a set of models or a set of theorems we introduce two new    compactness measures the corresponding classes and show that the    relative space efficiency of a pkr formalism in representing    modelstheorems is directly related to such classes in particular    we consider formalisms for nonmonotonic reasoning such as    circumscription and default logic as well as belief revision    operators and the stable model semantics for logic programs with    negation one interesting result is that formalisms with the same    time complexity do not necessarily belong to the same space    efficiency class



n  creignou h  daude and u   egly 2007 phase transition for random quantified xorformulas volume 29 pages 118



the qxorsat problem is the quantified version of the satisfiability problem xorsat in which the connective exclusiveor is used instead of the usual or we study the phase transition associated with random qxorsat instances we give a description of this phase transition in the case of one alternation of quantifiers thus performing an advanced practical and theoretical study on the phase transition of a quantified roblem



p  nightingale i  p gent c  jefferson and i  miguel 2013 short and long supports for constraint propagation volume 46 pages 145



specialpurpose constraint propagation algorithms frequently make implicit use of short supports  by examining a subset of the variables they can infer support a justification that a variablevalue pair may still form part of an assignment that satisfies the constraint for all other variables and values and save substantial work  but short supports have not been studied in their own right the two main contributions of this paper are the identification of short supports as important for constraint propagation and the introduction of haggisgac an efficient and effective general purpose propagation algorithm for exploiting short supports given the complexity of haggisgac we present it as an optimised version of a simpler algorithm shortgac although experiments demonstrate the efficiency of shortgac compared with other generalpurpose propagation algorithms where a compact set of short supports is available we show theoretically and experimentally that haggisgac is even better we also find that haggisgac performs better than gacschema on fulllength supports we also introduce a variant algorithm haggisgacstable which is adapted to avoid work on backtracking and in some cases can be faster and have significant reductions in memory use all the proposed algorithms are excellent for propagating disjunctions of constraints in all experiments with disjunctions we found our algorithms to be faster than constructive or and gacschema by at least an order of magnitude and up to three orders of magnitude



y  chali s  r joty and s  a hasan 2009 complex question answering unsupervised learning approaches and experiments volume 35 pages 147



complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topicoriented informative multidocument summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information in this paper we experiment with one empirical method and two unsupervised statistical machine learning techniques kmeans and expectation maximization em for computing relative importance of the sentences we compare the results of these approaches our experiments show that the empirical approach outperforms the other two techniques and em performs better than kmeans however the performance of these approaches depends entirely on the feature set used and the weighting of these features in order to measure the importance and relevance to the user query we extract different kinds of features ie lexical lexical semantic cosine similarity basic element tree kernel based syntactic and shallowsemantic for each of the document sentences we use a local search technique to learn the weights of the features to the best of our knowledge no study has used tree kernel functions to encode syntacticsemantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate queryfocused summaries or answers to complex questions for each of our methods of generating summaries ie empirical kmeans and em we show the effects of syntactic and shallowsemantic features over the bagofwords bow features



roy  barhaim ido  dagan and jonathan  berant 2015 knowledgebased textual inference via parsetree transformations volume 54 pages 157



textual inference is an important component in many applications for understanding natural language classical approaches to textual inference rely on logical representations for meaning which may be regarded as external to the natural language itself however practical applications usually adopt shallower lexical or lexicalsyntactic representations which correspond closely to language structure in many cases such approaches lack a principled meaning representation and inference framework we describe an inference formalism that operates directly on languagebased structures particularly syntactic parse trees new trees are generated by applying inference rules which provide a unified representation for varying types of inferences we use manual and automatic methods to generate these rules which cover generic linguistic structures as well as specific lexicalbased inferences  we also present a novel packed datastructure and a corresponding inference algorithm that allows efficient implementation of this formalism we proved the correctness of the new algorithm and established its efficiency analytically and empirically the utility of our approach was illustrated on two tasks unsupervised relation extraction from a large corpus and the recognizing textual entailment rte benchmarks 



n  onder g  c whelan and l  li 2006 engineering a conformant probabilistic planner volume 25 pages 115



we present a partialorder conformant probabilistic planner probapop which competed in the blind track of the probabilistic planning competition in ipc4 we explain how we adapt distance based heuristics for use with probabilistic domains probapop also incorporates heuristics based on probability of success we explain the successes and difficulties encountered during the design and implementation of probapop 



m  p wellman 1993 a marketoriented programming environment and its application to distributed multicommodity flow problems volume 1 pages 123





c  boutilier  t  dean and  s  hanks 1999 decisiontheoretic planning structural assumptions and computational leverage volume 11 pages 194



planning under uncertainty is a central problem in the study of    automated sequential decision making and has been addressed by    researchers in many different fields including ai planning decision    analysis operations research control theory and economics  while    the assumptions and perspectives adopted in these areas often differ    in substantial ways many planning problems of interest to researchers    in these fields can be modeled as markov decision processes mdps    and analyzed using the techniques of decision theory       this paper presents an overview and synthesis of mdprelated methods    showing how they provide a unifying framework for modeling many    classes of planning problems studied in ai it also describes    structural properties of mdps that when exhibited by particular    classes of problems can be exploited in the construction of optimal    or approximately optimal policies or plans  planning problems    commonly possess structure in the reward and value functions used to    describe performance criteria in the functions used to describe state    transitions and observations and in the relationships among features    used to describe states actions rewards and observations       specialized representations and algorithms employing these    representations can achieve computational leverage by exploiting    these various forms of structure  certain ai techniques  in    particular those based on the use of structured intensional    representations  can be viewed in this way  this paper surveys    several types of representations for both classical and    decisiontheoretic planning problems and planning algorithms that    exploit these representations in a number of different ways to ease    the computational burden of constructing policies or plans  it focuses    primarily on abstraction aggregation and decomposition techniques    based on aistyle representations



s  k murthy  s  kasif and  s  salzberg 1994 a system for induction of oblique decision trees volume 2 pages 132



this article describes a new system for induction ofoblique   decision trees  this system oc1 combines deterministic   hillclimbing with two forms of randomization to find a goodoblique   split in the form of a hyperplane at each node of a decisiontree   oblique decision tree methods are tuned especially for domains in   which the attributes are numeric although they can be adapted to   symbolic or mixed symbolicnumeric attributes  we presentextensive   empirical studies using both real and artificial data thatanalyze   oc1s ability to construct oblique trees that are smaller and more   accurate than their axisparallel counterparts  we also examinethe   benefits of randomization for the construction of oblique decisiontrees



f  baader  c  lutz  h  sturm and  f  wolter 2002 fusions of description logics and abstract description systems volume 16 pages 158



fusions are a simple way of combining logics for normal    modal logics fusions have been investigated in detail in particular    it is known that under certain conditions decidability transfers    from the component logics to their fusion  though description logics    are closely related to modal logics they are not necessarily    normal in addition abox reasoning in description logics is not    covered by the results from modal logics           in this paper we extend the decidability transfer results from normal    modal logics to a large class of description logics to cover    different description logics in a uniform way we introduce abstract    description systems which can be seen as a common generalization of    description and modal logics and show the transfer results in this    general setting



a  e howe and  e  dahlman 2002 a critical assessment of benchmark comparison in planning volume 17 pages 133



recent trends in planning research have led to empirical     comparison becoming commonplace the field has started to settle into    a methodology for such comparisons which for obvious practical    reasons requires running a subset of planners on a subset of    problems  in this paper we characterize the methodology and    examine eight implicit assumptions about the problems planners and    metrics used in many of these comparisons the problem assumptions    are pr1 the performance of a general purpose planner should not be    penalizedbiased if executed on a sampling of problems and domains    pr2 minor syntactic differences in representation do not affect    performance and pr3 problems should be solvable by strips capable    planners unless they require adl the planner assumptions are pl1    the latest version of a planner is the best one to use pl2 default    parameter settings approximate good performance and pl3 time    cutoffs do not unduly bias outcome the metrics assumptions are    m1 performance degrades similarly for each planner when run on    degraded runtime environments eg machine platform and m2 the    number of plan steps distinguishes performance we find that most of    these assumptions are not supported empirically in particular that    planners are affected differently by these assumptions we conclude    with a call to the community to devote research resources to    improving the state of the practice and especially to enhancing the    available benchmark problems



m  milani fard and j  pineau 2011 nondeterministic policies in markovian decision processes volume 40 pages 124



markovian processes have long been used to model stochastic environments reinforcement learning has emerged as a framework to solve sequential planning and decisionmaking problems in such environments in recent years attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in markovian environments although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decisionmaking they cannot be applied in their current form to decision support systems such as those in medical domains as they suggest policies that are often highly prescriptive and leave little room for the users input without the ability to provide flexible guidelines it is unlikely that these methods can gain ground with users of such systems

this paper introduces the new concept of nondeterministic policies to allow more flexibility in the users decisionmaking process while constraining decisions to remain near optimal solutions we provide two algorithms to compute nondeterministic policies in discrete domains we study the output and running time of these method on a set of synthetic and realworld problems in an experiment with human subjects we show that humans assisted by hints based on nondeterministic policies outperform both humanonly and computeronly agents in a web navigation task



r  j mooney and  m  e califf 1995 induction of firstorder decision lists results on learning the past tense of english verbs volume 3 pages 124



this paper presents a method for inducing logic programs    from examples that learns a new class of concepts called firstorder    decision lists defined as ordered lists of clauses each ending in a    cut  the method called foidl is based on foil quinlan 1990 but    employs intensional background knowledge and avoids the need for    explicit negative examples  it is particularly useful for problems    that involve rules with specific exceptions such as learning the    pasttense of english verbs a task widely studied in the context of    the symbolicconnectionist debate  foidl is able to learn concise    accurate programs for this problem from significantly fewer examples    than previous methods both connectionist and symbolic



r  micalizio and p  torasso 2014 cooperative monitoring to diagnose multiagent plans volume 51 pages 170



diagnosing the execution of a multiagent plan map means identifying and explaining action failures ie actions that did not reach their expected effects current approaches to map diagnosis are substantially centralized and assume that action failures are independent of each other

an experimental analysis demonstrates that the cwcm methodology together with the proposed diagnostic inferences are effective in identifying and explaining action failures even in scenarios where the system observability is significantly reduced



k  yip and  f  zhao 1996 spatial aggregation theory and applications volume 5 pages 126



visual thinking plays an important role in scientific    reasoning  based on the research in automating diverse reasoning    tasks about dynamical systems nonlinear controllers kinematic    mechanisms and fluid motion we have identified a style of visual    thinking imagistic reasoning  imagistic reasoning organizes    computations around imagelike analogue representations so that    perceptual and symbolic operations can be brought to bear to infer    structure and behavior  programs incorporating imagistic reasoning    have been shown to perform at an expert level in domains that defy    current analytic or numerical methods        we have developed a computational paradigm spatial aggregation to    unify the description of a class of imagistic problem solvers  a    program written in this paradigm has the following properties  it    takes a continuous field and optional objective functions as input    and produces highlevel descriptions of structure behavior or    control actions it computes a multilayer of intermediate    representations called spatial aggregates by forming equivalence    classes and adjacency relations  it employs a small set of generic    operators such as aggregation classification and localization to    perform bidirectional mapping between the informationrich field and    successively more abstract spatial aggregates it uses a data    structure the neighborhood graph as a common interface to modularize    computations  to illustrate our theory we describe the computational    structure of three implemented problem solvers  kam maps and    hipair  in terms of the spatial aggregation generic operators by    mixing and matching a library of commonly used routines



m  zhang x  xiao d  xiong and q  liu 2014 topicbased dissimilarity and sensitivity models for translation rule selection volume 50 pages 130



translation rule selection is a task of selecting appropriate translation rules for an ambiguous sourcelanguage segment as translation ambiguities are pervasive in statistical machine translation we introduce two topicbased models for translation rule selection which incorporates global topic information into translation disambiguation we associate each synchronous translation rule with source and targetside topic distributionswith these topic distributions we propose a topic dissimilarity model to select desirable less dissimilar rules by imposing penalties for rules with a large value of dissimilarity of their topic distributions to those of given documents in order to encourage the use of nontopic specific translation rules we also present a topic sensitivity model to balance translation rule selection between generic rules and topicspecific rules furthermore we project targetside topic distributions onto the sourceside topic model space so that we can benefit from topic information of both the source and target language we integrate the proposed topic dissimilarity and sensitivity model into hierarchical phrasebased machine translation for synchronous translation rule selection experiments show that our topicbased translation rule selection model can substantially improve translation quality



e  manisterski d  sarne and s  kraus 2008 cooperative search with concurrent interactions volume 32 pages 136



in this paper we show how taking advantage of autonomous agents capability to maintain parallel interactions with others and incorporating it into the cooperative economic search model results in a new search strategy which outperforms current strategies in use as a framework for our analysis we use the electronic marketplace where buyer agents have the incentive to search cooperatively the new search technique is quite intuitive however its analysis and the process of extracting the optimal search strategy are associated with several significant complexities these difficulties are derived mainly from the unbounded search space and simultaneous dual affects of decisions taken along the search we provide a comprehensive analysis of the model highlighting demonstrating and proving important characteristics of the optimal search strategy consequently we manage to come up with an efficient modular algorithm for extracting the optimal cooperative search strategy for any given environment a computational based comparative illustration of the system performance using the new search technique versus the traditional methods is given emphasizing the main differences in the optimal strategys structure and the advantage of using the proposed model



j  engelfriet 1998 monotonicity and persistence in preferential logics volume 8 pages 121



an important characteristic of many logics for artificial    intelligence is their nonmonotonicity this means that adding a    formula to the premises can invalidate some of the consequences there    may however exist formulae that can always be safely added to the    premises without destroying any of the consequences we say they    respect monotonicity also there may be formulae that when they are    a consequence can not be invalidated when adding any formula to the    premises we call them conservative we study these two classes of    formulae for preferential logics and show that they are closely    linked to the formulae whose truthvalue is preserved along the    preferential ordering we will consider some preferential logics for    illustration and prove syntactic characterization results for them    the results in this paper may improve the efficiency of theorem    provers for preferential logics



p  van beek and  d  w manchak 1996 the design and experimental analysis of algorithms for temporal reasoning volume 4 pages 118



many applications  from planning and scheduling to    problems in molecular biology  rely heavily on a temporal reasoning    component  in this paper we discuss the design and empirical    analysis of algorithms for a temporal reasoning system based on    allens influential intervalbased framework for representing temporal    information  at the core of the system are algorithms for determining    whether the temporal information is consistent and if so finding    one or more scenarios that are consistent with the temporal    information  two important algorithms for these tasks are a path    consistency algorithm and a backtracking algorithm  for the path    consistency algorithm we develop techniques that can result in up to    a tenfold speedup over an already highly optimized implementation    for the backtracking algorithm we develop variable and value ordering    heuristics that are shown empirically to dramatically improve the    performance of the algorithm  as well we show that a previously    suggested reformulation of the backtracking search problem can reduce    the time and space requirements of the backtracking search  taken    together the techniques we develop allow a temporal reasoning    component to solve problems that are of practical size



b  cseke and t  heskes 2011 properties of bethe free energies and message passing in gaussian models volume 41 pages 124



we address the problem of computing approximate marginals in gaussian probabilistic models by using mean field and fractional bethe approximations we define the gaussian fractional bethe free energy in terms of the moment parameters of the approximate marginals derive a lower and an upper bound on the fractional bethe free energy and establish a necessary condition for the lower bound to be bounded from below it turns out that the condition is identical to the pairwise normalizability condition which is known to be a sufficient condition for the convergence of the message passing algorithm we show that stable fixed points of the gaussian message passing algorithm are local minima of the gaussian bethe free energy by a counterexample we disprove the conjecture stating that the unboundedness of the free energy implies the divergence of the message passing algorithm



r  i brafman 2001 on reachability relevance and resolution in the planning as satisfiability approach volume 14 pages 128



in recent years there is a growing awareness of the    importance of reachability and relevancebased pruning techniques for    planning but little work specifically targets these techniques in    this paper we compare the ability of two classes of algorithms to    propagate and discover reachability and relevance constraints in    classical planning problems the first class of algorithms operates on    sat encoded planning problems obtained using the linear and graphplan    encoding schemes  it applies unitpropagation and more general    resolution steps involving larger clauses to these plan encodings    the second class operates at the plan level and contains two families    of pruning algorithms reachablek and relevantk  reachablek    provides a coherent description of a number of existing forward    pruning techniques used in numerous algorithms while relevantk    captures different grades of backward pruning  our results shed light    on the ability of different planencoding schemes to propagate    information forward and backward and on the relative merit of    planlevel and satlevel pruning methods



a  j coles a  i coles m  fox and d  long 2012 colin planning with continuous linear numeric change volume 44 pages 196



in this paper we describe colin a forwardchaining heuristic search planner capable of reasoning with continuous linear numeric change in addition to the full temporal semantics of pddl  through this work we make two advances to the stateoftheart in terms of expressive reasoning capabilities of planners the handling of continuous linear change and the handling of durationdependent effects in combination with duration inequalities both of which require tightly coupled temporal and numeric reasoning during planning  colin combines ffstyle forward chaining search with the use of a linear program lp to check the consistency of the interacting temporal and numeric constraints at each state  the lp is used to compute bounds on the values of variables in each state reducing the range of actions that need to be considered for application  in addition we develop an extension of the temporal relaxed planning graph heuristic of crikey3 to support reasoning directly with continuous change  we extend the range of task variables considered to be suitable candidates for specifying the gradient of the continuous numeric change effected by an action  finally we explore the potential for employing mixed integer programming as a tool for optimising the timestamps of the actions in the plan once a solution has been found to support this we further contribute a selection of extended benchmark domains that include continuous numeric effects  we present results for colin that demonstrate its scalability on a range of benchmarks and compare to existing stateoftheart planners 



j  y halpern and r  pucella 2006 a logic for reasoning about evidence volume 26 pages 134



we introduce a logic for reasoning about evidence that essentially



b  zanuttini 2003 new polynomial classes for logicbased abduction volume 19 pages 110



we address the problem of propositional logicbased    abduction ie the problem of searching for a best explanation for a    given propositional observation according to a given propositional    knowledge base we give a general algorithm based on the notion of    projection then we study restrictions over the representations of the    knowledge base and of the query and find new polynomial classes of    abduction problems



d  r wilson and  t  r martinez 1997 improved heterogeneous distance functions volume 6 pages 134



instancebased learning techniques typically handle    continuous and linear input values well but often do not handle    nominal input attributes appropriately  the value difference metric    vdm was designed to find reasonable distance values between nominal    attribute values but it largely ignores continuous attributes    requiring discretization to map continuous values into nominal values    this paper proposes three new heterogeneous distance functions called    the heterogeneous value difference metric hvdm the interpolated    value difference metric ivdm and the windowed value difference    metric wvdm  these new distance functions are designed to handle    applications with nominal attributes continuous attributes or both    in experiments on 48 applications the new distance metrics achieve    higher classification accuracy on average than three previous distance    functions on those datasets that have both nominal and continuous    attributes



m  j carman and c  a knoblock 2007 learning semantic definitions of online information sources volume 30 pages 150



the internet contains a very large number of information sources providing many types of data from weather forecasts to travel deals and financial information these sources can be accessed via webforms web services rss feeds and so on in order to make automated use of these sources we need to model them semantically but writing semantic descriptions for web services is both tedious and error prone in this paper we investigate the problem of automatically generating such models we introduce a framework for learning datalog definitions of web sources in order to learn these definitions our system actively invokes the sources and compares the data they produce with that of known sources of information it then performs an inductive logic search through the space of plausible source definitions in order to learn the best possible semantic model for each new source in this paper we perform an empirical evaluation of the system using realworld web sources the evaluation demonstrates the effectiveness of the approach showing that we can automatically learn complex models for real sources in reasonable time we also compare our system with a complex schema matching system showing that our approach can handle the kinds of problems tackled by the latter



g  tsatsaronis i  varlamis and m  vazirgiannis 2010 text relatedness based on a word thesaurus volume 37 pages 139



the computation of relatedness between two fragments of text in an automated manner requires taking into account a wide range of factors pertaining to the meaning the two fragments convey and the pairwise relations between their words without doubt a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words such a measure that captures well both aspects of text relatedness may help in many tasks such as text retrieval classification and clustering in this paper we present a new approach for measuring the semantic relatedness between words based on their implicit semantic links the approach exploits only a word thesaurus in order to devise implicit semantic links between words based on this approach we introduce omiotis a new measure of semantic relatedness between texts which capitalizes on the wordtoword semantic relatedness measure sr and extends it to measure the relatedness between texts we gradually validate our method we first evaluate the performance of the semantic relatedness measure between individual words covering wordtoword similarity and relatedness synonym identification and word analogy then we proceed with evaluating the performance of our method in measuring texttotext semantic relatedness in two tasks namely sentencetosentence similarity and paraphrase recognition experimental evaluation shows that the proposed method outperforms every lexiconbased method of semantic relatedness in the selected tasks and the used data sets and competes well against corpusbased and hybrid approaches 



n  roy  g  gordon and  s  thrun 2005 finding approximate pomdp solutions through belief compression volume 23 pages 140



standard value function approaches to finding policies for partially observable markov decision processes pomdps are generally considered to be intractable for large models the intractability of these algorithms is to a large extent a consequence of computing an exact optimal policy over the entire belief space  however in realworld pomdp problems computing the optimal policy for the full belief space is often unnecessary for good control even for problems with complicated policy classes the beliefs experienced by the controller often lie near a structured lowdimensional subspace embedded in the highdimensional belief space finding a good approximation to the optimal value function for only this subspace can be much easier than computing the full value functionp p  we introduce a new method for solving largescale pomdps by reducing the dimensionality of the belief space we use exponential family principal components analysis collins dasgupta  schapire 2002 to represent sparse highdimensional belief spaces using small sets of learned features of the belief state we then plan only in terms of the lowdimensional belief features by planning in this lowdimensional space we can find policies for pomdp models that are orders of magnitude larger than models that can be handled by conventional techniquesp p we demonstrate the use of this algorithm on a synthetic problem and on mobile robot navigation tasks



simone  villa and fabio  stella 2016 learning continuous time bayesian networks in nonstationary domains volume 57 pages 137



nonstationary continuous time bayesian networks are introduced they allow the parents set of each node to change over continuous time three settings are developed for learning nonstationary continuous time bayesian networks from data known transition times known number of epochs and unknown number of epochs a score function for each setting is derived and the corresponding learning algorithm is developed a set of numerical experiments on synthetic data is used to compare the effectiveness of nonstationary continuous time bayesian networks to that of nonstationary dynamic bayesian networks furthermore the performance achieved by nonstationary continuous time bayesian networks is compared to that achieved by stateoftheart algorithms on four realworld datasets namely drosophila saccharomyces cerevisiae songbird and macroeconomics 



d  cohen  m  cooper  p  jeavons and  a  krokhin 2004 a maximal tractable class of soft constraints volume 22 pages 122



many researchers in artificial intelligence are beginning to explore the use of soft constraints to express a set of possibly conflicting problem  requirements a soft constraint is a function defined on a collection of  variables which associates some measure of desirability with each possible  combination of values for those variables however the crucial question of  the computational complexity of finding the optimal solution to a collection of soft constraints has so far received very little attention in this paper we identify a class of soft binary constraints for which the problem of  finding the optimal solution is tractable in other words we show that for  any given set of such constraints there exists a polynomial time algorithm  to determine the assignment having the best overall combined measure of  desirability this tractable class includes many commonlyoccurring soft constraints such as as near as possible or as soon as possible after as well as crisp constraints such as greater than finally we show that  this tractable class is maximal in the sense that adding any other form of  soft binary constraint which is not in the class gives rise to a class of  problems which is nphard



d  geiger c  meek and y  wexler 2006 a variational inference procedure allowing internal structure for overlapping clusters and deterministic constraints volume 27 pages 123



we develop a novel algorithm called vip for structured variational approximate inference this algorithm extends known algorithms to allow efficient multiple potential updates for overlapping clusters and overcomes the difficulties imposed by deterministic constraints the algorithms convergence is proven and its applicability demonstrated  for genetic linkage analysis



d  ortizboyer  c  herv225smart237nez and  n  garc237apedrajas 2005 cixl2 a crossover operator for evolutionary algorithms based on population features volume 24 pages 148



in this paper we propose a crossover operator for evolutionary algorithms with real values that is based on the statistical theory of population distributions the operator is based on the theoretical distribution of the values of the genes of the best individuals in the population  the proposed operator takes into account the localization and dispersion features of the best individuals of the population with the objective that these features would be inherited by the offspring our aim is the optimization of the balance between exploration and exploitation in the search process    in order to test the efficiency and robustness of this crossover we have used a set of functions to be optimized with regard to different criteria such as multimodality separability regularity and epistasis with this set of functions we can extract conclusions in function of the problem at hand we analyze the results using anova and multiple comparison statistical tests  as an example of how our crossover can be used to solve artificial intelligence problems we have applied the proposed model to the problem of obtaining the weight of each network in a ensemble of neural networks the results obtained are above the performance of standard methods



c  thompson 2003 acquiring wordmeaning mappings for natural language interfaces volume 18 pages 144



this paper focuses on a system wolfie word learning from    interpreted examples that acquires a semantic lexicon from a corpus    of sentences paired with semantic representations  the lexicon    learned consists of phrases paired with meaning representations    wolfie is part of an integrated system that learns to transform    sentences into representations such as logical database queries        experimental results are presented demonstrating wolfies ability to    learn useful lexicons for a database interface in four different    natural languages  the usefulness of the lexicons learned by wolfie    are compared to those acquired by a similar system with results    favorable to wolfie  a second set of experiments demonstrates    wolfies ability to scale to larger and more difficult albeit    artificially generated corpora           in natural language acquisition it is difficult to gather the    annotated data needed for supervised learning however unannotated    data is fairly plentiful  active learning methods attempt to select    for annotation and training only the most informative examples and    therefore are potentially very useful in natural language    applications  however most results to date for active learning have    only considered standard classification tasks  to reduce annotation    effort while maintaining accuracy we apply active learning to    semantic lexicons  we show that active learning can significantly    reduce the number of annotated examples required to achieve a given    level of performance



e  davis 1999 order of magnitude comparisons of distance volume 10 pages 138



order of magnitude reasoning  reasoning by rough    comparisons of the sizes of quantities  is often called back of    the envelope calculation with the implication that the calculations    are quick though approximate  this paper exhibits an interesting    class of constraint sets in which order of magnitude reasoning is    demonstrably fast  specifically we present a polynomialtime    algorithm that can solve a set of constraints of the form points a    and b are much closer together than points c and d  we prove that    this algorithm can be applied if much closer together is    interpreted either as referring to an infinite difference in scale or    as referring to a finite difference in scale as long as the    difference in scale is greater than the number of variables in the    constraint set  we also prove that the firstorder theory over such    constraints is decidable



franz  baader meghyn  bienvenu carsten  lutz and frank  wolter 2016 query and predicate emptiness in ontologybased data access volume 56 pages 159



in ontologybased data access obda database querying is enriched with an ontology that provides domain knowledge and additional vocabulary for query formulation we identify query emptiness and predicate emptiness as two central reasoning services in this context query emptiness asks whether a given query has an empty answer over all databases formulated in a given vocabulary predicate emptiness is defined analogously but quantifies universally over all queries that contain a given predicate in this paper we determine the computational complexity of query emptiness and predicate emptiness in the el dllite and alcfamilies of description logics investigate the connection to ontology modules and perform a practical case study to evaluate the new reasoning services



t  walsh 2011 where are the hard manipulation problems volume 42 pages 129



voting is a simple mechanism to combine together the preferences of multiple agents unfortunately agents may try to manipulate the result by misreporting their preferences one barrier that might exist to such manipulation is computational complexity in particular it has been shown that it is nphard to compute how to manipulate a number of different voting rules how ever nphardness only bounds the worstcase complexity recent theoretical results suggest that manipulation may often be easy in practice in this paper we show that empirical studies are useful in improving our understanding of this issue we consider two settings which represent the two types of complexity results that have been identified in this area manipulation with unweighted votes by a single agent and manipulation with weighted votes by a coalition of agents in the first case we consider single transferable voting stv and in the second case we consider veto voting stv is one of the few voting rules used in practice where it is nphard to compute how a single agent can manipulate the result when votes are unweighted it also appears one of the harder voting rules to manipulate since it involves multiple rounds on the other hand veto voting is one of the simplest representatives of voting rules where it is nphard to compute how a coalition of weighted agents can manipulate the result in our experiments we sample a number of distributions of votes including uniform correlated and real world elections in many of the elections in our experiments it was easy to compute how to manipulate the result or to prove that manipulation was impossible even when we were able to identify a situation in which manipulation was hard to compute eg when votes are highly correlated and the election is hung we found that the computational difficulty of computing manipulations was somewhat precarious eg with such hung elections even a single uncorrelated voter was enough to make manipulation easy to compute



s  esmeir and s  markovitch 2008 anytime induction of lowcost lowerror classifiers a samplingbased approach volume 33 pages 131



machine learning techniques are gaining prevalence in the production of a wide range of classifiers for complex realworld applications with nonuniform testing and misclassification costs the increasing complexity of these applications poses a real challenge to resource management during learning and classification in this work we introduce act anytime costsensitive tree learner a novel framework for operating in such complex environments act is an anytime algorithm that allows learning time to be increased in return for lower classification costs it builds a tree topdown and exploits additional time resources to obtain better estimations for the utility of the different candidate splits using sampling techniques act approximates the cost of the subtree under each candidate split and favors the one with a minimal cost as a stochastic algorithm act is expected to be able to escape local minima into which greedy methods may be trapped experiments with a variety of datasets were conducted to compare act to the stateoftheart costsensitive tree learners the results show that for the majority of domains act produces significantly less costly trees act also exhibits good anytime behavior with diminishing returns



m  l littman  j  goldsmith and   mundhenk m 1998 the computational complexity of probabilistic planning volume 9 pages 136





j  hong 2001 goal recognition through goal graph analysis volume 15 pages 130



we present a novel approach to goal recognition based on a    twostage paradigm of graph construction and analysis first a graph    structure called a goal graph is constructed to represent the observed    actions the state of the world and the achieved goals as well as    various connections between these nodes at consecutive time    steps then the goal graph is analysed at each time step to recognise    those partially or fully achieved goals that are consistent with the    actions observed so far the goal graph analysis also reveals valid    plans for the recognised goals or part of these goals        our approach to goal recognition does not need a plan library  it    does not suffer from the problems in the acquisition and handcoding    of large plan libraries neither does it have the problems in    searching the plan space of exponential size we describe two    algorithms for goal graph construction and analysis in this    paradigm these algorithms are both provably sound polynomialtime    and polynomialspace the number of goals recognised by our algorithms    is usually very small after a sequence of observed actions has been    processed thus the sequence of observed actions is well explained by    the recognised goals with little ambiguity we have evaluated these    algorithms in the unix domain in which excellent performance has been    achieved in terms of accuracy efficiency and scalability



a  artale d  calvanese r  kontchakov and m  zakharyaschev 2009 the dllite family and relations volume 36 pages 169



the recently introduced series of description logics under the common moniker dllite has attracted attention of the description logic and semantic web communities due to the low computational complexity of inference on the one hand and the ability to represent  conceptual modeling formalisms on the other  the main aim of this article is to carry out a thorough and systematic investigation of inference in extensions of the original dllite logics along five axes by i adding the boolean connectives and ii number restrictions to concept constructs iii allowing role hierarchies iv allowing role disjointness symmetry asymmetry reflexivity irreflexivity and transitivity constraints and v adopting or dropping  the unique same assumption  we analyze the combined complexity of satisfiability for the resulting logics as well as the data complexity of instance checking and answering positive existential queries  our approach is based on embedding dllite logics in suitable fragments of the onevariable firstorder logic which provides useful insights into their properties and in particular computational behavior



s  kambhampati 2000 planning graph as a dynamic csp exploiting ebl ddb and other csp search techniques in graphplan volume 12 pages 134



this paper reviews the connections between graphplans    planninggraph and the dynamic constraint satisfaction problem and    motivates the need for adapting csp search techniques to the graphplan    algorithm  it then describes how explanation based learning    dependency directed backtracking dynamic variable ordering forward    checking sticky values and randomrestart search strategies can be    adapted to graphplan empirical results are provided to demonstrate    that these augmentations improve graphplans performance significantly    up to 1000x speedups on several benchmark problems  special    attention is paid to the explanationbased learning and dependency    directed backtracking techniques as they are empirically found to be    most useful in improving the performance of graphplan



g  wang q  song h  sun x  zhang b  xu and y  zhou 2013 a feature subset selection algorithm automatic recommendation method volume 47 pages 134



many feature subset selection fss algorithms have been proposed but not all of them are appropriate for a given feature selection problem at the same time so far there is rarely a good way to choose appropriate fss algorithms for the problem at hand thus fss algorithm automatic recommendation is very important and practically useful in this paper a meta learning based fss algorithm automatic recommendation method is presented the proposed method first identifies the data sets that are most similar to the one at hand by the knearest neighbor classification algorithm and the distances among these data sets are calculated based on the commonlyused data set characteristics then it ranks all the candidate fss algorithms according to their performance on these similar data sets and chooses the algorithms with best performance as the appropriate ones the performance of the candidate fss algorithms is evaluated by a multicriteria metric that takes into account not only the classification accuracy over the selected features but also the runtime of feature selection and the number of selected features the proposed recommendation method is extensively tested on 115 real world data sets with 22 wellknown and frequentlyused different fss algorithms for five representative classifiers the results show the effectiveness of our proposed fss algorithm recommendation method



marta  r costajuss224 srinivas  bangalore patrik  lambert llu237s   m224rquez and elena  montielponsoda 2016 introduction to the special issue on crosslanguage algorithms and applications volume 55 pages 115



with the increasingly global nature of our everyday interactions the need for multilin gual technologies to support efficient and effective information access and communication cannot be overemphasized computational modeling of language has been the focus of natural language processing a subdiscipline of artificial intelligence one of the current challenges for this discipline is to design methodologies and algorithms that are cross language in order to create multilingual technologies rapidly the goal of this jair special issue on crosslanguage algorithms and applications claa is to present leading re search in this area with emphasis on developing unifying themes that could lead to the development of the science of multi and crosslingualism in this introduction we provide the reader with the motivation for this special issue and summarize the contributions of the papers that have been included the selected papers cover a broad range of crosslingual technologies including machine translation domain and language adaptation for sentiment analysis crosslanguage lexical resources dependency parsing information retrieval and knowledge representation we anticipate that this special issue will serve as an invaluable resource for researchers interested in topics of crosslingual natural language processing



gert  de cooman jasper  de bock and m225rcio  alves diniz 2015 coherent predictive inference under exchangeability with imprecise probabilities volume 52 pages 195



coherent reasoning under uncertainty can be represented in a very general manner by coherent sets of desirable gambles in a context that does not allow for indecision this leads to an approach that is mathematically equivalent to working with coherent conditional probabilities if we do allow for indecision this leads to a more general foundation for coherent impreciseprobabilistic inference in this framework and for a given finite category set coherent predictive inference under exchangeability can be represented using bernstein coherent cones of multivariate polynomials on the simplex generated by this category set this is a powerful generalisation of de finettis representation theorem allowing for both imprecision and indecision

we define an inference system as a map that associates a bernstein coherent cone of polynomials with every finite category set many inference principles encountered in the literature can then be interpreted and represented mathematically as restrictions on such maps we discuss as particular examples two important inference principles representation insensitivitya strengthened version of walleys representation invarianceand specificity we show that there is an infinity of inference systems that satisfy these two principles amongst which we discuss in particular the skeptically cautious inference system the inference systems corresponding to a modified version of walley and bernards imprecise dirichlet multinomial models idmm the skeptical idmm inference systems and the haldane inference system we also prove that the latter produces the same posterior inferences as would be obtained using haldanes improper prior implying that there is an infinity of proper priors that produce the same coherent posterior inferences as haldanes improper one finally we impose an additional inference principle that allows us to characterise uniquely the immediate predictions for the idmm inference systems



b  bidyuk and r  dechter 2007 cutset sampling for bayesian networks volume 28 pages 148



the paper presents a new sampling methodology for bayesian networks that samples only a subset of variables and applies exact inference to the rest  cutset sampling is a network structureexploiting application of the raoblackwellisation principle to sampling in bayesian networks  it improves convergence by exploiting memorybased inference algorithms  it can also be viewed as an anytime approximation of the exact cutsetconditioning algorithm developed by pearl  cutset sampling can be implemented efficiently when the sampled variables constitute a loopcutset of the bayesian network and more generally when the induced width of the networks graph conditioned on the observed sampled variables is bounded by a constant w  we demonstrate empirically the benefit of this scheme on a range of benchmarks



s  chernova and m  veloso 2009 interactive policy learning through confidencebased autonomy volume 34 pages 125



we present confidencebased autonomy cba an interactive algorithm for policy learning from demonstration  the cba algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents  the first component confident execution enables the agent to identify states in which demonstration is required to request a demonstration from the human teacher and to learn a policy based on the acquired data  the algorithm selects demonstrations based on a measure of action selection confidence and our results show that using confident execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher  the second algorithmic component corrective demonstration enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance  cba and its individual components are compared and evaluated in a complex simulated driving domain  the complete cba algorithm results in the best overall learning performance successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning



f  belardinelli and a  lomuscio 2012 interactions between knowledge and time in a firstorder logic for multiagent systems completeness results volume 45 pages 145



we investigate a class of firstorder temporalepistemic logics for reasoning about multiagent systems we encode typical properties of systems including perfect recall synchronicity no learning and having a unique initial state in terms of variants of quantified interpreted systems a firstorder extension of interpreted systems we identify several monodic fragments of firstorder temporalepistemic logic and show their completeness with respect to their corresponding classes of quantified interpreted systems



m  alabbas and a  ramsay 2013 natural language inference for arabic using extended tree edit distance with subtrees volume 48 pages 122



many natural language processing nlp applications require the computation of similarities between pairs of syntactic or semantic trees many researchers have used tree edit distance for this task but this technique suffers from the drawback that it deals with single node operations only we have extended the standard tree edit distance algorithm to deal with subtree transformation operations as well as single nodes the extended algorithm with subtree operations tedst is more effective and flexible than the standard algorithm especially for applications that pay attention to relations among nodes eg in linguistic trees deleting a modifier subtree should be cheaper than the sum of deleting its components individually we describe the use of tedst for checking entailment between two arabic text snippets the preliminary results of using tedst were encouraging when compared with two stringbased approaches and with the standard algorithm



f  heras j  larrosa and a  oliveras 2008 minimaxsat an efficient weighted maxsat solver volume 31 pages 132



in this paper we introduce minimaxsat a new maxsat solver that is built on top of minisat it incorporates the best current sat and maxsat techniques it can handle hard clausesclauses of mandatory satisfaction as in sat soft clauses clauses whose falsification is penalized by a cost as in maxsat as well as pseudoboolean objective functions and constraints  its main features are learning and  backjumping on hard clauses resolutionbased and substractionbased  lower bounding and lazy propagation with the twowatched literal scheme  our empirical evaluation comparing a wide set of solving alternatives on a broad set of optimization benchmarks indicates that the performance of minimaxsat is usually close to the best specialized alternative and in some cases even better



pannaga  shivaswamy and thorsten  joachims 2015 coactive learning volume 53 pages 140



we propose coactive learning as a model of interaction between a learning system and a human user where both have the common goal of providing results of maximum utility to the user interactions in the coactive learning model take the following form at each step the system eg search engine receives a context eg query and predicts an object eg ranking the user responds by correcting the system if necessary providing a slightly improved but not necessarily optimal object as feedback we argue that such preference feedback can be inferred in large quantity from observable user behavior eg clicks in web search unlike the optimal feedback required in the expert model or the cardinal valuations required for bandit learning despite the relaxed requirements for the feedback we show that it is possible to adapt many existing online learning algorithms to the coactive framework in particular we provide algorithms that achieve square root regret in terms of cardinal utility even though the learning algorithm never observes cardinal utility values directly we also provide an algorithm with logarithmic regret in the case of strongly convex loss functions an extensive empirical study demonstrates the applicability of our model and algorithms on a movie recommendation task as well as ranking for web search



j  y halpern 1997 defining relative likelihood in partiallyordered preferential structures volume 7 pages 124



starting with a likelihood or preference order on worlds we    extend it to a likelihood ordering on sets of worlds in a natural way    and examine the resulting logic  lewis earlier considered such a    notion of relative likelihood in the context of studying    counterfactuals but he assumed a total preference order on worlds    complications arise when examining partial orders that are not present    for total orders  there are subtleties involving the exact approach    to lifting the order on worlds to an order on sets of worlds  in    addition the axiomatization of the logic of relative likelihood in    the case of partial orders gives insight into the connection between    relative likelihood and default reasoning



f  stulp a  fedrizzi l  m246senlechner and m  beetz 2012 learning and reasoning with actionrelated places for robust mobile manipulation volume 43 pages 142



we propose the concept of actionrelated place arplace as a powerful and flexible representation of taskrelated place in the context of mobile manipulation arplace represents robot base locations not as a single position but rather as a collection of positions each with an associated probability that the manipulation action will succeed when located there arplaces are generated using a predictive model that is acquired through experiencebased learning and take into account the uncertainty the robot has about its own location and the location of the object to be manipulated

when executing the task rather than choosing one specific goal position based only on the initial knowledge about the task context the robot instantiates an arplace and bases its decisions on this arplace which is updated as new information about the task becomes available to show the advantages of this leastcommitment approach we present a transformational planner that reasons about arplaces in order to optimize symbolic plans  our empirical evaluation demonstrates that using arplaces leads to more robust and efficient mobile manipulation in the face of state estimation uncertainty on our simulated robot



t  lang and m  toussaint 2010 planning with noisy probabilistic relational rules volume 39 pages 149



noisy probabilistic relational rules are a promising world model representation for several reasons they are compact and generalize over world instantiations they are usually interpretable and they can be learned effectively from the action experiences in complex worlds we investigate reasoning with such rules in grounded relational domains our algorithms exploit the compactness of rules for efficient and flexible decisiontheoretic planning as a first approach we combine these rules with the upper confidence bounds applied to trees uct algorithm based on lookahead trees our second approach converts these rules into a structured dynamic bayesian network representation and predicts the effects of action sequences using approximate inference and beliefs over world states we evaluate the effectiveness of our approaches for planning in a simulated complex 3d robot manipulation scenario with an articulated manipulator and realistic physics and in domains of the probabilistic planning competition empirical results show that our methods can solve problems where existing methods fail



e  bruni n  k  tran and m  baroni 2014 multimodal distributional semantics volume 49 pages 147



distributional semantic models derive computational representations of word meaning from the patterns of cooccurrence of words in text such models have been a success story of computational linguistics being able to provide reliable estimates of semantic relatedness for the many semantic tasks requiring them however distributional models extract meaning information exclusively from text which is an extremely impoverished basis compared to the rich perceptual sources that ground human semantic knowledge we address the lack of perceptual grounding of distributional models by exploiting computer vision techniques that automatically identify discrete visual words in images so that the distributional representation of a word can be extended to also encompass its cooccurrence with the visual words of images it is associated with we propose a flexible architecture to integrate text and imagebased distributional information and we show in a set of empirical tests that our integrated model is superior to the purely textbased approach and it provides somewhat complementary semantic information with respect to the latter



d  long and  m  fox 2003 the 3rd international planning competition results and analysis volume 20 pages 159



this paper reports the outcome of the third in the series of biennial international planning competitions held in association with the international conference on ai planning and scheduling aips in 2002 in addition to describing the domains the planners and the objectives of the competition the paper includes analysis of the results the results are analysed from several perspectives in order to address the questions of comparative performance between planners comparative difficulty of domains the degree of agreement between planners about the relative difficulty of individual problem instances and the question of how well planners scale relative to one another over increasingly difficult problems the paper addresses these questions through statistical analysis of the raw results of the competition in order to determine which results can be considered to be adequately supported by the data the paper concludes with a discussion of some challenges for the future of the competition series



n  l zhang and  t  kocka 2004 effective dimensions of hierarchical latent class models volume 21 pages 117



hierarchical latent class hlc models are treestructured bayesian networks where leaf nodes are observed while internal nodes are latent  there are no theoretically well justified model selection criteria for hlc models in particular and bayesian networks with latent nodes in general nonetheless empirical studies suggest that the bic score is a reasonable criterion to use in practice for learning hlc models  empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced  with effective model dimension in the penalty term of the bic score  effective dimensions are difficult to compute in this paper we prove a theorem that relates the effective dimension of an hlc model to the effective dimensions of a number of latent class models  the theorem makes it computationally feasible to compute the effective dimensions of large hlc models  the theorem can also be used to compute the effective dimensions of general tree models



s  katrenko p  w adriaans and m  van someren 2010 using local alignments for relation recognition volume 38 pages 148



this paper discusses the problem of marrying structural similarity with semantic relatedness for information extraction from text aiming at accurate recognition of relations we introduce local alignment kernels and explore various possibilities of using them for this task we give a definition of a local alignment la kernel based on the smithwaterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences we show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge our experiments suggest that the la kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin additional series of experiments have been conducted on the data sets of seven general relation types where the performance of the la kernel is comparable to the current stateoftheart results



m  cadoli  f  m donini  p  liberatore and  m  schaerf 2000 space efficiency of propositional knowledge representation formalisms volume 13 pages 131



we investigate the space efficiency of a propositional    knowledge representation pkr formalism intuitively the space    efficiency of a formalism f in representing a certain piece of    knowledge a is the size of the shortest formula of f that    represents a in this paper we assume that knowledge is    either a set of propositional interpretations models or a set of    propositional formulae theorems we provide a formal way of    talking about the relative ability of pkr formalisms to compactly    represent a set of models or a set of theorems we introduce two new    compactness measures the corresponding classes and show that the    relative space efficiency of a pkr formalism in representing    modelstheorems is directly related to such classes in particular    we consider formalisms for nonmonotonic reasoning such as    circumscription and default logic as well as belief revision    operators and the stable model semantics for logic programs with    negation one interesting result is that formalisms with the same    time complexity do not necessarily belong to the same space    efficiency class



n  creignou h  daude and u   egly 2007 phase transition for random quantified xorformulas volume 29 pages 118



the qxorsat problem is the quantified version of the satisfiability problem xorsat in which the connective exclusiveor is used instead of the usual or we study the phase transition associated with random qxorsat instances we give a description of this phase transition in the case of one alternation of quantifiers thus performing an advanced practical and theoretical study on the phase transition of a quantified roblem



p  nightingale i  p gent c  jefferson and i  miguel 2013 short and long supports for constraint propagation volume 46 pages 145



specialpurpose constraint propagation algorithms frequently make implicit use of short supports  by examining a subset of the variables they can infer support a justification that a variablevalue pair may still form part of an assignment that satisfies the constraint for all other variables and values and save substantial work  but short supports have not been studied in their own right the two main contributions of this paper are the identification of short supports as important for constraint propagation and the introduction of haggisgac an efficient and effective general purpose propagation algorithm for exploiting short supports given the complexity of haggisgac we present it as an optimised version of a simpler algorithm shortgac although experiments demonstrate the efficiency of shortgac compared with other generalpurpose propagation algorithms where a compact set of short supports is available we show theoretically and experimentally that haggisgac is even better we also find that haggisgac performs better than gacschema on fulllength supports we also introduce a variant algorithm haggisgacstable which is adapted to avoid work on backtracking and in some cases can be faster and have significant reductions in memory use all the proposed algorithms are excellent for propagating disjunctions of constraints in all experiments with disjunctions we found our algorithms to be faster than constructive or and gacschema by at least an order of magnitude and up to three orders of magnitude



y  chali s  r joty and s  a hasan 2009 complex question answering unsupervised learning approaches and experiments volume 35 pages 147



complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topicoriented informative multidocument summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information in this paper we experiment with one empirical method and two unsupervised statistical machine learning techniques kmeans and expectation maximization em for computing relative importance of the sentences we compare the results of these approaches our experiments show that the empirical approach outperforms the other two techniques and em performs better than kmeans however the performance of these approaches depends entirely on the feature set used and the weighting of these features in order to measure the importance and relevance to the user query we extract different kinds of features ie lexical lexical semantic cosine similarity basic element tree kernel based syntactic and shallowsemantic for each of the document sentences we use a local search technique to learn the weights of the features to the best of our knowledge no study has used tree kernel functions to encode syntacticsemantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate queryfocused summaries or answers to complex questions for each of our methods of generating summaries ie empirical kmeans and em we show the effects of syntactic and shallowsemantic features over the bagofwords bow features



roy  barhaim ido  dagan and jonathan  berant 2015 knowledgebased textual inference via parsetree transformations volume 54 pages 157



textual inference is an important component in many applications for understanding natural language classical approaches to textual inference rely on logical representations for meaning which may be regarded as external to the natural language itself however practical applications usually adopt shallower lexical or lexicalsyntactic representations which correspond closely to language structure in many cases such approaches lack a principled meaning representation and inference framework we describe an inference formalism that operates directly on languagebased structures particularly syntactic parse trees new trees are generated by applying inference rules which provide a unified representation for varying types of inferences we use manual and automatic methods to generate these rules which cover generic linguistic structures as well as specific lexicalbased inferences  we also present a novel packed datastructure and a corresponding inference algorithm that allows efficient implementation of this formalism we proved the correctness of the new algorithm and established its efficiency analytically and empirically the utility of our approach was illustrated on two tasks unsupervised relation extraction from a large corpus and the recognizing textual entailment rte benchmarks 



n  onder g  c whelan and l  li 2006 engineering a conformant probabilistic planner volume 25 pages 115



we present a partialorder conformant probabilistic planner probapop which competed in the blind track of the probabilistic planning competition in ipc4 we explain how we adapt distance based heuristics for use with probabilistic domains probapop also incorporates heuristics based on probability of success we explain the successes and difficulties encountered during the design and implementation of probapop 



m  p wellman 1993 a marketoriented programming environment and its application to distributed multicommodity flow problems volume 1 pages 123





c  boutilier  t  dean and  s  hanks 1999 decisiontheoretic planning structural assumptions and computational leverage volume 11 pages 194



planning under uncertainty is a central problem in the study of    automated sequential decision making and has been addressed by    researchers in many different fields including ai planning decision    analysis operations research control theory and economics  while    the assumptions and perspectives adopted in these areas often differ    in substantial ways many planning problems of interest to researchers    in these fields can be modeled as markov decision processes mdps    and analyzed using the techniques of decision theory       this paper presents an overview and synthesis of mdprelated methods    showing how they provide a unifying framework for modeling many    classes of planning problems studied in ai it also describes    structural properties of mdps that when exhibited by particular    classes of problems can be exploited in the construction of optimal    or approximately optimal policies or plans  planning problems    commonly possess structure in the reward and value functions used to    describe performance criteria in the functions used to describe state    transitions and observations and in the relationships among features    used to describe states actions rewards and observations       specialized representations and algorithms employing these    representations can achieve computational leverage by exploiting    these various forms of structure  certain ai techniques  in    particular those based on the use of structured intensional    representations  can be viewed in this way  this paper surveys    several types of representations for both classical and    decisiontheoretic planning problems and planning algorithms that    exploit these representations in a number of different ways to ease    the computational burden of constructing policies or plans  it focuses    primarily on abstraction aggregation and decomposition techniques    based on aistyle representations



s  k murthy  s  kasif and  s  salzberg 1994 a system for induction of oblique decision trees volume 2 pages 132



this article describes a new system for induction ofoblique   decision trees  this system oc1 combines deterministic   hillclimbing with two forms of randomization to find a goodoblique   split in the form of a hyperplane at each node of a decisiontree   oblique decision tree methods are tuned especially for domains in   which the attributes are numeric although they can be adapted to   symbolic or mixed symbolicnumeric attributes  we presentextensive   empirical studies using both real and artificial data thatanalyze   oc1s ability to construct oblique trees that are smaller and more   accurate than their axisparallel counterparts  we also examinethe   benefits of randomization for the construction of oblique decisiontrees



f  baader  c  lutz  h  sturm and  f  wolter 2002 fusions of description logics and abstract description systems volume 16 pages 158



fusions are a simple way of combining logics for normal    modal logics fusions have been investigated in detail in particular    it is known that under certain conditions decidability transfers    from the component logics to their fusion  though description logics    are closely related to modal logics they are not necessarily    normal in addition abox reasoning in description logics is not    covered by the results from modal logics           in this paper we extend the decidability transfer results from normal    modal logics to a large class of description logics to cover    different description logics in a uniform way we introduce abstract    description systems which can be seen as a common generalization of    description and modal logics and show the transfer results in this    general setting



a  e howe and  e  dahlman 2002 a critical assessment of benchmark comparison in planning volume 17 pages 133



recent trends in planning research have led to empirical     comparison becoming commonplace the field has started to settle into    a methodology for such comparisons which for obvious practical    reasons requires running a subset of planners on a subset of    problems  in this paper we characterize the methodology and    examine eight implicit assumptions about the problems planners and    metrics used in many of these comparisons the problem assumptions    are pr1 the performance of a general purpose planner should not be    penalizedbiased if executed on a sampling of problems and domains    pr2 minor syntactic differences in representation do not affect    performance and pr3 problems should be solvable by strips capable    planners unless they require adl the planner assumptions are pl1    the latest version of a planner is the best one to use pl2 default    parameter settings approximate good performance and pl3 time    cutoffs do not unduly bias outcome the metrics assumptions are    m1 performance degrades similarly for each planner when run on    degraded runtime environments eg machine platform and m2 the    number of plan steps distinguishes performance we find that most of    these assumptions are not supported empirically in particular that    planners are affected differently by these assumptions we conclude    with a call to the community to devote research resources to    improving the state of the practice and especially to enhancing the    available benchmark problems



m  milani fard and j  pineau 2011 nondeterministic policies in markovian decision processes volume 40 pages 124



markovian processes have long been used to model stochastic environments reinforcement learning has emerged as a framework to solve sequential planning and decisionmaking problems in such environments in recent years attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in markovian environments although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decisionmaking they cannot be applied in their current form to decision support systems such as those in medical domains as they suggest policies that are often highly prescriptive and leave little room for the users input without the ability to provide flexible guidelines it is unlikely that these methods can gain ground with users of such systems

this paper introduces the new concept of nondeterministic policies to allow more flexibility in the users decisionmaking process while constraining decisions to remain near optimal solutions we provide two algorithms to compute nondeterministic policies in discrete domains we study the output and running time of these method on a set of synthetic and realworld problems in an experiment with human subjects we show that humans assisted by hints based on nondeterministic policies outperform both humanonly and computeronly agents in a web navigation task



r  j mooney and  m  e califf 1995 induction of firstorder decision lists results on learning the past tense of english verbs volume 3 pages 124



this paper presents a method for inducing logic programs    from examples that learns a new class of concepts called firstorder    decision lists defined as ordered lists of clauses each ending in a    cut  the method called foidl is based on foil quinlan 1990 but    employs intensional background knowledge and avoids the need for    explicit negative examples  it is particularly useful for problems    that involve rules with specific exceptions such as learning the    pasttense of english verbs a task widely studied in the context of    the symbolicconnectionist debate  foidl is able to learn concise    accurate programs for this problem from significantly fewer examples    than previous methods both connectionist and symbolic



r  micalizio and p  torasso 2014 cooperative monitoring to diagnose multiagent plans volume 51 pages 170



diagnosing the execution of a multiagent plan map means identifying and explaining action failures ie actions that did not reach their expected effects current approaches to map diagnosis are substantially centralized and assume that action failures are independent of each other

an experimental analysis demonstrates that the cwcm methodology together with the proposed diagnostic inferences are effective in identifying and explaining action failures even in scenarios where the system observability is significantly reduced



k  yip and  f  zhao 1996 spatial aggregation theory and applications volume 5 pages 126



visual thinking plays an important role in scientific    reasoning  based on the research in automating diverse reasoning    tasks about dynamical systems nonlinear controllers kinematic    mechanisms and fluid motion we have identified a style of visual    thinking imagistic reasoning  imagistic reasoning organizes    computations around imagelike analogue representations so that    perceptual and symbolic operations can be brought to bear to infer    structure and behavior  programs incorporating imagistic reasoning    have been shown to perform at an expert level in domains that defy    current analytic or numerical methods        we have developed a computational paradigm spatial aggregation to    unify the description of a class of imagistic problem solvers  a    program written in this paradigm has the following properties  it    takes a continuous field and optional objective functions as input    and produces highlevel descriptions of structure behavior or    control actions it computes a multilayer of intermediate    representations called spatial aggregates by forming equivalence    classes and adjacency relations  it employs a small set of generic    operators such as aggregation classification and localization to    perform bidirectional mapping between the informationrich field and    successively more abstract spatial aggregates it uses a data    structure the neighborhood graph as a common interface to modularize    computations  to illustrate our theory we describe the computational    structure of three implemented problem solvers  kam maps and    hipair  in terms of the spatial aggregation generic operators by    mixing and matching a library of commonly used routines



m  zhang x  xiao d  xiong and q  liu 2014 topicbased dissimilarity and sensitivity models for translation rule selection volume 50 pages 130



translation rule selection is a task of selecting appropriate translation rules for an ambiguous sourcelanguage segment as translation ambiguities are pervasive in statistical machine translation we introduce two topicbased models for translation rule selection which incorporates global topic information into translation disambiguation we associate each synchronous translation rule with source and targetside topic distributionswith these topic distributions we propose a topic dissimilarity model to select desirable less dissimilar rules by imposing penalties for rules with a large value of dissimilarity of their topic distributions to those of given documents in order to encourage the use of nontopic specific translation rules we also present a topic sensitivity model to balance translation rule selection between generic rules and topicspecific rules furthermore we project targetside topic distributions onto the sourceside topic model space so that we can benefit from topic information of both the source and target language we integrate the proposed topic dissimilarity and sensitivity model into hierarchical phrasebased machine translation for synchronous translation rule selection experiments show that our topicbased translation rule selection model can substantially improve translation quality



e  manisterski d  sarne and s  kraus 2008 cooperative search with concurrent interactions volume 32 pages 136



in this paper we show how taking advantage of autonomous agents capability to maintain parallel interactions with others and incorporating it into the cooperative economic search model results in a new search strategy which outperforms current strategies in use as a framework for our analysis we use the electronic marketplace where buyer agents have the incentive to search cooperatively the new search technique is quite intuitive however its analysis and the process of extracting the optimal search strategy are associated with several significant complexities these difficulties are derived mainly from the unbounded search space and simultaneous dual affects of decisions taken along the search we provide a comprehensive analysis of the model highlighting demonstrating and proving important characteristics of the optimal search strategy consequently we manage to come up with an efficient modular algorithm for extracting the optimal cooperative search strategy for any given environment a computational based comparative illustration of the system performance using the new search technique versus the traditional methods is given emphasizing the main differences in the optimal strategys structure and the advantage of using the proposed model



j  engelfriet 1998 monotonicity and persistence in preferential logics volume 8 pages 121



an important characteristic of many logics for artificial    intelligence is their nonmonotonicity this means that adding a    formula to the premises can invalidate some of the consequences there    may however exist formulae that can always be safely added to the    premises without destroying any of the consequences we say they    respect monotonicity also there may be formulae that when they are    a consequence can not be invalidated when adding any formula to the    premises we call them conservative we study these two classes of    formulae for preferential logics and show that they are closely    linked to the formulae whose truthvalue is preserved along the    preferential ordering we will consider some preferential logics for    illustration and prove syntactic characterization results for them    the results in this paper may improve the efficiency of theorem    provers for preferential logics



p  van beek and  d  w manchak 1996 the design and experimental analysis of algorithms for temporal reasoning volume 4 pages 118



many applications  from planning and scheduling to    problems in molecular biology  rely heavily on a temporal reasoning    component  in this paper we discuss the design and empirical    analysis of algorithms for a temporal reasoning system based on    allens influential intervalbased framework for representing temporal    information  at the core of the system are algorithms for determining    whether the temporal information is consistent and if so finding    one or more scenarios that are consistent with the temporal    information  two important algorithms for these tasks are a path    consistency algorithm and a backtracking algorithm  for the path    consistency algorithm we develop techniques that can result in up to    a tenfold speedup over an already highly optimized implementation    for the backtracking algorithm we develop variable and value ordering    heuristics that are shown empirically to dramatically improve the    performance of the algorithm  as well we show that a previously    suggested reformulation of the backtracking search problem can reduce    the time and space requirements of the backtracking search  taken    together the techniques we develop allow a temporal reasoning    component to solve problems that are of practical size



b  cseke and t  heskes 2011 properties of bethe free energies and message passing in gaussian models volume 41 pages 124



we address the problem of computing approximate marginals in gaussian probabilistic models by using mean field and fractional bethe approximations we define the gaussian fractional bethe free energy in terms of the moment parameters of the approximate marginals derive a lower and an upper bound on the fractional bethe free energy and establish a necessary condition for the lower bound to be bounded from below it turns out that the condition is identical to the pairwise normalizability condition which is known to be a sufficient condition for the convergence of the message passing algorithm we show that stable fixed points of the gaussian message passing algorithm are local minima of the gaussian bethe free energy by a counterexample we disprove the conjecture stating that the unboundedness of the free energy implies the divergence of the message passing algorithm



r  i brafman 2001 on reachability relevance and resolution in the planning as satisfiability approach volume 14 pages 128



in recent years there is a growing awareness of the    importance of reachability and relevancebased pruning techniques for    planning but little work specifically targets these techniques in    this paper we compare the ability of two classes of algorithms to    propagate and discover reachability and relevance constraints in    classical planning problems the first class of algorithms operates on    sat encoded planning problems obtained using the linear and graphplan    encoding schemes  it applies unitpropagation and more general    resolution steps involving larger clauses to these plan encodings    the second class operates at the plan level and contains two families    of pruning algorithms reachablek and relevantk  reachablek    provides a coherent description of a number of existing forward    pruning techniques used in numerous algorithms while relevantk    captures different grades of backward pruning  our results shed light    on the ability of different planencoding schemes to propagate    information forward and backward and on the relative merit of    planlevel and satlevel pruning methods



a  j coles a  i coles m  fox and d  long 2012 colin planning with continuous linear numeric change volume 44 pages 196



in this paper we describe colin a forwardchaining heuristic search planner capable of reasoning with continuous linear numeric change in addition to the full temporal semantics of pddl  through this work we make two advances to the stateoftheart in terms of expressive reasoning capabilities of planners the handling of continuous linear change and the handling of durationdependent effects in combination with duration inequalities both of which require tightly coupled temporal and numeric reasoning during planning  colin combines ffstyle forward chaining search with the use of a linear program lp to check the consistency of the interacting temporal and numeric constraints at each state  the lp is used to compute bounds on the values of variables in each state reducing the range of actions that need to be considered for application  in addition we develop an extension of the temporal relaxed planning graph heuristic of crikey3 to support reasoning directly with continuous change  we extend the range of task variables considered to be suitable candidates for specifying the gradient of the continuous numeric change effected by an action  finally we explore the potential for employing mixed integer programming as a tool for optimising the timestamps of the actions in the plan once a solution has been found to support this we further contribute a selection of extended benchmark domains that include continuous numeric effects  we present results for colin that demonstrate its scalability on a range of benchmarks and compare to existing stateoftheart planners 



j  y halpern and r  pucella 2006 a logic for reasoning about evidence volume 26 pages 134



we introduce a logic for reasoning about evidence that essentially



