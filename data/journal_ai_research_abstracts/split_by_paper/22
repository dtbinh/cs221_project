





b  price and  c  boutilier 2003 accelerating reinforcement learning through implicit imitation volume 19 pages 569629



imitation can be viewed as a means of enhancing learning in multiagent environments  it augments an agents ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents  we propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases  roughly by observing a mentor a reinforcementlearning agent can extract information about its own capabilities in and the relative value of unvisited parts of the state space  we study two specific instantiations of this model one in which the learning agent and the mentor have identical abilities and one designed to deal with agents and mentors with different action sets  we illustrate the benefits of implicit imitation by integrating it with prioritized sweeping and demonstrating improved performance and convergence through observation of single and multiple mentors though we make some stringent assumptions regarding observability and possible interactions we briefly comment on extensions of the model that relax these restricitions



we present a new algorithm for probabilistic planning with no observability  our algorithm called  probabilisticff extends the heuristic forwardsearch machinery of conformantff to problems with probabilistic uncertainty about both the initial state and action effects specifically  probabilisticff combines conformantffs techniques with a powerful machinery for weighted model counting in weighted cnfs serving to elegantly define both the search space and the heuristic function our evaluation of  probabilisticff shows its fine scalability in a range of probabilistic domains constituting a several orders of magnitude improvement over previous results in this area we use a problematic case to point out the main open issue to be addressed by further research









h  e dixon  m  l ginsberg  e  m luks and  a  j parkes 2004 generalizing boolean satisfiability ii theory volume 22 pages 481534



this is the second of three planned papers describing zap a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers  the fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the boolean representation used our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance  this paper presents the theoretical basis for the ideas underlying zap arguing that existing ideas in this area exploit a single recurring structure in that multiple database axioms can be obtained by operating on a single axiom using a subgroup of the group of permutations on the literals in the problem  we argue that the group structure precisely captures the general structure at which earlier approaches hinted and give numerous examples of its use  we go on to extend the davisputnamlogemannloveland inference procedure to this broader setting and show that earlier computational improvements are either subsumed or left intact by the new method  the third paper in this series discusses zaps implementation and presents experimental performance results



allocating scarce resources among agents to maximize global utility is in general computationally challenging  we focus on problems where resources enable agents to execute actions in stochastic environments modeled as markov decision processes mdps such that the value of a resource bundle is defined as the expected value of the optimal mdp policy realizable given these resources  we present an algorithm that simultaneously solves the resourceallocation and the policyoptimization problems  this allows us to avoid explicitly representing utilities over exponentially many resource bundles leading to drastic often exponential reductions in computational complexity  we then use this algorithm in the context of selfinterested agents to design a combinatorial auction for allocating resources we empirically demonstrate the effectiveness of our approach by showing that it can in minutes optimally solve problems for which a straightforward combinatorial resourceallocation technique would require the agents to enumerate up to 2100 resource bundles and the auctioneer to solve an npcomplete problem with an input of that size









d  achlioptas h  jia and c  moore 2005 hiding satisfying assignments two are better than one volume 24 pages 623639



the evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances a plausible source of such instances consists of random ksat formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment a unfortunately instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics roughly speaking for a number of different algorithms a acts as a stronger and stronger attractor as the formulas density increases motivated by recent results on the geometry of the space of satisfying truth assignments of random ksat and naeksat formulas we introduce a simple twist on this basic model which appears to dramatically increase its hardness namely in addition to forbidding the clauses violated by the hidden assignment a we also forbid the clauses violated by its complement so that both a and compliment of a are satisfying it appears that under this symmetrization the effects of the two attractors largely cancel out making it much harder for algorithms to find any truth assignment we give theoretical and experimental evidence supporting this assertion 





haris  aziz casey  cahan charles  gretton philip  kilby nicholas  mattei and toby  walsh 2016 a study of proxies for shapley allocations of transport costs volume 56 pages 573611



we survey existing rules of thumb propose novel methods  and comprehensively evaluate a number of solutions to the problem of calculating the cost to serve each location in a singlevehicle transport setting cost to serve analysis has applications both strategically and operationally in transportation settings the problem is formally modeled as the traveling salesperson game tsg a cooperative transferable utility game in which agents correspond to locations in a traveling salesperson problem tsp the total cost to serve all locations in the tsp is the length of an optimal tour an allocation divides the total cost among individual locations thus providing the cost to serve each of them as one of the most important normative division schemes in cooperative games the shapley value gives a principled and fair allocation for a broad variety of games including the tsg we consider a number of direct and samplingbased procedures for calculating the shapley value and  prove that approximating the shapley value of the tsg within a constant factor is nphard treating the shapley value as an ideal baseline allocation we survey six proxies for it that are each relatively easy to compute some of these proxies are rules of thumb and some are procedures international delivery companies used as cost allocation methods we perform an experimental evaluation using synthetic euclidean games as well as games derived from realworld tours calculated for scenarios involving fastmoving goods where deliveries are made on a road network every day we explore several computationally tractable allocation techniques that are good proxies for the shapley value in problem instances of a size and complexity that is commercially relevant











i  ashlagi d  monderer and m  tennenholtz 2008 on the value of correlation volume 33 pages 575613



correlated equilibrium generalizes nash equilibrium to allow correlation devices correlated equilibrium captures the idea that in many systems there exists a trusted administrator who can recommend behavior to a set of agents but can not enforce such behavior this makes this solution concept most appropriate to the study of multiagent systems in ai  aumann showed an example of a game and of a correlated equilibrium in this game in which the agents welfare expected sum of players utilities is greater than their welfare in all mixedstrategy equilibria following the idea initiated by the price of anarchy literature this suggests the study of two major measures for the value of correlation in a game with nonnegative payoffs

in this work we initiate the study of the mediation and enforcement values providing several general results on the value of correlation as captured by these concepts we also present a set of results for the more specialized case of congestion games a class of games that received a lot of attention in the recent literature



t  grinshpoun a  grubshtein r  zivan a  netzer and a  meisels 2013 asymmetric distributed constraint optimization problems volume 47 pages 613647



distributed constraint optimization dcop is a powerful framework for representing and solving distributed combinatorial problems where the variables of the problem are owned by different agents many multiagent problems include constraints that produce different gains or costs for the participating agents asymmetric gains of constrained agents cannot be naturally represented by the standard dcop model

innovative algorithms that apply to the special properties of the proposed adcop model are presented in detail these include complete algorithms that have a substantial advantage in terms of runtime and network load over existing algorithms for standard dcops which use alternative representations  moreover standard incomplete algorithms ie local search algorithms are inapplicable to the existing dcop representations of asymmetric constraints and when they are applied to the new adcop framework they often fail to converge to a local optimum and yield poor results the local search algorithms proposed in the present paper converge to high quality solutions the experimental evidence that is presented reveals that the proposed local search algorithms for adcops achieve high quality solutions while preserving a high level of privacy







felix  brandt and christian  geist 2016 finding strategyproof social choice functions via sat solving volume 55 pages 565602



a promising direction in computational social choice is to address research problems using computeraided proving techniques in particular with sat solvers this approach has been shown to be viable not only for proving classic impossibility theorems such as arrows theorem but also for finding new impossibilities in the context of preference extensions in this paper we demonstrate that these computeraided techniques can also be applied to improve our understanding of strategyproof irresolute social choice functions these functions however requires a more evolved encoding as otherwise the search space rapidly becomes much too large our contribution is twofold we present an efficient encoding for translating such problems to sat and leverage this encoding to prove new results about strategyproofness with respect to kellys and fishburns preference extensions for example we show that no paretooptimal majoritarian social choice function satisfies fishburnstrategyproofness furthermore we explain how humanreadable proofs of such results can be extracted from minimal unsatisfiable cores of the corresponding sat formulas 



we present an approach to simultaneously reasoning about a video clip and an entire naturallanguage sentence the compositional nature of language is exploited to construct models which represent the meanings of entire sentences composed out of the meanings of the words in those sentences mediated by a grammar that encodes the predicateargument relations we demonstrate that these models faithfully represent the meanings of sentences and are sensitive to how the roles played by participants nouns their characteristics adjectives the actions performed verbs the manner of such actions adverbs and changing spatial relations between participants prepositions affect the meaning of a sentence and how it is grounded in video we exploit this methodology in three ways in the first a video clip along with a sentence are taken as input and the participants in the event described by the sentence are highlighted even when the clip depicts multiple similar simultaneous events in the second a video clip is taken as input without a sentence and a sentence is generated that describes an event in that clip in the third a corpus of video clips is paired with sentences which describe some of the events in those clips and the meanings of the words in those sentences are learned we learn these meanings without needing to specify which attribute of the video clips each word in a given sentence refers to the learned meaning representations are shown to be intelligible to humans









f  s225nchezmart237nez and m  l forcada 2009 inferring shallowtransfer machine translation rules from small parallel corpora volume 34 pages 605635



g  de cooman and e  miranda 2012 irrelevant and independent natural extension for sets of desirable gambles volume 45 pages 601640



the results in this paper add useful tools to the theory of sets of desirable gambles a growing toolbox for reasoning with partial probability assessments we investigate how to combine a number of marginal coherent sets of desirable gambles into a joint set using the properties of epistemic irrelevance and independence we provide formulas for the smallest such joint called their independent natural extension and study its main properties the independent natural extension of maximal coherent sets of desirable gambles allows us to define the strong product of sets of desirable gambles finally we explore an easy way to generalise these results to also apply for the conditional versions of epistemic irrelevance and independence   having such a set of tools that are easily implemented in computer programs is clearly beneficial to fields like ai with a clear interest in coherent reasoning under uncertainty using general and robust uncertainty models that require no full specification





this article considers the performance of the moa multiobjective search algorithm with heuristic information it is shown that in certain cases blind search can be more efficient than perfectly informed search in terms of both node and label expansions

a class of simple graph search problems is defined for which the number of nodes grows linearly with problem size and the number of nondominated labels grows quadratically it is proved that for these problems the number of node expansions performed by blind moa grows linearly with problem size while the number of such expansions performed with a perfectly informed heuristic grows quadratically it is also proved that the number of label expansions grows quadratically in the blind case and cubically in the informed case









m  michelson and c  a knoblock 2008 creating relational data from unstructured and ungrammatical data sources volume 31 pages 543590



in order for agents to act on behalf of users they will have to retrieve and integrate vast amounts of textual data on the world wide web however much of the useful data on the web is neither grammatical nor formally structured making querying difficult examples of these types of data sources are online classifieds like craigslist and auction item listings like ebay we call this unstructured ungrammatical data posts the unstructured nature of posts makes query and integration difficult because the attributes are embedded within the text also these attributes do not conform to standardized values which prevents queries based on a common attribute value the schema is unknown and the values may vary dramatically making accurate search difficult creating relational data for easy querying requires that we define a schema for the embedded attributes and extract values from the posts while standardizing these values  traditional information extraction ie is inadequate to perform this task because it relies on clues from the data such as structure or natural language neither of which are found in posts furthermore traditional information extraction does not incorporate data cleaning which is necessary to accurately query and integrate the source the twostep approach described in this paper creates relational data sets from unstructured and ungrammatical text by addressing both issues to do this we require a set of known entities called a reference set the first step aligns each post to each member of each reference set this allows our algorithm to define a schema over the post and include standard values for the attributes defined by this schema the second step performs information extraction for the attributes including attributes not easily represented by reference sets such as a price in this manner we create a relational structure over previously unstructured data supporting deep and accurate queries over the data as well as standard values for integration our experimental results show that our technique matches the posts to the reference set accurately and efficiently and outperforms stateoftheart extraction systems on the extraction task from posts



we design and analyze deterministic truthful approximation mechanisms for multiunit combinatorial auctions involving only a constant number of distinct goods each in arbitrary limited supply prospective buyers bidders have preferences over multisets of items ie for more than one unit per distinct good our objective is to determine allocations of multisets that maximize the social welfare our main results are for multiminded and submodular bidders in the first setting each bidder has a positive value for being allocated one multiset from a prespecified demand set of alternatives in the second setting each bidder is associated to a submodular valuation function that defines his value for the multiset he is allocated for multiminded bidders we design a truthful fptas that fully optimizes the social welfare while violating the supply constraints on goods within factor 1e for any fixed e0 ie the approximation applies to the constraints and not to the social welfare this result is best possible in that full optimization is impossible without violating the supply constraints for submodular bidders we obtain a ptas that approximates the optimum social welfare within factor 1e for any fixed e0 without violating the supply constraints this result is best possible as well our allocation algorithms are maximalinrange and yield truthful mechanisms when paired with vickreyclarkegroves payments







j  baum a  e nicholson and t  i dix 2012 proximitybased nonuniform abstractions for approximate planning volume 43 pages 477522



in a deterministic world a planning agent can be certain of the consequences of its planned sequence of actions not so however in dynamic stochastic domains where markov decision processes are commonly used unfortunately these suffer from the curse of dimensionality if the state space is a cartesian product of many small sets dimensions planning is exponential in the number of those dimensions





to harness modern multicore processors it is imperative to develop parallel versions of fundamental algorithms in this paper we compare different approaches to parallel bestfirst search in a sharedmemory setting we present a new method pbnf that uses abstraction to partition the state space and to detect duplicate states without requiring frequent locking pbnf allows speculative expansions when necessary to keep threads busy we identify and fix potential livelock conditions in our approach proving its correctness using temporal logic our approach is general allowing it to extend easily to suboptimal and anytime heuristic search in an empirical comparison on strips planning grid pathfinding and sliding tile puzzle problems using 8core machines we show that a weighted a and anytime weighted a implemented using pbnf yield faster search than improved versions of previous parallel search proposals







graphical models such as bayesian networks and markov networks play an important role in artificial intelligence and machine learning inference is a central problem to be solved on these networks this and other problems on these graph models are often known to be hard to solve in general but tractable on graphs with bounded treewidth therefore finding or approximating the treewidth of a graph is a fundamental problem related to inference in graphical models in this paper we study the approximability of a number of graph problems   treewidth and pathwidth of graphs minimum fillin oneshot black and blackwhite pebbling costs of directed acyclic graphs and a variety of different graph layout problems such as minimum cut linear arrangement and interval graph completion  we show that assuming  the recently introduced small set expansion conjecture all of these  problems are nphard to approximate to within any constant factor in polynomial time









p  liberatore 2004 on polynomial sized mdp succinct policies volume 21 pages 551577



policies of markov decision processes mdps determine the next action to execute from the current state and possibly the history the past states when the number of states is large succinct representations are often used to compactly represent both the mdps and the policies in a reduced amount of space in this paper some problems related to the size of succinctly represented policies are analyzed namely it is shown that some mdps have policies that can only be represented in space superpolynomial in the size of the mdp unless the polynomial hierarchy collapses this fact motivates the study of the problem of deciding whether a given mdp has a policy of a given size and reward since some algorithms for mdps work by finding a succinct representation of the value function the problem of deciding the existence of a succinct representation of a value function of a given size and reward is also considered







j  wu and r  givan 2010 automatic induction of bellmanerror features for probabilistic planning volume 38 pages 687755



description logic knowledge and action bases kab are a mechanism for providing both a semantically rich representation of the information on the domain of interest in terms of a description logic knowledge base and actions to change such information over time possibly introducing new objects we resort to a variant of dllite where the unique name assumption is not enforced and where equality between objects may be asserted and inferred actions are specified as sets of conditional effects where conditions are based on epistemic queries over the knowledge base tbox and abox and effects are expressed in terms of new aboxes in this setting we address verification of temporal properties expressed in a variant of firstorder mucalculus with quantification across states notably we show decidability of verification under a suitable restriction inspired by the notion of weak acyclicity in data exchange











h  palacios and h  geffner 2009 compiling uncertainty away in conformant planning problems with bounded width volume 35 pages 623675



tatsuya  imai and alex  fukunaga 2015 on a practical  integerlinear programming model for deletefree tasks and its use as a heuristic for costoptimal planning volume 54 pages 631677



we propose a new integerlinear programming model for the delete relaxation in costoptimal planning while a straightforward ip for the delete relaxation is impractical our enhanced model incorporates variable reduction techniques based on  landmarks relevancebased constraints dominated action elimination immediate action application and inverse action constraints resulting in an ip that can be used to directly solve deletefree planning problems we show that our ip model is competitive with previous stateoftheart solvers for deletefree problems the lprelaxation of the ip model is often a very good approximation to the ip providing an approach to approximating the optimal value of the deletefree task that is complementary to the wellknown lmcut heuristic we also show that constraints that partially consider delete effects can be added to our iplp models we embed the new iplp models into a forwardsearch based planner and show that the performance of the resulting planner on standard ipc benchmarks is comparable with the stateoftheart for costoptimal planning





r  mailler and v  r lesser 2006 asynchronous partial overlay  a new algorithm for solving distributed constraint satisfaction problems volume 25 pages 529576



distributed constraint satisfaction dcsp has long been considered an important problem in multiagent systems research  this is because many realworld problems can be represented as constraint satisfaction and these problems often present themselves in a distributed form  in this article we present a new complete distributed algorithm called asynchronous partial overlay apo for solving dcsps that is based on a cooperative mediation process  the primary ideas behind this algorithm are that agents when acting as a mediator centralize small relevant portions of the dcsp that these centralized subproblems overlap and that agents increase the size of their subproblems along critical paths within the dcsp as the problem solving unfolds  we present empirical evidence that shows that apo outperforms other known complete dcsp techniques





w  w cohen 1995 paclearning recursive logic programs negative results volume 2 pages 541573



in a companion paper it was shown that the class of constantdepth determinate kary recursive clauses is efficiently learnable in this paper we present negative results showing that any natural generalization of this class is hard to learn in valiants model of paclearnability in particular we show that the following program classes are cryptographically hard to learn programs with an unbounded number of constantdepth linear recursive clauses programs with one constantdepth determinate clause containing an unbounded number of recursive calls and programs with one linear recursive clause of constant locality these results immediately imply the nonlearnability of any more general class of programs we also show that learning a constantdepth determinate program with either two linear recursive clauses or one linear recursive clause and one nonrecursive clause is as hard as learning boolean dnf together with positive results from the companion paper these negative results establish a boundary of efficient learnability for recursive functionfree clauses 



in large systems it is important for agents to learn to act effectively but sophisticated multiagent learning algorithms generally do not scale  an alternative approach is to find restricted classes of games where simple efficient algorithms converge  it is shown that stage learning efficiently converges to nash equilibria in large anonymous games if bestreply dynamics converge  two features are identified that improve convergence first rather than making learning more difficult more agents are actually beneficial in many settings  second providing agents with statistical information about the behavior of others can significantly reduce the number of observations needed











d  cohen j  crampton a  gagarin g  gutin and m  jones 2014 iterative plan construction for the workflow satisfiability problem volume 51 pages 555577







d  terekhov t  t tran d  g down and jc  beck 2014 integrating queueing theory and scheduling for dynamic scheduling problems volume 50 pages 535572



literature on constraint satisfaction exhibits the definition of several structural properties that can be possessed by csps like inconsistency substitutability or interchangeability

because of the computational intractability of all the propertydetection problems by following the csp approach we then determine a number of relaxations which provide sufficient conditions for their tractability in particular we exploit forms of language restrictions and local reasoning









p  tadepalli and  b  k natarajan 1996 a formal framework for speedup learning from problems and solutions volume 4 pages 445475



speedup learning seeks to improve the computational    efficiency of problem solving with experience in this paper we    develop a formal framework for learning efficient problem solving from    random problems and their solutions we apply this framework to two    different representations of learned knowledge namely control rules    and macrooperators and prove theorems that identify sufficient    conditions for learning in each representation our proofs are    constructive in that they are accompanied with learning algorithms     our framework captures both empirical and explanationbased     speedup learning in a unified fashion  we illustrate our framework    with implementations in two domains symbolic integration and eight    puzzle this work integrates many strands of experimental and    theoretical work in machine learning including empirical learning of    control rules macrooperator learning explanationbased learning    ebl and probably approximately correct pac learning



j  v graca k  ganchev l  coheur f  pereira and b  taskar 2011 controlling complexity in partofspeech induction volume 41 pages 527551



we consider the problem of fully unsupervised learning of grammatical partofspeech categories from unlabeled text the standard maximumlikelihood hidden markov model for this task performs poorly because of its weak inductive bias and large model capacity we address this problem by refining the model and modifying the learning objective to control its capacity via para metric and nonparametric constraints our approach enforces wordcategory association sparsity adds morphological and orthographic features and eliminates hardtoestimate parameters for rare words we develop an efficient learning algorithm that is not much more computationally intensive than standard training we also provide an opensource implementation of the algorithm our experiments on five diverse languages bulgarian danish english portuguese spanish achieve significant improvements compared with previous methods for the same task



planning is concerned with the automated solution of action sequencing problems described in declarative languages giving the action preconditions and effects one important application area for such technology is the creation of new processes in business process management bpm which is essential in an ever more dynamic business environment a major obstacle for the application of planning in this area lies in the modeling obtaining a suitable model to plan with  ideally a description in pddl the most commonly used planning language  is often prohibitively complicated andor costly our core observation in this work is that this problem can be ameliorated by leveraging synergies with modelbased software development our application at sap one of the leading vendors of enterprise software demonstrates that even onetoone model reuse is possible

we compile sam into a variant of pddl and adapt an offtheshelf planner to solve this kind of problem thanks to the resulting technology business experts may create new processes simply by specifying the desired behavior in terms of status variable value changes effectively by describing the process in their own language





