

we consider the twofold problem of representing collective beliefs and aggregating these beliefs we propose a novel representation for collective beliefs that uses modular transitive relations over possible worlds they allow us to represent conflicting opinions and they have a clear semantics thus improving upon the quasitransitive relations often used in social choice we then describe a way to construct the belief state of an agent informed by a set of sources of varying degrees of reliability this construction circumvents arrows impossibility theorem in a satisfactory manner by accounting for the explicitly encoded conflicts we give a simple settheorybased operator for combining the information of multiple agents we show that this operator satisfies the desirable invariants of idempotence commutativity and associativity and thus is wellbehaved when iterated and we describe a computationally effective way of computing the resulting belief state finally we extend our framework to incorporate voting







an algorithm that learns from a set of examples should    ideally be able to exploit the available resources of a abundant    computing power and b domainspecific knowledge to improve its    ability to generalize  connectionist theoryrefinement systems which    use background knowledge to select a neural networks topology and    initial weights have proven to be effective at exploiting    domainspecific knowledge however most do not exploit available    computing power this weakness occurs because they lack the ability to    refine the topology of the neural networks they produce thereby    limiting generalization especially when given impoverished domain    theories we present the regent algorithm which uses a domainspecific     knowledge to help create an initial population of knowledgebased    neural networks and b genetic operators of crossover and mutation    specifically designed for knowledgebased networks to continually    search for better network topologies experiments on three realworld    domains indicate that our new algorithm is able to significantly    increase generalization compared to a standard connectionist    theoryrefinement system as well as our previous algorithm for    growing knowledgebased networks









honorable mention for the 2010 ijcaijair best paper prize



wikipedia provides a semantic network for computing semantic relatedness in a more structured fashion than a search engine and with more coverage than wordnet we present experiments on using wikipedia for computing semantic relatedness and compare it to wordnet on various benchmarking datasets existing relatedness measures perform better using wikipedia than a baseline given by google counts and we show that wikipedia outperforms wordnet on some datasets we also address the question whether and how wikipedia can be integrated into nlp applications as a knowledge base including wikipedia improves the performance of a machine learning based coreference resolution system indicating that it represents a valuable resource for nlp applications finally we show that our method can be easily used for languages other than english by computing semantic relatedness for a german dataset



as historically acknowledged in the reasoning about actions and change community intuitiveness of a logical domain description cannot be fully automated moreover like any other logical theory action theories may also evolve and thus knowledge engineers need revision methods to help in accommodating new incoming information about the behavior of actions in an adequate manner the present work is about changing action domain descriptions in multimodal logic its contribution is threefold first we revisit the semantics of action theory contraction proposed in previous work giving more robust operators that express minimal change based on a notion of distance between kripkemodels second we give algorithms for syntactical action theory contraction and establish their correctness with respect to our semantics for those action theories that satisfy a principle of modularity investigated in previous work since modularity can be ensured for every action theory and as we show here needs to be computed at most once during the evolution of a domain description it does not represent a limitation at all to the method here studied finally we state agmlike postulates for action theory contraction and assess the behavior of our operators with respect to them moreover we also address the revision counterpart of action theory change showing that it benefits from our semantics for contraction







argumentation is based on the exchange and valuation of interacting arguments followed by the selection of the most acceptable of them for example in order to take a decision to make a choice starting from the framework proposed by dung in 1995 our purpose is to introduce graduality in the selection of the best arguments ie to be able to partition the set of the arguments in more than the two usual subsets of selected and nonselected arguments in order to represent different levels of selection  our basic idea is that an argument is all the more acceptable if it can be preferred to its attackers  first we discuss general principles underlying a gradual valuation of arguments based on their interactions following these principles we define several valuation models for an abstract argumentation system  then we introduce graduality in the concept of acceptability of arguments we propose new acceptability classes and a refinement of existing classes taking advantage of an available gradual valuation







decentralized control of cooperative systems captures the operation of a group of decision makers that share a single global objective  the difficulty in solving optimally such problems arises when the agents lack full observability of the global state of the system when they operate the general problem has been shown to be nexpcomplete in this paper we identify classes of decentralized control problems whose complexity ranges between nexp and p in particular we study problems characterized by independent transitions independent observations and goaloriented objective functions  two algorithms are shown to solve optimally useful classes of goaloriented decentralized processes in polynomial time  this paper also studies information sharing among the decisionmakers which can improve their performance we distinguish between three ways in which agents can exchange information indirect communication direct communication and sharing state features that are not controlled by the agents  our analysis shows that for every class of problems we consider introducing direct or indirect communication does not change the worstcase complexity  the results provide a better understanding of the complexity of decentralized control problems that arise in practice and facilitate the development of planning algorithms for these problems







b  kveton m  hauskrecht and c  guestrin 2006 solving factored mdps with hybrid state and action variables volume 27 pages 153201



efficient representations and solutions for large decision problems with continuous and discrete variables are among the most important challenges faced by the designers of automated decision support systems in this paper we describe a novel hybrid factored markov decision process mdp model that allows for a compact representation of these problems and a new hybrid approximate linear programming halp framework that permits their efficient solutions the central idea of halp is to approximate the optimal value function by a linear combination of basis functions and optimize its weights by linear programming we analyze both theoretical and computational aspects of this approach and demonstrate its scaleup potential on several hybrid optimization problems





partially observable markov decision processes pomdps form an attractive and principled framework for agent planning under uncertainty  pointbased approximate techniques for pomdps compute a policy based on a finite set of points collected in advance from the agents belief space  we present a randomized pointbased value iteration algorithm called perseus  the algorithm performs approximate value backup stages ensuring that in each backup stage the value of each point in the belief set is improved the key observation is that a single backup may improve the value of many belief points  contrary to other pointbased methods perseus backs up only a randomly selected subset of points in the belief set sufficient for improving the value of each belief point in the set we show how the same idea can be extended to dealing with continuous action spaces  experimental results show the potential of perseus in large scale pomdp problems







this paper presents an evolutionary algorithm with a new    goalsequence domination scheme for better decision support in    multiobjective optimization the approach allows the inclusion of    advanced hardsoft priority and constraint information on each    objective component and is capable of incorporating multiple    specifications with overlapping or nonoverlapping objective functions    via logical or and and connectives to drive the search    towards multiple regions of tradeoff in addition we propose a    dynamic sharing scheme that is simple and adaptively estimated    according to the online population distribution without needing any a    priori parameter setting each feature in the proposed algorithm is    examined to show its respective contribution and the performance of    the algorithm is compared with other evolutionary optimization    methods it is shown that the proposed algorithm has performed well in    the diversity of evolutionary search and uniform distribution of    nondominated individuals along the final tradeoffs without    significant computational effort the algorithm is also applied to the    design optimization of a practical servo control system for hard disk    drives with a single voicecoilmotor actuator results of the    evolutionary designed servo control system show a superior closedloop    performance compared to classical pid or rpt approaches







topdown and bottomup theorem proving approaches each    have specific advantages and disadvantages  bottomup provers profit    from strong redundancy control but suffer from the lack of    goalorientation whereas topdown provers are goaloriented but often    have weak calculi when their proof lengths are considered  in order    to integrate both approaches we try to achieve cooperation between a    topdown and a bottomup prover in two different ways the first    technique aims at supporting a bottomup with a topdown prover a    topdown prover generates subgoal clauses they are then processed by    a bottomup prover  the second technique deals with the use of    bottomup generated lemmas in a topdown prover we apply our concept    to the areas of model elimination and superposition  we discuss the    ability of our techniques to shorten proofs as well as to reorder the    search space in an appropriate manner furthermore in order to    identify subgoal clauses and lemmas which are actually relevant for    the proof task we develop methods for a relevancybased filtering    experiments with the provers setheo and spass performed in the problem    library tptp reveal the high potential of our cooperation approaches







diana  grooters and henry  prakken 2016 two aspects of relevance in structured argumentation minimality and paraconsistency volume 56 pages 197245



this paper studies two issues concerning relevance in structured argumentation in the context of the aspic framework arising from the combined use of strict and defeasible inference rules one issue arises if the strict inference rules correspond to classical logic  a longstanding problem is how the trivialising effect of the classical ex falso principle can be avoided while satisfying consistency and closure postulates in this paper this problem is solved by disallowing chaining of strict rules resulting in a variant of the aspic framework called aspic and then disallowing the application of strict rules to inconsistent sets of formulas thus in effect rescher  manors paraconsistent notion of weak consequence is embedded in aspic

finally the combined results of this paper are shown to be a proper extension of classicallogic argumentation with preferences and defeasible rules





value iteration is a powerful yet inefficient algorithm for markov decision processes mdps because it puts the majority of its effort into backing up the entire state space which turns out to be unnecessary in many cases in order to overcome this problem many approaches have been proposed among them ilao and variants of rtdp are stateoftheart ones these methods use reachability analysis and heuristic search to avoid some unnecessary backups however none of these approaches build the graphical structure of the state transitions in a preprocessing step or use the structural information to systematically decompose a problem whereby generating an intelligent backup sequence of the state space in this paper we present two optimal mdp algorithms the first algorithm  topological value iteration tvi detects the structure of mdps and backs up states based on topological sequences it 1 divides an mdp into stronglyconnected components sccs and 2 solves these components sequentially tvi outperforms vi and other stateoftheart algorithms vastly when an mdp has multiple closetoequalsized sccs the second algorithm  focused  topological value iteration ftvi is an extension of tvi ftvi restricts its attention to connected components that are relevant for solving the mdp specifically it uses a small amount of heuristic search to eliminate provably suboptimal actions this pruning allows ftvi to find smaller connected components thus running faster  we demonstrate that ftvi outperforms tvi by an order of magnitude averaged across several domains surprisingly ftvi also significantly outperforms popular heuristicallyinformed mdp algorithms such as ilao lrtdp brtdp and bayesianrtdp in many domains sometimes by as much as two orders of magnitude finally we characterize the type of domains where ftvi excels  suggesting a way to an informed choice of solver







this paper proposes and experimentally validates a bayesian network model of a range finder adapted to dynamic environments all modeling assumptions are rigorously explained and all model parameters have a physical interpretation this approach results in a transparent and intuitive model with respect to the state of the art beam model this paper i proposes a different functional form for the probability of range measurements caused by unmodeled objects ii intuitively explains the discontinuity encountered in te state of the art beam model and iii reduces the number of model parameters while maintaining the same representational power for experimental data the proposed beam model is called rbbm short for rigorously bayesian beam model a maximum likelihood and a variational bayesian estimator both based on expectationmaximization are proposed to learn the model parameters







this paper presents a general and efficient framework for    probabilistic inference and learning from arbitrary uncertain    information it exploits the calculation properties of finite mixture    models conjugate families and factorization both the joint    probability density of the variables and the likelihood function of    the objective or subjective observation are approximated by a    special mixture model in such a way that any desired conditional    distribution can be directly obtained without numerical    integration we have developed an extended version of the expectation    maximization em algorithm to estimate the parameters of mixture    models from uncertain training examples indirect observations as a    consequence any piece of exact or uncertain information about both    input and output values is consistently handled in the inference and    learning stages this ability extremely useful in certain situations    is not found in most alternative methods the proposed framework is    formally justified from standard probabilistic principles and    illustrative examples are provided in the fields of nonparametric    pattern classification nonlinear regression and pattern    completion finally experiments on a real application and comparative    results over standard databases provide empirical evidence of the    utility of the method in a wide range of applications







the first trading agent competition tac was held from june    22nd to july 8th 2000  tac was designed to create a benchmark    problem in the complex domain of emarketplaces and to motivate    researchers to apply unique approaches to a common task  this article    describes attac2000 the firstplace finisher in tac  attac2000    uses a principled bidding strategy that includes several elements of    adaptivity  in addition to the success at the competition isolated    empirical results are presented indicating the robustness and    effectiveness of attac2000s adaptive strategy







f   hutter h   h hoos k   leytonbrown and t   stuetzle 2009 paramils an automatic algorithm configuration framework volume 36 pages 267306



the identification of performanceoptimizing parameter settings is an important part of the development and application of algorithms we describe an automatic framework for this algorithm configuration problem more formally we provide methods for optimizing a target algorithms performance on a given class of problem instances by varying a set of ordinal andor categorical parameters we review a family of localsearchbased algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations we describe the results of a comprehensive experimental evaluation of our methods based on the configuration of prominent complete and incomplete algorithms for sat we also present what is to our knowledge the first published work on automatically configuring the cplex mixed integer programming solver all the algorithms we considered had default parameter settings that were manually identified with considerable effort nevertheless using our automated algorithm configuration procedures we achieved substantial and consistent performance improvements





a major problem in machine learning is that of inductive    bias how to choose a learners hypothesis space so that it is large    enough to contain a solution to the problem being learnt yet small    enough to ensure reliable generalization from reasonablysized    training sets  typically such bias is supplied by hand through the    skill and insights of experts in this paper a model for automatically    learning bias is investigated the central assumption of the model is    that the learner is embedded within an environment of related learning    tasks within such an environment the learner can sample from multiple    tasks and hence it can search for a hypothesis space that contains    good solutions to many of the problems in the environment under    certain restrictions on the set of all hypothesis spaces available to    the learner we show that a hypothesis space that performs well on a    sufficiently large number of training tasks will also perform well    when learning novel tasks in the same environment  explicit bounds    are also derived demonstrating that learning multiple tasks within an    environment of related tasks can potentially give much better    generalization than learning a single task







g  tesauro d  c gondek j  lenchner j  fan and j  m prager 2013 analysis of watsons strategies for playing jeopardy volume 47 pages 205251



major advances in question answering technology were needed for

this article presents our approach to developing watsons gameplaying strategies comprising development of a faithful simulation model and then using learning and montecarlo methods within the simulator to optimize watsons strategic decisionmaking after giving a detailed description of each of our gamestrategy algorithms we then focus in particular on validating the accuracy of the simulators predictions and documenting performance improvements using our methods quantitative performance benefits are shown with respect to both simple heuristic strategies and actual human contestant performance in historical episodes  we further extend our analysis of human play to derive a number of valuable and counterintuitive examples illustrating how human contestants may improve their performance on the show





accessing or integrating data lexicalized in different languages is a challenge multilingual lexical resources play a fundamental role in reducing the language barriers to map concepts lexicalized in different languages in this paper we present a largescale study on the effectiveness of automatic translations to support two key crosslingual ontology mapping tasks the retrieval of candidate matches and the selection of the correct matches for inclusion in the final alignment  we conduct our experiments using four different large gold standards each one consisting of a pair of mapped wordnets to cover four different families of languages we categorize concepts based on their lexicalization type of words synonym richness position in a subconcept graph and analyze their distributions in the gold standards leveraging this categorization we measure several aspects of translation effectiveness such as wordtranslation correctness word sense coverage synset and synonym coverage finally we thoroughly discuss several findings of our study which we believe are helpful for the design of more sophisticated crosslingual mapping algorithms







paolo  liberatore 2015 revision by history volume 52 pages 287329



this article proposes a solution to the problem of obtaining plausibility information which is necessary to perform belief revision given a sequence of revisions together with their results derive a possible initial order that has generated them this is different from the usual assumption of starting from an allequal initial order and modifying it by a sequence of revisions four semantics for iterated revision are considered natural restrained lexicographic and reinforcement for each a necessary and sufficient condition to the existence of an order generating a given history of revisions and results is proved complexity is proved conp complete in all cases but one reinforcement revision with unbounded sequence length 





most classical scheduling formulations assume a fixed and known duration for each activity  in this paper we weaken this assumption requiring instead that each duration can be represented by an independent random variable with a known mean and variance the best solutions are ones which have a high probability of achieving a good makespan we first create a theoretical framework formally showing how monte carlo simulation can be combined with deterministic scheduling algorithms to solve this problem  we propose an associated deterministic scheduling problem whose solution is proved under certain conditions to be a lower bound for the probabilistic problem we then propose and investigate a number of techniques for solving such problems based on combinations of monte carlo simulation solutions to the associated deterministic problem and either constraint programming or tabu search our empirical results demonstrate that a combination of the use of the associated deterministic problem and monte carlo simulation results in algorithms that scale best both in terms of problem size and uncertainty further experiments point to the correlation between the quality of the deterministic solution and the quality of the probabilistic solution as a major factor responsible for this success







s  a wallace 2009 behavior bounding an efficient method for highlevel behavior comparison volume 34 pages 165208



in this paper we explore methods for comparing agent behavior with human behavior to assist with validation our exploration begins by considering a simple method of behavior comparison motivated by shortcomings in this initial approach we introduce behavior bounding an automated modelbased approach for comparing behavior that is inspired in part by mitchells version spaces we show that behavior bounding can be used to compactly represent both human and agent behavior we argue that relatively low amounts of human e64256ort are required to build maintain and use the data structures that underlie behavior bounding and we provide a theoretical basis for these arguments using notions of pac learnability next we show empirical results indicating that this approach is e64256ective at identifying differences in certain types of behaviors and that it performs well when compared against our initial benchmark methods finally we demonstrate that behavior bounding can produce information that allows developers to identify and 64257x problems in an agents behavior much more e64259ciently than standard debugging techniques







b  cuenca grau and b  motik 2012 reasoning over ontologies with hidden content the importbyquery approach volume 45 pages 197255



there is currently a growing interest in techniques for hiding parts of the signature of an ontology kh that is being reused by another ontology kv towards this goal in this paper we propose the importbyquery framework which makes the content of kh accessible through a limited query interface if kv reuses the symbols from kh in a certain restricted way one can reason over kv u kh by accessing only kv and the query interface we map out the landscape of the importbyquery problem in particular we outline the limitations of our framework and prove that certain restrictions on the expressivity of kh and the way in which kv reuses symbols from kh are strictly necessary to enable reasoning in our setting we also identify cases in which reasoning is possible and we present suitable importbyquery reasoning algorithms



i  p gent 2013 optimal implementation of watched literals and more general techniques volume 48 pages 231252



i prove that an implementation technique for scanning lists in backtracking search algorithms is optimal  the result applies to a simple general framework which i present applications include watched literal unit propagation in sat and a number of examples in constraint satisfaction  techniques like watched literals are known to be highly space efficient and effective in practice  when implemented in the circular approach described here  these techniques also have optimal run time per branch in bigo terms when amortized across a search tree  this also applies when multiple list elements must be found  the constant factor overhead of the worst case is only 2  replacing the existing nonoptimal implementation of unit propagation in minisat speeds up propagation by 29 though this is not enough to improve overall run time significantly





k  wong 2008 sound and complete inference rules for seconsequence volume 31 pages 205216



the notion of strong equivalence on logic programs with answer set semantics gives rise to a consequence relation on logic program rules called seconsequence we present a sound and complete set of inference rules for seconsequence on disjunctive logic programs





decentralized pomdps provide an expressive framework for multiagent sequential decision making however the complexity of these modelsnexpcomplete even for two agentshas limited their scalability we present a promising new class of approximation algorithms by developing novel connections between multiagent planning and machine learning we show how the multiagent planning problem can be reformulated as inference in a mixture of dynamic bayesian networks dbns this planningasinference approach paves the way for the application of efficient inference techniques in dbns to multiagent decision making to further improve scalability we identify certain conditions that are sufficient to extend the approach to multiagent systems with dozens of agents specifically we show that the necessary inference within the expectationmaximization framework can be decomposed into processes that often involve a small subset of agents thereby facilitating scalability we further show that a number of existing multiagent planning models satisfy these conditions experiments on large planning benchmarks confirm the benefits of our approach in terms of runtime and scalability with respect to existing techniques







many ai researchers are today striving to build agent teams    for complex dynamic multiagent domains with intended applications    in arenas such as education training entertainment information    integration and collective robotics  unfortunately uncertainties in    these complex dynamic domains obstruct coherent teamwork  in    particular team members often encounter differing incomplete and    possibly inconsistent views of their environment  furthermore team    members can unexpectedly fail in fulfilling responsibilities or    discover unexpected opportunities  highly flexible coordination and    communication is key in addressing such uncertainties  simply fitting    individual agents with precomputed coordination plans will not do for    their inflexibility can cause severe failures in teamwork and their    domainspecificity hinders reusability           our central hypothesis is that the key to such flexibility and    reusability is providing agents with general models of teamwork    agents exploit such models to autonomously reason about coordination    and communication providing requisite flexibility  furthermore the    models enable reuse across domains both saving implementation effort    and enforcing consistency  this article presents one general    implemented model of teamwork called steam  the basic building block    of teamwork in steam is joint intentions cohen  levesque 1991b    teamwork in steam is based on agents building up a partial    hierarchy of joint intentions this hierarchy is seen to parallel    grosz  krauss partial sharedplans 1996  furthermore in steam    team members monitor the teams and individual members performance    reorganizing the team as necessary  finally decisiontheoretic    communication selectivity in steam ensures reduction in communication    overheads of teamwork with appropriate sensitivity to the    environmental conditions  this article describes steams application    in three different complex domains and presents detailed empirical    results







y  zeng and p  doshi 2012 exploiting model equivalences for solving interactive dynamic influence diagrams volume 43 pages 211255



 we focus on  the problem of sequential decision  making in partially observable environments shared with  other agents of uncertain types having  similar or  conflicting objectives   this problem  has been previously  formalized by multiple  frameworks one  of which  is the interactive  dynamic   influence   diagram  idid   which generalizes  the  wellknown  influence  diagram to  the  multiagent setting  idids are graphical models and may be used to compute the policy  of an agent  given its  belief over  the physical  state and others models which changes as  the agent acts and observes in the  multiagent setting

as we may  expect solving idids is computationally  hard  this is predominantly due to the large space of candidate models ascribed to the other agents  and its exponential growth over  time  we present two methods  for reducing the size  of the model  space and stemming its  exponential  growth  both  these  methods involve  aggregating individual models into equivalence classes  our first method groups together behaviorally equivalent models and selects only those  models for  updating which will result in  predictive behaviors that are distinct  from others  in the updated  model space   the second method further compacts  the model space by focusing  on portions of the   behavioral  predictions    specifically   we  cluster  actionally equivalent models  that prescribe identical actions at a  single  time step  exactly  identifying  the equivalences  would require us to solve all models in the initial set  we avoid this by selectively  solving  some of  the  models  thereby introducing  an approximation    we   discuss   the   error   introduced   by   the approximation and  empirically demonstrate the  improved efficiency in solving idids due to the equivalences





realtime heuristic search algorithms satisfy a constant bound on the amount of planning per action independent of problem size as a result they scale up well as problems become larger this property would make them well suited for video games where artificial intelligence controlled agents must react quickly to user commands and to other agents actions on the downside realtime search algorithms employ learning methods that frequently lead to poor solution quality and cause the agent to appear irrational by revisiting the same problem states repeatedly the situation changed recently with a new algorithm d lrta which attempted to eliminate learning by automatically selecting subgoals d lrta is well poised for video games except it has a complex and memorydemanding precomputation phase during which it builds a database of subgoals in this paper we propose a simpler and more memoryefficient way of precomputing subgoals thereby eliminating the main obstacle to applying stateoftheart realtime search methods in video games the new algorithm solves a number of randomly chosen problems offline compresses the solutions into a series of subgoals and stores them in a database when presented with a novel problem online it queries the database for the most similar previously solved case and uses its subgoals to solve the problem in the domain of pathfinding on four large video game maps the new algorithm delivers solutions eight times better while using 57 times less memory and requiring 14 less precomputation time









s  schiffel and m  thielscher 2014 representing and reasoning about the rules of general games with imperfect information volume 49 pages 171206



a general game player is a system that can play previously unknown games just by being given their rules for this purpose the game description language gdl has been developed as a highlevel knowledge representation formalism to communicate game rules to players in this paper we address a fundamental limitation of stateoftheart methods and systems for general game playing namely their being confined to deterministic games with complete information about the game state we develop a simple yet expressive extension of standard gdl that allows for formalising the rules of arbitrary finite nplayer games with randomness and incomplete state knowledge in the second part of the paper we address the intricate reasoning challenge for general gameplaying systems that comes with the new description language we develop a full embedding of extended gdl into the situation calculus augmented by scherl and levesques knowledge fluent we formally prove that this provides a sound and complete reasoning method for players knowledge about game states as well as about the knowledge of the other players



pddl21 was designed to push the envelope of what planning algorithms can do and it has succeeded it adds two important features durative actionswhich take time and may have continuous effects and objective functions for measuring the quality of plans the concept of durative actions is flawed and the treatment of their semantics reveals too strong an attachment to the way many contemporary planners work future pddl innovators should focus on producing a clean semantics for additions to the language and let planner implementers worry about coupling their algorithms to problems expressed in the latest version of the language













grounding is the task of reducing a firstorder theory and finite domain to an equivalent propositional theory it is used as preprocessing phase in many logicbased reasoning systems such systems provide a rich firstorder input language to a user and can rely on efficient propositional solvers to perform the actual reasoning 

we first present our method for classical firstorder logic fo theories then we extend it to foid the extension of fo with inductive definitions which allows for more concise and comprehensive input theories we discuss implementation issues and experimentally validate the practical applicability of our method







recently model checking representation and search techniques    were shown to be efficiently applicable to planning in particular to    nondeterministic planning such planning approaches use ordered    binary decision diagrams obdds to encode a planning domain as a    nondeterministic finite automaton and then apply fast algorithms from    model checking to search for a solution obdds can effectively scale    and can provide universal plans for complex planning domains we are    particularly interested in addressing the complexities arising in    nondeterministic multiagent domains  in this article we present    umop a new universal obddbased planning framework for    nondeterministic multiagent domains we introduce a new planning    domain description language nadl to specify nondeterministic    multiagent domains  the language contributes the explicit definition    of controllable agents and uncontrollable environment agents we    describe the syntax and semantics of nadl and show how to build an    efficient obddbased representation of an nadl description  the umop    planning system uses nadl and different obddbased universal planning    algorithms it includes the previously developed strong and strong    cyclic planning algorithms in addition we introduce our new    optimistic planning algorithm that relaxes optimality guarantees and    generates plausible universal plans in some domains where no strong    nor strong cyclic solution exists we present empirical results    applying umop to domains ranging from deterministic and singleagent    with no environment actions to nondeterministic and multiagent with    complex environment actions umop is shown to be a rich and efficient    planning system









m  surdeanu l  marquez x  carreras and p  r comas 2007 combination strategies for semantic role labeling volume 29 pages 105151



this paper introduces and analyzes a battery of inference models for the problem of semantic role labeling one based on constraint satisfaction and several strategies that model the inference as a metalearning problem using discriminative classifiers these classifiers are developed with a rich set of novel features that encode proposition and sentencelevel information to our knowledge this is the first work that a performs a thorough analysis of learningbased inference models for semantic role labeling and b compares several inference strategies in this context we evaluate the proposed inference strategies in the framework of the conll2005 shared task using only automaticallygenerated syntactic information  the extensive experimental evaluation and analysis indicates that all the proposed inference strategies are successful they all outperform the current best results reported in the conll2005 evaluation exercise but each of the proposed approaches has its advantages and disadvantages several important traits of a stateoftheart srl combination strategy emerge from this analysis i individual models should be combined at the granularity of candidate arguments rather than at the granularity of complete solutions ii the best combination strategy uses an inference model based in learning and iii the learningbased inference benefits from maxmargin classifiers and  global feedback 



h  zhao x  zhang and c  kit 2013 integrative semantic dependency parsing via efficient largescale feature selection volume 46 pages 203233



semantic parsing ie the automatic derivation of meaning representation such as an instantiated predicateargument structure for a sentence plays a critical role in deep processing of natural language unlike all other top systems of semantic dependency parsing that have to rely on a pipeline framework to chain up a series of submodels each specialized for a specific subtask the one presented in this article integrates everything into one model in hopes of achieving desirable integrity and practicality for real applications while maintaining a competitive performance this integrative approach tackles semantic parsing as a word pair classification problem using a maximum entropy classifier we leverage adaptive pruning of argument candidates and largescale feature selection engineering to allow the largest feature space ever in use so far in this field it achieves a stateoftheart performance on the evaluation data set for conll2008 shared task on top of all but one top pipeline system confirming its feasibility and effectiveness







m  petrik and s  zilberstein 2009 a bilinear programming approach for multiagent planning volume 35 pages 235274



multiagent planning and coordination problems are common and known to be computationally hard  we show that a wide range of twoagent problems can be formulated as bilinear programs  we present a successive approximation algorithm that significantly outperforms the coverage set algorithm which is the stateoftheart method for this class of multiagent problems because the algorithm is formulated for bilinear programs it is more general and simpler to implement the new algorithm can be terminated at any time andunlike the coverage set algorithmit facilitates the derivation of a useful online performance bound it is also much more efficient on average reducing the computation time of the optimal solution by about four orders of magnitude  finally we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs



hannes  strass 2015 expressiveness of twovalued semantics for abstract dialectical frameworks volume 54 pages 193231



we analyse the expressiveness of brewka and woltrans abstract dialectical frameworks for twovalued semantics by expressiveness we mean the ability to encode a desired set of twovalued interpretations over a given propositional vocabulary a using only atoms from a we also compare adfs expressiveness with that of the twovalued semantics of abstract argumentation frameworks normal logic programs and propositional logic while the computational complexity of the twovalued model existence problem for all these languages is almost the same we show that the languages form a neat hierarchy with respect to their expressiveness we then demonstrate that this hierarchy collapses once we allow to introduce a linear number of new vocabulary elements we finally also analyse and compare the representational succinctness of adfs for twovalued model semantics that is their capability to represent twovalued interpretation sets in a spaceefficient manner







w  pullan and h  h hoos 2006 dynamic local search for the maximum clique problem volume 25 pages 159185



in this paper we introduce dlsmc a new stochastic local search algorithm for the maximum clique problem dlsmc alternates between phases of iterative improvement during which suitable vertices are added to the current clique and plateau search during which vertices of the current clique are swapped with vertices not contained in the current clique the selection of vertices is solely based on vertex penalties that are dynamically adjusted during the search and a perturbation mechanism is used to overcome search stagnation the behaviour of dlsmc is controlled by a single parameter penalty delay which controls the frequency at which vertex penalties are reduced we show empirically that dlsmc achieves substantial performance improvements over stateoftheart algorithms for the maximum clique problem over a large range of the commonly used dimacs benchmark instances



terminological knowledge representation systems tkrss are tools for designing and using knowledge bases that make use of terminological languages or concept languages  we analyze from a theoretical point of view a tkrs whose capabilities go beyond the ones of presently available tkrss the new features studied often required in practical applications can be summarized in three main points  first we consider a highly expressive terminological language called alcnr including general complements of concepts number restrictions and role conjunction second we allow to express inclusion statements between general concepts and terminological cycles as a particular case  third we prove the decidability of a number of desirable tkrsdeduction services like satisfiability subsumption and instance checking through a sound complete and terminating calculus for reasoning in alcnrknowledge bases our calculus extends the general technique of constraint systems  as a byproduct of the proof we get also the result that inclusion statements in alcnr can be simulated by terminological cycles if descriptive semantics is adopted 







there are two distinct approaches to solving reinforcement    learning problems namely searching in value function space and    searching in policy space  temporal difference methods and    evolutionary algorithms are wellknown examples of these approaches    kaelbling littman and moore recently provided an informative survey    of temporal difference methods  this article focuses on the    application of evolutionary algorithms to the reinforcement learning    problem emphasizing alternative policy representations credit    assignment methods and problemspecific genetic operators  strengths    and weaknesses of the evolutionary approach to reinforcement learning    are presented along with a survey of representative applications







this paper is a multidisciplinary review of empirical  statistical learning from a graphical model perspective  wellknown  examples of graphical models include bayesian networks directed  graphs representing a markov chain and undirected networks  representing a markov field  these graphical models are extended to  model data analysis and empirical learning using the notation of  plates  graphical operations for simplifying and manipulating a  problem are provided including decomposition differentiation andthe  manipulation of probability models from the exponential family  two  standard algorithm schemas for learning are reviewed in a graphical  framework gibbs sampling and the expectation maximizationalgorithm  using these operations and schemas some popular algorithms can be  synthesized from their graphical specification  this includes  versions of linear regression techniques for feedforward networks  and learning gaussian and discrete bayesian networks from data  the  paper concludes by sketching some implications for data analysis and  summarizing how some popular algorithms fall within the framework  presented  the main original contributions here are the decompositiontechniques  and the demonstration that graphical models provide a framework for  understanding and developing complex learning algorithms







the recursive leastsquares rls algorithm is one of the    most wellknown algorithms used in adaptive filtering system    identification and adaptive control its popularity is mainly due to    its fast convergence speed which is considered to be optimal in    practice in this paper rls methods are used to solve reinforcement    learning problems where two new reinforcement learning algorithms    using linear value function approximators are proposed and    analyzed the two algorithms are called rlstdlambda and fastahc    fast adaptive heuristic critic respectively rlstdlambda can be    viewed as the extension of rlstd0 from lambda0 to general lambda    within interval 01 so it is a multistep temporaldifference td    learning algorithm using rls methods the convergence with probability    one and the limit of convergence of rlstdlambda are proved for    ergodic markov chains compared to the existing lstdlambda    algorithm rlstdlambda has advantages in computation and is more    suitable for online learning the effectiveness of rlstdlambda is    analyzed and verified by learning prediction experiments of markov    chains with a wide range of parameter settings        the fastahc algorithm is derived by applying the proposed    rlstdlambda algorithm in the critic network of the adaptive    heuristic critic method unlike conventional ahc algorithm fastahc    makes use of rls methods to improve the learningprediction efficiency    in the critic learning control experiments of the cartpole balancing    and the acrobot swingup problems are conducted to compare the data    efficiency of fastahc with conventional ahc from the experimental    results it is shown that the data efficiency of learning control can    also be improved by using rls methods in the learningprediction    process of the critic the performance of fastahc is also compared    with that of the ahc method using lstdlambda furthermore it is    demonstrated in the experiments that different initial values of the    variance matrix in rlstdlambda are required to get better    performance not only in learning prediction but also in learning    control the experimental results are analyzed based on the existing    theoretical work on the transient phase of forgetting factor rls    methods







adjustable autonomy refers to entities dynamically varying    their own autonomy transferring decisionmaking control to other    entities typically agents transferring control to human users in key    situations  determining whether and when such transfersofcontrol    should occur is arguably the fundamental research problem in    adjustable autonomy previous work has investigated various approaches    to addressing this problem but has often focused on individual    agenthuman interactions  unfortunately domains requiring    collaboration between teams of agents and humans reveal two key    shortcomings of these previous approaches first these approaches use    rigid oneshot transfers of control that can result in unacceptable    coordination failures in multiagent settings  second they ignore    costs eg in terms of time delays or effects on actions to an    agents team due to such transfersofcontrol     to remedy these problems this article presents a novel approach to    adjustable autonomy based on the notion of a transferofcontrol    strategy  a transferofcontrol strategy consists of a conditional    sequence of two types of actions i actions to transfer    decisionmaking control eg from an agent to a user or vice versa    and ii actions to change an agents prespecified coordination    constraints with team members aimed at minimizing miscoordination    costs the goal is for highquality individual decisions to be made    with minimal disruption to the coordination of the team  we present a    mathematical model of transferofcontrol strategies the model guides    and informs the operationalization of the strategies using markov    decision processes which select an optimal strategy given an    uncertain environment and costs to the individuals and teams the    approach has been carefully evaluated including via its use in a    realworld deployed multiagent system that assists a research group    in its daily activities







we present a method for using standard techniques from satisfiability checking to automatically verify and discover theorems in an area of economic theory known as ranking sets of objects the key question in this area which has important applications in social choice theory and decision making under uncertainty is how to extend an agents preferences over a number of objects to a preference relation over nonempty sets of such objects certain combinations of seemingly natural principles for this kind of preference extension can result in logical inconsistencies which has led to a number of important impossibility theorems we first prove a general result that shows that for a wide range of such principles characterised by their syntactic form when expressed in a manysorted firstorder logic any impossibility exhibited at a fixed small domain size will necessarily extend to the general case we then show how to formulate candidates for impossibility theorems at a fixed domain size in propositional logic which in turn enables us to automatically search for general impossibility theorems using a sat solver when applied to a space of 20 principles for preference extension familiar from the literature this method yields a total of 84 impossibility theorems including both known and nontrivial new results







functionalitybased recognition systems recognize objects at    the category level by reasoning about how well the objects support the    expected function such systems naturally associate a measure of    goodness or membership value with a recognized object  this    measure of goodness is the result of combining individual measures or    membership values from potentially many primitive evaluations of    different properties of the objects shape a membership function is    used to compute the membership value when evaluating a primitive of a    particular physical property of an object  in previous versions of a    recognition system known as gruff the membership function for each of    the primitive evaluations was handcrafted by the system designer  in    this paper we provide a learning component for the gruff system    called omlet that automatically learns membership functions given a    set of example objects labeled with their desired category measure    the learning algorithm is generally applicable to any problem in which    lowlevel membership values are combined through an andor tree    structure to give a final overall membership value









aaai 2013 honorable mention for best paper



simple diffusion processes on networks have been used to model analyze and predict diverse phenomena such as spread of diseases information and memes more often than not the underlying network data is noisy and sampled this prompts the following natural question how sensitive are the diffusion dynamics and subsequent conclusions to uncertainty in the network structure 

in this paper we consider two popular diffusion models independent cascade ic model and linear threshold lt model we study how the expected number of vertices that are influencedinfected for particular initial conditions are affected by network perturbations through rigorous analysis under the assumption of a reasonable perturbation model we establish the following main results 1 for the ic model we characterize the sensitivity to network perturbation in terms of the critical probability for phase transition of the network we find that the expected number of infections is quite stable unless the transmission probability is close to the critical probability 2 we show that the standard lt model with uniform edge weights is relatively stable under network perturbations 3 we study these sensitivity questions using extensive simulations on diverse real world networks and find that our theoretical predictions for both models match the observations quite closely 4 experimentally the transient behavior ie the time series of the number of infections in both models appears to be more sensitive to network perturbations



this paper lays part of the groundwork for a domain theory of    negotiation that is a way of classifying interactions so that it is    clear given a domain which negotiation mechanisms and strategies are    appropriate  we define state oriented domains a general category of    interaction  necessary and sufficient conditions for cooperation are    outlined  we use the notion of worth in an altered definition of    utility thus enabling agreements in a wider class of jointgoal    reachable situations an approach is offered for conflict resolution    and it is shown that even in a conflict situation partial cooperative    steps can be taken by interacting agents that is agents in    fundamental conflict might still agree to cooperate up to a certain    point     a unified negotiation protocol unp is developed that can be used in    all types of encounters it is shown that in certain borderline    cooperative situations a partial cooperative agreement ie one    that does not achieve all agents goals might be preferred by all    agents even though there exists a rational agreement that would    achieve all their goals     finally we analyze cases where agents have incomplete information on    the goals and worth of other agents first we consider the case where    agents goals are private information and we analyze what goal    declaration strategies the agents might adopt to increase their    utility  then we consider the situation where the agents goals and    therefore standalone costs are common knowledge but the worth they    attach to their goals is private information  we introduce two    mechanisms one strict the other tolerant and analyze their    affects on the stability and efficiency of negotiation outcomes







we present an efficient exact algorithm for estimating state sequences from outputs or observations in imprecise hidden markov models ihmms the uncertainty linking one state to the next and that linking a state to its output is represented by a set of probability mass functions instead of a single such mass function we consider as best estimates for state sequences the maximal sequences for the posterior joint state model conditioned on the observed output sequence associated with a gain function that is the indicator of the state sequence this corresponds to and generalises finding the state sequence with the highest posterior probability in preciseprobabilistic hmms thereby making our algorithm a generalisation of the one by viterbi we argue that the computational complexity of our algorithm is at worst quadratic in the length of the ihmm cubic in the number of states and essentially linear in the number of maximal state sequences an important feature of our imprecise approach is that there may be more than one maximal sequence typically in those instances where its preciseprobabilistic counterpart is sensitive to the choice of prior for binary ihmms we investigate experimentally how the number of maximal state sequences depends on the model parameters we also present an application in optical character recognition demonstrating that our algorithm can be usefully applied to robustify the inferences made by its preciseprobabilistic counterpart







we study the complexity of costoptimal classical planning over propositional state variables and unaryeffect actions we discover novel problem fragments for which such optimization is tractable and identify certain conditions that differentiate between tractable and intractable problems these results are based on exploiting both structural and syntactic characteristics of planning problems specifically following brafman and domshlak 2003 we relate the complexity of planning and the topology of the causal graph the main results correspond to tractability of costoptimal planning for propositional problems with polytree causal graphs that either have o1bounded indegree or are induced by actions having at most one prevail condition each almost all our tractability results are based on a constructive proof technique that connects between certain tools from planning and tractable constraint optimization and we believe this technique is of interest on its own due to a clear evidence for its robustness







in this paper we reinvestigate windowing for rule learning    algorithms  we show that contrary to previous results for decision    tree learning windowing can in fact achieve significant runtime    gains in noisefree domains and explain the different behavior of rule    learning algorithms by the fact that they learn each rule    independently the main contribution of this paper is integrative    windowing a new type of algorithm that further exploits this property    by integrating good rules into the final theory right after they have    been discovered thus it avoids relearning these rules in subsequent    iterations of the windowing process experimental evidence in a    variety of noisefree domains shows that integrative windowing can in    fact achieve substantial runtime gains furthermore we discuss the    problem of noise in windowing and present an algorithm that is able to    achieve runtime gains in a set of experiments in a simple domain with    artificial noise







we introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some np search problems on average this is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices this quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism furthermore empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior and at the same location as seen in many previously studied classical search methods specifically difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems 







the ignoring delete lists relaxation is of paramount importance for both satisficing and optimal planning in earlier work it was observed that the optimal relaxation heuristic h has amazing qualities in many classical planning benchmarks in particular pertaining to the complete absence of local minima the proofs of this are handmade raising the question whether such proofs can be lead automatically by domain analysis techniques in contrast to earlier disappointing results  the analysis method has exponential runtime and succeeds only in two extremely simple benchmark domains  we herein answer this question in the affirmative we establish connections between causal graph structure and h topology this results in loworder polynomial time analysis methods implemented in a tool we call torchlight of the 12 domains where the absence of local minima has been proved torchlight gives strong success guarantees in 8 domains empirically its analysis exhibits strong performance in a further 2 of these domains plus in 4 more domains where local minima may exist but are rare in this way torchlight can distinguish easy domains from hard ones by summarizing structural reasons for analysis failure torchlight also provides diagnostic output indicating domain aspects that may cause local minima







description logics dls are suitable wellknown logics    for managing structured knowledge  they allow reasoning about    individuals and well defined concepts ie set of individuals with    common properties  the experience in using dls in applications has    shown that in many cases we would like to extend their capabilities    in particular their use in the context of multimedia information    retrieval mir leads to the convincement that such dls should allow    the treatment of the inherent imprecision in multimedia object content    representation and retrieval           in this paper we will present a fuzzy extension of alc combining    zadehs fuzzy logic with a classical dl in particular concepts    becomes fuzzy and thus reasoning about imprecise concepts is    supported  we will define its syntax its semantics describe its    properties and present a constraint propagation calculus for reasoning    in it









p  ghosh a  sharma pp  chakrabarti and p  dasgupta 2012 algorithms for generating ordered solutions for explicit andor structures volume 44 pages 275333



we present algorithms for generating alternative solutions for explicit acyclic andor structures in nondecreasing order of cost the proposed algorithms use a best first search technique and report the solutions using an implicit representation ordered by cost in this paper we present two versions of the search algorithm  a an initial version of the best first search algorithm asg which may present one solution more than once while generating the ordered solutions and b another version lasg which avoids the construction of the duplicate solutions the actual solutions can be reconstructed quickly from the implicit compact representation used we have applied the methods on a few test domains some of them are synthetic while the others are based on well known problems including the search space of the 5peg tower of hanoi problem the matrixchain multiplication problem and the problem of finding secondary structure of rna experimental results show the efficacy of the proposed algorithms over the existing approach our proposed algorithms have potential use in various domains ranging from knowledge based frameworks to service composition where the andor structure is widely used for representing problems



m  helmert 2006 the fast downward planning system volume 26 pages 191246







we consider the twofold problem of representing collective beliefs and aggregating these beliefs we propose a novel representation for collective beliefs that uses modular transitive relations over possible worlds they allow us to represent conflicting opinions and they have a clear semantics thus improving upon the quasitransitive relations often used in social choice we then describe a way to construct the belief state of an agent informed by a set of sources of varying degrees of reliability this construction circumvents arrows impossibility theorem in a satisfactory manner by accounting for the explicitly encoded conflicts we give a simple settheorybased operator for combining the information of multiple agents we show that this operator satisfies the desirable invariants of idempotence commutativity and associativity and thus is wellbehaved when iterated and we describe a computationally effective way of computing the resulting belief state finally we extend our framework to incorporate voting







an algorithm that learns from a set of examples should    ideally be able to exploit the available resources of a abundant    computing power and b domainspecific knowledge to improve its    ability to generalize  connectionist theoryrefinement systems which    use background knowledge to select a neural networks topology and    initial weights have proven to be effective at exploiting    domainspecific knowledge however most do not exploit available    computing power this weakness occurs because they lack the ability to    refine the topology of the neural networks they produce thereby    limiting generalization especially when given impoverished domain    theories we present the regent algorithm which uses a domainspecific     knowledge to help create an initial population of knowledgebased    neural networks and b genetic operators of crossover and mutation    specifically designed for knowledgebased networks to continually    search for better network topologies experiments on three realworld    domains indicate that our new algorithm is able to significantly    increase generalization compared to a standard connectionist    theoryrefinement system as well as our previous algorithm for    growing knowledgebased networks









honorable mention for the 2010 ijcaijair best paper prize



wikipedia provides a semantic network for computing semantic relatedness in a more structured fashion than a search engine and with more coverage than wordnet we present experiments on using wikipedia for computing semantic relatedness and compare it to wordnet on various benchmarking datasets existing relatedness measures perform better using wikipedia than a baseline given by google counts and we show that wikipedia outperforms wordnet on some datasets we also address the question whether and how wikipedia can be integrated into nlp applications as a knowledge base including wikipedia improves the performance of a machine learning based coreference resolution system indicating that it represents a valuable resource for nlp applications finally we show that our method can be easily used for languages other than english by computing semantic relatedness for a german dataset



as historically acknowledged in the reasoning about actions and change community intuitiveness of a logical domain description cannot be fully automated moreover like any other logical theory action theories may also evolve and thus knowledge engineers need revision methods to help in accommodating new incoming information about the behavior of actions in an adequate manner the present work is about changing action domain descriptions in multimodal logic its contribution is threefold first we revisit the semantics of action theory contraction proposed in previous work giving more robust operators that express minimal change based on a notion of distance between kripkemodels second we give algorithms for syntactical action theory contraction and establish their correctness with respect to our semantics for those action theories that satisfy a principle of modularity investigated in previous work since modularity can be ensured for every action theory and as we show here needs to be computed at most once during the evolution of a domain description it does not represent a limitation at all to the method here studied finally we state agmlike postulates for action theory contraction and assess the behavior of our operators with respect to them moreover we also address the revision counterpart of action theory change showing that it benefits from our semantics for contraction







argumentation is based on the exchange and valuation of interacting arguments followed by the selection of the most acceptable of them for example in order to take a decision to make a choice starting from the framework proposed by dung in 1995 our purpose is to introduce graduality in the selection of the best arguments ie to be able to partition the set of the arguments in more than the two usual subsets of selected and nonselected arguments in order to represent different levels of selection  our basic idea is that an argument is all the more acceptable if it can be preferred to its attackers  first we discuss general principles underlying a gradual valuation of arguments based on their interactions following these principles we define several valuation models for an abstract argumentation system  then we introduce graduality in the concept of acceptability of arguments we propose new acceptability classes and a refinement of existing classes taking advantage of an available gradual valuation







decentralized control of cooperative systems captures the operation of a group of decision makers that share a single global objective  the difficulty in solving optimally such problems arises when the agents lack full observability of the global state of the system when they operate the general problem has been shown to be nexpcomplete in this paper we identify classes of decentralized control problems whose complexity ranges between nexp and p in particular we study problems characterized by independent transitions independent observations and goaloriented objective functions  two algorithms are shown to solve optimally useful classes of goaloriented decentralized processes in polynomial time  this paper also studies information sharing among the decisionmakers which can improve their performance we distinguish between three ways in which agents can exchange information indirect communication direct communication and sharing state features that are not controlled by the agents  our analysis shows that for every class of problems we consider introducing direct or indirect communication does not change the worstcase complexity  the results provide a better understanding of the complexity of decentralized control problems that arise in practice and facilitate the development of planning algorithms for these problems







b  kveton m  hauskrecht and c  guestrin 2006 solving factored mdps with hybrid state and action variables volume 27 pages 153201



efficient representations and solutions for large decision problems with continuous and discrete variables are among the most important challenges faced by the designers of automated decision support systems in this paper we describe a novel hybrid factored markov decision process mdp model that allows for a compact representation of these problems and a new hybrid approximate linear programming halp framework that permits their efficient solutions the central idea of halp is to approximate the optimal value function by a linear combination of basis functions and optimize its weights by linear programming we analyze both theoretical and computational aspects of this approach and demonstrate its scaleup potential on several hybrid optimization problems





partially observable markov decision processes pomdps form an attractive and principled framework for agent planning under uncertainty  pointbased approximate techniques for pomdps compute a policy based on a finite set of points collected in advance from the agents belief space  we present a randomized pointbased value iteration algorithm called perseus  the algorithm performs approximate value backup stages ensuring that in each backup stage the value of each point in the belief set is improved the key observation is that a single backup may improve the value of many belief points  contrary to other pointbased methods perseus backs up only a randomly selected subset of points in the belief set sufficient for improving the value of each belief point in the set we show how the same idea can be extended to dealing with continuous action spaces  experimental results show the potential of perseus in large scale pomdp problems







this paper presents an evolutionary algorithm with a new    goalsequence domination scheme for better decision support in    multiobjective optimization the approach allows the inclusion of    advanced hardsoft priority and constraint information on each    objective component and is capable of incorporating multiple    specifications with overlapping or nonoverlapping objective functions    via logical or and and connectives to drive the search    towards multiple regions of tradeoff in addition we propose a    dynamic sharing scheme that is simple and adaptively estimated    according to the online population distribution without needing any a    priori parameter setting each feature in the proposed algorithm is    examined to show its respective contribution and the performance of    the algorithm is compared with other evolutionary optimization    methods it is shown that the proposed algorithm has performed well in    the diversity of evolutionary search and uniform distribution of    nondominated individuals along the final tradeoffs without    significant computational effort the algorithm is also applied to the    design optimization of a practical servo control system for hard disk    drives with a single voicecoilmotor actuator results of the    evolutionary designed servo control system show a superior closedloop    performance compared to classical pid or rpt approaches







topdown and bottomup theorem proving approaches each    have specific advantages and disadvantages  bottomup provers profit    from strong redundancy control but suffer from the lack of    goalorientation whereas topdown provers are goaloriented but often    have weak calculi when their proof lengths are considered  in order    to integrate both approaches we try to achieve cooperation between a    topdown and a bottomup prover in two different ways the first    technique aims at supporting a bottomup with a topdown prover a    topdown prover generates subgoal clauses they are then processed by    a bottomup prover  the second technique deals with the use of    bottomup generated lemmas in a topdown prover we apply our concept    to the areas of model elimination and superposition  we discuss the    ability of our techniques to shorten proofs as well as to reorder the    search space in an appropriate manner furthermore in order to    identify subgoal clauses and lemmas which are actually relevant for    the proof task we develop methods for a relevancybased filtering    experiments with the provers setheo and spass performed in the problem    library tptp reveal the high potential of our cooperation approaches







diana  grooters and henry  prakken 2016 two aspects of relevance in structured argumentation minimality and paraconsistency volume 56 pages 197245



this paper studies two issues concerning relevance in structured argumentation in the context of the aspic framework arising from the combined use of strict and defeasible inference rules one issue arises if the strict inference rules correspond to classical logic  a longstanding problem is how the trivialising effect of the classical ex falso principle can be avoided while satisfying consistency and closure postulates in this paper this problem is solved by disallowing chaining of strict rules resulting in a variant of the aspic framework called aspic and then disallowing the application of strict rules to inconsistent sets of formulas thus in effect rescher  manors paraconsistent notion of weak consequence is embedded in aspic

finally the combined results of this paper are shown to be a proper extension of classicallogic argumentation with preferences and defeasible rules





value iteration is a powerful yet inefficient algorithm for markov decision processes mdps because it puts the majority of its effort into backing up the entire state space which turns out to be unnecessary in many cases in order to overcome this problem many approaches have been proposed among them ilao and variants of rtdp are stateoftheart ones these methods use reachability analysis and heuristic search to avoid some unnecessary backups however none of these approaches build the graphical structure of the state transitions in a preprocessing step or use the structural information to systematically decompose a problem whereby generating an intelligent backup sequence of the state space in this paper we present two optimal mdp algorithms the first algorithm  topological value iteration tvi detects the structure of mdps and backs up states based on topological sequences it 1 divides an mdp into stronglyconnected components sccs and 2 solves these components sequentially tvi outperforms vi and other stateoftheart algorithms vastly when an mdp has multiple closetoequalsized sccs the second algorithm  focused  topological value iteration ftvi is an extension of tvi ftvi restricts its attention to connected components that are relevant for solving the mdp specifically it uses a small amount of heuristic search to eliminate provably suboptimal actions this pruning allows ftvi to find smaller connected components thus running faster  we demonstrate that ftvi outperforms tvi by an order of magnitude averaged across several domains surprisingly ftvi also significantly outperforms popular heuristicallyinformed mdp algorithms such as ilao lrtdp brtdp and bayesianrtdp in many domains sometimes by as much as two orders of magnitude finally we characterize the type of domains where ftvi excels  suggesting a way to an informed choice of solver







this paper proposes and experimentally validates a bayesian network model of a range finder adapted to dynamic environments all modeling assumptions are rigorously explained and all model parameters have a physical interpretation this approach results in a transparent and intuitive model with respect to the state of the art beam model this paper i proposes a different functional form for the probability of range measurements caused by unmodeled objects ii intuitively explains the discontinuity encountered in te state of the art beam model and iii reduces the number of model parameters while maintaining the same representational power for experimental data the proposed beam model is called rbbm short for rigorously bayesian beam model a maximum likelihood and a variational bayesian estimator both based on expectationmaximization are proposed to learn the model parameters







this paper presents a general and efficient framework for    probabilistic inference and learning from arbitrary uncertain    information it exploits the calculation properties of finite mixture    models conjugate families and factorization both the joint    probability density of the variables and the likelihood function of    the objective or subjective observation are approximated by a    special mixture model in such a way that any desired conditional    distribution can be directly obtained without numerical    integration we have developed an extended version of the expectation    maximization em algorithm to estimate the parameters of mixture    models from uncertain training examples indirect observations as a    consequence any piece of exact or uncertain information about both    input and output values is consistently handled in the inference and    learning stages this ability extremely useful in certain situations    is not found in most alternative methods the proposed framework is    formally justified from standard probabilistic principles and    illustrative examples are provided in the fields of nonparametric    pattern classification nonlinear regression and pattern    completion finally experiments on a real application and comparative    results over standard databases provide empirical evidence of the    utility of the method in a wide range of applications







the first trading agent competition tac was held from june    22nd to july 8th 2000  tac was designed to create a benchmark    problem in the complex domain of emarketplaces and to motivate    researchers to apply unique approaches to a common task  this article    describes attac2000 the firstplace finisher in tac  attac2000    uses a principled bidding strategy that includes several elements of    adaptivity  in addition to the success at the competition isolated    empirical results are presented indicating the robustness and    effectiveness of attac2000s adaptive strategy







f   hutter h   h hoos k   leytonbrown and t   stuetzle 2009 paramils an automatic algorithm configuration framework volume 36 pages 267306



the identification of performanceoptimizing parameter settings is an important part of the development and application of algorithms we describe an automatic framework for this algorithm configuration problem more formally we provide methods for optimizing a target algorithms performance on a given class of problem instances by varying a set of ordinal andor categorical parameters we review a family of localsearchbased algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations we describe the results of a comprehensive experimental evaluation of our methods based on the configuration of prominent complete and incomplete algorithms for sat we also present what is to our knowledge the first published work on automatically configuring the cplex mixed integer programming solver all the algorithms we considered had default parameter settings that were manually identified with considerable effort nevertheless using our automated algorithm configuration procedures we achieved substantial and consistent performance improvements





a major problem in machine learning is that of inductive    bias how to choose a learners hypothesis space so that it is large    enough to contain a solution to the problem being learnt yet small    enough to ensure reliable generalization from reasonablysized    training sets  typically such bias is supplied by hand through the    skill and insights of experts in this paper a model for automatically    learning bias is investigated the central assumption of the model is    that the learner is embedded within an environment of related learning    tasks within such an environment the learner can sample from multiple    tasks and hence it can search for a hypothesis space that contains    good solutions to many of the problems in the environment under    certain restrictions on the set of all hypothesis spaces available to    the learner we show that a hypothesis space that performs well on a    sufficiently large number of training tasks will also perform well    when learning novel tasks in the same environment  explicit bounds    are also derived demonstrating that learning multiple tasks within an    environment of related tasks can potentially give much better    generalization than learning a single task







g  tesauro d  c gondek j  lenchner j  fan and j  m prager 2013 analysis of watsons strategies for playing jeopardy volume 47 pages 205251



major advances in question answering technology were needed for

this article presents our approach to developing watsons gameplaying strategies comprising development of a faithful simulation model and then using learning and montecarlo methods within the simulator to optimize watsons strategic decisionmaking after giving a detailed description of each of our gamestrategy algorithms we then focus in particular on validating the accuracy of the simulators predictions and documenting performance improvements using our methods quantitative performance benefits are shown with respect to both simple heuristic strategies and actual human contestant performance in historical episodes  we further extend our analysis of human play to derive a number of valuable and counterintuitive examples illustrating how human contestants may improve their performance on the show





accessing or integrating data lexicalized in different languages is a challenge multilingual lexical resources play a fundamental role in reducing the language barriers to map concepts lexicalized in different languages in this paper we present a largescale study on the effectiveness of automatic translations to support two key crosslingual ontology mapping tasks the retrieval of candidate matches and the selection of the correct matches for inclusion in the final alignment  we conduct our experiments using four different large gold standards each one consisting of a pair of mapped wordnets to cover four different families of languages we categorize concepts based on their lexicalization type of words synonym richness position in a subconcept graph and analyze their distributions in the gold standards leveraging this categorization we measure several aspects of translation effectiveness such as wordtranslation correctness word sense coverage synset and synonym coverage finally we thoroughly discuss several findings of our study which we believe are helpful for the design of more sophisticated crosslingual mapping algorithms







paolo  liberatore 2015 revision by history volume 52 pages 287329



this article proposes a solution to the problem of obtaining plausibility information which is necessary to perform belief revision given a sequence of revisions together with their results derive a possible initial order that has generated them this is different from the usual assumption of starting from an allequal initial order and modifying it by a sequence of revisions four semantics for iterated revision are considered natural restrained lexicographic and reinforcement for each a necessary and sufficient condition to the existence of an order generating a given history of revisions and results is proved complexity is proved conp complete in all cases but one reinforcement revision with unbounded sequence length 





most classical scheduling formulations assume a fixed and known duration for each activity  in this paper we weaken this assumption requiring instead that each duration can be represented by an independent random variable with a known mean and variance the best solutions are ones which have a high probability of achieving a good makespan we first create a theoretical framework formally showing how monte carlo simulation can be combined with deterministic scheduling algorithms to solve this problem  we propose an associated deterministic scheduling problem whose solution is proved under certain conditions to be a lower bound for the probabilistic problem we then propose and investigate a number of techniques for solving such problems based on combinations of monte carlo simulation solutions to the associated deterministic problem and either constraint programming or tabu search our empirical results demonstrate that a combination of the use of the associated deterministic problem and monte carlo simulation results in algorithms that scale best both in terms of problem size and uncertainty further experiments point to the correlation between the quality of the deterministic solution and the quality of the probabilistic solution as a major factor responsible for this success







s  a wallace 2009 behavior bounding an efficient method for highlevel behavior comparison volume 34 pages 165208



in this paper we explore methods for comparing agent behavior with human behavior to assist with validation our exploration begins by considering a simple method of behavior comparison motivated by shortcomings in this initial approach we introduce behavior bounding an automated modelbased approach for comparing behavior that is inspired in part by mitchells version spaces we show that behavior bounding can be used to compactly represent both human and agent behavior we argue that relatively low amounts of human e64256ort are required to build maintain and use the data structures that underlie behavior bounding and we provide a theoretical basis for these arguments using notions of pac learnability next we show empirical results indicating that this approach is e64256ective at identifying differences in certain types of behaviors and that it performs well when compared against our initial benchmark methods finally we demonstrate that behavior bounding can produce information that allows developers to identify and 64257x problems in an agents behavior much more e64259ciently than standard debugging techniques







b  cuenca grau and b  motik 2012 reasoning over ontologies with hidden content the importbyquery approach volume 45 pages 197255



there is currently a growing interest in techniques for hiding parts of the signature of an ontology kh that is being reused by another ontology kv towards this goal in this paper we propose the importbyquery framework which makes the content of kh accessible through a limited query interface if kv reuses the symbols from kh in a certain restricted way one can reason over kv u kh by accessing only kv and the query interface we map out the landscape of the importbyquery problem in particular we outline the limitations of our framework and prove that certain restrictions on the expressivity of kh and the way in which kv reuses symbols from kh are strictly necessary to enable reasoning in our setting we also identify cases in which reasoning is possible and we present suitable importbyquery reasoning algorithms



i  p gent 2013 optimal implementation of watched literals and more general techniques volume 48 pages 231252



i prove that an implementation technique for scanning lists in backtracking search algorithms is optimal  the result applies to a simple general framework which i present applications include watched literal unit propagation in sat and a number of examples in constraint satisfaction  techniques like watched literals are known to be highly space efficient and effective in practice  when implemented in the circular approach described here  these techniques also have optimal run time per branch in bigo terms when amortized across a search tree  this also applies when multiple list elements must be found  the constant factor overhead of the worst case is only 2  replacing the existing nonoptimal implementation of unit propagation in minisat speeds up propagation by 29 though this is not enough to improve overall run time significantly





k  wong 2008 sound and complete inference rules for seconsequence volume 31 pages 205216



the notion of strong equivalence on logic programs with answer set semantics gives rise to a consequence relation on logic program rules called seconsequence we present a sound and complete set of inference rules for seconsequence on disjunctive logic programs





decentralized pomdps provide an expressive framework for multiagent sequential decision making however the complexity of these modelsnexpcomplete even for two agentshas limited their scalability we present a promising new class of approximation algorithms by developing novel connections between multiagent planning and machine learning we show how the multiagent planning problem can be reformulated as inference in a mixture of dynamic bayesian networks dbns this planningasinference approach paves the way for the application of efficient inference techniques in dbns to multiagent decision making to further improve scalability we identify certain conditions that are sufficient to extend the approach to multiagent systems with dozens of agents specifically we show that the necessary inference within the expectationmaximization framework can be decomposed into processes that often involve a small subset of agents thereby facilitating scalability we further show that a number of existing multiagent planning models satisfy these conditions experiments on large planning benchmarks confirm the benefits of our approach in terms of runtime and scalability with respect to existing techniques







many ai researchers are today striving to build agent teams    for complex dynamic multiagent domains with intended applications    in arenas such as education training entertainment information    integration and collective robotics  unfortunately uncertainties in    these complex dynamic domains obstruct coherent teamwork  in    particular team members often encounter differing incomplete and    possibly inconsistent views of their environment  furthermore team    members can unexpectedly fail in fulfilling responsibilities or    discover unexpected opportunities  highly flexible coordination and    communication is key in addressing such uncertainties  simply fitting    individual agents with precomputed coordination plans will not do for    their inflexibility can cause severe failures in teamwork and their    domainspecificity hinders reusability           our central hypothesis is that the key to such flexibility and    reusability is providing agents with general models of teamwork    agents exploit such models to autonomously reason about coordination    and communication providing requisite flexibility  furthermore the    models enable reuse across domains both saving implementation effort    and enforcing consistency  this article presents one general    implemented model of teamwork called steam  the basic building block    of teamwork in steam is joint intentions cohen  levesque 1991b    teamwork in steam is based on agents building up a partial    hierarchy of joint intentions this hierarchy is seen to parallel    grosz  krauss partial sharedplans 1996  furthermore in steam    team members monitor the teams and individual members performance    reorganizing the team as necessary  finally decisiontheoretic    communication selectivity in steam ensures reduction in communication    overheads of teamwork with appropriate sensitivity to the    environmental conditions  this article describes steams application    in three different complex domains and presents detailed empirical    results







y  zeng and p  doshi 2012 exploiting model equivalences for solving interactive dynamic influence diagrams volume 43 pages 211255



 we focus on  the problem of sequential decision  making in partially observable environments shared with  other agents of uncertain types having  similar or  conflicting objectives   this problem  has been previously  formalized by multiple  frameworks one  of which  is the interactive  dynamic   influence   diagram  idid   which generalizes  the  wellknown  influence  diagram to  the  multiagent setting  idids are graphical models and may be used to compute the policy  of an agent  given its  belief over  the physical  state and others models which changes as  the agent acts and observes in the  multiagent setting

as we may  expect solving idids is computationally  hard  this is predominantly due to the large space of candidate models ascribed to the other agents  and its exponential growth over  time  we present two methods  for reducing the size  of the model  space and stemming its  exponential  growth  both  these  methods involve  aggregating individual models into equivalence classes  our first method groups together behaviorally equivalent models and selects only those  models for  updating which will result in  predictive behaviors that are distinct  from others  in the updated  model space   the second method further compacts  the model space by focusing  on portions of the   behavioral  predictions    specifically   we  cluster  actionally equivalent models  that prescribe identical actions at a  single  time step  exactly  identifying  the equivalences  would require us to solve all models in the initial set  we avoid this by selectively  solving  some of  the  models  thereby introducing  an approximation    we   discuss   the   error   introduced   by   the approximation and  empirically demonstrate the  improved efficiency in solving idids due to the equivalences





realtime heuristic search algorithms satisfy a constant bound on the amount of planning per action independent of problem size as a result they scale up well as problems become larger this property would make them well suited for video games where artificial intelligence controlled agents must react quickly to user commands and to other agents actions on the downside realtime search algorithms employ learning methods that frequently lead to poor solution quality and cause the agent to appear irrational by revisiting the same problem states repeatedly the situation changed recently with a new algorithm d lrta which attempted to eliminate learning by automatically selecting subgoals d lrta is well poised for video games except it has a complex and memorydemanding precomputation phase during which it builds a database of subgoals in this paper we propose a simpler and more memoryefficient way of precomputing subgoals thereby eliminating the main obstacle to applying stateoftheart realtime search methods in video games the new algorithm solves a number of randomly chosen problems offline compresses the solutions into a series of subgoals and stores them in a database when presented with a novel problem online it queries the database for the most similar previously solved case and uses its subgoals to solve the problem in the domain of pathfinding on four large video game maps the new algorithm delivers solutions eight times better while using 57 times less memory and requiring 14 less precomputation time









s  schiffel and m  thielscher 2014 representing and reasoning about the rules of general games with imperfect information volume 49 pages 171206



a general game player is a system that can play previously unknown games just by being given their rules for this purpose the game description language gdl has been developed as a highlevel knowledge representation formalism to communicate game rules to players in this paper we address a fundamental limitation of stateoftheart methods and systems for general game playing namely their being confined to deterministic games with complete information about the game state we develop a simple yet expressive extension of standard gdl that allows for formalising the rules of arbitrary finite nplayer games with randomness and incomplete state knowledge in the second part of the paper we address the intricate reasoning challenge for general gameplaying systems that comes with the new description language we develop a full embedding of extended gdl into the situation calculus augmented by scherl and levesques knowledge fluent we formally prove that this provides a sound and complete reasoning method for players knowledge about game states as well as about the knowledge of the other players



pddl21 was designed to push the envelope of what planning algorithms can do and it has succeeded it adds two important features durative actionswhich take time and may have continuous effects and objective functions for measuring the quality of plans the concept of durative actions is flawed and the treatment of their semantics reveals too strong an attachment to the way many contemporary planners work future pddl innovators should focus on producing a clean semantics for additions to the language and let planner implementers worry about coupling their algorithms to problems expressed in the latest version of the language













grounding is the task of reducing a firstorder theory and finite domain to an equivalent propositional theory it is used as preprocessing phase in many logicbased reasoning systems such systems provide a rich firstorder input language to a user and can rely on efficient propositional solvers to perform the actual reasoning 

we first present our method for classical firstorder logic fo theories then we extend it to foid the extension of fo with inductive definitions which allows for more concise and comprehensive input theories we discuss implementation issues and experimentally validate the practical applicability of our method







recently model checking representation and search techniques    were shown to be efficiently applicable to planning in particular to    nondeterministic planning such planning approaches use ordered    binary decision diagrams obdds to encode a planning domain as a    nondeterministic finite automaton and then apply fast algorithms from    model checking to search for a solution obdds can effectively scale    and can provide universal plans for complex planning domains we are    particularly interested in addressing the complexities arising in    nondeterministic multiagent domains  in this article we present    umop a new universal obddbased planning framework for    nondeterministic multiagent domains we introduce a new planning    domain description language nadl to specify nondeterministic    multiagent domains  the language contributes the explicit definition    of controllable agents and uncontrollable environment agents we    describe the syntax and semantics of nadl and show how to build an    efficient obddbased representation of an nadl description  the umop    planning system uses nadl and different obddbased universal planning    algorithms it includes the previously developed strong and strong    cyclic planning algorithms in addition we introduce our new    optimistic planning algorithm that relaxes optimality guarantees and    generates plausible universal plans in some domains where no strong    nor strong cyclic solution exists we present empirical results    applying umop to domains ranging from deterministic and singleagent    with no environment actions to nondeterministic and multiagent with    complex environment actions umop is shown to be a rich and efficient    planning system









m  surdeanu l  marquez x  carreras and p  r comas 2007 combination strategies for semantic role labeling volume 29 pages 105151



this paper introduces and analyzes a battery of inference models for the problem of semantic role labeling one based on constraint satisfaction and several strategies that model the inference as a metalearning problem using discriminative classifiers these classifiers are developed with a rich set of novel features that encode proposition and sentencelevel information to our knowledge this is the first work that a performs a thorough analysis of learningbased inference models for semantic role labeling and b compares several inference strategies in this context we evaluate the proposed inference strategies in the framework of the conll2005 shared task using only automaticallygenerated syntactic information  the extensive experimental evaluation and analysis indicates that all the proposed inference strategies are successful they all outperform the current best results reported in the conll2005 evaluation exercise but each of the proposed approaches has its advantages and disadvantages several important traits of a stateoftheart srl combination strategy emerge from this analysis i individual models should be combined at the granularity of candidate arguments rather than at the granularity of complete solutions ii the best combination strategy uses an inference model based in learning and iii the learningbased inference benefits from maxmargin classifiers and  global feedback 



h  zhao x  zhang and c  kit 2013 integrative semantic dependency parsing via efficient largescale feature selection volume 46 pages 203233



semantic parsing ie the automatic derivation of meaning representation such as an instantiated predicateargument structure for a sentence plays a critical role in deep processing of natural language unlike all other top systems of semantic dependency parsing that have to rely on a pipeline framework to chain up a series of submodels each specialized for a specific subtask the one presented in this article integrates everything into one model in hopes of achieving desirable integrity and practicality for real applications while maintaining a competitive performance this integrative approach tackles semantic parsing as a word pair classification problem using a maximum entropy classifier we leverage adaptive pruning of argument candidates and largescale feature selection engineering to allow the largest feature space ever in use so far in this field it achieves a stateoftheart performance on the evaluation data set for conll2008 shared task on top of all but one top pipeline system confirming its feasibility and effectiveness







m  petrik and s  zilberstein 2009 a bilinear programming approach for multiagent planning volume 35 pages 235274



multiagent planning and coordination problems are common and known to be computationally hard  we show that a wide range of twoagent problems can be formulated as bilinear programs  we present a successive approximation algorithm that significantly outperforms the coverage set algorithm which is the stateoftheart method for this class of multiagent problems because the algorithm is formulated for bilinear programs it is more general and simpler to implement the new algorithm can be terminated at any time andunlike the coverage set algorithmit facilitates the derivation of a useful online performance bound it is also much more efficient on average reducing the computation time of the optimal solution by about four orders of magnitude  finally we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs



hannes  strass 2015 expressiveness of twovalued semantics for abstract dialectical frameworks volume 54 pages 193231



we analyse the expressiveness of brewka and woltrans abstract dialectical frameworks for twovalued semantics by expressiveness we mean the ability to encode a desired set of twovalued interpretations over a given propositional vocabulary a using only atoms from a we also compare adfs expressiveness with that of the twovalued semantics of abstract argumentation frameworks normal logic programs and propositional logic while the computational complexity of the twovalued model existence problem for all these languages is almost the same we show that the languages form a neat hierarchy with respect to their expressiveness we then demonstrate that this hierarchy collapses once we allow to introduce a linear number of new vocabulary elements we finally also analyse and compare the representational succinctness of adfs for twovalued model semantics that is their capability to represent twovalued interpretation sets in a spaceefficient manner







w  pullan and h  h hoos 2006 dynamic local search for the maximum clique problem volume 25 pages 159185



in this paper we introduce dlsmc a new stochastic local search algorithm for the maximum clique problem dlsmc alternates between phases of iterative improvement during which suitable vertices are added to the current clique and plateau search during which vertices of the current clique are swapped with vertices not contained in the current clique the selection of vertices is solely based on vertex penalties that are dynamically adjusted during the search and a perturbation mechanism is used to overcome search stagnation the behaviour of dlsmc is controlled by a single parameter penalty delay which controls the frequency at which vertex penalties are reduced we show empirically that dlsmc achieves substantial performance improvements over stateoftheart algorithms for the maximum clique problem over a large range of the commonly used dimacs benchmark instances



terminological knowledge representation systems tkrss are tools for designing and using knowledge bases that make use of terminological languages or concept languages  we analyze from a theoretical point of view a tkrs whose capabilities go beyond the ones of presently available tkrss the new features studied often required in practical applications can be summarized in three main points  first we consider a highly expressive terminological language called alcnr including general complements of concepts number restrictions and role conjunction second we allow to express inclusion statements between general concepts and terminological cycles as a particular case  third we prove the decidability of a number of desirable tkrsdeduction services like satisfiability subsumption and instance checking through a sound complete and terminating calculus for reasoning in alcnrknowledge bases our calculus extends the general technique of constraint systems  as a byproduct of the proof we get also the result that inclusion statements in alcnr can be simulated by terminological cycles if descriptive semantics is adopted 







there are two distinct approaches to solving reinforcement    learning problems namely searching in value function space and    searching in policy space  temporal difference methods and    evolutionary algorithms are wellknown examples of these approaches    kaelbling littman and moore recently provided an informative survey    of temporal difference methods  this article focuses on the    application of evolutionary algorithms to the reinforcement learning    problem emphasizing alternative policy representations credit    assignment methods and problemspecific genetic operators  strengths    and weaknesses of the evolutionary approach to reinforcement learning    are presented along with a survey of representative applications







this paper is a multidisciplinary review of empirical  statistical learning from a graphical model perspective  wellknown  examples of graphical models include bayesian networks directed  graphs representing a markov chain and undirected networks  representing a markov field  these graphical models are extended to  model data analysis and empirical learning using the notation of  plates  graphical operations for simplifying and manipulating a  problem are provided including decomposition differentiation andthe  manipulation of probability models from the exponential family  two  standard algorithm schemas for learning are reviewed in a graphical  framework gibbs sampling and the expectation maximizationalgorithm  using these operations and schemas some popular algorithms can be  synthesized from their graphical specification  this includes  versions of linear regression techniques for feedforward networks  and learning gaussian and discrete bayesian networks from data  the  paper concludes by sketching some implications for data analysis and  summarizing how some popular algorithms fall within the framework  presented  the main original contributions here are the decompositiontechniques  and the demonstration that graphical models provide a framework for  understanding and developing complex learning algorithms







the recursive leastsquares rls algorithm is one of the    most wellknown algorithms used in adaptive filtering system    identification and adaptive control its popularity is mainly due to    its fast convergence speed which is considered to be optimal in    practice in this paper rls methods are used to solve reinforcement    learning problems where two new reinforcement learning algorithms    using linear value function approximators are proposed and    analyzed the two algorithms are called rlstdlambda and fastahc    fast adaptive heuristic critic respectively rlstdlambda can be    viewed as the extension of rlstd0 from lambda0 to general lambda    within interval 01 so it is a multistep temporaldifference td    learning algorithm using rls methods the convergence with probability    one and the limit of convergence of rlstdlambda are proved for    ergodic markov chains compared to the existing lstdlambda    algorithm rlstdlambda has advantages in computation and is more    suitable for online learning the effectiveness of rlstdlambda is    analyzed and verified by learning prediction experiments of markov    chains with a wide range of parameter settings        the fastahc algorithm is derived by applying the proposed    rlstdlambda algorithm in the critic network of the adaptive    heuristic critic method unlike conventional ahc algorithm fastahc    makes use of rls methods to improve the learningprediction efficiency    in the critic learning control experiments of the cartpole balancing    and the acrobot swingup problems are conducted to compare the data    efficiency of fastahc with conventional ahc from the experimental    results it is shown that the data efficiency of learning control can    also be improved by using rls methods in the learningprediction    process of the critic the performance of fastahc is also compared    with that of the ahc method using lstdlambda furthermore it is    demonstrated in the experiments that different initial values of the    variance matrix in rlstdlambda are required to get better    performance not only in learning prediction but also in learning    control the experimental results are analyzed based on the existing    theoretical work on the transient phase of forgetting factor rls    methods







adjustable autonomy refers to entities dynamically varying    their own autonomy transferring decisionmaking control to other    entities typically agents transferring control to human users in key    situations  determining whether and when such transfersofcontrol    should occur is arguably the fundamental research problem in    adjustable autonomy previous work has investigated various approaches    to addressing this problem but has often focused on individual    agenthuman interactions  unfortunately domains requiring    collaboration between teams of agents and humans reveal two key    shortcomings of these previous approaches first these approaches use    rigid oneshot transfers of control that can result in unacceptable    coordination failures in multiagent settings  second they ignore    costs eg in terms of time delays or effects on actions to an    agents team due to such transfersofcontrol     to remedy these problems this article presents a novel approach to    adjustable autonomy based on the notion of a transferofcontrol    strategy  a transferofcontrol strategy consists of a conditional    sequence of two types of actions i actions to transfer    decisionmaking control eg from an agent to a user or vice versa    and ii actions to change an agents prespecified coordination    constraints with team members aimed at minimizing miscoordination    costs the goal is for highquality individual decisions to be made    with minimal disruption to the coordination of the team  we present a    mathematical model of transferofcontrol strategies the model guides    and informs the operationalization of the strategies using markov    decision processes which select an optimal strategy given an    uncertain environment and costs to the individuals and teams the    approach has been carefully evaluated including via its use in a    realworld deployed multiagent system that assists a research group    in its daily activities







we present a method for using standard techniques from satisfiability checking to automatically verify and discover theorems in an area of economic theory known as ranking sets of objects the key question in this area which has important applications in social choice theory and decision making under uncertainty is how to extend an agents preferences over a number of objects to a preference relation over nonempty sets of such objects certain combinations of seemingly natural principles for this kind of preference extension can result in logical inconsistencies which has led to a number of important impossibility theorems we first prove a general result that shows that for a wide range of such principles characterised by their syntactic form when expressed in a manysorted firstorder logic any impossibility exhibited at a fixed small domain size will necessarily extend to the general case we then show how to formulate candidates for impossibility theorems at a fixed domain size in propositional logic which in turn enables us to automatically search for general impossibility theorems using a sat solver when applied to a space of 20 principles for preference extension familiar from the literature this method yields a total of 84 impossibility theorems including both known and nontrivial new results







functionalitybased recognition systems recognize objects at    the category level by reasoning about how well the objects support the    expected function such systems naturally associate a measure of    goodness or membership value with a recognized object  this    measure of goodness is the result of combining individual measures or    membership values from potentially many primitive evaluations of    different properties of the objects shape a membership function is    used to compute the membership value when evaluating a primitive of a    particular physical property of an object  in previous versions of a    recognition system known as gruff the membership function for each of    the primitive evaluations was handcrafted by the system designer  in    this paper we provide a learning component for the gruff system    called omlet that automatically learns membership functions given a    set of example objects labeled with their desired category measure    the learning algorithm is generally applicable to any problem in which    lowlevel membership values are combined through an andor tree    structure to give a final overall membership value









aaai 2013 honorable mention for best paper



simple diffusion processes on networks have been used to model analyze and predict diverse phenomena such as spread of diseases information and memes more often than not the underlying network data is noisy and sampled this prompts the following natural question how sensitive are the diffusion dynamics and subsequent conclusions to uncertainty in the network structure 

in this paper we consider two popular diffusion models independent cascade ic model and linear threshold lt model we study how the expected number of vertices that are influencedinfected for particular initial conditions are affected by network perturbations through rigorous analysis under the assumption of a reasonable perturbation model we establish the following main results 1 for the ic model we characterize the sensitivity to network perturbation in terms of the critical probability for phase transition of the network we find that the expected number of infections is quite stable unless the transmission probability is close to the critical probability 2 we show that the standard lt model with uniform edge weights is relatively stable under network perturbations 3 we study these sensitivity questions using extensive simulations on diverse real world networks and find that our theoretical predictions for both models match the observations quite closely 4 experimentally the transient behavior ie the time series of the number of infections in both models appears to be more sensitive to network perturbations



this paper lays part of the groundwork for a domain theory of    negotiation that is a way of classifying interactions so that it is    clear given a domain which negotiation mechanisms and strategies are    appropriate  we define state oriented domains a general category of    interaction  necessary and sufficient conditions for cooperation are    outlined  we use the notion of worth in an altered definition of    utility thus enabling agreements in a wider class of jointgoal    reachable situations an approach is offered for conflict resolution    and it is shown that even in a conflict situation partial cooperative    steps can be taken by interacting agents that is agents in    fundamental conflict might still agree to cooperate up to a certain    point     a unified negotiation protocol unp is developed that can be used in    all types of encounters it is shown that in certain borderline    cooperative situations a partial cooperative agreement ie one    that does not achieve all agents goals might be preferred by all    agents even though there exists a rational agreement that would    achieve all their goals     finally we analyze cases where agents have incomplete information on    the goals and worth of other agents first we consider the case where    agents goals are private information and we analyze what goal    declaration strategies the agents might adopt to increase their    utility  then we consider the situation where the agents goals and    therefore standalone costs are common knowledge but the worth they    attach to their goals is private information  we introduce two    mechanisms one strict the other tolerant and analyze their    affects on the stability and efficiency of negotiation outcomes







we present an efficient exact algorithm for estimating state sequences from outputs or observations in imprecise hidden markov models ihmms the uncertainty linking one state to the next and that linking a state to its output is represented by a set of probability mass functions instead of a single such mass function we consider as best estimates for state sequences the maximal sequences for the posterior joint state model conditioned on the observed output sequence associated with a gain function that is the indicator of the state sequence this corresponds to and generalises finding the state sequence with the highest posterior probability in preciseprobabilistic hmms thereby making our algorithm a generalisation of the one by viterbi we argue that the computational complexity of our algorithm is at worst quadratic in the length of the ihmm cubic in the number of states and essentially linear in the number of maximal state sequences an important feature of our imprecise approach is that there may be more than one maximal sequence typically in those instances where its preciseprobabilistic counterpart is sensitive to the choice of prior for binary ihmms we investigate experimentally how the number of maximal state sequences depends on the model parameters we also present an application in optical character recognition demonstrating that our algorithm can be usefully applied to robustify the inferences made by its preciseprobabilistic counterpart







we study the complexity of costoptimal classical planning over propositional state variables and unaryeffect actions we discover novel problem fragments for which such optimization is tractable and identify certain conditions that differentiate between tractable and intractable problems these results are based on exploiting both structural and syntactic characteristics of planning problems specifically following brafman and domshlak 2003 we relate the complexity of planning and the topology of the causal graph the main results correspond to tractability of costoptimal planning for propositional problems with polytree causal graphs that either have o1bounded indegree or are induced by actions having at most one prevail condition each almost all our tractability results are based on a constructive proof technique that connects between certain tools from planning and tractable constraint optimization and we believe this technique is of interest on its own due to a clear evidence for its robustness







in this paper we reinvestigate windowing for rule learning    algorithms  we show that contrary to previous results for decision    tree learning windowing can in fact achieve significant runtime    gains in noisefree domains and explain the different behavior of rule    learning algorithms by the fact that they learn each rule    independently the main contribution of this paper is integrative    windowing a new type of algorithm that further exploits this property    by integrating good rules into the final theory right after they have    been discovered thus it avoids relearning these rules in subsequent    iterations of the windowing process experimental evidence in a    variety of noisefree domains shows that integrative windowing can in    fact achieve substantial runtime gains furthermore we discuss the    problem of noise in windowing and present an algorithm that is able to    achieve runtime gains in a set of experiments in a simple domain with    artificial noise







we introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some np search problems on average this is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices this quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism furthermore empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior and at the same location as seen in many previously studied classical search methods specifically difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems 







the ignoring delete lists relaxation is of paramount importance for both satisficing and optimal planning in earlier work it was observed that the optimal relaxation heuristic h has amazing qualities in many classical planning benchmarks in particular pertaining to the complete absence of local minima the proofs of this are handmade raising the question whether such proofs can be lead automatically by domain analysis techniques in contrast to earlier disappointing results  the analysis method has exponential runtime and succeeds only in two extremely simple benchmark domains  we herein answer this question in the affirmative we establish connections between causal graph structure and h topology this results in loworder polynomial time analysis methods implemented in a tool we call torchlight of the 12 domains where the absence of local minima has been proved torchlight gives strong success guarantees in 8 domains empirically its analysis exhibits strong performance in a further 2 of these domains plus in 4 more domains where local minima may exist but are rare in this way torchlight can distinguish easy domains from hard ones by summarizing structural reasons for analysis failure torchlight also provides diagnostic output indicating domain aspects that may cause local minima







description logics dls are suitable wellknown logics    for managing structured knowledge  they allow reasoning about    individuals and well defined concepts ie set of individuals with    common properties  the experience in using dls in applications has    shown that in many cases we would like to extend their capabilities    in particular their use in the context of multimedia information    retrieval mir leads to the convincement that such dls should allow    the treatment of the inherent imprecision in multimedia object content    representation and retrieval           in this paper we will present a fuzzy extension of alc combining    zadehs fuzzy logic with a classical dl in particular concepts    becomes fuzzy and thus reasoning about imprecise concepts is    supported  we will define its syntax its semantics describe its    properties and present a constraint propagation calculus for reasoning    in it









p  ghosh a  sharma pp  chakrabarti and p  dasgupta 2012 algorithms for generating ordered solutions for explicit andor structures volume 44 pages 275333



we present algorithms for generating alternative solutions for explicit acyclic andor structures in nondecreasing order of cost the proposed algorithms use a best first search technique and report the solutions using an implicit representation ordered by cost in this paper we present two versions of the search algorithm  a an initial version of the best first search algorithm asg which may present one solution more than once while generating the ordered solutions and b another version lasg which avoids the construction of the duplicate solutions the actual solutions can be reconstructed quickly from the implicit compact representation used we have applied the methods on a few test domains some of them are synthetic while the others are based on well known problems including the search space of the 5peg tower of hanoi problem the matrixchain multiplication problem and the problem of finding secondary structure of rna experimental results show the efficacy of the proposed algorithms over the existing approach our proposed algorithms have potential use in various domains ranging from knowledge based frameworks to service composition where the andor structure is widely used for representing problems



m  helmert 2006 the fast downward planning system volume 26 pages 191246





