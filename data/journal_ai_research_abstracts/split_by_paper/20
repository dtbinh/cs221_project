given appropriate representations of the semantic relations between carpenter and wood and between mason and stone for example vectors in a vector space model a suitable algorithm should be able to recognize that these relations are highly similar carpenter is to wood as mason is to stone the relations are analogous likewise with representations of dog house and kennel an algorithm should be able to recognize that the semantic composition of dog and house dog house is highly similar to kennel dog house and kennel are synonymous it seems that these two tasks recognizing relations and compositions are closely connected however up to now the best models for relations are significantly different from the best models for compositions in this paper we introduce a dualspace model that unifies these two tasks this model matches the performance of the best previous models for relations and compositions the dualspace model consists of a space for measuring domain similarity and a space for measuring function similarity carpenter and wood share the same domain the domain of carpentry mason and stone share the same domain the domain of masonry carpenter and mason share the same function the function of artisans wood and stone share the same function the function of materials in the composition dog house kennel has some domain overlap with both dog and house the domains of pets and buildings the function of kennel is similar to the function of house the function of shelters by combining domain and function similarities in various ways we can model relations compositions and other aspects of semantics



the automatic generation of decision trees based on offline reasoning on models of a domain is a reasonable compromise between the advantages of using a modelbased approach in technical domains and the constraints imposed by embedded applications  in this paper we extend the approach to deal with temporal information we introduce a notion of temporal decision tree which is designed to make use of relevant information as long as it is acquired and we present an algorithm for compiling such trees from a modelbased reasoning system









s  i hill and a  doucet 2007 a framework for kernelbased multicategory classification volume 30 pages 525564



a geometric framework for understanding multicategory classification is introduced through which many existing alltogether algorithms can be understood  the structure enables parsimonious optimisation through a direct extension of the binary methodology  the focus is on support vector classification with parallels drawn to related methods

it is also described how this architecture provides insights regarding how to further improve on the speed of existing multicategory classification algorithms  an initial example of how this might be achieved is developed in the formulation of a straightforward multicategory sequential minimal optimisation algorithm  proofofconcept experimental results have shown that this combined with the mapping of pairwise results is comparable with benchmark optimisation speeds



many description logics dls combine knowledge representation on an abstract logical level with an interface to concrete domains like numbers and strings with builtin predicates such as   and prefixof these hybrid dls have turned out to be useful in several application areas such as reasoning about conceptual database models  we propose to further extend such dls with key constraints that allow the expression of statements like us citizens are uniquely identified by their social security number based on this idea we introduce a number of natural description logics and perform a detailed analysis of their decidability and computational complexity  it turns out that naive extensions with key constraints easily lead to undecidability whereas more careful extensions yield nexptimecomplete dls for a variety of useful concrete domains



formal treatment of collaborative multiagent systems has been lagging behind the rapid progress in sequential decision making by individual agents  recent work in the area of decentralized markov decision processes mdps has contributed to closing this gap but the computational complexity of these models remains a serious obstacle to overcome this complexity barrier we identify a specific class of decentralized mdps in which the agents transitions are independent the class consists of independent collaborating agents that are tied together through a structured global reward function that depends on all of their histories of states and actions  we present a novel algorithm for solving this class of problems and examine its properties both as an optimal algorithm and as an anytime algorithm to our best knowledge this is the first algorithm to optimally solve a nontrivial subclass of decentralized mdps  it lays the foundation for further work in this area on both exact and approximate algorithms









p  viappiani b  faltings and p  pu 2006 preferencebased search using examplecritiquing with suggestions volume 27 pages 465503



we consider interactive tools that help users search for their most preferred item in a large collection of options in particular we examine examplecritiquing a technique for enabling users to incrementally construct preference models by critiquing example options that are presented to them we present novel techniques for improving the examplecritiquing technology by adding suggestions to its displayed options such suggestions are calculated based on an analysis of users current preference model and their potential hidden preferences we evaluate the performance of our modelbased suggestion techniques with both synthetic and real users  results show that such suggestions are highly attractive to users and can stimulate them to express more preferences to improve the chance of identifying their most preferred item by up to 78



we provide an overview of the organization and results of the deterministic part of the 4th international planning competition ie of the part concerned with evaluating systems doing deterministic planning ipc4 attracted even more competing systems than its already large predecessors and the competition event was revised in several important respects after giving an introduction to the ipc we briefly explain the main differences between the deterministic part of ipc4 and its predecessors we then introduce formally the language used called pddl22 that extends pddl21 by derived predicates and timed initial literals we list the competing systems and overview the results of the competition the entire set of data is far too large to be presented in full we provide a detailed summary the complete data is available in an online appendix we explain how we awarded the competition prizes  







intractable distributions present a common difficulty in    inference within the probabilistic knowledge representation framework    and variational methods have recently been popular in providing an    approximate solution in this article we describe a perturbational    approach in the form of a cumulant expansion which to lowest order    recovers the standard kullbackleibler variational bound    higherorder terms describe corrections on the variational approach    without incurring much further computational cost  the relationship    to other perturbational approaches such as tap is also elucidated  we    demonstrate the method on a particular class of undirected graphical    models boltzmann machines for which our simulation results confirm    improved accuracy and enhanced stability during learning











carlos  hern225ndez jorge  a baier and roberto  as237n 2016 timebounded bestfirst search for reversible and nonreversible search graphs volume 56 pages 547571



e  elkind p  faliszewski and a  slinko 2011 cloning in elections finding the possible winners volume 42 pages 529573



we consider the problem of manipulating elections by cloning candidates in our model a manipulator can replace each candidate c by several clones ie new candidates that are so similar to c that each voter simply replaces c in his vote with a block of these new candidates ranked consecutively the outcome of the resulting election may then depend on the number of clones as well as on how each voter orders the clones within the block we formalize what it means for a cloning manipulation to be successful which turns out to be a surprisingly delicate issue and for a number of common voting rules characterize the preference profiles for which a successful cloning manipulation exists we also consider the model where there is a cost associated with producing each clone and study the complexity of finding a minimumcost cloning manipulation finally we compare cloning with two related problems the problem of control by adding candidates and the problem of possible cowinners when new alternatives can join















c  b228ckstr246m and p  jonsson 2013 a refined view of causal graphs and component sizes spclosed graph classes and beyond volume 47 pages 575611



decentralized partially observable markov decision processes decpomdps provide a general model for decisionmaking under uncertainty in decentralized settings but are difficult to solve optimally nexpcomplete as a new way of solving these problems we introduce the idea of transforming a decpomdp into a continuousstate deterministic mdp with a piecewiselinear and convex value function this approach makes use of the fact that planning can be accomplished in a centralized offline manner while execution can still be decentralized this new decpomdp formulation which we call an occupancy mdp allows powerful pomdp and continuousstate mdp methods to be used for the first time to provide scalability we refine this approach by combining heuristic search and compact representations that exploit the structure present in multiagent domains without losing the ability to converge to an optimal solution  in particular we introduce a featurebased heuristic search value iteration fbhsvi algorithm that relies on featurebased compact representations pointbased updates and efficient action selection  a theoretical analysis demonstrates that fbhsvi terminates in finite time with an optimal solution we include an extensive empirical analysis using wellknown benchmarks thereby demonstrating that our approach provides significant scalability improvements compared to the state of the art









haonan  yu n  siddharth andrei  barbu and jeffrey  mark siskind 2015 a compositional framework for grounding language inference generation and acquisition in video volume 52 pages 601713





t  rahwan s  d ramchurn n  r jennings and a  giovannucci 2009 an anytime algorithm for optimal coalition structure generation volume 34 pages 521567



coalition formation is a fundamental type of interaction that involves the creation of coherent groupings of distinct autonomous agents in order to efficiently achieve their individual or collective goals forming effective coalitions is a major research challenge in the field of  multiagent systems central to this endeavour is the problem of determining which of the many possible coalitions to form in order to achieve some goal this usually requires calculating a value for every possible coalition known as the coalition value which indicates how beneficial that coalition would be if it was formed once these values  are calculated the agents usually need to find a combination of  coalitions in which every agent belongs to exactly one coalition and by which the overall outcome of the system is maximized however this coalition structure generation problem is extremely challenging due to the number of possible solutions that need to be examined which grows exponentially with the number of agents involved to date therefore many algorithms have been proposed to solve this problem using different techniques ranging from dynamic programming to integer programming to stochastic search all of which suffer from major limitations relating to execution time solution quality and memory requirements

with this in mind we develop an anytime algorithm to solve the coalition structure generation problem specifically the algorithm uses a novel representation of the search space which partitions the space of possible solutions into subspaces such that it is possible to compute upper and lower bounds on the values of the best coalition structures in them these bounds are then used to identify the subspaces that have no potential of containing the optimal solution so that they can be pruned the algorithm then searches through the remaining subspaces very efficiently using a branchandbound technique to avoid examining all the solutions within the searched subspaces in this setting we prove that our algorithm enumerates all coalition structures efficiently by avoiding redundant and invalid solutions automatically moreover in order to effectively test our algorithm we develop a new type of input distribution which allows us to generate more reliable benchmarks compared to the input distributions previously used in the field given this new distribution we show that for 27 agents our algorithm is able to find solutions that are optimal in 0175 of the time required by the fastest available algorithm in the literature the algorithm is anytime and if interrupted before it would have normally terminated it can still provide a solution that is guaranteed to be within a bound from the optimal one moreover the guarantees we provide on the quality of the solution are significantly better than those provided by the previous state of the art algorithms designed for this purpose for example for the worst case distribution given 25 agents our algorithm is able to find a 90 efficient solution in around 10 of time it takes to find the optimal solution









r  i brafman and g  shani 2012 replanning in domains with partial information and sensing actions volume 45 pages 565600





i  androutsopoulos g  lampouras and d  galanis 2013 generating natural language descriptions from owl ontologies the naturalowl system volume 48 pages 671715



we present naturalowl a natural language generation system that produces texts describing individuals or classes of owl ontologies unlike simpler owl verbalizers which typically express a single axiom at a time in controlled often not entirely fluent natural language primarily for the benefit of domain experts we aim to generate fluent and coherent multisentence texts for endusers with a system like naturalowl one can publish information in owl on the web along with automatically produced corresponding texts in multiple languages making the information accessible not only to computer programs and domain experts but also endusers we discuss the processing stages of naturalowl the optional domaindependent linguistic resources that the system can use at each stage and why they are useful we also present trials showing that when the domaindependent llinguistic resources are available naturalowl produces significantly better texts compared to a simpler verbalizer and that the resources can be created with relatively light effort







m  rk ryan 2008 exploiting subgraph structure in multirobot path planning volume 31 pages 497542





johan  kwisthout 2015 treewidth and the computational complexity of map approximations in bayesian networks volume 53 pages 699720



the problem of finding the most probable explanation to a designated set of variables given partial evidence the map problem is a notoriously intractable problem in bayesian networks both to compute exactly and to approximate it is known both from theoretical considerations and from practical experience that low treewidth is typically an essential prerequisite to efficient exact computations in bayesian networks in this paper we investigate whether the same holds for approximating map we define four notions of approximating map by value structure rank and expectation and argue that all of them are intractable in general we prove that efficient valueapproximations structureapproximations and rankapproximations of map instances with high treewidth will violate the exponential time hypothesis in contrast we show that map can sometimes be efficiently expectationapproximated even in instances with high treewidth if the most probable explanation has a high probability we introduce the complexity class fert analogous to the class ftp to capture this notion of fixedparameter expectationapproximability we suggest a roadmap to future research that yields fixedparameter tractable results for expectationapproximate map even in graphs with high treewidth







aaai 2010 outstanding paper award



g  j228ger and w  zhang 2010 an effective algorithm for and phase transitions of the directed hamiltonian cycle problem volume 39 pages 663687



the hamiltonian cycle problem hcp is an important combinatorial problem with applications in many areas it is among the first  problems used for studying intrinsic properties including phase transitions  of combinatorial problems while thorough theoretical and experimental analyses have been made on the hcp in undirected graphs a limited  amount of work has been done for the hcp in directed graphs dhcp 







k  hoki and t  kaneko 2014 largescale optimization for evaluation functions with minimax search volume 49 pages 527568



this paper presents a new method minimax tree optimization mmto to learn a heuristic evaluation function of a practical alphabeta search program the evaluation function may be a linear or nonlinear combination of weighted features and the weights are the parameters to be optimized to control the search results so that the move decisions agree with the game records of human experts a wellmodeled objective function to be minimized is designed moreover a numerical iterative method is used to nd local minima of the objective function and more than forty million parameters are adjusted by using a small number of hyper parameters this method was applied to shogi a major variant of chess in which the evaluation function must handle a larger state space than in chess experimental results show that the largescale optimization of the evaluation function improves the playing strength of shogi programs and the new method performs signicantly better than other methods implementation of the new method in our shogi program bonanza made substantial contributions to the programs rstplace nish in the 2013 world computer shogi championship additionally we present preliminary evidence of broader applicability of our method to other twoplayer games such as chess



in recent years there has been much interest in phase transitions of combinatorial problems  phase transitions have been successfully used to analyze combinatorial optimization problems characterize their typicalcase features and locate the hardest problem instances  in this paper we study phase transitions of the asymmetric traveling salesman problem atsp an nphard combinatorial optimization problem that has many realworld applications  using random instances of up to 1500 cities in which intercity distances are uniformly distributed we empirically show that many properties of the problem including the optimal tour cost and backbone size experience sharp transitions as the precision of intercity distances increases across a critical value  our experimental results on the costs of the atsp tours and assignment problem agree with the theoretical result that the asymptotic cost of assignment problem is pi 2 6 the number of cities goes to infinity  in addition we show that the average computational cost of the wellknown branchandbound subtour elimination algorithm for the problem also exhibits a thrashing behavior transitioning from easy to difficult as the distance precision increases  these results answer positively an open question regarding the existence of phase transitions in the atsp and provide guidance on how difficult atsp problem instances should be generated







the aviation safety reporting system collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents to effectively reduce these incidents it is vital to accurately identify why these incidents occurred more precisely given a set of possible causes or shaping factors this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report we investigate two approaches to cause identification both approaches exploit information provided by a semantic lexicon which is automatically constructed via thelen and riloffs basilisk framework augmented with our linguistic and algorithmic modifications the first approach labels a report using a simple heuristic which looks for the words and phrases acquired during the semantic lexicon learning process in the report the second approach recasts cause identification as a text classification problem employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports our experiments show that both the heuristicbased approach and the learningbased approach when given sufficient training data outperform the baseline system significantly







numerous formalisms and dedicated algorithms have been designed in the last decades to model and solve decision making problems some formalisms such as constraint networks can express simple decision problems while others are designed to take into account uncertainties unfeasible decisions and utilities even in a single formalism several variants are often proposed to model different types of uncertainty probability possibility or utility additive or not  in this article we introduce an algebraic graphical model that encompasses a large number of such formalisms 1 we first adapt previous structures from friedman chu and halpern for representing uncertainty utility and expected utility in order to deal with generic forms of sequential decision making 2 on these structures we then introduce composite graphical models that express information via variables linked by local functions thanks to conditional independence 3 on these graphical models we finally define a simple class of queries which can represent various scenarios in terms of observabilities and controllabilities a natural decisiontree semantics for such queries is completed by an equivalent operational semantics which induces generic algorithms  the proposed framework called the plausibilityfeasibilityutility pfu framework  not only provides a better understanding of the links between existing formalisms but it also covers yet unpublished frameworks such as possibilistic influence diagrams and unifies formalisms such as quantified boolean formulas and influence diagrams our backtrack and variable elimination generic algorithms are a first step towards unified algorithms





t  p michalak k  v aadithya p  l szczepanski b  ravindran and n  r jennings 2013 efficient computation of the shapley value for gametheoretic network centrality volume 46 pages 607650



the shapley valueprobably the most important normative payoff division scheme in coalitional gameshas recently been advocated as a useful measure of centrality in networks however although this approach has a variety of realworld applications including social and organisational networks biological networks and communication networks its computational properties have not been widely studied to date the only practicable approach to compute shapley valuebased centrality has been via monte carlo simulations which are computationally expensive and not guaranteed to give an exact answer against this background this paper presents the first study of the computational aspects of the shapley value for network centralities specifically we develop exact analytical formulae for shapley valuebased centrality in both weighted and unweighted networks and develop efficient polynomial time and exact algorithms based on them we empirically evaluate these algorithms on two reallife examples an infrastructure network representing the topology of the western states power grid and a collaboration network from the field of astrophysics and demonstrate that they deliver significant speedups over the monte carlo approach for instance in the case of unweighted networks our algorithms are able to return the exact solution about 1600 times faster than the monte carlo approximation even if we allow for a generous 10 error margin for the latter method



many realworld decision making tasks require us to choose among several expensive observations in a sensor network for example it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty in medical decision making tasks one needs to select which tests to administer before deciding on the most effective treatment it has been general practice to use heuristicguided procedures for selecting observations in this paper we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models for example our algorithms allow to optimally label hidden variables in hidden markov models hmms we provide results for both selecting the optimal subset of observations and for obtaining an optimal conditional observation plan

in addition we consider several extensions such as using our algorithms for scheduling observation selection for multiple sensors we demonstrate the effectiveness of our approach on several realworld datasets including a prototype sensor network deployment for energy conservation in buildings











ben  strasser adi  botea and daniel  harabor 2015 compressing optimal paths with run length encoding volume 54 pages 593629



structured game representations have recently attracted interest as models for multiagent artificial intelligence scenarios with rational behavior most commonly characterized by nash equilibria  this paper presents efficient exact algorithms for computing nash equilibria in structured game representations including both graphical games and multiagent influence diagrams maids  the algorithms are derived from a continuation method for normalform and extensiveform games due to govindan and wilson they follow a trajectory through a space of perturbed games and their equilibria exploiting game structure through fast computation of the jacobian of the payoff function  they are theoretically guaranteed to find at least one equilibrium of the game and may find more  our approach provides the first efficient algorithm for computing exact equilibria in graphical games with arbitrary topology and the first algorithm to exploit finegrained structural properties of maids experimental results are presented demonstrating the  effectiveness of the algorithms and comparing them to predecessors  the running time of the graphical game algorithm is similar to and often better than the running time of previous approximate algorithms the algorithm for maids can effectively solve games that are much larger than those solvable by previous methods







we study the process of multiagent reinforcement learning in the context ofload balancing in a distributed system without use of either centralcoordination or explicit communication  we first define a precise frameworkin which to study adaptive load balancing important features of which are itsstochastic nature and the purely local information available to individualagents  given this framework we show illuminating results on the interplaybetween basic adaptive behavior parameters and their effect on systemefficiency  we then investigate the properties of adaptive load balancing inheterogeneous populations and address the issue of exploration vsexploitation in that context  finally we show that naive use ofcommunication may not improve and might even harm system efficiency







this paper presents an approach to expertguided subgroup    discovery  the main step of the subgroup discovery process the    induction of subgroup descriptions is performed by a heuristic beam    search algorithm using a novel parametrized definition of rule    quality which is analyzed in detail  the other important steps of the    proposed subgroup discovery process are the detection of statistically    significant properties of selected subgroups and subgroup    visualization statistically significant properties are used to enrich    the descriptions of induced subgroups while the visualization shows    subgroup properties in the form of distributions of the numbers of    examples in the subgroups the approach is illustrated by the results    obtained for a medical problem of early detection of patient risk    groups





r  he e  brunskill and n  roy 2011 efficient planning under uncertainty with macroactions volume 40 pages 523570



deciding how to act in partially observable environments remains an active area of research identifying good sequences of decisions is particularly challenging when good control performance requires planning multiple steps into the future in domains with many states towards addressing this challenge we present an online forwardsearch algorithm called the posterior belief distribution pbd pbd leverages a novel method for calculating the posterior distribution over beliefs that result after a sequence of actions is taken given the set of observation sequences that could be received during this process this method allows us to efficiently evaluate the expected reward of a sequence of primitive actions which we refer to as macroactions we present a formal analysis of our approach and examine its performance on two very large simulation experiments scientific exploration and a target monitoring domain we also demonstrate our algorithm being used to control a real robotic helicopter in a target monitoring experiment which suggests that our approach has practical potential for planning in realworld large partially observable domains where a multistep lookahead is required to achieve good performance



opus is a branch and bound search algorithm that enables    efficient admissible search through spaces for which the order of    search operator application is not significant  the algorithms    search efficiency is demonstrated with respect to very large machine    learning search spaces  the use of admissible search is of potential    value to the machine learning community as it means that the exact    learning biases to be employed for complex learning tasks can be    precisely specified and manipulated  opus also has potential for    application in other areas of artificial intelligence notably truth    maintenance







increasing the expressiveness of qualitative spatial calculi is an essential step towards meeting the requirements of applications this can be achieved by combining existing calculi in a way that we can express spatial information using relations from multiple calculi the great challenge is to develop reasoning algorithms that are correct and complete when reasoning over the combined information previous work has mainly studied cases where the interaction between the combined calculi was small or where one of the two calculi was very simple in this paper we tackle the important combination of topological and directional information for extended spatial objects we combine some of the best known calculi in qualitative spatial reasoning the rcc8 algebra for representing topological information and the rectangle algebra ra and the cardinal direction calculus cdc for directional information we consider two different interpretations of the rcc8 algebra one uses a weak connectedness relation the other uses a strong connectedness relation in both interpretations we show that reasoning with topological and directional information is decidable and remains in np our computational complexity results unveil the significant differences between ra and cdc and that between weak and strong rcc8 models take the combination of basic rcc8 and basic cdc constraints as an example we show that the consistency problem is in p only when we use the strong rcc8 algebra and explicitly know the corresponding basic ra constraints















2010 ijcaijair best paper prize



it has been widely observed that there is no single dominant sat solver instead different solvers perform best on different instances rather than following the traditional approach of choosing the best solver for a given class of instances we advocate making this decision online on a perinstance basis building on previous work we describe satzilla an automated approach for constructing perinstance algorithm portfolios for sat that use socalled empirical hardness models to choose among their constituent solvers this approach takes as input a distribution of problem instances and a set of component solvers and constructs a portfolio optimizing a given objective function such as mean runtime percent of instances solved or score in a competition the excellent performance of satzilla was independently verified in the 2007 sat competition where our satzilla07 solvers won three gold one silver and one bronze medal in this article we go well beyond satzilla07 by making the portfolio construction scalable and completely automated and improving it by integrating local search solvers as candidate solvers by predicting performance score instead of runtime and by using hierarchical hardness models that take into account different types of sat instances we demonstrate the effectiveness of these new techniques in extensive experimental results on data sets including instances from the most recent sat competition



this paper presents new experimental evidence against the    utility of occams razor  asystematic procedure is presented for    postprocessing decision trees produced by c45  this procedure was    derived by rejecting occams razor and instead attending to the    assumption that similar objects are likely to belong to the same    class  it increases a decision trees complexity without altering the    performance of that tree on the training data from which it is    inferred  the resulting more complex decision trees are demonstrated    to have on average for a variety of common learning tasks higher    predictive accuracy than the less complex original decision trees    this result raises considerable doubt about the utility of occams    razor as it is commonly applied in modern machine learning









j  sturm c  stachniss and w  burgard 2011 a probabilistic framework for learning kinematic models of articulated objects volume 41 pages 477526



robots operating in domestic environments generally need to interact with articulated objects such as doors cabinets dishwashers or fridges in this work we present a novel probabilistic framework for modeling articulated objects as kinematic graphs vertices in this graph correspond to object parts while edges between them model their kinematic relationship in particular we present a set of parametric and nonparametric edge models and how they can robustly be estimated from noisy pose observations we furthermore describe how to estimate the kinematic structure and how to use the learned kinematic models for pose prediction and for robotic manipulation tasks we finally present how the learned models can be generalized to new and previously unseen objects  in various experiments using real robots with different camera systems as well as in simulation we show that our approach is valid accurate and efficient further we demonstrate that our approach has a broad set of applications in particular for the emerging fields of mobile manipulation and service robotics





p  d turney 2012 domain and function a dualspace model of semantic relations and compositions volume 44 pages 533585



given appropriate representations of the semantic relations between carpenter and wood and between mason and stone for example vectors in a vector space model a suitable algorithm should be able to recognize that these relations are highly similar carpenter is to wood as mason is to stone the relations are analogous likewise with representations of dog house and kennel an algorithm should be able to recognize that the semantic composition of dog and house dog house is highly similar to kennel dog house and kennel are synonymous it seems that these two tasks recognizing relations and compositions are closely connected however up to now the best models for relations are significantly different from the best models for compositions in this paper we introduce a dualspace model that unifies these two tasks this model matches the performance of the best previous models for relations and compositions the dualspace model consists of a space for measuring domain similarity and a space for measuring function similarity carpenter and wood share the same domain the domain of carpentry mason and stone share the same domain the domain of masonry carpenter and mason share the same function the function of artisans wood and stone share the same function the function of materials in the composition dog house kennel has some domain overlap with both dog and house the domains of pets and buildings the function of kennel is similar to the function of house the function of shelters by combining domain and function similarities in various ways we can model relations compositions and other aspects of semantics

