

e  wiewiora 2003 potentialbased shaping and qvalue initialization are equivalent volume 19 pages 205208



shaping has proven to be a powerful but precarious means of improving reinforcement learning performance ng harada and russell 1999 proposed the potentialbased shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior   in this note we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms more specifically we prove that a reinforcement learner with initial qvalues based on the shaping algorithms potential function make the same updates throughout learning as a learner receiving potentialbased shaping rewards we further prove that under a broad category of policies the behavior of these two learners are indistinguishable the comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithms benefit in addition the equivalence raises previously unaddressed issues concerning the efficiency of learning with potentialbased shaping





t  drakengren and  p  jonsson 1997 a complete classification of tractability in rcc5 volume 6 pages 211221



we investigate the computational properties of the spatial    algebra rcc5 which is a restricted version of the rcc framework for    spatial reasoning  the satisfiability problem for rcc5 is known to    be npcomplete but not much is known about its approximately four    billion subclasses we provide a complete classification of    satisfiability for all these subclasses into polynomial and    npcomplete respectively in the process we identify all maximal    tractable subalgebras which are four in total









a  felner r  e korf r  meshulam and r  c holte 2007 compressed pattern databases volume 30 pages 213247



s  qu and j  y chai 2010 contextbased word acquisition for situated dialogue in a virtual world volume 37 pages 247277



to tackle the vocabulary problem in conversational systems previous work has applied unsupervised learning approaches on cooccurring  speech and eye gaze during interaction to automatically acquire new words although these approaches have shown promise several issues related to human language behavior and humanmachine conversation have not been addressed first psycholinguistic studies have shown certain temporal regularities between human eye movement and language production while these regularities can potentially guide the acquisition process they have not been incorporated in the previous unsupervised approaches second conversational systems generally have an existing knowledge base about the domain and vocabulary while the existing knowledge can potentially help bootstrap and constrain the acquired new words it has not been incorporated in the previous models third eye gaze could serve different functions in humanmachine conversation some gaze streams may not be closely coupled with speech stream and thus are potentially detrimental to word acquisition automated recognition of closelycoupled speechgaze streams based on conversation context is important to address these issues we developed new approaches that incorporate user language behavior domain knowledge and conversation context in word acquisition we evaluated these approaches in the context of situated dialogue in a virtual world our experimental results have shown that incorporating the above three types of contextual information significantly improves word acquisition performance





a  montoyo  a  suarez  g  rigau and  m  palomar 2005 combining knowledge and corpusbased wordsensedisambiguation methods volume 23 pages 299330



in this paper we concentrate on the resolution of the lexical ambiguity that arises when a given word has several different meanings this specific task is commonly referred to as word sense disambiguation wsd the task of wsd consists of assigning the correct sense to words using an electronic dictionary as the source of word definitions we present two wsd methods based on two main methodological approaches in this research area a knowledgebased method and a corpusbased method our hypothesis is that wordsense disambiguation requires several knowledge sources in order to solve the semantic ambiguity of the words these sources can be of different kinds for example syntagmatic paradigmatic or statistical information our approach combines various sources of knowledge through combinations of the two wsd methods mentioned above mainly the paper concentrates on how to combine these methods and sources of information in order to achieve good results in the disambiguation finally this paper presents a comprehensive study and experimental work on evaluation of the methods and their combinations





s  park  e  h durfee and  w  p birmingham 2004 use of markov chains to design an agent bidding strategy for continuous double auctions volume 22 pages 175214



as computational agents are developed for increasingly complicated ecommerce applications the complexity of the decisions they face demands advances in artificial intelligence techniques for example an agent representing a seller in an auction should try to maximize the sellers profit by reasoning about a variety of possibly uncertain pieces of information such as the maximum prices various buyers might be willing to pay the possible prices being offered by competing sellers the rules by which the auction operates the dynamic arrival and matching of offers to buy and sell and so on a naive application of multiagent reasoning techniques would require the sellers agent to explicitly model all of the other agents through an extended time horizon rendering the problem intractable for many realisticallysized problems we have instead devised a new strategy that an agent can use to determine its bid price based on a more tractable markov chain model of the auction process  we have experimentally identified the conditions under which our new strategy works well as well as how well it works in comparison to the optimal performance the agent could have achieved had it known the future our results show that our new strategy in general performs well outperforming other tractable heuristic strategies in a majority of experiments and is particularly effective in a sellers market where many buy offers are available









i  muslea s  minton and c  a knoblock 2006 active learning with multiple views volume 27 pages 203233



j  p watson  l  d whitley and  a  e howe 2005 linking search space structure runtime dynamics and problem difficulty a step toward demystifying tabu search volume 24 pages 221261



tabu search is one of the most effective heuristics for locating highquality solutions to a diverse array of nphard combinatorial optimization problems despite the widespread success of tabu search researchers have a poor understanding of many key theoretical aspects of this algorithm including models of the highlevel runtime dynamics and identification of those search space features that influence problem  difficulty we consider these questions in the context of the jobshop  scheduling problem jsp a domain where tabu search algorithms have  been shown to be remarkably effective previously we demonstrated  that the mean distance between random local optima and the nearest  optimal solution is highly correlated with problem difficulty for a  wellknown tabu search algorithm for the jsp introduced by taillard in this paper we discuss various shortcomings of this measure and  develop a new model of problem difficulty that corrects these deficiencies we show that taillards algorithm can be modeled  with high fidelity as a simple variant of a straightforward random  walk the random walk model accounts for nearly all of the variability in the cost required to locate both optimal and suboptimal solutions to random jsps and provides an explanation for differences in the difficulty of random versus structured jsps finally we discuss and  empirically substantiate two novel predictions regarding tabu search  algorithm behavior first the method for constructing the initial solution is  highly unlikely to impact the performance of tabu search second tabu  tenure should be selected to be as small as possible while simultaneously  avoiding search stagnation values larger than necessary lead to  significant degradations in performance





d  e wilkins  t  j lee and  p  berry 2003 interactive execution monitoring of agent teams volume 18 pages 217261



there is an increasing need for automated support for humans    monitoring the activity of distributed teams of cooperating agents    both human and machine we characterize the domainindependent    challenges posed by this problem and describe how properties of    domains influence the challenges and their solutions we will    concentrate on dynamic datarich domains where humans are ultimately    responsible for team behavior thus the automated aid should    interactively support effective and timely decision making by the    human we present a domainindependent categorization of the types of    alerts a planbased monitoring system might issue to a user where    each type generally requires different monitoring techniques we    describe a monitoring framework for integrating many domainspecific    and taskspecific monitoring techniques and then using the concept of    value of an alert to avoid operator overload       we use this framework to describe an execution monitoring approach we    have used to implement execution assistants eas in two different    dynamic datarich realworld domains to assist a human in    monitoring team behavior one domain army small unit operations has    hundreds of mobile geographically distributed agents a combination    of humans robots and vehicles the other domain teams of unmanned    ground and air vehicles has a handful of cooperating robots both    domains involve unpredictable adversaries in the vicinity our    approach customizes monitoring behavior for each specific task plan    and situation as well as for user preferences our eas alert the    human controller when reported events threaten plan execution or    physically threaten team members alerts were generated in a timely    manner without inundating the user with too many alerts less than    10 percent of alerts are unwanted as judged by domain experts





t  lukasiewicz 1999 probabilistic deduction with conditional constraints over basic events volume 10 pages 199241



we study the problem of probabilistic deduction with    conditional constraints over basic events we show that globally    complete probabilistic deduction with conditional constraints over    basic events is nphard we then concentrate on the special case of    probabilistic deduction in conditional constraint trees we elaborate    very efficient techniques for globally complete probabilistic    deduction in detail for conditional constraint trees with point    probabilities we present a local approach to globally complete    probabilistic deduction which runs in linear time in the size of the    conditional constraint trees for conditional constraint trees with    interval probabilities we show that globally complete probabilistic    deduction can be done in a global approach by solving nonlinear    programs we show how these nonlinear programs can be transformed into    equivalent linear programs which are solvable in polynomial time in    the size of the conditional constraint trees







zenglin  xu  shandian  zhe yuan  qi and peng  yu 2016 association discovery and diagnosis of alzheimers disease with bayesian multiview learning volume 56 pages 247268



the analysis and diagnosis of alzheimers disease ad can be based on genetic variations eg single nucleotide polymorphisms snps and phenotypic traits eg magnetic resonance imaging mri features we consider two important and related tasks i to select genetic and phenotypical markers for ad diagnosis and ii to identify associations between genetic and phenotypical data while previous studies treat these two tasks separately they are tightly coupled because underlying associations between genetic variations and phenotypical features contain the biological basis for a disease here we present a new sparse bayesian approach for joint association study and disease diagnosis in this approach common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels in return the disease status can guide the discovery of relationships between data sources the sparse projection matrices not only reveal interactions between data sources but also select groups of biomarkers related to the disease moreover to take advantage of the linkage disequilibrium ld measuring the nonrandom association of alleles we incorporate a graph laplacian type of prior in the model to learn the model from data we develop an efficient variational inference algorithm analysis on an imaging genetics dataset for the study of alzheimers disease ad indicates that our model identifies biologically meaningful associations between genetic variations and mri features and achieves significantly higher accuracy for predicting ordinal ad stages than the competing methods



g r  santhanam s  basu and v  honavar 2011 representing and reasoning with qualitative preferences for compositional systems volume 42 pages 211274



many applications eg web service composition complex system design team formation etc rely on methods for identifying collections of objects or entities satisfying some functional requirement among the collections that satisfy the functional requirement it is often necessary to identify one or more collections that are optimal with respect to user preferences over a set of attributes that describe the nonfunctional properties of the collection

we provide algorithms that use this dominance relation to identify the set of most preferred collections we show that under certain conditions the algorithms are guaranteed to return only sound all complete or at least one weakly complete of the most preferred collections we present results of simulation experiments comparing the proposed algorithms with respect to a the quality of solutions number of most preferred solutions produced by the algorithms and b their performance and efficiency we also explore some interesting conjectures suggested by the results of our experiments that relate the properties of the user preferences the dominance relation and the algorithms







t  grinshpoun and a  meisels 2008 completeness and performance of the apo algorithm volume 33 pages 223258



asynchronous partial overlay apo is a search algorithm that uses cooperative mediation to solve distributed constraint satisfaction problems discsps the algorithm partitions the search into different subproblems of the discsp the original proof of completeness of the apo algorithm is based on the growth of the size of the subproblems the present paper demonstrates that this expected growth of subproblems does not occur in some situations leading to a termination problem of the algorithm the problematic parts in the apo algorithm that interfere with its completeness are identified and necessary modifications to the algorithm that fix these problematic parts are given the resulting version of the algorithm complete asynchronous partial overlay compapo ensures its completeness formal proofs for the soundness and completeness of compapo are given a detailed performance evaluation of compapo comparing it to other discsp algorithms is presented along with an extensive experimental evaluation of the algorithms unique behavior additionally an optimization version of the algorithm compoptapo is presented discussed and evaluated



b  vandegriend and  j  culberson 1998 the gnm phase transition is not hard for the hamiltonian cycle problem volume 9 pages 219245



using an improved backtrack algorithm with sophisticated    pruning techniques we revise previous observations correlating a high    frequency of hard to solve hamiltonian cycle instances with the gnm    phase transition between hamiltonicity and nonhamiltonicity instead    all tested graphs of 100 to 1500 vertices are easily solved       when we artificially restrict the degree sequence with a bounded    maximum degree although there is some increase in difficulty the    frequency of hard graphs is still low  when we consider more regular    graphs based on a generalization of knights tours we observe    frequent instances of really hard graphs but on these the average    degree is bounded by a constant  we design a set of graphs with a    feature our algorithm is unable to detect and so are very hard for our    algorithm but in these we can vary the average degree from o1 to    on  we have so far found no class of graphs correlated with the    gnm phase transition which asymptotically produces a high frequency    of hard instances





j  l ambite and  c  a knoblock 2001 planning by rewriting volume 15 pages 207261



domainindependent planning is a hard combinatorial    problem taking into account plan quality makes the task even more    difficult this article introduces planning by rewriting pbr a new    paradigm for efficient highquality domainindependent planning pbr    exploits declarative planrewriting rules and efficient local search    techniques to transform an easytogenerate but possibly suboptimal    initial plan into a highquality plan in addition to addressing the    issues of planning efficiency and plan quality this framework offers    a new anytime planning algorithm we have implemented this planner and    applied it to several existing domains the experimental results show    that the pbr approach provides significant savings in planning effort    while generating highquality plans







s  pado and m  lapata 2009 crosslingual annotation projection for semantic roles volume 36 pages 307340



 this article considers the task of automatically inducing rolesemantic annotations in the framenet paradigm for new languages  we propose a general framework that is based on annotation projection phrased as a graph optimization problem it is relatively inexpensive and has the potential to reduce the human effort involved in creating rolesemantic resources within this framework we present projection models that exploit lexical and syntactic information we provide an experimental evaluation on an englishgerman parallel corpus which demonstrates the feasibility of inducing highprecision german semantic role annotation both for manually and automatically annotated english data



s  tobies 2000 the complexity of reasoning with cardinality restrictions and nominals in expressive description logics volume 12 pages 199217



we study the complexity of the combination of the    description logics alcq and alcqi with a terminological formalism    based on cardinality restrictions on concepts these combinations can    naturally be embedded into c2 the two variable fragment of predicate    logic with counting quantifiers which yields decidability in    nexptime we show that this approach leads to an optimal solution for    alcqi as alcqi with cardinality restrictions has the same complexity    as c2 nexptimecomplete in contrast we show that for alcq the    problem can be solved in exptime this result is obtained by a    reduction of reasoning with cardinality restrictions to reasoning with    the in general weaker terminological formalism of general axioms for    alcq extended with nominals using the same reduction we show that    for the extension of alcqi with nominals reasoning with general    axioms is a nexptimecomplete problem finally we sharpen this result    and show that pure concept satisfiability for alcqi with nominals is    nexptimecomplete without nominals this problem is known to be    pspacecomplete







m  g bellemare y  naddaf j  veness and m  bowling 2013 the arcade learning environment an evaluation platform for general agents volume 47 pages 253279



in this article we introduce the arcade learning environment ale both a challenge problem and a platform and methodology for evaluating the development of general domainindependent ai technology  ale provides an interface to hundreds of atari 2600 game environments each one different interesting and designed to be a challenge for human players  ale presents significant research challenges for reinforcement learning model learning modelbased planning imitation learning transfer learning and intrinsic motivation  most importantly it provides a rigorous testbed for evaluating and comparing approaches to these problems  we illustrate the promise of ale by developing and benchmarking domainindependent agents designed using wellestablished ai techniques for both reinforcement learning and planning  in doing so we also propose an evaluation methodology made possible by ale reporting empirical results on over 55 different games  all of the software including the benchmark agents is publicly available



j246rg  tiedemann and zeljko  agi263 2016 synthetic treebanking for crosslingual dependency parsing volume 55 pages 209248



how do we parse the languages for which no treebanks are available this contribution addresses the crosslingual viewpoint on statistical dependency parsing in which we attempt to make use of resourcerich source language treebanks to build and adapt models for the underresourced target languages we outline the benefits and indicate the drawbacks of the current major approaches we emphasize synthetic treebanking the automatic creation of target language treebanks by means of annotation projection and machine translation we present competitive results in crosslingual dependency parsing using a combination of various techniques that contribute to the overall success of the method we further include a detailed discussion about the impact of partofspeech label accuracy on parsing results that provide guidance in practical applications of crosslingual methods for truly underresourced languages









shan  xue alan  fern and daniel  sheldon 2015 scheduling conservation designs for maximum flexibility via network cascade optimization volume 52 pages 331360



l  blumrosen n  nisan and i  segal 2007 auctions with severely bounded communication volume 28 pages 233266



we study auctions with severe bounds on the communication allowed each bidder  may only transmit t bits of information to the auctioneer we consider both welfare and profitmaximizing auctions under this communication restriction for both measures we determine the optimal auction and show that the loss incurred relative to unconstrained auctions is mild we prove nonsurprising properties of these kinds of auctions eg that in optimal mechanisms bidders  simply report the interval in which their valuation lies in  as well as some surprising properties eg that asymmetric auctions  are better than symmetric ones and that multiround auctions reduce the communication complexity only by a linear factor







r  jurca and b  faltings 2009 mechanisms for making crowds truthful volume 34 pages 209253



we consider schemes for obtaining truthful reports on a common but hidden signal from large groups of rational selfinterested agents one example are online feedback mechanisms where users provide observations about the quality of a product or service so that other users can have an accurate idea of what quality they can expect however i providing such feedback is costly and ii there are many motivations for providing incorrect feedback

in this paper we extend existing methods for designing incentivecompatible rewards by also considering collusion we analyze different scenarios where for example some or all of the agents collude for each scenario we investigate whether a collusionresistant incentivecompatible reward scheme exists and use automated mechanism design to specify an algorithm for deriving an efficient reward mechanism











i  kollia and b  glimm 2013 optimizing sparql query answering over owl ontologies volume 48 pages 253303



the sparql query language is currently being extended by the world wide web consortium w3c with socalled entailment regimes an entailment regime defines how queries are evaluated under more expressive semantics than sparqls standard simple entailment which is based on subgraph matching the queries are very expressive since variables can occur within complex concepts and can also bind to concept or role names

we provide a prototypical implementation and evaluate the efficiency of the proposed optimizations our experimental study shows that the static ordering usually outperforms the dynamic one when accurate statistics are available this changes however when the statistics are less accurate eg due to nondeterministic reasoning decisions for queries that go beyond conjunctive instance queries we observe an improvement of up to three orders of magnitude due to the proposed optimizations







m  hl van den briel t  vossen and s  kambhampati 2008 loosely coupled formulations for automated planning an integer programming perspective volume 31 pages 217257



diego  figueira santiago  figueira and carlos  areces 2015 model theory of xpath on data trees part i bisimulation and characterization volume 53 pages 271314



we investigate model theoretic properties of xpath with data inequality tests over the class of data trees ie the class of trees where each node contains a label from a finite alphabet and a data value from an infinite domain





l  leherte  j  glasgow  k  baxter  e  steeg and  s  fortier 1997 analysis of threedimensional protein images volume 7 pages 125159



a fundamental goal of research in molecular biology is to    understand protein structure protein crystallography is currently the    most successful method for determining the threedimensional 3d    conformation of a protein yet it remains labor intensive and relies    on an experts ability to derive and evaluate a protein scene model    in this paper the problem of protein structure determination is    formulated as an exercise in scene analysis  a computational    methodology is presented in which a 3d image of a protein is segmented    into a graph of critical points  bayesian and certainty factor    approaches are described and used to analyze critical point graphs and    identify meaningful substructures such as alphahelices and    betasheets  results of applying the methodologies to protein images    at low and medium resolution are reported  the research is related to    approaches to representation segmentation and classification in    vision as well as to topdown approaches to protein structure    prediction







jhm  lee and k  l leung 2012 consistency techniques for flowbased projectionsafe global cost functions in weighted constraint satisfaction volume 43 pages 257292



many combinatorial problems deal with preferences and violations the goal of which is to find solutions with the minimum cost  weighted constraint satisfaction is a framework for modeling such problems which consists of a set of cost functions to measure the degree of violation or preferences of different combinations of variable assignments  typical solution methods for weighted constraint satisfaction problems wcsps are based on branchandbound search which are made practical through the use of powerful consistency techniques such as ac fdac edac to deduce hidden cost information and value pruning during search  these techniques however are designed to be efficient only on binary and ternary cost functions which are represented in table form  in tackling many reallife problems high arity or global cost functions are required  we investigate efficient representation scheme and algorithms to bring the benefits of the consistency techniques to also high arity cost functions which are often derived from hard global constraints from classical constraint satisfaction

the literature suggests some global cost functions can be represented as flow networks and the minimum cost flow algorithm can be used to compute the minimum costs of such networks in polynomial time  we show that naive adoption of this flowbased algorithmic method for global cost functions can result in a stronger form of nullinverse consistency  we further show how the method can be modified to handle cost projections and extensions to maintain generalized versions of ac and fdac for cost functions with more than two variables  similar generalization for the stronger edac is less straightforward  we reveal the oscillation problem when enforcing edac on cost functions sharing more than one variable  to avoid oscillation we propose a weak version of edac and generalize it to weak edgac for nonbinary cost functions using various benchmarks involving the soft variants of hard global constraints alldifferent gcc same and regular empirical results demonstrate that our proposal gives improvements of up to an order of magnitude when compared with the traditional constraint optimization approach both in terms of time and pruning 





a  feldman g  provan and a  van gemund 2010 a modelbased active testing approach to sequential diagnosis volume 39 pages 301334



modelbased diagnostic reasoning often leads to a large number of diagnostic hypotheses the set of diagnoses can be reduced by taking into account extra observations passive monitoring measuring additional variables probing or executing additional tests sequential diagnosistest sequencing in this paper we combine the above approaches with techniques from automated test pattern generation atpg and modelbased diagnosis mbd into a framework called fractal framework for active testing algorithms apart from the inputs and outputs that connect a system to its environment in active testing we consider additional input variables to which a sequence of test vectors can be supplied we address the computationally hard problem of computing optimal control assignments as defined in fractal in terms of a greedy approximation algorithm called fractalg we compare the decrease in the number of remaining minimal cardinality diagnoses of fractalg to that of two more fractal algorithms fractalatpg and fractalp fractalatpg is based on atpg and sequential diagnosis while fractalp is based on probing and although not an active testing algorithm provides a baseline for comparing the lower bound on the number of reachable diagnoses for the fractal algorithms we empirically evaluate the tradeoffs of the three fractal algorithms by performing extensive experimentation on the iscas8574xxx benchmark of combinational circuits







k  r apt and g  schaefer 2014 selfishness level of strategic games volume 49 pages 207240



d  e smith 2003 the case for durative actions a commentary on pddl21 volume 20 pages 149154



the addition of durative actions to pddl21 sparked some controversy fox and long argued that actions should be considered as instantaneous but can start and stop processes  ultimately a limited notion of durative actions was incorporated into the language i argue that this notion is still impoverished and that the underlying philosophical position of regarding durative actions as being a shorthand for a start action process and stop action ignores the realities of modelling and execution for complex systems





2009 ijcaijair best paper prize



information about user preferences plays a key role in automated decision making in many domains it is desirable to assess such preferences in a qualitative rather than quantitative way in this paper we propose a qualitative graphical representation of preferences that reflects conditional dependence and independence of preference statements under a ceteris paribus all else being equal interpretation such a representation is often compact and arguably quite natural in many circumstances we provide a formal semantics for this model and describe how the structure of the network can be exploited in several inference tasks such as determining whether one outcome dominates is preferred to another ordering a set outcomes according to the preference relation and constructing the best outcome subject to available evidence





d  lesaint d  mehta b  osullivan l  quesada and n  wilson 2010 developing approaches  for solving a telecommunications feature subscription problem volume 38 pages 271305



call control features eg calldivert voicemail are primitive options to which users can subscribe offline to personalise their  service the configuration of a feature subscription involves choosing and sequencing features from a catalogue and is subject to  constraints that prevent undesirable feature interactions at runtime when the subscription requested by a user is inconsistent one  problem is to find an optimal relaxation  which is a generalisation of the feedback vertex  set problem on directed graphs and thus it is an nphard task we present several constraint programming formulations of the problem we also present formulations using partial  weighted maximum boolean satisfiability and mixed integer linear programming  we study all these formulations by experimentally comparing them  on a variety of randomly generated instances of the feature subscription problem





t  g dietterich 2000 hierarchical reinforcement learning with the maxq value function decomposition volume 13 pages 227303











p  f felzenszwalb and d  mcallester 2007 the generalized a architecture volume 29 pages 153190





n  goernitz m  kloft k  rieck and u  brefeld 2013 toward supervised anomaly detection volume 46 pages 235262



anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions however the predictive performance of purely unsupervised anomaly detection  often fails to match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation our first contribution shows that  classical semisupervised approaches originating from a supervised classifier are inappropriate and hardly detect  new and unknown anomalies we argue that semisupervised anomaly detection  needs to ground on the unsupervised learning paradigm and devise a novel algorithm that meets this requirement although being intrinsically nonconvex we further show that the optimization problem has a convex equivalent under relatively mild assumptions additionally we propose an active learning strategy to automatically filter candidates for  labeling in an empirical study on network intrusion detection data we observe that the proposed learning methodology requires much less labeled data than the stateoftheart while achieving higher detection accuracies







p  faliszewski e  hemaspaandra l  a hemaspaandra and j  rothe 2009 llull and copeland voting computationally resist bribery and constructive control volume 35 pages 275341





meir  kalech and shulamit  reches 2015 decision making with dynamic uncertain events volume 54 pages 233275



when to make a decision is a key question in decision making problems characterized by uncertainty in this paper we deal with decision making in environments where information arrives dynamically we address the tradeoff between waiting and stopping strategies on the one hand waiting to obtain more information reduces uncertainty but it comes with a cost stopping and making a decision based on an expected utility reduces the cost of waiting but the decision is based on uncertain information we propose an optimal algorithm and two approximation algorithms we prove that one approximation is optimistic  waits at least as long as the optimal algorithm while the other is pessimistic  stops not later than the optimal algorithm we evaluate our algorithms theoretically and empirically and show that the quality of the decision in both approximations is nearoptimal and much faster than the optimal algorithm also we can conclude from the experiments that the cost function is a key factor to chose the most effective algorithm







a  gerevini a  saetti and i  serina 2006 an approach to temporal planning and scheduling in domains with predictable exogenous events volume 25 pages 187231



n  nilsson 1994 teleoreactive programs for agent control volume 1 pages 139158



a formalism is presented for computing and organizing actions for autonomous agents in dynamic environments we introduce the notion of iteleoreactive tr programsi whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based  in addition to continuous feedback tr programs support parameter binding and recursion  a primary difference between tr programs and many other circuitbased systems is that the circuitry of tr programs is more compact it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs  in addition tr programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods  we briefly describe some experimental applications of tr programs in the control of simulated and actual mobile robots





r  rosati 1999 reasoning about minimal belief and negation as failure volume 11 pages 277300



we investigate the problem of reasoning in the propositional    fragment of mbnf the logic of minimal belief and negation as failure    introduced by lifschitz which can be considered as a unifying    framework for several nonmonotonic formalisms including default    logic autoepistemic logic circumscription epistemic queries and    logic programming  we characterize the complexity and provide    algorithms for reasoning in propositional mbnf  in particular we    show that entailment in propositional mbnf lies at the third level of    the polynomial hierarchy hence it is harder than reasoning in all the    above mentioned propositional formalisms for nonmonotonic reasoning    we also prove the exact correspondence between negation as failure in    mbnf and negative introspection in moores autoepistemic logic





s  minton  j  bresina and  m  drummond 1994 totalorder and partialorder planning a comparative analysis volume 2 pages 227262



for many years the intuitions underlying partialorder   planning were largely taken for granted only in the past few years   has there been renewed interest in the fundamental principles   underlying this paradigm  in this paper we present a rigorous   comparative analysis of partialorder and totalorder planning by   focusing on two specific planners that can be directly compared we   show that there are some subtle assumptions that underly the   widespread intuitions regarding the supposed efficiency of   partialorder planning for instance the superiority ofpartialorder   planning can depend critically upon the search strategy and the   structure of the search space  understanding the underlying   assumptions is crucial for constructing efficient planners





m  a walker  i  langkildegeary  h  wright hastie  j  wright and  a  gorin 2002 automatically training a problematic dialogue predictor for a spoken dialogue system volume 16 pages 293319



spoken dialogue systems promise efficient and natural access    to a large variety of information sources and services from any phone    however current spoken dialogue systems are deficient in their    strategies for preventing identifying and repairing problems that    arise in the conversation this paper reports results on automatically    training a problematic dialogue predictor to predict problematic    humancomputer dialogues using a corpus of 4692 dialogues collected    with the how may i help you sm spoken dialogue system  the    problematic dialogue predictor can be immediately applied to the    systems decision of whether to transfer the call to a human customer    care agent or be used as a cue to the systems dialogue manager to    modify its behavior to repair problems and even perhaps to prevent    them we show that a problematic dialogue predictor using    automaticallyobtainable features from the first two exchanges in the    dialogue can predict problematic dialogues 132 more accurately than    the baseline





a  darwiche and  p  marquis 2002 a knowledge compilation map volume 17 pages 229264







c  lecoutre s  cardon and j  vion 2011 secondorder consistencies volume 40 pages 175219



in this paper we propose a comprehensive study of secondorder consistencies ie consistencies identifying inconsistent pairs of values for constraint satisfaction we build a full picture of the relationships existing between four basic secondorder consistencies namely path consistency pc 3consistency 3c dual consistency dc and 2singleton arc consistency 2sac as well as their conservative and strong variants interestingly dual consistency is an original property that can be established by using the outcome of the enforcement of generalized arc consistency gac which makes it rather easy to obtain since constraint solvers typically maintain gac during search  on binary constraint  networks dc is equivalent to pc but its restriction to existing constraints called conservative dual consistency cdc is strictly stronger than  traditional conservative consistencies derived from path consistency namely partial path consistency ppc and conservative path consistency cpc  after introducing a general algorithm to enforce strong cdc we present the results of an experimentation over a wide range of benchmarks that demonstrate the interest of conservative dual consistency  in particular we show that enforcing cdc before search clearly improves the performance of mac the algorithm that maintains gac during search on several binary and nonbinary structured problems





g  pinkas and  r  dechter 1995 improving connectionist energy minimization volume 3 pages 223248



symmetric networks designed for energy minimization such as    boltzman machines and hopfield nets are frequently investigated for    use in optimization constraint satisfaction and approximation of    nphard problems nevertheless finding a global solution ie a    global minimum for the energy function is not guaranteed and even a    local solution may take an exponential number of steps  we propose an    improvement to the standard local activation function used for such    networks  the improved algorithm guarantees that a global minimum is    found in linear time for treelike subnetworks the algorithm called    activate is uniform and does not assume that the network is    treelike  it can identify treelike subnetworks even in cyclic    topologies arbitrary networks and avoid local minima along these    trees  for acyclic networks the algorithm is guaranteed to converge    to a global minimum from any initial state of the system    selfstabilization and remains correct under various types of    schedulers  on the negative side we show that in the presence of    cycles no uniform algorithm exists that guarantees optimality even    under a sequential asynchronous scheduler  an asynchronous scheduler    can activate only one unit at a time while a synchronous scheduler can    activate any number of units in a single time step  in addition no    uniform algorithm exists to optimize even acyclic networks when the    scheduler is synchronous  finally we show how the algorithm can be    improved using the cyclecutset scheme  the general algorithm called    activatewithcutset improves over activate and has some performance    guarantees that are related to the size of the networks cyclecutset









z  zhuang and m  pagnucco 2014 entrenchmentbased horn contraction volume 51 pages 227254



r  a helzerman and  m  p harper 1996 muse csp an extension to the constraint satisfaction problem volume 5 pages 239288



this paper describes an extension to the constraint    satisfaction problem csp called muse csp multiply segmented    constraint satisfaction problem  this extension is especially useful    for those problems which segment into multiple sets of partially    shared variables  such problems arise naturally in signal processing    applications including computer vision speech processing and    handwriting recognition  for these applications it is often    difficult to segment the data in only one way given the lowlevel    information utilized by the segmentation algorithms  muse csp can be    used to compactly represent several similar instances of the    constraint satisfaction problem  if multiple instances of a csp have    some common variables which have the same domains and constraints    then they can be combined into a single instance of a muse csp    reducing the work required to apply the constraints  we introduce the    concepts of muse node consistency muse arc consistency and muse path    consistency we then demonstrate how muse csp can be used to compactly    represent lexically ambiguous sentences and the multiple sentence    hypotheses that are often generated by speech recognition algorithms    so that grammar constraints can be used to provide parses for all    syntactically correct sentences  algorithms for muse arc and path    consistency are provided  finally we discuss how to create a muse    csp from a set of csps which are labeled to indicate when the same    variable is shared by more than a single csp





n  rivera l  illanes j  a baier and c  hernandez 2014 reconnection with the ideal tree a new approach to realtime search volume 50 pages 235264



many applications ranging from video games to dynamic robotics require solving singleagent deterministic search problems in partially known environments under very tight time constraints realtime heuristic search rths algorithms are specifically designed for those applications as a subroutine most of them invoke a standard but bounded search algorithm that searches for the goal in this paper we present frit a simple approach for singleagent deterministic search problems under tight constraints and partially known environments that unlike traditional rths does not search for the goal but rather searches for a path that connects the current state with a socalled ideal tree t  when the agent observes that an arc in the tree cannot be traversed in the actual environment it removes such an arc from t and then carries out a reconnection search whose objective is to find a path between the current state and any node in t  the reconnection search is done using an algorithm that is passed as a parameter to frit if such a parameter is an rths algorithm then the resulting algorithm can be an rths algorithm we show in addition that frit may be fed with a bounded complete blindsearch algorithm we evaluate our approach over grid pathfinding benchmarks including game maps and mazes our results show that frit used with rtaa a standard rths algorithm outperforms rtaa significantly by one order of magnitude under tight time constraints in addition fritdartaa substantially outperforms dartaa a stateoftheart rths algorithm usually obtaining solutions 50 cheaper on average when performing the same search effort finally fritbfs ie frit using breadthfirstsearch obtains bestquality solutions when time is limited compared to adaptive a and repeated a finally we show that bug2 a pathfindingspecific navigation algorithm outperforms fritbfs when planning time is extremely limited but when given more time the situation reverses





f  a oliehoek m  t j spaan and n  vlassis 2008 optimal and approximate qvalue functions for decentralized pomdps volume 32 pages 289353



decisiontheoretic planning is a popular approach to sequential decision making problems because it treats uncertainty in sensing and acting in a principled way in singleagent frameworks like mdps and pomdps planning can be carried out by resorting to qvalue functions an optimal qvalue function q is computed in a recursive manner by dynamic programming and then an optimal policy is extracted from q in this paper we study whether similar qvalue functions can be defined for decentralized pomdp models decpomdps and how policies can be extracted from such value functions we define two forms of the optimal qvalue function for decpomdps one that gives a normative description as the qvalue function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation this computation however is infeasible for all but the smallest problems therefore we analyze various approximate qvalue functions that allow for efficient computation we describe how they relate and we prove that they all provide an upper bound to the optimal qvalue function q  finally unifying some previous approaches for solving decpomdps we describe a family of algorithms for extracting policies from such qvalue functions and perform an experimental evaluation on existing test problems including a new firefighting benchmark problem





a  darwiche 1998 modelbased diagnosis using structured system descriptions volume 8 pages 165222



this paper presents a comprehensive approach for modelbased    diagnosis which includes proposals for characterizing and computing    preferred diagnoses assuming that the system description is augmented    with a system structure a directed graph explicating the    interconnections between system components  specifically we first    introduce the notion of a consequence which is a syntactically    unconstrained propositional sentence that characterizes all    consistencybased diagnoses and show that standard characterizations    of diagnoses such as minimal conflicts correspond to syntactic    variations on a consequence second we propose a new syntactic    variation on the consequence known as negation normal form nnf and    discuss its merits compared to standard variations  third we    introduce a basic algorithm for computing consequences in nnf given a    structured system description we show that if the system structure    does not contain cycles then there is always a linearsize    consequence in nnf which can be computed in linear time for arbitrary    system structures we show a precise connection between the complexity    of computing consequences and the topology of the underlying system    structure  finally we present an algorithm that enumerates the    preferred diagnoses characterized by a consequence the algorithm is    shown to take linear time in the size of the consequence if the    preference criterion satisfies some general conditions





d  a cohn  z  ghahramani and  m  i jordan 1996 active learning with statistical models volume 4 pages 129145



for many types of machine learning algorithms one can    compute the statistically optimal way to select training data  in    this paper we review how optimal data selection techniques have been    used with feedforward neural networks  we then show how the same    principles may be used to select data for two alternative    statisticallybased learning architectures mixtures of gaussians and    locally weighted regression  while the techniques for neural networks    are computationally expensive and approximate the techniques for    mixtures of gaussians and locally weighted regression are both    efficient and accurate  empirically we observe that the optimality    criterion sharply decreases the number of training examples the    learner needs in order to achieve good performance





s  joshi and r  khardon 2011 probabilistic relational planning  with first order decision diagrams volume 41 pages 231266



dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations in particular algebraic decision diagrams to capture domain dynamics and value functions  work on symbolic dynamic programming lifted these ideas to first order logic using several representation schemes  recent work introduced a first order variant of decision diagrams fodd and developed a value iteration algorithm for this representation this paper develops several improvements to the fodd algorithm that make the approach practical these include new reduction operators that decrease the size of the representation several speedup techniques and techniques for value approximation  incorporating these the paper presents a planning system foddplanner for solving relational stochastic planning problems  the system is evaluated on several domains including problems from the recent international planning competition and shows competitive performance with top ranking systems this is the first demonstration of feasibility of this approach and it shows that abstraction through compact representation is a promising approach to stochastic planning





r  kusters and  a  borgida 2001 whats in an attribute consequences for the least common subsumer volume 14 pages 167203



functional relationships between objects called    attributes are of considerable importance in knowledge    representation languages including description logics dls a study    of the literature indicates that papers have made often implicitly    different assumptions about the nature of attributes whether they are    always required to have a value or whether they can be partial    functions the work presented here is the first explicit study of this    difference for subclasses of the classic dl involving the sameas    concept constructor  it is shown that although determining    subsumption between concept descriptions has the same complexity    though requiring different algorithms the story is different in the    case of determining the least common subsumer lcs for attributes    interpreted as partial functions the lcs exists and can be computed    relatively easily even in this case our results correct and extend    three previous papers about the lcs of dls  in the case where    attributes must have a value the lcs may not exist and even if it    exists it may be of exponential size  interestingly it is possible    to decide in polynomial time if the lcs exists









m  fox d  long and d  magazzeni 2012 planbased policies for efficient multiple battery load management volume 44 pages 335382





honorable mention for the 2009 ijcaijair best paper prize



fast downward is a classical planning system based on heuristic search it can deal with general deterministic planning problems encoded in the propositional fragment of pddl22 including advanced features like adl conditions and effects and derived predicates axioms like other wellknown planners such as hsp and ff fast downward is a progression planner searching the space of world states of a planning task in the forward direction however unlike other pddl planning systems fast downward does not use the propositional pddl representation of a planning task directly instead the input is first translated into an alternative representation called multivalued planning tasks which makes many of the implicit constraints of a propositional planning task explicit exploiting this alternative representation fast downward uses hierarchical decompositions of planning tasks for computing its heuristic function called the causal graph heuristic which is very different from traditional hsplike heuristics based on ignoring negative interactions of operators



e  wiewiora 2003 potentialbased shaping and qvalue initialization are equivalent volume 19 pages 205208



shaping has proven to be a powerful but precarious means of improving reinforcement learning performance ng harada and russell 1999 proposed the potentialbased shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior   in this note we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms more specifically we prove that a reinforcement learner with initial qvalues based on the shaping algorithms potential function make the same updates throughout learning as a learner receiving potentialbased shaping rewards we further prove that under a broad category of policies the behavior of these two learners are indistinguishable the comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithms benefit in addition the equivalence raises previously unaddressed issues concerning the efficiency of learning with potentialbased shaping





t  drakengren and  p  jonsson 1997 a complete classification of tractability in rcc5 volume 6 pages 211221



we investigate the computational properties of the spatial    algebra rcc5 which is a restricted version of the rcc framework for    spatial reasoning  the satisfiability problem for rcc5 is known to    be npcomplete but not much is known about its approximately four    billion subclasses we provide a complete classification of    satisfiability for all these subclasses into polynomial and    npcomplete respectively in the process we identify all maximal    tractable subalgebras which are four in total









a  felner r  e korf r  meshulam and r  c holte 2007 compressed pattern databases volume 30 pages 213247



s  qu and j  y chai 2010 contextbased word acquisition for situated dialogue in a virtual world volume 37 pages 247277



to tackle the vocabulary problem in conversational systems previous work has applied unsupervised learning approaches on cooccurring  speech and eye gaze during interaction to automatically acquire new words although these approaches have shown promise several issues related to human language behavior and humanmachine conversation have not been addressed first psycholinguistic studies have shown certain temporal regularities between human eye movement and language production while these regularities can potentially guide the acquisition process they have not been incorporated in the previous unsupervised approaches second conversational systems generally have an existing knowledge base about the domain and vocabulary while the existing knowledge can potentially help bootstrap and constrain the acquired new words it has not been incorporated in the previous models third eye gaze could serve different functions in humanmachine conversation some gaze streams may not be closely coupled with speech stream and thus are potentially detrimental to word acquisition automated recognition of closelycoupled speechgaze streams based on conversation context is important to address these issues we developed new approaches that incorporate user language behavior domain knowledge and conversation context in word acquisition we evaluated these approaches in the context of situated dialogue in a virtual world our experimental results have shown that incorporating the above three types of contextual information significantly improves word acquisition performance





a  montoyo  a  suarez  g  rigau and  m  palomar 2005 combining knowledge and corpusbased wordsensedisambiguation methods volume 23 pages 299330



in this paper we concentrate on the resolution of the lexical ambiguity that arises when a given word has several different meanings this specific task is commonly referred to as word sense disambiguation wsd the task of wsd consists of assigning the correct sense to words using an electronic dictionary as the source of word definitions we present two wsd methods based on two main methodological approaches in this research area a knowledgebased method and a corpusbased method our hypothesis is that wordsense disambiguation requires several knowledge sources in order to solve the semantic ambiguity of the words these sources can be of different kinds for example syntagmatic paradigmatic or statistical information our approach combines various sources of knowledge through combinations of the two wsd methods mentioned above mainly the paper concentrates on how to combine these methods and sources of information in order to achieve good results in the disambiguation finally this paper presents a comprehensive study and experimental work on evaluation of the methods and their combinations





s  park  e  h durfee and  w  p birmingham 2004 use of markov chains to design an agent bidding strategy for continuous double auctions volume 22 pages 175214



as computational agents are developed for increasingly complicated ecommerce applications the complexity of the decisions they face demands advances in artificial intelligence techniques for example an agent representing a seller in an auction should try to maximize the sellers profit by reasoning about a variety of possibly uncertain pieces of information such as the maximum prices various buyers might be willing to pay the possible prices being offered by competing sellers the rules by which the auction operates the dynamic arrival and matching of offers to buy and sell and so on a naive application of multiagent reasoning techniques would require the sellers agent to explicitly model all of the other agents through an extended time horizon rendering the problem intractable for many realisticallysized problems we have instead devised a new strategy that an agent can use to determine its bid price based on a more tractable markov chain model of the auction process  we have experimentally identified the conditions under which our new strategy works well as well as how well it works in comparison to the optimal performance the agent could have achieved had it known the future our results show that our new strategy in general performs well outperforming other tractable heuristic strategies in a majority of experiments and is particularly effective in a sellers market where many buy offers are available









i  muslea s  minton and c  a knoblock 2006 active learning with multiple views volume 27 pages 203233



j  p watson  l  d whitley and  a  e howe 2005 linking search space structure runtime dynamics and problem difficulty a step toward demystifying tabu search volume 24 pages 221261



tabu search is one of the most effective heuristics for locating highquality solutions to a diverse array of nphard combinatorial optimization problems despite the widespread success of tabu search researchers have a poor understanding of many key theoretical aspects of this algorithm including models of the highlevel runtime dynamics and identification of those search space features that influence problem  difficulty we consider these questions in the context of the jobshop  scheduling problem jsp a domain where tabu search algorithms have  been shown to be remarkably effective previously we demonstrated  that the mean distance between random local optima and the nearest  optimal solution is highly correlated with problem difficulty for a  wellknown tabu search algorithm for the jsp introduced by taillard in this paper we discuss various shortcomings of this measure and  develop a new model of problem difficulty that corrects these deficiencies we show that taillards algorithm can be modeled  with high fidelity as a simple variant of a straightforward random  walk the random walk model accounts for nearly all of the variability in the cost required to locate both optimal and suboptimal solutions to random jsps and provides an explanation for differences in the difficulty of random versus structured jsps finally we discuss and  empirically substantiate two novel predictions regarding tabu search  algorithm behavior first the method for constructing the initial solution is  highly unlikely to impact the performance of tabu search second tabu  tenure should be selected to be as small as possible while simultaneously  avoiding search stagnation values larger than necessary lead to  significant degradations in performance





d  e wilkins  t  j lee and  p  berry 2003 interactive execution monitoring of agent teams volume 18 pages 217261



there is an increasing need for automated support for humans    monitoring the activity of distributed teams of cooperating agents    both human and machine we characterize the domainindependent    challenges posed by this problem and describe how properties of    domains influence the challenges and their solutions we will    concentrate on dynamic datarich domains where humans are ultimately    responsible for team behavior thus the automated aid should    interactively support effective and timely decision making by the    human we present a domainindependent categorization of the types of    alerts a planbased monitoring system might issue to a user where    each type generally requires different monitoring techniques we    describe a monitoring framework for integrating many domainspecific    and taskspecific monitoring techniques and then using the concept of    value of an alert to avoid operator overload       we use this framework to describe an execution monitoring approach we    have used to implement execution assistants eas in two different    dynamic datarich realworld domains to assist a human in    monitoring team behavior one domain army small unit operations has    hundreds of mobile geographically distributed agents a combination    of humans robots and vehicles the other domain teams of unmanned    ground and air vehicles has a handful of cooperating robots both    domains involve unpredictable adversaries in the vicinity our    approach customizes monitoring behavior for each specific task plan    and situation as well as for user preferences our eas alert the    human controller when reported events threaten plan execution or    physically threaten team members alerts were generated in a timely    manner without inundating the user with too many alerts less than    10 percent of alerts are unwanted as judged by domain experts





t  lukasiewicz 1999 probabilistic deduction with conditional constraints over basic events volume 10 pages 199241



we study the problem of probabilistic deduction with    conditional constraints over basic events we show that globally    complete probabilistic deduction with conditional constraints over    basic events is nphard we then concentrate on the special case of    probabilistic deduction in conditional constraint trees we elaborate    very efficient techniques for globally complete probabilistic    deduction in detail for conditional constraint trees with point    probabilities we present a local approach to globally complete    probabilistic deduction which runs in linear time in the size of the    conditional constraint trees for conditional constraint trees with    interval probabilities we show that globally complete probabilistic    deduction can be done in a global approach by solving nonlinear    programs we show how these nonlinear programs can be transformed into    equivalent linear programs which are solvable in polynomial time in    the size of the conditional constraint trees







zenglin  xu  shandian  zhe yuan  qi and peng  yu 2016 association discovery and diagnosis of alzheimers disease with bayesian multiview learning volume 56 pages 247268



the analysis and diagnosis of alzheimers disease ad can be based on genetic variations eg single nucleotide polymorphisms snps and phenotypic traits eg magnetic resonance imaging mri features we consider two important and related tasks i to select genetic and phenotypical markers for ad diagnosis and ii to identify associations between genetic and phenotypical data while previous studies treat these two tasks separately they are tightly coupled because underlying associations between genetic variations and phenotypical features contain the biological basis for a disease here we present a new sparse bayesian approach for joint association study and disease diagnosis in this approach common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels in return the disease status can guide the discovery of relationships between data sources the sparse projection matrices not only reveal interactions between data sources but also select groups of biomarkers related to the disease moreover to take advantage of the linkage disequilibrium ld measuring the nonrandom association of alleles we incorporate a graph laplacian type of prior in the model to learn the model from data we develop an efficient variational inference algorithm analysis on an imaging genetics dataset for the study of alzheimers disease ad indicates that our model identifies biologically meaningful associations between genetic variations and mri features and achieves significantly higher accuracy for predicting ordinal ad stages than the competing methods



g r  santhanam s  basu and v  honavar 2011 representing and reasoning with qualitative preferences for compositional systems volume 42 pages 211274



many applications eg web service composition complex system design team formation etc rely on methods for identifying collections of objects or entities satisfying some functional requirement among the collections that satisfy the functional requirement it is often necessary to identify one or more collections that are optimal with respect to user preferences over a set of attributes that describe the nonfunctional properties of the collection

we provide algorithms that use this dominance relation to identify the set of most preferred collections we show that under certain conditions the algorithms are guaranteed to return only sound all complete or at least one weakly complete of the most preferred collections we present results of simulation experiments comparing the proposed algorithms with respect to a the quality of solutions number of most preferred solutions produced by the algorithms and b their performance and efficiency we also explore some interesting conjectures suggested by the results of our experiments that relate the properties of the user preferences the dominance relation and the algorithms







t  grinshpoun and a  meisels 2008 completeness and performance of the apo algorithm volume 33 pages 223258



asynchronous partial overlay apo is a search algorithm that uses cooperative mediation to solve distributed constraint satisfaction problems discsps the algorithm partitions the search into different subproblems of the discsp the original proof of completeness of the apo algorithm is based on the growth of the size of the subproblems the present paper demonstrates that this expected growth of subproblems does not occur in some situations leading to a termination problem of the algorithm the problematic parts in the apo algorithm that interfere with its completeness are identified and necessary modifications to the algorithm that fix these problematic parts are given the resulting version of the algorithm complete asynchronous partial overlay compapo ensures its completeness formal proofs for the soundness and completeness of compapo are given a detailed performance evaluation of compapo comparing it to other discsp algorithms is presented along with an extensive experimental evaluation of the algorithms unique behavior additionally an optimization version of the algorithm compoptapo is presented discussed and evaluated



b  vandegriend and  j  culberson 1998 the gnm phase transition is not hard for the hamiltonian cycle problem volume 9 pages 219245



using an improved backtrack algorithm with sophisticated    pruning techniques we revise previous observations correlating a high    frequency of hard to solve hamiltonian cycle instances with the gnm    phase transition between hamiltonicity and nonhamiltonicity instead    all tested graphs of 100 to 1500 vertices are easily solved       when we artificially restrict the degree sequence with a bounded    maximum degree although there is some increase in difficulty the    frequency of hard graphs is still low  when we consider more regular    graphs based on a generalization of knights tours we observe    frequent instances of really hard graphs but on these the average    degree is bounded by a constant  we design a set of graphs with a    feature our algorithm is unable to detect and so are very hard for our    algorithm but in these we can vary the average degree from o1 to    on  we have so far found no class of graphs correlated with the    gnm phase transition which asymptotically produces a high frequency    of hard instances





j  l ambite and  c  a knoblock 2001 planning by rewriting volume 15 pages 207261



domainindependent planning is a hard combinatorial    problem taking into account plan quality makes the task even more    difficult this article introduces planning by rewriting pbr a new    paradigm for efficient highquality domainindependent planning pbr    exploits declarative planrewriting rules and efficient local search    techniques to transform an easytogenerate but possibly suboptimal    initial plan into a highquality plan in addition to addressing the    issues of planning efficiency and plan quality this framework offers    a new anytime planning algorithm we have implemented this planner and    applied it to several existing domains the experimental results show    that the pbr approach provides significant savings in planning effort    while generating highquality plans







s  pado and m  lapata 2009 crosslingual annotation projection for semantic roles volume 36 pages 307340



 this article considers the task of automatically inducing rolesemantic annotations in the framenet paradigm for new languages  we propose a general framework that is based on annotation projection phrased as a graph optimization problem it is relatively inexpensive and has the potential to reduce the human effort involved in creating rolesemantic resources within this framework we present projection models that exploit lexical and syntactic information we provide an experimental evaluation on an englishgerman parallel corpus which demonstrates the feasibility of inducing highprecision german semantic role annotation both for manually and automatically annotated english data



s  tobies 2000 the complexity of reasoning with cardinality restrictions and nominals in expressive description logics volume 12 pages 199217



we study the complexity of the combination of the    description logics alcq and alcqi with a terminological formalism    based on cardinality restrictions on concepts these combinations can    naturally be embedded into c2 the two variable fragment of predicate    logic with counting quantifiers which yields decidability in    nexptime we show that this approach leads to an optimal solution for    alcqi as alcqi with cardinality restrictions has the same complexity    as c2 nexptimecomplete in contrast we show that for alcq the    problem can be solved in exptime this result is obtained by a    reduction of reasoning with cardinality restrictions to reasoning with    the in general weaker terminological formalism of general axioms for    alcq extended with nominals using the same reduction we show that    for the extension of alcqi with nominals reasoning with general    axioms is a nexptimecomplete problem finally we sharpen this result    and show that pure concept satisfiability for alcqi with nominals is    nexptimecomplete without nominals this problem is known to be    pspacecomplete







m  g bellemare y  naddaf j  veness and m  bowling 2013 the arcade learning environment an evaluation platform for general agents volume 47 pages 253279



in this article we introduce the arcade learning environment ale both a challenge problem and a platform and methodology for evaluating the development of general domainindependent ai technology  ale provides an interface to hundreds of atari 2600 game environments each one different interesting and designed to be a challenge for human players  ale presents significant research challenges for reinforcement learning model learning modelbased planning imitation learning transfer learning and intrinsic motivation  most importantly it provides a rigorous testbed for evaluating and comparing approaches to these problems  we illustrate the promise of ale by developing and benchmarking domainindependent agents designed using wellestablished ai techniques for both reinforcement learning and planning  in doing so we also propose an evaluation methodology made possible by ale reporting empirical results on over 55 different games  all of the software including the benchmark agents is publicly available



j246rg  tiedemann and zeljko  agi263 2016 synthetic treebanking for crosslingual dependency parsing volume 55 pages 209248



how do we parse the languages for which no treebanks are available this contribution addresses the crosslingual viewpoint on statistical dependency parsing in which we attempt to make use of resourcerich source language treebanks to build and adapt models for the underresourced target languages we outline the benefits and indicate the drawbacks of the current major approaches we emphasize synthetic treebanking the automatic creation of target language treebanks by means of annotation projection and machine translation we present competitive results in crosslingual dependency parsing using a combination of various techniques that contribute to the overall success of the method we further include a detailed discussion about the impact of partofspeech label accuracy on parsing results that provide guidance in practical applications of crosslingual methods for truly underresourced languages









shan  xue alan  fern and daniel  sheldon 2015 scheduling conservation designs for maximum flexibility via network cascade optimization volume 52 pages 331360



l  blumrosen n  nisan and i  segal 2007 auctions with severely bounded communication volume 28 pages 233266



we study auctions with severe bounds on the communication allowed each bidder  may only transmit t bits of information to the auctioneer we consider both welfare and profitmaximizing auctions under this communication restriction for both measures we determine the optimal auction and show that the loss incurred relative to unconstrained auctions is mild we prove nonsurprising properties of these kinds of auctions eg that in optimal mechanisms bidders  simply report the interval in which their valuation lies in  as well as some surprising properties eg that asymmetric auctions  are better than symmetric ones and that multiround auctions reduce the communication complexity only by a linear factor







r  jurca and b  faltings 2009 mechanisms for making crowds truthful volume 34 pages 209253



we consider schemes for obtaining truthful reports on a common but hidden signal from large groups of rational selfinterested agents one example are online feedback mechanisms where users provide observations about the quality of a product or service so that other users can have an accurate idea of what quality they can expect however i providing such feedback is costly and ii there are many motivations for providing incorrect feedback

in this paper we extend existing methods for designing incentivecompatible rewards by also considering collusion we analyze different scenarios where for example some or all of the agents collude for each scenario we investigate whether a collusionresistant incentivecompatible reward scheme exists and use automated mechanism design to specify an algorithm for deriving an efficient reward mechanism











i  kollia and b  glimm 2013 optimizing sparql query answering over owl ontologies volume 48 pages 253303



the sparql query language is currently being extended by the world wide web consortium w3c with socalled entailment regimes an entailment regime defines how queries are evaluated under more expressive semantics than sparqls standard simple entailment which is based on subgraph matching the queries are very expressive since variables can occur within complex concepts and can also bind to concept or role names

we provide a prototypical implementation and evaluate the efficiency of the proposed optimizations our experimental study shows that the static ordering usually outperforms the dynamic one when accurate statistics are available this changes however when the statistics are less accurate eg due to nondeterministic reasoning decisions for queries that go beyond conjunctive instance queries we observe an improvement of up to three orders of magnitude due to the proposed optimizations







m  hl van den briel t  vossen and s  kambhampati 2008 loosely coupled formulations for automated planning an integer programming perspective volume 31 pages 217257



diego  figueira santiago  figueira and carlos  areces 2015 model theory of xpath on data trees part i bisimulation and characterization volume 53 pages 271314



we investigate model theoretic properties of xpath with data inequality tests over the class of data trees ie the class of trees where each node contains a label from a finite alphabet and a data value from an infinite domain





l  leherte  j  glasgow  k  baxter  e  steeg and  s  fortier 1997 analysis of threedimensional protein images volume 7 pages 125159



a fundamental goal of research in molecular biology is to    understand protein structure protein crystallography is currently the    most successful method for determining the threedimensional 3d    conformation of a protein yet it remains labor intensive and relies    on an experts ability to derive and evaluate a protein scene model    in this paper the problem of protein structure determination is    formulated as an exercise in scene analysis  a computational    methodology is presented in which a 3d image of a protein is segmented    into a graph of critical points  bayesian and certainty factor    approaches are described and used to analyze critical point graphs and    identify meaningful substructures such as alphahelices and    betasheets  results of applying the methodologies to protein images    at low and medium resolution are reported  the research is related to    approaches to representation segmentation and classification in    vision as well as to topdown approaches to protein structure    prediction







jhm  lee and k  l leung 2012 consistency techniques for flowbased projectionsafe global cost functions in weighted constraint satisfaction volume 43 pages 257292



many combinatorial problems deal with preferences and violations the goal of which is to find solutions with the minimum cost  weighted constraint satisfaction is a framework for modeling such problems which consists of a set of cost functions to measure the degree of violation or preferences of different combinations of variable assignments  typical solution methods for weighted constraint satisfaction problems wcsps are based on branchandbound search which are made practical through the use of powerful consistency techniques such as ac fdac edac to deduce hidden cost information and value pruning during search  these techniques however are designed to be efficient only on binary and ternary cost functions which are represented in table form  in tackling many reallife problems high arity or global cost functions are required  we investigate efficient representation scheme and algorithms to bring the benefits of the consistency techniques to also high arity cost functions which are often derived from hard global constraints from classical constraint satisfaction

the literature suggests some global cost functions can be represented as flow networks and the minimum cost flow algorithm can be used to compute the minimum costs of such networks in polynomial time  we show that naive adoption of this flowbased algorithmic method for global cost functions can result in a stronger form of nullinverse consistency  we further show how the method can be modified to handle cost projections and extensions to maintain generalized versions of ac and fdac for cost functions with more than two variables  similar generalization for the stronger edac is less straightforward  we reveal the oscillation problem when enforcing edac on cost functions sharing more than one variable  to avoid oscillation we propose a weak version of edac and generalize it to weak edgac for nonbinary cost functions using various benchmarks involving the soft variants of hard global constraints alldifferent gcc same and regular empirical results demonstrate that our proposal gives improvements of up to an order of magnitude when compared with the traditional constraint optimization approach both in terms of time and pruning 





a  feldman g  provan and a  van gemund 2010 a modelbased active testing approach to sequential diagnosis volume 39 pages 301334



modelbased diagnostic reasoning often leads to a large number of diagnostic hypotheses the set of diagnoses can be reduced by taking into account extra observations passive monitoring measuring additional variables probing or executing additional tests sequential diagnosistest sequencing in this paper we combine the above approaches with techniques from automated test pattern generation atpg and modelbased diagnosis mbd into a framework called fractal framework for active testing algorithms apart from the inputs and outputs that connect a system to its environment in active testing we consider additional input variables to which a sequence of test vectors can be supplied we address the computationally hard problem of computing optimal control assignments as defined in fractal in terms of a greedy approximation algorithm called fractalg we compare the decrease in the number of remaining minimal cardinality diagnoses of fractalg to that of two more fractal algorithms fractalatpg and fractalp fractalatpg is based on atpg and sequential diagnosis while fractalp is based on probing and although not an active testing algorithm provides a baseline for comparing the lower bound on the number of reachable diagnoses for the fractal algorithms we empirically evaluate the tradeoffs of the three fractal algorithms by performing extensive experimentation on the iscas8574xxx benchmark of combinational circuits







k  r apt and g  schaefer 2014 selfishness level of strategic games volume 49 pages 207240



d  e smith 2003 the case for durative actions a commentary on pddl21 volume 20 pages 149154



the addition of durative actions to pddl21 sparked some controversy fox and long argued that actions should be considered as instantaneous but can start and stop processes  ultimately a limited notion of durative actions was incorporated into the language i argue that this notion is still impoverished and that the underlying philosophical position of regarding durative actions as being a shorthand for a start action process and stop action ignores the realities of modelling and execution for complex systems





2009 ijcaijair best paper prize



information about user preferences plays a key role in automated decision making in many domains it is desirable to assess such preferences in a qualitative rather than quantitative way in this paper we propose a qualitative graphical representation of preferences that reflects conditional dependence and independence of preference statements under a ceteris paribus all else being equal interpretation such a representation is often compact and arguably quite natural in many circumstances we provide a formal semantics for this model and describe how the structure of the network can be exploited in several inference tasks such as determining whether one outcome dominates is preferred to another ordering a set outcomes according to the preference relation and constructing the best outcome subject to available evidence





d  lesaint d  mehta b  osullivan l  quesada and n  wilson 2010 developing approaches  for solving a telecommunications feature subscription problem volume 38 pages 271305



call control features eg calldivert voicemail are primitive options to which users can subscribe offline to personalise their  service the configuration of a feature subscription involves choosing and sequencing features from a catalogue and is subject to  constraints that prevent undesirable feature interactions at runtime when the subscription requested by a user is inconsistent one  problem is to find an optimal relaxation  which is a generalisation of the feedback vertex  set problem on directed graphs and thus it is an nphard task we present several constraint programming formulations of the problem we also present formulations using partial  weighted maximum boolean satisfiability and mixed integer linear programming  we study all these formulations by experimentally comparing them  on a variety of randomly generated instances of the feature subscription problem





t  g dietterich 2000 hierarchical reinforcement learning with the maxq value function decomposition volume 13 pages 227303











p  f felzenszwalb and d  mcallester 2007 the generalized a architecture volume 29 pages 153190





n  goernitz m  kloft k  rieck and u  brefeld 2013 toward supervised anomaly detection volume 46 pages 235262



anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions however the predictive performance of purely unsupervised anomaly detection  often fails to match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation our first contribution shows that  classical semisupervised approaches originating from a supervised classifier are inappropriate and hardly detect  new and unknown anomalies we argue that semisupervised anomaly detection  needs to ground on the unsupervised learning paradigm and devise a novel algorithm that meets this requirement although being intrinsically nonconvex we further show that the optimization problem has a convex equivalent under relatively mild assumptions additionally we propose an active learning strategy to automatically filter candidates for  labeling in an empirical study on network intrusion detection data we observe that the proposed learning methodology requires much less labeled data than the stateoftheart while achieving higher detection accuracies







p  faliszewski e  hemaspaandra l  a hemaspaandra and j  rothe 2009 llull and copeland voting computationally resist bribery and constructive control volume 35 pages 275341





meir  kalech and shulamit  reches 2015 decision making with dynamic uncertain events volume 54 pages 233275



when to make a decision is a key question in decision making problems characterized by uncertainty in this paper we deal with decision making in environments where information arrives dynamically we address the tradeoff between waiting and stopping strategies on the one hand waiting to obtain more information reduces uncertainty but it comes with a cost stopping and making a decision based on an expected utility reduces the cost of waiting but the decision is based on uncertain information we propose an optimal algorithm and two approximation algorithms we prove that one approximation is optimistic  waits at least as long as the optimal algorithm while the other is pessimistic  stops not later than the optimal algorithm we evaluate our algorithms theoretically and empirically and show that the quality of the decision in both approximations is nearoptimal and much faster than the optimal algorithm also we can conclude from the experiments that the cost function is a key factor to chose the most effective algorithm







a  gerevini a  saetti and i  serina 2006 an approach to temporal planning and scheduling in domains with predictable exogenous events volume 25 pages 187231



n  nilsson 1994 teleoreactive programs for agent control volume 1 pages 139158



a formalism is presented for computing and organizing actions for autonomous agents in dynamic environments we introduce the notion of iteleoreactive tr programsi whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based  in addition to continuous feedback tr programs support parameter binding and recursion  a primary difference between tr programs and many other circuitbased systems is that the circuitry of tr programs is more compact it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs  in addition tr programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods  we briefly describe some experimental applications of tr programs in the control of simulated and actual mobile robots





r  rosati 1999 reasoning about minimal belief and negation as failure volume 11 pages 277300



we investigate the problem of reasoning in the propositional    fragment of mbnf the logic of minimal belief and negation as failure    introduced by lifschitz which can be considered as a unifying    framework for several nonmonotonic formalisms including default    logic autoepistemic logic circumscription epistemic queries and    logic programming  we characterize the complexity and provide    algorithms for reasoning in propositional mbnf  in particular we    show that entailment in propositional mbnf lies at the third level of    the polynomial hierarchy hence it is harder than reasoning in all the    above mentioned propositional formalisms for nonmonotonic reasoning    we also prove the exact correspondence between negation as failure in    mbnf and negative introspection in moores autoepistemic logic





s  minton  j  bresina and  m  drummond 1994 totalorder and partialorder planning a comparative analysis volume 2 pages 227262



for many years the intuitions underlying partialorder   planning were largely taken for granted only in the past few years   has there been renewed interest in the fundamental principles   underlying this paradigm  in this paper we present a rigorous   comparative analysis of partialorder and totalorder planning by   focusing on two specific planners that can be directly compared we   show that there are some subtle assumptions that underly the   widespread intuitions regarding the supposed efficiency of   partialorder planning for instance the superiority ofpartialorder   planning can depend critically upon the search strategy and the   structure of the search space  understanding the underlying   assumptions is crucial for constructing efficient planners





m  a walker  i  langkildegeary  h  wright hastie  j  wright and  a  gorin 2002 automatically training a problematic dialogue predictor for a spoken dialogue system volume 16 pages 293319



spoken dialogue systems promise efficient and natural access    to a large variety of information sources and services from any phone    however current spoken dialogue systems are deficient in their    strategies for preventing identifying and repairing problems that    arise in the conversation this paper reports results on automatically    training a problematic dialogue predictor to predict problematic    humancomputer dialogues using a corpus of 4692 dialogues collected    with the how may i help you sm spoken dialogue system  the    problematic dialogue predictor can be immediately applied to the    systems decision of whether to transfer the call to a human customer    care agent or be used as a cue to the systems dialogue manager to    modify its behavior to repair problems and even perhaps to prevent    them we show that a problematic dialogue predictor using    automaticallyobtainable features from the first two exchanges in the    dialogue can predict problematic dialogues 132 more accurately than    the baseline





a  darwiche and  p  marquis 2002 a knowledge compilation map volume 17 pages 229264







c  lecoutre s  cardon and j  vion 2011 secondorder consistencies volume 40 pages 175219



in this paper we propose a comprehensive study of secondorder consistencies ie consistencies identifying inconsistent pairs of values for constraint satisfaction we build a full picture of the relationships existing between four basic secondorder consistencies namely path consistency pc 3consistency 3c dual consistency dc and 2singleton arc consistency 2sac as well as their conservative and strong variants interestingly dual consistency is an original property that can be established by using the outcome of the enforcement of generalized arc consistency gac which makes it rather easy to obtain since constraint solvers typically maintain gac during search  on binary constraint  networks dc is equivalent to pc but its restriction to existing constraints called conservative dual consistency cdc is strictly stronger than  traditional conservative consistencies derived from path consistency namely partial path consistency ppc and conservative path consistency cpc  after introducing a general algorithm to enforce strong cdc we present the results of an experimentation over a wide range of benchmarks that demonstrate the interest of conservative dual consistency  in particular we show that enforcing cdc before search clearly improves the performance of mac the algorithm that maintains gac during search on several binary and nonbinary structured problems





g  pinkas and  r  dechter 1995 improving connectionist energy minimization volume 3 pages 223248



symmetric networks designed for energy minimization such as    boltzman machines and hopfield nets are frequently investigated for    use in optimization constraint satisfaction and approximation of    nphard problems nevertheless finding a global solution ie a    global minimum for the energy function is not guaranteed and even a    local solution may take an exponential number of steps  we propose an    improvement to the standard local activation function used for such    networks  the improved algorithm guarantees that a global minimum is    found in linear time for treelike subnetworks the algorithm called    activate is uniform and does not assume that the network is    treelike  it can identify treelike subnetworks even in cyclic    topologies arbitrary networks and avoid local minima along these    trees  for acyclic networks the algorithm is guaranteed to converge    to a global minimum from any initial state of the system    selfstabilization and remains correct under various types of    schedulers  on the negative side we show that in the presence of    cycles no uniform algorithm exists that guarantees optimality even    under a sequential asynchronous scheduler  an asynchronous scheduler    can activate only one unit at a time while a synchronous scheduler can    activate any number of units in a single time step  in addition no    uniform algorithm exists to optimize even acyclic networks when the    scheduler is synchronous  finally we show how the algorithm can be    improved using the cyclecutset scheme  the general algorithm called    activatewithcutset improves over activate and has some performance    guarantees that are related to the size of the networks cyclecutset









z  zhuang and m  pagnucco 2014 entrenchmentbased horn contraction volume 51 pages 227254



r  a helzerman and  m  p harper 1996 muse csp an extension to the constraint satisfaction problem volume 5 pages 239288



this paper describes an extension to the constraint    satisfaction problem csp called muse csp multiply segmented    constraint satisfaction problem  this extension is especially useful    for those problems which segment into multiple sets of partially    shared variables  such problems arise naturally in signal processing    applications including computer vision speech processing and    handwriting recognition  for these applications it is often    difficult to segment the data in only one way given the lowlevel    information utilized by the segmentation algorithms  muse csp can be    used to compactly represent several similar instances of the    constraint satisfaction problem  if multiple instances of a csp have    some common variables which have the same domains and constraints    then they can be combined into a single instance of a muse csp    reducing the work required to apply the constraints  we introduce the    concepts of muse node consistency muse arc consistency and muse path    consistency we then demonstrate how muse csp can be used to compactly    represent lexically ambiguous sentences and the multiple sentence    hypotheses that are often generated by speech recognition algorithms    so that grammar constraints can be used to provide parses for all    syntactically correct sentences  algorithms for muse arc and path    consistency are provided  finally we discuss how to create a muse    csp from a set of csps which are labeled to indicate when the same    variable is shared by more than a single csp





n  rivera l  illanes j  a baier and c  hernandez 2014 reconnection with the ideal tree a new approach to realtime search volume 50 pages 235264



many applications ranging from video games to dynamic robotics require solving singleagent deterministic search problems in partially known environments under very tight time constraints realtime heuristic search rths algorithms are specifically designed for those applications as a subroutine most of them invoke a standard but bounded search algorithm that searches for the goal in this paper we present frit a simple approach for singleagent deterministic search problems under tight constraints and partially known environments that unlike traditional rths does not search for the goal but rather searches for a path that connects the current state with a socalled ideal tree t  when the agent observes that an arc in the tree cannot be traversed in the actual environment it removes such an arc from t and then carries out a reconnection search whose objective is to find a path between the current state and any node in t  the reconnection search is done using an algorithm that is passed as a parameter to frit if such a parameter is an rths algorithm then the resulting algorithm can be an rths algorithm we show in addition that frit may be fed with a bounded complete blindsearch algorithm we evaluate our approach over grid pathfinding benchmarks including game maps and mazes our results show that frit used with rtaa a standard rths algorithm outperforms rtaa significantly by one order of magnitude under tight time constraints in addition fritdartaa substantially outperforms dartaa a stateoftheart rths algorithm usually obtaining solutions 50 cheaper on average when performing the same search effort finally fritbfs ie frit using breadthfirstsearch obtains bestquality solutions when time is limited compared to adaptive a and repeated a finally we show that bug2 a pathfindingspecific navigation algorithm outperforms fritbfs when planning time is extremely limited but when given more time the situation reverses





f  a oliehoek m  t j spaan and n  vlassis 2008 optimal and approximate qvalue functions for decentralized pomdps volume 32 pages 289353



decisiontheoretic planning is a popular approach to sequential decision making problems because it treats uncertainty in sensing and acting in a principled way in singleagent frameworks like mdps and pomdps planning can be carried out by resorting to qvalue functions an optimal qvalue function q is computed in a recursive manner by dynamic programming and then an optimal policy is extracted from q in this paper we study whether similar qvalue functions can be defined for decentralized pomdp models decpomdps and how policies can be extracted from such value functions we define two forms of the optimal qvalue function for decpomdps one that gives a normative description as the qvalue function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation this computation however is infeasible for all but the smallest problems therefore we analyze various approximate qvalue functions that allow for efficient computation we describe how they relate and we prove that they all provide an upper bound to the optimal qvalue function q  finally unifying some previous approaches for solving decpomdps we describe a family of algorithms for extracting policies from such qvalue functions and perform an experimental evaluation on existing test problems including a new firefighting benchmark problem





a  darwiche 1998 modelbased diagnosis using structured system descriptions volume 8 pages 165222



this paper presents a comprehensive approach for modelbased    diagnosis which includes proposals for characterizing and computing    preferred diagnoses assuming that the system description is augmented    with a system structure a directed graph explicating the    interconnections between system components  specifically we first    introduce the notion of a consequence which is a syntactically    unconstrained propositional sentence that characterizes all    consistencybased diagnoses and show that standard characterizations    of diagnoses such as minimal conflicts correspond to syntactic    variations on a consequence second we propose a new syntactic    variation on the consequence known as negation normal form nnf and    discuss its merits compared to standard variations  third we    introduce a basic algorithm for computing consequences in nnf given a    structured system description we show that if the system structure    does not contain cycles then there is always a linearsize    consequence in nnf which can be computed in linear time for arbitrary    system structures we show a precise connection between the complexity    of computing consequences and the topology of the underlying system    structure  finally we present an algorithm that enumerates the    preferred diagnoses characterized by a consequence the algorithm is    shown to take linear time in the size of the consequence if the    preference criterion satisfies some general conditions





d  a cohn  z  ghahramani and  m  i jordan 1996 active learning with statistical models volume 4 pages 129145



for many types of machine learning algorithms one can    compute the statistically optimal way to select training data  in    this paper we review how optimal data selection techniques have been    used with feedforward neural networks  we then show how the same    principles may be used to select data for two alternative    statisticallybased learning architectures mixtures of gaussians and    locally weighted regression  while the techniques for neural networks    are computationally expensive and approximate the techniques for    mixtures of gaussians and locally weighted regression are both    efficient and accurate  empirically we observe that the optimality    criterion sharply decreases the number of training examples the    learner needs in order to achieve good performance





s  joshi and r  khardon 2011 probabilistic relational planning  with first order decision diagrams volume 41 pages 231266



dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations in particular algebraic decision diagrams to capture domain dynamics and value functions  work on symbolic dynamic programming lifted these ideas to first order logic using several representation schemes  recent work introduced a first order variant of decision diagrams fodd and developed a value iteration algorithm for this representation this paper develops several improvements to the fodd algorithm that make the approach practical these include new reduction operators that decrease the size of the representation several speedup techniques and techniques for value approximation  incorporating these the paper presents a planning system foddplanner for solving relational stochastic planning problems  the system is evaluated on several domains including problems from the recent international planning competition and shows competitive performance with top ranking systems this is the first demonstration of feasibility of this approach and it shows that abstraction through compact representation is a promising approach to stochastic planning





r  kusters and  a  borgida 2001 whats in an attribute consequences for the least common subsumer volume 14 pages 167203



functional relationships between objects called    attributes are of considerable importance in knowledge    representation languages including description logics dls a study    of the literature indicates that papers have made often implicitly    different assumptions about the nature of attributes whether they are    always required to have a value or whether they can be partial    functions the work presented here is the first explicit study of this    difference for subclasses of the classic dl involving the sameas    concept constructor  it is shown that although determining    subsumption between concept descriptions has the same complexity    though requiring different algorithms the story is different in the    case of determining the least common subsumer lcs for attributes    interpreted as partial functions the lcs exists and can be computed    relatively easily even in this case our results correct and extend    three previous papers about the lcs of dls  in the case where    attributes must have a value the lcs may not exist and even if it    exists it may be of exponential size  interestingly it is possible    to decide in polynomial time if the lcs exists









m  fox d  long and d  magazzeni 2012 planbased policies for efficient multiple battery load management volume 44 pages 335382





honorable mention for the 2009 ijcaijair best paper prize



fast downward is a classical planning system based on heuristic search it can deal with general deterministic planning problems encoded in the propositional fragment of pddl22 including advanced features like adl conditions and effects and derived predicates axioms like other wellknown planners such as hsp and ff fast downward is a progression planner searching the space of world states of a planning task in the forward direction however unlike other pddl planning systems fast downward does not use the propositional pddl representation of a planning task directly instead the input is first translated into an alternative representation called multivalued planning tasks which makes many of the implicit constraints of a propositional planning task explicit exploiting this alternative representation fast downward uses hierarchical decompositions of planning tasks for computing its heuristic function called the causal graph heuristic which is very different from traditional hsplike heuristics based on ignoring negative interactions of operators

