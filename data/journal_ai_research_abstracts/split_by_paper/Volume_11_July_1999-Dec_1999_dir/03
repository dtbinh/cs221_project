c  e brodley and  m  a friedl 1999 identifying mislabeled training data volume 11 pages 131167

this paper presents a new approach to identifying and    eliminating mislabeled training instances for supervised learning the    goal of this approach is to improve classification accuracies produced    by learning algorithms by improving the quality of the training data    our approach uses a set of learning algorithms to create classifiers    that serve as noise filters for the training data  we evaluate single    algorithm majority vote and consensus filters on five datasets that    are prone to labeling errors  our experiments illustrate that    filtering significantly improves classification accuracy for noise    levels up to 30 percent  an analytical and empirical evaluation of    the precision of our approach shows that consensus filters are    conservative at throwing away good data at the expense of retaining    bad data and that majority filters are better at detecting bad data at    the expense of throwing away good data  this suggests that for    situations in which there is a paucity of data consensus filters are    preferable whereas majority vote filters are preferable for    situations with an abundance of data

