





g  m weiss and  f  provost 2003 learning when training data are costly the effect of class distribution on tree induction volume 19 pages 315354



for large realworld inductive learning problems the number of training examples often must be limited due to the costs associated with procuring preparing and storing the training examples andor the computational costs associated with learning from them in such circumstances one question of practical importance is if only n training examples can be selected in what proportion should the classes be represented  in this article we help to answer this question by analyzing for a fixed trainingset size the relationship between the class distribution of the training data and the performance of classification trees induced from these data we study twentysix data sets and for each determine the best class distribution for learning  the naturally occurring class distribution is shown to generally perform well when classifier performance is evaluated using undifferentiated error rate 01 loss  however when the area under the roc curve is used to evaluate classifier performance a balanced distribution is shown to perform well  since neither of these choices for class distribution always generates the bestperforming classifier we introduce a budgetsensitive progressive sampling algorithm for selecting training examples based on the class associated with each example  an empirical analysis of this algorithm shows that the class distribution of the resulting training set yields classifiers with good nearlyoptimal classification performance



exact maxsat solvers compared with sat solvers apply little inference at each node of the proof tree commonly used sat inference rules like unit propagation produce a simplified formula that preserves satisfiability but unfortunately solving the maxsat problem for the simplified formula is not equivalent to solving it for the original formula in this paper we define a number of original inference rules that besides being applied efficiently transform maxsat instances into equivalent maxsat instances which are easier to solve the soundness of the rules that can be seen as refinements of unit resolution adapted to maxsat are proved in a novel and simple way via an integer programming transformation with the aim of finding out how powerful the inference rules are in practice we have developed a new maxsat solver called maxsatz which incorporates those rules and performed  an experimental investigation the results provide empirical evidence that maxsatz is very competitive at least on random max2sat random max3sat maxcut and graph 3coloring instances as well as on the benchmarks from the maxsat evaluation 2006









w  van der hoek d  walther and m  wooldridge 2010 reasoning about the transfer of control volume 37 pages 437477



 we present dclpc a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another the logical foundation of dclpc is clpc a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents stem from a distribution of atomic boolean variables to individual agents  the choices available to a coalition correspond to assignments to the variables the coalition controls the basic modal constructs of dclpc are of the form coalition c can cooperate to bring about phi dclpc extends clpc with dynamic logic modalities in which atomic programs are of the form agent i gives control of variable p to agent j as usual in dynamic logic these atomic programs may be combined using sequence iteration choice and test operators to form complex programs by combining such dynamic transfer programs with cooperation modalities it becomes possible to reason about how the power of agents and coalitions is affected by the transfer of control we give two alternative semantics for the logic a direct semantics in which we capture the distributions of boolean variables to agents and a more conventional kripke semantics we prove that these semantics are equivalent and then present an axiomatization for the logic we investigate the computational complexity of model checking and satisfiability for dclpc and show that both problems are pspacecomplete and hence no worse than the underlying logic clpc finally we investigate the characterisation of control in dclpc we distinguish between firstorder control  the ability of an agent or coalition to control some state of affairs through the assignment of values to the variables under the control of the agent or coalition  and secondorder control  the ability of an agent to exert control over the control that other agents have by transferring variables to other agents we give a logical characterisation of secondorder control





h  e dixon  m  l ginsberg  d  hofer  e  m luks and  a  j parkes 2005 generalizing boolean satisfiability iii implementation volume 23 pages 441531



this is the third of three papers describing zap a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern highperformance solvers the fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the boolean representation used our goal has been to define a representation in which this structure is apparent and can be exploited to improve computational performance  the first paper surveyed existing work that knowingly or not exploited problem structure to improve the performance of satisfiability engines and the second paper showed that this structure could be understood in terms of groups of permutations acting on individual clauses in any particular boolean theory  we conclude the series by discussing the techniques needed to implement our ideas and by reporting on their performance on a variety of problem instances





p  beame  h  kautz and  a  sabharwal 2004 towards understanding and harnessing the potential of clause learning volume 22 pages 319351





j  pineau g  gordon and s  thrun 2006 anytime pointbased approximations for large pomdps volume 27 pages 335380



the partially observable markov decision process has long been recognized as a rich framework for realworld planning and control problems especially in robotics however exact solutions in this framework are typically computationally intractable for all but the smallest problems  a wellknown technique for speeding up pomdp solving involves performing value backups at specific belief points rather than over the entire belief simplex the efficiency of this approach however depends greatly on the selection of points  this paper presents a set of novel techniques for selecting informative belief points which work well in practice the point selection procedure is combined with pointbased value backups to form an effective anytime pomdp algorithm called pointbased value iteration pbvi the first aim of this paper is to introduce this algorithm and present a theoretical analysis justifying the choice of belief selection technique the second aim of this paper is to provide a thorough empirical comparison between pbvi and other stateoftheart pomdp methods in particular the perseus algorithm in an effort to highlight their similarities and differences evaluation is performed using both standard pomdp domains and realistic robotic tasks







g  gottlob  g  greco and  f  scarcello 2005 pure nash equilibria hard and easy games volume 24 pages 357406







s  acid and l  m de campos 2003 searching for bayesian network structures in the space of restricted acyclic partially directed graphs volume 18 pages 445490



although many algorithms have been designed to construct    bayesian network structures using different approaches and principles    they all employ only two methods those based on independence    criteria and those based on a scoring function and a search procedure    although some methods combine the two within the scoresearch    paradigm the dominant approach uses local search methods in the space    of directed acyclic graphs dags where the usual choices for    defining the elementary modifications local changes that can be    applied are arc addition arc deletion and arc reversal in this    paper we propose a new local search method that uses a different    search space and which takes account of the concept of equivalence    between network structures restricted acyclic partially directed    graphs rpdags in this way the number of different configurations    of the search space is reduced thus improving efficiency moreover    although the final result must necessarily be a local optimum given    the nature of the search method the topology of the new search space    which avoids making early decisions about the directions of the arcs    may help to find better local optima than those obtained by searching    in the dag space detailed results of the evaluation of the proposed    search method on several test problems including the wellknown alarm    monitoring system are also presented





j  rintanen 1999 constructing conditional plans by a theoremprover volume 10 pages 323352



the research on conditional planning rejects the assumptions    that there is no uncertainty or incompleteness of knowledge with    respect to the state and changes of the system the plans operate on    without these assumptions the sequences of operations that achieve the    goals depend on the initial state and the outcomes of nondeterministic    changes in the system  this setting raises the questions of how to    represent the plans and how to perform plan search the answers are    quite different from those in the simpler classical framework  in    this paper we approach conditional planning from a new viewpoint that    is motivated by the use of satisfiability algorithms in classical    planning  translating conditional planning to formulae in the    propositional logic is not feasible because of inherent computational    limitations  instead we translate conditional planning to quantified    boolean formulae  we discuss three formalizations of conditional    planning as quantified boolean formulae and present experimental    results obtained with a theoremprover













p  d grunwald and j  y halpern 2011 making decisions using sets of probabilities updating time consistency and calibration volume 42 pages 393426











m  fox and  d  long 1998 the automatic inference of state invariants in tim volume 9 pages 367421



as planning is applied to larger and richer domains the    effort involved in constructing domain descriptions increases and    becomes a significant burden on the human application designer if    general planners are to be applied successfully to large and complex    domains it is necessary to provide the domain designer with some    assistance in building correctly encoded domains  one way of doing    this is to provide domainindependent techniques for extracting from    a domain description knowledge that is implicit in that description    and that can assist domain designers in debugging domain    descriptions this knowledge can also be exploited to improve the    performance of planners several researchers have explored the    potential of state invariants in speeding up the performance of    domainindependent planners in this paper we describe a process by    which state invariants can be extracted from the automatically    inferred type structure of a domain these techniques are being    developed for exploitation by stan a graphplan based planner that    employs state analysis techniques to enhance its performance





j  baxter  p  l bartlett and  l  weaver 2001 experiments with infinitehorizon policygradient estimation volume 15 pages 351381



in this paper we present algorithms that perform gradient    ascent of the average reward in a partially observable markov decision    process pomdp  these algorithms are based on gpomdp an algorithm    introduced in a companion paper baxter  bartlett this volume    which computes biased estimates of the performance gradient in pomdps    the algorithms chief advantages are that it uses only one free    parameter beta which has a natural interpretation in terms of    biasvariance tradeoff it requires no knowledge of the underlying    state and it can be applied to infinite state control and    observation spaces  we show how the gradient estimates produced by    gpomdp can be used to perform gradient ascent both with a traditional    stochasticgradient algorithm and with an algorithm based on    conjugategradients that utilizes gradient information to bracket    maxima in line searches experimental results are presented    illustrating both the theoretical results of baxter  bartlett this    volume on a toy problem and practical aspects of the algorithms on a    number of more realistic problems



planning as satisfiability as implemented in for instance the satplan tool is a highly competitive method for finding parallel stepoptimal plans a bottleneck in this approach is to prove the absence of plans of a certain length specifically if the optimal plan has n steps then it is typically very costly to prove that there is no plan of length n1 we pursue the idea of leading this proof within solution length preserving abstractions overapproximations of the original planning task this is promising because the abstraction may have a much smaller state space related methods are highly successful in model checking in particular we design a novel abstraction technique based on which one can in several widely used planning benchmarks construct abstractions that have exponentially smaller state spaces while preserving the length of an optimal plan









j  y halpern 2000 axiomatizing causal reasoning volume 12 pages 317337



causal models defined in terms of a collection of equations    as defined by pearl are axiomatized here axiomatizations are    provided for three successively more general classes of causal models    1 the class of recursive theories those without feedback 2 the    class of theories where the solutions to the equations are unique     3 arbitrary theories where the equations may not have solutions and if    they do they are not necessarily unique it is shown that to reason    about causality in the most general third class we must extend the    language used by galles and pearl in addition the complexity of the    decision procedures is characterized for all the languages and classes    of models considered



we introduce the framework of qualitative optimization problems or simply optimization problems to represent preference theories the formalism uses separate modules to describe the space of outcomes to be compared the generator and the preferences on outcomes the selector we consider two types of optimization problems they differ in the way the generator which we model by a propositional theory is interpreted by the standard propositional logic semantics and by the equilibriummodel answerset semantics under the latter interpretation of generators optimization problems directly generalize answerset optimization programs proposed previously we study strong equivalence of optimization problems which guarantees their interchangeability within any larger context we characterize several versions of strong equivalence obtained by restricting the class of optimization problems that can be used as extensions and establish the complexity of associated reasoning tasks understanding strong equivalence is essential for modular representation of optimization problems and rewriting techniques to simplify them without changing their inherent properties









ziyu  wang frank  hutter masrour  zoghi david  matheson and nando  de feitas 2016 bayesian optimization in a billion dimensions via random embeddings volume 55 pages 361387



bayesian optimization techniques have been successfully applied to robotics planning sensor placement recommendation advertising intelligent user interfaces and automatic algorithm configuration despite these successes the approach is restricted to problems of moderate dimension and several workshops on bayesian optimization have identified its scaling to highdimensions as one of the holy grails of the field in this paper we introduce a novel random embedding idea to attack this problem the resulting random embedding bayesian optimization rembo algorithm is very simple has important invariance properties and applies to domains with both categorical and continuous variables we present a thorough theoretical analysis of rembo empirical results confirm that rembo can effectively solve problems with billions of dimensions provided the intrinsic dimensionality is low they also show that rembo achieves stateoftheart performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver



a discourse typically involves numerous entities but few are mentioned more than once distinguishing those that die out after just one mention singleton from those that lead longer lives coreferent would dramatically simplify the hypothesis space for coreference resolution models leading to increased performance to realize these gains we build a classifier for predicting the singletoncoreferent distinction the models feature representations synthesize linguistic insights about the factors affecting discourse entity lifespans especially negation modality and attitude predication with existing results about the benefits of surface partofspeech and ngrambased features for coreference resolution the model is effective in its own right and the feature representations help to identify the anchor phrases in bridging anaphora as well furthermore incorporating the model into two very different stateoftheart coreference resolution systems one rulebased and the other learningbased yields significant performance improvements









a  s fukunaga and r  e korf 2007 bin completion algorithms for multicontainer packing knapsack and covering problems volume 28 pages 393429



many combinatorial optimization problems such as the bin packing and multiple knapsack problems involve assigning a set of discrete objects to multiple containers these problems can be used to model task and resource allocation problems in multiagent systems and distributed systms and can also be found as subproblems of scheduling problems we propose bin completion a branchandbound strategy for onedimensional multicontainer packing problems  bin completion combines a binoriented search space with a powerful dominance criterion that enables us to prune much of the space the performance of the basic bin completion framework can be enhanced by using a number of extensions including nogoodbased pruning techniques that allow further exploitation of the dominance criterion  bin completion is applied to four problems multiple knapsack bin covering mincost covering and bin packing  we show that our bin completion algorithms yield new stateoftheart results for the multiple knapsack bin covering and mincost covering problems outperforming previous algorithms by several orders of magnitude with respect to runtime on some classes of hard random problem instances  for the bin packing problem we demonstrate significant improvements compared to most previous results but show that bin completion is not competitive with current stateoftheart cuttingstock based approaches







f  bacchus s  dalmao and t  pitassi 2009 solving sat and bayesian inference with backtracking search volume 34 pages 391442





r  a rossi l  k mcdowell d  w aha and j  neville 2012 transforming graph data for statistical relational learning volume 45 pages 363441



relational data representations have become an increasingly important topic due to the recent proliferation of network datasets eg social biological information networks and a corresponding increase in the application of statistical relational learning srl algorithms to these domains in this article we examine and categorize techniques for transforming graphbased relational data to improve srl algorithms in particular appropriate transformations of the nodes links andor features of the data can dramatically affect the capabilities and results of srl algorithms we introduce an intuitive taxonomy for data representation transformations in relational domains that incorporates link transformation and node transformation as symmetric representation tasks more specifically the transformation tasks for both nodes and links include i predicting their existence ii predicting their label or type iii estimating their weight or importance and iv system atically constructing their relevant features we motivate our taxonomy through detailed examples and use it to survey competing approaches for each of these tasks we also dis cuss general conditions for transforming links nodes and features finally we highlight challenges that remain to be addressed







j  p delgrande and r  wassermann 2013 horn clause contraction functions volume 48 pages 475511



we present three new complexity results for classes of planning problems with simple causal graphs first we describe a polynomialtime algorithm that uses macros to generate plans for the class 3s of planning problems with binary state variables and acyclic causal graphs this implies that plan generation may be tractable even when a planning problem has an exponentially long minimal solution we also prove that the problem of plan existence for planning problems with multivalued variables and chain causal graphs is nphard finally we show that plan existence for planning problems with binary state variables and polytree causal graphs is npcomplete











matthew  l ginsberg 2015 satsisfiability and systematicity volume 53 pages 497540





j  frank  p  cheeseman and  j  stutz 1997 when gravity fails local search topology volume 7 pages 249281



local search algorithms for combinatorial search problems    frequently encounter a sequence of states in which it is impossible to    improve the value of the objective function moves through these    regions called plateau moves dominate the time spent in local    search  we analyze and characterize plateaus for three different    classes of randomly generated boolean satisfiability problems  we    identify several interesting features of plateaus that impact the    performance of local search algorithms  we show that local minima    tend to be small but occasionally may be very large  we also show    that local minima can be escaped without unsatisfying a large number    of clauses but that systematically searching for an escape route may    be computationally expensive if the local minimum is large  we show    that plateaus with exits called benches tend to be much larger than    minima and that some benches have very few exit states which local    search can use to escape  we show that the solutions ie global    minima of randomly generated problem instances form clusters which    behave similarly to local minima  we revisit several enhancements of    local search algorithms and explain their performance in light of our    results  finally we discuss strategies for creating the next    generation of local search algorithms



local consistency techniques such as kconsistency are a key  component of specialised solvers for constraint satisfaction problems in this paper we show that the power of using kconsistency techniques on a constraint satisfaction problem is precisely captured by using a particular inference rule which we call negativehyperresolution on the standard direct encoding of the problem into boolean clauses we also show that current clauselearning satsolvers will discover in expected polynomial time any inconsistency that can be deduced from a given set of clauses using negativehyperresolvents of a fixed size we combine these two results to show that without being explicitly designed to do so current clauselearning satsolvers efficiently simulate kconsistency techniques for all fixed values of k we then give some experimental results to show that this feature allows clauselearning satsolvers to efficiently solve certain families of constraint problems which are challenging for conventional constraintprogramming solvers











m  geist and o  pietquin 2010 kalman temporal differences volume 39 pages 483532



we analyze symmetric protocols to rationally coordinate on an asymmetric efficient allocation in an infinitely repeated nagent cresource allocation problems where the resources are all homogeneous bhaskar proposed one way to achieve this in 2agent 1resource games agents start by symmetrically randomizing their actions and as soon as they each choose different actions they start to follow a potentially asymmetric convention that prescribes their actions from then on we extend the concept of convention to the general case of infinitely repeated resource allocation games with n agents and c resources we show that for any convention there exists a symmetric subgameperfect equilibrium which implements it we present two conventions bourgeois where agents stick to the first allocation and market where agents pay for the use of resources and observe a global coordination signal which allows them to alternate between different allocations we define price of anonymity of a convention as a ratio between the maximum social payoff of any asymmetric strategy profile and the expected social payoff of the subgameperfect equilibrium which implements the convention we show that while the price of anonymity of the bourgeois convention is infinite the market convention decreases this price by reducing the conflict between the agents









j  hoffmann 2003 the metricff planning system translating ignoring delete lists to numeric state variables volume 20 pages 291341



planning with numeric state variables has been a challenge for many years and was a part of the 3rd international planning competition ipc3 currently one of the most popular and successful algorithmic techniques in strips planning is to guide search by a heuristic function where the heuristic is based on relaxing the planning task by ignoring the delete lists of the available actions     we present a natural extension of ignoring delete lists to numeric state variables preserving the relevant theoretical properties of the strips relaxation under the condition that the numeric task at hand is monotonic we then identify a subset of the numeric ipc3 competition language linear tasks where monotonicity can be achieved by preprocessing based on that we extend the algorithms used in the heuristic planning system ff to linear tasks the resulting system metricff is according to the ipc3 results which we discuss one of the two currently most efficient numeric planners





j  y halpern and  d  koller 2004 representation dependence in probabilistic inference volume 21 pages 319356



nondeductive reasoning systems are often representation dependent representing the same situation in two different ways may cause such a system to return two different answers  some have viewed this as a significant problem  for example the principle of maximum entropyhas been subjected to much criticism due to its representation dependence there has however been almost no work investigating representation dependence  in this paper we formalize this notion and show that it is not a problem specific to maximum entropy  in fact we show that any representationindependent probabilistic inference procedure that ignores irrelevant information is essentially entailment in a precise sense  moreover we show that representation independence is incompatible with even a weak default assumption of independence  we then show that invariance under a restricted class of representation changes can form a reasonable compromise between representation independence and other desiderata and provide a construction of a family of inference procedures that provides such restricted representation independence using relative entropy





j  wu and e  h durfee 2010 resourcedriven missionphasing techniques for constrained agents in stochastic environments volume 38 pages 415473



because an agents resources dictate what actions it can possibly take it should plan which resources it holds over time carefully considering its inherent limitations such as power or payload restrictions the competing needs of other agents for the same resources and the stochastic nature of the environment such agents can in general achieve more of their objectives if they can use  and even create  opportunities to change which resources they hold at various times  driven by resource constraints the agents could break their overall missions into an optimal series of phases optimally reconfiguring their resources at each phase and optimally using their assigned resources in each phase given their knowledge of the stochastic environment

in this paper we formally define and analyze this constrained sequential optimization problem in both the singleagent and multiagent contexts we present a family of mixed integer linear programming milp formulations of this problem that can optimally create phases when phases are not predefined accounting for costs and limitations in phase creation  because our formulations multaneously also find the optimal allocations of resources at each phase and the optimal policies for using the allocated resources at each phase they exploit structure across these coupled problems this allows them to find solutions significantly fasterorders of magnitude faster in larger problems than alternative solution techniques as we demonstrate empirically





t  di noia e  di sciascio and f  m donini 2007 semantic matchmaking as nonmonotonic reasoning a description logic approach volume 29 pages 269307



matchmaking arises when supply and demand meet in an electronic marketplace or when agents search for a web service to perform some task or even when recruiting agencies match curricula and job profiles  in such open environments the objective of a matchmaking process is to discover best available offers to a given request

finally we report on the implementation of the proposed matchmaking framework which has been used both as a mediator in emarketplaces and for semantic web services discovery







n  a snooke and m  h lee 2013 qualitative order of magnitude energyflowbased failure modes and effects analysis volume 46 pages 413447



f  bromberg d  margaritis and v  honavar 2009 efficient markov network structure discovery using independence tests volume 35 pages 449484



  we present two algorithms for learning the structure of a markov network from data  gsmn and gsimn  both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests  until very recently algorithms for structure learning were based on maximum likelihood estimation which has been proved to be nphard for markov networks due to the difficulty of estimating the parameters of the network needed for the computation of the data likelihood  the independencebased approach does not require the computation of the likelihood and thus both gsmn and gsimn can compute the structure efficiently as shown in our experiments  gsmn is an adaptation of the growshrink algorithm of margaritis and thrun for learning the structure of bayesian networks  gsimn extends gsmn by additionally exploiting pearls wellknown properties of the conditional independence relation to infer novel independences from known ones thus avoiding the performance of statistical tests to estimate them  to accomplish this efficiently gsimn uses the triangle theorem also introduced in this work which is a simplified version of the set of markov axioms  experimental comparisons on artificial and realworld data sets show gsimn can yield significant savings with respect to gsmn while generating a markov network with comparable or in some cases improved quality  we also compare gsimn to a forwardchaining implementation called gsimnfch that produces all possible conditional independences resulting from repeatedly applying  pearls theorems on the known conditional independence tests   the results of this comparison show that gsimn by the sole use of the triangle theorem is nearly optimal in terms of the set of independences tests that it infers





finding high quality plans for large planning problems is hard although some current anytime planners are often able to improve plans quickly they tend to reach a limit at which the plans produced are still very far from the best possible but these planners fail to find any further improvement even when given several hours of runtime

even starting from the best plans found by other means bdpo2 is able to continue improving plan quality often producing better plans than other anytime planners when all are given enough runtime the best results however are achieved by a combination of different techniques working together







u  endriss n  maudet f  sadri and f  toni 2006 negotiating socially optimal allocations of resources volume 25 pages 315348



a multiagent system may be thought of as an artificial society of autonomous software agents and we can apply concepts borrowed from welfare economics and social choice theory to assess the social welfare of such an agent society in this paper we study an abstract negotiation framework where agents can agree on multilateral deals to exchange bundles of indivisible resources we then analyse how these deals affect social welfare for different instances of the basic framework and different interpretations of the concept of social welfare itself in particular we show how certain classes of deals are both sufficient and necessary to guarantee that a socially optimal allocation of resources will be reached eventually 







p  m murphy and  m  j pazzani 1994 exploring the decision forest an empirical investigation of occams   razor in decision tree induction volume 1 pages 257275



we report on a series of experiments in which all decision trees consistent with the training data are constructed these experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees in particular we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data the experiments were performed on a massively parallel maspar computer the results of the experiments on several artificial and two real world problems indicate that for many of the problems investigated smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees





m  cristani 1999 the complexity of reasoning about spatial congruence volume 11 pages 361390



in the recent literature of artificial intelligence an    intensive research effort has been spent for various algebras of    qualitative relations used in the representation of temporal and    spatial knowledge on the problem of classifying the computational    complexity of reasoning problems for subsets of algebras  the main    purpose of these researches is to describe a restricted set of maximal    tractable subalgebras ideally in an exhaustive fashion with respect    to the hosting algebras      in this paper we introduce a novel algebra for reasoning about spatial    congruence show that the satisfiability problem in the spatial    algebra mc4 is npcomplete and present a complete classification of    tractability in the algebra based on the individuation of three    maximal tractable subclasses one containing the basic relations  the    three algebras are formed by 14 10 and 9 relations out of 16 which    form the full algebra





j  ortega 1995 on the informativeness of the dna promoter sequences domain theory volume 2 pages 361367



the dna promoter sequences domain theory and database havebecome popular for testing systems that integrate empirical andanalytical learning  this note reports a simple change andreinterpretation of the domain theory in terms of mofn conceptsinvolving no learning that results in an accuracy of 934 on the 106items of the database  moreover an exhaustive search of the space ofmofn domain theory interpretations indicates that the expectedaccuracy of a randomly chosen interpretation is 765 and that amaximum accuracy of 972 is achieved in 12 cases  this demonstratesthe informativeness of the domain theory without the complications ofunderstanding the interactions between various learning algorithms andthe theory  in addition our results help characterize the difficultyof learning using the dna promoters theory





j  f baget and  m  l mugnier 2002 extensions of simple conceptual graphs the complexity of rules and constraints volume 16 pages 425465



simple conceptual graphs are considered as the kernel of    most knowledge representation formalisms built upon sowas    model reasoning in this model can be expressed by a graph    homomorphism called projection whose semantics is usually given in    terms of positive conjunctive existential fol  we present here a    family of extensions of this model based on rules and constraints    keeping graph homomorphism as the basic operation we focus on the    formal definitions of the different models obtained including their    operational semantics and relationships with fol and we analyze the    decidability and complexity of the associated problems consistency    and deduction as soon as rules are involved in reasonings these    problems are not decidable but we exhibit a condition under which    they fall in the polynomial hierarchy  these results extend and    complete the ones already published by the authors  moreover we    systematically study the complexity of some particular cases obtained    by restricting the form of constraints andor rules





y  gao and  j  culberson 2002 an analysis of phase transition in nk landscapes volume 17 pages 309332



in this paper we analyze the decision version of the nk    landscape model from the perspective of threshold phenomena and phase    transitions under two random distributions the uniform probability    model and the fixed ratio model for the uniform probability model we    prove that the phase transition is easy in the sense that there is a    polynomial algorithm that can solve a random instance of the problem    with the probability asymptotic to 1 as the problem size tends to    infinity for the fixed ratio model we establish several upper bounds    for the solubility threshold and prove that random instances with    parameters above these upper bounds can be solved polynomially this    together with our empirical study for random instances generated below    and in the phase transition region suggests that the phase transition    of the fixed ratio model is also easy







a  atserias j  k fichte and m  thurley 2011 clauselearning algorithms with many restarts and boundedwidth resolution volume 40 pages 353373





r  khardon 1995 translating between horn representations and their characteristic models volume 3 pages 349372



characteristic models are an alternative model based    representation for horn expressions it has been shown that these two    representations are incomparable and each has its advantages over the    other it is therefore natural to ask what is the cost of translating    back and forth between these representations interestingly the same    translation questions arise in database theory where it has    applications to the design of relational databases this paper studies    the computational complexity of these problems        our main result is that the two translation problems are equivalent under    polynomial reductions and that they are equivalent to the corresponding    decision problem namely translating is equivalent to deciding whether a    given set of models is the set of characteristic models for a given horn    expression            we also relate these problems to the hypergraph transversal problem a    well known problem which is related to other applications in ai and for    which no polynomial time algorithm is known it is shown that in general    our translation problems are at least as hard as the hypergraph    transversal problem and in a special case they are equivalent to it



artifact systems are a novel paradigm for specifying and implementing business processes described in terms of interacting modules called artifacts artifacts consist of data and lifecycles accounting respectively for the relational structure of the artifacts states and their possible evolutions over time in this paper we put forward artifactcentric multiagent systems a novel formalisation of artifact systems in the context of multiagent systems operating on them differently from the usual processbased models of services we give a semantics that explicitly accounts for the data structures on which artifact systems are defined









jr  doppa a  fern and p  tadepalli 2014 hcsearch a learning framework for searchbased structured prediction volume 50 pages 369407





realtime heuristic search is a challenging type of agentcentered search because the agents planning time per action is bounded by a constant independent of problem size a common problem that imposes such restrictions is pathfinding in modern computer games where a large number of units must plan their paths simultaneously over large maps common search algorithms eg a ida d ara ad are inherently not realtime and may lose completeness when a constant bound is imposed on peraction planning time realtime search algorithms retain completeness but frequently produce unacceptably suboptimal solutions in this paper we extend classic and modern realtime search algorithms with an automated mechanism for dynamic depth and subgoal selection the new algorithms remain realtime and complete on large computer game maps they find paths within 7 of optimal while on average expanding roughly a single state per action this is nearly a threefold improvement in suboptimality over the existing stateoftheart algorithms and at the same time a 15fold improvement in the amount of planning per action









l  p kaelbling  m  l littman and  a  w moore 1996 reinforcement learning  a survey volume 4 pages 237285



this paper surveys the field of reinforcement learning from    a computerscience perspective it is written to be accessible to    researchers familiar with machine learning  both the historical basis    of the field and a broad selection of current work are summarized    reinforcement learning is the problem faced by an agent that learns    behavior through trialanderror interactions with a dynamic    environment  the work described here has a resemblance to work in    psychology but differs considerably in the details and in the use of    the word reinforcement  the paper discusses central issues of    reinforcement learning including trading off exploration and    exploitation establishing the foundations of the field via markov    decision theory learning from delayed reinforcement constructing    empirical models to accelerate learning making use of generalization    and hierarchy and coping with hidden state  it concludes with a    survey of some implemented systems and an assessment of the practical    utility of current methods for reinforcement learning







o  kurland and e  krikon 2011 the opposite of smoothing a language model approach to ranking queryspecific document clusters volume 41 pages 367395





2005 ijcaijair best paper prize



we describe and evaluate the algorithmic techniques that are    used in the ff planning system like the hsp system ff relies on    forward state space search using a heuristic that estimates goal    distances by ignoring delete lists unlike hsps heuristic our method    does not assume facts to be independent we introduce a novel search    strategy that combines hillclimbing with systematic search and we    show how other powerful heuristic information can be extracted and    used to prune the search space ff was the most successful automatic    planner at the recent aips2000 planning competition we review the    results of the competition give data for other benchmark domains and    investigate the reasons for the runtime performance of ff compared to    hsp



the focus of this paper is the calculation of similarity between two concepts from an ontology for a humanlike interaction system in order to facilitate this calculation a similarity function is proposed based on five dimensions sort compositional essential restrictive and descriptive constituting the structure of ontological knowledge the paper includes a proposal for computing a similarity function for each dimension of knowledge later on the similarity values obtained are weighted and aggregated to obtain a global similarity measure in order to calculate those weights associated to each dimension four training methods have been proposed the training methods differ in the element to fit the user concepts or pairs of concepts and a hybrid approach for evaluating the proposal the knowledge base was fed from wordnet and extended by using a knowledge editing toolkit cognos the evaluation of the proposal is carried out through the comparison of system responses with those given by human test subjects both providing a measure of the soundness of the procedure and revealing ways in which the proposal may be improved







y  chen b  w wah and c  hsu 2006 temporal planning using subgoal partitioning and resolution in sgplan volume 26 pages 323369



in this paper we present the partitioning of mutualexclusion mutex constraints in temporal planning problems and its implementation in the sgplan4 planner based on the strong locality of mutex constraints observed in many benchmarks of the fourth international planning competition ipc4 we propose to partition the constraints of a planning problem into groups based on their subgoals constraint partitioning leads to significantly easier subproblems that are similar to the original problem and that can be efficiently solved by the same planner with some modifications to its objective function we present a partitionandresolve strategy that looks for locally optimal subplans in constraintpartitioned temporal planning subproblems and that resolves those inconsistent global constraints across the subproblems we also discuss some implementation details of sgplan4 which include the resolution of violated global constraints techniques for handling producible resources landmark analysis path finding and optimization searchspace reduction and modifications of metricff when used as a basic planner in sgplan4  last we show results on the sensitivity of each of these techniques in qualitytime tradeoffs and experimentally demonstrate that sgplan4 is effective for solving the ipc3 and ipc4 benchmarks



