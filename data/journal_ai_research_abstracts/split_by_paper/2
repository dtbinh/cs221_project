



r  i brafman and  m  tennenholtz 2003 learning to coordinate efficiently a modelbased approach volume 19 pages 1123



in commoninterest stochastic games all players receive an identical payoff players participating in such games must learn to coordinate with each other in order to receive the highestpossible value a number of reinforcement learning algorithms have been proposed for this problem and some have been shown to converge to good solutions in the limit in this paper we show that using very simple modelbased algorithms much better ie polynomial convergence rates can be attained moreover our modelbased algorithms are guaranteed to converge to the optimal value unlike many of the existing algorithms





s  wermter and  v  weber 1997 screen learning a flat syntactic and semantic spoken language analysis using artificial neural networks volume 6 pages 3585



previous approaches of analyzing spontaneously spoken    language often have been based on encoding syntactic and semantic    knowledge manually and symbolically while there has been some    progress using statistical or connectionist language models many    current spoken language systems still use a relatively brittle    handcoded symbolic grammar or symbolic semantic component        in contrast we describe a socalled screening approach for learning    robust processing of spontaneously spoken language  a screening    approach is a flat analysis which uses shallow sequences of category    representations for analyzing an utterance at various syntactic    semantic and dialog levels  rather than using a deeply structured    symbolic analysis we use a flat connectionist analysis  this    screening approach aims at supporting speech and language processing    by using 1 datadriven learning and 2 robustness of connectionist    networks  in order to test this approach we have developed the    screen system which is based on this new robust learned and flat    analysis        in this paper we focus on a detailed description of screens    architecture the flat syntactic and semantic analysis the    interaction with a speech recognizer and a detailed evaluation    analysis of the robustness under the influence of noisy or incomplete    input  the main result of this paper is that flat representations    allow more robust processing of spontaneous spoken language than    deeply structured representations  in particular we show how the    faulttolerance and learning capability of connectionist networks can    support a flat analysis for providing more robust spokenlanguage    processing within an overall hybrid symbolicconnectionist framework





v  bulitko n  sturtevant j  lu and t  yau 2007 graph abstraction in realtime heuristic search volume 30 pages 51100



realtime heuristic search methods are used by situated agents in applications that require the amount of planning per move to be independent of the problem size such agents plan only a few actions at a time in a local search space and avoid getting trapped in local minima by improving their heuristic function over time we extend a wide class of realtime search algorithms with automaticallybuilt state abstraction and prove completeness and convergence of the resulting family of algorithms we then analyze the impact of abstraction in an extensive empirical study in realtime pathfinding abstraction is found to improve efficiency by providing better trading offs between planning time learning speed and other negatively correlated performance measures





u  zahavi a  felner n  burch and r  c holte 2010 predicting the performance of ida using conditional distributions volume 37 pages 4183



korf reid and edelkamp introduced a formula to predict the number of nodes ida will expand on a single iteration for a given consistent heuristic and experimentally demonstrated that it could make very accurate predictions in this paper we show that in addition to requiring the heuristic to be consistent their formulas predictions are accurate only at levels of the bruteforce search tree where the heuristic values obey the unconditional distribution that they defined and then used in their formula we then propose a new formula that works well without these requirements ie it can  make accurate predictions of idas performance for inconsistent heuristics and if the heuristic values in any

level do not obey the unconditional distribution in order to achieve this we introduce the conditional distribution of heuristic values which is a generalization of their unconditional heuristic distribution we also provide extensions of our formula that handle individual start states and the augmentation of ida with bidirectional pathmax bpmx a technique for propagating heuristic values when inconsistent heuristics are used experimental results demonstrate the accuracy of our new method and all its variations





p  e dunne 2005 extremal behaviour in multiagent contract negotiation volume 23 pages 4178



we examine properties of a model of resource allocation in which several agents exchange resources in order to optimise their individual holdings the schemes discussed relate to wellknown negotiation protocols proposed in earlier work and we consider a number of alternative notions of rationality covering both quantitative measures eg cooperative and individual rationality and more qualitative forms eg pigoudalton transfers while it is known that imposing particular rationality and structural restrictions may result in some reallocations of the resource set becoming unrealisable in this paper we address the issue of the number of restricted rational deals that may be required to implement a particular reallocation when it is possible to do so we construct examples showing that this number may be exponential in the number of resources m even when all of the agent utility functions are monotonic we further show that k agents may achieve in a single deal a reallocation requiring exponentially many rational deals if at most k1 agents can participate this same reallocation being unrealisable by any sequences of rational deals in which at most k2 agents are involved





karsten  martiny and ralf  m246ller 2016 pdt logic a probabilistic doxastic temporal logic for reasoning about beliefs in multiagent systems volume 57 pages 39112



we present probabilistic doxastic temporal pdt logic a formalism to represent and reason about probabilistic beliefs and their temporal evolution in multiagent systems this formalism enables the quantification of agents beliefs through probability intervals and incorporates an explicit notion of time we discuss how over time agents dynamically change their beliefs in facts temporal rules and other agents beliefs with respect to any new information they receive we introduce an appropriate formal semantics for pdt logic and show that it is decidable alternative options of specifying problems in pdt logic are possible for these problem specifications we develop different satisfiability checking algorithms and provide complexity results for the respective decision problems the use of probability intervals enables a formal representation of probabilistic knowledge without enforcing possibly incorrect exact probability values by incorporating an explicit notion of time pdt logic provides enriched possibilities to represent and reason about temporal relations





d  dubois  h  fargier and  h  prade 2004 ordinal and probabilistic representations of acceptance volume 22 pages 2356



an accepted belief is a proposition considered likely enough by an agent to be inferred from as if it were true this paper bridges the gap between probabilistic and logical representations of accepted beliefs to this end natural properties of relations on propositions describing relative strength of belief are augmented with some conditions ensuring that accepted beliefs form a deductively closed set this requirement turns out to be very restrictive in particular it is shown that the sets of accepted belief of an agent can always be derived from a family of possibility rankings of states an agent accepts a proposition in a given context if this proposition is considered more possible than its negation in this context for all possibility rankings in the family these results are closely connected to the nonmonotonic preferential inference system of kraus lehmann and magidor and the socalled plausibility functions of friedman and halpern the extent to which probability theory is compatible with acceptance relations is laid bare a solution to the lottery paradox which is considered as a major impediment to the use of nonmonotonic inference is proposed using a special kind of probabilities called lexicographic or bigstepped the setting of acceptance relations also proposes another way of approaching the theory of belief change after the works of grdenfors and colleagues our view considers the acceptance relation as a primitive object from which belief sets are derived in various contexts





a  epshteyn and g  dejong 2006 generative prior knowledge for discriminative classification volume 27 pages 2553



we present a novel framework for integrating prior knowledge into discriminative classifiers  our framework allows discriminative classifiers such as support vector machines svms to utilize prior knowledge specified in the generative setting the dual objective of fitting the data and respecting prior knowledge is formulated as a bilevel program which is solved approximately via iterative application of secondorder cone programming  to test our approach we consider the problem of using wordnet a semantic database of english language to improve lowsample classification accuracy of newsgroup categorization  wordnet is viewed as an approximate but readily available source of background knowledge and our framework is capable of utilizing it in a flexible way





p   j gmytrasiewicz and  p  doshi 2005 a framework for sequential planning in multiagent settings volume 24 pages 4979



this paper extends the framework of partially observable markov decision processes pomdps to multiagent settings by incorporating the notion of agent models into the state space  agents maintain beliefs over physical states of the environment and over models of other agents and they use bayesian updates to maintain their beliefs over time the solutions map belief states to actions models of other agents may include their belief states and are related to agent types considered in games of incomplete information  we express the agents autonomy by postulating that their models are not directly manipulable or observable by other agents  we show that important properties of pomdps such as convergence of value iteration the rate of convergence and piecewise linearity and convexity of the value functions carry over to our framework  our approach complements a more traditional approach to interactive settings which uses nash equilibria as a solution paradigm  we seek to avoid some of the drawbacks of equilibria which may be nonunique and do not capture offequilibrium behaviors  we do so at the cost of having to represent process and continuously revise models of other agents since the agents beliefs may be arbitrarily nested the optimal solutions to decision making problems are only asymptotically computable  however approximate belief updates and approximately optimal plans are computable we illustrate our framework using a simple application domain and we show examples of belief updates and value functions





a  t cemgil and  b  kappen 2003 monte carlo methods for tempo tracking and rhythm quantization volume 18 pages 4581



we present a probabilistic generative model for timing    deviations in expressive music performance the structure of the    proposed model is equivalent to a switching state space model the    switch variables correspond to discrete note locations as in a musical    score the continuous hidden variables denote the tempo  we formulate    two well known music recognition problems namely tempo tracking and    automatic transcription rhythm quantization as filtering and maximum    a posteriori map state estimation tasks exact computation of    posterior features such as the map state is intractable in this model    class so we introduce monte carlo methods for integration and    optimization we compare markov chain monte carlo mcmc methods such    as gibbs sampling simulated annealing and iterative improvement and    sequential monte carlo methods particle filters our simulation    results suggest better results with sequential methods  the methods    can be applied in both online and batch scenarios such as tempo    tracking and transcription and are thus potentially useful in a number    of music applications such as adaptive automatic accompaniment score    typesetting and music information retrieval





t  hogg 1999 solving highly constrained search problems with quantum computers volume 10 pages 3966



a previously developed quantum search algorithm for solving    1sat problems in a single step is generalized to apply to a range    of highly constrained ksat problems we identify a bound on the    number of clauses in satisfiability problems for which the    generalized algorithm can find a solution in a constant number of    steps as the number of variables increases this performance    contrasts with the linear growth in the number of steps required by    the best classical algorithms and the exponential number required    by classical and quantum methods that ignore the problem    structure in some cases the algorithm can also guarantee that    insoluble problems in fact have no solutions unlike previously    proposed quantum search algorithms





nasrin  taghizadeh and hesham  faili 2016 automatic wordnet development for lowresource languages using crosslingual wsd volume 56 pages 6187



8206wordnets are an effective resource for natural language processing and information retrieval8206 8206especially for semantic processing and meaning related tasks8206 8206so far8206 8206wordnets have been constructed for many languages8206 8206however8206 8206the automatic development of wordnets for lowresource languages has not been well studied8206 8206in this paper8206 8206an expectationmaximization algorithm is used to create high quality and large scale wordnets for poorresource languages8206 8206the proposed method benefits from possessing crosslingual word sense disambiguation and develops a wordnet by only using a bilingual dictionary and a monolingual corpus8206 8206the proposed method has been executed with persian language and the resulting wordnet has been evaluated through several experiments8206 8206the results show that the induced wordnet has a precision score of 90 and a recall score of 358206





r  booth t  meyer i  varzinczak and r  wassermann 2011 on the link between partial meet kernel and infra contraction and its application to horn logic volume 42 pages 3153



standard belief change assumes an underlying logic containing full classical propositional logic however there are good reasons for considering belief change in less expressive logics as well in this paper we build on recent investigations by delgrande on contraction for horn logic we show that the standard basic form of contraction partial meet is too strong in the horn case this result stands in contrast to delgrandes conjecture that orderly maxichoice is the appropriate form of contraction for horn logic we then define a more appropriate notion of basic contraction for the horn case influenced by the convexity property holding for full propositional logic and which we refer to as infra contraction the main contribution of this work is a result which shows that the construction method for horn contraction for belief sets based on our infra remainder sets corresponds exactly to hanssons classical kernel contraction for belief sets when restricted to horn logic this result is obtained via a detour through contraction for belief bases we prove that kernel contraction for belief bases produces precisely the same results as the belief base version of infra contraction the use of belief bases to obtain this result provides evidence for the conjecture that horn belief change is best viewed as a hybrid version of belief set change and belief base change one of the consequences of the link with base contraction is the provision of a representation result for horn contraction for belief sets in which a version of the coreretainment postulate features





b  lubin a  i juda r  cavallo s  lahaie j  shneidman and d  c parkes 2008 ice an expressive iterative combinatorial exchange volume 33 pages 3377



we present the design and analysis of the first fully expressive iterative combinatorial exchange ice the exchange incorporates a treebased bidding language tbbl that is concise and expressive for ces bidders specify lower and upper bounds in tbbl on their value for different trades and refine these bounds across rounds these bounds allow price discovery and useful preference elicitation in early rounds and allow termination with an efficient trade despite partial information on bidder valuations all computation in the exchange is carefully optimized to exploit the structure of the bidtrees and to avoid enumerating trades a proxied interpretation of a revealedpreference activity rule coupled with simple linear prices ensures progress across rounds the exchange is fully implemented and we give results demonstrating several aspects of its scalability and economic properties with simulated bidding strategies





honorable mention for the 2003 ijcaijair best paper prize



we examine the computational complexity of testing and    finding small plans in probabilistic planning domains with both flat    and propositional representations  the complexity of plan evaluation    and existence varies with the plan type sought we examine totally    ordered plans acyclic plans and looping plans and partially ordered    plans under three natural definitions of plan value  we show that    problems of interest are complete for a variety of complexity classes    pl p np conp pp nppp conppp and pspace  in the process of    proving that certain planning problems are complete for nppp we    introduce a new basic npppcomplete problem emajsat which    generalizes the standard boolean satisfiability problem to    computations involving probabilistic quantities our results suggest    that the development of good heuristics for emajsat could be    important for the creation of efficient algorithms for a wide variety    of problems





j  m siskind 2001 grounding the lexical semantics of verbs in visual perception using force dynamics and event logic volume 15 pages 3190



this paper presents an implemented system for recognizing    the occurrence of events described by simple spatialmotion verbs in    short image sequences the semantics of these verbs is specified with    eventlogic expressions that describe changes in the state of    forcedynamic relations between the participants of the event  an    efficient finite representation is introduced for the infinite sets of    intervals that occur when describing liquid and semiliquid events    additionally an efficient procedure using this representation is    presented for inferring occurrences of compound events described with    eventlogic expressions from occurrences of primitive events  using    force dynamics and event logic to specify the lexical semantics of    events allows the system to be more robust than prior systems based on    motion profile





m  bienvenu 2009 prime implicates and prime implicants from propositional to modal logic volume 36 pages 71128



prime implicates and prime implicants have proven relevant to a number of areas of artificial intelligence most notably abductive reasoning and knowledge compilation the purpose of this paper is to examine how these notions might be appropriately extended from propositional logic to the modal logic k we begin the paper by considering a number of potential definitions of clauses and terms for k the different definitions are evaluated with respect to a set of syntactic semantic and complexitytheoretic properties characteristic of the propositional definition  we then compare the definitions with respect to the properties of the notions of prime implicates and prime implicants that they induce while there is no definition that perfectly generalizes the propositional notions we show that there does exist one definition which satisfies many of the desirable properties of the propositional case in the second half of the paper we consider the computational properties of the selected definition to this end we provide sound and complete algorithms for generating and recognizing prime implicates and we show the prime implicate recognition task to be pspacecomplete we also prove upper and lower bounds on the size and number of prime implicates while the paper focuses on the logic k all of our results hold equally well for multimodal k and for concept expressions in the description logic alc 





f  barber 2000 reasoning on interval and pointbased disjunctive metric constraints in temporal contexts volume 12 pages 3586



we introduce a temporal model for reasoning on disjunctive    metric constraints on intervals and time points in temporal    contexts this temporal model is composed of a labeled temporal    algebra and its reasoning algorithms the labeled temporal algebra    defines labeled disjunctive metric pointbased constraints where each    disjunct in each input disjunctive constraint is univocally associated    to a label reasoning algorithms manage labeled constraints    associated label lists and sets of mutually inconsistent    disjuncts these algorithms guarantee consistency and obtain a minimal    network additionally constraints can be organized in a hierarchy of    alternative temporal contexts therefore we can reason on    contextdependent disjunctive metric constraints on intervals and    points moreover the model is able to represent nonbinary    constraints such that logical dependencies on disjuncts in    constraints can be handled  the computational cost of reasoning    algorithms is exponential in accordance with the underlying problem    complexity although some improvements are proposed





m  aramon bajestani and j c  beck 2013 scheduling a dynamic aircraft repair shop with limited repair resources volume 47 pages 3570



we address a dynamic repair shop scheduling problem in the context of military aircraft fleet management where the goal is to maintain a full complement of aircraft over the longterm a number of flights each with a requirement for a specific number and type of aircraft are already scheduled over a long horizon we need to assign aircraft to flights and schedule repair activities while considering the flights requirements repair capacity and aircraft failures the number of aircraft awaiting repair dynamically changes over time due to failures and it is therefore necessary to rebuild the repair schedule online to solve the problem we view the dynamic repair shop as successive static repair scheduling subproblems over shorter time periods we propose a complete approach based on the logicbased benders decomposition to solve the static subproblems and design different rescheduling policies to schedule the dynamic repair shop computational experiments demonstrate that the benders model is able to find and prove optimal solutions on average four times faster than a mixed integer programming model the rescheduling approach having both aspects of scheduling over a longer horizon and quickly adjusting the schedule increases aircraft available in the long term by 10 compared to the approaches having either one of the aspects alone





v237ctor m  s225nchezcartagena juan antonio  p233rezortiz and felipe  s225nchezmart237nez 2016 integrating rules and dictionaries from shallowtransfer machine translation into phrasebased statistical machine translation volume 55 pages 1761



we describe a hybridisation strategy whose objective is to integrate linguistic resources from shallowtransfer rulebased machine translation rbmt into phrasebased statistical machine translation pbsmt it basically consists of enriching the phrase table of a pbsmt system with bilingual phrase pairs matching transfer rules and dictionary entries from a shallowtransfer rbmt system this new strategy takes advantage of how the linguistic resources are used by the rbmt system to segment the sourcelanguage sentences to be translated and overcomes the limitations of existing hybrid approaches that treat the rbmt systems as a black box experimental results confirm that our approach delivers translations of  higher quality than existing ones and that it is specially useful when the parallel corpus available for training the smt system is small or when translating outofdomain texts that are well covered by the rbmt dictionaries a combination of this approach with a recently proposed unsupervised shallowtransfer rule inference algorithm results in a significantly greater translation quality than that of a baseline pbsmt in this case the only handcrafted resource used are the dictionaries commonly used in rbmt moreover the translation quality achieved by the hybrid system built with automatically inferred rules is similar to that obtained by those built with handcrafted rules





carmel  domshlak and vitaly  mirkis 2015 deterministic oversubscription planning as heuristic search abstractions and reformulations volume 52 pages 97169



while in classical planning the objective is to achieve one of the equally attractive goal states at as low total action cost as possible the objective in deterministic oversubscription planning osp is to achieve an as valuable as possible subset of goals within a fixed allowance of the total action cost although numerous applications in various fields share the latter objective no substantial algorithmic advances have been made in deterministic osp tracing the key sources of progress in classical planning we identify a severe lack of effective domainindependent approximations for osp 

with our focus here on optimal planning our goal is to bridge this gap two classes of approximation techniques have been found especially useful in the context of optimal classical planning those based on statespace  abstractions and those based on logical landmarks for goal reachability the question we  study here is whether some similarinspirit yet possibly mathematically  different approximation techniques can be developed for osp in the context of abstractions we define the notion of additive abstractions for osp study the complexity of deriving effective abstractions from a rich space of hypotheses and reveal some  substantial empirically relevant islands of tractability in the context of  landmarks we show how standard goalreachability landmarks of certain classical planning tasks  can be compiled into the osp task of interest resulting in an equivalent osp task with a lower cost allowance and thus with a smaller search space  our empirical evaluation confirms the effectiveness of the proposed techniques and opens a wide gate for further developments in oversubscription planning  





p  everaere s  konieczny and p  marquis 2007 the strategyproofness landscape of merging volume 28 pages 49105



merging operators aim at defining the beliefsgoals of a group of agents from the beliefsgoals of each member of the group whenever an agent of the group has preferences over the possible results of the merging process ie the possible merged bases she can try to rig the merging process by lying on her true beliefsgoals if this leads to better merged base according to her point of view obviously strategyproof operators are highly desirable in order to guarantee equity among agents even when some of them are not sincere in this paper we draw the strategyproof landscape for many merging operators from the literature including modelbased ones and formulabased ones both the general case and several restrictions on the merging process are considered





n  meuleau e  benazera r  i brafman e  a hansen and   mausam 2009 a heuristic search approach to planning with continuous resources in stochastic domains volume 34 pages 2759



we consider the problem of optimal planning in stochastic domains with resource constraints where the resources are continuous and the choice of action at each step depends on resource availability  we introduce the hao algorithm a generalization of the ao algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables where the continuous variables represent monotonic resources  like other heuristic search algorithms hao leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy  we show that this approach is especially effective when resource constraints limit how much of the state space is reachable  experimental results demonstrate its effectiveness in the domain that motivates our research automated planning for planetary exploration rovers







d  a cohen m  c cooper p  creed d  marx and a  z salamon 2012 the tractability of csp classes defined by forbidden patterns volume 45 pages 4778





c  yuan and b  malone 2013 learning optimal bayesian networks a shortest path perspective volume 48 pages 2365



in this paper learning a bayesian network structure that optimizes a scoring function for a given dataset is viewed as a shortest path problem in an implicit statespace search graph this perspective highlights the importance of two research issues the development of search strategies for solving the shortest path problem and the design of heuristic functions for guiding the search this paper introduces several techniques for addressing the issues one is an a search algorithm that learns an optimal bayesian network structure by only searching the most promising part of the solution space the others are mainly two heuristic functions the first heuristic function represents a simple relaxation of the acyclicity constraint of a bayesian network although admissible and consistent the heuristic may introduce too much relaxation and result in a loose bound the second heuristic function reduces the amount of relaxation by avoiding directed cycles within some groups of variables empirical results show that these methods constitute a promising approach to learning optimal bayesian network structures





mausam and d  s  weld 2008 planning with durative actions in stochastic domains volume 31 pages 3382



probabilistic planning problems are typically modeled as a markov decision process mdp mdps while an otherwise expressive model allow only for sequential nondurative actions this poses severe restrictions in modeling and solving a real world planning problem we extend the mdp model to incorporate  1 simultaneous action execution 2 durative actions and 3 stochastic durations we develop several algorithms to combat the computational explosion introduced by these features the key theoretical ideas used in building these algorithms are  modeling a complex problem as an mdp in extended stateaction space pruning of irrelevant actions sampling of relevant actions using informed heuristics to guide the search hybridizing different planners to achieve benefits of both approximating the problem and replanning our empirical evaluation illuminates the different merits in using various algorithms viz optimality empirical closeness to optimality theoretical error bounds and speed





krishna  s r dubba anthony  g cohn david  c hogg mehul   bhatt and frank  dylla 2015 learning relational event models from video volume 53 pages 4190



event models obtained automatically from video can be used in applications ranging from abnormal event detection to content based video retrieval when multiple agents are involved in the events characterizing events naturally suggests encoding interactions as relations learning event models from this kind of relational spatiotemporal data using relational learning techniques such as inductive logic programming ilp hold promise but have not been successfully applied to very large datasets which result from video data in this paper we present a novel framework remind relational event model induction for supervised relational learning of event models from large video datasets using ilp efficiency is achieved through the learning from interpretations setting and using a typing system that exploits the type hierarchy of objects in a domain the use of types also helps prevent over generalization furthermore we also present a typerefining operator and prove that it is optimal the learned models can be used for recognizing events from previously unseen videos we also present an extension to the framework by integrating an abduction step that improves the learning performance when there is noise in the input data the experimental results on several hours of video data from two challenging real world domains an airport domain and a physical action verbs domain suggest that the techniques are suitable to real world scenarios





t  drakengren and p  jonsson 1997 eight maximal tractable subclasses of allens algebra with metric time volume 7 pages 2545



this paper combines two important directions of research in temporal resoning that of finding maximal tractable subclasses of allens interval algebra and that of reasoning with metric temporal information eight new maximal tractable subclasses of allens interval algebra are presented some of them subsuming previously reported tractable algebras the algebras allow for metric temporal constraints on interval starting or ending points using the recent framework of horn dlrs two of the algebras can express the notion of sequentiality between intervals being the first such algebras admitting both qualitative and metric time





n  fu hc  lau p  varakantham and f  xiao 2012 robust local search for solving rcpspmax with durational uncertainty volume 43 pages 4386



scheduling problems in manufacturing logistics and project management have frequently been modeled using the framework of resource constrained project scheduling problems with minimum and maximum time lags rcpspmax due to the importance of these problems providing scalable solution schedules for rcpspmax problems is a topic of extensive research however all existing methods for solving rcpspmax assume that durations of activities are known with certainty an assumption that does not hold in real world scheduling problems where unexpected external events such as manpower availability weather changes etc lead to delays or advances in completion of activities thus in this paper our focus is on providing a scalable method for solving rcpspmax problems with durational uncertainty to that end we introduce the robust local search method consisting of three key ideas a introducing and studying the properties of two decision rule approximations used to compute start times of activities with respect to dynamic realizations of the durational uncertainty b deriving the expression for robust makespan of an execution strategy based on decision rule approximations and c a robust local search mechanism to efficiently compute activity execution strategies that are robust against durational uncertainty furthermore we also provide enhancements to local search that exploit temporal dependencies between activities our experimental results illustrate that robust local search is able to provide robust execution strategies efficiently





m  katz and c  domshlak 2010 implicit abstraction heuristics volume 39 pages 51126



statespace search with explicit abstraction heuristics is at the state of the art of costoptimal planning these heuristics are inherently limited nonetheless because the size of the abstract space must be bounded by some even if a very large constant targeting this shortcoming we introduce the notion of iadditive implicit abstractionsi in which the planning task is abstracted by instances of tractable fragments of optimal planning we then introduce a concrete setting of this framework called iforkdecompositioni that is based on two novel fragments of tractable costoptimal planning the induced admissible heuristics are then studied formally and empirically this  study testifies for the accuracy of the fork decomposition heuristics yet our empirical evaluation also stresses the tradeoff between their accuracy and the runtime complexity of computing them indeed some of the power of the explicit abstraction heuristics comes from precomputing the heuristic function offline and then determining ihsi for each evaluated state isi by a very fast lookup in a database by contrast while forkdecomposition heuristics can be calculated in polynomial time  computing them is far from being fast to address this problem we show that the timepernode complexity bottleneck of the forkdecomposition heuristics  can be successfully overcome we demonstrate that an equivalent of the explicit abstraction notion of a database exists for the forkdecomposition abstractions as well despite their exponentialsize abstract spaces we then verify empirically that heuristic search with the databased forkdecomposition heuristics favorably competes with the state of the art of costoptimal planning





l  climent r  j wallace m  a salido and f  barber 2014 robustness and stability in constraint programming under dynamism and uncertainty volume 49 pages 4978



many real life problems that can be solved by constraint programming come from uncertain and dynamic environments because of the dynamism the original problem may change over time and thus the solution found for the original problem may become invalid for this reason dealing with such problems has become an important issue in the fields of constraint programming in some cases there exist extant knowledge about the uncertain and dynamic environment in other cases this information is fragmentary or unknown in this paper we extend the concept of robustness and stability for constraint satisfaction problems csps with ordered domains where only limited assumptions need to be made as to possible changes we present a search algorithm that searches for both robust and stable solutions for csps of this nature it is wellknown that meeting both criteria simultaneously is a desirable objective for constraint solving in uncertain and dynamic environments we also present compelling evidence that our search algorithm outperforms other generalpurpose algorithms for dynamic csps using random instances and benchmarks derived from real life problems





m  fox and  d  long 2003 pddl21 an extension to pddl for expressing temporal planning domains volume 20 pages 61124



in recent years research in the planning community has moved increasingly toward s application of planners to realistic problems involving both time and many typ es of resources for example interest in planning demonstrated by the space res earch community has inspired work in observation scheduling planetary rover ex ploration and spacecraft control domains other temporal and resourceintensive domains including logistics planning plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application   the international planning competitions have acted as an important motivating fo rce behind the progress that has been made in planning since 1998 the third com petition held in 2002 set the planning community the challenge of handling tim e and numeric resources this necessitated the development of a modelling langua ge capable of expressing temporal and numeric properties of planning domains in this paper we describe the language pddl21 that was used in the competition  we describe the syntax of the language its formal semantics and the validation of concurrent plans we observe that pddl21 has considerable modelling power  exceeding the capabilities of current planning technology  and presents a number of important challenges to the research community





m  p wellman  d  m reeves  k  m lochner and  y  vorobeychik 2004 price prediction in a trading agent competition volume 21 pages 1936



the 2002 trading agent competition tac presented a challenging market game in the domain of travel shopping  one of the pivotal issues in this domain is uncertainty about hotel prices which have a significant influence on the relative cost of alternative trip schedules  thus virtually all participants employ some method for predicting hotel prices  we survey approaches employed in the tournament finding that agents apply an interesting diversity of techniques taking into account differing sources of evidence bearing on prices  based on data provided by entrants on their agents actual predictions in the tac02 finals and semifinals we analyze the relative efficacy of these approaches  the results show that taking into account gamespecific information about flight prices is a major distinguishing factor  machine learning methods effectively induce the relationship between flight and hotel prices from game data and a purely analytical approach based on competitive equilibrium analysis achieves equal accuracy with no historical data  employing a new measure of prediction quality we relate absolute accuracy to bottomline performance in the game





c  cayrol f  dupin de saintcyr and m  lagasquieschiex 2010 change in abstract argumentation frameworks adding an argument volume 38 pages 4984



in this paper we address the problem of change in an abstract argumentation system we focus on a particular change the addition of  a new argument which interacts with previous arguments we study the impact of such an addition on the outcome of the argumentation system more particularly on the set of its extensions several properties for this change operation are defined by comparing the new set of extensions to the initial one these properties are called structural when the comparisons are based on setcardinality or setinclusion relations several other properties are proposed where comparisons are based on the status of some particular arguments the accepted arguments these properties refer to the evolution of this status during the change eg monotony and priority to recency all these  properties may be more or less desirable according to specific applications they are studied under two particular semantics the grounded and preferred semantics





m  hauskrecht 2000 valuefunction approximations for partially observable markov decision processes volume 13 pages 3394



partially observable markov decision processes pomdps    provide an elegant mathematical framework for modeling complex    decision and planning problems in stochastic domains in which states    of the system are observable only indirectly via a set of imperfect    or noisy observations the modeling advantage of pomdps however    comes at a price  exact methods for solving them are computationally    very expensive and thus applicable in practice only to very simple    problems we focus on efficient approximation heuristic methods that    attempt to alleviate the computational problem and trade off accuracy    for speed we have two objectives here first we survey various    approximation methods analyze their properties and relations and    provide some new insights into their differences second we present a    number of new approximation methods and novel refinements of existing    techniques the theoretical results are supported by experiments on a    problem from the agent navigation domain







n  nisan and a  ronen 2007 computationally feasible vcg mechanisms volume 29 pages 1947





e  huang and r  e korf 2013 optimal rectangle packing an absolute placement approach volume 46 pages 4787



we consider the problem of finding all enclosing rectangles of minimum area that can contain a given set of rectangles without overlap  our rectangle packer chooses the xcoordinates of all the rectangles before any of the ycoordinates we then transform the problem into a perfectpacking problem with no empty space by adding additional rectangles to determine the ycoordinates we branch on the different rectangles that can be placed in each empty position our packer allows us to extend the known solutions for a consecutivesquare benchmark from 27 to 32 squares we also introduce three new benchmarks avoiding properties that make a benchmark easy such as rectangles with shared dimensions our third benchmark consists of rectangles of increasingly high precision to pack them efficiently we limit the rectangles coordinates and the bounding box dimensions to the set of subset sums of the rectangles dimensions overall our algorithms represent the current stateoftheart for this problem outperforming other algorithms by orders of magnitude depending on the benchmark





j  hoffmann p  bertoli m  helmert and m  pistore 2009 messagebased web service composition integrity constraints and planning under uncertainty a new connection volume 35 pages 49117



thanks to recent advances ai planning has become the underlying technique for several applications figuring prominently among these is automated web service composition wsc at the capability level where services are described in terms of preconditions and effects over ontological concepts a key issue in addressing wsc as planning is that ontologies are not only formal vocabularies they also axiomatize the possible relationships between concepts such axioms correspond to what has been termed integrity constraints in the actions and change literature and applying a web service is essentially a belief update operation the reasoning required for belief update is known to be harder than reasoning in the ontology itself the support for belief update is severely limited in current planning tools





sigve  hortemo s230ther jan arne  telle and martin   vatshelle 2015 solving sat and maxsat by dynamic programming  volume 54 pages 5982



we look at dynamic programming algorithms for propositional model counting also called sat and maxsat tools from graph structure theory in particular treewidth have been used to successfully identify tractable cases in many subfields of ai including sat constraint satisfaction problems csp bayesian reasoning and planning in this paper we attack sat and maxsat using similar but more modern graph structure tools the tractable cases will include formulas whose class of incidence graphs have not only unbounded treewidth but also unbounded cliquewidth we show that our algorithms extend all previous results for maxsat and sat achieved by dynamic programming along structural decompositions of the incidence graph of the input formula we present some limited experimental results comparing implementations of our algorithms to stateoftheart sat and maxsat solvers as a proof of concept that warrants further research





s  thiebaux c  gretton j  slaney d  price and f  kabanza 2006 decisiontheoretic planning with nonmarkovian rewards volume 25 pages 1774



a decision process in which rewards depend on history rather than merely on the current state is called a decision process with nonmarkovian rewards nmrdp in decisiontheoretic planning where many desirable behaviours are more naturally expressed as properties of execution sequences rather than as properties of states nmrdps form a more natural model than the commonly adopted fully markovian decision process mdp model while the more tractable solution methods developed for mdps do not directly apply in the presence of nonmarkovian rewards a number of solution methods for nmrdps have been proposed in the literature these all exploit a compact specification of the nonmarkovian reward function in temporal logic to automatically translate the nmrdp into an equivalent mdp which is solved using efficient mdp solution methods this paper presents nmrdpp nonmarkovian reward decision process planner a software platform for the development and experimentation of methods for decisiontheoretic planning with nonmarkovian rewards the current version of nmrdpp implements under a single interface a family of methods based on existing as well as new approaches which we describe in detail these include dynamic programming heuristic search and structured methods using nmrdpp we compare the methods and identify certain problem features that affect their performance nmrdpps treatment of nonmarkovian rewards is inspired by the treatment of domainspecific search control knowledge in the tlplan planner which it incorporates as a special case in the first international probabilistic planning competition nmrdpp was able to compete and perform well in both the domainindependent and handcoded tracks using search control knowledge in the latter 





2012 ifaamas award for influential papers in autonomous agents and multiagent systems



market price systems constitute a wellunderstood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead  in a imarketoriented programmingi approach to distributed problem solving we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy  walras provides basic constructs for defining computational market structures and protocols for deriving their corresponding price equilibria  in a particular realization of this approach for a form of multicommodity flow problem we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation and that the behavior of the system can be meaningfully analyzed in economic terms





p  resnik 1999 semantic similarity in a taxonomy an informationbased measure and its application to problems of ambiguity in natural language volume 11 pages 95130



this article presents a measure of semantic similarity in an    isa taxonomy based on the notion of shared information content    experimental evaluation against a benchmark set of human similarity    judgments demonstrates that the measure performs better than the    traditional edgecounting approach  the article presents algorithms    that take advantage of taxonomic similarity in resolving syntactic and    semantic ambiguity along with experimental results demonstrating    their effectiveness





a  j grove  j  y halpern and  d  koller 1994 random worlds and maximum entropy volume 2 pages 3388



given a knowledge base ikbi containing firstorder and statistical facts we consider a principled method called the irandomworlds methodi for computing a degree of belief that some formula iphii holds given ikbi  if  we are reasoning about a world or system consisting of ini individuals then we can consider all possible worlds or firstorder models withdomain i1ni that satisfy ikbi and compute thefraction of them in which iphii is true we define the degree of belief to be the asymptotic value of this fraction as ini grows large  we show that when the vocabulary underlying iphii andikbi uses constants and unary predicates only we can naturally associate an ientropyi with each world as ini grows largerthere are many more worlds with higher entropy  therefore we can usea i maximumentropyi computation to compute the degree of belief this result is in a similar spirit to previous work in physics and artificial intelligence but is far more general  of equal interest to the result itself are the limitations on its scope  most importantly the restriction to unary predicates seems necessary although the randomworlds method makes sense in general the connection to maximum entropy seems to disappear in the nonunary case  these observations suggest unexpected limitations to the applicability of maximumentropy methods





c  drummond 2002 accelerating reinforcement learning by composing solutions of automatically identified subtasks volume 16 pages 59104



this paper discusses a system that accelerates reinforcement    learning by using transfer from related tasks  without such    transfer even if two tasks are very similar at some abstract level    an extensive relearning effort is required  the system achieves    much of its power by transferring parts of previously learned    solutions rather than a single complete solution the system    exploits strong features in the multidimensional function produced    by reinforcement learning in solving a particular task these    features are stable and easy to recognize early in the learning    process they generate a partitioning of the state space and thus    the function  the partition is represented as a graph  this is    used to index and compose functions stored in a case base to form a    close approximation to the solution of the new task  experiments    demonstrate that function composition often produces more than an    order of magnitude increase in learning rate compared to a basic    reinforcement learning algorithm





r  barzilay and  n  elhadad 2002 inferring strategies for sentence ordering in multidocument news summarization volume 17 pages 3555



the problem of organizing information for multidocument    summarization so that the generated summary is coherent has received    relatively little attention  while sentence ordering for single    document summarization can be determined from the ordering of    sentences in the input article this is not the case for multidocument    summarization where summary sentences may be drawn from different    input articles in this paper we propose a methodology for studying    the properties of ordering information in the news genre and describe    experiments done on a corpus of multiple acceptable orderings we    developed for the task based on these experiments we implemented a    strategy for ordering information that combines constraints from    chronological order of events and topical relatedness  evaluation of    our augmented algorithm shows a significant improvement of the    ordering over two baseline strategies





y  zhou and y  zhang 2011 a logical study of partial entailment volume 40 pages 2556



we introduce a novel logical notionpartial entailmentto propositional logic in contrast with classical entailment that a formula p partially entails another formula q with respect to a background formula set gamma intuitively means that under the circumstance of gamma if p is true then some part of q will also be true we distinguish three different kinds of partial entailments and formalize them by using an extended notion of prime implicant we study their semantic properties which show that surprisingly partial entailments fail for many simple inference rules then we study the related computational properties which indicate that partial entailments are relatively difficult to be computed finally we consider a potential application of partial entailments in reasoning about rational agents





m  veloso and  p  stone 1995 flecs planning with a flexible commitment strategy volume 3 pages 2552



there has been evidence that leastcommitment planners can    efficiently handle planning problems that involve difficult goal    interactions  this evidence has led to the common belief that    delayedcommitment is the best possible planning strategy  however    we recently found evidence that eagercommitment planners can handle a    variety of planning problems more efficiently in particular those    with difficult operator choices  resigned to the futility of trying    to find a universally successful planning strategy we devised a    planner that can be used to study which domains and problems are best    for which planning strategies in this article we introduce this new    planning algorithm flecs which uses a flexible commitment strategy    with respect to planstep orderings  it is able to use any strategy    from delayedcommitment to eagercommitment  the combination of    delayed and eager operatorordering commitments allows flecs to take    advantage of the benefits of explicitly using a simulated execution    state and reasoning about planning constraints  flecs can vary its    commitment strategy across different problems and domains and also    during the course of a single planning problem  flecs represents a    novel contribution to planning in that it explicitly provides the    choice of which commitment strategy to use while planning  flecs    provides a framework to investigate the mapping from planning domains    and problems to efficient planning strategies





m  winikoff and s  cranefield 2014 on the testability of bdi agent systems volume 51 pages 71131



before deploying a software system we need to assure ourselves and stakeholders that the system will behave correctly this assurance is usually done by testing the system however it is intuitively obvious that adaptive systems including agentbased systems  can exhibit complex behaviour and are thus harder to test in this paper we examine this obvious intuition in the case of beliefdesireintention bdi agents  we analyse the size of the behaviour space of bdi agents  and show that although the intuition is correct the factors that influence the size are not what we expected them to be specifically we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected we also discuss the implications of these findings on the testability of bdi agents





r  beneliyahu 1996 a hierarchy of tractable subsets for computing stable models volume 5 pages 2752



finding the stable models of a knowledge base is a    significant computational problem in artificial intelligence this    task is at the computational heart of truth maintenance systems    autoepistemic logic and default logic  unfortunately it is nphard    in this paper we present a hierarchy of classes of knowledge bases    omega1omega2 with the following properties first omega1 is    the class of all stratified knowledge bases second if a knowledge    base pi is in omegak then pi has at most k stable models and all of    them may be found in time olnk where l is the length of the    knowledge base and n the number of atoms in pi third for an    arbitrary knowledge base pi we can find the minimum k such that pi    belongs to omegak in time polynomial in the size of pi and last    where k is the class of all knowledge bases it is the case that    unioni1 to infty omegai  k that is every knowledge base    belongs to some class in the hierarchy





y  wang y  zhang y  zhou and m  zhang 2014 knowledge forgetting in answer set programming volume 50 pages 3170



the ability of discarding or hiding irrelevant information has been

recognized as an important feature for knowledge based systems including answer set programming the notion of strong equivalence in answer set programming plays an important role for different problems as it gives rise to a substitution principle and amounts to knowledge equivalence of logic programs in this paper we uniformly propose a semantic knowledge forgetting  called ht and flpforgetting for logic programs under stable model and flpstable model semantics respectively our proposed knowledge forgetting discards exactly the knowledge of a logic program which is relevant to forgotten variables thus it preserves strong equivalence in the sense that strongly equivalent logic programs will remain strongly equivalent after forgetting the same variables we show that this semantic forgetting result is always expressible and we prove a representation theorem stating that the ht and flpforgetting can be precisely characterized by zhangzhous four forgetting postulates under the ht and flpmodel semantics respectively we also reveal underlying connections between the proposed forgetting and the forgetting of propositional logic and provide complexity results for decision problems in relation to the forgetting an application of the proposed forgetting is also considered in a conflict solving scenario





a  analyti g  antoniou c  v damasio and g  wagner 2008 extended rdf as a semantic foundation of rule markup languages volume 32 pages 3794



ontologies and automated reasoning are the building blocks of the semantic web initiative derivation rules can be included in an ontology to define derived concepts based on base concepts for example rules allow to define the extension of a class or property based on a complex relation between the extensions of the same or other classes and properties on the other hand the inclusion of negative information both in the form of negationasfailure and explicit negative information is also needed to enable various forms of reasoning in this paper we extend rdf graphs with weak and strong negation as well as derivation rules the erdf stable model semantics of the extended framework extended rdf is defined extending rdfs semantics a distinctive feature of our theory which is based on partial logic is that both truth and falsity extensions of properties and classes are considered allowing for truth value gaps our framework supports both closedworld and openworld reasoning through the explicit representation of the particular closedworld assumptions and the erdf ontological categories of total properties and total classes





g  gogic  c  h papadimitriou and  m  sideri 1998 incremental recompilation of knowledge volume 8 pages 2337



approximating a general formula from above and below by horn    formulas its horn envelope and horn core respectively was proposed    by selman and kautz 1991 1996 as a form of knowledge    compilation supporting rapid approximate reasoning on the negative    side this scheme is static in that it supports no updates and has    certain complexity drawbacks pointed out by kavvadias papadimitriou    and sideri 1993  on the other hand the many frameworks and schemes    proposed in the literature for theory update and revision are plagued    by serious complexitytheoretic impediments even in the horn case as    was pointed out by eiter and gottlob 1992 and is further    demonstrated in the present paper  more fundamentally these schemes    are not inductive in that they may lose in a single update any    positive properties of the represented sets of formulas small size    horn structure etc  in this paper we propose a new scheme    incremental recompilation which combines horn approximation and    modelbased updates this scheme is inductive and very efficient free    of the problems facing its constituents  a set of formulas is    represented by an upper and lower horn approximation  to update we    replace the upper horn formula by the horn envelope of its    minimumchange update and similarly the lower one by the horn core of    its update the key fact which enables this scheme is that horn    envelopes and cores are easy to compute when the underlying formula is    the result of a minimumchange update of a horn formula by a clause    we conjecture that efficient algorithms are possible for more complex    updates





g  brewka 1996 wellfounded semantics for extended logic programs with dynamic preferences volume 4 pages 1936



the paper describes an extension of wellfounded    semantics for logic programs with two types of negation in this    extension information about preferences between rules can be expressed    in the logical language and derived dynamically this is achieved by    using a reserved predicate symbol and a naming technique conflicts    among rules are resolved whenever possible on the basis of derived    preference information the wellfounded conclusions of prioritized    logic programs can be computed in polynomial time a legal reasoning    example illustrates the usefulness of the approach





l  xia and v  conitzer 2011 determining possible and necessary winners given partial orders volume 41 pages 2567



usually a voting rule requires agents to give their preferences as linear orders however in some cases it is impractical for an agent to give a linear order over all the alternatives it has been suggested to let agents submit partial orders instead then given a voting rule a profile of partial orders and an alternative candidate c two important questions arise first is it still possible for c to win and second is c guaranteed to win these are the possible winner and necessary winner problems respectively each of these two problems is further divided into two subproblems determining whether c is a unique winner that is c is the only winner or determining whether c is a cowinner that is c is in the set of winners 

we consider the setting where the number of alternatives is unbounded and the votes are unweighted we completely characterize the complexity of possiblenecessary winner problems for the following common voting rules a class of positional scoring rules including borda copeland maximin bucklin ranked pairs voting trees and plurality with runoff





n  l zhang and  w  zhang 2001 speeding up the convergence of value iteration in partially observable markov decision processes volume 14 pages 2951



partially observable markov decision processes pomdps have    recently become popular among many ai researchers because they serve    as a natural model for planning under uncertainty  value iteration is    a wellknown algorithm for finding optimal policies for pomdps  it    typically takes a large number of iterations to converge  this paper    proposes a method for accelerating the convergence of value iteration    the method has been evaluated on an array of benchmark problems and    was found to be very effective it enabled value iteration to converge    after only a few iterations on all the test problems







d  d maua c  p de campos and m  zaffalon 2012 solving limited memory influence diagrams volume 44 pages 97140







d  bryce s  kambhampati and d  e smith 2006 planning graph heuristics for belief space search volume 26 pages 3599





r  i brafman and  m  tennenholtz 2003 learning to coordinate efficiently a modelbased approach volume 19 pages 1123



in commoninterest stochastic games all players receive an identical payoff players participating in such games must learn to coordinate with each other in order to receive the highestpossible value a number of reinforcement learning algorithms have been proposed for this problem and some have been shown to converge to good solutions in the limit in this paper we show that using very simple modelbased algorithms much better ie polynomial convergence rates can be attained moreover our modelbased algorithms are guaranteed to converge to the optimal value unlike many of the existing algorithms





s  wermter and  v  weber 1997 screen learning a flat syntactic and semantic spoken language analysis using artificial neural networks volume 6 pages 3585



previous approaches of analyzing spontaneously spoken    language often have been based on encoding syntactic and semantic    knowledge manually and symbolically while there has been some    progress using statistical or connectionist language models many    current spoken language systems still use a relatively brittle    handcoded symbolic grammar or symbolic semantic component        in contrast we describe a socalled screening approach for learning    robust processing of spontaneously spoken language  a screening    approach is a flat analysis which uses shallow sequences of category    representations for analyzing an utterance at various syntactic    semantic and dialog levels  rather than using a deeply structured    symbolic analysis we use a flat connectionist analysis  this    screening approach aims at supporting speech and language processing    by using 1 datadriven learning and 2 robustness of connectionist    networks  in order to test this approach we have developed the    screen system which is based on this new robust learned and flat    analysis        in this paper we focus on a detailed description of screens    architecture the flat syntactic and semantic analysis the    interaction with a speech recognizer and a detailed evaluation    analysis of the robustness under the influence of noisy or incomplete    input  the main result of this paper is that flat representations    allow more robust processing of spontaneous spoken language than    deeply structured representations  in particular we show how the    faulttolerance and learning capability of connectionist networks can    support a flat analysis for providing more robust spokenlanguage    processing within an overall hybrid symbolicconnectionist framework





v  bulitko n  sturtevant j  lu and t  yau 2007 graph abstraction in realtime heuristic search volume 30 pages 51100



realtime heuristic search methods are used by situated agents in applications that require the amount of planning per move to be independent of the problem size such agents plan only a few actions at a time in a local search space and avoid getting trapped in local minima by improving their heuristic function over time we extend a wide class of realtime search algorithms with automaticallybuilt state abstraction and prove completeness and convergence of the resulting family of algorithms we then analyze the impact of abstraction in an extensive empirical study in realtime pathfinding abstraction is found to improve efficiency by providing better trading offs between planning time learning speed and other negatively correlated performance measures





u  zahavi a  felner n  burch and r  c holte 2010 predicting the performance of ida using conditional distributions volume 37 pages 4183



korf reid and edelkamp introduced a formula to predict the number of nodes ida will expand on a single iteration for a given consistent heuristic and experimentally demonstrated that it could make very accurate predictions in this paper we show that in addition to requiring the heuristic to be consistent their formulas predictions are accurate only at levels of the bruteforce search tree where the heuristic values obey the unconditional distribution that they defined and then used in their formula we then propose a new formula that works well without these requirements ie it can  make accurate predictions of idas performance for inconsistent heuristics and if the heuristic values in any

level do not obey the unconditional distribution in order to achieve this we introduce the conditional distribution of heuristic values which is a generalization of their unconditional heuristic distribution we also provide extensions of our formula that handle individual start states and the augmentation of ida with bidirectional pathmax bpmx a technique for propagating heuristic values when inconsistent heuristics are used experimental results demonstrate the accuracy of our new method and all its variations





p  e dunne 2005 extremal behaviour in multiagent contract negotiation volume 23 pages 4178



we examine properties of a model of resource allocation in which several agents exchange resources in order to optimise their individual holdings the schemes discussed relate to wellknown negotiation protocols proposed in earlier work and we consider a number of alternative notions of rationality covering both quantitative measures eg cooperative and individual rationality and more qualitative forms eg pigoudalton transfers while it is known that imposing particular rationality and structural restrictions may result in some reallocations of the resource set becoming unrealisable in this paper we address the issue of the number of restricted rational deals that may be required to implement a particular reallocation when it is possible to do so we construct examples showing that this number may be exponential in the number of resources m even when all of the agent utility functions are monotonic we further show that k agents may achieve in a single deal a reallocation requiring exponentially many rational deals if at most k1 agents can participate this same reallocation being unrealisable by any sequences of rational deals in which at most k2 agents are involved





karsten  martiny and ralf  m246ller 2016 pdt logic a probabilistic doxastic temporal logic for reasoning about beliefs in multiagent systems volume 57 pages 39112



we present probabilistic doxastic temporal pdt logic a formalism to represent and reason about probabilistic beliefs and their temporal evolution in multiagent systems this formalism enables the quantification of agents beliefs through probability intervals and incorporates an explicit notion of time we discuss how over time agents dynamically change their beliefs in facts temporal rules and other agents beliefs with respect to any new information they receive we introduce an appropriate formal semantics for pdt logic and show that it is decidable alternative options of specifying problems in pdt logic are possible for these problem specifications we develop different satisfiability checking algorithms and provide complexity results for the respective decision problems the use of probability intervals enables a formal representation of probabilistic knowledge without enforcing possibly incorrect exact probability values by incorporating an explicit notion of time pdt logic provides enriched possibilities to represent and reason about temporal relations





d  dubois  h  fargier and  h  prade 2004 ordinal and probabilistic representations of acceptance volume 22 pages 2356



an accepted belief is a proposition considered likely enough by an agent to be inferred from as if it were true this paper bridges the gap between probabilistic and logical representations of accepted beliefs to this end natural properties of relations on propositions describing relative strength of belief are augmented with some conditions ensuring that accepted beliefs form a deductively closed set this requirement turns out to be very restrictive in particular it is shown that the sets of accepted belief of an agent can always be derived from a family of possibility rankings of states an agent accepts a proposition in a given context if this proposition is considered more possible than its negation in this context for all possibility rankings in the family these results are closely connected to the nonmonotonic preferential inference system of kraus lehmann and magidor and the socalled plausibility functions of friedman and halpern the extent to which probability theory is compatible with acceptance relations is laid bare a solution to the lottery paradox which is considered as a major impediment to the use of nonmonotonic inference is proposed using a special kind of probabilities called lexicographic or bigstepped the setting of acceptance relations also proposes another way of approaching the theory of belief change after the works of grdenfors and colleagues our view considers the acceptance relation as a primitive object from which belief sets are derived in various contexts





a  epshteyn and g  dejong 2006 generative prior knowledge for discriminative classification volume 27 pages 2553



we present a novel framework for integrating prior knowledge into discriminative classifiers  our framework allows discriminative classifiers such as support vector machines svms to utilize prior knowledge specified in the generative setting the dual objective of fitting the data and respecting prior knowledge is formulated as a bilevel program which is solved approximately via iterative application of secondorder cone programming  to test our approach we consider the problem of using wordnet a semantic database of english language to improve lowsample classification accuracy of newsgroup categorization  wordnet is viewed as an approximate but readily available source of background knowledge and our framework is capable of utilizing it in a flexible way





p   j gmytrasiewicz and  p  doshi 2005 a framework for sequential planning in multiagent settings volume 24 pages 4979



this paper extends the framework of partially observable markov decision processes pomdps to multiagent settings by incorporating the notion of agent models into the state space  agents maintain beliefs over physical states of the environment and over models of other agents and they use bayesian updates to maintain their beliefs over time the solutions map belief states to actions models of other agents may include their belief states and are related to agent types considered in games of incomplete information  we express the agents autonomy by postulating that their models are not directly manipulable or observable by other agents  we show that important properties of pomdps such as convergence of value iteration the rate of convergence and piecewise linearity and convexity of the value functions carry over to our framework  our approach complements a more traditional approach to interactive settings which uses nash equilibria as a solution paradigm  we seek to avoid some of the drawbacks of equilibria which may be nonunique and do not capture offequilibrium behaviors  we do so at the cost of having to represent process and continuously revise models of other agents since the agents beliefs may be arbitrarily nested the optimal solutions to decision making problems are only asymptotically computable  however approximate belief updates and approximately optimal plans are computable we illustrate our framework using a simple application domain and we show examples of belief updates and value functions





a  t cemgil and  b  kappen 2003 monte carlo methods for tempo tracking and rhythm quantization volume 18 pages 4581



we present a probabilistic generative model for timing    deviations in expressive music performance the structure of the    proposed model is equivalent to a switching state space model the    switch variables correspond to discrete note locations as in a musical    score the continuous hidden variables denote the tempo  we formulate    two well known music recognition problems namely tempo tracking and    automatic transcription rhythm quantization as filtering and maximum    a posteriori map state estimation tasks exact computation of    posterior features such as the map state is intractable in this model    class so we introduce monte carlo methods for integration and    optimization we compare markov chain monte carlo mcmc methods such    as gibbs sampling simulated annealing and iterative improvement and    sequential monte carlo methods particle filters our simulation    results suggest better results with sequential methods  the methods    can be applied in both online and batch scenarios such as tempo    tracking and transcription and are thus potentially useful in a number    of music applications such as adaptive automatic accompaniment score    typesetting and music information retrieval





t  hogg 1999 solving highly constrained search problems with quantum computers volume 10 pages 3966



a previously developed quantum search algorithm for solving    1sat problems in a single step is generalized to apply to a range    of highly constrained ksat problems we identify a bound on the    number of clauses in satisfiability problems for which the    generalized algorithm can find a solution in a constant number of    steps as the number of variables increases this performance    contrasts with the linear growth in the number of steps required by    the best classical algorithms and the exponential number required    by classical and quantum methods that ignore the problem    structure in some cases the algorithm can also guarantee that    insoluble problems in fact have no solutions unlike previously    proposed quantum search algorithms





nasrin  taghizadeh and hesham  faili 2016 automatic wordnet development for lowresource languages using crosslingual wsd volume 56 pages 6187



8206wordnets are an effective resource for natural language processing and information retrieval8206 8206especially for semantic processing and meaning related tasks8206 8206so far8206 8206wordnets have been constructed for many languages8206 8206however8206 8206the automatic development of wordnets for lowresource languages has not been well studied8206 8206in this paper8206 8206an expectationmaximization algorithm is used to create high quality and large scale wordnets for poorresource languages8206 8206the proposed method benefits from possessing crosslingual word sense disambiguation and develops a wordnet by only using a bilingual dictionary and a monolingual corpus8206 8206the proposed method has been executed with persian language and the resulting wordnet has been evaluated through several experiments8206 8206the results show that the induced wordnet has a precision score of 90 and a recall score of 358206





r  booth t  meyer i  varzinczak and r  wassermann 2011 on the link between partial meet kernel and infra contraction and its application to horn logic volume 42 pages 3153



standard belief change assumes an underlying logic containing full classical propositional logic however there are good reasons for considering belief change in less expressive logics as well in this paper we build on recent investigations by delgrande on contraction for horn logic we show that the standard basic form of contraction partial meet is too strong in the horn case this result stands in contrast to delgrandes conjecture that orderly maxichoice is the appropriate form of contraction for horn logic we then define a more appropriate notion of basic contraction for the horn case influenced by the convexity property holding for full propositional logic and which we refer to as infra contraction the main contribution of this work is a result which shows that the construction method for horn contraction for belief sets based on our infra remainder sets corresponds exactly to hanssons classical kernel contraction for belief sets when restricted to horn logic this result is obtained via a detour through contraction for belief bases we prove that kernel contraction for belief bases produces precisely the same results as the belief base version of infra contraction the use of belief bases to obtain this result provides evidence for the conjecture that horn belief change is best viewed as a hybrid version of belief set change and belief base change one of the consequences of the link with base contraction is the provision of a representation result for horn contraction for belief sets in which a version of the coreretainment postulate features





b  lubin a  i juda r  cavallo s  lahaie j  shneidman and d  c parkes 2008 ice an expressive iterative combinatorial exchange volume 33 pages 3377



we present the design and analysis of the first fully expressive iterative combinatorial exchange ice the exchange incorporates a treebased bidding language tbbl that is concise and expressive for ces bidders specify lower and upper bounds in tbbl on their value for different trades and refine these bounds across rounds these bounds allow price discovery and useful preference elicitation in early rounds and allow termination with an efficient trade despite partial information on bidder valuations all computation in the exchange is carefully optimized to exploit the structure of the bidtrees and to avoid enumerating trades a proxied interpretation of a revealedpreference activity rule coupled with simple linear prices ensures progress across rounds the exchange is fully implemented and we give results demonstrating several aspects of its scalability and economic properties with simulated bidding strategies





honorable mention for the 2003 ijcaijair best paper prize



we examine the computational complexity of testing and    finding small plans in probabilistic planning domains with both flat    and propositional representations  the complexity of plan evaluation    and existence varies with the plan type sought we examine totally    ordered plans acyclic plans and looping plans and partially ordered    plans under three natural definitions of plan value  we show that    problems of interest are complete for a variety of complexity classes    pl p np conp pp nppp conppp and pspace  in the process of    proving that certain planning problems are complete for nppp we    introduce a new basic npppcomplete problem emajsat which    generalizes the standard boolean satisfiability problem to    computations involving probabilistic quantities our results suggest    that the development of good heuristics for emajsat could be    important for the creation of efficient algorithms for a wide variety    of problems





j  m siskind 2001 grounding the lexical semantics of verbs in visual perception using force dynamics and event logic volume 15 pages 3190



this paper presents an implemented system for recognizing    the occurrence of events described by simple spatialmotion verbs in    short image sequences the semantics of these verbs is specified with    eventlogic expressions that describe changes in the state of    forcedynamic relations between the participants of the event  an    efficient finite representation is introduced for the infinite sets of    intervals that occur when describing liquid and semiliquid events    additionally an efficient procedure using this representation is    presented for inferring occurrences of compound events described with    eventlogic expressions from occurrences of primitive events  using    force dynamics and event logic to specify the lexical semantics of    events allows the system to be more robust than prior systems based on    motion profile





m  bienvenu 2009 prime implicates and prime implicants from propositional to modal logic volume 36 pages 71128



prime implicates and prime implicants have proven relevant to a number of areas of artificial intelligence most notably abductive reasoning and knowledge compilation the purpose of this paper is to examine how these notions might be appropriately extended from propositional logic to the modal logic k we begin the paper by considering a number of potential definitions of clauses and terms for k the different definitions are evaluated with respect to a set of syntactic semantic and complexitytheoretic properties characteristic of the propositional definition  we then compare the definitions with respect to the properties of the notions of prime implicates and prime implicants that they induce while there is no definition that perfectly generalizes the propositional notions we show that there does exist one definition which satisfies many of the desirable properties of the propositional case in the second half of the paper we consider the computational properties of the selected definition to this end we provide sound and complete algorithms for generating and recognizing prime implicates and we show the prime implicate recognition task to be pspacecomplete we also prove upper and lower bounds on the size and number of prime implicates while the paper focuses on the logic k all of our results hold equally well for multimodal k and for concept expressions in the description logic alc 





f  barber 2000 reasoning on interval and pointbased disjunctive metric constraints in temporal contexts volume 12 pages 3586



we introduce a temporal model for reasoning on disjunctive    metric constraints on intervals and time points in temporal    contexts this temporal model is composed of a labeled temporal    algebra and its reasoning algorithms the labeled temporal algebra    defines labeled disjunctive metric pointbased constraints where each    disjunct in each input disjunctive constraint is univocally associated    to a label reasoning algorithms manage labeled constraints    associated label lists and sets of mutually inconsistent    disjuncts these algorithms guarantee consistency and obtain a minimal    network additionally constraints can be organized in a hierarchy of    alternative temporal contexts therefore we can reason on    contextdependent disjunctive metric constraints on intervals and    points moreover the model is able to represent nonbinary    constraints such that logical dependencies on disjuncts in    constraints can be handled  the computational cost of reasoning    algorithms is exponential in accordance with the underlying problem    complexity although some improvements are proposed





m  aramon bajestani and j c  beck 2013 scheduling a dynamic aircraft repair shop with limited repair resources volume 47 pages 3570



we address a dynamic repair shop scheduling problem in the context of military aircraft fleet management where the goal is to maintain a full complement of aircraft over the longterm a number of flights each with a requirement for a specific number and type of aircraft are already scheduled over a long horizon we need to assign aircraft to flights and schedule repair activities while considering the flights requirements repair capacity and aircraft failures the number of aircraft awaiting repair dynamically changes over time due to failures and it is therefore necessary to rebuild the repair schedule online to solve the problem we view the dynamic repair shop as successive static repair scheduling subproblems over shorter time periods we propose a complete approach based on the logicbased benders decomposition to solve the static subproblems and design different rescheduling policies to schedule the dynamic repair shop computational experiments demonstrate that the benders model is able to find and prove optimal solutions on average four times faster than a mixed integer programming model the rescheduling approach having both aspects of scheduling over a longer horizon and quickly adjusting the schedule increases aircraft available in the long term by 10 compared to the approaches having either one of the aspects alone





v237ctor m  s225nchezcartagena juan antonio  p233rezortiz and felipe  s225nchezmart237nez 2016 integrating rules and dictionaries from shallowtransfer machine translation into phrasebased statistical machine translation volume 55 pages 1761



we describe a hybridisation strategy whose objective is to integrate linguistic resources from shallowtransfer rulebased machine translation rbmt into phrasebased statistical machine translation pbsmt it basically consists of enriching the phrase table of a pbsmt system with bilingual phrase pairs matching transfer rules and dictionary entries from a shallowtransfer rbmt system this new strategy takes advantage of how the linguistic resources are used by the rbmt system to segment the sourcelanguage sentences to be translated and overcomes the limitations of existing hybrid approaches that treat the rbmt systems as a black box experimental results confirm that our approach delivers translations of  higher quality than existing ones and that it is specially useful when the parallel corpus available for training the smt system is small or when translating outofdomain texts that are well covered by the rbmt dictionaries a combination of this approach with a recently proposed unsupervised shallowtransfer rule inference algorithm results in a significantly greater translation quality than that of a baseline pbsmt in this case the only handcrafted resource used are the dictionaries commonly used in rbmt moreover the translation quality achieved by the hybrid system built with automatically inferred rules is similar to that obtained by those built with handcrafted rules





carmel  domshlak and vitaly  mirkis 2015 deterministic oversubscription planning as heuristic search abstractions and reformulations volume 52 pages 97169



while in classical planning the objective is to achieve one of the equally attractive goal states at as low total action cost as possible the objective in deterministic oversubscription planning osp is to achieve an as valuable as possible subset of goals within a fixed allowance of the total action cost although numerous applications in various fields share the latter objective no substantial algorithmic advances have been made in deterministic osp tracing the key sources of progress in classical planning we identify a severe lack of effective domainindependent approximations for osp 

with our focus here on optimal planning our goal is to bridge this gap two classes of approximation techniques have been found especially useful in the context of optimal classical planning those based on statespace  abstractions and those based on logical landmarks for goal reachability the question we  study here is whether some similarinspirit yet possibly mathematically  different approximation techniques can be developed for osp in the context of abstractions we define the notion of additive abstractions for osp study the complexity of deriving effective abstractions from a rich space of hypotheses and reveal some  substantial empirically relevant islands of tractability in the context of  landmarks we show how standard goalreachability landmarks of certain classical planning tasks  can be compiled into the osp task of interest resulting in an equivalent osp task with a lower cost allowance and thus with a smaller search space  our empirical evaluation confirms the effectiveness of the proposed techniques and opens a wide gate for further developments in oversubscription planning  





p  everaere s  konieczny and p  marquis 2007 the strategyproofness landscape of merging volume 28 pages 49105



merging operators aim at defining the beliefsgoals of a group of agents from the beliefsgoals of each member of the group whenever an agent of the group has preferences over the possible results of the merging process ie the possible merged bases she can try to rig the merging process by lying on her true beliefsgoals if this leads to better merged base according to her point of view obviously strategyproof operators are highly desirable in order to guarantee equity among agents even when some of them are not sincere in this paper we draw the strategyproof landscape for many merging operators from the literature including modelbased ones and formulabased ones both the general case and several restrictions on the merging process are considered





n  meuleau e  benazera r  i brafman e  a hansen and   mausam 2009 a heuristic search approach to planning with continuous resources in stochastic domains volume 34 pages 2759



we consider the problem of optimal planning in stochastic domains with resource constraints where the resources are continuous and the choice of action at each step depends on resource availability  we introduce the hao algorithm a generalization of the ao algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables where the continuous variables represent monotonic resources  like other heuristic search algorithms hao leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy  we show that this approach is especially effective when resource constraints limit how much of the state space is reachable  experimental results demonstrate its effectiveness in the domain that motivates our research automated planning for planetary exploration rovers







d  a cohen m  c cooper p  creed d  marx and a  z salamon 2012 the tractability of csp classes defined by forbidden patterns volume 45 pages 4778





c  yuan and b  malone 2013 learning optimal bayesian networks a shortest path perspective volume 48 pages 2365



in this paper learning a bayesian network structure that optimizes a scoring function for a given dataset is viewed as a shortest path problem in an implicit statespace search graph this perspective highlights the importance of two research issues the development of search strategies for solving the shortest path problem and the design of heuristic functions for guiding the search this paper introduces several techniques for addressing the issues one is an a search algorithm that learns an optimal bayesian network structure by only searching the most promising part of the solution space the others are mainly two heuristic functions the first heuristic function represents a simple relaxation of the acyclicity constraint of a bayesian network although admissible and consistent the heuristic may introduce too much relaxation and result in a loose bound the second heuristic function reduces the amount of relaxation by avoiding directed cycles within some groups of variables empirical results show that these methods constitute a promising approach to learning optimal bayesian network structures





mausam and d  s  weld 2008 planning with durative actions in stochastic domains volume 31 pages 3382



probabilistic planning problems are typically modeled as a markov decision process mdp mdps while an otherwise expressive model allow only for sequential nondurative actions this poses severe restrictions in modeling and solving a real world planning problem we extend the mdp model to incorporate  1 simultaneous action execution 2 durative actions and 3 stochastic durations we develop several algorithms to combat the computational explosion introduced by these features the key theoretical ideas used in building these algorithms are  modeling a complex problem as an mdp in extended stateaction space pruning of irrelevant actions sampling of relevant actions using informed heuristics to guide the search hybridizing different planners to achieve benefits of both approximating the problem and replanning our empirical evaluation illuminates the different merits in using various algorithms viz optimality empirical closeness to optimality theoretical error bounds and speed





krishna  s r dubba anthony  g cohn david  c hogg mehul   bhatt and frank  dylla 2015 learning relational event models from video volume 53 pages 4190



event models obtained automatically from video can be used in applications ranging from abnormal event detection to content based video retrieval when multiple agents are involved in the events characterizing events naturally suggests encoding interactions as relations learning event models from this kind of relational spatiotemporal data using relational learning techniques such as inductive logic programming ilp hold promise but have not been successfully applied to very large datasets which result from video data in this paper we present a novel framework remind relational event model induction for supervised relational learning of event models from large video datasets using ilp efficiency is achieved through the learning from interpretations setting and using a typing system that exploits the type hierarchy of objects in a domain the use of types also helps prevent over generalization furthermore we also present a typerefining operator and prove that it is optimal the learned models can be used for recognizing events from previously unseen videos we also present an extension to the framework by integrating an abduction step that improves the learning performance when there is noise in the input data the experimental results on several hours of video data from two challenging real world domains an airport domain and a physical action verbs domain suggest that the techniques are suitable to real world scenarios





t  drakengren and p  jonsson 1997 eight maximal tractable subclasses of allens algebra with metric time volume 7 pages 2545



this paper combines two important directions of research in temporal resoning that of finding maximal tractable subclasses of allens interval algebra and that of reasoning with metric temporal information eight new maximal tractable subclasses of allens interval algebra are presented some of them subsuming previously reported tractable algebras the algebras allow for metric temporal constraints on interval starting or ending points using the recent framework of horn dlrs two of the algebras can express the notion of sequentiality between intervals being the first such algebras admitting both qualitative and metric time





n  fu hc  lau p  varakantham and f  xiao 2012 robust local search for solving rcpspmax with durational uncertainty volume 43 pages 4386



scheduling problems in manufacturing logistics and project management have frequently been modeled using the framework of resource constrained project scheduling problems with minimum and maximum time lags rcpspmax due to the importance of these problems providing scalable solution schedules for rcpspmax problems is a topic of extensive research however all existing methods for solving rcpspmax assume that durations of activities are known with certainty an assumption that does not hold in real world scheduling problems where unexpected external events such as manpower availability weather changes etc lead to delays or advances in completion of activities thus in this paper our focus is on providing a scalable method for solving rcpspmax problems with durational uncertainty to that end we introduce the robust local search method consisting of three key ideas a introducing and studying the properties of two decision rule approximations used to compute start times of activities with respect to dynamic realizations of the durational uncertainty b deriving the expression for robust makespan of an execution strategy based on decision rule approximations and c a robust local search mechanism to efficiently compute activity execution strategies that are robust against durational uncertainty furthermore we also provide enhancements to local search that exploit temporal dependencies between activities our experimental results illustrate that robust local search is able to provide robust execution strategies efficiently





m  katz and c  domshlak 2010 implicit abstraction heuristics volume 39 pages 51126



statespace search with explicit abstraction heuristics is at the state of the art of costoptimal planning these heuristics are inherently limited nonetheless because the size of the abstract space must be bounded by some even if a very large constant targeting this shortcoming we introduce the notion of iadditive implicit abstractionsi in which the planning task is abstracted by instances of tractable fragments of optimal planning we then introduce a concrete setting of this framework called iforkdecompositioni that is based on two novel fragments of tractable costoptimal planning the induced admissible heuristics are then studied formally and empirically this  study testifies for the accuracy of the fork decomposition heuristics yet our empirical evaluation also stresses the tradeoff between their accuracy and the runtime complexity of computing them indeed some of the power of the explicit abstraction heuristics comes from precomputing the heuristic function offline and then determining ihsi for each evaluated state isi by a very fast lookup in a database by contrast while forkdecomposition heuristics can be calculated in polynomial time  computing them is far from being fast to address this problem we show that the timepernode complexity bottleneck of the forkdecomposition heuristics  can be successfully overcome we demonstrate that an equivalent of the explicit abstraction notion of a database exists for the forkdecomposition abstractions as well despite their exponentialsize abstract spaces we then verify empirically that heuristic search with the databased forkdecomposition heuristics favorably competes with the state of the art of costoptimal planning





l  climent r  j wallace m  a salido and f  barber 2014 robustness and stability in constraint programming under dynamism and uncertainty volume 49 pages 4978



many real life problems that can be solved by constraint programming come from uncertain and dynamic environments because of the dynamism the original problem may change over time and thus the solution found for the original problem may become invalid for this reason dealing with such problems has become an important issue in the fields of constraint programming in some cases there exist extant knowledge about the uncertain and dynamic environment in other cases this information is fragmentary or unknown in this paper we extend the concept of robustness and stability for constraint satisfaction problems csps with ordered domains where only limited assumptions need to be made as to possible changes we present a search algorithm that searches for both robust and stable solutions for csps of this nature it is wellknown that meeting both criteria simultaneously is a desirable objective for constraint solving in uncertain and dynamic environments we also present compelling evidence that our search algorithm outperforms other generalpurpose algorithms for dynamic csps using random instances and benchmarks derived from real life problems





m  fox and  d  long 2003 pddl21 an extension to pddl for expressing temporal planning domains volume 20 pages 61124



in recent years research in the planning community has moved increasingly toward s application of planners to realistic problems involving both time and many typ es of resources for example interest in planning demonstrated by the space res earch community has inspired work in observation scheduling planetary rover ex ploration and spacecraft control domains other temporal and resourceintensive domains including logistics planning plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application   the international planning competitions have acted as an important motivating fo rce behind the progress that has been made in planning since 1998 the third com petition held in 2002 set the planning community the challenge of handling tim e and numeric resources this necessitated the development of a modelling langua ge capable of expressing temporal and numeric properties of planning domains in this paper we describe the language pddl21 that was used in the competition  we describe the syntax of the language its formal semantics and the validation of concurrent plans we observe that pddl21 has considerable modelling power  exceeding the capabilities of current planning technology  and presents a number of important challenges to the research community





m  p wellman  d  m reeves  k  m lochner and  y  vorobeychik 2004 price prediction in a trading agent competition volume 21 pages 1936



the 2002 trading agent competition tac presented a challenging market game in the domain of travel shopping  one of the pivotal issues in this domain is uncertainty about hotel prices which have a significant influence on the relative cost of alternative trip schedules  thus virtually all participants employ some method for predicting hotel prices  we survey approaches employed in the tournament finding that agents apply an interesting diversity of techniques taking into account differing sources of evidence bearing on prices  based on data provided by entrants on their agents actual predictions in the tac02 finals and semifinals we analyze the relative efficacy of these approaches  the results show that taking into account gamespecific information about flight prices is a major distinguishing factor  machine learning methods effectively induce the relationship between flight and hotel prices from game data and a purely analytical approach based on competitive equilibrium analysis achieves equal accuracy with no historical data  employing a new measure of prediction quality we relate absolute accuracy to bottomline performance in the game





c  cayrol f  dupin de saintcyr and m  lagasquieschiex 2010 change in abstract argumentation frameworks adding an argument volume 38 pages 4984



in this paper we address the problem of change in an abstract argumentation system we focus on a particular change the addition of  a new argument which interacts with previous arguments we study the impact of such an addition on the outcome of the argumentation system more particularly on the set of its extensions several properties for this change operation are defined by comparing the new set of extensions to the initial one these properties are called structural when the comparisons are based on setcardinality or setinclusion relations several other properties are proposed where comparisons are based on the status of some particular arguments the accepted arguments these properties refer to the evolution of this status during the change eg monotony and priority to recency all these  properties may be more or less desirable according to specific applications they are studied under two particular semantics the grounded and preferred semantics





m  hauskrecht 2000 valuefunction approximations for partially observable markov decision processes volume 13 pages 3394



partially observable markov decision processes pomdps    provide an elegant mathematical framework for modeling complex    decision and planning problems in stochastic domains in which states    of the system are observable only indirectly via a set of imperfect    or noisy observations the modeling advantage of pomdps however    comes at a price  exact methods for solving them are computationally    very expensive and thus applicable in practice only to very simple    problems we focus on efficient approximation heuristic methods that    attempt to alleviate the computational problem and trade off accuracy    for speed we have two objectives here first we survey various    approximation methods analyze their properties and relations and    provide some new insights into their differences second we present a    number of new approximation methods and novel refinements of existing    techniques the theoretical results are supported by experiments on a    problem from the agent navigation domain







n  nisan and a  ronen 2007 computationally feasible vcg mechanisms volume 29 pages 1947





e  huang and r  e korf 2013 optimal rectangle packing an absolute placement approach volume 46 pages 4787



we consider the problem of finding all enclosing rectangles of minimum area that can contain a given set of rectangles without overlap  our rectangle packer chooses the xcoordinates of all the rectangles before any of the ycoordinates we then transform the problem into a perfectpacking problem with no empty space by adding additional rectangles to determine the ycoordinates we branch on the different rectangles that can be placed in each empty position our packer allows us to extend the known solutions for a consecutivesquare benchmark from 27 to 32 squares we also introduce three new benchmarks avoiding properties that make a benchmark easy such as rectangles with shared dimensions our third benchmark consists of rectangles of increasingly high precision to pack them efficiently we limit the rectangles coordinates and the bounding box dimensions to the set of subset sums of the rectangles dimensions overall our algorithms represent the current stateoftheart for this problem outperforming other algorithms by orders of magnitude depending on the benchmark





j  hoffmann p  bertoli m  helmert and m  pistore 2009 messagebased web service composition integrity constraints and planning under uncertainty a new connection volume 35 pages 49117



thanks to recent advances ai planning has become the underlying technique for several applications figuring prominently among these is automated web service composition wsc at the capability level where services are described in terms of preconditions and effects over ontological concepts a key issue in addressing wsc as planning is that ontologies are not only formal vocabularies they also axiomatize the possible relationships between concepts such axioms correspond to what has been termed integrity constraints in the actions and change literature and applying a web service is essentially a belief update operation the reasoning required for belief update is known to be harder than reasoning in the ontology itself the support for belief update is severely limited in current planning tools





sigve  hortemo s230ther jan arne  telle and martin   vatshelle 2015 solving sat and maxsat by dynamic programming  volume 54 pages 5982



we look at dynamic programming algorithms for propositional model counting also called sat and maxsat tools from graph structure theory in particular treewidth have been used to successfully identify tractable cases in many subfields of ai including sat constraint satisfaction problems csp bayesian reasoning and planning in this paper we attack sat and maxsat using similar but more modern graph structure tools the tractable cases will include formulas whose class of incidence graphs have not only unbounded treewidth but also unbounded cliquewidth we show that our algorithms extend all previous results for maxsat and sat achieved by dynamic programming along structural decompositions of the incidence graph of the input formula we present some limited experimental results comparing implementations of our algorithms to stateoftheart sat and maxsat solvers as a proof of concept that warrants further research





s  thiebaux c  gretton j  slaney d  price and f  kabanza 2006 decisiontheoretic planning with nonmarkovian rewards volume 25 pages 1774



a decision process in which rewards depend on history rather than merely on the current state is called a decision process with nonmarkovian rewards nmrdp in decisiontheoretic planning where many desirable behaviours are more naturally expressed as properties of execution sequences rather than as properties of states nmrdps form a more natural model than the commonly adopted fully markovian decision process mdp model while the more tractable solution methods developed for mdps do not directly apply in the presence of nonmarkovian rewards a number of solution methods for nmrdps have been proposed in the literature these all exploit a compact specification of the nonmarkovian reward function in temporal logic to automatically translate the nmrdp into an equivalent mdp which is solved using efficient mdp solution methods this paper presents nmrdpp nonmarkovian reward decision process planner a software platform for the development and experimentation of methods for decisiontheoretic planning with nonmarkovian rewards the current version of nmrdpp implements under a single interface a family of methods based on existing as well as new approaches which we describe in detail these include dynamic programming heuristic search and structured methods using nmrdpp we compare the methods and identify certain problem features that affect their performance nmrdpps treatment of nonmarkovian rewards is inspired by the treatment of domainspecific search control knowledge in the tlplan planner which it incorporates as a special case in the first international probabilistic planning competition nmrdpp was able to compete and perform well in both the domainindependent and handcoded tracks using search control knowledge in the latter 





2012 ifaamas award for influential papers in autonomous agents and multiagent systems



market price systems constitute a wellunderstood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead  in a imarketoriented programmingi approach to distributed problem solving we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy  walras provides basic constructs for defining computational market structures and protocols for deriving their corresponding price equilibria  in a particular realization of this approach for a form of multicommodity flow problem we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation and that the behavior of the system can be meaningfully analyzed in economic terms





p  resnik 1999 semantic similarity in a taxonomy an informationbased measure and its application to problems of ambiguity in natural language volume 11 pages 95130



this article presents a measure of semantic similarity in an    isa taxonomy based on the notion of shared information content    experimental evaluation against a benchmark set of human similarity    judgments demonstrates that the measure performs better than the    traditional edgecounting approach  the article presents algorithms    that take advantage of taxonomic similarity in resolving syntactic and    semantic ambiguity along with experimental results demonstrating    their effectiveness





a  j grove  j  y halpern and  d  koller 1994 random worlds and maximum entropy volume 2 pages 3388



given a knowledge base ikbi containing firstorder and statistical facts we consider a principled method called the irandomworlds methodi for computing a degree of belief that some formula iphii holds given ikbi  if  we are reasoning about a world or system consisting of ini individuals then we can consider all possible worlds or firstorder models withdomain i1ni that satisfy ikbi and compute thefraction of them in which iphii is true we define the degree of belief to be the asymptotic value of this fraction as ini grows large  we show that when the vocabulary underlying iphii andikbi uses constants and unary predicates only we can naturally associate an ientropyi with each world as ini grows largerthere are many more worlds with higher entropy  therefore we can usea i maximumentropyi computation to compute the degree of belief this result is in a similar spirit to previous work in physics and artificial intelligence but is far more general  of equal interest to the result itself are the limitations on its scope  most importantly the restriction to unary predicates seems necessary although the randomworlds method makes sense in general the connection to maximum entropy seems to disappear in the nonunary case  these observations suggest unexpected limitations to the applicability of maximumentropy methods





c  drummond 2002 accelerating reinforcement learning by composing solutions of automatically identified subtasks volume 16 pages 59104



this paper discusses a system that accelerates reinforcement    learning by using transfer from related tasks  without such    transfer even if two tasks are very similar at some abstract level    an extensive relearning effort is required  the system achieves    much of its power by transferring parts of previously learned    solutions rather than a single complete solution the system    exploits strong features in the multidimensional function produced    by reinforcement learning in solving a particular task these    features are stable and easy to recognize early in the learning    process they generate a partitioning of the state space and thus    the function  the partition is represented as a graph  this is    used to index and compose functions stored in a case base to form a    close approximation to the solution of the new task  experiments    demonstrate that function composition often produces more than an    order of magnitude increase in learning rate compared to a basic    reinforcement learning algorithm





r  barzilay and  n  elhadad 2002 inferring strategies for sentence ordering in multidocument news summarization volume 17 pages 3555



the problem of organizing information for multidocument    summarization so that the generated summary is coherent has received    relatively little attention  while sentence ordering for single    document summarization can be determined from the ordering of    sentences in the input article this is not the case for multidocument    summarization where summary sentences may be drawn from different    input articles in this paper we propose a methodology for studying    the properties of ordering information in the news genre and describe    experiments done on a corpus of multiple acceptable orderings we    developed for the task based on these experiments we implemented a    strategy for ordering information that combines constraints from    chronological order of events and topical relatedness  evaluation of    our augmented algorithm shows a significant improvement of the    ordering over two baseline strategies





y  zhou and y  zhang 2011 a logical study of partial entailment volume 40 pages 2556



we introduce a novel logical notionpartial entailmentto propositional logic in contrast with classical entailment that a formula p partially entails another formula q with respect to a background formula set gamma intuitively means that under the circumstance of gamma if p is true then some part of q will also be true we distinguish three different kinds of partial entailments and formalize them by using an extended notion of prime implicant we study their semantic properties which show that surprisingly partial entailments fail for many simple inference rules then we study the related computational properties which indicate that partial entailments are relatively difficult to be computed finally we consider a potential application of partial entailments in reasoning about rational agents





m  veloso and  p  stone 1995 flecs planning with a flexible commitment strategy volume 3 pages 2552



there has been evidence that leastcommitment planners can    efficiently handle planning problems that involve difficult goal    interactions  this evidence has led to the common belief that    delayedcommitment is the best possible planning strategy  however    we recently found evidence that eagercommitment planners can handle a    variety of planning problems more efficiently in particular those    with difficult operator choices  resigned to the futility of trying    to find a universally successful planning strategy we devised a    planner that can be used to study which domains and problems are best    for which planning strategies in this article we introduce this new    planning algorithm flecs which uses a flexible commitment strategy    with respect to planstep orderings  it is able to use any strategy    from delayedcommitment to eagercommitment  the combination of    delayed and eager operatorordering commitments allows flecs to take    advantage of the benefits of explicitly using a simulated execution    state and reasoning about planning constraints  flecs can vary its    commitment strategy across different problems and domains and also    during the course of a single planning problem  flecs represents a    novel contribution to planning in that it explicitly provides the    choice of which commitment strategy to use while planning  flecs    provides a framework to investigate the mapping from planning domains    and problems to efficient planning strategies





m  winikoff and s  cranefield 2014 on the testability of bdi agent systems volume 51 pages 71131



before deploying a software system we need to assure ourselves and stakeholders that the system will behave correctly this assurance is usually done by testing the system however it is intuitively obvious that adaptive systems including agentbased systems  can exhibit complex behaviour and are thus harder to test in this paper we examine this obvious intuition in the case of beliefdesireintention bdi agents  we analyse the size of the behaviour space of bdi agents  and show that although the intuition is correct the factors that influence the size are not what we expected them to be specifically we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected we also discuss the implications of these findings on the testability of bdi agents





r  beneliyahu 1996 a hierarchy of tractable subsets for computing stable models volume 5 pages 2752



finding the stable models of a knowledge base is a    significant computational problem in artificial intelligence this    task is at the computational heart of truth maintenance systems    autoepistemic logic and default logic  unfortunately it is nphard    in this paper we present a hierarchy of classes of knowledge bases    omega1omega2 with the following properties first omega1 is    the class of all stratified knowledge bases second if a knowledge    base pi is in omegak then pi has at most k stable models and all of    them may be found in time olnk where l is the length of the    knowledge base and n the number of atoms in pi third for an    arbitrary knowledge base pi we can find the minimum k such that pi    belongs to omegak in time polynomial in the size of pi and last    where k is the class of all knowledge bases it is the case that    unioni1 to infty omegai  k that is every knowledge base    belongs to some class in the hierarchy





y  wang y  zhang y  zhou and m  zhang 2014 knowledge forgetting in answer set programming volume 50 pages 3170



the ability of discarding or hiding irrelevant information has been

recognized as an important feature for knowledge based systems including answer set programming the notion of strong equivalence in answer set programming plays an important role for different problems as it gives rise to a substitution principle and amounts to knowledge equivalence of logic programs in this paper we uniformly propose a semantic knowledge forgetting  called ht and flpforgetting for logic programs under stable model and flpstable model semantics respectively our proposed knowledge forgetting discards exactly the knowledge of a logic program which is relevant to forgotten variables thus it preserves strong equivalence in the sense that strongly equivalent logic programs will remain strongly equivalent after forgetting the same variables we show that this semantic forgetting result is always expressible and we prove a representation theorem stating that the ht and flpforgetting can be precisely characterized by zhangzhous four forgetting postulates under the ht and flpmodel semantics respectively we also reveal underlying connections between the proposed forgetting and the forgetting of propositional logic and provide complexity results for decision problems in relation to the forgetting an application of the proposed forgetting is also considered in a conflict solving scenario





a  analyti g  antoniou c  v damasio and g  wagner 2008 extended rdf as a semantic foundation of rule markup languages volume 32 pages 3794



ontologies and automated reasoning are the building blocks of the semantic web initiative derivation rules can be included in an ontology to define derived concepts based on base concepts for example rules allow to define the extension of a class or property based on a complex relation between the extensions of the same or other classes and properties on the other hand the inclusion of negative information both in the form of negationasfailure and explicit negative information is also needed to enable various forms of reasoning in this paper we extend rdf graphs with weak and strong negation as well as derivation rules the erdf stable model semantics of the extended framework extended rdf is defined extending rdfs semantics a distinctive feature of our theory which is based on partial logic is that both truth and falsity extensions of properties and classes are considered allowing for truth value gaps our framework supports both closedworld and openworld reasoning through the explicit representation of the particular closedworld assumptions and the erdf ontological categories of total properties and total classes





g  gogic  c  h papadimitriou and  m  sideri 1998 incremental recompilation of knowledge volume 8 pages 2337



approximating a general formula from above and below by horn    formulas its horn envelope and horn core respectively was proposed    by selman and kautz 1991 1996 as a form of knowledge    compilation supporting rapid approximate reasoning on the negative    side this scheme is static in that it supports no updates and has    certain complexity drawbacks pointed out by kavvadias papadimitriou    and sideri 1993  on the other hand the many frameworks and schemes    proposed in the literature for theory update and revision are plagued    by serious complexitytheoretic impediments even in the horn case as    was pointed out by eiter and gottlob 1992 and is further    demonstrated in the present paper  more fundamentally these schemes    are not inductive in that they may lose in a single update any    positive properties of the represented sets of formulas small size    horn structure etc  in this paper we propose a new scheme    incremental recompilation which combines horn approximation and    modelbased updates this scheme is inductive and very efficient free    of the problems facing its constituents  a set of formulas is    represented by an upper and lower horn approximation  to update we    replace the upper horn formula by the horn envelope of its    minimumchange update and similarly the lower one by the horn core of    its update the key fact which enables this scheme is that horn    envelopes and cores are easy to compute when the underlying formula is    the result of a minimumchange update of a horn formula by a clause    we conjecture that efficient algorithms are possible for more complex    updates





g  brewka 1996 wellfounded semantics for extended logic programs with dynamic preferences volume 4 pages 1936



the paper describes an extension of wellfounded    semantics for logic programs with two types of negation in this    extension information about preferences between rules can be expressed    in the logical language and derived dynamically this is achieved by    using a reserved predicate symbol and a naming technique conflicts    among rules are resolved whenever possible on the basis of derived    preference information the wellfounded conclusions of prioritized    logic programs can be computed in polynomial time a legal reasoning    example illustrates the usefulness of the approach





l  xia and v  conitzer 2011 determining possible and necessary winners given partial orders volume 41 pages 2567



usually a voting rule requires agents to give their preferences as linear orders however in some cases it is impractical for an agent to give a linear order over all the alternatives it has been suggested to let agents submit partial orders instead then given a voting rule a profile of partial orders and an alternative candidate c two important questions arise first is it still possible for c to win and second is c guaranteed to win these are the possible winner and necessary winner problems respectively each of these two problems is further divided into two subproblems determining whether c is a unique winner that is c is the only winner or determining whether c is a cowinner that is c is in the set of winners 

we consider the setting where the number of alternatives is unbounded and the votes are unweighted we completely characterize the complexity of possiblenecessary winner problems for the following common voting rules a class of positional scoring rules including borda copeland maximin bucklin ranked pairs voting trees and plurality with runoff





n  l zhang and  w  zhang 2001 speeding up the convergence of value iteration in partially observable markov decision processes volume 14 pages 2951



partially observable markov decision processes pomdps have    recently become popular among many ai researchers because they serve    as a natural model for planning under uncertainty  value iteration is    a wellknown algorithm for finding optimal policies for pomdps  it    typically takes a large number of iterations to converge  this paper    proposes a method for accelerating the convergence of value iteration    the method has been evaluated on an array of benchmark problems and    was found to be very effective it enabled value iteration to converge    after only a few iterations on all the test problems







d  d maua c  p de campos and m  zaffalon 2012 solving limited memory influence diagrams volume 44 pages 97140







d  bryce s  kambhampati and d  e smith 2006 planning graph heuristics for belief space search volume 26 pages 3599

