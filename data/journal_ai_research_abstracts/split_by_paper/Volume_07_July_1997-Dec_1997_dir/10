d  monderer and  m  tennenholtz 1997 dynamic nonbayesian decision making volume 7 pages 231248

the model of a nonbayesian agent who faces a repeated game    with incomplete information against nature is an appropriate tool for    modeling general agentenvironment interactions  in such a model the    environment state controlled by nature may change arbitrarily and    the feedbackreward function is initially unknown the agent is not    bayesian that is he does not form a prior probability neither on the    state selection strategy of nature nor on his reward function  a    policy for the agent is a function which assigns an action to every    history of observations and actions  two basic feedback structures    are considered in one of them  the perfect monitoring case  the    agent is able to observe the previous environment state as part of his    feedback while in the other  the imperfect monitoring case  all    that is available to the agent is the reward obtained both of these    settings refer to partially observable processes where the current    environment state is unknown  our main result refers to the    competitive ratio criterion in the perfect monitoring case we prove    the existence of an efficient stochastic policy that ensures that the    competitive ratio is obtained at almost all stages with an arbitrarily    high probability where efficiency is measured in terms of rate of    convergence  it is further shown that such an optimal policy does not    exist in the imperfect monitoring case moreover it is proved that in    the perfect monitoring case there does not exist a deterministic    policy that satisfies our long run optimality criterion  in addition    we discuss the maxmin criterion and prove that a deterministic    efficient optimal strategy does exist in the imperfect monitoring case    under this criterion finally we show that our approach to longrun    optimality can be viewed as qualitative which distinguishes it from    previous work in this area

