



m  leisink and  b  kappen 2003 bound propagation volume 19 pages 139154



in this article we present an algorithm to compute bounds on the marginals of a graphical model  for several small clusters of nodes upper and lower bounds on the marginal values are computed independently of the rest of the network  the range of allowed probability distributions over the surrounding nodes is restricted using earlier computed bounds as we will show this can be considered as a set of constraints in a linear programming problem of which the objective function is the marginal probability of the center nodes  in this way knowledge about the maginals of neighbouring clusters is passed to other clusters thereby tightening the bounds on their marginals  we show that sharp bounds can be obtained for undirected and directed graphs that are used for practical applications but for which exact computations are infeasible





a  darwiche and  g  provan 1997 query dags a practical paradigm for implementing beliefnetwork inference volume 6 pages 147176



we describe a new paradigm for implementing inference in    belief networks which consists of two steps 1 compiling a belief    network into an arithmetic expression called a query dag qdag and    2 answering queries using a simple evaluation algorithm each node    of a qdag represents a numeric operation a number or a symbol for    evidence  each leaf node of a qdag represents the answer to a    network query that is the probability of some event of interest it    appears that qdags can be generated using any of the standard    algorithms for exact inference in belief networks we show how they    can be generated using clustering and conditioning algorithms the    time and space complexity of a qdag generation algorithm is no worse    than the time complexity of the inference algorithm on which it is    based the complexity of a qdag evaluation algorithm is linear in the    size of the qdag and such inference amounts to a standard evaluation    of the arithmetic expression it represents the intended value of    qdags is in reducing the software and hardware resources required to    utilize belief networks in online realworld applications the    proposed framework also facilitates the development of online    inference on different software and hardware platforms due to the    simplicity of the qdag evaluation algorithm interestingly enough    qdags were found to serve other purposes simple techniques for    reducing qdags tend to subsume relatively complex optimization    techniques for beliefnetwork inference such as networkpruning and    computationcaching



in this paper we present and evaluate a general framework for the design of truthful auctions for matching agents in a dynamic twosided market a single commodity such as a resource or a task is bought and sold by multiple buyers and sellers that arrive and depart over time our algorithm chain provides the first framework that allows a truthful dynamic double auction da to be constructed from a truthful singleperiod  ie static doubleauction rule the pricing and matching method of the chain construction is unique amongst dynamicauction rules that adopt the same building block  we examine experimentally the allocative efficiency of chain when instantiated on various singleperiod rules including the canonical mcafee doubleauction rule for a baseline we also consider nontruthful double auctions populated with zerointelligence plusstyle learning agents chainbased auctions perform well in comparison with other schemes especially as arrival intensity falls and agent valuations become more volatile 









p  d turney and p  pantel 2010 from frequency to meaning vector space models of semantics volume 37 pages 141188



computers understand very little of the meaning of human language this profoundly limits our ability to give instructions to computers the ability of computers to explain their actions to us and the ability of computers to analyse and process text vector space models vsms of semantics are beginning to address these limits this paper surveys the use of vsms for semantic processing of text we organize the literature on vsms according to the structure of the matrix in a vsm there are currently three broad classes of vsms based on termdocument wordcontext and pairpattern matrices yielding three classes of applications we survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category our goal in this survey is to show the breadth of applications of vsms for semantics to provide a new perspective on vsms for those who are already familiar with the area and to provide pointers into the literature for those who are less familiar with the field





d  gabelaia  r  kontchakov  a  kurucz  f  wolter and  m  zakharyaschev 2005 combining spatial and temporal logics expressiveness vs complexity volume 23 pages 167243



in this paper we construct and investigate a hierarchy of spatiotemporal formalisms that result from various combinations of propositional spatial and temporal logics such as the propositional temporal logic ptl the spatial logics rcc8 brcc8 s4u and their fragments the obtained results give a clear picture of the tradeoff between expressiveness and computational realisability within the hierarchy we demonstrate how different combining principles as well as spatial and temporal primitives can produce np pspace expspace 2expspacecomplete and even undecidable spatiotemporal logics out of components that are at most np or pspacecomplete





simone  parisi matteo  pirotta and marcello  restelli 2016 multiobjective reinforcement learning through continuous pareto manifold approximation volume 57 pages 187227



many realworld control applications from economics to robotics are characterized by the presence of multiple conflicting objectives in these problems the standard concept of optimality is replaced by paretooptimality and the goal is to find the pareto frontier a set of solutions representing different compromises among the objectives despite recent advances in multiobjective optimization achieving an accurate representation of the pareto frontier is still an important challenge  in this paper we propose a reinforcement learning policy gradient approach to learn a continuous approximation of the pareto   frontier in multiobjective markov decision problems momdps differently from previous policy gradient algorithms where n optimization routines are executed to have n solutions our approach performs a single gradient ascent run generating at each step an improved continuous approximation of the pareto frontier the idea is   to optimize the parameters of a function defining a manifold in the policy parameters space so that the corresponding image in the objectives space gets as close as possible to the true pareto frontier besides deriving how to compute and estimate such gradient we will also discuss the nontrivial issue of defining a metric to assess the quality of the candidate pareto frontiers finally the properties of the proposed approach are empirically evaluated on two problems a linearquadratic gaussian regulator and a water reservoir control task





p  derbeko r  elyaniv and r  meir 2004 explicit learning curves for transduction and application to clustering and compression algorithms volume 22 pages 117142



inductive learning is based on inferring a general rule from a finite data set and using it to label new data in transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points which are given to the learner prior to learning although transduction seems at the outset to be an easier task than induction there have not been many provably useful algorithms for transduction moreover the precise relation between induction and transduction has not yet been determined the main theoretical developments related to transduction were presented by vapnik more than twenty years ago one of vapniks basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail while tight this bound is given implicitly via a computational routine our first contribution is a somewhat looser but explicit characterization of a slightly extended pacbayesian version of vapniks transductive bound this characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement we then derive error bounds for compression schemes such as transductive support vector machines and for transduction algorithms based on clustering the main observation used for deriving these new error bounds and algorithms is that the unlabeled test points which in the transductive setting are known in advance can be used in order to construct useful data dependent prior distributions over the hypothesis space







j  oh k  choi and h  isahara 2006 a comparison of different machine transliteration models volume 27 pages 119151





p  w jordan and  m  a walker 2005 learning content selection rules for generating object descriptions in dialogue volume 24 pages 157194



a fundamental requirement of any taskoriented dialogue system is the ability to generate object descriptions that refer to objects in the task domain the subproblem of content selection for object descriptions in taskoriented dialogue has been the focus of much previous work and a large number of models have been proposed in this paper we use the annotated coconut corpus of taskoriented design dialogues to develop feature sets based on dale and reiters 1995 incremental model brennan and clarks 1996 conceptual pact model and jordans 2000b intentional influences model and use these feature sets in a machine learning experiment to automatically learn a model of content selection for object descriptions  since dale and reiters model requires a representation of discourse structure the corpus annotations are used to derive a representation based on grosz and sidners 1986 theory of the intentional structure of discourse as well as two very simple representations of discourse structure based purely on recency we then apply the ruleinduction program ripper to train and test the content selection component of an object description generator on a set of 393 object descriptions from the corpus to our knowledge this is the first reported experiment of a trainable content selection component for object description generation in dialogue three separate content selection models that are based on the three theoretical models all independently achieve accuracies significantly above the majority class baseline 17 on unseen test data with the intentional influences model 424 performing significantly better than either the incremental model 304 or the conceptual pact model 289 but the best performing models combine all the feature sets achieving accuracies near 60 surprisingly a simple recencybased representation of discourse structure does as well as one based on intentional structure  to our knowledge this is also the first empirical comparison of a representation of grosz and sidners model of discourse structure with a simpler model for any generation task





k  lerman  s  n minton and  c  a knoblock 2003 wrapper maintenance a machine learning approach volume 18 pages 149181



the proliferation of online information sources has led to an    increased use of wrappers for extracting data from web sources while    most of the previous research has focused on quick and efficient    generation of wrappers the development of tools for wrapper    maintenance has received less attention this is an important research    problem because web sources often change in ways that prevent the    wrappers from extracting data correctly  we present an efficient    algorithm that learns structural information about data from positive    examples alone we describe how this information can be used for two    wrapper maintenance applications wrapper verification and    reinduction the wrapper verification system detects when a wrapper is    not extracting correct data usually because the web source has    changed its format  the reinduction algorithm automatically recovers    from changes in the web source by identifying data on web pages so    that a new wrapper may be generated for this source  to validate our    approach we monitored 27 wrappers over a period of a year the    verification algorithm correctly discovered 35 of the 37 wrapper    changes and made 16 mistakes resulting in precision of 073 and    recall of 095 we validated the reinduction algorithm on ten web    sources we were able to successfully reinduce the wrappers obtaining    precision and recall values of 090 and 080 on the data    extraction task





n  friedman and  j  y halpern 1999 modeling  belief  in  dynamic  systems part  ii  revision  and  update volume 10 pages 117167



the study of belief change has been an active area in    philosophy and ai  in recent years two special cases of belief    change belief revision and belief update have been studied in    detail  in a companion paper friedman  halpern 1997 we introduce    a new framework to model belief change this framework combines    temporal and epistemic modalities with a notion of plausibility    allowing us to examine the change of beliefs over time in this paper    we show how belief revision and belief update can be captured in our    framework  this allows us to compare the assumptions made by each    method and to better understand the principles underlying them  in    particular it shows that katsuno and mendelzons notion of belief    update katsuno  mendelzon 1991a depends on several strong    assumptions that may limit its applicability in artificial    intelligence  finally our analysis allow us to identify a notion of    minimal change that underlies a broad range of belief change    operations including revision and update







kenji  kawaguchi yu  maruyama and xiaoyu  zheng 2016 global continuous optimization with error bound and fast convergence volume 56 pages 153195





j  lee and y  meng 2011 firstorder stable model semantics and firstorder loop formulas volume 42 pages 125180



lin and zhaos theorem on loop formulas states that in the propositional case the stable model semantics of a logic program can be completely characterized by propositional loop formulas but this result does not fully carry over to the firstorder case we investigate the precise relationship between the firstorder stable model semantics and firstorder loop formulas and study conditions under which the former can be represented by the latter in order to facilitate the comparison we extend the definition of a firstorder loop formula which was limited to a nondisjunctive program to a disjunctive program and to an arbitrary firstorder theory based on the studied relationship we extend the syntax of a logic program with explicit quantifiers which allows us to do reasoning involving nonherbrand stable models using firstorder reasoners such programs can be viewed as a special class of firstorder theories under the stable model semantics which yields more succinct loop formulas than the general language due to their restricted syntax





r  meir a  d  procaccia j  s  rosenschein and aviv  zohar 2008 complexity of strategic behavior in multiwinner elections volume 33 pages 149178



although recent years have seen a surge of interest in the computational aspects of social choice no specific attention has previously been devoted to elections with multiple winners eg elections of an assembly or committee in this paper we characterize the worstcase complexity of manipulation and control in the context of four prominent multiwinner voting systems under different formulations of the strategic agent226s goal





d  j cook and  r  c varnell 1998 adaptive parallel iterative deepening search volume 9 pages 139165



many of the artificial intelligence techniques developed to    date rely on heuristic search through large spaces  unfortunately    the size of these spaces and the corresponding computational effort    reduce the applicability of otherwise novel and effective algorithms    a number of parallel and distributed approaches to search have    considerably improved the performance of the search process        our goal is to develop an architecture that automatically selects    parallel search strategies for optimal performance on a variety of    search problems  in this paper we describe one such architecture    realized in the eureka system which combines the benefits of    many different approaches to parallel heuristic search  through    empirical and theoretical analyses we observe that features of the    problem space directly affect the choice of optimal parallel search    strategy  we then employ machine learning techniques to select the    optimal parallel search strategy for a given problem space  when a    new search task is input to the system eureka uses features    describing the search space and the chosen architecture to    automatically select the appropriate search strategy  eureka    has been tested on a mimd parallel processor a distributed network of    workstations and a single workstation using multithreading  results    generated from fifteen puzzle problems robot arm motion problems    artificial search spaces and planning problems indicate that     eureka outperforms any of the tested strategies used exclusively for    all problem instances and is able to greatly reduce the search time    for these applications





t  elomaa and  m  kaariainen 2001 an analysis of reduced error pruning volume 15 pages 163187



topdown induction of decision trees has been observed to    suffer from the inadequate functioning of the pruning phase  in    particular it is known that the size of the resulting tree grows    linearly with the sample size even though the accuracy of the tree    does not improve  reduced error pruning is an algorithm that has been    used as a representative technique in attempts to explain the problems    of decision tree learning      in this paper we present analyses of reduced error pruning in three    different settings  first we study the basic algorithmic properties    of the method properties that hold independent of the input decision    tree and pruning examples  then we examine a situation that    intuitively should lead to the subtree under consideration to be    replaced by a leaf node one in which the class label and attribute    values of the pruning examples are independent of each other  this    analysis is conducted under two different assumptions  the general    analysis shows that the pruning probability of a node fitting pure    noise is bounded by a function that decreases exponentially as the    size of the tree grows  in a specific analysis we assume that the    examples are distributed uniformly to the tree  this assumption lets    us approximate the number of subtrees that are pruned because they do    not receive any pruning examples      this paper clarifies the different variants of the reduced error    pruning algorithm brings new insight to its algorithmic properties    analyses the algorithm with less imposed assumptions than before and    includes the previously overlooked empty subtrees to the analysis







hl  chieu and ws  lee 2009 relaxed survey propagation for the weighted maximum satisfiability problem volume 36 pages 229266





g  a kaminka and  m  tambe 2000 robust agent teams via sociallyattentive monitoring volume 12 pages 105147



agents in dynamic multiagent environments must monitor    their peers to execute individual and group plans a key open question    is how much monitoring of other agents states is required to be    effective the monitoring selectivity problem  we investigate this    question in the context of detecting failures in teams of cooperating    agents via sociallyattentive monitoring which focuses on monitoring    for failures in the social relationships between the agents we    empirically and analytically explore a family of sociallyattentive    teamwork monitoring algorithms in two dynamic complex multiagent    domains under varying conditions of task distribution and    uncertainty we show that a centralized scheme using a complex    algorithm trades correctness for completeness and requires monitoring    all teammates in contrast a simple distributed teamwork monitoring    algorithm results in correct and complete detection of teamwork    failures despite relying on limited uncertain knowledge and    monitoring only key agents in a team  in addition we report on the    design of a sociallyattentive monitoring system and demonstrate its    generality in monitoring several coordination relationships    diagnosing detected failures and both online and offline applications







r  mourad c  sinoquet n  l zhang t  liu and p  leray 2013 a survey on latent tree models and applications volume 47 pages 157203





alejandro  moreo fern225ndez andrea  esuli and fabrizio  sebastiani 2016 distributional correspondence indexing for crosslingual and crossdomain sentiment classification volume 55 pages 131163



domain adaptation da techniques aim at enabling machine learning methods learn effective classifiers for a target domain when the only available training data belongs to a different source domain in this paper we present the distributional correspondence indexing dci method for domain adaptation in sentiment classification dci derives term representations in a vector space common to both domains where each dimension reflects its distributional correspondence to a pivot ie to a highly predictive term that behaves similarly across domains term correspondence is quantified by means of a distributional correspondence function dcf we propose a number of efficient dcfs that are motivated by the distributional hypothesis ie the hypothesis according to which terms with similar meaning tend to have similar distributions in text experiments show that dci obtains better performance than current stateoftheart techniques for crosslingual and crossdomain sentiment classification dci also brings about a significantly reduced computational cost and requires a smaller amount of human intervention as a final contribution we discuss a more challenging formulation of the domain adaptation problem in which both the crossdomain and crosslingual dimensions are tackled simultaneously







broes  de cat marc  denecker maurice  bruynooghe and peter  stuckey 2015 lazy model expansion interleaving grounding with search volume 52 pages 235286





a  d  procaccia and j  s rosenschein 2007 junta distributions and the averagecase complexity of manipulating elections volume 28 pages 157181



encouraging voters to truthfully reveal their preferences in an election has long been an important issue recently computational complexity has been suggested as a means of precluding strategic behavior previous studies have shown that some voting protocols are hard to manipulate but used nphardness as the complexity measure such a worstcase analysis may be an insufficient guarantee of resistance to manipulation

indeed we demonstrate that nphard manipulations may be tractable in the average case for this purpose we augment the existing theory of averagecase complexity with some new concepts in particular we consider elections distributed with respect to junta distributions which concentrate on hard instances we use our techniques to prove that scoring protocols are susceptible to manipulation by coalitions when the number of candidates is constant







m  binshtok r  i brafman c  domshlak and s  e shiomony 2009 generic preferences over subsets of structured objects volume 34 pages 133164



automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as question answering information extraction and summarization since most existing methods are supervised and require large corpora which for many languages do not exist we have concentrated our efforts to reduce the need for annotated data as much as possible this paper presents two different algorithms towards this goal the first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events in the first stage the algorithm learns a general classifier from an annotated corpus then inspired by the hypothesis of one type of temporal relation per discourse it extracts useful information from a cluster of topically related documents we show that by combining the global information of such a cluster with local decisions of a general classifier a bootstrapping crossdocument classifier can be built to extract temporal relations between events our experiments show that without any additional annotated data the accuracy of the proposed algorithm is higher than that of several previous successful systems the second proposed method for temporal relation extraction is based on the expectation maximization em algorithm within em we used different techniques such as a greedy bestfirst search and integer linear programming for temporal inconsistency removal we think that the experimental results of our em based algorithm as a first step toward a fully unsupervised temporal relation extraction method is encouraging











v  robu e  h gerding s  stein d  c parkes a  rogers and n  r jennings 2013 an online mechanism for multiunit demand and its application to plugin hybrid electric vehicle charging volume 48 pages 175230







b  glimm c  lutz i  horrocks and u  sattler 2008 conjunctive query answering for the description logic shiq volume 31 pages 157204





miquel  espl224gomis felipe  s225nchezmart237nez and mikel  l forcada 2015 using machine translation to provide targetlanguage edit hints in computer aided translation based on translation memories volume 53 pages 169222



this paper explores the use of generalpurpose machine translation mt in assisting the users of computeraided translation cat systems based on translation memory tm to identify the target words in the translation proposals that need to be changed either replaced or removed or kept unedited a task we term as wordkeeping recommendation mt is used as a black box to align source and target subsegments on the fly in the translation units tus suggested to the user sourcelanguage sl and targetlanguage tl segments in the matching tus are segmented into overlapping subsegments of variable length and machinetranslated into the tl and the sl respectively the bilingual subsegments obtained and the matching between the sl segment in the tu and the segment to be translated are employed to build the features that are then used by a binary classifier to determine the target words to be changed and those to be kept unedited in this approach mt results are never presented to the translator two approaches are presented in this work one using a wordkeeping recommendation system which can be trained on the tm used with the cat system and a more basic approach which does not require any training

experiments are conducted by simulating the translation of texts in several language pairs with corpora belonging to different domains and using three different mt systems we compare the performance obtained to that of previous works that have used statistical word alignment for wordkeeping recommendation and show that the mtbased approaches presented in this paper are more accurate in most scenarios in particular our results confirm that the mtbased approaches are better than the alignmentbased approach when using models trained on outofdomain tms additional experiments were performed to check how dependent the mtbased recommender is on the language pair and mt system used for training these experiments confirm a high degree of reusability of the recommendation models across various mt systems but a low level of reusability across language pairs





m  tambe 1997 towards flexible teamwork volume 7 pages 83124









g  pesant c  quimper and a  zanarini 2012 countingbased search branching heuristics for constraint satisfaction problems volume 43 pages 173210





m  o riedl and r  m young 2010 narrative planning balancing plot and character volume 39 pages 217268



narrative and in particular storytelling is an important part of the human experience  consequently computational systems that can reason about narrative can be more effective communicators entertainers educators and trainers  one of the central challenges in computational narrative reasoning is narrative generation the automated creation of meaningful event sequences  there are many factors  logical and aesthetic  that contribute to the success of a narrative artifact  central to this success is its understandability  we argue that the following two attributes of narratives are universal a the logical causal progression of plot and b character believability  character believability is the perception by the audience that the actions performed by characters do not negatively impact the audiences suspension of disbelief  specifically characters must be perceived by the audience to be intentional agents  in this article we explore the use of refinement search as a technique for solving the narrative generation problem  to find a sound and believable sequence of character actions that transforms an initial world state into a world state in which goal propositions hold we describe a novel refinement search planning algorithm  the intentbased partial order causal link ipocl planner  that in addition to creating causally sound plot progression reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why those characters commit to their goals we present the results of an empirical evaluation that demonstrates that narrative plans generated by the ipocl algorithm support audience comprehension of character intentions better than plans generated by conventional partialorder planners







j  y halpern and y  moses 2014 a procedural characterization of solution concepts in games volume 49 pages 143170





h  a geffner 2003 pddl 21 representation vs computation volume 20 pages 139144



i comment on the pddl 21 language and its use in the planning competition focusing on the choices made for accommodating time and concurrency  i also discuss some methodological issues that have to do with the move toward more expressive planning languages and the balance needed in planning research between semantics and computation





j  d park and a  darwiche 2006 complexity results and approximation strategies for map explanations volume 21 pages 101133



map is the problem of finding a most probable instantiation of a set of variables given evidence map has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation pr or the problem of computing the most probable explanation mpe this paper investigates the complexity of map in bayesian networks specifically we show that map is complete for nppp and provide further negative complexity results for algorithms based on variable elimination we also show that map remains hard even when mpe and pr become easy for example we show that map is npcomplete when the networks are restricted to polytrees and even then can not be effectively approximated  given the difficulty of computing map exactly and the difficulty of approximating map while providing useful guarantees on the resulting approximation we investigate best effort approximations we introduce a generic map approximation framework we provide two instantiations of the framework one for networks which are amenable to exact inference pr and one for networks for which even exact inference is too hard this allows map approximation on networks that are too complex to even exactly solve the easier problems pr and mpe experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques and provide accurate map estimates in many cases 





m  michelson and c  a knoblock 2010 constructing reference sets from unstructured ungrammatical text volume 38 pages 189221



vast amounts of text on the web are unstructured and ungrammatical such as classified ads auction listings forum postings etc we call such text posts despite their inconsistent structure and lack of grammar posts are full of useful information this paper presents work on semiautomatically building tables of relational information called reference sets by analyzing such posts directly reference sets can be applied to a number of tasks such as ontology maintenance and information extraction our referenceset construction method starts with just a small amount of background knowledge and constructs tuples representing the entities in the posts to form a reference set we also describe an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use to evaluate the utility of the machineconstructed reference sets we compare them to manually constructed reference sets in the context of referencesetbased information extraction our results show the reference sets constructed by our method outperform manually constructed reference sets we also compare the referencesetbased extraction approach using the machineconstructed reference set to supervised extraction approaches using generic features these results demonstrate that using machineconstructed reference sets outperforms the supervised methods even though the supervised methods require training data





honorable mention for the 2005 ijcaijair best paper prize



stochastic sampling algorithms while an attractive alternative to    exact algorithms in very large bayesian network models have been    observed to perform poorly in evidential reasoning with extremely    unlikely evidence to address this problem we propose an adaptive    importance sampling algorithm aisbn that shows promising    convergence rates even under extreme conditions and seems to    outperform the existing sampling algorithms consistently three    sources of this performance improvement are 1 two heuristics for    initialization of the importance function that are based on the    theoretical properties of importance sampling in finitedimensional    integrals and the structural advantages of bayesian networks 2 a    smooth learning method for the importance function and 3 a dynamic    weighting function for combining samples from different stages of the    algorithm       we tested the performance of the aisbn algorithm along with two state    of the art general purpose sampling algorithms likelihood weighting    fung  chang 1989 shachter  peot 1989 and selfimportance    sampling shachter  peot 1989 we used in our tests three large    real bayesian network models available to the scientific community    the cpcs network pradhan et al 1994 the pathfinder network    heckerman horvitz  nathwani 1990 and the andes network conati    gertner vanlehn  druzdzel 1997 with evidence as unlikely as    1041 while the aisbn algorithm always performed better than the    other two algorithms in the majority of the test cases it achieved    orders of magnitude improvement in precision of the results    improvement in speed given a desired precision is even more dramatic    although we are unable to report numerical results here as the other    algorithms almost never achieved the precision reached even by the    first few iterations of the aisbn algorithm













v  qazvinian d  r radev s  m mohammad b  dorr d  zajic m  whidby and t  moon 2013 generating extractive summaries of scientific paradigms volume 46 pages 165201



voting is a general method for aggregating the preferences of multiple agents  each agent ranks all the possible alternatives and based on this an aggregate ranking of the alternatives or at least a winning alternative is produced  however when there are many alternatives it is impractical to simply ask agents to report their complete preferences rather the agents preferences or at least the relevant parts thereof need to be elicited  this is done by asking the agents a hopefully small number of simple queries about their preferences such as comparison queries which ask an agent to compare two of the alternatives  prior work on preference elicitation in voting has focused on the case of unrestricted preferences  it has been shown that in this setting it is sometimes necessary to ask each agent almost as many queries as would be required to determine an arbitrary ranking of the alternatives  in contrast in this paper we focus on singlepeaked preferences  we show that such preferences can be elicited using only a linear number of comparison queries if either the order with respect to which preferences are singlepeaked is known or at least one other agents complete preferences are known  we show that using a sublinear number of queries does not suffice  we also consider the case of cardinally singlepeaked preferences  for this case we show that if the alternatives cardinal positions are known then an agents preferences can be elicited using only a logarithmic number of queries however we also show that if the cardinal positions are not known then a sublinear number of queries does not suffice  we present experimental results for all elicitation algorithms  we also consider the problem of only eliciting enough information to determine the aggregate ranking and show that even for this more modest objective a sublinear number of queries per agent does not suffice for known ordinal or unknown cardinal positions  finally we discuss whether and how these techniques can be applied when preferences are almost singlepeaked











llu237s  formiga alberto  barr243ncede241o llu237s  m224rquez carlos a  henr237quez and jos233 b  mari241o 2015 leveraging online user feedback to improve statistical machine translation volume 54 pages 159192







v  bulitko and g  lee 2006 learning in realtime search a unifying framework volume 25 pages 119157





j  c schlimmer and  l  a hermens 1993 software agents completing patterns and constructing user interfaces volume 1 pages 6189



to support the goal of allowing users to record and retrieve information this paper describes an interactive notetaking system for penbased computers with two distinctive features first it actively predicts what the user is going to write second it automatically constructs a custom buttonbox user interface on request the system is an example of a learningapprentice software agent a machine learning component characterizes the syntax and semantics of the users information a performance system uses this learned information to generate completion strings and construct a user interface  description of online appendix people like to record information  doing this on paper is initially efficient but lacks flexibility recording information on a computer is less efficient but more powerful in our new note taking softwre the user records information directly on a computer behind the interface an agent acts for the user to help it provides defaults and constructs a custom user interface  the demonstration is a quicktime movie of the note taking agent in action  the file is a binhexed selfextracting archive macintosh utilities for binhex are available from macarchiveumichedu quicktime is available from ftpapplecom in the dtsmacsyssoftquicktime 

as real logic programmers normally use cut  an effective learning procedure for logic programs should be able to deal with it  because the cut predicate has only a procedural meaning clauses containing cut cannot be learned using an extensional evaluation method as is done in most learning systems  on the other hand searching a space of possible programs instead of a space of independent clauses is unfeasible  an alternative solution is to generate first a candidate base program which covers the positive examples and then make it consistent by inserting cut where appropriate  the problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach we generalize this scheme and investigate the difficulties that arise  some of the major shortcomings are actually caused in general by the need for intensional evaluation as a conclusion the analysis of this paper suggests on precise and technical grounds that learning cut is difficult and current induction techniques should probably be restricted to purely declarative logic languages





d  calvanese  m  lenzerini and  d  nardi 1999 unifying classbased representation formalisms volume 11 pages 199240



the notion of class is ubiquitous in computer science and is    central in many formalisms for the representation of structured    knowledge used both in knowledge representation and in databases  in    this paper we study the basic issues underlying such representation    formalisms and single out both their common characteristics and their    distinguishing features  such investigation leads us to propose a    unifying framework in which we are able to capture the fundamental    aspects of several representation languages used in different    contexts  the proposed formalism is expressed in the style of    description logics which have been introduced in knowledge    representation as a means to provide a semantically wellfounded basis    for the structural aspects of knowledge representation systems the    description logic considered in this paper is a subset of first order    logic with nice computational characteristics  it is quite expressive    and features a novel combination of constructs that has not been    studied before  the distinguishing constructs are number    restrictions which generalize existence and functional dependencies    inverse roles which allow one to refer to the inverse of a    relationship and possibly cyclic assertions which are necessary for    capturing real world domains  we are able to show that it is    precisely such combination of constructs that makes our logic powerful    enough to model the essential set of features for defining class    structures that are common to frame systems objectoriented database    languages and semantic data models  as a consequence of the    established correspondences several significant extensions of each of    the above formalisms become available the high expressiveness of the    logic we propose and the need for capturing the reasoning in different    contexts forces us to distinguish between unrestricted and finite    model reasoning  a notable feature of our proposal is that reasoning    in both cases is decidable  we argue that by virtue of the high    expressive power and of the associated reasoning capabilities on both    unrestricted and finite models our logic provides a common core for    classbased representation formalisms





s  soderland and  w  lehnert 1994 wrapup a trainable discourse module for information extraction volume 2 pages 131158



the vast amounts of online text now available have ledto   renewed interest in information extraction ie systems thatanalyze   unrestricted text producing a structured representation ofselected   information from the text this paper presents a novel approachthat   uses machine learning to acquire knowledge for some of the higher   level ie processing  wrapup is a trainable ie discourse component   that makes intersentential inferences and identifies logicalrelations   among information extracted from the text  previous corpusbased   approaches were limited to lower level processing such as   partofspeech tagging lexical disambiguation and dictionary   construction  wrapup is fully trainable and not onlyautomatically   decides what classifiers are needed but even derives the featureset   for each classifier automatically performance equals that of a   partially trainable discourse module requiring manual customization   for each domain





e  di sciascio  f  m donini and  m  mongiello 2002 structured knowledge representation for image retrieval volume 16 pages 209257



we propose a structured approach to the problem of retrieval    of images by content and present a description logic that has been    devised for the semantic indexing and retrieval of images containing    complex objects        as other approaches do we start from lowlevel features extracted    with image analysis to detect and characterize regions in an    image however in contrast with featurebased approaches we provide    a syntax to describe segmented regions as basic objects and complex    objects as compositions of basic ones then we introduce a companion    extensional semantics for defining reasoning services such as    retrieval classification and subsumption  these services can be    used for both exact and approximate matching using similarity    measures           using our logical approach as a formal specification we implemented a    complete clientserver image retrieval system which allows a user to    pose both queries by sketch and queries by example a set of    experiments has been carried out on a testbed of images to assess the    retrieval capabilities of the system in comparison with expert users    ranking results are presented adopting a wellestablished measure of    quality borrowed from textual information retrieval





r  nock 2002 inducing interpretable voting classifiers without trading accuracy for simplicity theoretical results approximation algorithms volume 17 pages 137170



recent advances in the study of voting classification    algorithms have brought empirical and theoretical results clearly    showing the discrimination power of ensemble classifiers it has been    previously argued that the search of this classification power in the    design of the algorithms has marginalized the need to obtain    interpretable classifiers therefore the question of whether one    might have to dispense with interpretability in order to keep    classification strength is being raised in a growing number of machine    learning or data mining papers the purpose of this paper is to study    both theoretically and empirically the problem first we provide    numerous results giving insight into the hardness of the    simplicityaccuracy tradeoff for voting classifiers then we provide    an efficient topdown and prune induction heuristic widc mainly    derived from recent results on the weak learning and boosting    frameworks  it is to our knowledge the first attempt to build a    voting classifier as a base formula using the weak learning framework    the one which was previously highly successful for decision tree    induction and not the strong learning framework as usual for such    classifiers with boostinglike approaches while it uses a wellknown    induction scheme previously successful in other classes of concept    representations thus making it easy to implement and compare widc    also relies on recent or new results we give about particular cases of    boosting known as partition boosting and ranking loss    boosting experimental results on thirtyone domains most of which    readily available tend to display the ability of widc to produce    small accurate and interpretable decision committees





honorable mention for the 2014 ijcaijair best paper prize



this paper introduces a principled approach for the design of a scalable general reinforcement learning agent our approach is based on a direct approximation of aixi a bayesian optimality notion for general reinforcement learning agents previously it has been unclear whether the theory of aixi could motivate the design of practical algorithms we answer this hitherto open question in the affirmative by providing the first computationally feasible approximation to the aixi agent to develop our approximation we introduce a new montecarlo tree search algorithm along with an agentspecific extension to the context tree weighting algorithm empirically we present a set of encouraging results on a variety of stochastic and partially observable domains we conclude by proposing a number of directions for future research





c  g giraudcarrier and  t  r martinez 1995 an integrated framework for learning and reasoning volume 3 pages 147185



learning and reasoning are both aspects of what is    considered to be intelligence their studies within ai have been    separated historically learning being the topic of machine learning    and neural networks and reasoning falling under classical or    symbolic ai  however learning and reasoning are in many ways    interdependent this paper discusses the nature of some of these    interdependencies and proposes a general framework called flare that    combines inductive learning using prior knowledge together with    reasoning in a propositional setting several examples that test the    framework are presented including classical induction many important    reasoning protocols and two simple expert systems



we consider online planning in markov decision processes mdps in online planning the agent focuses on its current state only deliberates about the set of possible policies from that state onwards and when interrupted uses the outcome of that exploratory deliberation to choose what action to perform next formally the performance of algorithms for online planning is assessed in terms of simple regret the agents expected performance loss when the chosen action rather than an optimal one is followed

to date stateoftheart algorithms for online planning in general mdps are either best effort or guarantee only polynomialrate reduction of simple regret over time here we introduce a new montecarlo tree search algorithm brue that guarantees exponentialrate and smooth reduction of simple regret  at a high level brue is based on a simple yet nonstandard statespace sampling scheme mcts2e in which different parts of each sample are dedicated to different exploratory objectives we further extend brue with a variant of learning by forgetting the resulting parametrized algorithm bruealpha exhibits even more attractive formal guarantees than brue our  empirical evaluation shows that both brue and its generalization bruealpha are also very effective in practice and  compare favorably to the stateoftheart









j  r quinlan 1996 learning firstorder definitions of functions volume 5 pages 139161



firstorder learning involves finding a clauseform    definition of a relation from examples of the relation and relevant    background information  in this paper a particular firstorder    learning system is modified to customize it for finding definitions of    functional relations  this restriction leads to faster learning times    and in some cases to definitions that have higher predictive    accuracy  other firstorder learning systems might benefit from    similar specialization





m  goldenberg a  felner r  stern g  sharon n  sturtevant r  c holte and j  schaeffer 2014 enhanced partial expansion a volume 50 pages 141187



when solving instances of problem domains that feature a large branching factor a may generate a large number of nodes whose cost is greater than the cost of the optimal solution we designate such nodes as surplus generating surplus nodes and adding them to the open list may dominate both time and memory of the search a recently introduced variant of a called partial expansion a pea deals with the memory aspect of this problem when expanding a node n pea generates all of its children and puts into open only the children with f  f n n is reinserted in the open list with the f cost of the best discarded child this guarantees that surplus nodes are not inserted into open

in this paper we present a novel variant of a called enhanced partial expansion a epea that advances the idea of pea to address the time aspect given a priori domain and heuristic specific knowledge epea generates only the nodes with f  fn although epea is not always applicable or practical we study several variants of epea which make it applicable to a large number of domains and heuristics in particular the ideas of epea are applicable to ida and to the domains where pattern databases are traditionally used experimental studies show significant improvements in runtime and memory performance for several standard benchmark applications we provide several theoretical studies to facilitate an understanding of the new algorithm





c  v goldman and s  zilberstein 2008 communicationbased decomposition mechanisms for decentralized mdps volume 32 pages 169202



multiagent planning in stochastic environments can be framed formally as a decentralized markov decision problem many reallife distributed problems that arise in manufacturing multirobot coordination and information gathering scenarios can be formalized using this framework however finding the optimal solution in the general case is hard limiting the applicability of recently developed algorithms this paper provides a practical approach for solving decentralized control problems when communication among the decision makers is possible but costly  we develop the notion of communicationbased mechanism that allows us to decompose a decentralized mdp into multiple singleagent problems in this framework referred to as decentralized semimarkov decision process with direct communication decsmdpcom agents operate separately between communications we show that finding an optimal mechanism is equivalent to solving optimally a decsmdpcom we also provide a heuristic search algorithm that converges on the optimal decomposition  restricting the decomposition to some specific types of local behaviors reduces significantly the complexity of planning in particular we present a polynomialtime algorithm for the case in which individual agents perform goaloriented behaviors between communications the paper concludes with an additional tractable algorithm that enables the introduction of human knowledge thereby reducing the overall problem to finding the best time to communicate empirical results show that these approaches provide good approximate solutions





b  srivastava and  s  kambhampati 1998 synthesizing customized planners from specifications volume 8 pages 93128



existing plan synthesis approaches in artificial    intelligence fall into two categories  domain independent and domain    dependent  the domain independent approaches are applicable across a    variety of domains but may not be very efficient in any one given    domain  the domain dependent approaches need to be redesigned for    each domain separately but can be very efficient in the domain for    which they are designed  one enticing alternative to these approaches    is to automatically synthesize domain independent planners given the    knowledge about the domain and the theory of planning in this paper    we investigate the feasibility of using existing automated software    synthesis tools to support such synthesis specifically we describe    an architecture called clay in which the kestrel interactive    development system kids is used to derive a domaincustomized    planner through a semiautomatic combination of a declarative theory    of planning and the declarative control knowledge specific to a given    domain to semiautomatically combine them to derive domaincustomized    planners  we discuss what it means to write a declarative theory of    planning and control knowledge for kids and illustrate our approach    by generating a class of domainspecific planners using state space    refinements  our experiments show that the synthesized planners can    outperform classical refinement planners implemented as    instantiations of ucp kambhampati  srivastava 1995 using the same    control knowledge  we will contrast the costs and benefits of the    synthesis approach with conventional methods for customizing domain    independent planners





j  r quinlan 1996 improved use of continuous attributes in c45 volume 4 pages 7790



a reported weakness of c45 in domains with continuous    attributes is addressed by modifying the formation and evaluation of    tests on continuous attributes  an mdlinspired penalty is applied to    such tests eliminating some of them from consideration and altering    the relative desirability of all tests  empirical trials show that    the modifications lead to smaller decision trees with higher    predictive accuracies  results also confirm that a new version of    c45 incorporating these changes is superior to recent approaches that    use global discretization and that construct small trees with    multiinterval splits





s  p gujar and y  narahari 2011 redistribution mechanisms for assignment of heterogeneous objects volume 41 pages 131154



there are p heterogeneous objects to be assigned to n competing agents n  p each with unit demand it is required to design a groves mechanism for this assignment problem satisfying weak budget balance individual rationality and minimizing the budget imbalance this calls for designing an appropriate rebate function when the objects are identical this problem has been solved which we refer as wco mechanism we measure the performance of such mechanisms by the redistribution index we first prove an impossibility theorem which rules out linear rebate functions with nonzero redistribution index in heterogeneous object assignment motivated by this theorem we explore two approaches to get around this impossibility in the first approach we show that linear rebate functions with nonzero redistribution index are possible when the valuations for the objects have a certain type of relationship and we design a mechanism with linear rebate function that is worst case optimal in the second approach we show that rebate functions with nonzero efficiency are possible if linearity is relaxed we extend the rebate functions of the wco mechanism to heterogeneous objects assignment and conjecture them to be worst case optimal





c  boutilier and  r  i brafman 2001 partialorder planning with concurrent interacting actions volume 14 pages 105136



in order to generate plans for agents with multiple    actuators agent teams or distributed controllers we must be able to    represent and plan using concurrent actions with interacting    effects this has historically been considered a challenging task    requiring a temporal planner with the ability to reason explicitly    about time we show that with simple modifications the strips action    representation language can be used to represent interacting actions    moreover algorithms for partialorder planning require only small    modifications in order to be applied in such multiagent domains  we    demonstrate this fact by developing a sound and complete partialorder    planner for planning with concurrent interacting actions pomp that    extends existing partialorder planners in a straightforward    way these results open the way to the use of partialorder planners    for the centralized control of cooperative multiagent systems



we propose a novel languageindependent approach for improving machine translation for resourcepoor languages by exploiting their similarity to resourcerich ones more precisely we improve the translation from a resourcepoor source language x1 into a resourcerich language y given a bitext containing a limited number of parallel sentences for x1y and a larger bitext for x2y for some resourcerich language x2 that is closely related to x1 this is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages x1 and x2 in spelling word order and syntax offer 1 we improve the word alignments for the resourcepoor language 2 we further augment it with additional translation options and 3 we take care of potential spelling differences through appropriate transliteration the evaluation for indonesian english using malay and for spanish  english using portuguese and pretending spanish is resourcepoor shows an absolute gain of up to 135 and 337 bleu points respectively which is an improvement over the best rivaling approaches while using much less additional data overall our method cuts the amount of necessary real training data by a factor of 25











t  heskes 2006 convexity arguments for efficient minimization of the bethe and kikuchi free energies volume 26 pages 153190





m  leisink and  b  kappen 2003 bound propagation volume 19 pages 139154



in this article we present an algorithm to compute bounds on the marginals of a graphical model  for several small clusters of nodes upper and lower bounds on the marginal values are computed independently of the rest of the network  the range of allowed probability distributions over the surrounding nodes is restricted using earlier computed bounds as we will show this can be considered as a set of constraints in a linear programming problem of which the objective function is the marginal probability of the center nodes  in this way knowledge about the maginals of neighbouring clusters is passed to other clusters thereby tightening the bounds on their marginals  we show that sharp bounds can be obtained for undirected and directed graphs that are used for practical applications but for which exact computations are infeasible





a  darwiche and  g  provan 1997 query dags a practical paradigm for implementing beliefnetwork inference volume 6 pages 147176



we describe a new paradigm for implementing inference in    belief networks which consists of two steps 1 compiling a belief    network into an arithmetic expression called a query dag qdag and    2 answering queries using a simple evaluation algorithm each node    of a qdag represents a numeric operation a number or a symbol for    evidence  each leaf node of a qdag represents the answer to a    network query that is the probability of some event of interest it    appears that qdags can be generated using any of the standard    algorithms for exact inference in belief networks we show how they    can be generated using clustering and conditioning algorithms the    time and space complexity of a qdag generation algorithm is no worse    than the time complexity of the inference algorithm on which it is    based the complexity of a qdag evaluation algorithm is linear in the    size of the qdag and such inference amounts to a standard evaluation    of the arithmetic expression it represents the intended value of    qdags is in reducing the software and hardware resources required to    utilize belief networks in online realworld applications the    proposed framework also facilitates the development of online    inference on different software and hardware platforms due to the    simplicity of the qdag evaluation algorithm interestingly enough    qdags were found to serve other purposes simple techniques for    reducing qdags tend to subsume relatively complex optimization    techniques for beliefnetwork inference such as networkpruning and    computationcaching



in this paper we present and evaluate a general framework for the design of truthful auctions for matching agents in a dynamic twosided market a single commodity such as a resource or a task is bought and sold by multiple buyers and sellers that arrive and depart over time our algorithm chain provides the first framework that allows a truthful dynamic double auction da to be constructed from a truthful singleperiod  ie static doubleauction rule the pricing and matching method of the chain construction is unique amongst dynamicauction rules that adopt the same building block  we examine experimentally the allocative efficiency of chain when instantiated on various singleperiod rules including the canonical mcafee doubleauction rule for a baseline we also consider nontruthful double auctions populated with zerointelligence plusstyle learning agents chainbased auctions perform well in comparison with other schemes especially as arrival intensity falls and agent valuations become more volatile 









p  d turney and p  pantel 2010 from frequency to meaning vector space models of semantics volume 37 pages 141188



computers understand very little of the meaning of human language this profoundly limits our ability to give instructions to computers the ability of computers to explain their actions to us and the ability of computers to analyse and process text vector space models vsms of semantics are beginning to address these limits this paper surveys the use of vsms for semantic processing of text we organize the literature on vsms according to the structure of the matrix in a vsm there are currently three broad classes of vsms based on termdocument wordcontext and pairpattern matrices yielding three classes of applications we survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category our goal in this survey is to show the breadth of applications of vsms for semantics to provide a new perspective on vsms for those who are already familiar with the area and to provide pointers into the literature for those who are less familiar with the field





d  gabelaia  r  kontchakov  a  kurucz  f  wolter and  m  zakharyaschev 2005 combining spatial and temporal logics expressiveness vs complexity volume 23 pages 167243



in this paper we construct and investigate a hierarchy of spatiotemporal formalisms that result from various combinations of propositional spatial and temporal logics such as the propositional temporal logic ptl the spatial logics rcc8 brcc8 s4u and their fragments the obtained results give a clear picture of the tradeoff between expressiveness and computational realisability within the hierarchy we demonstrate how different combining principles as well as spatial and temporal primitives can produce np pspace expspace 2expspacecomplete and even undecidable spatiotemporal logics out of components that are at most np or pspacecomplete





simone  parisi matteo  pirotta and marcello  restelli 2016 multiobjective reinforcement learning through continuous pareto manifold approximation volume 57 pages 187227



many realworld control applications from economics to robotics are characterized by the presence of multiple conflicting objectives in these problems the standard concept of optimality is replaced by paretooptimality and the goal is to find the pareto frontier a set of solutions representing different compromises among the objectives despite recent advances in multiobjective optimization achieving an accurate representation of the pareto frontier is still an important challenge  in this paper we propose a reinforcement learning policy gradient approach to learn a continuous approximation of the pareto   frontier in multiobjective markov decision problems momdps differently from previous policy gradient algorithms where n optimization routines are executed to have n solutions our approach performs a single gradient ascent run generating at each step an improved continuous approximation of the pareto frontier the idea is   to optimize the parameters of a function defining a manifold in the policy parameters space so that the corresponding image in the objectives space gets as close as possible to the true pareto frontier besides deriving how to compute and estimate such gradient we will also discuss the nontrivial issue of defining a metric to assess the quality of the candidate pareto frontiers finally the properties of the proposed approach are empirically evaluated on two problems a linearquadratic gaussian regulator and a water reservoir control task





p  derbeko r  elyaniv and r  meir 2004 explicit learning curves for transduction and application to clustering and compression algorithms volume 22 pages 117142



inductive learning is based on inferring a general rule from a finite data set and using it to label new data in transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points which are given to the learner prior to learning although transduction seems at the outset to be an easier task than induction there have not been many provably useful algorithms for transduction moreover the precise relation between induction and transduction has not yet been determined the main theoretical developments related to transduction were presented by vapnik more than twenty years ago one of vapniks basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail while tight this bound is given implicitly via a computational routine our first contribution is a somewhat looser but explicit characterization of a slightly extended pacbayesian version of vapniks transductive bound this characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement we then derive error bounds for compression schemes such as transductive support vector machines and for transduction algorithms based on clustering the main observation used for deriving these new error bounds and algorithms is that the unlabeled test points which in the transductive setting are known in advance can be used in order to construct useful data dependent prior distributions over the hypothesis space







j  oh k  choi and h  isahara 2006 a comparison of different machine transliteration models volume 27 pages 119151





p  w jordan and  m  a walker 2005 learning content selection rules for generating object descriptions in dialogue volume 24 pages 157194



a fundamental requirement of any taskoriented dialogue system is the ability to generate object descriptions that refer to objects in the task domain the subproblem of content selection for object descriptions in taskoriented dialogue has been the focus of much previous work and a large number of models have been proposed in this paper we use the annotated coconut corpus of taskoriented design dialogues to develop feature sets based on dale and reiters 1995 incremental model brennan and clarks 1996 conceptual pact model and jordans 2000b intentional influences model and use these feature sets in a machine learning experiment to automatically learn a model of content selection for object descriptions  since dale and reiters model requires a representation of discourse structure the corpus annotations are used to derive a representation based on grosz and sidners 1986 theory of the intentional structure of discourse as well as two very simple representations of discourse structure based purely on recency we then apply the ruleinduction program ripper to train and test the content selection component of an object description generator on a set of 393 object descriptions from the corpus to our knowledge this is the first reported experiment of a trainable content selection component for object description generation in dialogue three separate content selection models that are based on the three theoretical models all independently achieve accuracies significantly above the majority class baseline 17 on unseen test data with the intentional influences model 424 performing significantly better than either the incremental model 304 or the conceptual pact model 289 but the best performing models combine all the feature sets achieving accuracies near 60 surprisingly a simple recencybased representation of discourse structure does as well as one based on intentional structure  to our knowledge this is also the first empirical comparison of a representation of grosz and sidners model of discourse structure with a simpler model for any generation task





k  lerman  s  n minton and  c  a knoblock 2003 wrapper maintenance a machine learning approach volume 18 pages 149181



the proliferation of online information sources has led to an    increased use of wrappers for extracting data from web sources while    most of the previous research has focused on quick and efficient    generation of wrappers the development of tools for wrapper    maintenance has received less attention this is an important research    problem because web sources often change in ways that prevent the    wrappers from extracting data correctly  we present an efficient    algorithm that learns structural information about data from positive    examples alone we describe how this information can be used for two    wrapper maintenance applications wrapper verification and    reinduction the wrapper verification system detects when a wrapper is    not extracting correct data usually because the web source has    changed its format  the reinduction algorithm automatically recovers    from changes in the web source by identifying data on web pages so    that a new wrapper may be generated for this source  to validate our    approach we monitored 27 wrappers over a period of a year the    verification algorithm correctly discovered 35 of the 37 wrapper    changes and made 16 mistakes resulting in precision of 073 and    recall of 095 we validated the reinduction algorithm on ten web    sources we were able to successfully reinduce the wrappers obtaining    precision and recall values of 090 and 080 on the data    extraction task





n  friedman and  j  y halpern 1999 modeling  belief  in  dynamic  systems part  ii  revision  and  update volume 10 pages 117167



the study of belief change has been an active area in    philosophy and ai  in recent years two special cases of belief    change belief revision and belief update have been studied in    detail  in a companion paper friedman  halpern 1997 we introduce    a new framework to model belief change this framework combines    temporal and epistemic modalities with a notion of plausibility    allowing us to examine the change of beliefs over time in this paper    we show how belief revision and belief update can be captured in our    framework  this allows us to compare the assumptions made by each    method and to better understand the principles underlying them  in    particular it shows that katsuno and mendelzons notion of belief    update katsuno  mendelzon 1991a depends on several strong    assumptions that may limit its applicability in artificial    intelligence  finally our analysis allow us to identify a notion of    minimal change that underlies a broad range of belief change    operations including revision and update







kenji  kawaguchi yu  maruyama and xiaoyu  zheng 2016 global continuous optimization with error bound and fast convergence volume 56 pages 153195





j  lee and y  meng 2011 firstorder stable model semantics and firstorder loop formulas volume 42 pages 125180



lin and zhaos theorem on loop formulas states that in the propositional case the stable model semantics of a logic program can be completely characterized by propositional loop formulas but this result does not fully carry over to the firstorder case we investigate the precise relationship between the firstorder stable model semantics and firstorder loop formulas and study conditions under which the former can be represented by the latter in order to facilitate the comparison we extend the definition of a firstorder loop formula which was limited to a nondisjunctive program to a disjunctive program and to an arbitrary firstorder theory based on the studied relationship we extend the syntax of a logic program with explicit quantifiers which allows us to do reasoning involving nonherbrand stable models using firstorder reasoners such programs can be viewed as a special class of firstorder theories under the stable model semantics which yields more succinct loop formulas than the general language due to their restricted syntax





r  meir a  d  procaccia j  s  rosenschein and aviv  zohar 2008 complexity of strategic behavior in multiwinner elections volume 33 pages 149178



although recent years have seen a surge of interest in the computational aspects of social choice no specific attention has previously been devoted to elections with multiple winners eg elections of an assembly or committee in this paper we characterize the worstcase complexity of manipulation and control in the context of four prominent multiwinner voting systems under different formulations of the strategic agent226s goal





d  j cook and  r  c varnell 1998 adaptive parallel iterative deepening search volume 9 pages 139165



many of the artificial intelligence techniques developed to    date rely on heuristic search through large spaces  unfortunately    the size of these spaces and the corresponding computational effort    reduce the applicability of otherwise novel and effective algorithms    a number of parallel and distributed approaches to search have    considerably improved the performance of the search process        our goal is to develop an architecture that automatically selects    parallel search strategies for optimal performance on a variety of    search problems  in this paper we describe one such architecture    realized in the eureka system which combines the benefits of    many different approaches to parallel heuristic search  through    empirical and theoretical analyses we observe that features of the    problem space directly affect the choice of optimal parallel search    strategy  we then employ machine learning techniques to select the    optimal parallel search strategy for a given problem space  when a    new search task is input to the system eureka uses features    describing the search space and the chosen architecture to    automatically select the appropriate search strategy  eureka    has been tested on a mimd parallel processor a distributed network of    workstations and a single workstation using multithreading  results    generated from fifteen puzzle problems robot arm motion problems    artificial search spaces and planning problems indicate that     eureka outperforms any of the tested strategies used exclusively for    all problem instances and is able to greatly reduce the search time    for these applications





t  elomaa and  m  kaariainen 2001 an analysis of reduced error pruning volume 15 pages 163187



topdown induction of decision trees has been observed to    suffer from the inadequate functioning of the pruning phase  in    particular it is known that the size of the resulting tree grows    linearly with the sample size even though the accuracy of the tree    does not improve  reduced error pruning is an algorithm that has been    used as a representative technique in attempts to explain the problems    of decision tree learning      in this paper we present analyses of reduced error pruning in three    different settings  first we study the basic algorithmic properties    of the method properties that hold independent of the input decision    tree and pruning examples  then we examine a situation that    intuitively should lead to the subtree under consideration to be    replaced by a leaf node one in which the class label and attribute    values of the pruning examples are independent of each other  this    analysis is conducted under two different assumptions  the general    analysis shows that the pruning probability of a node fitting pure    noise is bounded by a function that decreases exponentially as the    size of the tree grows  in a specific analysis we assume that the    examples are distributed uniformly to the tree  this assumption lets    us approximate the number of subtrees that are pruned because they do    not receive any pruning examples      this paper clarifies the different variants of the reduced error    pruning algorithm brings new insight to its algorithmic properties    analyses the algorithm with less imposed assumptions than before and    includes the previously overlooked empty subtrees to the analysis







hl  chieu and ws  lee 2009 relaxed survey propagation for the weighted maximum satisfiability problem volume 36 pages 229266





g  a kaminka and  m  tambe 2000 robust agent teams via sociallyattentive monitoring volume 12 pages 105147



agents in dynamic multiagent environments must monitor    their peers to execute individual and group plans a key open question    is how much monitoring of other agents states is required to be    effective the monitoring selectivity problem  we investigate this    question in the context of detecting failures in teams of cooperating    agents via sociallyattentive monitoring which focuses on monitoring    for failures in the social relationships between the agents we    empirically and analytically explore a family of sociallyattentive    teamwork monitoring algorithms in two dynamic complex multiagent    domains under varying conditions of task distribution and    uncertainty we show that a centralized scheme using a complex    algorithm trades correctness for completeness and requires monitoring    all teammates in contrast a simple distributed teamwork monitoring    algorithm results in correct and complete detection of teamwork    failures despite relying on limited uncertain knowledge and    monitoring only key agents in a team  in addition we report on the    design of a sociallyattentive monitoring system and demonstrate its    generality in monitoring several coordination relationships    diagnosing detected failures and both online and offline applications







r  mourad c  sinoquet n  l zhang t  liu and p  leray 2013 a survey on latent tree models and applications volume 47 pages 157203





alejandro  moreo fern225ndez andrea  esuli and fabrizio  sebastiani 2016 distributional correspondence indexing for crosslingual and crossdomain sentiment classification volume 55 pages 131163



domain adaptation da techniques aim at enabling machine learning methods learn effective classifiers for a target domain when the only available training data belongs to a different source domain in this paper we present the distributional correspondence indexing dci method for domain adaptation in sentiment classification dci derives term representations in a vector space common to both domains where each dimension reflects its distributional correspondence to a pivot ie to a highly predictive term that behaves similarly across domains term correspondence is quantified by means of a distributional correspondence function dcf we propose a number of efficient dcfs that are motivated by the distributional hypothesis ie the hypothesis according to which terms with similar meaning tend to have similar distributions in text experiments show that dci obtains better performance than current stateoftheart techniques for crosslingual and crossdomain sentiment classification dci also brings about a significantly reduced computational cost and requires a smaller amount of human intervention as a final contribution we discuss a more challenging formulation of the domain adaptation problem in which both the crossdomain and crosslingual dimensions are tackled simultaneously







broes  de cat marc  denecker maurice  bruynooghe and peter  stuckey 2015 lazy model expansion interleaving grounding with search volume 52 pages 235286





a  d  procaccia and j  s rosenschein 2007 junta distributions and the averagecase complexity of manipulating elections volume 28 pages 157181



encouraging voters to truthfully reveal their preferences in an election has long been an important issue recently computational complexity has been suggested as a means of precluding strategic behavior previous studies have shown that some voting protocols are hard to manipulate but used nphardness as the complexity measure such a worstcase analysis may be an insufficient guarantee of resistance to manipulation

indeed we demonstrate that nphard manipulations may be tractable in the average case for this purpose we augment the existing theory of averagecase complexity with some new concepts in particular we consider elections distributed with respect to junta distributions which concentrate on hard instances we use our techniques to prove that scoring protocols are susceptible to manipulation by coalitions when the number of candidates is constant







m  binshtok r  i brafman c  domshlak and s  e shiomony 2009 generic preferences over subsets of structured objects volume 34 pages 133164



automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as question answering information extraction and summarization since most existing methods are supervised and require large corpora which for many languages do not exist we have concentrated our efforts to reduce the need for annotated data as much as possible this paper presents two different algorithms towards this goal the first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events in the first stage the algorithm learns a general classifier from an annotated corpus then inspired by the hypothesis of one type of temporal relation per discourse it extracts useful information from a cluster of topically related documents we show that by combining the global information of such a cluster with local decisions of a general classifier a bootstrapping crossdocument classifier can be built to extract temporal relations between events our experiments show that without any additional annotated data the accuracy of the proposed algorithm is higher than that of several previous successful systems the second proposed method for temporal relation extraction is based on the expectation maximization em algorithm within em we used different techniques such as a greedy bestfirst search and integer linear programming for temporal inconsistency removal we think that the experimental results of our em based algorithm as a first step toward a fully unsupervised temporal relation extraction method is encouraging











v  robu e  h gerding s  stein d  c parkes a  rogers and n  r jennings 2013 an online mechanism for multiunit demand and its application to plugin hybrid electric vehicle charging volume 48 pages 175230







b  glimm c  lutz i  horrocks and u  sattler 2008 conjunctive query answering for the description logic shiq volume 31 pages 157204





miquel  espl224gomis felipe  s225nchezmart237nez and mikel  l forcada 2015 using machine translation to provide targetlanguage edit hints in computer aided translation based on translation memories volume 53 pages 169222



this paper explores the use of generalpurpose machine translation mt in assisting the users of computeraided translation cat systems based on translation memory tm to identify the target words in the translation proposals that need to be changed either replaced or removed or kept unedited a task we term as wordkeeping recommendation mt is used as a black box to align source and target subsegments on the fly in the translation units tus suggested to the user sourcelanguage sl and targetlanguage tl segments in the matching tus are segmented into overlapping subsegments of variable length and machinetranslated into the tl and the sl respectively the bilingual subsegments obtained and the matching between the sl segment in the tu and the segment to be translated are employed to build the features that are then used by a binary classifier to determine the target words to be changed and those to be kept unedited in this approach mt results are never presented to the translator two approaches are presented in this work one using a wordkeeping recommendation system which can be trained on the tm used with the cat system and a more basic approach which does not require any training

experiments are conducted by simulating the translation of texts in several language pairs with corpora belonging to different domains and using three different mt systems we compare the performance obtained to that of previous works that have used statistical word alignment for wordkeeping recommendation and show that the mtbased approaches presented in this paper are more accurate in most scenarios in particular our results confirm that the mtbased approaches are better than the alignmentbased approach when using models trained on outofdomain tms additional experiments were performed to check how dependent the mtbased recommender is on the language pair and mt system used for training these experiments confirm a high degree of reusability of the recommendation models across various mt systems but a low level of reusability across language pairs





m  tambe 1997 towards flexible teamwork volume 7 pages 83124









g  pesant c  quimper and a  zanarini 2012 countingbased search branching heuristics for constraint satisfaction problems volume 43 pages 173210





m  o riedl and r  m young 2010 narrative planning balancing plot and character volume 39 pages 217268



narrative and in particular storytelling is an important part of the human experience  consequently computational systems that can reason about narrative can be more effective communicators entertainers educators and trainers  one of the central challenges in computational narrative reasoning is narrative generation the automated creation of meaningful event sequences  there are many factors  logical and aesthetic  that contribute to the success of a narrative artifact  central to this success is its understandability  we argue that the following two attributes of narratives are universal a the logical causal progression of plot and b character believability  character believability is the perception by the audience that the actions performed by characters do not negatively impact the audiences suspension of disbelief  specifically characters must be perceived by the audience to be intentional agents  in this article we explore the use of refinement search as a technique for solving the narrative generation problem  to find a sound and believable sequence of character actions that transforms an initial world state into a world state in which goal propositions hold we describe a novel refinement search planning algorithm  the intentbased partial order causal link ipocl planner  that in addition to creating causally sound plot progression reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why those characters commit to their goals we present the results of an empirical evaluation that demonstrates that narrative plans generated by the ipocl algorithm support audience comprehension of character intentions better than plans generated by conventional partialorder planners







j  y halpern and y  moses 2014 a procedural characterization of solution concepts in games volume 49 pages 143170





h  a geffner 2003 pddl 21 representation vs computation volume 20 pages 139144



i comment on the pddl 21 language and its use in the planning competition focusing on the choices made for accommodating time and concurrency  i also discuss some methodological issues that have to do with the move toward more expressive planning languages and the balance needed in planning research between semantics and computation





j  d park and a  darwiche 2006 complexity results and approximation strategies for map explanations volume 21 pages 101133



map is the problem of finding a most probable instantiation of a set of variables given evidence map has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation pr or the problem of computing the most probable explanation mpe this paper investigates the complexity of map in bayesian networks specifically we show that map is complete for nppp and provide further negative complexity results for algorithms based on variable elimination we also show that map remains hard even when mpe and pr become easy for example we show that map is npcomplete when the networks are restricted to polytrees and even then can not be effectively approximated  given the difficulty of computing map exactly and the difficulty of approximating map while providing useful guarantees on the resulting approximation we investigate best effort approximations we introduce a generic map approximation framework we provide two instantiations of the framework one for networks which are amenable to exact inference pr and one for networks for which even exact inference is too hard this allows map approximation on networks that are too complex to even exactly solve the easier problems pr and mpe experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques and provide accurate map estimates in many cases 





m  michelson and c  a knoblock 2010 constructing reference sets from unstructured ungrammatical text volume 38 pages 189221



vast amounts of text on the web are unstructured and ungrammatical such as classified ads auction listings forum postings etc we call such text posts despite their inconsistent structure and lack of grammar posts are full of useful information this paper presents work on semiautomatically building tables of relational information called reference sets by analyzing such posts directly reference sets can be applied to a number of tasks such as ontology maintenance and information extraction our referenceset construction method starts with just a small amount of background knowledge and constructs tuples representing the entities in the posts to form a reference set we also describe an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use to evaluate the utility of the machineconstructed reference sets we compare them to manually constructed reference sets in the context of referencesetbased information extraction our results show the reference sets constructed by our method outperform manually constructed reference sets we also compare the referencesetbased extraction approach using the machineconstructed reference set to supervised extraction approaches using generic features these results demonstrate that using machineconstructed reference sets outperforms the supervised methods even though the supervised methods require training data





honorable mention for the 2005 ijcaijair best paper prize



stochastic sampling algorithms while an attractive alternative to    exact algorithms in very large bayesian network models have been    observed to perform poorly in evidential reasoning with extremely    unlikely evidence to address this problem we propose an adaptive    importance sampling algorithm aisbn that shows promising    convergence rates even under extreme conditions and seems to    outperform the existing sampling algorithms consistently three    sources of this performance improvement are 1 two heuristics for    initialization of the importance function that are based on the    theoretical properties of importance sampling in finitedimensional    integrals and the structural advantages of bayesian networks 2 a    smooth learning method for the importance function and 3 a dynamic    weighting function for combining samples from different stages of the    algorithm       we tested the performance of the aisbn algorithm along with two state    of the art general purpose sampling algorithms likelihood weighting    fung  chang 1989 shachter  peot 1989 and selfimportance    sampling shachter  peot 1989 we used in our tests three large    real bayesian network models available to the scientific community    the cpcs network pradhan et al 1994 the pathfinder network    heckerman horvitz  nathwani 1990 and the andes network conati    gertner vanlehn  druzdzel 1997 with evidence as unlikely as    1041 while the aisbn algorithm always performed better than the    other two algorithms in the majority of the test cases it achieved    orders of magnitude improvement in precision of the results    improvement in speed given a desired precision is even more dramatic    although we are unable to report numerical results here as the other    algorithms almost never achieved the precision reached even by the    first few iterations of the aisbn algorithm













v  qazvinian d  r radev s  m mohammad b  dorr d  zajic m  whidby and t  moon 2013 generating extractive summaries of scientific paradigms volume 46 pages 165201



voting is a general method for aggregating the preferences of multiple agents  each agent ranks all the possible alternatives and based on this an aggregate ranking of the alternatives or at least a winning alternative is produced  however when there are many alternatives it is impractical to simply ask agents to report their complete preferences rather the agents preferences or at least the relevant parts thereof need to be elicited  this is done by asking the agents a hopefully small number of simple queries about their preferences such as comparison queries which ask an agent to compare two of the alternatives  prior work on preference elicitation in voting has focused on the case of unrestricted preferences  it has been shown that in this setting it is sometimes necessary to ask each agent almost as many queries as would be required to determine an arbitrary ranking of the alternatives  in contrast in this paper we focus on singlepeaked preferences  we show that such preferences can be elicited using only a linear number of comparison queries if either the order with respect to which preferences are singlepeaked is known or at least one other agents complete preferences are known  we show that using a sublinear number of queries does not suffice  we also consider the case of cardinally singlepeaked preferences  for this case we show that if the alternatives cardinal positions are known then an agents preferences can be elicited using only a logarithmic number of queries however we also show that if the cardinal positions are not known then a sublinear number of queries does not suffice  we present experimental results for all elicitation algorithms  we also consider the problem of only eliciting enough information to determine the aggregate ranking and show that even for this more modest objective a sublinear number of queries per agent does not suffice for known ordinal or unknown cardinal positions  finally we discuss whether and how these techniques can be applied when preferences are almost singlepeaked











llu237s  formiga alberto  barr243ncede241o llu237s  m224rquez carlos a  henr237quez and jos233 b  mari241o 2015 leveraging online user feedback to improve statistical machine translation volume 54 pages 159192







v  bulitko and g  lee 2006 learning in realtime search a unifying framework volume 25 pages 119157





j  c schlimmer and  l  a hermens 1993 software agents completing patterns and constructing user interfaces volume 1 pages 6189



to support the goal of allowing users to record and retrieve information this paper describes an interactive notetaking system for penbased computers with two distinctive features first it actively predicts what the user is going to write second it automatically constructs a custom buttonbox user interface on request the system is an example of a learningapprentice software agent a machine learning component characterizes the syntax and semantics of the users information a performance system uses this learned information to generate completion strings and construct a user interface  description of online appendix people like to record information  doing this on paper is initially efficient but lacks flexibility recording information on a computer is less efficient but more powerful in our new note taking softwre the user records information directly on a computer behind the interface an agent acts for the user to help it provides defaults and constructs a custom user interface  the demonstration is a quicktime movie of the note taking agent in action  the file is a binhexed selfextracting archive macintosh utilities for binhex are available from macarchiveumichedu quicktime is available from ftpapplecom in the dtsmacsyssoftquicktime 

as real logic programmers normally use cut  an effective learning procedure for logic programs should be able to deal with it  because the cut predicate has only a procedural meaning clauses containing cut cannot be learned using an extensional evaluation method as is done in most learning systems  on the other hand searching a space of possible programs instead of a space of independent clauses is unfeasible  an alternative solution is to generate first a candidate base program which covers the positive examples and then make it consistent by inserting cut where appropriate  the problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach we generalize this scheme and investigate the difficulties that arise  some of the major shortcomings are actually caused in general by the need for intensional evaluation as a conclusion the analysis of this paper suggests on precise and technical grounds that learning cut is difficult and current induction techniques should probably be restricted to purely declarative logic languages





d  calvanese  m  lenzerini and  d  nardi 1999 unifying classbased representation formalisms volume 11 pages 199240



the notion of class is ubiquitous in computer science and is    central in many formalisms for the representation of structured    knowledge used both in knowledge representation and in databases  in    this paper we study the basic issues underlying such representation    formalisms and single out both their common characteristics and their    distinguishing features  such investigation leads us to propose a    unifying framework in which we are able to capture the fundamental    aspects of several representation languages used in different    contexts  the proposed formalism is expressed in the style of    description logics which have been introduced in knowledge    representation as a means to provide a semantically wellfounded basis    for the structural aspects of knowledge representation systems the    description logic considered in this paper is a subset of first order    logic with nice computational characteristics  it is quite expressive    and features a novel combination of constructs that has not been    studied before  the distinguishing constructs are number    restrictions which generalize existence and functional dependencies    inverse roles which allow one to refer to the inverse of a    relationship and possibly cyclic assertions which are necessary for    capturing real world domains  we are able to show that it is    precisely such combination of constructs that makes our logic powerful    enough to model the essential set of features for defining class    structures that are common to frame systems objectoriented database    languages and semantic data models  as a consequence of the    established correspondences several significant extensions of each of    the above formalisms become available the high expressiveness of the    logic we propose and the need for capturing the reasoning in different    contexts forces us to distinguish between unrestricted and finite    model reasoning  a notable feature of our proposal is that reasoning    in both cases is decidable  we argue that by virtue of the high    expressive power and of the associated reasoning capabilities on both    unrestricted and finite models our logic provides a common core for    classbased representation formalisms





s  soderland and  w  lehnert 1994 wrapup a trainable discourse module for information extraction volume 2 pages 131158



the vast amounts of online text now available have ledto   renewed interest in information extraction ie systems thatanalyze   unrestricted text producing a structured representation ofselected   information from the text this paper presents a novel approachthat   uses machine learning to acquire knowledge for some of the higher   level ie processing  wrapup is a trainable ie discourse component   that makes intersentential inferences and identifies logicalrelations   among information extracted from the text  previous corpusbased   approaches were limited to lower level processing such as   partofspeech tagging lexical disambiguation and dictionary   construction  wrapup is fully trainable and not onlyautomatically   decides what classifiers are needed but even derives the featureset   for each classifier automatically performance equals that of a   partially trainable discourse module requiring manual customization   for each domain





e  di sciascio  f  m donini and  m  mongiello 2002 structured knowledge representation for image retrieval volume 16 pages 209257



we propose a structured approach to the problem of retrieval    of images by content and present a description logic that has been    devised for the semantic indexing and retrieval of images containing    complex objects        as other approaches do we start from lowlevel features extracted    with image analysis to detect and characterize regions in an    image however in contrast with featurebased approaches we provide    a syntax to describe segmented regions as basic objects and complex    objects as compositions of basic ones then we introduce a companion    extensional semantics for defining reasoning services such as    retrieval classification and subsumption  these services can be    used for both exact and approximate matching using similarity    measures           using our logical approach as a formal specification we implemented a    complete clientserver image retrieval system which allows a user to    pose both queries by sketch and queries by example a set of    experiments has been carried out on a testbed of images to assess the    retrieval capabilities of the system in comparison with expert users    ranking results are presented adopting a wellestablished measure of    quality borrowed from textual information retrieval





r  nock 2002 inducing interpretable voting classifiers without trading accuracy for simplicity theoretical results approximation algorithms volume 17 pages 137170



recent advances in the study of voting classification    algorithms have brought empirical and theoretical results clearly    showing the discrimination power of ensemble classifiers it has been    previously argued that the search of this classification power in the    design of the algorithms has marginalized the need to obtain    interpretable classifiers therefore the question of whether one    might have to dispense with interpretability in order to keep    classification strength is being raised in a growing number of machine    learning or data mining papers the purpose of this paper is to study    both theoretically and empirically the problem first we provide    numerous results giving insight into the hardness of the    simplicityaccuracy tradeoff for voting classifiers then we provide    an efficient topdown and prune induction heuristic widc mainly    derived from recent results on the weak learning and boosting    frameworks  it is to our knowledge the first attempt to build a    voting classifier as a base formula using the weak learning framework    the one which was previously highly successful for decision tree    induction and not the strong learning framework as usual for such    classifiers with boostinglike approaches while it uses a wellknown    induction scheme previously successful in other classes of concept    representations thus making it easy to implement and compare widc    also relies on recent or new results we give about particular cases of    boosting known as partition boosting and ranking loss    boosting experimental results on thirtyone domains most of which    readily available tend to display the ability of widc to produce    small accurate and interpretable decision committees





honorable mention for the 2014 ijcaijair best paper prize



this paper introduces a principled approach for the design of a scalable general reinforcement learning agent our approach is based on a direct approximation of aixi a bayesian optimality notion for general reinforcement learning agents previously it has been unclear whether the theory of aixi could motivate the design of practical algorithms we answer this hitherto open question in the affirmative by providing the first computationally feasible approximation to the aixi agent to develop our approximation we introduce a new montecarlo tree search algorithm along with an agentspecific extension to the context tree weighting algorithm empirically we present a set of encouraging results on a variety of stochastic and partially observable domains we conclude by proposing a number of directions for future research





c  g giraudcarrier and  t  r martinez 1995 an integrated framework for learning and reasoning volume 3 pages 147185



learning and reasoning are both aspects of what is    considered to be intelligence their studies within ai have been    separated historically learning being the topic of machine learning    and neural networks and reasoning falling under classical or    symbolic ai  however learning and reasoning are in many ways    interdependent this paper discusses the nature of some of these    interdependencies and proposes a general framework called flare that    combines inductive learning using prior knowledge together with    reasoning in a propositional setting several examples that test the    framework are presented including classical induction many important    reasoning protocols and two simple expert systems



we consider online planning in markov decision processes mdps in online planning the agent focuses on its current state only deliberates about the set of possible policies from that state onwards and when interrupted uses the outcome of that exploratory deliberation to choose what action to perform next formally the performance of algorithms for online planning is assessed in terms of simple regret the agents expected performance loss when the chosen action rather than an optimal one is followed

to date stateoftheart algorithms for online planning in general mdps are either best effort or guarantee only polynomialrate reduction of simple regret over time here we introduce a new montecarlo tree search algorithm brue that guarantees exponentialrate and smooth reduction of simple regret  at a high level brue is based on a simple yet nonstandard statespace sampling scheme mcts2e in which different parts of each sample are dedicated to different exploratory objectives we further extend brue with a variant of learning by forgetting the resulting parametrized algorithm bruealpha exhibits even more attractive formal guarantees than brue our  empirical evaluation shows that both brue and its generalization bruealpha are also very effective in practice and  compare favorably to the stateoftheart









j  r quinlan 1996 learning firstorder definitions of functions volume 5 pages 139161



firstorder learning involves finding a clauseform    definition of a relation from examples of the relation and relevant    background information  in this paper a particular firstorder    learning system is modified to customize it for finding definitions of    functional relations  this restriction leads to faster learning times    and in some cases to definitions that have higher predictive    accuracy  other firstorder learning systems might benefit from    similar specialization





m  goldenberg a  felner r  stern g  sharon n  sturtevant r  c holte and j  schaeffer 2014 enhanced partial expansion a volume 50 pages 141187



when solving instances of problem domains that feature a large branching factor a may generate a large number of nodes whose cost is greater than the cost of the optimal solution we designate such nodes as surplus generating surplus nodes and adding them to the open list may dominate both time and memory of the search a recently introduced variant of a called partial expansion a pea deals with the memory aspect of this problem when expanding a node n pea generates all of its children and puts into open only the children with f  f n n is reinserted in the open list with the f cost of the best discarded child this guarantees that surplus nodes are not inserted into open

in this paper we present a novel variant of a called enhanced partial expansion a epea that advances the idea of pea to address the time aspect given a priori domain and heuristic specific knowledge epea generates only the nodes with f  fn although epea is not always applicable or practical we study several variants of epea which make it applicable to a large number of domains and heuristics in particular the ideas of epea are applicable to ida and to the domains where pattern databases are traditionally used experimental studies show significant improvements in runtime and memory performance for several standard benchmark applications we provide several theoretical studies to facilitate an understanding of the new algorithm





c  v goldman and s  zilberstein 2008 communicationbased decomposition mechanisms for decentralized mdps volume 32 pages 169202



multiagent planning in stochastic environments can be framed formally as a decentralized markov decision problem many reallife distributed problems that arise in manufacturing multirobot coordination and information gathering scenarios can be formalized using this framework however finding the optimal solution in the general case is hard limiting the applicability of recently developed algorithms this paper provides a practical approach for solving decentralized control problems when communication among the decision makers is possible but costly  we develop the notion of communicationbased mechanism that allows us to decompose a decentralized mdp into multiple singleagent problems in this framework referred to as decentralized semimarkov decision process with direct communication decsmdpcom agents operate separately between communications we show that finding an optimal mechanism is equivalent to solving optimally a decsmdpcom we also provide a heuristic search algorithm that converges on the optimal decomposition  restricting the decomposition to some specific types of local behaviors reduces significantly the complexity of planning in particular we present a polynomialtime algorithm for the case in which individual agents perform goaloriented behaviors between communications the paper concludes with an additional tractable algorithm that enables the introduction of human knowledge thereby reducing the overall problem to finding the best time to communicate empirical results show that these approaches provide good approximate solutions





b  srivastava and  s  kambhampati 1998 synthesizing customized planners from specifications volume 8 pages 93128



existing plan synthesis approaches in artificial    intelligence fall into two categories  domain independent and domain    dependent  the domain independent approaches are applicable across a    variety of domains but may not be very efficient in any one given    domain  the domain dependent approaches need to be redesigned for    each domain separately but can be very efficient in the domain for    which they are designed  one enticing alternative to these approaches    is to automatically synthesize domain independent planners given the    knowledge about the domain and the theory of planning in this paper    we investigate the feasibility of using existing automated software    synthesis tools to support such synthesis specifically we describe    an architecture called clay in which the kestrel interactive    development system kids is used to derive a domaincustomized    planner through a semiautomatic combination of a declarative theory    of planning and the declarative control knowledge specific to a given    domain to semiautomatically combine them to derive domaincustomized    planners  we discuss what it means to write a declarative theory of    planning and control knowledge for kids and illustrate our approach    by generating a class of domainspecific planners using state space    refinements  our experiments show that the synthesized planners can    outperform classical refinement planners implemented as    instantiations of ucp kambhampati  srivastava 1995 using the same    control knowledge  we will contrast the costs and benefits of the    synthesis approach with conventional methods for customizing domain    independent planners





j  r quinlan 1996 improved use of continuous attributes in c45 volume 4 pages 7790



a reported weakness of c45 in domains with continuous    attributes is addressed by modifying the formation and evaluation of    tests on continuous attributes  an mdlinspired penalty is applied to    such tests eliminating some of them from consideration and altering    the relative desirability of all tests  empirical trials show that    the modifications lead to smaller decision trees with higher    predictive accuracies  results also confirm that a new version of    c45 incorporating these changes is superior to recent approaches that    use global discretization and that construct small trees with    multiinterval splits





s  p gujar and y  narahari 2011 redistribution mechanisms for assignment of heterogeneous objects volume 41 pages 131154



there are p heterogeneous objects to be assigned to n competing agents n  p each with unit demand it is required to design a groves mechanism for this assignment problem satisfying weak budget balance individual rationality and minimizing the budget imbalance this calls for designing an appropriate rebate function when the objects are identical this problem has been solved which we refer as wco mechanism we measure the performance of such mechanisms by the redistribution index we first prove an impossibility theorem which rules out linear rebate functions with nonzero redistribution index in heterogeneous object assignment motivated by this theorem we explore two approaches to get around this impossibility in the first approach we show that linear rebate functions with nonzero redistribution index are possible when the valuations for the objects have a certain type of relationship and we design a mechanism with linear rebate function that is worst case optimal in the second approach we show that rebate functions with nonzero efficiency are possible if linearity is relaxed we extend the rebate functions of the wco mechanism to heterogeneous objects assignment and conjecture them to be worst case optimal





c  boutilier and  r  i brafman 2001 partialorder planning with concurrent interacting actions volume 14 pages 105136



in order to generate plans for agents with multiple    actuators agent teams or distributed controllers we must be able to    represent and plan using concurrent actions with interacting    effects this has historically been considered a challenging task    requiring a temporal planner with the ability to reason explicitly    about time we show that with simple modifications the strips action    representation language can be used to represent interacting actions    moreover algorithms for partialorder planning require only small    modifications in order to be applied in such multiagent domains  we    demonstrate this fact by developing a sound and complete partialorder    planner for planning with concurrent interacting actions pomp that    extends existing partialorder planners in a straightforward    way these results open the way to the use of partialorder planners    for the centralized control of cooperative multiagent systems



we propose a novel languageindependent approach for improving machine translation for resourcepoor languages by exploiting their similarity to resourcerich ones more precisely we improve the translation from a resourcepoor source language x1 into a resourcerich language y given a bitext containing a limited number of parallel sentences for x1y and a larger bitext for x2y for some resourcerich language x2 that is closely related to x1 this is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages x1 and x2 in spelling word order and syntax offer 1 we improve the word alignments for the resourcepoor language 2 we further augment it with additional translation options and 3 we take care of potential spelling differences through appropriate transliteration the evaluation for indonesian english using malay and for spanish  english using portuguese and pretending spanish is resourcepoor shows an absolute gain of up to 135 and 337 bleu points respectively which is an improvement over the best rivaling approaches while using much less additional data overall our method cuts the amount of necessary real training data by a factor of 25











t  heskes 2006 convexity arguments for efficient minimization of the bethe and kikuchi free energies volume 26 pages 153190

