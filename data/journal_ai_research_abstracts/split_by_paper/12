code optimization and high level synthesis can be posed as constraint satisfaction and optimization problems  such as graph coloring used in register allocation  graph coloring is also used to model more traditional csps relevant to ai such as planning timetabling and scheduling  provably optimal solutions may be desirable for commercial and defense applications additionally for applications such as register allocation and code optimization naturallyoccurring instances    of graph coloring are often small and can be solved optimally a recent  wave of improvements in algorithms for boolean satisfiability sat and 01 integer linear programming ilp suggests generic problemreduction  methods rather than problemspecific heuristics because 1 heuristics may be upset by new constraints 2 heuristics tend to ignore structure and 3 many relevant problems are provably inapproximable 



as examples such as the monty hall puzzle show applying conditioning to update a probability distribution on a naive space which does not take into account the protocol used can often lead to counterintuitive results  here we examine why  a criterion known as car coarsening at random in the statistical literature characterizes when naive conditioning in a naive space works  we show that the car condition holds rather infrequently and we provide a procedural characterization of it by giving a randomized algorithm that generates all and only distributions for which car holds this substantially extends previous characterizations of car  we also consider more generalized notions of update such as jeffrey conditioning and minimizing relative entropy mre  we give a generalization of the car condition that characterizes when jeffrey conditioning leads to appropriate answers and show that there exist some very simple settings in which mre essentially never gives the right results  this generalizes and interconnects previous results obtained in the literature on car and mre









g  stoilos g  stamou j  z pan v  tzouvaras and i  horrocks 2007 reasoning with very expressive fuzzy description logics volume 30 pages 273320



it is widely recognized today that the management of imprecision and vagueness will yield more intelligent and realistic knowledgebased applications description logics dls are a family of knowledge representation languages that have gained considerable attention the last decade mainly due to their decidability and the existence of empirically high performance of reasoning algorithms in this paper we extend the well known fuzzy alc dl to the fuzzy shin dl which extends the fuzzy alc dl with transitive role axioms s inverse roles i role hierarchies h and number restrictions n we illustrate why transitive role axioms are difficult to handle in the presence of fuzzy interpretations and how to handle them properly then we extend these results by adding role hierarchies and finally number restrictions the main contributions of the paper are the decidability proof of the fuzzy dl languages fuzzysi and fuzzyshin as well as decision procedures for the knowledge base satisfiability problem of the fuzzysi and fuzzyshin



decentralized planning in uncertain environments is a complex task generally dealt with by using a decisiontheoretic approach mainly through the framework of decentralized partially observable markov decision processes decpomdps although decpomdps are a general and powerful modeling tool solving them is a task with an overwhelming complexity that can be doubly exponential in this paper we study an alternate formulation of decpomdps relying on a sequenceform representation of policies from this formulation we show how to derive mixed integer linear programming milp problems that once solved give exact optimal solutions to the decpomdps we show that these milps can be derived either by using some combinatorial characteristics of the optimal solutions of the decpomdps or by using concepts borrowed from game theory through an experimental validation on classical test problems from the decpomdp literature we compare our approach to existing algorithms results show that mathematical programming outperforms dynamic programming but is less efficient than forward search except for some particular problems

the main contributions of this work are the use of mathematical programming for decpomdps and a better understanding of decpomdps and of their solutions besides we argue that our alternate representation of decpomdps could be helpful for designing novel algorithms looking for approximate solutions to decpomdps







many current largescale multiagent team implementations can be characterized as following the beliefdesireintention bdi paradigm with explicit representation of team plans  despite their promise current bdi team approaches lack tools for quantitative performance analysis under uncertainty distributed partially observable markov decision problems pomdps are well suited for such analysis but the complexity of finding optimal policies in such models is highly intractable the key contribution of this article is a hybrid bdipomdp approach where bdi team plans are exploited to improve pomdp tractability and pomdp analysis improves bdi team plan performance  concretely we focus on role allocation a fundamental problem in bdi  teams which agents to allocate to the different roles in the team the  article provides three key contributions first we describe a role  allocation technique that takes into account future uncertainties in the domain prior work in multiagent role allocation has failed to address such uncertainties to that end we introduce rmtdp rolebased markov team decision problem a new distributed pomdp model for analysis of role allocations our technique gains in tractability by significantly curtailing rmtdp policy search in particular bdi team plans provide incomplete rmtdp policies and the rmtdp policy search fills the gaps in such incomplete policies by searching for the best role allocation our second key contribution is a novel decomposition technique to  further improve rmtdp policy search efficiency even though limited  to searching role allocations there are still combinatorially many  role allocations and evaluating each in rmtdp to identify the best  is extremely difficult our decomposition technique exploits the  structure in the bdi team plans to significantly prune the search space of role allocations our third key contribution is a  significantly faster policy evaluation algorithm suited for  our bdipomdp hybrid approach finally we also present experimental  results from two domains mission rehearsal simulation and  robocuprescue disaster rescue simulation

















l  liu and m  truszczynski 2006 properties and applications of programs with monotone and convex constraints volume 27 pages 299334



we present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus the approach is based on formal concept analysis fca a method mainly used for the analysis of data ie for investigating and processing explicitly given information  we follow harris distributional hypothesis and model the context of a certain term as a vector representing syntactic dependencies which are automatically acquired from the text corpus with a linguistic parser  on the basis of this context information fca produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy  the approach is evaluated by comparing the resulting concept hierarchies with handcrafted taxonomies for two domains tourism and finance  we also directly compare our approach with hierarchical agglomerative clustering as well as with bisectionkmeans as an instance of a divisive clustering algorithm furthermore we investigate the impact of using different measures weighting the contribution of each attribute as well as of applying a particular smoothing technique to cope with data sparseness







unary operator domains  ie domains in which operators    have a single effect  arise naturally in many control problems  in    its most general form the problem of strips planning in unary    operator domains is known to be as hard as the general strips planning    problem  both are pspacecomplete however unary operator domains    induce a natural structure called the domains causal graph this    graph relates between the preconditions and effect of each domain    operator causal graphs were exploited by williams and nayak in order    to analyze plan generation for one of the controllers in nasas    deepspace one spacecraft there they utilized the fact that when    this graph is acyclic a serialization ordering over any subgoal can    be obtained quickly in this paper we conduct a comprehensive study of    the relationship between the structure of a domains causal graph and    the complexity of planning in this domain  on the positive side we    show that a nontrivial polynomial time plan generation algorithm    exists for domains whose causal graph induces a polytree with a    constant bound on its node indegree on the negative side we show    that even plan existence is hard when the graph is a directedpath    singly connected dag  more generally we show that the number of    paths in the causal graph is closely related to the complexity of    planning in the associated domain  finally we relate our results to    the question of complexity of planning with serializable subgoals







stacked generalization is a general method of using a    highlevel model to combine lowerlevel models to achieve greater    predictive accuracy  in this paper we address two crucial issues    which have been considered to be a black art in classification tasks    ever since the introduction of stacked generalization in 1992 by    wolpert the type of generalizer that is suitable to derive the    higherlevel model and the kind of attributes that should be used as    its input  we find that best results are obtained when the    higherlevel model combines the confidence and not just the    predictions of the lowerlevel ones   we demonstrate the effectiveness of stacked generalization for combining     three different types of learning algorithms for classification tasks    we also compare the performance of stacked generalization with    majority vote and published results of arcing and bagging







zhiqiang  zhuang zhe  wang kewen  wang and guilin  qi 2016 dllite contraction and revision volume 56 pages 329378



two essential tasks in managing description logic knowledge bases are eliminating problematic axioms and incorporating newly formed ones such elimination and incorporation are formalised as the operations of contraction and revision in belief change in this paper we deal with contraction and revision for the dllite family through a modeltheoretic approach standard description logic semantics yields an infinite number of models for dllite knowledge bases thus it is difficult to develop algorithms for contraction and revision that involve dl models  the key to our approach is the introduction of an alternative semantics called type semantics which can replace the standard semantics in characterising the standard inference tasks of dllite type semantics has several advantages over the standard one it is more succinct and importantly with a finite signature the semantics always yields a finite number of models we then define modelbased contraction and revision functions for dllite knowledge bases under type semantics and provide representation theorems for them finally the finiteness and succinctness of type semantics allow us to develop tractable algorithms for instantiating the functions





c  yuan h  lim and t  lu 2011 most relevant explanation in bayesian networks volume 42 pages 309352



a major inference task in bayesian networks is explaining why some variables are observed in their particular states using a set of target variables existing methods for solving this problem often generate explanations that are either too simple underspecified or too complex overspecified in this paper we introduce a method called most relevant explanation mre which finds a partial instantiation of the target variables that maximizes the generalized bayes factor gbf as the best explanation for the given evidence our study shows that gbf has several theoretical properties that enable mre to automatically identify the most relevant target variables in forming its explanation in particular conditional bayes factor cbf defined as the gbf of a new explanation conditioned on an existing explanation provides a soft measure on the degree of relevance of the variables in the new explanation in explaining the evidence given the existing explanation as a result mre is able to automatically prune less relevant variables from its explanation we also show that cbf is able to capture well the explainingaway phenomenon that is often represented in bayesian networks moreover we define two dominance relations between the candidate solutions and use the relations to generalize mre to find a set of top explanations that is both diverse and representative case studies on several benchmark diagnostic bayesian networks show that mre is often able to find explanatory hypotheses that are not only precise but also concise





a  kakas p  mancarella f  sadri k  stathis and f  toni 2008 computational logic foundations of kgp agents volume 33 pages 285348



this paper presents the computational logic foundations of a model of agency called the kgp knowledge goals and plan model this model allows the specification of heterogeneous agents that can interact with each other and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments  kgp provides a highly modular agent architecture that integrates a collection of reasoning and physical capabilities synthesised within transitions that update the agents state in response to reasoning sensing and acting transitions are orchestrated by cycle theories that specify the order in which transitions are executed while taking into account the dynamic context and agent preferences as well as selection operators for providing inputs to transitions





we present a new approach to path planning called the    ariadnes clew algorithm it is designed to find paths in    highdimensional continuous spaces and applies to robots with many    degrees of freedom in static as well as dynamic environments  ones    where obstacles may move the ariadnes clew algorithm comprises two    subalgorithms called search and explore applied in an interleaved    manner explore builds a representation of the accessible space while    search looks for the target both are posed as optimization problems    we describe a real implementation of the algorithm to plan paths for a    six degrees of freedom arm in a dynamic environment where another six    degrees of freedom arm is used as a moving obstacle experimental    results show that a path is found in about one second without any    preprocessing







the theoretical properties of qualitative spatial reasoning    in the rcc8 framework have been analyzed extensively however no    empirical investigation has been made yet our experiments show that    the adaption of the algorithms used for qualitative temporal reasoning    can solve large rcc8 instances even if they are in the phase    transition region  provided that one uses the maximal tractable    subsets of rcc8 that have been identified by us in particular we    demonstrate that the orthogonal combination of heuristic methods is    successful in solving almost all apparently hard instances in the    phase transition region up to a certain size in reasonable time









m  feldman and t  tamir 2009 approximate strong equilibrium in job scheduling games volume 36 pages 387414



a nash equilibrium ne is a strategy profile resilient to unilateral deviations and is predominantly used in the analysis of multiagent systems a downside of ne is that it is not necessarily stable against deviations by coalitions yet as we show in this paper in some cases ne does exhibit stability against coalitional deviations in that the benefits from a joint deviation are bounded in this sense ne approximates strong equilibrium

our results indicate that lpt performs better than a general ne  however lpt is not the best possible approximation in particular we present a polynomial time approximation scheme ptas for the makespan minimization problem which provides a schedule with irmin of 1epsilon for any given epsilon with respect to computational complexity we show that given an ne on m  3 identical machines or m  2 unrelated machines it is nphard to determine whether a given coalition can deviate such that every member decreases its cost



the local search algorithm wsat is one of the most    successful algorithms for solving the satisfiability sat problem it    is notably effective at solving hard random 3sat instances near the    socalled satisfiability threshold but still shows a peak in search    cost near the threshold and large variations in cost over different    instances we make a number of significant contributions to the    analysis of wsat on highcost random instances using the    recentlyintroduced concept of the backbone of a sat instance the    backbone is the set of literals which are entailed by an instance we    find that the number of solutions predicts the cost well for    smallbackbone instances but is much less relevant for the    largebackbone instances which appear near the threshold and dominate    in the overconstrained region we show a very strong correlation    between search cost and the hamming distance to the nearest solution    early in wsats search this pattern leads us to introduce a measure    of the backbone fragility of an instance which indicates how    persistent the backbone is as clauses are removed we propose that    highcost random instances for local search are those with very large    backbones which are also backbonefragile we suggest that the decay    in cost beyond the satisfiability threshold is due to increasing    backbone robustness the opposite of backbone fragility our    hypothesis makes three correct predictions first that the backbone    robustness of an instance is negatively correlated with the local    search cost when other factors are controlled for second that    backboneminimal instances which are 3sat instances altered so as to    be more backbonefragile are unusually hard for wsat third that the    clauses most often unsatisfied during search are those whose deletion    has the most effect on the backbone in understanding the pathologies    of local search methods we hope to contribute to the development of    new and better techniques









p  r costa and l  m botelho 2013 learning by observation of agent software images volume 47 pages 313349



learning by observation can be of key importance whenever agents sharing similar features want to learn from each other this paper presents an agent architecture that enables software agents to learn by direct observation of the actions executed by expert agents while they are performing a task this is possible because the proposed architecture displays information that is essential for observation making it possible for software agents to observe each other 

results show that agents are able to learn in conditions where common supervised learning algorithms fail such as when agents do not know the results of their actions a priori or when not all the effects of the actions are visible the results also show that our approach provides better results than other learning methods since it requires shorter learning periods



in todays world we follow news which is distributed globally significant events are reported by different sources and in different languages in this work we address the problem of tracking of events in a large multilingual stream within a recently developed system event registry we examine two aspects of this problem how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event  taking a multilingual stream and clusters of articles from each language we compare different crosslingual document similarity measures based on wikipedia this allows us to compute the similarity of any two articles regardless of language building on previous work we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data using this capability we then propose an approach to link clusters of articles across languages which represent the same event we provide an extensive evaluation of the system as a whole as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm









diederik  marijn roijers shimon  whiteson and frans  a oliehoek 2015 computing convex coverage sets for faster multiobjective coordination volume 52 pages 399443



in this article we propose new algorithms for multiobjective coordination graphs mocogs key to the efficiency of these algorithms is that they compute a convex coverage set ccs instead of a pareto coverage set pcs not only is a ccs a sufficient solution set for a large class of problems it also has important characteristics that facilitate more efficient solutions we propose two main algorithms for computing a ccs in mocogs convex multiobjective variable elimination cmove computes a ccs by performing a series of agent eliminations which can be seen as solving a series of local multiobjective subproblems variable elimination linear support vels iteratively identifies the single weight vector w that can lead to the maximal possible improvement on a partial ccs and calls variable elimination to solve a scalarized instance of the problem for w vels is faster than cmove for small and medium numbers of objectives and can compute an 949approximate ccs in a fraction of the runtime in addition we propose variants of these methods that employ andor tree search instead of variable elimination to achieve memory efficiency we analyze the runtime and space complexities of these methods prove their correctness and compare them empirically against a naive baseline and an existing pcs method both in terms of memoryusage and runtime  our results show that by focusing on the ccs these methods achieve much better scalability in the number of agents than the current state of the art



in the recent years several research efforts have focused on the concept of time granularity and its applications a first stream of research investigated the mathematical models behind the notion of granularity and the algorithms to manage temporal data based on those models a second stream of research investigated symbolic formalisms providing a set of algebraic operators to define granularities in a compact and compositional way however only very limited manipulation algorithms have been proposed to operate directly on the algebraic representation making it unsuitable to use the symbolic formalisms in applications that need manipulation of granularities

from a technical point of view we propose an hybrid algorithm that interleaves the conversion of calendar subexpressions into periodical sets with the minimization of the period length the algorithm returns setbased granularity representations having minimal period length which is the most relevant parameter for the performance of the considered reasoning services extensive experimental work supports the techniques used in the algorithm and shows the efficiency and effectiveness of the algorithm







p  doshi and p  j gmytrasiewicz 2009 monte carlo sampling methods for approximating interactive pomdps volume 34 pages 297337



partially observable markov decision processes pomdps provide a principled framework for sequential planning in uncertain single agent settings an extension of pomdps to multiagent settings called interactive pomdps ipomdps replaces pomdp belief spaces with interactive hierarchical belief systems which represent an agents belief about the physical world about beliefs of other agents and about their beliefs about others beliefs this modification makes the difficulties of obtaining solutions due to complexity of the belief and policy spaces even more acute we describe a general method for obtaining approximate solutions of ipomdps based on particle filtering pf we introduce the interactive pf which descends the levels of the interactive belief hierarchies and samples and propagates beliefs at each level the interactive pf is able to mitigate the belief space complexity but it does not address the policy space complexity to mitigate the policy space complexity  sometimes also called the curse of history  we utilize a complementary method based on sampling likely observations while building the look ahead reachability tree while this approach does not completely address the curse of history it beats back the curses impact substantially we provide experimental results and chart future work









a  m rush and m  j  collins 2012 a tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing volume 45 pages 305362



b  ten cate e  franconi and i  seylan 2013 beth definability in expressive description logics volume 48 pages 347414



the beth definability property a wellknown property from classical logic is investigated in the context of description logics if a general ltbox implicitly defines an lconcept in terms of a given signature where l is a description logic then does there always exist over this signature an explicit definition in l for the concept this property has been studied before and used to optimize reasoning in description logics in this paper a complete classification of beth definability is provided for extensions of the basic description logic alc with transitive roles inverse roles role hierarchies andor functionality restrictions both on arbitrary and on finite structures moreover we present a tableaubased algorithm which computes explicit definitions of at most double exponential size this algorithm is optimal because it is also shown that the smallest explicit definition of an implicitly defined concept may be double exponentially long in the size of the input tbox finally if explicit definitions are allowed to be expressed in firstorder logic then we show how to compute them in single exponential time







b  cuenca grau i  horrocks y  kazakov and u  sattler 2008 modular reuse of ontologies theory and practice volume 31 pages 273318



in this paper we propose a set of tasks that are relevant for the modular reuse of ontologies in order to formalize these tasks as reasoning problems we introduce the notions of conservative extension safety and module for a very general class of logicbased ontology languages we investigate the general properties of and relationships between these notions  and study the relationships between the relevant reasoning problems we have previously identified to study the computability of these problems we   consider in particular description logics dls which provide the formal underpinning of the w3c web ontology language owl and show that all the  problems we consider are undecidable or algorithmically unsolvable for the description logic underlying owl dl  in order to achieve a practical solution we identify conditions sufficient for an ontology to reuse a set of symbols safelythat is without changing their meaning we provide the notion of a safety class which characterizes any sufficient condition for safety and identify a family of safety classescalled localitywhich enjoys a collection of desirable properties  we use the notion of a safety class to extract modules from ontologies and we provide various modularization algorithms that are appropriate to the properties of the particular safety class in use  finally we show practical benefits of our safety checking and module extraction algorithms



timothy  a mann shie  mannor and doina  precup 2015 approximate value iteration with temporally extended actions volume 53 pages 375438



temporally extended actions have proven useful for reinforcement learning but their duration also makes them valuable for efficient planning the options framework provides a concrete way to implement and reason about temporally extended actions existing literature has demonstrated the value of planning with options empirically but there is a lack of theoretical analysis formalizing when planning with options is more efficient than planning with primitive actions we provide a general analysis of the convergence rate of a popular approximate value iteration avi algorithm called fitted value iteration fvi with options our analysis reveals that longer duration options and a pessimistic estimate of the value function both lead to faster convergence furthermore options can improve convergence even when they are suboptimal and sparsely distributed throughout the statespace next we consider the problem of generating useful options for planning based on a subset of landmark states this suggests a new algorithm landmarkbased avi lavi that represents the value function only at the landmark states we analyze both fvi and lavi using the proposed landmarkbased options and compare the two algorithms our experimental results in three different domains demonstrate the key properties from the analysis our theoretical and experimental results demonstrate that options can play an important role in avi by decreasing approximation error and inducing fast convergence





partially observable markov decision processes pomdps are    a natural model for planning problems where effects of actions are    nondeterministic and the state of the world is not completely    observable  it is difficult to solve pomdps exactly  this paper    proposes a new approximation scheme  the basic idea is to transform a    pomdp into another one where additional information is provided by an    oracle the oracle informs the planning agent that the current state    of the world is in a certain region  the transformed pomdp is    consequently said to be region observable it is easier to solve than    the original pomdp  we propose to solve the transformed pomdp and use    its optimal policy to construct an approximate policy for the original    pomdp  by controlling the amount of additional information that the    oracle provides it is possible to find a proper tradeoff between    computational time and approximation quality  in terms of algorithmic    contributions we study in details how to exploit region observability    in solving the transformed pomdp to facilitate the study we also    propose a new exact algorithm for general pomdps  the algorithm is    conceptually simple and yet is significantly more efficient than all    previous exact algorithms







aaai 2010 outstanding paper award



planning as satisfiability is a principal approach to planning with many eminent advantages the existing planning as satisfiability techniques usually use encodings compiled from strips we introduce a novel sat encoding scheme sase based on the sas formalism the new scheme exploits the structural information in sas resulting in an encoding that is both more compact and efficient for planning we prove the correctness of the new encoding by establishing an isomorphism between the solution plans of sase and that of strips based encodings we further analyze the transition variables newly introduced in sase to explain why it accommodates modern sat solving algorithms and improves performance we give empirical statistical results to support our analysis we also develop a number of techniques to further reduce the encoding size of sase and conduct experimental studies to show the strength of each individual technique finally we report extensive experimental results to demonstrate significant improvements of sase over the stateoftheart strips based encoding schemes in terms of both time and memory efficiency





b  banerjee and b  chandrasekaran 2010 a constraint satisfaction framework for executing perceptions and actions in diagrammatic reasoning volume 39 pages 373427



diagrammatic reasoning dr is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on languagelike representations the research reported in this paper is a contribution to building a general purpose dr system as an extension to a soarlike problem solving architecture the work is in a framework in which dr is modeled as a process where subtasks are solved as appropriate either by inference from symbolic representations or by interaction with a diagram ie perceiving specified information from a diagram or modifyingcreating objects in a diagram in specified ways according to problem solving needs the perceptions and actions in most dr systems built so far are handcoded for the specific application even when the rest of the system is built using the general architecture the absence of a general framework for executing perceptionsactions poses as a major hindrance to using them opportunistically  the essence of openended search in problem solving

our goal is to develop a framework for executing a wide variety of specified perceptions and actions across tasksdomains without human intervention we observe that the domaintaskspecific visual perceptionsactions can be transformed into domaintaskindependent spatial problems we specify a spatial problem as a quantified constraint satisfaction problem in the real domain using an openended vocabulary of properties relations and actions involving three kinds of diagrammatic objects  points curves regions solving a spatial problem from this specification requires computing the equivalent simplified quantifierfree expression the complexity of which is inherently doubly exponential we represent objects as configuration of simple elements to facilitate decomposition of complex problems into simpler and similar subproblems we show that if the symbolic solution to a subproblem can be expressed concisely quantifiers can be eliminated from spatial problems in loworder polynomial time using similar previously solved subproblems this requires determining the similarity of two problems the existence of a mapping between them computable in polynomial time and designing a memory for storing previously solved problems so as to facilitate search the efficacy of the idea is shown by time complexity analysis we demonstrate the proposed approach by executing perceptions and actions involved in dr tasks in two army applications







t  eiter m  fink t  krennwallner c  redl and p  sch252ller 2014 efficient hexprogram evaluation based on unfounded sets volume 49 pages 269321



hexprograms extend logic programs under the answer set semantics with external computations through external atoms as reasoning from ground horn programs with nonmonotonic external atoms of polynomial complexity is already on the second level of the polynomial hierarchy minimality checking of answer set candidates needs special attention to this end we present an approach based on unfounded sets as a generalization of related techniques for asp programs the unfounded set detection is expressed as a propositional sat problem for which we provide two different encodings and optimizations to them we then integrate our approach into a previously developed evaluation framework for hexprograms which is enriched by additional learning techniques that aim at avoiding the reconstruction of the same or related unfounded sets furthermore we provide a syntactic criterion that allows one to skip the minimality check in many cases an experimental evaluation shows that the new approach significantly decreases runtime



the model checking integrated planning system mips is a temporal least commitment heuristic search planner based on a flexible objectoriented workbench architecture its design clearly separates explicit and symbolic directed exploration algorithms from the set of online and offline computed estimates and associated data structures   mips has shown distinguished performance in the last two international planning competitions in the last event the description language was extended from pure propositional planning to include numerical state variables action durations and plan quality objective functions plans were no longer sequences of actions but timestamped schedules   as a participant of the fully automated track of the competition mips has proven to be a general system in each track and every benchmark domain it efficiently computed plans of remarkable quality this article introduces and analyzes the most important algorithmic novelties that were necessary to tackle the new layers of expressiveness in the benchmark problems and to achieve a high level of performance   the extensions include critical path analysis of sequentially generated plans to generate corresponding optimal parallel plans  the linear time algorithm to compute the parallel plan bypasses known np hardness results for partial ordering by scheduling plans with respect to the set of actions and the imposed precedence relations the efficiency of this algorithm also allows us to improve the exploration guidance for each encountered planning state the corresponding approximate sequential plan is scheduled   one major strength of mips is its static analysis phase that grounds and simplifies parameterized predicates functions and operators that infers knowledge to minimize the state description length and that detects domain object symmetries  the latter aspect is analyzed in detail   mips has been developed to serve as a complete and optimal state space planner with admissible estimates exploration engines and branching cuts in the competition version however certain performance compromises had to be made including floating point arithmetic weighted heuristic search exploration according to an inadmissible estimate and parameterized optimization







abstract we introduce an abductive method for a coherent integration of independent datasources the idea is to compute a list of datafacts that should be inserted to the amalgamated database or retracted from it in order to restore its consistency this method is implemented by an abductive solver called asystem that applies sldnfaresolution on a metatheory that relates different possibly contradicting input databases we also give a pure modeltheoretic analysis of the possible ways to recover consistent data from an inconsistent database in terms of those models of the database that exhibit as minimal inconsistent information as reasonably possible this allows us to characterize the recovered databases in terms of the preferred ie most consistent models of the theory the outcome is an abductivebased application that is sound and complete with respect to a corresponding modelbased preferential semantics and  to the best of our knowledge  is more expressive thus more general than any other implementation of coherent integration of databases







in many multiagent domains a set of agents exert effort towards a joint outcome yet the individual effort levels cannot be easily observed a typical example for such a scenario is routing in communication networks where the sender can only observe whether the packet reached its destination but often has no information about the actions of the intermediate routers which influences the final outcome  we study a setting where a principal needs to motivate a team of agents whose combination of hidden efforts stochastically determines an outcome  in a companion paper we devise and study a  basic combinatorial agency model for this setting where the principal is restricted to inducing a pure nash equilibrium  here we study various implications of this restriction first we show that in contrast to the case of observable efforts inducing a mixedstrategies equilibrium may be beneficial for the principal second we present a sufficient condition for technologies for which no gain can be generated third we bound the principals gain for various families of technologies finally we study the robustness of mixed equilibria to coalitional deviations and the computational hardness of the optimal mixed equilibria







we tackle the problem of planning in nondeterministic    domains by presenting a new approach to conformant planning    conformant planning is the problem of finding a sequence of actions    that is guaranteed to achieve the goal despite the nondeterminism of    the domain our approach is based on the representation of the    planning domain as a finite state automaton we use symbolic model    checking techniques in particular binary decision diagrams to    compactly represent and efficiently search the automaton in this    paper we make the following contributions first we present a general    planning algorithm for conformant planning which applies to fully    nondeterministic domains with uncertainty in the initial condition    and in action effects the algorithm is based on a breadthfirst    backward search and returns conformant plans of minimal length if a    solution to the planning problem exists otherwise it terminates    concluding that the problem admits no conformant solution second we    provide a symbolic representation of the search space based on binary    decision diagrams bdds which is the basis for search techniques    derived from symbolic model checking the symbolic representation    makes it possible to analyze potentially large sets of states and    transitions in a single computation step thus providing for an    efficient implementation  third we present cmbp conformant model    based planner an efficient implementation of the data structures and    algorithm described above directly based on bdd manipulations which    allows for a compact representation of the search layers and an    efficient implementation of the search steps finally we present an    experimental comparison of our approach with the stateoftheart    conformant planners cgp qbfplan and gpt our analysis includes all    the planning problems from the distribution packages of these systems    plus other problems defined to stress a number of specific factors    our approach appears to be the most effective cmbp is strictly more    expressive than qbfplan and cgp and in all the problems where a    comparison is possible cmbp outperforms its competitors sometimes by    orders of magnitude







r  vieira a  f moreira m  wooldridge and r  h bordini 2007 on the formal semantics of speechact based communication in an agentoriented programming language volume 29 pages 221267



a  metodi m  codish and p  j stuckey 2013 boolean equipropagation for concise and efficient sat encodings of  combinatorial problems volume 46 pages 303341



we present an approach to propagationbased sat encoding of combinatorial problems boolean equipropagation where constraints are modeled as boolean functions which propagate information about equalities between boolean literals  this information is then applied to simplify the cnf encoding of the constraints  a key factor is that considering only a small fragment of a constraint model at one time enables us to apply stronger and even complete reasoning to detect equivalent literals in that fragment once detected equivalences apply to simplify the entire constraint model and facilitate further reasoning on other fragments equipropagation in combination with partial evaluation and constraint simplification provide the foundation for a powerful approach to satbased finite domain constraint solving  we introduce a tool called bee bengurion equipropagation encoder based on these ideas and demonstrate for a variety of benchmarks that our approach leads to a considerable reduction in the size of cnf encodings and subsequent speedups in sat solving times









r  daly and q  shen 2009 learning bayesian network equivalence classes with ant colony optimization volume 35 pages 391447





yujiao   zhou bernardo  cuenca grau yavor  nenov mark  kaminski and ian  horrocks 2015 pagoda payasyougo ontology query answering using a datalog reasoner volume 54 pages 309367



answering conjunctive queries over ontologyenriched datasets is a core reasoning task for many applications query answering is however computationally very expensive which has led to the development of query answering procedures that sacrifice either expressive power of the ontology language or the completeness of query answers in order to improve scalability in this paper we describe a hybrid approach to query answering over owl 2 ontologies that combines a datalog reasoner with a fullyfledged owl 2 reasoner in order to provide scalable payasyougo performance the key feature of our approach is that it delegates the bulk of the computation to the datalog reasoner and resorts to expensive owl 2 reasoning only as necessary to fully answer the query furthermore although our main goal  is to efficiently answer queries over owl 2 ontologies and data our technical results are very general and our approach is applicable to firstorder knowledge representation languages that can be captured by rules allowing for existential quantification and disjunction in the head our only assumption is the availability of a datalog reasoner and a fullyfledged reasoner for the language of interest  both of which are used as black boxes we have implemented our techniques in the pagoda system which combines the datalog reasoner rdfox and the  owl 2 reasoner hermit our extensive evaluation shows that pagoda succeeds in providing scalable payasyougo query answering for a wide range of owl 2 ontologies datasets and queries







p  adjiman p  chatalic f  goasdoue m  c rousset and l  simon 2006 distributed reasoning in a peertopeer setting application to the semantic web volume 25 pages 269314



learning the past tense of english verbs  a seemingly minor aspect of language acquisition  has generated heated debates since 1986 and has become a landmark task for testing the adequacy of cognitive modeling several artificial neural networks anns have been implemented and a challenge for better symbolic models has been posed  in this paper we present a generalpurpose symbolic pattern associator spa based upon the decisiontree learning algorithm id3  we conduct extensive headtohead comparisons on the generalization ability between ann models and the spa under different representations we conclude that the spa generalizes the past tense of unseen verbs better than ann models by a wide margin and we offer insights as to why this should be the case  we also discuss a new default strategy for decisiontree learning algorithms







multiagent systems mas promise to offer solutions to    problems where established older paradigms fall short in order to    validate such claims that are repeatedly made in software agent    publications empirical indepth studies of advantages and weaknesses    of multiagent solutions versus conventional ones in practical    applications are needed climate control in large buildings is one    application area where multiagent systems and marketoriented    programming in particular have been reported to be very successful    although central control solutions are still the standard practice    we have therefore constructed and implemented a variety of market    designs for this problem as well as different standard control    engineering solutions this article gives a detailed analysis and    comparison so as to learn about differences between standard versus    agent approaches and yielding new insights about benefits and    limitations of computational markets an important outcome is that    local information plus market communication produces global    control







temporal difference td methods constitute a class of   methods for learning predictions in multistep prediction problems   parameterized by a recency factor lambda currently the most important   application of these methods is to temporal credit assignment in   reinforcement learning well known reinforcement learning algorithms   such as ahc or qlearning may be viewed as instances of td learning   this paper examines the issues of the efficient and general   implementation of tdlambda for arbitrary lambda for use with   reinforcement learning algorithms optimizing the discounted sum of   rewards the traditional approach based on eligibility traces is   argued to suffer from both inefficiency and lack of generality the   ttd truncated temporal differences procedure is proposed as an   alternative that indeed only approximates tdlambda but requires   very little computation per action and can be used with arbitrary   function representation methods  the idea from which it is derived is   fairly simple and not new but probably unexplored so far encouraging   experimental results are presented suggesting that using lambda  0   with the ttd procedure allows one to obtain a significant learning   speedup at essentially the same cost as usual td0 learning







we consider the problem of designing the the utility    functions of the utilitymaximizing agents in a multiagent system so    that they work synergistically to maximize a global utility the    particular problem domain we explore is the control of network routing    by placing agents on all the routers in the network  conventional    approaches to this task have the agents all use the ideal shortest    path routing algorithm ispa  we demonstrate that in many cases due    to the sideeffects of one agents actions on another agents    performance having agents use ispas is suboptimal as far as global    aggregate cost is concerned even when they are only used to route    infinitesimally small amounts of traffic  the utility functions of    the individual agents are not aligned with the global utility    intuitively speaking  as a particular example of this we present an    instance of braess paradox in which adding new links to a network    whose agents all use the ispa results in a decrease in overall    throughput we also demonstrate that loadbalancing in which the    agents decisions are collectively made to optimize the global cost    incurred by all traffic currently being routed is suboptimal as far    as global cost averaged across time is concerned this is also due to    sideeffects in this case of current routing decision on future    traffic the mathematics of collective intelligence coin is    concerned precisely with the issue of avoiding such deleterious    sideeffects in multiagent systems both over time and space we    present key concepts from that mathematics and use them to derive an    algorithm whose ideal version should have better performance than that    of having all agents use the ispa even in the infinitesimal limit we    present experiments verifying this and also showing that a    machinelearningbased version of this coin algorithm in which costs    are only imprecisely estimated via empirical means a version    potentially applicable in the real world also outperforms the ispa    despite having access to less information than does the ispa in    particular this coin algorithm almost always avoids braess paradox







common wisdom has it that small distinctions in the    probabilities parameters quantifying a belief network do not matter    much for the results of probabilistic queries yet one can develop    realistic scenarios under which small variations in network parameters    can lead to significant changes in computed queries a pending    theoretical question is then to analytically characterize parameter    changes that do or do not matter in this paper we study the    sensitivity of probabilistic queries to changes in network parameters    and prove some tight bounds on the impact that such parameters can    have on queries our analytic results pinpoint some interesting    situations under which parameter changes do or do not matter these    results are important for knowledge engineers as they help them    identify influential network parameters they also help explain some    of the previous experimental results and observations with regards to    network robustness against parameter changes







in action domains where agents may have erroneous beliefs reasoning about the effects of actions involves reasoning about belief change  in this paper we use a transition system approach to reason about the evolution of an agents beliefs as actions are executed  some  actions cause an agent to perform belief revision while others cause an agent to perform belief update but the interaction between revision and update can be nonelementary  we present a set of rationality properties describing the interaction between revision and update and we introduce a new class of belief change operators for reasoning about alternating sequences of revisions and updates  our belief change operators can be characterized in terms of a natural shifting operation on total preorderings over interpretations  we compare our approach with related work on iterated belief change due to action and we conclude with some directions for future research







this paper presents an approach to learning from situated    interactive tutorial instruction within an ongoing agent  tutorial    instruction is a flexible and thus powerful paradigm for teaching    tasks because it allows an instructor to communicate whatever types of    knowledge an agent might need in whatever situations might arise  to    support this flexibility however the agent must be able to learn    multiple kinds of knowledge from a broad range of instructional    interactions  our approach called situated explanation achieves    such learning through a combination of analytic and inductive    techniques  it combines a form of explanationbased learning that is    situated for each instruction with a full suite of contextually guided    responses to incomplete explanations  the approach is implemented in    an agent called instructosoar that learns hierarchies of new tasks    and other domain knowledge from interactive natural language    instructions  instructosoar meets three key requirements of flexible    instructability that distinguish it from previous systems 1 it can    take known or unknown commands at any instruction point 2 it can    handle instructions that apply to either its current situation or to a    hypothetical situation specified in language as in for instance    conditional instructions and 3 it can learn from instructions    each class of knowledge it uses to perform tasks









r  nissim and r  brafman 2014 distributed heuristic forward search for multiagent planning volume 51 pages 293332



this paper deals with the problem of classical planning for multiple cooperative agents who have private information about their local state and capabilities they do not want to reveal two main approaches have recently been proposed to solve this type of problem  one is based on reduction to distributed constraint satisfaction and the other on partialorder planning techniques in classical singleagent planning constraintbased and partialorder planning techniques are currently dominated by heuristic forward search the question arises whether it is possible to formulate a distributed heuristic forward search algorithm for privacypreserving classical multiagent planning our work provides a positive answer to this question in the form of a general approach to distributed statespace search in which each agent performs only the part of the state expansion relevant to it the resulting algorithms are simple and efficient  outperforming previous algorithms by orders of magnitude  while offering similar flexibility to that of forwardsearch algorithms for singleagent planning furthermore one particular variant of our general approach yields a distributed version of the a algorithm that is the first costoptimal distributed algorithm for privacypreserving planning



a new method is proposed for exploiting causal    independencies in exact bayesian network inference  a bayesian    network can be viewed as representing a factorization of a joint    probability into the multiplication of a set of conditional    probabilities  we present a notion of causal independence that    enables one to further factorize the conditional probabilities into a    combination of even smaller factors and consequently obtain a    finergrain factorization of the joint probability  the new    formulation of causal independence lets us specify the conditional    probability of a variable given its parents in terms of an associative    and commutative operator such as or sum or max on the    contribution of each parent  we start with a simple algorithm ve for    bayesian network inference that given evidence and a query variable    uses the factorization to find the posterior distribution of the    query we show how this algorithm can be extended to exploit causal    independence empirical studies based on the cpcs networks for    medical diagnosis show that this method is more efficient than    previous methods and allows for inference in larger networks than    previous algorithms













d  dubois h  fargier and j  bonnefon 2008 on the qualitative comparison of decisions having  positive and negative features volume 32 pages 385417



making a decision is often a matter of listing and comparing positive and negative arguments in such cases the evaluation scale for decisions should be considered bipolar that is negative and positive values should be explicitly distinguished that is what is done for example in cumulative prospect theory however contraryto the latter framework that presupposes genuine numerical assessments human agents often decide on the basis of an ordinal ranking of the pros and the cons and by focusing on the most salient arguments in other terms the decision process is qualitative as well as bipolar in this article based on a bipolar extension of possibility theory we define and axiomatically characterize several decision rules tailored for the joint handling of positive and negative arguments in an ordinal setting the simplest rules can be viewed as extensions of the maximin and maximax criteria to the bipolar case and consequently suffer from poor decisive power more decisive rules that refine the former are also proposed these refinements agree both with principles of efficiency and with the spirit of orderofmagnitude reasoning that prevails in qualitative decision theory the most refined decision rule uses leximin rankings of the pros and the cons and the ideas of counting arguments of equal strength and cancelling pros by cons it is shown to come down to a special case of cumulative prospect theory and to subsume the take the best heuristic studied by cognitive psychologists





termination of logic programs with negated body atoms here    called general logic programs is an important topic one reason is    that many computational mechanisms used to process negated atoms like    clarks negation as failure and chans constructive negation are    based on termination conditions  this paper introduces a methodology    for proving termination of general logic programs wrt the prolog    selection rule  the idea is to distinguish parts of the program    depending on whether or not their termination depends on the selection    rule to this end the notions of low weakly up and upacceptable    program are introduced  we use these notions to develop a methodology    for proving termination of general logic programs and show how    interesting problems in nonmonotonic reasoning can be formalized and    implemented by means of terminating general logic programs







d  korzhyk z  yin c  kiekintveld v  conitzer and m  tambe 2011 stackelberg vs nash in security games an extended investigation of interchangeability equivalence and uniqueness volume 41 pages 297327



there has been significant recent interest in gametheoretic approaches to security with much of the recent research focused on utilizing the leaderfollower stackelberg game model among the major applications are the armor program deployed at lax airport and the iris program in use by the us federal air marshals fams the foundational assumption for using stackelberg games is that security forces leaders acting first commit to a randomized strategy while their adversaries followers choose their best response after surveillance of this randomized strategy  yet in many situations a leader may face uncertainty about the followers surveillance capability previous work fails to address how a leader should compute her strategy given such uncertainty

we provide five contributions in the context of a general class of security games first we show that the nash equilibria in security games are interchangeable thus alleviating the equilibrium selection problem second under a natural restriction on security games any stackelberg strategy is also a nash equilibrium strategy and furthermore the solution is unique in a class of security games of which armor is a key exemplar third when faced with a follower that can attack multiple targets many of these properties no longer hold fourth we show experimentally that in most but not all games where the restriction does not hold the stackelberg strategy is still a nash equilibrium strategy but this is no longer true when the attacker can attack multiple targets finally as a possible direction for future research we propose an extensiveform game model that makes the defenders uncertainty about the attackers ability to observe explicit





the growing need to manage and exploit the proliferation of    online data sources is opening up new opportunities for bringing    people closer to the resources they need  for instance consider a    recommendation service through which researchers can receive daily    pointers to journal papers in their fields of interest we survey some    of the known approaches to the problem of technical paper    recommendation and ask how they can be extended to deal with multiple    information sources more specifically we focus on a variant of this    problem  recommending conference paper submissions to reviewing    committee members  which offers us a testbed to try different    approaches  using whirl  an information integration system  we    are able to implement different recommendation algorithms derived from    information retrieval principles we also use a novel autonomous    procedure for gathering reviewer interest information from the web we    evaluate our approach and compare it to other methods using preference    data provided by members of the aaai98 conference reviewing committee    along with data about the actual submissions









p  haslum 2012 narrative planning compilations to classical planning volume 44 pages 383395



a model of story generation recently proposed by riedl and young

specialised planner





a  ramani i  l markov k  a sakallah and f  a aloul 2006 breaking instanceindependent symmetries in exact graph coloring volume 26 pages 289322



code optimization and high level synthesis can be posed as constraint satisfaction and optimization problems  such as graph coloring used in register allocation  graph coloring is also used to model more traditional csps relevant to ai such as planning timetabling and scheduling  provably optimal solutions may be desirable for commercial and defense applications additionally for applications such as register allocation and code optimization naturallyoccurring instances    of graph coloring are often small and can be solved optimally a recent  wave of improvements in algorithms for boolean satisfiability sat and 01 integer linear programming ilp suggests generic problemreduction  methods rather than problemspecific heuristics because 1 heuristics may be upset by new constraints 2 heuristics tend to ignore structure and 3 many relevant problems are provably inapproximable 

