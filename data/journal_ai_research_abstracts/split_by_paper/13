



f  lin 2003 compiling causal theories to successor state axioms and stripslike systems volume 19 pages 279314



we describe a system for specifying the effects of actions unlike those commonly used in ai planning our system uses an action description language that allows one to specify the effects of actions using domain rules which are state constraints that can entail new action effects from old ones declaratively an action domain in our language corresponds to a nonmonotonic causal theory in the situation calculus procedurally such an action domain is compiled into a set of    logical theories one for each action in the domain from which fully instantiated  successor statelike axioms and stripslike systems are then generated we expect the system to be a useful tool for knowledge engineers writing action specifications for classical ai planning systems golog systems and other systems where formal specifications of actions are needed









c  m li f  manya and j  planes 2007 new inference rules for maxsat volume 30 pages 321359



d  l chen j  kim and r  j mooney 2010 training a multilingual sportscaster using perceptual context to learn language volume 37 pages 397435



we present a novel framework for learning to interpret and generate language using only perceptual context as supervision  we demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both english and korean without any languagespecific prior knowledge  training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace  the system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation we also present a novel algorithm for learning which events are worth describing  human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain





j  larrosa  e  morancho and  d  niso 2005 on the practical use of variable elimination in constraint optimization problems stilllife as a case study volume 23 pages 421440



variable elimination is a general technique for constraint processing it is often discarded because of its high space complexity however it can be extremely useful when combined with other techniques in this paper we study the applicability of variable elimination to the challenging problem of finding stilllifes  we illustrate several alternatives variable elimination as a standalone algorithm interleaved with search and as a source of good quality lower bounds  we show that these techniques are the best known option both theoretically and empirically in our experiments we have been able to solve the n20 instance which is far beyond reach with alternative approaches





honorable mention for the 2007 ijcaijair best paper prize



we explore a method for computing admissible heuristic evaluation functions for search problems it utilizes pattern databases which are precomputed tables of the exact cost of solving various subproblems of an existing problem unlike standard pattern database heuristics however we partition our problems into disjoint subproblems so that the costs of solving the different subproblems can be added together without overestimating the cost of solving the original problem previously we showed how to statically partition the slidingtile puzzles into disjoint groups of tiles to compute an admissible heuristic using the same partition for each state and problem instance here we extend the method and show that it applies to other domains as well we also present another method for additive heuristics which we call dynamically partitioned pattern databases here we partition the problem into disjoint subproblems for each state of the search dynamically we discuss the pros and cons of each of these methods and apply both methods to three different problem domains the slidingtile puzzles the 4peg towers of hanoi problem and finding an optimal vertex cover of a graph we find that in some problem domains static partitioning is most effective while in others dynamic partitioning is a better choice in each of these problem domains either statically partitioned or dynamically partitioned pattern database heuristics are the best known heuristics for the problem





we study properties of programs with monotone and convex constraints we extend to these formalisms concepts and results from normal logic programming they include the notions of strong and uniform equivalence with their characterizations tight programs and fages lemma program completion and loop formulas our results provide an abstract account of properties of some recent extensions of logic programming with aggregates especially the formalism of lparse programs they imply a method to compute stable models of lparse programs by means of offtheshelf solvers of pseudoboolean constraints which is often much faster than the smodels system







r  khardon  d  roth and  r  a servedio 2005 efficiency versus convergence of boolean kernels for online learning algorithms volume 24 pages 341356



the paper studies machine learning problems where each example is described using a set of boolean features and where hypotheses are represented by linear threshold elements  one method of increasing the expressiveness of learned hypotheses in this context is to expand the feature set to include conjunctions of basic features this can be done explicitly or where possible by using a kernel function focusing on the well known perceptron and winnow algorithms the paper demonstrates a tradeoff between the computational efficiency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm  we first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions  we show that these kernels can be used to efficiently run the perceptron algorithm over a feature space of exponentially many conjunctions however we also show that using such kernels the perceptron algorithm can provably make an exponential number of mistakes even when learning simple functions  we then consider the question of whether kernel functions can analogously be used to run the multiplicativeupdate winnow algorithm over an expanded feature space of exponentially many conjunctions known upper bounds imply that the winnow algorithm can learn disjunctive normal form dnf formulae with a polynomial mistake bound in this setting  however we prove that it is computationally hard to simulate winnows behavior for learning dnf over such a feature set this implies that the kernel functions which correspond to running winnow for this problem are not efficiently computable and that there is no general construction that can run winnow with kernels





p  f patelschneider and  r  sebastiani 2003 a new general method to generate random modal formulae for testing decision procedures volume 18 pages 351389



the recent emergence of heavilyoptimized modal decision    procedures has highlighted the key role of empirical testing in this    domain  unfortunately the introduction of extensive empirical tests    for modal logics is recent and so far none of the proposed test    generators is very satisfactory  to cope with this fact we present a    new random generation method that provides benefits over previous    methods for generating empirical tests  it fixes and much generalizes    one of the bestknown methods the random cnfm test allowing for    generating a much wider variety of problems covering in principle the    whole input space  our new method produces much more suitable test    sets for the current generation of modal decision procedures  we    analyze the features of the new method by means of an extensive    collection of empirical tests

independence  the study of what is relevant to a given    problem of reasoning  has received an increasing attention from the    ai community in this paper we consider two basic forms of    independence namely a syntactic one and a semantic one we show    features and drawbacks of them  in particular while the syntactic    form of independence is computationally easy to check there are cases    in which things that intuitively are not relevant are not recognized    as such  we also consider the problem of forgetting ie distilling    from a knowledge base only the part that is relevant to the set of    queries constructed from a subset of the alphabet while such process    is computationally hard it allows for a simplification of subsequent    reasoning and can thus be viewed as a form of compilation once the    relevant part of a knowledge base has been extracted all reasoning    tasks to be performed can be simplified





t  s jaakkola and  m  i jordan 1999 variational probabilistic inference and the qmrdt network volume 10 pages 291322



we describe a variational approximation method for efficient    inference in largescale probabilistic models  variational methods    are deterministic procedures that provide approximations to marginal    and conditional probabilities of interest  they provide alternatives    to approximate inference methods based on stochastic sampling or    search  we describe a variational approach to the problem of    diagnostic inference in the quick medical reference qmr network    the qmr network is a largescale probabilistic graphical model built    on statistical and expert knowledge  exact probabilistic inference is    infeasible in this model for all but a small set of cases  we    evaluate our variational inference algorithm on a large set of    diagnostic test cases comparing the algorithm to a stateoftheart    stochastic sampling method







petr  savick253 and petr  ku269era 2016 generating models of a matched formula with a polynomial delay volume 56 pages 379402



a matched formula is a cnf formula whose incidence graph admits a matching which matches a distinct variable to every clause such a formula is always satisfiable matched formulas are used for example in the area of parametrized complexity we prove that the problem of counting the number of the models satisfying assignments of a matched formula is pcomplete on the other hand we define a class of formulas generalizing the matched formulas and prove that for a formula in this class one can choose in polynomial time a variable suitable for splitting the tree for the search of the models of the formula as a consequence the models of a formula from this class in particular of any matched formula can be generated sequentially with a delay polynomial in the size of the input on the other hand we prove that this task cannot be performed efficiently for linearly satisfiable formulas which is a generalization of matched formulas containing the class considered above 





e  talvitie and s  singh 2011 learning to make predictions in partially observable environments without a generative model volume 42 pages 353392



when faced with the problem of learning a model of a highdimensional environment a common approach is to limit the model to make only a restricted set of predictions thereby simplifying the learning problem these partial models may be directly useful for making decisions or may be combined together to form a more complete structured model however in partially observable nonmarkov environments standard modellearning methods learn generative models ie models that provide a probability distribution over all possible futures such as pomdps it is not straightforward to restrict such models to make only certain predictions and doing so does not always simplify the learning problem in this paper we present prediction profile models nongenerative partial models for partially observable systems that make only a given set of predictions and are therefore far simpler than generative models in some cases we formalize the problem of learning a prediction profile model as a transformation of the original modellearning problem and show empirically that one can learn prediction profile models that make a small set of important predictions even in systems that are too complex for standard generative models





e  amir and a  chang 2008 learning partially observable deterministic action models volume 33 pages 349402



we present exact algorithms for identifying deterministicactions effects and preconditions in dynamic partially observable domains  they apply when one does not know the action modelthe way actions affect the world of a domain and must learn it from partial observations over time such scenarios are common in real world applications they are challenging for ai tasks because traditional domain structures that underly tractability eg conditional independence fail there eg world features become correlated our work departs from traditional assumptions about partial observations and action models in particular it focuses on problems in which actions are deterministic of simple logical structure and observation models have all features observed with some frequency we yield tractable algorithms for the modified problem for such domains 



g  di caro and  m  dorigo 1998 antnet distributed stigmergetic control for communications networks volume 9 pages 317365



this paper introduces antnet a novel approach to the    adaptive learning of routing tables in communications networks    antnet is a distributed mobile agents based monte carlo system that    was inspired by recent work on the ant colony metaphor for solving    optimization problems antnets agents concurrently explore the    network and exchange collected information  the communication among    the agents is indirect and asynchronous mediated by the network    itself this form of communication is typical of social insects and is    called stigmergy  we compare our algorithm with six stateoftheart    routing algorithms coming from the telecommunications and machine    learning fields  the algorithms performance is evaluated over a set    of realistic testbeds  we run many experiments over real and    artificial ip datagram networks with increasing number of nodes and    under several paradigmatic spatial and temporal traffic distributions    results are very encouraging  antnet showed superior performance    under all the experimental conditions with respect to its competitors    we analyze the main characteristics of the algorithm and try to    explain the reasons for its superiority





j  baxter and  p  l bartlett 2001 infinitehorizon policygradient estimation volume 15 pages 319350



gradientbased approaches to direct policy search in    reinforcement learning have received much recent attention as a means    to solve problems of partial observability and to avoid some of the    problems associated with policy degradation in valuefunction methods    in this paper we introduce gpomdp a simulationbased algorithm for    generating a biased estimate of the gradient of the average reward in    partially observable markov decision processes pomdps controlled by    parameterized stochastic policies a similar algorithm was proposed by    kimura et al 1995 the algorithms chief advantages are that it    requires storage of only twice the number of policy parameters uses    one free beta which has a natural interpretation in terms of    biasvariance tradeoff and requires no knowledge of the underlying    state we prove convergence of gpomdp and show how the correct choice    of the parameter beta is related to the mixing time of the    controlled pomdp we briefly describe extensions of gpomdp to    controlled markov chains continuous state observation and control    spaces multipleagents higherorder derivatives and a version for    training stochastic policies with internal states  in a companion    paper baxter et al this volume we show how the gradient estimates    generated by gpomdp can be used in both a traditional stochastic    gradient algorithm and a conjugategradient procedure to find local    optima of the average reward









c  domshlak j  hoffmann and a  sabharwal 2009 friends or foes on planning as satisfiability and abstract cnf encodings volume 36 pages 415469



b  nebel 2000 on the compilability and expressive power of propositional planning formalisms volume 12 pages 271315



the recent approaches of extending the graphplan algorithm    to handle more expressive planning formalisms raise the question of    what the formal meaning of expressive power is we formalize the    intuition that expressive power is a measure of how concisely planning    domains and plans can be expressed in a particular formalism by    introducing the notion of compilation schemes between planning    formalisms  using this notion we analyze the expressiveness of a    large family of propositional planning formalisms ranging from basic    strips to a formalism with conditional effects partial state    specifications and propositional formulae in the preconditions  one    of the results is that conditional effects cannot be compiled away if    plan size should grow only linearly but can be compiled away if we    allow for polynomial growth of the resulting plans this result    confirms that the recently proposed extensions to the graphplan    algorithm concerning conditional effects are optimal with respect to    the compilability framework  another result is that general    propositional formulae cannot be compiled into conditional effects if    the plan size should be preserved linearly  this implies that    allowing general propositional formulae in preconditions and effect    conditions adds another level of difficulty in generating a plan









w  faber m  truszczy324ski and s  woltran 2013 strong equivalence of qualitative optimization problems volume 47 pages 351391



chienju  ho aleksandrs  slivkins and jennifer  wortman vaughan 2016 adaptive contract design for crowdsourcing markets bandit algorithms for repeated principalagent problems volume 55 pages 317359



crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete the payment for a particular task is typically set by the tasks requester and may be adjusted based on the quality of the completed work for example through the use of bonus payments in this paper we study the requesters problem of dynamically adjusting qualitycontingent payments for tasks we consider a multiround version of the wellknown principalagent model whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester in particular our formulation significantly generalizes the budgetfree online task pricing problems studied in prior work we treat this problem as a multiarmed bandit problem with each arm representing a potential contract to cope with the large and in fact infinite number of arms we propose a new algorithm agnosticzooming which discretizes the contract space into a finite number of regions effectively treating each region as a single arm this discretization is adaptively refined so that more promising regions of the contract space are eventually discretized more finely we analyze this algorithm showing that it achieves regret sublinear in the time horizon and substantially improves over nonadaptive discretization which is the only competing approach in the literature our results advance the state of art on several different topics the theory of crowdsourcing markets principalagent problems multiarmed bandits and dynamic pricing









mariecatherine  de marneffe marta  recasens and christopher  potts 2015 modeling the lifespan of discourse entities with application to coreference resolution volume 52 pages 445475



s  r jodogne and j  h piater 2007 closedloop learning of visual control policies volume 28 pages 349391



in this paper we present a general flexible framework for learning mappings from images to actions by interacting with the environment the basic idea is to introduce a featurebased image classifier in front of a reinforcement learning algorithm the classifier partitions the visual space according to the presence or absence of few highly informative local descriptors that are incrementally selected in a sequence of attempts to remove perceptual aliasing we also address the problem of fighting overfitting in such a greedy algorithm finally we show how highlevel visual features can be generated when the power of local descriptors is insufficient for completely disambiguating the aliased states this is done by building a hierarchy of composite features that consist of recursive spatial combinations of visual features we demonstrate the efficacy of our algorithms by solving three visual navigation tasks and a visual version of the classical car on the hill control problem







y  li p  musilek m  reformat and l  wyardscott 2009 identification of pleonastic it using the web volume 34 pages 339389



in a significant minority of cases certain pronouns especially the pronoun it can be used without referring to any specific entity this phenomenon of pleonastic pronoun usage poses serious problems for systems aiming at even a shallow understanding of natural language texts in this paper a novel approach is proposed to identify such uses of it the extrapositional cases are identified using a series of queries against the web and the cleft cases are identified using a simple set of syntactic rules the system is evaluated with four sets of news articles containing 679 extrapositional cases as well as 78 cleft constructs the identification results are comparable to those obtained by human efforts



dual decomposition and more generally lagrangian relaxation is a classical method for combinatorial optimization it has recently been applied to several inference problems in natural language processing nlp this tutorial gives an overview of the technique we describe example algorithms describe formal guarantees for the method and describe practical issues in implementing the algorithms while our examples are predominantly drawn from the nlp literature the material should be of general relevance to inference problems in machine learning a central theme of this tutorial is that lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms allowing inference in models that go significantly beyond previous work on lagrangian relaxation for inference in graphical models









g  casini and u  straccia 2013 defeasible inheritancebased description logics volume 48 pages 415473



defeasible inheritance networks are a nonmonotonic framework that deals with hierarchical knowledge on the other hand rational closure is acknowledged as a landmark of the preferential approach to nonmonotonic reasoning we will combine these two approaches and define a new nonmonotonic closure operation for propositional knowledge bases that combines the advantages of both then we redefine such a procedure for description logics dls a family of logics wellsuited to model structured information in both cases we will provide a simple reasoning method that is built on top of the classical entailment relation and thus is amenable of an implementation based on existing reasoners eventually we evaluate our approach on wellknown landmark test examples







o  gimenez and a  jonsson 2008 the complexity of planning problems with simple causal graphs volume 31 pages 319351





felix  brandt markus  brill edith  hemaspaandra and lane  a hemaspaandra 2015 bypassing combinatorial protections polynomialtime algorithms for singlepeaked electorates volume 53 pages 439496



for many election systems bribery and related attacks have been shown nphard using constructions on combinatorially rich structures such as partitions and covers  this paper shows that for voters who follow the most central politicalscience model of electoratessinglepeaked preferencesthose hardness protections vanish  by using singlepeaked preferences to simplify combinatorial covering challenges we for the first time show that nphard bribery problemsincluding those for kemeny and llull electionsfall to polynomial time for singlepeaked electorates  by using singlepeaked preferences to simplify combinatorial partition challenges we for the first time show that nphard partitionofvoters problems fall to polynomial time for singlepeaked electorates  we show that for singlepeaked electorates the winner problems for dodgson and kemeny elections though thetatwocomplete in the general case fall to polynomial time  and we completely classify the complexity of weighted coalition manipulation for scoring protocols in singlepeaked electorates



d  monderer and  m  tennenholtz 1997 dynamic nonbayesian decision making volume 7 pages 231248



the model of a nonbayesian agent who faces a repeated game    with incomplete information against nature is an appropriate tool for    modeling general agentenvironment interactions  in such a model the    environment state controlled by nature may change arbitrarily and    the feedbackreward function is initially unknown the agent is not    bayesian that is he does not form a prior probability neither on the    state selection strategy of nature nor on his reward function  a    policy for the agent is a function which assigns an action to every    history of observations and actions  two basic feedback structures    are considered in one of them  the perfect monitoring case  the    agent is able to observe the previous environment state as part of his    feedback while in the other  the imperfect monitoring case  all    that is available to the agent is the reward obtained both of these    settings refer to partially observable processes where the current    environment state is unknown  our main result refers to the    competitive ratio criterion in the perfect monitoring case we prove    the existence of an efficient stochastic policy that ensures that the    competitive ratio is obtained at almost all stages with an arbitrarily    high probability where efficiency is measured in terms of rate of    convergence  it is further shown that such an optimal policy does not    exist in the imperfect monitoring case moreover it is proved that in    the perfect monitoring case there does not exist a deterministic    policy that satisfies our long run optimality criterion  in addition    we discuss the maxmin criterion and prove that a deterministic    efficient optimal strategy does exist in the imperfect monitoring case    under this criterion finally we show that our approach to longrun    optimality can be viewed as qualitative which distinguishes it from    previous work in this area









p  jeavons and j  petke 2012 local consistency and satsolvers volume 43 pages 329351





s  rudolph and b  glimm 2010 nominals inverses counting and conjunctive queries or why infinity is your friend volume 39 pages 429481



description logics are knowledge representation formalisms that provide for example the logical underpinning of the w3c owl standards conjunctive queries the standard query language in databases have recently gained significant attention as an expressive formalism for querying description logic knowledge bases several different techniques for deciding conjunctive query entailment are available for a wide range of dls nevertheless the combination of nominals inverse roles and number restrictions in owl 1 and owl 2 dl causes unsolvable problems for the techniques hitherto available we tackle this problem and present a decidability result for entailment of unions of conjunctive queries in the dl alchoiqb that contains all three problematic constructors simultaneously provided that queries contain only simple roles our result also shows decidability of entailment of unions of conjunctive queries in the logic that underpins owl 1 dl and we believe that the presented results will pave the way for further progress towards conjunctive query entailment decision procedures for the description logics underlying the owl standards







l  cigler and b  faltings 2014 symmetric subgameperfect equilibria in resource allocation volume 49 pages 323361



a  gerevini  a  saetti and  i  serina 2003 planning through stochastic local search and temporal action graphs in lpg volume 20 pages 239290



we present some techniques for planning in domains specified with the recent standard language pddl21 supporting durative actions and numerical quantities these techniques are implemented in lpg a domainindependent planner that took part in the 3rd international planning competition ipc lpg is an incremental any time system  producing multicriteria quality plans the core of the system is  based on a stochastic local search method and on a graphbased  representation called temporal action graphs tagraphs   this paper focuses on temporal planning introducing tagraphs and proposing some techniques to guide the search in lpg using this representation the experimental results of the 3rd ipc as well as further results presented in this paper show that our techniques  can be very effective often lpg outperforms all other fullyautomated  planners of the 3rd ipc in terms of speed to derive a solution or  quality of the solutions that can be produced





m  j nederhof and  g  satta 2004 idlexpressions a formalism for representing and parsing finite languages in natural language processing volume 21 pages 287317



we  propose  a  formalism  for  representation  of  finite  languages referred to  as the class of idlexpressions  which combines concepts that were  only considered in  isolation in existing  formalisms  the suggested  applications  are  in  natural  language  processing  more specifically  in surface  natural language  generation and  in machine translation where a sentence is  obtained by first generating a large set of candidate sentences represented  in a compact way and then by filtering  such a  set  through  a parser   we  study several  formal properties of idlexpressions and compare this new formalism with more standard  ones   we  also  present  a  novel  parsing  algorithm  for idlexpressions  and  prove a  nontrivial  upper  bound  on its  time complexity





a  feldman g  provan and a  van gemund 2010 approximate modelbased diagnosis using greedy stochastic search volume 38 pages 371413



we propose a stochastic fault diagnosis algorithm called safari which trades off guarantees of computing minimal diagnoses for computational efficiency we empirically demonstrate using the 74xxx and iscas85 suites of benchmark combinatorial circuits that safari achieves several ordersofmagnitude speedup over two wellknown deterministic algorithms cda and ha for multiplefault diagnoses further safari can compute a range of multiplefault diagnoses that cda and ha cannot we also prove that safari is optimal for a range of propositional fault models such as the widelyused weakfault models models with ignorance of abnormal behavior we discuss the optimality of safari in a class of strongfault circuit models with stuckat failure modes by modeling the algorithm itself as a markov chain we provide exact bounds on the minimality of the diagnosis computed safari also displays strong anytime behavior and will return a diagnosis after any nontrivial inference time





research on agent communication languages has typically taken the speech acts paradigm as its starting point despite their manifest attractions speechact models of communication have several serious disadvantages as a foundation for communication in artificial agent systems in particular it has proved to be extremely difficult to give a satisfactory semantics to speechact based agent communication languages in part the problem is that speechact semantics typically make reference to the mental states of agents their beliefs desires and intentions and there is in general no way to attribute such attitudes to arbitrary computational agents in addition agent programming languages have only had their semantics formalised for abstract standalone versions neglecting aspects such as communication primitives with respect to communication implemented agent programming languages have tended to be rather ad hoc this paper addresses both of these problems by giving semantics to speechact based messages received by an agentspeak agent agentspeak is a logicbased agent programming language which incorporates the main features of the prs model of reactive planning systems the paper builds upon a structural operational semantics to agentspeak that we developed in previous work the main contributions of this paper are as follows an extension of our earlier work on the theoretical foundations of agentspeak interpreters a computationally grounded semantics for the core performatives used in speechact based agent communication languages and a welldefined extension of agentspeak that supports agent communication









a  coles a  coles m  fox and d  long 2013 a hybrid lprpg heuristic for modelling numeric resource flows in planning volume 46 pages 343412



although the use of metric fluents is fundamental to many practical planning problems the study of heuristics to support fully automated planners working with these fluents remains relatively unexplored the most widely used heuristic is the relaxation of metric fluents into intervalvalued variables  an idea first proposed a decade ago other heuristics depend on domain encodings that supply additional information about fluents such as capacity constraints or other resourcerelated annotations 

we present a heuristic for numeric planning problems building on the propositional relaxed planning graph but using a mathematical program for numeric reasoning  we define a class of producerconsumer planning problems and demonstrate how the numeric constraints in these can be modelled in a mixed integer program mip  this mip is then combined with a metric relaxed planning graph rpg heuristic to produce an integrated hybrid heuristic the mip tracks resource use more accurately than the usual relaxation but relaxes the ordering of actions while the rpg captures the causal propositional aspects of the problem  we discuss how these two components interact to produce a single unified heuristic and go on to explore how further numeric features of planning problems can be integrated into the mip we show that encoding a limited subset of the propositional problem to augment the mip can yield more accurate guidance partly by exploiting structure such as propositional landmarks and propositional resources  our results show that the use of this heuristic enhances scalability on problems where numeric resource interaction is key in finding a solution



bayesian networks are a useful tool in the representation of uncertain knowledge this paper proposes a new algorithm called acoe to learn the structure of a bayesian network it does this by conducting a search through the space of equivalence classes of bayesian networks using ant colony optimization aco to this end two novel extensions of traditional aco techniques are proposed and implemented firstly multiple types of moves are allowed secondly moves can be given in terms of indices that are not based on construction graph nodes the results of testing show that acoe performs better than a greedy search and other stateoftheart and metaheuristic algorithms whilst searching in the space of equivalence classes











fazlul  hasan siddiqui and patrik  haslum 2015 continuing plan quality optimisation volume 54 pages 369435



in a peertopeer inference system each peer can reason locally but can also solicit some of its acquaintances which are peers sharing part of its vocabulary in this paper we consider peertopeer inference systems in which the local theory of each peer is a set of propositional clauses defined upon a local vocabulary an important characteristic of peertopeer inference systems is that the global theory the union of all peer theories is not known as opposed to partitionbased reasoning systems the main contribution of this paper is to provide the first consequence finding algorithm in a peertopeer setting deca it is anytime and computes consequences gradually from the solicited peer to peers that are more and more distant we exhibit a sufficient condition on the acquaintance graph of the peertopeer inference system for guaranteeing the completeness of this algorithm another important contribution is to apply this general distributed reasoning setting to the setting of the semantic web through the somewhere semantic peertopeer data management system the last contribution of this paper is to provide an experimental analysis of the scalability of the peertopeer infrastructure that we propose on large networks of 1000 peers 







d  j cook and  l  b holder 1994 substructure discovery using minimum description length and background knowledge volume 1 pages 231255



the ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data  we describe a new version of our subdue substructure discovery system based on the minimum description length principle  the subdue system discovers substructures that compress the original data and represent structural concepts in the data  by replacing previouslydiscovered substructures in the data multiple passes of subdue produce a hierarchical description of the structural regularities in the data  subdue uses a computationallybounded inexact graph match that identifies similar but not identical instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints in addition to the minimum description length principle other background knowledge can be used by subdue to guide the search towards more appropriate substructures  experiments in a variety of domains demonstrate subdues ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain description of online appendix this is a compressed tar file containing the subdue discovery system written in c  the program accepts as input databases represented in graph form and will output discovered substructures with their corresponding value





s  argamonengelson and  i  dagan 1999 committeebased sample selection for probabilistic classifiers volume 11 pages 335360



in many realworld learning tasks it is expensive to    acquire a sufficient number of labeled examples for training  this    paper investigates methods for reducing annotation cost by sample    selection in this approach during training the learning program    examines many unlabeled examples and selects for labeling only those    that are most informative at each stage this avoids redundantly    labeling examples that contribute little new information       our work follows on previous research on query by committee extending    the committeebased paradigm to the context of probabilistic    classification  we describe a family of empirical methods for    committeebased sample selection in probabilistic classification    models which evaluate the informativeness of an example by measuring    the degree of disagreement between several model variants  these    variants the committee are drawn randomly from a probability    distribution conditioned by the training set labeled so far       the method was applied to the realworld natural language processing    task of stochastic partofspeech tagging  we find that all variants    of the method achieve a significant reduction in annotation cost    although their computational efficiency differs  in particular the    simplest variant a two member committee with no parameters to tune    gives excellent results  we also show that sample selection yields a    significant reduction in the size of the model used by the tagger





s  hanks and  d  s weld 1995 a domainindependent algorithm for plan adaptation volume 2 pages 319360



the paradigms of transformational planning casebased   planning and plan debugging all involve a process known as i plan   adaptation i  modifying or repairing an old plan so it solves a new   problem  in this paper we provide a domainindependent algorithm for   plan adaptation demonstrate that it is sound complete and   systematic and compare it to other adaptation algorithms in the   literature    our approach is based on a view of planning as searching a graph of   partial plans  generative planning starts at the graphs root and   moves from node to node using planrefinement operators  in planning   by adaptation a library plan  an arbitrary node in the plan   graph  is the starting point for the search and the planadaptation   algorithm can apply both the same refinement operators available to a   generative planner and can also retract constraints and steps from the   plan  our algorithms completeness ensures that the adaptation   algorithm will eventually search the entire graph and its systematicity    ensures that it will do so without redundantly searching any parts of   the graph





d  v pynadath and  m  tambe 2002 the communicative multiagent team decision problem analyzing teamwork theories and models volume 16 pages 389423



despite the significant progress in multiagent teamwork    existing research does not address the optimality of its prescriptions    nor the complexity of the teamwork problem  without a    characterization of the optimalitycomplexity tradeoffs it is    impossible to determine whether the assumptions and approximations    made by a particular theory gain enough efficiency to justify the    losses in overall performance  to provide a tool for use by    multiagent researchers in evaluating this tradeoff we present a    unified framework the communicative multiagent team decision problem    commtdp  the commtdp model combines and extends existing    multiagent theories such as decentralized partially observable markov    decision processes and economic team theory  in addition to their    generality of representation commtdps also support the analysis of    both the optimality of team performance and the computational    complexity of the agents decision problem  in analyzing complexity    we present a breakdown of the computational complexity of constructing    optimal teams under various classes of problem domains along the    dimensions of observability and communication cost  in analyzing    optimality we exploit the commtdps ability to encode existing    teamwork theories and models to encode two instantiations of joint    intentions theory taken from the literature  furthermore the    commtdp model provides a basis for the development of novel team    coordination algorithms  we derive a domainindependent criterion for    optimal communication and provide a comparative analysis of the two    joint intentions instantiations with respect to this optimal policy    we have implemented a reusable domainindependent software package    based on commtdps to analyze teamwork coordination strategies and we    demonstrate its use by encoding and evaluating the two joint    intentions strategies within an example domain





r  bod 2002 a unified model of structural organization in language and music volume 17 pages 289308



is there a general model that can predict the perceived    phrase structure in language and music while it is usually assumed    that humans have separate faculties for language and music this work    focuses on the commonalities rather than on the differences between    these modalities aiming at finding a deeper faculty our key idea    is that the perceptual system strives for the simplest structure the    simplicity principle but in doing so it is biased by the    likelihood of previous structures the likelihood principle we    present a series of dataoriented parsing dop models that combine    these two principles and that are tested on the penn treebank and the    essen folksong collection our experiments show that 1 a combination    of the two principles outperforms the use of either of them and 2    exactly the same model with the same parameter setting achieves    maximum accuracy for both language and music we argue that our    results suggest an interesting parallel between linguistic and musical    structuring





p  faliszewski e  hemaspaandra and l  a hemaspaandra 2011 multimode control attacks on elections volume 40 pages 305351



in 1992 bartholdi tovey and trick opened the study of control attacks on electionsattempts to improve the election outcome by such actions as addingdeleting candidates or voters  that work has led to many results on how algorithms can be used to find attacks on elections and how complexitytheoretic hardness results can be used as shields against attacks however all the work in this line has assumed that the attacker employs just a single type of attack  in this paper we model and study the case in which the attacker launches a multipronged ie multimode attack  we do so to more realistically capture the richness of reallife settings for example an attacker might simultaneously try to suppress some voters attract new voters into the election and introduce a spoiler candidate our model provides a unified framework for such varied attacks  by constructing polynomialtime multiprong attack algorithms we prove that for various election systems even such concerted flexible attacks can be perfectly planned in deterministic polynomial time





a  broggi and  s  berte 1995 visionbased road detection in automotive systems a realtime expectationdriven approach volume 3 pages 325348



the main aim of this work is the development of a    visionbased road detection system fast enough to cope with the    difficult realtime constraints imposed by moving vehicle    applications  the hardware platform a specialpurpose massively    parallel system has been chosen to minimize system production and    operational costs           this paper presents a novel approach to expectationdriven lowlevel    image segmentation which can be mapped naturally onto meshconnected    massively parallel simd architectures capable of handling hierarchical    data structures  the input image is assumed to contain a distorted    version of a given template a multiresolution stretching process is    used to reshape the original template in accordance with the acquired    image content minimizing a potential function  the distorted    template is the process output









f  belardinelli a  lomuscio and f  patrizi 2014 verification of agentbased artifact systems volume 51 pages 333376



j  c schlimmer and  p  c wells 1996 quantitative results comparing three intelligent interfaces forinformation capture a case study adding name information into a volume 5 pages 329349



efficiently entering information into a computer is key to    enjoying the benefits of computing this paper describes three    intelligent user interfaces handwriting recognition adaptive menus    and predictive fillin in the context of adding a personus name and    address to an electronic organizer tests show handwriting recognition    is slower than typing on an onscreen soft keyboard while adaptive    menus and predictive fillin can be twice as fast this paper also    presents strategies for applying these three interfaces to other    information collection domains



aamas 2013 best paper award finalist



attackerdefender stackelberg security games ssgs have emerged as an important research area in multiagent systems however existing ssgs models yield fixed static schedules which fail in dynamic domains where defenders face execution uncertainty ie in domains where defenders may face unanticipated disruptions of their schedules a concrete example is an application involving checking fares on trains where a defenders schedule is frequently interrupted by fare evaders making static schedules useless

to address this shortcoming this paper provides four main contributions first we present a novel general bayesian stackelberg game model for security resource allocation in dynamic uncertain domains in this new model execution uncertainty is handled by using a markov decision process mdp for generating defender policies second we study the problem of computing a stackelberg equilibrium for this game and exploit problem structure to reduce it to a polynomialsized optimization problem shifting to evaluation our third contribution shows in simulation that our mdpbased policies overcome the failures of previous ssg algorithms in so doing we can now build a complete system that enables handling of schedule interruptions and consequently to conduct some of the first controlled experiments on ssgs in the field hence as our final contribution we present results from a realworld experiment on metro trains in los angeles validating our mdpbased model and most importantly concretely measuring the benefits of ssgs for security resource allocation









v  bulitko m  lustrek j  schaeffer y  bjornsson and s  sigmundarson 2008 dynamic control in realtime heuristic search volume 32 pages 419452



t  walsh 1996 a divergence critic for inductive proof volume 4 pages 209235



inductive theorem provers often diverge this paper    describes a simple critic a computer program which monitors the    construction of inductive proofs attempting to identify diverging    proof attempts divergence is recognized by means of a difference    matching procedure the critic then proposes lemmas and    generalizations which ripple these differences away so that the    proof can go through without divergence the critic enables the    theorem prover spike to prove many theorems completely automatically    from the definitions alone







s  a siddiqi and j  huang 2011 sequential diagnosis by abstraction volume 41 pages 329365



when a system behaves abnormally sequential diagnosis takes a sequence of measurements of the system until the faults causing the abnormality are identified and the goal is to reduce the diagnostic cost defined here as the number of measurements to propose measurement points previous work employs a heuristic based on reducing the entropy over a computed set of diagnoses this approach generally has good performance in terms of diagnostic cost but can fail to diagnose large systems when the set of diagnoses is too large focusing on a smaller set of probable diagnoses scales the approach but generally leads to increased average diagnostic costs in this paper we propose a new diagnostic framework employing four new techniques which scales to much larger systems with good performance in terms of diagnostic cost

first we propose a new heuristic for measurement point selection that can be computed efficiently without requiring the set of diagnoses once the system is modeled as a bayesian network and compiled into a logical form known as ddnnf second we extend hierarchical diagnosis a technique based on system abstraction from our previous work to handle probabilities so that it can be applied to sequential diagnosis to allow larger systems to be diagnosed third for the largest systems where even hierarchical diagnosis fails we propose a novel method that converts the system into one that has a smaller abstraction and whose diagnoses form a superset of those of the original system the new system can then be diagnosed and the result mapped back to the original system finally we propose a novel cost estimation function which can be used to choose an abstraction of the system that is more likely to provide optimal average cost experiments with iscas85 benchmark circuits indicate that our approach scales to all circuits in the suite except one that has a flat structure not susceptible to useful abstraction 



j  hoffmann and  b  nebel 2001 the ff planning system fast plan generation through heuristic search volume 14 pages 253302











e  albacete j  calle e  castro and d  cuadra 2012 semantic similarity measures applied to an ontology for humanlike interaction volume 44 pages 397421







