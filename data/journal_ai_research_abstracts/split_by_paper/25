a  petcu b  faltings and d  c parkes 2008 mdpop faithful distributed implementation of efficient social choice problems volume 32 pages 705755









f  rossi k  b venable and n  yorkesmith 2006 uncertainty in soft temporal constraint problemsa general framework and controllability algorithms forthe fuzzy case volume 27 pages 617674



j  hoffmann 2005 where ignoring delete lists works local search topology in planning benchmarks volume 24 pages 685758



between 1998 and 2004 the planning community has seen vast progress in terms of the sizes of benchmark examples that domainindependent planners can tackle successfully the key technique behind this progress is the use of heuristic functions based on relaxing the planning task at hand where the relaxation is to assume that all delete lists are empty the unprecedented success of such methods in many commonly used benchmark examples calls for an understanding of what classes of domains these methods are well suited for   in the investigation at hand we derive a formal background to such an understanding we perform a case study covering a range of 30 commonly used strips and adl benchmark domains including all examples used in the first four international planning competitions we prove connections between domain structure and local search topology  heuristic cost surface properties  under an idealized version of the heuristic functions used in modern planners the idealized heuristic function is called h and differs from the practically used functions in that it returns the length of an optimal relaxed plan which is nphard to compute we identify several key characteristics of the topology under h concerning the existencenonexistence of unrecognized dead ends as well as the existencenonexistence of constant upper bounds on the difficulty of escaping local minima and benches these distinctions divide the set of all planning domains into a taxonomy of classes of varying h topology as it turns out many of the 30 investigated domains lie in classes with a relatively easy topology most particularly 12 of the domains lie in classes where ffs search algorithm provided with h is a polynomial solving mechanism   we also present results relating h to its approximation as implemented in ff the behavior regarding dead ends is provably the same we summarize the results of an empirical investigation showing that in many domains the topological qualities of h are largely inherited by the approximation the overall investigation gives a rare example of a successful analysis of the connections between typicalcase problem structure and search performance the theoretical investigation also gives hints on how the topological phenomena might be automatically recognizable by domain analysis techniques we outline some preliminary steps we made into that direction 





isabel  cenamor tom225s  de la rosa and fernando  fern225ndez 2016 the ibacop planning system instancebased configured portfolios volume 56 pages 657691



sequential planning portfolios are very powerful in exploiting the complementary strength of different automated planners the main challenge of a portfolio planner is to define which base planners to run to assign the running time for each planner and to decide in what order they should be carried out to optimize a planning metric portfolio configurations are usually derived empirically from training benchmarks and remain fixed for an evaluation phase in this work we create a perinstance configurable portfolio which is able to adapt itself to every planning task the proposed system preselects a group of candidate planners using a paretodominance filtering approach and then it decides which planners to include and the time assigned according to predictive models these models estimate whether a base planner will be able to solve the given problem and if so how long it will take we define different portfolio strategies to combine the knowledge generated by the models the experimental evaluation shows that the resulting portfolios provide an improvement when compared with noninformed strategies one of the proposed portfolios was the winner of the sequential satisficing track of the international planning competition held in 2014









e  amig243 j  gonzalo j  artiles and f  verdejo 2011 combining evaluation metrics via the unanimous improvement ratio and its application to clustering tasks volume 42 pages 689718



e  burns w  ruml and m  b do 2013 heuristic search when time matters volume 47 pages 697740



in many applications of shortestpath algorithms it is impractical to   find a provably optimal solution one can only hope to achieve an appropriate balance between search time and solution cost that respects the users preferences preferences come in many forms we consider utility functions that linearly tradeoff search time and solution cost many natural utility functions can be expressed in this form for example when solution cost represents the makespan of a plan equally weighting search time and plan makespan minimizes the time from the arrival of a goal until it is achieved current stateoftheart approaches to optimizing utility functions rely on anytime algorithms and the use of extensive training data to compute a termination policy we propose a more direct approach called bugsy that incorporates the utility function directly into the search obviating the need for a separate termination policy we describe a new method based on offline parameter tuning and a novel benchmark domain for planning under time pressure based on platformstyle video games we then present what we believe to be the first empirical study of applying anytime monitoring to heuristic search and we compare it with our proposals our results suggest that the parameter tuning technique can give the best performance if a representative set of training instances is available if not then bugsy is the algorithm of choice as it performs well and does not require any offline training this work extends the tradition of research on metareasoning for search by illustrating the benefits of embedding lightweight reasoning about time into the search algorithm itself







xiaoyuan  zhu and changhe  yuan 2016 exact algorithms for mre inference volume 55 pages 653683



most relevant explanation mre is an inference task in bayesian networks that finds the most relevant partial instantiation of target variables as an explanation for given evidence by maximizing the generalized bayes factor gbf no exact mre algorithm has been developed previously except exhaustive search this paper fills the void by introducing two breadthfirst branchandbound bfbnb algorithms for solving mre based on novel upper bounds of gbf one upper bound is created by decomposing the computation of gbf using a target blanket decomposition of evidence variables the other upper bound improves the first bound in two ways one is to split the target blankets that are too large by converting auxiliary nodes into pseudotargets so as to scale to large problems the other is to perform summations instead of maximizations on some of the target variables in each target blanket our empirical evaluations show that the proposed bfbnb algorithms make exact mre inference tractable in bayesian networks that could not be solved previously







o  gim233nez and a  jonsson 2009 planning over chain causal graphs for variables with domains of size 5 is nphard volume 34 pages 675706



we study the behavior of the a search algorithm when coupled with a heuristic h satisfying 1epsilon1h  h 1epsilon2h where 0  epsilon1 epsilon2  1 are small constants and h denotes the optimal cost to a solution we prove a rigorous general upper bound on the time complexity of a search on trees that depends on both the accuracy of the heuristic and the distribution of solutions our upper bound is essentially tight in the worst case in fact we show nearly matching lower bounds that are attained even by nonadversarially chosen solution sets induced by a simple stochastic model a consequence of our rigorous results is that the effective branching factor of the search will be reduced as long as epsilon1epsilon2  1 and the number of nearoptimal solutions in the search tree is not too large we go on to provide an upper bound for a search on graphs and in this context establish a bound on running time determined by the spectrum of the graph







for almost two decades monotonic or delete free relaxation has been one of the key auxiliary tools in the practice of domainindependent deterministic planning in the particular contexts of both satisficing and optimal planning it  underlies most stateoftheart heuristic functions while satisficing planning for monotonic tasks is polynomialtime optimal planning for monotonic tasks is npequivalent here we establish both negative and positive results on the complexity of some wide fragments of optimal monotonic planning with the fragments being defined around the causal graph topology our results shed some light on the link between the complexity of general  optimal planning and the complexity of optimal planning for the respective  monotonic relaxations







we consider the iterated belief change that occurs following an alternating sequence of actions and observations  at each instant an agent has beliefs about the actions that have occurred as well as beliefs about the resulting state of the world  we represent such problems by a sequence of ranking functions so an agent assigns a quantitative plausibility value to every action and every state at each point in time  the resulting formalism is able to represent fallible belief erroneous perception exogenous actions and failed actions  we illustrate that our framework is a generalization of several existing approaches to belief change and it appropriately captures the nonelementary interaction between belief update and belief revision





j  lee and r  palla 2012 reformulating the situation calculus and the event calculus in the general theory of stable models and in answer set programming volume 43 pages 571620



circumscription and logic programs under the stable model semantics are two wellknown nonmonotonic formalisms the former has served as a basis of classical logic based action formalisms such as the situation calculus the event calculus and temporal action logics the latter has served as a basis of a family of action languages such as language a and several of its descendants based on the discovery that circumscription and the stable model semantics coincide on a class of canonical formulas we reformulate the situation calculus and the event calculus in the general theory of stable models we also present a translation that turns the reformulations further into answer set programs so that efficient answer set solvers can be applied to compute the situation calculus and the event calculus 







m  l bonet s  buss and j  johannsen 2014 improved separations of regular resolution from clause learning proof systems volume 49 pages 669703



m  babaioff and  n  nisan 2004 concurrent auctions across the supply chain volume 21 pages 595629



with the recent technological feasibility of electronic commerce over the internet much attention has been given to the design of electronic markets for various types of electronicallytradable goods such markets however will normally need to function in some relationship with markets for other related goods usually those downstream or upstream in the supply chain  thus for example an electronic market for rubber tires for trucks will likely need to be strongly influenced by the rubber market as well as by the truck market    in this paper we design protocols for exchange of information between a sequence of markets along a single supply chain  these protocols allow each of these markets to function separately while the information exchanged ensures efficient global behavior across the supply chain  each market that forms a link in the supply chain operates as a double auction where the bids on one side of the double auction come from bidders in the corresponding segment of the industry and the bids on the other side are synthetically generated by the protocol to express the combined information from all other links in the chain  the double auctions in each of the markets can be of several types and we study several variants of incentive compatible double auctions comparing them in terms of their efficiency and of the market revenue







k  su a  sattar g  lv and y  zhang 2009 variable forgetting in reasoning about knowledge volume 35 pages 677716



in this paper we investigate knowledge reasoning within a simple framework called knowledge structure we use variable forgetting as a basic operation for one agent to reason about its own or other agents knowledge in our framework two notions namely agents observable variables and the weakest sufficient condition play important roles in knowledge reasoning given a background knowledge base and a set of observable variables for each agent we show that the notion of an agent knowing a formula can be defined as a weakest sufficient condition of the formula under background knowledge base moreover we show how to capture the notion of common knowledge by using a generalized notion of weakest sufficient condition also we show that public announcement operator can be conveniently dealt with via our notion of knowledge structure further we explore the computational complexity of the problem whether an epistemic formula is realized in a knowledge structure in the general case this problem is pspacehard however for some interesting subcases it can be reduced to conp finally we discuss possible applications of our framework in some interesting domains such as the automated analysis of the wellknown muddy children puzzle and the verification of the revised needhamschroeder protocol we believe that there are many scenarios where the natural presentation of the available information about knowledge is under the form of a knowledge structure what makes it valuable compared with the corresponding multiagent s5 kripke structure is that it can be much more succinct







f  wu j  madhavan and a  halevy 2011 identifying aspects for websearch queries volume 40 pages 677700





p  nguyen m  hilario and a  kalousis 2014 using metamining to support data mining workflow planning and optimization volume 51 pages 605644



knowledge discovery in databases is a complex process that involves many different data processing and learning operators todays knowledge discovery support systems can contain several hundred operators  a major challenge is to assist the user in designing workflows which are not only valid but also  ideally  optimize some performance measure associated with the user goal  in this paper we present such a system the system relies on a metamining module which analyses past data mining experiments and extracts metamining models which associate dataset characteristics with workflow descriptors in view of workflow performance optimization the metamining model is used within a data mining workflow planner to guide the planner during the workflow planning we learn the metamining models  using a similarity learning approach and extract the workflow descriptors by mining the workflows for generalized relational patterns accounting also for domain knowledge provided by a data mining ontology  we evaluate the quality of the data mining workflows that the system produces on a collection of real world datasets coming from biology and show that it produces workflows that are significantly better than alternative methods that can only do workflow selection and not planning





d  d maua c  p de campos a  benavoli and a  antonucci 2014 probabilistic inference in credal networks new complexity results volume 50 pages 603637









a  petcu b  faltings and d  c parkes 2008 mdpop faithful distributed implementation of efficient social choice problems volume 32 pages 705755

