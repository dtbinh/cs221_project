p  cichosz 1995 truncating temporal differences on the efficient implementation of    tdlambda for reinforcement learning volume 2 pages 287318

temporal difference td methods constitute a class of   methods for learning predictions in multistep prediction problems   parameterized by a recency factor lambda currently the most important   application of these methods is to temporal credit assignment in   reinforcement learning well known reinforcement learning algorithms   such as ahc or qlearning may be viewed as instances of td learning   this paper examines the issues of the efficient and general   implementation of tdlambda for arbitrary lambda for use with   reinforcement learning algorithms optimizing the discounted sum of   rewards the traditional approach based on eligibility traces is   argued to suffer from both inefficiency and lack of generality the   ttd truncated temporal differences procedure is proposed as an   alternative that indeed only approximates tdlambda but requires   very little computation per action and can be used with arbitrary   function representation methods  the idea from which it is derived is   fairly simple and not new but probably unexplored so far encouraging   experimental results are presented suggesting that using lambda  0   with the ttd procedure allows one to obtain a significant learning   speedup at essentially the same cost as usual td0 learning

