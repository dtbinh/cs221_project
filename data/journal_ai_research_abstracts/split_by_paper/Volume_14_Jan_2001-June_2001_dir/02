n  l zhang and  w  zhang 2001 speeding up the convergence of value iteration in partially observable markov decision processes volume 14 pages 2951

partially observable markov decision processes pomdps have    recently become popular among many ai researchers because they serve    as a natural model for planning under uncertainty  value iteration is    a wellknown algorithm for finding optimal policies for pomdps  it    typically takes a large number of iterations to converge  this paper    proposes a method for accelerating the convergence of value iteration    the method has been evaluated on an array of benchmark problems and    was found to be very effective it enabled value iteration to converge    after only a few iterations on all the test problems

