p  nightingale i  p gent c  jefferson and i  miguel 2013 short and long supports for constraint propagation volume 46 pages 145

specialpurpose constraint propagation algorithms frequently make implicit use of short supports  by examining a subset of the variables they can infer support a justification that a variablevalue pair may still form part of an assignment that satisfies the constraint for all other variables and values and save substantial work  but short supports have not been studied in their own right the two main contributions of this paper are the identification of short supports as important for constraint propagation and the introduction of haggisgac an efficient and effective general purpose propagation algorithm for exploiting short supports given the complexity of haggisgac we present it as an optimised version of a simpler algorithm shortgac although experiments demonstrate the efficiency of shortgac compared with other generalpurpose propagation algorithms where a compact set of short supports is available we show theoretically and experimentally that haggisgac is even better we also find that haggisgac performs better than gacschema on fulllength supports we also introduce a variant algorithm haggisgacstable which is adapted to avoid work on backtracking and in some cases can be faster and have significant reductions in memory use all the proposed algorithms are excellent for propagating disjunctions of constraints in all experiments with disjunctions we found our algorithms to be faster than constructive or and gacschema by at least an order of magnitude and up to three orders of magnitude



e  huang and r  e korf 2013 optimal rectangle packing an absolute placement approach volume 46 pages 4787

we consider the problem of finding all enclosing rectangles of minimum area that can contain a given set of rectangles without overlap  our rectangle packer chooses the xcoordinates of all the rectangles before any of the ycoordinates we then transform the problem into a perfectpacking problem with no empty space by adding additional rectangles to determine the ycoordinates we branch on the different rectangles that can be placed in each empty position our packer allows us to extend the known solutions for a consecutivesquare benchmark from 27 to 32 squares we also introduce three new benchmarks avoiding properties that make a benchmark easy such as rectangles with shared dimensions our third benchmark consists of rectangles of increasingly high precision to pack them efficiently we limit the rectangles coordinates and the bounding box dimensions to the set of subset sums of the rectangles dimensions overall our algorithms represent the current stateoftheart for this problem outperforming other algorithms by orders of magnitude depending on the benchmark



c  sauper and r  barzilay 2013 automatic aggregation by joint modeling of aspects and values volume 46 pages 89127

we present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis  our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product eg sushi and miso for a japanese restaurant and determines the corresponding sentiment of each aspect  this approach directly enables discovery of highlyrated or inconsistent aspects of a product  our generative model admits an efficient variational meanfield inference algorithm  it is also easily extensible and we describe several modifications and their effects on model structure and inference  we test our model on two tasks joint aspect identification and sentiment analysis on a set of yelp reviews and aspect identification alone on a set of medical summaries  we evaluate the performance of the model on aspect identification sentiment analysis and perword labeling accuracy  we demonstrate that our model outperforms applicable baselines by a considerable margin yielding up to 32 relative error reduction on aspect identification and up to 20 relative error reduction on sentiment analysis



m  guo e  markakis k  r apt and v  conitzer 2013 undominated groves mechanisms volume 46 pages 129163

the family of groves mechanisms which includes the wellknown vcg mechanism also known as the clarke mechanism is a family of efficient and strategyproof mechanisms unfortunately the groves mechanisms are generally not budget balanced that is under such mechanisms payments may flow into or out of the system of the agents resulting in deficits or reduced utilities for the agents we consider the following problem within the family of groves mechanisms we want to identify mechanisms that give the agents the highest utilities under the constraint that these mechanisms must never incur deficits




v  qazvinian d  r radev s  m mohammad b  dorr d  zajic m  whidby and t  moon 2013 generating extractive summaries of scientific paradigms volume 46 pages 165201

researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature  we show how we can use citations to produce automatically generated readily consumable technical extractive summaries we first propose clexrank a model for summarizing single scientific articles based on citations which employs community detection and extracts salient informationrich sentences next we further extend our experiments  to summarize a set of papers which cover the same scientific topic we generate extractive summaries of a set of question answering qa and dependency parsing dp papers their abstracts and their citation sentences and show that citations have unique information amenable to creating a summary



h  zhao x  zhang and c  kit 2013 integrative semantic dependency parsing via efficient largescale feature selection volume 46 pages 203233

semantic parsing ie the automatic derivation of meaning representation such as an instantiated predicateargument structure for a sentence plays a critical role in deep processing of natural language unlike all other top systems of semantic dependency parsing that have to rely on a pipeline framework to chain up a series of submodels each specialized for a specific subtask the one presented in this article integrates everything into one model in hopes of achieving desirable integrity and practicality for real applications while maintaining a competitive performance this integrative approach tackles semantic parsing as a word pair classification problem using a maximum entropy classifier we leverage adaptive pruning of argument candidates and largescale feature selection engineering to allow the largest feature space ever in use so far in this field it achieves a stateoftheart performance on the evaluation data set for conll2008 shared task on top of all but one top pipeline system confirming its feasibility and effectiveness



n  goernitz m  kloft k  rieck and u  brefeld 2013 toward supervised anomaly detection volume 46 pages 235262

anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions however the predictive performance of purely unsupervised anomaly detection  often fails to match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation our first contribution shows that  classical semisupervised approaches originating from a supervised classifier are inappropriate and hardly detect  new and unknown anomalies we argue that semisupervised anomaly detection  needs to ground on the unsupervised learning paradigm and devise a novel algorithm that meets this requirement although being intrinsically nonconvex we further show that the optimization problem has a convex equivalent under relatively mild assumptions additionally we propose an active learning strategy to automatically filter candidates for  labeling in an empirical study on network intrusion detection data we observe that the proposed learning methodology requires much less labeled data than the stateoftheart while achieving higher detection accuracies



s  ordyniak and s  szeider 2013 parameterized complexity results for exact bayesian network structure learning volume 46 pages 263302

bayesian network structure learning is the notoriously difficult problem of discovering a bayesian network that optimally represents a given set of training data  in this paper we study the computational worstcase complexity of exact bayesian network structure learning under graph theoretic restrictions on the directed superstructure  the superstructure is an undirected graph that contains as subgraphs the skeletons of solution networks we introduce the directed superstructure as a natural generalization of its undirected counterpart our results apply to several variants of scorebased bayesian network structure learning where the score of a network decomposes into local scores of its nodes
results we show that exact bayesian network structure learning can be carried out in nonuniform polynomial time if the superstructure has bounded treewidth and in linear time if in addition the superstructure has bounded maximum degree furthermore we show that if the directed superstructure is acyclic then exact bayesian network structure learning can be carried out in quadratic time we complement these positive results with a number of hardness results we show that both restrictions treewidth and degree are essential and cannot be dropped without loosing uniform polynomial time tractability subject to a complexitytheoretic assumption similarly exact bayesian network structure learning remains nphard for almost acyclic directed superstructures  furthermore we show that the restrictions remain essential if we do not search for a globally optimal network but aim to improve a given network by means of at most k arc additions arc deletions or arc reversals kneighborhood local search  



a  metodi m  codish and p  j stuckey 2013 boolean equipropagation for concise and efficient sat encodings of  combinatorial problems volume 46 pages 303341

we present an approach to propagationbased sat encoding of combinatorial problems boolean equipropagation where constraints are modeled as boolean functions which propagate information about equalities between boolean literals  this information is then applied to simplify the cnf encoding of the constraints  a key factor is that considering only a small fragment of a constraint model at one time enables us to apply stronger and even complete reasoning to detect equivalent literals in that fragment once detected equivalences apply to simplify the entire constraint model and facilitate further reasoning on other fragments equipropagation in combination with partial evaluation and constraint simplification provide the foundation for a powerful approach to satbased finite domain constraint solving  we introduce a tool called bee bengurion equipropagation encoder based on these ideas and demonstrate for a variety of benchmarks that our approach leads to a considerable reduction in the size of cnf encodings and subsequent speedups in sat solving times



a  coles a  coles m  fox and d  long 2013 a hybrid lprpg heuristic for modelling numeric resource flows in planning volume 46 pages 343412

although the use of metric fluents is fundamental to many practical planning problems the study of heuristics to support fully automated planners working with these fluents remains relatively unexplored the most widely used heuristic is the relaxation of metric fluents into intervalvalued variables  an idea first proposed a decade ago other heuristics depend on domain encodings that supply additional information about fluents such as capacity constraints or other resourcerelated annotations 
we present a heuristic for numeric planning problems building on the propositional relaxed planning graph but using a mathematical program for numeric reasoning  we define a class of producerconsumer planning problems and demonstrate how the numeric constraints in these can be modelled in a mixed integer program mip  this mip is then combined with a metric relaxed planning graph rpg heuristic to produce an integrated hybrid heuristic the mip tracks resource use more accurately than the usual relaxation but relaxes the ordering of actions while the rpg captures the causal propositional aspects of the problem  we discuss how these two components interact to produce a single unified heuristic and go on to explore how further numeric features of planning problems can be integrated into the mip we show that encoding a limited subset of the propositional problem to augment the mip can yield more accurate guidance partly by exploiting structure such as propositional landmarks and propositional resources  our results show that the use of this heuristic enhances scalability on problems where numeric resource interaction is key in finding a solution



n  a snooke and m  h lee 2013 qualitative order of magnitude energyflowbased failure modes and effects analysis volume 46 pages 413447

this paper presents a structured power and energyflowbased qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow the modelling is split into two parts
the novel aspects of the work are an order of magnitudeom qualitative network analyser to represent any power domain and topology including multiple power sources a feature that was not required for earlier specialised electrical versions of the approach secondly the representation of generalised energy related behaviour as statebased local models is presented as a modelling strategy that can be more vivid and intuitive for a range of topologically complex applications than qualitative equationbased representationsthe twolevel modelling strategy allows the broad system behaviour coverage of qualitative simulation to be exploited for the fmea task while limiting the difficulties of qualitative ambiguity explanation that can arise from abstracted numerical models we have used the method to support an automated fmea system with examples of an aircraft fuel system and domestic a heating system discussed in this paper



f  a oliehoek m  t j spaan c  amato and s  whiteson 2013 incremental clustering and expansion for faster optimal planning in decpomdps volume 46 pages 449509

this article presents the stateoftheart in optimal solution methods for decentralized partially observable markov decision processes decpomdps which are general models for collaborative multiagent planning under uncertainty building off the generalized multiagent a gmaa algorithm which reduces the problem to a tree of oneshot collaborative bayesian games cbgs we describe several advances that greatly expand the range of decpomdps that can be solved optimally  first we introduce lossless incremental clustering of the cbgs solved by gmaa which achieves exponential speedups without sacrificing optimality  second we introduce incremental expansion of nodes in the gmaa search tree which avoids the need to expand all children the number of which is in the worst case doubly exponential in the nodes depth  this is particularly beneficial when little clustering is possible  in addition we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger decpomdps  we provide theoretical guarantees that when a suitable heuristic is used both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent  finally we present extensive empirical results demonstrating that gmaaice an algorithm that synthesizes these advances can optimally solve decpomdps of unprecedented size




m  ono b  c williams and lars  blackmore 2013 probabilistic planning for continuous dynamic systems under bounded risk volume 46 pages 511577

this paper presents a modelbased planner called the probabilistic sulu planner  or the psulu planner which controls stochastic systems in a goal directed manner within userspecified risk bounds the objective of the psulu planner is to allow users to command continuous stochastic systems such as unmanned aerial and space vehicles in a manner that is both intuitive and safe to this end we first develop a new plan representation called a chanceconstrained qualitative state plan ccqsp through which users can specify the desired evolution of the plant state as well as the acceptable level of risk an example of a ccqsp statement is go to a through b within 30 minutes with less than 0001 probability of failure  we then develop the psulu planner which can tractably solve a ccqsp planning problem in order to enable ccqsp planning we develop the following two capabilities in this paper 1 risksensitive planning with risk bounds and 2 goaldirected planning in a continuous domain with temporal constraints the first capability is to ensures that the probability of failure is bounded the second capability is essential for the planner to solve problems with a continuous state space such as vehicle path planning we demonstrate the capabilities of the psulu planner by simulations on two realworld scenarios the path planning and scheduling of a personal aerial vehicle as well as the space rendezvous of an autonomous cargo spacecraft 



d  h wolpert and j  w bono 2013 predicting behavior in unstructured bargaining with a probability distribution volume 46 pages 579605

in experimental tests of human behavior in unstructured bargaining games typically many joint utility outcomes are found to occur not just one this suggests we predict the outcome of such a game as a probability distribution this is in contrast to what is conventionally done eg in the nash bargaining solution which is predict a single outcome we show how to translate nashs bargaining axioms to provide a distribution over outcomes rather than a single outcome we then prove that a subset of those axioms forces the distribution over utility outcomes to be a powerlaw distribution unlike nashs original result our result holds even if the feasible set is finite when the feasible set is convex and comprehensive the mode of the power law distribution is the harsanyi bargaining solution and if we require symmetry it is the nash bargaining solution however in general these modes of the joint utility distribution are not the experimentalists bayesoptimal predictions for the joint utility nor are the bargains corresponding to the modes of those joint utility distributions the modes of the distribution over bargains in general since more than one bargain may result in the same joint utility after introducing distributional bargaining solution concepts we show how an external regulator can use them to optimally design an unstructured bargaining scenario throughout we demonstrate our analysis in computational experiments involving flight rerouting negotiations in the national airspace system we emphasize that while our results are formulated for unstructured bargaining they can also be used to make predictions for noncooperative games where the modeler knows the utility functions of the players over possible outcomes of the game but does not know the move spaces the players use to determine those outcomes



t  p michalak k  v aadithya p  l szczepanski b  ravindran and n  r jennings 2013 efficient computation of the shapley value for gametheoretic network centrality volume 46 pages 607650

the shapley valueprobably the most important normative payoff division scheme in coalitional gameshas recently been advocated as a useful measure of centrality in networks however although this approach has a variety of realworld applications including social and organisational networks biological networks and communication networks its computational properties have not been widely studied to date the only practicable approach to compute shapley valuebased centrality has been via monte carlo simulations which are computationally expensive and not guaranteed to give an exact answer against this background this paper presents the first study of the computational aspects of the shapley value for network centralities specifically we develop exact analytical formulae for shapley valuebased centrality in both weighted and unweighted networks and develop efficient polynomial time and exact algorithms based on them we empirically evaluate these algorithms on two reallife examples an infrastructure network representing the topology of the western states power grid and a collaboration network from the field of astrophysics and demonstrate that they deliver significant speedups over the monte carlo approach for instance in the case of unweighted networks our algorithms are able to return the exact solution about 1600 times faster than the monte carlo approximation even if we allow for a generous 10 error margin for the latter method



b  bagheri hariri d  calvanese m  montali g  de giacomo r  de masellis and p  felli 2013 description logic knowledge and action bases volume 46 pages 651686

description logic knowledge and action bases kab are a mechanism for providing both a semantically rich representation of the information on the domain of interest in terms of a description logic knowledge base and actions to change such information over time possibly introducing new objects we resort to a variant of dllite where the unique name assumption is not enforced and where equality between objects may be asserted and inferred actions are specified as sets of conditional effects where conditions are based on epistemic queries over the knowledge base tbox and abox and effects are expressed in terms of new aboxes in this setting we address verification of temporal properties expressed in a variant of firstorder mucalculus with quantification across states notably we show decidability of verification under a suitable restriction inspired by the notion of weak acyclicity in data exchange




s  cai k  su c  luo and a  sattar 2013 numvc an efficient local search algorithm for minimum vertex cover volume 46 pages 687716

the minimum vertex cover mvc problem is a prominent nphard combinatorial optimization problem of great importance in both theory and application local search has proved successful for this problem however there are two main drawbacks in stateoftheart mvc local search algorithms first they select a pair of vertices to exchange simultaneously which is timeconsuming secondly although using edge weighting techniques to diversify the search these algorithms lack mechanisms for decreasing the weights to address these issues we propose two new strategies twostage exchange and edge weighting with forgetting the twostage exchange strategy selects two vertices to exchange separately and performs the exchange in two stages the strategy of edge weighting with forgetting not only increases weights of uncovered edges but also decreases some weights for each edge periodically these two strategies are used in designing a new mvc local search algorithm which is referred to as numvc
we conduct extensive experimental studies on the standard benchmarks namely dimacs and bhoslib the experiment comparing numvc with stateoftheart heuristic algorithms show that numvc is at least competitive with the nearest competitor namely pls on the dimacs benchmark and clearly dominates all competitors on the bhoslib benchmark also experimental results indicate that numvc finds an optimal solution much faster than the current best exact algorithm for maximum clique on random instances as well as some structured ones moreover we study the effectiveness of the two strategies and the runtime behaviour through experimental analysis
