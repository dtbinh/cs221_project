e  manisterski d  sarne and s  kraus 2008 cooperative search with concurrent interactions volume 32 pages 136

in this paper we show how taking advantage of autonomous agents capability to maintain parallel interactions with others and incorporating it into the cooperative economic search model results in a new search strategy which outperforms current strategies in use as a framework for our analysis we use the electronic marketplace where buyer agents have the incentive to search cooperatively the new search technique is quite intuitive however its analysis and the process of extracting the optimal search strategy are associated with several significant complexities these difficulties are derived mainly from the unbounded search space and simultaneous dual affects of decisions taken along the search we provide a comprehensive analysis of the model highlighting demonstrating and proving important characteristics of the optimal search strategy consequently we manage to come up with an efficient modular algorithm for extracting the optimal cooperative search strategy for any given environment a computational based comparative illustration of the system performance using the new search technique versus the traditional methods is given emphasizing the main differences in the optimal strategys structure and the advantage of using the proposed model



a  analyti g  antoniou c  v damasio and g  wagner 2008 extended rdf as a semantic foundation of rule markup languages volume 32 pages 3794

ontologies and automated reasoning are the building blocks of the semantic web initiative derivation rules can be included in an ontology to define derived concepts based on base concepts for example rules allow to define the extension of a class or property based on a complex relation between the extensions of the same or other classes and properties on the other hand the inclusion of negative information both in the form of negationasfailure and explicit negative information is also needed to enable various forms of reasoning in this paper we extend rdf graphs with weak and strong negation as well as derivation rules the erdf stable model semantics of the extended framework extended rdf is defined extending rdfs semantics a distinctive feature of our theory which is based on partial logic is that both truth and falsity extensions of properties and classes are considered allowing for truth value gaps our framework supports both closedworld and openworld reasoning through the explicit representation of the particular closedworld assumptions and the erdf ontological categories of total properties and total classes



b  van den broek w  wiegerinck and b  kappen 2008 graphical model inference in optimal control of stochastic multiagent systems volume 32 pages 95122

in this article we consider the issue of optimal control in collaborative multiagent systems with stochastic dynamics  the agents have a joint task in which they have to reach a number of target states  the dynamics of the agents contains additive control and additive noise and the autonomous part factorizes over the agents  full observation of the global state is assumed  the goal is to minimize the accumulated joint cost which consists of integrated instantaneous costs and a joint end cost  the joint end cost expresses the joint task of the agents the instantaneous costs are quadratic in the control and factorize over the agents  the optimal control is given as a weighted linear combination of singleagent to singletarget controls  the singleagent to singletarget controls are expressed in terms of diffusion processes  these controls when not closed form expressions are formulated in terms of path integrals which are calculated approximately by metropolishastings sampling  the weights in the control are interpreted as marginals of a joint distribution over agent to target assignments  the structure of the latter is represented by a graphical model and the marginals are obtained by graphical model inference  exact inference of the graphical model will break down in large systems and so approximate inference methods are needed  we use naive mean field approximation and belief propagation to approximate the optimal control in systems with linear dynamics  we compare the approximate inference methods with the exact solution and we show that they can accurately compute the optimal control  finally we demonstrate the control method in multiagent systems with nonlinear dynamics consisting of up to 80 agents that have to reach an equal number of target states



d  terekhov and j   c beck 2008 a constraint programming approach for solving a queueing control problem volume 32 pages 123167

in a facility with front room and back room operations it is useful to switch workers between the rooms in order to cope with changing customer demand assuming stochastic customer arrival and service times we seek a policy for switching workers such that the expected customer waiting time is minimized while the expected back room staffing is sufficient to perform all work  three novel constraint programming models and several shaving procedures for these models are presented experimental results show that a model based on closedform expressions together with a combination of shaving procedures is the most efficient this model is able to find and prove optimal solutions for many problem instances within a reasonable runtime previously the only available approach was a heuristic algorithm furthermore a hybrid method combining the heuristic and the best constraint programming method is shown to perform as well as the heuristic in terms of solution quality over time while achieving the same performance in terms of proving optimality as the pure constraint programming model this is the first work of which we are aware that solves such queueingbased problems with constraint programming



c  v goldman and s  zilberstein 2008 communicationbased decomposition mechanisms for decentralized mdps volume 32 pages 169202

multiagent planning in stochastic environments can be framed formally as a decentralized markov decision problem many reallife distributed problems that arise in manufacturing multirobot coordination and information gathering scenarios can be formalized using this framework however finding the optimal solution in the general case is hard limiting the applicability of recently developed algorithms this paper provides a practical approach for solving decentralized control problems when communication among the decision makers is possible but costly  we develop the notion of communicationbased mechanism that allows us to decompose a decentralized mdp into multiple singleagent problems in this framework referred to as decentralized semimarkov decision process with direct communication decsmdpcom agents operate separately between communications we show that finding an optimal mechanism is equivalent to solving optimally a decsmdpcom we also provide a heuristic search algorithm that converges on the optimal decomposition  restricting the decomposition to some specific types of local behaviors reduces significantly the complexity of planning in particular we present a polynomialtime algorithm for the case in which individual agents perform goaloriented behaviors between communications the paper concludes with an additional tractable algorithm that enables the introduction of human knowledge thereby reducing the overall problem to finding the best time to communicate empirical results show that these approaches provide good approximate solutions



m  katz and c  domshlak 2008 new islands of tractability of costoptimal planning volume 32 pages 203288

we study the complexity of costoptimal classical planning over propositional state variables and unaryeffect actions we discover novel problem fragments for which such optimization is tractable and identify certain conditions that differentiate between tractable and intractable problems these results are based on exploiting both structural and syntactic characteristics of planning problems specifically following brafman and domshlak 2003 we relate the complexity of planning and the topology of the causal graph the main results correspond to tractability of costoptimal planning for propositional problems with polytree causal graphs that either have o1bounded indegree or are induced by actions having at most one prevail condition each almost all our tractability results are based on a constructive proof technique that connects between certain tools from planning and tractable constraint optimization and we believe this technique is of interest on its own due to a clear evidence for its robustness



f  a oliehoek m  t j spaan and n  vlassis 2008 optimal and approximate qvalue functions for decentralized pomdps volume 32 pages 289353

decisiontheoretic planning is a popular approach to sequential decision making problems because it treats uncertainty in sensing and acting in a principled way in singleagent frameworks like mdps and pomdps planning can be carried out by resorting to qvalue functions an optimal qvalue function q is computed in a recursive manner by dynamic programming and then an optimal policy is extracted from q in this paper we study whether similar qvalue functions can be defined for decentralized pomdp models decpomdps and how policies can be extracted from such value functions we define two forms of the optimal qvalue function for decpomdps one that gives a normative description as the qvalue function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation this computation however is infeasible for all but the smallest problems therefore we analyze various approximate qvalue functions that allow for efficient computation we describe how they relate and we prove that they all provide an upper bound to the optimal qvalue function q  finally unifying some previous approaches for solving decpomdps we describe a family of algorithms for extracting policies from such qvalue functions and perform an experimental evaluation on existing test problems including a new firefighting benchmark problem




f  t liu k  m ting y  yu and z  h zhou 2008 spectrum of variablerandom trees volume 32 pages 355384

in this paper we show that a continuous spectrum of randomisation exists in which most existing tree randomisations are only operating around the two ends of the spectrum  that leaves a huge part of the spectrum largely unexplored we propose a base learner vrtree which generates trees with variablerandomness  vrtrees are able to span from the conventional deterministic trees to the completerandom trees using a probabilistic parameter using vrtrees as the base models we explore the entire spectrum of randomised ensembles together with bagging and random subspace  we discover that the two halves of the spectrum have their distinct characteristics and the understanding of which allows us to propose a new approach in building better decision tree ensembles  we name this approach coalescence which coalesces a number of points in the randomhalf of the spectrum coalescence acts as a committee of experts to cater for unforeseeable conditions presented in training data  coalescence is found to perform better than any single operating point in the spectrum without the need to tune to a specific level of randomness  in our empirical study coalescence ranks top among the benchmarking ensemble methods including random forests random subspace and c5 boosting and only coalescence is significantly better than bagging and maxdiverse ensemble among all the methods in the comparison  although coalescence is not significantly better than random forests we have identified conditions under which one will perform better than the other



d  dubois h  fargier and j  bonnefon 2008 on the qualitative comparison of decisions having  positive and negative features volume 32 pages 385417

making a decision is often a matter of listing and comparing positive and negative arguments in such cases the evaluation scale for decisions should be considered bipolar that is negative and positive values should be explicitly distinguished that is what is done for example in cumulative prospect theory however contraryto the latter framework that presupposes genuine numerical assessments human agents often decide on the basis of an ordinal ranking of the pros and the cons and by focusing on the most salient arguments in other terms the decision process is qualitative as well as bipolar in this article based on a bipolar extension of possibility theory we define and axiomatically characterize several decision rules tailored for the joint handling of positive and negative arguments in an ordinal setting the simplest rules can be viewed as extensions of the maximin and maximax criteria to the bipolar case and consequently suffer from poor decisive power more decisive rules that refine the former are also proposed these refinements agree both with principles of efficiency and with the spirit of orderofmagnitude reasoning that prevails in qualitative decision theory the most refined decision rule uses leximin rankings of the pros and the cons and the ideas of counting arguments of equal strength and cancelling pros by cons it is shown to come down to a special case of cumulative prospect theory and to subsume the take the best heuristic studied by cognitive psychologists




v  bulitko m  lustrek j  schaeffer y  bjornsson and s  sigmundarson 2008 dynamic control in realtime heuristic search volume 32 pages 419452

realtime heuristic search is a challenging type of agentcentered search because the agents planning time per action is bounded by a constant independent of problem size a common problem that imposes such restrictions is pathfinding in modern computer games where a large number of units must plan their paths simultaneously over large maps common search algorithms eg a ida d ara ad are inherently not realtime and may lose completeness when a constant bound is imposed on peraction planning time realtime search algorithms retain completeness but frequently produce unacceptably suboptimal solutions in this paper we extend classic and modern realtime search algorithms with an automated mechanism for dynamic depth and subgoal selection the new algorithms remain realtime and complete on large computer game maps they find paths within 7 of optimal while on average expanding roughly a single state per action this is nearly a threefold improvement in suboptimality over the existing stateoftheart algorithms and at the same time a 15fold improvement in the amount of planning per action



b  c csaji and l  monostori 2008 adaptive stochastic resource control a machine learning approach volume 32 pages 453486

the paper investigates stochastic resource allocation problems with scarce reusable resources and nonpreemtive timedependent interconnected tasks this approach is a natural generalization of several standard resource management problems such as scheduling and transportation problems first reactive solutions are considered and defined as control policies of suitably reformulated markov decision processes mdps we argue that this reformulation has several favorable properties such as it has finite state and action spaces it is aperiodic hence all policies are proper and the space of control policies can be safely restricted next approximate dynamic programming adp methods such as fitted qlearning are suggested for computing an efficient control policy in order to compactly maintain the costtogo function two representations are studied hash tables and support vector regression svr particularly nusvrs several additional improvements such as the application of limitedlookahead rollout algorithms in the initial phases action space decomposition task clustering and distributed sampling are investigated too finally experimental results on both benchmark and industryrelated data are presented



f  stulp and m  beetz 2008 refining the execution of abstract actions with learned action models volume 32 pages 487523

robots reason about abstract actions such as go to position l in order to decide what to do or to generate plans for their intended course of action the use of abstract actions enables robots to employ small action libraries which reduces the search space for decision making when executing the actions however the robot must tailor the abstract actions to the specific task and situation context at hand
in this article we propose a novel robot action execution system that learns success and performance models for possible specializations of abstract actions at execution time the robot uses these models to optimize the execution of abstract actions to the respective task contexts the robot can so use abstract actions for efficient reasoning without compromising the performance of action execution we show the impact of our action execution model in three robotic domains and on two kinds of action execution problems 1 the instantiation of free action parameters to optimize the expected performance of action sequences 2 the automatic introduction of additional subgoals to make action sequences more reliable



s  bouveret and j  lang 2008 efficiency and envyfreeness in fair division of indivisible goods logical representation and complexity volume 32 pages 525564

we consider the problem of allocating fairly a set of indivisible goods among agents from the point of view of compact representation and computational complexity we start by assuming that agents have dichotomous preferences expressed by propositional formulae we express efficiency and envyfreeness in a logical setting which reveals unexpected connections to nonmonotonic reasoning then we identify the complexity of determining whether there exists an efficient and envyfree allocation for several notions of efficiency when preferences are represented in a succinct way as well as restrictions of this problem we first study the problem under the assumption that preferences are dichotomous and then in the general case



l  xu f  hutter h  h hoos and k  leytonbrown 2008 satzilla portfoliobased algorithm selection for sat volume 32 pages 565606




2010 ijcaijair best paper prize

it has been widely observed that there is no single dominant sat solver instead different solvers perform best on different instances rather than following the traditional approach of choosing the best solver for a given class of instances we advocate making this decision online on a perinstance basis building on previous work we describe satzilla an automated approach for constructing perinstance algorithm portfolios for sat that use socalled empirical hardness models to choose among their constituent solvers this approach takes as input a distribution of problem instances and a set of component solvers and constructs a portfolio optimizing a given objective function such as mean runtime percent of instances solved or score in a competition the excellent performance of satzilla was independently verified in the 2007 sat competition where our satzilla07 solvers won three gold one silver and one bronze medal in this article we go well beyond satzilla07 by making the portfolio construction scalable and completely automated and improving it by integrating local search solvers as candidate solvers by predicting performance score instead of runtime and by using hierarchical hardness models that take into account different types of sat instances we demonstrate the effectiveness of these new techniques in extensive experimental results on data sets including instances from the most recent sat competition



l  bordeaux m  cadoli and t  mancini 2008 a unifying framework for structural properties of csps definitions complexity tractability volume 32 pages 607629

literature on constraint satisfaction exhibits the definition of several structural properties that can be possessed by csps like inconsistency substitutability or interchangeability
because of the computational intractability of all the propertydetection problems by following the csp approach we then determine a number of relaxations which provide sufficient conditions for their tractability in particular we exploit forms of language restrictions and local reasoning



f  yang  j  culberson  r  holte u  zahavi and a  felner 2008 a general theory of additive state space abstractions volume 32 pages 631662

informally a set of abstractions of a state space s is additive if the distance between any two states in s is always greater than or equal to the sum of the corresponding distances in the abstract spaces the first known additive abstractions called disjoint pattern databases were experimentally demonstrated to produce state of the art performance on certain state spaces however previous applications were restricted to state spaces with special properties which precludes disjoint pattern databases from being defined for several commonly used testbeds such as rubiks cube topspin and the pancake puzzle in this paper we give a general definition of additive abstractions that can be applied to any state space and prove that heuristics based on additive abstractions are consistent as well as admissible we use this new definition to create additive abstractions for these testbeds and show experimentally that well chosen additive abstractions can reduce search time substantially for the 184topspin puzzle and by three orders of magnitude over state of the art methods for the 17pancake puzzle we also derive a way of testing if the heuristic value returned by additive abstractions is provably too low and show that the use of this test can reduce search time for the 15puzzle and topspin by roughly a factor of two



s  ross j  pineau s  paquet and b  chaibdraa 2008 online planning algorithms for pomdps volume 32 pages 663704

partially observable markov decision processes pomdps provide a rich framework for sequential decisionmaking under uncertainty in stochastic domains however solving a pomdp is often intractable except for small problems due to their complexity here we focus on online approaches that alleviate the computational complexity by computing good local policies at each decision step during the execution online algorithms generally consist of a lookahead search to find the best action to execute at each time step in an environment our objectives here are to survey the various existing online pomdp methods analyze their properties and discuss their advantages and disadvantages and to thoroughly evaluate these online approaches in different environments under various metrics return error bound reduction lower bound improvement our experimental results indicate that stateoftheart online heuristic search methods can handle large pomdp domains efficiently



a  petcu b  faltings and d  c parkes 2008 mdpop faithful distributed implementation of efficient social choice problems volume 32 pages 705755

in the efficient social choice problem the goal is to assign values subject to side constraints to a set of variables to maximize the total utility across a population of agents where each agent has private information about its utility function in this paper we model the social choice problem as a distributed constraint optimization problem dcop in which each agent can  communicate with other agents that share an interest in one or more variables whereas existing dcop algorithms can be easily manipulated by an agent either by misreporting private  information or deviating from the algorithm we introduce mdpop the first dcop algorithm that provides a faithful distributed implementation for efficient social choice this provides a concrete example of how the methods of mechanism design can be unified with those of distributed optimization faithfulness ensures that no agent can benefit by unilaterally deviating from any aspect of the protocol neither informationrevelation computation nor communication and whatever the private information of other agents we allow for payments by agents to a central bank which is the only central authoritythat we require to achieve faithfulness we carefully integrate the vickreyclarkegroves vcg mechanism with the dpop algorithm such that  each agent is only asked to perform computation report information and send messages that is in its own best interest determining agent is payment requires solving the social choice problem without agent i here we present a method to reuse computation performed in solving the main problem in a way that is robust against manipulation by the excluded agent experimental results on structured problems show that as much as 87 of the computation required for solving the marginal problems can be avoided by reuse providing very good scalability in the number of agents




j  delgrande y  jin and f  j pelletier 2008 compositional belief update volume 32 pages 757791

in this paper we explore a class of belief update operators in which the definition of the operator is compositional with respect to the sentence to be added the goal is to provide an update operator that is intuitive in that its definition is based on a recursive decomposition of the update sentences structure and that may be reasonably implemented  in addressing update we first provide a definition phrased in terms of the models of a knowledge base  while this operator satisfies a core group of the benchmark katsunomendelzon update postulates not all of the postulates are satisfied  other katsunomendelzon postulates can be obtained by suitably restricting the syntactic form of the sentence for update as we show  in restricting the syntactic form of the sentence for update we also obtain a hierarchy of update operators with winsletts standard semantics as the most basic interesting approach captured  we subsequently give an algorithm which captures this approach in the general case the algorithm is exponential but with some notunreasonable assumptions we obtain an algorithm that is linear in the size of the knowledge base  hence the resulting approach has much better complexity characteristics than other operators in some situations  we also explore other compositional belief change operators erasure is developed as a dual operator to update we show that a forget operator is definable in terms of update and we give a definition of the compositional revision operator  we obtain that compositional revision under the most natural definition yields the satoh revision operator



l  miclet s  bayoudh and a  delhay 2008 analogical dissimilarity definition algorithms and two experiments in machine learning volume 32 pages 793824

this  paper defines the notion of analogical dissimilarity between four objects with a special focus on objects structured as sequences firstly it studies the case where the four objects have a null analogical dissimilarity ie are in analogical proportion secondly when one of these objects is unknown it gives algorithms to compute it thirdly it tackles the problem of defining  analogical dissimilarity which is a measure of how far four objects are from being in analogical proportion in particular when objects are sequences it gives a definition and an algorithm based on an optimal alignment of the four sequences it gives also learning algorithms ie methods to find the  triple of objects in a learning sample which has the least analogical dissimilarity with a given object two practical experiments are described the first is a classification problem on benchmarks of binary and nominal data the second shows how the generation of sequences by solving analogical equations enables a handwritten character recognition system to rapidly be adapted to a new writer



g  m coghill a  srinivasan and r  d king 2008 qualitative system identification from imperfect data volume 32 pages 825877

experience in the physical sciences suggests that the only realistic means of understanding complex systems is through the use of mathematical models typically this has come to mean the identification of quantitative models expressed as differential equations quantitative modelling works best when the structure of the model ie the form of the equations is known and the primary concern is one of estimating the values of the parameters in the model for complex biological systems the modelstructure is rarely known and the modeler has to deal with both modelidentification and parameterestimation in this paper we are concerned with providing automated assistance to the first of these problems specifically we examine the identification by machine of the structural relationships between experimentally observed variables these relationship will be expressed in the form of qualitative abstractions of a quantitative model such qualitative models may not only provide clues to the precise quantitative model but also assist in understanding the essence of  that model our position in this paper is that background knowledge incorporating system modelling principles can be used to constrain effectively the set of good qualitative models utilising the modelidentification framework provided by inductive logic programming ilp we present empirical support for this position using a series of increasingly complex artificial datasets the results are obtained with qualitative and quantitative data subject to varying amounts of noise and different degrees of sparsity  the results also point to the presence of a set of qualitative states which we term kernel subsets that may be necessary for a qualitative modellearner to learn correct models we demonstrate scalability of the method to biological system modelling by identification of the glycolysis metabolic pathway from data



y  wang n  l zhang and t  chen 2008 latent tree models and approximate inference in bayesian networks volume 32 pages 879900

we propose a novel method for approximate inference in bayesian networks bns the idea is to sample data from a bn learn a latent tree model ltm from the data offline and when online make inference with the ltm instead of the original bn because ltms are treestructured inference takes linear time in the meantime they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost



n  c a moore and p  prosser 2008 the ultrametric constraint and its application to phylogenetics volume 32 pages 901938

a phylogenetic tree shows the evolutionary relationships among species internal nodes of the tree represent speciation events and leaf nodes correspond to species a goal of phylogenetics is to combine such trees into larger trees called supertrees whilst respecting the relationships in the original trees  a rooted tree exhibits an ultrametric property that is for any three leaves of the tree it must be that one pair has a deeper most recent common ancestor than the other pairs or that all three have the same most recent common ancestor this inspires a constraint programming encoding for rooted trees we present an efficient constraint that enforces the ultrametric property over a symmetric array of constrained integer variables with the inevitable property that the lower bounds of any three variables are mutually supportive we show that this allows an efficient constraintbased solution to the supertree construction problem we demonstrate that the versatility of constraint programming can be exploited to allow solutions to variants of the supertree construction problem



e  h gerding r  k dash a  byde and n  r jennings 2008 optimal strategies for simultaneous vickrey auctions with perfect substitutes volume 32 pages 939982

we derive optimal strategies for a bidding agent that participates in multiple simultaneous secondprice auctions with perfect substitutes we prove that if everyone else bids locally in a single auction the global bidder should always place nonzero bids in all available auctions provided there are no budget constraints with a budget however the optimal strategy is to bid locally if this budget is equal or less than the valuation furthermore for a wide range of valuation distributions we prove that the problem of finding the optimal bids reduces to two dimensions if all auctions are identical finally we address markets with both sequential and simultaneous auctions nonidentical auctions and the allocative efficiency of the market
