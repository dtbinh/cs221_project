b  bidyuk and r  dechter 2007 cutset sampling for bayesian networks volume 28 pages 148

the paper presents a new sampling methodology for bayesian networks that samples only a subset of variables and applies exact inference to the rest  cutset sampling is a network structureexploiting application of the raoblackwellisation principle to sampling in bayesian networks  it improves convergence by exploiting memorybased inference algorithms  it can also be viewed as an anytime approximation of the exact cutsetconditioning algorithm developed by pearl  cutset sampling can be implemented efficiently when the sampled variables constitute a loopcutset of the bayesian network and more generally when the induced width of the networks graph conditioned on the observed sampled variables is bounded by a constant w  we demonstrate empirically the benefit of this scheme on a range of benchmarks



p  everaere s  konieczny and p  marquis 2007 the strategyproofness landscape of merging volume 28 pages 49105

merging operators aim at defining the beliefsgoals of a group of agents from the beliefsgoals of each member of the group whenever an agent of the group has preferences over the possible results of the merging process ie the possible merged bases she can try to rig the merging process by lying on her true beliefsgoals if this leads to better merged base according to her point of view obviously strategyproof operators are highly desirable in order to guarantee equity among agents even when some of them are not sincere in this paper we draw the strategyproof landscape for many merging operators from the literature including modelbased ones and formulabased ones both the general case and several restrictions on the merging process are considered



h  jia c  moore and d  strain 2007 generating hard satisfiable formulas by hiding solutions deceptively volume 28 pages 107118

to test incomplete search algorithms for constraint satisfaction problems such as 3sat we need a source of hard but satisfiable benchmark instances  a simple way to do this is to choose a random truth assignment a and then choose clauses randomly from among those satisfied by a however this method tends to produce easy problems since the majority of literals point toward the hidden assignment a last year achlioptas jia and moore proposed a problem generator that cancels this effect by hiding both a and its complement while the resulting formulas appear to be just as hard for dpll algorithms as random 3sat formulas with no hidden assignment they can be solved by walksat in only polynomial time  
here we propose a new method to cancel the attraction to a by choosing a clause with t  0 literals satisfied by a with probability proportional to qt for some q  1  by varying q we can generate formulas whose variables have no bias ie which are equally likely to be true or false we can even cause the formula to deceptively point away from a  we present theoretical and experimental results suggesting that these formulas are exponentially hard both for dpll algorithms and for incomplete algorithms such as walksat



a  i coles and a  j smith 2007 marvin a heuristic search planner with online macroaction learning volume 28 pages 119156

this paper describes marvin a planner that competed in the fourth international planning competition ipc 4 marvin uses actionsequencememoisation techniques to generate macroactions which are then used during search for a solution plan we provide an overview of its architecture and search behaviour detailing the algorithms used we also empirically demonstrate the effectiveness of its features in various planning domains in particular the effects on performance due to the use of macroactions the novel features of its search behaviour and the native support of adl and derived predicates



a  d  procaccia and j  s rosenschein 2007 junta distributions and the averagecase complexity of manipulating elections volume 28 pages 157181

encouraging voters to truthfully reveal their preferences in an election has long been an important issue recently computational complexity has been suggested as a means of precluding strategic behavior previous studies have shown that some voting protocols are hard to manipulate but used nphardness as the complexity measure such a worstcase analysis may be an insufficient guarantee of resistance to manipulation
indeed we demonstrate that nphard manipulations may be tractable in the average case for this purpose we augment the existing theory of averagecase complexity with some new concepts in particular we consider elections distributed with respect to junta distributions which concentrate on hard instances we use our techniques to prove that scoring protocols are susceptible to manipulation by coalitions when the number of candidates is constant



j  c beck and n  wilson 2007 proactive algorithms for job shop scheduling with probabilistic durations volume 28 pages 183232

most classical scheduling formulations assume a fixed and known duration for each activity  in this paper we weaken this assumption requiring instead that each duration can be represented by an independent random variable with a known mean and variance the best solutions are ones which have a high probability of achieving a good makespan we first create a theoretical framework formally showing how monte carlo simulation can be combined with deterministic scheduling algorithms to solve this problem  we propose an associated deterministic scheduling problem whose solution is proved under certain conditions to be a lower bound for the probabilistic problem we then propose and investigate a number of techniques for solving such problems based on combinations of monte carlo simulation solutions to the associated deterministic problem and either constraint programming or tabu search our empirical results demonstrate that a combination of the use of the associated deterministic problem and monte carlo simulation results in algorithms that scale best both in terms of problem size and uncertainty further experiments point to the correlation between the quality of the deterministic solution and the quality of the probabilistic solution as a major factor responsible for this success



l  blumrosen n  nisan and i  segal 2007 auctions with severely bounded communication volume 28 pages 233266

we study auctions with severe bounds on the communication allowed each bidder  may only transmit t bits of information to the auctioneer we consider both welfare and profitmaximizing auctions under this communication restriction for both measures we determine the optimal auction and show that the loss incurred relative to unconstrained auctions is mild we prove nonsurprising properties of these kinds of auctions eg that in optimal mechanisms bidders  simply report the interval in which their valuation lies in  as well as some surprising properties eg that asymmetric auctions  are better than symmetric ones and that multiround auctions reduce the communication complexity only by a linear factor



e  a hansen and r  zhou 2007 anytime heuristic search volume 28 pages 267297

we describe how to convert the heuristic search algorithm a into an anytime algorithm that finds a sequence of improved solutions and eventually converges to an optimal solution the approach we adopt uses weighted heuristic search to find an approximate solution quickly and then continues the weighted search to find improved solutions as well as to improve a bound on the suboptimality of the current solution when the time available to solve a search problem is limited or uncertain this creates an anytime heuristic search algorithm that allows a flexible tradeoff between search time and solution quality we analyze the properties of the resulting anytime a algorithm and consider its performance in three domains slidingtile puzzles strips planning and multiple sequence alignment to illustrate the generality of this approach we also describe how to transform the memoryefficient search algorithm recursive bestfirst search rbfs into an anytime algorithm



c  bettini s  mascetti and x   s wang 2007 supporting temporal reasoning by mapping calendar expressions to minimal periodic sets volume 28 pages 299348

in the recent years several research efforts have focused on the concept of time granularity and its applications a first stream of research investigated the mathematical models behind the notion of granularity and the algorithms to manage temporal data based on those models a second stream of research investigated symbolic formalisms providing a set of algebraic operators to define granularities in a compact and compositional way however only very limited manipulation algorithms have been proposed to operate directly on the algebraic representation making it unsuitable to use the symbolic formalisms in applications that need manipulation of granularities
from a technical point of view we propose an hybrid algorithm that interleaves the conversion of calendar subexpressions into periodical sets with the minimization of the period length the algorithm returns setbased granularity representations having minimal period length which is the most relevant parameter for the performance of the considered reasoning services extensive experimental work supports the techniques used in the algorithm and shows the efficiency and effectiveness of the algorithm



s  r jodogne and j  h piater 2007 closedloop learning of visual control policies volume 28 pages 349391

in this paper we present a general flexible framework for learning mappings from images to actions by interacting with the environment the basic idea is to introduce a featurebased image classifier in front of a reinforcement learning algorithm the classifier partitions the visual space according to the presence or absence of few highly informative local descriptors that are incrementally selected in a sequence of attempts to remove perceptual aliasing we also address the problem of fighting overfitting in such a greedy algorithm finally we show how highlevel visual features can be generated when the power of local descriptors is insufficient for completely disambiguating the aliased states this is done by building a hierarchy of composite features that consist of recursive spatial combinations of visual features we demonstrate the efficacy of our algorithms by solving three visual navigation tasks and a visual version of the classical car on the hill control problem



a  s fukunaga and r  e korf 2007 bin completion algorithms for multicontainer packing knapsack and covering problems volume 28 pages 393429

many combinatorial optimization problems such as the bin packing and multiple knapsack problems involve assigning a set of discrete objects to multiple containers these problems can be used to model task and resource allocation problems in multiagent systems and distributed systms and can also be found as subproblems of scheduling problems we propose bin completion a branchandbound strategy for onedimensional multicontainer packing problems  bin completion combines a binoriented search space with a powerful dominance criterion that enables us to prune much of the space the performance of the basic bin completion framework can be enhanced by using a number of extensions including nogoodbased pruning techniques that allow further exploitation of the dominance criterion  bin completion is applied to four problems multiple knapsack bin covering mincost covering and bin packing  we show that our bin completion algorithms yield new stateoftheart results for the multiple knapsack bin covering and mincost covering problems outperforming previous algorithms by several orders of magnitude with respect to runtime on some classes of hard random problem instances  for the bin packing problem we demonstrate significant improvements compared to most previous results but show that bin completion is not competitive with current stateoftheart cuttingstock based approaches



f  lin and y  chen 2007 discovering classes of strongly equivalent logic programs volume 28 pages 431451

in this paper we apply computeraided theorem discovery technique to discover theorems about strongly equivalent logic programs under the answer set semantics our discovered theorems capture new classes of strongly equivalent logic programs that can lead to new program simplification rules that preserve strong equivalence specifically with the help of computers we discovered exact conditions that capture the strong equivalence between a rule and the empty set between two rules between two rules and one of the two rules between two rules and another rule and between three rules and two of the three rules



b  j clement e  h durfee and a  c barrett 2007 abstract reasoning for planning and coordination volume 28 pages 453515

the judicious use of abstraction can help planning agents to identify key interactions between actions and resolve them without getting bogged down in details however ignoring the wrong details can lead agents into building plans that do not work or into costly backtracking and replanning once overlooked interdependencies come to light we claim that associating systematicallygenerated summary information with plans abstract operators can ensure plan correctness even for asynchronouslyexecuted plans that must be coordinated across multiple agents while still achieving valuable efficiency gains in this paper we formally characterize hierarchical plans whose actions have temporal extent and describe a principled method for deriving summarized state and metric resource information for such actions we provide sound and complete algorithms along with heuristics to exploit summary information during hierarchical refinement planning and plan coordination our analyses and experiments show that under clearcut and reasonable conditions using summary information can speed planning as much as doubly exponentially even for plans involving interacting subproblems



y  gao and j  culberson 2007 consistency and random constraint satisfaction models volume 28 pages 517557

in this paper we study the possibility of designing nontrivial random csp models by exploiting the intrinsic connection between structures and typicalcase hardness we show that constraint consistency a notion that has been developed to improve  the efficiency of csp algorithms is in fact the key to the design of random csp models that have interesting phase transition behavior and guaranteed exponential resolution complexity without putting much restriction on the parameter of constraint tightness or the domain size of the problem  we propose a very flexible framework for constructing problem instances withinteresting behavior and develop a variety of concrete methods to construct specific  random csp models that enforce different levels of constraint consistency 
a series of experimental studies with interesting observations are carried out to illustrate the effectiveness of introducing structural elements in random instances to verify the robustness of our proposal and to investigate features of some specific models based on our framework that are highly related to the behavior of backtracking search algorithms      
