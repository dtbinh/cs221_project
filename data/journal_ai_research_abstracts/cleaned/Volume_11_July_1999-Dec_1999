c  boutilier  t  dean and  s  hanks 1999 decisiontheoretic planning structural assumptions and computational leverage volume 11 pages 194

planning under uncertainty is a central problem in the study of    automated sequential decision making and has been addressed by    researchers in many different fields including ai planning decision    analysis operations research control theory and economics  while    the assumptions and perspectives adopted in these areas often differ    in substantial ways many planning problems of interest to researchers    in these fields can be modeled as markov decision processes mdps    and analyzed using the techniques of decision theory       this paper presents an overview and synthesis of mdprelated methods    showing how they provide a unifying framework for modeling many    classes of planning problems studied in ai it also describes    structural properties of mdps that when exhibited by particular    classes of problems can be exploited in the construction of optimal    or approximately optimal policies or plans  planning problems    commonly possess structure in the reward and value functions used to    describe performance criteria in the functions used to describe state    transitions and observations and in the relationships among features    used to describe states actions rewards and observations       specialized representations and algorithms employing these    representations can achieve computational leverage by exploiting    these various forms of structure  certain ai techniques  in    particular those based on the use of structured intensional    representations  can be viewed in this way  this paper surveys    several types of representations for both classical and    decisiontheoretic planning problems and planning algorithms that    exploit these representations in a number of different ways to ease    the computational burden of constructing policies or plans  it focuses    primarily on abstraction aggregation and decomposition techniques    based on aistyle representations



p  resnik 1999 semantic similarity in a taxonomy an informationbased measure and its application to problems of ambiguity in natural language volume 11 pages 95130

this article presents a measure of semantic similarity in an    isa taxonomy based on the notion of shared information content    experimental evaluation against a benchmark set of human similarity    judgments demonstrates that the measure performs better than the    traditional edgecounting approach  the article presents algorithms    that take advantage of taxonomic similarity in resolving syntactic and    semantic ambiguity along with experimental results demonstrating    their effectiveness



c  e brodley and  m  a friedl 1999 identifying mislabeled training data volume 11 pages 131167

this paper presents a new approach to identifying and    eliminating mislabeled training instances for supervised learning the    goal of this approach is to improve classification accuracies produced    by learning algorithms by improving the quality of the training data    our approach uses a set of learning algorithms to create classifiers    that serve as noise filters for the training data  we evaluate single    algorithm majority vote and consensus filters on five datasets that    are prone to labeling errors  our experiments illustrate that    filtering significantly improves classification accuracy for noise    levels up to 30 percent  an analytical and empirical evaluation of    the precision of our approach shows that consensus filters are    conservative at throwing away good data at the expense of retaining    bad data and that majority filters are better at detecting bad data at    the expense of throwing away good data  this suggests that for    situations in which there is a paucity of data consensus filters are    preferable whereas majority vote filters are preferable for    situations with an abundance of data



d  opitz and  r  maclin 1999 popular ensemble methods an empirical study volume 11 pages 169198

an ensemble consists of a set of individually trained    classifiers such as neural networks or decision trees whose    predictions are combined when classifying novel instances  previous    research has shown that an ensemble is often more accurate than any of    the single classifiers in the ensemble  bagging breiman 1996c and    boosting freund  shapire 1996 shapire 1990 are two relatively    new but popular methods for producing ensembles  in this paper we    evaluate these methods on 23 data sets using both neural networks and    decision trees as our classification algorithm  our results clearly    indicate a number of conclusions  first while bagging is almost    always more accurate than a single classifier it is sometimes much    less accurate than boosting  on the other hand boosting can create    ensembles that are less accurate than a single classifier     especially when using neural networks  analysis indicates that the    performance of the boosting methods is dependent on the    characteristics of the data set being examined  in fact further    results show that boosting ensembles may overfit noisy data sets thus    decreasing its performance  finally consistent with previous    studies our work suggests that most of the gain in an ensembles    performance comes in the first few classifiers combined however    relatively large gains can be seen up to 25 classifiers when boosting    decision trees



d  calvanese  m  lenzerini and  d  nardi 1999 unifying classbased representation formalisms volume 11 pages 199240

the notion of class is ubiquitous in computer science and is    central in many formalisms for the representation of structured    knowledge used both in knowledge representation and in databases  in    this paper we study the basic issues underlying such representation    formalisms and single out both their common characteristics and their    distinguishing features  such investigation leads us to propose a    unifying framework in which we are able to capture the fundamental    aspects of several representation languages used in different    contexts  the proposed formalism is expressed in the style of    description logics which have been introduced in knowledge    representation as a means to provide a semantically wellfounded basis    for the structural aspects of knowledge representation systems the    description logic considered in this paper is a subset of first order    logic with nice computational characteristics  it is quite expressive    and features a novel combination of constructs that has not been    studied before  the distinguishing constructs are number    restrictions which generalize existence and functional dependencies    inverse roles which allow one to refer to the inverse of a    relationship and possibly cyclic assertions which are necessary for    capturing real world domains  we are able to show that it is    precisely such combination of constructs that makes our logic powerful    enough to model the essential set of features for defining class    structures that are common to frame systems objectoriented database    languages and semantic data models  as a consequence of the    established correspondences several significant extensions of each of    the above formalisms become available the high expressiveness of the    logic we propose and the need for capturing the reasoning in different    contexts forces us to distinguish between unrestricted and finite    model reasoning  a notable feature of our proposal is that reasoning    in both cases is decidable  we argue that by virtue of the high    expressive power and of the associated reasoning capabilities on both    unrestricted and finite models our logic provides a common core for    classbased representation formalisms



d  e moriarty  a  c schultz and  j  j grefenstette 1999 evolutionary algorithms for reinforcement learning volume 11 pages 241276

there are two distinct approaches to solving reinforcement    learning problems namely searching in value function space and    searching in policy space  temporal difference methods and    evolutionary algorithms are wellknown examples of these approaches    kaelbling littman and moore recently provided an informative survey    of temporal difference methods  this article focuses on the    application of evolutionary algorithms to the reinforcement learning    problem emphasizing alternative policy representations credit    assignment methods and problemspecific genetic operators  strengths    and weaknesses of the evolutionary approach to reinforcement learning    are presented along with a survey of representative applications



r  rosati 1999 reasoning about minimal belief and negation as failure volume 11 pages 277300

we investigate the problem of reasoning in the propositional    fragment of mbnf the logic of minimal belief and negation as failure    introduced by lifschitz which can be considered as a unifying    framework for several nonmonotonic formalisms including default    logic autoepistemic logic circumscription epistemic queries and    logic programming  we characterize the complexity and provide    algorithms for reasoning in propositional mbnf  in particular we    show that entailment in propositional mbnf lies at the third level of    the polynomial hierarchy hence it is harder than reasoning in all the    above mentioned propositional formalisms for nonmonotonic reasoning    we also prove the exact correspondence between negation as failure in    mbnf and negative introspection in moores autoepistemic logic



f  ygge and  h  akkermans 1999 decentralized markets versus central control a comparative study volume 11 pages 301333




2004 ijcaijair best paper prize

multiagent systems mas promise to offer solutions to    problems where established older paradigms fall short in order to    validate such claims that are repeatedly made in software agent    publications empirical indepth studies of advantages and weaknesses    of multiagent solutions versus conventional ones in practical    applications are needed climate control in large buildings is one    application area where multiagent systems and marketoriented    programming in particular have been reported to be very successful    although central control solutions are still the standard practice    we have therefore constructed and implemented a variety of market    designs for this problem as well as different standard control    engineering solutions this article gives a detailed analysis and    comparison so as to learn about differences between standard versus    agent approaches and yielding new insights about benefits and    limitations of computational markets an important outcome is that    local information plus market communication produces global    control



s  argamonengelson and  i  dagan 1999 committeebased sample selection for probabilistic classifiers volume 11 pages 335360

in many realworld learning tasks it is expensive to    acquire a sufficient number of labeled examples for training  this    paper investigates methods for reducing annotation cost by sample    selection in this approach during training the learning program    examines many unlabeled examples and selects for labeling only those    that are most informative at each stage this avoids redundantly    labeling examples that contribute little new information       our work follows on previous research on query by committee extending    the committeebased paradigm to the context of probabilistic    classification  we describe a family of empirical methods for    committeebased sample selection in probabilistic classification    models which evaluate the informativeness of an example by measuring    the degree of disagreement between several model variants  these    variants the committee are drawn randomly from a probability    distribution conditioned by the training set labeled so far       the method was applied to the realworld natural language processing    task of stochastic partofspeech tagging  we find that all variants    of the method achieve a significant reduction in annotation cost    although their computational efficiency differs  in particular the    simplest variant a two member committee with no parameters to tune    gives excellent results  we also show that sample selection yields a    significant reduction in the size of the model used by the tagger



m  cristani 1999 the complexity of reasoning about spatial congruence volume 11 pages 361390

in the recent literature of artificial intelligence an    intensive research effort has been spent for various algebras of    qualitative relations used in the representation of temporal and    spatial knowledge on the problem of classifying the computational    complexity of reasoning problems for subsets of algebras  the main    purpose of these researches is to describe a restricted set of maximal    tractable subalgebras ideally in an exhaustive fashion with respect    to the hosting algebras      in this paper we introduce a novel algebra for reasoning about spatial    congruence show that the satisfiability problem in the spatial    algebra mc4 is npcomplete and present a complete classification of    tractability in the algebra based on the individuation of three    maximal tractable subclasses one containing the basic relations  the    three algebras are formed by 14 10 and 9 relations out of 16 which    form the full algebra



d  fox  w  burgard and  s  thrun 1999 markov localization for mobile robots in dynamic environments volume 11 pages 391427




honorable mention for the 2004 ijcaijair best paper prize

localization that is the estimation of a robots location    from sensor data is a fundamental problem in mobile robotics  this    papers presents a version of markov localization which provides    accurate position estimates and which is tailored towards dynamic    environments the key idea of markov localization is to maintain a    probability density over the space of all locations of a robot in its    environment our approach represents this space metrically using a    finegrained grid to approximate densities  it is able to globally    localize the robot from scratch and to recover from localization    failures  it is robust to approximate models of the environment such    as occupancy grid maps and noisy sensors such as ultrasound    sensors  our approach also includes a filtering technique which    allows a mobile robot to reliably estimate its position even in    densely populated environments in which crowds of people block the    robots sensors for extended periods of time  the method described    here has been implemented and tested in several realworld    applications of mobile robots including the deployments of two mobile    robots as interactive museum tourguides



j  y halpern 1999 coxs theorem revisited volume 11 pages 429435

the assumptions needed to prove coxs theorem are discussed    and examined  various sets of assumptions under which a coxstyle    theorem can be proved are provided although all are rather strong    and arguably not natural
