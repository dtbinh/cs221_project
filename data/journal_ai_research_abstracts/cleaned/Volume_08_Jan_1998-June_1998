j  engelfriet 1998 monotonicity and persistence in preferential logics volume 8 pages 121

an important characteristic of many logics for artificial    intelligence is their nonmonotonicity this means that adding a    formula to the premises can invalidate some of the consequences there    may however exist formulae that can always be safely added to the    premises without destroying any of the consequences we say they    respect monotonicity also there may be formulae that when they are    a consequence can not be invalidated when adding any formula to the    premises we call them conservative we study these two classes of    formulae for preferential logics and show that they are closely    linked to the formulae whose truthvalue is preserved along the    preferential ordering we will consider some preferential logics for    illustration and prove syntactic characterization results for them    the results in this paper may improve the efficiency of theorem    provers for preferential logics



g  gogic  c  h papadimitriou and  m  sideri 1998 incremental recompilation of knowledge volume 8 pages 2337

approximating a general formula from above and below by horn    formulas its horn envelope and horn core respectively was proposed    by selman and kautz 1991 1996 as a form of knowledge    compilation supporting rapid approximate reasoning on the negative    side this scheme is static in that it supports no updates and has    certain complexity drawbacks pointed out by kavvadias papadimitriou    and sideri 1993  on the other hand the many frameworks and schemes    proposed in the literature for theory update and revision are plagued    by serious complexitytheoretic impediments even in the horn case as    was pointed out by eiter and gottlob 1992 and is further    demonstrated in the present paper  more fundamentally these schemes    are not inductive in that they may lose in a single update any    positive properties of the represented sets of formulas small size    horn structure etc  in this paper we propose a new scheme    incremental recompilation which combines horn approximation and    modelbased updates this scheme is inductive and very efficient free    of the problems facing its constituents  a set of formulas is    represented by an upper and lower horn approximation  to update we    replace the upper horn formula by the horn envelope of its    minimumchange update and similarly the lower one by the horn core of    its update the key fact which enables this scheme is that horn    envelopes and cores are easy to compute when the underlying formula is    the result of a minimumchange update of a horn formula by a clause    we conjecture that efficient algorithms are possible for more complex    updates



s  argamonengelson and  m  koppel 1998 tractability of theory patching volume 8 pages 3965

in this paper we consider the problem of theory patching    in which we are given a domain theory some of whose components are    indicated to be possibly flawed and a set of labeled training    examples for the domain concept  the theory patching problem is to    revise only the indicated components of the theory such that the    resulting theory correctly classifies all the training examples    theory patching is thus a type of theory revision in which revisions    are made to individual components of the theory  our concern in this    paper is to determine for which classes of logical domain theories the    theory patching problem is tractable  we consider both propositional    and firstorder domain theories and show that the theory patching    problem is equivalent to that of determining what information    contained in a theory is stable regardless of what revisions might    be performed to the theory  we show that determining stability is    tractable if the input theory satisfies two conditions that revisions    to each theory component have monotonic effects on the classification    of examples and that theory components act independently in the    classification of examples in the theory  we also show how the    concepts introduced can be used to determine the soundness and    completeness of particular theory patching algorithms



a  moore and  m  s lee 1998 cached sufficient statistics for efficient machine learning with large datasets volume 8 pages 6791

this paper introduces new algorithms and data structures for    quick counting for machine learning datasets  we focus on the    counting task of constructing contingency tables but our approach is    also applicable to counting the number of records in a dataset that    match conjunctive queries  subject to certain assumptions the costs    of these operations can be shown to be independent of the number of    records in the dataset and loglinear in the number of nonzero entries    in the contingency table      we provide a very sparse data structure the adtree to minimize    memory use we provide analytical worstcase bounds for this structure    for several models of data distribution  we empirically demonstrate    that tractablysized data structures can be produced for large    realworld datasets by a using a sparse tree structure that never    allocates memory for counts of zero b never allocating memory for    counts that can be deduced from other counts and c not bothering to    expand the tree fully near its leaves       we show how the adtree can be used to accelerate bayes net structure    finding algorithms rule learning algorithms and feature selection    algorithms and we provide a number of empirical results comparing    adtree methods against traditional direct counting approaches  we    also discuss the possible uses of adtrees in other machine learning    methods and discuss the merits of adtrees in comparison with    alternative representations such as kdtrees rtrees and frequent sets



b  srivastava and  s  kambhampati 1998 synthesizing customized planners from specifications volume 8 pages 93128

existing plan synthesis approaches in artificial    intelligence fall into two categories  domain independent and domain    dependent  the domain independent approaches are applicable across a    variety of domains but may not be very efficient in any one given    domain  the domain dependent approaches need to be redesigned for    each domain separately but can be very efficient in the domain for    which they are designed  one enticing alternative to these approaches    is to automatically synthesize domain independent planners given the    knowledge about the domain and the theory of planning in this paper    we investigate the feasibility of using existing automated software    synthesis tools to support such synthesis specifically we describe    an architecture called clay in which the kestrel interactive    development system kids is used to derive a domaincustomized    planner through a semiautomatic combination of a declarative theory    of planning and the declarative control knowledge specific to a given    domain to semiautomatically combine them to derive domaincustomized    planners  we discuss what it means to write a declarative theory of    planning and control knowledge for kids and illustrate our approach    by generating a class of domainspecific planners using state space    refinements  our experiments show that the synthesized planners can    outperform classical refinement planners implemented as    instantiations of ucp kambhampati  srivastava 1995 using the same    control knowledge  we will contrast the costs and benefits of the    synthesis approach with conventional methods for customizing domain    independent planners



j  fuernkranz 1998 integrative windowing volume 8 pages 129164

in this paper we reinvestigate windowing for rule learning    algorithms  we show that contrary to previous results for decision    tree learning windowing can in fact achieve significant runtime    gains in noisefree domains and explain the different behavior of rule    learning algorithms by the fact that they learn each rule    independently the main contribution of this paper is integrative    windowing a new type of algorithm that further exploits this property    by integrating good rules into the final theory right after they have    been discovered thus it avoids relearning these rules in subsequent    iterations of the windowing process experimental evidence in a    variety of noisefree domains shows that integrative windowing can in    fact achieve substantial runtime gains furthermore we discuss the    problem of noise in windowing and present an algorithm that is able to    achieve runtime gains in a set of experiments in a simple domain with    artificial noise



a  darwiche 1998 modelbased diagnosis using structured system descriptions volume 8 pages 165222

this paper presents a comprehensive approach for modelbased    diagnosis which includes proposals for characterizing and computing    preferred diagnoses assuming that the system description is augmented    with a system structure a directed graph explicating the    interconnections between system components  specifically we first    introduce the notion of a consequence which is a syntactically    unconstrained propositional sentence that characterizes all    consistencybased diagnoses and show that standard characterizations    of diagnoses such as minimal conflicts correspond to syntactic    variations on a consequence second we propose a new syntactic    variation on the consequence known as negation normal form nnf and    discuss its merits compared to standard variations  third we    introduce a basic algorithm for computing consequences in nnf given a    structured system description we show that if the system structure    does not contain cycles then there is always a linearsize    consequence in nnf which can be computed in linear time for arbitrary    system structures we show a precise connection between the complexity    of computing consequences and the topology of the underlying system    structure  finally we present an algorithm that enumerates the    preferred diagnoses characterized by a consequence the algorithm is    shown to take linear time in the size of the consequence if the    preference criterion satisfies some general conditions



l  finkelstein and  s  markovitch 1998 a selective macrolearning algorithm and its application to the nxn slidingtile puzzle volume 8 pages 223263

one of the most common mechanisms used for speeding up    problem solvers is macrolearning  macros are sequences of basic    operators acquired during problem solving  macros are used by the    problem solver as if they were basic operators  the major problem    that macrolearning presents is the vast number of macros that are    available for acquisition  macros increase the branching factor of    the search space and can severely degrade problemsolving efficiency    to make macro learning useful a program must be selective in    acquiring and utilizing macros  this paper describes a general method    for selective acquisition of macros solvable training problems are    generated in increasing order of difficulty  the only macros acquired    are those that take the problem solver out of a local minimum to a    better state  the utility of the method is demonstrated in several    domains including the domain of nxn slidingtile puzzles after    learning on small puzzles the system is able to efficiently solve    puzzles of any size
