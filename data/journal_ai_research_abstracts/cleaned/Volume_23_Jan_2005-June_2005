n  roy  g  gordon and  s  thrun 2005 finding approximate pomdp solutions through belief compression volume 23 pages 140

standard value function approaches to finding policies for partially observable markov decision processes pomdps are generally considered to be intractable for large models the intractability of these algorithms is to a large extent a consequence of computing an exact optimal policy over the entire belief space  however in realworld pomdp problems computing the optimal policy for the full belief space is often unnecessary for good control even for problems with complicated policy classes the beliefs experienced by the controller often lie near a structured lowdimensional subspace embedded in the highdimensional belief space finding a good approximation to the optimal value function for only this subspace can be much easier than computing the full value functionp p  we introduce a new method for solving largescale pomdps by reducing the dimensionality of the belief space we use exponential family principal components analysis collins dasgupta  schapire 2002 to represent sparse highdimensional belief spaces using small sets of learned features of the belief state we then plan only in terms of the lowdimensional belief features by planning in this lowdimensional space we can find policies for pomdp models that are orders of magnitude larger than models that can be handled by conventional techniquesp p we demonstrate the use of this algorithm on a synthetic problem and on mobile robot navigation tasks



p  e dunne 2005 extremal behaviour in multiagent contract negotiation volume 23 pages 4178

we examine properties of a model of resource allocation in which several agents exchange resources in order to optimise their individual holdings the schemes discussed relate to wellknown negotiation protocols proposed in earlier work and we consider a number of alternative notions of rationality covering both quantitative measures eg cooperative and individual rationality and more qualitative forms eg pigoudalton transfers while it is known that imposing particular rationality and structural restrictions may result in some reallocations of the resource set becoming unrealisable in this paper we address the issue of the number of restricted rational deals that may be required to implement a particular reallocation when it is possible to do so we construct examples showing that this number may be exponential in the number of resources m even when all of the agent utility functions are monotonic we further show that k agents may achieve in a single deal a reallocation requiring exponentially many rational deals if at most k1 agents can participate this same reallocation being unrealisable by any sequences of rational deals in which at most k2 agents are involved



j  m porta and  e  celaya 2005 reinforcement learning for agents with many sensors and actuators acting in categorizable environments volume 23 pages 79122

in this paper we confront the problem of applying reinforcement learning to agents that perceive the environment through many sensors and that can perform  parallel actions using many actuators as is the case in complex autonomous robots we argue that reinforcement learning can only be successfully applied to this case  if strong assumptions are made on the characteristics of the environment in which  the learning is performed so that the relevant sensor readings and motor commands can be  readily identified the introduction of such assumptions leads to stronglybiased  learning systems that can eventually lose the generality of traditional  reinforcementlearning algorithms  in this line we observe that in realistic situations the reward received by the robot  depends only on a reduced subset of all the executed actions and that only a reduced  subset of the sensor inputs possibly different in each situation and for each action  are relevant to predict the reward we formalize this property in the so called  categorizability assumption and we present an algorithm that takes advantage of  the categorizability of the environment allowing a decrease in the learning time with  respect to existing reinforcementlearning algorithms results of the application of the  algorithm to a couple of simulated realisticrobotic problems landmarkbased navigation  and the sixlegged robot gait generation are reported to validate our approach and to  compare it to existing flat and generalizationbased reinforcementlearning approaches



w  zhang and  n  l zhang 2005 restricted value iteration theory and algorithms volume 23 pages 123165

value iteration is a popular algorithm for finding near optimal policies for pomdps  it is inefficient due to the need to account for the entire belief space which necessitates the solution of large numbers of linear programs  in this paper we study value iteration restricted to belief subsets we show that together with properly chosen belief subsets restricted value iteration yields nearoptimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time we also apply restricted value iteration to two interesting classes of pomdps namely informative pomdps and neardiscernible pomdps



d  gabelaia  r  kontchakov  a  kurucz  f  wolter and  m  zakharyaschev 2005 combining spatial and temporal logics expressiveness vs complexity volume 23 pages 167243

in this paper we construct and investigate a hierarchy of spatiotemporal formalisms that result from various combinations of propositional spatial and temporal logics such as the propositional temporal logic ptl the spatial logics rcc8 brcc8 s4u and their fragments the obtained results give a clear picture of the tradeoff between expressiveness and computational realisability within the hierarchy we demonstrate how different combining principles as well as spatial and temporal primitives can produce np pspace expspace 2expspacecomplete and even undecidable spatiotemporal logics out of components that are at most np or pspacecomplete



c  cayrol and  m  c lagasquieschiex 2005 graduality in argumentation volume 23 pages 245297

argumentation is based on the exchange and valuation of interacting arguments followed by the selection of the most acceptable of them for example in order to take a decision to make a choice starting from the framework proposed by dung in 1995 our purpose is to introduce graduality in the selection of the best arguments ie to be able to partition the set of the arguments in more than the two usual subsets of selected and nonselected arguments in order to represent different levels of selection  our basic idea is that an argument is all the more acceptable if it can be preferred to its attackers  first we discuss general principles underlying a gradual valuation of arguments based on their interactions following these principles we define several valuation models for an abstract argumentation system  then we introduce graduality in the concept of acceptability of arguments we propose new acceptability classes and a refinement of existing classes taking advantage of an available gradual valuation



a  montoyo  a  suarez  g  rigau and  m  palomar 2005 combining knowledge and corpusbased wordsensedisambiguation methods volume 23 pages 299330

in this paper we concentrate on the resolution of the lexical ambiguity that arises when a given word has several different meanings this specific task is commonly referred to as word sense disambiguation wsd the task of wsd consists of assigning the correct sense to words using an electronic dictionary as the source of word definitions we present two wsd methods based on two main methodological approaches in this research area a knowledgebased method and a corpusbased method our hypothesis is that wordsense disambiguation requires several knowledge sources in order to solve the semantic ambiguity of the words these sources can be of different kinds for example syntagmatic paradigmatic or statistical information our approach combines various sources of knowledge through combinations of the two wsd methods mentioned above mainly the paper concentrates on how to combine these methods and sources of information in order to achieve good results in the disambiguation finally this paper presents a comprehensive study and experimental work on evaluation of the methods and their combinations



n  v chawla and   karakoulas 2005 learning from labeled and unlabeled data an empirical study across techniques and domains volume 23 pages 331366

there has been increased interest in devising learning techniques that combine unlabeled data with labeled data  ie semisupervised learning however to the best of our knowledge no study has been performed across various techniques and different types and amounts of labeled and unlabeled data moreover most of the published work on semisupervised learning techniques assumes that the labeled and unlabeled data come from the same distribution it is possible for the labeling process to be associated with a selection bias such that the distributions of data points in the labeled and unlabeled sets are different not correcting for such bias can result in biased function approximation with potentially poor performance in this paper we present an empirical study of various semisupervised learning techniques on a variety of datasets we attempt to answer various questions such as the effect of independence or relevance amongst features the effect of the size of the labeled and unlabeled sets and the effect of noise we also investigate the impact of sampleselection bias on the semisupervised learning techniques under study and implement a bivariate probit technique particularly designed to correct for such bias



r  nair and  m  tambe 2005 hybrid bdipomdp framework for multiagent teaming volume 23 pages 367420

many current largescale multiagent team implementations can be characterized as following the beliefdesireintention bdi paradigm with explicit representation of team plans  despite their promise current bdi team approaches lack tools for quantitative performance analysis under uncertainty distributed partially observable markov decision problems pomdps are well suited for such analysis but the complexity of finding optimal policies in such models is highly intractable the key contribution of this article is a hybrid bdipomdp approach where bdi team plans are exploited to improve pomdp tractability and pomdp analysis improves bdi team plan performance  concretely we focus on role allocation a fundamental problem in bdi  teams which agents to allocate to the different roles in the team the  article provides three key contributions first we describe a role  allocation technique that takes into account future uncertainties in the domain prior work in multiagent role allocation has failed to address such uncertainties to that end we introduce rmtdp rolebased markov team decision problem a new distributed pomdp model for analysis of role allocations our technique gains in tractability by significantly curtailing rmtdp policy search in particular bdi team plans provide incomplete rmtdp policies and the rmtdp policy search fills the gaps in such incomplete policies by searching for the best role allocation our second key contribution is a novel decomposition technique to  further improve rmtdp policy search efficiency even though limited  to searching role allocations there are still combinatorially many  role allocations and evaluating each in rmtdp to identify the best  is extremely difficult our decomposition technique exploits the  structure in the bdi team plans to significantly prune the search space of role allocations our third key contribution is a  significantly faster policy evaluation algorithm suited for  our bdipomdp hybrid approach finally we also present experimental  results from two domains mission rehearsal simulation and  robocuprescue disaster rescue simulation



j  larrosa  e  morancho and  d  niso 2005 on the practical use of variable elimination in constraint optimization problems stilllife as a case study volume 23 pages 421440

variable elimination is a general technique for constraint processing it is often discarded because of its high space complexity however it can be extremely useful when combined with other techniques in this paper we study the applicability of variable elimination to the challenging problem of finding stilllifes  we illustrate several alternatives variable elimination as a standalone algorithm interleaved with search and as a source of good quality lower bounds  we show that these techniques are the best known option both theoretically and empirically in our experiments we have been able to solve the n20 instance which is far beyond reach with alternative approaches



h  e dixon  m  l ginsberg  d  hofer  e  m luks and  a  j parkes 2005 generalizing boolean satisfiability iii implementation volume 23 pages 441531

this is the third of three papers describing zap a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern highperformance solvers the fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the boolean representation used our goal has been to define a representation in which this structure is apparent and can be exploited to improve computational performance  the first paper surveyed existing work that knowingly or not exploited problem structure to improve the performance of satisfiability engines and the second paper showed that this structure could be understood in terms of groups of permutations acting on individual clauses in any particular boolean theory  we conclude the series by discussing the techniques needed to implement our ideas and by reporting on their performance on a variety of problem instances



t  zimmerman and  s  kambhampati 2005 using memory to transform search on the planning graph volume 23 pages 533585

the graphplan algorithm for generating optimal makespan plans containing parallel sets of actions remains one of the most effective ways to generate such plans  however despite enhancements on a range of fronts the approach is currently dominated in terms of speed by state space planners that employ distancebased heuristics to quickly generate serial plans  we report on a family of strategies that employ available memory to construct a search trace so as to learn from various aspects of graphplans iterative search episodes in order to expedite search in subsequent episodes  the planning approaches can be partitioned into two classes according to the type and extent of search experience captured in the trace  the planners using the more aggressive tracing method are able to avoid much of graphplans redundant search effort while planners in the second class trade off this aspect in favor of a much higher degree of freedom than graphplan in traversing the space of states generated during regression search on the planning graph  the tactic favored by the second approach exploiting the search trace to transform the depthfirst ida nature of graphplans search into an iterative state space view is shown to be the more powerful  we demonstrate that distancebased state space heuristics can be adapted to informed traversal of the search trace used by the second class of planners and develop an augmentation targeted specifically at planning graph search  guided by such a heuristic the stepoptimal version of the planner in this class clearly dominates even a highly enhanced version of graphplan  by adopting beam search on the search trace we then show that virtually optimal parallel plans can be generated at speeds quite competitive with a modern heuristic state space planner



s  schroedl 2005 an improved search algorithm for optimal multiplesequence alignment volume 23 pages 587623

multiple sequence alignment msa is a ubiquitous problem in computational biology although it is nphard to find an optimal solution for an arbitrary number of sequences due to the importance of this problem researchers are trying to push the limits of exact algorithms further since msa can be cast as a classical path finding problem it is attracting a growing number of ai researchers interested in heuristic search algorithms as a challenge with actual practical relevance  in this paper we first review two previous complementary lines of research based on hirschbergs algorithm dynamic programming needs oknk1 space to store both the search frontier and the nodes needed to reconstruct the solution path for k sequences of length n best first search on the other hand has the advantage of bounding the search space that has to be explored using a heuristic  however it is necessary to maintain all explored nodes up to the final solution in order to prevent the search from reexpanding them at higher cost  earlier approaches to reduce the closed list are either incompatible with pruning methods for the open list or must retain at least the boundary of the closed list  in this article we present an algorithm that attempts at combining the respective advantages like a it uses a heuristic for pruning the search space but reduces both the maximum open and closed size to oknk1 as in dynamic programming  the underlying idea is to conduct a series of searches with successively increasing upper bounds but using the dp ordering as the key for the open priority queue  with a suitable choice of thresholds in practice a running time below four times that of a can be expected  in our experiments we show that our algorithm outperforms one of the currently most successful algorithms for optimal multiple sequence alignments partial expansion a both in time and memory moreover we apply a refined heuristic based on optimal alignments not only of pairs of sequences but of larger subsets  this idea is not new however to make it practically relevant we show that it is equally important to bound the heuristic computation appropriately or the overhead can obliterate any possible gain  furthermore we discuss a number of improvements in time and space efficiency with regard to practical implementations  our algorithm used in conjunction with higherdimensional heuristics is able to calculate for the first time the optimal alignment for almost all of the problems in reference 1 of the benchmark database balibase



g  barish and  c  a knoblock 2005 an expressive language and efficient execution system for software agents volume 23 pages 625666

software agents can be used to automate many of the tedious timeconsuming information processing tasks that humans currently have to complete manually  however to do so agent plans must be capable of representing the myriad of actions and control flows required to perform those tasks  in addition since these tasks can require integrating multiple sources of remote information  typically a slow iobound process  it is desirable to make execution as efficient as possible  to address both of these needs we present a flexible software agent plan language and a highly parallel execution system that enable the efficient execution of expressive agent plans the plan language allows complex tasks to be more easily expressed by providing a variety of operators for flexibly processing the data as well as supporting subplans for modularity and recursion for indeterminate looping  the executor is based on a streaming dataflow model of execution to maximize the amount of operator and data parallelism possible at runtime  we have implemented both the language and executor in a system called theseus  our results from testing theseus show that streaming dataflow execution can yield significant speedups over both traditional serial von neumann as well as nonstreaming dataflowstyle execution that existing software and robot agent execution systems currently support  in addition we show how plans written in the language we present can represent certain types of subtasks that cannot be accomplished using the languages supported by network query engines  finally we demonstrate that the increased expressivity of our plan language does not hamper performance specifically we show how data can be integrated from multiple remote sources just as efficiently using our architecture as is possible with a stateoftheart streamingdataflow network query engine



l  carsten  c  areces  i  horrocks and  u  sattler 2005 keys nominals and concrete domains volume 23 pages 667726

many description logics dls combine knowledge representation on an abstract logical level with an interface to concrete domains like numbers and strings with builtin predicates such as   and prefixof these hybrid dls have turned out to be useful in several application areas such as reasoning about conceptual database models  we propose to further extend such dls with key constraints that allow the expression of statements like us citizens are uniquely identified by their social security number based on this idea we introduce a number of natural description logics and perform a detailed analysis of their decidability and computational complexity  it turns out that naive extensions with key constraints easily lead to undecidability whereas more careful extensions yield nexptimecomplete dls for a variety of useful concrete domains
