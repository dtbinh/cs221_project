t  walsh 2011 where are the hard manipulation problems volume 42 pages 129

voting is a simple mechanism to combine together the preferences of multiple agents unfortunately agents may try to manipulate the result by misreporting their preferences one barrier that might exist to such manipulation is computational complexity in particular it has been shown that it is nphard to compute how to manipulate a number of different voting rules how ever nphardness only bounds the worstcase complexity recent theoretical results suggest that manipulation may often be easy in practice in this paper we show that empirical studies are useful in improving our understanding of this issue we consider two settings which represent the two types of complexity results that have been identified in this area manipulation with unweighted votes by a single agent and manipulation with weighted votes by a coalition of agents in the first case we consider single transferable voting stv and in the second case we consider veto voting stv is one of the few voting rules used in practice where it is nphard to compute how a single agent can manipulate the result when votes are unweighted it also appears one of the harder voting rules to manipulate since it involves multiple rounds on the other hand veto voting is one of the simplest representatives of voting rules where it is nphard to compute how a coalition of weighted agents can manipulate the result in our experiments we sample a number of distributions of votes including uniform correlated and real world elections in many of the elections in our experiments it was easy to compute how to manipulate the result or to prove that manipulation was impossible even when we were able to identify a situation in which manipulation was hard to compute eg when votes are highly correlated and the election is hung we found that the computational difficulty of computing manipulations was somewhat precarious eg with such hung elections even a single uncorrelated voter was enough to make manipulation easy to compute



r  booth t  meyer i  varzinczak and r  wassermann 2011 on the link between partial meet kernel and infra contraction and its application to horn logic volume 42 pages 3153

standard belief change assumes an underlying logic containing full classical propositional logic however there are good reasons for considering belief change in less expressive logics as well in this paper we build on recent investigations by delgrande on contraction for horn logic we show that the standard basic form of contraction partial meet is too strong in the horn case this result stands in contrast to delgrandes conjecture that orderly maxichoice is the appropriate form of contraction for horn logic we then define a more appropriate notion of basic contraction for the horn case influenced by the convexity property holding for full propositional logic and which we refer to as infra contraction the main contribution of this work is a result which shows that the construction method for horn contraction for belief sets based on our infra remainder sets corresponds exactly to hanssons classical kernel contraction for belief sets when restricted to horn logic this result is obtained via a detour through contraction for belief bases we prove that kernel contraction for belief bases produces precisely the same results as the belief base version of infra contraction the use of belief bases to obtain this result provides evidence for the conjecture that horn belief change is best viewed as a hybrid version of belief set change and belief base change one of the consequences of the link with base contraction is the provision of a representation result for horn contraction for belief sets in which a version of the coreretainment postulate features



k  c wang and a  botea 2011 mapp a scalable multiagent path planning algorithm with tractability and completeness guarantees volume 42 pages 5590

multiagent path planning is a challenging problem with numerous reallife applications  running a centralized search such as a in the combined state space of all units is complete and costoptimal but scales poorly as the state space size is exponential in the number of mobile units  traditional decentralized approaches such as far and  whca are faster and more scalable being based on problem decomposition  however such methods are incomplete and provide no guarantees with respect to the running time or the solution quality  they are not necessarily able to tell in a reasonable time whether they would succeed in finding a solution to a given instance 
experiments were run on realistic game grid maps  mapp solved 9986 of all mobile units which is 1822 better than the percentage of far and whca  mapp marked 9882 of all units as provably solvable during the first stage of plan computation  parts of mapps computation can be reused across instances on the same map  speedwise mapp is competitive or significantly faster than whca depending on whether mapp performs all computations from scratch  when data that mapp can reuse are preprocessed offline and readily available mapp is slower than the very fast far algorithm by a factor of 218 on average  mapps solutions are on average 20 longer than fars solutions and 731 longer than whcas solutions 



r  hoshino and k  kawarabayashi 2011 scheduling bipartite tournaments to minimize total travel distance volume 42 pages 91124

in many professional sports leagues teams from opposing leaguesconferences compete against one another playing interleague games  this is an example of a bipartite tournament  in this paper we consider the problem of reducing the total travel distance of bipartite tournaments by analyzing interleague scheduling from the perspective of discrete optimization  this research has natural applications to sports scheduling especially for leagues such as the national basketball association nba where teams must travel long distances across north america to play all their games thus consuming much time money and greenhouse gas emissions
we introduce the bipartite traveling tournament problem bttp the interleague variant of the wellstudied traveling tournament problem we prove that the 2nteam bttp is npcomplete but for small values of n a distanceoptimal interleague schedule can be generated from an algorithm based on minimumweight 4cyclecovers  we apply our theoretical results to the 12team nippon professional baseball npb league in japan producing a provablyoptimal schedule requiring 42950 kilometres of total team travel a 16 reduction compared to the actual distance traveled by these teams during the 2010 npb season  we also develop a nearlyoptimal interleague tournament for the 30team nba league just 38 higher than the trivial theoretical lower bound



j  lee and y  meng 2011 firstorder stable model semantics and firstorder loop formulas volume 42 pages 125180

lin and zhaos theorem on loop formulas states that in the propositional case the stable model semantics of a logic program can be completely characterized by propositional loop formulas but this result does not fully carry over to the firstorder case we investigate the precise relationship between the firstorder stable model semantics and firstorder loop formulas and study conditions under which the former can be represented by the latter in order to facilitate the comparison we extend the definition of a firstorder loop formula which was limited to a nondisjunctive program to a disjunctive program and to an arbitrary firstorder theory based on the studied relationship we extend the syntax of a logic program with explicit quantifiers which allows us to do reasoning involving nonherbrand stable models using firstorder reasoners such programs can be viewed as a special class of firstorder theories under the stable model semantics which yields more succinct loop formulas than the general language due to their restricted syntax



p  dai mausam   d  s weld and j  goldsmith 2011 topological value iteration algorithms volume 42 pages 181209

value iteration is a powerful yet inefficient algorithm for markov decision processes mdps because it puts the majority of its effort into backing up the entire state space which turns out to be unnecessary in many cases in order to overcome this problem many approaches have been proposed among them ilao and variants of rtdp are stateoftheart ones these methods use reachability analysis and heuristic search to avoid some unnecessary backups however none of these approaches build the graphical structure of the state transitions in a preprocessing step or use the structural information to systematically decompose a problem whereby generating an intelligent backup sequence of the state space in this paper we present two optimal mdp algorithms the first algorithm  topological value iteration tvi detects the structure of mdps and backs up states based on topological sequences it 1 divides an mdp into stronglyconnected components sccs and 2 solves these components sequentially tvi outperforms vi and other stateoftheart algorithms vastly when an mdp has multiple closetoequalsized sccs the second algorithm  focused  topological value iteration ftvi is an extension of tvi ftvi restricts its attention to connected components that are relevant for solving the mdp specifically it uses a small amount of heuristic search to eliminate provably suboptimal actions this pruning allows ftvi to find smaller connected components thus running faster  we demonstrate that ftvi outperforms tvi by an order of magnitude averaged across several domains surprisingly ftvi also significantly outperforms popular heuristicallyinformed mdp algorithms such as ilao lrtdp brtdp and bayesianrtdp in many domains sometimes by as much as two orders of magnitude finally we characterize the type of domains where ftvi excels  suggesting a way to an informed choice of solver



g r  santhanam s  basu and v  honavar 2011 representing and reasoning with qualitative preferences for compositional systems volume 42 pages 211274

many applications eg web service composition complex system design team formation etc rely on methods for identifying collections of objects or entities satisfying some functional requirement among the collections that satisfy the functional requirement it is often necessary to identify one or more collections that are optimal with respect to user preferences over a set of attributes that describe the nonfunctional properties of the collection
we provide algorithms that use this dominance relation to identify the set of most preferred collections we show that under certain conditions the algorithms are guaranteed to return only sound all complete or at least one weakly complete of the most preferred collections we present results of simulation experiments comparing the proposed algorithms with respect to a the quality of solutions number of most preferred solutions produced by the algorithms and b their performance and efficiency we also explore some interesting conjectures suggested by the results of our experiments that relate the properties of the user preferences the dominance relation and the algorithms



r  ribeiro and d  martins de matos 2011 centralityasrelevance support sets and similarity as geometric proximity volume 42 pages 275308

in automatic summarization centralityasrelevance means that the most important content of an information source or a collection of information sources corresponds to the most central passages considering a representation where such notion makes sense graph spatial etc we assess the main paradigms and introduce a new centralitybased relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content geometric proximity is used to compute semantic relatedness centrality relevance is determined by considering the whole input source and not only local information and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized the method consists in creating for each passage of the input source a support set consisting only of the most semantically related passages then the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets this model produces extractive summaries that are generic and language and domainindependent thorough automatic evaluation shows that the method achieves stateoftheart performance both in written text and automatically transcribed speech summarization including when compared to considerably more complex approaches




c  yuan h  lim and t  lu 2011 most relevant explanation in bayesian networks volume 42 pages 309352

a major inference task in bayesian networks is explaining why some variables are observed in their particular states using a set of target variables existing methods for solving this problem often generate explanations that are either too simple underspecified or too complex overspecified in this paper we introduce a method called most relevant explanation mre which finds a partial instantiation of the target variables that maximizes the generalized bayes factor gbf as the best explanation for the given evidence our study shows that gbf has several theoretical properties that enable mre to automatically identify the most relevant target variables in forming its explanation in particular conditional bayes factor cbf defined as the gbf of a new explanation conditioned on an existing explanation provides a soft measure on the degree of relevance of the variables in the new explanation in explaining the evidence given the existing explanation as a result mre is able to automatically prune less relevant variables from its explanation we also show that cbf is able to capture well the explainingaway phenomenon that is often represented in bayesian networks moreover we define two dominance relations between the candidate solutions and use the relations to generalize mre to find a set of top explanations that is both diverse and representative case studies on several benchmark diagnostic bayesian networks show that mre is often able to find explanatory hypotheses that are not only precise but also concise



e  talvitie and s  singh 2011 learning to make predictions in partially observable environments without a generative model volume 42 pages 353392

when faced with the problem of learning a model of a highdimensional environment a common approach is to limit the model to make only a restricted set of predictions thereby simplifying the learning problem these partial models may be directly useful for making decisions or may be combined together to form a more complete structured model however in partially observable nonmarkov environments standard modellearning methods learn generative models ie models that provide a probability distribution over all possible futures such as pomdps it is not straightforward to restrict such models to make only certain predictions and doing so does not always simplify the learning problem in this paper we present prediction profile models nongenerative partial models for partially observable systems that make only a given set of predictions and are therefore far simpler than generative models in some cases we formalize the problem of learning a prediction profile model as a transformation of the original modellearning problem and show empirically that one can learn prediction profile models that make a small set of important predictions even in systems that are too complex for standard generative models



p  d grunwald and j  y halpern 2011 making decisions using sets of probabilities updating time consistency and calibration volume 42 pages 393426

we consider how an agent should update her beliefs when her beliefs are represented by a set p of probability distributions given that the agent makes decisions using the minimax criterion perhaps the beststudied and most commonlyused criterion in the literature we adopt a gametheoretic framework where the agent plays against a bookie who chooses some distribution from p we consider two reasonable games that differ in what the bookie knows when he makes his choice anomalies that have been observed before like time inconsistency can be understood as arising because different games are being played against bookies with different information we characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information finally we consider the relationship between updating and calibration when uncertainty is described by sets of probabilities our results emphasize the key role of the rectangularity condition of epstein and schneider



d  golovin and a  krause 2011 adaptive submodularity theory and applications in active learning and stochastic optimization volume 42 pages 427486




2013 ijcaijair best paper

many problems in artificial intelligence require adaptively making a sequence of decisions with uncertain outcomes under partial observability solving such stochastic optimization problems is a fundamental but notoriously difficult challenge  in this paper we introduce the concept of adaptive submodularity generalizing submodular set functions to adaptive policies  we prove that if a problem satisfies this property a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy in addition to providing  performance guarantees for both stochastic maximization and coverage adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations we illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse ai applications including management of sensing resources viral marketing and active learning proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases improve approximation guarantees and handle natural generalizations 



m  alviano f  calimeri w  faber n  leone and s  perri 2011 unfounded sets and wellfounded semantics of answer set programs with aggregates  volume 42 pages 487527

logic programs with aggregates lpa are one of the major linguistic extensions to logic programming lp in this work we propose a generalization of the notions of unfounded set and wellfounded semantics for programs with monotone and antimonotone aggregates lpama programs in particular we present a new notion of unfounded set for lpama programs which is a sound generalization of the original definition for standard aggregatefree lp on this basis we define a wellfounded operator for lpama programs the fixpoint of which is called wellfounded model or wellfounded semantics for lpama programs the most important properties of unfounded sets and the wellfounded semantics for standard lp are retained by this generalization notably existence and uniqueness of the wellfounded model together with a strong relationship to the answer set semantics for lpama programs we show that one of the dwellfounded semantics defined by pelov denecker and bruynooghe for a broader class of aggregates using approximating operators coincides with the wellfounded model as defined in this work on lpama programs we also discuss some complexity issues most importantly we give a formal proof of tractable computation of the wellfounded model for lpa programs moreover we prove that for general lpa programs which may contain aggregates that are neither monotone nor antimonotone deciding satisfaction of aggregate expressions with respect to partial interpretations is conpcomplete as a consequence a wellfounded semantics for general lpa programs that allows for tractable computation is unlikely to exist which justifies the restriction on lpama programs finally we present a prototype system extending dlv which supports the wellfounded semantics for lpama programs at the time of writing the only implemented system that does so experiments with this prototype show significant computational advantages of aggregate constructs over equivalent aggregatefree encodings



e  elkind p  faliszewski and a  slinko 2011 cloning in elections finding the possible winners volume 42 pages 529573

we consider the problem of manipulating elections by cloning candidates in our model a manipulator can replace each candidate c by several clones ie new candidates that are so similar to c that each voter simply replaces c in his vote with a block of these new candidates ranked consecutively the outcome of the resulting election may then depend on the number of clones as well as on how each voter orders the clones within the block we formalize what it means for a cloning manipulation to be successful which turns out to be a surprisingly delicate issue and for a number of common voting rules characterize the preference profiles for which a successful cloning manipulation exists we also consider the model where there is a cost associated with producing each clone and study the complexity of finding a minimumcost cloning manipulation finally we compare cloning with two related problems the problem of control by adding candidates and the problem of possible cowinners when new alternatives can join



m  ponsen s  de jong and m  lanctot 2011 computing approximate nash equilibria and robust bestresponses using sampling volume 42 pages 575605

this article discusses two contributions to decisionmaking in complex partially observable stochastic games first we apply two stateoftheart search techniques that use montecarlo sampling to the task of approximating a nashequilibrium ne in such games namely montecarlo tree search mcts and montecarlo counterfactual regret minimization mccfr mcts has been proven to approximate a ne in perfectinformation games we show that the algorithm quickly finds a reasonably strong strategy but not a ne in a complex imperfect information game ie poker mccfr on the other hand has theoretical ne convergence guarantees in such a game we apply mccfr for the first time in poker based on our experiments we may conclude that mcts is a valid approach if one wants to learn reasonably strong strategies fast whereas mccfr is the better choice if the quality of the strategy is most important 




p  r conrad and b  c williams 2011 drake an efficient executive for temporal plans with choice volume 42 pages 607659

this work presents drake a dynamic executive for temporal plans with choice dynamic plan execution strategies allow an autonomous agent to react quickly to unfolding events improving the robustness of the agent prior work developed methods for dynamically dispatching simple temporal networks and further research enriched the expressiveness of the plans executives could handle including discrete choices which are the focus of this work however in some approaches to date these additional choices induce significant storage or latency requirements to make flexible execution possible 
drake is designed to leverage the low latency made possible by a preprocessing step called compilation while avoiding high memory costs through a compact representation we leverage the concepts of labels and environments taken from prior work in assumptionbased truth maintenance systems atms to concisely record the implications of the discrete choices exploiting the structure of the plan to avoid redundant reasoning or storage our labeling and maintenance scheme called the labeled value set maintenance system is distinguished by its focus on properties fundamental to temporal problems and more generally weighted graph algorithms in particular the maintenance system focuses on maintaining a minimal representation of nondominated constraints we benchmark drakes performance on random structured problems and find that drake reduces the size of the compiled representation by a factor of over 500 for large problems while incurring only a modest increase in runtime latency compared to prior work in compiled executives for temporal plans with discrete choices



j  m pe241a 2011 finding consensus bayesian network structures volume 42 pages 661687

suppose that multiple experts or learning algorithms provide us with alternative bayesian network bn structures over a domain and that we are interested in combining them into a single consensus bn structure specifically we are interested in that the consensus bn structure only represents independences all the given bn structures agree upon and that it has as few parameters associated as possible in this paper we prove that there may exist several nonequivalent consensus bn structures and that finding one of them is nphard thus we decide to resort to heuristics to find an approximated consensus bn structure in this paper we consider the heuristic proposed by matzkevich and abramson which builds upon two algorithms called methods a and b for efficiently deriving the minimal directed independence map of a bn structure relative to a given node ordering methods a and b are claimed to be correct although no proof is provided a proof is just sketched in this paper we show that methods a and b are not correct and propose a correction of them



e  amig243 j  gonzalo j  artiles and f  verdejo 2011 combining evaluation metrics via the unanimous improvement ratio and its application to clustering tasks volume 42 pages 689718

many artificial intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings a problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings this paper introduces the unanimous improvement ratio uir a measure that complements standard metric combination criteria such as van rijsbergens fmeasure and indicates how robust the measured differences are to changes in the relative weights of the individual metrics uir is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted
besides discussing the theoretical foundations of uir this paper presents empirical results that confirm the validity and usefulness of the metric for the text clustering problem where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them remarkably our experiments show that uir can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed



p  a bonatti m  faella and l  sauro 2011 defeasible inclusions in lowcomplexity dls volume 42 pages 719764

some of the applications of owl and rdf eg biomedical knowledge representation and semantic policy formulation call for extensions of these languages with nonmonotonic constructs such as inheritance with overriding  nonmonotonic description logics have been studied for many years however no practical such knowledge representation languages exist due to a combination of semantic difficulties and high computational complexity independently lowcomplexity description logics such as dllite and el have been introduced and incorporated in the owl standard  therefore it is interesting to see whether the syntactic restrictions characterizing dllite and el bring computational benefits to their nonmonotonic versions too in this paper we extensively investigate the computational complexity of circumscription when knowledge bases are formulated in dlliter el and fragments thereof  we identify fragments whose complexity ranges from p to the second level of the polynomial hierarchy as well as fragments whose complexity raises to pspace and beyond



p  vytelingum t  d voice s  d ramchurn a  rogers and n  r jennings 2011 theoretical and practical foundations of largescale agentbased microstorage in the smart grid volume 42 pages 765813




aamas 2010 irobot best paper award

in this paper we present a novel decentralised management technique that allows electricity microstorage devices deployed within individual homes as part of a smart electricity grid to converge to profitable and efficient behaviours specifically we propose the use of software agents residing on the users smart meters to automate and optimise the charging cycle of microstorage devices in the home to minimise its costs and we present a study of both the theoretical underpinnings and the implications of a practical solution of using software agents for such microstorage management first by formalising the strategic choice each agent makes in deciding when to charge its battery we develop a gametheoretic framework within which we can analyse the competitive equilibria of an electricity grid populated by such agents and hence predict the best consumption profile for that population given their battery properties and individual load profiles our framework also allows us to compute theoretical bounds on the amount of storage that will be adopted by the population second to analyse the practical implications of microstorage deployments in the grid we present a novel algorithm that each agent can use to optimise its battery storage profile in order to minimise its owners costs this algorithm uses a learning strategy that allows it to adapt as the price of electricity changes in realtime and we show that the adoption of these strategies results in the system converging to the theoretical equilibria finally we empirically evaluate the adoption of our microstorage management technique within a complex setting based on the uk electricity market where agents may have widely varying load profiles battery types and learning rates in this case our approach yields savings of up to 14 in energy cost for an average consumer using a storage device with a capacity of less than 45 kwh and up to a 7 reduction in carbon emissions resulting from electricity generation with only domestic consumers adopting microstorage and commercial and industrial consumers not changing their demand moreover corroborating our theoretical bound an equilibrium is shown to exist where no more than 48 of households would wish to own storage devices and where social welfare would also be improved yielding overall annual savings of nearly 16315b



j  wu r  kalyanam and r  givan 2011 stochastic enforced hillclimbing volume 42 pages 815850


enforced hillclimbing is an effective deterministic hillclimbing technique that deals with local optima using breadthfirst search a process called basin flooding  we propose and evaluate a stochastic generalization




m  l ginsberg 2011 drfill crosswords and an implemented solver for singly weighted csps volume 42 pages 851886

we describe drfill a program that solves americanstyle crossword puzzles  from a technical perspective drfill works by converting crosswords to weighted csps and then using a variety of novel techniques to find a solution  these techniques include generally applicable heuristics for variable and value selection a variant of limited discrepancy search and postprocessing and partitioning ideas  branch and bound is not used as it was incompatible with postprocessing and was determined experimentally to be of little practical value  drfillls performance on crosswords from the american crossword puzzle tournament suggests that it ranks among the top fifty or so crossword solvers in the world



n  agmon g  a kaminka and s  kraus 2011 multirobot adversarial patrolling facing a fullknowledge opponent volume 42 pages 887916

the problem of adversarial multirobot patrol has gained interest in recent years mainly due to its immediate relevance to various security applications in this problem robots are required to repeatedly visit a target area in a way that maximizes their chances of detecting an adversary trying to penetrate through the patrol path when facing a strong adversary that knows the patrol strategy of the robots if the robots use a deterministic patrol algorithm then in many cases it is easy for the adversary to penetrate undetected in fact in some of those cases the adversary can guarantee penetration therefore this paper presents a nondeterministic patrol framework for the robots assuming that the strong adversary will take advantage of its knowledge and try to penetrate through the patrols weakest spot hence an optimal algorithm is one that maximizes the chances of detection in that point we therefore present a polynomialtime algorithm for determining an optimal patrol under the markovian strategy assumption for the robots such that the probability of detecting the adversary in the patrols weakest spot is maximized we build upon this framework and describe an optimal patrol strategy for several robotic models based on their movement abilities directed or undirected and sensing abilities perfect or imperfect and in different environment models  either patrol around a perimeter closed polygon or an open fence open polyline




d  gabbay d  pearce and a  valverde 2011 interpolable formulas in equilibrium logic and answer set programming volume 42 pages 917943

interpolation is an important property of classical and many nonclassical logics that has been shown to have interesting applications in computer science and ai here we study the interpolation property for the the nonmonotonic system of equilibrium logic establishing weaker or stronger forms of interpolation depending on the precise interpretation of the inference relation these results also yield a form of interpolation for ground logic programs under the answer sets semantics for disjunctive logic programs we also study the property of uniform interpolation that is closely related to the concept of variable forgetting the firstorder version of equilibrium logic has analogous interpolation properties whenever the collection of equilibrium models is firstorder definable since this is the case for socalled safe programs and theories it applies to the usual situations that arise in practical answer set programming
