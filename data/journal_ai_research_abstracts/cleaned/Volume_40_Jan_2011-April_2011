m  milani fard and j  pineau 2011 nondeterministic policies in markovian decision processes volume 40 pages 124

markovian processes have long been used to model stochastic environments reinforcement learning has emerged as a framework to solve sequential planning and decisionmaking problems in such environments in recent years attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in markovian environments although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decisionmaking they cannot be applied in their current form to decision support systems such as those in medical domains as they suggest policies that are often highly prescriptive and leave little room for the users input without the ability to provide flexible guidelines it is unlikely that these methods can gain ground with users of such systems
this paper introduces the new concept of nondeterministic policies to allow more flexibility in the users decisionmaking process while constraining decisions to remain near optimal solutions we provide two algorithms to compute nondeterministic policies in discrete domains we study the output and running time of these method on a set of synthetic and realworld problems in an experiment with human subjects we show that humans assisted by hints based on nondeterministic policies outperform both humanonly and computeronly agents in a web navigation task



y  zhou and y  zhang 2011 a logical study of partial entailment volume 40 pages 2556

we introduce a novel logical notionpartial entailmentto propositional logic in contrast with classical entailment that a formula p partially entails another formula q with respect to a background formula set gamma intuitively means that under the circumstance of gamma if p is true then some part of q will also be true we distinguish three different kinds of partial entailments and formalize them by using an extended notion of prime implicant we study their semantic properties which show that surprisingly partial entailments fail for many simple inference rules then we study the related computational properties which indicate that partial entailments are relatively difficult to be computed finally we consider a potential application of partial entailments in reasoning about rational agents



h  aziz y  bachrach e  elkind and m  paterson 2011 falsename manipulations in weighted voting games volume 40 pages 5793

weighted voting is a classic model of cooperation among agents in decisionmaking domains in such games each player has a weight and a coalition of players wins the game if its total weight meets or exceeds a given quota a players power in such games is usually not directly proportional to his weight and is measured by a power index the most prominent among which are the shapleyshubik index and the banzhaf indexin this paper we investigate by how much a player can change his power as measured by the shapleyshubik index or the banzhaf index by means of a falsename manipulation ie splitting his weight among two or more identities for both indices we provide upper and lower bounds on the effect of weightsplitting we then show that checking whether a beneficial split exists is nphard and discuss efficient algorithms for restricted cases of this problem as well as randomized algorithms for the general case we also provide an experimental evaluation of these algorithms finally we examine related forms of manipulative behavior such as annexation where a player subsumes other players or merging where several players unite into one we characterize the computational complexity of such manipulations and provide limits on their effects for the banzhaf index we describe a new paradox which we term the annexation nonmonotonicity paradox



j  veness ks  ng m  hutter w  uther and d  silver 2011 a montecarlo aixi approximation volume 40 pages 95142




honorable mention for the 2014 ijcaijair best paper prize

this paper introduces a principled approach for the design of a scalable general reinforcement learning agent our approach is based on a direct approximation of aixi a bayesian optimality notion for general reinforcement learning agents previously it has been unclear whether the theory of aixi could motivate the design of practical algorithms we answer this hitherto open question in the affirmative by providing the first computationally feasible approximation to the aixi agent to develop our approximation we introduce a new montecarlo tree search algorithm along with an agentspecific extension to the context tree weighting algorithm empirically we present a set of encouraging results on a variety of stochastic and partially observable domains we conclude by proposing a number of directions for future research



c  geist and u  endriss 2011 automated search for impossibility theorems in social choice theory ranking sets of objects volume 40 pages 143174

we present a method for using standard techniques from satisfiability checking to automatically verify and discover theorems in an area of economic theory known as ranking sets of objects the key question in this area which has important applications in social choice theory and decision making under uncertainty is how to extend an agents preferences over a number of objects to a preference relation over nonempty sets of such objects certain combinations of seemingly natural principles for this kind of preference extension can result in logical inconsistencies which has led to a number of important impossibility theorems we first prove a general result that shows that for a wide range of such principles characterised by their syntactic form when expressed in a manysorted firstorder logic any impossibility exhibited at a fixed small domain size will necessarily extend to the general case we then show how to formulate candidates for impossibility theorems at a fixed domain size in propositional logic which in turn enables us to automatically search for general impossibility theorems using a sat solver when applied to a space of 20 principles for preference extension familiar from the literature this method yields a total of 84 impossibility theorems including both known and nontrivial new results



c  lecoutre s  cardon and j  vion 2011 secondorder consistencies volume 40 pages 175219

in this paper we propose a comprehensive study of secondorder consistencies ie consistencies identifying inconsistent pairs of values for constraint satisfaction we build a full picture of the relationships existing between four basic secondorder consistencies namely path consistency pc 3consistency 3c dual consistency dc and 2singleton arc consistency 2sac as well as their conservative and strong variants interestingly dual consistency is an original property that can be established by using the outcome of the enforcement of generalized arc consistency gac which makes it rather easy to obtain since constraint solvers typically maintain gac during search  on binary constraint  networks dc is equivalent to pc but its restriction to existing constraints called conservative dual consistency cdc is strictly stronger than  traditional conservative consistencies derived from path consistency namely partial path consistency ppc and conservative path consistency cpc  after introducing a general algorithm to enforce strong cdc we present the results of an experimentation over a wide range of benchmarks that demonstrate the interest of conservative dual consistency  in particular we show that enforcing cdc before search clearly improves the performance of mac the algorithm that maintains gac during search on several binary and nonbinary structured problems



y  wang c  hang and m  p singh 2011 a probabilistic approach for maintaining trust based on evidence volume 40 pages 221267

leading agentbased trust models address two important needs  first they show how an agent may estimate the trustworthiness of another agent based on prior interactions  second they show how agents may share their knowledge in order to cooperatively assess the trustworthiness of others  however in reallife settings information relevant to trust is usually obtained piecemeal not all at once  unfortunately the problem of maintaining trust has drawn little attention  existing approaches handle trust updates in a heuristic not a principled manner
this paper builds on a formal model that considers probability and certainty as two dimensions of trust  it proposes a mechanism using which an agent can update the amount of trust it places in other agents on an ongoing basis  this paper shows via simulation that the proposed approach a provides accurate estimates of the trustworthiness of agents that change behavior frequently and b captures the dynamic behavior of the agents  this paper includes an evaluation based on a real dataset drawn from amazon marketplace a leading ecommerce site



a  hunter and j  p delgrande 2011 iterated belief change due to actions and observations volume 40 pages 269304

in action domains where agents may have erroneous beliefs reasoning about the effects of actions involves reasoning about belief change  in this paper we use a transition system approach to reason about the evolution of an agents beliefs as actions are executed  some  actions cause an agent to perform belief revision while others cause an agent to perform belief update but the interaction between revision and update can be nonelementary  we present a set of rationality properties describing the interaction between revision and update and we introduce a new class of belief change operators for reasoning about alternating sequences of revisions and updates  our belief change operators can be characterized in terms of a natural shifting operation on total preorderings over interpretations  we compare our approach with related work on iterated belief change due to action and we conclude with some directions for future research



p  faliszewski e  hemaspaandra and l  a hemaspaandra 2011 multimode control attacks on elections volume 40 pages 305351

in 1992 bartholdi tovey and trick opened the study of control attacks on electionsattempts to improve the election outcome by such actions as addingdeleting candidates or voters  that work has led to many results on how algorithms can be used to find attacks on elections and how complexitytheoretic hardness results can be used as shields against attacks however all the work in this line has assumed that the attacker employs just a single type of attack  in this paper we model and study the case in which the attacker launches a multipronged ie multimode attack  we do so to more realistically capture the richness of reallife settings for example an attacker might simultaneously try to suppress some voters attract new voters into the election and introduce a spoiler candidate our model provides a unified framework for such varied attacks  by constructing polynomialtime multiprong attack algorithms we prove that for various election systems even such concerted flexible attacks can be perfectly planned in deterministic polynomial time




a  atserias j  k fichte and m  thurley 2011 clauselearning algorithms with many restarts and boundedwidth resolution volume 40 pages 353373

 we offer a new understanding of some aspects of practical satsolvers that are based on dpll with unitclause propagation clauselearning and restarts we do so by analyzing a concrete algorithm which we claim is faithful to what practical solvers do in particular before making any new decision or restart the solver repeatedly applies the unitresolution rule until saturation and leaves no component to the mercy of nondeterminism except for some internal randomness we prove the perhaps surprising fact that although the solver is not explicitly designed for it with high probability it ends up behaving as widthk resolution after no more than on2k2 conflicts and restarts where n is the number of variables in other words widthk resolution can be thought of as on2k2 restarts of the unitresolution rule with learning



x  tannier and p  muller 2011 evaluating temporal graphs built from texts via transitive reduction volume 40 pages 375413

temporal information has been the focus of recent attention in information extraction leading to some standardization effort in particular for the task of relating events in a text this task raises the problem of comparing two annotations of a given text because relations between events in a story are intrinsically interdependent and cannot be evaluated separately  a proper evaluation measure is also crucial in the context of a machine learning approach to the problem  finding a common comparison referent at the text level is not obvious and we argue here in favor of a shift from eventbased measures to measures on a unique textual object a minimal underlying temporal graph or more formally the transitive reduction of the graph of relations between event boundaries we support it by an  investigation of its properties on synthetic data and on a wellknow  temporal corpus




w  ruml m  b do r  zhou and m  pj fromherz 2011 online planning and scheduling an application to controlling modular printers volume 40 pages 415468

we present a case study of artificial intelligence techniques applied to the control of production printing equipment  like many other realworld applications this complex domain requires highspeed autonomous decisionmaking and robust continual operation  to our knowledge this work represents the first successful industrial application of embedded domainindependent temporal planning  our system handles execution failures and multiobjective preferences  at its heart is an online algorithm that combines techniques from statespace planning and partialorder scheduling  we suggest that this general architecture may prove useful in other applications as more intelligent systems operate in continual online settings  our system has been used to drive several commercial prototypes and has enabled a new product architecture for our industrial partner  when compared with stateoftheart offline planners our system is hundreds of times faster and often finds better plans  our experience demonstrates that domainindependent ai planning based on heuristic search can flexibly handle time resources replanning and multiple objectives in a highspeed practical application without requiring handcoded control knowledge



a  rahman and v  ng 2011 narrowing the modeling gap a clusterranking approach to coreference resolution volume 40 pages 469521

traditional learningbased coreference resolvers operate by training the mentionpair model for determining whether two mentions are coreferent or not though conceptually simple and easy to understand the mentionpair model is linguistically rather unappealing and lags far behind the heuristicbased coreference models proposed in the prestatistical nlp era in terms of sophistication two independent lines of recent research have attempted to improve the mentionpair model one by acquiring the mentionranking model to rank preceding mentions for a given anaphor and the other by training the entitymention model to determine whether a preceding cluster is coreferent with a given mention we propose a clusterranking approach to coreference resolution which combines the strengths of the mentionranking model and the entitymention model and is therefore theoretically more appealing than both of these models in addition we seek to improve cluster rankers via two extensions 1 lexicalization and 2 incorporating knowledge of anaphoricity by jointly modeling anaphoricity determination and coreference resolution experimental results on the ace data sets demonstrate the superior performance of cluster rankers to competing approaches as well as the effectiveness of our two extensions



r  he e  brunskill and n  roy 2011 efficient planning under uncertainty with macroactions volume 40 pages 523570

deciding how to act in partially observable environments remains an active area of research identifying good sequences of decisions is particularly challenging when good control performance requires planning multiple steps into the future in domains with many states towards addressing this challenge we present an online forwardsearch algorithm called the posterior belief distribution pbd pbd leverages a novel method for calculating the posterior distribution over beliefs that result after a sequence of actions is taken given the set of observation sequences that could be received during this process this method allows us to efficiently evaluate the expected reward of a sequence of primitive actions which we refer to as macroactions we present a formal analysis of our approach and examine its performance on two very large simulation experiments scientific exploration and a target monitoring domain we also demonstrate our algorithm being used to control a real robotic helicopter in a target monitoring experiment which suggests that our approach has practical potential for planning in realworld large partially observable domains where a multistep lookahead is required to achieve good performance



i  a kash e  j friedman and j  y halpern 2011 multiagent learning in large anonymous games volume 40 pages 571598

in large systems it is important for agents to learn to act effectively but sophisticated multiagent learning algorithms generally do not scale  an alternative approach is to find restricted classes of games where simple efficient algorithms converge  it is shown that stage learning efficiently converges to nash equilibria in large anonymous games if bestreply dynamics converge  two features are identified that improve convergence first rather than making learning more difficult more agents are actually beneficial in many settings  second providing agents with statistical information about the behavior of others can significantly reduce the number of observations needed



v  aravantinos r  caferra and n  peltier 2011 decidability and undecidability results for propositional schemata volume 40 pages 599656

we define a logic of propositional formula schemata adding to the syntax of propositional logic indexed propositions and iterated connectives ranging over intervals parameterized by arithmetic variables  the satisfiability problem is shown to be undecidable for this new logic but we introduce a very general class of schemata called boundlinear for which this problem becomes decidable  this result is obtained by reduction to a particular class of schemata called regular for which we provide a sound and complete terminating proof procedure  this schemata calculus allows one to capture proof patterns corresponding to a large class of problems specified in propositional logic we also show that the satisfiability problem becomes again undecidable for slight extensions of this class thus demonstrating that boundlinear schemata represent a good compromise between expressivity and decidability



l  bordeaux g  katsirelos n  narodytska and m  y vardi 2011 the complexity of integer bound propagation volume 40 pages 657676

bound propagation is an important artificial intelligence technique used in constraint programming tools to deal with numerical constraints it is typically embedded within a search procedure branch and prune and used at every node of the search tree to narrow down the search space so it is critical that it be fast the procedure invokes constraint propagators until a common fixpoint is reached but the known algorithms for this have a pseudopolynomial worstcase time complexity they are fast indeed when the variables have a small numerical range but they have the wellknown problem of being prohibitively slow when these ranges are large an important question is therefore whether stronglypolynomial algorithms exist that compute the common bound consistent fixpoint of a set of constraints this paper answers this question in particular we show that this fixpoint computation is in fact npcomplete even when restricted to binary linear constraints



f  wu j  madhavan and a  halevy 2011 identifying aspects for websearch queries volume 40 pages 677700

many websearch queries serve as the beginning of an exploration of an unknown space of information rather than looking for a specific web page to answer such queries effec tively the search engine should attempt to organize the space of relevant information in a way that facilitates exploration
aspector combines two sources of information to compute aspects we discover candidate aspects by analyzing query logs and cluster them to eliminate redundancies we then use a masscollaboration knowledge base eg wikipedia to compute candidate aspects for queries that occur less frequently and to group together aspects that are likely to be semantically related we present a user study that indicates that the aspects we compute are rated favorably against three competing alternatives  related searches proposed by google cluster labels assigned by the clusty search engine and navigational searches proposed by bing



a  cimatti a  griggio and r  sebastiani 2011 computing small unsatisfiable cores in satisfiability modulo theories volume 40 pages 701728

the problem of finding small unsatisfiable cores for sat formulas has  recently received a lot of interest mostly for its applications in formal verification  however propositional logic is often not expressive  enough for representing many interesting verification problems which can be more naturally addressed in the framework of satisfiability modulo theories smt  surprisingly the problem of finding unsatisfiable cores in smt has received very little attention in the literature
we have evaluated our algorithm with a very extensive empirical test on smtlib benchmarks which confirms the validity and potential of this approach 



w  li p  poupart and p  van beek 2011 exploiting structure in weighted model counting approaches to probabilistic inference volume 40 pages 729765

previous studies have demonstrated that encoding a bayesian network into a sat formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference  in this paper we present techniques for improving this approach for bayesian networks with noisyor and noisymax relationstwo relations that are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify in particular we present two sat encodings for noisyor and two encodings for noisymax that exploit the structure or semantics of the relations to improve both time and space efficiency and we prove the correctness of the encodings we experimentally evaluated our techniques on largescale real and randomly generated bayesian networks  on these benchmarks our techniques gave speedups of up to two orders of magnitude over the best previous approaches for networks with noisyormax relations and scaled up to larger networks as well our techniques extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach




t  de la rosa s  jimenez r  fuentetaja and d  borrajo 2011 scaling up heuristic planning with relational decision trees volume 40 pages 767813


current evaluation functions for heuristic planning are expensive to compute in numerous planning problems these functions provide good guidance to the solution so they are worth the expense however when  evaluation functions are misguiding or when planning problems are large enough lots of node evaluations must be computed which severely limits the scalability of heuristic planners in this paper we present a novel solution for reducing node evaluations in heuristic planning based on machine learning particularly we define the task of learning search control for heuristic planning as a relational classification task and we use an offtheshelf relational classification tool to address this learning task our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain these planning contexts are defined by the set of helpful actions of the current state the goals remaining to be achieved and the static predicates of the planning task this paper shows two methods for guiding the search of a heuristic planner with the learned classifiers the first one consists of using the resulting classifier as an action policy the second one consists of applying the classifier to generate lookahead states within a best first search algorithm experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than stateoftheart planners




h  papadopoulos v  vovk and a  gammerman 2011 regression conformal prediction with nearest neighbours volume 40 pages 815840

in this paper we apply conformal prediction cp to the knearest neighbours regression knnr algorithm and propose ways of extending the typical nonconformity measure used for regression so far unlike traditional regression methods which produce point predictions conformal predictors output predictive regions that satisfy a given confidence level the regions produced by any conformal predictor are automatically valid however their tightness and therefore usefulness depends on the nonconformity measure used by each cp in effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm we define six novel nonconformity measures based on the knearest neighbours regression algorithm and develop the corresponding cps following both the original transductive and the inductive cp approaches a comparison of the predictive regions produced by our measures with those of the typical regression measure suggests that a major improvement in terms of predictive region tightness is achieved by the new measures
