d  ortizboyer  c  herv225smart237nez and  n  garc237apedrajas 2005 cixl2 a crossover operator for evolutionary algorithms based on population features volume 24 pages 148

in this paper we propose a crossover operator for evolutionary algorithms with real values that is based on the statistical theory of population distributions the operator is based on the theoretical distribution of the values of the genes of the best individuals in the population  the proposed operator takes into account the localization and dispersion features of the best individuals of the population with the objective that these features would be inherited by the offspring our aim is the optimization of the balance between exploration and exploitation in the search process    in order to test the efficiency and robustness of this crossover we have used a set of functions to be optimized with regard to different criteria such as multimodality separability regularity and epistasis with this set of functions we can extract conclusions in function of the problem at hand we analyze the results using anova and multiple comparison statistical tests  as an example of how our crossover can be used to solve artificial intelligence problems we have applied the proposed model to the problem of obtaining the weight of each network in a ensemble of neural networks the results obtained are above the performance of standard methods



p   j gmytrasiewicz and  p  doshi 2005 a framework for sequential planning in multiagent settings volume 24 pages 4979

this paper extends the framework of partially observable markov decision processes pomdps to multiagent settings by incorporating the notion of agent models into the state space  agents maintain beliefs over physical states of the environment and over models of other agents and they use bayesian updates to maintain their beliefs over time the solutions map belief states to actions models of other agents may include their belief states and are related to agent types considered in games of incomplete information  we express the agents autonomy by postulating that their models are not directly manipulable or observable by other agents  we show that important properties of pomdps such as convergence of value iteration the rate of convergence and piecewise linearity and convexity of the value functions carry over to our framework  our approach complements a more traditional approach to interactive settings which uses nash equilibria as a solution paradigm  we seek to avoid some of the drawbacks of equilibria which may be nonunique and do not capture offequilibrium behaviors  we do so at the cost of having to represent process and continuously revise models of other agents since the agents beliefs may be arbitrarily nested the optimal solutions to decision making problems are only asymptotically computable  however approximate belief updates and approximately optimal plans are computable we illustrate our framework using a simple application domain and we show examples of belief updates and value functions



p  geibel and  f  wysotzki 2005 risksensitive reinforcement learning applied to control under constraints volume 24 pages 81108

in this paper we consider markov decision processes mdps with error states  error states are those states entering which is undesirable or dangerous we define the risk with respect to a policy as the probability of entering such a state when the policy is pursued we consider the problem of finding good policies whose risk is smaller than some userspecified threshold and formalize it as a constrained mdp with two criteria the first criterion corresponds to the value function originally given we will show that the risk can be formulated as a second criterion function based on a cumulative return whose definition is independent of the original value function  we present a model free heuristic reinforcement learning algorithm that aims at finding good deterministic policies  it is based on weighting the original value function and the risk the weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function the algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column this control task was originally formulated as an optimal control problem with chance constraints and it was solved under certain assumptions on the model to obtain an optimal solution the power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed



p  j hawkins  v  lagoon and  p  j stuckey 2005 solving set constraint satisfaction problems using robdds volume 24 pages 109156

in this paper we present a new approach to modeling finite set domain constraint problems using reduced ordered binary decision diagrams robdds we show that it is possible to construct an efficient set domain propagator which compactly represents many set domains and set constraints using robdds  we demonstrate that the robddbased approach provides unprecedented flexibility in modeling constraint satisfaction problems leading to performance improvements we also show that the robddbased modeling approach can be extended to the modeling of integer and multiset constraint problems in a straightforward manner since domain propagation is not always practical we also show how to incorporate less strict consistency notions into the robdd framework such as set bounds cardinality bounds and lexicographic bounds consistency finally we present experimental results that demonstrate the robddbased solver performs better than various more conventional constraint solvers on several standard set constraint problems



p  w jordan and  m  a walker 2005 learning content selection rules for generating object descriptions in dialogue volume 24 pages 157194

a fundamental requirement of any taskoriented dialogue system is the ability to generate object descriptions that refer to objects in the task domain the subproblem of content selection for object descriptions in taskoriented dialogue has been the focus of much previous work and a large number of models have been proposed in this paper we use the annotated coconut corpus of taskoriented design dialogues to develop feature sets based on dale and reiters 1995 incremental model brennan and clarks 1996 conceptual pact model and jordans 2000b intentional influences model and use these feature sets in a machine learning experiment to automatically learn a model of content selection for object descriptions  since dale and reiters model requires a representation of discourse structure the corpus annotations are used to derive a representation based on grosz and sidners 1986 theory of the intentional structure of discourse as well as two very simple representations of discourse structure based purely on recency we then apply the ruleinduction program ripper to train and test the content selection component of an object description generator on a set of 393 object descriptions from the corpus to our knowledge this is the first reported experiment of a trainable content selection component for object description generation in dialogue three separate content selection models that are based on the three theoretical models all independently achieve accuracies significantly above the majority class baseline 17 on unseen test data with the intentional influences model 424 performing significantly better than either the incremental model 304 or the conceptual pact model 289 but the best performing models combine all the feature sets achieving accuracies near 60 surprisingly a simple recencybased representation of discourse structure does as well as one based on intentional structure  to our knowledge this is also the first empirical comparison of a representation of grosz and sidners model of discourse structure with a simpler model for any generation task



m  tj spaan and  n  vlassis 2005 perseus randomized pointbased value iteration for pomdps volume 24 pages 195220

partially observable markov decision processes pomdps form an attractive and principled framework for agent planning under uncertainty  pointbased approximate techniques for pomdps compute a policy based on a finite set of points collected in advance from the agents belief space  we present a randomized pointbased value iteration algorithm called perseus  the algorithm performs approximate value backup stages ensuring that in each backup stage the value of each point in the belief set is improved the key observation is that a single backup may improve the value of many belief points  contrary to other pointbased methods perseus backs up only a randomly selected subset of points in the belief set sufficient for improving the value of each belief point in the set we show how the same idea can be extended to dealing with continuous action spaces  experimental results show the potential of perseus in large scale pomdp problems



j  p watson  l  d whitley and  a  e howe 2005 linking search space structure runtime dynamics and problem difficulty a step toward demystifying tabu search volume 24 pages 221261

tabu search is one of the most effective heuristics for locating highquality solutions to a diverse array of nphard combinatorial optimization problems despite the widespread success of tabu search researchers have a poor understanding of many key theoretical aspects of this algorithm including models of the highlevel runtime dynamics and identification of those search space features that influence problem  difficulty we consider these questions in the context of the jobshop  scheduling problem jsp a domain where tabu search algorithms have  been shown to be remarkably effective previously we demonstrated  that the mean distance between random local optima and the nearest  optimal solution is highly correlated with problem difficulty for a  wellknown tabu search algorithm for the jsp introduced by taillard in this paper we discuss various shortcomings of this measure and  develop a new model of problem difficulty that corrects these deficiencies we show that taillards algorithm can be modeled  with high fidelity as a simple variant of a straightforward random  walk the random walk model accounts for nearly all of the variability in the cost required to locate both optimal and suboptimal solutions to random jsps and provides an explanation for differences in the difficulty of random versus structured jsps finally we discuss and  empirically substantiate two novel predictions regarding tabu search  algorithm behavior first the method for constructing the initial solution is  highly unlikely to impact the performance of tabu search second tabu  tenure should be selected to be as small as possible while simultaneously  avoiding search stagnation values larger than necessary lead to  significant degradations in performance



v  bayerzubek and  t  g dietterich 2005 integrating learning from examples into the search for diagnostic policies volume 24 pages 263303

this paper studies the problem of learning diagnostic policies from training examples a diagnostic policy is a complete description of the decisionmaking actions of a diagnostician ie tests followed by a diagnostic decision for all possible combinations of test results  an optimal diagnostic policy is one that minimizes the expected total cost which is the sum of measurement costs and misdiagnosis costs  in most diagnostic settings there is a tradeoff between these two kinds of costs  this paper formalizes diagnostic decision making as a markov decision process mdp the paper introduces a new family of systematic search algorithms based on the ao algorithm to solve this mdp  to make ao efficient the paper describes an admissible heuristic that enables ao to prune large parts of the search space  the paper also introduces several greedy algorithms including some improvements over previouslypublished methods the paper then addresses the question of learning diagnostic policies from examples  when the probabilities of diseases and test results are computed from training data there is a great danger of overfitting to reduce overfitting regularizers are integrated into the search algorithms  finally the paper compares the proposed methods on five benchmark diagnostic data sets  the studies show that in most cases the systematic search methods produce better diagnostic policies than the greedy methods in addition the studies show that for training sets of realistic size the systematic search algorithms are practical on todays desktop computers



p  cimiano  a  hotho and  s  staab 2005 learning concept hierarchies from text corpora using formal concept analysis volume 24 pages 305339

we present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus the approach is based on formal concept analysis fca a method mainly used for the analysis of data ie for investigating and processing explicitly given information  we follow harris distributional hypothesis and model the context of a certain term as a vector representing syntactic dependencies which are automatically acquired from the text corpus with a linguistic parser  on the basis of this context information fca produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy  the approach is evaluated by comparing the resulting concept hierarchies with handcrafted taxonomies for two domains tourism and finance  we also directly compare our approach with hierarchical agglomerative clustering as well as with bisectionkmeans as an instance of a divisive clustering algorithm furthermore we investigate the impact of using different measures weighting the contribution of each attribute as well as of applying a particular smoothing technique to cope with data sparseness



r  khardon  d  roth and  r  a servedio 2005 efficiency versus convergence of boolean kernels for online learning algorithms volume 24 pages 341356

the paper studies machine learning problems where each example is described using a set of boolean features and where hypotheses are represented by linear threshold elements  one method of increasing the expressiveness of learned hypotheses in this context is to expand the feature set to include conjunctions of basic features this can be done explicitly or where possible by using a kernel function focusing on the well known perceptron and winnow algorithms the paper demonstrates a tradeoff between the computational efficiency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm  we first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions  we show that these kernels can be used to efficiently run the perceptron algorithm over a feature space of exponentially many conjunctions however we also show that using such kernels the perceptron algorithm can provably make an exponential number of mistakes even when learning simple functions  we then consider the question of whether kernel functions can analogously be used to run the multiplicativeupdate winnow algorithm over an expanded feature space of exponentially many conjunctions known upper bounds imply that the winnow algorithm can learn disjunctive normal form dnf formulae with a polynomial mistake bound in this setting  however we prove that it is computationally hard to simulate winnows behavior for learning dnf over such a feature set this implies that the kernel functions which correspond to running winnow for this problem are not efficiently computable and that there is no general construction that can run winnow with kernels



g  gottlob  g  greco and  f  scarcello 2005 pure nash equilibria hard and easy games volume 24 pages 357406




2008 ijcaijair best paper prize

we investigate complexity issues related to pure nash equilibria of strategic games we show that even in very restrictive settings determining whether a game has a pure nash equilibrium is nphard while deciding whether a game has a strong nash equilibrium is sigmap2complete  we then study practically relevant restrictions that lower the complexity in particular we are interested in quantitative and qualitative restrictions of the way each players payoff depends on moves of other players  we say that a game has small neighborhood if the utility function for each player depends only on the actions of a logarithmically small number of other players the dependency structure of a game g can be expressed by a graph dgg or by a hypergraph hg by relating nash equilibrium problems to constraint satisfaction problems csps we show that if g has small neighborhood and if hg has bounded hypertree width or if dgg has bounded treewidth then finding pure nash and pareto equilibria is feasible in polynomial time if the game is graphical then these problems are logcflcomplete and thus in the class nc2 of highly parallelizable problems



p  s dutta  n  r jennings and  l  moreau 2005 cooperative information sharing to improve distributed learning in multiagent systems volume 24 pages 407463

effective coordination of agents actions in partiallyobservable domains is a major challenge of multiagent systems research  to address this many researchers have developed techniques that allow the agents to make decisions based on estimates of the states and actions of other agents that are typically learnt using some form of machine learning algorithm  nevertheless many of these approaches fail to provide an actual means by which the necessary information is made available so that the estimates can be learnt  to this end we argue that cooperative communication of state information between agents is one such mechanism  however in a dynamically changing environment the accuracy and timeliness of this communicated information determine the fidelity of the learned estimates and the usefulness of the actions taken based on these  given this we propose a novel informationsharing protocol  posttaskcompletion sharing for the distribution of state information  we then show through a formal analysis the improvement in the quality of estimates produced using our strategy over the widely used protocol of sharing information between nearest neighbours  moreover communication heuristics designed around our informationsharing principle are subjected to empirical evaluation along with other benchmark strategies including littmans qrouting and stones tpotrl in a simulated callrouting application  these studies conducted across a range of environmental settings show that compared to the different benchmarks used our strategy generates an improvement of up to 60 in the call connection rate of more than 1000 in the ability to connect longdistance calls and incurs as low as 025 of the message overhead



q  b vo and  n  y foo 2005 reasoning about action an argumentation  theoretic approach volume 24 pages 465518

we present a uniform nonmonotonic solution to the problems of reasoning about action on the basis of an argumentationtheoretic approach our theory is provably correct relative to a sensible minimisation policy introduced on top of a temporal propositional logic sophisticated problem domains can be formalised in our framework  as much attention of researchers in the field has been paid to the traditional and basic problems in reasoning about actions such as the frame the qualification and the ramification problems approaches to these problems within our formalisation lie at heart of the expositions presented in this paper



j  hoffmann and s   edelkamp 2005 the deterministic part of ipc4 an overview volume 24 pages 519579

we provide an overview of the organization and results of the deterministic part of the 4th international planning competition ie of the part concerned with evaluating systems doing deterministic planning ipc4 attracted even more competing systems than its already large predecessors and the competition event was revised in several important respects after giving an introduction to the ipc we briefly explain the main differences between the deterministic part of ipc4 and its predecessors we then introduce formally the language used called pddl22 that extends pddl21 by derived predicates and timed initial literals we list the competing systems and overview the results of the competition the entire set of data is far too large to be presented in full we provide a detailed summary the complete data is available in an online appendix we explain how we awarded the competition prizes  



a  botea m  enzenberger m  mueller and j  schaeffer 2005 macroff improving ai planning with automatically learned macrooperators volume 24 pages 581621

despite recent progress in ai planning many benchmarks remain challenging for current planners in many domains the performance of a planner can greatly be improved by discovering and exploiting information about the domain structure that is not explicitly encoded in the initial pddl formulation in this paper we present and compare two automated methods that learn relevant information from previous experience in a domain and use it to solve new problem instances our methods share a common fourstep strategy first a domain is analyzed and structural information is extracted then macrooperators are generated based on the previously discovered structure a filtering and ranking procedure selects the most useful macrooperators finally the selected macros are used to speed up future searches  we have successfully used such an approach in the fourth international planning competition ipc4 our system macroff extends hoffmanns stateoftheart planner ff 23 with support for two kinds of macrooperators and with engineering enhancements we demonstrate the effectiveness of our ideas on benchmarks from international planning competitions our results indicate a large reduction in search effort in those complex domains where structural information can be inferred   



d  achlioptas h  jia and c  moore 2005 hiding satisfying assignments two are better than one volume 24 pages 623639

the evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances a plausible source of such instances consists of random ksat formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment a unfortunately instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics roughly speaking for a number of different algorithms a acts as a stronger and stronger attractor as the formulas density increases motivated by recent results on the geometry of the space of satisfying truth assignments of random ksat and naeksat formulas we introduce a simple twist on this basic model which appears to dramatically increase its hardness namely in addition to forbidding the clauses violated by the hidden assignment a we also forbid the clauses violated by its complement so that both a and compliment of a are satisfying it appears that under this symmetrization the effects of the two attractors largely cancel out making it much harder for algorithms to find any truth assignment we give theoretical and experimental evidence supporting this assertion 



n  samaras and k  stergiou 2005 binary encodings of nonbinary constraint satisfaction problems algorithms and experimental results volume 24 pages 641684

a nonbinary constraint satisfaction problem csp can be solved directly using extended versions of binary techniques alternatively the nonbinary problem can be translated into an equivalent binary one in this case it is generally accepted that the translated problem can be solved by applying wellestablished techniques for binary csps in this paper we evaluate the applicability of the latter approach we demonstrate that the use of standard techniques for binary csps in the encodings of nonbinary problems is problematic and results in models that are very rarely competitive with the nonbinary representation to overcome this we propose specialized arc consistency and search algorithms for binary encodings and we evaluate them theoretically and empirically we consider three binary representations the hidden variable encoding the dual encoding and the double encoding theoretical and empirical results show that for certain classes of nonbinary constraints binary encodings are a competitive option and in many cases a better one than the nonbinary representation 



j  hoffmann 2005 where ignoring delete lists works local search topology in planning benchmarks volume 24 pages 685758

between 1998 and 2004 the planning community has seen vast progress in terms of the sizes of benchmark examples that domainindependent planners can tackle successfully the key technique behind this progress is the use of heuristic functions based on relaxing the planning task at hand where the relaxation is to assume that all delete lists are empty the unprecedented success of such methods in many commonly used benchmark examples calls for an understanding of what classes of domains these methods are well suited for   in the investigation at hand we derive a formal background to such an understanding we perform a case study covering a range of 30 commonly used strips and adl benchmark domains including all examples used in the first four international planning competitions we prove connections between domain structure and local search topology  heuristic cost surface properties  under an idealized version of the heuristic functions used in modern planners the idealized heuristic function is called h and differs from the practically used functions in that it returns the length of an optimal relaxed plan which is nphard to compute we identify several key characteristics of the topology under h concerning the existencenonexistence of unrecognized dead ends as well as the existencenonexistence of constant upper bounds on the difficulty of escaping local minima and benches these distinctions divide the set of all planning domains into a taxonomy of classes of varying h topology as it turns out many of the 30 investigated domains lie in classes with a relatively easy topology most particularly 12 of the domains lie in classes where ffs search algorithm provided with h is a polynomial solving mechanism   we also present results relating h to its approximation as implemented in ff the behavior regarding dead ends is provably the same we summarize the results of an empirical investigation showing that in many domains the topological qualities of h are largely inherited by the approximation the overall investigation gives a rare example of a successful analysis of the connections between typicalcase problem structure and search performance the theoretical investigation also gives hints on how the topological phenomena might be automatically recognizable by domain analysis techniques we outline some preliminary steps we made into that direction 



s  sanghai p  domingos and d  weld 2005 relational dynamic bayesian networks volume 24 pages 759797

stochastic processes that involve the creation of objects and relations over time are widespread but relatively poorly studied for example accurate fault diagnosis in factory assembly processes requires inferring the probabilities of erroneous assembly operations but doing this efficiently and accurately is difficult modeled as dynamic bayesian networks these processes have discrete variables with very large domains and extremely high dimensionality in this paper we introduce relational dynamic bayesian networks rdbns which are an extension of dynamic bayesian networks dbns to firstorder logic rdbns are a generalization of dynamic probabilistic relational models dprms which we had proposed in our previous work to model dynamic uncertain domains we first extend the raoblackwellised particle filtering described in our earlier work to rdbns next we lift the assumptions associated with raoblackwellization in rdbns and propose two new forms of particle filtering the first one uses abstraction hierarchies over the predicates to smooth the particle filters estimates the second employs kernel density estimation with a kernel function specifically designed for relational domains experiments show these two methods greatly outperform standard particle filtering on the task of assembly plan execution monitoring 




m  beetz and h  grosskreutz 2005 probabilistic hybrid action models for predicting concurrent perceptdriven robot behavior volume 24 pages 799849

this article develops probabilistic hybrid action models phams a realistic causal model for predicting the behavior generated by modern perceptdriven robot plans phams represent aspects of robot behavior that cannot be represented by most action models used in ai planning the temporal structure of continuous control processes their nondeterministic effects several modes of their interferences and the achievement of triggering conditions in closedloop robot plans 




hls  younes m  l littman d  weissman and j  asmuth 2005 the first probabilistic track of the international planning competition volume 24 pages 851887

the 2004 international planning competition ipc4 included a probabilistic planning track for the first time we describe the new domain specification language we created for the track our evaluation methodology the competition domains we developed and the results of the participating teams



m  jaeger 2005 ignorability in statistical and probabilistic inference volume 24 pages 889917

when dealing with incomplete data in statistical learning or incomplete observations in probabilistic inference one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive one asks for conditions under which it can be safely ignored such conditions are given by the missing at random mar and coarsened at random car assumptions in this paper we provide an indepth analysis of several questions relating to marcar assumptions main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process this question is complicated by the fact that several distinct versions of marcar assumptions exist we therefore first provide an overview over these different versions in which we highlight the distinction between distributional and coarsening variable induced versions we show that distributional versions are less restrictive and sufficient for most applications we then address from two different perspectives the question of when the marcar assumption is warranted first we provide a static analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations here we obtain an equivalence characterization that improves and extends a recent result by grunwald and halpern we then turn to a procedural analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data or observation generating process the main result of this analysis is that the stronger coarsened completely at random ccar condition is arguably the most reasonable assumption as it alone corresponds to data coarsening procedures that satisfy a natural robustness property 




mhl  van den briel and s  kambhampati 2005 optiplan unifying ipbased and graphbased planning volume 24 pages 919931

the optiplan planning system is the first integer programmingbased planner that successfully participated in the international planning competition this engineering note describes the architecture of optiplan and provides the integer programming formulation that enabled it to perform reasonably well in the competition we also touch upon some recent developments that make integer programming encodings significantly more competitive



b  bonet and h  geffner 2005 mgpt a probabilistic planner based on heuristic search volume 24 pages 933944

we describe the version of the gpt planner used in the probabilistic track of the 4th international planning competition ipc4 this version called mgpt solves markov decision processes specified in the ppddl language by extracting and using different classes of lower bounds along with various heuristicsearch algorithms the lower bounds are extracted from deterministic relaxations where the alternative probabilistic effects of an action are mapped into different independent deterministic actions the heuristicsearch algorithms use these lower bounds for focusing the updates and delivering a consistent value function over all states reachable from the initial state and the greedy policy 

