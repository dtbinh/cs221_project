s  k murthy  s  kasif and  s  salzberg 1994 a system for induction of oblique decision trees volume 2 pages 132

this article describes a new system for induction ofoblique   decision trees  this system oc1 combines deterministic   hillclimbing with two forms of randomization to find a goodoblique   split in the form of a hyperplane at each node of a decisiontree   oblique decision tree methods are tuned especially for domains in   which the attributes are numeric although they can be adapted to   symbolic or mixed symbolicnumeric attributes  we presentextensive   empirical studies using both real and artificial data thatanalyze   oc1s ability to construct oblique trees that are smaller and more   accurate than their axisparallel counterparts  we also examinethe   benefits of randomization for the construction of oblique decisiontrees



a  j grove  j  y halpern and  d  koller 1994 random worlds and maximum entropy volume 2 pages 3388

given a knowledge base ikbi containing firstorder and statistical facts we consider a principled method called the irandomworlds methodi for computing a degree of belief that some formula iphii holds given ikbi  if  we are reasoning about a world or system consisting of ini individuals then we can consider all possible worlds or firstorder models withdomain i1ni that satisfy ikbi and compute thefraction of them in which iphii is true we define the degree of belief to be the asymptotic value of this fraction as ini grows large  we show that when the vocabulary underlying iphii andikbi uses constants and unary predicates only we can naturally associate an ientropyi with each world as ini grows largerthere are many more worlds with higher entropy  therefore we can usea i maximumentropyi computation to compute the degree of belief this result is in a similar spirit to previous work in physics and artificial intelligence but is far more general  of equal interest to the result itself are the limitations on its scope  most importantly the restriction to unary predicates seems necessary although the randomworlds method makes sense in general the connection to maximum entropy seems to disappear in the nonunary case  these observations suggest unexpected limitations to the applicability of maximumentropy methods



t  kitani  y  eriguchi and  m  hara 1994 pattern matching and discourse processing in information extraction   from japanese text volume 2 pages 89110

information extraction is the task of automaticallypicking   up information of interest from an unconstrained text  informationof   interest is usually extracted in two steps  first sentence level   processing locates relevant pieces of information scatteredthroughout   the text second discourse processing merges coreferential   information to generate the output  in the first step pieces of   information are locally identified without recognizing any   relationships among them  a key word search or simple patternsearch   can achieve this purpose  the second step requires deeperknowledge   in order to understand relationships among separately identified   pieces of information  previous information extraction systems   focused on the first step partly because they were not required to   link up each piece of information with other pieces  to link the   extracted pieces of information and map them onto a structuredoutput   format complex discourse processing is essential  this paperreports   on a japanese information extraction system that merges information   using a pattern matcher and discourse processor  evaluationresults   show a high level of system performance which approaches human   performance



s  safra and  m  tennenholtz 1994 on planning while learning volume 2 pages 111129

this paper introduces a framework for planning while   learning where an agent is given a goal to achieve in anenvironment   whose behavior is only partially known to the agent      we discuss the tractability of various plandesign processes we   show that for a large natural class of planning while learning   systems a plan can be presented and verified in a reasonable time   however coming up algorithmically with a plan even for simple   classes of systems is apparently intractable      we emphasize the role of offline plandesign processes andshow   that in most natural cases the verification projection part canbe   carried out in an efficient algorithmic manner



s  soderland and  w  lehnert 1994 wrapup a trainable discourse module for information extraction volume 2 pages 131158

the vast amounts of online text now available have ledto   renewed interest in information extraction ie systems thatanalyze   unrestricted text producing a structured representation ofselected   information from the text this paper presents a novel approachthat   uses machine learning to acquire knowledge for some of the higher   level ie processing  wrapup is a trainable ie discourse component   that makes intersentential inferences and identifies logicalrelations   among information extracted from the text  previous corpusbased   approaches were limited to lower level processing such as   partofspeech tagging lexical disambiguation and dictionary   construction  wrapup is fully trainable and not onlyautomatically   decides what classifiers are needed but even derives the featureset   for each classifier automatically performance equals that of a   partially trainable discourse module requiring manual customization   for each domain



w  l buntine 1994 operations for learning with graphical models volume 2 pages 159225

this paper is a multidisciplinary review of empirical  statistical learning from a graphical model perspective  wellknown  examples of graphical models include bayesian networks directed  graphs representing a markov chain and undirected networks  representing a markov field  these graphical models are extended to  model data analysis and empirical learning using the notation of  plates  graphical operations for simplifying and manipulating a  problem are provided including decomposition differentiation andthe  manipulation of probability models from the exponential family  two  standard algorithm schemas for learning are reviewed in a graphical  framework gibbs sampling and the expectation maximizationalgorithm  using these operations and schemas some popular algorithms can be  synthesized from their graphical specification  this includes  versions of linear regression techniques for feedforward networks  and learning gaussian and discrete bayesian networks from data  the  paper concludes by sketching some implications for data analysis and  summarizing how some popular algorithms fall within the framework  presented  the main original contributions here are the decompositiontechniques  and the demonstration that graphical models provide a framework for  understanding and developing complex learning algorithms



s  minton  j  bresina and  m  drummond 1994 totalorder and partialorder planning a comparative analysis volume 2 pages 227262

for many years the intuitions underlying partialorder   planning were largely taken for granted only in the past few years   has there been renewed interest in the fundamental principles   underlying this paradigm  in this paper we present a rigorous   comparative analysis of partialorder and totalorder planning by   focusing on two specific planners that can be directly compared we   show that there are some subtle assumptions that underly the   widespread intuitions regarding the supposed efficiency of   partialorder planning for instance the superiority ofpartialorder   planning can depend critically upon the search strategy and the   structure of the search space  understanding the underlying   assumptions is crucial for constructing efficient planners



t  g dietterich and  g  bakiri 1995 solving multiclass learning problems via errorcorrecting output codes volume 2 pages 263286

multiclass learning problems involve finding a definitionfor an unknown function ifix whose range is a discrete setcontaining iki  2 values ie iki classes  thedefinition is acquired by studying collections of training examples ofthe form xi i f ixi  existing approaches tomulticlass learning problems include direct application of multiclassalgorithms such as the decisiontree algorithms c45 and cartapplication of binary concept learning algorithms to learn individualbinary functions for each of the i k i classes and application ofbinary concept learning algorithms with distributed outputrepresentations  this paper compares these three approaches to a newtechnique in which errorcorrecting codes are employed as adistributed output representation  we show that these outputrepresentations improve the generalization performance of both c45and backpropagation on a wide range of multiclass learning tasks  wealso demonstrate that this approach is robust with respect to changesin the size of the training sample the assignment of distributedrepresentations to particular classes and the application ofoverfitting avoidance techniques such as decisiontree pruningfinally we show thatlike the other methodsthe errorcorrectingcode technique can provide reliable class probability estimatestaken together these results demonstrate that errorcorrecting outputcodes provide a generalpurpose method for improving the performanceof inductive learning programs on multiclass problems



p  cichosz 1995 truncating temporal differences on the efficient implementation of    tdlambda for reinforcement learning volume 2 pages 287318

temporal difference td methods constitute a class of   methods for learning predictions in multistep prediction problems   parameterized by a recency factor lambda currently the most important   application of these methods is to temporal credit assignment in   reinforcement learning well known reinforcement learning algorithms   such as ahc or qlearning may be viewed as instances of td learning   this paper examines the issues of the efficient and general   implementation of tdlambda for arbitrary lambda for use with   reinforcement learning algorithms optimizing the discounted sum of   rewards the traditional approach based on eligibility traces is   argued to suffer from both inefficiency and lack of generality the   ttd truncated temporal differences procedure is proposed as an   alternative that indeed only approximates tdlambda but requires   very little computation per action and can be used with arbitrary   function representation methods  the idea from which it is derived is   fairly simple and not new but probably unexplored so far encouraging   experimental results are presented suggesting that using lambda  0   with the ttd procedure allows one to obtain a significant learning   speedup at essentially the same cost as usual td0 learning



s  hanks and  d  s weld 1995 a domainindependent algorithm for plan adaptation volume 2 pages 319360

the paradigms of transformational planning casebased   planning and plan debugging all involve a process known as i plan   adaptation i  modifying or repairing an old plan so it solves a new   problem  in this paper we provide a domainindependent algorithm for   plan adaptation demonstrate that it is sound complete and   systematic and compare it to other adaptation algorithms in the   literature    our approach is based on a view of planning as searching a graph of   partial plans  generative planning starts at the graphs root and   moves from node to node using planrefinement operators  in planning   by adaptation a library plan  an arbitrary node in the plan   graph  is the starting point for the search and the planadaptation   algorithm can apply both the same refinement operators available to a   generative planner and can also retract constraints and steps from the   plan  our algorithms completeness ensures that the adaptation   algorithm will eventually search the entire graph and its systematicity    ensures that it will do so without redundantly searching any parts of   the graph



j  ortega 1995 on the informativeness of the dna promoter sequences domain theory volume 2 pages 361367

the dna promoter sequences domain theory and database havebecome popular for testing systems that integrate empirical andanalytical learning  this note reports a simple change andreinterpretation of the domain theory in terms of mofn conceptsinvolving no learning that results in an accuracy of 934 on the 106items of the database  moreover an exhaustive search of the space ofmofn domain theory interpretations indicates that the expectedaccuracy of a randomly chosen interpretation is 765 and that amaximum accuracy of 972 is achieved in 12 cases  this demonstratesthe informativeness of the domain theory without the complications ofunderstanding the interactions between various learning algorithms andthe theory  in addition our results help characterize the difficultyof learning using the dna promoters theory



p  d turney 1995 costsensitive classification empirical evaluation of a hybrid    genetic decision tree induction algorithm volume 2 pages 369409

this paper introduces icet a new algorithm for   costsensitive classification icet uses a genetic algorithm to evolve   a population of biases for a decision tree induction algorithm the   fitness function of the genetic algorithm is the average cost of   classification when using the decision tree including both the costs   of tests features measurements and the costs of classification   errors icet is compared here with three other algorithms for   costsensitive classification  eg2 csid3 and idx  and also with   c45 which classifies without regard to cost the five algorithms are   evaluated empirically on five realworld medical datasets three sets   of experiments are performed  the first set examines the baseline   performance of the five algorithms on the five datasets and   establishes that icet performs significantly better than its   competitors the second set tests the robustness of icet under a   variety of conditions and shows that icet maintains its advantage the   third set looks at icets search in bias space and discovers a way to   improve the search



s  k donoho and  l  a rendell 1995 rerepresenting and restructuring domain theories  a constructive    induction approach volume 2 pages 411446

theory revision integrates inductive learning and background   knowledge by combining training examples with a coarse domain theory   to produce a more accurate theory  there are two challenges that   theory revision and other theoryguided systems face  first a   representation language appropriate for the initial theory may be   inappropriate for an improved theory  while the original   representation may concisely express the initial theory a more   accurate theory forced to use that same representation may be bulky   cumbersome and difficult to reach  second a theory structure   suitable for a coarse domain theory may be insufficient for a   finetuned theory  systems that produce only small local changes to   a theory have limited value for accomplishing complex structural   alterations that may be required      consequently advanced theoryguided learning systems require flexible    representation and flexible structure  an analysis of various theory    revision systems and theoryguided learning systems reveals specific    strengths and weaknesses in terms of these two desired properties     designed to capture the underlying qualities of each system a new    system uses theoryguided constructive induction  experiments in    three domains show improvement over previous theoryguided systems     this leads to a study of the behavior limitations and potential of    theoryguided constructive induction



p  david 1995 using pivot consistency to decompose and solve functional csps volume 2 pages 447474

many studies have been carried out in order to increase thesearch efficiency of constraint satisfaction problems among themsome make use of i structural i properties of the constraintnetwork others take into account i semantic i properties of theconstraints generally assuming that i all i the constraints possessthe given property  in this paper we propose a new decompositionmethod benefiting from both semantic properties of i functional iconstraints not i bijective i constraints and structuralproperties of the network furthermore not all the constraints needto be functional  we show that under some conditions the existenceof solutions can be guaranteed  we first characterize a particularsubset of the variables which we name a i root seti   we thenintroduce i pivot consistency i a new local consistency which is aweak form of path consistency and can be achieved in on2d2complexity instead of on3d3 for path consistency and wepresent associated properties in particular we show that anyconsistent instantiation of the root set can be linearly extended to a solution which leads to the presentation of the aforementioned new method for solving by decomposing functional csps



a  schaerf  y  shoham and  m  tennenholtz 1995 adaptive load balancing a study in multiagent learning volume 2 pages 475500

we study the process of multiagent reinforcement learning in the context ofload balancing in a distributed system without use of either centralcoordination or explicit communication  we first define a precise frameworkin which to study adaptive load balancing important features of which are itsstochastic nature and the purely local information available to individualagents  given this framework we show illuminating results on the interplaybetween basic adaptive behavior parameters and their effect on systemefficiency  we then investigate the properties of adaptive load balancing inheterogeneous populations and address the issue of exploration vsexploitation in that context  finally we show that naive use ofcommunication may not improve and might even harm system efficiency



w  w cohen 1995 paclearning recursive logic programs efficient algorithms volume 2 pages 501539

we present algorithms that learn certain classes of   functionfree recursive logic programs in polynomial time from   equivalence queries  in particular we show that a single kary   recursive constantdepth determinate clause is learnable twoclause   programs consisting of one learnable recursive clause and one   constantdepth determinate nonrecursive clause are also learnable if   an additional basecase oracle is assumed  these results   immediately imply the paclearnability of these classes  although   these classes of learnable recursive programs are very constrained it   is shown in a companion paper that they are maximally general in that   generalizing either class in any natural way leads to a   computationally difficult learning problem  thus taken together with   its companion paper this paper establishes a boundary of efficient   learnability for recursive logic programs



w  w cohen 1995 paclearning recursive logic programs negative results volume 2 pages 541573

in a companion paper it was shown that the class of constantdepth determinate kary recursive clauses is efficiently learnable in this paper we present negative results showing that any natural generalization of this class is hard to learn in valiants model of paclearnability in particular we show that the following program classes are cryptographically hard to learn programs with an unbounded number of constantdepth linear recursive clauses programs with one constantdepth determinate clause containing an unbounded number of recursive calls and programs with one linear recursive clause of constant locality these results immediately imply the nonlearnability of any more general class of programs we also show that learning a constantdepth determinate program with either two linear recursive clauses or one linear recursive clause and one nonrecursive clause is as hard as learning boolean dnf together with positive results from the companion paper these negative results establish a boundary of efficient learnability for recursive functionfree clauses 



s  j russell and  d  subramanian 1995 provably boundedoptimal agents volume 2 pages 575609

since its inception artificial intelligence has relied  upon a theoretical foundation centered around i perfect rationality i as   the desired property of intelligent systems we argue as others have   done that this foundation is inadequate because it imposes   fundamentally unsatisfiable requirements as a result there has   arisen a wide gap between theory and practice in ai hindering   progress in the field we propose instead a property called i bounded   optimalityi roughly speaking an agent is boundedoptimal if its   program is a solution to the constrained optimization problem   presented by its architecture and the task environment we show how to   construct agents with this property for a simple class of machine   architectures in a broad class of realtime environments we   illustrate these results using a simple model of an automated mail   sorting facility  we also define a weaker property i asymptotic   bounded optimalityi abo that generalizes the notion of optimality in   classical complexity theory  we then construct i universal i abo   programs ie programs that are abo no matter what realtime   constraints are applied  universal abo programs can be used as   building blocks for more complex systems we conclude with a   discussion of the prospects for bounded optimality as a theoretical   basis for ai and relate it to similar trends in philosophy   economics and game theory
