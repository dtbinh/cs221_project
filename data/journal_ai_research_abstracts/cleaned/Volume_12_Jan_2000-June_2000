s  kambhampati 2000 planning graph as a dynamic csp exploiting ebl ddb and other csp search techniques in graphplan volume 12 pages 134

this paper reviews the connections between graphplans    planninggraph and the dynamic constraint satisfaction problem and    motivates the need for adapting csp search techniques to the graphplan    algorithm  it then describes how explanation based learning    dependency directed backtracking dynamic variable ordering forward    checking sticky values and randomrestart search strategies can be    adapted to graphplan empirical results are provided to demonstrate    that these augmentations improve graphplans performance significantly    up to 1000x speedups on several benchmark problems  special    attention is paid to the explanationbased learning and dependency    directed backtracking techniques as they are empirically found to be    most useful in improving the performance of graphplan



f  barber 2000 reasoning on interval and pointbased disjunctive metric constraints in temporal contexts volume 12 pages 3586

we introduce a temporal model for reasoning on disjunctive    metric constraints on intervals and time points in temporal    contexts this temporal model is composed of a labeled temporal    algebra and its reasoning algorithms the labeled temporal algebra    defines labeled disjunctive metric pointbased constraints where each    disjunct in each input disjunctive constraint is univocally associated    to a label reasoning algorithms manage labeled constraints    associated label lists and sets of mutually inconsistent    disjuncts these algorithms guarantee consistency and obtain a minimal    network additionally constraints can be organized in a hierarchy of    alternative temporal contexts therefore we can reason on    contextdependent disjunctive metric constraints on intervals and    points moreover the model is able to represent nonbinary    constraints such that logical dependencies on disjuncts in    constraints can be handled  the computational cost of reasoning    algorithms is exponential in accordance with the underlying problem    complexity although some improvements are proposed



r  m neal 2000 on deducing conditional independence from dseparation in causal graphs with feedback research note volume 12 pages 8791

pearl and dechter 1996 claimed that the dseparation    criterion for conditional independence in acyclic causal networks also    applies to networks of discrete variables that have feedback cycles    provided that the variables of the system are uniquely determined by    the random disturbances  i show by example that this is not true in    general  some condition stronger than uniqueness is needed such as    the existence of a causal dynamics guaranteed to lead to the unique    solution



k  xu and  w  li 2000 exact phase transitions in random constraint satisfaction problems volume 12 pages 93103

in this paper we propose a new type of random csp model    called model rb which is a revision to the standard model b it is    proved that phase transitions from a region where almost all problems    are satisfiable to a region where almost all problems are    unsatisfiable do exist for model rb as the number of variables    approaches infinity  moreover the critical values at which the phase    transitions occur are also known exactly by relating the hardness of    model rb to model b it is shown that there exist a lot of hard    instances in model rb



g  a kaminka and  m  tambe 2000 robust agent teams via sociallyattentive monitoring volume 12 pages 105147

agents in dynamic multiagent environments must monitor    their peers to execute individual and group plans a key open question    is how much monitoring of other agents states is required to be    effective the monitoring selectivity problem  we investigate this    question in the context of detecting failures in teams of cooperating    agents via sociallyattentive monitoring which focuses on monitoring    for failures in the social relationships between the agents we    empirically and analytically explore a family of sociallyattentive    teamwork monitoring algorithms in two dynamic complex multiagent    domains under varying conditions of task distribution and    uncertainty we show that a centralized scheme using a complex    algorithm trades correctness for completeness and requires monitoring    all teammates in contrast a simple distributed teamwork monitoring    algorithm results in correct and complete detection of teamwork    failures despite relying on limited uncertain knowledge and    monitoring only key agents in a team  in addition we report on the    design of a sociallyattentive monitoring system and demonstrate its    generality in monitoring several coordination relationships    diagnosing detected failures and both online and offline applications



j  baxter 2000 a model of inductive bias learning volume 12 pages 149198

a major problem in machine learning is that of inductive    bias how to choose a learners hypothesis space so that it is large    enough to contain a solution to the problem being learnt yet small    enough to ensure reliable generalization from reasonablysized    training sets  typically such bias is supplied by hand through the    skill and insights of experts in this paper a model for automatically    learning bias is investigated the central assumption of the model is    that the learner is embedded within an environment of related learning    tasks within such an environment the learner can sample from multiple    tasks and hence it can search for a hypothesis space that contains    good solutions to many of the problems in the environment under    certain restrictions on the set of all hypothesis spaces available to    the learner we show that a hypothesis space that performs well on a    sufficiently large number of training tasks will also perform well    when learning novel tasks in the same environment  explicit bounds    are also derived demonstrating that learning multiple tasks within an    environment of related tasks can potentially give much better    generalization than learning a single task



s  tobies 2000 the complexity of reasoning with cardinality restrictions and nominals in expressive description logics volume 12 pages 199217

we study the complexity of the combination of the    description logics alcq and alcqi with a terminological formalism    based on cardinality restrictions on concepts these combinations can    naturally be embedded into c2 the two variable fragment of predicate    logic with counting quantifiers which yields decidability in    nexptime we show that this approach leads to an optimal solution for    alcqi as alcqi with cardinality restrictions has the same complexity    as c2 nexptimecomplete in contrast we show that for alcq the    problem can be solved in exptime this result is obtained by a    reduction of reasoning with cardinality restrictions to reasoning with    the in general weaker terminological formalism of general axioms for    alcq extended with nominals using the same reduction we show that    for the extension of alcqi with nominals reasoning with general    axioms is a nexptimecomplete problem finally we sharpen this result    and show that pure concept satisfiability for alcqi with nominals is    nexptimecomplete without nominals this problem is known to be    pspacecomplete



a  becker  r  baryehuda and  d  geiger 2000 randomized algorithms for the loop cutset problem volume 12 pages 219234

we show how to find a minimum weight loop cutset in a    bayesian network with high probability finding such a loop cutset is    the first step in the method of conditioning for inference  our    randomized algorithm for finding a loop cutset outputs a minimum loop    cutset after oc 6k kn steps with probability at least     1  1  16kc6k where c  1 is a constant specified by the    user k is the minimal size of a minimum weight loop cutset and n is    the number of vertices  we also show empirically that a variant of    this algorithm often finds a loop cutset that is closer to the minimum    weight loop cutset than the ones found by the best deterministic    algorithms known



j  singer  i  p gent and  a  smaill 2000 backbone fragility and the local search cost peak volume 12 pages 235270

the local search algorithm wsat is one of the most    successful algorithms for solving the satisfiability sat problem it    is notably effective at solving hard random 3sat instances near the    socalled satisfiability threshold but still shows a peak in search    cost near the threshold and large variations in cost over different    instances we make a number of significant contributions to the    analysis of wsat on highcost random instances using the    recentlyintroduced concept of the backbone of a sat instance the    backbone is the set of literals which are entailed by an instance we    find that the number of solutions predicts the cost well for    smallbackbone instances but is much less relevant for the    largebackbone instances which appear near the threshold and dominate    in the overconstrained region we show a very strong correlation    between search cost and the hamming distance to the nearest solution    early in wsats search this pattern leads us to introduce a measure    of the backbone fragility of an instance which indicates how    persistent the backbone is as clauses are removed we propose that    highcost random instances for local search are those with very large    backbones which are also backbonefragile we suggest that the decay    in cost beyond the satisfiability threshold is due to increasing    backbone robustness the opposite of backbone fragility our    hypothesis makes three correct predictions first that the backbone    robustness of an instance is negatively correlated with the local    search cost when other factors are controlled for second that    backboneminimal instances which are 3sat instances altered so as to    be more backbonefragile are unusually hard for wsat third that the    clauses most often unsatisfied during search are those whose deletion    has the most effect on the backbone in understanding the pathologies    of local search methods we hope to contribute to the development of    new and better techniques



b  nebel 2000 on the compilability and expressive power of propositional planning formalisms volume 12 pages 271315

the recent approaches of extending the graphplan algorithm    to handle more expressive planning formalisms raise the question of    what the formal meaning of expressive power is we formalize the    intuition that expressive power is a measure of how concisely planning    domains and plans can be expressed in a particular formalism by    introducing the notion of compilation schemes between planning    formalisms  using this notion we analyze the expressiveness of a    large family of propositional planning formalisms ranging from basic    strips to a formalism with conditional effects partial state    specifications and propositional formulae in the preconditions  one    of the results is that conditional effects cannot be compiled away if    plan size should grow only linearly but can be compiled away if we    allow for polynomial growth of the resulting plans this result    confirms that the recently proposed extensions to the graphplan    algorithm concerning conditional effects are optimal with respect to    the compilability framework  another result is that general    propositional formulae cannot be compiled into conditional effects if    the plan size should be preserved linearly  this implies that    allowing general propositional formulae in preconditions and effect    conditions adds another level of difficulty in generating a plan



j  y halpern 2000 axiomatizing causal reasoning volume 12 pages 317337

causal models defined in terms of a collection of equations    as defined by pearl are axiomatized here axiomatizations are    provided for three successively more general classes of causal models    1 the class of recursive theories those without feedback 2 the    class of theories where the solutions to the equations are unique     3 arbitrary theories where the equations may not have solutions and if    they do they are not necessarily unique it is shown that to reason    about causality in the most general third class we must extend the    language used by galles and pearl in addition the complexity of the    decision procedures is characterized for all the languages and classes    of models considered



j  koehler and  j  hoffmann 2000 on reasonable and forced goal orderings and their use in an agendadriven planning algorithm volume 12 pages 338386

the paper addresses the problem of computing goal orderings    which is one of the longstanding issues in ai planning  it makes two    new contributions  first it formally defines and discusses two    different goal orderings which are called the reasonable and the    forced ordering both orderings are defined for simple strips    operators as well as for more complex adl operators supporting    negation and conditional effects the complexity of these orderings is    investigated and their practical relevance is discussed secondly two    different methods to compute reasonable goal orderings are developed    one of them is based on planning graphs while the other investigates    the set of actions directly finally it is shown how the ordering    relations which have been derived for a given set of goals g can be    used to compute a socalled goal agenda that divides g into an ordered    set of subgoals any planner can then in principle use the goal    agenda to plan for increasing sets of subgoals  this can lead to an    exponential complexity reduction as the solution to a complex    planning problem is found by solving easier subproblems since only a    polynomial overhead is caused by the goal agenda computation a    potential exists to dramatically speed up planning algorithms as we    demonstrate in the empirical evaluation where we use this method in    the	ipp planner



m  a walker 2000 an application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email volume 12 pages 387416

this paper describes a novel method by which a spoken    dialogue system can learn to choose an optimal dialogue strategy from    its experience interacting with human users  the method is based on a    combination of reinforcement learning and performance modeling of    spoken dialogue systems  the reinforcement learning component applies    qlearning watkins 1989 while the performance modeling component    applies the paradise evaluation framework walker et al 1997 to    learn the performance function reward used in reinforcement    learning  we illustrate the method with a spoken dialogue system    named elvis email voice interactive system that supports access to    email over the phone  we conduct a set of experiments for training an    optimal dialogue strategy on a corpus of 219 dialogues in which human    users interact with elvis over the phone we then test that strategy    on a corpus of 18 dialogues  we show that elvis can learn to optimize    its strategy selection for agent initiative for reading messages and    for summarizing email folders
